{"id": "1608.07775", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2016", "title": "Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content", "abstract": "Multimedia or spoken content is more attractive information than plain text content, but the former is more difficult to display on screen and select by the user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. Therefore, it is highly attractive to develop machines that can automatically understand spoken content and summarize the key information that humans can browse through. In this endeavor, a new task has recently been proposed, namely, the machine understanding of spoken content. The original goal was defined as a listening comprehension test by TOEFL, a demanding academic English test for non-native English learners. For this task, an attention-based Multi-Hop Recurrent Neural Network (AMRNN) architecture has also been proposed, which only takes into account the sequential relationship within speech expressions. In this essay, we propose a new hierarchical attention model (HAM) that includes multiple attention mechanisms for structured gut expressions.", "histories": [["v1", "Sun, 28 Aug 2016 06:48:14 GMT  (302kb,D)", "https://arxiv.org/abs/1608.07775v1", null], ["v2", "Sat, 1 Oct 2016 03:19:40 GMT  (302kb,D)", "http://arxiv.org/abs/1608.07775v2", "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken Language Technology (SLT 2016)"], ["v3", "Sun, 1 Jan 2017 12:17:13 GMT  (302kb,D)", "http://arxiv.org/abs/1608.07775v3", "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken Language Technology (SLT 2016)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wei fang", "jui-yang hsu", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1608.07775"}, "pdf": {"name": "1608.07775.pdf", "metadata": {"source": "CRF", "title": "HIERARCHICAL ATTENTION MODEL FOR IMPROVED MACHINE COMPREHENSION OF SPOKEN CONTENT", "authors": ["Wei Fang", "Jui-Yang Hsu", "Hung-yi Lee", "Lin-Shan Lee"], "emails": ["b02901054@ntu.edu.tw,", "b02901085@ntu.edu.tw,", "hungyilee@ntu.edu.tw,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Index Terms\u2014 spoken question answering, TOEFL, deep learning, attention model"}, {"heading": "1. INTRODUCTION", "text": "With the popularity of shared videos, social networks, online courses, etc., the quantity of multimedia or spoken content is growing much faster beyond what human beings can view or listen to. Accessing large collections of multimedia or spoken content is difficult and time-consuming for humans, even if these materials are more attractive for humans than plain text information. Hence, it is desirable that machines can automatically listen to and understand the spoken content, and extract or even visualize the key information for humans. An initial attempt towards the above goal of machine comprehension of spoken content was presented recently in an initial task [1],\nin which the machine is given an audio story, and required to answer the questions related to that audio story, as an evaluation regarding how well the machine comprehends the audio story. TOEFL listening comprehension test is such a task but for human English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework for the above TOEFL task was proposed, and encouraging initial results were reported recently [1]. This paper presents an improved framework with better results over the same task.\nThe listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [2, 3]. In SQA, when the users enter questions in either text or spoken form, the machine needs to find the answer from some audio files. SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [4] or relied on knowledge bases [5] to find the proper answer. Sibyl [6] is a factoid SQA system, while Question Answering in Speech Transcripts (QAST) [7] has been a wellknown evaluation program for years. However, most previous works on SQA mainly focused on factoid question answering\nar X\niv :1\n60 8.\n07 77\n5v 3\n[ cs\n.C L\n] 1\nJ an\n2 01\nlike \u201cWhat is the name of the highest mountain in Taiwan?\u201d Sometimes this kind of questions may be correctly answered by simply extracting the key terms from a properly chosen utterance without understanding the given spoken content. More difficult questions that cannot be answered without understanding the whole spoken content seemed rarely dealt with previously. On the other hand, most Question Answering works focused on understanding text documents [8\u201311]. Even though MovieQA [12] tried to answer questions related to movies, they only used the text and images in the movies. Machine comprehension of spoken content thus remains to be a less investigated problem.\nOn the other hand, neural models have been extensively explored for question answering tasks [13\u201316]. Specifically, reasoning systems incorporating memory and attention mechanisms such as the Memory Network (MemNN) [17] were shown to be very successful, and the End-to-end Memory Network (MemN2N) [18], a variant of MemNN, can be trained end-to-end without labeled supporting facts. In these models, the sentences were encoded by bag-of-words (BoW) representations, without considering the word order. The Attention-based Multi-hop Recurrent Neural Network (AMRNN) mentioned above in the previous work [1] utilizes the attention mechanism with recurrent neural networks (RNN) [19] to construct sentence representations considering the word order, but didn\u2019t take the syntactic structure of sentences into account yet. Recently, tree-structured models [20] obtained from the syntactic structures of the sentences were shown to be able to produce more robust representations and capture better semantics in certain tasks. But they have not been applied on question answering tasks with attention mechanism."}, {"heading": "2. TASK DESCRIPTION AND CONTRIBUTIONS", "text": "In this paper, we take the TOEFL listening comprehension test as the corpus for experiments [1]. TOEFL tests knowledge and skills of academic English for global English learners whose native languages are not English. Each example problem set consists of an audio story and a question with four answer choices. Among these choices, one or two of them are correct. An example is shown in Fig. 1. The machine has to select the correct answer(s) out of the four choices. The questions here are not very easy because the answer cannot be found by simply matching the question and the choices without understanding the story. For example, there are questions regarding the gist of the story or the conclusion for the conversation. So this is a relatively challenging task for stateof-the-art spoken language understanding technologies, and the proposed approaches should be general enough to tackle different question types.\nIn this paper, we propose a Hierarchical Attention Model (HAM) to construct tree-structured sentence representations for sentences from their parsing trees and estimate attention\nweights on different nodes of the hierarchies. This model is evaluated on the above TOEFL listening comprehension test. The experiments showed improved performance over existing baselines including the previous work using AMRNN [1].\n3. HIERARCHICAL ATTENTION MODEL (HAM)\nThe proposed Hierarchical Attention Model (HAM) is shown in Fig. 2 in the form matched to the TOEFL task. In this model, tree-structured long short-term memory networks (Tree-LSTM, small blue blocks in Fig. 2) is used to obtain the representations for the sentences and phrases in the audio stories, questions and choices. The detailed operation of a Tree-LSTM is shown in Fig. 3 and explained below. The story module on the top of Fig. 2 computes tree-structured representations for the sentences in the transcriptions of the input audio story using Tree-LSTMs. The question module on the middle left generates a question vector representation from the word sequence of the question. The memory module in the middle includes attention modules (small green blocks) to draw question-related attention weights for different nodes of the sentences in the story. The detailed operation of the attention module is in Fig. 4 and explained below. Finally, the answer module at the bottom evaluates the confidence scores of the answer choices and generates the answer. The details of the different components of the model are given below."}, {"heading": "3.1. Tree-LSTM", "text": "Two variants of Tree-LSTM can be used: the Child-Sum TreeLSTM and the N-ary Tree-LSTM. A Child-Sum Tree-LSTM over a dependency tree is referred to as a Dependency TreeLSTM, which is used here due to its relatively compact structure. The dependency tree structure of a sentence \u201cThe conventions may vary\u201d is shown on the left part of Fig. 3, in which each node corresponds to a word (head word of the node) in the sentence.\nTree-LSTM generates a vector representation for each node in the dependency tree1 based on the vector representations of its child nodes as in the right part of Fig. 3. Each node j has a hidden state hj as its vector representation and a set of memory cells cj . Just as the standard LSTM [22], it has the input gates ij and output gates oj for the memory cells, and the forget gates fjk that controls the information flowing in from its child node k. It also takes an input xj , which is the vector representation of the head word of the node.\nThe hidden state hj at node j is the representation for a phrase consisting of words in the subtree rooted at node j. Below is how hj of a node j is obtained from its child nodes k. First, the hidden states of the child nodes are summed,\nh\u0303j = \u2211\nk\u2208C(j) hk, (1)\nwhere C(j) denotes the set of children of node j. The representation of the head word xj of node j and h\u0303j in (1) are used to control the input gates ij and output gates oj ,\nij = \u03c3 ( W (i)xj + U (i)h\u0303j + b (i) ) , (2) oj = \u03c3 ( W (o)xj + U (o)h\u0303j + b (o) ) . (3)\nEach child k inC(j) has its own forget gate fjk obtained from hk and xj ,\nfjk = \u03c3 ( W (f)xj + U (f)hk + b (f) ) . (4)\nFinally, the hidden state hj and memory cells cj are obtained as the following:\nuj = tanh ( W (u)xj + U (u)h\u0303j + b (u) ) , (5) cj = ij \u2217 uj + \u2211\nk\u2208C(j) fjk \u2217 ck, (6)\nhj = oj \u2217 tanh(cj), (7) where \u2217 denotes elementwise multiplication.\n1Dependency parses produced by the Stanford Neural Network Dependency Parser [21]."}, {"heading": "3.2. Story and Question Modules", "text": "The story module and the question module produce representations for the sentences respectively in the story and in the question with the Tree-LSTM as explained in the subsection 3.1. For story module, the hidden vectors for all nodes in the tree structures of each sentence in the story are stored for future use. On the other hand, the question module produces the hidden states of the root nodes, VSi , of the Tree-LSTMs for the sentences Si in the question 2. The question vector VQ is the sum of VSi for all Si in a question, to be used below."}, {"heading": "3.3. Memory Module", "text": "The memory module aims to extract the information in the story relevant to the question VQ based on representations obtained from the story module. It consists of two components: the attention mechanism and the multi-hopping.\nAttention Mechanism\nThe attention mechanism is shown in Fig. 4. Let O = {o1, o2, ..., oT } be the set of vector representations for the story. There are two different ways to obtain these vectors:\n\u2022 Phrase-level: O = {o1, o2, ..., oT }, where each ot is the hidden state of a node in the Tree-LSTM of the sentences, or each ot represents a phrase. So T is much larger than the number of sentences in the story. This is shown in the lower part of Fig. 4.\n\u2022 Sentence-level: O = {o1, o2, ..., oT }, where each ot is the hidden state of the root node of the Tree-LSTM over a sentence in the story, or each ot represents a sentence. So T is equal to the number of sentences.\nThe vectors in the setO are first transformed into memory vectors M = {m1,m2, ...,mT } and evidence vectors C = {c1, c2, ..., cT } by the embedding matrices W (m) and W (c),\nmt =W (m)ot, ct =W (c)ot. (8)\nThe question vector VQ obtained in the question module is also transformed into an initial query vector q0 by an embedding matrix W (q).\nq0 =W (q)VQ. (9)\nCosine similarity is used to compute the attention score \u03b7t between the query vector q0 and each memory vector mt, which is further normalized by a SoftMax function, as shown in the upper left part of Fig. 4, to give the attention weights \u03b1 = (\u03b11, \u03b12, ..., \u03b1T ),\n\u03b7t = q0 mt, \u03b1t = e\u03b7t\n\u2211T i=1 e \u03b7i , (10)\n2A question may have multiple sentences.\nwhere denotes cosine similarity. Each attention weight \u03b1t corresponds to a memory vector mt and an evidence vector ct that represent a phrase or a sentence. The story vector s0 is then the weighted sum of the evidence vectors ct with the attention as weights, as in the upper right part of Fig. 4,\ns0 = T\u2211\nt=1\n\u03b1tct. (11)\nMulti-hopping\nAs shown in Fig. 2, the sum of the initial query vector q0 and the story vector s0 obtained in (11) can be used as a new query vector q1 to compute a new set of attention weights in exactly same way, and subsequently a new story vector s1 can be obtained. This process can be repeated, allowing better focusing in the key information. The output of the final hop qn is the memory module output."}, {"heading": "3.4. Answer Module", "text": "Suppose there are K answer choices with N correct answers. The answer module encodes the answer choices into choice vectors VCi . Cosine similarities between the memory module output qn and the choice vectors VCi , after passing through a SoftMax layer, give the predicted choice distribution p\u0302. The target distribution p is defined as:\npi = { 1 N , if choice i is a correct answer 0, otherwise\n(12)\nfor 1 \u2264 i \u2264 K. The KL-divergence between p and p\u0302 is taken as the cost function for training. The choices with topN scores are selected for a question with N answers."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "The TOEFL listening comprehension test dataset included 963 problems in total, with a train/dev/test split of 717/124/122. There were two versions of each story, manual and ASR\ntranscriptions. The latter was obtained using CMU Sphinx recognizer [23] with a word error rate of 34.32%. The size of the hidden layer for Tree-LSTM [20] and the embedding size of the memory module were both 75. AdaGrad [24] was used with an initial learning rate of 0.002."}, {"heading": "4.2. Baselines", "text": "We compared the proposed model with several baselines as summarized below. The first two were trivial baselines, while the rest were neural network based approaches. In the trivial baselines, we used pre-trained GloVe [25] vectors to obtain the vector representation for each word. Hence, each utterance in the stories, questions and choices could be represented as a fixed-length vector by averaging the word vectors. Cosine distance between vector representations was used to evaluate the similarity between two sentences.\n(a) Question/choice similarity [1]: With the vector representations for the choices/question mentioned above, the choice most similar to the question was selected.\n(b) Sliding window of utterances [12, 26]: We slid a window of 5 utterances over the story, and chose the window the most similar to the question as the related information in the story. The choice with the highest similarity to this window was then selected.\n(c) Deep LSTM Reader (DLR) [16]: We fed the story followed by the question into a Deep LSTM encoder to obtain the representation of each story/question pair. The choices were encoded with the LSTM encoder, and the one whose vector representation with the highest similarity to the story/question pair was selected. Bidirectional LSTM network with hidden layer size of 75 was used as the encoder. All weights in the LSTM units were shared.\n(d) End-to-end Memory Network (MemN2N) [18]: We slightly modified the original MemN2N to adapt to this task. An embedding matrix was used to embed each choice. We then evaluated the similarity between the output of the last hop and the choice embeddings, and chose the highest one as the answer. The embedding size of the\nmemory network was 300. The hop size was tuned from 1 to 3 with the development set.\n(e) Attention-based Multi-hop Recurrent Neural Network (AMRNN) [1]: This is the approach used by the previous work. 1-of-N encoding for each word in the question and the choices were entered to a bidirectional GRU network to obtain the vector representations for the question, VQ, and the choices. The vector representation for each word in the story was entered to a bidirectional GRU network to obtain semantic embedding of each word. The cosine similarity between each of them and VQ was taken as the attention weight. Using these weights, the vector VS representing the story/question pair using word/sentence-level attention was computed. Finally, the choice whose vector had the highest similarity score to VS is selected. The hidden layer size for both the forward and backward GRU networks were 128. The number of hop was tuned from 1 to 3 with the development set.\n(f) Tree-LSTM [20]: Similar to the proposed HAM but without attention. We fed the story/question, one sentence at a time, into a Tree-LSTM encoder as described in Subsection 3.1, then summed the vectors for the sentences in both the story and the query to obtain the vector representation of each pair. The choices were encoded similarly. The selection was based on the highest similarity to the story/question pair. The size of hidden layer for Tree-LSTM was 50."}, {"heading": "4.3. Results", "text": "We used the accuracy (percentage of questions answered correctly) as our evaluation metric. The models were trained on manual transcriptions of the stories and questions/answers of the training set and tested on the manual (column labeled \u201cManual\u201d ) and ASR transcriptions (column labeled \u201cASR\u201d) of the testing set. The results are in Table 1. The upper section (a) and (b) are trivial baselines; the middle section (c)-(f) are neural network based baselines; while the lower section (g1)-(g4) are for the proposed HAM with different hops and attention levels. Due to the non-negligible performance variations for all the neural network based models due to random initialization, for fair comparison, we reported the mean accuracies and standard deviations (in parentheses) over 10 runs.\nThe relatively low accuracies for the trivial baselines in rows (a) and (b) indicated that it is hard to answer correctly without understanding the story, and the information within a short window was inadequate.\nThe Deep LSTM Reader in row (c), successful in certain QA tasks, was also low (32.4% and 34.3%), similar to the sliding window in row (b). So simply encoding the story and question with a LSTM encoder was not adequate.\nThe MemN2N in row (d), however, was much better (45.2% and 44.4%). This verified the attention over the stories actually extracted better representations for the stories.\nThe AMRNN in row (e) combined the attention mechanism with the long distance information accumulation in recurrent networks offered reasonably good results on both word-level (42.5% and 40.1%) and sentence-level (42.4% and 42.2%) attention, but slightly lower than MemN2N in row (d). This implied that sequential representations for stories were inadequate even with attention mechanism.3\nThe Tree-LSTM in row (f) exploiting the hidden structures in the sentences but without attention performed relatively well (46.5% and 44.9%), better than MemN2N, although possibly confused by the irrelevant sentences in the stories. This revealed that encoding the hierarchical structures of the sentences was helpful in understanding them.\nThe approach HAM proposed in this paper is in part (g), respectively for phrase/sentence-level attention with 1 and 2 hops, all much higher than all baselines above. The 1-hop sentence-level attention model achieved the highest mean accuracy of 49.1% on the manual transcriptions, significantly higher than all baselines, while the 2-hop phrase-level attention model achieved the highest mean accuracy of 48.8% on ASR results, only slightly lower than the former. We also observed that increased hops improved the performance for phrase-level attention, but not for sentence-level attention. This is probably because for phrase-level reasoning, the model first selected the key phrases in the first hop and then changed its attention based on these key-phrases on the second hop. For sentence-level reasoning, only a few key sentences were selected in the first hop, while more hops were not able to find additional key sentences.\n3The previous work [1] showed that AMRNN outperformed MemN2N in which 10 models with random initialization were trained, and the best on the development set was used for testing."}, {"heading": "4.4. Analysis", "text": "Fig. 5 shows an example for the phrases (manual transcription) in the story for the nodes with top 3 phrase-level attentions for hop 1 and 2, respectively. The story was about how the climate system strikes a balance between cooling and heating. The question is \u201cWhat is the radiation budget?\u201d, and the correct choice is \u201cThe balance between incoming and reflecting solar energy\u201d. We can see how the model selected in hop 1 the key phrases related to the definition of radiation budget, such as sunlight, transmission and reflection. In hop 2, some other longer phrases including the abstract definition of radiation budget were selected. Fig. 6 is the same except for word chunks for nodes with top 3 sentence-level attentions. We see the selected sentences (word chunks) didn\u2019t change much in the hop 2, but the weights for the selected sentences became higher, which may have incorrectly selected sentences and further emphasized them. This may explain why for sentence-level attention hop 2 gave a slightly lower accuracy.\nWe further analyzed the performance of several models for different types of questions. The TOEFL questions can be divided into 3 categories [27, p.123-p.153]. Type1 questions are for basic comprehension of the story. Type2 questions\ngo beyond basic comprehension, but test the understanding of the functions of utterances or the attitude the speaker expresses. Type3 questions further require the ability of making connections between different parts of the story, making inferences, drawing conclusions, or forming generalizations. We labeled the question types manually on the dataset, resulting in 81 Type1 questions, 15 Type2 questions, and 26 Type3 questions. The results for the different models for the different types of questions are in Fig. 7. We see Type3 questions received the highest scores across all models, suggesting that these neural network based models performed better on relatively difficult reasoning questions than more factoid questions. Also, the proposed HAM outperformed other baselines on Types 1 and 3 questions, verifying that hierarchical attention is useful. On the other hand, the proposed HAM has the lowest accuracies on Type2 questions, probably because they are the most difficult for neural network based models.\n5. CONCLUSIONS\nIn this work we presented a Hierarchical Attention Model (HAM) over tree-structured sentence representations, and showed it offered improved performance for machine comprehension of spoken content, based on the TOEFL listening comprehension task. Compared to other neural network based models, the proposed model utilizes multi-hopped attention over tree-structured rather than sequential representations, so information of multiple granularity can be better extracted. This approach is robust with respect to ASR errors, since performance with 34.32% of ASR word error rate was found to be almost the same as those with reference transcriptions.\n6. ACKNOWLEDGEMENTS\nWe thank Bo-Hsiang Tseng for assistance with providing us the TOEFL listening comprehension dataset and the detailed statistics of AMRNN [1], and Juei-Yu Chang, An Huang, YuAn Chen and Ping-Hsuan Tsai for labeling the 3 different types of questions in the testing set."}, {"heading": "7. REFERENCES", "text": "[1] B.-H. Tseng, S.-S. Shen, H.-Y. Lee, and L.-S. Lee, \u201cTowards machine learning comprehension of spoken content: Initial toefl listening comprehension test by machine,\u201d in INTERSPEECH, 2016.\n[2] P. R. C. i Umbert, J. Turmo Borra\u0300s, and L. M. Villodre, \u201cSpoken question answering,\u201d .\n[3] P. R. C. i Umbert, Factoid question answering for spoken documents, Ph.D. thesis, Universitat Polite\u0300cnica de Catalunya, 2012.\n[4] S.-R. Shiang, H.-Y. Lee, and L.-S. Lee, \u201cSpoken question answering using tree-structured conditional random fields and two-layer random walk.,\u201d in INTERSPEECH, 2014, pp. 263\u2013267.\n[5] B. Hixon, P. Clark, and H. Hajishirzi, \u201cLearning knowledge graphs for question answering through conversational dialog,\u201d in Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, 2015.\n[6] P. R. Comas, J. Turmo, and L. Ma\u0300rquez, \u201cSibyl, a factoid question-answering system for spoken documents,\u201d ACM Transactions on Information Systems (TOIS), vol. 30, no. 3, pp. 19, 2012.\n[7] J. Turmo, P. R. Comas, S. Rosset, L. Lamel, N. Moreau, and D. Mostefa, \u201cOverview of qast 2008,\u201d in Workshop of the Cross-Language Evaluation Forum for European Languages. Springer, 2008, pp. 314\u2013324.\n[8] A. Bordes, S. Chopra, and J. Weston, \u201cQuestion answering with subgraph embeddings,\u201d arXiv preprint arXiv:1406.3676, 2014.\n[9] N. P. Er and I. Cicekli, \u201cA factoid question answering system using answer pattern matching.,\u201d in IJCNLP, 2013, pp. 854\u2013858.\n[10] M. Iyyer, J. L. Boyd-Graber, L. M. B. Claudino, R. Socher, and H. Daume\u0301 III, \u201cA neural network for factoid question answering over paragraphs.,\u201d in EMNLP, 2014, pp. 633\u2013644.\n[11] A. Fader, L. Zettlemoyer, and O. Etzioni, \u201cOpen question answering over curated and extracted knowledge bases,\u201d in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 1156\u20131165.\n[12] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler, \u201cMovieqa: Understanding stories in movies through question-answering,\u201d CoRR, vol. abs/1512.02902, 2015.\n[13] A. Bordes, N. Usunier, S. Chopra, and J. Weston, \u201cLarge-scale simple question answering with memory networks,\u201d CoRR, vol. abs/1506.02075, 2015.\n[14] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska, I. Gulrajani, and R. Socher, \u201cAsk me anything: Dynamic memory networks for natural language processing,\u201d CoRR, vol. abs/1506.07285, 2015.\n[15] C. Xiong, S. Merity, and R. Socher, \u201cDynamic memory networks for visual and textual question answering,\u201d CoRR, vol. abs/1603.01417, 2016.\n[16] K. M. Hermann, T. Kocisky\u0301, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, \u201cTeaching machines to read and comprehend,\u201d CoRR, vol. abs/1506.03340, 2015.\n[17] J. Weston, S. Chopra, and A. Bordes, \u201cMemory networks,\u201d CoRR, vol. abs/1410.3916, 2014.\n[18] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, \u201cWeakly supervised memory networks,\u201d CoRR, vol. abs/1503.08895, 2015.\n[19] A. Graves, A.-R. Mohamed, and G. E. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d CoRR, vol. abs/1303.5778, 2013.\n[20] K. S. Tai, R. Socher, and C. D. Manning, \u201cImproved semantic representations from tree-structured long shortterm memory networks,\u201d CoRR, vol. abs/1503.00075, 2015.\n[21] D. Chen and C. D. Manning, \u201cA fast and accurate dependency parser using neural networks.,\u201d in EMNLP, 2014, pp. 740\u2013750.\n[22] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.\n[23] W. Walker, P. Lamere, P. Kwok, B. Raj, R. Singh, E. Gouvea, P. Wolf, and J. Woelfel, \u201cSphinx-4: A flexible open source framework for speech recognition,\u201d Tech. Rep., Mountain View, CA, USA, 2004.\n[24] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121\u20132159, 2011.\n[25] J. Pennington, R. Socher, and C. D. Manning, \u201cGlove: Global vectors for word representation.,\u201d EMNLP, vol. 14, pp. 1532\u201343, 2014.\n[26] M. Datar, A. Gionis, P. Indyk, and R. Motwani, \u201cMaintaining stream statistics over sliding windows,\u201d SIAM J. Comput., vol. 31, no. 6, pp. 1794\u20131813, June 2002.\n[27] The official guide to the TOEFL test / ETS, McGrawHill, New York, NY, 2012."}], "references": [{"title": "Towards machine learning comprehension of spoken content: Initial toefl listening comprehension test by machine", "author": ["B.-H. Tseng", "S.-S. Shen", "H.-Y. Lee", "L.-S. Lee"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Spoken question answering", "author": ["P.R.C. i Umbert", "J. Turmo Borr\u00e0s", "L.M. Villodre"], "venue": ".", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Factoid question answering for spoken documents", "author": ["P.R.C. i Umbert"], "venue": "Ph.D. thesis, Universitat Polite\u0300cnica de Catalunya,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Spoken question answering using tree-structured conditional random fields and two-layer random walk", "author": ["S.-R. Shiang", "H.-Y. Lee", "L.-S. Lee"], "venue": "INTERSPEECH, 2014, pp. 263\u2013267.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning knowledge graphs for question answering through conversational dialog", "author": ["B. Hixon", "P. Clark", "H. Hajishirzi"], "venue": "Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Sibyl, a factoid question-answering system for spoken documents", "author": ["P.R. Comas", "J. Turmo", "L. M\u00e0rquez"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 30, no. 3, pp. 19, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Overview of qast 2008", "author": ["J. Turmo", "P.R. Comas", "S. Rosset", "L. Lamel", "N. Moreau", "D. Mostefa"], "venue": "Workshop of the Cross-Language Evaluation Forum for European Languages. Springer, 2008, pp. 314\u2013324.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A factoid question answering system using answer pattern matching", "author": ["N.P. Er", "I. Cicekli"], "venue": "IJCNLP, 2013, pp. 854\u2013858.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J.L. Boyd-Graber", "L.M.B. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "EMNLP, 2014, pp. 633\u2013644.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 1156\u20131165.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CoRR, vol. abs/1512.02902, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "CoRR, vol. abs/1506.02075, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "CoRR, vol. abs/1506.07285, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "CoRR, vol. abs/1603.01417, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "CoRR, vol. abs/1506.03340, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "CoRR, vol. abs/1410.3916, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "CoRR, vol. abs/1503.08895, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G.E. Hinton"], "venue": "CoRR, vol. abs/1303.5778, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved semantic representations from tree-structured long shortterm memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "CoRR, vol. abs/1503.00075, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "EMNLP, 2014, pp. 740\u2013750.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Sphinx-4: A flexible open source framework for speech recognition", "author": ["W. Walker", "P. Lamere", "P. Kwok", "B. Raj", "R. Singh", "E. Gouvea", "P. Wolf", "J. Woelfel"], "venue": "Tech. Rep., Mountain View, CA, USA, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121\u20132159, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, vol. 14, pp. 1532\u201343, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Maintaining stream statistics over sliding windows", "author": ["M. Datar", "A. Gionis", "P. Indyk", "R. Motwani"], "venue": "SIAM J. Comput., vol. 31, no. 6, pp. 1794\u20131813, June 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1813}], "referenceMentions": [{"referenceID": 0, "context": "An initial attempt towards the above goal of machine comprehension of spoken content was presented recently in an initial task [1], Story", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "An Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework for the above TOEFL task was proposed, and encouraging initial results were reported recently [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "The listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [2, 3].", "startOffset": 102, "endOffset": 108}, {"referenceID": 2, "context": "The listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [2, 3].", "startOffset": 102, "endOffset": 108}, {"referenceID": 3, "context": "SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [4] or relied on knowledge bases [5] to find the proper answer.", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [4] or relied on knowledge bases [5] to find the proper answer.", "startOffset": 143, "endOffset": 146}, {"referenceID": 5, "context": "Sibyl [6] is a factoid SQA system, while Question Answering in Speech Transcripts (QAST) [7] has been a wellknown evaluation program for years.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "Sibyl [6] is a factoid SQA system, while Question Answering in Speech Transcripts (QAST) [7] has been a wellknown evaluation program for years.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "On the other hand, most Question Answering works focused on understanding text documents [8\u201311].", "startOffset": 89, "endOffset": 95}, {"referenceID": 8, "context": "On the other hand, most Question Answering works focused on understanding text documents [8\u201311].", "startOffset": 89, "endOffset": 95}, {"referenceID": 9, "context": "On the other hand, most Question Answering works focused on understanding text documents [8\u201311].", "startOffset": 89, "endOffset": 95}, {"referenceID": 10, "context": "On the other hand, most Question Answering works focused on understanding text documents [8\u201311].", "startOffset": 89, "endOffset": 95}, {"referenceID": 11, "context": "Even though MovieQA [12] tried to answer questions related to movies, they only used the text and images in the movies.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "On the other hand, neural models have been extensively explored for question answering tasks [13\u201316].", "startOffset": 93, "endOffset": 100}, {"referenceID": 13, "context": "On the other hand, neural models have been extensively explored for question answering tasks [13\u201316].", "startOffset": 93, "endOffset": 100}, {"referenceID": 14, "context": "On the other hand, neural models have been extensively explored for question answering tasks [13\u201316].", "startOffset": 93, "endOffset": 100}, {"referenceID": 15, "context": "On the other hand, neural models have been extensively explored for question answering tasks [13\u201316].", "startOffset": 93, "endOffset": 100}, {"referenceID": 16, "context": "Specifically, reasoning systems incorporating memory and attention mechanisms such as the Memory Network (MemNN) [17] were shown to be very successful, and the End-to-end Memory Network (MemN2N) [18], a variant of MemNN, can be trained end-to-end without labeled supporting facts.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Specifically, reasoning systems incorporating memory and attention mechanisms such as the Memory Network (MemNN) [17] were shown to be very successful, and the End-to-end Memory Network (MemN2N) [18], a variant of MemNN, can be trained end-to-end without labeled supporting facts.", "startOffset": 195, "endOffset": 199}, {"referenceID": 0, "context": "The Attention-based Multi-hop Recurrent Neural Network (AMRNN) mentioned above in the previous work [1] utilizes the attention mechanism with recurrent neural networks (RNN) [19] to construct sentence representations considering the word order, but didn\u2019t take the syntactic structure of sentences into account yet.", "startOffset": 100, "endOffset": 103}, {"referenceID": 18, "context": "The Attention-based Multi-hop Recurrent Neural Network (AMRNN) mentioned above in the previous work [1] utilizes the attention mechanism with recurrent neural networks (RNN) [19] to construct sentence representations considering the word order, but didn\u2019t take the syntactic structure of sentences into account yet.", "startOffset": 174, "endOffset": 178}, {"referenceID": 19, "context": "Recently, tree-structured models [20] obtained from the syntactic structures of the sentences were shown to be able to produce more robust representations and capture better semantics in certain tasks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "In this paper, we take the TOEFL listening comprehension test as the corpus for experiments [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The experiments showed improved performance over existing baselines including the previous work using AMRNN [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 21, "context": "Just as the standard LSTM [22], it has the input gates ij and output gates oj for the memory cells, and the forget gates fjk that controls the information flowing in from its child node k.", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "1Dependency parses produced by the Stanford Neural Network Dependency Parser [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "The latter was obtained using CMU Sphinx recognizer [23] with a word error rate of 34.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "The size of the hidden layer for Tree-LSTM [20] and the embedding size of the memory module were both 75.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "AdaGrad [24] was used with an initial learning rate of 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "In the trivial baselines, we used pre-trained GloVe [25] vectors to obtain the vector representation for each word.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "(a) Question/choice similarity [1]: With the vector representations for the choices/question mentioned above, the choice most similar to the question was selected.", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "(b) Sliding window of utterances [12, 26]: We slid a window of 5 utterances over the story, and chose the window the most similar to the question as the related information in the story.", "startOffset": 33, "endOffset": 41}, {"referenceID": 25, "context": "(b) Sliding window of utterances [12, 26]: We slid a window of 5 utterances over the story, and chose the window the most similar to the question as the related information in the story.", "startOffset": 33, "endOffset": 41}, {"referenceID": 15, "context": "(c) Deep LSTM Reader (DLR) [16]: We fed the story followed by the question into a Deep LSTM encoder to obtain the representation of each story/question pair.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "(d) End-to-end Memory Network (MemN2N) [18]: We slightly modified the original MemN2N to adapt to this task.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "(e) Attention-based Multi-hop Recurrent Neural Network (AMRNN) [1]: This is the approach used by the previous work.", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "(f) Tree-LSTM [20]: Similar to the proposed HAM but without attention.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "3The previous work [1] showed that AMRNN outperformed MemN2N in which 10 models with random initialization were trained, and the best on the development set was used for testing.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "We thank Bo-Hsiang Tseng for assistance with providing us the TOEFL listening comprehension dataset and the detailed statistics of AMRNN [1], and Juei-Yu Chang, An Huang, YuAn Chen and Ping-Hsuan Tsai for labeling the 3 different types of questions in the testing set.", "startOffset": 137, "endOffset": 140}], "year": 2017, "abstractText": "Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.", "creator": "LaTeX with hyperref package"}}}