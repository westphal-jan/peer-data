{"id": "1608.08898", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines", "abstract": "The proposed work expands the extreme learning machine technology to adapt to the problems of multi-label classification. In contrast to the single-label problem, both the number of labels to which the sample belongs and each of these multi-label classification target labels are identified, leading to increasing complexity. The proposed high-speed multi-label classifier is applied to six benchmark datasets consisting of different application areas such as multimedia, text and biology. Classifier training time and testing time are compared with those of modern methods. Experimental studies show that our proposed technique has faster execution speed and better performance for all six datasets, outperforming all existing multi-label classification methods.", "histories": [["v1", "Wed, 31 Aug 2016 14:56:12 GMT  (423kb)", "http://arxiv.org/abs/1608.08898v1", "12 pages, 2 figures, 10 tables"]], "COMMENTS": "12 pages, 2 figures, 10 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["meng joo er", "rajasekar venkatesan", "ning wang"], "accepted": false, "id": "1608.08898"}, "pdf": {"name": "1608.08898.pdf", "metadata": {"source": "CRF", "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines", "authors": ["Ning Wang", "Meng Joo Er", "Rajasekar Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "International Conference on Extreme Learning Machines, 2015 Meng Joo Er is a Chair Professor with Marine Engineering College, Dalian Maritime University, Dalian 116026, China, and together with Rajasekar Venkatesan are with the School of Electrical and Electronics Engineering in NTU, Singapore; Ning Wang is with Marine Engineering College, Dalian Maritime University, Dalian 116026, China.\nlearning machines for multi-label classification problem is proposed and discussed. Multi-label classification is a superset of traditional binary and multiclass classification problems. The proposed work extends the extreme learning machine technique to adapt to the multi-label problems. As opposed to the singlelabel problem, both the number of labels the sample belongs to, and each of those target labels are to be identified for multi-label classification resulting in increased complexity. The proposed high speed multi-label classifier is applied to six benchmark datasets comprising of different application areas such as multimedia, text and biology. The training time and testing time of the classifier are compared with those of the state-of-the-arts methods. Experimental studies show that for all the six datasets, our proposed technique have faster execution speed and better performance, thereby outperforming all the existing multi-label classification methods.\nKeywords: Classification, extreme learning machine, high-speed, multi-label."}, {"heading": "1 Introduction", "text": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc. Recent realization of the omnipresence of multi-label prediction tasks in real world problems has drawn increased research attention [7].\nClassification in machine learning is defined as \u201cGiven a set of training examples composed of pairs {xi,yi}, find a function f(x) that maps each attribute vector xi to its associated class yi, i = 1,2,\u2026.,n, where n is the total number of training samples\u201d [8]. These classification problems are called single-label classification. Single-label classification problems involve mapping each of the input vectors to its unique target class from a pool of target classes. However, there are several classification problems in which the target classes are not mutually exclusive and the input samples belong to more than one target class. These problems cannot be classified using single-label classification thus resulting in the development of several multi-label classifiers to mitigate this limitation. By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],\nmultimedia, biology, music categorization, genomics, emotion, image and video categorization and so on. Several classifiers are developed to address the multi-label problem and are available in the literature. Multi-label problems are more difficult and more complex compared to single-label problems due to its generality [14]. In this paper, we propose a high-speed multi-label classifier based on extreme learning machines (ELM). The proposed ELM-based approach outperforms all existing multi-label classifiers with respect to training time and testing time and other performance metrics.\nThe rest of the paper is organized as follows. A brief overview of different types of multi-label classifiers available in the literature is discussed in Section II. Section III describes the proposed approach for multi-label problems. Different benchmark metrics for multi-label datasets and experimentation specifications are discussed in Section IV. In Section V, a comparative study of the proposed method with existing methods and related discussions are carried out. Finally, concluding remarks are given in Section VI."}, {"heading": "2 Multi-label Classifier", "text": "The definition for multi-label learning as given by [15] is; \u201cGiven a training set, S = (xi, yi), 1 \u2264 i \u2264 n, consisting of n training instances, (xi \u03f5 X, yi \u03f5 Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multi-label classifier h:X\u2192Y that optimizes some specific evaluation function or loss function\u201d.\nLet pi be the probability that the input sample is assigned to ith class from a pool of M target classes. For single-label classification such as binary and multi-class classification the following equality condition holds true.\n\u2211 \ud835\udc5d\ud835\udc56 = 1 (1)\nThis equality does not hold for multi-label problems as each sample may have more than one target class. Also, it can be seen that the binary classification problems, the multi-class problems and ordinal regression problems are specific instances of the multi-label problems with the number of labels corresponding to each data sample restricted to 1 [16].\nThe multi-label learning problem can be summarized as follows:\n\u2500 There exists an input space that contains tuples (features or attributes) of size D of\ndifferent data types such as Boolean, discrete or continuous. xi \u03f5 X, xi = (xi1,xi2,\u2026.xiD).\n\u2500 A label space of tuple size M exists which is given as, L = {\u03b61, \u03b62,\u2026., \u03b6M} . \u2500 Each data sample is given as a pair of tuples (input space and label space respec-\ntively). {(xi,yi) | xi \u03f5 X, yi \u03f5 Y, Y \u2286 L, 1\u2264i\u2264N} where N is the number of training samples. \u2500 A training model that maps the input tuple to the output tuple with high speed, high\naccuracy and less complexity.\nSeveral approaches for solving multi-label problem are available in the literature. Earlier categorization of the multi-label (ML) methods [17] classify the methods into two categories, namely, Problem Transformation (PT) and Algorithm Adaptation (AA)\nmethods. This categorization is extended to include a third category of methods by Gjorgji Madjarov et al [18] called Ensemble methods (EN). Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18]. As adapted from [18], an overview of multi-label methods available in the literature is given in Fig. 1.\nBased on the machine learning algorithm used, the multi-label techniques can be categorized as shown in Fig. 2, adapted from [18]. This paper proposes a high speed multi-label learning technique based on ELM, which outperforms all the existing techniques based on speed and performance."}, {"heading": "3 Proposed Approach", "text": "The extreme learning machine is a learning technique that operates on a single-layer feedforward neural network. The key advantage of the ELM over the traditional backpropagation (BP) neural network is that it has the smallest number of parameters to be adjusted and it can be trained with very high speed. The traditional BP network needs to be initialized and several parameters tuned and improper selection of which can result in local optima. On the other hand, in ELM, the initial weights and the hidden layer bias can be selected at random and the network can be trained for the output weights in order to perform the classification [19-22]. The key steps in extending the ELM to multi-label problems is in the pre-processing and post-processing of data. In multi-label problems, each input sample may belong to one or more samples. The number of labels an input sample belongs to is not previously known. Therefore, both the number of labels and the target labels are to be identified for the test input samples and also the\ndegree of multi-labelness varies among different datasets. This results in increased complexity of the multi-label problem resulting in much longer training and testing time of the multi-label classification technique. The proposed algorithm exploits the inherent high speed nature of the ELM resulting in both high speed and superior performance compared with the existing multi-label classification techniques.\nConsider N training samples of the form {(xi,yi)} where xi in the input denoted as xi = [xi1,xi2,\u2026,xin]T \u03f5 Rn and yi is the target label set, yi = [yi1,yi2,\u2026yim]T. As opposed to traditional single-label case, the target label is not a single label but is a subset of labels from the label space given as Y\u2286L, L = {\u03b61, \u03b62,\u2026., \u03b6M}. Let \ud835\udc41 be the number of hidden layer neurons, the output \u2018o\u2019 of the SLFN is given by\n\u2211 \ud835\udefd\ud835\udc56\ud835\udc54\ud835\udc56(\ud835\udc65\ud835\udc57) = \u2211 \ud835\udefd\ud835\udc56\ud835\udc54(\ud835\udc64\ud835\udc56 . \ud835\udc65\ud835\udc57 + \ud835\udc4f\ud835\udc56) = \ud835\udc5c\ud835\udc57\n\ud835\udc41\n\ud835\udc56=1\n\ud835\u0305\udc41\n\ud835\udc56=1\n(2)\nwhere, \u03b2i = [\u03b2i1,\u03b2i2,\u2026\u03b2im]T is the output weight, g(x) is the activation function, wi =\n[wi1,wi2,\u2026win]T is the input weight and bi is the hidden layer bias.\nFor the ELM, the input weights wi and the hidden layer bias bi are randomly assigned. Therefore, the network must be trained for \u03b2i such that the output of the network is equal to the target class so that the error difference between the actual output and the predicted output is 0.\n\u2211\u2016\ud835\udc5c\ud835\udc57 \u2212 \ud835\udc66\ud835\udc57\u2016\n\ud835\udc41\n\ud835\udc57=1\n= 0 (3)\nThus, the ELM classifier output can be as follows:\n\u2211 \ud835\udefd\ud835\udc56\ud835\udc54(\ud835\udc64\ud835\udc56 . \ud835\udc65\ud835\udc57 + \ud835\udc4f\ud835\udc56) = \ud835\udc66\ud835\udc57\n\ud835\udc41\n\ud835\udc56=1\n(4)\nThe above equation can be written in following matrix form:\nH\u03b2 = Y (5)\nThe output weights of the ELM network can be estimated using the equation\n\u03b2 = H+Y (6a)\nwhere H+ is the Moore-Penrose inverse of the hidden layer output matrix H and it\ncan be calculated as follows:\nH+ = (HTH)-1HT (6b)\nThe theory and mathematics behind the ELM have been extensively discussed in [23-25] and hence are not re-stated here. The steps involved in multi-label ELM classifier are given below.\nInitialization of Parameters. Fundamental parameters such as the number of hidden layer neurons and the activation function are initialized.\nProcessing of Inputs. In the multi-label case, each input sample can be associated with more than one class labels. Hence, each of the input samples will have the associated output label as a m-tuple with 0 or 1 representing the belongingness to each of the labels in the label space L. The label set denoting the belongingness for each of the labels is converted from unipolar representation to bipolar representation.\nELM Training. The processed input is then supplied to the basic batch learning ELM. Let H be the hidden layer output matrix, \u03b2 be the output weights and Y be the target label, the ELM can be represented in a compact form as H\u03b2 = Y where Y\u2286L, L = {\u03b61, \u03b62,\u2026., \u03b6M}. In the training phase, the input weights and the hidden layer bias are randomly assigned and the output weights \u03b2 are estimated as \u03b2 = H+Y, where H+ = (HT H)1HT gives the Moore-Penrose generalized inverse of the hidden layer output matrix.\nELM Testing. In the testing phase, the test data sample is evaluated using the values of \u03b2 obtained during the training phase. The network then predicts the target output using the equation Y = H\u03b2. The predicted output Y obtained is a set of real numbers of dimension equal to the number of labels.\nPost-processing and Multi-label Identification. The key challenge in multi-label classification is that the input sample may belong to one or more than one of the target labels. The number of labels that the sample corresponds to is completely unknown. Hence, a thresholding-based label association is proposed. The L dimensioned rawpredicted output is compared with a threshold value. The index values of the predicted output Y which are greater than the threshold fixed represents the belongingness of the input sample to the corresponding class.\nSetting the threshold value is of critical importance. Threshold setting has to be made in such a way that it maximizes the difference between the values of the label the data belongs to and the labels the data does not. The distribution of the raw output values is categorized into a range of values that represent the belongingness of the label and the range of values that represent the non-belongingness of the label to a particular sample. From the distribution, a particular value is chosen that maximizes the separation between the two categories of the labels. It is to be highlighted that there are no ELMbased multi-label classifier in the literature thus far. The proposed method is the first to adapt the ELM for multi-label problems and make extensive experimentation and results comparison and analysis with the state-of-the-arts multi-label classification techniques."}, {"heading": "4 Experimentation", "text": "This section describes the different multi-label dataset metrics and gives the experimental design used to evaluate the proposed method. Multi-label datasets have a unique property called the degree of multi-labelness. The number of labels, the number of samples having multiple labels, the average number of labels corresponding to a particular sample varies among different datasets. Two dataset metrics are available in the literature to quantitatively measure the multi-labelness of a dataset. They are Label Cardinality (LC) and Label Density (LD). Consider there are N training samples and the dataset is of the form {(xi,yi)} where xi in the input data and yi is the target label set. The target label set is a subset of labels from the label space with M elements given as Y\u2286L, L = {\u03b61, \u03b62\u2026 \u03b6M}.\nDefinition 4.1 [17] Label Cardinality of the dataset is the average number of labels\nof the examples in the dataset.\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \u2212 \ud835\udc36\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc61\ud835\udc66 =\n1 \ud835\udc41 \u2211|\ud835\udc4c\ud835\udc56|\n\ud835\udc41\n\ud835\udc56=1\n(7)\nLabel Cardinality signifies the average number of labels present in the dataset. Definition 4.2 [17] Label Density of the dataset is the average number of labels of\nthe examples in the dataset divided by |L|.\n\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59 \u2212 \ud835\udc37\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc66 =\n1 \ud835\udc41 \u2211 |\ud835\udc4c\ud835\udc56| |\ud835\udc3f|\n\ud835\udc41\n\ud835\udc56=1\n(8)\nLabel density takes into consideration the number of labels present in the dataset. The properties of two datasets have same label cardinality, but different label density can vary significantly and may result in different behavior of the training algorithm [14]. The influence of label density and label cardinality on multi-label learning is analyzed by Flavia et al in 2013 [26]. The proposed method is experimented with six benchmark datasets comprising of different application areas and its results are compared with 9 existing state-of-the-art methods. The datasets are chosen in such a way that they exhibit diverse nature of characteristics and the wide range of label density and label cardinality. The datasets are obtained from KEEL multi-label dataset repository and the specifications of the dataset are given in Table 1. The details of state-ofthe-arts multi-label techniques used for result comparison are given in Table 2."}, {"heading": "5 Results and Discussions", "text": "This section discusses the results obtained by the proposed method and compares it with the existing methods. The results obtained from the proposed method are evaluated for consistency, performance and speed."}, {"heading": "5.1 Consistency", "text": "Consistency is a key feature that is essential for any new technique proposed. The proposed algorithm should provide consistent results with minimal variance. Being an ELM based algorithm, since the initial weights are assigned in random, it is critical to evaluate the consistency of the proposed technique. The unique feature of multi-label classification is the possibility of partial correctness of the classifier, i.e. one or more of the multiple labels to which the sample instance belongs and/or the number of labels the sample instance belongs can be identified partially correctly. Therefore, calculating the error rate for multi-label problems is not same as that of traditional binary or multiclass problems. In order to quantitatively measure the correctness of the classifier, the hamming loss performance metric is used. To evaluate the consistency of the proposed method, a 5 fold and a 10 fold cross validation of hamming loss metric is evaluated for each of the six datasets and is tabulated.\nTable 3. Consistency table \u2013 cross validation\nDataset Hamming Loss - 5-fcv Hamming Loss - 10-fcv\nEmotion 0.2492(\u00b10.0058) 0.2509(\u00b10.0050)\nYeast 0.1906(\u00b10.0025) 0.1911(\u00b10.0031) Scene 0.0854(\u00b10.0029) 0.0851(\u00b10.0033) Corel5k 0.0086(\u00b10.0005) 0.0090(\u00b10.0006)\nEnron 0.0474(\u00b10.0022) 0.0472(\u00b10.0015) Medical 0.0108(\u00b10.0008) 0.0109(\u00b10.0009)\nFrom the table 3, it can be seen that the proposed technique is consistent in its performance over repeated executions and cross validations thus demonstrating the consistency of the technique."}, {"heading": "5.2 Performance Metrics", "text": "As foreshadowed, the unique feature of multi-label classification is the possibility of partial correctness of the classifier. Therefore, a set of quantitative performance evaluation metrics is used to validate the performance of the multi-label classifier. The performance metrics are hamming loss, accuracy, precision, recall and F1-measure. A comparison of performance metrics such as hamming loss, precision, recall, accuracy and F1 measure of the proposed technique is shown in Tables 4-8. The performance of state-of-the-art techniques is adapted from [18]. From the tables, it is clear that the proposed method works uniformly well on all datasets. The proposed method outperforms all the existing methods in most cases and remains one of the top classification techniques in other cases."}, {"heading": "5.3 Speed", "text": "The performance of the proposed method in terms of execution speed is evaluated by comparing the training time and the testing time of the algorithm used. The proposed method is applied to 6 datasets of different domains with a wide range of label density and label cardinality and the training time and the testing time are compared with other state-of-the-art techniques. The comparison table of training time and testing time is given in Table 9 and Table 10 respectively.\nIn summary, the proposed method outperforms all existing multi-label learning techniques in terms of training and testing time by several orders of magnitude. From the results, it can be seen that the proposed method is the fastest multi-label classifier when compared to the current state-of-the-arts techniques. The speed of the proposed classifier is many-fold greater than existing methods. Also, from the comparison results of other performance metrics such as hamming loss, accuracy, precision, recall and F1measure, it can be seen that the proposed method remains one of the top positions in each case. Also, the F1-measure of the proposed approach outperforms the most recent method which uses canonical correlation analysis (CCA) with ELM for multi-label\nproblems [27] in most cases. The key advantage of the proposed method is that it surpasses all existing state-of-the-arts methods in terms of speed and simultaneously while remaining one of the top learning techniques in terms of other 5 performance metrics."}, {"heading": "6 Conclusion", "text": "The proposed high speed multi-label classifier executes with both fast speed and high accuracy. It is to be highlighted that there are no extreme-learning-machine-based multi-label classifiers existing in the literature thus far. The proposed method is applied to 6 benchmark datasets of different domains and a wide range of label density and label cardinality. The results are compared with 9 state-of-the-arts multi-label classifiers. It can be seen from the results that the proposed method surpasses all state-of-thearts methods in terms of speed and remain one of the top techniques in terms of other performance metrics. Thus, the proposed ELM-based multi-label classifier can be a better alternative for a wide range of multi-label classification techniques in order to achieve greater accuracy and very high speed."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of P. R. China (under Grants 51009017 and 51379002), Applied Basic Research Funds from Ministry of Transport of P. R. China (under Grant 2012-329-225-060), China Postdoctoral Science Foundation (under Grant 2012M520629), Program for Liaoning Excellent Talents in University (under Grant LJQ2013055). The second author would like to thank Nanyang Technological University for supporting this work by providing NTU RSS."}], "references": [{"title": "A Preliminary approach to the multi-label classification problem of Portuguese juridical documents", "author": ["T. Gonclaves", "P. Quaresma"], "venue": "Progress in Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Nedellec C, Rouveirol C (Ed.), ECML, LNCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Evaluation of Two Systems on Multi-class Multi-label document classification", "author": ["X. Luo", "A.N. Zincir Heywood"], "venue": "ISMIS 2005, LNCS (LNAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Experiments with multi-label text classifier on the Reuters collection", "author": ["D. Tikk", "G Biro"], "venue": "Proceedings of the International Conference on Computational Cybernetics (ICCC 2003),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and Development in information retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Mining Multi-label Data\u201d, Data Mining and Knowledge Discovery Handbook", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A Tutorial on Multi-label Classification Techniques", "author": ["A.C. de Carvalho", "A.A. Freitas"], "venue": "Foundations of Computational Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "Neural Information Processing Systems, NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A k-nearest neighbour based algorithm for multi-label classification", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Proceedings of the 1st IEEE international Conference on Granular Computing, Beijing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Brouwn, \u201cMulti-label semantic scene classification", "author": ["M. Boutell", "X. Shen", "C.J. Luo"], "venue": "Technical Report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Multi-label machine learning and its application to semantic scene classification. Storage and Retrieval Methods and Applications for Multimedia", "author": ["X. Shen", "M. Boutell", "J. Luo", "C. Brown"], "venue": "Yeung MM, Lienhart RW, Li CS (Ed.), Proceedings of the SPIE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Efficient Approximation Algorithms for Multi-label Map Labelling", "author": ["B. Zhu", "C.K. Poon"], "venue": "Algorithms and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "ML-kNN: A lazy learning approach to multi-label learning", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Sorower, \"A literature survey on algorithms for multi-label learning.", "author": ["S. M"], "venue": "Oregon State University, Corvallis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Kernel methods for multi-labelled classification and categorical regression problems", "author": ["A. Elisseeff", "J. Weston J"], "venue": "Technical Report, BIOwulf Technologies,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Multi-label Classification: An Overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["G. Madjarov", "D. Kocev", "D. Gjorgjevikj", "S. Dzeroski"], "venue": "Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A novel extreme learning control framework of unmanned surface vehicles", "author": ["N. Wang", "J.C. Sun", "M.J. Er", "Y.C. Liu"], "venue": "IEEE Transactions on Cybernetics, Accepted for Publication,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Generalized single-hidden layer feedforward networks for regression problems", "author": ["N. Wang", "M.J. Er", "M. Han"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Extreme learning machines: a survey,", "author": ["G.B. Huang", "D.H. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Extreme learning machine: algorithm, theory and applications", "author": ["S. Ding", "H. Zhao", "Y. Zhang", "X. Xu", "R. Nie"], "venue": "Artificial Intelligence Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Analyzing the Influence of Cardinality and Density", "author": ["F.C. Bernardini", "R.B. da Silva", "E.M. Meza", "R. das Ostras\u2013RJ\u2013Brazil"], "venue": "Characteristics on Multi-Label Learning\u201d,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc.", "startOffset": 156, "endOffset": 161}, {"referenceID": 1, "context": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc.", "startOffset": 156, "endOffset": 161}, {"referenceID": 2, "context": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc.", "startOffset": 156, "endOffset": 161}, {"referenceID": 3, "context": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc.", "startOffset": 156, "endOffset": 161}, {"referenceID": 4, "context": "In recent years, the problem of multi-label classification is gaining much importance motivated by increasing application areas such as text categorization [1-5], marketing, music categorization, emotion, genomics, medical diagnosis [6], image and video categorization, etc.", "startOffset": 156, "endOffset": 161}, {"referenceID": 5, "context": "Recent realization of the omnipresence of multi-label prediction tasks in real world problems has drawn increased research attention [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": ",n, where n is the total number of training samples\u201d [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],", "startOffset": 165, "endOffset": 171}, {"referenceID": 8, "context": "By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],", "startOffset": 165, "endOffset": 171}, {"referenceID": 9, "context": "By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],", "startOffset": 213, "endOffset": 220}, {"referenceID": 10, "context": "By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],", "startOffset": 213, "endOffset": 220}, {"referenceID": 11, "context": "By the recent advancements in technology, the application areas of multi-label classifiers spread across various domains such as text categorization, bioinformatics [9-10], medical diagnosis, scene classification [11-12], map labeling [13],", "startOffset": 235, "endOffset": 239}, {"referenceID": 12, "context": "Multi-label problems are more difficult and more complex compared to single-label problems due to its generality [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The definition for multi-label learning as given by [15] is; \u201cGiven a training set, S = (xi, yi), 1 \u2264 i \u2264 n, consisting of n training instances, (xi \u03b5 X, yi \u03b5 Y) drawn from an unknown distribution D, the goal of multi-label learning is to produce a multi-label classifier h:X\u2192Y that optimizes some specific evaluation function or loss function\u201d.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Also, it can be seen that the binary classification problems, the multi-class problems and ordinal regression problems are specific instances of the multi-label problems with the number of labels corresponding to each data sample restricted to 1 [16].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "Earlier categorization of the multi-label (ML) methods [17] classify the methods into two categories, namely, Problem Transformation (PT) and Algorithm Adaptation (AA)", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This categorization is extended to include a third category of methods by Gjorgji Madjarov et al [18] called Ensemble methods (EN).", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18].", "startOffset": 127, "endOffset": 141}, {"referenceID": 6, "context": "Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18].", "startOffset": 127, "endOffset": 141}, {"referenceID": 13, "context": "Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18].", "startOffset": 127, "endOffset": 141}, {"referenceID": 15, "context": "Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18].", "startOffset": 127, "endOffset": 141}, {"referenceID": 16, "context": "Several review articles are available in the literature that describe various methods available for multi-label classification [7,8,15,17,18].", "startOffset": 127, "endOffset": 141}, {"referenceID": 16, "context": "As adapted from [18], an overview of multi-label methods available in the literature is given in Fig.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "2, adapted from [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "On the other hand, in ELM, the initial weights and the hidden layer bias can be selected at random and the network can be trained for the output weights in order to perform the classification [19-22].", "startOffset": 192, "endOffset": 199}, {"referenceID": 18, "context": "On the other hand, in ELM, the initial weights and the hidden layer bias can be selected at random and the network can be trained for the output weights in order to perform the classification [19-22].", "startOffset": 192, "endOffset": 199}, {"referenceID": 19, "context": "The theory and mathematics behind the ELM have been extensively discussed in [23-25] and hence are not re-stated here.", "startOffset": 77, "endOffset": 84}, {"referenceID": 20, "context": "The theory and mathematics behind the ELM have been extensively discussed in [23-25] and hence are not re-stated here.", "startOffset": 77, "endOffset": 84}, {"referenceID": 15, "context": "1 [17] Label Cardinality of the dataset is the average number of labels of the examples in the dataset.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "2 [17] Label Density of the dataset is the average number of labels of the examples in the dataset divided by |L|.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "The properties of two datasets have same label cardinality, but different label density can vary significantly and may result in different behavior of the training algorithm [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "The influence of label density and label cardinality on multi-label learning is analyzed by Flavia et al in 2013 [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "The performance of state-of-the-art techniques is adapted from [18].", "startOffset": 63, "endOffset": 67}], "year": 2015, "abstractText": "In this paper a high speed neural network classifier based on extreme learning machines for multi-label classification problem is proposed and discussed. Multi-label classification is a superset of traditional binary and multiclass classification problems. The proposed work extends the extreme learning machine technique to adapt to the multi-label problems. As opposed to the singlelabel problem, both the number of labels the sample belongs to, and each of those target labels are to be identified for multi-label classification resulting in increased complexity. The proposed high speed multi-label classifier is applied to six benchmark datasets comprising of different application areas such as multimedia, text and biology. The training time and testing time of the classifier are compared with those of the state-of-the-arts methods. Experimental studies show that for all the six datasets, our proposed technique have faster execution speed and better performance, thereby outperforming all the existing multi-label classification methods.", "creator": "Microsoft\u00ae Word 2013"}}}