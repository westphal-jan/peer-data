{"id": "1608.08974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models", "abstract": "Deep neural networks have made remarkable progress in many areas of AI research in recent years and have achieved state-of-the-art results. However, it is often unsatisfactory not to know why they predict what they are doing. In this paper, we address the problem of interpreting models to answer questions visually. In particular, we are interested in which part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To address this problem, we use two visualization techniques - guided feedback and occlusion - to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these important maps.", "histories": [["v1", "Wed, 31 Aug 2016 18:11:29 GMT  (485kb,D)", "https://arxiv.org/abs/1608.08974v1", null], ["v2", "Fri, 9 Sep 2016 19:51:06 GMT  (486kb,D)", "http://arxiv.org/abs/1608.08974v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["yash goyal", "akrit mohapatra", "devi parikh", "dhruv batra"], "accepted": false, "id": "1608.08974"}, "pdf": {"name": "1608.08974.pdf", "metadata": {"source": "CRF", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models", "authors": ["Yash Goyal", "Akrit Mohapatra", "Devi Parikh", "Dhruv Batra", "Virginia Tech"], "emails": ["dbatra}@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "We are witnessing an excitement in the research community and frenzy in media regarding advances in AI. Fueled by a combination of massive datasets and advances in deep neural networks (DNNs), the community has made remarkable progress in the past few years on a variety of \u2018low-level\u2019 AI tasks such as image classification (Szegedy et al., 2015) machine translation (Brea et al., 2011; Sutskever et al., 2014) and speech recognition (Hinton et al., 2012). Neural networks are also demonstrating potential in \u2018high-level\u2019 AI tasks such as learning\nto play Go (Silver et al., 2016), answering reading comprehension questions by understanding short stories (Bordes et al., 2015; Weston et al., 2015), and even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).\nUnfortunately, when today\u2019s machine perception and intelligent systems fail, they fail in a spectacularly disgraceful manner, without warning or explanation, leaving the user staring at an incoherent output, wondering why the system did what it did.\nIn this work, we focus on Visual Question Answering, where given an image and a free-form natural language question about the image, (e.g., \u201cWhat color are the girl\u2019s shoes?\u201d, or \u201cIs the boy jumping?\u201d), the machine has to produce a natural language answer as its output (e.g. \u201cblue\u201d, or \u201cyes\u201d). Specifically, we try to interpret a recent state-of-art VQA model (Lu et al., 2015) trained on recently released VQA (Antol et al., 2015) dataset. This VQA\nar X\niv :1\n60 8.\n08 97\n4v 2\n[ cs\n.C V\n] 9\nS ep\n2 01\nmodel uses Convolutional Neural Network (CNN) based embedding of the image, Long-Short Term Memory (LSTM) based embedding of the question, combines these two embeddings, and uses a multilayer perceptron as a classifier to predict a probability distribution over answers.\nSustained interactions1 with the system make it clear that the system has a non-trivial level of intelligence (e.g., it is able to recognize people, objects, etc. in the image). However, it is deeply unsatisfying to not know why the system predicts what it does (especially the glaring mistakes). The root cause is lack of transparency or interpretability. With a few rare exceptions, the emphasis in machine learning, computer vision, and AI communities today is on building systems with good predictive performance, not transparency. As a result, the users of these intelligent systems perceive them as inscrutable black boxes that cannot be understood or trusted.\nWe are interested in the question of transparency \u2013 why does a VQA system do what it does? (See Fig. 1). Specifically, what evidence in the test input (image and question) supports a particular prediction? In the context of VQA, this question can be expressed as two subproblems: \u2022 What words in the question does the model \u201clis-\nten to\u201d in order to answer the question? \u2022 What pixels in the image does the model \u201clook\nat\u201d while answering the question? In this work, we use two visualization methods to tackle the above problems. The first method (Sec. 3.1) uses guided backpropagation (Springenberg et al., 2015) to analyze important words in the question and important regions in the image. In the second method (Sec. 3.2), we occlude portions of input and observe the change in prediction probabilities of the model, to compute importance of question words and image regions. In Sec. 4, we present qualitative and quantitative analyses of these image/question \u2018importance maps\u2019 \u2013 question importance maps are analyzed using their Part-of-Speech (POS) tags; image importance maps are compared to \u2018human attention maps\u2019 or maps showing where humans look for answering a question about the image (Das et al., 2016). We found that even without\n1Demo available here: http://cloudcv.org/vqa/ (Agrawal et al., 2015)\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question."}, {"heading": "2 Related Work", "text": "Many gradient based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in recent years in the field of computer vision to visualize deep convolutional neural networks. But most of them focused on the task of image classification on iconic images where the main object occupies most of the image. Our work differs in 2 ways \u2013 1) we also compute gradients w.r.t. the input question, and 2) we use guided backpropagation (Springenberg et al., 2015) for the task of VQA, where the model can look at different regions in the same image for different questions. As per our literature review, we are the first to study this problem for VQA.\nOur occlusion experiment is inspired by (Zeiler and Fergus, 2014) who mask small regions in the image with a gray patch, and observe the output of an image classification model. We evaluate if the model looks at the same regions in the image as humans do, while answering a question about the image.\nA few recent works (Ribeiro et al., 2016; Baehrens et al., 2010; Liu and Wang, 2012) have begun to study the task of providing interpretable posthoc explanations for classifier predictions. Such methods typically involve fitting or training a secondary interpretable mechanism on top of the base \u2018black-box\u2019 classifier predictions. In contrast, our work directly computes importance maps from the model of interest without another layer of training (which could obfuscate the analysis)."}, {"heading": "3 Approach", "text": "At a high level, we view a VQA model as a learned function a = fw(i, q) that takes in an input image i and a question about the image q, is parameterized by parameters w, and produces an answer a. In order to gauge the importance of components of i and q (i.e. pixels and words), we consider the best linear approximation to f around each test point\n(itest, qtest):\nf(i, q) '\ufe38\ufe37\ufe37\ufe38 best linear fit f(itest, qtest)\n+[i\u2212 itest, q \u2212 qtest]\u1d40\u2207f(itest, qtest) (1)\nIntuitively, the two key quantities we need to compute are \u2202f(itest, qtest)/\u2202i and \u2202f(itest, qtest)/\u2202q, i.e. the partial derivatives of the function w.r.t. each of the inputs (image and question). These expressions superficially look similar to gradients computed in backpropagation-based training of neural networks. However, there are two key differences \u2013 (i) we compute partial derivatives of the probability of predicted output, not the ground-truth output; and (ii) we compute partial derivatives w.r.t. inputs (i.e. image pixel intensities and word embeddings), not parameters.\nDue to linearization above, elements of these partial derivatives tell us the effect of those pixels/words on the final prediction. These may be computed in the following two ways."}, {"heading": "3.1 Guided Backpropagation", "text": "Guided backpropagation (Springenberg et al., 2015) is a gradient-based visualization technique used to visualize activations of neurons in different layers in CNNs. It has been shown to perform better than its counterparts such as deconvolution (Zeiler and Fergus, 2014) especially for visualizing higher order layers. Intuitively speaking, it is a modified version\nof backpropagation that restricts negative gradients from flowing backwards towards input layer, resulting in sharper image visualizations.\nSpecifically, Guided BP is identical to classical BP except in the way the backward pass is computed in Rectified Linear Units (ReLUs). Let hl denote the input to layer l and hl+1 denote the output. Recall that a ReLU is defined as hl+1 = relu(hl) = max(hl, 0). Let Gl+1 = \u2202f/\u2202hl+1 denote the partial derivative w.r.t. the output of the ReLU (received as input in the backward pass). The key difference between the two backprops (BP) is:\nGl = [[hl > 0]] \u00b7Gl+1 [Classical BP] (2)\nGl = [[hl > 0]]\u00b7[[Gl+1 > 0]]\u00b7Gl+1[Guided BP] (3)\ni.e. guided BP blocks negative gradients from flowing back in ReLUs. For more details, please refer to (Springenberg et al., 2015).\nWe use guided BP to compute \u2018gradients\u2019 of the probability of predicted answer w.r.t. inputs (image and question). Note that the language pathway in the models we typically use, does not contain ReLUs, thus these are true gradients (not just gradient-based visualizations) on the language side. We interpret the words/pixels with the highest (magnitude) gradients received as the most important for the model since small changes in these lead to largest changes in the model\u2019s confidence in the predicted answer."}, {"heading": "3.2 Discrete Derivatives", "text": "In this method, we systematically occlude subsets of the input, forward propagate the masked input through the VQA model, and compute the change in the probability of the answer predicted with the unmasked original input. Since there are 2 inputs to the model, we focus on one input at a time, keeping the other input fixed (mimicing partial derivatives). Specifically, to compute importance of a question word, we mask that word by dropping it from the question, and feed the masked question with original image as inputs to the model. The importance score of the question word is computed as the change in probability of the original predicted answer.\nWe follow the same procedure on the images to compute importance of image regions. We divide the image into a grid of size 16 x 16, occlude one cell at a time with a gray patch2, feed in the perturbed image with the entire question to the model, and compute the decrease in the probability of the original predicted answer. The generated importance maps are shown in Fig. 2.\nMore results and interactive visualizations can be found on authors\u2019 webpages.3"}, {"heading": "4 Results", "text": "While image/question importance maps on individual inputs provide crucial insight into the innerworkings of a model (e.g., see Fig. 2), what do the aggregate statistics of these maps tell us about the model?"}, {"heading": "4.1 Analyzing Image Importance", "text": "(Das et al., 2016) recently collected human attention annotations for (question, image) pairs from VQA dataset (Antol et al., 2015). Given a blurry image and a question, humans were asked to deblur the regions in the image that were helpful in answering the question.\nWe evaluate the quality of image importance maps obtained from the two methods (guided backpropagation and occlusion) by comparing them to\n2a gray patch of intensities (R, G, B) = (123.68, 116.779, 103.939), mean RGB pixel values across a large image dataset ImageNet (Deng et al., 2009) on which the CNN is trained.\n3Question importance maps: https://mlp.ece.vt. edu/masked_ques_vis/. Image importance maps: https://mlp.ece.vt.edu/masked_image_vis/\nthe human attention maps. The human attention dataset contains annotations for 1374 (question, image) pairs from VQA (Antol et al., 2015) validation set. Following the evaluation protocol in (Das et al., 2016), we take the absolute value of the importance maps and compute their mean rank-correlation with the human attention maps. Specifically, we first scale both the image importance and human attention maps to 14x14, normalize them spatially and rank the pixels according to their spatial attention, and then compute correlation between these two ranked lists. The results are shown in Table 1. We find that both importance maps (occlusion and guided BP) are weakly positively correlated with human attention maps, although it is far from interhuman correlation. Thus, our techniques revealed an interesting finding \u2013 that even without attention mechanisms, VQA models may be implicitly attending to relevant regions in the image."}, {"heading": "4.2 Analyzing Question Importance", "text": "Since there is no human attention dataset for questions, we instead analyze the importance maps for questions using their POS tags. Our hypothesis is that wh-words and nouns should matter most to a\n\u2018sensible\u2019 model\u2019s prediction. We plot the probability of a word being most important in a question given that it has a certain POS tag. To get reliable statistics, we picked 15 most frequent POS tags from the VQA validation dataset, and grouped similar tags into one category, e.g. WDT, WP, WRB are grouped as wh-words. The histogram can be seen in Fig. 3. Indeed, wh-words are most important followed by adjectives and nouns. Adjectives and nouns rank high because many questions tend to ask about characteristics of objects, or objects themselves. This finding suggests that the language model part of the VQA model is strong and is able to learn to focus on appropriate words without any explicit attention procedure.\nNote that for many occlusions, the model\u2019s predicted answer is different from the original predicted answer. In fact, we found that the number of times the predicted answer changes correlates with the model\u2019s accuracy. It is able to predict success/failure accurately 72% of the times. This suggests that features that characterize these importance maps can provide useful signals for predicting the model\u2019s oncoming failures."}, {"heading": "5 Conclusion", "text": "In this paper, we experimented with two visualization methods \u2013 guided backpropagation and occlusion \u2013 to interpret deep learning models for the task of Visual Question Answering. Although we focus on only one VQA model in this work, the methods are generalizable to all other end-to-end VQA models. The occlusion method can even be applied to any (non-end-to-end) VQA model considering it as a black box. We believe that these methods and results can be helpful in interpreting the current VQA models, and designing the next generation of VQA models.\nAcknowledgements. This work was supported in part by the following: National Science Foundation CAREER awards to DB and DP, Army Research Office YIP awards to DB and DP, ICTAS Junior Faculty awards to DB and DP, Army Research Lab grant W911NF-15-2-0080 to DP and DB, Office of Naval Research grant N00014-14-1-0679 to DB, Paul G. Allen Family Foundation Allen Distinguished Investigator award to DP, Google Faculty Research award\nto DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor."}], "references": [{"title": "Cloudcv: Large-scale distributed computer vision as a cloud service", "author": ["Clint Solomon Mathialagan", "Yash Goyal", "Neelima Chavali", "Prakriti Banik", "Akrit Mohapatra", "Ahmed Osman", "Dhruv Batra"], "venue": "In Mobile Cloud Vi-", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "How to explain individual classification decisions", "author": ["Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert M\u00fcller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Baehrens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baehrens et al\\.", "year": 2010}, {"title": "Large-scale simple question answering with memory networks. CoRR, abs/1506.02075", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Sequence learning with hidden units in spiking neural networks", "author": ["Brea et al.2011] Johanni Brea", "Walter Senn", "JeanPascal Pfister"], "venue": "In NIPS", "citeRegEx": "Brea et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brea et al\\.", "year": 2011}, {"title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions", "author": ["Das et al.2016] Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": "In EMNLP", "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What has my classifier learned? Visualizing the classification rules of bag-of-feature model by support region detection", "author": ["Liu", "Wang2012] L. Liu", "L. Wang"], "venue": "In CVPR", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["Lu et al.2015] Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In NIPS", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "author": ["Sameer Singh", "Carlos Guestrin"], "venue": "In Knowledge Discovery and Data Mining (KDD)", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neu", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In ICLR Workshop Track", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR Workshop Track", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott E. Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Towards AIComplete Question Answering: A Set of Prerequisite", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "Toy Tasks. CoRR,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["Zeiler", "Fergus2014] Matthew D. Zeiler", "Rob Fergus"], "venue": "In ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Fueled by a combination of massive datasets and advances in deep neural networks (DNNs), the community has made remarkable progress in the past few years on a variety of \u2018low-level\u2019 AI tasks such as image classification (Szegedy et al., 2015) machine translation (Brea et al.", "startOffset": 220, "endOffset": 242}, {"referenceID": 4, "context": ", 2015) machine translation (Brea et al., 2011; Sutskever et al., 2014) and speech recognition (Hinton et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 15, "context": ", 2015) machine translation (Brea et al., 2011; Sutskever et al., 2014) and speech recognition (Hinton et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 6, "context": ", 2014) and speech recognition (Hinton et al., 2012).", "startOffset": 31, "endOffset": 52}, {"referenceID": 12, "context": "to play Go (Silver et al., 2016), answering reading comprehension questions by understanding short stories (Bordes et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": ", 2016), answering reading comprehension questions by understanding short stories (Bordes et al., 2015; Weston et al., 2015), and", "startOffset": 82, "endOffset": 124}, {"referenceID": 17, "context": ", 2016), answering reading comprehension questions by understanding short stories (Bordes et al., 2015; Weston et al., 2015), and", "startOffset": 82, "endOffset": 124}, {"referenceID": 1, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 10, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 9, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 8, "context": "Specifically, we try to interpret a recent state-of-art VQA model (Lu et al., 2015) trained on recently released VQA (Antol et al.", "startOffset": 66, "endOffset": 83}, {"referenceID": 1, "context": ", 2015) trained on recently released VQA (Antol et al., 2015) dataset.", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "1) uses guided backpropagation (Springenberg et al., 2015) to analyze important words in the question and important regions in the image.", "startOffset": 31, "endOffset": 58}, {"referenceID": 5, "context": "4, we present qualitative and quantitative analyses of these image/question \u2018importance maps\u2019 \u2013 question importance maps are analyzed using their Part-of-Speech (POS) tags; image importance maps are compared to \u2018human attention maps\u2019 or maps showing where humans look for answering a question about the image (Das et al., 2016).", "startOffset": 309, "endOffset": 327}, {"referenceID": 0, "context": "org/vqa/ (Agrawal et al., 2015) explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.", "startOffset": 9, "endOffset": 31}, {"referenceID": 13, "context": "Many gradient based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in recent years in the field of computer vision to visualize deep convolutional neural networks.", "startOffset": 28, "endOffset": 103}, {"referenceID": 14, "context": "Many gradient based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in recent years in the field of computer vision to visualize deep convolutional neural networks.", "startOffset": 28, "endOffset": 103}, {"referenceID": 14, "context": "the input question, and 2) we use guided backpropagation (Springenberg et al., 2015) for the task of VQA, where the model can look at different regions in the same image for different questions.", "startOffset": 57, "endOffset": 84}, {"referenceID": 11, "context": "A few recent works (Ribeiro et al., 2016; Baehrens et al., 2010; Liu and Wang, 2012) have begun to study the task of providing interpretable posthoc explanations for classifier predictions.", "startOffset": 19, "endOffset": 84}, {"referenceID": 2, "context": "A few recent works (Ribeiro et al., 2016; Baehrens et al., 2010; Liu and Wang, 2012) have begun to study the task of providing interpretable posthoc explanations for classifier predictions.", "startOffset": 19, "endOffset": 84}, {"referenceID": 14, "context": "Guided backpropagation (Springenberg et al., 2015) is a gradient-based visualization technique used to visualize activations of neurons in different layers in CNNs.", "startOffset": 23, "endOffset": 50}, {"referenceID": 14, "context": "For more details, please refer to (Springenberg et al., 2015).", "startOffset": 34, "endOffset": 61}, {"referenceID": 5, "context": "(Das et al., 2016) recently collected human attention annotations for (question, image) pairs from VQA dataset (Antol et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": ", 2016) recently collected human attention annotations for (question, image) pairs from VQA dataset (Antol et al., 2015).", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "The human attention dataset contains annotations for 1374 (question, image) pairs from VQA (Antol et al., 2015) validation set.", "startOffset": 91, "endOffset": 111}, {"referenceID": 5, "context": "Following the evaluation protocol in (Das et al., 2016), we take the absolute value of the importance maps and compute their mean rank-correlation with the human attention maps.", "startOffset": 37, "endOffset": 55}], "year": 2016, "abstractText": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques \u2013 guided backpropagation and occlusion \u2013 to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.", "creator": "LaTeX with hyperref package"}}}