{"id": "1609.00565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "Skipping Word: A Character-Sequential Representation based Framework for Question Answering", "abstract": "Recent work using artificial neural networks based on the distributed representation of words increases the performance of various tasks for learning natural language, especially when answering questions. However, they also involve some related problems, such as corpus selection for embedding learning, dictionary transformation for various learning tasks, etc. In this paper, we propose to model sentences simply using strings and then use Convolutionary Neural Networks to integrate character learning with training for point-to-point selection of answers. Compared to deep models prepared for the strategy of text embedding (WE), our method of sequential representation (CSR) demonstrates a much simpler process and more stable performance across different benchmarks. Extensive experiments with two benchmark response datasets show the competitive performance compared to modern methods.", "histories": [["v1", "Fri, 2 Sep 2016 11:57:46 GMT  (160kb,D)", "http://arxiv.org/abs/1609.00565v1", "to be accepted as CIKM2016 short paper"]], "COMMENTS": "to be accepted as CIKM2016 short paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lingxun meng", "yan li", "mengyi liu", "peng shu"], "accepted": false, "id": "1609.00565"}, "pdf": {"name": "1609.00565.pdf", "metadata": {"source": "CRF", "title": "Skipping Word: A Character-Sequential Representation based Framework for Question Answering", "authors": ["Lingxun Meng", "Yan Li", "Mengyi Liu", "Peng Shu"], "emails": ["menglingxun@sogou-", "mengyi.liu}@vipl.ict.ac.cn", "shupeng203672@sogou-", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords semantic matching; deep learning; convolutional neural networks; word embeddings;"}, {"heading": "1. INTRODUCTION", "text": "Inspired by the achievements of convolutional networks (a.k.a, ConvNets) in the field of computer vision, more and more researchers constitute ConvNets for kinds of natural language processing tasks, e.g., text classification [5], short text pair re-ranking [14, 11], e.t.c. Nearly all of them pointed out that ConvNets could successfully capture local syntactic structure to boost performance. In addition to the appropriate use of ConvNets, researchers also owed their success to recent distributed representation of words, like word2vec [9]. No doubt perfect word embedding (WE) could greatly boost the performance, but there are so many English word embedding systems available online, trained with different\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM\u201916 , October 24-28, 2016, Indianapolis, IN, USA c\u00a9 2016 ACM. ISBN 978-1-4503-4073-1/16/10. . . $15.00\nDOI: http://dx.doi.org/10.1145/2983323.2983861\ncorpus, algorithms and preprocessing methods. Thus, evaluating the effectiveness of different word embedding systems becomes a laborious job. Moreover, for other languages, like Chinese, there exists few word embedding systems available, so this further makes the selection of right corpus, right word segmentation tools and even right algorithms more expensive and difficult.\nRecently, some researchers start to use character representation to constitute different natural language models based on the fact that character embedding model not only alleviates the parametric burden significantly, but also brings more ability of handling morphologically rich language and out-of-vocabulary words [6]. Besides, character-level representation has also been successfully investigated for text classification [16], where one-hot representation is feed into Deep ConvNets. No matter what algorithms are used to construct task-specific semantic learning framework, the same concept is shared that characters make words, and then words construct whole sentences. In fact of reality, we believe that root word is the primary lexical unit of comprehension, so direct construction from characters to sentences is reasonable and enough for semantic learning which forms our original motivation.\nIn this paper, we take the first attempt to construct text matching based on character-sequential representation (CSR). Different from previous works, we first directly model sentences as sequences of characters, skipping the traditional word-level comprehension, and then utilize shallow weightsharing ConvNets to learn the matching task. Experiments on two popular question answering benchmarks show the competitive performance compared with the other state-ofthe-art methods."}, {"heading": "2. SEMANTIC MATCH LEARNING", "text": "Text matching is generally believed as the key of many natural language processing tasks, such as question answering, information retrieval and relevance classification. The majority of previous works formulated sentence, phrase or text using semantic parsing methods, which require experts to handcraft grammars and knowledge base schema [1]. Recently, lots of works spring up by using ConvNets to constitute text based on word representation [11]. Our method follows this constitution, but replaces the word look-up table with more fundamental character look-up table, and the look-up table, convolutional filters and normalization parameters are shared between question-answer pairs (see Fig. 1 for better understanding).\nar X\niv :1\n60 9.\n00 56\n5v 1\n[ cs\n.C L\n] 2\nS ep\n2 01\n6"}, {"heading": "2.1 Character-Sequential Representation", "text": "The sentence in this paper is represented as a sequence of characters: [c0, ..., c|s|\u22121], where ci is drawn from the character set C. Each character is represented by its corresponding index in the character set C, namely {0, \u00b7 \u00b7 \u00b7 , |C| \u2212 1}. To learn the character representation, we design the first layer of our model as a look-up table, i.e., character embedding matrix W \u2208 Rd\u00d7|C|, where each column represents a character embedding c which is d-dimensional. Forwarding from this layer, we get our sentence represented as follow:\nS = [c0 c1 \u00b7 \u00b7 \u00b7 c|s|\u22121]. (1)\nDifferent from previous character embedding methods, we construct sentences directly from characters, skipping the transitional step of modeling word-level comprehension (i.e., character-sentence rather than character-word-sentence). We think the comprehension unit is radical, so semantic learning from character representation is feasible."}, {"heading": "2.2 Convolution and Pooling", "text": "The convolution layer consists of a filter bank F \u2208 Rn\u00d7c\u00d7w, along with filter biases b \u2208 Rn, where n and w refer to the number and width of filters respectively, and c denotes the channels of data from the lower layer. More specifically, for the first convolution layer, c equals to the embedding dimension d, which means to convolve across the characters to learn the pattern. In our construction, all the convolution operation is one-dimensional one. Formally, the output of the convolution with filter bank F over sentence S is computed as follow:\nT = [ti,j ] = F \u2217 S + b = [F Ti Sj\u2212w+1:j + bi] (2)\nwhere i indexes the number of filter bank F , Fi \u2208 R(d\u2217w) is the vectorization of each filter map, Sj\u2212w+1:j vectorizes a sliding window matrix covering embeddings from the index (j \u2212 w + 1)th to jth.\nThe above equation formulates two types of convolution: wide and narrow . The previous works [4] pointed out that\nusing wide type of convolution was able to better and more frequently reach boundaries of sentences than the narrow type. More importantly, the wide convolution operation could ignore the requisition of the narrow type that filter width must be smaller than input data width to guarantee a valid non-empty result. In our implementation, we use narrow type since our character based sequence is not sensitive to boundary information that much. However, we consist that using wild type may always guarantee a better performance. In this way, we get the output T \u2208 Rn\u00d7(|s|\u2212w+1).\nPooling operation is often used to eliminate the differences in length for sentence representation, and it is also benefit to filter out unimportant or noisy characters and words. Technically, there exist two types of pooling strategy, i.e., average pooling and max pooling, and max pooling is more widely used due to its fast convergence and less computational complexity. Recently, k-max pooling strategy [4] was proposed to extract k large activation values rather than the topmost one, and this could construct a deeper architecture with several convolutional layers. For simplification, we adopt the normal max pooling strategy in this work. We get our result, pool(T ): Rn\u00d7(|s|\u2212w+1) \u2192 Rn."}, {"heading": "2.3 Batch Normalization", "text": "Batch Normalization (BN) [2] was originally proposed to reduce the changes in distribution of each layers input during training, which was called internal covariance shift . As an important component of deep networks, it allows us to use much higher learning rates and be less careful about initial data preprocessing and weights initialization. Furthermore, it can also act as a regularization, in some cases eliminating the use of dropout.\nSpecifically, for the given output T after the convolution operation, we will normalize each ti along the filter number axis as follow:\nt\u0302i = ti \u2212 E[ti]\u221a\nVar[ti] , (3)\nwhere the expectation and variance are computed over the whole training data set. A pair of parameters \u03b3i and \u03b2i (with the same dimension with ti) are further used to scale and shift the normalized value as follow:\nBN(T ) = [\u03b3i \u00b7 t\u0302i + \u03b2i]. (4)\nAbove parameters are learnt along with the whole model training, and it usually suggests that Batch Normalization should better be used before nonlinearity activation and covers both fully connected layers and convolutional layers.\n2.4 Pointwise Learning to Rank Since our character-sequential model focuses on binary classification task, e.g., question answering, we deploy the cross-entropy loss to discriminatively train our framework as follow:\nL\u03b8 = \u2212 1\nN N\u2211 i=1 M\u2211 j=1 (y (i) j log p (i) j ) + \u03bb\u2016F \u2016 2 (5)\nwhere p (i) j is the j th probability output of sample ith through our networks, y (i) j is the corresponding ground truth, \u03b8 contains all the parameters, i.e., \u03b8 = {W ,F , b,\u03b3,\u03b2}, \u03bb is set to be 5e\u22124.\nWe use Stochastic Gradient Descent (SGD) to optimize our network, and AdaDelta is used to automatically adapt the learning rate during the training procedure. For higher performance, hyper-parameter selection is conducted on the development set, and BN layer after each convolution layer is also added to speed up the network optimization. In addition, dropout is applied after the first hidden layer for regularization, and early stopping is used to prevent over fitting with a patience of 5 epochs."}, {"heading": "3. EXPERIMENTS", "text": "We evaluate the proposed method on two benchmarks of answer selection problem, i.e., TrecQA [12] and WikiQA [13]."}, {"heading": "3.1 Datasets", "text": "TrecQA1 was collected from Text Retrieval Conference (TREC) QA track (8-13) data, and the correctness of the selected answer was guaranteed by manual judgment for parts of the dataset.\nWikiQA2 is another open domain question-answer dataset, collected from real queries of Bing search engine without hu-\n1http://cs.stanford.edu/people/mengqiu/data/ qg-emnlp07-data.tgz 2http://research.microsoft.com/en-us/downloads/ 4495da01-db8c-4041-a7f6-7984a4f6a905/\nman editorial revision. Table 1 summarizes the statistics on the two datasets."}, {"heading": "3.2 Experimental Setup", "text": "We set the maximum word length of questions and answers to be 30/80 for TrecQA dataset, and 50/50 for WikiQA dataset respectively. Alike, we set the maximum character length of questions and answers to be 192/386 for TrecQA dataset, and 125/386 for WikiQA dataset respectively. All the words can not be found in the pre-trained word look-up table indexed by an extra index named UNK IND.\nFollowing [16], our character set C also consists of 71 characters, including 26 english lower letters, 10 digits, 33 other characters, the new line character, the padding symbol and the unknown symbol. The characters are as follows:\nabcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:\u2019\u201d/\\delimiter\u201d026B30D @#$%\u02dc\u02c6\u2018&*+-=<>()[]{}"}, {"heading": "4. RESULTS AND DISCUSSIONS", "text": "The configuration parameters of both word embedding (WE) and character-sequential representation (CSR) based networks are listed in Table 2 (we carefully implemented WE based network architecture according to [11]). Additionally, similarity weight matrix (sim(x, y) = xTMy, where M is the similarity matrix) between question answer pairs is learnt along with WE models training, that is mentioned in [11] to perform a better matching. For fair comparison, the simple word-level overlap feature and IDF-weighted word overlap feature are used in both WE and CSR based models.\nWe use the open deep learning framework Caffe [3] to complete our implementation. Due to the randomness caused by max-pooling implementation in Caffe, we run 10 times the training and report the mean and variance value as our final result.\nFor quantitative evaluation, we use two classic measurements, i.e., Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), which are consistent with previous works. Specifically, the official trec eval scorer tool is used to compute the above measurements3.\nWe use three sets of word embedding for WE based models: (i) word2vec4 [9] is trained on part of Google News dataset, containing 100 billion words; (ii) GloVe-twitter [10] is trained on 2 billion tweets, containing 27 billion tokens; (iii) GloVe-6B5 is trained on Wikipedia data and the\n3http://trec.nist.gov/trec eval/ 4https://code.google.com/archive/p/word2vec/ 5http://nlp.stanford.edu/projects/glove/\nfifth English Gigawords with totally 6 Billion tokens. The dimension of our character embedding for both datasets is set to 50, and standard setup is adopted of considering questions that have both correct and negative answers for evaluation. Table 3 shows the results of three versions of WE models, and please kindly note that our WE based model implementations are all slightly better than the original paper reports. We can also see that different pre-trained WE model results in unstable performance even in the same dataset, especially TrecQA, and the same embedding tool shows totally different performance on different datasets. Although our CSR based model adopts end-to-end strategy to learn the representation directly from the data without extra corpus, it exhibits competitive result in both datasets.\nTable 4 compares the proposed model with the previous state-of-the-art methods. Different from the comparative methods using noisy sampled TRAIN-ALL as training set, we report our final results trained on the TRAIN set, which is much smaller than TRAIN-ALL. It is obvious that the proposed CSR exhibits its effectiveness, even without any extra-corpus for embedding pre-training."}, {"heading": "5. CONCLUSION", "text": "In this paper, we address the problem of word embedding (WE) based answer selection that extra-corpus are needed for perfect word embedding. To solve this problem, we propose an end-to-end character-sequential representation (CSR) based ConvNets for answer selection, which simply models sentences by character sequences, and then integrates character embedding learning with point-wise answer ranking training. Extensive experiments provide further evidence that construction directly from characters to sentences works as well as or even better than the character-wordsentence hierarchical formula. We hope this work can bring some insights to the research and industrial communities\nthat corpus-free and skip-word representation from character directly to sentence as an end-to-end strategy could play its role on various languages with its advantages of simplification and concise body."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We thank all the co-workers in wireless sponsored search advertising (ADWR) team for details discussion and consummation. The authors also thank Dingding Qian and Kai Chen for the paper submission suggestion. This paper is partially supported by Natural Science Foundation of Zhejiang Province, P.R.China under contract No.LQ15F020005."}, {"heading": "7. REFERENCES", "text": "[1] J. Berant, A. Chou, R. Frostig, and P. Liang.\nSemantic parsing on freebase from question-answer pairs. In EMNLP, 2013.\n[2] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, 2015.\n[3] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. CoRR, 2014.\n[4] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural network for modelling sentences. In ACL, pages 655\u2013665, 2014.\n[5] Y. Kim. Convolutional neural networks for sentence classification. EMNLP, 2014.\n[6] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language models. AAAI, 2016.\n[7] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. CoRR, 2014.\n[8] Y. Miao, L. Yu, and P. Blunsom. Neural variational inference for text processing. CoRR, 2015.\n[9] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013.\n[10] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, pages 1532\u20131543, 2014.\n[11] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR, pages 373\u2013382. ACM, 2015.\n[12] M. Wang, N. A. Smith, and T. Mitamura. What is the jeopardy model? a quasi-synchronous grammar for qa. In EMNLP-CoNLL, volume 7, pages 22\u201332, 2007.\n[13] Y. Yang, W.-t. Yih, and C. Meek. Wikiqa: A challenge dataset for open-domain question answering. In EMNLP, pages 2013\u20132018. Citeseer, 2015.\n[14] W.-t. Yih, M.-W. Chang, C. Meek, and A. Pastusiak. Question answering using enhanced lexical semantic models. ACL, 2013.\n[15] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman. Deep learning for answer sentence selection. NIPS Deep Learning and Representation Learning Workshop, 2014.\n[16] X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classification. In NIPS, pages 649\u2013657, 2015."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Y. Yang", "W.-t. Yih", "C. Meek"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["W.-t. Yih", "M.-W. Chang", "C. Meek", "A. Pastusiak"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 54, "endOffset": 62}, {"referenceID": 8, "context": "In addition to the appropriate use of ConvNets, researchers also owed their success to recent distributed representation of words, like word2vec [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "Recently, some researchers start to use character representation to constitute different natural language models based on the fact that character embedding model not only alleviates the parametric burden significantly, but also brings more ability of handling morphologically rich language and out-of-vocabulary words [6].", "startOffset": 318, "endOffset": 321}, {"referenceID": 15, "context": "Besides, character-level representation has also been successfully investigated for text classification [16], where one-hot representation is feed into Deep ConvNets.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "The majority of previous works formulated sentence, phrase or text using semantic parsing methods, which require experts to handcraft grammars and knowledge base schema [1].", "startOffset": 169, "endOffset": 172}, {"referenceID": 10, "context": "Recently, lots of works spring up by using ConvNets to constitute text based on word representation [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The previous works [4] pointed out that using wide type of convolution was able to better and more frequently reach boundaries of sentences than the narrow type.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Recently, k-max pooling strategy [4] was proposed to extract k large activation values rather than the topmost one, and this could construct a deeper architecture with several convolutional layers.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Batch Normalization (BN) [2] was originally proposed to reduce the changes in distribution of each layers input during training, which was called internal covariance shift .", "startOffset": 25, "endOffset": 28}, {"referenceID": 11, "context": ", TrecQA [12] and WikiQA [13].", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": ", TrecQA [12] and WikiQA [13].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "Following [16], our character set C also consists of 71 characters, including 26 english lower letters, 10 digits, 33 other characters, the new line character, the padding symbol and the unknown symbol.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "The configuration parameters of both word embedding (WE) and character-sequential representation (CSR) based networks are listed in Table 2 (we carefully implemented WE based network architecture according to [11]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "Additionally, similarity weight matrix (sim(x, y) = xMy, where M is the similarity matrix) between question answer pairs is learnt along with WE models training, that is mentioned in [11] to perform a better matching.", "startOffset": 183, "endOffset": 187}, {"referenceID": 2, "context": "We use the open deep learning framework Caffe [3] to complete our implementation.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "We use three sets of word embedding for WE based models: (i) word2vec [9] is trained on part of Google News dataset, containing 100 billion words; (ii) GloVe-twitter [10] is trained on 2 billion tweets, containing 27 billion tokens; (iii) GloVe-6B is trained on Wikipedia data and the http://trec.", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "We use three sets of word embedding for WE based models: (i) word2vec [9] is trained on part of Google News dataset, containing 100 billion words; (ii) GloVe-twitter [10] is trained on 2 billion tweets, containing 27 billion tokens; (iii) GloVe-6B is trained on Wikipedia data and the http://trec.", "startOffset": 166, "endOffset": 170}, {"referenceID": 11, "context": "Jeopardy[12] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "6852 PV[7] .", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "6058 LCLR[14] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "7700 Bigram[15] .", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "6086 Bigram[15] .", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "7846 LCLR[14] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "6190 NASM [8] .", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "8117 CNN[13] .", "startOffset": 8, "endOffset": 12}], "year": 2016, "abstractText": "Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}