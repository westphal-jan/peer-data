{"id": "1609.00843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "An Online Universal Classifier for Binary, Multi-class and Multi-label Classification", "abstract": "Classification involves learning the mapping function that links input patterns to the corresponding target label. There are two main categories of classification problems: single and multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there is no classifier in the literature that can perform all three types of classification. In this paper, a novel universal online classifier is proposed that can perform all three types of classification. As it is a high-speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated on the basis of data sets of binary, multi-class and multi-label problems.", "histories": [["v1", "Sat, 3 Sep 2016 17:03:14 GMT  (845kb)", "http://arxiv.org/abs/1609.00843v1", "6 pages, 6 tables"]], "COMMENTS": "6 pages, 6 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["meng joo er", "rajasekar venkatesan", "ning wang"], "accepted": false, "id": "1609.00843"}, "pdf": {"name": "1609.00843.pdf", "metadata": {"source": "CRF", "title": "An Online Universal Classifier for Binary, Multi- class and Multi-label Classification", "authors": ["Meng Joo Er", "Ning Wang"], "emails": ["EMJER@ntu.edu.sg", "RAJA0046@e.ntu.edu.sg", "n.wang.dmu.cn@gmail.com"], "sections": [{"heading": null, "text": "function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are subcategories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types.\nKeywords\u2014Universal, Classification, Binary, Multi-class,\nMulti-label, Online, Extreme learning machines, Data stream.\nI. INTRODUCTION\nMachine learning classification is the process of approximating the mapping function that maps the input sample to target class/label [1]. In traditional classification problems, the input samples correspond to only one target label. This type of classification is called single-label classification. Binary classification involves classifying the input data samples into either of two sets based on a specific classification metric. The number of disjoint labels is 2 for binary classification. Disease diagnosis [2, 3], quality control, spam detection [4], malware detection are some of the major application areas of this method. But there are several real world application problems involving multiple target labels resulting in the development of multi-class classification. Multi-class classification involves classifying the input samples into more than two classes. Character recognition [5, 6], biometric identification [7] and security, face recognition are some of the application areas of multi-class classification. The single-label classification techniques are based on the assumption of unique target label association. i.e. each input sample corresponds to only one target label. In other words, the labels corresponding to the input samples form a disjoint set.\nHowever, in many real world applications, the input samples correspond to multiple target labels. This condition of\nclassification, where the input data correspond to a set of class labels instead of one, is called multi-label classification. Multilabel classification has become a rapidly emerging field of machine learning due to the wide range of application domains and the omnipresence of multi-label problems in real world scenarios [8]. The application areas of multi-label classification includes image, music and video categorization [9], medical diagnosis, bioinformatics, multimedia, genomics etc. In contrast to single-label classification, each sample may have multiple target labels in multi-label classification [10]. The generalization of multi-label classification results in the increased complexity of the classifier. Different classification techniques based on multi-layer perceptrons (MLP), decision trees (DT), k-nearest neighbors (kNN), support vector machine (SVM), extreme learning machine (ELM), na\u00efve bayes classifier etc. have been developed for each of the classification types and is available in the literature [1, 11-14].\nBased on the learning style, the machine learning techniques can be classified into batch learning and online learning techniques. In batch learning, the data required for the training of the classifier are collected in prior. The entire training data is processed concurrently for the estimation of the system parameters. The requirement to have all the training data in prior to training poses a serious constraint in the application of batch learning techniques. On the other hand, in online learning, the system parameters are updated in an iterative manner with sequential data. Therefore, online learning techniques are preferred over batch learning techniques for streaming data applications [15, 16].\nThere are several machine learning techniques available for binary, multi-class and multi-label classifications individually. There are no classifiers available in the literature that is capable of performing all three types of classification. In this paper, we propose an extreme learning machine based online universal classifier that is independent of classification type and can perform all three types of classification (binary, multi-class and multi-label). The developed classifier will identify both the classification type and the target labels associated with the input samples. The proposed classifier is experimented with datasets corresponding to each of the three classification types and is evaluated for consistency, speed and performance. The results are compared with the state-of-the-art techniques of the individual classification type.\nThe rest of the paper is organized as follows. Section 2 briefly summarizes the background information pertaining to the different classification types and on extreme learning machines. Section 3 describes the steps involved in the proposed approach. The details of the experimental design and the dataset specifications are discussed in Section 4. Section 5 summarizes the performance of the proposed classifier and the comparison of the experimental results with state-of-the-art techniques and concluding remarks are given in Section 6.\nII. BACKGROUND AND PRELIMINARIES"}, {"heading": "A. Classification", "text": "Based on the label association to the input samples, the classification methods can be categorized into single-label classification and multi-label classification.\n1) Single-label Classification\nSingle label classification is a function approximator that associates the input samples to a unique target label \u2018l\u2019 from a set of disjoint labels \u2018L\u2019. The single-label classification problem can be further divided into two categories: Binary and multi-class classification [10]. When the input data samples are categorized into one of two classes, it is called binary classification. When the input samples correspond to one among a pool of target labels, it is called multi-class classification. Binary classification is the most basic classification and forms the basic requirement a technique should fulfill to be a classification method. Hence, all the classification methods and the variants available in the literature thus far can be used for binary classification. There are several methods existing in the literature to solve the multiclass classification problems. The existing multi-class classification methods can be classified into three groups.\n Extended methods from binary classification\n Decomposition to binary classification methods\n Hierarchical Classification methods\nExtended Methods from Binary Classification. Some of the binary classification techniques can be directly extended to support the multi-class classification problems. Multi-class classification techniques based on multi-layer perceptron, decision trees, k-nearest neighbors, support vector machines, extreme learning machines and na\u00efve bayes classifier are examples of algorithm adaptation methods.\nDecomposition to Binary Classification Methods. The decomposition methods, as the name implies decompose the multi-class classification problem into multiple binary classification problems and employs the existing binary classifiers to solve it. Several methods have been proposed in the literature [11] that use decomposition to solve the multiclass classification problems [17, 18].\nHierarchical Classification. In hierarchical classification, the classes are arranged in the form of a tree. The parent node is divided to have leaf nodes such that each of the leaf node classes will be the subset of parent node classes. The similar procedure is extended until the leaf nodes have only one class\nlabel [19]. Realization of each of the nodes in the tree is performed using a binary classifier.\n2) Multi-label Classification\nMulti-label classification has gained much importance in recent years due to its wide range of application domains. As opposed to single-label classification, each input sample is associated with a set of target labels in multi-label classification. The number of target labels corresponding to each input is not fixed and varies dynamically. This results in increased complexity in the implementation of multi-label classifier [20]. Several methods have been developed for multilabel classification and is available in the literature. The existing techniques are grouped under three categories [10, 21].\n Algorithm Adaptation Methods\n Problem Transformation Methods\n Ensemble Methods\nAlgorithm Adaptation Methods. In the algorithm adaptation method, the base algorithm itself is extended to adapt for the multi-label classification. Several base algorithms have their multi-label variants such as Boosting, kNN, Decision Trees, Neural Networks, and SVM.\nProblem Transformation Methods. In the problem transformation method, the multi-label classification is transformed to multiple binary or multi-class classification problems. Upon transforming the multi-label problem into multiple single-label problem, this technique utilizes the existing single-label classifiers to perform the classification and combining the results of all the single-label classifiers to find results for multi-label classification.\nEnsemble Methods. Ensemble methods use an ensemble of algorithm adaptation and problem transformation methods and combine the results to perform multi-label classification.\nThe proposed method belongs to the category of algorithm adaptation method. The base algorithm itself is extended to adapt to all classification types."}, {"heading": "B. Extreme Learning Machine", "text": "Extreme learning machine (ELM) is proposed by Huang et al [22] has gained much attention due to its unique advantage of very high learning speed and random assignment of input weights. The universal approximation capability of single layer feedforward neural network is also preserved in ELM. Several variants of ELM have been developed and is available in the literature [23-28]. The proposed approach uses ELM based online universal classifier. A condensed overview of ELM is discussed below.\nIn ELM, the input weights of the neural network are randomly assigned. Therefore, only the output weights of the network are to be trained. Let N be the number of training\nsamples and be the number of hidden layer neurons. The output equation of ELM based network in matrix form is represented as\nH\u03b2 = Y (1)\nwhere H is the hidden layer output matrix of the network. The outputs of the hidden layer neurons corresponding to each input sample is populated as the column values of the H matrix. H is given by,\n(2)\nxi = [xi1,xi2,\u2026,xin]T is the input data sample of dimension n, g(x) is the activation function, wi = [wi1,wi2,\u2026win]T is the input weight vector and b is the bias value of the network. \u03b2 is the output weight matrix, Y is the target output corresponding to the input samples. During the training phase, the input sample and the output labels are given as inputs and the output weights of the ELM network is estimated using the equation\n\u03b2 = H+Y (3)\nH+ = (HTH)-1HT gives the Moore-Penrose inverse of the H matrix. In the testing phase, the data samples are provided as the input and with the estimated \u03b2 values, the target labels corresponding the input is predicted by the network. The mathematical background behind the functioning of the ELM has been extensively discussed in the literature [29, 30].\nIII. PROPOSED APPROACH\nAn online universal classifier capable of performing classification on binary, multi-class and multi-label datasets is proposed. It is to be highlighted that there are no universal classifiers available in the literature that can classify all three classification types. Also, the proposed method is an online classifier and hence can be used for streaming data applications. The generality of the problem specification results in increased complexity in achieving universal classification technique. There are three key challenges to be addressed to achieve universal classifier.\n1. Identification of classification type\n2. Estimating the number of target labels corresponding to each input sample\n3. Identifying each of the associated target labels.\nThe proposed approach is based on the online variant of ELM called online sequential extreme learning machine. The proposed approach falls under the category of an algorithm adaptation method in which the base algorithm is extended to adapt to the requirements of the universal classification. The various phases of the proposed algorithm are summarized.\nInitialization Phase. Initialization Phase involves setting up the fundamental network parameters for the target classification problem. Being an ELM based technique, the input weights and the bias values are randomly initialized. The number of hidden layer neurons and the activation function are assigned. The number of hidden layer neurons is to be selected such that the problem of overfitting is avoided.\nData Pre-processing Phase. The proposed algorithm needs to be capable of classifying both single-label and multi-label classification problems. The representation of data varies\namong each of the classification types. In binary and multiclass classification, the output is represented as a single value which identifies the unique target class that is associated with the input sample. On the other hand, in multi-label classification, since each input can have multiple labels, the output is represented as a vector with dimensions equal to the total number of output labels. Thus, proper pre-processing of data is essential in achieving universal classifier. In the proposed approach, the target label of all three classification types is represented as a vector with dimension equal to the number of output labels. Each element in the vector signifies the belongingness of the input to the corresponding label.\nOnline Training Phase. During the training phase, the data samples and the target labels are provided as the input and the output weight values are estimated iteratively by online training. The proposed method is based on the online variant of ELM. The online training phase has two steps.\nInitial Block Step: Let N0 be the number of training samples in the initial block of data, the initial output weight values are calculated using equations\nM0 = (H0TH0)-1 (4)\n\u03b20 = M0H0TY (5)\nSequential Training Step: Upon completion of the initial block step, the subsequent data samples arriving sequentially are processed in the sequential training step. The output weight is updated iteratively with sequentially arriving data blocks using the recursive least square technique [24, 26]. The sequential update of output weight is given by the equations\n(6)\n(7)"}, {"heading": "By the end of the training phase, the values of \u03b2 are estimated.", "text": "Testing Phase. In the testing phase, the target output of the input samples is predicted using the values of \u03b2 estimated from the training phase and the input data samples. The raw output values of the network are evaluated using the relation Y = H\u03b2. The raw output value obtained from the testing phase is then processed to address the three challenges of the universal classifier.\nClassification Phase. In the classification phase, the raw output values Y obtained from the training phase is used to predict the classification type, number of associated target labels and identifying each of the target labels corresponding to each input sample.\nIdentifying the Classification Type: The classification type of binary, multi-class or multi-label is identified using the classification type (CT) value and dimension of output vector \u2018l\u2019. The CT value is evaluated using the equation.\n(8)\nwhere Y is the raw output vector and HS(x) is the heaviside function. Identification of classification type based on all possible valid combinations of CT and L is given in Table 1.\nEstimating the Number of Target Labels: Upon establishing the classification type, the number of target labels is then estimated as given in Table 2. For binary and multi-class classification, the number of target labels is one, since each input belongs to unique target labels. For multi-label classification, the CT value corresponds to the number of target labels associated with the input data sample.\nIdentifying the Target Labels: The target labels are identified using the belongingness vector. The belongingness vector B is given as,\nB = HS(Y) (9)\nwhere Y is the raw output value and HS(x) is Heaviside function. Each element of the vector B denotes the belongingness of the input to the corresponding label. Thus, the label index of the non-zero entries of the B gives the target labels associated with the input samples. Upon estimating the target labels, the performance metrics of the classifier are evaluated. Thus, the proposed technique is capable of classifying all three types of classification problems. The overview of the proposed approach is summarized.\nIV. EXPERIMENTATION\nThe experimental design, dataset specifications and the comparison methods used to evaluate the proposed method are discussed in this section. Since the proposed method is capable of classifying all the classification types, the datasets from both single-label and multi-label classification problems are chosen for experimentation. In multi-label problems, different datasets have different degree of multi-labelness. Therefore, two metrics, Label cardinality and label density are used to quantitatively measure the degree of multi-labelness. Label cardinality gives the average number of labels corresponding to the input data. Therefore, for binary and multi-class classification, label cardinality will always be 1. Label density on the other hand also considers the number of labels in evaluation. Label cardinality and label density are very important metrics in the dataset specification for multi-label data. For example, label cardinality of 4.24 signifies that each of the input samples corresponds to more than 4 labels on an average. Two datasets having same label cardinality, but different label density can significantly vary the performance of the classifier. The specifications of the dataset used for experimentation are given in the Table 3.\nAlgorithm: Proposed Universal Classifier Algorithm\n1. Initialization of parameters 2. Formatting input to uniform representation 3. Initial block training\nInput: Initial N0 samples of data in the form {(xi,yi)} Output: \u03b20 Evaluation: M0 = (H0TH0)-1 \u03b20 = M0H0TY0\n4. Sequential training Input: Sequentially arriving data blocks in the form\n{(xi,yi)} Output: \u03b2k Evaluation:\n5. Evaluating raw output value Y Input: Data sample xi\nOutput: Y\nEvaluation:\nY = H\u03b2\n6. Evaluating CT value: 7. Identifying classification type based on CT and L 8. Evaluating number of associated target labels 9. Calculating the belongingness vector: B=H(Y) 10. Identifying the associated target labels 11. Evaluation of performance metrics corresponding to\nclassification type\nThe proposed method is evaluated using 5 single-label (2 binary and 3 multi-class) datasets and 4 multi-label datasets. The datasets cover a wide range of feature dimension, number of labels, label density and label cardinality. The performance results of the proposed method on these datasets are compared with state-of-the-art techniques in the specific classification type. The performance of the proposed method on single-label classification datasets are compared with Support Vector Machine (SVM), kNN (k-Nearest Neighbor), MLP (Multilayer Perceptron) and ELM (Extreme Learning Machine) based techniques. The results of the multi-label classification datasets are compared with the state-of-the-art methods based on SVM, kNN, DT (Decision Trees) and RF (Random Forest).\nV. RESULTS AND DISCUSSIONS\nThis section summarizes the experimental results of the proposed universal classifier on the datasets specified in table. Being an online method, the proposed algorithm can be used for streaming data applications."}, {"heading": "A. Consistency", "text": "Consistency is one of the key virtues of any new technique developed. An algorithm that is inconsistent with results on different trials is unreliable. Cross-validation is one of the effective ways to evaluate the consistency of the method. A 10- fold cross validation is performed for each of the datasets. In single-label classification problems, since each of the sample belongs to only one output, the performance of the classifier can be evaluated using the percentage of accuracy.\nTABLE 3. DATASET SPECIFICATIONS\nClassification type Dataset Number of\nlabels\nFeature\ndimension\nNumber of\nSamples\nLabel\nCardinality\nLabel\nDensity\nSingle-\nlabel\nBinary Diabetes 2 8 768 1.00 0.500\nIonosphere 2 34 351 1.00 0.500\nMulti-\nclass\nIris 3 4 150 1.00 0.333\nWaveform 3 21 5000 1.00 0.333\nBalance-scale 3 4 625 1.00 0.333\nMulti-label\nScene 6 294 2407 1.07 0.178\nYeast 14 103 2417 4.24 0.303\nCorel5k 374 499 5000 3.53 0.009\nEnron 53 1001 1702 3.38 0.064\nHowever, multi-label classification poses a unique\nproblem of partial correctness of the results. Therefore, a\ndifferent set of performance metrics is used for evaluation.\nHamming loss is one of the key performance metric for multi-\nlabel classification. It is the quantitative measure of the\nnumber of times the sample-label pair is misclassified. Lower\nthe hamming loss, better the performance of the classifier.\nHamming loss is evaluated as the summation of misclassified\nsample-label pair averaged over the total number of samples\nand labels. The performance of the binary and multi-class\nclassifier is evaluated using percentage of accuracy and the\nperformance of multi-label classifier is evaluated using\nhamming loss for the consistency evaluation. The results\nobtained are tabulated in Table 4. From the table, it can be\nseen that the proposed universal classifier is highly consistent\nfor all datasets from binary, multi-class and multi-label\nclassification."}, {"heading": "B. Speed", "text": "The execution speed of the proposed classifier is evaluated in terms of training time and testing time. Execution speed of the classifier plays a vital role for streaming data applications. In order to perform real-time streaming data classification, the execution speed of the classifier should be less than the arrival rate of the streaming data. Therefore, a high speed classifier is essential for real-time streaming data applications. The proposed universal classifier exploits the inherent high-speed nature of the ELM. The training time and the testing time of the proposed universal algorithm for each dataset is given in Table 4. From the table, it is evident that the proposed classifier is capable of performing classification of all types with high speed, thus facilitating its application for real-time streaming data."}, {"heading": "C. Performance Comparison", "text": "There are no universal classifier available in the literature to perform direct comparison with the proposed method. Therefore, the performance of the proposed classifier is compared with the state-of-the-art techniques in each of the classification type. For single-label classification datasets, the performance of the proposed method is compared with similar binary and multi-class techniques based on SVM, kNN, MLP and ELM. For multi-label classification problems, the performance of the proposed classifier is compared with SVM, kNN, DT and RF based techniques.\npossibility of partial correctness in multi-label classification\nresults in the need for evaluation of other parameters such as\naccuracy and F1 measure for multi-label classification. The\nresults are tabulated in Table 6. From the comparison table it\nis evident that the proposed universal classifier performs\nuniformly well in datasets of all classification types.\nlearning machine is proposed. It is to be highlighted that there\nare no classifiers available in the literature that can classify\nbinary, multi-class and multi-label classification. The\nproposed online universal classifier is experimented with nine\ndifferent datasets of different classification types and the\nresults are compared with state-of-the-art techniques in each\ntype of classification problem. The proposed classifier is\nevaluated in terms of consistency, speed and performance. The\nhigh speed nature of the proposed classifier makes it suitable\nfor real-time streaming data applications.\nACKNOWLEDGEMENT\nThe authors would like to acknowledge the funding support\nfrom the Ministry of Education, Singapore (Tier 1 AcRF,\nRG30/14), the National Natural Science Foundation of P. R.\nChina (under Grants 51009017 and 51379002), Applied Basic\nResearch Funds from Ministry of Transport of P. R. China\n(under Grant 2012-329-225-060), and Pro-gram for Liaoning\nExcellent Talents in University (under Grant LJQ2013055).\nRajasekar Venkatesan is supported by NTU Research Student\nScholarship.\nREFERENCES\n[1] A. C. de Carvalho and A. A. Freitas, \"A tutorial on multi-label classification techniques,\" in Foundations of Computational Intelligence Volume 5, ed: Springer, 2009, pp. 177-195.\n[2] M. L. Gatza, J. E. Lucas, W. T. Barry, J. W. Kim, Q. Wang, M. D. Crawford, et al., \"A pathway-based classification of human breast cancer,\" Proceedings of the National Academy of Sciences, vol. 107, pp. 6994-6999, 2010.\n[3] K. Polat, S. G\u00fcne\u015f, and A. Arslan, \"A cascade learning system for classification of diabetes disease: Generalized discriminant analysis and least square support vector machine,\" Expert Systems with Applications, vol. 34, pp. 482-487, 2008.\n[4] Y. Song, A. Ko\u0142cz, and C. L. Giles, \"Better Naive Bayes classification for high\u2010precision spam detection,\" Software: Practice and Experience, vol. 39, pp. 1003-1024, 2009.\n[5] M. N. Ayyaz, I. Javed, and W. Mahmood, \"Handwritten Character Recognition Using Multiclass SVM Classification with Hybrid Feature Extraction,\" Pakistan journal of Engineering and Application Science, vol. 10, pp. 57-67, 2012.\n[6] B. P. Chacko, V. V. Krishnan, G. Raju, and P. B. Anto, \"Handwritten character recognition using wavelet energy and extreme learning machine,\" International Journal of Machine Learning and Cybernetics, vol. 3, pp. 149-161, 2012.\n[7] Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf, \"Deepface: Closing the gap to human-level performance in face verification,\" in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2014, pp. 1701-1708.\n[8] G. Tsoumakas, I. Katakis, and I. Vlahavas, \"Mining multi-label data,\" in Data mining and knowledge discovery handbook, ed: Springer, 2010, pp. 667-685.\n[9] M. Boutell, X. Shen, J. Luo, and C. Brown, \"Multi-label semantic scene classification,\" technical report, dept. comp. sci. u. rochester2003.\n[10] G. Tsoumakas and I. Katakis, \"Multi-label classification: An overview,\" Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006.\n[11] G. Bo and H. Xianwu, \"SVM multi-class classification,\" Journal of Data Acquisition & Processing, vol. 21, pp. 334-339, 2006.\n[12] M.-L. Zhang and Z.-H. Zhou, \"A review on multi-label learning algorithms,\" Knowledge and Data Engineering, IEEE Transactions on, vol. 26, pp. 1819-1837, 2014.\n[13] M. Sahare and H. Gupta, \"A review of multi-class classification for imbalanced data,\" International Journal of Advanced Computer Research, vol. 2, pp. 160-164, 2012.\n[14] Z.-H. Zhou, M.-L. Zhang, S.-J. Huang, and Y.-F. Li, \"Multi-instance multi-label learning,\" Artificial Intelligence, vol. 176, pp. 2291-2320, 2012.\n[15] M. Pratama, S. G. Anavatti, J. Meng, and E. D. Lughofer, \"pClass: An Effective Classifier for Streaming Examples,\" Fuzzy Systems, IEEE Transactions on, vol. 23, pp. 369-386, 2015.\n[16] M. Pratama, J. Lu, S. Anavatti, E. Lughofer, and C.-P. Lim, \"An Incremental Meta-Cognitive-based Scaffolding Fuzzy Neural Network.\"\n[17] M. Pal, \"Multiclass approaches for support vector machine based land cover classification,\" arXiv preprint arXiv:0802.2411, 2008.\n[18] C.-W. Hsu and C.-J. Lin, \"A comparison of methods for multiclass support vector machines,\" Neural Networks, IEEE Transactions on, vol. 13, pp. 415-425, 2002.\n[19] S. Kumar, J. Ghosh, and M. M. Crawford, \"Hierarchical fusion of multiple classifiers for hyperspectral data analysis,\" Pattern Analysis & Applications, vol. 5, pp. 210-220, 2002.\n[20] M.-L. Zhang and Z.-H. Zhou, \"ML-KNN: A lazy learning approach to multi-label learning,\" Pattern recognition, vol. 40, pp. 2038-2048, 2007.\n[21] G. Madjarov, D. Kocev, D. Gjorgjevikj, and S. D\u017eeroski, \"An extensive experimental comparison of methods for multi-label learning,\" Pattern Recognition, vol. 45, pp. 3084-3104, 2012.\n[22] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, \"Extreme learning machine: a new learning scheme of feedforward neural networks,\" in Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, 2004, pp. 985-990.\n[23] Y. Lan, Y. C. Soh, and G.-B. Huang, \"Ensemble of online sequential extreme learning machine,\" Neurocomputing, vol. 72, pp. 3391-3395, 8// 2009.\n[24] N.-Y. Liang, G.-B. Huang, P. Saratchandran, and N. Sundararajan, \"A fast and accurate online sequential learning algorithm for feedforward networks,\" Neural Networks, IEEE Transactions on, vol. 17, pp. 1411- 1423, 2006.\n[25] H.-J. Rong, Y.-S. Ong, A.-H. Tan, and Z. Zhu, \"A fast pruned-extreme learning machine for classification problem,\" Neurocomputing, vol. 72, pp. 359-366, 12// 2008.\n[26] B. Li, J. Wang, Y. Li, and Y. Song, \"An improved on-line sequential learning algorithm for extreme learning machine,\" Advances in Neural Networks\u2013ISNN 2007, pp. 1087-1093, 2007.\n[27] L. Jiahua, V. Chi-Man, and W. Pak-Kin, \"Sparse Bayesian Extreme Learning Machine for Multi-classification,\" Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, pp. 836-843, 2014.\n[28] W. Zong, G.-B. Huang, and Y. Chen, \"Weighted extreme learning machine for imbalance learning,\" Neurocomputing, vol. 101, pp. 229- 242, 2/4/ 2013.\n[29] S. Ding, H. Zhao, Y. Zhang, X. Xu, and R. Nie, \"Extreme learning machine: algorithm, theory and applications,\" Artificial Intelligence Review, vol. 44, pp. 103-115, 2013.\n[30] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, \"Extreme learning machine: theory and applications,\" Neurocomputing, vol. 70, pp. 489-501, 2006."}], "references": [{"title": "A tutorial on multi-label classification techniques", "author": ["A.C. de Carvalho", "A.A. Freitas"], "venue": "Foundations of Computational Intelligence Volume 5, ed: Springer, 2009, pp. 177-195.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A pathway-based classification of human breast cancer", "author": ["M.L. Gatza", "J.E. Lucas", "W.T. Barry", "J.W. Kim", "Q. Wang", "M.D. Crawford"], "venue": "Proceedings of the National Academy of Sciences, vol. 107, pp. 6994-6999, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A cascade learning system for classification of diabetes disease: Generalized discriminant analysis and least square support vector machine", "author": ["K. Polat", "S. G\u00fcne\u015f", "A. Arslan"], "venue": "Expert Systems with Applications, vol. 34, pp. 482-487, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Better Naive Bayes classification for high\u2010precision spam detection", "author": ["Y. Song", "A. Ko\u0142cz", "C.L. Giles"], "venue": "Software: Practice and Experience, vol. 39, pp. 1003-1024, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Handwritten Character Recognition Using Multiclass SVM Classification with Hybrid Feature Extraction", "author": ["M.N. Ayyaz", "I. Javed", "W. Mahmood"], "venue": "Pakistan journal of Engineering and Application Science, vol. 10, pp. 57-67, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten character recognition using wavelet energy and extreme learning machine", "author": ["B.P. Chacko", "V.V. Krishnan", "G. Raju", "P.B. Anto"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 3, pp. 149-161, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2014, pp. 1701-1708.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining multi-label data", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Data mining and knowledge discovery handbook, ed: Springer, 2010, pp. 667-685.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-label semantic scene classification", "author": ["M. Boutell", "X. Shen", "J. Luo", "C. Brown"], "venue": "technical report, dept. comp. sci. u. rochester2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "SVM multi-class classification", "author": ["G. Bo", "H. Xianwu"], "venue": "Journal of Data Acquisition & Processing, vol. 21, pp. 334-339, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "A review on multi-label learning algorithms", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 26, pp. 1819-1837, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1819}, {"title": "A review of multi-class classification for imbalanced data", "author": ["M. Sahare", "H. Gupta"], "venue": "International Journal of Advanced Computer Research, vol. 2, pp. 160-164, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-instance multi-label learning", "author": ["Z.-H. Zhou", "M.-L. Zhang", "S.-J. Huang", "Y.-F. Li"], "venue": "Artificial Intelligence, vol. 176, pp. 2291-2320, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "pClass: An Effective Classifier for Streaming Examples", "author": ["M. Pratama", "S.G. Anavatti", "J. Meng", "E.D. Lughofer"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 23, pp. 369-386, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiclass approaches for support vector machine based land cover classification", "author": ["M. Pal"], "venue": "arXiv preprint arXiv:0802.2411, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["C.-W. Hsu", "C.-J. Lin"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, pp. 415-425, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Hierarchical fusion of multiple classifiers for hyperspectral data analysis", "author": ["S. Kumar", "J. Ghosh", "M.M. Crawford"], "venue": "Pattern Analysis & Applications, vol. 5, pp. 210-220, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Pattern recognition, vol. 40, pp. 2038-2048, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["G. Madjarov", "D. Kocev", "D. Gjorgjevikj", "S. D\u017eeroski"], "venue": "Pattern Recognition, vol. 45, pp. 3084-3104, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on, 2004, pp. 985-990.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Ensemble of online sequential extreme learning machine", "author": ["Y. Lan", "Y.C. Soh", "G.-B. Huang"], "venue": "Neurocomputing, vol. 72, pp. 3391-3395, 8// 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, pp. 1411- 1423, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast pruned-extreme learning machine for classification problem", "author": ["H.-J. Rong", "Y.-S. Ong", "A.-H. Tan", "Z. Zhu"], "venue": "Neurocomputing, vol. 72, pp. 359-366, 12// 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "An improved on-line sequential learning algorithm for extreme learning machine", "author": ["B. Li", "J. Wang", "Y. Li", "Y. Song"], "venue": "Advances in Neural Networks\u2013ISNN 2007, pp. 1087-1093, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse Bayesian Extreme Learning Machine for Multi-classification", "author": ["L. Jiahua", "V. Chi-Man", "W. Pak-Kin"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, pp. 836-843, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Weighted extreme learning machine for imbalance learning", "author": ["W. Zong", "G.-B. Huang", "Y. Chen"], "venue": "Neurocomputing, vol. 101, pp. 229- 242, 2/4/ 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Extreme learning machine: algorithm, theory and applications", "author": ["S. Ding", "H. Zhao", "Y. Zhang", "X. Xu", "R. Nie"], "venue": "Artificial Intelligence Review, vol. 44, pp. 103-115, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70, pp. 489-501, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning classification is the process of approximating the mapping function that maps the input sample to target class/label [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Disease diagnosis [2, 3], quality control, spam detection [4], malware detection are some of the major application areas of this method.", "startOffset": 18, "endOffset": 24}, {"referenceID": 2, "context": "Disease diagnosis [2, 3], quality control, spam detection [4], malware detection are some of the major application areas of this method.", "startOffset": 18, "endOffset": 24}, {"referenceID": 3, "context": "Disease diagnosis [2, 3], quality control, spam detection [4], malware detection are some of the major application areas of this method.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Character recognition [5, 6], biometric identification [7] and security, face recognition are some of the application areas of multi-class classification.", "startOffset": 22, "endOffset": 28}, {"referenceID": 5, "context": "Character recognition [5, 6], biometric identification [7] and security, face recognition are some of the application areas of multi-class classification.", "startOffset": 22, "endOffset": 28}, {"referenceID": 6, "context": "Character recognition [5, 6], biometric identification [7] and security, face recognition are some of the application areas of multi-class classification.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Multilabel classification has become a rapidly emerging field of machine learning due to the wide range of application domains and the omnipresence of multi-label problems in real world scenarios [8].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "The application areas of multi-label classification includes image, music and video categorization [9], medical diagnosis, bioinformatics, multimedia, genomics etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "In contrast to single-label classification, each sample may have multiple target labels in multi-label classification [10].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "have been developed for each of the classification types and is available in the literature [1, 11-14].", "startOffset": 92, "endOffset": 102}, {"referenceID": 10, "context": "have been developed for each of the classification types and is available in the literature [1, 11-14].", "startOffset": 92, "endOffset": 102}, {"referenceID": 11, "context": "have been developed for each of the classification types and is available in the literature [1, 11-14].", "startOffset": 92, "endOffset": 102}, {"referenceID": 12, "context": "have been developed for each of the classification types and is available in the literature [1, 11-14].", "startOffset": 92, "endOffset": 102}, {"referenceID": 13, "context": "have been developed for each of the classification types and is available in the literature [1, 11-14].", "startOffset": 92, "endOffset": 102}, {"referenceID": 14, "context": "Therefore, online learning techniques are preferred over batch learning techniques for streaming data applications [15, 16].", "startOffset": 115, "endOffset": 123}, {"referenceID": 9, "context": "The single-label classification problem can be further divided into two categories: Binary and multi-class classification [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "Several methods have been proposed in the literature [11] that use decomposition to solve the multiclass classification problems [17, 18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "Several methods have been proposed in the literature [11] that use decomposition to solve the multiclass classification problems [17, 18].", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "Several methods have been proposed in the literature [11] that use decomposition to solve the multiclass classification problems [17, 18].", "startOffset": 129, "endOffset": 137}, {"referenceID": 17, "context": "The similar procedure is extended until the leaf nodes have only one class label [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "This results in increased complexity in the implementation of multi-label classifier [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "The existing techniques are grouped under three categories [10, 21].", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "The existing techniques are grouped under three categories [10, 21].", "startOffset": 59, "endOffset": 67}, {"referenceID": 20, "context": "Extreme learning machine (ELM) is proposed by Huang et al [22] has gained much attention due to its unique advantage of very high learning speed and random assignment of input weights.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 22, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 23, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 24, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 25, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 26, "context": "Several variants of ELM have been developed and is available in the literature [23-28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 27, "context": "The mathematical background behind the functioning of the ELM has been extensively discussed in the literature [29, 30].", "startOffset": 111, "endOffset": 119}, {"referenceID": 28, "context": "The mathematical background behind the functioning of the ELM has been extensively discussed in the literature [29, 30].", "startOffset": 111, "endOffset": 119}, {"referenceID": 22, "context": "The output weight is updated iteratively with sequentially arriving data blocks using the recursive least square technique [24, 26].", "startOffset": 123, "endOffset": 131}, {"referenceID": 24, "context": "The output weight is updated iteratively with sequentially arriving data blocks using the recursive least square technique [24, 26].", "startOffset": 123, "endOffset": 131}], "year": 2016, "abstractText": "Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are subcategories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. Keywords\u2014Universal, Classification, Binary, Multi-class, Multi-label, Online, Extreme learning machines, Data stream.", "creator": "Microsoft\u00ae Word 2016"}}}