{"id": "1609.02746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification", "abstract": "This article describes our deep learning approach to mood analysis in Twitter as part of the SemEval 2016 task 4. We use a Convolutionary Neural Network to determine mood and participate in all subtasks, i.e. two-step, three-step and five-step mood classification and two-step and five-step mood quantification. We achieve competitive results in mood classification and quantification on a two-step scale, ranking fifth and fourth (third and second using alternative metrics), although we only use pre-trained embedding that does not contain mood information. We achieve good results in mood classification on a three-step scale and rank eighth out of 35, while we perform poorly on the five-step mood classification and quantification. An error analysis shows that this is due to the low expressiveness of the negative feeling model and the inability to take proper information into account. We propose improvements to address these and other issues.", "histories": [["v1", "Fri, 9 Sep 2016 11:16:56 GMT  (90kb)", "http://arxiv.org/abs/1609.02746v1", "Published in Proceedings of SemEval-2016, 5 pages"]], "COMMENTS": "Published in Proceedings of SemEval-2016, 5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1609.02746"}, "pdf": {"name": "1609.02746.pdf", "metadata": {"source": "CRF", "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["firstname.lastname@insight-centre.org", "firstname@aylien.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n02 74\n6v 1\n[ cs\n.C L\n] 9\nS ep"}, {"heading": "1 Introduction", "text": "Social media allows hundreds of millions of people to interact and engage with each other, while expressing their thoughts about the things that move them. Sentiment analysis (Pang and Lee, 2008) allows us to gain insights about opinions towards persons, objects, and events in the public eye and is used nowadays to gauge public opinion towards companies or products, to analyze customer satisfaction, and to detect trends.\nIts immediacy allowed Twitter to become an important platform for expressing opinions and public discourse, while the accessibility of large quantities of data in turn made it the focal point of social media sentiment analysis research.\nRecently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and message-level sentiment classification (Severyn and Moschitti, 2015).\nPast SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field. SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before.\nWe apply our deep learning-based model for sentiment analysis to all subtasks of SemEval-2016 Task 4: three-point scale message polarity classification (subtask A), two-point and five-point scale topic sentiment classification (subtasks B and C respectively), and two-point and five-point scale topic sentiment quantification (subtasks D and E respectively).\nOur model achieves excellent results for subtasks B and D, ranks competitively for subtask A, while performing poorly for subtasks C and E. We perform an error analysis of our model to obtain a better understanding of strengths and weaknesses of a deep learning-based approach particularly for these new tasks and subsequently propose improvements."}, {"heading": "2 Related work", "text": "Deep-learning based approaches have recently dominated the state-of-the-art in sentiment analysis. Kim (2014) uses a one-layer convolutional neural network to achieve top performance on various sentiment analysis datasets, demonstrating the utility of pre-trained embeddings.\nState-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network.\nIn contrast, we observe distantly supervised data not to be as important for some tasks as long as sufficient training data is available."}, {"heading": "3 Model", "text": "The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011).\nThe model takes as input a text, which is padded to length n. We represent the text as a concatentation of its word embeddings x1:n where xi \u2208 Rk is the k-dimensional vector of the i-th word in the text.\nThe convolutional layer slides filters of different window sizes over the word embeddings. Each filter with weights w \u2208 Rhk generates a new feature ci for a window of h words according to the following operation:\nci = f(w \u00b7 xi:i+h\u22121 + b) (1)\nNote that b \u2208 R is a bias term and f is a non-linear function, ReLU (Nair and Hinton, 2010) in our case. The application of the filter over each possible window of h words or characters in the sentence produces the following feature map:\nc = [c1, c2, ..., cn\u2212h+1] (2)\nMax-over-time pooling in turn condenses this feature vector to its most important feature by taking its maximum value and naturally deals with variable input lengths.\nA final softmax layer takes the concatenation of the maximum values of the feature maps produced by all filters and outputs a probability distribution over all output classes."}, {"heading": "4 Methodology", "text": ""}, {"heading": "4.1 Datasets", "text": "For every subtask, the organizers provide a training, development, and development test set for training and tuning. We use the concatentation of the training and development test set for each subtask for training and use the development set for validation.\nAdditionally, the organizers make training and development data from SemEval-2013 and trial data from 2016 available that can be used for training and tuning for subtask A and subtasks B, C, D, and E respectively. We experiment with adding these datasets to the respective subtask. Interestingly, adding them slightly increases loss on the validation set, while providing a significant performance boost on past development test sets, which we view as a proxy for performance on the 2016 test set. For this reason, we include these datasets for training of all our models.\nWe notably do not select the model that achieves the lowest loss on the validation set, but choose the one that maximizes the FPN1 score, i.e. the arithmetic mean of the F1 of positive and negative tweets, which has historically been used to evaluate the SemEval message polarity classification subtask. We observe that the lowest loss does not necessarily lead to the lowest FPN1 , as it does not include F1 of neutral tweets."}, {"heading": "4.2 Pre-processing", "text": "For pre-processing, we use a script adapted from the pre-processing script1 used for training GloVe vectors (Pennington et al., 2014). Besides normalizing urls and mentions, we notably normalize happy and sad smileys, extract hashtags, and insert tags for repeated, elongated, and all caps characters."}, {"heading": "4.3 Word embeddings", "text": "Past research (Kim, 2014; Severyn and Moschitti, 2015) found a good\n1http://nlp.stanford.edu/projects/glove/ preprocess-twitter.rb\ninitialization of word embeddings to be crucial in training an accurate sentiment model.\nWe thus evaluate the following evaluation schemes: random initialization, initialization using pre-trained GloVe vectors, fine-tuning pretrained embeddings on a distantly supervised corpus (Severyn and Moschitti, 2015), and fine-tuning pre-trained embeddings on 40k tweets with crowdsourced Twitter annotations. Perhaps counterintuitively, we find that fine-tuning embeddings on a distantly supervised or crowd-sourced corpus does not improve performance on past development test sets when including the additionally provided data for training. We hypothesize that additional training data facilitates learning of the underlying semantics, thereby reducing the need for sentiment-specific embeddings. Our scores partially echo this theory.\nFor this reason, we initialize our word embeddings simply with 200-dimensional GloVe vectors trained on 2B tweets. Word embeddings for unknown words are initialized randomly."}, {"heading": "4.4 Hyperparameters and pre-processing", "text": "We tune hyperparameters over a wide range of values via random search on the validation set. We find that the following hyperparameters, which are similar to ones used by Kim (2014), yield the best performance across all subtasks: mini-batch size of 10, maximum sentence length of 50 tokens, word embedding size of 200 dimensions, dropout rate of 0.3, l2 regularization of 0.01, filter lengths of 3, 4, and 5 with 100 filter maps each.\nWe train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012), and early stopping."}, {"heading": "4.5 Task adaptation and quantification", "text": "To adapt our model to the different tasks, we simply adjust the number of output neurons to conform to the scale used in the task at hand (two-point scale in subtasks B and D, three-point scale in subtask A, five-point scale in subtasks C and E).\nWe perform a simple quantification for subtasks D and E by aggregating the classified tweets for each topic and reporting their distribution across sentiments. We would thus expect our results on subtasks B and D and results on subtasks C and E to be closely correlated."}, {"heading": "5 Evaluation", "text": "We report results of our model in Tables 1 and 2 (subtask A), Table 3 (subtask B), Tables 5 and 6 (subtask C), Table 4 (subtask D), and Table 7 (subtask E). For some subtasks, the organizers make available alternative metrics. We observe that the choice of the scoring metric influences results considerably, with our system always placing higher if ranked by one of the alternative metrics.\nSubtask A. We obtain competitive performance on subtask A in Table 1. Analysis of results on the progress test sets in Table 2 reveals that our system achieves competitive F1 scores for positive and neutral tweets, but only low F1 scores for negative tweets due to low recall. This is mirrored in Table 1, where we rank higher for accuracy than for recall. The scoring metric for subtask A, FPOS1 accentuates F1 for positive and negative tweets, thereby ignoring our good performance on neutral tweets and leading to only mediocre ranks on the progress test sets for our system.\nSubtasks B and D. We achieve a competitive fifth rank for subtask B by the official recall metric in Table 3. However, ranked by F1 (as in subtask A), we place third \u2013 and second if ranked by accuracy. Similarly, for subtask D, we rank fourth (with a differential of 0.001 to the second rank) by KLD, but second and first if ranked by AE and RAE respectively. Jointly, these results demonstrate that classifi-\ncation performance is a good indicator for quantification without using any more sophisticated quantification methods. These results are in line with past research (Kim, 2014) showcasing that even a conceptually simple neural network-based approach can achieve excellent results given enough training data per class. These results also highlight that embeddings trained using distant supervision, which should be particularly helpful for this task as they are fine-tuned using the same classes, i.e. positive and negative, are not necessary given enough data.\nSubtasks C and E. We achieve mediocre results for subtask C in Table 5, only ranking sixth \u2013 however, placing third by the alternative metric. Similarly, we only achieve an unsatisfactory eighth rank for subtask E in Table 7. An error analysis for subtask C in Table 6 reveals that the model is able to differentiate between neutral, positive, and very positive tweets with good accuracy. However, similarly to results in subtask A, we find that it lacks expressiveness for negative sentiment and completely fails to capture very negative tweets due to their low number in the training data. Additionally, it is unable to take into account sentiment order to reduce error for very positive and very negative tweets."}, {"heading": "5.1 Improvements", "text": "We propose different improvements to enable the model to better deal with some of the encountered challenges.\nNegative sentiment. The easiest way to enable our model to better capture negative sentiment is to include more negative tweets in the training data. Additionally, using distantly supervised data for fine-tuning embeddings would likely have helped to mitigate this deficit. In order to allow the model to better differentiate between different sentiments on a five-point scale, it would be interesting to evaluate ways to create a more fine-grained distantly supervised corpus using e.g. a wider range of smileys and emoticons or certain hashtags indicating a high degree of elation or distress.\nOrdinal classification. Instead of treating all classes as independent, we can enable the model to take into account ordinal information by simply modifying the labels as in (Cheng et al., 2008). A more sophisticated approach would organically integrate label-dependence into the network.\nQuantification. Instead of deriving the topiclevel sentiment distribution by predicting tweetlevel sentiment, we can directly minimize the Kullback-Leibler divergence for each topic. If the feedback from optimizing this objective proves to be too indirect to provide sufficient signals, we can jointly optimize tweet-level as well as topic-level sentiment as in (Kotzias, 2015)."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented our deep learningbased approach to Twitter sentiment analysis for two-point, three-point, and five-point scale sentiment classification and two-point and five-point\nscale sentiment quantification. We reviewed the different aspects we took into consideration in creating our model. We rank fifth and a close fourth (third and second by alternative metrics) on twopoint scale classification and quantification despite using only pre-trained embeddings that contain no sentiment information. We analysed our weaker performance on three-point scale sentiment classification and five-point scale sentiment classification and quantification and found that the model lacks expressiveness to capture negative sentiment and is unable to take into account class order. Finally, we proposed improvements to resolve these deficits."}, {"heading": "Acknowledgments", "text": "This project has emanated from research conducted with the financial support of the Irish Research Council (IRC) under Grant Number EBPPG/2014/30 and with Aylien Ltd. as Enterprise Partner. This publication has emanated from research supported in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289."}], "references": [{"title": "A neural network approach to ordinal regression", "author": ["Zheng Wang Zheng Wang", "G. Pollastri"], "venue": "IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intel-", "citeRegEx": "Cheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2008}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "From Group to Individual Labels using Deep Features", "author": ["Dimitrios Kotzias"], "venue": null, "citeRegEx": "Kotzias.,? \\Q2015\\E", "shortCiteRegEx": "Kotzias.", "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th Inter-", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "author": ["Nakov et al.2016] Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Fabrizio Sebastiani"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation,", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Opinion Mining and Sentiment Analysis. Foundations and trends in information retrieval, 2(1-2):1\u2013135", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "author": ["Alan Ritter", "Preslav Nakov", "Veselin Stoyanov"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Rosenthal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2014}, {"title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif M Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": null, "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "UNITN : Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Learning Sentiment-Specific Word Embedding", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and message-level sentiment classification (Severyn and Moschitti, 2015).", "startOffset": 125, "endOffset": 136}, {"referenceID": 9, "context": "Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field.", "startOffset": 56, "endOffset": 104}, {"referenceID": 10, "context": "Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field.", "startOffset": 56, "endOffset": 104}, {"referenceID": 6, "context": "SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before.", "startOffset": 20, "endOffset": 40}, {"referenceID": 3, "context": "Kim (2014) uses a one-layer convolutional neural network to achieve top performance on various sentiment analysis datasets, demonstrating the utility of pre-trained embeddings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al.", "startOffset": 179, "endOffset": 196}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network.", "startOffset": 180, "endOffset": 217}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network.", "startOffset": 180, "endOffset": 386}, {"referenceID": 1, "context": "The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011).", "startOffset": 75, "endOffset": 99}, {"referenceID": 8, "context": "For pre-processing, we use a script adapted from the pre-processing script1 used for training GloVe vectors (Pennington et al., 2014).", "startOffset": 108, "endOffset": 133}, {"referenceID": 3, "context": "Past research (Kim, 2014; Severyn and Moschitti, 2015) found a good", "startOffset": 14, "endOffset": 54}, {"referenceID": 3, "context": "We find that the following hyperparameters, which are similar to ones used by Kim (2014), yield the best performance across all subtasks: mini-batch size of 10, maximum sentence length of 50 tokens, word embedding size of 200 dimensions, dropout rate of 0.", "startOffset": 78, "endOffset": 89}, {"referenceID": 13, "context": "We train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012), and early stopping.", "startOffset": 94, "endOffset": 108}, {"referenceID": 3, "context": "These results are in line with past research (Kim, 2014) showcasing that even a conceptually simple neural network-based approach can achieve excellent results given enough training data per class.", "startOffset": 45, "endOffset": 56}, {"referenceID": 0, "context": "Instead of treating all classes as independent, we can enable the model to take into account ordinal information by simply modifying the labels as in (Cheng et al., 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 4, "context": "If the feedback from optimizing this objective proves to be too indirect to provide sufficient signals, we can jointly optimize tweet-level as well as topic-level sentiment as in (Kotzias, 2015).", "startOffset": 179, "endOffset": 194}], "year": 2016, "abstractText": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on fivepoint scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues.", "creator": "LaTeX with hyperref package"}}}