{"id": "1609.03250", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "DESPOT: Online POMDP Planning with Regularization", "abstract": "The partially observable Markov Decision Process (POMDP) provides a basic general framework for planning under uncertainty, but the optimal solution of POMDPs is mathematically insoluble due to the \"curse of dimensionality\" and the \"curse of history.\" To overcome these challenges, we introduce the Determined Sparse Partially Observable Tree (DESPOT), a sparse approach to the standard belief tree, for online planning under uncertainty. A DESPOT focuses on a series of randomly selected scenarios and accurately captures the \"execution\" of all strategies under these scenarios. We show that the best policy derived from a DESPOT is nearly optimal, with a limit of regret depending on the size of the optimal strategy's representation. Based on this result, we provide an online planning algorithm that seeks a policy that optimizes a regulated objective function.", "histories": [["v1", "Mon, 12 Sep 2016 02:12:13 GMT  (691kb,D)", "https://arxiv.org/abs/1609.03250v1", "35 pages"], ["v2", "Wed, 8 Mar 2017 07:28:31 GMT  (684kb,D)", "http://arxiv.org/abs/1609.03250v2", "36 pages"], ["v3", "Tue, 19 Sep 2017 03:29:57 GMT  (684kb,D)", "http://arxiv.org/abs/1609.03250v3", "36 pages"]], "COMMENTS": "35 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adhiraj somani", "nan ye", "david hsu", "wee sun lee"], "accepted": true, "id": "1609.03250"}, "pdf": {"name": "1609.03250.pdf", "metadata": {"source": "CRF", "title": "DESPOT: Online POMDP Planning with Regularization", "authors": ["Nan Ye", "Adhiraj Somani", "David Hsu", "Wee Sun Lee"], "emails": ["N.YE@QUT.EDU.AU", "ADHIRAJSOMANI@GMAIL.COM", "DYHSU@COMP.NUS.EDU.SG", "LEEWS@COMP.NUS.EDU.SG"], "sections": [{"heading": null, "text": "framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the \u201ccurse of dimensionality\u201d and the \u201ccurse of history\u201d. To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the \u201cexecution\u201d of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for real-time vehicle control. The source code for the algorithm is available online."}, {"heading": "1. Introduction", "text": "The partially observable Markov decision process (POMDP) (Smallwood & Sondik, 1973) provides a principled general framework for planning in partially observable stochastic environments. It has a wide range of applications ranging from robot control (Roy, Burgard, Fox, & Thrun, 1999), resource management (Chade\u0300s, Carwardine, Martin, Nicol, Sabbadin, & Buffet, 2012) to medical diagnosis (Hauskrecht & Fraser, 2000). However, solving POMDPs optimally is computationally intractable (Papadimitriou & Tsitsiklis, 1987; Madani, Hanks, & Condon, 1999). Even approximating the optimal solution is difficult (Lusena, Goldsmith, & Mundhenk, 2001). There has been substantial progress in the last decade (Pineau, Gordon, & Thrun, 2003; Smith & Simmons, 2004; Poupart, 2005; Kurniawati, Hsu, & Lee, 2008; Silver & Veness, 2010; Bai, Hsu, Lee, & Ngo, 2011). However, it remains a challenge today to scale up POMDP planning and tackle POMDPs with very large state spaces and complex dynamics to achieve near real-time performance in practical applications (see, e.g., Figures 2 and 3).\nIntuitively, POMDP planning faces two main difficulties. One is the \u201ccurse of dimensionality\u201d. A large state space is a well-known difficulty for planning: the number of states grows exponentially with the number of state variables. Furthermore, in a partially observable environment, the agent must reason in the space of beliefs, which are probability distributions over the states. If one naively\nc\u00a92017 AI Access Foundation. All rights reserved.\nar X\niv :1\n60 9.\n03 25\n0v 3\n[ cs\n.A I]\n1 9\nSe p\n20 17\ndiscretizes the belief space, the number of discrete beliefs then grows exponentially with the number of states. The second difficulty is the \u201ccurse of history\u201d: the number of action-observation histories under consideration for POMDP planning grows exponentially with the planning horizon. Both curses result in exponential growth of computational complexity and major barriers to large-scale POMDP planning.\nThis paper presents a new anytime online algorithm1 for POMDP planning. Online POMDP planning chooses one action at a time and interleaves planning and plan execution (Ross, Pineau, Paquet, & Chaib-Draa, 2008). At each time step, the agent performs lookahead search on a belief tree rooted at the current belief (Figure 1a) and executes immediately the best action found. To attack the two curses, our algorithm exploits two basic ideas: sampling and anytime heuristic search, described below.\nTo speed up lookahead search, we introduce the Determinized Sparse Partially Observable Tree (DESPOT) as a sparse approximation to the standard belief tree. The construction of a DESPOT leverages a set of randomly sampled scenarios. Intuitively, a scenario is a sequence of random numbers that determinizes the execution of a policy under uncertainty and generates a unique trajectory of states and observations given an action sequence. A DESPOT encodes the execution of all policies under a fixed set of K sampled scenarios (Figure 1b). It alleviates the \u201ccurse of dimensionality\u201d by sampling states from a belief and alleviates the \u201ccurse of history\u201d by sampling observations. A standard belief tree of height D contains all possible action-observation histories and thus O(|A|D|Z|D) belief nodes, where |A| is the number of actions and |Z| is the number of observations. In contrast, a corresponding DESPOT contains only histories under the K sampled scenarios and thus O(|A|DK) belief nodes. A DESPOT is much sparser than a standard belief tree when K is small but converges to the standard belief tree as K grows.\nTo approximate the standard belief tree well, the number of scenarios,K, required by a DESPOT may be exponentially large in the worst case. The worst case, however, may be uncommon in prac-\n1. The source code for the algorithm is available at http://bigbird.comp.nus.edu.sg/pmwiki/farm/ appl/.\ntice. We give a competitive analysis that compares our computed policy against near optimal policies and show that K \u2208 O(|\u03c0| ln(|\u03c0||A||Z|)) is sufficient to guarantee near-optimal performance for the lookahead search, when a POMDP admits a near-optimal policy \u03c0 with representation size |\u03c0| (Theorem 3.2). Consequently, if a POMDP admits a compact near-optimal policy, it is sufficient to use a DESPOT much smaller than a standard belief tree, significantly speeding up the lookahead search. Our experimental results support this view in practice: K as small as 500 can work well for some large POMDPs.\nAs a DESPOT is constructed from sampled scenarios, lookahead search may overfit the sampled scenarios and find a biased policy. To achieve the desired performance bound, our algorithm uses regularization, which balances the estimated value of a policy under the sampled scenarios and the size of the policy. Experimental results show that when overfitting occurs, regularization is helpful in practice.\nTo further improve the practical performance of lookahead search, we introduce an anytime heuristic algorithm to search a DESPOT. The algorithm constructs a DESPOT incrementally under the guidance of a heuristic. Whenever the maximum planning time is reached, it outputs the action with the best regularized value based on the partially constructed DESPOT, thus endowing the algorithm with the anytime characteristic. We show that if the search heuristic is admissible, our algorithm eventually finds the optimal action, given sufficient time. We further show that if the heuristic is not admissible, the performance of the algorithm degrades gracefully. Graceful degradation is important, as it allows practitioners to use their intuition to design good heuristics that are not necessarily admissible over the entire belief space.\nExperiments show that the anytime DESPOT algorithm is successful on very large POMDPs with up to 1056 states. The algorithm is also capable of handling complex dynamics. We implemented it on a robot vehicle for intention-aware autonomous driving among many pedestrians (Figure 2). It achieved real-time performance on this system (Bai, Cai, Ye, Hsu, & Lee, 2015). In addition, the DESPOT algorithm is a core component of our autonomous mine detection strategy that won the 2015 Humanitarian Robotics and Automation Technology Challenge (?) (Figure 3).\nIn the following, Section 2 reviews the background on POMDPs and related work. Section 3 defines the DESPOT formally and presents the theoretical analysis to motivate the use of a regularized objective function for lookahead search. Section 4 presents the anytime online POMDP algorithm,\nwhich searches a DESPOT for a near-optimal policy. Section 5 presents experiments that evaluate our algorithm and compare it with the state of the art. Section 6 discusses the strengths and the limitations of this work as well as opportunities for further work. We conclude with a summary in Section 7."}, {"heading": "2. Background", "text": "We review the basics of online POMDP planning and related works in this section."}, {"heading": "2.1 Online POMDP Planning", "text": "A POMDP models an agent acting in a partially observable stochastic environment. It can be specified formally as a tuple (S,A,Z, T,O,R), where S is a set of states, A is a set of agent actions, and Z is a set of observations. When the agent takes action a \u2208 A in state s \u2208 S, it moves to a new state s\u2032 \u2208 S with probability T (s, a, s\u2032) = p(s\u2032|s, a) and receives observation z \u2208 Z with probability O(s\u2032, a, z) = p(z|s\u2032, a). It also receives a real-valued reward R(s, a).\nA POMDP agent does not know the true state, but receives observations that provide partial information on the state. The agent thus maintains a belief, represented as a probability distribution over S. It starts with an initial belief b0. At time t, it updates the belief according to Bayes\u2019 rule, by incorporating information from the action at taken and the resulting observation ot:\nbt(s \u2032) = \u03b7O(s\u2032, at, zt) \u2211 s\u2208S T (s, at, s \u2032)bt\u22121(s), (1)\nwhere \u03b7 is a normalizing constant. The belief bt = \u03c4(bt\u22121, at, zt) = \u03c4(\u03c4(bt\u22122, at\u22121, zt\u22121), at, zt) = \u00b7 \u00b7 \u00b7 = \u03c4(\u00b7 \u00b7 \u00b7 \u03c4(\u03c4(b0, a1, b1), a2, b2), . . . , at, zt) is a sufficient statistic that contains all the information from the history of actions and observations (a1, z1, a2, z2, . . . , at, zt).\nA policy \u03c0 : B 7\u2192 A is a mapping from the belief space B to the action space A. It prescribes an action \u03c0(b) \u2208 A at the belief b \u2208 B. For infinite-horizon POMDPs, the value of a policy \u03c0 at a belief b is the expected total discounted reward that the agent receives by executing \u03c0:\nV\u03c0(b) = E ( \u221e\u2211 t=0 \u03b3tR ( st, \u03c0(bt) ) \u2223\u2223 b0 = b). (2)\nThe constant \u03b3 \u2208 [0, 1) is a discount factor, which expresses preferences for immediate rewards over future ones.\nIn online POMDP planning, the agent starts with an initial belief. At each time step, it searches for an optimal action a\u2217 at the current belief b. The agent executes the action a\u2217 and receives a new observation z. It updates the belief using (1). The process then repeats.\nTo search for an optimal action a\u2217, one way is to construct a belief tree (Figure 1a), with the current belief b0 as the initial belief at the root of the tree. The agent performs lookahead search on the tree for a policy \u03c0 that maximizes the value V\u03c0(b0) at b0, and sets a\u2217 = \u03c0(b0). Each node of the tree represents a belief. To simplify the notation, we use the same notation b to represent both the node and the associated belief. A node branches into |A| action edges, and each action edge further branches into |Z| observation edges. If a node and its child represent beliefs b and b\u2032, respectively, then b\u2032 = \u03c4(b, a, z) for some a \u2208 A and z \u2208 Z.\nTo obtain an approximately optimal policy, we may truncate the tree at a maximum depthD and search for the optimal policy on the truncated tree. At each leaf node, we simulate a user-specified default policy to obtain a lower-bound estimate on its optimal value. A default policy, for example, can be a random policy or a heuristic. At each internal node b, we apply Bellman\u2019s principle of optimality:\nV \u2217(b) = max a\u2208A {\u2211 s\u2208S b(s)R(s, a) + \u03b3 \u2211 z\u2208Z p(z|b, a)V \u2217 ( \u03c4(b, a, z) )} , (3)\nwhich computes the maximum value of action branches and the average value of observation branches weighted by the observation probabilities. We then perform a post-order traversal on the belief tree and use (3) to compute recursively the maximum value at each node and obtain the best action at the root node b0 for execution.\nSuppose that at each internal node b in a belief tree, we retain only one action branch, which represents the chosen action at b, and remove all other branches. This transforms a belief tree into a policy tree. Each internal node b of a policy tree has a single out-going action edge, which specifies the action at b. Each leaf node is associated with a default policy for execution at the node. Our DESPOT algorithm uses this policy tree representation. We define the size of such a policy as the number of internal policy tree nodes. A singleton policy tree thus has size 0."}, {"heading": "2.2 Related Work", "text": "There are two main approaches to POMDP planning: offline policy computation and online search. In offline planning, the agent computes beforehand a policy contingent upon all possible future outcomes and executes the computed policy based on the observations received. One main advantage of offline planning is fast policy execution, as the policy is precomputed. Early work on POMDP planning often takes the offline approach. See, e.g., the work of Kaelbling, Littman, and Cassandra (1998) and Zhang and Zhang (2001). Although offline planning algorithms have made major progress in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Smith & Simmons, 2005; Kurniawati et al., 2008), they still face significant difficulty in scaling up to very large POMDPs, as they must plan for all beliefs and future contingencies.\nOnline planning interleaves planning with plan execution. At each time step, it plans locally and chooses an optimal action for the current belief only, by performing lookahead search in the neighborhood of the current belief. It then executes the chosen action immediately. Planning for the current belief is computationally attractive. First, it is simpler to search for an optimal action at a\nsingle belief than to do so for all beliefs, as offline policy computation does. Second, it allows us to exploit local structure to reduce the search space size. One limitation of the online approach is the constraint on planning time. As it interleaves planning and plan execution, the plan must be ready for execution within short time in some applications.\nA recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling. Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008). This idea dates back to the early work of Satia and Lave (1973) . Branch-and-bound pruning maintains upper and lower bounds on the value at each belief tree node and use them to prune suboptimal subtrees and improve computational efficiency (Paquet, Tobin, & Chaib-Draa, 2005). This idea is also present in earlier work on offline POMDP planning (Smith & Simmons, 2004). Monte Carlo sampling explores only a randomly sampled subset of observation branches at each node of the belief tree (Bertsekas & Castanon, 1999; Yoon, Fern, Givan, & Kambhampati, 2008; Kearns, Mansour, & Ng, 2002; Silver & Veness, 2010). Our DESPOT algorithm contains all three ideas, but is most closely associated with Monte Carlo sampling. Below we examine some of the earlier Monte Carlo sampling algorithms and DESPOT\u2019s connection with them.\nThe rollout algorithm (Bertsekas & Castanon, 1999) is an early example of Monte Carlo sampling for planning under uncertainty. It is originally designed for Markov decision processes (MDPs), but can be easily adapted to solve POMDPs as well. It estimates the value of a default heuristic policy by performing K simulations and then chooses the best action by one-step lookahead search over the estimated values. Although a one-step lookahead policy improves over the default policy, it may be far from the optimum because of the very short, one-step search horizon.\nLike the rollout algorithm, the hindsight optimization algorithm (HO) (Chong, Givan, & Chang, 2000; Yoon et al., 2008) is intended for MDPs, but can be adapted for POMDPs. While both HO and DESPOT sample K scenarios for planning, HO builds one tree with O(|A|D) nodes for each scenario, independent of others, and thus K trees in total. It searches each tree for an optimal plan and averages the values of theseK optimal plans to choose a best action. HO and related algorithms have been quite successful in recent international probabilistic planning competitions. However, HO plans for each scenario independently; it optimizes an upper bound on the value of a POMDP and not the true value itself. In contrast, DESPOT captures allK scenarios in a single tree ofO(|A|DK) nodes and hedges against all K scenarios simultaneously during the planning. It converges to the true optimal value of the POMDP as K grows.\nThe work of Kearns, Mansour, and Ng (1999) and that of Ng and Jordan (2000) use sampled scenarios as well, but for offline POMDP policy computation. They provide uniform convergence bounds in terms of the complexity of the policy class under consideration, e.g., the VapnikChervonenkis dimension. In comparison, DESPOT uses sampled scenarios for online instead of offline planning. Furthermore, our competitive analysis of DESPOT compares our computed policy against an optimal policy and produces a bound that depends on the size of the optimal policy. The bound benefits from the existence of a small near-optimal policy and naturally leads to a regularized objective function for online planning; in contrast, the algorithms by Kearns et al. (1999) and Ng and Jordan (2000) do not exploit the existence of good small policies within the class of policies under consideration.\nThe sparse sampling (SS) algorithm (Kearns et al., 2002) and the DESPOT algorithm both construct sparse approximations to a belief tree. SS samples a constant number C of observation branches for each action. A sparse sampling tree contains O(|A|DCD) nodes, while a DESPOT\ncontains O(|A|DK) nodes. Our analysis shows that K can be much smaller than CD when a POMDP admits a small near-optimal policy (Theorem 3.2). In such cases, the DESPOT algorithm is computationally more efficient.\nPOMCP (Silver & Veness, 2010) performs Monte Carlo tree search (MCTS) on a belief tree. It combines optimistic action exploration and random observation sampling, and relies on the UCT algorithm (Kocsis & Szepesvari, 2006) to trade off exploration and exploitation. POMCP is simple to implement and is one of the best in terms of practical performance on large POMDPs. However, it can be misguided by the upper confidence bound (UCB) heuristic of the UCT algorithm and be overly greedy. Although it converges to an optimal action in the limit, its worst-case running time\nis extremely poor: \u2126( D\u22121\ufe37 \ufe38\ufe38 \ufe37 exp(exp(. . . exp(1) . . .))) (Coquelin & Munos, 2007).\nAmong the Monte Carlo sampling algorithms, one unique feature of DESPOT is the use of regularization to avoid overfitting to sampled scenarios: it balances the estimated performance value of a policy and the policy size during the online search, improving overall performance for suitable tasks.\nAnother important issue for online POMDP planning is belief representation. Most online POMDP algorithms, including the Monte Carlo sampling algorithms, explicitly represent the belief as a probability distribution over the state space. This severely limits their scalability on POMDPs with very large state spaces, because a single belief update can take time quadratic in the number of states. Notable exceptions are POMCP and DESPOT. Both represent the belief as a set of sampled states and do not perform belief update over the entire state space during the online search.\nOnline search and offline policy computation are complementary and can be combined, by using approximate or partial policies computed offline as the default policies at the leaves of the search tree for online planning (Bertsekas & Castanon, 1999; Gelly & Silver, 2007), or as macro-actions to shorten the search horizon (He, Brunskill, & Roy, 2011).\nThis paper extends our earlier work (Somani, Ye, Hsu, & Lee, 2013). It provides an improved anytime online planning algorithm, an analysis of this algorithm, and new experimental results."}, {"heading": "3. Determinized Sparse Partially Observable Trees", "text": "A DESPOT is a sparse approximation of a standard belief tree. While a standard belief tree captures the execution of all policies under all possible scenarios, a DESPOT captures the execution of all policies under a set of randomly sampled scenarios (Figure 1b). A DESPOT contains all the action branches, but only the observation branches encountered under the sampled scenarios.\nWe define DESPOT constructively by applying a deterministic simulative model to all possible action sequences under K sampled scenarios. A scenario is an abstract simulation trajectory with some start state s0. Formally, a scenario for a belief b is an infinite random sequence \u03c6 = (s0, \u03c61, \u03c62, . . .), in which the start state s0 is sampled according to b and each \u03c6i is a real number sampled independently and uniformly from the range [0, 1]. The deterministic simulative model is a function g : S \u00d7A\u00d7R 7\u2192 S \u00d7Z, such that if a random number \u03c6 is distributed uniformly over [0, 1], then (s\u2032, z\u2032) = g(s, a, \u03c6) is distributed according to p(s\u2032, z\u2032|s, a) = T (s, a, s\u2032)O(s\u2032, a, z\u2032). When simulating this model for an action sequence (a1, a2, . . .) under a scenario (s0, \u03c61, \u03c62, . . .), we get a simulation trajectory (s0, a1, s1, z1, a2, s2, z2, . . .), where (st, zt) = g(st\u22121, at, \u03c6t) for t = 1, 2, . . . . The simulation trajectory traces out a path (a1, z1, a2, z2, . . .) from the root of the standard belief tree. We add all the nodes and edges on this path to the DESPOT D being con-\nstructed. Each node b of D contains a set \u03a6b of all scenarios that it encounters. We insert the scenario (s0, \u03c60, \u03c61, . . .) into the set \u03a6b0 at the root b0 and insert the scenario (st, \u03c6t+1, \u03c6t+2, . . .) into the set \u03a6bt at the belief node bt reached at the end of the subpath (a1, z1, a2, z2, . . . , at, zt), for t = 1, 2, . . . . Repeating this process for every action sequence under every sampled scenario completes the construction of D.\nIn summary, a DESPOT is a randomly sampled subtree of a standard belief tree. It is completely determined by the set of K random sequences sampled a priori, hence the name Determinized Sparse Partially Observable Tree. Each DESPOT node b represents a belief and contains a set \u03a6b of scenarios. The start states of the scenarios in \u03a6b form a particle set that represents b approximately. While a standard belief tree of height D has O(|A|D|Z|D) nodes, a corresponding DESPOT has O(|A|DK) nodes for |A| \u2265 2, because of reduced observation branching under the sampled scenarios.\nIt is possible to search for near-optimal policies using a DESPOT instead of a standard belief tree. The empirical value V\u0302\u03c0(b) of a policy \u03c0 under the sampled scenarios encoded in a DESPOT is the average total discounted reward obtained by simulating the policy under each scenario. Formally, let V\u03c0,\u03c6 be the total discounted reward of the trajectory obtained by simulating \u03c0 under a scenario \u03c6 \u2208 \u03a6b for some node b in a DESPOT, then\nV\u0302\u03c0(b) = \u2211 \u03c6\u2208\u03a6b V\u03c0,\u03c6 |\u03a6b| ,\nwhere |\u03a6b| is the number of scenarios in \u03a6b. Since V\u0302\u03c0(b) converges to V\u03c0(b) almost surely as K \u2192 \u221e, the problem of finding an optimal policy at b can be approximated as that of doing so under the sampled scenarios. One concern, however, is overfitting: a policy optimized for finitely many sampled scenarios may not be optimal in general, as many scenarios are not sampled. To control overfitting, we regularize the empirical value of a policy by adding a term that penalizes large policy size. We now provide theoretical analysis to justify this approach.\nOur first result bounds the error of estimating the values of all policies derived from DESPOTs of a given size. The result implies that a DESPOT constructed with a small number of scenarios is sufficient for approximate policy evaluation. The second result shows that by optimizing this bound, which is equivalent to maximizing a regularized value function, we obtain a policy that is competitive with the best small policy.\nFormally, a DESPOT policy \u03c0 is a policy tree derived from a DESPOT D: \u03c0 contains the same root as the DESPOT D, but only one action branch at each internal node. To execute \u03c0, an agent starts at the root of \u03c0. At each time step, it takes the action specified by the action edge at the node. Upon receiving the resulting observation, it follows the corresponding observation edge to the next node. The agent may encounter an observation not present in \u03c0, as \u03c0 contains only the observation branches under the sampled scenarios. In this case, the agent follows a default policy from then on. Similarly, it follows the default policy when reaching a leaf node of \u03c0. Consider the set \u03a0b0,D,K , which contains all DESPOT policies derived from DESPOTs of height D, constructed with all possible K sampled scenarios for a belief b0. We now bound the error on the estimated value of an arbitrary DESPOT policy in \u03a0b0,D,K . To simplify the presentation, we assume without loss of generality that all rewards are non-negative and are bounded by Rmax. For models with bounded negative rewards, we can shift all rewards by a constant to make them non-negative. The shift does not affect the optimal policy.\nTheorem 3.1 For any given constants \u03c4, \u03b1 \u2208 (0, 1), any belief b0, and any positive integers D and K, every DESPOT policy tree \u03c0 \u2208 \u03a0b0,D,K satisfies\nV\u03c0(b0) \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0(b0)\u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3) \u00b7\nln(4/\u03c4) + |\u03c0| ln ( KD|A||Z| ) \u03b1K\n(4)\nwith probability at least 1\u2212 \u03c4 , where V\u0302\u03c0(b0) is the estimated value of \u03c0 under a set of K scenarios randomly sampled according to b0.\nAll proofs are presented in the appendix. Intuitively, the result says that all DESPOT policies in \u03a0b0,D,K satisfy the bound given in (4), with high probability. The bound holds for any constant \u03b1 \u2208 (0, 1), which is a parameter that can be tuned to tighten the bound. A smaller \u03b1 value reduces the approximation error in the first term on the right-hand side of (4), but increases the additive error in the second term. The additive error depends on the size of \u03c0. It also grows logarithmically with |A| and |Z|. The estimation thus scales well with large action and observation spaces. We can make this estimation error arbitrarily small by choosing a suitable number of sampled scenarios, K.\nThe next theorem states that we can obtain a near-optimal policy \u03c0\u0302 by maximizing the RHS of (4), which accounts for both the estimated performance and the size of a policy.\nTheorem 3.2 Let \u03c0 be an arbitrary policy at a belief b0. Let \u03a0D be the set of policies derived from a DESPOT D that has height D and is constructed with K scenarios sampled randomly according to b0. For any given constants \u03c4, \u03b1 \u2208 (0, 1), if\n\u03c0\u0302 = arg max \u03c0\u2032\u2208\u03a0D { 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0\u2032(b0)\u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3) \u00b7 |\u03c0\u2032| ln ( KD|A||Z| ) \u03b1K } , (5)\nthen\nV\u03c0\u0302(b0) \u2265 1\u2212\u03b11+\u03b1V\u03c0(b0)\u2212 Rmax (1+\u03b1)(1\u2212\u03b3)\n( ln(8/\u03c4)+|\u03c0| ln ( KD|A||Z| ) \u03b1K + (1\u2212 \u03b1) (\u221a 2 ln(2/\u03c4) K + \u03b3 D )) with probability at least 1\u2212 \u03c4 .\nTheorem 3.2 bounds the performance of \u03c0\u0302, the policy maximizing (5) in the DESPOT, in terms of the performance of another policy \u03c0. Any policy can be used as the policy \u03c0 for comparison in the competitive analysis. Hence, the performance of \u03c0\u0302 can be compared to the performance of an optimal policy. If the optimal policy has small representation size, the approximation error of \u03c0\u0302 is correspondingly small. The performance of \u03c0\u0302 is also robust. If the optimal policy has large size, but is well approximated by a small policy \u03c0 of size |\u03c0|, then we can obtain \u03c0\u0302 with small approximation error, by choosing K to be O(|\u03c0| ln(|\u03c0||A||Z|)). Since a DESPOT has size O(|A|DK), the choice of K allows us to trade off computation cost and approximation accuracy.\nThe objective function in (5) has the form\nV\u0302\u03c0(b0)\u2212 \u03bb|\u03c0| (6)\nfor some \u03bb \u2265 0, similar to that of regularized utility functions in many machine learning algorithms. This motivates the regularized objective function for our online planning algorithm described in the next section."}, {"heading": "4. Online Planning with DESPOTs", "text": "Following the standard online planning framework (Section 2.1), our algorithm iterates over two main steps: action selection and belief update. For belief update, we use a standard particle filtering method, sequential importance resampling (SIR) (Gordon, Salmond, & Smith, 1993).\nWe now present two action selection methods. In Section 4.1, we describe a conceptually simple dynamic programming method that constructs a DESPOT fully before finding the optimal action. For very large POMDPs, constructing the DESPOT fully is not practical. In Sections 4.2 to 4.4, we describe an anytime DESPOT algorithm that performs anytime heuristic search. The anytime algorithm constructs a DESPOT incrementally under the guidance of a heuristic and scales up to very large POMDPs in practice. In Section 4.5, we show that the algorithm converges to an optimal policy when the heuristic is admissible and that the performance of the algorithm degrades gracefully even when the heuristic is not admissible."}, {"heading": "4.1 Dynamic Programming", "text": "We construct a fixed DESPOT D with K randomly sampled scenarios and want to derive from D a policy that maximizes the regularized empirical value (6) under the sampled scenarios:\nmax \u03c0\u2208\u03a0D\n{ V\u0302\u03c0(b0)\u2212 \u03bb|\u03c0| } ,\nwhere b0 is the current belief, at the root of D. Recall that a DESPOT policy is represented as a policy tree. For each node b of \u03c0, we define the regularized weighted discounted utility (RWDU):\n\u03bd\u03c0(b) = |\u03a6b| K \u03b3\u2206(b)V\u0302\u03c0b(b)\u2212 \u03bb|\u03c0b|, (7)\nwhere |\u03a6b| is the number of scenarios passing through node b, \u03b3 is the discount factor, \u2206(b) is the depth of b in the policy tree \u03c0, \u03c0b is the subtree rooted at b, and |\u03c0b| is the size of \u03c0b. The ratio |\u03a6b|/K is an empirical estimate of the probability of reaching b. Clearly, \u03bd\u03c0(b0) = V\u0302\u03c0(b0)\u2212 \u03bb|\u03c0|, which we want to optimize.\nFor every node b of D, define \u03bd\u2217(b) as the maximum RWDU of b over all policies in \u03a0D. Assume that D has finite depth. We now present a dynamic programming procedure that computes \u03bd\u2217(b0) recursively from bottom up. At a leaf node b of D, we simulate a default policy \u03c00 under the sampled scenarios. According to our definition in Section 2.1, |\u03c00| = 0. Thus,\n\u03bd\u2217(b) = |\u03a6b| K \u03b3\u2206(b)V\u0302\u03c00(b). (8)\nAt each internal node b, let \u03c4(b, a, z) be the child of b following the action branch a and the observation branch z at b. Then,\n\u03bd\u2217(b) = max { |\u03a6b| K \u03b3\u2206(b)V\u0302\u03c00(b), max a\u2208A { \u03c1(b, a) + \u2211 z\u2208Zb,a \u03bd\u2217(\u03c4(b, a, z)) }} , (9)\nwhere\n\u03c1(b, a) = 1\nK \u2211 \u03c6\u2208\u03a6b \u03b3\u2206(b)R(s\u03c6, a)\u2212 \u03bb,\nthe state s\u03c6 is the start state of the scenario \u03c6, and Zb,a is the set of observations following the action branch a at the node b. The outer maximization in (9) chooses between executing the default policy or expanding the subtree at b. As the RWDU contains a regularization term, the latter is beneficial only if the subtree at b is relatively small. This effectively prevents expanding a large subtree, when there are an insufficient number of sampled scenarios to estimate the value of a policy at b accurately. The inner in (9) maximization chooses among the different actions available. When the algorithm terminates, the maximizer at the root b0 of D gives the best action at b0.\nIf D has unbounded depth, it is sufficient to truncate D to a depth of dRmax/\u03bb(1\u2212 \u03b3)e+ 1 and run the above algorithm, provided that \u03bb > 0. The reason is that an optimal regularized policy \u03c0\u0302 cannot include the truncated nodes of D. Otherwise, \u03c0\u0302 has size at least dRmax/\u03bb(1\u2212 \u03b3)e + 1 and thus RWDU \u03bd\u03c0\u0302(b0) < 0. Since the default policy \u03c00 has RWDU \u03bd\u03c00(b0) \u2265 0, \u03c00 is then better than \u03c0\u0302, a contradiction.\nThis dynamic programming algorithm runs in time linear in the number of nodes in D. We first simulate the deterministic model to construct the tree, then do a bottom-up dynamic programming to initialize V\u0302\u03c00(b), and finally compute \u03bd\n\u2217(b) using Equation (9). Each step takes time linear in the number of nodes in D given previous steps are done, and thus the total running time is O(|A|DK)."}, {"heading": "4.2 Anytime Heuristic Search", "text": "The bottom-up dynamic programming algorithm in the previous section constructs the full DESPOT D in advance. This is generally not practical because there are exponentially many nodes. To scale up, we now present an anytime forward search algorithm2 that avoids constructing the DESPOT fully in advance. It selects the action by incrementally constructing a DESPOT D rooted at the current belief b0, using heuristic search (Smith & Simmons, 2004; Kurniawati et al., 2008), and approximating the optimal RWDU \u03bd\u2217(b0). We describe the main components of the algorithm below. The complete pseudocode is given in Appendix B.\nTo guide the heuristic search, we maintain a lower bound `(b) and an upper bound \u00b5(b) on the optimal RWDU at each node b of D, so that `(b) \u2264 \u03bd\u2217(b) \u2264 \u00b5(b). To prune the search tree, we additionally maintain an upper bound U(b) on the empirical value V\u0302 \u2217(b) of the optimal regularized policy so that U(b) \u2265 V\u0302 \u2217(b) and compute an initial lower bound L0(b) with L0(b) \u2264 V\u0302 \u2217(b). In particular, we use L0(b) = V\u0302\u03c00(b) for the default policy \u03c00 at b.\nAlgorithm 1 provides a high-level sketch of the algorithm. We construct and search a DESPOT D incrementally, using K sampled scenarios (line 1). Initially, D contains only a single root node with belief b0 and the associated initial upper and lower bounds (lines 2\u20133). The algorithm makes a series of explorations to expand D and reduce the gap between the bounds \u00b5(b0) and `(b0) at the root node b0 ofD. Each exploration follows a heuristic and traverses a promising path from the root of D to add new nodes to D (line 6). Specifically, it keeps on choosing and expanding a promising leaf node and adds its child nodes into D until current leaf node is not heuristically promising. The algorithm then traces the path back to the root and performs backup on the upper and lower bounds at each node along the way, using Bellman\u2019s principle (line 7). The explorations continue, until\n2. This algorithm differs from an earlier version (Somani et al., 2013) in a subtle, but important way. The new algorithm optimizes the RWDU directly by interleaving incremental DESPOT construction and backup. The earlier one performs incremental DESPOT construction without regularization and then optimizes the RWDU over the constructed DESPOT. As a result, the new algorithm is guaranteed to converge to an optimal regularized policy derived from the full DESPOT, while the earlier one is not.\nAlgorithm 1 BUILDDESPOT(b0) 1: Sample randomly a set \u03a6b0 of K scenarios from the current belief b0. 2: Create a new DESPOT D with a single node b0 as the root. 3: Initialize U(b0), L0(b0), \u00b5(b0), and `(b0). 4: (b0)\u2190 \u00b5(b0)\u2212 `(b0). 5: while (b0) > 0 and the total running time is less than Tmax do 6: b\u2190 EXPLORE(D, b0). 7: BACKUP(D, b). 8: (b0)\u2190 \u00b5(b0)\u2212 `(b0). 9: return `\nAlgorithm 2 EXPLORE(D, b) 1: while \u2206(b) \u2264 D, E(b) > 0, and PRUNE(D, b) = FALSE do 2: if b is a leaf node in D then 3: Expand b one level deeper. Insert each new child b\u2032 of b into D. Initialize U(b\u2032), L0(b\u2032),\n\u00b5(b\u2032), and `(b\u2032). 4: a\u2217 \u2190 arg maxa\u2208A \u00b5(b, a). 5: z\u2217 \u2190 arg maxz\u2208Zb,a\u2217 E(\u03c4(b, a\n\u2217, z)). 6: b\u2190 \u03c4(b, a\u2217, z\u2217). 7: if \u2206(b) > D then 8: MAKEDEFAULT(b). 9: return b.\nthe gap between the bounds \u00b5(b0) and `(b0) reaches a target level 0 \u2265 0 or the allocated online planning time runs out (line 5)."}, {"heading": "4.2.1 FORWARD EXPLORATION", "text": "Let (b) = \u00b5(b) \u2212 `(b) denote the gap between the upper and lower RWDU bounds at a node b. Each exploration aims to reduce the current gap (b0) at the root b0 to \u03be (b0) for some given constant 0 < \u03be < 1 (Algorithm 2). An exploration starts at the root b0. At each node b along the exploration path, we choose the action branch optimistically according to the upper bound \u00b5(b):\na\u2217 = arg max a\u2208A \u00b5(b, a) = arg max a\u2208A\n{ \u03c1(b, a) + \u2211 z\u2208Zb,a \u00b5(b\u2032) } , (10)\nwhere b\u2032 = \u03c4(b, a, z) is the child of b following the action branch a and the observation branch z at b. We then choose the observation branch z that leads to a child node b\u2032 = \u03c4(b, a\u2217, z) maximizing the excess uncertainty E(b\u2032) at b\u2032:\nz\u2217 = arg max z\u2208Zb,a\u2217 E(b\u2032) = arg max z\u2208Zb,a\u2217\n{ (b\u2032)\u2212 |\u03a6b\n\u2032 | K \u00b7 \u03be (b0)\n} . (11)\nIntuitively, the excess uncertainty E(b\u2032) measures the difference between the current gap at b\u2032 and the \u201cexpected\u201d gap at b\u2032 if the target gap \u03be (b0) at b0 is satisfied. Our exploration strategy seeks to\nAlgorithm 3 MAKEDEFAULT(b) U(b)\u2190 L0(b). \u00b5(b)\u2190 `0(b). `(b)\u2190 `0(b).\nreduce the excess uncertainty in a greedy manner. See Lemma 4.1 in Section 4.5, the work of Smith and Simmons (2005) and the work of Kurniawati et al. (2008) for justifications of this strategy.\nIf the exploration encounters a leaf node b, we expand b by creating a child b\u2032 of b for each action a \u2208 A and each observation encountered under a scenario \u03c6 \u2208 \u03a6b. For each new child b\u2032, we need to compute the initial bounds \u00b50(b\u2032), `0(b\u2032), U0(b\u2032), and L0(b\u2032). The RWDU bounds \u00b50(b\u2032) and `0(b\u2032) can be expressed in terms of the empirical value bounds U0(b\u2032) and L0(b\u2032), respectively. Applying the default policy \u03c00 at b\u2032 and using the definition of RWDU in (7), we have\n`0(b \u2032) = \u03bd\u03c00(b \u2032) = |\u03a6b\u2032 | K \u03b3\u2206(b \u2032)L0(b \u2032),\nas |\u03c00| = 0. For the initial upper bound \u00b50(b\u2032), there are two cases. If the policy for maximizing the RWDU at b\u2032 is the default policy, then we can set \u00b50(b\u2032) = `0(b\u2032). Otherwise, the optimal policy has size at least 1, and it follows from (7) that \u00b50(b\u2032) = |\u03a6b\u2032 | K \u03b3 \u2206(b\u2032)U0(b \u2032) \u2212 \u03bb is an upper bound. So we have\n\u00b50(b \u2032) = max { `0(b\n\u2032), |\u03a6b\u2032 | K \u03b3\u2206(b \u2032)U0(b\n\u2032)\u2212 \u03bb } .\nThere are various way to construct the initial empirical value bounds U0 and L0. We defer the discussion to Sections 4.3 and 4.4.\nNote that a node at a depth more than D executes the default policy, and the bounds are set accordingly using the MAKEDEFAULT procedure (Algorithm 3). We explain the termination conditions for exploration next."}, {"heading": "4.2.2 TERMINATION OF EXPLORATION AND PRUNING", "text": "We terminate the exploration at a node b under three conditions (Algorithm 2, line 1). First, \u2206(b) > D, i.e., the maximum tree height is exceeded. Second, E(b) < 0, indicating that the expected gap at b is reached and further exploration from b onwards may be unprofitable. Finally, b is blocked by an ancestor node b\u2032:\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)(U(b\u2032)\u2212 L0(b\u2032)) \u2264 \u03bb \u00b7 `(b\u2032, b), (12)\nwhere `(b\u2032, b) is the number of nodes on the path from b\u2032 to b. The intuition behind this last condition is that there is insufficient number of sampled scenarios at the ancestor node b\u2032. Further expanding b and thus enlarging the policy subtree at b\u2032 may cause overfitting and reduce the regularized utility at b\u2032. We thus prune the search by applying the default policy at b and setting the bounds accordingly by calling MAKEDEFAULT. More details are available in Lemma 4.2, which derives the condition (12) and proves that pruning the search does not compromise the optimality of the algorithm.\nThe pruning process continues backwards from b towards the root of D (Algorithm 4). As the bounds are updated, new nodes may satisfy the condition for pruning and are pruned away.\nAlgorithm 4 PRUNE(D, b) 1: BLOCKED \u2190 FALSE. 2: for each node x on the path from b to the root of D do 3: if x is blocked by any ancestor node in D then 4: MAKEDEFAULT(x). 5: BACKUP(D, x). 6: BLOCKED \u2190 TRUE. 7: else 8: break 9: return BLOCKED\nAlgorithm 5 BACKUP(D, b) 1: for each node x on the path from b to the root of D do 2: Perform backup on \u00b5(x), `(x), and U(x)."}, {"heading": "4.2.3 BACKUP", "text": "When the exploration terminates, we trace the path back to the root and perform backup on the bounds at each node b along the way, using Bellman\u2019s principle (Algorithm 5):\n\u00b5(b) = max { `0(b),max\na\u2208A\n{ \u03c1(b, a) + \u2211 z\u2208Zb,a \u00b5(b\u2032) }} ,\n`(b) = max { `0(b),max\na\u2208A\n{ \u03c1(b, a) + \u2211 z\u2208Zb,a `(b\u2032) }} ,\nU(b) = max a\u2208A\n{ 1 |\u03a6b| \u2211 \u03c6\u2208\u03a6b R(s\u03c6, a) + \u03b3 \u2211 z\u2208Zb,a |\u03a6b\u2032 | |\u03a6b| U(b\u2032) } ,\nwhere b\u2032 is a child of b with b\u2032 = \u03c4(b, a, z)."}, {"heading": "4.2.4 RUNNING TIME", "text": "Suppose that the anytime search algorithm invokes EXPLORE N times. EXPLORE traverses a path from the root to a leaf node of a DESPOTD, visiting at mostD+K\u22121 nodes along the way because a path has at most D nodes, and at most K \u2212 1 nodes not on the path can be added due to node expansions (Algorithm 2). At each node, the following steps dominate the running time. Checking the condition for pruning (line 1) takes timeO(D2) in total and thusO(D) per node. Adding a new node to D and initializing the bounds (lines 3 and 8) take time O(I) (assuming I is an upper bound of the cost). Choosing the action branch (line 4) takes timeO(|A|). Choosing the observation branch (line 5) takes time min{|Z|,K} \u2208 O(K), which is loose because only the sampled observation branches are involved. Thus, the running time at each node is O(D + I + |A| + K), and the total running time is O ( N(D +K)(D + I + |A|+K) ) .\nThe anytime search algorithm constructs a partial DESPOT with at mostN(D+K) nodes, while the dynamic programming algorithm (Section 4.1) constructs a DESPOT fully with O(|A|DK) nodes. While the bounds are not directly comparable, N(D + K) is typically much smaller than |A|DK in many practical settings. This is the main difference between the two algorithms. The\nanytime search algorithm takes slightly more time at each node in order to prune the DESPOT. The trade-off is overall beneficial for reduced DESPOT size."}, {"heading": "4.3 Initial Upper Bounds", "text": "For illustration purposes, we discuss several methods for constructing the initial upper bound U0(b) at a DESPOT node b. There are, of course, many alternatives. The flexibility in constructing upper and lower bounds for improved performance is one strength of DESPOT.\nThe simplest one is the uninformed bound\nU0(b) = Rmax/(1\u2212 \u03b3). (13)\nWhile this bound is loose, it is easy to compute and may yield good results when combined with suitable lower bounds.\nHindsight optimization (Yoon et al., 2008) provides a principled method to construct an upper bound algorithmically. Given a fixed scenario \u03c6 = (s0, \u03c61, \u03c62, . . .), computing an upper bound on the total discounted reward achieved by any arbitrary policy is a deterministic planning problem. When the state space is sufficiently small, we can solve it by dynamic programming on a trellis ofD time slices. Trellis nodes represent states, and edges represent actions at each time step. Let u(t, s) be the maximum total reward on the scenario (st, \u03c6t+1, \u03c6t+2, . . . , \u03c6D) at state s \u2208 S and time step t. For all s \u2208 S and t = 0, 1, . . . , D \u2212 1, we set\nu(D, s) = Rmax/(1\u2212 \u03b3)\nand u(t, s) = max\na\u2208A\n{ R(s, a) + \u03b3 u(t+ 1, s\u2032) } ,\nwhere s\u2032 is the new state given by the deterministic simulative model g(s, a, \u03c6t+1). Then u(0, s0) gives the upper bound under \u03c6 = (s0, \u03c61, \u03c62, . . . ). We repeat this procedure for every \u03c6 \u2208 \u03a6b, and set\nU0(b) = 1 |\u03a6b| \u2211 \u03c6\u2208\u03a6b u(0, s\u03c6), (14)\nwhere s\u03c6 is the start state of \u03c6. For a set of K scenarios, this bound can be pre-computed in O(K|S||A|D) time and stored, before online planning starts. To tighten this bound further, we may exploit domain-specific knowledge or other techniques to initialize u(D, s) either exactly or heuristically, instead of using the uninformed bound. If u(D, s) is a true upper bound on the total discounted reward, then the resulting U0(b) is also a true upper bound.\nHindsight optimization may be too expensive to compute when the state space is large. Instead, we may do approximate hindsight optimization, by constructing a domain-specific heuristic upper bound uH(s0) on the total discounted reward for each scenario \u03c6 = (s0, \u03c61, \u03c62, . . .) and then use the average\nU0(b) = 1 |\u03a6b| \u2211 \u03c6\u2208\u03a6b uH(s\u03c6) (15)\nas an upper bound. This upper bound depends on the state only and is often simpler to compute. Domain dependent knowledge can be used in constructing this bound \u2013 this is often crucial in practical problems. In addition, uH(s\u03c6) need not be a true upper bound. An approximation suffices. Our\nanalysis in Section 4.5 shows that the DESPOT algorithm is robust against upper bound approximation error, and the performance of the algorithm degrades gracefully. We call this class of upper bounds, approximate hindsight optimization.\nA useful approximate hindsight optimization bound can be obtained by assuming that the states are fully observable, converting the POMDP into a corresponding MDP, and solving for its optimal value function VMDP. The expected value V (b) = \u2211 s\u2208S b(s)VMDP(s) is an upper bound on the optimal value V \u2217(b) for the POMDP, and\nU0(b) = 1 |\u03a6b| \u2211 \u03c6\u2208\u03a6b VMDP(s\u03c6) (16)\napproximates V (b) by taking the average over the start states of the sampled scenarios. Like the domain-specific heuristic bound, the MDP bound (16) is in general not a true upper bound of the RWDU, but only an approximation, because the MDP is not restricted to the set of sampled scenarios. It takes O(|S|2|A|D) time to solve the MDP using value iteration, but the running time can be significantly faster for MDPs with sparse transitions."}, {"heading": "4.4 Initial Lower Bounds and Default Policies", "text": "The DESPOT algorithm requires a default policy \u03c00. The simplest default policy is a fixed-action policy with the highest expected total discounted reward (Smith & Simmons, 2004). One can also handcraft a better policy that chooses an action based on the past history of actions and observations (Silver & Veness, 2010). However, it is often not easy to determine what the next action should be, given the past history of actions and observations. As in the case of upper bounds, it is often more intuitive to work with states rather than beliefs. We describe a class of methods that we call scenariobased policies. In a scenario-based policy, we construct a mapping f : S 7\u2192 A that specifies an action at a given state. We then specify a function that maps a belief to a state \u039b: B 7\u2192 S and let the default policy be \u03c00(b) = f(\u039b(b)). As an example, let \u039b(b) be the mode of the distribution b (for a DESPOT node, this is the most frequent start state under all scenarios in \u03a6b). We then let f be an optimal policy for the underlying MDP to obtain what we call the mode-MDP policy.\nScenario-based policies considerably ease the difficulty of constructing effective default policies. However, depending on the choice of \u039b, they may not satisfy Theorem 3.1, which assumes that the value of a default policy on one scenario is independent of the value of the policy on another scenario. In particular, the mode-MDP policy violates this assumption and may overfit to the sampled scenarios. However, in practice, we expect the benefit of being able to construct good default policies to usually outweigh the concerns of overfitting with the default policy.\nGiven a default policy \u03c00, we obtain the initial lower bound L0(b) at a DESPOT node b by simulating \u03c00 for a finite number of steps under each scenario \u03a6b and calculating the average total discounted reward."}, {"heading": "4.5 Analysis", "text": "The dynamic programming algorithm builds a full DESPOT D. The anytime forward search algorithm builds a DESPOT incrementally and terminates with a partial DESPOTD\u2032, which is a subtree of D. The main objective of the analysis is to show that the optimal regularized policy \u03c0\u0302 derived from D\u2032 converges to the optimal regularized policy in D. Furthermore, the performance of the anytime algorithm degrades gracefully even when the upper bound U0 is not strictly admissible.\nWe start with some lemmas to justify the choices made in the anytime algorithm. Lemma 4.1 says that the excess uncertainty at a node b is bounded by the sum of excess uncertainty over its children under the action branch a\u2217 that has the highest upper bound \u00b5(b, a\u2217). This provides a greedy means to reduce excess uncertainty by recursively exploring the action branch a\u2217 and the observation branch with the highest excess uncertainty, justifying (10) and (11) as the action and observation selection criteria.\nLemma 4.1 For any DESPOT node b, if E(b) > 0 and a\u2217 = arg maxa\u2208A \u00b5(b, a), then E(b) \u2264 \u2211\nz\u2208Zb,a\u2217 E(b\u2032),\nwhere b\u2032 = \u03c4(b, a\u2217, z) is a child of b.\nThis lemma generalizes a similar result in (Smith & Simmons, 2004) by taking regularization into account.\nLemma 4.2 justifies PRUNE, which prunes the search when a node is blocked by any of its ancestors.\nLemma 4.2 Let b\u2032 be an ancestor of b in a DESPOT D and `(b\u2032, b) be the number of nodes on the path from b\u2032 to b. If\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)(U(b\u2032)\u2212 L0(b\u2032)) \u2264 \u03bb \u00b7 `(b\u2032, b),\nthen b cannot be a belief node in an optimal regularized policy derived from D.\nWe proceed to analyze the performance of the optimal regularized policy \u03c0\u0302 derived from the partial DESPOT constructed. The action output by the anytime DESPOT algorithm is the action \u03c0\u0302(b0), because the initialization and the computation of the lower bound ` via the backup equations are exactly that for finding an optimal regularized policy value in the partial DESPOT.\nWe now state the main results in the next two theorems. Both assume that the initial upper bound U0 is \u03b4-approximate: U0(b) \u2265 V\u0302 \u2217(b)\u2212 \u03b4, for every DESPOT node b. If the initial upper bound is 0-approximate, that is, it is indeed an upper bound for V\u0302 \u2217(b), then we say the heuristic is admissible. First, consider the case in which the maximum online planning time per step Tmax is bounded.\nTheorem 4.1 Suppose that Tmax is bounded and that the anytime DESPOT algorithm terminates with a partial DESPOT D\u2032 that has gap (b0) between the upper and lower bounds at the root b0. The optimal regularized policy \u03c0\u0302 derived from D\u2032 satisfies\n\u03bd\u03c0\u0302(b0) \u2265 \u03bd\u2217(b0)\u2212 (b0)\u2212 \u03b4,\nwhere \u03bd\u2217(b0) is the value of an optimal regularized policy derived from the full DESPOT D at b0.\nSince (b0) decreases monotonically as Tmax grows, the above result shows that the performance of \u03c0\u0302 approaches that of an optimal regularized policy as the running time increases. Furthermore, the error in initial upper bound approximation affects the final result by at most \u03b4. Next we consider unbounded maximum planning time Tmax. The objective here is to show that despite unbounded Tmax, the anytime algorithm terminates in finite time with a near-optimal or optimal regularized policy.\nTheorem 4.2 Suppose that Tmax is unbounded and 0 is the target gap between the upper and lower bound at the root of the partial DESPOT constructed by the anytime DESPOT algorithm. Let \u03bd\u2217(b0) be the value of an optimal regularized policy derived from the full DESPOT D at b0.\n(1) If 0 > 0, then the algorithm terminates in finite time with a near-optimal regularized policy \u03c0\u0302 satisfying\n\u03bd\u03c0\u0302(b0) \u2265 \u03bd\u2217(b0)\u2212 0 \u2212 \u03b4.\n(2) If 0 = 0, \u03b4 = 0, and the regularization constant \u03bb > 0, then the algorithm terminates in finite time with an optimal regularized policy \u03c0\u0302, i.e., \u03bd\u03c0\u0302(b0) = \u03bd\u2217(b0).\nIn the case 0 > 0, the algorithm aims for an approximately optimal regularized policy. Compared with Theorem 4.1, here the maximum planning time limit Tmax is removed, and the algorithm achieves the target gap 0 exactly, after sufficient computation time. In the case 0 = 0, the algorithm aims for an optimal regularized policy. Two additional conditions are required to guarantee finitetime termination and optimality. Clearly one is a true upper bound with no approximation error, i.e., \u03b4 = 0. The other is a strictly positive regularization constant. This assumption implies that there is a finite optimal regularized policy, and thus allows the algorithm to terminate in finite time."}, {"heading": "5. Experiments", "text": "We now compare the anytime DESPOT algorithm with three state-of-the-art POMDP algorithms (Section 5.1). We also study the effects of regularization (Section 5.2) and initial bounds (Section 5.3) on the performance of our algorithm."}, {"heading": "5.1 Performance Comparison", "text": "We compare DESPOT with SARSOP (Kurniawati et al., 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al., 2008), and POMCP (Silver & Veness, 2010). SARSOP is one of the fastest off-line POMDP algorithms. While it cannot compete with online algorithms on scalability, it often provides better results on POMDPs of moderate size and helps to calibrate the performance of online algorithms. AEMS2 is an early successful online POMDP algorithm. Again it is not designed to scale to very large state and observation spaces and is used here as calibration on moderate sized problems. POMCP scales up extremely well in practice (Silver & Veness, 2010) and allows us to calibrate the performance of DESPOT on very large problems.\nWe implemented DESPOT and AEMS2 ourselves. We used the authors\u2019 implementation of POMCP (Silver & Veness, 2010), but improved the implementation to support a very large number of observations and strictly adhere to the time limit for online planning. We used the APPL package for SARSOP (Kurniawati et al., 2008). All algorithms were implemented in C++.\nFor each algorithm, we tuned the key parameters on each domain through offline training, using a data set distinct from the online test data set, as we expect this to be the common usage mode for online planning. Specifically, the regularization parameter \u03bb for DESPOT was selected offline from the set {0, 0.01, 0.1, 1, 10} by running the algorithm with a training set distinct from the online test set. Similarly, the exploration constant c of POMCP was chosen from the set {1, 10, 100, 1000, 10000} for the best performance. Other parameters of the algorithms are set to reasonable values independent of the domain being considered. Specifically, we chose \u03be = 0.95 as in SARSOP (Kurniawati et al., 2008). We chose D = 90 for DESPOT because \u03b3D \u2248 0.01 when\n\u03b3 = 0.95, which is the typical discount factor used. We chose K = 500, but a smaller value may work as well.\nAll the algorithms were evaluated on the same experimental platform. The online POMDP algorithms were given exactly 1 second per step to choose an action.\nThe test domains range in size from small to extremely large. The results are reported in Table 1. In summary, SARSOP and AEMS2 have good performance on the smaller domains, but cannot scale up. POMCP scales up to very large domains, but has poor performance on some domains. DESPOT has strong overall performance. On the smaller domains, it matches with SARSOP and AEMS2 in performance. On the large domains, it matches and sometimes outperforms POMCP. The details on each domain are described below."}, {"heading": "5.1.1 TAG", "text": "Tag is a standard POMDP benchmark introduced by Pineau et al. (2003). A robot and a target operate in a grid with 29 possible positions (Figure 4a). The robot\u2019s goal is to find and tag the target that intentionally runs away. They start at random initial positions. The robot knows its own position, but can observe the target\u2019s position only if they are in the same grid cell. The robot can either stay in the same position or move to the four adjacent positions, paying a cost of \u22121 for each move. It can also attempt to tag the target. It is rewarded +10, if the attempt is successful, and is penalized \u221210 otherwise. To complete the task successfully, a good policy exploits the target\u2019s dynamics to \u201cpush\u201d it against a corner of the environment.\nFor DESPOT, we use the hindsight optimization bound for the initial upper bound U0 and initialize hindsight optimization by setting U(D, s) to be the optimal MDP value (Section 4.3). We use the mode-MDP policy for the default policy \u03c00 (Section 4.4).\nPOMCP cannot use the mode-MDP policy, as it requires default policies that depend on the history only. We use the Tag implementation that comes as part of the authors\u2019 POMCP package, but improved its default policy. The original default policy tags when both the robot and the target\nlie in a corner. Otherwise the robot randomly chooses an action that avoids doubling back or going into the walls. The improved policy tags whenever the agent and the target lie in the same grid cell, otherwise avoids doubling back or going into the walls, yielding better results based on our experiments.\nOn this moderate-size domain, SARSOP achieves the best result. AEMS2 and DESPOT have comparable performance. POMCP\u2019s performance is much weaker, partly because of the limitation on its default policy."}, {"heading": "5.1.2 LASER TAG", "text": "Theorem 3.1 suggests that DESPOT may perform well even when the observation space is large, provided that a small good policy exists. We now consider Laser Tag, an expanded version of Tag with a large observation space. In Laser Tag, the robot moves in a 7 \u00d7 11 rectangular grid with obstacles placed randomly in eight grid cells (Figure 4b). The robot\u2019s and target\u2019s behaviors remain the same as before. However, the robot does not know its own position exactly and is distributed uniformly over the grid initially. To localize, it is equipped with a laser range finder that measures the distances in eight directions. The side length of each cell is 1. The laser reading in each direction is generated from a normal distribution centered at the true distance of the robot to the nearest obstacle in that direction, with a standard deviation of 2.5. The readings are rounded to\nthe nearest integers. So an observation comprises a set of eight integers, and the total number of observations is roughly 1.5\u00d7 106.\nDESPOT uses a domain-specific method, which we call Shortest Path (SP) for the upper bound. For every possible initial target position, we compute an upper bound by assuming that the target stays stationary and that the robot follows a shortest path to tag the target. We then take the average over the sampled scenarios. DESPOT\u2019s default policy is similar to the one used by POMCP in Tag, but it uses the most likely robot position to choose actions that avoid doubling back and running into walls. So it is not a scenario-based policy but a hybrid policy that makes use of both the belief and the history.\nAs the robot does not know its exact location, it is more difficult for POMCP\u2019s default policy to avoid going into walls and doubling back. Hence, we only implemented the action of tagging whenever the robot and target are in the same location, and did not implement wall and doubling back avoidance.\nWith the very large observation space, we are not able to successfully run SARSOP and AEMS2. DESPOT achieves substantially better result than POMCP on this task."}, {"heading": "5.1.3 ROCK SAMPLE", "text": "Next we consider Rock Sample, a well-established benchmark with a large state space (Smith & Simmons, 2004). In RS(n, k), a robot rover moves on an n \u00d7 n grid containing k rocks, each of which may be good or bad (Figure 4c). The robot\u2019s goal is to visit and sample the good rocks, and exit the east boundary upon completion. At each step, the robot may move to an adjacent cell, sense a rock, or sample a rock. Sampling gives a reward of +10 if the rock is good and \u221210 otherwise. Moving and sensing have reward 0. Moving or sampling do not produce any observation, or equivalently, null observation is produced. Sensing a rock produces an observation, GOOD or BAD, with probability of being correct decreasing exponentially with the robot\u2019s distance from the rock. To obtain high total reward, the robot navigates the environment and senses rocks to identify the good ones; at the same time, it exploits the information gained to visit and sample the good rocks.\nFor upper bounds, DESPOT uses the MDP upper bound, which is a true upper bound in this case. For default policy, it uses a simple fixed-action default policy that always moves to the east.\nPOMCP uses the default policy described in (Silver & Veness, 2010). The robot travels from rock location to rock location. At each rock location, the robot samples the rock if there are more GOOD observations there than BAD observations. If all remaining rocks have a greater number of BAD observations, the robot moves to the east boundary and exits.\nOn the smallest domain RS(7, 8), the offline algorithm SARSOP obtains the best result overall. Among the three online algorithms, DESPOT has the best result. On RS(11, 11), DESPOT matches SARSOP in performance and is better than POMCP. On the largest instance RS(15, 15), SARSOP and AEMS2 cannot be completed successfully. It is also interesting to note that although the default policy for DESPOT is weaker than that of POMCP, DESPOT still achieves better results."}, {"heading": "5.1.4 POCMAN", "text": "Pocman (Silver & Veness, 2010) is a partially observable variant of the popular video game Pacman (Figure 4d). In Pocman, an agent and four ghosts move in a 17 \u00d7 19 maze populated with food pellets. Each agent move incurs a cost of \u22121. Each food pellet provides a reward of +10. If\nthe agent is captured by a ghost, the game terminates with a penalty of \u2212100. In addition, there are four power pills. Within the next 15 time steps after eating a power pill, the agent can eat a ghost encountered and receives a reward of +25. A ghost chases the agent if the agent is within a Manhattan distance of 5, but runs away if the agent possesses a power pill. The agent does not know the exact ghost locations, but receives information on whether it sees a ghost in each of the cardinal directions, on whether it hears a ghost within a Manhattan distance of 2, on whether it feels a wall in each of the four cardinal directions, and on whether it smells food pellets in adjacent or diagonally adjacent cells. Pocman has an extremely large state space of roughly 1056 states.\nFor DESPOT, we compute an approximate hindsight optimization bound at a DESPOT node b by summing the following quantities for each scenario in \u03a6b and taking the average over all scenarios: the reward for eating each pellet discounted by its distance from pocman, the reward for clearing the level discounted by the maximum distance to a pellet, the default per-step reward of \u22121 for a number of steps equal to the maximum distance to a pellet, the penalty for eating a ghost discounted by the distance to the closest ghost being chased if any, the penalty for dying discounted by the average distance to the ghosts, and half the penalty for hitting a wall if the agent tries to double back along its direction of movement. Under the default policy, when the agent detects a ghost, it chases a ghost if it possess a power pill and runs away from the ghost otherwise. When the agent does not detect any ghost, it makes a random move that avoids doubling back or going into walls. Due to the large scale of this domain, we usedK = 100 scenarios in the experiments in order to stay within the allocated 1 second online planning time. For this problem, POMCP uses the same default policy as DESPOT.\nOn this large-scale domain, DESPOT has slightly better performance than POMCP, while SARSOP and AEMS2 cannot run successfully."}, {"heading": "5.1.5 BRIDGE CROSSING", "text": "The experiments above indicate that both POMCP and DESPOT can handle very large POMDPs. However, the UCT search strategy (Kocsis & Szepesvari, 2006) which POMCP depends on has very poor worst-case behavior (Coquelin & Munos, 2007). We designed Bridge Crossing, a very simple domain, to illustrate this.\nIn Bridge Crossing, a person attempts to cross a narrow bridge over a mountain pass in the dark. He starts out at one end of bridge, but is uncertain of his exact initial position because of the darkness. At any time, he may call for rescue and terminate the attempt. In the POMDP model, the man has 10 discretized positions x \u2208 {0, 1, . . . , 9} along the bridge, with the person at the end of x = 0. He is uncertain about his initial position, with a maximum error of 1. He can move forward or backward, with cost \u22121. However, moving forward at x = 9 has cost 0, indicating successful crossing. For simplicity, we assume no movement noise. The person can call for rescue, with cost \u2212x\u2212 20, where x is his current position. The person has no observations while in the middle of the bridge. His attempt terminates when he successfully crosses the bridge or calls for rescue.\nDESPOT uses the uninformed upper bound and the trivial default policy of calling for rescue immediately. POMCP uses the same default policy.\nThis is an open-loop planning problem. A policy is simply a sequence of actions, as there are no observations. There are 3 possible actions at each step, and thus when considering policies of length at most 10, there are at most 31 + 32 + . . . + 310 < 311 policies. A simple breadth-first enumeration of these policies is sufficient to identify the optimal one: keep moving forward for\na maximum of 10 steps. Indeed, both SARSOP and AEMS2 obtain this optimal policy, as does DESPOT. While the initial upper bound and the default policy for DESPOT are uninformed, the backup operations improve the bounds and guide the search towards the right direction to close the gap between the upper and lower bounds. In contrast, POMCP\u2019s performance on this domain is poor, because it is misled by its default policy and has an extremely poor convergence rate for cases such as this. While the optimal policy is to move forward all the way, the Monte Carlo simulations employed by POMCP suggests doing exactly the opposite at each step\u2014move backward and call for rescue\u2014because calling for rescue early near the starting point incurs lower cost. Increasing the exploration constant c may somewhat alleviate this difficulty, but does not resolve it substantively."}, {"heading": "5.2 Benefits of Regularization", "text": "We now study the effect of regularization on the performance of DESPOT. If a POMDP has a large number of observations, the size of the corresponding belief tree and the optimal policy may be large as well. Overfitting to the sampled scenarios is thus more likely to occur, and we would expect that regularization may help. Indeed, Table 2 shows that Tag, Rock Sample, and Bridge Crossing, which all have a small or moderate number of observations, do not benefit much from regularization. The remaining two, Laser Tag and Pocman, which have a large number of observations, benefit more significantly from regularization.\nTo understand better the underlying cause, we designed another simple domain, Adventurer, with a variable number of observations. An adventurer explores an ancient ruin modeled as a 1\u00d7 5 grid. He is initially located in the leftmost cell. The treasure is located in the rightmost cell, with value uniformly distributed over a finite setX . If the adventurer reaches the rightmost cell and stays there for one step to dig up the treasure, he receives the value of the treasure as the reward. The adventurer can drive left, drive right, or stay in the same place. Each move may severely damage his vehicle with probability 0.5, because of the rough terrain. The damage incurs a cost of \u221210 and terminates the adventure. The adventurer has a noisy sensor that reports the value of the treasure at each time step. The sensor reading is accurate with probability 0.7, and the noise is evenly distributed over other possible values in X . At each time step, the adventurer must decide, based on the sensor readings received so far, whether to drive on in hope of getting the treasure or to stay put and protect his vehicle. With a discount factor of 0.95, the optimal policy is in fact to stay in the same place.\nWe studied two settings empirically: X = {101, 150} and X = {101, 102, . . . , 150}, which result in 2 and 50 observations, respectively. For each setting, we constructed 1,000 full DESPOTs from K = 500 randomly sampled scenarios. We compute the optimal action without regularization\nfor each DESPOT. In the first setting with 2 observations, the computed action is always to stay in the same place. This is indeed optimal. In the second setting with 50 observations, about half of the computed actions are to move right. This is suboptimal. Why did this happen?\nRecall that the sampled scenarios are split among the observation branches. With 2 observations, each observation branch has 250 scenarios on average, after the first step. This is sufficient to represent the uncertainty well. With 50 observations, each observation branch has only about 5 scenarios on average. Because of the sampling variance, the algorithm easily underestimates the probability and the expected cost of damage to the vehicle. In other words, it overfits to the sampled scenarios. Overfitting thus occurs much more often with a large number of observations.\nRegularization helps to alleviate overfitting. For the second setting, we ran the algorithm with and without regularization. Without regularization, the average total discounted reward is \u22126.06\u00b1 0.24. With regularization, the average total discounted reward is 0\u00b1 0, which is optimal."}, {"heading": "5.3 Effect of Initial Bounds", "text": "Both DESPOT and POMCP rely on Monte Carlo simulation as a key algorithmic technique. They differ in two main aspects. One is their search strategies. We have seen in Section 5.1 that POMCP\u2019s search strategy is more greedy, while DESPOT\u2019s search strategy is more robust. Another difference is their default policies. While POMCP requires history-based default policies, DESPOT provides greater flexibility. Here we look into the benefits of this flexibility and also illustrate several additional techniques for constructing effective default policies and initial upper bounds. The benefits of cleverly-crafted default policies and initial upper bounds are domain-dependent. We examine three domains \u2014 Tag, Laser Tag, and Pocman \u2014 to give some intuition. Tag and Laser Tag are closely related for easy comparison, and both Laser Tag and Pocman are large-scale domains. The results for Tag, Laser Tag, and Pocman are shown on Tables 3 and 4.\nFor Tag and Laser Tag, we considered the following default policies for both problems.\n\u2022 NORTH is a domain-specific, fixed-action policy that always moves the robot to the adjacent grid cell to the north at every time step.\n\u2022 HIST is the improved history-based policy used by POMCP in the Tag experiment (Section 5.1.1).\n\u2022 HYBRID is the hybrid policy used by DESPOT in the Laser Tag experiment (Section 5.1.2).\n\u2022 The mode-MDP policy is the scenario-based policy described in Section 4.4 and used by DESPOT in the Tag experiment (Section 5.1.1). It first estimates the most likely state and then applies the optimal MDP policy accordingly.\n\u2022 MODE-SP is a variant of the MODE-MDP policy. Instead of the optimal MDP policy, it applies a handcrafted domain-specific policy, which takes the action that moves the robot towards the target along a shortest path in the most likely state.\nNext, consider the initial upper bounds. UI is the uninformed upper bound (13). MDP is the MDP upper bound (16). SP is the domain-specific bound used in Laser Tag (Section 5.1.2). We can use any of these three bounds, UI, MDP, or SP, to initialize U(D, s) and obtain a corresponding hindsight optimization bound.\nFor Pocman, we considered three default policies. NORTH is the policy of always moving to cell to the north. RANDOM is the policy which moves randomly to a legal adjacent cell. REACTIVE is the policy described in Section 5.1.4. We also considered two initial upper bounds. UI is the uninformed upper bound (13). AHO is the approximate hindsight optimization bound described in Section 5.1.4.\nThe results in Tables 3 and 4 offer several observations. First, DESPOT\u2019s online search considerably improves the default policy that it starts with, regardless of the specific default policy and upper bound used. This indicates the importance of online search. Second, while some initial upper bounds are approximate, they nevertheless yield very good results, even compared with true upper bounds. This is illustrated by the approximate MDP bounds for Tag and Laser Tag as well as the approximate hindsight optimization bound for Pocman. Third, the simple uninformed upper bound can sometimes yield good results, when paired with suitable default policies. Finally, these observations are consistent across domains of different sizes, e.g., Tag and Laser Tag.\nIn these examples, default policies seem to have much more impact than initial upper bounds do. It is also interesting to note that for LaserTag, HYBRID is one of the worst-performing default policies, but leads to the best performance ultimately. So a stronger default policy does not always lead to better performance. There are other factors that may affect the performance, and flexibility in constructing both the upper bounds and initial policies can be useful."}, {"heading": "6. Discussion", "text": "One key idea of DESPOT is to use a set of randomly sampled scenarios as an approximate representation of uncertainty and plan over the sampled scenarios. This works well if there exists a compact, near-optimal policy. The basic idea extends beyond POMDP planning and applies to other planning tasks under uncertainty, e.g., MDP planning and belief-space MDP planning.\nThe search strategy of the anytime DESPOT algorithm has several desirable properties. It is asymptotically sound and complete (Theorem 4.2). It is robust against imperfect heuristics (Theorem 4.2). It is also flexible and allows easy incorporation of domain knowledge. However, there are many alternatives, e.g., the UCT strategy used in POMCP. While UCT has very poor performance in the worst case, it is simpler to implement, an important practical consideration. Further, it avoids the overhead of computing the upper and lower bounds during the search and thus could be more efficient in some cases. In practice, there is trade-off between simplicity and robustness.\nLarge observation spaces may cause DESPOT to overfit to the sampled scenarios (see Section 5.2) and pose a major challenge. Unfortunately, this may happen with many common sensors, such as cameras and laser range finders. Regularization alleviates the difficulty. We are currently investigating several other approaches: structure the observation space hierarchically and importance sampling.\nThe default policy plays an important role in DESPOT. A good default policy reduces the size of the optimal policy. It also helps guide the heuristic search in the anytime search algorithm. In practice, we want the default policy to incorporate as much domain knowledge as possible for good performance. Another interesting direction is to learn the default policy during the search."}, {"heading": "7. Conclusions", "text": "This paper presents DESPOT, a new approach to online POMDP planning. The main underlying idea is to plan according to a set of sampled scenarios while avoiding overfitting to the samples. Theoretical analysis shows that a DESPOT compactly captures the \u201cexecution\u201d of all policies under the sampled scenarios and yields a near-optimal policy, provided that there is a small near-optimal policy. The analysis provides the justification for our overall planning approach based on sampled scenarios and the need for regularization. Experimental results indicate strong performance of the anytime DESPOT algorithm in practice. On moderately-sized POMDPs, DESPOT is competitive with SARSOP and AEMS2, but it scales up much better. On large-scale POMDPs with up to 1056 states, DESPOT matches and sometimes outperforms POMCP."}, {"heading": "Acknowledgments", "text": "The work was mainly performed while N. Ye was with the Department of Computer Science, National University of Singapore. We are grateful to the anonymous reviewers for carefully reading the manuscript and providing many suggestions which helped greatly in improving the paper. The work is supported in part by National Research Foundation Singapore through the SMART IRG program, US Air Force Research Laboratory under agreements FA2386-12-1-4031 and FA2386-15-1-4010, a Vice Chancellor\u2019s Research Fellowship provided by Queensland University of Technology and an Australian Laureate Fellowship (FL110100281) provided by Australian Research Council."}, {"heading": "Appendix A. Proofs", "text": "Proof of Theorem 3.1\nWe will need the following lemma from (Haussler, 1992, p. 103) Lemma 9, part (2).\nLemma A.1 (Haussler\u2019s bound) Let Z1, . . . , Zn be i.i.d random variables with range 0 \u2264 Zi \u2264M , E(Zi) = \u00b5, and \u00b5\u0302 = 1n \u2211n i=1 Zi, 1 \u2264 i \u2264 n. Assume \u03bd > 0 and 0 < \u03b1 < 1. Then\nPr (d\u03bd(\u00b5\u0302, \u00b5) > \u03b1) < 2e \u2212\u03b12\u03bdn/M\nwhere d\u03bd(r, s) = |r\u2212s| \u03bd+r+s . As a consequence,\nPr ( \u00b5 <\n1\u2212 \u03b1 1 + \u03b1 \u00b5\u0302\u2212 \u03b1 1 + \u03b1 \u03bd\n) < 2e\u2212\u03b1 2\u03bdn/M .\nLet \u03a0i be the class of policy trees in \u03a0b0,D,K and having size i. The next lemma bounds the size of \u03a0i.\nLemma A.2 |\u03a0i| \u2264 ii\u22122(|A||Z|)i.\nProof. Let \u03a0\u2032i be the class of rooted ordered trees of size i. A policy tree in \u03a0i is obtained from some tree in \u03a0\u2032i by assigning one of the |A| possible action labels to each node, and one of at most |Z| possible labels to each edge, thus |\u03a0i| \u2264 |A|i \u00b7 |Z|i\u22121. |\u03a0\u2032i|. To bound |\u03a0\u2032i|, note that it is not more than the number of all trees with i labeled nodes, because the in-order labeling of a tree in \u03a0\u2032i corresponds to a labeled tree. By Cayley\u2019s formula (1889), the number of trees with i labeled nodes is ii\u22122, thus |\u03a0\u2032i| \u2264 i(i\u22122). Therefore |\u03a0i| \u2264 ii\u22122 \u00b7 |A|i \u00b7 |Z|i\u22121 \u2264 ii\u22122(|A||Z|)i.\nIn the following, we often abbreviate V\u03c0(b0) and V\u0302\u03c0(b0) as V\u03c0 and V\u0302\u03c0 respectively, since we will only consider the true and empirical values for a fixed but arbitrary b0. Our proof follows a line of reasoning similar to that of Wang, Won, Hsu, and Lee (2012).\nTheorem 3.1 For any \u03c4, \u03b1 \u2208 (0, 1), any belief b0, and any positive integers D and K, with probability at least 1\u2212 \u03c4 , every DESPOT policy tree \u03c0 \u2208 \u03a0b0,D,K satisfies\nV\u03c0(b0) \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0(b0)\u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3) \u00b7\nln(4/\u03c4) + |\u03c0| ln ( KD|A||Z| ) \u03b1K ,\nwhere the random variable V\u0302\u03c0(b0) is the estimated value of \u03c0 under the set ofK scenarios randomly sampled according to b0.\nProof. Consider an arbitrary policy tree \u03c0 \u2208 \u03a0b0,D,K . We know that for a random scenario \u03c6 for the belief b0, executing the policy \u03c0 w.r.t. \u03c6 gives us a sequence of states and observations distributed according to the distributions P (s\u2032|s, a) and P (z|s, a). Therefore, for \u03c0, its true value V\u03c0 equals E (V\u03c0,\u03c6), where the expectation is over the distribution of scenarios. On the other hand, since V\u0302\u03c0 = 1 K \u2211K k=1 V\u03c0,\u03c6k , and the scenarios \u03c61,\u03c62, . . . ,\u03c6K are independently sampled, Lemma A.1 gives\nPr ( V\u03c0 <\n1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 \u03b1 1 + \u03b1 |\u03c0|\n) < 2e\u2212\u03b1 2 |\u03c0|K/M (17)\nwhere M = (Rmax)/(1\u2212 \u03b3), and |\u03c0| is chosen such that\n2e\u2212\u03b1 2 |\u03c0|K/M = \u03c4/(2|\u03c0|2|\u03a0|\u03c0||). (18)\nBy the union bound, we have\nPr ( \u2203\u03c0 \u2208 \u03a0b0,D,K such that V\u03c0 <\n1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 \u03b1 1 + \u03b1 |\u03c0| ) \u2264 \u221e\u2211 i=1 \u2211 \u03c0\u2208\u03a0i Pr ( V\u03c0 < 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 \u03b1 1 + \u03b1 |\u03c0| ) .\nBy the choice of i\u2019s and Inequality (17), the right hand side of the above inequality is bounded by \u2211\u221e i=1 |\u03a0i| \u00b7 [\u03c4/(2i2|\u03a0i|)] = \u03c02\u03c4/12 < \u03c4 , where the well-known identity \u2211\u221e i=1 1/i 2 = \u03c02/6 is used. Hence,\nPr ( \u2203\u03c0 \u2208 \u03a0b0,D,K [ V\u03c0 <\n1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 \u03b1 1 + \u03b1 |\u03c0|\n]) < \u03c4. (19)\nEquivalently, with probability 1\u2212 \u03c4 , every \u03c0 \u2208 \u03a0b0,D,K satisfies\nV\u03c0 \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 \u03b1 1 + \u03b1 |\u03c0|. (20)\nTo complete the proof, we now give an upper bound on |\u03c0|. From Equation (18), we can solve\nfor |\u03c0| to get i = Rmax \u03b1(1\u2212\u03b3) \u00b7 ln(4/\u03c4)+ln(i2|\u03a0i|) \u03b1K . For any \u03c0 in \u03a0b0,D,K , its size is at most KD, and i2|\u03a0i| \u2264 (i|A||Z|)i \u2264 (KD|A||Z|)i by Lemma A.2. Thus we have\n|\u03c0| \u2264 Rmax \u03b1(1\u2212 \u03b3) \u00b7 ln(4/\u03c4) + |\u03c0| ln(KD|A||Z|) \u03b1K .\nCombining this with Inequality (20), we get\nV\u03c0 \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0 \u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3) \u00b7 ln(4/\u03c4) + |\u03c0| ln(KD|A||Z|) \u03b1K .\nThis completes the proof.\nProof of Theorem 3.2\nWe need the following lemma for proving Theorem 3.2.\nLemma A.3 For a fixed policy \u03c0 and any \u03c4 \u2208 (0, 1), with probability at least 1\u2212 \u03c4 .\nV\u0302\u03c0 \u2265 V\u03c0 \u2212 Rmax 1\u2212 \u03b3\n\u221a 2 ln(1/\u03c4)\nK\nProof. Let \u03c0 be a policy and V\u03c0 and V\u0302\u03c0 as mentioned. Hoeffding\u2019s inequality (1963) gives us Pr ( V\u0302\u03c0 \u2265 V\u03c0 \u2212 ) \u2265 1\u2212 e\u2212K 2/(2M2),\nwhere M = Rmax/(1\u2212 \u03b3). Let \u03c4 = e\u2212K 2/(2M2) and solve for , then we get\nPr ( V\u0302\u03c0 \u2265 V\u03c0 \u2212\nRmax 1\u2212 \u03b3\n\u221a 2 ln(1/\u03c4)\nK\n) \u2265 1\u2212 \u03c4.\nTheorem 3.2 Let \u03c0 be an arbitrary policy at a belief b0. Let \u03a0D be the set of policies derived from a DESPOT D that has height D and is constructed from K scenarios sampled randomly according to b0. For any \u03c4, \u03b1 \u2208 (0, 1), if\n\u03c0\u0302 = arg max \u03c0\u2032\u2208\u03a0D\n{ 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0\u2032(b0)\u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3) \u00b7 |\u03c0\u2032| ln ( KD|A||Z| ) \u03b1K } ,\nthen\nV\u03c0\u0302(b0) \u2265 1\u2212\u03b11+\u03b1V\u03c0(b0)\u2212 Rmax (1+\u03b1)(1\u2212\u03b3)\n( ln(8/\u03c4)+|\u03c0| ln ( KD|A||Z| ) \u03b1K + (1\u2212 \u03b1) (\u221a 2 ln(2/\u03c4) K + \u03b3 D )) ,\nwith probability at least 1\u2212 \u03c4 .\nProof. By Theorem 1, with probability at least 1\u2212 \u03c4/2,\nV\u03c0\u0302 \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0\u0302 \u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3)\n[ ln(8/\u03c4) + |\u03c0\u0302| ln(KD|A||Z|)\n\u03b1K\n] .\nSuppose the above inequality holds on a random set of K scenarios. Note that there is a \u03c0\u2032 \u2208 \u03a0b0,D,K which is a subtree of \u03c0 and has the same trajectories on these scenarios up to depth D. By the choice of \u03c0\u0302, it follows that the following hold on the same scenarios\nV\u03c0\u0302 \u2265 1\u2212 \u03b1 1 + \u03b1 V\u0302\u03c0\u2032 \u2212 Rmax (1 + \u03b1)(1\u2212 \u03b3)\n[ ln(8/\u03c4) + |\u03c0\u2032| ln(KD|A||Z|)\n\u03b1K\n] .\nIn addition, |\u03c0| \u2265 |\u03c0\u2032|, and V\u0302\u03c0\u2032 \u2265 V\u0302\u03c0 \u2212 \u03b3D(Rmax)/(1 \u2212 \u03b3) since \u03c0\u2032 and \u03c0 only differ from depth D onwards, under the chosen scenarios. It follows that for these scenarios, we have\nV\u03c0\u0302 \u2265 1\u2212 \u03b1 1 + \u03b1\n( V\u0302\u03c0 \u2212 \u03b3D\nRmax 1\u2212 \u03b3\n) \u2212 Rmax\n(1 + \u03b1)(1\u2212 \u03b3)\n[ ln(8/\u03c4) + |\u03c0| ln(KD|A||Z|)\n\u03b1K\n] . (21)\nHence, the above inequality holds with probability at least 1\u2212 \u03c4/2. By Lemma A.3, with probability at least 1\u2212 \u03c4/2, we have\nV\u0302\u03c0 \u2265 V\u03c0 \u2212 Rmax 1\u2212 \u03b3\n\u221a 2 ln(2/\u03c4)\nK . (22)\nBy the union bound, with probability at least 1 \u2212 \u03c4 , both Inequality (21) and Inequality (22) hold, which imply the inequality in the theorem holds. This completes the proof.\nProof of Theorem 4.1 and Theorem 4.2\nLemma 4.1 For any DESPOT node b, if E(b) > 0 and a\u2217 = arg maxa\u2208A \u00b5(b, a), then\nE(b) \u2264 \u2211\nz\u2208Zb,a\u2217 E(b\u2032),\nwhere b\u2032 = \u03c4(b, a\u2217, z) is a child of b.\nProof. Let CH(b, a\u2217) be all the set of all b\u2032 of the form b\u2032 = \u03c4(b, a, z) for some z \u2208 Zb,a\u2217 , that is, the set of children nodes of b in the DESPOT tree. If E(b) > 0, then \u00b5(b) \u2212 `(b) > 0, and thus \u00b5(b) 6= `0(b). Hence we have\n\u00b5(b) = \u00b5(b, a\u2217) = \u03c1(b, a\u2217) + \u2211\nb\u2032\u2208CH(b,a\u2217)\n\u00b5(b\u2032), and\n`(b) \u2265 `(b, a\u2217) \u2265 \u03c1(b, a\u2217) + \u2211\nb\u2032\u2208CH(b,a\u2217)\n`(b\u2032),\nSubtracting the first equation by the second inequality, we have \u00b5(b)\u2212 `(b) \u2264 \u2211\nb\u2032\u2208CH(b,a\u2217)\n[\u00b5(b\u2032)\u2212 `(b\u2032)].\nNote that |\u03a6b| K \u00b7 \u03be \u00b7 (b0) = \u2211 b\u2032\u2208CH(b,a\u2217) |\u03a6b\u2032 | K \u00b7 \u03be \u00b7 (b0).\nWe have \u2211 b\u2032\u2208CH(b,a\u2217) [\u00b5(b\u2032)\u2212 `(b\u2032)\u2212 |\u03a6b \u2032 | K \u00b7 \u03be \u00b7 (b0)] \u2265 \u00b5(b)\u2212 `(b)\u2212 |\u03a6b| K \u00b7 \u03be (b0).\nThat is, E(b) \u2264 \u2211\nb\u2032\u2208CH(b,a\u2217)E(b \u2032).\nLemma 4.2 Let b\u2032 be an ancestor of b in a DESPOT D and `(b\u2032, b) be the number of nodes on the path from b\u2032 to b. If\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)(U(b\u2032)\u2212 L0(b\u2032)) \u2264 \u03bb \u00b7 `(b\u2032, b),\nthen b cannot be a belief node in an optimal regularized policy derived from D.\nProof. If b is included in the policy \u03c0 maximizing the regularized utility, then for any node b\u2032 on the path from b to the root, the subtree \u03c0b\u2032 under b\u2032 satisfies\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)U(b\u2032)\u2212 \u03bb|\u03c0b\u2032 |\n\u2265|\u03a6b \u2032 |\nK \u03b3\u2206(b \u2032)V\u0302\u03c0b\u2032 (b \u2032)\u2212 \u03bb|\u03c0b\u2032 | (since U(b\u2032) > V\u0302\u03c0b\u2032 (b \u2032))\n> |\u03a6b\u2032 | K \u03b3\u2206(b \u2032)L0(b \u2032), (since b\u2032 is not pruned)\nwhich implies |\u03a6b\u2032 | K \u03b3\u2206(b \u2032)U(b\u2032)\u2212 \u03bb|\u03c0b\u2032 | > |\u03a6b\u2032 | K \u03b3\u2206(b \u2032)L0(b \u2032).\nRearranging the terms in the above inequality, we have\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)[U(b\u2032)\u2212 L0(b\u2032)] > \u03bb|\u03c0b\u2032 |.\nWe have `(b\u2032, b) \u2264 |\u03c0b\u2032 | as b is not pruned and thus \u03c0b\u2032 need to include all nodes from b\u2032 to b. Hence,\n|\u03a6b\u2032 | K \u03b3\u2206(b \u2032)[U(b\u2032)\u2212 L0(b\u2032)] > \u03bb \u00b7 `(b\u2032, b).\nTheorem 4.1 Suppose that Tmax is bounded and that the anytime DESPOT algorithm terminates with a partial DESPOT D\u2032 that has gap (b0) between the upper and lower bounds at the root b0. The optimal regularized policy \u03c0\u0302 derived from D\u2032 satisfies\n\u03bd\u03c0\u0302(b0) \u2265 \u03bd\u2217(b0)\u2212 (b0)\u2212 \u03b4,\nwhere \u03bd\u2217(b0) is the value of an optimal regularized policy derived from the full DESPOT D at b0.\nProof. Let U \u20320(b) = U0(b) + \u03b4, then U \u20320 is an exact upper bound. Let \u00b5\u20320 be the corresponding initial upper bound, and \u00b5\u2032 be the corresponding upper bound on \u03bd\u2217(b). Then \u00b5\u20320 is a valid initial upper bound for \u03bd\u2217(b) and the backup equations ensure that \u00b5\u2032(b) is a valid upper bound for \u03bd\u2217(b). On the other hand, it can be easily shown by induction that \u00b5(b) + \u03b3\u2206(b) |\u03a6b|K \u03b4 \u2265 \u00b5\n\u2032(b). As a special case for b = b0, we have \u00b5(b) + \u03b4 \u2265 \u00b5\u2032(b0). Hence, when the algorithm terminates, we also have \u00b5(b0) + \u03b4 \u2265 \u00b5\u2032(b0) \u2265 \u03bd\u2217(b0). Equivalently, \u03bd\u03c0\u0302 = `(b0) \u2265 \u03bd\u2217(b0) \u2212 (\u00b5(b0) \u2212 `(b0)) \u2212 \u03b4 = \u03bd\u2217(b0) \u2212 (b0) \u2212 \u03b4. The first equality holds because the initialization and the computation of the lower bound ` via the backup equations are exactly that for finding an optimal regularized policy value in the partial DESPOT.\nTheorem 4.2 Suppose that Tmax is unbounded and 0 is the target gap between the upper and lower bound at the root of the partial DESPOT constructed by the anytime DESPOT algorithm. Let \u03bd\u2217(b0) be the value of an optimal policy derived from the full DESPOT D at b0.\n(1) If 0 > 0, then the algorithm terminates in finite time with a near-optimal policy \u03c0\u0302 satisfying\n\u03bd\u03c0\u0302(b0) \u2265 \u03bd\u2217(b0)\u2212 0 \u2212 \u03b4.\n(2) If 0 = 0, \u03b4 = 0, and the regularization constant \u03bb > 0, then the algorithm terminates in finite time with an optimal policy \u03c0\u0302, i.e., \u03bd\u03c0\u0302(b0) = \u03bd\u2217(b0).\nProof. (1) It suffices to show that eventually \u00b5(b0) \u2212 `(b0) \u2264 0, which then implies \u03bd\u03c0\u0302(b0) \u2265\n\u03bd\u2217(b0)\u2212 0 \u2212 \u03b4 by Theorem 4.1. We first show that only nodes within certain depth d0 will ever be expanded. In particular, we can choose any d0 > 0 such that \u03be 0 > \u03b3d0Rmax/(1 \u2212 \u03b3). For any node b beyond depth d0, we have (b) \u2264 \u03b3\u2206(b) |\u03a6b|K Rmax 1\u2212\u03b3 \u2264 \u03b3 d0 |\u03a6b| K Rmax 1\u2212\u03b3 < |\u03a6b| K \u00b7 \u03be \u00b7 0. Since (b0) > 0 before the algorithm terminates, we have E(b) = (b) \u2212 |\u03a6b|K \u00b7 \u03be \u00b7 (b0) < (b) \u2212 |\u03a6b| K \u00b7 \u03be \u00b7 0 < 0, and b will not be expanded during the search. On the other hand, the forward search heuristic guarantees that each exploration closes the gap of at least one node or expands at least one node. To see this, note that an exploration terminates when its depth exceeds the limit D, or when a node is pruned, or when it encounters a node b with E(b) < 0. In the first two cases, the gap for the last node is closed. We show that for the last case, the exploration must have expanded at least one node. By Lemma 4.1, if E(b) > 0, then the next chosen node b\u2032 satisfies E(b\u2032) > 0, otherwise the upper bound for E(b) will be at most 0, a contradiction. Since the root b0 has positive E, thus the exploration will follow a path with nodes with positive E to reach a leaf node, and then expand it. Termination may then happen as the next chosen node has negative E.\nSince each node can be expanded or closed at most once, but only nodes within depth d0 can be considered, thus after finitely many steps (b0) = \u00b5(b0)\u2212 `(b0) \u2264 0, and the search terminates.\n(2) Following the proof of (1), when = 0, before \u00b5(b0) = `(b0), each exploration either closes the gap of at least one node, or expands at least one leaf node. However, both can only be done finitely many times because the Prune procedure ensures only nodes within a depth of d Rmax\u03bb(1\u2212\u03b3)e+ 1 can be expanded, and the gap of each such node can only be closed once only. Thus the search will terminate in finite time with \u00b5(b0) = `(b0). Since the upper bound is a true one, we have \u03bd\u03c0\u0302(b0) = `(b0) = \u03bd\n\u2217(b0), and thus \u03c0\u0302 is an optimal regularized policy derived from the infinite horizon DESPOT."}, {"heading": "Appendix B. Pseudocode for Anytime DESPOT", "text": "Algorithm 6 Anytime DESPOT\nInput\n\u03b2 : Initial belief.\n0 : The target gap between \u00b5(b0) and `(b0).\n\u03be : The rate of target gap reduction.\nK : The number of sampled scenarios.\nD : The maximum depth of the DESPOT.\n\u03bb : Regularization constant.\nTmax : The maximum online planning time per step.\n1: Initialize b\u2190 \u03b2. 2: loop 3: `\u2190 BUILDDESPOT(b). 4: a\u2217 \u2190 maxa\u2208A `(b, a). 5: if L0(b) > `(b, a\u2217) then 6: a\u2217 \u2190 \u03c00(b) 7: Execute a\u2217. 8: Receive observation z. 9: b\u2190 \u03c4(b, a\u2217, z).\nBUILDDESPOT(b0) 1: Sample randomly a set \u03a6b0 ofK scenarios\nfrom the current belief b0. 2: Create a new DESPOT D with a single\nnode b as the root. 3: Initialize U(b0), L0(b0), \u00b5(b0), and `(b0). 4: (b0)\u2190 \u00b5(b0)\u2212 `(b0). 5: while (b0) > 0 and the total running\ntime is less than Tmax do 6: b\u2190 EXPLORE(D, b0). 7: BACKUP(D, b). 8: (b0)\u2190 \u00b5(b0)\u2212 `(b0). 9: return `\nEXPLORE(D, b) 1: while \u2206(b) \u2264 D, E(b) > 0, and\nPRUNE(D, b) = FALSE do 2: if b is a leaf node in D then 3: Expand b one level deeper. Insert\neach new child b\u2032 of b into D, and initialize U(b\u2032), L0(b\u2032), \u00b5(b\u2032), and `(b\u2032).\n4: a\u2217 \u2190 arg maxa\u2208A \u00b5(b, a). 5: z\u2217 \u2190 arg maxz\u2208Zb,a\u2217 E(\u03c4(b, a\n\u2217, z)). 6: b\u2190 \u03c4(b, a\u2217, z\u2217). 7: if \u2206(b) > D then 8: MAKEDEFAULT(b). 9: return b.\nPRUNE(D, b) 1: BLOCKED \u2190 FALSE. 2: for each node x on the path from b to the\nroot of D do 3: if x is blocked by any ancestor node in D then 4: MAKEDEFAULT(x); 5: BACKUP(D, x). 6: BLOCKED \u2190 TRUE. 7: else 8: break 9: return BLOCKED\nMAKEDEFAULT(b) 1: U(b)\u2190 L0(b). 2: \u00b5(b)\u2190 `0(b). 3: `(b)\u2190 `0(b).\nBACKUP(D, b) 1: for each node x on the path from b to the\nroot of D do 2: Perform backup on \u00b5(x), `(x), and\nU(x)."}], "references": [{"title": "Monte carlo value iteration for continuous-state POMDPs", "author": ["H. Bai", "D. Hsu", "W.S. Lee", "V. Ngo"], "venue": "Algorithmic Foundations of Robotics IX,", "citeRegEx": "Bai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2011}, {"title": "Intention-Aware Online POMDP Planning for Autonomous Driving in a Crowd", "author": ["H. Bai", "S. Cai", "N. Ye", "D. Hsu", "W. Lee"], "venue": "In Proc. IEEE Int. Conf. on Robotics & Automation", "citeRegEx": "Bai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2015}, {"title": "Rollout algorithms for stochastic scheduling problems", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": "J. Heuristics,", "citeRegEx": "Bertsekas and Castanon,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1999}, {"title": "A Theorem on Trees", "author": ["A. Cayley"], "venue": "Quart. J. Math, 23, 376\u2013378.", "citeRegEx": "Cayley,? 1889", "shortCiteRegEx": "Cayley", "year": 1889}, {"title": "MOMDPs: A Solution for Modelling Adaptive Management Problems", "author": ["I. Chad\u00e8s", "J. Carwardine", "T. Martin", "S. Nicol", "R. Sabbadin", "O. Buffet"], "venue": "In Proc. AAAI Conf. on Artificial Intelligence", "citeRegEx": "Chad\u00e8s et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chad\u00e8s et al\\.", "year": 2012}, {"title": "A framework for simulation-based network control via hindsight optimization", "author": ["E. Chong", "R. Givan", "H. Chang"], "venue": "In Proc. IEEE Conf. on Decision & Control,", "citeRegEx": "Chong et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chong et al\\.", "year": 2000}, {"title": "Bandit algorithms for tree search", "author": ["Coquelin", "P.-A", "R. Munos"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "Coquelin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Coquelin et al\\.", "year": 2007}, {"title": "Combining online and offline knowledge in UCT", "author": ["S. Gelly", "D. Silver"], "venue": "In Proc. Int. Conf. on Machine Learning", "citeRegEx": "Gelly and Silver,? \\Q2007\\E", "shortCiteRegEx": "Gelly and Silver", "year": 2007}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A. Smith"], "venue": "IEE Proc. F on Radar & Signal Processing,", "citeRegEx": "Gordon et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 1993}, {"title": "Planning treatment of ischemic heart disease with partially observable Markov decision processes", "author": ["M. Hauskrecht", "H. Fraser"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Hauskrecht and Fraser,? \\Q2000\\E", "shortCiteRegEx": "Hauskrecht and Fraser", "year": 2000}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and Computation, 100(1), 78\u2013150.", "citeRegEx": "Haussler,? 1992", "shortCiteRegEx": "Haussler", "year": 1992}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["R. He", "E. Brunskill", "N. Roy"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. American statistical association, 58(301), 13\u201330.", "citeRegEx": "Hoeffding,? 1963", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["M. Kearns", "Y. Mansour", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesvari"], "venue": "In Proc. Eur. Conf. on Machine Learning,", "citeRegEx": "Kocsis and Szepesvari,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesvari", "year": 2006}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W. Lee"], "venue": "In Proc. Robotics: Science and Systems", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Nonapproximability results for partially observable Markov decision processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Lusena et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 2001}, {"title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "author": ["O. Madani", "S. Hanks", "A. Condon"], "venue": "In Proc. AAAI Conf. on Artificial Intelligence,", "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A. Ng", "M. Jordan"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "An online POMDP algorithm for complex multiagent environments", "author": ["S. Paquet", "L. Tobin", "B. Chaib-Draa"], "venue": "In Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proc. Int. Jnt. Conf. on Artificial Intelligence,", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Exploiting structure to efficiently solve large scale partially observable Markov decision processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis, University of Toronto.", "citeRegEx": "Poupart,? 2005", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "AEMS: An anytime online search algorithm for approximate policy refinement in large POMDPs", "author": ["S. Ross", "B. Chaib-Draa"], "venue": "In Proc. Int. Jnt. Conf. on Artificial Intelligence,", "citeRegEx": "Ross and Chaib.Draa,? \\Q2007\\E", "shortCiteRegEx": "Ross and Chaib.Draa", "year": 2007}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-Draa"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Coastal navigation: mobile robot navigation with uncertainty in dynamic environments", "author": ["N. Roy", "W. Burgard", "D. Fox", "S. Thrun"], "venue": "In Proc. IEEE Int. Conf. on Robotics & Automation", "citeRegEx": "Roy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Roy et al\\.", "year": 1999}, {"title": "Markovian decision processes with probabilistic observation of states", "author": ["J. Satia", "R. Lave"], "venue": "Management Science,", "citeRegEx": "Satia and Lave,? \\Q1973\\E", "shortCiteRegEx": "Satia and Lave", "year": 1973}, {"title": "Monte-Carlo planning in large POMDPs", "author": ["D. Silver", "J. Veness"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Silver and Veness,? \\Q2010\\E", "shortCiteRegEx": "Silver and Veness", "year": 2010}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Smith and Simmons,? \\Q2004\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2004}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R. Simmons"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "DESPOT: POMDP planning with regularization", "author": ["A. Somani", "N. Ye", "D. Hsu", "W. Lee"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Somani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Somani et al\\.", "year": 2013}, {"title": "Perseus: Randomized point-based value iteration for POMDPs", "author": ["M. Spaan", "N. Vlassis"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "Spaan and Vlassis,? \\Q2005\\E", "shortCiteRegEx": "Spaan and Vlassis", "year": 2005}, {"title": "Monte Carlo Bayesian reinforcement learning", "author": ["Y. Wang", "K. Won", "D. Hsu", "W. Lee"], "venue": "In Proc. Int. Conf. on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Probabilistic planning via determinization", "author": ["YE", "HSU SOMANI", "S. LEE Yoon", "A. Fern", "R. Givan", "S. Kambhampati"], "venue": null, "citeRegEx": "YE et al\\.,? \\Q2008\\E", "shortCiteRegEx": "YE et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "There has been substantial progress in the last decade (Pineau, Gordon, & Thrun, 2003; Smith & Simmons, 2004; Poupart, 2005; Kurniawati, Hsu, & Lee, 2008; Silver & Veness, 2010; Bai, Hsu, Lee, & Ngo, 2011).", "startOffset": 55, "endOffset": 205}, {"referenceID": 23, "context": "Although offline planning algorithms have made major progress in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Smith & Simmons, 2005; Kurniawati et al., 2008), they still face significant difficulty in scaling up to very large POMDPs, as they must plan for all beliefs and future contingencies.", "startOffset": 78, "endOffset": 170}, {"referenceID": 17, "context": "Although offline planning algorithms have made major progress in recent years (Pineau et al., 2003; Spaan & Vlassis, 2005; Smith & Simmons, 2005; Kurniawati et al., 2008), they still face significant difficulty in scaling up to very large POMDPs, as they must plan for all beliefs and future contingencies.", "startOffset": 78, "endOffset": 170}, {"referenceID": 26, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling.", "startOffset": 82, "endOffset": 101}, {"referenceID": 26, "context": "Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008).", "startOffset": 69, "endOffset": 113}, {"referenceID": 15, "context": "The sparse sampling (SS) algorithm (Kearns et al., 2002) and the DESPOT algorithm both construct sparse approximations to a belief tree.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling. Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008). This idea dates back to the early work of Satia and Lave (1973) .", "startOffset": 83, "endOffset": 352}, {"referenceID": 23, "context": "A recent survey lists three main ideas for online planning via belief tree search (Ross et al., 2008): heuristic search, branch-and-bound pruning, and Monte Carlo sampling. Heuristic search employs a heuristic to guide the belief tree search (Ross & Chaib-Draa, 2007; Ross et al., 2008). This idea dates back to the early work of Satia and Lave (1973) . Branch-and-bound pruning maintains upper and lower bounds on the value at each belief tree node and use them to prune suboptimal subtrees and improve computational efficiency (Paquet, Tobin, & Chaib-Draa, 2005). This idea is also present in earlier work on offline POMDP planning (Smith & Simmons, 2004). Monte Carlo sampling explores only a randomly sampled subset of observation branches at each node of the belief tree (Bertsekas & Castanon, 1999; Yoon, Fern, Givan, & Kambhampati, 2008; Kearns, Mansour, & Ng, 2002; Silver & Veness, 2010). Our DESPOT algorithm contains all three ideas, but is most closely associated with Monte Carlo sampling. Below we examine some of the earlier Monte Carlo sampling algorithms and DESPOT\u2019s connection with them. The rollout algorithm (Bertsekas & Castanon, 1999) is an early example of Monte Carlo sampling for planning under uncertainty. It is originally designed for Markov decision processes (MDPs), but can be easily adapted to solve POMDPs as well. It estimates the value of a default heuristic policy by performing K simulations and then chooses the best action by one-step lookahead search over the estimated values. Although a one-step lookahead policy improves over the default policy, it may be far from the optimum because of the very short, one-step search horizon. Like the rollout algorithm, the hindsight optimization algorithm (HO) (Chong, Givan, & Chang, 2000; Yoon et al., 2008) is intended for MDPs, but can be adapted for POMDPs. While both HO and DESPOT sample K scenarios for planning, HO builds one tree with O(|A|D) nodes for each scenario, independent of others, and thus K trees in total. It searches each tree for an optimal plan and averages the values of theseK optimal plans to choose a best action. HO and related algorithms have been quite successful in recent international probabilistic planning competitions. However, HO plans for each scenario independently; it optimizes an upper bound on the value of a POMDP and not the true value itself. In contrast, DESPOT captures allK scenarios in a single tree ofO(|A|DK) nodes and hedges against all K scenarios simultaneously during the planning. It converges to the true optimal value of the POMDP as K grows. The work of Kearns, Mansour, and Ng (1999) and that of Ng and Jordan (2000) use sampled scenarios as well, but for offline POMDP policy computation.", "startOffset": 83, "endOffset": 2629}, {"referenceID": 18, "context": "The work of Kearns, Mansour, and Ng (1999) and that of Ng and Jordan (2000) use sampled scenarios as well, but for offline POMDP policy computation.", "startOffset": 55, "endOffset": 76}, {"referenceID": 14, "context": "The bound benefits from the existence of a small near-optimal policy and naturally leads to a regularized objective function for online planning; in contrast, the algorithms by Kearns et al. (1999) and Ng and Jordan (2000) do not exploit the existence of good small policies within the class of policies under consideration.", "startOffset": 177, "endOffset": 198}, {"referenceID": 14, "context": "The bound benefits from the existence of a small near-optimal policy and naturally leads to a regularized objective function for online planning; in contrast, the algorithms by Kearns et al. (1999) and Ng and Jordan (2000) do not exploit the existence of good small policies within the class of policies under consideration.", "startOffset": 177, "endOffset": 223}, {"referenceID": 17, "context": "It selects the action by incrementally constructing a DESPOT D rooted at the current belief b0, using heuristic search (Smith & Simmons, 2004; Kurniawati et al., 2008), and approximating the optimal RWDU \u03bd(b0).", "startOffset": 119, "endOffset": 167}, {"referenceID": 33, "context": "This algorithm differs from an earlier version (Somani et al., 2013) in a subtle, but important way.", "startOffset": 47, "endOffset": 68}, {"referenceID": 30, "context": "5, the work of Smith and Simmons (2005) and the work of Kurniawati et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 17, "context": "5, the work of Smith and Simmons (2005) and the work of Kurniawati et al. (2008) for justifications of this strategy.", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "We compare DESPOT with SARSOP (Kurniawati et al., 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al.", "startOffset": 30, "endOffset": 55}, {"referenceID": 26, "context": ", 2008), AEMS2 (Ross & Chaib-Draa, 2007; Ross et al., 2008), and POMCP (Silver & Veness, 2010).", "startOffset": 15, "endOffset": 59}, {"referenceID": 17, "context": "We used the APPL package for SARSOP (Kurniawati et al., 2008).", "startOffset": 36, "endOffset": 61}, {"referenceID": 17, "context": "95 as in SARSOP (Kurniawati et al., 2008).", "startOffset": 16, "endOffset": 41}, {"referenceID": 26, "context": "We thus report both, with the results in earlier work (Ross et al., 2008; Silver & Veness, 2010) in parentheses.", "startOffset": 54, "endOffset": 96}, {"referenceID": 23, "context": "Tag is a standard POMDP benchmark introduced by Pineau et al. (2003). A robot and a target operate in a grid with 29 possible positions (Figure 4a).", "startOffset": 48, "endOffset": 69}, {"referenceID": 3, "context": "By Cayley\u2019s formula (1889), the number of trees with i labeled nodes is ii\u22122, thus |\u03a0i| \u2264 i(i\u22122).", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "By Cayley\u2019s formula (1889), the number of trees with i labeled nodes is ii\u22122, thus |\u03a0i| \u2264 i(i\u22122). Therefore |\u03a0i| \u2264 ii\u22122 \u00b7 |A|i \u00b7 |Z|i\u22121 \u2264 ii\u22122(|A||Z|)i. In the following, we often abbreviate V\u03c0(b0) and V\u0302\u03c0(b0) as V\u03c0 and V\u0302\u03c0 respectively, since we will only consider the true and empirical values for a fixed but arbitrary b0. Our proof follows a line of reasoning similar to that of Wang, Won, Hsu, and Lee (2012).", "startOffset": 3, "endOffset": 414}, {"referenceID": 12, "context": "Hoeffding\u2019s inequality (1963) gives us", "startOffset": 0, "endOffset": 30}], "year": 2017, "abstractText": "The partially observable Markov decision process (POMDP) provides a principled general framework for planning under uncertainty, but solving POMDPs optimally is computationally intractable, due to the \u201ccurse of dimensionality\u201d and the \u201ccurse of history\u201d. To overcome these challenges, we introduce the Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation of the standard belief tree, for online planning under uncertainty. A DESPOT focuses online planning on a set of randomly sampled scenarios and compactly captures the \u201cexecution\u201d of all policies under these scenarios. We show that the best policy obtained from a DESPOT is near-optimal, with a regret bound that depends on the representation size of the optimal policy. Leveraging this result, we give an anytime online planning algorithm, which searches a DESPOT for a policy that optimizes a regularized objective function. Regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. The algorithm demonstrates strong experimental results, compared with some of the best online POMDP algorithms available. It has also been incorporated into an autonomous driving system for real-time vehicle control. The source code for the algorithm is available online.", "creator": "TeX"}}}