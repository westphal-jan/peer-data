{"id": "1609.03777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks", "abstract": "Recurring neural network models (RNN) at the character level (CLMs) are extremely useful for modeling invisible words by nature, but their performance is generally much worse than word-level language models (WLMs), because CLMs have to take into account a longer history of tokens in order to accurately predict the next one. We address this problem by proposing hierarchical RNN architectures that consist of multiple modules at different clock speeds. Despite the multi-stroke structures, the input and output layers work with character timing, making the existing RNN CLM training approaches directly applicable without modifications. Our CLM models show a better perplexity than Kneser-Ney (KN) 5 gram WLMs on the One Billion Word benchmark, where only 2% of the parameters are used. We also present examples of real-time character speech recognition in Wall Street Journal (WSJ), where traditional mono CLM models are replaced by a number of 30% RN parameters.", "histories": [["v1", "Tue, 13 Sep 2016 11:41:48 GMT  (174kb)", "http://arxiv.org/abs/1609.03777v1", "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016"], ["v2", "Thu, 2 Feb 2017 13:49:41 GMT  (178kb)", "http://arxiv.org/abs/1609.03777v2", "Submitted to NIPS 2016 on May 20, 2016 (v1), accepted to ICASSP 2017 (v2)"]], "COMMENTS": "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1609.03777"}, "pdf": {"name": "1609.03777.pdf", "metadata": {"source": "CRF", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks", "authors": ["Kyuyeon Hwang"], "emails": ["kyuyeon.hwang@gmail.com;", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n03 77\n7v 1\n[ cs\n.L G\n] 1\n3 Se"}, {"heading": "1 Introduction", "text": "Language models show the probability distribution over sequences of words or characters, and they are very important for many speech and document processing applications including speech recognition, text generation, and machine translation (Rabiner & Juang, 1993; Sutskever et al., 2011; Brown et al., 1990). A language model can be classified into character-, word-, and context-levels according to the unit of the input and output. In the character-level language model, the probability distribution of the next characters are generated based on the past character sequences. Since the number of alphabets is small in English, for example, the input and output of the character-level language model is quite simple. However, the word-level language model is usually needed because the character-level modeling is disadvantaged in utilizing the long period of past sequences. However, the problem of the word-level model is the complexity of the input and output because the vocabulary size to be supported can be bigger than 1 million in real applications.\nLanguage models have long been developed by analyzing a large amount of texts and storing the probability distribution of word sequences into the memory. The statistical language model demands a large memory space, often exceeding 1 GB, not only because the vocabulary size is large but also their combinations needs to be considered. In recent years, the language modeling based on recurrent neural networks (RNNs) are actively investigated (Mikolov et al., 2010). The RNN based language modeling demands much less parameters when compared to the statistical language model. But, the word-level language modeling still demands very complex input and output. Consider a one million word RNN, it needs one million signal line if the input is one-hot encoded. The amount of computation for generating the probability distribution of one million words is also prohibitive.\nSubmitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016.\nIn this work, we propose a hierarchical RNN based language model that combines the advantageous characteristics of both character- and word-level language models. The proposed network consists of a low-level and a high-level RNNs. The low-level RNN employs the character-level input and output, and provides the short-term embedding to the high-level RNN that operates as the word-level RNN. The high-level RNN do not need complex input and output because it receives the characterembedding information from the low-level network, and sends the word-prediction information back to the low-level in a compressed form. Thus, when considering the input and output, the proposed network is a character-level language model (CLM) (Sutskever et al., 2011), but it contains a wordlevel model inside. The low-level module operates with the character input clock, while the highlevel one runs with the space (<w>) that separates words. This hierarchical language model can be extended to be able to process a longer period of information, such as sentences, topics, or other contexts. The proposed hierarchical language model can be trained end-to-end with the character based texts, such as Wikipedia or Wall Street Journal (WSJ) corpus (Paul & Baker, 1992).\nThis paper is organized as follows. Section 2 describes the related work, and Section 3 explains the character-level language modeling using RNNs. RNN modeling including the external clock and reset signals is shown in Section 4, and the proposed language model using a hierarchical RNN is presented in Section 5. Section 6 gives the experimental results, and concluding remarks are given in Section 7."}, {"heading": "2 Related work", "text": "There has been many attempts to make LMs understand character-level inputs. One of the most successful approaches is to encode the arbitrary character sequence to fixed dimensional vector, which is called word embedding, and feed this vector to the word-level RNN LMs. Kim et al. (2015) used convolutional neural networks (CNNs) to generate word embeddings, and achieved the state of the art results on English Penn Treebank corpus (Marcus et al., 1993). The similar CNN-based embedding approach is used by Jozefowicz et al. (2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are employed instead of CNNs for word embedding. However, in all of these approaches, LMs still generate the output probabilities at the word-level. Although the character-level modeling approach of the output word probability is introduced using CNN softmax in Jozefowicz et al. (2016), the base LSTM still runs with a word-level clock.\nOur approach is different from the above ones in many ways. First, our base model is the characterlevel RNN LMs, instead of WLMs, and we extend this model to enhance the model to consider long-term contexts. Therefore, the output probabilities are generated with a character-level clocks. This property is extremely useful for character-level beam search for end-to-end speech recognition (Hwang & Sung, 2016). Also, the input and outputs of the our model is the same as the traditional character-level RNNs, thus the same training algorithm and recipe can be used without any modifications. Furthermore, the proposed models have significantly less number of parameters compared to WLM-based ones, since the size of our model does not directly depend on the vocabulary size of the training set. Note that a similar hierarchical concept has been used for character-level machine translation (Ling et al., 2015b). However, we propose more general hierarchical unidirectional RNN architecture that can be applied for various applications."}, {"heading": "3 Character-level language modeling with RNNs", "text": "Character-level language models (CLMs) need to consider longer sequence of history tokens to predict the next token than the word-level language models (WLMs), due to the smaller unit of tokens. Therefore, traditional N -gram models cannot be employed for CLMs. Thanks to the recent advances in RNNs, RNN-based CLMs has begun to show satisfactory performances (Sutskever et al., 2011; Hermans & Schrauwen, 2013). Especially, deep long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) based CLMs show excellent performance and successfully applied to end-to-end speech recognition system (Hwang & Sung, 2016).\nFor training RNN CLMs, training data should be first converted to the sequence of one-hot encoded character vectors, xt, where the characters include word boundary symbols, <w> or space, and optionally sentence boundary symbols, <s>. Then, as shown in Figure 1, the RNN is trained to predict\nthe next character xt+1 by minimizing the cross-entropy loss of the softmax output (Bridle, 1990) that represents the probability distributions of the next character."}, {"heading": "4 RNNs with external clock and reset signals", "text": "In this section, we generalize the existing RNN structures and extend them with external clocks and reset signals. The extended models become the basic building blocks of the hierarchical RNNs.\nMost types of RNNs can be generalized as\nst = f(xt, st\u22121) (1) yt = g(st) (2)\nwhere xt is the input, st is the state, yt is the output at time step t, f(\u00b7) is the recurrence function, and g(\u00b7) is the output function. For example, Elman networks can be represented as\nst = ht = \u03c3(Whxxt +Whhht\u22121 + bh) (3)\nyt = ht (4)\nwhere ht is the activation of the hidden layer, \u03c3(\u00b7) is the activation function, Whx and Whh are the weight matrices and bh is the bias vector.\nLSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) and peephole connections (Gers et al., 2003) can also be converted to the generalize form. The forward equations of the LSTM layer are as follows:\nit = \u03c3(Wixxt +Wihht\u22121 +Wimmt\u22121 + bi) (5)\nft = \u03c3(Wfxxt +Wfhht\u22121 +Wfmmt\u22121 + bf ) (6)\nmt = ft \u25e6mt\u22121 + it \u25e6 tanh(Wmxxt +Wmhht\u22121 + bm) (7)\not = \u03c3(Woxxt +Wohht\u22121 +Wommt + bo) (8) ht = ot \u25e6 tanh(mt) (9)\nwhere it, ft, and ot are the input, forget, and output gate values, respectively, mt is the memory cell activation, ht is the output activation, \u03c3(\u00b7) is the logistic sigmoid function, and \u25e6 is the element-wise multiplication operator. These equations can be generalized by setting st = [mt,ht] and yt = ht.\nAny generalized RNNs can be converted to the ones that incorporate an external clock signal, ct, as\nst = (1 \u2212 ct)st\u22121 + ctf(xt, st\u22121) (10) yt = g(st) (11)\nwhere ct is 0 or 1. The RNN updates its state and output only when ct = 1. Otherwise, when ct = 0, the state and output values remain the same as those of the previous step.\nThe reset of RNNs is performed by setting st\u22121 to 0. Specifically, (10) becomes\nst = (1\u2212 ct)(1 \u2212 rt)st\u22121 + ctf(xt, (1\u2212 rt)st\u22121) (12)\nwhere the reset signal rt = 0 or 1. When rt = 1, the RNN forgets the previous contexts.\nIf the original RNN equations are differentiable, the extended equations with clock and reset signals are also differentiable. Therefore, the existing gradient-based training algorithms for RNNs, such as backpropagation through time (BPTT), can be employed for training the extended versions without any modifications."}, {"heading": "5 Character-level LM with a hierarchical RNN", "text": "The proposed hierarchical RNN (HRNN) architectures have several RNN modules with different clock rates as depicted in Figure 2. The higher level module employs a slower clock rate than the lower module, and the lower level module is reset at every clock of the higher level module. Specifically, if there are L hierarchy levels, then the RNN consists of L submodules. Each submodule l operates with an external clock cl,t and a reset signal rl,t, where l = 1, \u00b7 \u00b7 \u00b7 , L. The lowest level module, l = 1, has the fastest clock rate, that is, c1,t = 1 for all t. On the other hand, the higher level modules, l > 1, have slower clock rates and cl,t can be 1 only when cl\u22121,t = 1. Also, the lower level modules l < L are reset by the higher level clock signals, that is, rl,t = cl+1,t.\nThe hidden activations of a module, l < L, are fed to the next higher level module, l + 1, delayed by one time step to avoid unwanted reset by rl,t = cl+1,t = 1. This hidden activation vector, or embedding vector, contains compressed short-term context information. The reset of the module by the higher level clock signals helps the module to concentrate on compressing only the short term information, rather than considering longer dependencies. The next higher level module, l+1, process this short-term information to generate the long-term context vector, which is fed back to the lower level module, l. There is no delay for this context propagation.\nFor character-level language modeling, we use two-level (L = 2) HRNN with letting l = 1 be a character-level module and l = 2 be a word-level module. The word-level module is clocked at the word boundary input, <w>, which is usually a whitespace character. The input and softmax output layer is connected to the character-level module, and the current word boundary token (e.g. <w> or <s>) information is given to the word-level module. Since this HRNNs have a scalable architecture, we can extend this HRNN CLM to model sentence-level contexts by adding a sentence-level module, l = 3. In this case, the sentence-level clock, c3,t becomes 1 when the input character is a sentence boundary token <s>. Also, the word-level module should be clocked at both the word boundary\ninput, <w>, and the sentence boundary input, <s>. Likewise, the model can be extended to include further higher level modules, such as paragraph-level modules or topic modeling modules. In this paper, the experiments are performed with the two-level HRNN CLMs.\nWe propose two types of two-level HRNN CLM architectures. As shown in Figure 3, both models have two LSTM layers per submodule. In the HLSTM-A architecture, both LSTM layers in the character-level module receives one-hot encoded character input. Therefore, the second layer of the character-level module is a generative model conditioned by the context vector. On the other hand, in HLSTM-B, the second LSTM layer of the character-level module does not have direct connection to the character inputs. Instead, a word embedding from the first LSTM layer is fed to the second LSTM layer, which makes the first and second layers of the character-level module work together to estimate the next character probabilities when the context vector is given. The experimental results show that HLSTM-B is more efficient for CLM applications.\nSince the character-level modules are reset by the word-boundary token (i.e. <w> or whitespace), the context vector from the word-level module is the only source for the inter-word context information. Therefore, the model is trained to generate the context vector that contains useful information about the probability distribution of the next word. From this perspective, the word-level module in both HRNN CLM architectures can be considered as a word-level RNN LM, where the input is a word embedding vector and the output is a compressed descriptor of the next word probabilities."}, {"heading": "6 Experiments", "text": "The proposed HRNN based CLMs are evaluated with two text datasets: the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) and One Billion Word Benchmark (Chelba et al., 2013). Also, we present an end-to-end speech recognition example, where HLSTM CLMs are employed for prefix tree-based beam search decoding and greatly improves recognition accuracies.\nThe RNNs are trained with truncated backpropagation through time (BPTT) (Werbos, 1990; Williams & Peng, 1990). Also, ADADELTA (Zeiler, 2012) and Nesterov momentum (Nesterov, 1983) is applied for weight update. No regularization method, such as dropout (Hinton et al., 2012), is employed. The training is accelerated using GPUs by training multiple sequences in parallel (Hwang & Sung, 2015)."}, {"heading": "6.1 Perplexity", "text": ""}, {"heading": "6.1.1 Wall Street Journal (WSJ) corpus", "text": "Dataset The Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) is designed for training and benchmarking automatic speech recognition systems. For the perplexity (PPL) experiments, we used the non-verbalized punctuation (NVP) version of the LM training data inside the corpus. The\ndataset consists of about 37 million words, where one percent of the total data is held out for the final evaluation and does not participate in training. All alphabets are converted to the upper case.\nExperimental results Table 1 shows the perplexities of traditional mono-clock deep LSTM and HLSTM based CLMs on the held-out set. Note that the size NxM means that the network consists of N LSTM layers, where each layer contains M memory cells. The HLSTM models show better perplexity performanes even when the number of LSTM cells or parameters is much smaller than that of the deep LSTM networks. Especially, HLSTM-B network with the size of 4x512 has about 9% lower perplexity than deep LSTM (4x1024) model, even with only 29% of parameters.\nImportance of reset It is important to reset the character-level modules at the word-level clocks for helping the character-level modules to better concentrate on the short-term information. As observed in Table 1, removing the reset functionality of the character-level module of the HLSTMB model results in degraded performance.\nComparison with WLMs The non-ensemble perplexities of WLMs in the literature are presented in Table 2. The Kneser-Ney (KN) smoothed 5-gram model (KN-5) (Kneser & Ney, 1995) is a strong non-neural WLM baseline. With the standard deep RNN based CLMs, it is very hard to beat KN-5 in terms of perplexity. However, it is surprising that all HLSTM models in Table 1 shows better perplexities than KN-5 does. The RNN based WLM model combined with the maximum entropy 4-gram feature (Mikolov & Zweig, 2012; Mikolov, 2012) shows much better results than the proposed CLM models. However, like most of the WLMs, it also needs a very large number (2 G) of parameters and cannot handle out-of-vocabulary (OOV) words."}, {"heading": "6.1.2 One Billion Word Benchmark", "text": "Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset contains about 0.8 billion words and roughly 800 thousand words of vocabulary. We followed the standard way of splitting the training and test data as in Chelba et al. (2013). Each byte of UTF-8 encoded text is regarded as a character. Therefore, the size of the character set is 256.\nExperimental results Due to the large amount of training data and weeks of training time, only two HLSTM-B experiments are conducted with the size of 4x512 and 4x1024. As shown in Table 3, there are large gap (22.5) in word-level perplexity between the two models. Therefore, further improvement in perplexity can be expected with bigger networks.\nComparison with WLMs The perplexities of other WLMs are summarized in Table 4 The proposed HLSTM-B model (4x1024) shows better perplexities than the interpolated KN-5 model with 1.1 billion n-grams (Chelba et al., 2013) even though the number of parameters of our model is only 2% of that of the KN-5 model. However, much lower perplexities are reported with sparse non-negative matrix LM and the maximum entropy feature based RNN model (Chelba et al., 2013), where the number of parameters are 33 G and 20 G, respectively. Recently, Jozefowicz et al. (2016)\nreported the state of the art perplexity of 30.0 with a single model that has 1 G parameters. The model is basically a very large LSTM LM. However, the convolutional neural network (CNN) is used to generate word embedding of arbitrary character sequences as the input of the LSTM LM. Therefore, this model can handle OOV word inputs, however, still the model runs with a word-level clock."}, {"heading": "6.2 End-to-end automatic speech recognition (ASR)", "text": "In this section, we apply the proposed CLMs to the end-to-end automatic speech recognition (ASR) system to evaluate the models in more practical situation than just measuring perplexities. The CLMs are trained with WSJ LM training data as in Section 6.1.1. Unlike WLMs, the proposed CLMs have very small number of parameters, so they can be employed for real-time character-level beam search. All the experiments in this section run in real-time with NVIDIA GeForce GTX Titan X GPU.\nThe incremental speech recognition system proposed in Hwang & Sung (2016) is used for the evaluation. The acoustic model is 4x512 unidirectional LSTM and end-to-end trained with connectionist temporal classification (CTC) loss (Graves et al., 2006) using the non-verbalized punctuation (NVP) portion of WSJ SI-284 training set. The acoustic features are 40-dimensional log-mel filterbank coefficients, energy and their delta and double-delta values, which are extracted every 10 ms with 25 ms Hamming window. The beam-search decoding is performed on a prefix-tree with depth-pruning and width-pruning (Hwang & Sung, 2016). The insertion bonus is 1.6, the LM weight is 2.0, and the beam width is 512.\nThe results are summarized in Table 5. It is observed that the perplexity of LM and the word error rate (WER) have strong correlation. As shown in the table, we can achieve a better WER by replacing the traditional deep LSTM (4x1024) CLM with the proposed HLSTM-B (4x512) CLM, while reducing the number of LM parameters to 30%."}, {"heading": "7 Concluding remarks", "text": "In this paper, hierarchical RNN (HRNN) based character-level LMs are proposed. The HRNN consists of several submodules with different clock rates. Therefore, it is capable of learning long-term dependencies as well as short-term details. We presented two HRNN structures, HLSTM-A and HLSTM-B, for character-level language modeling. The experimental results on One Billion Benchmark show that HLSTM-B networks significantly outperform Kneser-Ney 5-gram LMs with only 2% of parameters. Although other RNN-based word-level LMs show better performance than our models, they have impractically many parameters. On the other hand, as shown in the WSJ speech recognition example, the proposed model can be employed for the real-time speech recognition with less than 10 million parameters. Also, character LMs can handle OOV words by nature, which is a great advantage for the end-to-end speech recognition and many NLP tasks. One of the interesting future work is training the clock signals, instead of manual design. Also, it will be interesting to see how this hierarchical architecture works when the level of hierarchy increases."}], "references": [{"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "A statistical approach to machine translation", "author": ["Brown", "Peter F", "Cocke", "John", "Pietra", "Stephen A Della", "Vincent J Della", "Jelinek", "Fredrick", "Lafferty", "John D", "Mercer", "Robert L", "Roossin", "Paul S"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp", "Robinson", "Tony"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Michiel", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "arXiv preprint arXiv:1601.06581,", "citeRegEx": "Hwang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2016}, {"title": "BlackOut: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Ji", "Shihao", "SVN Vishwanathan", "Satish", "Nadathur", "Anderson", "Michael J", "Dubey", "Pradeep"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u00eds", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Ling", "Wang", "Trancoso", "Isabel", "Dyer", "Chris", "Black", "Alan W"], "venue": "arXiv preprint arXiv:1511.04586,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical language models based on neural networks. Presentation at Google", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "Mountain View,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Paul", "Douglas B", "Baker", "Janet M"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Paul et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Paul et al\\.", "year": 1992}, {"title": "Fundamentals of speech recognition", "author": ["Rabiner", "Lawrence", "Juang", "Biing-Hwang"], "venue": null, "citeRegEx": "Rabiner et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Rabiner et al\\.", "year": 1993}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["Shazeer", "Noam", "Pelemans", "Joris", "Chelba", "Ciprian"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Shazeer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Werbos", "Paul J"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos and J.,? \\Q1990\\E", "shortCiteRegEx": "Werbos and J.", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Ronald J", "Peng", "Jing"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Language models show the probability distribution over sequences of words or characters, and they are very important for many speech and document processing applications including speech recognition, text generation, and machine translation (Rabiner & Juang, 1993; Sutskever et al., 2011; Brown et al., 1990).", "startOffset": 241, "endOffset": 308}, {"referenceID": 1, "context": "Language models show the probability distribution over sequences of words or characters, and they are very important for many speech and document processing applications including speech recognition, text generation, and machine translation (Rabiner & Juang, 1993; Sutskever et al., 2011; Brown et al., 1990).", "startOffset": 241, "endOffset": 308}, {"referenceID": 20, "context": "In recent years, the language modeling based on recurrent neural networks (RNNs) are actively investigated (Mikolov et al., 2010).", "startOffset": 107, "endOffset": 129}, {"referenceID": 25, "context": "Thus, when considering the input and output, the proposed network is a character-level language model (CLM) (Sutskever et al., 2011), but it contains a wordlevel model inside.", "startOffset": 108, "endOffset": 132}, {"referenceID": 17, "context": "(2015) used convolutional neural networks (CNNs) to generate word embeddings, and achieved the state of the art results on English Penn Treebank corpus (Marcus et al., 1993).", "startOffset": 152, "endOffset": 173}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity.", "startOffset": 60, "endOffset": 81}, {"referenceID": 11, "context": "Kim et al. (2015) used convolutional neural networks (CNNs) to generate word embeddings, and achieved the state of the art results on English Penn Treebank corpus (Marcus et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "The similar CNN-based embedding approach is used by Jozefowicz et al. (2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are employed instead of CNNs for word embedding.", "startOffset": 61, "endOffset": 154}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are employed instead of CNNs for word embedding. However, in all of these approaches, LMs still generate the output probabilities at the word-level. Although the character-level modeling approach of the output word probability is introduced using CNN softmax in Jozefowicz et al. (2016), the base LSTM still runs with a word-level clock.", "startOffset": 61, "endOffset": 462}, {"referenceID": 25, "context": "Thanks to the recent advances in RNNs, RNN-based CLMs has begun to show satisfactory performances (Sutskever et al., 2011; Hermans & Schrauwen, 2013).", "startOffset": 98, "endOffset": 149}, {"referenceID": 3, "context": "LSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) and peephole connections (Gers et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": ", 2000) and peephole connections (Gers et al., 2003) can also be converted to the generalize form.", "startOffset": 33, "endOffset": 52}, {"referenceID": 2, "context": "The proposed HRNN based CLMs are evaluated with two text datasets: the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) and One Billion Word Benchmark (Chelba et al., 2013).", "startOffset": 156, "endOffset": 177}, {"referenceID": 7, "context": "No regularization method, such as dropout (Hinton et al., 2012), is employed.", "startOffset": 42, "endOffset": 63}, {"referenceID": 11, "context": "input # Params PPL Sigmoid RNN-2048 (Ji et al., 2015) No 4.", "startOffset": 36, "endOffset": 53}, {"referenceID": 2, "context": "1B n-grams (Chelba et al., 2013) No 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 24, "context": "6 Sparse non-negative matrix LM (Shazeer et al., 2015) No 33 G 52.", "startOffset": 32, "endOffset": 54}, {"referenceID": 2, "context": "9 RNN-1024 + ME 9-gram feature (Chelba et al., 2013) No 20 G 51.", "startOffset": 31, "endOffset": 52}, {"referenceID": 12, "context": "3 CNN input + 2xLSTM-8192-2048 (Jozefowicz et al., 2016) Yes 1.", "startOffset": 31, "endOffset": 56}, {"referenceID": 2, "context": "2 One Billion Word Benchmark Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset contains about 0.", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "2 One Billion Word Benchmark Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset contains about 0.8 billion words and roughly 800 thousand words of vocabulary. We followed the standard way of splitting the training and test data as in Chelba et al. (2013). Each byte of UTF-8 encoded text is regarded as a character.", "startOffset": 69, "endOffset": 273}, {"referenceID": 2, "context": "1 billion n-grams (Chelba et al., 2013) even though the number of parameters of our model is only 2% of that of the KN-5 model.", "startOffset": 18, "endOffset": 39}, {"referenceID": 2, "context": "However, much lower perplexities are reported with sparse non-negative matrix LM and the maximum entropy feature based RNN model (Chelba et al., 2013), where the number of parameters are 33 G and 20 G, respectively.", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": "1 billion n-grams (Chelba et al., 2013) even though the number of parameters of our model is only 2% of that of the KN-5 model. However, much lower perplexities are reported with sparse non-negative matrix LM and the maximum entropy feature based RNN model (Chelba et al., 2013), where the number of parameters are 33 G and 20 G, respectively. Recently, Jozefowicz et al. (2016)", "startOffset": 19, "endOffset": 379}, {"referenceID": 5, "context": "The acoustic model is 4x512 unidirectional LSTM and end-to-end trained with connectionist temporal classification (CTC) loss (Graves et al., 2006) using the non-verbalized punctuation (NVP) portion of WSJ SI-284 training set.", "startOffset": 125, "endOffset": 146}], "year": 2016, "abstractText": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multiclock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than KneserNey (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "creator": "LaTeX with hyperref package"}}}