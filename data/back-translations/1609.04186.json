{"id": "1609.04186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Neural Machine Translation with Supervised Attention", "abstract": "The attention mechanism is attractive to neural machine translation because it is able to dynamically encode a source sentence by creating an alignment between target word and source word. Unfortunately, it has been shown to be inferior in terms of alignment accuracy to conventional alignment models. In this paper, we analyze and explain this problem from the perspective of reordering and propose a supervised attention that is learned with the help of conventional alignment models. Experiments on two translation tasks from Chinese to English show that the supervised attention mechanism produces better alignments that lead to significant increases over the standard attention-based NMT.", "histories": [["v1", "Wed, 14 Sep 2016 09:31:40 GMT  (261kb,D)", "http://arxiv.org/abs/1609.04186v1", "This paper was submitted into COLING2016 on July 10, and it is under review"]], "COMMENTS": "This paper was submitted into COLING2016 on July 10, and it is under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lemao liu", "masao utiyama", "rew finch", "eiichiro sumita"], "accepted": false, "id": "1609.04186"}, "pdf": {"name": "1609.04186.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Supervised Attention", "authors": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "emails": ["lmliu@nict.go.jp", "first.last@nict.go.jp"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper, is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015). One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance.\nAn attention mechanism is designed to predict the alignment of a target word with respect to source words. In order to facilitate incremental decoding, it tries to make this alignment prediction without any information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see \u00a72 for more details). However, it differs from conventional alignment models that are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-toEnglish as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.1 In contrast, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models.\nInspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run off-theshelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the conventional aligners delivers higher quality alignment, it is expected that the alignment in the supervised attention NMT will be improved leading to better end-to-end translation performance. One advantage of the proposed SA-NMT is that it implements the supervision of attention as a regularization in the joint\n1 We do agree that NMT is a supervised model with respect to translation rather than reordering.\nar X\niv :1\n60 9.\n04 18\n6v 1\n[ cs\n.C L\n] 1\n4 Se\np 20\n16\ntraining objective (\u00a73.2). Furthermore, since the supervision of attention lies in the middle of the entire network architecture rather than the top ( as in the supervision of translation (see Figure 1(b)), it serves to mitigate the vanishing gradient problem during the back-propagation (Szegedy et al., 2015).\nThis paper makes the following contributions:\n\u2022 It revisits the attention model from the point view of reordering (\u00a72), and propose a supervised attention for NMT that is supervised by statistical alignment models (\u00a73). The proposed approach is simple and easy to be implemented, and it is generally applicable to any attention-based NMT models, although in this case it is implemented on top of the model in (Bahdanau et al., 2015).\n\u2022 On two Chinese-to-English translation tasks, it empirically shows that the proposed approach gives rise to improved performance (\u00a74): on a large scale task, it outperforms three baselines including a state-of-the-art Moses, and leads to improvements of up to 2.5 BLEU points over the strongest baseline; on a low resource task, it even obtains about 5 BLEU points over the attention based NMT system on which is it based.\n2 Revisiting Neural Machine Translation\nhtht 1\n\u21b5t ct\nyt 1 yt\nhtht 1\nct\nyt 1 yt\n\u21b5t\nEx\nEx\nhtht 1\n\u21b5t ct\nyt 1 yt\nhtht 1\nct\nyt 1 yt\n\u21b5t\nEx\nEx\n(a) NMT (b) SA-NMT\nFigure 1: The computational graphs of both (a) NMT and (b) SA-NMT at timestep t. Circles denote the hidden variables; while squares denote the observable variables, which receive supervision during training. The difference (marked in red) in (b) regarding to (a) is treating \u03b1t as an observable variable instead of a hidden variable.\nSuppose x = \u3008x1, x2, \u00b7 \u00b7 \u00b7 , xm\u3009 denotes a source sentence, y = \u3008y1, y2, \u00b7 \u00b7 \u00b7 , yn\u3009 a target sentence. In addition, let x<t = \u3008x1, x2, \u00b7 \u00b7 \u00b7 , xt\u22121\u3009 denote a prefix of x. Neural Machine Translation (NMT) directly maps a source sentence into a target under an encode-decode framework. In the encoding stage, it uses two bidirectional recurrent neural networks to encode x into a sequence of vectors Ex = \u3008Ex1 , Ex2 , \u00b7 \u00b7 \u00b7 , Exm\u3009, with Exi representing the concatenation of two vectors for ith source word from two directional RNNs. In the decoding stage, it generates the target translation from the conditional probability over the pair of sequences x and y via a recurrent neural network parametrized by \u03b8 as follows:\np(y | x; \u03b8) = n\u220f\nt=1\np(yt | y<t, Ex) = n\u220f\nt=1\nsoftmax ( g(yt\u22121, ht, ct) ) [yt] (1)\nwhere ht and ct respectively denote an RNN hidden state (i.e. a vector) and a context vector at timestep t; g is a transformation function mapping into a vector with dimension of the target vocabulary size; and [i] denotes the ith component of a vector.2 Furthermore, ht = f(ht\u22121, yt\u22121, ct) is defined by an activation function, i.e. a Gated Recurrent Unit (Chung et al., 2014); and the context vector ct is a dynamical source representation at timestep t, and calculated as the weighted sum of source encodingsEx, i.e. ct = \u03b1>t Ex.\n2In that sense, yt in Eq.(1) also denotes the index of this word in its vocabulary.\nHere the weight \u03b1t implements an attention mechanism, and \u03b1t,i is the alignment probability of yt being aligned to xi. \u03b1t is derived through a feedforward neural network a as follows:\n\u03b1t = a(yt\u22121, ht\u22121, Ex) (2)\nwhere a consists of two layers, the top one being a softmax layer. We skip the detailed definitions of a together with Ex, f and g, and refer the readers to (Bahdanau et al., 2015) instead.3 Figure 1(a) shows one slice of computational graph for NMT definition at time step t.\nTo train NMT, the following negative log-likelyhood is minimized: \u2212 \u2211 i log p(yi | xi; \u03b8) (3)\nwhere \u2329 xi,yi \u232a is a bilingual sentence pair from a given training corpus, p(yi | xi; \u03b8) is as defined in Eq.(1). Note that even though the training is conducted in a supervised manner with respect to translation, i.e., y are observable in Figure 1(a), the attention is learned in a unsupervised manner, since \u03b1 is hidden.\nIn Figure 1(a), \u03b1t can not be dependent on yt, as the target word yt is unknown at the timestep t \u2212 1 during the testing. Therefore, at timestep t \u2212 1, NMT firstly tries to calculate \u03b1t, through which NMT figures out those source words will be translated next, even though the next target word yt is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment \u03b1 directly over x and y as follows:\np(\u03b1 | x,y) = exp(F (x,y, \u03b1))\u2211 a exp(F (x,y, \u03b1))\nwhere F denotes either a log-probability log p(y, \u03b1 | x) for a generative model like IBM models (Brown et al., 1993) or a feature function for discriminative models (Liu and Sun, 2015). In order to infer \u03b1t, alignment models can readily use the entire y, of course including yt as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of NMT is limited in end-to-end translation."}, {"heading": "3 Supervised Attention", "text": "In this section, we introduce supervised attention to improve the alignment, which consequently leads to better translation performance for NMT. Our basic idea is simple: similar to conventional SMT, it firstly uses a conventional aligner to obtain the alignment on the training corpus; then it employs these alignment results as supervision to train the NMT. During testing, decoding proceeds in exactly the same manner as standard NMT, since there is no alignment supervision available for unseen test sentences."}, {"heading": "3.1 Preprocessing Alignment Supervision", "text": "As described in \u00a72, the attention model outputs a soft alignment \u03b1, such that \u03b1t is a normalized probability distribution. In contrast, most aligners are typically oriented to grammar induction for conventional SMT, and they usually output \u2018hard\u2019 alignments, such as (Och and Ney, 2000). They only indicate whether a target word is aligned to a source word or not, and this might not correspond to a distribution for each target word. For example, one target word may align to multiple source words, or no source words at all.\nTherefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens \u2018eol\u2019 added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as \u03b1\u0302.\n3In the original paper, \u03b1t is independent on the yt\u22121 in Eq.(2), \u03b1t is independent on the yt\u22121 in Eq.(2), but this dependency was retained in our direct baseline NMT2."}, {"heading": "3.2 Jointly Supervising Translation and Attention", "text": "We propose a soft constraint method to jointly supervise the translation and attention as follows:\n\u2212 \u2211 i log p(yi | xi; \u03b8) + \u03bb\u00d7\u2206(\u03b1i, \u03b1\u0302i; \u03b8) (4)\nwhere \u03b1i is as defined in Eq. (1), \u2206 is a loss function that penalizes the disagreement between \u03b1i and \u03b1\u0302i, and \u03bb > 0 is a hyper-parameter that balances the preference between likelihood and disagreement. In this way, we treat the attention variable \u03b1 as an observable variable as shown in Figure 1(b), and this is different from the standard NMT as shown in Figure 1(a) in essence. Note that this training objective resembles to that in multi-task learning (Evgeniou and Pontil, 2004). Our supervised attention method has two further advantages: firstly, it is able to alleviate overfitting by means of the \u03bb; and secondly it is capable of addressing the vanishing gradient problem because the supervision of \u03b1 is more close to Ex than y as in Figure 1(b).\nIn order to quantify the disagreement between \u03b1i and \u03b1\u0302i, three different methods are investigated in our experiments:\n\u2022 Mean Squared Error (MSE)\n\u2206(\u03b1i, \u03b1\u0302i; \u03b8) = \u2211 m \u2211 n 1 2 ( \u03b1(\u03b8)im,n \u2212 \u03b1\u0302im,n )2 MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly encourages \u03b1(\u03b8)im,n to be equal to \u03b1\u0302 i m,n.\n\u2022 Multiplication (MUL)\n\u2206(\u03b1i, \u03b1\u0302i; \u03b8) = \u2212 log (\u2211\nm \u2211 n \u03b1(\u03b8)im,n \u00d7 \u03b1\u0302im,n )\nMUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016). Note that different from those in (Cheng et al., 2016), \u03b1\u0302 is not a parametrized variable but a constant in this paper.\n\u2022 Cross Entropy (CE) \u2206(\u03b1i, \u03b1\u0302i; \u03b8) = \u2212 \u2211 m \u2211 n \u03b1\u0302im,n \u00d7 log\u03b1(\u03b8)im,n\nSince for each t, \u03b1(\u03b8)t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004)."}, {"heading": "4 Experiments", "text": "We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel domain, which is a low resource task and thus is very challenging for NMT. We used the case-insensitive BLEU4 to evaluate translation quality and adopted the multi-bleu.perl as its implementation."}, {"heading": "4.1 The Large Scale Translation Task", "text": ""}, {"heading": "4.1.1 Preparation", "text": "We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences).\nWe compared the proposed approach with three strong baselines: \u2022 Moses: a phrase-based machine translation system (Koehn et al., 2007);\n\u2022 NMT1: an attention based NMT (Bahdanau et al., 2015) system at https://github.com/lisagroundhog/GroundHog; \u2022 NMT2: another implementation of (Bahdanau et al., 2015) at https://github.com/nyu-dl/dl4mt-\ntutorial. We developed the proposed approach based on NMT2, and denoted it as SA-NMT.\nWe followed the standard pipeline to run Moses. GIZA++ with grow-diag-final-and was used to build the translation model. We trained a 5-gram target language model on the Gigaword corpus, and used a lexicalized distortion model. All experiments were run with the default settings.\nTo train NMT1, NMT2 and SA-NMT, we employed the same settings for fair comparison. Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 4 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012). Particularly for SA-NMT, we employed a conventional word aligner to obtain the word alignment on the training data before training SA-NMT. In this paper, we used two different aligners, which are fast align and GIZA++. We tuned the hyper-parameter \u03bb to be 0.3 on the development set, to balance the preference between the translation and alignment. Training was conducted on a single Tesla K40 GPU machine. Each update took about 3.0 seconds for both NMT2 and SA-NMT, and 2.4 seconds for NMT1. Roughly, it took about 10 days to NMT2 to finish 300000 updates."}, {"heading": "4.1.2 Settings on External Alignments", "text": "We implemented three different losses to supervise the attention as described in \u00a73.2. To explore their behaviors on the development set, we employed the GIZA++ to generate the alignment on the training set prior to the training SA-NMT. In Table 1, we can see that MUL is better than MSE. Furthermore, CE performs best among all losses, and thus we adopt it for the following experiments.\nIn addition, we also run fast align to generate alignments as the supervision for SA-NMT and the results were reported in Table 2. We can see that GIZA++ performs slightly better than fast align and thus we fix the external aligner as GIZA++ in the following experiments."}, {"heading": "4.1.3 Results on Large Scale Translation Task", "text": "Figure 2 shows the learning curves of NMT2 and SA-NMT on the development set. We can see that NMT2 generally obtains higher BLEU as the increasing of updates before peaking at update of 150000, while it is unstable from then on. On the other hand, SA-NMT delivers much better BLEU for the beginning updates and performs more steadily along with the updates, although it takes more updates to reach the peaking point.\n4This excludes all the sentences longer than 50 words in either source or target side only for NMT systems, but for Moses we use the entire training data.\nTable 3 reports the main end-to-end translation results for the large scale task. We find that both standard NMT generally outperforms Moses except NMT1 on nist05. The proposed SA-NMT achieves significant and consistent improvements over all three baseline systems, and it obtains the averaged gains of 2.2 BLEU points on test sets over its direct baseline NMT2. It is clear from these results that our supervised attention mechanism is highly effective in practice."}, {"heading": "4.1.4 Results and Analysis on Alignment", "text": "As explained in \u00a72, standard NMT can not use the target word information to predict its aligned source words, and thus might fail to predict the correct source words for some target words. For example, for the sentence in the training set in Figure 3 (a), NMT2 aligned \u2018following\u2019 to \u2018\u76ae\u8bfa\u5951\u7279 (gloss: pinochet)\u2019 rather than \u2018\u7ee7 (gloss: follow)\u2019, and worse still it aligned the word \u2018.\u2019 to \u2018\u5728 (gloss: in)\u2019 rather than \u2018\u3002\u2019 even though this word is relatively easy to align correctly. In contrast, with the help of information from the target word itself, GIZA++ successfully aligned both \u2018following\u2019 and \u2018.\u2019 to the expected source words (see Figure3(c)). With the alignment results from GIZA++ as supervision, we can see that our SA-NMT can imitate GIZA++ and thus align both words correctly. More importantly, for sentences in the unseen test set, like GIZA++, SA-NMT confidently aligned \u2018but\u2019 and \u2018.\u2019 to their correct source words respectively as in Figure3(b), where NMT2 failed. It seems that SA-NMT can learn its alignment behavior from GIZA++, and subsequently apply the alignment abilities it has learned to unseen test sentences.\nTable 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following (Luong and Manning, 2015), we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised counterpart (NMT2). This shows that the proposed approach is able to realize our\nintuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge \u03bb in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014)."}, {"heading": "4.2 Results on the Low Resource Translation Task", "text": "For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively. We trained a 4-gram language model on the target side of training corpus for running Moses. For training all NMT systems, we employed the same settings as those in the large scale task, except that vocabulary size is 6000, batch size is 16, and the hyper-parameter \u03bb = 1 for SA-NMT.\nTable 5 reports the final results. Firstly, we can see that both standard neural machine translation systems NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains\nmuch over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially.\nWhile our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work."}, {"heading": "5 Related Work", "text": "Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs.\nIt has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model. At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as (Zhang et al., 2016). Our work is inspired by these works in spirit, and it can be considered to be a recurrent neural network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs."}, {"heading": "6 Conclusion", "text": "It has been shown that attention mechanism in NMT is worse than conventional word alignment models in its alignment accuracy. This paper firstly provides an explanation for this by viewing the atten- tion mechanism from the point view of reordering. Then it proposes a supervised attention for NMT with guidance from external conventional alignment models, inspired by the supervised reordering models in conventional SMT. Experiments on two Chinese-to-English translation tasks show that the proposed approach achieves better alignment results leading to significant gains relative to standard attention based NMT."}, {"heading": "Acknowledgements", "text": "We would like to thank Xugang Lu for invaluable discussions on this work."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation. CoRR, abs/1606.02006", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": null, "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A survey of word reordering in statistical machine translation: Computational models and language phenomena", "author": ["Bisazza", "Federico2016] Arianna Bisazza", "Marcello Federico"], "venue": "Computational Linguistics,", "citeRegEx": "Bisazza et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bisazza et al\\.", "year": 2016}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "Proceedings of IJCAI", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "Proceedings of NAACL-HLT", "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "Proceedings of ACL", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Pontil2004] Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder NMT model. CoRR, abs/1601.03317", "author": ["Feng et al.2016] Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of ACL: Demonstrations", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Theory of Point Estimation", "author": ["Lehmann", "Casella1998] E.L. Lehmann", "G. Casella"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1998}, {"title": "A neural reordering model for phrase-based translation", "author": ["Li et al.2014] Peng Li", "Yang Liu", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang"], "venue": "In Proceedings of COLING", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Liu", "Sun2015] Yang Liu", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D. Manning"], "venue": "In Proceedings of IWSLT", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Improved statistical alignment models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of ACL,", "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "The Cross Entropy Method: A Unified Approach To Combinatorial Optimization, Monte-carlo Simulation (Information Science and Statistics)", "author": ["Rubinstein", "Kroese2004] Reuven Y. Rubinstein", "Dirk P. Kroese"], "venue": null, "citeRegEx": "Rubinstein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Recurrent neural networks for word alignment model", "author": ["Taro Watanabe", "Eiichiro Sumita"], "venue": "In Proceedings of ACL", "citeRegEx": "Tamura et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of ACL", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Xiong et al.2006] Deyi Xiong", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of ACL", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Word alignment modeling with context dependent deep neural network", "author": ["Yang et al.2013] Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method. CoRR", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Learning local word reorderings for hierarchical phrase-based statistical machine translation", "author": ["Zhang et al.2016] Jingyi Zhang", "Masao Utiyama", "Eiichiro Sumita", "Hai Zhao", "Graham Neubig", "Satoshi Nakamura"], "venue": "Machine Translation", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 115, "endOffset": 162}, {"referenceID": 21, "context": "1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 115, "endOffset": 162}, {"referenceID": 1, "context": "Among different variants of NMT, attention based NMT, which is the focus of this paper, is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 140, "endOffset": 183}, {"referenceID": 16, "context": "Among different variants of NMT, attention based NMT, which is the focus of this paper, is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 140, "endOffset": 183}, {"referenceID": 8, "context": "However, it differs from conventional alignment models that are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-toEnglish as reported in (Cheng et al.", "startOffset": 116, "endOffset": 173}, {"referenceID": 4, "context": ", 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-toEnglish as reported in (Cheng et al., 2016)).", "startOffset": 238, "endOffset": 258}, {"referenceID": 8, "context": "Specifically, similar to conventional SMT, we first run off-theshelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.", "startOffset": 120, "endOffset": 139}, {"referenceID": 22, "context": "Furthermore, since the supervision of attention lies in the middle of the entire network architecture rather than the top ( as in the supervision of translation (see Figure 1(b)), it serves to mitigate the vanishing gradient problem during the back-propagation (Szegedy et al., 2015).", "startOffset": 261, "endOffset": 283}, {"referenceID": 1, "context": "The proposed approach is simple and easy to be implemented, and it is generally applicable to any attention-based NMT models, although in this case it is implemented on top of the model in (Bahdanau et al., 2015).", "startOffset": 189, "endOffset": 212}, {"referenceID": 5, "context": "a Gated Recurrent Unit (Chung et al., 2014); and the context vector ct is a dynamical source representation at timestep t, and calculated as the weighted sum of source encodingsEx, i.", "startOffset": 23, "endOffset": 43}, {"referenceID": 1, "context": "We skip the detailed definitions of a together with Ex, f and g, and refer the readers to (Bahdanau et al., 2015) instead.", "startOffset": 90, "endOffset": 113}, {"referenceID": 3, "context": "p(\u03b1 | x,y) = exp(F (x,y, \u03b1)) \u2211 a exp(F (x,y, \u03b1)) where F denotes either a log-probability log p(y, \u03b1 | x) for a generative model like IBM models (Brown et al., 1993) or a feature function for discriminative models (Liu and Sun, 2015).", "startOffset": 145, "endOffset": 165}, {"referenceID": 4, "context": "As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models.", "startOffset": 93, "endOffset": 113}, {"referenceID": 7, "context": "Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly.", "startOffset": 235, "endOffset": 256}, {"referenceID": 14, "context": "MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 4, "context": "MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 4, "context": "Note that different from those in (Cheng et al., 2016), \u03b1\u0302 is not a parametrized variable but a constant in this paper.", "startOffset": 34, "endOffset": 54}, {"referenceID": 11, "context": "We compared the proposed approach with three strong baselines: \u2022 Moses: a phrase-based machine translation system (Koehn et al., 2007);", "startOffset": 114, "endOffset": 134}, {"referenceID": 1, "context": "\u2022 NMT1: an attention based NMT (Bahdanau et al., 2015) system at https://github.", "startOffset": 31, "endOffset": 54}, {"referenceID": 1, "context": "com/lisagroundhog/GroundHog; \u2022 NMT2: another implementation of (Bahdanau et al., 2015) at https://github.", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": "Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 4 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012).", "startOffset": 127, "endOffset": 150}, {"referenceID": 27, "context": ", 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 4 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012).", "startOffset": 349, "endOffset": 363}, {"referenceID": 26, "context": "(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014).", "startOffset": 165, "endOffset": 205}, {"referenceID": 23, "context": "(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014).", "startOffset": 165, "endOffset": 205}, {"referenceID": 18, "context": "Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.", "startOffset": 65, "endOffset": 105}, {"referenceID": 6, "context": "Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.", "startOffset": 65, "endOffset": 105}, {"referenceID": 0, "context": "While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size.", "startOffset": 186, "endOffset": 207}, {"referenceID": 0, "context": "While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size.", "startOffset": 186, "endOffset": 355}, {"referenceID": 28, "context": "At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as (Zhang et al., 2016).", "startOffset": 204, "endOffset": 224}, {"referenceID": 19, "context": "Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment.", "startOffset": 0, "endOffset": 119}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al.", "startOffset": 0, "endOffset": 621}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al.", "startOffset": 0, "endOffset": 698}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model.", "startOffset": 0, "endOffset": 785}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model. At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as (Zhang et al.", "startOffset": 0, "endOffset": 898}], "year": 2016, "abstractText": "The attention mechanisim is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.", "creator": "LaTeX with hyperref package"}}}