{"id": "1609.05294", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2016", "title": "Sparse Boltzmann Machines with Structure Learning as Applied to Text Analysis", "abstract": "We examine the possibilities and advantages of structural learning for deep models. In a first step, we examine the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unattended text analysis. We present a method for learning the so-called Sparse Boltzmann Machines, in which each hidden unit is associated with a subset of visible units, rather than with all. Empirical results show that the method produces models with significantly improved model fit and interpretability compared to RBMs, in which each hidden unit is associated with all visible units.", "histories": [["v1", "Sat, 17 Sep 2016 08:17:36 GMT  (175kb,D)", "http://arxiv.org/abs/1609.05294v1", null], ["v2", "Tue, 20 Sep 2016 11:51:20 GMT  (175kb,D)", "http://arxiv.org/abs/1609.05294v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhourong chen", "nevin l zhang", "dit-yan yeung", "peixian chen"], "accepted": true, "id": "1609.05294"}, "pdf": {"name": "1609.05294.pdf", "metadata": {"source": "CRF", "title": "Sparse Boltzmann Machines with Structure Learning as Applied to Text Analysis", "authors": ["Zhourong Chen", "Nevin L. Zhang", "Dit-Yan Yeung", "Peixian Chen"], "emails": [], "sections": [{"heading": "Introduction", "text": "Deep learning has achieved great successes in recent years. It has produced superior results in a range of applications, including image classification (Krizhevsky, Sutskever, and Hinton, 2012), speech recognition (Hinton et al., 2012; Mikolov et al., 2011), language translation (Sutskever, Vinyals, and Le, 2014) and so on. It is now time to ask whether it is possible and beneficial to learn structures for deep models.\nTo learn the structure of a deep model, we need to determine the number of hidden layers and the number of hidden units at each layer. More importantly, we need to determine the connections between neighboring layers. This implies that we need to talk about sparse models where neighboring layers are not fully connected.\nSparseness is desirable and full connectivity is unnecessary. In fact, Han et al. (2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al., 1998) can be pruned without incurring any accuracy loss. The convolutional layers of CNNs are sparse, and the fact is considered one of the key factors that lead to the success of CNNs. Moreover, it is well known that overfitting is a serious problem in deep models. One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training. The possibility of randomly dropping connections has also been explored in Wan et al. (2013). Sparseness offers an interesting alternative. It amounts to deterministically drop out connections.\nHow can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak con-\nnections (Han et al., 2015). The drawbacks of this method are that it is computationally wasteful and doesn\u2019t provide a way to determine the number of hidden units. We would like to develop a method that determines the number of hidden units and the connections between units automatically. The key intuition is that a hidden unit should be connected to a group of strongly correlated units at the level below. This idea is used in convolutional layers of CNNs, where a unit is connected to pixels in a small patch of an image. In image analysis, spatial proximity implies strong correlation.\nTo apply the intuition to applications other than image analysis, we need to identify groups of strongly correlated variables for which latent variables should be introduced. Hierarchical Latent Tree Analysis (HLTA) (Liu et al 2014, Chen et al 2016) offers a plausible solution. HLTA first partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modelled using a single latent variable. It introduces a latent variable for each group. Then it converts the latent variables into observed variables via data completion and repeats the process to produce a hierarchy. The output of HLTA is a hierarchical latent tree model where the observed variables are at the bottom and there are multiple layers of latent variables on top. To obtain a non-tree sparse deep model, we propose to use the tree model as a skeleton and introduce additional connections to model the residual correlations not captured by the tree.\nIn this paper, we fully develop and test the idea in the context of RBMs, which have a single layer of hidden units and are building blocks of Deep Belief Networks and Deep Boltzmann Machines. The target domain is unsupervised text analysis. We present an algorithm for learning what we call Sparse Boltzmann Machines. Empirically, we show that the full-connectivity restriction of RBMs can easily lead to overfitting, and that Sparse Boltzmann Machines are effective in avoiding overfitting. We also demonstrate that Sparse Boltzmann Machines are more interpretable than RBMs."}, {"heading": "Related Works", "text": "The concept of sparse RBMs were first mentioned in (Lee, Ekanadham, and Ng, 2008). The authors use sparse RBMs to build sparse Deep Belief Networks and extract some interesting features. However, in their paper, sparse RBMs were not defined from the perspective of sparse connections\nar X\niv :1\n60 9.\n05 29\n4v 1\n[ cs\n.L G\n] 1\n7 Se\np 20\n16\nbut sparse hidden unit activations. And it was achieved by adding a regularization term to the objective function when training the parameters. There is no structure learning.\nNetwork pruning is also a potential way to optimize the structure of a neural network. Biased weight decay was the early approach to pruning. Later, Optimal Brain Damage (Cun, Denker, and Solla, 1990) and Optimal Brain Surgeon (Hassibi, Stork, and Com, 1993) suggested that magnitudebased pruning may not be the best strategies and they proposed pruning methods based on the Hessian of the loss function. With respect to deep neural networks, Han et al. (2015) proposed to compress a network through a threestep process: train, prune connections, and retrain. We call it redundancy pruning. In contrast, Srinivas and Babu (2015) proposed to prune redundant neurons directly. They all reduced the number of parameters vastly with slight or even no performance loss. The drawback of network pruning is that the original networks should be large enough and hence some computation would be wasted on those unnecessary parameters during pre-training."}, {"heading": "Restricted Boltzmann Machines", "text": "An Restricted Boltzmann Machine (RBM) (Smolensky, 1986) is a two-layer undirected graphical model with a layer of K visible units {v1, . . . , vK} and a layer of F hidden units {h1, . . . , hF }. The two layers are fully connected to each other, while there are no connections between units at the same layer. An example is shown in Figure 1. In the simplest case, all the units are assumed to be binary. An energy function is defined over all the units as follows:\nE(v,h) = \u2212 F\u2211\nj=1 K\u2211 k=1 W kj hjv k \u2212 K\u2211 k=1 vkbk \u2212 F\u2211 j=1 hjaj (1)\nwhere aj and bk are bias parameters for the hidden and visible units respectively, while W kj is the connection weight between hidden unit hj and visible unit vk. The energy function defines a joint probability over v and h as follows:\nP (v,h) = exp(\u2212E(v,h))/Z (2) where Z = \u2211 v\u2032,h\u2032 exp(\u2212E(v\n\u2032, h\u2032)) is a normalization term called the partition function. An important property of RBM is that the conditional distributions P (h|v) and P (v|h) factorize as below:\nP (h|v) = \u220f j P (hj |v) P (v|h) = \u220f k P (vk|h) (3)\nP (hj = 1|v) = \u03c3(aj + K\u2211\nk=1\nW kj v k) (4)\nP (vk = 1|h) = \u03c3(bk + F\u2211\nj=1\nW kj hj) (5)\nwhere \u03c3(x) = 1/(1 + e\u2212x) is the logistic function. The model parameters of an RBM are learned using the Contrastive Divergence algorithm (Hinton, 2002), which maximizes the data likelihood via stochastic gradient descent.\nIn Hinton and Salakhutdinov (2009), RBM was used for topic modeling and the proposed model was called Replicated Softmax. Suppose the vocabulary size is K. Let us represent a document with D tokens as a binary matrix U of sizeK \u2217D with uki = 1 if the ith token is the kth word in the vocabulary. The energy function of document U and hidden units h is defined as follows:\nE(U ,h) = \u2212 F\u2211\nj=1 K\u2211 k=1 W kj hj u\u0302 k \u2212 K\u2211 k=1 u\u0302kbk \u2212D F\u2211 j=1 hjaj (6)\nwhere u\u0302k = \u2211D\ni=1 u k i denotes the count for the k th word. The conditional probabilities P (hj = 1|U) can be calculated as:\nP (hj = 1|U) = \u03c3(Daj + K\u2211\nk=1\nW kj u\u0302 k) (7)\nThe motivation behind Replicated Softmax is to properly model word counts in documents of varying lengths through weight sharing. It was shown to generalize better than Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan, 2003) in terms of log-probability on held-out documents and accuracy on retrieval tasks. In this paper, we will use Replicated Softmax for text analysis."}, {"heading": "Sparse Boltzmann Machines", "text": "In this section, we will propose our new models, Sparse Boltzmann Machines (SBMs). An SBM is a two-layer undirected graphical model with a layer of K visible units {v1, . . . , vK} and a layer of F hidden units {h1, . . . , hF }. The hidden units in SBMs are directly linked up to form a tree structure, while each hidden unit is also individually connected to a subset of the visible units. See Figure 2 for an example SBM. In SBM, the number of hidden units and the connectivities are both learned from data.\nOne technical difference between SBMs and RBMs is that there are direct connections among the hidden units in SBMs. We call them hidden connections. The reason why we introduce the hidden connections into our models is that, the hidden connections provide a way to relate a hidden unit to a visible unit without a direct connection. For example, in Figure 2, hidden unit h1 is not directly connected to visible unit v4. However, the existence of the hidden connection between h1 and h2 introduces a path connecting h1 and v4, which can help us to better model the correlation between the two units. This is crucial in reducing the number of connections between hidden units and visible units. To avoid the connections among the hidden units becoming too dense, we restrict them to form a tree structure."}, {"heading": "Parameter Learning", "text": "SBMs also can be extended for text analysis as RBMs are extended to Replicated Softmax. Here we will introduce SBMs in the context of Replicated Softmax and use the same notations in the previous section. Let G be a graph representing the model structure. Edge (j, k) belongs to G if and only if there is a link between the visible unit vk and hidden unit hj . Edge (j, l) belongs to G if and only if there is a link between the hidden unit hj and hidden unit hl. Also let Wjl be the weight on the connection between hidden unit hj and hidden unit hl. Then the energy function of an SBM for document U and hidden units h is as below:\nE(U ,h) =\u2212 \u2211\n(j,k)\u2208G\nW kj hj u\u0302 k \u2212 K\u2211 k=1 u\u0302kbk\n\u2212D F\u2211\nj=1\nhjaj \u2212D \u2211\n(j,l)\u2208G\nWjlhjhl.\n(8)\nSimilar to Replicated Softmax, our model defines the joint distribution as:\nP (U ,h) = 1 Z exp(\u2212E(U ,h)), (9)\nwhere Z = \u2211 U\u2032 \u2211 h exp(\u2212E(U\n\u2032,h)). Note that the summation over U \u2032 is done over all the possible documents with the same length as U .\nLet U\u0304 = {Un}Nn=1 be a collection of N documents with potentially different lengths D1, . . . , DN . We assume that P (U\u0304) = \u220fN n=1 P (Un), where P (Un) = \u2211 h P (Un,h). The objective of training an SBM for U\u0304 is to maximize the loglikelihood of the documents logP (U\u0304). We maximize the objective function via stochastic gradient descent. The partial derivatives of logP (U\u0304) w.r.t the parameters W kj , bk and aj remain the same as in Replicated Softmax:\n\u2202 logP (U\u0304) \u2202W kj = N\u2211 n=1 (EP (hj |Un)[hj u\u0302 k n]\u2212 EP (U,h)[hj u\u0302k]) (10)\n\u2202 logP (U\u0304) \u2202bk = N\u2211 n=1 (u\u0302kn \u2212 EP (U)[u\u0302k])\n\u2202 logP (U\u0304) \u2202aj = N\u2211 n=1 Dn(EP (hj |Un)[hj ]\u2212 EP (hj)[hj ])\nwhile the partial derivative of logP (U\u0304) w.r.t the new parameter Wjl for fixed j and l is:\n\u2202 logP (U\u0304) \u2202Wjl = N\u2211 n=1 Dn(EP (h|Un)[hjhl]\u2212 EP (h)[hjhl]) (11)\nThe first terms in these partial derivatives require the computation of the conditional probabilities P (hj |Un) and P (h|Un). In Replicated Softmax, P (hj |Un) can be calculated using Equation (7). While in SBMs, due to the connections between hidden units, P (h|Un) no longer factorizes and hence Equation (7) cannot be applied. Nevertheless, since the hidden units in Sparse Boltzmann Machines are linked as a tree structure, we can easily compute the value of P (hj |Un) and P (h|Un) by conducting message propagation (Murphy, 2012) in the model.\nThe second terms in these derivatives require taking an expectation with respect to the distribution defined by the model, which is intractable. Thus as in Replicated Softmax, we adopt the Contrastive Divergence algorithm to approximate the second terms by running Gibbs sampling chains in the model. Specifically, the Gibbs chains are initialized at the training data and run for T full steps to draw samples from the model. In SBMs, given a document U and the value of all the other hidden units h\u2212j , the conditional probability to sample a hidden unit hj becomes:\nP (hj = 1|U ,h\u2212j) = \u03c3( \u2211\n(j,k)\u2208G\nW kj u\u0302 k +Daj+\nD \u2211\n(j,l)\u2208G\nWjlhl +D \u2211\n(l,j)\u2208G\nWljhl)\nwhile the conditional probability to sample an visible unit remains the same as in Replicated Softmax."}, {"heading": "Structure Learning", "text": "We regard SBMs as a method to model correlations among the visible units. Learning an SBM hence amounts to building a latent structure to explain the correlations. Recently, Liu, Zhang, and Chen (2014) and Chen et al. (2016) proposed a method, called HLTA, for learning a Hierarchical Latent Tree Model (HLTM) from data. Our structure learning algorithm for SBMs is built upon their work. We expand the tree model from HLTA to obtain the structure of an SBM.\nHLTA learns a tree model T with a layer of observed variables at the bottom and multiple layers of latent variables. Note that the visible units and hidden units in SBMs are called observed variables and latent variables in HLTM respectively. The left panel in Figure 3 and Figure 4 illustrate example models that HLTA produces. Each latent variable in the model is connected to a set of highly-correlated variables in the layer below. The number of latent variables at\neach layer is determined automatically by the algorithm. The number L of latent layers in T is controllable. In this paper, we set L = 2. Let Hl be the lth latent layer in T . Also let VZ be the set of observed variables which are located in the subtree rooted at latent variable Z in T .\nTo build the structure of an SBM from T , we first remove all the latent layers except the top layer HL. Then we connect each latent variable Z inHL to the set of observed variables VZ . We use the resulting structure as a skeleton T \u2032 of the corresponding SBM. This is illustrated in Figure 3, where the hidden units h1, h2 in SBM correspond to Z21, Z22 in T respectively. Note that the skeleton is still a tree structure, where each node has only one parent.\nAs to remove the tree-structure constraint, we conduct an expansion step to increase the number of \u201cfan-out\u201d connections for each hidden unit in T \u2032. The key question is how to determine the new set of visible units that a hidden unit should be connected to. We introduce our method using Z21 (correspondingly h1 in T \u2032) and v7 in Figure 3 as an example. To determine whether Z21 should also be connected to v7, we consider the empirical conditional mutual information I(Z21, v7|Z22, U\u0304), where Z22 is the root of the subtree that v7 is in. To estimate the value, we first estimate the empirical joint distribution p\u0302(Z21, Z22, v7). We go through all the documents and compute p(Z21, Z22|Un) for each document Un in U\u0304 by conducting inference in T . Then we collect the statistics of Z21, Z22 and v7 to get p\u0302(Z21, Z22, v7). After that, I(Z21, v7|Z22, U\u0304) can be estimated as: I(Z21, v7|Z22, U\u0304) =\u2211 Z22 p\u0302(Z22) \u2211 v7 \u2211 Z21 p\u0302(Z21, v7|Z22)log p\u0302(Z21, v7|Z22) p\u0302(Z21|Z22)p\u0302(v7|Z22) .\nAll the distributions in the above formula can be derived from the joint distribution p\u0302(Z21, Z22, v7).\nIf the correlation between Z21 and v7 is properly modeled in T , the two variables should be conditionally independent given Z22, and hence I(Z21, v7|Z22, U\u0304) should be zero. Therefore, if I(Z21, v7|Z22, U\u0304) is not 0, then we can conclude that the correlation between Z21 and v7 is not properly modeled in the model, and the model needs to be expanded by adding new connections between the two variables.\nOur algorithm, called SBM-SFC (SBM-Structure from Correlation), is given in Algorithm 1. It considers the latent variables one at a time. For a given latent variable Z (suppose the corresponding hidden unit in T \u2032 is h), it computes the conditional mutual information between Z and each unconnected observed variable, and sorts the observed variables in descending order with respect to the conditional mutual information. Then in T \u2032, it connects hidden unit h to the visible units corresponding to the top M observed variables with the highest conditional mutual information.M is a predefined parameter, which normally is set to the value such that each hidden unit is connected to 0.2 \u2217 K hidden units. After the above expansion step is done for each hidden unit in T \u2032, the whole structure of an SBM is determined."}, {"heading": "Experiments", "text": "In this section we test the performance of our Sparse Boltzmann Machines on three text datasets of different scales:\nAlgorithm 1 SBM-SFC(T ) Inputs: T\u2014Graph of an HLTM, U\u0304\u2014Collection of training docu-\nments, M\u2014Number of new connections for each hidden unit.\nOutputs: Graph T \u2032 of a corresponding SBM. 1: T \u2032 \u2190 \u2205, HL \u2190 graph of the top latent layer in T 2: V \u2190 observed variables in T 3: T \u2032.add graph(HL), T \u2032.add units(V ) 4: for variable Z in HL do 5: VZ \u2190 observed variables in subtree rooted at variable Z 6: T \u2032.add edges(Z, VZ), I \u2190 \u2205 7: for V \u2032 in (V \u2212 VZ) do 8: Z\u2032 \u2190 root of the subtree containing V \u2032 9: IZ,V \u2032 \u2190 I(Z, V \u2032|Z\u2032, U\u0304) 10: I.add(IZ,V \u2032) 11: end for 12: I \u2190 sort(I , \u2018descend\u2019) 13: for V \u2032 in the top MZ pairs in I do 14: T \u2032.add edge(V \u2032, Z) 15: end for 16: end for 17: return T \u2032\nNIPS proceeding papers1, CiteULike articles2, and New York Times dataset3. Experimental results show that SBMs perform consistently well over the three datasets in terms of model generalizability, and SBMs always give much better interpretability."}, {"heading": "Datasets", "text": "NIPS proceeding papers consist of 1,740 NIPS papers published from 1987 to 1999. We randomly sample 1,640 papers as training data, 50 as validation data and the remaining 50 as test data. We pre-process the data and choose 1,000 most frequent words throughout the corpus. In this way each document is represented as a vector of 1,000 dimensions, with each element being the number of times the word appears in current document.\nCiteULike article collection contains 16,980 articles. Similarly, we randomly divide it into training data with 12,000 articles, validation data with 1,000 articles and test data with 3,980 articles. 2,000 words with highest average TF-IDF values are chosen to represent the articles.\nThe New York Times dataset includes 300,000 documents, among which we randomly pick 290,000 documents for training, 1,000 for validation and 9,000 for testing. 10,000 words with highest average TF-IDF values are chosen to represent the documents."}, {"heading": "Training", "text": "We divide the training data into mini-batches for training. The batch sizes of dataset NIPS, CiteULike and New York Times are 10, 100 and 1,000 respectively. Model parameters are updated after each mini-batch. Assuming that going through all the mini-batches counts as one epoch, we set the maximum number of training epochs to 50. And we train all\n1Available at http://www.cs.nyu.edu/\u223croweis/data.html 2Available at http://www.wanghao.in/data/ctrsr datasets.rar 3Available at http://archive.ics.uci.edu/ml/datasets/Bag+of+Words\nthe models using the Contrastive Divergence algorithm with T = 10 full Gibbs steps.\nAs for RBM-based Replicated Softmax, we determine the optimal number of hidden units over the validation data with 10 units as the step size. While for Sparse Boltzmann Machines, we firstly train a two-layer HLTM and then increase the number of connections such that every hidden unit is connected to 20% of the visible units that are most correlated. A mask matrix is applied to the connection matrix after each parameter update so as to force the sparse connectivity. The numbers of hidden units automatically determined by our algorithm are 112, 194 and 326 for dataset NIPS, CiteULike and New York Times respectively."}, {"heading": "Evaluations", "text": "The log-probability on held-out data is used to gauge the generalization performance of Replicated Softmax and Sparse Boltzmann Machines. As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax. We extend AIS to Sparse Boltzmann Machines in our experiments. In AIS, we use 500 \u201cinverse temperatures\u201d \u03b2k spaced uniformly from 0 to 0.5, 3,000 \u03b2k spaced uniformly from 0.5 to 0.9, and 6,500 \u03b2k spaced uniformly from 0.9 to 1.0, with a total of 10,000 intermediate distributions. The estimates are averaged over 100 AIS runs for each held-out document. Then we calculate the average per-word perplexity as exp(\u2212 1N \u2211N n=1 1 Dn logP (Un)). A smaller score indicates better generalization performance. Due to the high computation cost, we follow the experiments in Hinton and Salakhutdinov (2009) and randomly sample 50 documents from the validation data to calculate the score. While for test, we use all the 50 test documents in NIPS dataset, and randomly sample 500 documents from test data in CiteULike and New York Times datasets."}, {"heading": "Results", "text": "Overfitting of Fully-Connected RBMs We first empirically show that, the fully-connected structure in Replicated Softmax can easily lead to overfitting once the number of hidden units (and hence the number of parameters) gets too large. Figure 5 depicts the average perplexity scores over validation data for Replicated Softmax with different number of hidden units after 30 epochs. We can see that, the optimal numbers of hidden units for the three datasets are 110, 60 and 120 respectively. After that, the performances of the models get worse when the numbers of hidden units\ngradually increase. Therefore, selecting a proper number of hidden units is crucial to Replicated Softmax since they are very likely to overfit the training data.\nGeneralizability of Sparse Boltzmann Machines and Replicated Softmax In this part, we compare the generalization performance of Sparse Boltzmann Machines with Replicated Softmax. We denote our method as SBM-SFC. Two variants of Replicated Softmax included in comparison are RS\u2217 and RS+. RS\u2217 trains Replicated Softmax with the optimal number of hidden units. RS+ produces Replicated Softmax with the same number of hidden units as SBMSFC. Since this number is normally larger than the optimal number, we denote the method as RS+. As we can see in Table 1, SBM-SFC consistently outperforms RS\u2217 and RS+ over the three datasets. This confirms that Replicated Softmax with full connectivity is prone to overfitting. It also shows that SBMs can lead to better model fit than fully connected RBMs. This is true even when the number of hidden units in RBMs is optimized through held-out validation. Moreover, the poor performance of RS+ shows that the performance gain of SBM-SFC cannot be attributed to the larger number of hidden units.\nComparisons with Redundancy Pruning We also compare our method with the redundancy pruning method which produces Replicated Softmax with sparse connections (Han et al., 2015). We denote the method as RS+ Pruned. It starts from a fully trained model, produced by RS+, and prunes the connections gradually until the number of connections is reduced to be the same as the model by SBM-SFC. For each hidden unit, it prunes the set of connections with the smallest absolute weight value. Then it retrains the pruned model for 1 epoch, and conducts pruning again. The pruning and retraining process is repeated until the desired sparsity is reached. In our experiments, the pruning process took 80, 40 and 40 epochs on the three datasets respectively. As shown in Table 1, SBM-SFC achieves comparable model fit as RS+ Pruned. It shows that our structure learning algorithm is effective and can ease the overfitting problem of fully connected structure as well as the pruning method does. Our method has three advantages over RS+ Pruned. First, the iterative pruning process of RS+ Pruned is computationally expensive. Second, it does not offer a way to determine the number of hidden units. One can do this using held-out validation, but that would be computationally prohibitive. Third, as will be seen later, the models produced by RS+ Pruned are not as interpretable as those obtained by our method.\nNecessity of Hidden Connections In SBMs, we impose a tree structure among the hidden units. Is this necessary? To answer the question, we compare SBM-SFC with a method for Replicated Softmax denoted as RS+ SFC. The model produced by RS+ SFC is the same as that by SBM-SFC, except that there are no connections among the hidden units. As we can see in Table 1, SBM-SFC always performs better than RS+ SFC. This supports our conjecture that the hidden connections are necessary in our models. The result is not surprising. In a multiple layer model, units at a layer are connected via units at higher layers. In a two layer model, there are no higher layers. Hence it is natural to connect the second-layer units directly. To generalize our work to multiple layers, we will need to add connections only among the hidden units at the top layer.\nInterpretability of Sparse Boltzmann Machines and Replicated Softmax Next we compare the interpretability of Sparse Boltzmann Machines and Replicated Softmax. Here is how we interpret hidden units. For each hidden unit, we sort the words in descending order of the absolute value of the connection weights between the words and the hidden unit. The top 10 words with the highest absolute weights are chosen to characterize the hidden unit. We propose to measure the \u201cinterpretability\u201d of a hidden unit by considering how similar pairs of words in the top-10 list are. The similarity between two words is determined using a word2vec model (Mikolov et al., 2013a,b) trained on part of the Google News datasets 4, where each word is mapped to a high dimensional vector. The similarity between two words is defined as the consine similarity of the two corresponding vectors. High similarity suggests that the two words appear in similar contexts. Let L be the list of words representing a hidden unit. We define the compactness of L to be the average similarity between pairs of words in L. We also call it the interpretability score of the hidden unit. Note that\n4https://code.google.com/archive/p/word2vec/\nsome of the words in Lmight not be in the vocabulary of the word2vec model we use. This happens infrequently. When it does, the words are simply skipped. Suppose there are F hidden units in a model. Let C1, ...CF be the interpretability scores of hidden units. We define the interpretability score of the model as: Q = 1F \u2211F f=1 Cf . Obviously the score depends heavily on the number of hidden units. Table 2 reports the interpretability scores of the models produced by RS+, RS+ Pruned and SBM-SFC. The models all have the same number of hidden units and hence their interpertability scores are comparable. SBM-SFC consistently performs the best over the three datasets, showing superior coherency and compactness in the characterizations of the hidden units and thus better model interpretability. Table 3 shows the characterizations of selected hidden units in the models produced by SBM-SFC. They are clearly meaningful."}, {"heading": "Conclusions", "text": "Overfitting in deep models is caused not only by excessive amount of hidden units, but also excessive amount of connections. In this paper we have developed, for models with a single hidden layer, a method to determine the number of hidden units and the connections among the units. The models obtained by the method are significantly better, in terms of held-out likelihood, than RBMs where the hidden and observed units are fully connected. This is true even when the number of hidden units in RBMs is optimized by heldout validation. In comparison with redundancy pruning, our method is more efficient and is able to determine the number of hidden units. Moreover, it produces more interpretable models. In the future, we will generalize the structure learning method to models with multiple hidden layers."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Progressive EM for latent tree models and hierarchical topic detection", "author": ["P. Chen", "N.L. Zhang", "L.K.M. Poon", "Z. Chen"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 1498\u20131504.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "Advances in Neural Information Processing Systems, 598\u2013605.", "citeRegEx": "Cun et al\\.,? 1990", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems 28. Curran Associates, Inc. 1135\u20131143.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["B. Hassibi", "D.G. Stork", "S.C.R. Com"], "venue": "Advances in Neural Information Processing Systems 5, 164\u2013171.", "citeRegEx": "Hassibi et al\\.,? 1993", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 22. 1607\u20131614.", "citeRegEx": "Hinton and Salakhutdinov,? 2009", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. N Sainath"], "venue": "IEEE Signal Processing", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 2278\u20132324.", "citeRegEx": "Lecun et al\\.,? 1998", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 873\u2013880.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Hierarchical latent tree analysis for topic detection", "author": ["T. Liu", "N.L. Zhang", "P. Chen"], "venue": "Machine Learning and Knowledge Discovery in Databases 2014, 256\u2013272.", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, 196\u2013 201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations Workshops.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2012\\E", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing 11(2):125\u2013139.", "citeRegEx": "Neal,? 2001", "shortCiteRegEx": "Neal", "year": 2001}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "Proceedings of the 25th International Conference on Machine Learning, 872\u2013879.", "citeRegEx": "Salakhutdinov and Murray,? 2008", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["P. Smolensky"], "venue": "1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, 194\u2013281.", "citeRegEx": "Smolensky,? 1986", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["S. Srinivas", "R.V. Babu"], "venue": "Proceedings of the British Machine Vision Conference, 31.1\u201331.12.", "citeRegEx": "Srinivas and Babu,? 2015", "shortCiteRegEx": "Srinivas and Babu", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 1058\u20131066.", "citeRegEx": "Wan et al\\.,? 2013", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "It has produced superior results in a range of applications, including image classification (Krizhevsky, Sutskever, and Hinton, 2012), speech recognition (Hinton et al., 2012; Mikolov et al., 2011), language translation (Sutskever, Vinyals, and Le, 2014) and so on.", "startOffset": 154, "endOffset": 197}, {"referenceID": 12, "context": "It has produced superior results in a range of applications, including image classification (Krizhevsky, Sutskever, and Hinton, 2012), speech recognition (Hinton et al., 2012; Mikolov et al., 2011), language translation (Sutskever, Vinyals, and Le, 2014) and so on.", "startOffset": 154, "endOffset": 197}, {"referenceID": 9, "context": "(2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al., 1998) can be pruned without incurring any accuracy loss.", "startOffset": 115, "endOffset": 135}, {"referenceID": 20, "context": "One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training.", "startOffset": 45, "endOffset": 70}, {"referenceID": 3, "context": "How can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak connections (Han et al., 2015).", "startOffset": 123, "endOffset": 141}, {"referenceID": 2, "context": "In fact, Han et al. (2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 2, "context": "(2015) have shown that many weak connections in the fully connected layers of Convolutional Neural Networks (CNNs) (Lecun et al., 1998) can be pruned without incurring any accuracy loss. The convolutional layers of CNNs are sparse, and the fact is considered one of the key factors that lead to the success of CNNs. Moreover, it is well known that overfitting is a serious problem in deep models. One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training. The possibility of randomly dropping connections has also been explored in Wan et al. (2013). Sparseness offers an interesting alternative.", "startOffset": 118, "endOffset": 644}, {"referenceID": 3, "context": "With respect to deep neural networks, Han et al. (2015) proposed to compress a network through a threestep process: train, prune connections, and retrain.", "startOffset": 38, "endOffset": 56}, {"referenceID": 3, "context": "With respect to deep neural networks, Han et al. (2015) proposed to compress a network through a threestep process: train, prune connections, and retrain. We call it redundancy pruning. In contrast, Srinivas and Babu (2015) proposed to prune redundant neurons directly.", "startOffset": 38, "endOffset": 224}, {"referenceID": 18, "context": "An Restricted Boltzmann Machine (RBM) (Smolensky, 1986) is a two-layer undirected graphical model with a layer of K visible units {v, .", "startOffset": 38, "endOffset": 55}, {"referenceID": 7, "context": "The model parameters of an RBM are learned using the Contrastive Divergence algorithm (Hinton, 2002), which maximizes the data likelihood via stochastic gradient descent.", "startOffset": 86, "endOffset": 100}, {"referenceID": 5, "context": "In Hinton and Salakhutdinov (2009), RBM was used for topic modeling and the proposed model was called Replicated Softmax.", "startOffset": 3, "endOffset": 35}, {"referenceID": 1, "context": "Figure 4: An example HLTM from Chen et al. (2016).", "startOffset": 31, "endOffset": 50}, {"referenceID": 15, "context": "Nevertheless, since the hidden units in Sparse Boltzmann Machines are linked as a tree structure, we can easily compute the value of P (hj |Un) and P (h|Un) by conducting message propagation (Murphy, 2012) in the model.", "startOffset": 191, "endOffset": 205}, {"referenceID": 1, "context": "Recently, Liu, Zhang, and Chen (2014) and Chen et al. (2016) proposed a method, called HLTA, for learning a Hierarchical Latent Tree Model (HLTM) from data.", "startOffset": 42, "endOffset": 61}, {"referenceID": 16, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 116, "endOffset": 160}, {"referenceID": 17, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 116, "endOffset": 160}, {"referenceID": 5, "context": "As exactly computing these value is intractable (due to the partition function), Annealed Importance Sampling (AIS) (Neal, 2001; Salakhutdinov and Murray, 2008) was used in Hinton and Salakhutdinov (2009) to estimate the partition function of Replicated Softmax.", "startOffset": 173, "endOffset": 205}, {"referenceID": 5, "context": "Due to the high computation cost, we follow the experiments in Hinton and Salakhutdinov (2009) and randomly sample 50 documents from the validation data to calculate the score.", "startOffset": 63, "endOffset": 95}, {"referenceID": 3, "context": "Comparisons with Redundancy Pruning We also compare our method with the redundancy pruning method which produces Replicated Softmax with sparse connections (Han et al., 2015).", "startOffset": 156, "endOffset": 174}], "year": 2016, "abstractText": "We are interested in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to a subset of the visible units instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all visible units.", "creator": "LaTeX with hyperref package"}}}