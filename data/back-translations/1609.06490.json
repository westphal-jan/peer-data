{"id": "1609.06490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "One Sentence One Model for Neural Machine Translation", "abstract": "Neural machine translation (NMT) is becoming a new state of the art, achieving promising translation results using a simple neural network of encoder and decoder. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network is not the best fit for the specific test sentences. In this paper, we propose the dynamic NMT, which learns a general network as usual and then refines the network for each sentence. Fine-tuning is done on a small set of bilingual training data obtained by looking for similarity to the test sentence. Extensive experiments show that this method can significantly improve translation performance, especially when very similar sentences are available.", "histories": [["v1", "Wed, 21 Sep 2016 10:28:57 GMT  (769kb,D)", "http://arxiv.org/abs/1609.06490v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaoqing li", "jiajun zhang", "chengqing zong"], "accepted": false, "id": "1609.06490"}, "pdf": {"name": "1609.06490.pdf", "metadata": {"source": "CRF", "title": "One Sentence One Model for Neural Machine Translation", "authors": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "emails": ["xqli@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "Introduction", "text": "Neural machine translation achieved great success recently (Kalchbrenner and Blunsom 2013; Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al. 2015; Jean et al. 2015; Tu et al. 2016; Mi et al. 2016).\nHow to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepancy between training and testing, the authors propose to moderately replace gold reference with generated output during training. Wiseman and Rush (2016) take a similar approach and regard training as beam search optimization.\nHowever, no matter how the network parameters are learnt, they are fixed after the training is finished in all cur-\nrent NMT practice. And the same model is applied to every testing sentence. A potential issue of this practice is that a neural network needs to be able to compress all translation knowledge into a fixed set of parameters, which is very hard in reality. So we propose to learn a specific model for each testing sentence by paying more attention to those related sentences. In particular, we propose a learning on-thefly strategy for parameter fine-tuning. First, a general model is learnt from the whole training data. Then, for each testing sentence, we find some similar sentence pairs from the training data and use them to fine tune the parameters.\nThis procedure resembles how human do translation. Given a sentence, especially one we are not familiar, we always would like to search for some similar sentences and see how they are translated. Various translation knowledge can be learned from these examples, such as how to translate a lexicon or phrase in a specific context, and how to reorder the translation of different blocks according to some syntactic clues. Once our translation knowledge is refreshed, we can handle the sentence with much higher confidence.\nThere are two key aspects for the method. One is how to define similarity and the other is how to find similar sentence pairs efficiently. For similarity measure, we tried string based similarity and hidden representation based similarity. Our approach has two additional steps compared with plain decoding: finding similar sentence pairs and fine tuning. To improve the efficiency, we used the technique of inverted index for fast retrieval. We also studied how the size of similar data influences the decoding time.\nExperimental results show our approach can effectively improve the translation performance, especially when highly similar sentences are available."}, {"heading": "Background", "text": "In this section, we will briefly introduce the NMT system from Bahdanau et al. (2015), which will be used later in the experiments. However, our approach is model independent and can be applied to other NMT systems.\nGiven a source sentence s = (s1, s2, ...sm) and its translation t = (t1, t2, ..., tn), NMT models the translation probability with a single neural network as follows,\nar X\niv :1\n60 9.\n06 49\n0v 1\n[ cs\n.C L\n] 2\n1 Se\np 20\n16"}, {"heading": "Test n", "text": ""}, {"heading": "Fine tune", "text": "p(t|s) = n\u220f i=1 p(ti|t<i, s) (1)\nwhere the conditional probability is parameterized with the encoder-decoder framework. The encoder reads the source sentence and encodes it into a sequence of hidden states h = h1, h2, ..., hm with bidirectional GRU.\nhi = [ \u2212\u2192 h i; \u2190\u2212 h i] (2)\n\u2212\u2192 h i = \u2212\u2192 \u03c6 ( \u2212\u2192 h i\u22121, xi) (3)\n\u2190\u2212 h i = \u2190\u2212 \u03c6 ( \u2190\u2212 h i+1, xi) (4)\nwhere xi is the embedding of current word, and the recurrent activation functions \u2212\u2192 \u03c6 and \u2190\u2212 \u03c6 are gated recurrent units.\nThe decoder consists of a recurrent neural network and an attention mechanism. The recurrent neural network computes a hidden state for each target position as follows,\nzj = \u03c6(zj\u22121, yj\u22121, cj) (5) where zj\u22121 is the previous hidden state, yj\u22121 is the embedding of previous word and cj is the context vector obtained by the attention machenism, which decides which source words to look at when predicting current target word.\ncj = m\u2211 i=1 \u03b1i,jhi (6)\nand the weight \u03b1i,j is calculated as follows,\n\u03b1i,j = exp(ei,j)\u2211m k=1 exp(ek,j)\n(7)\nei,j = fATT (zj\u22121, hi) (8) Then the probability of generating a specific target word\nw will be computed by\np(tj = w|t<i, s) = softmax(z>j yw) (9)\nwhere yw is the embedding of target word w."}, {"heading": "Tuning on-the-fly", "text": "As illustrated in Figure 1, the learning strategy of our approach is simple. First, we learn a general model from the whole training corpus. Then, for each testing sentence, we extract a small subset from the training data, consisting of sentence pairs whose source sides are similar to the testing sentence. This subset is used to fine tune the general model and a specific model is obtained for the testing sentence.\nThis procedure can be formulated as two stage optimization. The first stage is to to find a set of network parameters \u03b8 to maximize the log likelihood of the whole training data D = {(s(1), t(1)), (s(2), t(2)), ..., (s(N), t(N))}.\n\u03b8\u0302 = arg max \u03b8 {L(\u03b8)}\n= arg max \u03b8 {log N\u220f k=1 P (t(k)|s(k); \u03b8)} = arg max \u03b8 { N\u2211 k=1 |t(k)|\u2211 i=1 logP (t (k) i |s (k), t (k) <i ; \u03b8)}\nThe second stage is to find a set of parameters in the neighbourhood of \u03b8\u0302 to maximize the log likelihood of a subset of data similar to the testing sentence x.\n\u03b8\u0304 = arg max \u03b8\u2208N (\u03b8\u0302)\n{log \u220f\ns(k)\u223cs\nP (t(k)|s(k); \u03b8)}\nIn the following parts, we will discuss how to evaluate similarity between two sentences and how to quickly find similar sentences from training data."}, {"heading": "Similarity Measure", "text": "There are many methods to evaluate the similarity between two sentences. In this paper, we consider three of them. The first is based on Levenshtein distance, which counts at least how many operations do we need to convert one sequence to another. The operations include insertion, deletion and substitution. Levenshtein distance reflects the surface similarity\nof two sentences, and it does not consider the meaning of the sentence.\nsimLD(s1, s2) = LD(s1, s2)\nmax(len(s1), len(s2)) The second measure is based on average word embedding (Mikolov et al. 2013) of the sentence. Although this sentence representation is simple, it has been shown competitive to many complex sentence representations in many tasks.\nsimvec(s1, s2) = cos(\n\u2211|s1| i=1 vec(s1[i])\nlen(s1) ,\n\u2211|s2| j=1 vec(s2[j])\nlen(s2) )\nThe third measure is based on the hidden states of the encoder in NMT. Unlike word embedding, the hidden states of the encoder contains context information. What\u2019s more, the hidden states is learnt in the translation task. For this similarity measure, we need to run the encoder first with the general model learnt offline to get the representation of the testing sentence. This representation will be compared with the representation of training sentences, which need only to be calculated once in an offline manner.\nsimenc(s1, s2) = cos(\n\u2211|s1| i=1 h1[i]\nlen(s1) ,\n\u2211|s2| j=1 h2[j]\nlen(s2) )\nwhere h1[i] and h2[j] are the hidden states of the two sentences, which are calculated according to equations (2) - (4).\nFinding similar sentences efficiently The training corpus for neural machine translation usually contains millions of sentences. For a given testing sentence, comparing it with every training sentence will be too time consuming. So we propose to filter the training corpus first by only considering those which have common words with the testing sentence, and then compute similarity with the filtered set.\nWe use inverted index for fast retrieval. Each training sentence is given a unique index. And we maintain a word to indexes map, recording the sentence indexes where each word appears. For efficiency consideration, we ignore the most frequent words, which usually are function words and punctuations. Then for each word in a testing sentence, we find all sentences which contain the word. And the union of these sentences are used as the filtered set.\nHowever, calculating Levenshtein distance between the testing sentence and each sentence in the filtered set is still not fast enough. So we propose to further reduce the set with a simpler similarity measure, i.e. dice coefficients.\nsimdice = 2|set(w \u2208 s1) \u2229 set(w \u2208 s2)| |set(w \u2208 s1)|+ |set(s \u2208 s2)|\nWe first calculate the dice coefficients between the testing sentence and each sentence in the filtered set, then reduce the size of the set to a given threshold, e.g. 1000, by keeping the sentences with the highest dice coefficients. Finally, we will calculate Levenshtein distance for the reduced set.\nFor the other two similarity measures, calculating cosine similarity can be done efficiently with linear algebra library. So there is no need to further reduce the filtered set."}, {"heading": "Fine tuning", "text": "The process of fine tuning is almost the same with offline training. The main difference is that the data size used for fine tuning is very small, usually containing only a few sentence pairs. So we need to be careful about overfitting. To this end, we go over the tuning data for only one pass. Learning rate is another factor need to be attended. Too large learning rate will cause overfitting, and too small learning rate will make it hard to learn translation knowledge from the tuning data. According to our pilot study, optimization methods with adaptive learning rate, such as Adadelta (Zeiler 2012), work as well as SGD with carefully tuned learning rate, so we adopt it in our experiments.\nHandle the case with low similarity We cannot always find very similar sentences to the testing sentence, especially when there is not enough in-domain training data. In this case, we propose to find sentences to maximize phrase coverage. The phrase we mention here has the same meaning as the one in phrase-based machine translation, which denotes any consecutive word sequence. Our motivation is to select a subset of training data which can cover as many phrases in the testing sentence as possible. The method to find the subset is shown in Algorithm 1.\nInput: testing sentence x, training data D, phrase table PT Output: a subset of training data Dx Dx \u2190 \u03c6; for i\u2190 1 to max phrase len do\nfor j \u2190 1 to |x| \u2212 i do check if x[j : j + i] in PT ; if True then\nforeach phrase \u2208 phrase pairs do find a sentence pair containing phrase; add the sentence pair to Dx;\nend else\ncontinue; end\nend end\nAlgorithm 1: Find a subset to maximize phrase coverage\nThe algorithm iterates over all possible phrases in the testing sentence and check if it is contained in the phrase table, which is extracted according to aligned bilingual corpus. The table contains a list of phrase pairs in the following form,\nsource ||| target ||| score1 score2 score3 score4 The four scores for each phrase are direct phrase translation probability \u03c6(t|s), inverse phrase translation probability \u03c6(s|t), direct lexical weighting lex(t|s), inverse lexical weighting lex(s|t), which are used to evaluate the quality of the phrase pair from different angles. The direct phrase translation probability and lexical weighting are calculated as follows. The inverse ones are calculated similarly.\n\u03c6(t|s) = Count(s, t) Count(s)\nlex(t|s) = |t|\u220f i=1\n1 |{j|(i, j) \u2208 a}| \u2211 \u2200(i,j)\u2208a p(ei|fj)\nA source phrase may corresponds to many (up to hundreds or thousands) target phrases, we filter them according to the average of the above four scores and keep those with the highest score. If a phrase in the testing sentence matches some source side in the phrase table, we will find a sentence pair in the training data which contains the source side and one of its high-score target side. Since there may be many sentence pairs containing such phrase pair, we choose one with the largest likelihood as follows, which means the sentence pair is simple and easy to learn.\n(s\u0302, t\u0302) = arg max phrase in (s,t) |t|\u220f i=1 P (ti|t<i, s; \u03b8)\nThe translation probability of each training sentence pair is calculated offline with the general network parameters.\nWe don\u2019t use the phrase pairs as training data to finetune the network parameters. There are two reasons. First, context information is not available for choosing the proper phrase translation. Second, training on phrase pairs will harm the recurrent weights of the network, because they are not complete sentences1."}, {"heading": "Experiments", "text": "We evaluate our method on the Chinese to English translation task. Translation quality is measured by the BLEU metric (Papineni et al. 2002)."}, {"heading": "Datasets", "text": "We conduct experiments on two datasets. One is on the United Nations Parallel Corpus2, which is composed of official records and other parliamentary documents of the United Nations. Since this data is from a narrow domain, we can easily find similar sentences for many testing sentences. The training data contains 1M sentence pairs extracted from the corpus, and the testing data contains 5 groups of sentence pairs, with 200 sentence pairs in each group. The most similar3 sentence we can find for the sentences in each group falls into the similarity range of 0-0.2, 0.2-0.4, 0.4-0.6, 0.6- 0.8 and 0.8-1.0, respectively. We also randomly selected 1,000 sentence pairs as development set.\nThe training data of the other dataset is selected from LDC4, which contains about 1.2M sentence pairs, whose sources ranges from news, laws, hansard records, weblogs, spoken dialogues, etc. And we use NIST 03 as development\n1We also tried to fix the recurrent weights and tune the word embeddings only, it performs better than tuning all weights, but still worse than the approach of tuning on complete sentences.\n2http://conferences.unite.un.org/UNCorpus 3The similarity is calculated based on Levenshtein distance. 4https://www.ldc.upenn.edu/\nset, and NIST 04 to 06 as testing set. In contrast to the UN data, we can hardly find very similar sentences to the testing one in this setting."}, {"heading": "Experiment Setting", "text": "The hyperparameters used in our network are described as follows. We limit both the source and target vocabulary to 30k in our experiments. The number of hidden units is 1,000 for both the encoder and decoder. And the embedding dimension is 500 for all source and target tokens. The network parameters are updated with the Adadelta algorithm for both training and fine tuning.\nWhen finding similar sentences based on phrase coverage, we keep top two target phrase for each source phrase. And if a source phrase appears more than 1,000 times in the bilingual corpus, it will be discarded, because it\u2019s unnecessary to re-learn how to translate these common phrases."}, {"heading": "Experiments on UN Data", "text": "We first conduct experiments of the UN corpus, studying which similarity measure is better, and how many similar sentences should be used.\nSimilarity Measure Figure 2 shows the performances of the three similarity measures when different number of similar sentences are used. There are two observations accord-\ning to this figure. First, the performance of the similarity based on Levenshtein distance is always better than the other two, the similarity based on encoder states is slightly worse, and the similarity based on average word embedding is the worst. Since Levenshtein distance only cares string similarity, similar sentences found according to this measure will have more words in common with the testing sentence, thus more parameters related to the word embedding can be updated. And the encoder states takes context information into consideration when compared with averaged word embedding, so it has better performance.\nSecond, the performance gap between different similarity measures become smaller when more similar sentences are used. This is due to the fact that there will be a larger overlapping in the sentences found by the three measures when more sentences are used.\nTo further check the difference between the three measures, we fix the number of sentences used for fine tuning as 4, and show the performance of the three measures on testing sentences in different similarity range in Figure 3. It can be seen from the figure that, when very un-similar (0-0.4) or very similar (0.8-1.0) sentences can be found for the testing sentence, the performances of the three measures have little difference. When the sentences in a relatively high range (0.4-0.8), especially in (0.6-0.8), can be found for the testing sentence, the performance of the Levenshtein distance based similarity is obviously better.\nData Size According to Figure 2, using only 1 similar sentence for fine-tuning performs best. However, this figure only shows the overall performance on the whole testing data. If we dive into testing sentences with different similarity range, the trend will be different, as shown in Figure 4. We adopt the Levenshtein distance based similarity in this experiment. It can be seen from the figure that, if\nvery similar sentences (0.4-1.0) can be found for the testing sentence, using only 1 similar sentence can greatly improve the performance, using more does not provide further help and may even degrade the performance. However, when the found sentences are not very similar (0-0.4), the improvement brought by fine-tuning is much smaller, and using more sentences, such as 16, is better than using one. Less sentences will lead to more severe overfitting, which will make the model remember how to reproduce the translations of the sentences. This is desirable when very similar sentences can be found, but it will produce negative effect otherwise.\nThe best performance we can get for each group of testing sentences are shown in Table 1. It can be seen from the table that more than 10 BLEU points can be gained when we can find very similar (0.6-1) sentences to the testing one. However, if we cannot find very similar sentences (0-0.4), only minor (around 1 BLEU point) improvement can be gained.\nInfluence on Efficiency The influence of data size on efficiency is shown in Figure 5. The time cost in the figure only includes fine-tuning time and decoding time. The retrieval time, i.e., time of finding similar sentences, is not shown because it is relatively small compared to the other two. Retrieval with edit distance measure is the slowest one. But it is still less than 1/3 of the decoding time. We can see from the figure if less or equal than 32 sentences are used for finetuning for each testing sentence, the time cost is controlled within two times of the baseline. If we use 128 sentences, the time cost increases to 4 times."}, {"heading": "Experiments on LDC Data", "text": "In this experiment, we can only find similar sentences in the range of 0-0.4 for more than 90% of the testing sentences. And according to our study on the development set, the num-\nber of sentences used for fine-tuning needs to be increased to 128 to get the best performance when the similarity is low. We think the reason is due to the diversity of the training data. Sentences in the low similarity range may have totally different topics and styles with the testing one. In order to avoid the influence of these unwanted data, more sentences need to be used.\nThe performances on the testing data are shown in Table 2. They are obtained with the following setting, if very similar sentences (0.4-1) can be found, we use only 1 sentence for fine-tuning, otherwise we use 128 sentences. On average, 1.2 BLEU points can be gained on the three testing sets, which is consistent with the experimental results on the UN dataset when very similar sentences cannot be found.\nThe performances of finding similar sentences based on phrase coverage are also shown in the table. The average improvement is 1.45 BLEU points, slightly better than the approach of finding similar sentences directly. And the average sentence number used for fine-tuning is 31, much less than 128. So the time cost is almost halved (see Figure 5)."}, {"heading": "Result Analysis", "text": "We show two examples in Table 3. The above one is the case where highly similar sentence can be found to the testing sentence. After fine-tuning, the model remembers how to generate the translation for the similar sentence. Based on the backbone, it can produce a correct translation for the testing sentence with a minor modification. In the lower example, we can only find a not so similar sentence to the testing one. However, the sentence pair found in the example can remind the model how to translate the phrase \u201c\u65b9\u62ec\u53f7\u201d, whose translation is missing in the baseline system."}, {"heading": "Related Work", "text": "Neural machine translation has a short history of only a few years. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) first propose to use the encoder-decoder architecture to do sequence to sequence mapping. At the same time,\nSutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems.\nRecent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al. 2015; Jean et al. 2015), unawareness of coverage (Tu et al. 2016; Mi et al. 2016) etc, making use of monolingual data (Cheng et al. 2016; Sennrich, Haddow, and Birch 2015), extending to multi-lingual(Dong et al. 2015; Zoph and Knight 2016) and multi-modal (Hitschler and Riezler 2016) scenarios.\nIn statistical machine translation, there are some work making use of similar sentences by means of translation memory (Koehn and Senellart 2010; Ma et al. 2011; Wang et al. 2013; Li, Way, and Liu 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don\u2019t need any modification to the model, and it can bring improvement in all similarity level.\nFinding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder 1997) may be a better choice.\nConclusion In this paper, we propose to learn a specific model for each testing sentence. This is accomplished by two-stage training. An general model is learnt offline on the whole bilingual training corpus. During testing, a small batch of similar sentences are extract to fine-tune the network parameters onthe-fly. Experimental results demonstrate the effectiveness of this approach. When highly similar sentences are available, the improvement can exceed 10 BLEU points. Since our method is model independent, it can also be applied to other tasks beyond machine translation."}, {"heading": "Linguistics and the 7th International Joint Conference on", "text": "Natural Language Processing (Volume 1: Long Papers), 11\u2013 19. Beijing, China: Association for Computational Linguistics.\n[Ma et al. 2011] Ma, Y.; He, Y.; Way, A.; and van Genabith, J. 2011. Consistent translation using discriminative learning: a translation memory-inspired approach. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 1239\u20131248. Association for Computational Linguistics.\n[Mi et al. 2016] Mi, H.; Sankaran, B.; Wang, Z.; and Ittycheriah, A. 2016. A coverage embedding model for neural machine translation. arXiv preprint arXiv:1605.03148.\n[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111\u2013 3119.\n[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, 311\u2013318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics.\n[Sennrich, Haddow, and Birch 2015] Sennrich, R.; Haddow, B.; and Birch, A. 2015. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709.\n[Shen et al. 2015] Shen, S.; Cheng, Y.; He, Z.; He, W.; Wu, H.; Sun, M.; and Liu, Y. 2015. Minimum risk training for neural machine translation. arXiv preprint arXiv:1512.02433.\n[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112.\n[Tu et al. 2016] Tu, Z.; Lu, Z.; Liu, Y.; Liu, X.; and Li, H. 2016. Coverage-based neural machine translation. arXiv preprint arXiv:1601.04811.\n[Wang et al. 2013] Wang, K.; Zong, C.; Su, K.-Y.; et al. 2013. Integrating translation memory into phrase-based machine translation during decoding. In ACL (1), 11\u201321.\n[Wiseman and Rush 2016] Wiseman, S., and Rush, A. M. 2016. Sequence-to-sequence learning as beam-search optimization. arXiv preprint arXiv:1606.02960.\n[Zeiler 2012] Zeiler, M. D. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.\n[Zoph and Knight 2016] Zoph, B., and Knight, K. 2016. Multi-source neural translation. arXiv preprint arXiv:1601.00710."}], "references": [{"title": "Y", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": "2015. Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau. Cho. and Bengio 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bengio", "year": 2015}, {"title": "1997", "author": ["Broder", "A. Z"], "venue": "On the resemblance and containment of documents. In Compression and Complexity of Sequences", "citeRegEx": "Broder 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Cheng"], "venue": "arXiv preprint arXiv:1606.04596", "citeRegEx": "Cheng,? \\Q2016\\E", "shortCiteRegEx": "Cheng", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Cho"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong"], "venue": "In Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Dong,? \\Q2015\\E", "shortCiteRegEx": "Dong", "year": 2015}, {"title": "and Riezler", "author": ["J. Hitschler"], "venue": "S.", "citeRegEx": "Hitschler and Riezler 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Jean,? \\Q2015\\E", "shortCiteRegEx": "Jean", "year": 2015}, {"title": "and Blunsom", "author": ["N. Kalchbrenner"], "venue": "P.", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Senellart", "author": ["P. Koehn"], "venue": "J.", "citeRegEx": "Koehn and Senellart 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative framework of integrating translation memory features into smt", "author": ["Way Li", "L. Liu 2014] Li", "A. Way", "Q. Liu"], "venue": "In Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Luong,? \\Q2015\\E", "shortCiteRegEx": "Luong", "year": 2015}, {"title": "Consistent translation using discriminative learning: a translation memory-inspired approach", "author": ["Ma"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Ma,? \\Q2011\\E", "shortCiteRegEx": "Ma", "year": 2011}, {"title": "A coverage embedding model for neural machine translation", "author": ["Mi"], "venue": "arXiv preprint arXiv:1605.03148", "citeRegEx": "Mi,? \\Q2016\\E", "shortCiteRegEx": "Mi", "year": 2016}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Haddow Sennrich", "R. Birch 2015] Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen,? \\Q2015\\E", "shortCiteRegEx": "Shen", "year": 2015}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Tu"], "venue": "arXiv preprint arXiv:1601.04811", "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "Integrating translation memory into phrase-based machine translation during decoding", "author": ["Wang"], "venue": "In ACL", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "A", "author": ["S. Wiseman", "Rush"], "venue": "M.", "citeRegEx": "Wiseman and Rush 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Knight", "author": ["B. Zoph"], "venue": "K.", "citeRegEx": "Zoph and Knight 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Neural machine translation (NMT) becomes a new state-ofthe-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.", "creator": "LaTeX with hyperref package"}}}