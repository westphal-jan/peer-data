{"id": "1609.07434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural Network-Implemented Pong Game", "abstract": "We present the first self-enhancing reinforcement learning model of reward modulated training, implemented through a continuously improved \"intuitive\" neural network; one agent was trained to play the arcade video game Pong with two reward-based alternatives, one in which the paddle was randomly placed during training, and a second in which the paddle was simultaneously trained on three additional neural networks, so that he could develop a sense of \"safety,\" as his own predicted paddle position is likely to be, in order to return the ball. If the agent was less than 95% sure to return the ball, the policy used an intuitive neural network to place the paddle. We trained both architectures for an equal number of eras and tested the learning performance by letting the trained programs play against a near-perfect opponent, and found that the reinforcement learning model, which uses an intuitive paddle network in addition to its intuitive placement during almost perfect placement in the reward architecture.", "histories": [["v1", "Fri, 23 Sep 2016 17:11:53 GMT  (634kb)", "http://arxiv.org/abs/1609.07434v1", "7 pages, 3 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["matt oberdorfer", "matt abuzalaf"], "accepted": false, "id": "1609.07434"}, "pdf": {"name": "1609.07434.pdf", "metadata": {"source": "CRF", "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural Network-Implemented Pong Game", "authors": ["Matt Oberdorfer", "Matt Abuzalaf"], "emails": ["matt@frostdatacapital.com", "matthew.abuzalaf@yale.edu"], "sections": [{"heading": "1. Introduction", "text": "In artificial intelligence, reinforcement learning is an important field of study as it pertains to the ongoing development of \u201csmart technology.\u201d One method by which an agent can regulate its learning is through reward modulation, wherein the agent is trained as it receives rewards for desirable actions in order to form an optimal policy [1]. An extension to this approach is that the agent can also receive a \u201cpunishment\u201d of sorts for incorrect actions, so as to train the agent to avoid repeating the same response in future instances. Over successive iterations, the network develops a sort of predictive insight that should ultimately allow it to make a quick\ndetermination of the actions it needs to take in order to earn a reward.\nWe are introducing reward-modulated learning model that uses inputs and outputs of a prediction neural network to develop an \u201cintuition\u201d policy that is based on the results of the network\u2019s actions, and ultimately, a welltrained agent can use this intuition to make predictions even in a complex environment, where simple pattern recognition may not capture all possible intricacies.\nOne key to the successful training of any reinforcement-learning model is an ability to make quick and accurate predictions about the actions it needs to take to be rewarded. If the\nagent can be trained to make a good prediction in the context of its environment, then it can be further developed to accomplish particular goals in that setting. The obstacle to overcome in the implementation of such an approach, however, tends to be the difficulty in setting up a neural network that can learn efficiently and accurately to predict successful behavior in a previously unknown environment.\nAt the most basic level, success is constituted here by using such inputs for the network that will capture the fullness of the problem and make a prediction that reflects this complexity. Once this is accounted for, more features can be added, such as additional rewards that are weighted based on their relative importance to the problem, switches that allow for alternation between training styles, and even the implementation of additional learning networks that run in parallel with the original.\nTo test how our learning model may capture and learn a complex problem, the arcade game Pong was selected as a good candidate. It has been shown [2] that arcade games can represent both a challenge problem and provide a platform and methodology for evaluating the development of general, domain-independent AI technology.\nAs one of the earliest arcade games, the program\u2019s environment is not particularly complicated: at its basis, the environment involves passing a ball back and forth between paddles as one would do in a game of ping-pong (Figure 1). At the same time, however, the game reflects a degree of complexity in terms of the number of ways any particular return can end up going. The ball can bounce off the top and bottom walls. It can reflect off the paddle at a variety of different angles. And if the ball hits the corner of the paddle, then the return will be made at a greater speed. These factors make for a game where it may not immediately be so simple to predict where to place the paddle for a successful return, and as such, this complex environment is an excellent candidate for an agent to learn by reward-modulated training.\nWe made use of two reward-modulated neural network architectures of different complexities, differing primarily in their methods of placing the agent paddle while the program was training. The first architecture, referred to as the \u201csimple reward\u201d architecture, simply placed the RewardModulated Learning Agent (RMLA) paddle in a random position every time the ball was returned in its direction, only then training when a successful hit was made.\nThe second architecture, referred to as the \u201cfournetwork\u201d architecture, was more complex, using the same-reward modulation as before while at the same time implementing three additional neural networks in order to allow the RMLA to learn a \u201ccertainty\u201d about whether or not its paddle placement was correct. This allowed for the use of the fourth \u201cintuition\u201d network as a policy to expedite the training process by minimizing the need for random paddle placement during reward training. The certainty and intuition networks enable the learning model to assess the state and derive actions similar results as Q-learning [3] while entirely based on back propagation networks."}, {"heading": "2. Related Work", "text": "Reinforcement learning is an area of machine learning wherein a machine learns how to perform optimally by its own trial-and-error processes, rather than by external human correction. In reinforcement learning, the selection of correct decisions is made so as to maximize preordained \u201crewards\u201d in the long term, and the network is strengthened upon reception of these rewards so that it performs similarly in future instances. [1] In general, this process works as follows: over a series of stages or events, the learning agent will make some arbitrary number of decisions or interactions. The policy of the RMLA guides its behavior at any given time in the process (which can be random or deterministic). Positive actions or stages in the process are assigned some numeric representation of desirability by means of a reward function, and it is the RMLA\u2019s ultimate goal to maximize its reward in the long run, thus essentially \u201clearning\u201d positive behaviors within the context of its environment along the way.\nThere are several methods by which reinforcement can occur. One traditional method is learning from delayed rewards [4], a process by which the program uses temporal differences over successive estimates to try to push closer to an optimal policy [5].\nAn alternative approach is using rewardmodulation to train its neural network. In this method, the network is engaged and trained when the agent carries out a desirable action, which, is pre-designated by the reward function [6]. As the network is trained, it learns to contextualize the actions that earned it the reward as a functional output of its variable inputs, therefore being able to replicate the action that led to a reward when a similar situation presents itself in future instances. A practical comparison of confidence estimation methods for neural network architectures examined their effectiveness [7]. In 2013, a deep Q-learning variation was used to create an agent that learns how to play a variety of Atari games [8] and outperform human players."}, {"heading": "3. Regulating Reward Training by Means of Certainty Prediction", "text": "In our approach, we used multiple layers of representation [9] combined with sequenced neural networks [10] to develop a policy that consists of backpropagation networks. We used extensive reward modulation in training, and the key question asked was whether additional functionality could be implemented in order to help the reward network learn how to play faster than it would simply through the brute-force method of placing the paddle randomly.\nThe result of this inquisition was the creation of the \u201cfour-network\u201d architecture, wherein the placement of the paddle during reward training was regulated by three additional networks, which would learn to reliably estimate a confidence for the likelihood that the agent paddle would successfully return the ball and subsequently use this estimation to conclude where to place the paddle during training. The Hypothetically, this would result in a more efficient training session and a faster convergence of the reward network\u2019s paddle position prediction to the correct value in every epoch.\nThe arcade game used in this experiment was a Pong implementation featuring the basic functionality of the original Atari Pong game, including two moving paddles, a ball that could bounce off of the paddles, as well as the top and bottom of the screen, a \u201cserve\u201d function that would serve the ball in a random direction after a score, and a \u201ccorner hit\u201d feature that would bounce the ball at a faster speed off the corner of the paddle. In this version, the agent paddle would track the movement of the ball with a small amount of lag such that it was good at defense but also imperfect at the same time. Basic additions to the program included a scoretracking function, a switch that would place the player paddle under near-perfect computer control (the paddle was made to \u201cwobble\u201d in place so that it would miss occasionally), and a \u201cspeed-up\u201d button that increased the speed of gameplay so that thousands of epochs of training could occur quickly.\nThe simple reward network uses a 6:10:1 architecture to learn how to predict paddle position based on six inputs, the y-position of the ball relative to the top of the screen, the yposition of the ball relative to the bottom of the screen, the y-velocity of the ball if positive, the y-velocity of the ball if negative, the x-velocity of the ball, and the angle phi the ball makes with respect to the horizontal, all normalized so that they take on values between 0 and 1. The\ntraining is reward-modulated, and a reward function is set such that back-propagation occurs whenever the agent paddle successfully makes contact with and returns the ball. A switch is programmed to turn \u201creward training\u201d on and off, and when training is active, the agent paddle is placed randomly. The program is only rewarded when the random placement yields a successful return.\nFor the four-network reward architecture, three additional networks are added to supplement the previous prediction network, which is now constructed with a 6:12:1 architecture. Two of the three new networks are the \u201cPositive Reward\u201d and \u201cNegative Reward\u201d networks with 8:16:2 architectures. The inputs are the same as before but are supplemented with the normalized paddle position prediction defined as the output of the original network, along with that prediction value subtracted from 1. The outputs of these networks involve the position prediction as before in addition to a binary variable. For the Positive Reward network, it takes on a value of 1 for a hit and 0 for a miss. For the Negative Reward network, the opposite is true- the binary variable registers a 0 for a hit and a 1 for a miss.\nThe final network in the four-network program is the \u201cIntuition Network,\u201d which uses an 8:24:3 architecture (the outputs are the position prediction and two binary variables responding to a hit and a miss).\nThe four-network architecture\u2019s policy and structure follows the flow displayed in Figure 2 at the bottom of this document. To summarize: the Prediction network feeds into Positive Reward, Negative Reward, and Intuition networks, which run in parallel and whose outputs determine the placement of the paddle when reward training is switched on.\nThe binary outputs of the Positive Reward and Negative Reward networks are used in the construction of a \u201ccertainty\u201d prediction, which involves the product of the activated positive reward output with the difference of the activated negative reward output from 1. If this certainty value is above a certain threshold (95% in this case), and if the mean of the last 10 certainties is also above a certain threshold (80% in this case), then the paddle will be placed on the basis of the trained prediction, rather than randomly. If the value is below this threshold but above a certain lower threshold, the Intuition Network is invoked in order to place the paddle based on the position where certainty is estimated to be maximized. And if the certainty is below even the low threshold, the paddle will\nbe placed randomly, as it is then theoretically more likely to yield a successful return than would a confidently wrong placement.\nWe create a series of experiments to validate the hypothesis that this adaptive reward-modulated training is more efficient at correct placement than is placing the paddle in a random position for every training epoch."}, {"heading": "4. Results of Experiments", "text": "One of the two goals was simply to check whether or not the reward-modulated learning was successful at training the agent to play Pong. The second goal was to compare the two programs, the \u201cSimple Reward\u201d and \u201cFourNetwork\u201d architectures, in order to see whether the introduction of confidence-regulated reward training could result in a significantly more efficient training process.\nTo test, the two games were run with reward training switched on, and agent performance was checked at certain intervals of training epochs: 500K, 1M, 2M, and 5M. At these many epochs, reward training was switched off and the agent\nFigure 3\nwas allowed to perform on its own. Once the trained agent had scored 10,000 points with training off, the \u201chuman\u201d score was recorded, and the reward training continued. The relative success and performance of the two systems could then be recorded and compared.\nAs represented by Figure 3 and Table 1 above, both reward networks were able to outscore the near-perfect \u201chuman\u201d player (the computerized perfect ball tracker with a slight wobble) at some point between 500,000 and 1,000,000 epochs of training, and as training continued, both trained agent programs continued to improve their margins of victory over the near-perfect player. Theoretically, continued training for millions of additional epochs would allow for the continued betterment of the agent programs until they approach \u201cperfect\u201d predictive performance.\nIn comparing the two programs, we validated that the more developed four-network architecture yields better results. Namely, at the same number of reward training intervals, the four-network program begins to outperform the simple reward program by an increasingly large margin, though both programs manage to learn how to outscore the human player. In other words, the four-network program should of the two choices theoretically develop perfect predictive capability more quickly."}, {"heading": "5. Analysis and Evaluation", "text": "To weigh in on the first question specified in the results section, it became evident that the reward-modulated training methods applied here were successful. If success is qualified as the\nreliability of the RMLA in it being able to outscore a particularly good \u201chuman player,\u201d then the result is an astounding success for both the simple reward and four-network programs.\nBoth begin to outscore the \u201chuman\u201d at some point between 500K and 1M epochs of training, which, with the speed-up function programmed into the script, can be accomplished in under five minutes. At this point, an actual imperfect human playing against the agent would be soundly defeated time and time again. With additional training, both programs begin to win by greater margins and become essentially perfect within a relatively short amount of time. This reflects well on the choice of variable inputs and architectures for the two networks, as these are the key elements in the agent\u2019s success and efficiency.\nAs for the comparison of the two approaches\u2019 relative performances, it also became evident that the four-network architecture is by far the more efficient of the two. Looking at Figure 2 and Table 1, it is evident that while the simple reward network may appear to perform better initially (this may simply be the result of variability from the small sample size of epochs trained), the four-network program demonstrably has the better performance as the training goes on.\nMore importantly, its margin of improvement between intervals is larger than that for the simple reward program. In other words, it gets better faster. At its relative rate of improvement, it can be projected that the four-network program will converge to perfect performance much more quickly than would the simple reward program, and since efficiency is key in the operation of neural networks, the fact that this method can adaptively regulate and expedite the reward training process is particularly noteworthy."}, {"heading": "6. Conclusion", "text": "We successfully validated the hypothesis that our reward-modulated learning model\noutperforms near-perfect machine and human opponents playing the arcade game pong. The reward modulation model and policy consisted of four sequenced neural networks whereby the second and third network predicted the certainty of receiving a reward that was predicted by the first network. A fourth neural network was trained to generate an action in case the certainty was low. Even with a simple reward network that involved the use of nothing more than random paddle placement to train the agent on successful returns, the program was able to learn how to outscore its opponent in less than a million training epochs, which translates to less than five minutes of training in real time. Additionally, performance only grew better with additional training, and it seems that in a reasonably short amount of time, the simple reward network would have trained a nearperfect agent. The particularly noteworthy conclusion here, pertains to the further success of the reward modulated machine learning model in adapting and optimizing the reward training process such that it allowed the agent to increase performance at a faster rate, converging towards perfection much more efficiently than other reward architectures, which appears to have much more pronounced diminishing returns on improvement."}, {"heading": "7. Suggestions for Further Research", "text": "A comparison between this and other methods of training an agent to play arcade games, such as deep Q-learning [8], will help to validate which model shows better training convergence and task accomplishments. In future experiments, we will examine whether the four-network method developed here could be generalized and implemented for other challenges including other arcade games. If in an equivalent number of training epochs our approach of rewardmodulated program could outperform Qlearning based algorithms, then the four-network method developed here would be an alternative machine learning method for use in more\ngeneral applications than the one featured in this experiment."}, {"heading": "8. References", "text": "[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement Learning: An Introduction, MIT Press 2012 [2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, p.p. 253\u2013279, 2013 [3] Christopher JCH Watkins and Peter Dayan. Qlearning. Machine learning, p.p. 279\u2013292, 1992. [4] Poole, David, and Alan Mackworth, \"Q-learning\" Artificial Intelligence, 2010 [5] C. Watkins, \u201cLearning From Delayed Rewards\u201d, (Ph.D. Thesis) Cambridge University, Cambridge, England, 1989 [6] Aswolinskiy, Witali, and Gordon Pipa, \u201cRMSORN: a reward-modulated self-organizing recurrent neural network\u201d, Frontiers, Computational Neuroscience, 2015 [7] G. Papadopoulos, P.J. Edwards, A.F. Murray, \u201cConfidence Estimation Methods for Neural Networks: A Practical Comparison\u201d, ESANN'2000 proceedings, p.p. 75-80, 2000 [8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, \u201cPlaying Atari with Deep Reinforcement Learning\u201d, arXiv:1312.5602v1 [cs.LG], 2013 [9] Geoffrey E Hinton. Learning multiple layers of representation. Trends in cognitive sciences, p.p. 428\u2013434, 2007 [10] Alex Graves, \u201cGenerating Sequences With Recurrent Neural Networks\u201d arXiv:1308.0850v5, [cs.NE], 2014"}], "references": [{"title": "Reinforcement Learning: An Introduction, MIT", "author": ["Sutton", "Richard S", "Andrew G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, p.p", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Qlearning. Machine learning, p.p", "author": ["Christopher JCH Watkins", "Peter Dayan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Learning From Delayed Rewards\u201d, (Ph.D", "author": ["C. Watkins"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "RM- SORN: a reward-modulated self-organizing recurrent neural network\u201d, Frontiers", "author": ["Aswolinskiy", "Witali", "Gordon Pipa"], "venue": "Computational Neuroscience,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Confidence Estimation Methods for Neural Networks: A Practical Comparison\u201d, ESANN'2000 proceedings, p.p", "author": ["G. Papadopoulos", "P.J. Edwards", "A.F. Murray"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Learning multiple layers of representation", "author": ["Geoffrey E Hinton"], "venue": "Trends in cognitive sciences, p.p", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "[cs.NE],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "\u201d One method by which an agent can regulate its learning is through reward modulation, wherein the agent is trained as it receives rewards for desirable actions in order to form an optimal policy [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "It has been shown [2] that arcade games can represent both a challenge problem and provide a platform and methodology for evaluating the development of", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "to assess the state and derive actions similar results as Q-learning [3] while entirely based on", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "[1] In general, this process works as follows: over a series of stages or events, the learning agent will make some arbitrary number of decisions or interactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "an optimal policy [5].", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "when the agent carries out a desirable action, which, is pre-designated by the reward function [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "methods for neural network architectures examined their effectiveness [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "In our approach, we used multiple layers of representation [9] combined with sequenced neural networks [10] to develop a policy that consists of backpropagation networks.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "In our approach, we used multiple layers of representation [9] combined with sequenced neural networks [10] to develop a policy that consists of backpropagation networks.", "startOffset": 103, "endOffset": 107}], "year": 2016, "abstractText": "We present the first reinforcement-learning model to self-improve its reward-modulated training implemented through a continuously improving \u201cintuition\u201d neural network. An agent was trained how to play the arcade video game Pong with two reward-based alternatives, one where the paddle was placed randomly during training, and a second where the paddle was simultaneously trained on three additional neural networks such that it could develop a sense of \u201ccertainty\u201d as to how probable its own predicted paddle position will be to return the ball. If the agent was less than 95% certain to return the ball, the policy used an intuition neural network to place the paddle. We trained both architectures for an equivalent number of epochs and tested learning performance by letting the trained programs play against a near-perfect opponent. Through this, we found that the reinforcement learning model that uses an intuition neural network for placing the paddle during reward training quickly overtakes the simple architecture in its ability to outplay the near-perfect opponent, additionally outscoring that opponent by an increasingly wide margin after additional epochs of training.", "creator": "Word"}}}