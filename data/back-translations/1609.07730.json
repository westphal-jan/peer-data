{"id": "1609.07730", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) relies heavily on word-level modeling to learn semantic representations of input sentences. However, for languages without natural word boundaries (e.g. Chinese), where input sentences must be tokenized first, traditional NMT faces two problems: 1) it is difficult to find optimal tokenization granularity for source set modeling, and 2) errors in 1-best tokenizations can spread to the NMT encoder. To solve these problems, we propose grid-based Recurrent Neural Network (RNN) encoders for NMT that generalize the standard RNN to word lattice topology. The proposed encoders take as input a word grid that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily large numbers of inputs and hidden states in previous steps of time.", "histories": [["v1", "Sun, 25 Sep 2016 10:59:01 GMT  (3718kb,D)", "http://arxiv.org/abs/1609.07730v1", null], ["v2", "Fri, 9 Dec 2016 13:03:42 GMT  (3845kb,D)", "http://arxiv.org/abs/1609.07730v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jinsong su", "zhixing tan", "deyi xiong", "rongrong ji", "xiaodong shi", "yang liu"], "accepted": true, "id": "1609.07730"}, "pdf": {"name": "1609.07730.pdf", "metadata": {"source": "CRF", "title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "authors": ["Jinsong Su", "Zhixing Tan", "Deyi Xiong", "Yang Liu"], "emails": ["jssu@xmu.edu.cn,", "playinf@stu.xmu.edu.cn", "dyxiong@suda.edu.cn,", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In the last two years, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has obtained state-of-the-art translation performance on some language pairs. In contrast to conventional statistical machine translation that models latent structures and correspondences between the source and target language in a pipeline, NMT trains a unified encoder-decoder\n(Cho et al., 2014; Sutskever et al., 2014) neural network for translation, where an encoder maps the input sentence into a fixed-length vector, and a decoder generates a translation from the encoded vector.\nMost NMT systems adopt RNNs (Mikolov et al., 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) to learn vector representations of source sentences and to generate target sentences due to the strong capability of RNNs in capturing long-distance dependencies. Generally, such representations and generations are learned and performed at the word level. Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016). In these methods, each input sentence is first segmented into a sequence of words, after which RNNs are used to learn word representations and perform generations at the character level. Note that all approaches so far exploit word boundary information for learning source representations. Such a modeling philosophy works well for languages like English. However, it is not a desirable choice for languages without natural word delimiters such as Chinese. The underlying reasons are twofold: 1) the optimal tokenization granularity is always a paradox for machine translation since coarse granularity causes data sparseness while fine granularity results in the loss of useful information, and 2) there may exist some errors in 1-best tokenizations, which potentially propagate to source sentence representations. Therefore, it is necessary to offer multiple tokenizations instead of a single tokenized sequence to NMT for better source sentence modelling.\nIn order to handle these tokenization issues, we propose word-lattice based RNN encoders for NMT. Word lattice is a packed representation of\nar X\niv :1\n60 9.\n07 73\n0v 1\n[ cs\n.C L\n] 2\n5 Se\np 20\n16\nmany tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013). Generalizing the standard RNN to word lattice topology, we expect to not only eliminate the negative effect caused by 1-best tokenization errors but also make the encoder more expressive and flexible to learn semantic representations of source sentences. In this work, we investigate and compare two wordlattice based RNN encoders: 1) a Shallow WordLattice Based GRU Encoder which builds on the combinations of inputs and hidden states derived from multiple tokenizations without any change to the architecture of the standard GRU, and 2) a Deep Word-Lattice Based GRU Encoder which learns and updates tokenization-specific vectors for gates, inputs and hidden states, and then generates hidden state vectors for current units. In both encoders, many different tokenizations can be simultaneously exploited for input sentence modeling.\nTo demonstrate the effectiveness of the proposed encoders, we carry out experiments on Chinese-English translation tasks. Experiment results show that: (1) it is really necessary to exploit word boundary information for learning accurate embeddings of input Chinese sentences; (2) word-lattice based RNN encoders outperform the standard RNN encoder in NMT. To the best of our knowledge, this is the first attempt to build NMT on word lattices."}, {"heading": "2 Neural Machine Translation", "text": "The dominant NMT model is an attention-based one (Bahdanau et al., 2015), which includes an encoder network and a decoder network with attention mechanism.\nThe encoder is generally a bidirectional RNN. The forward RNN reads a source sentence x=x1, x2...xI from left to right and applies the recurrent activation function \u03c6 to learn semantic representation of word sequence x1:i as\u2212\u2192 h i=\u03c6( \u2212\u2192 h i\u22121, ~xi). Similarly, the backward RNN scans the source sentence in a reverse direction and learns the semantic representation \u2190\u2212 h i of the word sequence xi:I . Finally, we concatenate the hidden states of the two RNNs to form the source annotation hi = [ \u2212\u2192 h Ti , \u2190\u2212 h Ti ]\nT , which encodes information about the i-th word with respect to all the other surrounding words in the source sentence.\nThe decoder is a forward RNN producing the translation y in the following way:\np(yj |y<j ,x) = g(yj\u22121, sj ,mj), (1)\nhere g(\u00b7) is a non-linear function, and sj and mj denote the decoding state and the source context at the jth time step, respectively. In particular, si is computed as follows:\nsj = f(sj\u22121, yj\u22121,mj), (2)\nwhere f(\u00b7) is an activation function such as GRU function. According to the attention mechanism, we define mj as the weighted sum of the source annotations {hi}:\nmj = I\u2211 i=1 \u03b1j,i \u00b7 hi, (3)\nwhere \u03b1j,i measures how well yj and hi match as below:\n\u03b1j,i = exp(ej,i)\u2211I\ni\u2032=1 exp(ej,i\u2032) , (4)\nej,i = v T a tanh(Wasj\u22121 + Uahi), (5)\nwhere Wa, Ua and va are the weight matrices of attention model. With this model, the decoder automatically selects words in the source sentence that are relevant to target words being generated."}, {"heading": "3 Word-Lattice based RNN Encoders", "text": "In this section, we study how to incorporate word lattice into the RNN encoder of NMT. We first briefly review the standard GRU, which is the basis of our encoder units. Then, we describe how to generate the word lattice of each input sentence. Finally, we describe word-lattice based GRU encoders in detail."}, {"heading": "3.1 The Standard GRU", "text": "GRU is a new type of hidden unit motivated by LSTM. As illustrated in Figure 2(a), at time step t, GRU has two gates: 1) a reset gate rt which determines how to combine the new input xt with the previous memory ht\u22121, and 2) an update gate zt that defines how much of the previous memory to keep around. Formally, the GRU transition equations are as follows:\nrt = \u03c3(W (r)xt + U (r)ht\u22121), (6)\nzt = \u03c3(W (z)xt + U (z)ht\u22121), (7)\nh\u0303t = \u03c6(Wxt + U(rt ht\u22121)), (8) ht = zt ht\u22121 + (1\u2212 zt) h\u0303t, (9)\nwhere \u03c3 is the logistic sigmoid function, denotes the elementwise multiplication, and W \u2217 and U\u2217 are the weight matrices, respectively.\nSimilar to LSTM, GRU overcomes exploding or vanishing gradient problem (Hochreiter and Schmidhuber, 1997) of the conventional RNN. However, GRU is easier to compute and implement than LSTM. Therefore, in this work, we adopt GRU to build our word lattice based encoders. However, our method can be applied on other RNNs as well."}, {"heading": "3.2 Word Lattice", "text": "As shown in Figure 1, a word lattice is a directed graph G = \u3008V,E\u3009, where V is node set and E is edge set. Given the word lattice of a character sequence c1:N=c1...cN , node vi \u2208 V denotes the position between ci and ci+1, edge ei:j \u2208 E departs from vi and arrives at vj from left to right, covering the subsequence ci+1:j that is recognized as a possible word. To generate word lattices, we train many word segmenters using multiple corpora with different annotation standards, such as the Peking University Corpus (PKU), the Microsoft Research Corpus (MSR) and the Penn Chinese Treebank 6.0 (CTB). For each input sentence, we tokenize it using these different segmenters and generate a word lattice by merging the predicted tokenizations that are shared in different segmenters. For example, in Figure 1, both CTB and MSR segmenters produce the same tokenization \u201c\u4242-\u656f-\u486e\u201d, which is merged into the edge e0:3 in the lattice. Obviously, word lattice has richer network topology than a single word\nsequence. It encodes many tokenizations for the same character subsequence, and thus there may exist multiple inputs and preceding hidden states at each time step, which can not be simultaneously modelled in the standard RNN."}, {"heading": "3.3 Word-lattice based RNN with GRU encoders", "text": "To deal with the above problem, we propose wordlattice based GRU encoders for NMT. Similar to the dominant NMT model (Bahdanau et al., 2015), our encoders are bidirectional RNNs with GRU. Here we only introduce the forward RNN. The backward RNN can be extended in a similar way.\nOur encoders scan a source sentence character by character. Only at potential word boundaries, hidden states are generated from many input candidate words and the corresponding preceding hidden states. Specifically, at time step t, we first identify all edges pointing to vt, each of which covers different input words with preceding hidden states. In particular, for the kth edge1, we denote its input word vector and the corresponding preceding hidden state as x(k)t and h (k) pre, respectively. As shown in Figure 1, the word lattice contains two edges e0:3 and e1:3, both of which link to v3. Therefore there exist two input words \u201c\u4242-\u656f\u486e\u201d and \u201c\u656f-\u486e\u201d with different preceding hidden states at the 3rd time step. We then propose two word-lattice based GRUs to exploit multiple tokenizations simultaneously:\n1We index the edges from left to right according to the positions of their starting nodes.\n(1) Shallow Word-Lattice based GRU (SWLGRU). The architecture of SWL-GRU is shown in Figure 2(b). At the potential word boundary of the tth time step, we combine all possible word embeddings {x\u2217t } into a compressed xt. Similarly, the hidden state vectors {h\u2217pre} of preceding time steps are also compressed into hpre. Then, both xt and hpre are fed into the standard GRU to generate the final hidden state vector ht. Obviously, here we do not change the inner architecture of the standard GRU. The combination of multiple word embeddings into one compressed vector, the combination of all preceding hidden states and the corresponding GRU are defined as follows:\nxt = g(x (1) t , x (2) t , ...), (10) hpre = g(h (1) pre, h (2) pre, ...), (11)\nrt = \u03c3(W (r)xt + U (r)hpre), (12)\nzt = \u03c3(W (z)xt + U (z)hpre), (13)\nh\u0303t = \u03c6(Wxt + U(rt hpre)), (14) ht = zt ht\u22121 + (1\u2212 zt) h\u0303t, (15)\nwhere Eqs. (10)-(11) are used to generate the compressed representations xt and hpre, the others are the same as those of the standard GRU, and g(*) is a composition function, for which we will investigate two definitions later on.\n(2) Deep Word-Lattice based GRU (DWLGRU). The architecture of DWL-GRU is illustrated in Figure 2(c). In this unit, we set and update the reset gate r(k)t , the update gate z (k) t and the hidden state vector h(k)t that are specific to the kth edge ending with ct, and then generate a composed hidden state vector ht from {h(\u2217)t }. Different from SWL-GRU, DWL-GRU merges the hidden states specific to different tokenizations rather than the inputs and the preceding hidden states. Formally, the transition equations are defined as follows:\nr (k) t = \u03c3(W (r)x (k) t + U (r)h(k)pre), (16) z (k) t = \u03c3(W (z)x (k) t + U (z)h(k)pre), (17) h\u0303 (k) t = \u03c6(Wx (k) t + U(rtk h(k)pre), (18) h (k) t = z (k) t h(k)pre + (1\u2212 z (k) t ) h\u0303 (k) t , (19) ht = g(h (1) t , h (2) t , ...), (20)\nwhere Eqs. (16)-(19) calculate the gating vectors and the hidden state vectors depending on different tokenizations, and Eq. (20) is used to produce the final hidden state vector at the time step t.\nFor the composition function g(*) involved in the two word-lattice based GRUs, we explore the following two functions: 1) Pooling Operation Function which enables our encoders to automatically capture the most important part for the source sentence modeling, and 2) Gating Operation Function which is able to automatically learn the weights of components in word-lattice based GRUs. Taking ht as an example, we define it as follows\nht = K\u2211 k=1 \u03c3(htkU (g) + b(g))\nK\u2211 k=1 \u03c3(htkU (g) + b(g)) htk (21)\nwhere U (g) and b(g) are the matrix and the bias term, respectively."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "We evaluated the proposed encoders on NIST Chinese-English translation tasks. Our training data consists of 1.25M sentence pairs extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06, with 27.9M Chinese words and 34.5M English words. We chosed the NIST 2005 dataset as the validation set and the NIST 2002, 2003, 2004, 2006, and 2008 datasets as test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. To alleviate the impact of the instability of NMT training, we trained NMT systems five times for each experiment and reported the average BLEU scores. Furthermore, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. To obtain lattices of Chinese sentences, we used the toolkit2 released by Stanford to train word segmenters on CTB, PKU, and MSR corpora.\nTo train neural networks efficiently, we used the most frequent 50K words in Chinese and English as our vocabularies. Such vocabularies cover 98.5%, 98.6%, 99.3%, and 97.3% Chinese words in CTB, PKU, MSR, and lattice corpora, and 99.7% English words, respectively. In addition, all the out-of-vocabulary words are mapped into a special token UNK. We only kept the sentence pairs that are not longer than 70 source characters and 50 target words, which cover 89.9% of the parallel sentences. We apply Rmsprop (Graves, 2013) (momentum = 0, \u03c1 = 0.99, and = 1\u00d7 10\u22124) to train models until there is no BLEU improvement for 5 epochs on the validation set. During this procedure, we set the following hyper-parameters: word embedding dimension as 320, hidden layer size as 512, learning rate as 5 \u00d7 10\u22124, batch size as 80, gradient norm as 1.0. All the other settings are the same as in (Bahdanau et al., 2015)."}, {"heading": "4.2 Baselines", "text": "We refer to the attention-based NMT system with the proposed encoders as LatticeNMT, which has four variants with different network units and composition operations. We compared them against the following state-of-the-art SMT and\n2http://nlp.stanford.edu/software/segmenter.html#Download\nNMT systems:\n\u2022 Moses3: an open source phrase-based translation system with default configurations and a 4-gram language model trained on the target portion of training data. \u2022 RNNSearch (Bahdanau et al., 2015): an in-\nhouse attention-based NMT system, which has slightly better performance and faster speed than GroundHog.4 In addition to the RNNSearch with word-based segmentations, we also compared to that with characterbased segmentations to study whether word boundary information is helpful for NMT. \u2022 MultiSourceNMT: an attention-based NMT\nsystem which also uses many tokenizations of each source sentence as input. However, it performs combination at the final time step, significantly different from the proposed word-lattice based GRUs. Similarly, it has two variants with different composition functions."}, {"heading": "4.3 Overall Performance", "text": "Table 1 reports the experiment results. Obviously, LatticeNMT significantly outperforms non-lattice NMT systems in all cases. When using DWL-GRU with the gating composition operation, the LatticeNMT system outperforms Moses, RNNSearch, MultiSourceNMT by at least gains of 1.7, 1.17, and 0.82 BLEU points, respectively. Parameters. Word- and character-based RNNSearch systems have 55.5M and 44.2M parameters, respectively. Only NMT systems with gating operation, introduce no more than 2.7K parameters over those parameters of RNNSearch. Speed. We used a single GPU device TitanX to train models. It takes one hour to train 9,000 and 4,200 mini-batches for word- and characterbased RNNSearch systems, respectively. The training speeds of MultiSoucceNMT and LatticeNMT systems are slower than that of wordbased RNNSearch: about 4,800\u223c6,000 minibatches are processed in one hour.\nFrom the table, we further have the following findings:\n(1) On all word-segmented corpora (e.g., CTB, PKU, MSR), RNNSearch performs better than character-based NMT system, which demonstrates that for Chinese which is not morphologically\n3http://www.statmt.org/moses/ 4https://github.com/lisa-groundhog/GroundHog\nrich, word boundary information is very useful for source sentence encoding in NMT. For this, we speculate that the character-level encoding has two disadvantages in comparison with the word-level encoding. First, when a source sentence is represented as a sequence of characters rather than words, the sequence length grows significantly. However, translating long sequences still remains a great challenge for the attention-based NMT (Bentivogli et al., 2016). Second, the characterlevel encoder is unable to capture word-level interactions for learning the representations of entire sentences.\n(2) Multiple inputs with different word segmentation standards are useful for NMT. Furthermore, multiple inputs based on word lattices achieve better results. The underlying reason may be that rather than only at final time step, all hidden states of word-lattice based encoders at potential word boundaries are influenced by many different tokenizations. This enables different tokenizations to fully play complementary roles in learning source sentence representations.\n(3) No matter which composition function is used, DWL-GRU is slightly more effective than SWL-GRU. These results accord with our intuition since DWL-GRU exploits multiple tokenizations at a more fine-grained level than SWL-GRU."}, {"heading": "4.4 Analysis", "text": "In order to take a deep look into the reasons why our encoders are more effective than the conven-\ntional encoder, we study the 1-best translations produced by two systems. The first system is the RNNSearch using CTB segmentations, denoted by RNNSearch(CTB). It yields the best performance among all non-lattice NMT systems. The other is LatticeNMT(DG) using DWL-GRU with the gating composition operation, which performs better than other LatticeNMT systems. We find that the utilization of word lattice brings the following advantages:\n(1) Word lattices alleviate 1-best tokenization errors that further cause translation errors. As\nillustrated in Figure 3, in the source sentence \u201cc1\u52e5c2\u568ac3\u55d6c4\u38a1c5\u26a7c6\u48fcc7\u371ac8\u4277...\u201d, the character subsequence c7:8\u201c\u371a \u4277\u201d is incorrectly segmented into two words \u201c\u371a\u201d and \u201c\u4277\u201d by the CTB segmenter. As a result, the word sequence \u201c\u48fc \u371a \u4277\u201d is incorrectly translated into \u201cthe two sides\u201d. In contrast, LatticeNMT(DG) automatically chooses the right tokenization \u201c\u371a-\u4277\u201d due to its higher bidirectional gating weights (0.982 and 0.962). This allows the NMT system to choose the right translation \u201cthe two defendants\u201d for the word sequence \u201c\u48fc\u371a-\u4277\u201d.\n(2) Word lattices endow NMT encoders with highly expressible and flexible capabilities to learn the semantic representations of input sen-\n\ud835\udc630 \ud835\udc631 \ud835\udc632 \ud835\udc633\ud835\udc501:\u8d77 \ud835\udc502:\u8bc9 \ud835\udc503:\u4e66 \ud835\udc630 \ud835\udc635 \ud835\udc6364 \ud835\udc637 \ud835\udc638\ud835\udc504:\u79f0 \ud835\udc505:\uff0c \ud835\udc506:\u4e24 \ud835\udc507:\u88ab \ud835\udc508:\u544a\n\u8d77-\u8bc9-\u4e66 \u79f0\nthe indictment said that \u2026, the two sides \u2026\nthe indictment claimed that the two defendants \u2026\nCTB Segmentation:\nCTB Translation:\nLattice:\nLattice Translation:\nthe indictment said that the two defendants \u2026Reference:\n\u2026\n\ud835\udc630 \ud835\udc633 \ud835\udc635 \ud835\udc636\ud835\udc634 \ud835\udc637 \ud835\udc638 \uff0c \u4e24 \u88ab \u544a\n\u8d77-\u8bc9-\u4e66\ud835\udc520:3\n\u8d77-\u8bc9\ud835\udc520:2 \u4e66\ud835\udc522:3\n\u79f0\ud835\udc523:4 \uff0c\ud835\udc524:5 \u4e24\ud835\udc525:6 \u88ab-\u544a\ud835\udc527:8\n\u88ab\ud835\udc526:7 \u544a\ud835\udc527:8\n\u2026\nFigure 3: Translations of RNNSearch(CTB) and LatticeNMT(DG).\ntences. To test this, we considered 1-best segmentations as a special kind of word lattice and used the trained LatticeNMT(DG) system to decode the test sets with 1-best CTB, PKU, MSR segmentations, respectively. Intuitively, if LatticeNMT(DG) mainly depends on 1-best segmentations, it will have similar performances on lattice and the 1-best segmentation corpora. From Table 3, we find that when decoding paths are constrained by 1-best segmentations, the performance of LatticeNMT(DG) system degrades significantly. Therefore, our LatticeNMT system is able to explore tokenizations with different annotation standards.\n(3) We also find that the lattice-based method reduces the number of UNK words to a certain extent. We calculated the percentages of the maximal character spans of inputs covered by invocabulary words, and compared the CTB, PKU, MSR and lattice corpora. Results are shown in Table 3. We observe that the lattice corpus has the highest percentage of character spans covered by in-vocabulary words."}, {"heading": "5 Related Work", "text": "Our work is related to previous studies that learn sentence representations with deep neural networks. Among them, the most straightforward method is the neural Bag-of-Words model, which, however, neglects the important information of word order. Therefore, many researchers resort to order-sensitive models, falling into one of two classes: (1) sequence models and (2) topological models. The most typical sequence models are RNNs (Mikolov et al., 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al., 2014; Chen et al., 2015). Further, some researchers ex-\ntend standard RNNs to non-sequential ones, such as multi-dimensional RNNs (Graves et al., 2007), grid RNNs (Kalchbrenner et al., 2015) and higher order RNNs (Soltani and Jiang, 2016). In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b). In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).\nConcerning NMT, the conventional NMT relies almost exclusively on word-level source sentence modelling with explicit tokenizations (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Jussa\u0300 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings. A slightly different approach was proposed by Lee et al. (2015), where they explicitly marked each character with its relative location in a word. Recently, Chung et al. (2016) evaluated a character-level decoder without explicit segmentations for NMT. However, their encoder is still a subword-level one. Overall, word boundary information is very important for the encoder modelling of NMT.\nIn contrast to the above approaches, we incorporate word lattice into RNN encoders, which, to the best of our knowledge, has never been investigated before in NMT. The most related models to\nours are those proposed by Tai et al., (2015), Le et al., (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling. Kalchbrenner et al., (2015) studied grid RNNs where the inputs are arranged in a multi-dimensional grid. In higher order RNNs proposed by Soltani and Jiang (2016), more memory units are used to record more preceding states, which are all recurrently fed to the hidden layers as feedbacks through different weighted paths. Our work is also significantly different from these models. We introduce word lattices rather than parse trees and forests to improve sentence modelling, and thus our network structures depend on the input word lattices, significantly different from the prefixed structures of grid RNNs or high order RNNs. More importantly, our encoders are able to simultaneously process multiple input vectors specific to different tokenizations, while tree-structured LSTMs, grid RNNs and high order RNNs deal with only one input vector at each time step."}, {"heading": "6 Conclusions and Future Work", "text": "This paper has presented word-lattice based RNN encoders for NMT. Compared with the standard RNN encoders, our encoders simultaneously exploit the inputs and preceding hidden states depending on multiple tokenizations for source sentence modelling. Thus, they not only alleviate error propagations of 1-best tokenizations, but also are more expressive and flexible than the standard encoder. Experiment results on Chinese-English translation show that our encoders lead to significant improvements over a variety of baselines.\nIn the future, we plan to continue our work in the following directions. In this work, our network structures depend on the word lattices of source sentences. We will extend our models to incorporate the segmentation model into source sentence representation learning. In this way, tokenization and translation are allowed to collaborate with each other. Additionally, we are interested in exploring better combination strategies to improve our encoders."}], "references": [{"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "arXiv:1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Sentence modeling with gated recursive neural network", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proc. of EMNLP2015 Short Paper, pages 655\u2013665.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proc. of ACL2016, pages 1693\u20131703.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u2019eon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u2018a", "Jos\u2563e A.R. Fonollosa"], "venue": "In Proc. of ACL2016 Shot Paper,", "citeRegEx": "Costa.Juss.a and Fonollosa.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss.a and Fonollosa.", "year": 2016}, {"title": "Generalizing word lattice translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "Proc. of ACL2008, pages 1012\u20131020.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Alex Graves", "Santiago Fernandez", "J\u306argen Schmidhuber."], "venue": "Proc. of ICANN2007, pages 549\u2013558.", "citeRegEx": "Graves et al\\.,? 2007", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv:1308.0850v5.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Karl M. Hermann", "Phil Blunsom."], "venue": "Proc. of ACL2013, pages 894\u2013904.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber."], "venue": "Neural Computation, pages 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Proc. of NIPS2014, pages 2042\u20132050.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Mohit Iyyer", "Jordan Boyd Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u2019e III"], "venue": "In Proc. of EMNLP2014,", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Word lattice reranking for chinese word segmentation and part-of-speech tagging", "author": ["Wenbin Jiang", "HaiTao Mi", "Qun Liu."], "venue": "Proc. of COLING2008, pages 385\u2013392.", "citeRegEx": "Jiang et al\\.,? 2008", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proc. of EMNLP2013, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proc. of ACL2014, pages 655\u2013665.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proc. of EMNLP2014 Short Paper, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proc. of EMNLP2004, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proc. of SEM2015, pages 10\u201319.", "citeRegEx": "Le and Zuidema.,? 2015a", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "The forest convolutional network-compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proc. of EMNLP2015, pages 1155\u20131164.", "citeRegEx": "Le and Zuidema.,? 2015b", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Naver machine translation system for wat 2015", "author": ["Hyoung-Gyu Lee", "JaeSong Lee", "Jun-Seok Kim", "Chang-Ki. Lee."], "venue": "Proc. of WAT2015, pages 69\u201373.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."], "venue": "Proc. of EMNLP2015, pages 2326\u20132335.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."], "venue": "Proc. of INTERSPEECH2010, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."], "venue": "Proc. of EMNLP2015, pages 2315\u20132325.", "citeRegEx": "Mou et al\\.,? 2015", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL2002, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. of ICML2011, pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Higher order recurrent neural networks", "author": ["Rohollah Soltani", "Hui Jiang."], "venue": "arXiv:1605.00064.", "citeRegEx": "Soltani and Jiang.,? 2016", "shortCiteRegEx": "Soltani and Jiang.", "year": 2016}, {"title": "Training very deep networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "Jurgen Schmidhuber."], "venue": "Proc. of NIPS2015, pages 2368\u20132376.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. of NIPS2014, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. of ACL2015, pages 1556\u20131566.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A lattice-based framework for joint chinese word segmentation, pos tagging and parsing", "author": ["Zhiguo Wang", "Chengqing Zong", "Nianwen Xue."], "venue": "Proc. of ACL2013 Short Paper, pages 623\u2013627.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "Proc. of ICML2015, pages 1604\u20131612.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "In the last two years, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has obtained state-of-the-art translation performance on some language pairs.", "startOffset": 27, "endOffset": 106}, {"referenceID": 30, "context": "In the last two years, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has obtained state-of-the-art translation performance on some language pairs.", "startOffset": 27, "endOffset": 106}, {"referenceID": 2, "context": "In contrast to conventional statistical machine translation that models latent structures and correspondences between the source and target language in a pipeline, NMT trains a unified encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) neural network for translation, where an encoder maps the input sentence into a fixed-length vector, and a decoder generates a translation from the encoded vector.", "startOffset": 201, "endOffset": 243}, {"referenceID": 30, "context": "In contrast to conventional statistical machine translation that models latent structures and correspondences between the source and target language in a pipeline, NMT trains a unified encoder-decoder (Cho et al., 2014; Sutskever et al., 2014) neural network for translation, where an encoder maps the input sentence into a fixed-length vector, and a decoder generates a translation from the encoded vector.", "startOffset": 201, "endOffset": 243}, {"referenceID": 24, "context": "Most NMT systems adopt RNNs (Mikolov et al., 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 10, "context": ", 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 46, "endOffset": 80}, {"referenceID": 2, "context": ", 2010) such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) to learn vector representations of source sentences and to generate target sentences due to the strong capability of RNNs in capturing long-distance dependencies.", "startOffset": 112, "endOffset": 130}, {"referenceID": 22, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 5, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 3, "context": "Very recently, we have also seen successful approaches in character-level NMT (Ling et al., 2015; Costa-Juss\u2018a and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 78, "endOffset": 151}, {"referenceID": 6, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 13, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 32, "context": "many tokenizations, which has been successfully used in a variety of NLP tasks (Dyer et al., 2008; Jiang et al., 2008; Wang et al., 2013).", "startOffset": 79, "endOffset": 137}, {"referenceID": 10, "context": "Similar to LSTM, GRU overcomes exploding or vanishing gradient problem (Hochreiter and Schmidhuber, 1997) of the conventional RNN.", "startOffset": 71, "endOffset": 105}, {"referenceID": 26, "context": "The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 18, "context": "Furthermore, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences.", "startOffset": 52, "endOffset": 65}, {"referenceID": 8, "context": "We apply Rmsprop (Graves, 2013) (momentum = 0, \u03c1 = 0.", "startOffset": 17, "endOffset": 31}, {"referenceID": 0, "context": "However, translating long sequences still remains a great challenge for the attention-based NMT (Bentivogli et al., 2016).", "startOffset": 96, "endOffset": 121}, {"referenceID": 24, "context": "The most typical sequence models are RNNs (Mikolov et al., 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 10, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 30, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 19, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 33, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 23, "context": ", 2010) with LSTM (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014; Le and Zuidema, 2015a; Zhu et al., 2015; Liu et al., 2015) or GRU (Cho et al.", "startOffset": 18, "endOffset": 135}, {"referenceID": 2, "context": ", 2015) or GRU (Cho et al., 2014; Chen et al., 2015).", "startOffset": 15, "endOffset": 52}, {"referenceID": 1, "context": ", 2015) or GRU (Cho et al., 2014; Chen et al., 2015).", "startOffset": 15, "endOffset": 52}, {"referenceID": 7, "context": "Further, some researchers extend standard RNNs to non-sequential ones, such as multi-dimensional RNNs (Graves et al., 2007), grid RNNs (Kalchbrenner et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 16, "context": ", 2007), grid RNNs (Kalchbrenner et al., 2015) and higher order RNNs (Soltani and Jiang, 2016).", "startOffset": 19, "endOffset": 46}, {"referenceID": 28, "context": ", 2015) and higher order RNNs (Soltani and Jiang, 2016).", "startOffset": 30, "endOffset": 55}, {"referenceID": 27, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 9, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 12, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 25, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 31, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 20, "context": "In topological models, sentence representations are composed following given topological structures over words (Socher et al., 2011; Hermann and Blunsom, 2013; Iyyer et al., 2014; Mou et al., 2015; Tai et al., 2015; Le and Zuidema, 2015b).", "startOffset": 111, "endOffset": 238}, {"referenceID": 4, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 15, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 17, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 11, "context": "In addition to the aforementioned models, convolutional neural networks are also widely used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).", "startOffset": 112, "endOffset": 191}, {"referenceID": 30, "context": "Concerning NMT, the conventional NMT relies almost exclusively on word-level source sentence modelling with explicit tokenizations (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words.", "startOffset": 131, "endOffset": 196}, {"referenceID": 2, "context": "Concerning NMT, the conventional NMT relies almost exclusively on word-level source sentence modelling with explicit tokenizations (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words.", "startOffset": 131, "endOffset": 196}, {"referenceID": 29, "context": "In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings.", "startOffset": 138, "endOffset": 163}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al.", "startOffset": 8, "endOffset": 246}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings.", "startOffset": 8, "endOffset": 418}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings. A slightly different approach was proposed by Lee et al. (2015), where they explicitly marked each character with its relative location in a word.", "startOffset": 8, "endOffset": 585}, {"referenceID": 2, "context": ", 2014; Cho et al., 2014; Bahdanau et al., 2015), which tends to suffer from the problem of unknown words. To address this problem, researchers have proposed alternative characterbased modelling. In this respect, Costa-Juss\u00e0 and Fonollosa (2016) applied character-based embeddings in combination with convolutional and highway layers (Srivastava et al., 2015) to produce word embeddings. Similarly, Ling et al. (2015) used a bidirectional LSTM to generate semantic representations of words based on character embeddings. A slightly different approach was proposed by Lee et al. (2015), where they explicitly marked each character with its relative location in a word. Recently, Chung et al. (2016) evaluated a character-level decoder without explicit segmentations for NMT.", "startOffset": 8, "endOffset": 698}, {"referenceID": 26, "context": "ours are those proposed by Tai et al., (2015), Le et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 26, "context": "ours are those proposed by Tai et al., (2015), Le et al., (2015b), Kalchbrenner et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016).", "startOffset": 11, "endOffset": 39}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al.", "startOffset": 11, "endOffset": 65}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling.", "startOffset": 11, "endOffset": 85}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling.", "startOffset": 11, "endOffset": 146}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling. Kalchbrenner et al., (2015) studied grid RNNs where the inputs are arranged in a multi-dimensional grid.", "startOffset": 11, "endOffset": 250}, {"referenceID": 15, "context": ", (2015b), Kalchbrenner et al., (2015), Soltani and Jiang (2016). Tai et al., (2015) presented treestructured LSTMs, while Le and Zuidema (2015b) further introduced the forest convolutional network for sentence modelling. Kalchbrenner et al., (2015) studied grid RNNs where the inputs are arranged in a multi-dimensional grid. In higher order RNNs proposed by Soltani and Jiang (2016), more memory units are used to record more preceding states, which are all recurrently fed to the hidden layers as feedbacks through different weighted paths.", "startOffset": 11, "endOffset": 385}], "year": 2016, "abstractText": "Neural machine translation (NMT) heavily relies on word-level modelling to learn semantic representations of input sentences. However, for languages without natural word delimiters (e.g., Chinese) where input sentences have to be tokenized first, conventional NMT is confronted with two issues: 1) it is difficult to find an optimal tokenization granularity for source sentence modelling, and 2) errors in 1-best tokenizations may propagate to the encoder of NMT. To handle these issues, we propose word-lattice based Recurrent Neural Network (RNN) encoders for NMT, which generalize the standard RNN to word lattice topology. The proposed encoders take as input a word lattice that compactly encodes multiple tokenizations, and learn to generate new hidden states from arbitrarily many inputs and hidden states in preceding time steps. As such, the word-lattice based encoders not only alleviate the negative impact of tokenization errors but also are more expressive and flexible to embed input sentences. Experiment results on ChineseEnglish translation demonstrate the superiorities of the proposed encoders over the conventional encoder.", "creator": "TeX"}}}