{"id": "1609.07843", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Pointer Sentinel Mixture Models", "abstract": "Newer neural network sequence models with Softmax classifiers achieve their best speech modeling performance only with very large hidden states and large vocabulary, and even when context makes the prediction clear, they struggle to predict rare or invisible words. We introduce the pointer guard blending architecture for neural sequence models that is capable of either reproducing a word from the current context or generating a word from a standard Softmax classifier. Our pointer guard LSTM model achieves state-of-the-art speech modeling performance on Penn Treebank (70.9 perplexity) using far fewer parameters than a standard Softmax LSTM. To assess how well language models can exploit longer contexts and handle more realistic vocabularies and larger corpora, we also introduce the freely available WikiText corpus.", "histories": [["v1", "Mon, 26 Sep 2016 04:06:13 GMT  (654kb,D)", "http://arxiv.org/abs/1609.07843v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["stephen merity", "caiming xiong", "james bradbury", "richard socher"], "accepted": true, "id": "1609.07843"}, "pdf": {"name": "1609.07843.pdf", "metadata": {"source": "META", "title": "Pointer Sentinel Mixture Models", "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "emails": ["SMERITY@SALESFORCE.COM", "CXIONG@SALESFORCE.COM", "JAMES.BRADBURY@SALESFORCE.COM", "RSOCHER@SALESFORCE.COM"], "sections": [{"heading": "1. Introduction", "text": "A major difficulty in language modeling is learning when to predict specific words from the immediate context. For instance, imagine a new person is introduced and two paragraphs later the context would allow one to very accurately predict this person\u2019s name as the next word. For standard neural sequence models to predict this name, they would have to encode the name, store it for many time steps in their hidden state, and then decode it when appropriate. As the hidden state is limited in capacity and the optimization of such models suffer from the vanishing gradient problem, this is a lossy operation when performed over many timesteps. This is especially true for rare words.\nModels with soft attention or memory components have been proposed to help deal with this challenge, aiming to allow for the retrieval and use of relevant previous hidden\n1Available for download at the WikiText dataset site\np(Yellen) = g pvocab(Yellen) + (1 g) pptr(Yellen)\nzebra\nChair Janet Yellen \u2026 raised rates . Ms. ???Fed\n\u2026\nYellenRosenthalBernankeaardvark\n\u2026\n\u2026\nSentinel\n\u2026\nPo in te r\nSo ftm\nax R N N\npvocab(Yellen)\ngpptr(Yellen)\nFigure 1. Illustration of the pointer sentinel-RNN mixture model. g is the mixture gate which uses the sentinel to dictate how much probability mass to give to the vocabulary.\nstates, in effect increasing hidden state capacity and providing a path for gradients not tied to timesteps. Even with attention, the standard softmax classifier that is being used in these models often struggles to correctly predict rare or previously unknown words.\nPointer networks (Vinyals et al., 2015) provide one potential solution for rare and out of vocabulary (OoV) words as a pointer network uses attention to select an element from the input as output. This allows it to produce previously unseen input tokens. While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input.\nWe introduce a mixture model, illustrated in Fig. 1, that combines the advantages of standard softmax classifiers with those of a pointer component for effective and efficient language modeling. Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gu\u0308lc\u0327ehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel. The model improves the state of the art perplexity on the Penn Treebank. Since this commonly used dataset is small and no other freely available alternative exists that allows for learning long range dependencies, we also introduce a new benchmark dataset for language modeling called WikiText.\nar X\niv :1\n60 9.\n07 84\n3v 1\n[ cs\n.C L\n] 2\n6 Se\np 20\n16"}, {"heading": "2. The Pointer Sentinel for Language Modeling", "text": "Given a sequence of words w1, . . . , wN\u22121, our task is to predict the next word wN .\n2.1. The softmax-RNN Component\nRecurrent neural networks (RNNs) have seen widespread use for language modeling (Mikolov et al., 2010) due to their ability to, at least in theory, retain long term dependencies. RNNs employ the chain rule to factorize the joint probabilities over a sequence of tokens: p(w1, . . . , wN ) =\u220fN\ni=1 p(wi|w1, . . . , wi\u22121). More precisely, at each time step i, we compute the RNN hidden state hi according to the previous hidden state hi\u22121 and the input xi such that hi = RNN(xi, hi\u22121). When all the N \u2212 1 words have been processed by the RNN, the final state hN\u22121 is fed into a softmax layer which computes the probability over a vocabulary of possible words:\npvocab(w) = softmax(UhN\u22121), (1)\nwhere pvocab \u2208 RV , U \u2208 RV\u00d7H , H is the hidden size, and V the vocabulary size. RNNs can suffer from the vanishing gradient problem. The LSTM (Hochreiter & Schmidhuber, 1997) architecture has been proposed to deal with this by updating the hidden state according to a set of gates. Our work focuses on the LSTM but can be applied to any RNN architecture that ends in a vocabulary softmax."}, {"heading": "2.2. The Pointer Network Component", "text": "In this section, we propose a modification to pointer networks for language modeling. To predict the next word in the sequence, a pointer network would select the member of the input sequence p(w1, . . . , wN\u22121) with the maximal attention score as the output.\nThe simplest way to compute an attention score for a specific hidden state is an inner product with all the past hidden states h, with each hidden state hi \u2208 RH . However, if we want to compute such a score for the most recent word (since this word may be repeated), we need to include the last hidden state itself in this inner product. Taking the inner product of a vector with itself results in the vector\u2019s magnitude squared, meaning the attention scores would be strongly biased towards the most recent word. Hence we project the current hidden state to a query vector q first. To produce the query q we compute\nq = tanh(WhN\u22121 + b), (2)\nwhere W \u2208 RH\u00d7H , b \u2208 RH , and q \u2208 RH . To generate the pointer attention scores, we compute the match between the previous RNN output states hi and the query q by taking the inner product, followed by a softmax activation function to obtain a probability distribution:\nzi = q Thi, (3)\na = softmax(z), (4)\nwhere z \u2208 RL, a \u2208 RL, and L is the total number of hidden\nstates. The probability mass assigned to a given word is the sum of the probability mass given to all token positions where the given word appears:\npptr(w) = \u2211\ni\u2208I(w,x) ai, (5)\nwhere I(w, x) results in all positions of the word w in the input x and pptr \u2208 RV . This technique, referred to as pointer sum attention, has been used for question answering (Kadlec et al., 2016).\nGiven the length of the documents used in language modeling, it may not be feasible for the pointer network to evaluate an attention score for all the words back to the beginning of the dataset. Instead, we may elect to maintain only a window of the L most recent words for the pointer to match against. The length L of the window is a hyperparameter that can be tuned on a held out dataset or by empirically analyzing how frequently a word at position t appears within the last L words.\nTo illustrate the advantages of this approach, consider a long article featuring two sentences President Obama discussed the economy and President Obama then flew to Prague. If the query was Which President is the article about?, probability mass could be applied to Obama in either sentence. If the question was instead Who flew to Prague?, only the latter occurrence of Obama provides the proper context. The attention sum model ensures that, as long as the entire attention probability mass is distributed on the occurrences of Obama, the pointer network can achieve zero loss. This flexibility provides supervision without forcing the model to put mass on supervision signals that may be incorrect or lack proper context. This feature becomes an important component in the pointer sentinel mixture model."}, {"heading": "2.3. The Pointer Sentinel Mixture Model", "text": "While pointer networks have proven to be effective, they cannot predict output words that are not present in the input, a common scenario in language modeling. We propose to resolve this by using a mixture model that combines a standard softmax with a pointer.\nOur mixture model has two base distributions: the softmax vocabulary of the RNN output and the positional vocabulary of the pointer model. We refer to these as the RNN component and the pointer component respectively. To combine the two base distributions, we use a gating function g = p(zi = k|xi) where zi is the latent variable stating which base distribution the data point belongs to. As we only have two base distributions, g can produce a scalar in the range [0, 1]. A value of 0 implies that only the pointer\nis used and 1 means only the softmax-RNN is used.\np(yi|xi) = g pvocab(yi|xi) + (1\u2212 g) pptr(yi|xi). (6)\nWhile the models could be entirely separate, we re-use many of the parameters for the softmax-RNN and pointer components. This sharing minimizes the total number of parameters in the model and capitalizes on the pointer network\u2019s supervision for the RNN component."}, {"heading": "2.4. Details of the Gating Function", "text": "To compute the new pointer sentinel gate g, we modify the pointer component. In particular, we add an additional element to z, the vector of attention scores as defined in Eq. 3. This element is computed using an inner product between the query and the sentinel2 vector s \u2208 RH . This change can be summarized by changing Eq. 4 to:\na = softmax ([ z; qT s ]) . (7)\nWe define a \u2208 RV+1 to be the attention distribution over both the words in the pointer window as well as the sentinel state. We interpret the last element of this vector to be the gate value: g = a[V + 1].\nAny probability mass assigned to g is given to the standard softmax vocabulary of the RNN. The final updated, normalized pointer probability over the vocabulary in the window then becomes:\npptr(yi|xi) = 1\n1\u2212 g a[1 : V ], (8)\nwhere we denoted [1 : V ] to mean the first V elements of the vector. The final mixture model is the same as Eq. 6 but with the updated Eq. 8 for the pointer probability.\nThis setup encourages the model to have both components compete: use pointers whenever possible and back-off to the standard softmax otherwise. This competition, in particular, was crucial to obtain our best model. By integrating the gating function directly into the pointer computation, it is influenced by both the RNN hidden state and the pointer window\u2019s hidden states."}, {"heading": "2.5. Motivation for the Sentinel as Gating Function", "text": "To make the best decision possible regarding which component to use the gating function must have as much context as possible. As we increase both the number of timesteps and the window of words for the pointer component to consider, the RNN hidden state by itself isn\u2019t guaranteed to\n2A sentinel value is inserted at the end of a search space in order to ensure a search algorithm terminates if no matching item is found. Our sentinel value terminates the pointer search space and distributes the rest of the probability mass to the RNN vocabulary.\naccurately recall the identity or order of words it has recently seen (Adi et al., 2016). This is an obvious limitation of encoding a variable length sequence into a fixed dimensionality vector.\nIn our task, where we may want a pointer window where the length L is in the hundreds, accurately modeling all of this information within the RNN hidden state is impractical. The position of specific words is also a vital feature as relevant words eventually fall out of the pointer component\u2019s window. To correctly model this would require the RNN hidden state to store both the identity and position of each word in the pointer window. This is far beyond what the fixed dimensionality hidden state of an RNN is able to accurately capture.\nFor this reason, we integrate the gating function directly into the pointer network by use of the sentinel. The decision to back-off to the softmax vocabulary is then informed by both the query q, generated using the RNN hidden state hN\u22121, and from the contents of the hidden states in the pointer window itself. This allows the model to accurately query what hidden states are contained in the pointer window and avoid having to maintain state for when a word may have fallen out of the pointer window."}, {"heading": "2.6. Pointer Sentinel Loss Function", "text": "We minimize the cross-entropy loss of \u2212\u2211j y\u0302ij log p(yij |xi), where y\u0302i is a one hot encoding of the correct output. During training, as y\u0302i is one hot, only a single mixed probability p(yij) must be computed for calculating the loss. This can result in a far more efficient GPU implementation. At prediction time, when we want all values for p(yi|xi), a maximum of L word probabilities must be mixed, as there is a maximum of L unique words in the pointer window of length L. This mixing can occur on the CPU where random access indexing is more efficient than the GPU.\nFollowing the pointer sum attention network, the aim is to place probability mass from the attention mechanism on the correct output y\u0302i if it exists in the input. In the case of our mixture model the pointer loss instead becomes:\n\u2212 log g + \u2211 i\u2208I(y,x) ai  , (9) where I(y, x) results in all positions of the correct output y in the input x. The gate g may be assigned all probability mass if, for instance, the correct output y\u0302i exists only in the softmax-RNN vocabulary. Furthermore, there is no penalty if the model places the entire probability mass, on any of the instances of the correct word in the input window. If the pointer component places the entirety of the probability mass on the gate g, the pointer network incurs\nno penalty and the loss is entirely determined by the loss of the softmax-RNN component."}, {"heading": "2.7. Parameters and Computation Time", "text": "The pointer sentinel-LSTM mixture model results in a relatively minor increase in parameters and computation time, especially when compared to the size of the models required to achieve similar performance using standard LSTM models.\nThe only two additional parameters required by the model are those required for computing q, specifically W \u2208 RH\u00d7H and b \u2208 RH , and the sentinel vector embedding, s \u2208 RH . This is independent of the depth of the RNN as the the pointer component only interacts with the output of the final RNN layer. The additional H2 + 2H parameters are minor compared to a single LSTM layer\u2019s 8H2 + 4H parameters. Most state of the art models also require multiple LSTM layers.\nIn terms of additional computation, a pointer sentinelLSTM of window size L only requires computing the query q (a linear layer with tanh activation), a total of L parallelizable inner product calculations, and the attention scores for the L resulting scalars via the softmax function."}, {"heading": "3. Related Work", "text": "Considerable research has been dedicated to the task of language modeling, from traditional machine learning techniques such as n-grams to neural sequence models in deep learning.\nMixture models composed of various knowledge sources have been proposed in the past for language modeling. Rosenfeld (1996) uses a maximum entropy model to combine a variety of information sources to improve language modeling on news text and speech. These information sources include complex overlapping n-gram distributions and n-gram caches that aim to capture rare words. The ngram cache could be considered similar in some ways to our model\u2019s pointer network, where rare or contextually relevant words are stored for later use.\nBeyond n-grams, neural sequence models such as recurrent neural networks have been shown to achieve state of the art results (Mikolov et al., 2010). A variety of RNN regularization methods have been explored, including a number of dropout variations (Zaremba et al., 2014; Gal, 2015) which prevent overfitting of complex LSTM language models. Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).\nIn order to increase capacity and minimize the impact of vanishing gradients, some language and translation mod-\nels have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words.\nAttention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output. In the above example, only January or March would be available as options, as February does not appear in the input. The use of pointer networks have been shown to help with geometric problems (Vinyals et al., 2015), code generation (Ling et al., 2016), summarization (Gu et al., 2016; Gu\u0308lc\u0327ehre et al., 2016), question answering (Kadlec et al., 2016). While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input.\nGu\u0308lc\u0327ehre et al. (2016) introduce a pointer softmax model that can generate output from either the vocabulary softmax of an RNN or the location softmax of the pointer network. Not only does this allow for producing OoV words which are not in the input, the pointer softmax model is able to better deal with rare and unknown words than a model only featuring an RNN softmax. Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use. For neural machine translation, the switching network is conditioned on the representation of the context of the source text and the hidden state of the decoder. The pointer network is not used as a source of information for switching network as in our model. The pointer and RNN softmax are scaled\naccording to the switching network and the word or location with the highest final attention score is selected for output. Although this approach uses both a pointer and RNN component, it is not a mixture model and does not combine the probabilities for a word if it occurs in both the pointer location softmax and the RNN vocabulary softmax. In our model the word probability is a mix of both the RNN and pointer components, allowing for better predictions when the context may be ambiguous.\nExtending this concept further, the latent predictor network (Ling et al., 2016) generates an output sequence conditioned on an arbitrary number of base models where each base model may have differing granularity. In their task of code generation, the output could be produced one character at a time using a standard softmax or instead copy entire words from referenced text fields using a pointer network. As opposed to Gu\u0308lc\u0327ehre et al. (2016), all states which produce the same output are merged by summing their probabilities. Their model however requires a more complex training process involving the forward-backward algorithm for Semi-Markov models to prevent an exponential explosion in potential paths."}, {"heading": "4. WikiText - A Benchmark for Language Modeling", "text": "We first describe the most commonly used language modeling dataset and its pre-processing in order to then motivate the need for a new benchmark dataset."}, {"heading": "4.1. Penn Treebank", "text": "In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words. As part of the pre-processing performed by Mikolov et al. (2010), words were lower-cased, numbers were replaced with N, newlines were replaced with \u3008eos\u3009, and all other punctuation was removed. The vocabulary is the most frequent 10k words with the rest of the tokens be-\ning replaced by an \u3008unk\u3009 token. For full statistics, refer to Table 1."}, {"heading": "4.2. Reasons for a New Dataset", "text": "While the processed version of the PTB above has been frequently used for language modeling, it has many limitations. The tokens in PTB are all lower case, stripped of any punctuation, and limited to a vocabulary of only 10k words. These limitations mean that the PTB is unrealistic for real language use, especially when far larger vocabularies with many rare words are involved. Fig. 3 illustrates this using a Zipfian plot over the training partition of the PTB. The curve stops abruptly when hitting the 10k vocabulary. Given that accurately predicting rare words, such as named entities, is an important task for many applications, the lack of a long tail for the vocabulary is problematic.\nOther larger scale language modeling datasets exist. Unfortunately, they either have restrictive licensing which prevents widespread use or have randomized sentence ordering (Chelba et al., 2013) which is unrealistic for most language use and prevents the effective learning and evaluation of longer term dependencies. Hence, we constructed a language modeling dataset using text extracted from Wikipedia and will make this available to the community."}, {"heading": "4.3. Construction and Pre-processing", "text": "We selected articles only fitting the Good or Featured article criteria specified by editors on Wikipedia. These articles have been reviewed by humans and are considered well written, factually accurate, broad in coverage, neutral in point of view, and stable. This resulted in 23,805 Good articles and 4,790 Featured articles. The text for each article was extracted using the Wikipedia API. Extracting the raw text from Wikipedia mark-up is nontrivial due to the large number of macros in use. These macros are used extensively and include metric conversion, abbreviations, language notation, and date handling.\nOnce extracted, specific sections which primarily featured lists were removed by default. Other minor bugs, such as sort keys and Edit buttons that leaked in from the HTML, were also removed. Mathematical formulae and LATEX code, were replaced with \u3008formula\u3009 tokens. Normalization and tokenization were performed using the Moses tokenizer (Koehn et al., 2007), slightly augmented to further split numbers (8,600\u2192 8 @,@ 600) and with some additional minor fixes. Following Chelba et al. (2013) a vocabulary was constructed by discarding all words with a count below 3. Words outside of the vocabulary were mapped to the \u3008unk\u3009 token, also a part of the vocabulary. To ensure the dataset is immediately usable by existing language modeling tools, we have provided the dataset in the\nAlgorithm 1 Calculate truncated BPTT where every k1 timesteps we run back propagation for k2 timesteps\nfor t = 1 to t = T do Run the RNN for one step, computing ht and zt\nif t divides k1 then Run BPTT from t down to t\u2212 k2\nend if end for\nsame format and following the same conventions as that of the PTB dataset above."}, {"heading": "4.4. Statistics", "text": "The full WikiText dataset is over 103 million words in size, a hundred times larger than the PTB. It is also a tenth the size of the One Billion Word Benchmark (Chelba et al., 2013), one of the largest publicly available language modeling benchmarks, whilst consisting of articles that allow for the capture and usage of longer term dependencies as might be found in many real world tasks.\nThe dataset is available in two different sizes: WikiText-2 and WikiText-103. Both feature punctuation, original casing, a larger vocabulary, and numbers. WikiText-2 is two times the size of the Penn Treebank dataset. WikiText-103 features all extracted articles. Both datasets use the same articles for validation and testing with the only difference being the vocabularies. For full statistics, refer to Table 1."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Training Details", "text": "As the pointer sentinel mixture model uses the outputs of the RNN from up to L timesteps back, this presents a challenge for training. If we do not regenerate the stale historical outputs of the RNN when we update the gradients, backpropagation through these stale outputs may result in incorrect gradient updates. If we do regenerate all stale outputs of the RNN, the training process is far slower. As we can make no theoretical guarantees on the impact of stale outputs on gradient updates, we opt to regenerate the window of RNN outputs used by the pointer component after each gradient update.\nWe also use truncated backpropagation through time (BPTT) in a different manner to many other RNN language models. Truncated BPTT allows for practical time-efficient training of RNN models but has fundamental trade-offs that are rarely discussed.\nFor running truncated BPTT, BPTT is run for k2 timesteps every k1 timesteps, as seen in Algorithm 1. For many RNN\nlanguage modeling training schemes, k1 = k2, meaning that every k timesteps truncated BPTT is performed for the k previous timesteps. This results in only a single RNN output receiving backpropagation for k timesteps, with the other extreme being that the first token receives backpropagation for 0 timesteps. This issue is compounded by the fact that most language modeling code split the data temporally such that the boundaries are always the same. As such, most words in the training data will never experience a full backpropagation for k timesteps.\nIn our task, the pointer component always looks L timesteps into the past if L past timesteps are available. We select k1 = 1 and k2 = L such that for each timestep we perform backpropagation for L timesteps and advance one timestep at a time. Only the loss for the final predicted word is used for backpropagation through the window."}, {"heading": "5.2. Model Details", "text": "Our experimental setup reflects that of Zaremba et al. (2014) and Gal (2015). We increased the number of timesteps used during training from 35 to 100, matching the length of the window L. Batch size was increased to 32 from 20. We also halve the learning rate when validation perplexity is worse than the previous iteration, stopping training when validation perplexity fails to improve for three epochs or when 64 epochs are reached. The gradients are rescaled if their global norm exceeds 1 (Pascanu et al., 2013b).3 We evaluate the medium model configuration which features a hidden size of H = 650 and a two layer LSTM. We compare against the large model configu-\n3The highly aggressive clipping is likely due to the increased BPTT length. Even with such clipping early batches may experience excessively high perplexity, though this settles rapidly.\nration which features a hidden size of 1500 and a two layer LSTM.\nWe produce results for two model types, an LSTM model that uses dropout regularization and the pointer sentinelLSTM model. The variants of dropout used were zoneout (Krueger et al., 2016) and variational inference based dropout (Gal, 2015). Zoneout, which stochastically forces some recurrent units to maintain their previous values, was used for the recurrent connections within the LSTM. Variational inference based dropout, where the dropout mask for a layer is locked across timesteps, was used on the input to each RNN layer and also on the output of the final RNN layer. We used a value of 0.5 for both dropout connections."}, {"heading": "5.3. Comparison over Penn Treebank", "text": "Table 2 compares the pointer sentinel-LSTM to a variety of other models on the Penn Treebank dataset. The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016). The medium pointer sentinel-LSTM model also achieves lower perplexity than the large LSTM models. Note that the best performing large variational LSTM model uses computationally intensive Monte Carlo (MC) dropout averaging. Monte Carlo dropout averaging is a general improvement for any sequence model that uses dropout but comes at a greatly increased test time cost. In Gal (2015) it requires rerunning the test model with 1000 different dropout masks. The pointer sentinel-LSTM is able to achieve these results with far fewer parameters than other models with comparable performance, specifically with less than a third the parameters used in the large variational LSTM models.\nWe also test a variational LSTM that uses zoneout, which\nserves as the RNN component of our pointer sentinelLSTM mixture. This variational LSTM model performs BPTT for the same length L as the pointer sentinel-LSTM, where L = 100 timesteps. The results for this model ablation are worse than that of Gal (2015)\u2019s variational LSTM without Monte Carlo dropout averaging."}, {"heading": "5.4. Comparison over WikiText-2", "text": "As WikiText-2 is being introduced in this dataset, there are no existing baselines. We provide two baselines to compare the pointer sentinel-LSTM against: our variational LSTM using zoneout and the medium variational LSTM used in Gal (2015).4 Attempts to run the Gal (2015) large model variant, a two layer LSTM with hidden size 1500, resulted in out of memory errors on a 12GB K80 GPU, likely due to the increased vocabulary size. We chose the best hyperparameters from PTB experiments for all models.\nTable 3 shows a similar gain made by the pointer sentinelLSTM over the variational LSTM models. The variational LSTM from Gal (2015) again beats out the variational LSTM used as a base for our experiments."}, {"heading": "6. Analysis", "text": ""}, {"heading": "6.1. Impact on Rare Words", "text": "A hypothesis as to why the pointer sentinel-LSTM can outperform an LSTM is that the pointer component allows the model to effectively reproduce rare words. An RNN may be able to better use the hidden state capacity by deferring to the pointer component. The pointer component may also allow for a sharper selection of a single word than may be possible using only the softmax.\nFigure 4 shows the improvement of perplexity when comparing the LSTM to the pointer sentinel-LSTM with words split across buckets according to frequency. It shows that the pointer sentinel-LSTM has stronger improvements as words become rarer. Even on the Penn Treebank, where there is a relative absence of rare words due to only selecting the most frequent 10k words, we can see the pointer sentinel-LSTM mixture model provides a direct benefit.\nWhile the improvements are largest on rare words, we can see that the pointer sentinel-LSTM is still helpful on relatively frequent words. This may be the pointer component directly selecting the word or through the pointer supervision signal improving the RNN by allowing gradients to flow directly to other occurrences of the word in that window.\n4https://github.com/yaringal/BayesianRNN"}, {"heading": "6.2. Qualitative Analysis of Pointer Usage", "text": "In a qualitative analysis, we visualized the gate use and pointer attention for a variety of examples in the validation set, focusing on predictions where the gate primarily used the pointer component. These visualizations are available in the supplementary material.\nAs expected, the pointer component is heavily used for rare names such as Seidman (23 times in training), Iverson (7 times in training), and Rosenthal (3 times in training).\nThe pointer component was also heavily used when it came to other named entity names such as companies like Honeywell (8 times in training) and Integrated (41 times in training, though due to lowercasing of words this includes integrated circuits, fully integrated, and other generic usage).\nSurprisingly, the pointer component was also used for many frequent tokens. For selecting the unit of measurement (tons, kilograms, . . . ) or the short scale of numbers (thousands, millions, billions, . . . ), the pointer would refer to previous recent usage. This is to be expected, especially when phrases are of the form increased from N tons to N tons. The model can even be found relying on a mixture of the softmax and the pointer for predicting certain frequent verbs such as said.\nFinally, the pointer component can be seen pointing to words at the very end of the 100 word window (position 97), a far longer horizon than the 35 steps that most language models truncate their backpropagation training to. This illustrates why the gating function must be integrated into the pointer component. If the gating function could only use the RNN hidden state, it would need to be wary of words that were near the tail of the pointer, especially if it was not able to accurately track exactly how long it\nwas since seeing a word. By integrating the gating function into the pointer component, we avoid the RNN hidden state having to maintain this intensive bookkeeping."}, {"heading": "7. Conclusion", "text": "We introduced the pointer sentinel mixture model and the WikiText language modeling dataset. This model achieves state of the art results in language modeling over the Penn Treebank while using few additional parameters and little additional computational complexity at prediction time.\nWe have also motivated the need to move from Penn Treebank to a new language modeling dataset for long range dependencies, providing WikiText-2 and WikiText-103 as potential options. We hope this new dataset can serve as a platform to improve handling of rare words and the usage of long term dependencies in language modeling."}], "references": [{"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Adi", "Yossi", "Kermany", "Einat", "Belinkov", "Yonatan", "Lavi", "Ofer", "Goldberg", "Yoav"], "venue": "arXiv preprint arXiv:1608.04207,", "citeRegEx": "Adi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp", "Robinson", "Tony"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Language Modeling with Sum-Product Networks", "author": ["Cheng", "Wei-Chen", "Kok", "Stanley", "Pham", "Hoai Vu", "Chieu", "Hai Leong", "Chai", "Kian Ming Adam"], "venue": "In INTERSPEECH,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Gal", "Yarin"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal and Yarin.,? \\Q2015\\E", "shortCiteRegEx": "Gal and Yarin.", "year": 2015}, {"title": "Long ShortTerm Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text Understanding with the Attention Sum Reader Network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Ondruska", "Peter", "Iyyer", "Mohit", "Bradbury", "James", "Gulrajani", "Ishaan", "Zhong", "Victor", "Paulus", "Romain", "Socher", "Richard"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Latent Predictor Networks for Code Generation", "author": ["Ling", "Wang", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Kocisk\u00fd", "Tom\u00e1s", "Senior", "Andrew", "Fumin", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Marcus", "Mitchell P", "Santorini", "Beatrice", "Marcinkiewicz", "Mary Ann"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1s", "Cernock\u00fd", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A Maximum Entropy Approach to Adaptive Statistical Language Modeling", "author": ["Rosenfeld", "Roni"], "venue": null, "citeRegEx": "Rosenfeld and Roni.,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld and Roni.", "year": 1996}, {"title": "End-To-End Memory Networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "author": ["Xiong", "Caiming", "Merity", "Stephen", "Socher", "Richard"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent Highway Networks", "author": ["Zilly", "Julian Georg", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1607.03474,", "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "The softmax-RNN Component Recurrent neural networks (RNNs) have seen widespread use for language modeling (Mikolov et al., 2010) due to their ability to, at least in theory, retain long term dependencies.", "startOffset": 106, "endOffset": 128}, {"referenceID": 6, "context": "This technique, referred to as pointer sum attention, has been used for question answering (Kadlec et al., 2016).", "startOffset": 91, "endOffset": 112}, {"referenceID": 0, "context": "accurately recall the identity or order of words it has recently seen (Adi et al., 2016).", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": "Beyond n-grams, neural sequence models such as recurrent neural networks have been shown to achieve state of the art results (Mikolov et al., 2010).", "startOffset": 125, "endOffset": 147}, {"referenceID": 16, "context": "A variety of RNN regularization methods have been explored, including a number of dropout variations (Zaremba et al., 2014; Gal, 2015) which prevent overfitting of complex LSTM language models.", "startOffset": 101, "endOffset": 134}, {"referenceID": 17, "context": "Other work has improved language modeling performance by modifying the RNN architecture to better handle increased recurrence depth (Zilly et al., 2016).", "startOffset": 132, "endOffset": 152}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 14, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 7, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 15, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016).", "startOffset": 57, "endOffset": 183}, {"referenceID": 6, "context": "A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016).", "startOffset": 212, "endOffset": 233}, {"referenceID": 8, "context": ", 2015), code generation (Ling et al., 2016), summarization (Gu et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 6, "context": ", 2016), question answering (Kadlec et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 8, "context": "Extending this concept further, the latent predictor network (Ling et al., 2016) generates an output sequence conditioned on an arbitrary number of base models where each base model may have differing granularity.", "startOffset": 61, "endOffset": 80}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output.", "startOffset": 58, "endOffset": 943}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output. In the above example, only January or March would be available as options, as February does not appear in the input. The use of pointer networks have been shown to help with geometric problems (Vinyals et al., 2015), code generation (Ling et al., 2016), summarization (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016), question answering (Kadlec et al., 2016). While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input. G\u00fcl\u00e7ehre et al. (2016) introduce a pointer softmax model that can generate output from either the vocabulary softmax of an RNN or the location softmax of the pointer network.", "startOffset": 58, "endOffset": 1545}, {"referenceID": 1, "context": "els have also added a soft attention or memory component (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Cheng et al., 2016; Kumar et al., 2016; Xiong et al., 2016; Ahn et al., 2016). These mechanisms allow for the retrieval and use of relevant previous hidden states. Soft attention mechanisms need to first encode the relevant word into a state vector and then decode it again, even if the output word is identical to the input word used to compute that hidden state or memory. A drawback to soft attention is that if, for instance, January and March are both equally attended candidates, the attention mechanism may blend the two vectors, resulting in a context vector closest to February (Kadlec et al., 2016). Even with attention, the standard softmax classifier being used in these models often struggles to correctly predict rare or previously unknown words. Attention-based pointer mechanisms were introduced in Vinyals et al. (2015) where the pointer network is able to select elements from the input as output. In the above example, only January or March would be available as options, as February does not appear in the input. The use of pointer networks have been shown to help with geometric problems (Vinyals et al., 2015), code generation (Ling et al., 2016), summarization (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016), question answering (Kadlec et al., 2016). While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input. G\u00fcl\u00e7ehre et al. (2016) introduce a pointer softmax model that can generate output from either the vocabulary softmax of an RNN or the location softmax of the pointer network. Not only does this allow for producing OoV words which are not in the input, the pointer softmax model is able to better deal with rare and unknown words than a model only featuring an RNN softmax. Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use. For neural machine translation, the switching network is conditioned on the representation of the context of the source text and the hidden state of the decoder. The pointer network is not used as a source of information for switching network as in our model. The pointer and RNN softmax are scaled according to the switching network and the word or location with the highest final attention score is selected for output. Although this approach uses both a pointer and RNN component, it is not a mixture model and does not combine the probabilities for a word if it occurs in both the pointer location softmax and the RNN vocabulary softmax. In our model the word probability is a mix of both the RNN and pointer components, allowing for better predictions when the context may be ambiguous. Extending this concept further, the latent predictor network (Ling et al., 2016) generates an output sequence conditioned on an arbitrary number of base models where each base model may have differing granularity. In their task of code generation, the output could be produced one character at a time using a standard softmax or instead copy entire words from referenced text fields using a pointer network. As opposed to G\u00fcl\u00e7ehre et al. (2016), all states which produce the same output are merged by summing their probabilities.", "startOffset": 58, "endOffset": 3252}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al.", "startOffset": 167, "endOffset": 188}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words.", "startOffset": 168, "endOffset": 229}, {"referenceID": 9, "context": "Penn Treebank In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words. As part of the pre-processing performed by Mikolov et al. (2010), words were lower-cased, numbers were replaced with N, newlines were replaced with \u3008eos\u3009, and all other punctuation was removed.", "startOffset": 168, "endOffset": 382}, {"referenceID": 2, "context": "Unfortunately, they either have restrictive licensing which prevents widespread use or have randomized sentence ordering (Chelba et al., 2013) which is unrealistic for most language use and prevents the effective learning and evaluation of longer term dependencies.", "startOffset": 121, "endOffset": 142}, {"referenceID": 2, "context": "Following Chelba et al. (2013) a vocabulary was constructed by discarding all words with a count below 3.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "It is also a tenth the size of the One Billion Word Benchmark (Chelba et al., 2013), one of the largest publicly available language modeling benchmarks, whilst consisting of articles that allow for the capture and usage of longer term dependencies as might be found in many real world tasks.", "startOffset": 62, "endOffset": 83}, {"referenceID": 15, "context": "Model Details Our experimental setup reflects that of Zaremba et al. (2014) and Gal (2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": "Model Details Our experimental setup reflects that of Zaremba et al. (2014) and Gal (2015). We increased the number of timesteps used during training from 35 to 100, matching the length of the window L.", "startOffset": 54, "endOffset": 91}, {"referenceID": 17, "context": "The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016).", "startOffset": 108, "endOffset": 128}, {"referenceID": 17, "context": "The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016). The medium pointer sentinel-LSTM model also achieves lower perplexity than the large LSTM models. Note that the best performing large variational LSTM model uses computationally intensive Monte Carlo (MC) dropout averaging. Monte Carlo dropout averaging is a general improvement for any sequence model that uses dropout but comes at a greatly increased test time cost. In Gal (2015) it requires rerunning the test model with 1000 different dropout masks.", "startOffset": 109, "endOffset": 513}, {"referenceID": 11, "context": "0 Pascanu et al. (2013a) - Deep RNN 6M \u2212 107.", "startOffset": 2, "endOffset": 25}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.", "startOffset": 2, "endOffset": 71}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.", "startOffset": 2, "endOffset": 123}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.", "startOffset": 2, "endOffset": 163}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.", "startOffset": 2, "endOffset": 234}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.", "startOffset": 2, "endOffset": 301}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.", "startOffset": 2, "endOffset": 371}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.4\u00b1 0.0 Kim et al. (2016) - CharCNN 19M \u2212 78.", "startOffset": 2, "endOffset": 444}, {"referenceID": 3, "context": "5 Cheng et al. (2014) - Sum-Prod Net 5M\u2021 \u2212 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9\u00b1 0.2 79.7\u00b1 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M \u2212 78.6\u00b1 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9\u00b1 0.3 75.2\u00b1 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M \u2212 73.4\u00b1 0.0 Kim et al. (2016) - CharCNN 19M \u2212 78.9 Zilly et al. (2016) - Variational RHN 32M 72.", "startOffset": 2, "endOffset": 485}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively.", "startOffset": 33, "endOffset": 55}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively.", "startOffset": 33, "endOffset": 70}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively. The medium pointer sentinel-LSTM model achieves lower perplexity than the large LSTM model of Gal (2015) while using a third of the parameters and without using the computationally expensive Monte Carlo (MC) dropout averaging at test time.", "startOffset": 33, "endOffset": 252}, {"referenceID": 16, "context": "For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively. The medium pointer sentinel-LSTM model achieves lower perplexity than the large LSTM model of Gal (2015) while using a third of the parameters and without using the computationally expensive Monte Carlo (MC) dropout averaging at test time. Parameter numbers with \u2021 are estimates based upon our understanding of the model and with reference to Kim et al. (2016).", "startOffset": 33, "endOffset": 508}], "year": 2016, "abstractText": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinelLSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.1", "creator": "LaTeX with hyperref package"}}}