{"id": "1609.08194", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Online Segment to Segment Neural Transduction", "abstract": "We introduce an online neural sequence into the sequence model that learns to switch between encoding and decoding segments of input when reading. By independently tracking the encoding and decoding of representations, our algorithm enables precise polynomial marginalization of latent segmentation during training, and during decoding the beam search, the best alignment path is found along with the predicted output sequence. Our model deals with the bottleneck of vanilla encoder decoders that need to read and store the entire input sequence in their well-defined hidden states before they can be output. It differs from previous observant models in that our model does not treat attention weights as an output of a deterministic function, but assigns a sequential latent variable attention that can be marginalized and allows online generation experiments to show abstract marginal marginal insignation and base fraction.", "histories": [["v1", "Mon, 26 Sep 2016 21:13:49 GMT  (30kb)", "http://arxiv.org/abs/1609.08194v1", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["lei yu", "jan buys", "phil blunsom"], "accepted": true, "id": "1609.08194"}, "pdf": {"name": "1609.08194.pdf", "metadata": {"source": "CRF", "title": "Online Segment to Segment Neural Transduction", "authors": ["Lei Yu", "Jan Buys"], "emails": ["phil.blunsom}@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n08 19\n4v 1\n[ cs\n.C L\n] 2\n6 Se\nWe introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders."}, {"heading": "1 Introduction", "text": "The problem of mapping from one sequence to another is an importance challenge of natural language processing. Common applications include machine translation and abstractive sentence summarisation. Traditionally this type of problem has been tackled by a combination of hand-crafted features, alignment models, segmentation heuristics, and language models, all of which are tuned separately.\nThe recently introduced encoder-decoder paradigm has proved very successful for machine translation, where an input sequence is encoded into a fixed-length vector and an output sequence is then decoded from said vector (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). This architecture is appealing, as it makes it possible to tackle the problem of sequence-to-sequence mapping by training a large neural network in an end-to-end fashion. However it is difficult for a fixed-length vector to memorize all the necessary information of an input sequence, especially for long sequences. Often a very large encoding needs to be employed in order to capture the longest sequences, which invariably wastes capacity and computation for short sequences. While the attention mechanism of Bahdanau et al. (2015) goes some way to address this issue, it still requires the full input to be seen before any output can be produced.\nIn this paper we propose an architecture to tackle the limitations of the vanilla encoder-decoder model, a segment to segment neural transduction model (SSNT) that learns to generate and align simultaneously. Our model is inspired by the HMM word alignment model proposed for statistical machine translation (Vogel et al., 1996; Tillmann et al., 1997); we impose a monotone restriction on the alignments but incorporate recurrent dependencies on the input which enable rich locally non-monotone alignments to be captured. This is similar to the sequence transduction model of Graves (2012), but we propose alignment distributions which are parameterised separately, making\nthe model more flexible and allowing online inference.\nOur model introduces a latent segmentation which determines correspondences between tokens of the input sequence and those of the output sequence. The aligned hidden states of the encoder and decoder are used to predict the next output token and to calculate the transition probability of the alignment. We carefully design the input and output RNNs such that they independently update their respective hidden states. This enables us to derive an exact dynamic programme to marginalize out the hidden segmentation during training and an efficient beam search to generate online the best alignment path together with the output sequence during decoding. Unlike previous recurrent segmentation models that only capture dependencies in the input (Graves et al., 2006; Kong et al., 2016), our segmentation model is able to capture unbounded dependencies in both the input and output sequences while still permitting polynomial inference.\nWhile attentive models treat the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out. Our model is general and could be incorporated into any RNNbased encoder-decoder architecture, such as Neural Turing Machines (Graves et al., 2014), memory networks (Weston et al., 2015; Kumar et al., 2016) or stack-based networks (Grefenstette et al., 2015), enabling such models to process data online.\nWe conduct experiments on two different transduction tasks, abstractive sentence summarisation (sequence to sequence mapping at word level) and morphological inflection generation (sequence to sequence mapping at character level). We evaluate our proposed algorithms in both the online setting, where the input is encoded with a unidirectional LSTM, and where the whole input is available such that it can be encoded with a bidirectional network. The experimental results demonstrate the effectiveness of SSNT \u2014 it consistently output performs the baseline encoder-decoder approach while requiring significantly smaller hidden layers, thus showing that the segmentation model is able to learn to break one large transduction task into a series of smaller encodings and decodings. When bidirectional encodings are used the segmentation model\noutperforms an attention-based benchmark. Qualitative analysis shows that the alignments found by our model are highly intuitive and demonstrates that the model learns to read ahead the required number of tokens before producing output."}, {"heading": "2 Model", "text": "Let xI1 be the input sequence of length I and y J 1 the output sequence of length J . Let yj denote the jth token of y. Our goal is to model the conditional distribution\np(y|x) = J\u220f\nj=1\np(yj|y j\u22121 1 ,x). (1)\nWe introduce a hidden alignment sequence aJ1 where each aj = i corresponds to an input position i \u2208 {1, . . . , I} that we want to focus on when generating yj . Then p(y|x) is calculated by marginalizing over all the hidden alignments,\np(y|x) = \u2211\na p(y,a|x) (2)\n\u2248 \u2211\na \u220fJ j=1 p(aj |aj\u22121,y j\u22121 1 ,x)\n\ufe38 \ufe37\ufe37 \ufe38\ntransition probability\n\u00b7\np(yj|y j\u22121 1 , aj ,x). \ufe38 \ufe37\ufe37 \ufe38\nword prediction\nFigure 1 illustrates the model graphically. Each path from the top left node to the right-most column in the graph corresponds to an alignment. We constrain the alignments to be monotone, i.e. only forward and downward transitions are permitted at each point in the grid. This constraint enables the model to learn to perform online generation. Additionally, the model learns to align input and output segments, which means that it can learn local reorderings by memorizing phrases. Another possible constraint on the alignments would be to ensure that the entire input sequence is consumed before last output word is emitted, i.e. all valid alignment paths have to end in the bottom right corner of the grid. However, we do not enforce this constraint in our setup.\nThe probability contributed by an alignment is obtained by accumulating the probability of word predictions at each point on the path and the transition probability between points. The transition probabilities and the word output probabilities are modeled by neural networks, which are described in detail in the following sub-sections."}, {"heading": "2.1 Probabilities of Output Word Predictions", "text": "The input sentence x is encoded with a Recurrent Neural Network (RNN), in particular an LSTM (Hochreiter and Schmidhuber, 1997). The encoder can either be a unidirectional or bidirectional LSTM. If a unidirectional encoder is used the model is able to read input and generate output symbols online. The hidden state vectors are computed as\nh \u2192 i = RNN(h \u2192 i\u22121, v (e)(xi)), (3)\nh \u2190 i = RNN(h \u2190 i+1, v (e)(xi)), (4)\nwhere v(e)(xi) denotes the vector representation of the token x, and h\u2192i and h \u2190 i are the forward and backward hidden states, respectively. For a bidirectional encoder, they are concatenated as hi =\n[h\u2192i ;h \u2190 i ]; and for unidirectional encoder hi = h \u2192 i . The hidden state sj of the RNN for the output sequence y is computed as\nsj = RNN(sj\u22121, v (d)(yj\u22121)), (5)\nwhere v(d)(yj\u22121) is the encoded vector of the previously generated output word yj\u22121. That is, sj encodes yj\u221211 .\nTo calculate the probability of the next word, we concatenate the aligned hidden state vectors sj and haj and feed the result into a softmax layer,\np(yj = l|y j\u22121 1 , aj ,x)\n= p(yj = l|haj , sj) = softmax(Ww[haj ; sj ] + bw)l.\n(6)\nThe word output distribution in Graves (2012) is parameterised in similar way.\nFigure 2 illustrates the model structure. Note that the hidden states of the input and output decoders are kept independent to permit tractable inference, while the output distributions are conditionally dependent on both."}, {"heading": "2.2 Transition Probabilities", "text": "As the alignments are constrained to be monotone, we can treat the transition from timestep j to j+1 as a sequence of shift and emit operations. Specifically, at each input position, a decision of shift or emit is made by the model; if the operation is emit then the next output word is generated; otherwise, the model will shift to the next input word. While the multinomial distribution is an alternative for parameterising alignments, the shift/emit parameterisation does not place an upper limit on the jump size, as a multinomial distribution would, and biases the model towards shorter jump sizes, which a multinomial model would have to learn.\nWe describe two methods for modelling the alignment transition probability. The first approach is independent of the input or output words. To parameterise the alignment distribution in terms of shift and emit operations we use a geometric distribution,\np(aj|aj\u22121) = (1\u2212 e) aj\u2212aj\u22121e, (7)\nwhere e is the emission probability. This transition probability only has one parameter e, which can be\nestimated directly by maximum likelihood as\ne =\n\u2211\nn Jn\u2211 n In + \u2211 n Jn , (8)\nwhere In and Jn are the lengths of the input and output sequences of training example n, respectively.\nFor the second method we model the transition probability with a neural network,\np(a1 = i) = i\u22121\u220f\nd=1\n(1\u2212 p(ed,1))p(ei,1),\np(aj = i|aj\u22121 = k) =\ni\u22121\u220f\nd=k\n(1\u2212 p(ed,j))p(ei,j),\n(9)\nwhere p(ei,j) denotes the probability of emit for the alignment aj = i. This probability is obtained by feeding [hi; sj] into a feed forward neural network,\np(ei,j) = \u03c3(MLP(Wt[hi; sj ] + bt)). (10)\nFor simplicity, p(aj = i|aj\u22121 = k, sj ,hik) is abbreviated as p(aj = i|aj\u22121 = k)."}, {"heading": "3 Training and Decoding", "text": "Since there are an exponential number of possible alignments, it is computationally intractable to\nexplicitly calculate every p(y,a|x) and then sum them to get the conditional probability p(y|x). We instead approach the problem using a dynamicprogramming algorithm similar to the forwardbackward algorithm for HMMs (Rabiner, 1989)."}, {"heading": "3.1 Training", "text": "For an input x and output y, the forward variable \u03b1(i, j) = p(aj = i,y j 1|x). The value of \u03b1(i, j) is computed by summing over the probabilities of every path that could lead to this cell. Formally, \u03b1(i, j) is defined as follows:\nFor i \u2208 [1, I]:\n\u03b1(i, 1) = p(a1 = i)p(y1|hi, s1). (11)\nFor j \u2208 [2, J ], i \u2208 [1, I]:\n\u03b1(i, j) = p(yj|hi, sj)\u00b7 (12) i\u2211\nk=1\n\u03b1(k, j \u2212 1)p(aj = i|aj\u22121 = k).\nThe backward variables, defined as \u03b2(i, j) = p(yJj+1|aj = i,y j 1,x), are computed as:\nFor i \u2208 [1, I]:\n\u03b2(i, J) = 1. (13)\nFor j \u2208 [1, J \u2212 1], i \u2208 [1, I]:\n\u03b2(i, j) =\nI\u2211\nk=i\np(aj+1 = k|aj = i)\u03b2(k, j + 1)\u00b7\np(yj+1|hk, sj+1). (14)\nDuring training we estimate the parameters by minimizing the negative log likelihood of the training set S:\nL(\u03b8) = \u2212 \u2211\n(x,y)\u2208S\nlog p(y|x;\u03b8)\n= \u2212 \u2211\n(x,y)\u2208S\nlog\nI\u2211\ni=1\n\u03b1(i, J).\n(15)\nLet \u03b8j be the neural network parameters w.r.t. the model output at position j. The gradient is computed as:\n\u2202 log p(y|x;\u03b8)\n\u2202\u03b8 =\nJ\u2211\nj=1\nI\u2211\ni=1\n\u2202 log p(y|x;\u03b8)\n\u2202\u03b1(i, j)\n\u2202\u03b1(i, j)\n\u2202\u03b8j .\n(16)\nThe derivative w.r.t. the forward weights is\n\u2202 log p(y|x;\u03b8)\n\u2202\u03b1(i, j) =\n\u03b2(i, j)\np(y|x;\u03b8) . (17)\nThe derivative of the forward weights w.r.t. the model parameters at position j is\n\u2202\u03b1(i, j)\n\u2202\u03b8j =\n\u2202p(yj|hi, sj)\n\u2202\u03b8j\n\u03b1(i, j)\np(yj |hi, sj)\n+ p(yj|hi, sj)\ni\u2211\nk=1\n\u03b1(j \u2212 1, k) \u2202\n\u2202\u03b8j p(aj=i|aj\u22121=k).\n(18)\nFor the geometric distribution transition probability model \u2202\n\u2202\u03b8j p(aj = i|aj\u22121 = k) = 0."}, {"heading": "3.2 Decoding", "text": "Algorithm 1 DP search algorithm\nInput: source sentence x Output: best output sentence y\u2217 Initialization: Q \u2208 RI\u00d7Jmax , bp \u2208 NI\u00d7Jmax , W \u2208 NI\u00d7Jmax, Iend, Jend. for i \u2208 [1, I] do\nQ[i, 1]\u2190 maxy\u2208V p(a1 = i)p(y|hi, s1) bp[i, 1]\u2190 0 W [i, 1]\u2190 argmaxy\u2208V p(a1 = i)p(y|hi, s1)\nend for for j \u2208 [2, Jmax] do\nfor i \u2208 [1, I] do Q[i, j]\u2190 maxy\u2208V ,k\u2208[1,i]Q[k, j \u2212 1]\u00b7\np(aj = i|aj\u22121 = k)p(y|hi, sj) bp[i, j],W [i, j] \u2190 argmaxy\u2208V ,k\u2208[1,i] \u00b7 Q[k, j \u2212 1]p(aj = i|aj\u22121 = k)p(y|hi, sj)\nend for Iend \u2190 argmaxiQ[i, j] if W [Iend, j] = EOS then\nJend \u2190 j break\nend if end for return a sequence of words stored in W by following backpointers starting from (Iend, Jend).\nFor decoding, we aim to find the best output sequence y\u2217 for a given input sequence x:\ny\u2217 = argmax y p(y|x) (19)\nThe search algorithm is based on dynamic programming (Tillmann et al., 1997). The main idea is to create a path probability matrix Q, and fill each cell Q[i, j] by recursively taking the most probable path that could lead to this cell. We present the greedy search algorithm in Algorithm 1. We also implemented a beam search that tracks the k best partial sequences at position (i, j). The notation bp refers to backpointers, W stores words to be predicted, V denotes the output vocabulary, Jmax is the maximum length of the output sequences that the model is allowed to predict."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our model on two representative natural language processing tasks, sentence compression and morphological inflection. The primary aim of this evaluation is to assess whether our proposed architecture is able to outperform the baseline encoder-decoder model by overcoming its encoding bottleneck. We further benchmark our results against an attention model in order to determine whether our alternative alignment strategy is able to provide similar benefits while processing the input online."}, {"heading": "4.1 Abstractive Sentence Summarisation", "text": "Sentence summarisation is the task of generating a condensed version of a sentence while preserving its meaning. In abstractive sentence summarisation, summaries are generated from the given vocabulary without the constraint of copying words in the input sentence. Rush et al. (2015) compiled a data set for this task from the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), where sentence-summary pairs are obtained by pairing the headline of each article with its first sentence. Rush et al. (2015) use the splits of 3.8m/190k/381k for training, validation and testing. In previous work on this dataset, Rush et al. (2015) proposed an attention-based model with feed-forward neural networks, and Chopra et al. (2016) proposed an attention-based recurrent encoder-decoder, similar to one of our baselines.\nDue to computational constraints we place the following restrictions on the training and validation set:\n1. The maximum lengths for the input sentences\nand summaries are 50 and 25, respectively.\n2. For each sentence-summary pair, the product of the input and output lengths should be no greater than 500.\nWe use the filtered 172k pairs for validation and sample 1m pairs for training. While this training set is smaller than that used in previous work (and therefore our results cannot be compared directly against reported results), it serves our purpose for evaluating our algorithm against sequence to sequence and attention-based approaches under identical data conditions. Following from previous work (Rush et al., 2015; Chopra et al., 2016; Gu\u0308lc\u0327ehre et al., 2016), we report results on a randomly sampled test set of 2000 sentence-summary pairs. The quality of the generated summaries are evaluated by three versions of ROUGE for different match lengths, namely ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longestcommon substring).\nFor training, we use Adam (Kingma and Ba, 2015) for optimization, with an initial learning rate of 0.001. The mini-batch size is set to 32. The number of hidden units H is set to 256 for both our model and the baseline models, and dropout of 0.2 is applied to the input of LSTMs. All hyperparameters were optimised via grid search on\nthe perplexity of the validation set. We use greedy decoding to generate summaries.\nTable 1 displays the ROUGE-F1 scores of our models on the test set, together with baseline models, including the attention-based model. Our models achieve significantly better results than the vanilla encoder-decoder and outperform the attention-based model. The fact that SSNT+ performs better is in line with our expectations, as the neural network-parameterised alignment model is more expressive than that modelled by geometric distribution.\nTo make further comparison, we experimented with different sizes of hidden units and adding more layers to the baseline encoder-decoder. Table 2 lists the configurations of different models and their corresponding perplexities on the validation set. We can see that the vanilla encoder-decoder tends to get better results by adding more hidden units and stacking more layers. This is due to the limitation of compressing information into a fixed-size vector. It has to use larger vectors and deeper structure in order to memorize more information. By contrast, our model can do well with smaller networks. In fact, even with 1 layer and 128 hidden units, our model works much better than the vanilla encoder-decoder with 3 layers and 256 hidden units per layer."}, {"heading": "4.2 Morphological Inflection", "text": "Morphological inflection generation is the task of predicting the inflected form of a given lexical item based on a morphological attribute. The transformation from a base form to an inflected form usually includes concatenating it with a prefix or a suffix and substituting some characters. For example, the inflected form of a German stem abgang is abga\u0308ngen\nwhen the case is dative and the number is plural. In our experiments, we use the same dataset as Faruqui et al. (2016). This dataset was originally created by Durrett and DeNero (2013) from Wiktionary, containing inflections for German nouns (de-N), German verbs (de-V), Spanish verbs (es-V), Finnish noun and adjective (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (nl-V) and French verbs (fr-V). The number of inflection types for each language ranges from 8 to 57. The number of base forms, i.e. the number of instances in each dataset, ranges from 2000 to 11200. The predefined split is 200/200 for dev and test sets, and the rest of the data for training.\nOur model is trained separately for each type of inflection, the same setting as the factored model described in Faruqui et al. (2016). The model is trained to predict the character sequence of the inflected form given that of the stem. The output is evaluated by accuracies of string matching. For all the experiments on this task we use 128 hidden units for the LSTMs and apply dropout of 0.5 on the input and output of the LSTMs. We use Adam (Kingma and Ba, 2015) for optimisation with initial learning rate of 0.001. During decoding, beam search is employed with beam size of 30.\nTable 3 gives the average accuracy of the uniSSNT+, biSSNT+, vanilla encoder-decoder, and attention-based models. The model with the best previous average result \u2014 denoted as adaptedseq2seq (FTND16) (Faruqui et al., 2016) \u2014 is also included for comparison. Our biSSNT+ model outperforms the vanilla encoder-decoder by a large margin and almost matches the state-of-the-art result\non this task. As mentioned earlier, a characteristic of these datasets is that the stems and their corresponding inflected forms mostly overlap. Compare to the vanilla encoder-decoder, our model is better at copying and finding correspondences between prefix, stem and suffix segments.\nTable 4 compares the results of biSSNT+ and previous models on each individual dataset. DDN13 and NCK15 denote the models of Durrett and DeNero (2013) and Nicolai et al. (2015), respectively. Both models tackle the task by feature engineering. FTND16 (Faruqui et al., 2016) adapted the vanilla encoderdecoder by feeding the i-th character of the encoded string as an extra input into the i-th position of the decoder. It can be considered as a special case of our model by forcing a fixed diagonal alignment between input and output sequences. Our model achieves comparable results to these models on all the datasets. Notably it outperforms other models on the Finnish noun and adjective, and verbs datasets, whose stems and inflected forms are the longest."}, {"heading": "5 Alignment Quality", "text": "Figure 3 presents visualisations of segment alignments generated by our model for sample instances from both tasks. We see that the model is able to learn the correct correspondences between segments of the input and output sequences. For instance, the alignment follows a nearly diagonal path for the example in Figure 3c, where the input and output sequences are identical. In Figure 3b, it learns to add\nthe prefix \u2018ge\u2019 at the start of the sequence and replace \u2018en\u2019 with \u2018t\u2019 after copying \u2018zock\u2019. We observe that the model is robust on long phrasal mappings. As shown in Figure 3a, the mapping between \u2018the wall street journal asia, the asian edition of the us-based business daily\u2019 and \u2018wall street journal asia\u2019 demonstrates that our model learns to ignore phrasal modifiers containing additional information. We also find some examples of word reordering, e.g., the phrase \u2018industrial production in france\u2019 is reordered as \u2018france industrial output\u2019 in the model\u2019s predicted output."}, {"heading": "6 Related Work", "text": "Our work is inspired by the seminal HMM alignment model (Vogel et al., 1996; Tillmann et al., 1997) proposed for machine translation. In contrast to that work, when predicting a target word we additionally condition on all previously generated words, which is enabled by the recurrent neural models. This means that the model also functions as a conditional language model. It can therefore be applied directly, while traditional\nmodels have to be combined with a language model through a noisy channel in order to be effective. Additionally, instead of EM training on the most likely alignments at each iteration, our model is trained with direct gradient descent, marginalizing over all the alignments.\nLatent variables have been employed in neural network-based models for sequence labelling tasks in the past. Examples include connectionist temporal classification (CTC) (Graves et al., 2006) for speech recognition and the more recent segmental recurrent neural networks (SRNNs) (Kong et al., 2016), with applications on handwriting recognition and part-of-speech tagging. Weighted finite-state transducers (WFSTs) have also been augmented to encode input sequences with bidirectional LSTMs (Rastogi et al., 2016), permitting exact inference over all possible output strings. While these models have been shown to achieve appealing performance on different applications, they have common limitations in terms of modelling dependencies between labels. It is not possible for CTCs to model explicit dependencies. SRNNs\nand neural WFSTs model fixed-length dependencies, making it is difficult to carry out effective inference as the dependencies become longer.\nOur model shares the property of the sequence transduction model of Graves (2012) in being able to model unbounded dependencies between output tokens via an output RNN. This property makes it possible to apply our model to tasks like summarisation and machine translation that require the tokens in the output sequence to be modelled highly dependently. Graves (2012) models the joint distribution over outputs and alignments by inserting null symbols (representing shift operations) into the output sequence. During training the model uses dynamic programming to marginalize over permutations of the null symbols, while beam search is employed during decoding. In contrast our model defines a separate latent alignment variable, which adds flexibility to the way the alignment distribution can be defined (as a geometric distribution or parameterised by a neural network) and how the alignments can be constrained, without redefining the dynamic program. In addition to marginalizing during training, our decoding algorithm also makes use of dynamic programming, allowing us to use either no beam or small beam sizes.\nOur work is also related to the attentionbased models first introduced for machine translation (Bahdanau et al., 2015). Luong et al. (2015) proposed two alternative attention mechanisms: a global method that attends all words in the input sentence, and a local one that points to parts of the input words. Another variation on this theme are pointer networks (Vinyals et al., 2015), where the outputs are pointers to elements of the variablelength input, predicted by the attention distribution. Jaitly et al. (2016) propose an online sequence to sequence model with attention that conditions on fixed-sized blocks of the input sequence and emits output tokens corresponding to each block. The model is trained with alignment information to generate supervised segmentations.\nAlthough our model shares the same idea of joint training and aligning with the attention-based models, our design has fundamental differences and advantages. While attention-based models treat the attention weights as output of a deterministic function (soft-alignment), in our model the attention\nweights correspond to a hidden variable, that can be marginalized out using dynamic programming. Further, our model\u2019s inherent online nature permits it the flexibility to use its capacity to chose how much input to encode before decoding each segment."}, {"heading": "7 Conclusion", "text": "We have proposed a novel segment to segment neural transduction model that tackles the limitations of vanilla encoder-decoders that have to read and memorize an entire input sequence in a fixed-length context vector before producing any output. By introducing a latent segmentation that determines correspondences between tokens of the input and output sequences, our model learns to generate and align jointly. During training, the hidden alignment is marginalized out using dynamic programming, and during decoding the best alignment path is generated alongside the predicted output sequence. By employing a unidirectional LSTM as encoder, our model is capable of doing online generation. Experiments on two representative natural language processing tasks, abstractive sentence summarisation and morphological inflection generation, showed that our model significantly outperforms encoderdecoder baselines while requiring much smaller hidden layers. For future work we would like to incorporate attention-based models to our framework to enable such models to process data online."}, {"heading": "Acknowledgments", "text": "We thank Chris Dyer, Karl Moritz Hermann, Edward Grefenstette, Toma\u0301s\u030c Ko\u030ccisky\u0301, Gabor Melis, Yishu Miao and many others for their helpful comments. The first author is funded by EPSRC."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Alexander M", "author": ["Sumit Chopra", "Michael Auli"], "venue": "Rush.", "citeRegEx": "Chopra et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Durrett", "DeNero2013] Greg Durrett", "John DeNero"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Graham Neubig", "author": ["Manaal Faruqui", "Yulia Tsvetkov"], "venue": "and Chris Dyer.", "citeRegEx": "Faruqui et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ke Chen", "author": ["David Graff", "Junbo Kong"], "venue": "and Kazuaki Maeda.", "citeRegEx": "Graff et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Faustino Gomez", "author": ["Alex Graves", "Santiago Fern\u00e1ndez"], "venue": "and J\u00fcrgen Schmidhuber.", "citeRegEx": "Graves et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Greg Wayne", "author": ["Alex Graves"], "venue": "and Ivo Danihelka.", "citeRegEx": "Graves et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Bowen Zhou", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sungjin Ahn", "Ramesh Nallapati"], "venue": "and Yoshua Bengio.", "citeRegEx": "G\u00fcl\u00e7ehre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ilya Sutskever", "author": ["Navdeep Jaitly", "David Sussillo", "Quoc V. Le", "Oriol Vinyals"], "venue": "and Samy Bengio.", "citeRegEx": "Jaitly et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICIR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Chris Dyer", "author": ["Lingpeng Kong"], "venue": "and Noah A Smith.", "citeRegEx": "Kong et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ishaan Gulrajani", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska"], "venue": "and Richard Socher.", "citeRegEx": "Kumar et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Christopher D", "author": ["Thang Luong", "Hieu Pham"], "venue": "Manning.", "citeRegEx": "Luong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Matthew Gormley", "author": ["Courtney Napoles"], "venue": "and Benjamin Van Durme.", "citeRegEx": "Napoles et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Colin Cherry", "author": ["Garrett Nicolai"], "venue": "and Grzegorz Kondrak.", "citeRegEx": "Nicolai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Ryan Cotterell", "author": ["Pushpendre Rastogi"], "venue": "and Jason Eisner.", "citeRegEx": "Rastogi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sumit Chopra", "author": ["Alexander M. Rush"], "venue": "and Jason Weston.", "citeRegEx": "Rush et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hermann Ney", "author": ["Christoph Tillmann", "Stephan Vogel"], "venue": "and Alex Zubiaga.", "citeRegEx": "Tillmann et al.1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hermann Ney", "author": ["Stephan Vogel"], "venue": "and Christoph Tillmann.", "citeRegEx": "Vogel et al.1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "creator": "LaTeX with hyperref package"}}}