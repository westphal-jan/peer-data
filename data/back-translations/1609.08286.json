{"id": "1609.08286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Online Unsupervised Multi-view Feature Selection", "abstract": "In the age of big data, it is common to have data with multiple modalities, or to come from multiple sources known as \"multi-view data.\" Multi-view data is usually unlabeled and comes from high-dimensional spaces (such as speech vocabulary), so unattended multi-view feature selection is critical for many applications. However, it is not trivial due to the following challenges. First, there are too many instances or the feature dimensionality is too large. Therefore, the data may not fit into the memory. How to select useful features with limited storage space? Second, how to select features from streaming data and master the concept drift. Third, how to use consistent and complementary information from different perspectives to improve feature selection in the situation when the data is too large or come as streams? To get the most out of our knowledge, none of the previous work can solve all the challenges simultaneously.", "histories": [["v1", "Tue, 27 Sep 2016 07:10:16 GMT  (1032kb)", "http://arxiv.org/abs/1609.08286v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weixiang shao", "lifang he", "chun-ta lu", "xiaokai wei", "philip s yu"], "accepted": false, "id": "1609.08286"}, "pdf": {"name": "1609.08286.pdf", "metadata": {"source": "CRF", "title": "Online Unsupervised Multi-view Feature Selection", "authors": ["Weixiang Shao", "Lifang He", "Chun-Ta Lu", "Xiaokai Wei", "Philip S. Yu"], "emails": ["psyu}@uic.edu", "lifanghescut@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n08 28\n6v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n16\nIn this paper, we propose an Online unsupervised MultiView Feature Selection, OMVFS, which deals with largescale/streaming multi-view data in an online fashion. OMVFS embeds unsupervised feature selection into a clustering algorithm via nonnegative matrix factorizatio with sparse learning. It further incorporates the graph regularization to preserve the local structure information and help select discriminative features. Instead of storing all the historical data, OMVFS processes the multi-view data chunk by chunk and aggregates all the necessary information into several small matrices. By using the buffering technique, the proposed OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, OMVFS can capture the concept drifts in the data streams. Extensive experiments on four real-world datasets show the effectiveness and efficiency of the proposed OMVFS method. More importantly, OMVFS is about 100 times faster than the off-line methods.\nI. INTRODUCTION\nIn many real-world applications, data are often with multiple modalities or coming from multiple sources. Such data are called multi-view data. For example, in web image retrieval, the visual information of images and their textual tags can be regarded as two views; in web document clustering, one web document may be translated into different languages, each language can be seen as one view. Usually, multiple views provide complementary information for the semantically same data. Multi-view learning was proposed to combine different views to obtain better performance than relying on just one single view [1]. However, many of the views may come from high-dimensional spaces (such as language vocabularies).\nThus, accurate unsupervised feature selection on these highdimensional multi-view data is crucial to many applications such as model interpretation and storage reduction.\nFeature selection has been studied for decades [2]. It can be categorized into supervised feature selection and unsupervised feature selection in terms of the label availability. Supervised feature selection [3] uses the class labels to effectively select discriminative features to distinguish samples from different classes. As the data explodes, most of the data are unlabeled and expensive to obtain the labels. Recently, several approaches on unsupervised feature selection has been proposed [4], [5]. Without the label information, most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7]. In this paper, we will only focus on the unsupervised feature selection.\nMost recently, as complementary information can be obtained from different views, unsupervised multi-view feature selection has drawn lots of attention [8]\u2013[10]. For example, [9] is the first to use spectral clustering and \u21132,1-norm regression to multi-view data in social media. [8] integrates all features and learns the weights for every feature with respect to each cluster individually via a new joint structured sparsity-inducing norms. For image and text data, [10] uses image local learning regularized orthogonal nonnegative matrix factorization to learn pseudo labels and simultaneously perform robust joint \u21132,1-norm minimization to select discriminative features.\nHowever, several challenges prevent us from applying existing unsupervised feature selection methods to the real-world multi-view data:\n1) As information explodes, multi-view data may contain too many instances or the feature dimensionality may be too large, such that the data cannot fit in memory. How to select useful features with limited memory space is the first challenge. 2) In many real-world applications, the data may come in as streams and concept drift [11] may happen. How to select features from streaming data and handles the concept drift is the second challenge. 3) Multi-view data always exhibits the heterogeneity of the features. Different views may share some consistent and complementary information. The third challenge is how to combine features from different views while take advantages of the consistent and complementary information to improve the feature selection in the\nsituation when the data are too big or come in as streams.\nTo the best of our knowledge, all the existing feature selection methods only focus on one or two of the challenges. For example, [8]\u2013[10] only solve unsupervised feature selection problem for multi-view data. [12]\u2013[14] solve the problem of feature selection on large-scale/streaming data in a single view. None of them can solve all the challenges simultaneously.\nIn this paper, we propose a novel method, Online unsupervised Multi-View Feature Selection (OMVFS), to solve the problem of multi-view feature selection on largescale/streaming data. OMVFS embeds the unsupervised feature selection into the Nonnegative Matrix Factorization (NMF) based clustering objective function. It further adopts the graph regularization to preserve the local structure information and help select discriminative features. By learning a consensus clustering indicator matrix, the proposed OMVFS integrates all the views in different feature spaces. Instead of storing all the historical data, OMVFS processes the multiview data chunk by chunk and aggregates necessary information from all the previous data into several small matrices. These aggregated matrices will be used in the learning of feature selection matrices and be updated as new data come in. The contributions of this paper can be summarized as following:\n1) The proposed OMVFS method is the first attempt to solve the problem of online unsupervised multi-view feature selection on large-scale/streaming data. 2) OMVFS can process the data chunks in an online fashion and aggregate information about all the previous data into small matrices. The aggregated information can be used to help feature selection in the future. Thus, the proposed method can greatly reduce the memory requirement and scale up to large data without appreciable sacrifice of performance. 3) By using the buffering technique, OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, it can capture the concept drifts in the data streams. 4) Through extensive experiments on real-world datasets, we demonstrate that the effectiveness of proposed OMVFS is comparative and even better than the best off-line method. More importantly, OMVFS is about 100 times faster than the off-line methods.\nThe rest of this paper is organized as follows. In the next section, problem formulation and some backgrounds are given. The details of the proposed OMVFS method are presented in Sections III, and IV. Extensive experimental results and analysis are shown in Section V. Related work is discussed in Section VI and followed by the conclusion in Section VII."}, {"heading": "II. PRELIMINARIES", "text": "In this section, we will briefly describe the problem of unsupervised multi-view feature selection. Then some background knowledge about unsupervised feature selection will be introduced."}, {"heading": "A. Problem Description", "text": "Before we describe the formulation of the problem, we summarize some notations used in this paper in Table I.\nThroughout this paper, matrices are written as boldface capital letters (e.g., M \u2208 Rn\u00d7m) and vectors are denoted as boldface lowercase letters (e.g., mi). \u2016\u00b7\u2016F is the matrix Frobenius norm and Tr(\u00b7) is the trace of a square matrix. The \u21132,1-norm is defined as \u2016M\u20162,1 = \u2211n i=1 mi = \u2211n i=1 \u221a \u2211m j=1 M 2 i,j .\nAssume we are given a dataset with N instances in nv views {X(v), v = 1, 2, ..., nv}, where X(v) \u2208 R N\u00d7Dv + represents the nonnegative data in the v-th view and Dv is the feature dimension in the v-th view. Our goal is to leverage complementary information from multi-view data to select pv features from the v-th view, while dealing with high-dimensional and largescale problems in an online fashion."}, {"heading": "B. Unsupervised Feature Selection using NMF", "text": "Nonnegative Matrix Factorization (NMF) has been widely used in unsupervised learning such as clustering and dimension reduction. In this paper, we embed the feature selection into a NMF based clustering algorithm. Let X \u2208 RN\u00d7D+ denote a nonnegative data matrix, where each row represents an instance and each column represents one normalized attribute (each column \u2016X.,i\u20162 = 1). Assume we would like to cluster the data into K clusters, NMF will factorize the data matrix X into two nonnegative matrices. We denote the two nonnegative matrices factors as U \u2208 RN\u00d7K+ and V \u2208 R D\u00d7K + . The objective function for NMF can be formulated as below:\nmin U,V\nL = \u2016X\u2212UVT \u20162F\ns.t. U \u2265 0,V \u2265 0. (1)\nIn practice, for clustering problem, an orthogonality constraint is usually added to U [15]\u2013[17]. Thus, Eq. (1) can be rewritten as:\nmin U,V\n\u2016X\u2212UVT \u20162F\ns.t. UTU = I,U \u2265 0,V \u2265 0. (2)\nThe orthogonality constraint on U can be seen as a relaxed form from the original clustering indicator constraint, where U \u2208 {0, 1}N\u00d7K,UT1 = 1.\nAnother advantage of this orthogonality constraint on U is to allow us to perform feature selection using V. It has been proved that the \u21132-norm of the rows in V represents the importance of the features in X regarding to the reconstruction\nerror [18]. Thus, we can add a selection matrix diag(p) to X and V to only select the important features that minimize the reconstruction error:,\nmin U,V\n\u2016Xdiag(p)\u2212U(diag(p)V)T \u20162F\ns.t. UTU = I,U \u2265 0,V \u2265 0\np \u2208 {0, 1}D,pT1 = r,\n(3)\nwhere r is the number of selected feature, p is the indicator vector, where pi = 1 indicates that the i-th feature is selected.\nNow, Eq. (3) becomes a mixed integer programming, which is very difficult to solve. [18] proposed to relax Eq. (3) to the following problem and proved the equivalent between Eq. (3) and Eq. (4):\nmin U,V\n\u2016X\u2212UVT \u20162F + \u03b2\u2016V\u20162,1\ns.t. UTU = I,U \u2265 0,V \u2265 0, (4)\nwhere \u03b2 is the parameter to control the sparsity of V. By adding the \u21132,1-norm on V, we force some of the rows in V close to 0. Thus, we can achieve feature selection by sorting the features according to the row norms of V in descending order, and selecting the top ranked ones. To facilitate discussions, we call U the cluster indicator matrix, and V the feature selection matrix."}, {"heading": "III. METHOD", "text": "The proposed online unsupervised multi-view feature selection is based on nonnegative matrix factorization and it processes the multi-view data chunk by chunk and aggregates all the historical information into small matrices with low computational and storage complexity. We will first describe how to derive the objective function."}, {"heading": "A. Objective of OMVFS", "text": "Given data in nv views {X(v) \u2208 R N\u00d7Dv + , v = 1, 2, ..., nv}, we aim to find a feature selection matrix for each view and a consensus cluster indicator matrix, which integrates information of all the views. Following the constrained NMF framework in Section II-B, we can form the objective function\nmin U,{V(v)}\nnv \u2211\nv=1\n(\n\u2016X(v) \u2212UV(v) T \u20162F + \u03b2v\u2016V (v)\u20162,1\n)\ns.t. UTU = I,U \u2265 0,V(v) \u2265 0, v = 1, 2, ..., nv,\n(5)\nwhere U is the consensus cluster indicator matrix, V(v) is the feature selection matrix for the v-th view, and \u03b2v is the parameter that controls the sparsity of V(v).\nTo take advantage of local manifold information from the structure of the original data {X(v)}, i.e., similar data instances should have similar labels, we add the spectral clustering objective term for every view\nmin Tr(U(v) T L(v)U(v)) (6)\nwhere L(v) = Dw (v)\u2212W(v) is the Laplacian matrix for the vth view, W(v) \u2208 RN\u00d7N is the similarity matrix based on X(v) and Dw (v) is a diagonal matrix with its diagonal elements as\nthe row sums of W(v). Now, we can obtain the objective function for OMVFS as:\nmin U,{V(v)}\nnv \u2211\nv=1\n(\n\u2016X(v) \u2212UV(v) T \u20162F + \u03b1vtr(U TL(v)U) + \u03b2v\u2016V (v)\u20162,1\n)\ns.t. UTU = I,U \u2265 0,V(v) \u2265 0, v = 1, 2, ..., nv\n(7)\nwhere \u03b1v is the importance of the spectral clustering term for the v-th view.\nIt is worth noting that in order to solve the problem in Eq. (7), we need to have the entire data {X(v)} in memory. However, in real-world applications, the data may be too large to fit into the memory or may only come on stream. Hence, it is crucial to solve the above optimization problem in an incremental way. Let X(v)t \u2208 R m\u00d7Dv + denote the data received at time t in the v-th view, and X(v)[t] \u2208 R mt\u00d7Dv + denote all the data received up to time t, where m is the number of instances (size of the data chunk) received at each time. The objective function at time t is:\nmin U[t],{V(v)}\nnv \u2211\nv=1\n(\n\u2016X (v) [t] \u2212U[t]V (v)T \u20162F + \u03b1vTr(U T [t]L (v) [t] U[t])\n)\n+\nnv \u2211\nv=1\n\u03b2v\u2016V (v)\u20162,1\ns.t. U[t] TU[t] = I,U[t] \u2265 0,V (v) \u2265 0, v = 1, 2, ..., nv\n(8)\nwhere U[t] \u2208 R mt\u00d7K + is the consensus cluster indicator matrix for all the instances received up to time t and L(v)[t] \u2208 R mt\u00d7mt is the Laplacian matrix constructed from X(v)[t] ."}, {"heading": "B. Optimization", "text": "In the previous section, we derived the objective function of OMVFS. To solve OMVFS, we first rewrite the optimization problem as follows\nmin U[t],{V(v)}\nnv \u2211\nv=1\n(\n\u2016X (v) [t] \u2212U[t]V (v)T \u20162F + \u03b1vTr(U T [t]L (v) [t] U[t])\n)\n+\nnv \u2211\nv=1\n\u03b2v\u2016V (v)\u20162,1 + \u03b3\u2016U T [t]U[t] \u2212 I\u2016 2 F\ns.t. U[t] \u2265 0,V (v) \u2265 0, v = 1, 2, ..., nv\n(9)\nwhere \u03b3 > 0 is a parameter to control the orthogonality condition. In practice, \u03b3 should be large enough to ensure the orthogonality is satisfied (\u03b3 is set to 107 throughout the experiments). From Eq. (9), we can see that at each time t, we need to optimize U[t] and {V(v)} nv v=1. However, the objective function is not jointly convex, so we have to update U[t] and {V(v)} nv v=1 in an alternating way. Thus, there are two subproblems in OMVFS: 1) Optimize U[t] with {V(v)} nv v=1 Fixed: To optimize U[t] with {V(v)}nvv=1 fixed at time t, we only need to minimize the following objective:\nJt(U[t]) =\nnv \u2211\nv=1\n(\n\u2016X (v) [t] \u2212U[t]V (v)T \u20162F + \u03b1vtr(U T [t]L (v) [t] U[t])\n)\n+ \u03b3\u2016UT[t]U[t] \u2212 I\u2016 2 F\ns.t. U[t] \u2265 0 (10)\nTaking the first-order derivative, the gradient of Jt with respect to U[t] is\n\u2202Jt \u2202U[t] =2U[t]\nnv \u2211\nv=1\nV(v) T V(v) \u2212 2\nnv \u2211\nv=1\nX (v) [t] V (v) [t]\n+ 2M[t]U[t] + 2\u03b3T[t] \u2212 2\u03b3U[t]\n(11)\nwhere, M[t] = \u2211nv v=1 \u03b1vL (v) [t] and T[t] = U[t]U T [t]U[t].\nUsing the Karush-Kuhn-Tucker (KKT) complementary condition for the nonnegativity constraint on U[t], we can get the update rule for U[t] [16], [19]:\n(U[t])i,k \u2190 (U[t])i,k\n\u221a \u221a \u221a \u221a ( \u2211nv v=1 X (v) [t] V (v) + \u03bbU[t] +M \u2212 [t]U[t])i,k\n(U[t] \u2211nv v=1 V (v)TV(v) + \u03b3T[t] +M + [t]U[t])i,k\n(12) where (M+[t])i,j = 1 2 ( \u2016(M[t])i,j\u2016+ (M[t])i,j )\n, (M\u2212[t])i,j = 1 2 ( \u2016(M[t])i,j\u2016 \u2212 (M[t])i,j ) and M[t] = M + [t] \u2212M \u2212 [t].\n2) Optimize {V(v)}nvv=1 with U[t] Fixed: From Eq. (9), we can observe that the optimization of V(v) is independent with different v when U[t] is fixed. We only need to minimize the following objective function for the v-th view:\nJt(V (v)) = \u2016X (v) [t] \u2212U[t]V (v)T \u20162F + \u03b2v\u2016V (v)\u20162,1\ns.t. V(v) \u2265 0 (13)\nWe observe that the above objective Jt can be decomposed as:\nJt(V (v)) =\nt \u2211\ni=1\n\u2016X (v) i \u2212UiV (v)T \u20162F + \u03b2v\u2016V (v)\u20162,1\ns.t. V(v) \u2265 0\n(14)\nwhere X(v)i is the data chunk received at time i and Ui is the cluster indicator matrix for data received at time i.\nTaking the first-order derivative, the gradient of Jt with respect to V(v) is\n\u2202Jt \u2202V(v)\n= 2V(v) t \u2211\ni=1\nUTi Ui \u2212 2\nt \u2211\ni=1\nX (v) i\nT\nUi + \u03b2vD (v)V(v)\n(15) where D(v) is a diagonal matrix with the j-th diagonal element given by D(v)j,j = 1\n\u2016v (v) j \u20162 and v(v)j is the j-th row of V (v) and\n\u2016 \u00b7 \u20162 is the \u21132 norm. For the sake of convenience, we introduce two terms At and B(v)t :\nAt =\nt \u2211\ni=1\nUTi Ui = At\u22121 +U T t Ut (16)\nB (v) t =\nt \u2211\ni=1\nX (v) i\nT\nUi = B (v) t\u22121 +X (v) t\nT\nUt (17)\nBoth At and B (v) t can be computed incrementally with low\nstorage. Thus, Eq. 15 can be rewritten as\n\u2202Jt \u2202V(v) =2V(v)(At\u22121 +U T t Ut)\n\u2212 2(B (v) t\u22121 +X (v) t\nT\nUt) + \u03b2vD (v)V(v)\n(18)\nUsing the KKT complementary condition for the nonnegativity constraint on V(v), we get the update rule for V(v):\nV (v) j,k \u2190 V (v) j,k\n\u221a \u221a \u221a \u221a \u221a \u221a\n(\nB (v) t\u22121 +X (v) t\nT\nUt\n)\nj,k (\nV(v)(At\u22121 +UTt Ut) + 1 2\u03b2vD (v)V(v) ) j,k\n(19)"}, {"heading": "C. Further Optimization Using Buffering", "text": "From the update rules (12), we observe that the update process for U[t] still needs all the data and all the cluster indicator matrices up to time t to reside in the memory. Thus, the memory usage will continue growing as more data come in. Also, at each time t, we need to calculate the new similarity matrix W(v)[t] in order to get the Laplacian matrix L (v) [t] . Even if we only calculate the similarity between the m new instances and all the mt instances, the time complexity will still increase as the data pile up. What\u2019s worse, the size of L(v)[t] (mt \u00d7 mt) will grow quadratically. To overcome this deficiency, we adopt the buffering technique by keeping a limited number of samples for each update step. Another benefit of adopting buffering is that, by only considering the structure information in the buffer, we can capture the concept drift in streaming data. The intuition behind buffering is based on the assumption regarding to streaming data:\nASSUMPTION 1: The incoming data are more related with the recent data than the far more old data, and the underline concept distribution of incoming data are more similar to those of recent data.\nThus, we consider the graph/structure information only within a time window. Assume that the buffer size is s, i.e., we only consider the most recent s data chunks received, we define X(v)[s,t] = [X (v) t\u2212s+1;X (v) t\u2212s+2; . . . ;X (v) t ] \u2208 R sm\u00d7Dv + as the most recent s data chunks received up to time t and U[s,t] = [Ut\u2212s+1;Ut\u2212s+2; . . . ;Ut] \u2208 R sm\u00d7K + as the cluster indicator matrix for them. W(v)[s,t] \u2208 R sm\u00d7sm and L (v) [s,t] \u2208 R sm\u00d7sm are the similarity matrix constructed from the data in the buffer and the Laplacian matrix from W(v)[s,t]. Note that, for the similarity matrix W(v)[s,t], we do not need to recompute the similarities among all the sm instances (sm \u00d7 sm pairwise similarities). We only need to calculate the similarities between the m new instances with all the sm instances (m\u00d7sm pairwise similarities). Thus, we can redefine the objective function (10) by substituting X(v)[t] , U[t] and L (v) [t] with the above new definitions. The new update rule for U[s,t] is\n(U[s,t])i,k \u2190 (U[s,t])i,k\n\u221a \u221a \u221a \u221a ( \u2211nv v=1 X (v) [s,t]V (v) + \u03bbU[s,t] +M \u2212 [s,t]U[s,t])i,k\n(U[s,t] \u2211nv v=1 V (v)TV(v) + \u03b3T[s,t] +M + [s,t]U[s,t])i,k\n(20) For the feature selection matrices {V(v)}nvv=1, we fortu-\nnately observe that At and {B (v) t } nv v=1 accumulate the information of all the previous t chunks. Therefore, we can still use update rule (19) without any modification, which means that\nemploying the buffering technique has no effect on updating the feature selection matrices."}, {"heading": "IV. ONLINE UNSUPERVISED MULTI-VIEW FEATURE SELECTION", "text": "In this section we will present the whole OMVFS method. Fig. 1 shows the workflow of the OMVFS method at time t. From Fig. 1 we can see that, at time t, OMVFS takes At\u22121, {B\n(v) t\u22121} nv v=1, W[s,t\u22121], and {X v [s,t\u22121]} nv v=1 from time t\u22121 and combines with the incoming data {Xvt } nv v=1 to update all the matrices. It is worth mentioning that OMVFS does not need to store all the previous data. Instead, it only stores some small buffered data {Xv[s,t\u22121]} nv v=1 and W[s,t\u22121]. All the necessary information from the previous data is aggregated and stored in small matrices At\u22121 and {B (v) t\u22121} nv v=1. The computation of these aggregated matrices can be done efficiently and incrementally. The complete algorithm procedure is shown in Algorithm 1. There are several things need to be clarified.\nAlgorithm 1: OMVFS algorithm.\nInput: Data matrices {X(v)}. The number of clusters K, the batch size m, the buffer size s. Parameters {\u03b1v} and {\u03b2v}. Output: Feature selection matrices {Vv}.\n1 Initialize V(v) randomly for each view v. 2 Initialize X(v)[s,0], U[s,0], W (v) [s,0] as empty matrices. 3 A (v) 0 = 0, B (v) 0 = 0 for each view v. 4 for t = 1 : \u2308N/m\u2309 do 5 Draw X(v)t for all the views. 6 Initialize Ut randomly. 7 for v = 1 : nv do 8 Construct X(v)\n[s,t] from X(v) [s,t\u22121] .\n9 Construct U[s,t] from U[s,t\u22121]. 10 Construct W(v)[s,t] and L (v) [s,t] from W (v) [s,t\u22121].\n11 repeat 12 Update U[s,t] according to Eq. (20). 13 for v = 1 : nv do 14 Update V(v) according to Eq. (19).\n15 until Convergence; 16 At = At\u22121 +U T t Ut 17 B (v) t = B (v) t\u22121 +X (v) t T Ut 18 for v = 1 : nv do 19 Sort the features for X(v) according to the \u21132-norm of the\nrows in V(v) in descending order.\nFirst, at the beginning of the algorithm (t = 0), OMVFS will initialize the buffered data X(v)[s,t], the cluster indicator matrix for buffered data U[s,t] and similarity matrix for buffered data W (v) [s,t] with empty matrices. The feature selection matrices {V(v)} will also be initialized randomly at t = 0. Second, as the data come in, if the buffer is full, OMVFS will remove the oldest data (e.g., X(v)t\u2212s), and concatenate the new data to the buffer. The procedure for updating the matrices in the buffer is shown in Fig. 2. The grey shadow areas are the parts for the oldest data, and the red shadow areas are the parts for the incoming new data."}, {"heading": "A. Convergence Analysis", "text": "Although the objective function for the proposed OMVFS at time t is not jointly convex with U[s,t] and {V(v)}, the alternating optimization strategy used in OMVFS is guaranteed to converge to the local minimum [20]. The proof is similar to that of Theorem 1 in [21] and we omit it. It is important to note that our method essentially applies stochastic gradient descent [22] to the new coming data. Since we perform a stochastic approximation for minimizing an objective function that is written as a sum of differentiable functions. This method is expected to decrease the objective function in every iteration on the new data [23]. Therefore, the same convergence proof can be adapted to the problem setting and algorithm design that considered here. In the experiments, we also find that the proposed OMVFS method converges within 200 iterations for all the datasets used."}, {"heading": "B. Complexity Analysis", "text": "There are two subproblems for OMVFS algorithm: optimizing U[s,t], and optimizing {V(v)}. The computation cost for updating U[s,t] depends on the calculation in Eq. (20). After analyzing the equation, we can find that the computational cost to update U[s,t] is O(nvsmDK)+O(nvs2m2K), where s is the buffer size, m is the size of data chunk, and D is the average feature dimension for all the views. Since D is usually very large, we can assume that D \u226b sm. Thus, the computational cost for updating U[s,t] is O(nvsmDK).\nThe compuational cost for updating V(v) depends on Eq. (19). The computation for D(v) is O(DvK). Since D(v) is diagonal, it only takes O(DvK) to calculate D(v)V(v). It is easy to verify that it take O(mDvK) to update V(v), and O(nvmDK) to update {V(v)}. Thus, the time complexity for OMVFS to process one data chunk is O(tnvsmDK), which leads to the overall time complexity to O(tnvsNDK). Here, t is the average number of iterations to converge. Considering s is usually very small (less than 5 in the experiments), the time complexity is approximately O(tnvNDK). We can observe that the time complexity is linear to all the components (number of views, number of instances, feature dimensionality and the number of clusters).\nMost of the offline methods require at least O(nvND) space, however the proposed OMVFS only requires O(nvsmD) + O(nvs\n2m2) \u2248 O(nvsmD) memory space, which makes OMVFS suitable for large-scale/streaming data."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "A. Dataset", "text": "In this paper, two small real-world datasets and two large real-world datasets are used to evaluate the proposed OMVFS method. The summary of these four datasets is shown in Table II, and the details of the datasets are as follows:\n\u2022 CNN and FOX1: These two datasets were crawled from CNN and FOX web news [10]. The category information contained in the RSS feeds for each news article can be viewed as reliable class label. Each instance can be represented in two views, the text view and image view. Titles, abstracts, and text body contents are extracted as the text view data, and the image associated with the article is stored as the image view data. All text content is stemmed by portStemmer, and l2-normalized TF-IDF is used as text features, which results in 35, 719 features for CNN and 27, 072 features for FOX. For image features, seven groups of color features and five textural features are used [10], which results in 996 features for both datasets. \u2022 YouTube Multiview Video Games (YouTube)2: This dataset consists of feature values and class labels for about 120,000 videos (instances) [24]. Each instance is described by up to 13 feature types, from 3 high level\n1https://sites.google.com/site/qianmingjie/home/datasets/ 2https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview\n+Video+Games+Dataset"}, {"heading": "B. Comparison Methods", "text": "We compare the proposed OMVC method with several state-of-art methods. The differences between these comparison methods are summarized in Table III, and the details of comparison methods are as follows:\n\u2022 OMVFS: OMVFS is the online unsupervised multi-view feature selection method proposed in this paper 4. \u2022 LapScore: Laplacian Score [26] is a single view unsupervised feature selection method, which evaluates the importance of a feature via its power of locality preservation. \u2022 EUFS: Embedded Unsupervised Feature Selection [18] is one of the most recent single view unsupervised feature selection method. It directly embeds unsupervised feature selection algorithm into a clustering algorithm via sparse learning. \u2022 FSDS: Unsupervised Feature Selection on Data Stream [14] is one of the state-of-the-art unsupervised feature selection algorithms, which handles large volume single view data stream efficiently. It adopts the idea of matrix sketching to efficiently maintain a low-rank approximation of the observed data and applies regularized regression to select the important features. \u2022 MVUFS Multi-View Unsupervised Feature Selection [27] is the most advanced off-line unsupervised feature\n3http://archive.ics.uci.edu/ml/machine-learning-databases/00259/ 4code available at: https://github.com/software-shao"}, {"heading": "C. Experiment Settings", "text": "In our experiments, two widely used evaluation metrics, Accuracy (ACC) and Normalized Mutual Information (NMI), are used to measure the clustering performance [28]. We apply different methods to the four data sets, then the multiview spherical K-means algorithm [29] is applied to get the clustering solution. Since LapScore, EUFS and FSDS only work for single view data, in the experiments, we apply these three methods on each of the view to select features. It is worth mentioning that MVUFS is an off-line multiview feature selection method, which takse all data into consideration and can often achieve better performance than online methods. Furthermore, LapScore, EUFS, MVUFS are all off-line methods, which cannot handle large datasets. Thus, only the two online methods, FSDS and OMVFS, are applied to the two large datasets.\nMost of the comparison methods use graph/similarity matrices. In the experiments, we used the same kernal/similarity matrices as stated in the original papers. For the proposed OMVFS, we used the Gaussian kernel. If not stated, for the online methods (FSDS and OMVFS) the size of data chunk is set to 200 for the two small datasets and 1000 for the two large datasets. The buffer size is set to 2 and \u03b3 is set to 107 for the proposed OMVFS method. For the sake of convenience, the parameters \u03b1v and \u03b2v are all set equally for different views. For FSDS, we do grid search in {1, 2, ..., 10} for the index of parameter \u03b1. For all the other parameters in the comparison methods, we do grid search in {10\u22122, 10\u22121, ..., 102}. We also vary the number of selected features as {100, 200, ..., 600} for all the views. The performance of the best parameter setting is reported for all the methods."}, {"heading": "D. Results on Small Datasets", "text": "In order to compare the proposed OMVFS method with other comparison methods, we first apply all the methods to two small datasets, FOX and CNN. Fig. 3 shows the performance of all the methods on FOX and CNN data with different numbers of selected features.\nFrom the Fig. 3, we can observe that as the number of selected feature increases, the performance will increase for most of the cases. For both datasets, the two multi-view feature selection methods, MVUFS (off-line) and OMVFS (online), outperform the single view feature selection methods. This\nobservation suggests that combining different views would benefit the feature selection for multi-view data.\nFor the FOX data, we can observe from Fig. 3a and Fig. 3b that the performance of the proposed OMVFS is very close and even better than that of the best off-line method. Among the three single view methods, FSDS and EUFS can get better performance than LapScore. Although FSDS may suffer from low performance when including only 100 features, its performance will increase dramatically when including more than 200 features. This may indicate that the online FSDS selects less discriminative features as the top features, however, more discriminative features will be included if we increase the size of the feature set.\nFor the CNN data, we can see that the proposed OMVFS has similar performance as MVUFS when the feature size is less than 300. However, OMVFS outperforms MVUFS if more than 300 features are included. Comparing the statics of CNN and FOX data, we can find that CNN has higher feature dimensions and more classes than FOX. The proposed OMVFS may select more discriminative features in data with higher dimensions than MVUFS.\nFrom the results on FOX and CNN, we can conclude that the proposed OMVFS method outperforms all the single view feature selection methods (online and off-line). It can also get close or even better performance than the most advanced offline multi-view feature selection method."}, {"heading": "E. Result on Large Datasets", "text": "Since the proposed OMVFS is designed for large-scale multi-view datasets, we test OMVFS on the two largest public available multi-view datasets. We also compare OMVFS with the most recent online single view feature selection method, FSDS. The results on both datasets are reported in Fig. 4. However, due to the extremely high runtime and memory consumption, LapScore, EUFS and MVUFS cannot be applied to the two large datasets.\nFrom Fig. 4, it can be easily observed that the proposed OMVFS methods achieves better performance than FSDS in most cases. Especially when the number of selected feature is small (i.e., less than 300), OMVFS is much better than FSDS. For example, on YouTube data, when we only select 100 features, the NMI for OMVFS is about 0.58, while the NMI for FSDS is only 0.28. This observation suggests that unlike FSDS, which tends to include less discriminative features in the top selected feature set, the proposed OMVFS selects more discriminative features even in a small set of selected features."}, {"heading": "F. Scalability Comparison", "text": "In order to show the scalability of the proposed OMVFS method under different data sizes, we run OMVFS on the Reuters and YouTube data and report the runtime for different data sizes (number of instances) in Fig. 5. Also, to show the scalability of OMVFS method under different feature dimensions, we randomly sample different numbers of features from the two large datasets and run OMVFS on the sampled features. The runtime for different feature dimensions is reported in Fig. 6.\nWe select MVUFS and FSDS as comparison methods, since MVUFS is the only one that deals with multi-view data and FSDS is the only one that handles large-scale/streaming data. Because FSDS is a single view method, we run FSDS on each view and report the total runtime for all the views.\nFrom Fig. 5, we can observe that the two online methods, FSDS and OMVFS, are much faster than the off-line MVUFS and the runtime of the two online methods are linear with respect to the data size. It only takes about 3 seconds for FSDS and OMVFS on data with size 1000 on Reuters data, while MVUFS takes about 300 seconds. Like the FSDS method, the proposed OMVFS only uses the current data chunks and all the historical data is aggregated. OMVFS considers all the views simultaneously while FSDS can only work on single view. Although the two online methods have similar runtime\nwith small data, the proposed OMVFS uses less time as the data size increases. Further, it can be found that OMVFS is about 100 times faster than MVUFS on both datasets.\nBecause of the high memory usage and slow runtime of MVUFS, we only report the runtime for OMVFS and FSDS in Fig. 6. From Fig. 6, we can observe that although FSDS is faster when we only sample small number of features (10 % or 20%), the runtime of OMVFS grows much slower than FSDS as the dimension of features increases. For Reuters data, the runtime of OMVFS only increases from 240 seconds to 480 seconds when the number of features increases from 10% to 100%. Thus, the proposed OMVFS is faster when the feature dimension becomes really large."}, {"heading": "G. Stability under Concept Drift", "text": "It is well-known that online/streaming algorithms are generally sensitive to the order of data, or concept drift [11]. To test the performance of OMVFS in such scenarios, we use 12,000 instances from Reuters data. We randomly create unbalanced data with two dominant classes and randomly change the dominant classes for every 3,000 instances in the data stream. OMVFS is then applied to the data stream with concept drifts. As the data came in, we report the performance of OMVFS with different sizes of feature set in Fig. 7. We also compare against a scheme where a static feature subset (with 200 features) is used without adapting to concept drift. This static feature subset is determined by OMVFS using only the first 3,000 instances in the data stream.\nFrom Fig. 7, we can observe that the approach based on static feature subset performs quite close to OMVFS in the beginging. However, as new data come in and concept drift becomes more prominent, the performance of static feature subset decreases. On the other side, the performance of the proposed OMVFS will not decrease as more data come in. Instead, the performance increases as OMVFS combines the information in the new data with the aggregated information from previous data. Also, from Fig. 7, we can see that in general, the more features we select, the better performance OMVFS would achieve."}, {"heading": "H. Parameter Study", "text": "There are two sets of parameters in the proposed methods: {\u03b1v} and {\u03b2v}. Here, we explore the effects of the two parameter sets. For the sake of convenience, we set \u03b1v to be equal for different views and also set \u03b2v equally. We ran OMVFS with different values for {\u03b1v} and {\u03b2v} on FOX data. Basically, we fix one of the parameters and ran OMVFS with different values for the other. We report the performance with\ndifferent sizes of the selected features. The results in ACC and NMI are shown in Fig. 8.\nFrom Fig. 8, we can see that for most of the cases, the proposed OMVFS method is not very sensitive to the parameters \u03b1v and \u03b2v. However, the performance does increase as the number of selected features increases.\nAnother parameter in the proposed OMVFS method is the batch size m, which is a common parameter for streaming algorithms. To test the performance of OMVFS under different batch sizes, we run OMVFS on FOX data with different batch sizes and numbers of selected features. The results in ACC and NMI are shown in Fig. 9. From Fig. 9, we can clearly observe that the performance of OMVFS is very stable under different batch sizes."}, {"heading": "VI. RELATED WORK", "text": "There are three areas of related works upon which the proposed model is built. Feature selection, especially unsupervised feature selection [4], [5], is the first area that is related to this work. Most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7]. In terms of the number of views available, unsupervised feature selection can be categorized into single view feature selection and multi-view feature selection. Most of the conventional unsupervised feature selection methods are designed for single view. As more and more multi-view data are generated, the limitation of conventional feature selection methods becomes more obvious. Multi-view feature selection has drawn more attention in recent years. Several effective methods have been proposed to solve unsupervised feature selection problem for various multi-view scenarios [8]\u2013[10]. However, all the previous multi-view feature selection methods suffer from the scalability issues. They all require that the whole data fit into the memory. The proposed OMVFS method, however, only takes a small amount of memory space and uses less computational time without sacrificing the performance, which makes it more suitable for large-scale/streaming data.\nMulti-view unsupervised learning is the second area that is related to our work. A few directions were explored to solve multi-view unsupervised learning in recent years. For example, [30], [31] propose to use canonical correlation analysis to combine different views. [32], [33] explore the option to model\nthe multi-view learning as a joint NMF problem. [34], [35] use tensor to model the multi-view data.\nNonnegative matrix factorization [20], especially online NMF, is the third area that is related to our work. NMF has been successfully used in unsupervised learning [15], [21]. However, traditional NMF cannot deal with large-scale data. Different variations were proposed in the last few years. For example, [36] proposed an online NMF algorithm for document clustering. [37] proposed an efficient online NMF algorithm (OR-NMF) that takes one sample or a chunk of samples per step and updates the bases via robust hastic approximation. All the online NMF methods either focus on clustering or dimension reduction. None of them are designed for feature selection. Furthermore, none of them can handle multi-view data. However, our proposed OMVFS directly embeds feature selection into the online joint NMF framework with graph regularization, which handles large/streaming multi-view data."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we present possibly the first attempt to solve the online unsupervised multi-view feature selection problem. Based on NMF, the proposed method OMVFS directly embeds the feature selection into the graph regularized clustering algorithm. A joint NMF is used to learn a consensus clustering indicator matrix, which makes OMVFS integrate information from different views. OMVFS also adopts the graph regularization to preserve the local structure information and help select more discriminative features. Solving the optimization problem in an incremental way, OMVFS can process the data chunks one by one without storing all the historical data, which greately reduces the memory requirement. Also, by using the buffering technique, OMVFS can reduce the computational and storage cost while taking advantage of the structure information. The buffering will also help OMVFS capture the concept drift in the data streams. Extensive experiments conducted on two small datasets and two large datasets demonstrate the effectiveness and efficiency of the proposed OMVFS algorithm. Without sacrificing performance, the proposed OMVFS is about 100 times faster than the best off-line multi-view feature selection method."}], "references": [{"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, vol. 23, no. 7-8, pp. 2031\u20132038, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "ICML, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "ICML, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised feature selection for multicluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "KDD, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Embedded unsupervised feature selection.", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "in AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Unsupervised feature selection using nonnegative spectral analysis.", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Robust unsupervised feature selection.", "author": ["M. Qian", "C. Zhai"], "venue": "in IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["H. Wang", "F. Nie", "H. Huang"], "venue": "ICML, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised feature selection for multi-view data in social media.", "author": ["J. Tang", "X. Hu", "H. Gao", "H. Liu"], "venue": "in SDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Unsupervised feature selection for multi-view clustering on text-image web news data", "author": ["M. Qian", "C. Zhai"], "venue": "CIKM, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining concept-drifting data streams using ensemble classifiers", "author": ["H. Wang", "W. Fan", "P.S. Yu", "J. Han"], "venue": "KDD, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "A fast clustering-based feature subset selection algorithm for high-dimensional data", "author": ["Q. Song", "J. Ni", "G. Wang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 1, pp. 1\u201314, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Pass-efficient unsupervised feature selection", "author": ["C. Maung", "H. Schweitzer"], "venue": "NIPS, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised feature selection on data streams", "author": ["H. Huang", "S. Yoo", "S.P. Kasiviswanathan"], "venue": "CIKM, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "KDD, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for orthogonal nonnegative matrix factorization", "author": ["S. Choi"], "venue": "IJCNN, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative matrix factorization with orthogonality constraints", "author": ["J.-H. Yoo", "S.-J. Choi"], "venue": "Journal of computing science and engineering, vol. 4, no. 2, pp. 97\u2013109, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Embedded unsupervised feature selection", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "AAAI, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548\u2013 1560, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning and stochastic approximations", "author": ["\u2014\u2014"], "venue": "On-line learning in neural networks, vol. 17, no. 9, p. 142.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}, {"title": "On using nearly-independent feature families for high precision and confidence", "author": ["O. Madani", "M. Georg", "D.A. Ross"], "venue": "Machine Learning, vol. 92, pp. 457\u2013477, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning from multiple partially observed views-an application to multilingual text categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "NIPS, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "NIPS, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised feature selection for multi-view clustering on text-image web news data", "author": ["M. Qian", "C. Zhai"], "venue": "CIKM, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Document clustering based on nonnegative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR, 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-view clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "ICDM, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-view Clustering via Canonical Correlation Analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering on multiple incomplete datasets via collective kernel learning", "author": ["W. Shao", "X. Shi", "P. Yu"], "venue": "ICDM, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-View Clustering via Joint Nonnegative Matrix Factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple Incomplete Views Clustering via Weighted Nonnegative Matrix Factorization with L2,1 Regularization", "author": ["W. Shao", "L. He", "P.S. Yu"], "venue": "ECML PKDD, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiview partitioning via tensor methods", "author": ["X. Liu", "S. Ji", "W. Gl\u00e4nzel", "B. De Moor"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 5, pp. 1056\u20131069, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Clustering on multi-source incomplete data via tensor modeling and factorization", "author": ["W. Shao", "L. He", "S.Y. Philip"], "venue": "PAKDD, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient document clustering via online nonnegative matrix factorizations.", "author": ["F. Wang", "P. Li", "A.C. K\u00f6nig"], "venue": "in SDM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Online nonnegative matrix factorization with robust stochastic approximation", "author": ["N. Guan", "D. Tao", "Z. Luo", "B. Yuan"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 7, pp. 1087\u2013 1099, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multi-view learning was proposed to combine different views to obtain better performance than relying on just one single view [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "Feature selection has been studied for decades [2].", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Supervised feature selection [3] uses the class labels to effectively select discriminative features to distinguish samples from different classes.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Recently, several approaches on unsupervised feature selection has been proposed [4], [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Recently, several approaches on unsupervised feature selection has been proposed [4], [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Without the label information, most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Without the label information, most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "Most recently, as complementary information can be obtained from different views, unsupervised multi-view feature selection has drawn lots of attention [8]\u2013[10].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Most recently, as complementary information can be obtained from different views, unsupervised multi-view feature selection has drawn lots of attention [8]\u2013[10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "For example, [9] is the first to use spectral clustering and l2,1-norm regression to multi-view data in social media.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "[8] integrates all features and learns the weights for every feature with respect to each cluster individually via a new joint structured sparsity-inducing norms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "For image and text data, [10] uses image local learning regularized orthogonal nonnegative matrix factorization to learn pseudo labels and simultaneously perform robust joint l2,1-norm minimization to select discriminative features.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "2) In many real-world applications, the data may come in as streams and concept drift [11] may happen.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "For example, [8]\u2013[10] only solve unsupervised feature selection problem for multi-view data.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "For example, [8]\u2013[10] only solve unsupervised feature selection problem for multi-view data.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "[12]\u2013[14] solve the problem of feature selection on large-scale/streaming data in a single view.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[12]\u2013[14] solve the problem of feature selection on large-scale/streaming data in a single view.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "In practice, for clustering problem, an orthogonality constraint is usually added to U [15]\u2013[17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "In practice, for clustering problem, an orthogonality constraint is usually added to U [15]\u2013[17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "error [18].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "[18] proposed to relax Eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Using the Karush-Kuhn-Tucker (KKT) complementary condition for the nonnegativity constraint on U[t], we can get the update rule for U[t] [16], [19]:", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Although the objective function for the proposed OMVFS at time t is not jointly convex with U[s,t] and {V}, the alternating optimization strategy used in OMVFS is guaranteed to converge to the local minimum [20].", "startOffset": 207, "endOffset": 211}, {"referenceID": 19, "context": "The proof is similar to that of Theorem 1 in [21] and we omit it.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "It is important to note that our method essentially applies stochastic gradient descent [22] to the new coming data.", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "This method is expected to decrease the objective function in every iteration on the new data [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "The summary of these four datasets is shown in Table II, and the details of the datasets are as follows: \u2022 CNN and FOX1: These two datasets were crawled from CNN and FOX web news [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 9, "context": "For image features, seven groups of color features and five textural features are used [10], which results in 996 features for both datasets.", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "\u2022 YouTube Multiview Video Games (YouTube)2: This dataset consists of feature values and class labels for about 120,000 videos (instances) [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 23, "context": "\u2022 Reuters Multilingual Text Data (Reuters)3: The text collection contains feature characteristics of documents originally written in five different languages (English, French, German, Spanish and Italian), and their translations, over a common set of 6 topic categories [25].", "startOffset": 270, "endOffset": 274}, {"referenceID": 24, "context": "\u2022 LapScore: Laplacian Score [26] is a single view unsupervised feature selection method, which evaluates the importance of a feature via its power of locality preservation.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "\u2022 EUFS: Embedded Unsupervised Feature Selection [18] is one of the most recent single view unsupervised feature selection method.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "\u2022 FSDS: Unsupervised Feature Selection on Data Stream [14] is one of the state-of-the-art unsupervised feature selection algorithms, which handles large volume single view data stream efficiently.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "\u2022 MVUFS Multi-View Unsupervised Feature Selection [27] is the most advanced off-line unsupervised feature", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "Experiment Settings In our experiments, two widely used evaluation metrics, Accuracy (ACC) and Normalized Mutual Information (NMI), are used to measure the clustering performance [28].", "startOffset": 179, "endOffset": 183}, {"referenceID": 27, "context": "We apply different methods to the four data sets, then the multiview spherical K-means algorithm [29] is applied to get the clustering solution.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Stability under Concept Drift It is well-known that online/streaming algorithms are generally sensitive to the order of data, or concept drift [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "Feature selection, especially unsupervised feature selection [4], [5], is the first area that is related to this work.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Feature selection, especially unsupervised feature selection [4], [5], is the first area that is related to this work.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Several effective methods have been proposed to solve unsupervised feature selection problem for various multi-view scenarios [8]\u2013[10].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "Several effective methods have been proposed to solve unsupervised feature selection problem for various multi-view scenarios [8]\u2013[10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 28, "context": "For example, [30], [31] propose to use canonical correlation analysis to combine different views.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "For example, [30], [31] propose to use canonical correlation analysis to combine different views.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "[32], [33] explore the option to model", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], [33] explore the option to model", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "[34], [35] use tensor to model the multi-view data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34], [35] use tensor to model the multi-view data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Nonnegative matrix factorization [20], especially online NMF, is the third area that is related to our work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "NMF has been successfully used in unsupervised learning [15], [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "NMF has been successfully used in unsupervised learning [15], [21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "For example, [36] proposed an online NMF algorithm for document clustering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "[37] proposed an efficient online NMF algorithm (OR-NMF) that takes one sample or a chunk of samples per step and updates the bases via robust hastic approximation.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In the era of big data, it is becoming common to have data with multiple modalities or coming from multiple sources, known as \u201cmulti-view data\u201d. Since multi-view data are usually unlabeled and come from high-dimensional spaces (such as language vocabularies), unsupervised multi-view feature selection is crucial to many applications such as model interpretation and storage reduction. However, it is nontrivial due to the following challenges. First, the data may not fit in memory, because there are too many instances or the feature dimensionality is too large. How to select useful features with limited memory space? Second, the data may come in as streams and concept drift may happen. How to select features from streaming data and handles the concept drift? Third, different views may share some consistent and complementary information. How to leverage the consistent and complementary information from different views to improve the feature selection in the situation when the data are too big or come in as streams? To the best of our knowledge, none of the previous works can solve all the challenges simultaneously. In this paper, we propose an Online unsupervised MultiView Feature Selection, OMVFS, which deals with largescale/streaming multi-view data in an online fashion. OMVFS embeds unsupervised feature selection into a clustering algorithm via nonnegative matrix factorizatio with sparse learning. It further incorporates the graph regularization to preserve the local structure information and help select discriminative features. Instead of storing all the historical data, OMVFS processes the multi-view data chunk by chunk and aggregates all the necessary information into several small matrices. By using the buffering technique, the proposed OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, OMVFS can capture the concept drifts in the data streams. Extensive experiments on four real-world datasets show the effectiveness and efficiency of the proposed OMVFS method. More importantly, OMVFS is about 100 times faster than the off-line methods.", "creator": "LaTeX with hyperref package"}}}