{"id": "1609.08293", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "The Effects of Data Size and Frequency Range on Distributional Semantic Models", "abstract": "This paper examines the effects of data size and frequency range on semantic distribution models. We compare the performance of a number of representative models for different test settings across data of different sizes and test objects of different frequencies. Our results show that neural network-based models perform worse on small amounts of data and that the most reliable model for data of different sizes and frequency ranges is the reverse factored model.", "histories": [["v1", "Tue, 27 Sep 2016 07:38:29 GMT  (28kb)", "http://arxiv.org/abs/1609.08293v1", "Accepted at EMNLP 2016"]], "COMMENTS": "Accepted at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["magnus sahlgren", "alessandro lenci"], "accepted": true, "id": "1609.08293"}, "pdf": {"name": "1609.08293.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mange@[gavagai|sics].se", "alessandro.lenci@unipi.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n08 29\n3v 1\n[ cs\n.C L\n] 2\n7 Se"}, {"heading": "1 Introduction", "text": "Distributional Semantic Models (DSMs) have become a staple in natural language processing. The various parameters of DSMs \u2014 e.g. size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood. The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; O\u0308sterlund et al., 2015; Sahlgren et al., 2016). The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parameterization of the models, since the various processing models all result in more or less equiv-\nalent DSMs (provided that the parameterization is comparable).\nOne of the least researched aspects of DSMs is the effect on the various models of data size and frequency range of the target items. The only previous work in this direction that we are aware of is Asr et al. (2016), who report that on small data (the CHILDES corpus), simple matrix-based models outperform neural network-based ones. Unfortunately, Asr et al. do not include any experiments using the same models applied to bigger data, making it difficult to compare their results with previous studies, since implementational details and parameterization will be different.\nThere is thus still a need for a consistent and fair comparison of the performance of various DSMs when applied to data of varying sizes. In this paper, we seek an answer to the question: which DSM should we opt for if we only have access to limited amounts of data? We are also interested in the related question: which DSM should we opt for if our target items are infrequent? The latter question is particularly crucial, since one of the major assets of DSMs is their applicability to create semantic representations for ever-expanding vocabularies from text feeds, in which new words may continuously appear in the low-frequency ranges.\nIn the next section, we introduce the contending DSMs and the general experiment setup, before turning to the experiments and our interpretation of the results. We conclude with some general advice."}, {"heading": "2 Distributional Semantic Models", "text": "One could classify DSMs in many different ways, such as the type of context and the method to build distributional vectors. Since our main goal here is to gain an understanding of the effect of data size and frequency range on the various models, we focus primarily on the differences in processing models, hence the following typology of DSMs."}, {"heading": "Explicit matrix models", "text": "We here include what could be referred to as explicit models, in which each vector dimension corresponds to a specific context (Levy and Goldberg, 2014). The baseline model is a simple co-occurrence matrix F (in the following referred to as CO for Co-Occurrence). We also include the model that results from applying Positive Pointwise Mutual Information (PPMI) to the co-occurrence matrix. PPMI is defined as simply discarding any negative values of the PMI, computed as:\nPMI(a, b) = log fab \u00d7 T\nfafb (1)\nwhere fab is the co-occurrence count of word a and word b, fa and fb are the individual frequencies of the words, and T is the number of tokens in the data.1"}, {"heading": "Factorized matrix models", "text": "This type of model applies an additional factorization of the weighted co-occurrence counts. We here include two variants of applying Singular Value Decomposition (SVD) to the PPMI-weighting cooccurrence matrix; one version that discards all but the first couple of hundred latent dimensions (TSVD for truncated SVD), and one version that instead removes the first couple of hundred latent dimensions (ISVD for inverted SVD). SVD is defined in the standard way:\nF = U\u03a3V T (2)\n1We also experimented with smoothed PPMI, which raises the context counts to the power of \u03b1 and normalizes them (Levy et al., 2015), thereby countering the tendency of mutual information to favor infrequent events: f(b) = #(b) \u03b1 \u2211\nb #(b) \u03b1 , but it\ndid not lead to any consistent improvements compared to PPMI.\nwhere U holds the eigenvectors of F , \u03a3 holds the eigenvalues, and V \u2208 U(w) is a unitary matrix mapping the original basis of F into its eigenbasis. Since V is redundant due to invariance under unitary transformations, we can represent the factorization of F\u0302 in its most compact form F\u0302 \u2261 U\u03a3."}, {"heading": "Hashing models", "text": "A different approach to reduce the dimensionality of DSMs is to use a hashing method such as Random Indexing (RI) (Kanerva et al., 2000), which accumulates distributional vectors ~d(a) in an online fashion:\n~d(a)\u2190 ~d(ai)+\nc\u2211\nj=\u2212c,j 6=0\nw(x(i+j))\u03c0j~r(x(i+j)) (3)\nwhere c is the extension of the context window, w(b) is a weight that quantifies the importance of context term b,2 ~rd(b) is a sparse random index vector that acts as a fingerprint of context term b, and \u03c0j is a permutation that rotates the random index vectors one step to the left or right, depending on the position of the context items within the context windows, thus enabling the model to take word order into account (Sahlgren et al., 2008)."}, {"heading": "Neural network models", "text": "There are many variations of DSMs that use neural networks as processing model, ranging from simple recurrent networks (Elman, 1990) to more complex deep architectures (Collobert and Weston, 2008). The incomparably most popular neural network model is the one implemented in the word2vec library, which uses the softmax for predicting b given a (Mikolov et al., 2013):\np(b|a) = exp(~b \u00b7 ~a)\n\u2211 b\u2032\u2208C exp( ~b\u2032 \u00b7 ~a) (4)\nwhere C is the set of context words, and~b and ~a are the vector representations for the context and target words, respectively. We include two versions of this general model; Continuous Bag of Words (CBOW)\n2We use w(b) = e\u2212\u03bb\u00b7 f(b) V where f(b) is the frequency of context item b, V is the total number of unique context items seen thus far (i.e. the current size of the growing vocabulary), and \u03bb is a constant that we set to 60 (Sahlgren et al., 2016).\nthat predicts a word based on the context, and SkipGram Negative Sampling (SGNS) that predicts the context based on the current word."}, {"heading": "3 Experiment setup", "text": "Since our main focus in this paper is the performance of the above-mentioned DSMs on data of varying sizes, we use one big corpus as starting point, and split the data into bins of varying sizes. We opt for the ukWaC corpus (Ferraresi et al., 2008), which comprises some 1.6 billion words after tokenization and lemmatization. We produce sub-corpora by taking the first 1 million, 10 million, 100 million, and 1 billion words.\nSince the co-occurrence matrix built from the 1 billion-word ukWaC sample is very big (more than 4,000,000 \u00d7 4,000,000), we prune the co-occurrence matrix to 50,000 dimensions before the factorization step by simply removing infrequent context items.3 As comparison, we use 200 dimensions for TSVD, 2,800 (3,000-200) dimensions for ISVD, 2,000 dimensions for RI, and 200 dimensions for CBOW and SGNS. These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; O\u0308sterlund et al., 2015). All DSMs use the same parameters as far as possible with a narrow context window of \u00b12 words, which has been shown to produce good results in semantic tasks (Sahlgren, 2006; Bullinaria and Levy, 2012).\nWe use five standard benchmark tests in these experiments; two multiple-choice vocabulary tests (the TOEFL synonyms and the ESL synonyms), and three similarity/relatedness rating benchmarks (SimLex-999 (SL) (Hill et al., 2015), MEN (Bruni et al., 2014), and Stanford Rare Words (RW) (Luong et al., 2013)). The vocabulary tests measure the synonym relation, while the similarity rating tests measure a broader notion of semantic similarity (SL and RW) or relatedness (MEN).4 The\n3Such drastic reduction has a negative effect on the performance of the factorized methods for the 1 billion word data, but unfortunately is necessary for computational reasons.\n4It is likely that the results on the similarity tests could be improved by using a wider context window, but such improvement would probably be consistent across all models, and is thus outside the scope of this paper.\nresults for the vocabulary tests are given in accuracy (i.e., percentage of correct answers), while the results for the similarity tests are given in Spearman rank correlation."}, {"heading": "4 Comparison by data size", "text": "Table 1 summarizes the results over the different test settings. The most notable aspect of these results is that the neural networks models do not produce competitive results for the smaller data, which corroborates the results by Asr et al. (2016). The best results for the smallest data are produced by the factorized models, with both TSVD and ISVD producing top scores in different test settings. It should\nbe noted, however, that even the top scores for the smallest data set are substandard; only two models (PPMI and TSVD) manage to beat the random baseline of 25% for the TOEFL tests, and none of the models manage to beat the random baseline for the ESL test.\nThe ISVD model produces consistently good results; it yields the best overall results for the 10 million and 100 million-word data, and is competitive with SGNS on the 1 billion word data. Figure 1 shows the average results and their standard deviations over all test settings.5 It is obvious that there are no huge differences between the various models, with the exception of the baseline CO model, which consistently underperforms. The TSVD and RI models have comparable performance across the different data sizes, which is systematically lower than the PPMI model. The ISVD model is the most consistently good model, with the neural network-based models steadily improving as data becomes bigger.\nLooking at the different datasets, SL and RW are the hardest ones for all the models. In the case of SL, this confirms the results in (Hill et al., 2015), and might be due to the general bias of DSMs towards semantic relatedness, rather than genuine semantic similarity, as represented in SL. The substandard performance on RW might instead be due to the low frequency of the target items. It is interesting to note that these are benchmark tests in which neural\n5Although rank correlation is not directly comparable with accuracy, they are both bounded between zero and one, which means we can take the average to get an idea about overall performance.\nmodels perform the worst even when trained on the largest data."}, {"heading": "5 Comparison by frequency range", "text": "In order to investigate how each model handles different frequency ranges, we split the test items into three different classes that contain about a third of the frequency mass of the test items each. This split was produced by collecting all test items into a common vocabulary, and then sorting this vocabulary by its frequency in the ukWaC 1 billionword corpus. We split the vocabulary into 3 equally large parts; the HIGH range with frequencies ranging from 3,515,086 (\u201cdo\u201d) to 16,830 (\u201corganism\u201d), the MEDIUM range with frequencies ranging between 16,795 (\u201cdesirable\u201d) and 729 (\u201cprickly\u201d), and the LOW range with frequencies ranging between 728 (\u201cboardwalk\u201d) to hapax legomenon. We then split each individual test into these three ranges, depending on the frequencies of the test items. Test pairs were included in a given frequency class if and only if both the target and its relatum occur in the frequency range for that class. For the constituent words in the test item that belong to different frequency ranges, which is the most common case, we use a separate MIXED class. The resulting four classes contain 1,387 items for the HIGH range, 656 items for the MEDIUM range, 350 items for the LOW range, and 3,458 items for the MIXED range.6\nTable 2 (next side) shows the average results over the different frequency ranges for the various DSMs trained on the 1 billion-word ukWaC data. We also include the highest and lowest individual test scores (signified by \u2191 and \u2193), in order to get an idea about the consistency of the results. As can be seen in the table, the most consistent model is ISVD, which produces the best results in both the MEDIUM and MIXED frequency ranges. The neural network models SGNS and CBOW produce the best results in the HIGH and LOW range, respectively, with CBOW clearly outperforming SGNS in the latter case. The major difference between these models is that CBOW predicts a word based on a context, while SGNS predicts a context based on a word. Clearly, the former approach is more beneficial for\n6233 test terms did not occur in the 1 billion-word corpus.\nlow-frequent items. The PPMI, TSVD and RI models perform similarly across the frequency ranges, with RI producing somewhat lower results in the MEDIUM range, and TSVD producing somewhat lower results in the LOW range. The CO model underperforms in all frequency ranges. Worth noting is the fact that all models that are based on an explicit matrix (i.e. CO, PPMI, TSVD and ISVD) produce better results in the MEDIUM range than in the HIGH range.\nThe arguably most interesting results are in the LOW range. Unsurprisingly, there is a general and significant drop in performance for low frequency items, but with interesting differences among the various models. As already mentioned, the CBOW model produces the best results, closely followed by PPMI and RI. It is noteworthy that the low-dimensional embeddings of the CBOW model only gives a modest improvement over the highdimensional explicit vectors of PPMI. The worst results are produced by the ISVD model, which scores even lower than the baseline CO model. This might be explained by the fact that ISVD removes the latent dimensions with largest variance, which are arguably the most important dimensions for very lowfrequent items. Increasing the number of latent dimensions with high variance in the ISVD model improves the results in the LOW range (16.59 when removing only the top 100 dimensions)."}, {"heading": "6 Conclusion", "text": "Our experiments confirm the results of Asr et al. (2016), who show that neural network-based models are suboptimal to use for smaller amounts of data. On the other hand, our results also show that\nnone of the standard DSMs work well in situations with small data. It might be an interesting novel research direction to investigate how to design DSMs that are applicable to small-data scenarios.\nOur results demonstrate that the inverted factorized model (ISVD) produces the most robust results over data of varying sizes, and across several different test settings. We interpret this finding as further corroborating the results of Bullinaria and Levy (2012), and O\u0308sterlund et al. (2015), with the conclusion that the inverted factorized model is a robust competitive alternative to the widely used SGNS and CBOW neural network-based models.\nWe have also investigated the performance of the various models on test items in different frequency ranges, and our results in these experiments demonstrate that all tested models perform optimally in the medium-to-high frequency ranges. Interestingly, all models based on explicit count matrices (CO, PPMI, TSVD and ISVD) produce somewhat better results for items of medium frequency than for items of high frequency. The neural network-based models and ISVD, on the other hand, produce the best results for high-frequent items.\nNone of the tested models perform optimally for low-frequent items. The best results for lowfrequent test items in our experiments were produced using the CBOW model, the PPMI model and the RI model, all of which uses weighted context items without any explicit factorization. By contrast, the ISVD model underperforms significantly for the low-frequent items, which we suggest is an effect of removing latent dimensions with high variance.\nThis interpretation suggests that it might be interesting to investigate hybrid models that use different processing models \u2014 or at least different parame-\nterizations \u2014 for different frequency ranges, and for different data sizes. We leave this as a suggestion for future research."}, {"heading": "7 Acknowledgements", "text": "This research was supported by the Swedish Research Council under contract 2014-28199."}], "references": [{"title": "Comparing predictive and co-occurrence based models of lexical semantics trained on childdirected speech", "author": ["Asr et al.2016] Fatemeh Asr", "Jon Willits", "Michael Jones"], "venue": "Proceedings of CogSci", "citeRegEx": "Asr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Asr et al\\.", "year": 2016}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd", "author": ["Bullinaria", "Levy2012] John Bullinaria", "Joseph P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english", "author": ["Eros Zanchetta", "Marco Baroni", "Silvia Bernardini"], "venue": "Proceedings of WAC-4,", "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Random indexing of text samples for latent semantic analysis", "author": ["Jan Kristofersson", "Anders Holst"], "venue": "In Proceedings of CogSci,", "citeRegEx": "Kanerva et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kanerva et al\\.", "year": 2000}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Factorization of latent variables in distributional semantic models", "author": ["David \u00d6dling", "Magnus Sahlgren"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "\u00d6sterlund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00d6sterlund et al\\.", "year": 2015}, {"title": "Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation", "author": ["Riordan", "Jones2011] Brian Riordan", "Michael N. Jones"], "venue": "Topics in Cognitive Science,", "citeRegEx": "Riordan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Riordan et al\\.", "year": 2011}, {"title": "Permutations as a means to encode order in word space", "author": ["Anders Holst", "Pentti Kanerva"], "venue": "In Proceedings of CogSci,", "citeRegEx": "Sahlgren et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren et al\\.", "year": 2008}, {"title": "The Gavagai Living Lexicon", "author": ["Amaru Cuba Gyllensten", "Fredrik Espinoza", "Ola Hamfors", "Anders Holst", "Jussi Karlgren", "Fredrik Olsson", "Per Persson", "Akshay Viswanathan"], "venue": "Proceedings of LREC", "citeRegEx": "Sahlgren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sahlgren et al\\.", "year": 2016}, {"title": "The WordSpace Model", "author": ["Magnus Sahlgren"], "venue": "Phd thesis,", "citeRegEx": "Sahlgren.,? \\Q2006\\E", "shortCiteRegEx": "Sahlgren.", "year": 2006}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds et al.2004] Julie Weeds", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of COLING,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 20, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 18, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 11, "context": "size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures \u2014 have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood.", "startOffset": 136, "endOffset": 243}, {"referenceID": 1, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 11, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 19, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 14, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 17, "context": "The impact of various processing models \u2014 matrix-based models, neural networks, and hashing methods \u2014 have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., 2015; Schnabel et al., 2015; \u00d6sterlund et al., 2015; Sahlgren et al., 2016).", "startOffset": 189, "endOffset": 299}, {"referenceID": 0, "context": "The only previous work in this direction that we are aware of is Asr et al. (2016), who report that on small data (the CHILDES corpus), simple matrix-based models outperform neural network-based ones.", "startOffset": 65, "endOffset": 83}, {"referenceID": 11, "context": "We also experimented with smoothed PPMI, which raises the context counts to the power of \u03b1 and normalizes them (Levy et al., 2015), thereby countering the tendency of mutual information to favor infrequent events: f(b) = #(b) \u03b1 \u2211 b #(b) \u03b1 , but it did not lead to any consistent improvements compared to PPMI.", "startOffset": 111, "endOffset": 130}, {"referenceID": 8, "context": "A different approach to reduce the dimensionality of DSMs is to use a hashing method such as Random Indexing (RI) (Kanerva et al., 2000), which accumulates distributional vectors ~ d(a) in an online fashion:", "startOffset": 114, "endOffset": 136}, {"referenceID": 16, "context": "where c is the extension of the context window, w(b) is a weight that quantifies the importance of context term b,2 ~rd(b) is a sparse random index vector that acts as a fingerprint of context term b, and \u03c0 is a permutation that rotates the random index vectors one step to the left or right, depending on the position of the context items within the context windows, thus enabling the model to take word order into account (Sahlgren et al., 2008).", "startOffset": 424, "endOffset": 447}, {"referenceID": 5, "context": "There are many variations of DSMs that use neural networks as processing model, ranging from simple recurrent networks (Elman, 1990) to more complex deep architectures (Collobert and Weston, 2008).", "startOffset": 119, "endOffset": 132}, {"referenceID": 13, "context": "The incomparably most popular neural network model is the one implemented in the word2vec library, which uses the softmax for predicting b given a (Mikolov et al., 2013):", "startOffset": 147, "endOffset": 169}, {"referenceID": 17, "context": "the current size of the growing vocabulary), and \u03bb is a constant that we set to 60 (Sahlgren et al., 2016).", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "We opt for the ukWaC corpus (Ferraresi et al., 2008), which comprises some 1.", "startOffset": 28, "endOffset": 52}, {"referenceID": 16, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 13, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 14, "context": "These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren et al., 2008; Mikolov et al., 2013; \u00d6sterlund et al., 2015).", "startOffset": 84, "endOffset": 180}, {"referenceID": 18, "context": "All DSMs use the same parameters as far as possible with a narrow context window of \u00b12 words, which has been shown to produce good results in semantic tasks (Sahlgren, 2006; Bullinaria and Levy, 2012).", "startOffset": 157, "endOffset": 200}, {"referenceID": 7, "context": "We use five standard benchmark tests in these experiments; two multiple-choice vocabulary tests (the TOEFL synonyms and the ESL synonyms), and three similarity/relatedness rating benchmarks (SimLex-999 (SL) (Hill et al., 2015), MEN (Bruni et al.", "startOffset": 207, "endOffset": 226}, {"referenceID": 2, "context": ", 2015), MEN (Bruni et al., 2014), and Stanford Rare Words (RW) (Luong et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 12, "context": ", 2014), and Stanford Rare Words (RW) (Luong et al., 2013)).", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": "The most notable aspect of these results is that the neural networks models do not produce competitive results for the smaller data, which corroborates the results by Asr et al. (2016). The best results for the smallest data are produced by the factorized models, with both TSVD and ISVD producing top scores in different test settings.", "startOffset": 167, "endOffset": 185}, {"referenceID": 7, "context": "In the case of SL, this confirms the results in (Hill et al., 2015), and might be due to the general bias of DSMs towards semantic relatedness, rather than genuine semantic similarity, as represented in SL.", "startOffset": 48, "endOffset": 67}, {"referenceID": 0, "context": "Our experiments confirm the results of Asr et al. (2016), who show that neural network-based models are suboptimal to use for smaller amounts of data.", "startOffset": 39, "endOffset": 57}, {"referenceID": 14, "context": "We interpret this finding as further corroborating the results of Bullinaria and Levy (2012), and \u00d6sterlund et al. (2015), with the conclusion that the inverted factorized model is a robust competitive alternative to the widely used SGNS and CBOW neural network-based models.", "startOffset": 98, "endOffset": 122}], "year": 2016, "abstractText": "This paper investigates the effects of data size and frequency range on distributional semantic models. We compare the performance of a number of representative models for several test settings over data of varying sizes, and over test items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model.", "creator": "gnuplot 4.6 patchlevel 4"}}}