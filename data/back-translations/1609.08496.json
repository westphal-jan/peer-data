{"id": "1609.08496", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "abstract": "Deriving topics from the overwhelming amount of short texts becomes a critical but challenging task for many content-analytical tasks such as Content Charactering, User Interest Profiling, and Emerging Topic Detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent dirichlet allocation (LDA) do not solve this probability particularly well, since there are very limited word correlations available in short texts. In this paper, we examine how to integrate knowledge of external word correlations into short texts to improve the coherence of topic modeling. Building on the recent results of word summaries that learn semantic representations for words from a large corpus, we introduce a novel method, the Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word summaries, but occurring information by comparing short texts in a large corpus, we introduce a novel method, the embedding-based Topic Model (ETM), to reconstruct latent topics from short texts.", "histories": [["v1", "Tue, 27 Sep 2016 15:26:07 GMT  (343kb,D)", "http://arxiv.org/abs/1609.08496v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["jipeng qiang", "ping chen", "tong wang", "xindong wu"], "accepted": false, "id": "1609.08496"}, "pdf": {"name": "1609.08496.pdf", "metadata": {"source": "CRF", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "authors": ["Jipeng Qiang", "Ping Chen", "Tong Wang", "Xindong Wu"], "emails": ["qjp2100@163.com"], "sections": [{"heading": null, "text": "Keywords Topic Modeling; Short Text; Word Embdddings; Markov Random Field"}, {"heading": "1. INTRODUCTION", "text": "Topic modeling has been proven to be useful for automatic topic discovery from a huge volume of texts. Topic model views texts as a mixture of probabilistic topics, where a topic is represented by a probability distribution over words. Based on the assumption that each text of a collection is\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nmodeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26]. With the rapid development of the World Wide Web, short text has been an important information source not only in traditional web site, e.g., web page title, text advertisement, and image caption, but in emerging social media, e.g., tweet, status message, and question in Q&A websites. Compared with long texts, such as news article and academic paper, topic discovery from short texts has the following three challenges: only very limited word co-occurrence information is available, the frequency of words plays a less discriminative role, and the limited contexts make it more difficult to identify the senses of ambiguous words [22]. Therefore, LDA cannot work very well on short texts [34, 3]. Finally, how to extract topics from short texts remains a challenging research problem [10, 27].\nTwo major heuristic strategies have been adopted to deal with how to discover the latent topics from short texts. One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [33, 35]. Therefore, many models for short texts were proposed based on this simple assumption [3, 34]. But, the problem of very limited word co-occurrence information in short texts has not been solved yet. The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29]. However, these schemes are heuristic and highly dependent on the data, which is not fit for short texts such as news titles, advertisements or image captions. Figure 1 shows an example to explain the shortcomings of existing short text topic models. We can see s1 and s2 probably include two topics. \u2019Obama\u2019 and \u2019President\u2019 are likely to come from the same topic, and \u2019NBA\u2019 and \u2019Bulls\u2019 are from another topic. The simple assumption that each text is sampled from only one latent topic is unsuited to these texts. And if we directly aggregate the three short texts into two long pseudo-texts, it is very hard to decide how to aggregate these texts since they do not share the same words. But, it is very clear that s1 is more similar to s2 than s3.\nTo overcome these inherent weaknesses and keep the advantages of both strategies, we propose a novel method,\nar X\niv :1\n60 9.\n08 49\n6v 1\n[ cs\n.C L\n] 2\n7 Se\np 20\nEmbedding-based Topic Model (ETM), to discover latent topics from short texts. Our method leverages recent results by word embeddings that obtain vector representations for words[15, 19]. The authors demonstrated that semantic relationships are often preserved in vector operations on word vectors, e.g., vec(King) - vec(man) + vec(woman) is close to vec(Queen), where vex(x) denotes the vector of word x. This suggests that distances between embedded word vectors are to some degree semantically meaningful. For example, all distances in Figure 1 are computed by word embedding model [19].\nETM has the following three steps. ETM firstly builds distributed word embeddings from a large corpus, and then aggregates short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings, thus alleviates the problem of very limited word co-occurrence information in short texts. Finally, ETM discovers latent topics from pseudo-texts based on the complex assumption that each text of a collection is modeled over a set of topics. Gaining insights from [32], ETM adopts a Markov Random Field regularized model based on collapsed Gibbs sampling which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling. Within a long pseudo-text, if two words are labeled as similar according to word embedding, a binary potential function is defined to encourage them to share the same latent topic. Through this way, ETM can effectively identify the senses of ambiguous words.\nTo measure the performance of ETM, we conduct experiments on two real-world short text datasets, Tweet 2011 and Google News. Experiments demonstrate that ETM can discover more prominent and coherent topics than the baselines. When applying the learned topic proportions of texts in clustering task, we also find that ETM can infer significantly better topic distribution than the baselines.\nThe remainder of the paper is organized as follows. Section 2 discusses related work. Section 3 presents the proposed framework for short text topic modeling. Section 4 reports experimental results on real-world datasets."}, {"heading": "2. RELATED WORK", "text": "In this section, we briefly describe the related work from the following two aspects: long text topic modeling and short text topic modeling."}, {"heading": "2.1 Long Text Topic Modeling", "text": "Nigam et al. [17] proposed a mixture of unigrams model\nbased on the assumption that each document is generated by one topic. This simple assumption is often too limited to effectively model a large collection of long texts. The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4]. In a sense, the complex assumption captures the possibility that a document may contain multiple topics. Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [5] and Latent Dirichlet Allocation (LDA) [1] have shown promising results.\nIn recent years, knowledge-based topic models have been proposed, which ask human users to provide some prior domain knowledge to guide the model to produce better topics instead of purely relying on how often words co-occur in different contexts. For example, Chen and Liu encode the Must-Links (meaning that two words should be in the same topic) and Cannot-Links (meaning that two words should not be in the same topic) between words over the topic-word multinomials [2]. Besides, two recently proposed models, i.e., a quadratic regularized topic model based on semi-collapsed Gibbs sampler [16] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[32], share the idea of incorporate the correlation between words. All these models only deal with long texts, and perform poorly on short texts."}, {"heading": "2.2 Short Text Topic Modeling", "text": "The earliest works on short text topic models mainly focused on exploiting external knowledge to enrich the representation of short texts. For instance, Jin et al. [7] first found the related long texts for each short text, and learned topics over short texts and their related long texts using LDA. Phan et al. [20] learned the topics on another largescale dataset using a conventional topic model such as PLSA and LDA for short text classification. However, these models are only effective when the additional data are closely related to the original data. Furthermore, finding such additional data may be expensive or even impossible.\nAs a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29]. Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [23, 28]. Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [13, 22]. In a sense, each short text is considered to be generated from a long pseudo-document. The strategy can be regarded as an application of the authortopic model [25] to tweets, where each tweet (text) has a single author. For example, some models aggregated all the tweets of a user as a pseudo-text [29]. As these tweets with the same hashtag may come from a topic, Mehrotra et al. [13] aggregated all tweets into a pseudo-text based on hashtags. The other scheme directly aggregates short texts into long pseudo-texts through clustering methods [22], in which the clustering method will face this same problem of very limited word co-occurrence information. However, the above approaches cannot be readily applied to more general forms of short texts which provide hardly any such context information.\nRecently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [22, 34]. Therefore, many topic models adopted this assumption to discover the latent topics in short texts. Zhao et al. [35] empirically compared the data with traditional news media, and proposed a Twitter-LDA model by assuming that one tweet is generated from one topic. Yin and Wang [34] also adopted this assumption for topic inference based on Gibbs sampling. However, these models failed to solve the problem of very limited word co-occurrence information in short texts. Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[16, 32], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling."}, {"heading": "3. ALGORITHM", "text": "In this section, we discuss our method, Embedding-based Topic Model (ETM), for identifying the key topics underlying a collection of short texts. Our model ETM includes three steps. First, we build distributed word embeddings for the vocabulary of the collection. Second, we aggregate short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings. We implement K-means using a new metric, Word Mover\u2019s Distance (WMD) [9], to compute the distance between two short texts. Third, we adopt a Markov Random Field regularized model which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling. The framework of ETM is shown in Figure 2."}, {"heading": "3.1 Word Embeddings", "text": "Mikolov et al. introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [14, 15]. Word2Vec applies a standard technique such as skip-gram on the given corpus. The model avoids non-linear transformations and therefore makes training extremely efficient. This enables learning of embedded word vectors from huge datasets with billions of words, and millions of words\nin the vocabulary. Word embeddings can capture subtle semantic relationships between words, such as vec(Berlin) - vec(Germany) + vec(France)\u2248 vec(Pairs) and vec(Einstein) - vec(scientist) + vec(Picasso) \u2248 vec(painter), where vec(x) denotes the vector of word x [15].\nDifferent from Word2Vec that only utilizes local context windows, Pennington et al. later introduced a new global log-bilinear regression model, Glob2Vec, which combines global word-word co-occurrence counts from a corpus, and local context windows based learning similar to Word2Vec to deliver an improved word vector representation."}, {"heading": "3.2 Short Text Clustering", "text": "After obtaining word embeddings of each word, we use the typical cosine distance measure for the distance between words, i.e., for word vector vx and word vector vy, we define the distance\nd(vx, vy) = 1\u2212 vx \u2016 vx \u20162 \u00d7 vy\u2016 vy \u20162\n(1)\nConsider a collection of short texts, S = {s1, s2, ..., si, ..., sn}, for a vocabulary of V words, where si represents the i th text. We assume each text is represented as normalized bag-of-words (nBOW) vector, ri \u2208 RV is the vector of si, a V -dimension vector, ri,j=\nci,j\u2211V v=1 ci,v where ci,j denotes\nthe occurrence times of the jth word of the vocabulary in text si. We can see that a nBOW vector is very sparse as only a few words appear in each text. For example, given three short texts in Figure 1, if we adopt these metrics (e.g., Euclidean distance, Manhattan distance [8], Cosine Similarity ) to measure distance between two texts, it is hard to find their difference. Therefore, we introduce a new metric, called the Word Mover\u2019s Distance (WMD)[9], to compute the distance between texts. WMD computes the minimum cumulative cost that words from one text need to travel to match exactly the words of the other text as the distance of texts, in which the distance bewteen words is computed by word embeddings.\nLet ri and rj be the nBOW representation of si and sj . Each word of ri can be allowed to travel to the word of rj . Let T \u2208 Rm\u00d7m be a flow matrix, where Tu,v represents how much of the weight of word u of ri travels to word v of rj . To transform all weights of ri into rj , we guarantee that the entire outgoing flow from vertex u equals to ri,u, namely\u2211 v Tu,v = ri,u. Correspondingly, the amount of incoming\nflow to vertex v must equal to rj,v, namely, \u2211 u Tu,v = rj,v. At last, we can define the distance of two texts as the minimum cumulative cost required to flow from all words of one text to the other text, namely, \u2211 u,v Tu,vd(u, v). Given the constraints, the distance between two texts can be solved using the following linear programming,\nmax T\u22650 m\u2211 u,v Tu,vd(u, v)\nsuch that : m\u2211 v Tu,v = ri,u \u2200u \u2208 {1, 2, ...,m}\nm\u2211 u Tu,v = rj,v \u2200v \u2208 {1, 2, ...,m}\n(2)\nThe above optimization is a special case of the Earth\nMover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18]. The best average time complexity of solving the WMD problem is O(m3logm), where m is the number of unique words in the text. To speed up the optimization problem, we relax the WMD optimization problem and remove one of the two constraints. Consequently, the optimization becomes,\nmax T\u22650 m\u2211 u,v Tu,vd(u, v)\nsuch that : m\u2211 v Tu,v = ri,u \u2200u \u2208 {1, 2, ...,m} (3)\nThe optimal solution is the probability of each word in one text is moved to the most similar word in the other text. The time complexity of WMD can be reduced to O(mlogm).\nOnce the distance between texts have been computed, we aggregate short texts into long pseudo-texts based on Kmeans clustering. Given the number of long pseudo-texts L, we compute a score for each short text by averaging the distance between this text and all short texts of each long pseudo-text, that is,\nScore(si \u2208 lj) = \u2211 su\u2208lj d(si, su)\n| lj | (4)\nWhere d(si, su) is the distance between text si and su, | lj | represents the number of short texts in pseudo-text lj . In each iteration, for each short text, we choose the smallest score as its long pseudo-text. After a few iterations, we can obtain long pseudo-texts for all short texts."}, {"heading": "3.3 Topic Inference", "text": "In this subsection, we present how to infer the topics from the long pseudo-texts using Markov Random Field Regularized (MRF) Model and parameter estimation based on collapsed Gibbs sampling.\n3.3.1 Model Descritpion We adopt the MRF model to learn the latent topics which\ncan incorporate word distances into topic modeling for encouraging words labeled similarly to share the same topic assignment [32]. Here, we continue to use word embeddings to compute the distance between words. We can see from Figure 3, MRF model extends the standard LDA model [1] by imposing a Markov Random Field on the latent topic layer.\nSuppose the corpus contains K topics and long pseudotexts with L texts over V unique words in the vocabulary. Following the standard LDA, \u03a6 is represented by a K \u00d7 V matrix where the kth row \u03c6k represents the distribution of words in topic k, \u0398 is represented by a L \u00d7 K where the lth row \u03b8l represents the topic distribution for the lth long pseudo-texts, \u03b1 and \u03b2 are hyperparameters, zli denotes the topic identities assigned to the ith word in the lth long pseudo-text.\nThe key idea is that if the distance between two words in one pseudo-text is smaller than a threshold, they are more likely to belong to the same topic. For example, in Figure 1, \u2019President\u2019 and \u2019Obama\u2019 (\u2019Bulls\u2019 and \u2019NBA\u2019) are likely to belong to the same topic. Based on this idea, MRF model\ndefines a Markov Random Field over the latent topic. Given a long pseudo-text l consisting of nl words {wli}nli=1. If the distance between any word pair (wli,wlj) in l is smaller than a threshold, MRF model creates an undirected edge between their topic assignments (zli,zlj). Finally, MRF creates an undirected graph Gl for the lth pseudo-text, where nodes are latent topic assignments {zli}nli=1 and edges connect the topic assignments of correlated words. For example, in Figure 3, Gl is consisted of five nodes (zl1,zl2,zl3,zl4,zl5) and five edges {(zl1,zl2,), (zl1,zl3,), (zl2,zl4,), (zl2,zl5,), (zl3,zl5)} .\nThe same to LDA, MRF model uses the unary potential for zli as p(zli | \u03b8l). The difference is MRF model defines binary potential over each edge (zli, zlj) of Gl as exp{I(zli = zlj)}, which produces a large value if the two topic assignments are the same and generates a small value if the two topic assignments are different, where I(\u00b7) is the indicator function. Hence, similar words in one pseudo-text have a high probability to be put into the same topic. The joint probability of all topic assignments zl = {zli}nli=1 in MRF model can be calculated as\np(zl | \u03b8l, \u03bb) = nl\u220f i=1 p(zli | \u03b8l)exp{\u03bb \u2211 (li,lj)\u2208Pl I(zli = zlj) | Pl | }\n(5) where Pl represents all edges of Gl and | Pl | is the number of all edges in Gl. Here, \u03bb is a user-specified parameter that controls the tradeoff between unary potential and binary potential. If \u03bb=0, MRF model is reduced to LDA. Different from LDA that topic label zli is determined by topic distribution \u03b8l, zli in MRF model depends on both \u03b8l and the topic assignments of similar words in the lth pseudo-text.\nFormally, the generative process of MRF model is described as follows.\n1) Draw \u0398 \u223c Dirichlet(\u03b1) 2) For each topic k \u2208 [1,K] a) draw \u03c6k \u223c Dirichlet(\u03b2) 3) For each pseudo-text l in long pseudo-texts\na) draw topic assignments zl for all words in pseudo-\ntext l using Eq.(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4]. Variational Inference tends to approximate some of the parameters, such as \u03a6 and \u0398, not explicitly estimate them, may face the problem of local optimum. Therefore, different from this paper [32] based on Variational Inference, we will use collapsed Gibbs sampling to estimate parameters under Dirichlet priors in this paper.\nThese parameters that need to be estimated include the topic assignments of z, the multinomial distribution parameters \u03a6 and \u0398. Using the technique of collapsed Gibbs sampling, we only need to sample the topic assignments of z by integrating out \u03c6 and \u03b8 according to the following condition distribution:\np(zli = k | zl,\u2212li,wl,\u2212li) =(nkl,\u2212li + \u03b1) n wli k,\u2212li + \u03b2\nnk,\u2212li + V \u03b2\nexp(\u03bb\n\u2211 j\u2208Nli (zlj = k)\n| Nli | )\n(6)\nwhere zli denotes the topic assignment for word wli in the lth pseudo-text, zl,\u2212li denotes the topic assignments for all words except wli in the lth pseudo-text, n k l,\u2212li is the number of times assigned to topic k excluding wli in the lth pseudotext, n\nwli k,\u2212li is the number of times word wli assigned to topic\nk excluding wli, nk,\u2212li is the number of occurrences of all words V that belongs to topic k excluding wli, Nli denotes the words that are labeled to be similar to wi in the lth pseudo-text, and | Nli | is the number of words in Nli.\nUsing the counts of the topic assignments of long pseudotexts, we can estimate the topic-word distribution of \u03c6 and text-topic distribution \u03b8 as follows,\n\u03c6wk = nwk + \u03b2\nnk + V \u03b2 (7)\n\u03b8lk = nl,k + \u03b1\nnl +K\u03b1 (8)\nwhere nwk is the number of times word w assigned to topic k, nl,k is the number of times word wli assigned to topic k in the lth pseudo-text, and nl is the number of words in the lth pseudo-text.\nFor each short text s of long pseudo-texts, we can obtain its topic assignments as follows:\np(s = k) = ns\u220f j=1 \u03c6 wj k (9)\nwhere ns is the number of words in short text s, wj is the j th word of s.\n3.3.2 Parameter Estimation There are three types of variables (z, \u03a6 and \u0398) to be\nestimated for our model ETM. For the lth pseudo-text, the joint distribution of all known and hidden variables is given by the hyperparameters:\np(zl, \u03b8l,wl,\u03a6 | \u03b1, \u03b2, \u03bb) =p(\u03a6|\u03b2) \u00b7 nl\u220f li=1 p(wli | \u03c6zli)\n\u00b7 p(zl | \u03b8l, \u03bb) \u00b7 p(\u03b8l | \u03b1)\n(10)\nWe can obtain the likelihood of the lth pseudo-text wl of the joint event of all words by integrating out \u03c6 and \u03b8 and summing over zli. p(wl | \u03b1, \u03b2, \u03bb) = \u222b \u222b\np(\u03b8l | \u03b1)\u00b7p(\u03a6|\u03b2)\u00b7 nl\u220f li=1 p(wli | \u03c6zli ,\u03a6, \u03bb)\n(11) Finally, the likelihood of all pseudo-texts W = {wl}Ll=1 is determined by the product of the likelihood of the independent pseudo-texts:\np(W | \u03b1, \u03b2, \u03bb) = L\u220f l=1 p(wl | \u03b1, \u03b2, \u03bb) (12)\nWe try to formally derive the conditional distribution p(zli = k | zl,\u2212li,wl,\u2212li) used in our ETM algorithm as follows.\np(zli = k | zl,\u2212li,wl,\u2212li) = p(w, z | \u03b1, \u03b2, \u03bb)\np(w, zl,\u2212li | \u03b1, \u03b2, \u03bb)\n\u221d p(w, z | \u03b1, \u03b2, \u03bb) p(wl,\u2212li, zl,\u2212li | \u03b1, \u03b2, \u03bb)\n(13)\nFrom the graphical model of ETM, we can see\np(w, z | \u03b1, \u03b2, \u03bb) = p(w | z, \u03b2)p(z | \u03b1, \u03bb) (14)\nThe same to LDA, the target distribution p(w | z, \u03b2) is obtained by integrating over \u03c6,\np(w | z, \u03b2) = K\u220f\nzli=1\n\u2206(nzli + \u03b2)\n\u2206(\u03b2) ,nzli = {n (w) zli } V w=1 (15)\nwhere n (w) zli is the number of word w occurring in topic zli. Here, we adopt the \u2206 function in Heinrich (2009),\nand we can have \u2206(\u03b2) = \u220fV w=1 \u0393(\u03b2)\n\u0393(V \u03b2) and \u2206(nzli + \u03b2) =\u220f\nw\u2208w \u0393(n w k +\u03b2)\n\u0393(nk+V \u03b2) , where \u0393 denotes the gamma function. According to Equation (5), we can get\np(zl | \u03b8l, \u03bb) = exp{\u03bb \u2211 (li,lj)\u2208Pl \u2211K k=1(zlizlj) | Pl | } K\u220f k=1 \u03b8 nkl k (16)\nSimilarly, p(zl | \u03b1, \u03bb) can be obtained by integrating out \u0398 as\np(z | \u03b1, \u03bb) = \u222b p(z | \u0398, \u03bb)p(\u0398 | \u03b1)\n= L\u220f l=1 exp{\u03bb \u2211 (li,lj)\u2208Pl \u2211K k=1(zlizlj) | Pl | }\u2206(nl + \u03b1) \u2206(\u03b1)\n(17)\nwhere p(\u0398 | \u03b1) is a Dirichlet distribution, and nl = {n(k)l } K k=1.\nFinally, we put the joint distribution p(w, z | \u03b1, \u03b2, \u03bb) into Equation (13), the conditional distribution in Equation (6) can be derived\np(zli = k | zl,\u2212li,wl,\u2212li) \u221d p(w, z | \u03b1, \u03b2, \u03bb)\np(wl,\u2212li, zl,\u2212li | \u03b1, \u03b2, \u03bb)\n\u221d \u2206(nl + \u03b1) \u2206(nl,\u2212li + \u03b1) \u2206(nzli + \u03b2) \u2206(nzl,\u2212li + \u03b2) exp(\u03bb\n\u2211 j\u2208Nli (zlj = k)\n| Nli | )\n\u221d (nkl,\u2212li + \u03b1) n wli k,\u2212li + \u03b2\nnk,\u2212li + V \u03b2 exp(\u03bb\n\u2211 j\u2208Nli (zlj = k)\n| Nli | )\n(18)"}, {"heading": "4. EXPERIMENTS", "text": "In this section, we show the experimental results to demonstrate the effectiveness of our model by comparing it with five baselines on two real-world datasets."}, {"heading": "4.1 Datasets Description and Setup", "text": "Datasets: We study the empirical performance of ETM on two short text datasets.\n\u2022 Tweet2011: Tweet2011 collection is a standard short text collection published on TREC 2011 microblog track1, which includes approximately 16 million tweets sampled between January 23rd and February 8th, 2011.\n\u2022 GoogleNews: Similar to existing papers [34], we utilize Google news2 as a dataset to evaluate the performance of topic models. On Google news dataset, all news articles are grouped into clusters automatically. We took a snapshot of the Google news on April 27, 2015, and crawled the titles of 6,974 news articles belonging to 134 categories.\nFor each dataset, we conduct the following preprocessing: (1) Convert letters into lowercase; (2) Remove non-latin characters and stop words; (3) Remove words whose length are smaller than 3 or larger than 20; (4) Remove words with frequency less than 3.\nComparison Methods: We compare our model ETM3\nwith the following baselines:\n\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3]. Unigrams and DMM use the simple assumption that each text is sampled from only one latent topic. BTM learns topics by directly modeling the generation of word co-occurrence patterns in the corpus.\n\u2022 Two Long text topic models, LDA [4] and MRF-LDA [32]. LDA is the most widely used topic model. MRFLDA is one novel model designed to incorporate word knowledge into topic modeling.\nFor LDA, we use this package4 and the code5 for Unigrams, which are provided online. For BTM6 and MRFLDA7, we use the tools released by the authors. For DMM,\n1http://trec.nist.gov/data/tweets/ 2http://news.google.com 3The source code can be downloaded at https://github.com/qiang2100/ETM 4http://www.arbylon.net/projects/ 5https://github.com/ariddell/mixture-of-unigrams 6https://github.com/xiaohuiyan/BTM 7http://www.cs.cmu.edu/ pengtaox/\nwe implement its code since the authors did not release the code.\nWord Embeddings: Word2Vec [15] and Glob2Vec [19] are different word embeddings. As Glob2Vec has better performance than Word2Vec [19], the pre-trained embeddings by Glob2Vec based on Wikipedia8 is incorporated into our model and MRF-LDA.\nParameter Settings: For the baselines, we chooses the parameters according to their original papers. For LDA, Unigrams and BTM, both hyperparameters \u03b1 and \u03b2 are set to 50/K and 0.01. For DMM and ETM, both hyperparameters \u03b1 and \u03b2 are set to 0.1. For MRF-LDA, \u03b1=0.5 and \u03bb=1. For ETM, \u03bb is set to 1. For our model and MRF-LDA, words pairs with distance lower than 0.4 are labeled as correlated. The number of pseudo-texts is set as n/50, where n is the number of all short texts in the corpus."}, {"heading": "4.2 Experimental Results", "text": "4.2.1 Qualitative Evaluation On Tweet2011 dataset, there is no category information\nfor each tweet. Manual labeling might be difficult due to the incomplete and informal content of tweets. Fortunately, some tweets are labeled by their authors with hashtags in the form of \u2019#keyword\u2019 or \u2019@keyword\u2019. We manually choose 10 frequent hashtags as labels and collect documents with their hashtags. These hashtags are \u2019NBA\u2019, \u2019NASA\u2019, \u2019Art\u2019, \u2019Apple\u2019, \u2019Barackobama\u2019, \u2019Worldprayr\u2019, \u2019Starbucks\u2019, \u2019Job\u2019, \u2019Travel\u2019, \u2019Oscars\u2019, respectively.\nTable 1 shows some topics learned by the six models on the Tweet2011 dataset. Each topic is visualized by the top ten words. Words that are noisy and lack of representativeness are highlighted in bold. These four topics are about \u2019NBA\u2019, \u2019NASA\u2019, \u2019Art\u2019 and \u2019Apple\u2019, respectively. From Tabel 1, our model ETM can learn more coherent topics with fewer noisy and meaningless words than all baseline models. Long text topic models (LDA and MRF-LDA) that model each text as a mixture of topics does not fit for short texts, as short text suffers from the sparsity of word co-occurrence patterns. Because short text only consists of a few words, MRF-LDA incorporating word correlation knowledge cannot improve the coherence of topic modeling. Consequently, noise words such as better, great, good, today which cannot effectively represent a topic due to their high frequency. Compared to long text, short text probably contains only one topic. Therefore, short text topic models (Unigrams and DMM) adopt a simple assumption that each text is generated by one topic work well on short texts compared to long text topic models. Similar to Unigrams and DMM, BTM posits that unordered word-pair co-occurring in a short text share the same topic drawn from a mixture of topics that can help improve the coherence of topic modeling. But, the existing short text topic models suffer from two problems. On one hand, the frequency of words in short text plays a less discriminative role than long text, making it hard to infer which words are more correlated in each text. On the other hand, these models bring in little additional word co-occurrence information and cannot alleviate the sparsity problem. As a consequence, the topics extracted from these three short text topic models are not satisfying. For example, Topic 3 learned by Unigrams contains less relevant words such as blog, good, and check. The Apple topic (Topic 4) by DMM\n8http://nlp.stanford.edu/projects/glove/\nand BTM consists of meaning-less words such as time, good, etc.\nOur method ETM incorporates the word correlation knowledge provided by words embedding over the latent topic to cluster short texts to generate long pseudo-text. In this condition, the frequency of words in pseudo-text plays an important role to discover the topics based on this assumption each text is modeled as a mixture of topics. Simultaneously, our model ETM uses the word correlation knowledge over the latent topic to encourage correlated words to share the same topic label. Hence, although similar words may not have high co-occurrence in the corpus, they remain have a high probability to be put into the same topic. Consequently, from Table 1 we can see that the topics learned by our model are far better than those learned by the baselines. The learned topics have high coherence and contain fewer noisy and irrelevant words. Our model also can recognize the topic words that only have a few occurrences in the collection. For instance, the word flight from Topic 2, writer from topic 3, and tablet can only be recognized by our model ETM.\nTable 2 shows some topics leaned from GoogleNews dataset. The four topics are events on April 27, 2015, which are \u201dNepal earthquake\u201d, \u201dIran nuclear\u201d, \u201dIndonesia Bali\u201d, and \u201dYemen airstrikes\u201d. From this table, we observe that the topic learned by our method are not only better in coherence than those learned from long text topic models (LDA and MRF-LDA), but better than short text topic models (Unigrams, DMM, and BTM), which again demonstrates the effectiveness of our model.\n4.2.2 Quantitative Evaluation Similar to [31, 32], we also evaluate our model in a quan-\ntitative manner based on the coherence measure (CM) to assess how coherent the learned topics are. For each topic, we choose the top 10 candidate words and ask human annotators to judge whether they are relevant to the corresponding topic. First, annotators need to judge whether a topic is interpretable or not. If not, the 10 words of the topic are labeled as irrelevant, or which words are identified by annotators as relevant words for this topic. Coherence measure (CM) is defined as the ratio of the number of relevant words to the total number of candidate words. In our experiments, four graduate students participated the labeling. For each dataset, we choose 10 topics for labeling.\nTable 3 and Table 4 show the coherence measure of topics inferred on Tweet2011 and GoogleNews datasets, respectively. We can see our model ETM significantly outperforms the baseline models. On Tweet2011 dataset, ETM achieves an average coherence measure of 72.5%, which is larger than long text topic models (LDA and MRF-LDA) with a large margin. Compared to short text topic models, ETM still\nhas a big improvement. In GoogleNews dataset, our model is also much better than the baselines. In conclusion, our model produces better results on both datasets compared to the baselines, which demonstrate the effectiveness of our model in exploring word correlation knowledge from words embeddings to improve the quality of topic modeling.\n4.2.3 Short Text Clustering We further compare the performance of all models in clus-\ntering on Tweet2011 and GoogleNews datasets. To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [6, 34]. NMI is an external clustering validation metric that effectively measures the amount of statistical information shared by the random variables representing the cluster assignments and the user-labeled class assignments of the data points. NMI value is always a number between 0 and 1, where 1 represents the best result and 0 means a random text partitioning. We run each model 20 times on each dataset and report the mean and standard deviation of their NMI values.\nTable 5 shows the performance of all models on the two datasets. First, we can see that ETM performs significantly better than long text topic models (LDA and MRF-LDA). This is because long text topic models do not consider the problem that only very limited word co-occurrence information is available in short texts. Second, we can find that ETM outperforms short text topic models (Unigrams, DMM, and BTM). This is because topic models lack the mechanism to incorporate word correlation knowledge and generate the words independently. Therefore, we can conclude that our model fits for short texts compared to the baselines. Meanwhile, from the standard deviation of all results, we can see that the standard deviation of ETM is smaller than all other methods. This demonstrates that our assumption for short texts can reasonably simulate the generative process."}, {"heading": "5. CONCLUSION", "text": "We propose a novel model, Embedding-based Topic Modeling (ETM), to discover the latent topics from short texts. ETM first aggregates short texts into long pseudo-texts by\nincorporating the semantic knowledge from word embeddings, then infers topics from long pseudo-texts using Markov Random Field regularized model, which encourages words labeled as similar to share the same topic assignment. Therefore, by incorporating the semantic knowledge ETM can alleviate the problem of very limited word co-occurrence information in short texts . Experimental results on two realworld short datasets corroborate its effectiveness both qualitatively and quantitatively over the state-of-the-art methods."}, {"heading": "6. ADDITIONAL AUTHORS", "text": ""}, {"heading": "7. REFERENCES", "text": "[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent\ndirichlet allocation. the Journal of machine Learning research, 3:993\u20131022, 2003.\n[2] Z. Chen and B. Liu. Mining topics in documents: standing on the shoulders of big data. In SIGKDD, pages 1116\u20131125. ACM, 2014.\n[3] X. Cheng, X. Yan, Y. Lan, and J. Guo. Btm: Topic modeling over short texts. Knowledge and Data Engineering, IEEE Transactions on, 26(12):2928\u20132941, 2014.\n[4] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl 1):5228\u20135235, 2004.\n[5] T. Hofmann. Probabilistic latent semantic indexing. In In SIGIR, pages 50\u201357. ACM, 1999.\n[6] R. Huang, G. Yu, Z. Wang, J. Zhang, and L. Shi. Dirichlet process mixture model for document clustering with feature partition. Knowledge and Data Engineering, IEEE Transactions on, 25(8):1748\u20131759, 2013.\n[7] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. Transferring topical knowledge from auxiliary long texts for short text clustering. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 775\u2013784. ACM, 2011.\n[8] E. F. Krause. Taxicab geometry: An adventure in non-Euclidean geometry. Courier Corporation, 2012.\n[9] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger. From word embeddings to document distances. In ICML, pages 957\u2013966, 2015.\n[10] J. H. Lau, N. Collier, and T. Baldwin. On-line trend analysis with topic models:\\# twitter trends detection topic model online. In COLING, pages 1519\u20131534, 2012.\n[11] C. X. Lin, B. Zhao, Q. Mei, and J. Han. Pet: a statistical model for popular events tracking in social communities. In SIGKDD, pages 929\u2013938. ACM, 2010.\n[12] H. Ling and K. Okada. An efficient earth mover\u2019s distance algorithm for robust histogram comparison. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(5):840\u2013853, 2007.\n[13] R. Mehrotra, S. Sanner, W. Buntine, and L. Xie. Improving lda topic models for microblogs via tweet pooling and automatic labeling. In SIGIR, pages 889\u2013892, 2013.\n[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111\u20133119, 2013.\n[16] D. Newman, E. V. Bonilla, and W. Buntine. Improving topic coherence with regularized topic models. In NIPS, pages 496\u2013504, 2011.\n[17] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using em. Machine learning, 39(2-3):103\u2013134, 2000.\n[18] O. Pele and M. Werman. Fast and robust earth mover\u2019s distances. In ICCV, pages 460\u2013467, 2009.\n[19] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.\n[20] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. Learning to classify short and sparse text & web with hidden topics from large-scale data collections. In WWW, pages 91\u2013100. ACM, 2008.\n[21] O. Phelan, K. McCarthy, and B. Smyth. Using twitter to recommend real-time topical news. In Proceedings of the third ACM conference on Recommender systems, pages 385\u2013388. ACM, 2009.\n[22] X. Quan, C. Kit, Y. Ge, and S. J. Pan. Short and sparse text topic modeling via self-aggregation. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 2270\u20132276, 2015.\n[23] D. Ramage, S. T. Dumais, and D. J. Liebling. Characterizing microblogs with topic models. International AAAI Conference on Weblogs and Social Media, 5(4):130\u2013137, 2010.\n[24] Y. Rubner, C. Tomasi, and L. J. Guibas. A metric for distributions with applications to image databases. In Computer Vision, 1998. Sixth International Conference on, pages 59\u201366, 1998.\n[25] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. Griffiths. Probabilistic author-topic models for information discovery. In SIGKDD, pages 306\u2013315, 2004.\n[26] T. Wang, V. Viswanath, and P. Chen. Extended topic model for word dependency. In ACL: short paper, pages 506\u2013510, 2015.\n[27] X. Wang, Y. Wang, W. Zuo, and G. Cai. Exploring social context for topic identification in short and noisy texts. In AAAI, 2015.\n[28] Y. Wang, E. Agichtein, and M. Benzi. Tm-lda: efficient online modeling of latent topic transitions in social media. In SIGKDD, pages 123\u2013131. ACM, 2012.\n[29] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: finding topic-sensitive influential twitterers. In WSDM, pages 261\u2013270, 2010.\n[30] L. A. Wolsey and G. L. Nemhauser. Integer and combinatorial optimization. John Wiley & Sons, 2014.\n[31] P. Xie and E. P. Xing. Integrating document clustering and topic modeling. Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, 2013.\n[32] P. Xie, D. Yang, and E. P. Xing. Incorporating word\ncorrelation knowledge into topic modeling. In Conference of the North American Chapter of the Association for Computational Linguistics, 2015.\n[33] X. Yan, J. Guo, Y. Lan, J. Xu, and X. Cheng. A probabilistic model for bursty topic discovery in microblogs. In AAAI, pages 353\u2013359, 2015.\n[34] J. Yin and J. Wang. A dirichlet multinomial mixture model-based approach for short text clustering. In SIGKDD, pages 233\u2013242, 2014.\n[35] W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, and X. Li. Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, pages 338\u2013349. 2011."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Mining topics in documents: standing on the shoulders of big data", "author": ["Z. Chen", "B. Liu"], "venue": "In SIGKDD,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Btm: Topic modeling over short texts. Knowledge and Data Engineering", "author": ["X. Cheng", "X. Yan", "Y. Lan", "J. Guo"], "venue": "IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In SIGIR, pages 50\u201357", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Dirichlet process mixture model for document clustering with feature partition", "author": ["R. Huang", "G. Yu", "Z. Wang", "J. Zhang", "L. Shi"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Transferring topical knowledge from auxiliary long texts for short text clustering", "author": ["O. Jin", "N.N. Liu", "K. Zhao", "Y. Yu", "Q. Yang"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Taxicab geometry: An adventure in non-Euclidean geometry", "author": ["E.F. Krause"], "venue": "Courier Corporation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "On-line trend analysis with topic models:\\# twitter trends detection topic model online", "author": ["J.H. Lau", "N. Collier", "T. Baldwin"], "venue": "In COLING,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Pet: a statistical model for popular events tracking in social communities", "author": ["C.X. Lin", "B. Zhao", "Q. Mei", "J. Han"], "venue": "In SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "An efficient earth mover\u2019s distance algorithm for robust histogram comparison", "author": ["H. Ling", "K. Okada"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Improving lda topic models for microblogs via tweet pooling and automatic labeling", "author": ["R. Mehrotra", "S. Sanner", "W. Buntine", "L. Xie"], "venue": "In SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Improving topic coherence with regularized topic models", "author": ["D. Newman", "E.V. Bonilla", "W. Buntine"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Fast and robust earth mover\u2019s distances", "author": ["O. Pele", "M. Werman"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["X.-H. Phan", "L.-M. Nguyen", "S. Horiguchi"], "venue": "In WWW,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Using twitter to recommend real-time topical news", "author": ["O. Phelan", "K. McCarthy", "B. Smyth"], "venue": "In Proceedings of the third ACM conference on Recommender systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Short and sparse text topic modeling via self-aggregation", "author": ["X. Quan", "C. Kit", "Y. Ge", "S.J. Pan"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Characterizing microblogs with topic models", "author": ["D. Ramage", "S.T. Dumais", "D.J. Liebling"], "venue": "International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A metric for distributions with applications to image databases", "author": ["Y. Rubner", "C. Tomasi", "L.J. Guibas"], "venue": "In Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Probabilistic author-topic models for information discovery", "author": ["M. Steyvers", "P. Smyth", "M. Rosen-Zvi", "T. Griffiths"], "venue": "In SIGKDD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Extended topic model for word dependency", "author": ["T. Wang", "V. Viswanath", "P. Chen"], "venue": "In ACL: short paper,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Exploring social context for topic identification in short and noisy texts", "author": ["X. Wang", "Y. Wang", "W. Zuo", "G. Cai"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Tm-lda: efficient online modeling of latent topic transitions in social media", "author": ["Y. Wang", "E. Agichtein", "M. Benzi"], "venue": "In SIGKDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Twitterrank: finding topic-sensitive influential twitterers", "author": ["J. Weng", "E.-P. Lim", "J. Jiang", "Q. He"], "venue": "In WSDM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Integer and combinatorial optimization", "author": ["L.A. Wolsey", "G.L. Nemhauser"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Integrating document clustering and topic modeling", "author": ["P. Xie", "E.P. Xing"], "venue": "Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Incorporating word  correlation knowledge into topic modeling", "author": ["P. Xie", "D. Yang", "E.P. Xing"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A probabilistic model for bursty topic discovery in microblogs", "author": ["X. Yan", "J. Guo", "Y. Lan", "J. Xu", "X. Cheng"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A dirichlet multinomial mixture model-based approach for short text clustering", "author": ["J. Yin", "J. Wang"], "venue": "In SIGKDD,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Comparing twitter and traditional media using topic models", "author": ["W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "E.-P. Lim", "H. Yan", "X. Li"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 3, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 25, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 21, "context": "Compared with long texts, such as news article and academic paper, topic discovery from short texts has the following three challenges: only very limited word co-occurrence information is available, the frequency of words plays a less discriminative role, and the limited contexts make it more difficult to identify the senses of ambiguous words [22].", "startOffset": 346, "endOffset": 350}, {"referenceID": 33, "context": "Therefore, LDA cannot work very well on short texts [34, 3].", "startOffset": 52, "endOffset": 59}, {"referenceID": 2, "context": "Therefore, LDA cannot work very well on short texts [34, 3].", "startOffset": 52, "endOffset": 59}, {"referenceID": 9, "context": "Finally, how to extract topics from short texts remains a challenging research problem [10, 27].", "startOffset": 87, "endOffset": 95}, {"referenceID": 26, "context": "Finally, how to extract topics from short texts remains a challenging research problem [10, 27].", "startOffset": 87, "endOffset": 95}, {"referenceID": 32, "context": "One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [33, 35].", "startOffset": 249, "endOffset": 257}, {"referenceID": 34, "context": "One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [33, 35].", "startOffset": 249, "endOffset": 257}, {"referenceID": 2, "context": "Therefore, many models for short texts were proposed based on this simple assumption [3, 34].", "startOffset": 85, "endOffset": 92}, {"referenceID": 33, "context": "Therefore, many models for short texts were proposed based on this simple assumption [3, 34].", "startOffset": 85, "endOffset": 92}, {"referenceID": 12, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 21, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 28, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 14, "context": "Our method leverages recent results by word embeddings that obtain vector representations for words[15, 19].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "Our method leverages recent results by word embeddings that obtain vector representations for words[15, 19].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "For example, all distances in Figure 1 are computed by word embedding model [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "Gaining insights from [32], ETM adopts a Markov Random Field regularized model based on collapsed Gibbs sampling which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17] proposed a mixture of unigrams model based on the assumption that each document is generated by one topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 0, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 3, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 4, "context": "Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [5] and Latent Dirichlet Allocation (LDA) [1] have shown promising results.", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [5] and Latent Dirichlet Allocation (LDA) [1] have shown promising results.", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "For example, Chen and Liu encode the Must-Links (meaning that two words should be in the same topic) and Cannot-Links (meaning that two words should not be in the same topic) between words over the topic-word multinomials [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 15, "context": ", a quadratic regularized topic model based on semi-collapsed Gibbs sampler [16] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[32], share the idea of incorporate the correlation between words.", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": ", a quadratic regularized topic model based on semi-collapsed Gibbs sampler [16] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[32], share the idea of incorporate the correlation between words.", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "[7] first found the related long texts for each short text, and learned topics over short texts and their related long texts using LDA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] learned the topics on another largescale dataset using a conventional topic model such as PLSA and LDA for short text classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 213, "endOffset": 217}, {"referenceID": 28, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 252, "endOffset": 256}, {"referenceID": 22, "context": "Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [23, 28].", "startOffset": 120, "endOffset": 128}, {"referenceID": 27, "context": "Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [23, 28].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [13, 22].", "startOffset": 223, "endOffset": 231}, {"referenceID": 21, "context": "Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [13, 22].", "startOffset": 223, "endOffset": 231}, {"referenceID": 24, "context": "The strategy can be regarded as an application of the authortopic model [25] to tweets, where each tweet (text) has a single author.", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "For example, some models aggregated all the tweets of a user as a pseudo-text [29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "[13] aggregated all tweets into a pseudo-text based on hashtags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The other scheme directly aggregates short texts into long pseudo-texts through clustering methods [22], in which the clustering method will face this same problem of very limited word co-occurrence information.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [22, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 33, "context": "Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [22, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 34, "context": "[35] empirically compared the data with traditional news media, and proposed a Twitter-LDA model by assuming that one tweet is generated from one topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Yin and Wang [34] also adopted this assumption for topic inference based on Gibbs sampling.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[16, 32], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.", "startOffset": 100, "endOffset": 108}, {"referenceID": 31, "context": "Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[16, 32], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "We implement K-means using a new metric, Word Mover\u2019s Distance (WMD) [9], to compute the distance between two short texts.", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [14, 15].", "startOffset": 209, "endOffset": 217}, {"referenceID": 14, "context": "introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [14, 15].", "startOffset": 209, "endOffset": 217}, {"referenceID": 14, "context": "Word embeddings can capture subtle semantic relationships between words, such as vec(Berlin) vec(Germany) + vec(France)\u2248 vec(Pairs) and vec(Einstein) - vec(scientist) + vec(Picasso) \u2248 vec(painter), where vec(x) denotes the vector of word x [15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 7, "context": ", Euclidean distance, Manhattan distance [8], Cosine Similarity ) to measure distance between two texts, it is hard to find their difference.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Therefore, we introduce a new metric, called the Word Mover\u2019s Distance (WMD)[9], to compute the distance between texts.", "startOffset": 76, "endOffset": 79}, {"referenceID": 23, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 23, "endOffset": 31}, {"referenceID": 29, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 23, "endOffset": 31}, {"referenceID": 11, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 31, "context": "We adopt the MRF model to learn the latent topics which can incorporate word distances into topic modeling for encouraging words labeled similarly to share the same topic assignment [32].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "We can see from Figure 3, MRF model extends the standard LDA model [1] by imposing a Markov Random Field on the latent topic layer.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 3, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 303, "endOffset": 306}, {"referenceID": 31, "context": "Therefore, different from this paper [32] based on Variational Inference, we will use collapsed Gibbs sampling to estimate parameters under Dirichlet priors in this paper.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "\u2022 GoogleNews: Similar to existing papers [34], we utilize Google news as a dataset to evaluate the performance of topic models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "\u2022 Two Long text topic models, LDA [4] and MRF-LDA [32].", "startOffset": 34, "endOffset": 37}, {"referenceID": 31, "context": "\u2022 Two Long text topic models, LDA [4] and MRF-LDA [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "Word Embeddings: Word2Vec [15] and Glob2Vec [19] are different word embeddings.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Word Embeddings: Word2Vec [15] and Glob2Vec [19] are different word embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "As Glob2Vec has better performance than Word2Vec [19], the pre-trained embeddings by Glob2Vec based on Wikipedia is incorporated into our model and MRF-LDA.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "Similar to [31, 32], we also evaluate our model in a quantitative manner based on the coherence measure (CM) to assess how coherent the learned topics are.", "startOffset": 11, "endOffset": 19}, {"referenceID": 31, "context": "Similar to [31, 32], we also evaluate our model in a quantitative manner based on the coherence measure (CM) to assess how coherent the learned topics are.", "startOffset": 11, "endOffset": 19}, {"referenceID": 5, "context": "To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [6, 34].", "startOffset": 129, "endOffset": 136}, {"referenceID": 33, "context": "To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [6, 34].", "startOffset": 129, "endOffset": 136}], "year": 2016, "abstractText": "Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn semantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudotexts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}