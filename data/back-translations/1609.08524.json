{"id": "1609.08524", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS", "abstract": "In this article, we introduce UbuntuWorld 1.0 LTS - a platform for the development of automated technical support agents in the Ubuntu operating system. In particular, we propose to use the Bash terminal as a simulator of the Ubuntu environment for a learning-based agent and demonstrate the usefulness of introducing reinforcement learning techniques (RL) for basic problem solving and troubleshooting in this environment. We provide a plug-and-play interface to the simulator as a python package into which various types of agents can be plugged and evaluated, and provide ways to integrate data from online support forums such as Ask Ubuntu into the learning process of an automated agent. Finally, we show that using this data significantly improves the learning efficiency of the agent. We believe that this platform can be used as a real test bed for the exploration of automated technical support.", "histories": [["v1", "Tue, 27 Sep 2016 16:42:30 GMT  (4966kb,D)", "http://arxiv.org/abs/1609.08524v1", null], ["v2", "Sat, 12 Aug 2017 21:31:02 GMT  (4956kb,D)", "http://arxiv.org/abs/1609.08524v2", "Appeared (under the same title) in AAAI/IAAI 2017"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tathagata chakraborti", "kartik talamadupula", "kshitij p fadnis", "murray campbell", "subbarao kambhampati"], "accepted": true, "id": "1609.08524"}, "pdf": {"name": "1609.08524.pdf", "metadata": {"source": "CRF", "title": "UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS", "authors": ["Tathagata Chakraborti", "Kshitij P. Fadnis", "Murray Campbell", "Subbarao Kambhampati"], "emails": [], "sections": [{"heading": null, "text": "Building effective conversational agents has long been the holy grail of Artificial Intelligence (Turing 1950). Research in this direction has, however, largely recognized that different modes of conversation require widely different capabilities from an automated agent, depending on the particular context of the interaction; the focus has thus been on approaches targeted at specific applications. For example, conversational agents in the form of chat bots are required to be more creative, responsive and human-like; while for automation in the context of customer service, qualities like precision and brevity are more relevant. Indeed, human agents while providing customer support make a conscious effort to be as structured as possible in their interactions with the user. For successful automation in this particular mode of dialog (that we refer to as end-to-end goal-directed conversational systems or e2eGCS) we identify the following typical characteristics -\n- End-to-end. This is the ability of the agent to build and operate on knowledge directly from raw inputs as is available from the world and generate the desired behavior.\n- Goal-directed. The interaction is these settings are targeted at achieving specific goals, i.e. to solve a particular problem or reach a desired state.\n- General purpose. It is infeasible to build fundamentally different support agents for every possible environment, \u2217Bulk of the work was done during an internship (Summer, 2016) at the Thomas J. Watson Research Center, Yorktown, NY.\nand hence there must be a learning component to the agent that facilitates automated building of domain knowledge.\n- Adaptive. An agent must learn to adapt to its experience and update its knowledge, and this further underlines the importance of an agent\u2019s capability to learn.\n- Integrated. Finally, the agent must be able to interact with the human in the loop and integrate (and subsequently learn from) human intelligence in order to solve a wide variety of problems effectively.\nOne of the canonical examples of such systems is technical support. As in the case of customer service in general, automation for technical support requires an agent ascribing to the e2eGCS paradigm to be able to:\n\u2022 learn a model or understanding of its environment automatically by means of experience, data and exploration;\n\u2022 evaluate its knowledge given a context, and learn to sense for more information to solve a given problem; and\n\u2022 interact with the customer, maybe in multiple turns, in a natural fashion to solve a given problem effectively.\nIn this paper we specifically address the learning problem, and make a first attempt to lay a pathway towards achieving fully fleshed-out e2eGCS of the future. Technical support is a particular instance of customer service that deals with problems related to the operation of a specific piece of technology, which means there often exists an underlying (albeit unspecified) model to the operation of such a system, and the model learning proposition becomes especially attractive in this context. However, the critical problem here is that the engineers who build the technology, the people who use it, and the ones who provide support for it are often distinct from each other. One solution then would be to make the architects of the system also build the support engine following the same software specifications; this quickly becomes intractable (and might well require its own support!). A more worthwhile alternative is to learn this model automatically; such an approach, while being considerably simpler to follow, is also likely to be more effective in capturing domain knowledge and providing directed personalized support.\nThe specific domain we look at in this work is technical support in the Ubuntu operating system. This is undoubtedly a real-world environment where support is most sought after, as evident from the thriving community on the official online\nar X\niv :1\n60 9.\n08 52\n4v 1\n[ cs\n.A I]\n2 7\nSe p\n20 16\nUbuntu help forum Ask Ubuntu , a question and answer site for Ubuntu users and developers belonging to the Stack Exchange network of Q&A sites. Ask Ubuntu currently boasts of more than 370k registered users and 238k questions asked till date, and ranks at #3 in the family of 158 Stack Exchange communities in terms of traffic or number of users (as of August 2016). A closer look however reveals that this rank is not an indicator of the quality of support - in terms of percentage of questions actually answered (Stack Exchange 2016), Ask Ubuntu operates at a lowly rate of 65%, ranking just five places off the bottom of the list. Further, as shown in Figure 1, the number of posts that go unanswered is exploding in recent times (Ask Ubuntu 2014). While there are many causes that may have led to these dire circumstances, some of which we discuss below, one thing is quite certain - Ubuntu needs support, and there isn\u2019t enough of it out there.\nMotivation Ask Ubuntu\u2019s afflictions may be largely attributed to the following main causes -\n1. New users clogging up the system with simple problems that experienced users do not care to respond to.\n2. Duplicate questions, due to large number of users who do not bother to look up existing solutions before posting.\n3. An unhealthy newcomer to expert ratio in the community as a result of Ubuntu\u2019s rapidly growing popularity.\n4. The continuous roll out of new software/OS versions and corresponding problems with dependencies.\n5. Incompletely specified problems, including insufficient state information and error logs leaving members of the community little to work with.\nWe claim here that a large number of these problems can readily be solved through automation. While it may not be reasonable to expect an automated agent to learn the most nuanced details of the Ubuntu OS and solve niche issues that the experts on Ask Ubuntu are more capable of addressing, the large majority of problems faced by users on the forum are readily addressable, being either (1) simple problems faced by newbies that may be directly solved from the documentation, and hence may be learned from exploration in the terminal; or (2) duplicates of existing issues which may\nhave already been solved, and hence may be learned using relevant data from Ask Ubuntu. The learning approach then also indirectly addresses issues (3) by freeing up (and in turn tapping into) support from Ask Ubuntu and (4,5) since the domain knowledge built up over time as well as local state information sensed by the integrated support agent may be useful in providing more directed and personalized support.\nFigure 2 provides an illustration of the kind of tasks we are interested in. Consider the simple task of opening a text file. It can be achieved in a single step using gedit, or it can be arbitrarily harder depending on the actual state of the system - the agent might need to install gedit if it is not available, and it may need to access the internet and sudo permissions to do so. We want our agent to learn these workflows and dependencies by itself by exploring the Bash environment so that, for example, when an error comes up regarding administrative rights, the agent knows it needs to execute the sudo command. Of course, this is one of the many traces that the agent will need to explore before it learns the correct choices. Ironically, we want to turn to Ask Ubuntu itself in order to make the agent\u2019s life a little easier.\nAs we noted before, users on Ask Ubuntu have been solving similar problems for a long time, and the solutions to\ntheir issues can provide valuable guidance to the learning agent. For example, Figure 2 shows an example where the learning agent queries a TF/IDF based reverse index on the Ask Ubuntu forum data in Lucene (Lucene 2016) with the terminal output as the context, to receive a suggestion of using sudo in this state. Thus in addition to wanting our RL agent to explore and build a model of Ubuntu, we also want to make sure that the exploration is smart given the abundance of data already available on troubleshooting in Ubuntu in online forums such as Ask Ubuntu. We refer to this as data-driven reinforcement learning.\nRelated Work Bringing AI techniques \u2013 particularly reasoning and decision-making \u2013 to the problem of automated software agents has a rich history within the automated planning community. Of particular importance is the work on building softbots for UNIX by Etzioni et al. (Etzioni, Lesh, and Segal 1993; Etzioni and Weld 1994), which is the most comprehensive previous study on this problem. Indeed, as we introduced earlier, many of the issues that are cataloged in that work remain of importance today. The current work builds upon the work of Etzioni et al., particularly their focus on a goal-oriented approach to the problem (Etzioni et al. 1993); however, it goes beyond that work in actually realizing a learning-based agent framework (in our case, reinforcement learning) for the Ubuntu technical support domain. Succinctly, we seek to automate in the largest possible degree the promise of the softbot approach by: (1) exploiting the Bash shell as a robust simulator for learning agents to explore the world; and (2) using the large amounts of data generated by human experts on the internet.\nOn the learning side, Branavan et al.\u2019s work on using reinforcement learning (RL) to map natural language instructions to sequences of executable actions (Branavan et al. 2009) explores a similar problem setting in a Windows OS\ndomain. However, that work focuses on the application of RL techniques to the language processing problem, and on mapping text instructions to executable actions. In contrast, our work focuses on learning task-oriented models for solving the e2eGCS problem. Thus the most relevant prior explorations into this area are complementary to our work in different ways; while the softbot work lays the groundwork for our architecture, Branavan et al.\u2019s work provides a report on using learning on a related but different problem.\nContributions The contributions of the paper are - \u2022 We provide a platform UbuntuWorld 1.0 LTS based\non the Ubuntu OS, and its interface to the Bash terminal, where different types of agents can be plugged in and evaluated. This can be adopted as a valuable real-world test bed for research in automated technical support.\n\u2022 We propose data-driven RL as a viable solution to the model learning problem for automated technical support in Ubuntu, by augmenting human intelligence in the form of data from online technical support forums like Ask Ubuntu to aid the traditional RL process.\nUbuntuWorld 1.0 LTS The main components of the proposed system (Figure 3) are the agents, the environment and, of course, the user. The environment here is, as mentioned before, the Ubuntu operating system. The user and the agents are the two main actors in the setting, they interact with the environment, or with each other, in different capacities to perform tasks."}, {"heading": "The Agent Ecosystem", "text": "Though the user only gets to interact with a generic agent interface, the internal nature of the agent itself could be one of several types depending on the type of technology used:\nRandom Agent The Random Agent does not have any learning component, it performs actions at random till it achieves its goals. This is used as a baseline to evaluate how difficult the planning problems are and how much a learning agent can gain in terms of performance.\nPlanning Agent The Planning Agent uses PDDL (Mcdermott et al. 1998) models of (usually, a subsection of) the domain to compute plans. It is integrated with the Fast-Downward planner (Helmert 2006) that can be used to produce the optimal plan given a problem and domain description. The problem description is built on the fly given the current state being sensed by the agent and the target goal state. The domain itself may either be hand-coded from the software developer\u2019s knowledge or learned from execution traces (Zhuo, Nguyen, and Kambhampati 2013). The former can serve as the ground truth for evaluating the performance of various agents, while the latter provides a valuable baseline to compare to the other learning agents.\nRL Agent The reinforcement learning (RL) paradigm involves learning policies or models of the environment by acting and learning from experiences in the world. One of the standard forms of RL is Q-learning (Sutton and Barto 1998), where an agent learns a function Q : S \u00d7 A \u2192 R that maps state-action pairs to real values that signify the usefulness or utility of doing action a \u2208 A in state s \u2208 S. The learning step is the well-known Bellman update when a transition from state s to s\u2032 is observed due to an action a, and a reward R : S \u00d7A\u00d7 S \u2192 R is received -\nQ(s, a)\u2190 (1\u2212 \u03b1)Q(s, a) + \u03b1{R(s, a, s\u2032) + \u03b3max a\u2208A Q(s\u2032, a)}\nHere, \u03b1 is the learning rate, and \u03b3 is the discount factor. During the learning phase, the agent can do what is known as a exploration-exploitation trade-off by picking an action based on several intentions - (1) exploits the learned representation; (2) explores by querying the Ask Ubuntu data and choosing an action a that maximizes the similarity between the action documentation Da and the relevant questions and their solutions in the forum posts; and (3) explores by choosing the next action at random. The agent can pick from these choices given probability thresholds , \u03b2, and (1 \u2212 \u2212 \u03b2), and is referred to as a \u201cdata-driven\u201d -random Q-learning agent.\na|s\u2190 argmax a\u2208A Q(s, a) (1)\na|s\u2190 argmax a\u2208A Da \u2229 AskUbuntu+(Fa) (2)\na|s \u223c U(1, |A|) (3)\nFor our environment, the reward function is defined as follows. The agent gets a negative reward every time it does an action, so that it learns to prefer shorter policies. If, however, the state changes due to an action, the amount of negative reward is less, since the agent at least tried an action that was applicable in the current state. Finally, there is a large reward when the agent attains a state that models the goal.\nR(s, a, s\u2032) = \u221210 if s\u2032 6= \u22a5 += 5 if s\u2032 6|= s += 100 if s\u2032 |= G"}, {"heading": "The Environment", "text": "The Environment in our case is the Ubuntu OS, which both the agent and the user have access to via Bash commands on the terminal. Through the terminal the agent can execute actions, as well as sense the state of different environment variables and the current output on the terminal. The way these interactions are used depends on the specific type of the agent. Currently, the agents only have access to actions whose effects are all reversible, i.e. the UbuntuWorld 1.0 LTS environment is currently ergodic.\nAgent Interactions As mentioned previously, both the user and the agent can interact with the environment through the terminal to accomplish specific tasks. The user can also interact with the agent and ask it to complete basic tasks (automated problem solving), as well as invoke the agent in case she encounters an error on the terminal (automated troubleshooting). The agent may, in trying to solve a task, interact with the user in trying to find the correct parameters for an action or ask for more clarifications to solve the task, or even query Ask Ubuntu to search for a possible solution to a problem. We will demonstrate a few of these capabilities in succeeding sections.\nImplementation Details The system architecture has three main components (Figure 4) - the Agent Class, the Environment Class and the ubuntuconfig package.\nThe Agent may be asked to solve a task, or train and test on a set of problem instances. The base agent implements the Random Agent, while all the other agents such as the Planning Agent and the RL Agent inherit from it. The key difference is (1) how, given a state, the \u201dget next action\u201d process is done, e.g. the Random Agent picks the next action at random, the Planning Agent re-plans optimally from the current state and picks the first action from the remaining plan, and the Q-learning RL Agent picks the action that has the maximum Q-value in the current state; and (2) what the Agents do with the result of executing he action, e.g. the Random Agent ignores it, while the learning agents may use it to learn a representation - such as a PDDL domain or a Qfunction - of the environment. Finally the Agents also have abilities to take snapshots of themselves and reboot, and display learning curves and progress statistics during training.\nThe Environment Class acts as the interface between the Agent Class and the ubuntuconfig package by using generic interaction semantics - the agent can access the Environment by sending an action to it and receiving the output as a result of it, while the specific environment package implements the actual execution pathways and provides the action footprints to the environment. Thus specific agents and environments may be swapped in and out while their interface remains identical.\nThus the two main functionalities of the Environment Class are (1) reading in an environment description, such as from ubuntuconfig, and setting up the environment; and (2) simulating this environment as required by the agents plugged into it. It can also generate training and testing problem instances given the environment description. The UbuntuWorld Class inherits from the basic Environment Class\nand implements Ubuntu specific methods that can sense and set values of state variables as required.\nFinally, it may not always be a good idea to run the agents on an environment directly - e.g. installing software takes time, and trying to train agents whose potential performances are completely unknown may be a waste of resources. Keeping this in mind, the Environment Class also implements a wrapper that emulates an environment description without running it. This, of course, cannot be done with the full environment, since the model is not known. It can, however, in the simulation mode emulate a known part of the environment and help in debugging and setting up (for example) the parameters of the learning agent, etc.\nThe ubuntuconfig Package contains the description of the UbuntuWorld domain, i.e. the actions available to the agent, state variables that can be sensed, methods to execute each of these actions and parse their outputs, etc.\nEach action in the UbuntuWorld environment is implemented as a separate class - the individual classes implement how the interactions with the terminal play out for specific actions or commands, e.g. a permissions (sudo) check followed by a memory usage check for the apt-get command. Each action class comes with methods to execute it, get its output, and optionally check for its success (this is not used in the RL setting since the model is not known).\nThe Command Class implements the basic functionalities of all commands, including a generic interaction with the shell with or without invocation with the sudo prefix. Specific action classes inherit from it and implement their own\nparameters and shell interactions. Apart from the modular and concise nature of the command definitions, making the Ubuntu commands available as separate class objects also leaves the processing at an agent\u2019s end as general purpose as possible, with scope for caching and reuse depending on the nature of the agent. If the commands do not have any unique semantics, then these command classes are generated automatically from a list of the action names and their bindings to specific Bash commands in Ubuntu. Since this is the case most of the time (i.e. most Bash commands do not involve sophisticated interactions with the shell) this alleviates scalability concerns with this particular approach, while at the same time providing surprising flexibility with how the Ubuntu shell may be accessed by automated agents."}, {"heading": "Experiments and Looking Forward", "text": "As a preliminary evaluation of our system, the environment was set up to handle open/close, install/remove, internet access, and root privilege tasks. A sample planning domain and problem file is provided at http://bit.ly/ 2c8kJ4Q and http://bit.ly/2clwwKI to illustrate a simple open file taskNote that the state representation in the Q-function integrates the goal information as well as the current value of the state variables, in order to ensure that the agent learns goal-directed policies.\nLearning rate. Figure 5 shows the performance of a simple -random Q-learning RL Agent trained on the emulator on simple tasks involving opening files. We measure the performance of an agent in terms of the lengths of the sequences (plans) required to solve a given problem, and compare these with those of the optimal plan. This optimal length is generated by the Planning Agent using the underlying complete PDDL model (which acts as the ground truth), and the Random Planner (which acts as a simple baseline).\nThe data-driven agent. Figure 6 shows the relative convergence rates of an -random RL Agent with and without data support. The agents use feedback from the Bash terminal to query the Ask Ubuntu reverse index in Lucene (\u03b2 was varied according to a damped sine function to alternate between -random and data driven exploration). The accepted answers to the top 5 posts are then used to query the man page descriptions of the Ubuntu commands available to the agent in order to determine the set of relevant actions to perform next. The promising boost in the learning rate reiterates the need for integrating human intelligence in the form of existing data available on online technical support forums into the learning process of automated agents.\nDemo: Interacting with the agent. We offer a demonstration of our system deployed on an Ubuntu shell \u2013 a screen capture of the demonstration is available at the following link: http://bit.ly/2coKICX. First, the user asks for suggestions on how to open Firefox (or asks the agent to open Firefox), and the (trained RL) agent responds by evaluating its Q-function with the current state and available actions. Then we make the task a bit harder by uninstalling Firefox and asking again. The agent now responds by activating its root privileges, installs Firefox and opens it, thus\n(a) Learning performance of the RL agent (in terms of episode lengths) in course of training over 1000 problems instances, replayed four additional times. The episodes were terminated beyond 30 steps. The agent shows clear signs of learning beyond 3000 episodes.\n(b) Test performance (plan lengths) of the RL agent in 200 randomly generated tasks, against the optimal and the random agents. The performance is close to and mimics closely the optimal plans, while being a significant improvement from the random agent.\nFigure 5: Training and testing performances on simple tasks involving opening files from various start configurations.\ndemonstrating that it has learned simple dependencies in the Ubuntu OS and can help the user with issues with the same.\nWork in progress. As an emerging application of AI techniques, there are many avenues of extension. First, we are expanding the scope of the environment both in terms of the terminal commands available to the agent, as well as the model or representation of the world being learned. Another area of future improvement centers on the data-driven - random Q-learning agent; the current agent is a preliminary exploration into using existing unstructured data to aid the learning process. We are currently looking at augmenting the retrieval mechanism with more advanced word-embedding techniques in order to retrieve the most relevant posts from the vast amount of unstructured data available in online technical support forums like Ask Ubuntu and Ubuntu Chat Forum, and as well as structured data available as documentation in manual pages and release notes.\nFinally, the current work is able to use a basic tabular Qlearning approach because the size of the environment as well as the action space is currently quite limited. As the action and state space sizes increase, our current approach will\nhave to make way for more scalable RL approaches such as function approximation (Sutton et al. 1999), and newer approaches that take into account large action spaces in discrete domains (Dulac-Arnold et al. 2015)."}], "references": [{"title": "L", "author": ["S.R.K. Branavan", "H. Chen", "Zettlemoyer"], "venue": "S.; and Barzilay, R.", "citeRegEx": "Branavan et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep reinforcement learning in large discrete action spaces. arXiv:1512.07679v2", "author": ["Dulac-Arnold"], "venue": null, "citeRegEx": "Dulac.Arnold,? \\Q2015\\E", "shortCiteRegEx": "Dulac.Arnold", "year": 2015}, {"title": "and Weld", "author": ["O. Etzioni"], "venue": "D.", "citeRegEx": "Etzioni and Weld 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "C", "author": ["O. Etzioni", "H.M. Levy", "R.B. Segal", "Thekkath"], "venue": "A.", "citeRegEx": "Etzioni et al. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Building softbots for unix (preliminary report)", "author": ["Lesh Etzioni", "O. Segal 1993] Etzioni", "N. Lesh", "R. Segal"], "venue": "Technical report", "citeRegEx": "Etzioni et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 1993}, {"title": "Pddl - the planning domain definition language", "author": ["Mcdermott"], "venue": "Technical Report TR-98-003,", "citeRegEx": "Mcdermott,? \\Q1998\\E", "shortCiteRegEx": "Mcdermott", "year": 1998}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "S", "author": ["R.S. Sutton", "D.A. McAllester", "Singh"], "venue": "P.; Mansour, Y.; et al.", "citeRegEx": "Sutton et al. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["Turing"], "venue": "M.", "citeRegEx": "Turing 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "H", "author": ["Zhuo"], "venue": "H.; Nguyen, T.; and Kambhampati, S.", "citeRegEx": "Zhuo. Nguyen. and Kambhampati 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper we present UbuntuWorld 1.0 LTS a platform for developing automated technical support agents in the Ubuntu operating system. Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu environment for a learning-based agent, and demonstrate the usefulness of adopting reinforcement learning (RL) techniques for basic problem solving and troubleshooting in this environment. We provide a plug-and-play interface to the simulator as a python package where different types of agents can be plugged in and evaluated, and provide pathways for integrating data from online support forums like Ask Ubuntu into an automated agent\u2019s learning process. Finally, we show that the use of this data significantly improves the agent\u2019s learning efficiency. We believe that this platform can be adopted as a real-world test bed for research on automated technical support. Building effective conversational agents has long been the holy grail of Artificial Intelligence (Turing 1950). Research in this direction has, however, largely recognized that different modes of conversation require widely different capabilities from an automated agent, depending on the particular context of the interaction; the focus has thus been on approaches targeted at specific applications. For example, conversational agents in the form of chat bots are required to be more creative, responsive and human-like; while for automation in the context of customer service, qualities like precision and brevity are more relevant. Indeed, human agents while providing customer support make a conscious effort to be as structured as possible in their interactions with the user. For successful automation in this particular mode of dialog (that we refer to as end-to-end goal-directed conversational systems or e2eGCS) we identify the following typical characteristics End-to-end. This is the ability of the agent to build and operate on knowledge directly from raw inputs as is available from the world and generate the desired behavior. Goal-directed. The interaction is these settings are targeted at achieving specific goals, i.e. to solve a particular problem or reach a desired state. General purpose. It is infeasible to build fundamentally different support agents for every possible environment, \u2217Bulk of the work was done during an internship (Summer, 2016) at the Thomas J. Watson Research Center, Yorktown, NY. and hence there must be a learning component to the agent that facilitates automated building of domain knowledge. Adaptive. An agent must learn to adapt to its experience and update its knowledge, and this further underlines the importance of an agent\u2019s capability to learn. Integrated. Finally, the agent must be able to interact with the human in the loop and integrate (and subsequently learn from) human intelligence in order to solve a wide variety of problems effectively. One of the canonical examples of such systems is technical support. As in the case of customer service in general, automation for technical support requires an agent ascribing to the e2eGCS paradigm to be able to: \u2022 learn a model or understanding of its environment automatically by means of experience, data and exploration; \u2022 evaluate its knowledge given a context, and learn to sense for more information to solve a given problem; and \u2022 interact with the customer, maybe in multiple turns, in a natural fashion to solve a given problem effectively. In this paper we specifically address the learning problem, and make a first attempt to lay a pathway towards achieving fully fleshed-out e2eGCS of the future. Technical support is a particular instance of customer service that deals with problems related to the operation of a specific piece of technology, which means there often exists an underlying (albeit unspecified) model to the operation of such a system, and the model learning proposition becomes especially attractive in this context. However, the critical problem here is that the engineers who build the technology, the people who use it, and the ones who provide support for it are often distinct from each other. One solution then would be to make the architects of the system also build the support engine following the same software specifications; this quickly becomes intractable (and might well require its own support!). A more worthwhile alternative is to learn this model automatically; such an approach, while being considerably simpler to follow, is also likely to be more effective in capturing domain knowledge and providing directed personalized support. The specific domain we look at in this work is technical support in the Ubuntu operating system. This is undoubtedly a real-world environment where support is most sought after, as evident from the thriving community on the official online ar X iv :1 60 9. 08 52 4v 1 [ cs .A I] 2 7 Se p 20 16 Figure 1: Graph (http://bit.ly/2blmZk1) showing the number of zombie posts from 01/2011 to 07/2016. These are the posts that have remained unanswered for more than 72 hours, and their number is growing exponentially. Ubuntu help forum Ask Ubuntu , a question and answer site for Ubuntu users and developers belonging to the Stack Exchange network of Q&A sites. Ask Ubuntu currently boasts of more than 370k registered users and 238k questions asked till date, and ranks at #3 in the family of 158 Stack Exchange communities in terms of traffic or number of users (as of August 2016). A closer look however reveals that this rank is not an indicator of the quality of support in terms of percentage of questions actually answered (Stack Exchange 2016), Ask Ubuntu operates at a lowly rate of 65%, ranking just five places off the bottom of the list. Further, as shown in Figure 1, the number of posts that go unanswered is exploding in recent times (Ask Ubuntu 2014). While there are many causes that may have led to these dire circumstances, some of which we discuss below, one thing is quite certain Ubuntu needs support, and there isn\u2019t enough of it out there. Motivation Ask Ubuntu\u2019s afflictions may be largely attributed to the following main causes 1. New users clogging up the system with simple problems that experienced users do not care to respond to. 2. Duplicate questions, due to large number of users who do not bother to look up existing solutions before posting. 3. An unhealthy newcomer to expert ratio in the community as a result of Ubuntu\u2019s rapidly growing popularity. 4. The continuous roll out of new software/OS versions and corresponding problems with dependencies. 5. Incompletely specified problems, including insufficient state information and error logs leaving members of the community little to work with. We claim here that a large number of these problems can readily be solved through automation. While it may not be reasonable to expect an automated agent to learn the most nuanced details of the Ubuntu OS and solve niche issues that the experts on Ask Ubuntu are more capable of addressing, the large majority of problems faced by users on the forum are readily addressable, being either (1) simple problems faced by newbies that may be directly solved from the documentation, and hence may be learned from exploration in the terminal; or (2) duplicates of existing issues which may Figure 2: Use case querying Ask Ubuntu for guidance. have already been solved, and hence may be learned using relevant data from Ask Ubuntu. The learning approach then also indirectly addresses issues (3) by freeing up (and in turn tapping into) support from Ask Ubuntu and (4,5) since the domain knowledge built up over time as well as local state information sensed by the integrated support agent may be useful in providing more directed and personalized support. Figure 2 provides an illustration of the kind of tasks we are interested in. Consider the simple task of opening a text file. It can be achieved in a single step using gedit, or it can be arbitrarily harder depending on the actual state of the system the agent might need to install gedit if it is not available, and it may need to access the internet and sudo permissions to do so. We want our agent to learn these workflows and dependencies by itself by exploring the Bash environment so that, for example, when an error comes up regarding administrative rights, the agent knows it needs to execute the sudo command. Of course, this is one of the many traces that the agent will need to explore before it learns the correct choices. Ironically, we want to turn to Ask Ubuntu itself in order to make the agent\u2019s life a little easier. As we noted before, users on Ask Ubuntu have been solving similar problems for a long time, and the solutions to Figure 3: A schematic representation of the UbuntuWorld 1.0 LTS prototype. their issues can provide valuable guidance to the learning agent. For example, Figure 2 shows an example where the learning agent queries a TF/IDF based reverse index on the Ask Ubuntu forum data in Lucene (Lucene 2016) with the terminal output as the context, to receive a suggestion of using sudo in this state. Thus in addition to wanting our RL agent to explore and build a model of Ubuntu, we also want to make sure that the exploration is smart given the abundance of data already available on troubleshooting in Ubuntu in online forums such as Ask Ubuntu. We refer to this as data-driven reinforcement learning. Related Work Bringing AI techniques \u2013 particularly reasoning and decision-making \u2013 to the problem of automated software agents has a rich history within the automated planning community. Of particular importance is the work on building softbots for UNIX by Etzioni et al. (Etzioni, Lesh, and Segal 1993; Etzioni and Weld 1994), which is the most comprehensive previous study on this problem. Indeed, as we introduced earlier, many of the issues that are cataloged in that work remain of importance today. The current work builds upon the work of Etzioni et al., particularly their focus on a goal-oriented approach to the problem (Etzioni et al. 1993); however, it goes beyond that work in actually realizing a learning-based agent framework (in our case, reinforcement learning) for the Ubuntu technical support domain. Succinctly, we seek to automate in the largest possible degree the promise of the softbot approach by: (1) exploiting the Bash shell as a robust simulator for learning agents to explore the world; and (2) using the large amounts of data generated by human experts on the internet. On the learning side, Branavan et al.\u2019s work on using reinforcement learning (RL) to map natural language instructions to sequences of executable actions (Branavan et al. 2009) explores a similar problem setting in a Windows OS domain. However, that work focuses on the application of RL techniques to the language processing problem, and on mapping text instructions to executable actions. In contrast, our work focuses on learning task-oriented models for solving the e2eGCS problem. Thus the most relevant prior explorations into this area are complementary to our work in different ways; while the softbot work lays the groundwork for our architecture, Branavan et al.\u2019s work provides a report on using learning on a related but different problem. Contributions The contributions of the paper are \u2022 We provide a platform UbuntuWorld 1.0 LTS based on the Ubuntu OS, and its interface to the Bash terminal, where different types of agents can be plugged in and evaluated. This can be adopted as a valuable real-world test bed for research in automated technical support. \u2022 We propose data-driven RL as a viable solution to the model learning problem for automated technical support in Ubuntu, by augmenting human intelligence in the form of data from online technical support forums like Ask Ubuntu to aid the traditional RL process.", "creator": "LaTeX with hyperref package"}}}