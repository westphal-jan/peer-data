{"id": "1609.09171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Empirical Evaluation of RNN Architectures on Sentence Classification Task", "abstract": "Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP, and the two most popular RNN architectures are Tail Model and Pooling Model. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare the performance of the three RNN structures in sentence classification. Experimental results show that the tail model and the hybrid model consistently perform better than the pooling model, and the hybrid model is comparable to the tail model.", "histories": [["v1", "Thu, 29 Sep 2016 01:53:08 GMT  (170kb)", "http://arxiv.org/abs/1609.09171v1", null], ["v2", "Sat, 8 Oct 2016 15:16:57 GMT  (195kb)", "http://arxiv.org/abs/1609.09171v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lei shen", "junlin zhang"], "accepted": false, "id": "1609.09171"}, "pdf": {"name": "1609.09171.pdf", "metadata": {"source": "CRF", "title": "Empirical Evaluation of RNN Architectures on Sentence Classification Task", "authors": ["Lei Shen", "Junlin Zhang"], "emails": ["lorashen@126.com,", "zhangjlh@chanjet.com"], "sections": [{"heading": null, "text": "Keywords: RNN\u00b7LSTM\u00b7Sentence Classification"}, {"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20]. RNN is now the most popular method in sentence classification which is also a typical NLP task.\nThere are two widely used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately. However, there is no work focusing on comparing the performance of these various network architectures.\nThis paper presents the first empirical study using LSTMs to evaluate various RNN structures on sentence classification task. We also present a hybrid architecture that combines \u201cTail Model\u201d with \u201cPooling Model\u201d in this paper. Experimental results show that the \u201cTail Model\u201d and \u201cHybrid Model\u201d consistently get a better performance over \u201cPooling Model\u201d, and \u201cHybrid Model\u201d is comparable with \u201cTail Model\u201d."}, {"heading": "2 Related Work", "text": "A recurrent neural network [6] is introduced to process arbitrary length sequence and widely used in nowadays NLP tasks. Since simple recurrent network is difficult to\ntrain because the gradients will either vanish or explode [1], various improvements to the basic architecture were proposed and the Long Short Term Memory network is perhaps the most successful one. LSTM was originally introduced by Hochreiter and Schmidhuber [10] and in recent years some kinds of simplified LSTM were also introduced such as Gated Recurrent Units [3]. LSTM is used in our comparison since LSTM is comparable to GRU and the two both outperform basic RNN [4]. We use the LSTM structure that is introduced by Gers et al. [7] for comparison, since Greff et al. [9] evaluate the LSTM variants and find the model by Gers et al. performs comparable with other variants.\nRNN has been successfully applied in NLP tasks and various RNN structures have been proposed. However, to our best knowledge, we didn\u2019t notice any work that evaluates the performance of the various RNN structures. So the following part of this paper will present the model comparison on sentence classification task."}, {"heading": "3 Model", "text": "There exist two commonly used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately [15,19,20]. They both use RNN to provide feature layer for the fully connected layer in classification tasks. On the other hand, we regard the two different structures may be complementary for each other. So the third RNN structure is proposed in this paper and it will be called \u201cHybrid Model\u201d in the following part of paper.\nFigure 1 demonstrates the network structure of the \u201cTail BLSTM Model\u201d in which the hidden layer is the concatenated hidden states of last node in the forward RNN and backward RNN. So the hidden layer is represented as following:\nh = \u210e\u20d7 , \u210e\u20d6 (1)\nWhere \u210e\u20d7 and \u210e\u20d6 mean the hidden state of the last node in forward RNN and backward\nRNN respectively. It\u2019s obvious that \u210e\u20d7 is the semantic mapping of sentence in forward\ndirection and \u210e\u20d6 contains the semantic information of sentence in reverse direction. \u210e can be regarded as the sentence feature and is fed into the next fully connected layer for classification:\n= ( \u210e + ) (2)\nHere is a softmax function in all of our experiments.\nThe other widely used RNN structure is \u201cPooling Model\u201d (Fig.2). The pooling BLSTM model can be regarded as each BLSTM node\u2019s voting for the feature layer and \u201cmean pooling\u201d is a common model for voting. \u201cMean pooling\u201d calculates the\nvalue of the i\u2019th position in vector \u210e by averaging the corresponding value of each hidden state vector from BLSTM nodes as following:\n\u210e\n= \u2211\n(3)\nHere we suppose the length of BLSTM sequence is m and \u210e is the hidden state vec-\ntor of the j\u2019th BLSTM node. The alternative for the \u201cmean pooling\u201d is \u201cmax pooling\u201d, which selects the max value of each position in all hidden state vectors. That implies the value of the i\u2019th\nposition in vector \u210e is calculated as:\n\u210e\n= max \u210e 1 \u2264 \u2264 (4)\nWe didn\u2019t find any previous work that compares the performance of these two pooling methods(mean vs. max), so we also design some experiments to compare that.\nIt\u2019s natural to speculate that the \u201cTail Model\u201d and \u201cPooling Model\u201d can be complementary for each other as there would be feature fusion. So we propose the third model called \u201cHybrid model\u201d (Fig.3) which concatenates two different feature layers as following:\n\u210e = \u210e\u20d7 , \u210e\u20d6 , \u210e (5)\nThe fully connected layer of \u201cHybrid Model\u201d is also represented by Eq. (2). The Tail, Pooling and Hybrid models based on LSTM are similar to BLSTM."}, {"heading": "4 Datasets and Experimental Setup", "text": ""}, {"heading": "4.1 Dataset", "text": "In order to evaluate the performance of the aforementioned three RNN structures, we design experiments on the following datasets for sentence classification task:\nMR: The dataset of movie reviews with one sentence per review. The task is to detect positive/negative reviews [17].\nSST-1: Stanford Sentiment Treebank\u2014an extension of Movie Review but with fine-grained labels (very positive, positive, neutral, negative, very negative).The dataset is split into train/dev/test set and re-labeled by Socher et al. [18].\nSST-2: Same data as SST-1 but only with binary labels. All neutral reviews are removed. The dataset is also split into train/dev/test set.\nSubj: Subjectivity dataset. The task is to classify sentences into two categories: subjective and objective [16].\nTREC: TREC question classification dataset, which classifies a question into 6 question types (person, location, numeric information, etc.). The dataset is split into train/dev/test set [13].\nCR: Customer reviews of various products (cameras, MP3s etc.). The task is to predict positive/negative reviews [11].\nThe statistics of the datasets are in table 1."}, {"heading": "4.2 Experiment Design", "text": "We design two groups of experiments: one is based on the BLSTM and the other is based on LSTM model. And the \u201cPooling Model\u201d and the \u201cHybrid Model\u201d each contains two experiments in order to compare the performance of max pooling and mean pooling strategy. Thus there are five experiments in each group for each dataset to evaluate the performance of these RNN architectures."}, {"heading": "4.3 Parameters and Setup", "text": "We use word2vec vectors which were trained on 100 billion words from Google News as word embedding which can be accessed publicly. The vectors\u2019 dimensionality is 300 and those words not in the vectors are set randomly. We make the embedding static through all experiments to avoid the influence of the word embedding parameters in comparison. The initial weights of network parameters are drawn from a uniform distribution with standard deviation of 0.08 and the dropout is set to be 0.5.\nInstead of achieving best performance for one single model, our focus is to compare the model\u2019s performance on sentence classification task under comparable condition. In order to make the parameters of the RNN models comparable, we set unidirectional LSTM\u2019s unit size of hidden layer to be 300 and bidirectional LSTM\u2019s unit size to be 185. Thus parameter size will be 7.2*100000 for both unidirectional and bidirectional LSTM in all experiments.\nThe Cross-Entropy criterion is used as loss function and stochastic gradient descent with Adagrad [5] is used for optimization. Gradients are computed through full BPTT for LSTM [8]. For those datasets that have no standard dev set, we randomly choose 10% of the training set as the dev set."}, {"heading": "5 Results and Analysis", "text": "The experimental results are listed in the table 2(LSTM based models) and table 3(BLSTM based models).\nbest run result either from \u201cTail Model\u201d or \u201cHybrid Model\u201d, we can see the performance boost ranging from 1.10% to 7.85% on different datasets (Table 4). Similar performance improvements can also be observed in BLSTM models on all datasets (Table 5). The following reason may explain the experiment results: any word in sentence can vote for the final classification in \u201cPooling Model\u201d and that may weaken the obvious feature\u2019s function in sentence classification tasks.\nThe above experiment results analysis suggests we should give priority to the \u201cTail Model\u201d in sentence classification tasks given the three optional model choices since it is more simple than \u201cHybrid Model\u201d.\nThough the performance boosts of several experiments are not large enough to show an overwhelming advantage, we believe that the BLSTM model has advantage because of the diversity of datasets and models in experiments. These experiment results probably indicate the BLSTM model can have better performance over LSTM model if both models use similar parameter settings and parameter number. We speculate this minor advantage may come from the BLSTM model\u2019s ability of capturing more features though these extra features don\u2019t have much effect on the model performance.\nAs for the pooling strategy, it seems that there is no obvious winner for max pooling and mean pooling model."}, {"heading": "6 Conclusion", "text": "In this paper, we present the first empirical study using LSTMs to evaluate performance of various RNN structures on sentence classification task. We also present a hybrid architecture that combines \u201cTail Model\u201d with \u201cPooling Model\u201d in this paper. Experimental results show that the \u201cTail Model\u201d and \u201cHybrid Model\u201d consistently get a better performance over \u201cPooling Model\u201d, and \u201cHybrid Model\u201d is comparable to \u201cTail Model\u201d.\nReferences\n1. Bengio, Y., Simard, P., Frasconi, P.: Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166 (1994) 2. Cho, K., van Merrienboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: Encoder\u2013decoder approaches. In: Proceedings of Workshop on SSST (2014) 3. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using RNN encoder-decoder for statistical machine translation. In: Proceedings of EMNLP (2014) 4. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recurrent neural networks on sequence modeling. In: Proceedings of NIPS (2014) 5. Duchi, J., Hazan, E., Singer, Y.: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 12:2121\u20132159 (2011) 6. Elman, J.L.: Finding structure in time. Cognitive Science, 14(2):179--211 (1990) 7. Gers, A., Schmidhuber, J., Xummins, F.: Learning to forget: Continual prediction with\nLSTM. In: Proceedings of ICANN (1999) 8. Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional LSTM\nand other neural network architectures. Neural Networks, 18(5-6): 602-610 (2005) 9. Greff, K., Srivastava, R., Koutnik, J., Steunebrink, B., Schmidhuber, J.: LSTM: A search\nspace odyssey. In: arXiv:1503.04069 (2015) 10. Hochreiter, S., Schmidhuber, J.: Long short term memory. Neural Computation 9(8):\n1735-1780 (1997) 11. Hu, M., Liu, B.: Mining and Summarizing Customer Reviews. In: Proceedings of ACM\nSIGKDD (2004)\n12. Iyyer, M., Boyd-Graber, J., Claudino,L., Socher, R., Daume III, H.: A neural network for factoid question answering over paragraphs. In: Proceedings of EMNLP (2014) 13. Li, X., Roth, D.: Learning question classifiers. In: Proceedings of ACL (2002) 14. Liu,P., Qiu,X., Huang, X.: Recurrent neural network for text classification with multi-task\nlearning. In: Proceedings of IJCAI (2016) 15. Oliveira, L., Rodrigo, A.: Humor detection in Yelp reviews.\nhttps://cs224d.stanford.edu/reports/OliveiraLuke.pdf 16. Pang, B., Lee, L.: A sentimental education: Sentiment analysis using subjectivity\nsummarization based on minimum cuts. In: Proceedings of ACL (2004) 17. Pang, B., Lee, L.: Seeing stars: Exploiting class relationships for sentiment categorization\nwith respect to rating scales. In: Proceedings of ACL (2005) 18. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., Potts, C.: Recursive\ndeep models for semantic compositionality over a sentiment treebank. In: Proceedings of EMNLP (2013) 19. Tang, D., Qin, B., Liu, T.: Document modeling with gated recurrent neural network for sentiment classification. In: Proceedings of EMNLP (2015) 20. Wang, D., Nyberg, E.: A Long short-term memory model for answer sentence selection in question answering. In: Proceedings of ACL (2015)"}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, 5(2):157-166", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Proceedings of Workshop on SSST", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "Proceedings of NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14(2):179--211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["A. Gers", "J. Schmidhuber", "F. Xummins"], "venue": "Proceedings of ICANN", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5-6): 602-610", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R. Srivastava", "J. Koutnik", "B. Steunebrink", "J. Schmidhuber"], "venue": "arXiv:1503.04069", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8): 1735-1780", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Mining and Summarizing Customer Reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of ACM SIGKDD", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daume III"], "venue": "Proceedings of EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "Proceedings of ACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "Proceedings of IJCAI", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of ACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of EMNLP", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A Long short-term memory model for answer sentence selection in question answering", "author": ["D. Wang", "E. Nyberg"], "venue": "Proceedings of ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 11, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 13, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 17, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 18, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 5, "context": "A recurrent neural network [6] is introduced to process arbitrary length sequence and widely used in nowadays NLP tasks.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "train because the gradients will either vanish or explode [1], various improvements to the basic architecture were proposed and the Long Short Term Memory network is perhaps the most successful one.", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "LSTM was originally introduced by Hochreiter and Schmidhuber [10] and in recent years some kinds of simplified LSTM were also introduced such as Gated Recurrent Units [3].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "LSTM was originally introduced by Hochreiter and Schmidhuber [10] and in recent years some kinds of simplified LSTM were also introduced such as Gated Recurrent Units [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "LSTM is used in our comparison since LSTM is comparable to GRU and the two both outperform basic RNN [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "[7] for comparison, since Greff et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] evaluate the LSTM variants and find the model by Gers et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "There exist two commonly used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately [15,19,20].", "startOffset": 119, "endOffset": 129}, {"referenceID": 18, "context": "There exist two commonly used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately [15,19,20].", "startOffset": 119, "endOffset": 129}, {"referenceID": 15, "context": "The task is to detect positive/negative reviews [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The task is to classify sentences into two categories: subjective and objective [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "The dataset is split into train/dev/test set [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "The task is to predict positive/negative reviews [11].", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "The Cross-Entropy criterion is used as loss function and stochastic gradient descent with Adagrad [5] is used for optimization.", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Gradients are computed through full BPTT for LSTM [8].", "startOffset": 50, "endOffset": 53}], "year": 2016, "abstractText": "Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are \u201cTail Model\u201d and \u201cPooling Model\u201d. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the \u201cTail Model\u201d and \u201cHybrid Model\u201d consistently get a better performance over \u201cPooling Model\u201d, and \u201cHybrid Model\u201d is comparable with \u201cTail Model\u201d.", "creator": "\u00fe\u00ff"}}}