{"id": "1609.09315", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders", "abstract": "We present a novel semi-supervised approach to sequence transduction and apply it to semantic parsing. The unattended component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a set of semantic parsing tasks that focus on areas with limited access to marked training data, and expand these data sets with synthetically generated logical forms.", "histories": [["v1", "Thu, 29 Sep 2016 12:20:13 GMT  (807kb,D)", "http://arxiv.org/abs/1609.09315v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["tom\u00e1s kocisk\u00fd", "g\u00e1bor melis", "edward grefenstette", "chris dyer", "wang ling", "phil blunsom", "karl moritz hermann"], "accepted": true, "id": "1609.09315"}, "pdf": {"name": "1609.09315.pdf", "metadata": {"source": "CRF", "title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders", "authors": ["Tom\u00e1\u0161 Ko\u010disk\u00fd", "G\u00e1bor Melis", "Edward Grefenstette", "Chris Dyer", "Wang Ling", "Phil Blunsom", "Karl Moritz Hermann"], "emails": ["tkocisky@google.com", "melisgl@google.com", "etg@google.com", "cdyer@google.com", "lingwang@google.com", "pblunsom@google.com", "kmh@google.com"], "sections": [{"heading": null, "text": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms."}, {"heading": "1 Introduction", "text": "Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation (Bahdanau et al., 2015), syntactic constituency parsing (Vinyals et al., 2015), and semantic role labelling (Zhou and Xu, 2015). A key requirement for effectively training such models is an abundance of supervised data.\nIn this paper we focus on learning mappings from input sequences x to output sequences y in domains where the latter are easily obtained, but annotation in the form of (x, y) pairs is sparse or expensive to produce, and propose a novel architecture that accommodates semi-supervised training on sequence transduction tasks. To this end, we augment the transduction objective (x 7\u2192 y) with an autoencoding objective where the input sequence is treated as a latent variable (y 7\u2192 x 7\u2192 y), enabling training from both labelled pairs and unpaired output sequences.\nThis is common in situations where we encode natural language into a logical form governed by some grammar or database.\nWhile such an autoencoder could in principle be constructed by stacking two sequence transducers, modelling the latent variable as a series of discrete symbols drawn from multinomial distributions creates serious computational challenges, as it requires marginalising over the space of latent sequences \u03a3\u2217x. To avoid this intractable marginalisation, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in \u03a3x from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood.\nWe demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the GEOQUERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks.\nIn most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data.\nar X\niv :1\n60 9.\n09 31\n5v 1\n[ cs\n.C L\n] 2\n9 Se\np 20"}, {"heading": "2 Model", "text": "Our sequential autoencoder is shown in Figure 1. At a high level, it can be seen as two sequenceto-sequence models with attention (Bahdanau et al., 2015) chained together. More precisely, the model consists of four LSTMs (Hochreiter and Schmidhuber, 1997), hence the name SEQ4. The first, a bidirectional LSTM, encodes the sequence y; next, an LSTM with stochastic output, described below, draws a sequence of distributions x\u0303 over words in vocabulary \u03a3x. The third LSTM encodes these distributions for the last one to attend over and reconstruct y as y\u0302. We now give the details of these parts."}, {"heading": "2.1 Encoding y", "text": "The first LSTM of the encoder half of the model reads the sequence y, represented as a sequence of one-hot vectors over the vocabulary \u03a3y, using a bidirectional RNN into a sequence of vectors hy1:Ly where Ly is the sequence length of y,\nhyt = ( f\u2192y (yt, h y,\u2192 t\u22121 ); f \u2190 y (yt, h y,\u2190 t+1 ) ) , (1)\nwhere f\u2192y , f \u2190 y are non-linear functions applied at each time step to the current token yt and their recurrent states hy,\u2192t\u22121 , h y,\u2190 t+1 , respectively.\nBoth the forward and backward functions project the one-hot vector into a dense vector via an embedding matrix, which serves as input to an LSTM."}, {"heading": "2.2 Predicting a Latent Sequence x\u0303", "text": "Subsequently, we wish to predict x. Predicting a discrete sequence of symbols through draws from multinomial distributions over a vocabulary is not an option, as we would not be able to backpropagate through this discrete choice. Marginalising over the possible latent strings or estimating the gradient through na\u0131\u0308ve Monte Carlo methods would be a prohibitively high variance process because the number of strings is exponential in the maximum length (which we would have to manually specify) with the vocabulary size as base. To allow backpropagation, we instead predict a sequence of distributions x\u0303 over the symbols of \u03a3x with an RNN attending over\ny1 y2 y3 y4 < s >\nx\u03031 x\u03032\n\u00b52, log( 2 )2\u00b51, log( 2 )1\nx\u03033\n\u00b53, log( 2)3\n\u270f1 \u270f2 \u270f3\nhx1 h x 2 h x 3\nhx\u03031 h x\u0303 2 h x\u0303 3h y 1 h y 2 h y 3 h y 4\nhy\u03021 h y\u0302 2 h y\u0302 3 h y\u0302 4\n< s >\ny\u03021 y\u03022 y\u03023 y\u03024\ny\u03021 y\u03022 y\u03023\nFigure 2: Unsupervised case of the SEQ4 model.\nhy = hy1:Ly , which will later serve to reconstruct y:\nx\u0303 = q(x|y) = Lx\u220f t=1 q(x\u0303t|{x\u03031, \u00b7 \u00b7 \u00b7 , x\u0303t\u22121}, hy) (2)\nwhere q(x|y) models the mapping y 7\u2192 x. We define q(x\u0303t|{x\u03031, \u00b7 \u00b7 \u00b7 , x\u0303t\u22121}, hy) in the following way:\nLet the vector x\u0303t be a distribution over the vocabulary \u03a3x drawn from a logistic-normal distribution1, the parameters of which, \u00b5t, log(\u03c32)t \u2208 R|\u03a3x|, are predicted by attending by an LSTM attending over the outputs of the encoder (Equation 2), where |\u03a3x| is the size of the vocabulary \u03a3x. The use of a logistic normal distribution serves to regularise the model in the semi-supervised learning regime, which is described at the end of this section. Formally, this process, depicted in Figure 2, is as follows:\nhx\u0303t = fx\u0303(x\u0303t\u22121, h x\u0303 t\u22121, h y) (3)\n\u00b5t, log(\u03c3 2 t ) = l(h x\u0303 t ) (4)\n\u223c N (0, I) (5) \u03b3t = \u00b5t + \u03c3t (6)\nx\u0303t = softmax(\u03b3t) (7)\nwhere the fx\u0303 function is an LSTM and l a linear transformation to R2|\u03a3x|. We use the reparametrisation trick from Kingma and Welling (2014) to draw from the logistic normal, allowing us to backpropagate through the sampling process.\n1The logistic-normal distribution is the exponentiated and normalised (i.e. taking softmax) normal distribution."}, {"heading": "2.3 Encoding x", "text": "Moving on to the decoder part of our model, in the third LSTM, we embed2 and encode x\u0303:\nhxt = ( f\u2192x (x\u0303t, h x,\u2192 t\u22121 ); f \u2190 x (x\u0303t, h x,\u2190 t+1 ) ) (8)\nWhen x is observed, during supervised training and also when making predictions, instead of the distribution x\u0303 we feed the one-hot encoded x to this part of the model."}, {"heading": "2.4 Reconstructing y", "text": "In the final LSTM, we decode into y:\np(y\u0302|x\u0303) = Ly\u220f t=1 p(y\u0302t|{y\u03021, \u00b7 \u00b7 \u00b7 , y\u0302t\u22121}, hx\u0303) (9)\nEquation 9 is implemented as an LSTM attending over hx\u0303 producing a sequence of symbols y\u0302 based on recurrent states hy\u0302, aiming to reproduce input y:\nhy\u0302t = fy\u0302(y\u0302t\u22121, h y\u0302 t\u22121, h x\u0303) (10)\ny\u0302t \u223c softmax(l\u2032(hy\u0302t )) (11)\nwhere fy\u0302 is the non-linear function, and the actual probabilities are given by a softmax function after a linear transformation l\u2032 of hy\u0302. At training time, rather than y\u0302t\u22121 we feed the ground truth yt\u22121."}, {"heading": "2.5 Loss function", "text": "The complete model described in this section gives a reconstruction function y 7\u2192 y\u0302. We define a loss on this reconstruction which accommodates the unsupervised case, where x is not observed in the training data, and the supervised case, where (x, y) pairs are available. Together, these allow us to train the SEQ4 model in a semi-supervised setting, which experiments will show provides some benefits over a purely supervised training regime.\nUnsupervised case When x isn\u2019t observed, the loss we minimise during training is the reconstruction loss on y, expressed as the negative loglikelihood NLL(y\u0302, y) of the true labels y relative to the predictions y\u0302. To this, we add as a regularising\n2Multiplying the distribution over words and an embedding matrix averages the word embedding of the entire vocabulary weighted by their probabilities.\nterm the KL divergence KL[q(\u03b3|y)\u2016p(\u03b3)] which effectively penalises the mean and variance of q(\u03b3|y) from diverging from those of a prior p(\u03b3), which we model as a diagonal Gaussian N (0, I). This has the effect of smoothing the logistic normal distribution from which we draw the distributions over symbols of x, guarding against overfitting of the latent distributions over x to symbols seen in the supervised case discussed below. The unsupervised loss is therefore formalised as\nLunsup = NLL(y\u0302, y) + \u03b1KL[q(\u03b3|y)\u2016p(\u03b3)] (12)\nwith regularising factor \u03b1 is tuned on validation, and\nKL[q(\u03b3|y)\u2016p(\u03b3)] = Lx\u2211 i=1 KL[q(\u03b3i|y)\u2016p(\u03b3)] (13)\nWe use a closed form of these individual KL divergences, described by Kingma and Welling (2014).\nSupervised case When x is observed, we additionally minimise the prediction loss on x, expressed as the negative log-likelihoodNLL(x\u0303, x) of the true labels x relative to the predictions x\u0303, and do not impose the KL loss. The supervised loss is thus\nLsup = NLL(x\u0303, x) +NLL(y\u0302, y) (14)\nIn both the supervised and unsupervised case, because of the continuous relaxation on generating x\u0303 and the reparameterisation trick, the gradient of the losses with regard to the model parameters is well defined throughout SEQ4.\nSemi-supervised training and inference We train with a weighted combination of the supervised and unsupervised losses described above. Once trained, we simply use the x 7\u2192 y decoder segment of the model to predict y from sequences of symbols x represented as one-hot vectors. When the decoder is trained without the encoder in a fully supervised manner, it serves as our supervised sequenceto-sequence baseline model under the name S2S."}, {"heading": "3 Tasks and Data Generation", "text": "We apply our model to three tasks outlined in this section. Moreover, we explain how we generated additional unsupervised training data for two of these tasks. Examples from all datasets are in Table 1."}, {"heading": "3.1 GeoQuery", "text": "The first task we consider is the prediction of a query on the GEO corpus which is a frequently used benchmark for semantic parsing. The corpus contains 880 questions about US geography together with executable queries representing those questions. We follow the approach established by Zettlemoyer and Collins (2005) and split the corpus into 600 training and 280 test cases. Following common practice, we augment the dataset by referring to the database during training and test time. In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016).\nMost prior work on the GEO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent paper by Dong and Lapata (2016) is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our SEQ4 network."}, {"heading": "3.2 Open Street Maps", "text": "The second task we tackle with our model is the NLMAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries over the geographical OpenStreetMap database. The dataset contains natural language question in both English and German but we focus only on single language semantic parsing, similar to the first task in Haas and Riezler (2016). We use the data as it is, with the only pre-processing step being the tokenization of both natural language and query form3."}, {"heading": "3.3 Navigational Instructions to Actions", "text": "The SAIL corpus and task were developed to train agents to follow free-form navigational route instructions in a maze environment (MacMahon et al., 2006; Chen and Mooney, 2011). It consists of a small number of mazes containing features such as objects, wall and floor types. These mazes come together with a large number of human instructions paired with the required actions4 to reach the goal\n3We removed quotes, added spaces around (), and separated the question mark from the last word in each question.\n4There are four actions: LEFT, RIGHT, GO, STOP.\nstate described in those instructions. We use the sentence-aligned version of the SAIL route instruction dataset containing 3,236 sentences (Chen and Mooney, 2011). Following previous work, we accept an action sequence as correct if and only if the final position and orientation exactly match those of the gold data. We do not perform any pre-processing on this dataset."}, {"heading": "3.4 Data Generation", "text": "As argued earlier, we are focusing on tasks where aligned data is sparse and expensive to obtain, while it should be cheap to get unsupervised, monomodal data. Albeit that is a reasonable assumption for real world data, the datasets considered have no such component, thus the approach taken here is to generate random database queries or maze paths, i.e. the machine readable side of the data, and train a semi-supervised model. The alternative not explored here would be to generate natural language questions or instructions instead, but that is more difficult to achieve without human intervention. For this reason, we generate the machine readable side of the data for GEOQUERY and SAIL tasks5.\nFor GEOQUERY, we fit a 3-gram Kneser-Ney (Chen and Goodman, 1999) model to the queries in the training set and sample about 7 million queries from it. We ensure that the sampled queries are different from the training queries, but do not enforce validity. This intentionally simplistic approach is to demonstrate the applicability of our model.\nThe SAIL dataset has only three mazes. We added a fourth one and over 150k random paths, including duplicates. The new maze is larger (21\u00d7 21 grid) than the existing ones, and seeks to approximately replicate the key statistics of the other three mazes (maximum corridor length, distribution of objects, etc). Paths within that maze are created by randomly sampling start and end positions."}, {"heading": "4 Experiments", "text": "We evaluate our model on the three tasks in multiple settings. First, we establish a supervised baseline to compare the S2S model with prior work. Next, we\n5Our randomly generated unsupervised datasets can be downloaded from http://deepmind.com/ publications\ntrain our SEQ4 model in a semi-supervised setting on the entire dataset with the additional monomodal training data described in the previous section.\nFinally, we perform an \u201cablation\u201d study where we discard some of the training data and compare S2S to SEQ4. S2S is trained solely on the reduced data in a supervised manner, while SEQ4 is once again trained semi-supervised on the same reduced data plus the machine readable part of the discarded data (SEQ4-) or on the extra generated data (SEQ4+).\nTraining We train the model using standard gradient descent methods. As none of the datasets used here contain development sets, we tune hyperparameters by cross-validating on the training data. In the case of the SAIL corpus we train on three folds (two mazes for training and validation, one for test each) and report weighted results across the folds following prior work (Mei et al., 2016)."}, {"heading": "4.1 GeoQuery", "text": "The evaluation metric for GEOQUERY is the accuracy of exactly predicting the machine readable query. As results in Table 2 show, our supervised S2S baseline model performs slightly better than the comparable model by Dong and Lapata (2016). The semi-supervised SEQ4 model with the additional generated queries improves on it further.\nThe ablation study in Table 3 demonstrates a widening gap between supervised and semi-\n6Jia and Liang (2016) used hand crafted grammars to generate additional supervised training data."}, {"heading": "5% 21.9 30.1 26.2", "text": ""}, {"heading": "10% 39.7 42.1 42.1", "text": ""}, {"heading": "25% 62.4 70.4 67.1", "text": ""}, {"heading": "50% 80.3 81.2 80.4", "text": ""}, {"heading": "75% 85.3 84.1 85.1", "text": ""}, {"heading": "100% 86.5 86.5 87.3", "text": "supervised as the amount of labelled training data gets smaller. This suggests that our model can leverage unlabelled data even when only small amount of labelled data is available."}, {"heading": "4.2 Open Street Maps", "text": "We report results for the NLMAPS corpus in Table 4, comparing the supervised S2S model to the results posted by Haas and Riezler (2016). While their model used a semantic parsing pipeline including alignment, stemming, language modelling and CFG inference, the strong performance of the S2S model demonstrates the strength of fairly vanilla attentionbased sequence-to-sequence models. It should be pointed out that the previous work reports the number of correct answers when queries were executed against the dataset, while we evaluate on the strict accuracy of the generated queries. While we expect these numbers to be nearly equivalent, our evaluation is strictly harder as it does not allow for reordering of query arguments and similar relaxations.\nWe investigate the SEQ4 model only via the ablation study in Table 5 and find little gain through the semi-supervised objective. Our attempt at cheaply generating unsupervised data for this task was not successful, likely due to the complexity of the underlying database."}, {"heading": "4.3 Navigational Instructions to Actions", "text": "Model extension The experiments for the SAIL task differ slightly from the other two tasks in that the language input does not suffice for choosing an"}, {"heading": "5% 3.22 3.74", "text": ""}, {"heading": "10% 17.61 17.12", "text": ""}, {"heading": "25% 33.74 33.50", "text": ""}, {"heading": "50% 49.52 53.72", "text": ""}, {"heading": "75% 66.93 66.45", "text": ""}, {"heading": "100% 78.03 78.03", "text": "action. While a simple instruction such as \u2018turn left\u2019 can easily be translated into the action sequence LEFT-STOP, more complex instructions such as \u2018Walk forward until you see a lamp\u2019 require knowledge of the agent\u2019s position in the maze.\nTo accomplish this we modify the model as follows. First, when encoding action sequences, we concatenate each action with a representation of the maze at the given position, representing the mazestate akin to Mei et al. (2016) with a bag-of-features vector. Second, when decoding action sequences, the RNN outputs an action which is used to update the agent\u2019s position and the representation of that new position is fed into the RNN as its next input.\nTraining regime We cross-validate over the three mazes in the dataset and report overall results weighted by test size (cf. Mei et al. (2016)). Both our supervised and semi-supervised model perform worse than the state-of-the-art (see Table 6), but the latter enjoys a comfortable margin over the former. As the S2S model broadly reimplements the work of Mei et al. (2016), we put the discrepancy in performance down to the particular design choices that we did not follow in order to keep the model here as general as possible and comparable across tasks.\nThe ablation studies (Table 7) show little gain for the semi-supervised approach when only using data from the original training set, but substantial improvement with the additional unsupervised data."}, {"heading": "5 Discussion", "text": "Supervised training The prediction accuracies of our supervised baseline S2S model are mixed with respect to prior results on their respective tasks. For GEOQUERY, S2S performs significantly better than the most similar model from the literature (Dong and Lapata, 2016), mostly due to the fact that y and x are"}, {"heading": "5% 37.79 41.48 43.44", "text": ""}, {"heading": "10% 40.77 41.26 48.67", "text": ""}, {"heading": "25% 43.76 43.95 51.19", "text": ""}, {"heading": "50% 48.01 49.42 55.97", "text": ""}, {"heading": "75% 48.99 49.20 57.40", "text": ""}, {"heading": "100% 49.49 49.49 58.28", "text": "encoded with bidirectional LSTMs. With a unidirectional LSTM we get similar results to theirs.\nOn the SAIL corpus, S2S performs worse than the state of the art. As the models are broadly equivalent we attribute this difference to a number of taskspecific choices and optimisations7 made in Mei et al. (2016) which we did not reimplement for the sake of using a common model across all three tasks.\nFor NLMAPS, S2S performs much better than the state-of-the-art, exceeding the previous best result by 11% despite a very simple tokenization method\n7In particular we don\u2019t use beam search and ensembling.\nand a lack of any form of entity anonymisation.\nSemi-supervised training In both the case of GEOQUERY and the SAIL task we found the semisupervised model to convincingly outperform the fully supervised model. The effect was particularly notable in the case of the SAIL corpus, where performance increased from 58.60% accuracy to 63.25% (see Table 6). It is worth remembering that the supervised training regime consists of three folds of tuning on two maps with subsequent testing on the third map, which carries a risk of overfitting to the training maps. The introduction of the fourth unsupervised map clearly mitigates this effect. Table 8 shows some examples of unsupervised logical forms being transformed into natural language, which demonstrate how the model can learn to sensibly ground unsupervised data.\nAblation performance The experiments with additional unsupervised data prove the feasibility of our approach and clearly demonstrate the usefulness of the SEQ4 model for the general class of sequence-to-sequence tasks where supervised data is hard to come by. To analyse the model further, we also look at the performance of both S2S and SEQ4 when reducing the amount of supervised training data available to the model. We compare three settings: the supervised S2S model with reduced training data, SEQ4- which uses the removed training data in an unsupervised fashion (throwing away the natural language) and SEQ4+ which uses the randomly generated unsupervised data described in Section 3. The S2S model behaves as expected on all three tasks, its performance dropping with the size of the training data. The performance of SEQ4and SEQ4+ requires more analysis.\nIn the case of GEOQUERY, having unlabelled data from the true distribution (SEQ4-) is a good thing\nwhen there is enough of it, as clearly seen when only 5% of the original dataset is used for supervised training and the remaining 95% is used for unsupervised training. The gap shrinks as the amount of supervised data is increased, which is as expected. On the other hand, using a large amount of extra, generated data from an approximating distribution (SEQ4+) does not help as much initially when compared with the unsupervised data from the true distribution. However, as the size of the unsupervised dataset in SEQ4- becomes the bottleneck this gap closes and eventually the model trained on the extra data achieves higher accuracy.\nFor the SAIL task the semi-supervised models do better than the supervised results throughout, with the model trained on randomly generated additional data consistently outperforming the model trained only on the original data. This gives further credence to the risk of overfitting to the training mazes already mentioned above.\nFinally, in the case of the NLMAPS corpus, the semi-supervised approach does not appear to help much at any point during the ablation. These indistinguishable results are likely due to the task\u2019s complexity, causing the ablation experiments to either have to little supervised data to sufficiently ground the latent space to make use of the unsupervised data, or in the higher percentages then too little unsupervised data to meaningfully improve the model."}, {"heading": "6 Related Work", "text": "Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment.\nSemantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013).\nWhile a large number of relevant literature fo-\ncuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue.\nDiscrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable. While our model is not exactly marginalisable either, the continuous relaxation makes training far more tractable. A related idea was recently presented in Gu\u0308lc\u0327ehre et al. (2015), who use monolingual data to improve machine translation by fusing a sequence-to-sequence model and a language model."}, {"heading": "7 Conclusion", "text": "We described a method for augmenting a supervised sequence transduction objective with an autoencoding objective, thereby enabling semi-supervised training where previously a scarcity of aligned data might have held back model performance. Across multiple semantic parsing tasks we demonstrated the effectiveness of this approach, improving model performance by training on randomly generated unsupervised data in addition to the original data.\nGoing forward it would be interesting to further analyse the effects of sampling from a logisticnormal distribution as opposed to a softmax in order to better understand how this impacts the distribution in the latent space. While we focused on tasks with little supervised data and additional unsupervised data in y, it would be straightforward to reverse the model to train it with additional labelled data in x, i.e. on the natural language side. A natural extension would also be a formulation where semisupervised training was performed in both x and y.\nFor instance, machine translation lends itself to such a formulation where for many language pairs parallel data may be scarce while there is an abundance of monolingual data."}], "references": [{"title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of NIPS.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Alignment-based Compositional Semantics for Instruction Following", "author": ["Jacob Andreas", "Dan Klein."], "venue": "Proceedings of EMNLP, September.", "citeRegEx": "Andreas and Klein.,? 2015", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Semantic Parsing as Machine Translation", "author": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark."], "venue": "Proceedings of ACL, August.", "citeRegEx": "Andreas et al\\.,? 2013", "shortCiteRegEx": "Andreas et al\\.", "year": 2013}, {"title": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics, 1(1):49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Learning Compact Lexicons for CCG Semantic Parsing", "author": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of EMNLP, October.", "citeRegEx": "Artzi et al\\.,? 2014", "shortCiteRegEx": "Artzi et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semantic Parsing via Paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of ACL, June.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman."], "venue": "Computer Speech & Language, 13(4):359\u2013393.", "citeRegEx": "Chen and Goodman.,? 1999", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "Learning to Interpret Natural Language Navigation Instructions from Observations", "author": ["David L. Chen", "Raymond J. Mooney."], "venue": "Proceedings of AAAI, August.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Language to Logical Form with Neural Attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1601.01280.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "On Using Monolingual Corpora", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "A corpus and semantic parser for multilingual natural language querying of openstreetmap", "author": ["Carolin Haas", "Stefan Riezler."], "venue": "Proceedings of NAACL, June.", "citeRegEx": "Haas and Riezler.,? 2016", "shortCiteRegEx": "Haas and Riezler.", "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "SemanticsBased Machine Translation with Hyperedge Replacement Grammars", "author": ["Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight."], "venue": "Proceedings of COLING 2012, December.", "citeRegEx": "Jones et al\\.,? 2012", "shortCiteRegEx": "Jones et al\\.", "year": 2012}, {"title": "Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision", "author": ["Joohyun Kim", "Raymond J. Mooney."], "venue": "Proceedings of EMNLP-CoNLL, July.", "citeRegEx": "Kim and Mooney.,? 2012", "shortCiteRegEx": "Kim and Mooney.", "year": 2012}, {"title": "Adapting Discriminative Reranking to Grounded Language Learning", "author": ["Joohyun Kim", "Raymond Mooney."], "venue": "Proceedings of ACL, August.", "citeRegEx": "Kim and Mooney.,? 2013", "shortCiteRegEx": "Kim and Mooney.", "year": 2013}, {"title": "AutoEncoding Variational Bayes", "author": ["Diederik P. Kingma", "Max Welling."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Lexical Generalization in CCG Grammar Induction for Semantic Parsing", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "In Proceedings of EMNLP. Citeseer.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Learning Dependency-based Compositional Semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Proceedings of the ACL-HLT.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein."], "venue": "Computational Linguistics, 39(2):389\u2013446.", "citeRegEx": "Liang et al\\.,? 2013", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Walk the Talk: Connecting Language, Knowledge, and Action in Route Instructions", "author": ["Matt MacMahon", "Brian Stankiewicz", "Benjamin Kuipers."], "venue": "Proceedings of AAAI.", "citeRegEx": "MacMahon et al\\.,? 2006", "shortCiteRegEx": "MacMahon et al\\.", "year": 2006}, {"title": "Discrete-state variational autoencoders for joint discovery and factorization of relations", "author": ["Diego Marcheggiani", "Ivan Titov."], "venue": "Transactions of ACL.", "citeRegEx": "Marcheggiani and Titov.,? 2016", "shortCiteRegEx": "Marcheggiani and Titov.", "year": 2016}, {"title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Proceedings of AAAI.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Large-scale Semantic Parsing without QuestionAnswer Pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "author": ["Simon Suster", "Ivan Titov", "Gertjan van Noord."], "venue": "CoRR, abs/1603.09128.", "citeRegEx": "Suster et al\\.,? 2016", "shortCiteRegEx": "Suster et al\\.", "year": 2016}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning for Semantic Parsing with Statistical Machine Translation", "author": ["Yuk Wah Wong", "Raymond J. Mooney."], "venue": "Proceedings of NAACL.", "citeRegEx": "Wong and Mooney.,? 2006", "shortCiteRegEx": "Wong and Mooney.", "year": 2006}, {"title": "Learning to Parse Database Queries using Inductive Logic Programming", "author": ["John M. Zelle", "Raymond J. Mooney."], "venue": "Proceedings of AAAI/IAAI, pages 1050\u2013 1055, August.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI, pages 658\u2013666. AUAI Press.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online Learning of Relaxed CCG Grammars for Parsing to Logical Form", "author": ["Luke Zettlemoyer", "Michael Collins."], "venue": "Proceedings of EMNLP-CoNLL, June.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}, {"title": "Type-driven incremental semantic parsing with polymorphism", "author": ["Kai Zhao", "Liang Huang."], "venue": "arXiv preprint arXiv:1411.5379.", "citeRegEx": "Zhao and Huang.,? 2014", "shortCiteRegEx": "Zhao and Huang.", "year": 2014}, {"title": "End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of ACL.", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation (Bahdanau et al., 2015), syntactic constituency parsing (Vinyals et al.", "startOffset": 207, "endOffset": 230}, {"referenceID": 28, "context": ", 2015), syntactic constituency parsing (Vinyals et al., 2015), and semantic role labelling (Zhou and Xu, 2015).", "startOffset": 40, "endOffset": 62}, {"referenceID": 34, "context": ", 2015), and semantic role labelling (Zhou and Xu, 2015).", "startOffset": 37, "endOffset": 56}, {"referenceID": 18, "context": "To avoid this intractable marginalisation, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in \u03a3x from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step.", "startOffset": 170, "endOffset": 196}, {"referenceID": 30, "context": "We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the GEOQUERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al.", "startOffset": 111, "endOffset": 158}, {"referenceID": 29, "context": "We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the GEOQUERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al.", "startOffset": 111, "endOffset": 158}, {"referenceID": 23, "context": "We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the GEOQUERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap.", "startOffset": 190, "endOffset": 213}, {"referenceID": 12, "context": ", 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap.", "startOffset": 49, "endOffset": 73}, {"referenceID": 5, "context": "At a high level, it can be seen as two sequenceto-sequence models with attention (Bahdanau et al., 2015) chained together.", "startOffset": 81, "endOffset": 104}, {"referenceID": 13, "context": "More precisely, the model consists of four LSTMs (Hochreiter and Schmidhuber, 1997), hence the name SEQ4.", "startOffset": 49, "endOffset": 83}, {"referenceID": 18, "context": "We use the reparametrisation trick from Kingma and Welling (2014) to draw from the logistic normal, allowing us to backpropagate through the sampling process.", "startOffset": 40, "endOffset": 66}, {"referenceID": 18, "context": "We use a closed form of these individual KL divergences, described by Kingma and Welling (2014).", "startOffset": 70, "endOffset": 96}, {"referenceID": 30, "context": "We follow the approach established by Zettlemoyer and Collins (2005) and split the corpus into 600 training and 280 test cases.", "startOffset": 38, "endOffset": 69}, {"referenceID": 9, "context": "In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016). Most prior work on the GEO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus.", "startOffset": 144, "endOffset": 167}, {"referenceID": 9, "context": "In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016). Most prior work on the GEO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent paper by Dong and Lapata (2016) is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our SEQ4 network.", "startOffset": 144, "endOffset": 351}, {"referenceID": 12, "context": "The second task we tackle with our model is the NLMAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries over the geographical OpenStreetMap database.", "startOffset": 66, "endOffset": 90}, {"referenceID": 12, "context": "The second task we tackle with our model is the NLMAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries over the geographical OpenStreetMap database. The dataset contains natural language question in both English and German but we focus only on single language semantic parsing, similar to the first task in Haas and Riezler (2016). We use the data as it is, with the only pre-processing step being the tokenization of both natural language and query form3.", "startOffset": 66, "endOffset": 455}, {"referenceID": 23, "context": "The SAIL corpus and task were developed to train agents to follow free-form navigational route instructions in a maze environment (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 130, "endOffset": 176}, {"referenceID": 8, "context": "The SAIL corpus and task were developed to train agents to follow free-form navigational route instructions in a maze environment (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 130, "endOffset": 176}, {"referenceID": 8, "context": "We use the sentence-aligned version of the SAIL route instruction dataset containing 3,236 sentences (Chen and Mooney, 2011).", "startOffset": 101, "endOffset": 124}, {"referenceID": 7, "context": "For GEOQUERY, we fit a 3-gram Kneser-Ney (Chen and Goodman, 1999) model to the queries in the training set and sample about 7 million queries from it.", "startOffset": 41, "endOffset": 65}, {"referenceID": 19, "context": "1 Liang et al. (2013) 87.", "startOffset": 2, "endOffset": 22}, {"referenceID": 19, "context": "9 Kwiatkowski et al. (2011) 88.", "startOffset": 2, "endOffset": 28}, {"referenceID": 19, "context": "9 Kwiatkowski et al. (2011) 88.6 Zhao and Huang (2014) 88.", "startOffset": 2, "endOffset": 55}, {"referenceID": 19, "context": "9 Kwiatkowski et al. (2011) 88.6 Zhao and Huang (2014) 88.9 Kwiatkowski et al. (2013) 89.", "startOffset": 2, "endOffset": 86}, {"referenceID": 31, "context": "using the train/test split from (Zettlemoyer and Collins, 2005).", "startOffset": 32, "endOffset": 63}, {"referenceID": 25, "context": "In the case of the SAIL corpus we train on three folds (two mazes for training and validation, one for test each) and report weighted results across the folds following prior work (Mei et al., 2016).", "startOffset": 180, "endOffset": 198}, {"referenceID": 9, "context": "As results in Table 2 show, our supervised S2S baseline model performs slightly better than the comparable model by Dong and Lapata (2016). The semi-supervised SEQ4 model with the additional generated queries improves on it further.", "startOffset": 116, "endOffset": 139}, {"referenceID": 12, "context": "We report results for the NLMAPS corpus in Table 4, comparing the supervised S2S model to the results posted by Haas and Riezler (2016). While their model used a semantic parsing pipeline including alignment, stemming, language modelling and CFG inference, the strong performance of the S2S model demonstrates the strength of fairly vanilla attentionbased sequence-to-sequence models.", "startOffset": 112, "endOffset": 136}, {"referenceID": 25, "context": "First, when encoding action sequences, we concatenate each action with a representation of the maze at the given position, representing the mazestate akin to Mei et al. (2016) with a bag-of-features vector.", "startOffset": 158, "endOffset": 176}, {"referenceID": 25, "context": "Mei et al. (2016)).", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "Mei et al. (2016)). Both our supervised and semi-supervised model perform worse than the state-of-the-art (see Table 6), but the latter enjoys a comfortable margin over the former. As the S2S model broadly reimplements the work of Mei et al. (2016), we put the discrepancy in performance down to the particular design choices that we did not follow in order to keep the model here as general as possible and comparable across tasks.", "startOffset": 0, "endOffset": 249}, {"referenceID": 9, "context": "For GEOQUERY, S2S performs significantly better than the most similar model from the literature (Dong and Lapata, 2016), mostly due to the fact that y and x are", "startOffset": 96, "endOffset": 119}, {"referenceID": 1, "context": "22 Andreas and Klein (2015) 59.", "startOffset": 3, "endOffset": 28}, {"referenceID": 1, "context": "22 Andreas and Klein (2015) 59.60 Kim and Mooney (2013) 62.", "startOffset": 3, "endOffset": 56}, {"referenceID": 1, "context": "22 Andreas and Klein (2015) 59.60 Kim and Mooney (2013) 62.81 Artzi et al. (2014) 64.", "startOffset": 3, "endOffset": 82}, {"referenceID": 1, "context": "22 Andreas and Klein (2015) 59.60 Kim and Mooney (2013) 62.81 Artzi et al. (2014) 64.36 Artzi and Zettlemoyer (2013) 65.", "startOffset": 3, "endOffset": 117}, {"referenceID": 25, "context": "As the models are broadly equivalent we attribute this difference to a number of taskspecific choices and optimisations7 made in Mei et al. (2016) which we did not reimplement for the sake of using a common model across all three tasks.", "startOffset": 129, "endOffset": 147}, {"referenceID": 30, "context": "Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al.", "startOffset": 106, "endOffset": 130}, {"referenceID": 10, "context": "Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al.", "startOffset": 146, "endOffset": 167}, {"referenceID": 15, "context": ", 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 19, "context": ", 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al.", "startOffset": 39, "endOffset": 114}, {"referenceID": 3, "context": ", 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al.", "startOffset": 39, "endOffset": 114}, {"referenceID": 26, "context": ", 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al.", "startOffset": 39, "endOffset": 114}, {"referenceID": 29, "context": ", 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013).", "startOffset": 31, "endOffset": 76}, {"referenceID": 2, "context": ", 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013).", "startOffset": 31, "endOffset": 76}, {"referenceID": 31, "context": "While a large number of relevant literature focuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al.", "startOffset": 97, "endOffset": 128}, {"referenceID": 6, "context": "While a large number of relevant literature focuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al.", "startOffset": 200, "endOffset": 224}, {"referenceID": 21, "context": "While a large number of relevant literature focuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011).", "startOffset": 320, "endOffset": 340}, {"referenceID": 6, "context": "While a large number of relevant literature focuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues.", "startOffset": 201, "endOffset": 378}, {"referenceID": 0, "context": ") This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable.", "startOffset": 166, "endOffset": 186}, {"referenceID": 0, "context": ") This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable. While our model is not exactly marginalisable either, the continuous relaxation makes training far more tractable. A related idea was recently presented in G\u00fcl\u00e7ehre et al. (2015), who use monolingual data to improve machine translation by fusing a sequence-to-sequence model and a language model.", "startOffset": 167, "endOffset": 401}], "year": 2016, "abstractText": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.", "creator": "LaTeX with hyperref package"}}}