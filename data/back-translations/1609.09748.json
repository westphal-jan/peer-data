{"id": "1609.09748", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Characterization of experts in crowdsourcing platforms", "abstract": "Crowdsourcing platforms make it possible to suggest simple human intelligence tasks to a large number of participants who perform these tasks. Workers often receive a small sum of money, or the platforms include other incentive mechanisms, such as improving the reputation of workers if they perform the tasks correctly. Unlike other work in the literature, we assume that participants can provide partial or incomplete answers if they are unsure whether their answers are correct. We model such partial or incomplete answers using belief functions and derive a metric that characterizes each participant's skill level. This metric is based on precise and accuracy levels that represent two parts of the skill level. The degree of precision reflects the participants \"reliability level and the degree of accuracy reflects the participants\" level of knowledge. We also analyze our model through simulation and show that our model can lead to more reliable experts.", "histories": [["v1", "Fri, 30 Sep 2016 14:23:42 GMT  (345kb,D)", "http://arxiv.org/abs/1609.09748v1", "in The 4th International Conference on Belief Functions, Sep 2016, Prague, Czech Republic"]], "COMMENTS": "in The 4th International Conference on Belief Functions, Sep 2016, Prague, Czech Republic", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["amal ben rjab", "mouloud kharoune", "zoltan miklos", "arnaud martin"], "accepted": false, "id": "1609.09748"}, "pdf": {"name": "1609.09748.pdf", "metadata": {"source": "CRF", "title": "Characterization of experts in crowdsourcing platforms", "authors": ["Amal Ben Rjab", "Mouloud Kharoune", "Zoltan Miklos", "Arnaud Martin"], "emails": ["benrjabamal.ihec@gmail.com", "Mouloud.Kharoune@univ-rennes1.fr", "zoltan.miklos@irisa.fr", "Arnaud.Martin@univ-rennes1.fr"], "sections": [{"heading": null, "text": "Keywords: Crowdsourcing, expert, expertise level, exactitude and precision degrees."}, {"heading": "1 Introduction", "text": "Crowdsourcing is term for \u201cthe act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call\u201d [1]. Crowdsourcing platforms are used more and more often to execute tasks that are hard for computers but easy for humans. This form of realizing small human intelligence tasks through a large number of individuals has been used in various domains; and plays a more and more important role. It is also considered as a style of future work [2] that can be crucial for example in the context of decision support [3]. Controlling the quality of obtained data and identifying the workers who tend to give correct ar X\niv :1\n60 9.\n09 74\n8v 1\n[ cs\n.A I]\n3 0\nSe p\n20 16\nanswers in this environment still a major problem. The absence of quality control of participants (and their responses) reduces the efficiency of these platforms [4].\nOne often refers to a participant who gives exact and precise answers as an expert [5]. Several works [4] [6\u20138] were proposed to identify the experts in this context. These methods assume that if a worker accepts to complete a task, he will give an answer, even if he is not sure about it. In other words, they make the assumption that a worker does not skip a question. Also, existing crowdsourcing platforms do not allow to give partial results. For example, if the tasks involve a multiple choice question with answers A, B, C and D, a worker cannot say that the correct answer either Aor B (he is not sure about), but certainly not C orD.\nSome works use first \u201cgold\u201d data on which real answers are known [9]. In that case, a degree of exactitude (the percentage of answers that is not wrong) and a degree of precision (the percentage of answers that is not partial) could be learn to measure the expertise level. Here, we assume we that do not have such data.\nIn our work, we construct a model where we allow situations where a worker skips some questions or answers them partially. In our model we make use of belief functions that is a powerful framework to take into account such imperfection of data. We propose a novel expert identification technique that by calculating a degree of exactitude (based on a level of answers that is not wrong) and a degree of precision (based on a level of answers that is not partial). The \u201cideal\u201d worker has a high degree of exactitude and a high degree of precision. For example, in the multiple choice question case, if the correct answer is A then clearly the answer A is better than an answer Aor B (higher degree of precision).\nThe degrees of exactitude and precision are complementary, so using both of them together can lead to better expert identification methods. The rest of the paper is organized as follows. Section 2 formulates the expert identification problem more precisely, together with some relevant related work. We present our approach in Section 3. The experimental evaluation is presented in Section 4."}, {"heading": "2 Expert identification in the context of crowdsourcing", "text": ""}, {"heading": "2.1 Notions of an expert", "text": "An expert in the context of crowdsourcing, is the person who provides a large number of correct, complete and reliable answers. The person who acquired a set of knowledge and skills about a particular area. He can extract knowledge and relevant responses with a minimum cognitive effort. He is identified in crowdsourcing platforms by: the precision and the exactitude of responses, the capability to detect the tasks a priori, the knowledge, skills and learning level."}, {"heading": "2.2 Expert identification methods", "text": "Evaluating quality of workers and identifying experts in crowdsourcing represents a standing problem. Many authors found that taking randomly workers\nis a good choice [10] and others found that establishing a good strategy for selecting experts is more interesting [4]. Several researches have been exploring this area, but essentially there are two basic approaches to identify the experts: Use \u201cgold\u201d data: Provide participants the questions that we already know the answers and identify the workers who give the correct responses as the experts. Use multiple workers: Give a score for each participant which represents his qualities and skills. In this context, Ipeirotis et al. improved in [4] the expectation maximization algorithm (EM) to generate a scalar score representing the quality of each worker. [6] proposed an evaluation of the participants by the set of labels. [7] based on behavioral observation to define a typology of workers.\n[8] proposed an algorithm based on the graphs (SPEAR) to classify the users and to identify the experts. Various methods proposed to identify the experts. But, all these methods have a such level of imprecision and inaccuracy results. In order to ensure a certain identification, we propose to model this imperfection. We proposed an identification of experts with using the theory of belief functions [11,12] which represents a mathematical theory for representing imperfect information and gives a complete framework to model the participant\u2019s answers."}, {"heading": "3 Identification of the experts", "text": "We would like to identify the experts in a crowdsourcing platform. We assume that the questions (tasks) and a list of answers from the crowd workers available. However, we do not assume any access to a \u201cgold\u201d data that would contain all the correct answers. Such a ground truth would clearly largely simplify the identification of experts. Therefore, we develop novel techniques -based on the theory of belief functions- to calculate the exactitude and precision degrees.\nWe use the following formalism. We note the responses rUj proposed by each\nparticipant Uj with a mass of belief m \u2126k Uj . Each response is specific for each question Qk (k = {1, \u00b7 \u00b7 \u00b7 ,K}) which has a specific frame of discernment \u2126k with \u2126k = {\u03c9Qk1 , . . . , \u03c9Qknk }. The frame \u2126k is the set of all possible responses of Qk question. Therefore, we obtain a matrix of mass of belief of size s participants/lines and K questions/columns given by:\nQ1 . . . Qk . . . QK\nU1 ... Uj ... Us  m\u21261U1 . . . m \u2126k U1 . . . m\u2126KU1 ... ... ... m\u21261Uj . . . m \u2126k Uj . . . m\u2126KUj ... ... ...\nm\u21261Us . . . m \u2126k Us . . . m\u2126KUs\n (1)"}, {"heading": "3.1 Exactitude degree", "text": "The exactitude degree is based on the average of the distance between the response proposed by the participant m\u2126kUj and all the responses of the other par-\nticipants m\u2126kU\u03b5s\u22121 . This representation of all other participants is obtained by the average of the responses proposed by the s\u2212 1 participants for the kth question, such as:\nm\u2126kU\u03b5s\u22121 (X) =\n1\ns\u2212 1 s\u22121\u2211 j=1 mj(X) (2)\nThe distance is then calculated by the distance of Jousselme [13]: dJ(m \u2126k Ui ,m\u2126kU\u03b5s\u22121 ). According to this distance, we calculate the exactitude degree for each participant Uj as follows:\nIEUj = 1\u2212 1\nr(Uj) K\u2211 k=1 d\u2126kUj (3)\nThe assumption behind this method is the majority of participants give a correct answer. This assumption is currently made in information fusion and crowdsourcing.\nThe exactitude degree can be used to identify the experts. For this purpose, we use the k-means algorithm (with k = 2 for expert/non expert). The set of experts is given by the cluster with the higher average of exactitude degree."}, {"heading": "3.2 Precision degree", "text": "Based on the model of responses given by the mass functions m\u2126kUj , we can define a degree of precision.\nWe recall that we allow the participants to give partial answers, that is crucial for calculating the precision degree. The usual model of responses (that is, the worker must give a complete answer), we could not define a such degree.\nWe note \u03b4\u2126kUj the specificity degree of the mass function m \u2126k Uj . It is defined\nby [14] as follows:\n\u03b4\u2126kUj = 1\u2212 \u2211\nX\u22082\u2126k\nm\u2126kUj (X) log2(|X|) log2(|\u2126k|)\n(4)\nThis specificity degree allows to translate the precision level of each response independently of the other participant\u2019s responses. To measure the degree of precision of each participant IPUj , we propose to calculate the average of the specificity degrees for all the kth questions. Such as:\nIPUj = 1\nr(Uj) K\u2211 k=1 \u03b4\u2126kUj (5)\nWe determine the experts by using k-means (with k = 2). We do not need the assumption on the majority of participant\u2019s answers."}, {"heading": "3.3 Global degree", "text": "In order to obtain a global degree, we combine both degrees in a single degree for each participant. The global degree is given by a weighted average as follows:\nGDUj = \u03b2UjIEUj + (1\u2212 \u03b2Uj )IPUj (6)\nThe weight \u03b2Uj is introduced to give more or less importance for each degree. Hereafter, we do not make any difference between the participants in the crowd."}, {"heading": "4 Experimentation", "text": "In the following, we generate some mass functions in order to evaluate our approach in the context where there is not use of gold data. We generate three kinds of participants. The experts are those who provide precise and exact responses, in the generation of the masses a singleton is expected on the correct answer. However, if the expert is not totally sure of him, the ignorance is also a focal element. The imprecise experts are those who provide exact but imprecise answers, the correct singleton can be in a disjunction and the ignorance can also be a focal element. The ignorants (sometimes called spammers) are those who give random responses with mass functions taken randomly. To verify the efficiency of our approach we make several experiments with 100 participants, 100 questions where each experiment is repeated 10 times.\nThe precision or the exactitude degree alone is insufficient to identify the experts. The global degree of the equation (6) allows to identify precise and exact responses simultaneously. In a first experiment (with results illustrated in Figure 1), we vary the experts\u2019 number, without generating imprecise experts, from 10 % to 90 % with the global degree in order to prove the ability of our method to identify precise and exact responses simultaneously. In order to demonstrate the importance of each degree we vary in each case the weight \u03b2Uj from 0.1 to 0.9. 100 % Good classification rate with \u03b2Uj = 0.5 reflects that both exactitude and precision degrees have the importance to identify experts. Our algorithm identifies correctly the experts and puts all the other participants in the class of the ignorant.\nTo verify the stability of the good classification rates, we vary in the next experiment (with results illustrated in Figure 2) the number of questions with 35 % of experts, 35 % of imprecise experts and 30 % of ignorants for 10 iterations, we calculate the three degrees. We measure this stability with a perturbation rate calculated by the standard deviation between the different good classification rate exchange on 10 iterations. This experiment shows that it is necessary to have a certain number of questions in order to ensure a better identification.\nWe can found that 30 questions provide a reliable good classification rate. All the previous experiments show the ability of our method to identify the experts in the context of uncertain and imprecise responses. The recourse to the theory of belief functions ensures a reliable identification. It solves the problem of imperfection and provides a certain frame of characterization. With both\ndegrees, we detect the exactitude and precision level of each participant and we correctly identify the experts in the crowd. To confirm the interest of the theory of the belief functions, we compare our belief approach with the probabilistic approach corresponding to the mass function m\u2126kUj which models the responses proposed by each participant Uj given by the pignistic probability:\nBetP m \u2126k Uj\n(\u03c9k) = \u2211\nX\u2286\u2126k,\u03c9k\u2208X\nm\u2126kUj (X)\n(1\u2212m\u2126kUj (\u2205))|X| (7)\nWith the same principle in section 3, we calculate the exactitude degree as follows:\nEP (Uj) = 1\u2212 1\nr(Uj) K\u2211 k=1 d\u2126kUj (8)\nWhere d\u2126kUj is the Euclidean distance on the probabilities. We have to do the same assumption on the majority of correct answers. We use k-means to characterize the experts. In this way, we obtain a probabilistic approach available to detect experts. We limit the comparison by the exactitude degree, due to the impossibility to determine the specificity degree with the probability. We vary in this experiment the percentage of experts and imprecise experts at the same time. The results are illustrated in Figure 3. This figure shows the interest of\nthe use of the belief functions theory to identify the experts and imprecise experts. The probabilistic approach cannot identify the experts from the imprecise experts, it loose the information of exactitude and could not model the imprecision. The regression of the good classification rate to 0 % reflects this inability. Whereas with the belief approach the precise and imprecise experts are better discriminated with all the variations. In complex environment like the crowdsourcing, the theory of belief functions can consider all the imperfection of the participant\u2019s responses."}, {"heading": "5 Conclusion", "text": "We introduced a new technique for characterizing the experts in a crowdsourcing platform by using the belief functions theory, to improve the quality of data that\none could obtain from such platforms. We use a model where the crowd workers are allowed to skip a question or provide partial answers. Based on a belief model of the participant\u2019s responses, we calculated two complementary degrees: An exactitude degree translates the knowledge level of the participants and a precision degree reflects their reliability level. We showed the ability of these degrees to help for the expert identification and we demonstrated the interest of the theory of the belief functions in a comparison with the probability theory."}], "references": [{"title": "The rise of crowdsourcing,", "author": ["J. Howe"], "venue": "Wired magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The future of crowd work,", "author": ["A. Kittur", "J.V. Nickerson", "M. Bernstein", "E. Gerber", "A. Shaw", "J. Zimmerman", "M. Lease", "J. Horton"], "venue": "Proceedings of the 2013 conference on Computer supported cooperative work. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "What can crowdsourcing do for decision support?", "author": ["C.-M. Chiu", "T.-P. Liang", "E. Turban"], "venue": "Decision Support Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Quality management on amazon mechanical turk,", "author": ["P.G. Ipeirotis", "F. Provost", "J. Wang"], "venue": "Proceedings of the ACM SIGKDD workshop on human computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Caract\u00e9risation dexperts dans les plate-formes de crowdsourcing,", "author": ["A.B. Rjab", "M. Kharoune", "Z. Miklos", "A. Martin", "B.B. Yaghlane"], "venue": "e\u0300me Conference sur la Logique Floue et ses Applications (LFA), Poitiers, France,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Quality control of crowd labeling through expert evaluation,", "author": ["F.K. Khattak", "A. Salleb-Aouissi"], "venue": "Proceedings of the NIPS 2nd Workshop on Computational Social Science and the Wisdom of Crowds,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Worker types and personality traits in crowdsourcing relevance labels,", "author": ["G. Kazai", "J. Kamps", "N. Milic-Frayling"], "venue": "Proceedings of the 20th ACM international conference on Information and knowledge management", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Telling experts from spammers: expertise ranking in folksonomies,", "author": ["M.G. Noll", "C.-m. Au Yeung", "N. Gibbins", "C. Meinel", "N. Shadbolt"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Worker evaluation in crowdsourcing: Gold data or-multipleworkers?", "author": ["P. Ipeirotis"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Upper and lower probabilities induced by a multivalued mapping,", "author": ["A.P. Dempster"], "venue": "The annals of mathematical statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1967}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton university press Princeton,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1976}, {"title": "Contradiction measures and specificity degrees of basic belief assignments,", "author": ["F. Smarandache", "A. Martin", "C. Osswald"], "venue": "in International Conference on Information Fusion, Chicago,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Crowdsourcing is term for \u201cthe act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call\u201d [1].", "startOffset": 213, "endOffset": 216}, {"referenceID": 1, "context": "It is also considered as a style of future work [2] that can be crucial for example in the context of decision support [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "It is also considered as a style of future work [2] that can be crucial for example in the context of decision support [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The absence of quality control of participants (and their responses) reduces the efficiency of these platforms [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "One often refers to a participant who gives exact and precise answers as an expert [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "Several works [4] [6\u20138] were proposed to identify the experts in this context.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Several works [4] [6\u20138] were proposed to identify the experts in this context.", "startOffset": 18, "endOffset": 23}, {"referenceID": 6, "context": "Several works [4] [6\u20138] were proposed to identify the experts in this context.", "startOffset": 18, "endOffset": 23}, {"referenceID": 7, "context": "Several works [4] [6\u20138] were proposed to identify the experts in this context.", "startOffset": 18, "endOffset": 23}, {"referenceID": 8, "context": "Some works use first \u201cgold\u201d data on which real answers are known [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "is a good choice [10] and others found that establishing a good strategy for selecting experts is more interesting [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "improved in [4] the expectation maximization algorithm (EM) to generate a scalar score representing the quality of each worker.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "[6] proposed an evaluation of the participants by the set of labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] based on behavioral observation to define a typology of workers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed an algorithm based on the graphs (SPEAR) to classify the users and to identify the experts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We proposed an identification of experts with using the theory of belief functions [11,12] which represents a mathematical theory for representing imperfect information and gives a complete framework to model the participant\u2019s answers.", "startOffset": 83, "endOffset": 90}, {"referenceID": 10, "context": "We proposed an identification of experts with using the theory of belief functions [11,12] which represents a mathematical theory for representing imperfect information and gives a complete framework to model the participant\u2019s answers.", "startOffset": 83, "endOffset": 90}, {"referenceID": 11, "context": "It is defined by [14] as follows:", "startOffset": 17, "endOffset": 21}], "year": 2016, "abstractText": "Crowdsourcing platforms enable to propose simple human intelligence tasks to a large number of participants who realise these tasks. The workers often receive a small amount of money or the platforms include some other incentive mechanisms, for example they can increase the workers reputation score, if they complete the tasks correctly. We address the problem of identifying experts among participants, that is, workers, who tend to answer the questions correctly. Knowing who are the reliable workers could improve the quality of knowledge one can extract from responses. As opposed to other works in the literature, we assume that participants can give partial or incomplete responses, in case they are not sure that their answers are correct. We model such partial or incomplete responses with the help of belief functions, and we derive a measure that characterizes the expertise level of each participant. This measure is based on precise and exactitude degrees that represent two parts of the expertise level. The precision degree reflects the reliability level of the participants and the exactitude degree reflects the knowledge level of the participants. We also analyze our model through simulation and demonstrate that our richer model can lead to more reliable identification of experts.", "creator": "LaTeX with hyperref package"}}}