{"id": "1610.01291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or Lexical Resources?", "abstract": "This paper presents an approach that combines lexical-semantic resources and distributed word representations applied to Machine Translation (MT) evaluation. This study is based on the enrichment of a well-known MT evaluation metric: METEOR. This metric enables approximate agreement (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are conducted within the framework of the metrics task of WMT 2014. We show that distributed representations represent a good alternative to lexical-semantic resources for MT evaluation and can even provide interesting additional information. Enhanced versions of METEOR that use vector representations are made available on our Github page.", "histories": [["v1", "Wed, 5 Oct 2016 07:18:42 GMT  (240kb)", "http://arxiv.org/abs/1610.01291v1", "accepted to COLING 2016 conference"]], "COMMENTS": "accepted to COLING 2016 conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christophe servan", "alexandre berard", "zied elloumi", "herv\\'e blanchon", "laurent besacier"], "accepted": false, "id": "1610.01291"}, "pdf": {"name": "1610.01291.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Christophe Servan", "Alexandre B\u00e9rard", "Zied Elloumi", "Herv\u00e9 Blanchon", "Laurent Besacier"], "emails": ["@imag.fr", "@systran.fr", "@lne.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n01 29\n1v 1\n[ cs\n.C L\n] 5\nO ct\n2 01\n6"}, {"heading": "1 Introduction", "text": "Learning vector representations of words using neural networks has generated a strong enthusiasm in the NLP research community. In particular, many contributions were proposed after the work of (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) on training word embeddings. The main reasons for this strong interest are: the proposal of a simple and efficient neural architecture to learn word vector representations, the availability of an open source tool Word2Vec1 and the rapid structuring of a user community2. Later on, several contributions have extended the work of Mikolov on word vectors to phrases (sequences of words) (Mikolov et al., 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015). All these vector representations capture similarities between words, phrases or sentences at different levels (morphological, semantic).\nHowever, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (S\u00e9rasset, 2012). Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora.\nIn short, both approaches (lexical resources and word embeddings) have their pros and cons. However, few studies have attempted to compare and combine them. Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity, sentiment analysis, finding of synonyms). More recently, Panchenko (2016) and Rothe and Sch\u00fctze (2015) extended word embeddings to sense embeddings and tried to compare them to lexical synsets.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n1http://word2vec.googlecode.com/svn/trunk/ 2https://groups.google.com/d/forum/word2vec-toolkit\nContributions: this article attempts to review the contribution of vector representations to measure sentence similarity. We compare them with similarity measures based on lexical resources such as WordNet or DBnary. Machine Translation (MT) evaluation was identified as a particularly interesting application to investigate, since MT evaluation is still an open problem nowadays. More precisely, we propose to augment a well known MT evaluation metric (METEOR (Banerjee and Lavie, 2005)) which allows an approximate matching (through synonymy or morphological similarity) between MT hypothesis and reference. The augmented versions of METEOR proposed (using word embeddings, lexical resources or both) allow us to objectively compare the contribution of each approach to measure sentence similarity. For this, correlations between METEOR and human judgements (of MT outputs) are measured within the framework of WMT 2014 Metrics task. The code of the augmented versions of METEOR is also provided on our Github page3.\nOutline: in section 2 (Related Work), we quickly present METEOR, lexical resources and word embeddings. Section 3 presents our propositions to augment METEOR in order to conduct a fair comparison between lexical resources and vector representations respectively. Section 4 presents our experiments made within the framework of WMT 2014, as well as quantitative and qualitative analyses. Finally, section 5 concludes this work and gives some perpectives."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 An automatic metric for MT evaluation: METEOR", "text": ""}, {"heading": "2.1.1 The origins", "text": "METEOR was proposed to compensate BLEU\u2019s and NIST\u2019s weaknesses (Papineni et al., 2002; Doddington, 2002). In short, METEOR was created to better correlate with human judgements by using more than word-to-word alignments between a hypothesis and some references. The alignment is made according to three modules: the first stage uses exact match between word surface forms (Exact module), the second one compares word stems (Stems module) and the third one uses synonyms (Synonym module) from a lexical resource such as WordNet (available for English only in METEOR).\nOne contribution of this paper is to propose an alternative to Stems and Synonym modules: our proposed add-on will be called Vectors module later on."}, {"heading": "2.1.2 Recent extensions of METEOR", "text": "METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate \u2013 HTER (Snover et al., 2006)). HTER is a semi-automatic post-editing based metric, which measures the edit distance between a hypothesis and a reference. METEOR-NEXT proposes to go further than just word-to-word alignment by using phrase-to-phrase alignments. For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b). More recently, another version called METEOR Universal used bitexts to extract paraphrases (Denkowski and Lavie, 2014).\nMETEOR was also extended by using Word Sense Disambiguation (WSD) techniques (Apidianaki and Marie, 2015). The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English). A better correlation with human judgement at segment level was observed using WSD in METEOR.\nFinally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (S\u00e9rasset, 2012). The new target languages equipped with a Synonym module were French, German, Spanish, Russian and English."}, {"heading": "2.2 Lexical resources", "text": ""}, {"heading": "2.2.1 WordNet", "text": "WordNet is a well known lexical resource for English. Created at the University of Princeton (Fellbaum, 1998), it is used in several NLP tasks such as Machine Translation, Word Sense Disam-\n3https://github.com/cservan/METEOR-E\nbiguation, Cross-lingual Information Retrieval, etc. WordNet links nouns, verbs, adjectives and adverbs to a set of synonyms called \u201csynsets\u201d. Each synset represents a specific concept.\nSynsets are linked to each other according to semantic, conceptual and lexical relations. Words with multiple meanings correspond to multiple synsets and meanings are sorted according to their frequency. WordNet is available in several languages (Arabic, French, etc.) but these versions are not freely available. In METEOR, only English WordNet is used to match hypothesis and reference words according to their meanings. It contains more than 117,000 synsets.\nTo extract lemmatized forms, METEOR uses a function called Morphy-7WN1 which firstly checks special cases in an exception list and secondly uses rules to lemmatize words according to their syntactic class."}, {"heading": "2.2.2 DBnary", "text": "DBnary is a multilingual lexical resource in RDF format (Klyne and Carroll, 2004). This resource has been collected by S\u00e9rasset (2012). Lexical data are represented using the LEMON vocabulary (McCrae et al., 2011). Most Part-of-Speech tags are linked with Olia standards or Lexinfo vocabularies (Chiarcos and Sukhareva, 2015; Cimiano et al., 2011) which makes them reusable in many contexts.\nDBnary is downloadable or available online through a SPARQL access point. Lexical data are automatically extracted from Wiktionary, Wikipedia\u2019s dictionary for 21 languages4 .\nAmong available lexical data, one may find 2.9M lexical entries (with parts-of-speech, canonical form for all of them, along with pronunciations when available and inflected forms for some languages). Lexical entries are subdivided into 2.5M lexical senses (with their definitions and some usage example).\nDBnary also contains more than 4.6M translations going from the 21 extracted sources languages to more than 1500 different target languages. Additionally, DBnary contains lexicosemantic relations (syno/anto-nyms, hypo/hypero-nyms, etc.). Table 1 shows the size of the data for languages involved in the experiments later reported in this paper. Additional figures are available on the DBnary public web site5.\nLemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets."}, {"heading": "2.3 Monolingual and bilingual embeddings", "text": ""}, {"heading": "2.3.1 Overview", "text": "Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012). The main idea is to learn a word representation according to its context: the surrounding words (Baroni and Zamparelli, 2010). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. When word vectors are available, a similarity between two words can be measured by a metric such as a cosine similarity.\nUsing word-embeddings for machine translation evaluation is appealing since they can be used to compute similarity between words or phrases in the same language (monolingual embeddings capture intrinsically synonymy or morphological closeness) or in two different languages (bilingual embeddings allow to directly compute a distance between two sentences in different languages). We use the MultiVec (B\u00e9rard et al., 2016) toolkit for computing and managing the continuous representations of texts.\n4Bulgarian, Dutch, English, Finnish, French, German, (Modern) Greek, Indonesian, Italian, Japanese, Latin, Lithuanian, Malagasy, Norwegian, Polish, Portuguese, Russian, Serbo-Croat, Spanish, Swedish and Turkish\n5http://kaiko.getalp.org/about-dbnary/\nIt includes word2vec (Mikolov et al., 2013a), paragraph vector (Le and Mikolov, 2014b) and bilingual distributed representations (Luong et al., 2015) features."}, {"heading": "2.3.2 Use of vector representations in NLP evaluation", "text": "Zou et al. (2013) proposed to use bilingual word embeddings to detect similarities for word alignment. This information is used as an additional parameter in a phrase-based machine translation system. (Banchs et al., 2015) proposed to explore a metric funded on latent semantic analysis (Salton et al., 1975) to extract semantic embeddings and measure the similarity between two sentences. Finally, these word embeddings were used to enrich ROUGE, a metric for evaluating automatic summarization (Ng and Abrecht, 2015).\nAs far as MT evaluation is concerned, Gupta et al. (2015) proposed a metric based on neural network language models jointly with dependency trees to link an hypothesis to a reference. Meanwhile, Vela and Tan (2015) proposed an approach to model document embeddings to predict translation adequacy.\nThese works are close to ours but they propose metrics which need to be learned and optimized to a specific task or domain. In our work, we use word embeddings trained once and for all on a (large) general corpus. Our detailed methodology to augment METEOR metric is presented in the next section."}, {"heading": "3 Augmented METEOR", "text": ""}, {"heading": "3.1 Data and protocol", "text": "We evaluate our augmented METEOR through WMT14 framework (metrics task (Machacek and Bojar, 2014)). This framework enables us to estimate the correlation of proposed evaluation metric with human judgements for several machine translation outputs and several language pairs (English-French, English-German, English-Russian, and vice versa). In our experiments, we use segment level Kendall\u2019s \u03c4 correlation coefficient, as proposed in WMT14 (based on systems ranking at sentence level by humans, compared to automatic metric ranking).\nWe augment METEOR in two ways: firstly, we replace the use of lexical resources by the use of word embeddings. In other words, we replace Stem and Synonym modules by our new Vector module. Secondly, we combine lexical resources and word embeddings by using jointly Stem, Synonym and our Vector module.\nTo summarize, the following variants of METEOR are evaluated:\n\u2022 METEOR Baseline: the METEOR score is estimated using Exact, Stem, Synonym and Paraphrase modules for English as a target language and Exact, Stem and Paraphrase modules for other target languages,\n\u2022 METEOR DBnary: similar to METEOR Baseline but Synonym module is available for any target language since it uses DBnary resource instead of Wordnet,\n\u2022 METEOR Vector: the Stem and Synonym modules are replaced by the Vector module ;\n\u2022 METEOR Baseline + Vector: the METEOR Baseline configuration is augmented with the Vector module ;\n\u2022 METEOR DBnary + Vector: the METEOR DBnary configuration is augmented with the Vector module.\n3.2 METEOR DBnary\nAs mentioned in section 2.1, the Synonym module of METEOR uses WordNet\u2019s synsets (117K entries for English). As an alternative, we use another lexical resource: DBnary (S\u00e9rasset, 2012), as proposed recently by Elloumi et al. (2015). This allows us to use Synonym module for any target language: French, German, Spanish, Russian and English.\nMore precisely, synonym relations are extracted from DBnary using SPARQL request on the DBnary server6. We extract data for English, French, Russian and German languages. The extraction process outputs relations in the following format: lemma \u2192 Synonym. Then, these data are projected to the WordNet format used in METEOR code. This process gives an identifier (ID) for each lemma and builds a list of synonym IDs for each lemma such as: lemma \u2192 ID_Syn1, ID_Syn2, ID_Syn3.\nThe first two lines of Table 3 compare METEOR DBnary and METEOR Baseline for several FrenchEnglish MT systems submitted to WMT14 (Bojar et al., 2014).\nMETEOR DBnary improved the score by 0.7 points from METEOR Baseline. In other words, DBnary seems to match more synonyms than WordNet, despite the fact that WordNet is 3.3 time bigger than DBnary in English. This could be due to the fact that WordNet has only 4 morpho-syntactic categories (Noun, Verbs, Adjectives and Adverbs) while DBnary has more morpho-syntactic categories.\n3.3 METEOR Vector\nAs mentioned in section 2.3.2, we propose to replace lexical resources by word embeddings. Word embeddings capture the context of the words. Consequently, similar word vectors may correspond to synonyms or morphological variants (see section 2.3).\nIn our Vector module, the matching between two words is done using a similarity score derived from the cosine similarity. If the similarity score is higher than a threshold, the words are considered as matched (potential synonyms or morphological proximity). In our experiments, we evaluate using: (a) a default threshold fixed to 0.80 (b) an oracle threshold obtained empirically on the WMT14 data set (Machacek and Bojar, 2014).\nTable 2 summarizes data used to train monolingual word embeddings and bilingual word embeddings. These word embeddings were trained with a CBOW model, a vector size of 50 and a windows size \u00b15 words, thanks to the MultiVec toolkit (B\u00e9rard et al., 2016).\nThe results presented in table 3 show that word embeddings (Vector module) can efficiently replace lexical resources (Synonym and Stem modules) to match words in the translation hypothesis with those in the reference. In addition, their combination shows a good potential to match even more words between hypothesis and reference. In the next section, we evaluate if the proposed versions of augmented METEOR better correlate with human judgements."}, {"heading": "4 Correlations of Augmented METEOR with Human Judgements", "text": ""}, {"heading": "4.1 Results of different METEOR configurations", "text": "In these experiments, we present results obtained with the Vector module based on two threshold values: a default one (0.80) and an oracle one which maximizes the correlation with human judgement.\n6http://kaiko.getalp.org/about-dbnary/online-access/\nTable 4 presents the correlation scores obtained within the framework of WMT14 metrics task (Machacek and Bojar, 2014)7. The evaluation is done according to several translation tasks: from English to French (en\u2013fr), German (en\u2013de) and Russian (en\u2013ru), and vice versa. French, German and Russian as target languages represent a growing difficulty due to their morphology. English as target language allows to compare the lexical databases (Wordnet vs DBnary).\nTo English. Firstly, when the translation direction is to English, we can observe that METEOR Baseline and METEOR Vector get equivalent results in average. METEOR DBnary also obtains similar results to METEOR Baseline. When we combine WordNet lexical resource and word embeddings (METEOR Baseline + Vector), the reference score is increased by 0, 005 points. If the combination is done with DBnary\u2019s lexical data (METEOR DBnary + Vector), the improvement is similar.\nFor Vector module, optimization of the threshold slightly improves the average correlation. Combination of METEOR Baseline + Vector or METEOR DBnary + Vector improves by 0, 002 points when the threshold is optimized.\nFrom English. Secondly, when the translation direction is from English, we can observe an improvement of the correlation score obtained with METEOR DBnary, compared with METEOR Baseline. This is due to the fact that for French, German and Russian as target languages, METEOR Baseline does not use any Synonym module. Our METEOR Vector with the default threshold also gets better correlation scores compared to METEOR DBnary (+0.003 points in average). The combinations METEOR Baseline + Vector and METEOR DBnary + Vector further improve correlations with human judgements (+0.001 points in average). Finally, when we use an oracle threshold for Vector module, improvements are bigger and can reach 0.013 points in average, compared to METEOR Baseline."}, {"heading": "4.2 Investigating more embeddings configurations", "text": "In the previous section, METEOR Vector used a simple and monolingual word embedding configuration. This section investigates more configurations (monolingual and bilingual) to improve METEOR.\nIn this experiment, we focus only on METEOR Vector. Indeed, the monolingual (baseline) shown in table 6 corresponds to the line METEOR Vector in Table 4. Firstly, we propose to train our embeddings on bitexts (Table 2) using bivec approach (Luong et al., 2015). We also try to train pseudo-bilingual embeddings on a pseudo bitext with target language text and POS tags (see an example in Table 5). The main idea is to strongly link words with their syntactic class when learning word embeddings. We call this kind of model pseudo-bilingual with POS. In the same way, we train bilingual models called\n7For better readability, we do not add standard deviations in the tables. These numbers will be, however, provided in supplementary material put on the paper web page (https://github.com/cservan/METEOR-E/paper).\npseudo-bilingual with lemmas, where the POS tags are replaced by lemmas. In addition, we also learn word embeddings with lemmas only and bilingual models with lemmas only.\nIn the Table 6, we compare several training configuration of the word embeddings through the same protocol as previous section (only average correlations are reported while the detailed results will be provided as supplementary material on the paper web page). When we observe the average results, the bilingual embeddings seem not to be as efficient as the monolingual baseline. The pseudo-bilingual approaches with POS and Lemmas obtained slightly the same results as the monolingual baseline regarding all the configurations we have. Finally, the monolingual model learned on lemmas (instead of words) tends to be slightly better when the translation direction is to English. However, this trend should be confirmed in a future investigation."}, {"heading": "4.3 Discussion", "text": "The correlation scores obtained with the enriched metric tend to suggest that distributed representations are as powerful as lexico-semantic resources for automatic MT evaluation. Furthermore, vector representations can bring additional information, and they are definitely useful when no lexical resource is available in the target language.\nConsidering the average correlation scores obtained, the configurations METEOR Vector and METEOR DBnary are comparable, except on German language, for which METEOR Vector obtained a better correlation score. On the other hand, when we combine lexical data with Vector module (METEOR DBnary + Vector), we observe a small increase of the correlation score, in particular when threshold is tuned, which suggests a tunable version of METEOR.\nFinally, several embeddings variants were trained but it seems that monolingual models are efficient enough for the specific task (MT evaluation) considered here."}, {"heading": "4.3.1 Examples", "text": "To illustrate the word matching obtained by our versions of METEOR, we analyze two examples from the evaluation data set. In these examples, we present the alignments obtained with METEOR DBnary + Vector.\nThe example presented in table 7 shows rbmt 1 system output submitted during the WMT14 translation task. METEOR baseline found only alignments for words with the same surface forms (\u201cqu\u2019 \u201d, \u201cil\u201d, \u201cest\u201d, etc. \u2013 these forms are found identical thanks to the Exact module and are not highlighted here). The Synonym module based on DBnary makes it possible to find a correspondence between words\n\u201cemploy\u00e9es\u201d \u2013 \u201cutilis\u00e9es\u201d and \u201cpour\u201d \u2013 \u201cdans\u201d. Lastly, Vector module indicates that words \u201cpense\u201d and \u201cestime\u201d are contextually closed, just as the words \u201cje\u201d and \u201cj\u201d\u2019. When the example is only evaluated with METEOR Vector, words \u201cemploy\u00e9es\u201d and \u201cutilis\u00e9es\u201d are also paired with the default threshold (0.80). On the other hand, the words \u201cb\u00e9n\u00e9fice\u201d and \u201cint\u00e9r\u00eat\u201d are paired by the module Vector only if the decision threshold is lowered to 0.75.\nIn the second example presented in table 8, the hypothesis is provided by rbmt 4 system. As in the previous example, the correspondences found with Synonym module based on DBnary (framed by one continuous line) are supplemented by those found by Vector module (dotted line): Synonym module found \u201ccr\u00e9ateur\u201d \u2013 \u201cp\u00e8re\u201d and \u201cfaisait\u201d \u2013 \u201cfaire\u201d; while \u201cdu\u201d and \u201cde\u201d are aligned thanks to Vector module.\nThese examples illustrate the complementarity between lexical resources and word embeddings for sentence similarity detection. Word vectors can enable to match important words (like \u201cpense\u201d and \u201cestime\u201d in our first example), but also empty words (like \u201cdu\u201d et \u201cde\u201d in our second example)."}, {"heading": "4.3.2 Limitations of Word Embeddings", "text": "So far, we did not deal with Out-Of-Vocabulary (OOV) words in METEOR Vector. By OOV we mean words that do not have a vector representation because they were not found in the training corpus for word embeddings. In that case, no matching can occur between the word in the hypothesis and words in reference. Consequently, it might be interesting to carefully select the training corpus for word vectors so that it will be close enough to the machine translation outputs to evaluate. This could be considered in future works."}, {"heading": "5 Conclusion and Perspectives", "text": "In this paper, we proposed to compare text similarity measures based on vector representations with similarity measures based on lexico-semantic resources. Our work was applied to machine translation evaluation and we extended an existing evaluation metric called METEOR. Our experiments have shown that word vector representations can be useful when no lexical resource is available in the target language. Moreover, it seems that these representations can bring complementary information in addition to lexical resources (experiments done for French, English, German and Russian as target languages).\nOur future works on this topic will focus on the use of phrase embeddings to complement the Paraphrase module of METEOR. We also plan to introduce a syntax flavor in our Vector module by weighting the cosine distances differently according to the morpho-syntactic category of the words. Finally, we will study the adaptation of our approach to other metrics such as TER-Plus, for instance.\nThe tool, the data and the models presented in this paper will be put online8 to facilitate reproducibility of the experiments we carried out."}, {"heading": "Acknowledgements", "text": "This work was supported by the KEHATH project funded by the French National Agency for Research (ANR) under the grant number ANR-14-CE24-0016-03."}], "references": [{"title": "METEOR-WSD: Improved Sense Matching in MT Evaluation. In the Proceedings of the 9th Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST\u20199)", "author": ["Apidianaki", "Marie2015] Marianna Apidianaki", "Benjamin Marie"], "venue": null, "citeRegEx": "Apidianaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Apidianaki et al\\.", "year": 2015}, {"title": "Adequacy\u2013Fluency Metrics: Evaluating MT in the Continuous Space Model Framework", "author": ["Luis F. D\u2019Haro", "Haizhou Li"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Banchs et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2015}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP. In The 10th edition of the Language Resources and Evaluation Conference (LREC 2016)", "author": ["Christophe Servan", "Olivier Pietquin", "Laurent Besacier"], "venue": null, "citeRegEx": "B\u00e9rard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "B\u00e9rard et al\\.", "year": 2016}, {"title": "Olia \u2013 ontologies of linguistic annotation", "author": ["Chiarcos", "Maria Sukhareva"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Chiarcos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiarcos et al\\.", "year": 2015}, {"title": "Lexinfo: A declarative model for the lexicon-ontology interface", "author": ["Cimiano et al.2011] P. Cimiano", "P. Buitelaar", "J. McCrae", "M. Sintek"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Cimiano et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Extending the meteor machine translation evaluation metric to the phrase level", "author": ["Denkowski", "Lavie2010a] Michael Denkowski", "Alon Lavie"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Denkowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2010}, {"title": "METEOR-NEXT and the METEOR paraphrase tables: Improved evaluation support for five target languages", "author": ["Denkowski", "Lavie2010b] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR", "citeRegEx": "Denkowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2010}, {"title": "METEOR Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington"], "venue": "In Proceedings of the second international conference on Human Language Technology Research", "citeRegEx": "Doddington.,? \\Q2002\\E", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "METEOR for Multiple Target Languages using DBnary", "author": ["Elloumi et al.2015] Zied Elloumi", "Herv\u00e9 Blanchon", "Gilles Serasset", "Laurent Besacier"], "venue": "In Proceedings of MT Summit", "citeRegEx": "Elloumi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elloumi et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons. CoRR", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Machine Translation Evaluation using Recurrent Neural Networks", "author": ["Gupta et al.2015] Rohit Gupta", "Constantin Orasan", "Josef Van Genabith"], "venue": "In Proceedings Workshop on Machine Translation (WMT),", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Resource Description Framework (RDF): Concepts and Abstract Syntax", "author": ["Klyne", "Carroll2004] Graham Klyne", "Jeremy J. Carroll"], "venue": "Technical report", "citeRegEx": "Klyne et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Klyne et al\\.", "year": 2004}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Mikolov2014a] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of The 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Mikolov2014b] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML\u201914)", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Results of the WMT14 Metrics Shared Task", "author": ["Machacek", "Bojar2014] Matous Machacek", "Ondrej Bojar"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation", "citeRegEx": "Machacek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Machacek et al\\.", "year": 2014}, {"title": "Linking Lexical Resources and Ontologies on the Semantic Web with Lemon, pages 245\u2013259", "author": ["McCrae et al.2011] John McCrae", "Dennis Spohr", "Philipp Cimiano"], "venue": null, "citeRegEx": "McCrae et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McCrae et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In The Workshop Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Entity linking meets word sense disambiguation: a unified approach. Transactions of the Association for Computational Linguistics, 2:231\u2013244", "author": ["Moro et al.2014] Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": null, "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "Babelnet: Building a very large multilingual semantic network", "author": ["Navigli", "Ponzetto2010] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Better Summarization Evaluation with Word Embeddings for ROUGE", "author": ["Ng", "Abrecht2015] Jun-Ping Ng", "Viktoria Abrecht"], "venue": "The Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Best of both worlds: Making word sense embeddings interpretable", "author": ["Alexander Panchenko"], "venue": "In the 10th edition of the Language Resources and Evaluation Conference", "citeRegEx": "Panchenko.,? \\Q2016\\E", "shortCiteRegEx": "Panchenko.", "year": 2016}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "A Vector Space Model for Automatic Indexing", "author": ["G. Salton", "A. Wong", "C.S. Yang"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1975}, {"title": "Treetagger| a language independent part-of-speech tagger. Institut f\u00fcr Maschinelle Sprachverarbeitung, Universit\u00e4t Stuttgart, 43:28", "author": ["Helmut Schmid"], "venue": null, "citeRegEx": "Schmid.,? \\Q1995\\E", "shortCiteRegEx": "Schmid.", "year": 1995}, {"title": "Dbnary: Wiktionary as a lemon-based multilingual lexical resource in rdf", "author": ["Gilles S\u00e9rasset"], "venue": "Semantic Web Journal-Special issue on Multilingual Linked Open Data,", "citeRegEx": "S\u00e9rasset.,? \\Q2012\\E", "shortCiteRegEx": "S\u00e9rasset.", "year": 2012}, {"title": "A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas", "author": ["Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": null, "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric", "author": ["Nitin Madnani", "Bonnie J Dorr", "Richard Schwartz"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation", "citeRegEx": "Snover et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2009}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Predicting Machine Translation Adequacy with Document Embeddings", "author": ["Vela", "Tan2015] Mihaela Vela", "Liling Tan"], "venue": "In Proceedings Workshop on Machine Translation (WMT),", "citeRegEx": "Vela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vela et al\\.", "year": 2015}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": ", 2013b; Le and Mikolov, 2014a) and to bilingual representations (Luong et al., 2015).", "startOffset": 65, "endOffset": 85}, {"referenceID": 15, "context": "However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (S\u00e9rasset, 2012).", "startOffset": 182, "endOffset": 198}, {"referenceID": 35, "context": "However, although these representations can be semantically informative, they do not exactly replace fine-grained information available in lexical-semantic resources such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010), or DBnary (S\u00e9rasset, 2012).", "startOffset": 249, "endOffset": 265}, {"referenceID": 30, "context": "Such lexical resources are also more easily interpretable by humans as shown in (Panchenko, 2016), but their construction is costly while word embeddings can be trained ad infinitum on any monolingual or bilingual corpora.", "startOffset": 80, "endOffset": 97}, {"referenceID": 14, "context": "Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources.", "startOffset": 19, "endOffset": 41}, {"referenceID": 14, "context": "Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity, sentiment analysis, finding of synonyms). More recently, Panchenko (2016) and Rothe and Sch\u00fctze (2015) extended word embeddings to sense embeddings and tried to compare them to lexical synsets.", "startOffset": 19, "endOffset": 391}, {"referenceID": 14, "context": "Pioneering work of Faruqui et al. (2014) proposed to refine representations learning using lexical resources. The idea is to force words connected in the lexical network, to have a close representation (for example through a synonymy link). The technique proposed is evaluated on several benchmarks (word similarity, sentiment analysis, finding of synonyms). More recently, Panchenko (2016) and Rothe and Sch\u00fctze (2015) extended word embeddings to sense embeddings and tried to compare them to lexical synsets.", "startOffset": 19, "endOffset": 420}, {"referenceID": 31, "context": "1 The origins METEOR was proposed to compensate BLEU\u2019s and NIST\u2019s weaknesses (Papineni et al., 2002; Doddington, 2002).", "startOffset": 77, "endOffset": 118}, {"referenceID": 12, "context": "1 The origins METEOR was proposed to compensate BLEU\u2019s and NIST\u2019s weaknesses (Papineni et al., 2002; Doddington, 2002).", "startOffset": 77, "endOffset": 118}, {"referenceID": 36, "context": "2 Recent extensions of METEOR METEOR-NEXT (Denkowski and Lavie, 2010a) was proposed to better correlate with HTER (Humantargeted Translation Edit Rate \u2013 HTER (Snover et al., 2006)).", "startOffset": 158, "endOffset": 179}, {"referenceID": 37, "context": "For this, phrase databases were created for several languages like English (Snover et al., 2009), German, French or Czech (Denkowski and Lavie, 2010b).", "startOffset": 75, "endOffset": 96}, {"referenceID": 27, "context": "The authors used Babelfly (Moro et al., 2014) for several langage pairs (translation from French, Hindi, German, Czech and Russian to English).", "startOffset": 26, "endOffset": 45}, {"referenceID": 35, "context": "(2015) proposed to replace WordNet by DBnary (S\u00e9rasset, 2012).", "startOffset": 45, "endOffset": 61}, {"referenceID": 13, "context": "Finally, to extend the use of Synonym module to target languages others than English, Elloumi et al. (2015) proposed to replace WordNet by DBnary (S\u00e9rasset, 2012).", "startOffset": 86, "endOffset": 108}, {"referenceID": 15, "context": "Created at the University of Princeton (Fellbaum, 1998), it is used in several NLP tasks such as Machine Translation, Word Sense Disam-", "startOffset": 39, "endOffset": 55}, {"referenceID": 23, "context": "Lexical data are represented using the LEMON vocabulary (McCrae et al., 2011).", "startOffset": 56, "endOffset": 77}, {"referenceID": 7, "context": "Most Part-of-Speech tags are linked with Olia standards or Lexinfo vocabularies (Chiarcos and Sukhareva, 2015; Cimiano et al., 2011) which makes them reusable in many contexts.", "startOffset": 80, "endOffset": 132}, {"referenceID": 33, "context": "This resource has been collected by S\u00e9rasset (2012). Lexical data are represented using the LEMON vocabulary (McCrae et al.", "startOffset": 36, "endOffset": 52}, {"referenceID": 34, "context": "Lemmatized forms for DBnary are based on the TreeTagger module (Schmid, 1995), which enables us to find the corresponding synsets.", "startOffset": 63, "endOffset": 77}, {"referenceID": 4, "context": "1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012).", "startOffset": 63, "endOffset": 149}, {"referenceID": 38, "context": "1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012).", "startOffset": 63, "endOffset": 149}, {"referenceID": 8, "context": "1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012).", "startOffset": 63, "endOffset": 149}, {"referenceID": 17, "context": "1 Overview Learning word embeddings is an active research area (Bengio et al., 2003; Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012).", "startOffset": 63, "endOffset": 149}, {"referenceID": 5, "context": "We use the MultiVec (B\u00e9rard et al., 2016) toolkit for computing and managing the continuous representations of texts.", "startOffset": 20, "endOffset": 41}, {"referenceID": 21, "context": ", 2013a), paragraph vector (Le and Mikolov, 2014b) and bilingual distributed representations (Luong et al., 2015) features.", "startOffset": 93, "endOffset": 113}, {"referenceID": 1, "context": "(Banchs et al., 2015) proposed to explore a metric funded on latent semantic analysis (Salton et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": ", 2015) proposed to explore a metric funded on latent semantic analysis (Salton et al., 1975) to extract semantic embeddings and measure the similarity between two sentences.", "startOffset": 72, "endOffset": 93}, {"referenceID": 37, "context": "2 Use of vector representations in NLP evaluation Zou et al. (2013) proposed to use bilingual word embeddings to detect similarities for word alignment.", "startOffset": 50, "endOffset": 68}, {"referenceID": 1, "context": "(Banchs et al., 2015) proposed to explore a metric funded on latent semantic analysis (Salton et al., 1975) to extract semantic embeddings and measure the similarity between two sentences. Finally, these word embeddings were used to enrich ROUGE, a metric for evaluating automatic summarization (Ng and Abrecht, 2015). As far as MT evaluation is concerned, Gupta et al. (2015) proposed a metric based on neural network language models jointly with dependency trees to link an hypothesis to a reference.", "startOffset": 1, "endOffset": 377}, {"referenceID": 1, "context": "(Banchs et al., 2015) proposed to explore a metric funded on latent semantic analysis (Salton et al., 1975) to extract semantic embeddings and measure the similarity between two sentences. Finally, these word embeddings were used to enrich ROUGE, a metric for evaluating automatic summarization (Ng and Abrecht, 2015). As far as MT evaluation is concerned, Gupta et al. (2015) proposed a metric based on neural network language models jointly with dependency trees to link an hypothesis to a reference. Meanwhile, Vela and Tan (2015) proposed an approach to model document embeddings to predict translation adequacy.", "startOffset": 1, "endOffset": 534}, {"referenceID": 35, "context": "As an alternative, we use another lexical resource: DBnary (S\u00e9rasset, 2012), as proposed recently by Elloumi et al.", "startOffset": 59, "endOffset": 75}, {"referenceID": 13, "context": "As an alternative, we use another lexical resource: DBnary (S\u00e9rasset, 2012), as proposed recently by Elloumi et al. (2015). This allows us to use Synonym module for any target language: French, German, Spanish, Russian and English.", "startOffset": 101, "endOffset": 123}, {"referenceID": 5, "context": "These word embeddings were trained with a CBOW model, a vector size of 50 and a windows size \u00b15 words, thanks to the MultiVec toolkit (B\u00e9rard et al., 2016).", "startOffset": 134, "endOffset": 155}, {"referenceID": 21, "context": "Firstly, we propose to train our embeddings on bitexts (Table 2) using bivec approach (Luong et al., 2015).", "startOffset": 86, "endOffset": 106}], "year": 2016, "abstractText": "This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. This metric enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page.", "creator": "LaTeX with hyperref package"}}}