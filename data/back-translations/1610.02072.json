{"id": "1610.02072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "An efficient high-probability algorithm for Linear Bandits", "abstract": "For the linear bandit problem, we extend the analysis of the CombEXP algorithm by [R. Combes, M. S. Talebi Mazraeh Shahi, A. Proutiere and M. Lelarge. Combinatorial bandits revisited. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pp. 2116-2124. Curran Associates, Inc., 2015.", "histories": [["v1", "Thu, 6 Oct 2016 21:14:16 GMT  (22kb)", "http://arxiv.org/abs/1610.02072v1", "17 pages"], ["v2", "Thu, 13 Oct 2016 16:09:41 GMT  (22kb)", "http://arxiv.org/abs/1610.02072v2", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["g\\'abor braun", "sebastian pokutta"], "accepted": false, "id": "1610.02072"}, "pdf": {"name": "1610.02072.pdf", "metadata": {"source": "META", "title": "An efficient high-probability algorithm for linear bandits", "authors": ["G\u00e1bor Braun", "Sebastian Pokutta"], "emails": ["gabor.braun@isye.gatech.edu", "sebastian.pokutta@isye.gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n02 07\n2v 1\n[ cs\n.D S]\n6 O\nFor the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of O(T2/3) for time horizon T. While this bound is weaker than the optimal O( \u221a\nT) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions."}, {"heading": "1 Introduction", "text": "We study sequential prediction problemswith linear losses and bandit feedback against an adaptive adversary. At every round t the forecaster chooses an action xt, and the adversary chooses a loss function Lt, and the forecaster suffers the loss Lt(xt). The forecaster learns only the suffered loss after each round, while the adversary learns the forecaster\u2019s action xt. The forecaster\u2019s aim is to minimize regret, which is the difference between the incurred loss and the loss of the best single action in hindsight:\n\u2211 t\u2208T Lt(xt)\u2212 min x\u2208A \u2211t\u2208T Lt(x).\nIn this work we focus on establishing regret bounds holding with high-probability with an efficient algorithm.\nFor algorithms with bandit feedback, exploration (occasionally playing random actions for learning) is a crucial feature, however it does not have to be explicit as recently shown in Neu [2015], where exploration is achieved via skewing loss estimators. One of the most studied regret minimization algorithm is EXP, which iteratively updates the probabilities of each action via multiplication with factors exponential in its (estimated) loss. The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n. At the same time linear losses come naturally into play when considering actions with a combinatorial structure, such as e.g., matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the\nEXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( \u221a\nT) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( \u221a T) regret with high probability. While these regret bounds practically do not depend on the number of actions, both maintain a distribution over the (possibly exponentially large) action set A, which is infeasible in general due to the large data size, even though ComBand is still efficient for many specific problems. Recently, a modification of the ComBand algorithm called CombEXP (see Algorithm 1) was derived in Combes et al. [2015], which achieves general computational efficiency by not maintaining a distribution of xt, but only the desired expectation x\u0302t of the distribution, and generating a new sparse approximate distribution at every round.\nIn this work we provide a high-probability regret bound of O(T2/3) for CombEXP against adaptive adversaries, while generalizing it to general polytopes. The obtained bounds are any-time, i.e., the parameter choice is independent\nof the time horizon T. Finally, our algorithmmaintains computational efficiency given an efficient linear programming oracle over the underlying polytope (the convex hull of actions). For comparison, we also show an O(T2/3) regret in the high-probability setting for the original ComBand.\nThe maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvo\u00df [2014].\nRelated work\nOur work is most closely related to the line of works on combinatorial bandit problems. The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and Comb-\nEXP appeared in Combes et al. [2015]. Using interior point methods, an efficient algorithm with O( \u221a\nT) expected regret for linear bandit problems has been established in Abernethy et al. [2008].\nFor multiarmed bandit problems, the original version of EXP3 has high-probability regret \u2126(T2/3) against some\nadaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( \u221a\nT) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).\nFor convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al., 2016, Theorem 1] with polynomial running time provided the number of constraints of the underlying polytope is polynomial in the dimension. However the case of convex loss does not subsume the combinatorial/linear case, as with convex loss all inner points of the convex set are actions; with linear losses the actions are limited to the vertices of the underlying polytope in most cases.\nWe refer the interested reader to the excellent survey of Bubeck and Cesa-Bianchi [2012] on bandit problems.\nContribution\nOur main contribution is a high-probability regret bound for CombEXP from Combes et al. [2015] for adaptive adversaries over actions coming from arbitrary polytopes P \u2286 Rn. Our algorithm, being a slight generalization of CombExp, maintains computational efficiency. In particular, our contribution can be summarized as follows:\n(i) High-probability bounds for an efficient algorithm. For CombEXP we establish a high-probability regret of\nO\n( B2 + nB\nmin{\u03bb, 1} \u00b7 ln 2n + 2 \u03b4\n)\nT2/3,\nwith probability 1\u2212 \u03b4, where B is the \u21132-diameter of P, and \u03bb is a lower bound on the smallest eigenvalue of the exploration covariance matrix, see Theorem 3.1 for the exact regret bound.\nFor comparison we show that the same method already provides a high-probability regret bound of O(T2/3) for the original ComBand, albeit a suboptimal one as GeometricHedge achieves O( \u221a T) regret.\n(ii) Generalization of CombEXP and computational efficiency. We generalize CombEXP to actions arising from ar-\nbitrary polytopes contained in Rn and to the case of adaptive adversaries. We maintain computational efficiency of CombExp providing running times relative to a linear programming oracle over the underlying polytope P, separating the complexity for learning from the complexity of linear optimization over P.\nAll our bounds are any-time, i.e., holding uniformly for all times T. In particular, our parameter choices are independent of T.\nOutline\nAfter a brief summary of the regret minimization framework in Section 2, we reanalyze CombEXP in Section 3. For completeness we present a similar analysis for ComBand in Section 4.\nWe relegated various related materials to the the Appendix. In Section A we provide an any-time version of EXP with time-varying parameters maintaining generalized distributions, defined by an arbitrary convex set in the positive\northant, instead of the probability simplex. We prove an O( \u221a T) regret bound in the full information case by standard\narguments, which forms the basis for our regret bounds for the bandit case. In Section B we recall concentration inequalities that we use to establish high-probability bounds. Finally, in Sections C and D we provide (already known) efficient algorithms for projection and distribution generation, which are key components in our algorithms. We include those for completeness of exposition and to make parameters explicit."}, {"heading": "2 Preliminaries", "text": "We will briefly recall the regret minimization framework to define our notation. In the sequential prediction problem with linear losses, at every round t the forecaster chooses an action xt from a finite set A \u2286 Rn and the adversary chooses a loss vector Lt \u2208 Rn. The forecaster suffers the loss \u2113t := L\u22bat xt. The goal of the forecaster is to minimize the regret\nT\n\u2211 t=1\nL \u22ba t xt \u2212 min x\u2208A\nT\n\u2211 t=1\nL \u22ba\nt x.\nAgainst an oblivious adversary, who chooses the Lt independently of the forecaster\u2019s actions, this is the extra loss suffered by not playing the best single action in hindsight. However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas.\nWith bandit feedback the forecaster learns only the loss \u2113t but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions.\nWe make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions.\nFollowing Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u00b5 on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u00b5 [yy\u22ba] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u00b5 is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and \u03bb = 1/n using the scalar product on R n induced by the additional structure. John\u2019s ellipsoid can be approximately estimated with a worse lower bound \u03bb = 1/n3/2 by Gr\u00f6tschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [\u22121,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [\u2212C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P. In this paper we deliberately avoid using the scalar product induced by the structure to be able to directly use the bounds available in the original space of the problem. Fortunately, the uniform distribution on an approximate barycentric spanner has a close to optimal minimal eigenvalue even in the original space, see Lemma E.1, which allows us to preserve sparsity of the original space. As such we assume that we have access to an exploration distribution over actions with sparse support of size n, where n is the dimension of the vector space, from which we can efficiently sample. Note that for specific problems exploration distributions with better minimal eigenvalue can be explicitly given; we refer the interested reader to Cesa-Bianchia and Lugosi [2012] and follow-up work for a large set of such examples.\nLet u := Ey\u223c\u00b5 [y] denote the expectation of \u00b5 and let e denote the Euler constant. Instead of dealing directly with A, it will be more convenient to use the convex hull P of A, then A contains the vertex set of P (and in many applications the two are equal). We shall use the Kullback\u2013Leibler divergence as Bregman divergence of the function\nf (x1, . . . , xn) = \u2211 n i=1 xi ln xi for projection:\nKL(x, y) = n\n\u2211 i=1 xi ln xi yi \u2212 n \u2211 i=1 xi + n \u2211 i=1 yi.\nIn the following, for a vector a \u2208 Rn wewill useRn>a := {x \u2208 Rn | xi > ai for all i \u2208 [n]} to denote the a-positive orthant. Moreover, a linear optimization oracle (or LP oracle) over a polytope P \u2286 Rn finds for any linear objective c \u2208 Rn a vertex x of P minimizing c\u22bax.\nIn all our bounds below, the O-notation only hides an absolute constant, i.e., all parameters of the algorithms are explicit. However, in Section 1 the O-notation hides also other parameters, like the dimension n."}, {"heading": "3 A high-probability regret bound for CombEXP", "text": "We provide an adaptation of CombEXP (Algorithm 1) with an O(T2/3) regret with high probability against adaptive adversaries, while maintaining computational efficiency. In a nutshell, EXP is run on the coordinates of the desired expectation x\u0302t of xt, and a new distribution over vertices xt of P is generated in every round. In order to obtain an efficient algorithm, we allow errors in the most resource-consuming components of the algorithm: the projection step and the distribution generation. The accuracy of distribution generation is controlled by a parameter \u03b5, and helps maintaining a distribution with sparse support, to allow fast sampling and fast computation of the covariance matrix Ct. The positive parameters \u03b7t, \u03b3t control the learning rate and exploration rate of the algorithm. The role of the shifting vector a \u2208 Rn is to avoid singularity issues with Kullback\u2013Leibler divergence. Except for the shifting vector a, these ideas already appeared in Combes et al. [2015].\nThe algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix. All the other steps are fast, depending only polynomially on the dimension.\nThemajor factor for the running time of sampling from the distribution (3), and computing the covariancematrix (4) is the sparsity of the generated distribution, i.e., the number of possible outcomes. Sparse distributions (number of outcomes polynomial in the dimension) of sufficient accuracy can be efficiently generated by the decomposition algorithm fromMirrokni et al. [2015], which we summarize as Algorithm 5 in Section D for the reader\u2019s convenience. Common choices of the exploration distribution \u00b5 are sparse, as discussed above, notwithstanding non-sparse distributions for \u00b5 are also acceptable which have an efficient sampling method and a precomputed covariance matrix. Therefore we will disregard the complexity of sampling and computation of the covariance matrix.\nFinally, the projection step (Line 10) can be efficiently accomplished by the Frank\u2013Wolfe algorithm (also called conditional gradient), which we recall in Algorithm 4 in Section C. Note that if Algorithm 4 is used for the projection step, it already provides a sparse linear decomposition of the desired expectation x\u0302t+1 with accuracy \u03b5 = 0, and therefore makes a separate linear decomposition step unnecessary. Nevertheless it might be advantageous for specific polytopes to use a specialized, more efficient projection algorithm and/or decomposition algorithm.\nAll in all, we measure complexity of only the most time-consuming tasks: projection and linear decomposition, requiring the linear decomposition to be sparse. We report complexity of Algorithms 4 and 5 mentioned above in the total number of linear optimization oracle calls over P. This relative complexity is often useful in applications where fast linear programming oracles are available.\nNow we are ready to state our main theorem on the regret and complexity of CombEXP.\nTheorem 3.1 (High-probability regret bound for CombEXP for adaptive adversaries). For n \u2265 1 and with the choice\n\u03b3t := t\u22121/3\n2 and \u03b7t := min{\u03b32t , \u03b3t\u03bb}\nAlgorithm 1 achieves for any time T \u2265 1 the following regret: With probability at least 1 \u2212 \u03b4, for any x \u2208 P we have T\n\u2211 t=1\n(L\u22bat xt \u2212 L \u22ba t x) \u2264 ( 4 KL(a + x, a + x\u03021) min{1, 2\u03bbT1/3} + B1 \u221a 3 \u03bb ln n + 2 \u03b4 + ( (e \u2212 2)\u2016a\u20161 + B1 \u03bb + 2 + B(B + \u03b5) \u03bb ) 3 4 ) T2/3\n+O\n(\nmax\n{\n1, B2 max{(\u2016a\u20161 + B1)/\u03bb, \u03b5}\n\u03bb , B1 max{1, B} \u03bb , B + \u03b5\u221a\n\u03bb\n}\u221a T ) ln 2n + 2\n\u03b4 . (1)\nIn particular, assuming \u03b1 \u2212 ai \u2264 zi \u2264 \u03b2 \u2212 ai for some 0 < \u03b1 < \u03b2 for all z \u2208 P:\nAlgorithm 1 CombEXP\nRequire: polytope P \u2286 Rn>\u2212a, positive parameters \u03b5, \u03b71 \u2265 \u03b72 \u2265 . . ., and 1/2 \u2265 \u03b31, \u03b32, . . . Ensure: vertices xt of P as actions 1: x\u03021 \u2208 P arbitrary 2: for t = 1 to T do 3: Find distribution pt with \u2016Ex\u223cpt [x]\u2212 x\u0302t\u20162 \u2264 \u03b3t\u03b5 {approximate distribution} 4: qt \u2190 (1 \u2212 \u03b3t)pt + \u03b3t\u00b5 5: Sample xt \u223c qt. 6: Observe loss \u2113t := L \u22ba\nt xt 7: Ct \u2190 Ex\u223cqt [xx\u22ba] 8: L\u0302t \u2190 \u2113tC\u22121t xt 9: yt+1,i \u2190 (ai + x\u03021,i)1\u2212\u03b7t+1/\u03b7t(ai + x\u0302t,i)\u03b7t+1/\u03b7t exp(\u2212\u03b7t+1L\u0302t,i)\u2212 ai for all i \u2208 [n] 10: Find x\u0302t+1 \u2208 P with KL(a + z, a + x\u0302t+1) \u2264 KL(a + z, a + yt+1) + \u03b3t\u03b7t+1 for all z \u2208 P {approximate\nprojection}\n11: end for\n(i) Regret boundWe have KL(a + x, a + x\u03021) \u2264 4B 2 \u03b1 so that the upper bound on the regret is proportional to T 2/3.\nWith probability at least 1 \u2212 \u03b4, for any x \u2208 P we have T\n\u2211 t=1\n(L\u22bat xt \u2212 L \u22ba t x) \u2264 O ( B2\n\u03b1 +\nB1 + (B + \u03b5) 2 + \u2016a\u20161\nmin{\u03bb, \u221a \u03bb} \u00b7 ln 2n + 2 \u03b4\n)\nT2/3.\n(ii) Complexity. Using Algorithm 4 both for projection and distribution generation (Lines 3 and 10) with \u03b5 = 0, the\nalgorithm makes altogether O (\nB4\u03b2 \u03b13 min{1,\u03bb2}\n)\nT3 oracle calls to a linear optimization oracle over P.\nAlternatively using a specialized projection algorithm in Line 10, and Algorithm 5 for distribution generation in Line 3, then Algorithm 5 calls a linear optimization oracle over P at most O(B2/\u03b52)T5/3 times across all rounds.\nObviously, the O(B2/\u03b52)T5/3 oracle calls in the last sentence does not contain the complexity of the specialized projection algorithm.\nNote that the bounds in Theorem 3.1 are any-time guarantees as the parameters of the algorithm do not depend on the time horizon T. The constant factor in the regret bound can be slightly improved by a more sophisticated choice of the \u03b3t and \u03b7t, however, we preferred simple formulae for these parameters. Just as for EXP3, the choice of parameters is different for the best expected regret and the best high-probability regret."}, {"heading": "3.1 Proof of Theorem 3.1", "text": "In this section we will prove Theorem 3.1. We focus on the main regret bound, Equation (1), the other results easily follow from it. See Propositions C.1 and D.1 for the complexity of Algorithms 4 and 5. The inequality KL(a + y, a + x\u03021) \u2264 4B 2 \u03b1 is derived using ln z \u2264 z \u2212 1:\nKL(a + y, a + x\u03021) = n\n\u2211 i=1\n(ai + yi) ln ai + yi\nai + x\u03021,i \u2212\nn\n\u2211 i=1\n(ai + yi) + n\n\u2211 i=1 (ai + x\u03021,i)\n\u2264 n\n\u2211 i=1 (ai + yi)\n( ai + yi\nai + x\u03021,i \u2212 1\n)\n\u2212 n\n\u2211 i=1\n(ai + yi) + n\n\u2211 i=1\n(ai + x\u03021,i) = n\n\u2211 i=1 (yi \u2212 x\u03021,i)2 ai + x\u03021,i \u2264 \u2016y \u2212 x\u03021\u2016 2 2 \u03b1 \u2264 4B 2 \u03b1 .\nThe proof of Equation (1) follows the standard approach, whereby we break-up the regret estimation into various\npieces, which we estimate separately:\nT\n\u2211 t=1\n(L\u22bat xt \u2212 L \u22ba t x) \u2264 T\n\u2211 t=1\n( L \u22ba\nt xt \u2212 L\u0302 \u22ba t x\u0302t )\n\ufe38 \ufe37\ufe37 \ufe38\nLemma 3.5\n+ T\n\u2211 t=1\n(L\u0302\u22bat x\u0302t \u2212 L\u0302 \u22ba t x)\n\ufe38 \ufe37\ufe37 \ufe38\nLemmas 3.3 and 3.4\n+ T\n\u2211 t=1\n( L\u0302 \u22ba\nt x \u2212 L \u22ba t x )\n\ufe38 \ufe37\ufe37 \ufe38\nLemma 3.6\n. (2)\nLet Et [\u2212] := E [\u2212 | x1, L1, . . . , xt\u22121, Lt\u22121, Lt] denote the conditional expectation operator given the history preceding round t and also the adversary\u2019s action in round t. In particular, Ct = Et [ xtx \u22ba\nt\n] = (1 \u2212 \u03b3t)Pt + \u03b3t J, with\nPt := Ex\u223cpt [xx \u22ba]. We first establish some basic bounds on quantities occurring in Algorithm 1.\nLemma 3.2 (Basic bounds). Let y \u2208 P be arbitrary.\n\u2016C\u22121t \u20162 \u2264 1\n\u03b3t\u03bb (3)\n\u2016L\u0302t\u20162 \u2264 B\n\u03b3t\u03bb (4)\n\u2016Lt\u20162 \u2264 B\n\u03bb (5)\nProof. Equation (3) follows from Ct \u03b3t J \u03b3t\u03bbI. Inequality (4) follows via\n|L\u0302t| = |\u2113t \u00b7 C\u22121t xt| \u2264 |\u2113t| \u00b7 \u2016C\u22121t \u20162 \u00b7 \u2016xt\u20162 \u2264 B\n\u03b3t\u03bb .\nFinally, (5) follows from the estimation\n\u2016L\u22bat \u20162 = \u2016L \u22ba t J J \u22121\u20162 = \u2225 \u2225 \u2225Ey\u223c\u00b5 [ L\u22bat yy \u22ba J\u22121 ]\u2225 \u2225 \u2225\n2 \u2264 Ey\u223c\u00b5\n[ \u2016L\u22bat y \u00b7 y\u22ba J\u22121\u20162 ] \u2264 Ey\u223c\u00b5\n\n \u2016L\u22bat y\u20162 \ufe38 \ufe37\ufe37 \ufe38\n\u22641\n\u2016y\u20162 \ufe38\ufe37\ufe37\ufe38\n\u2264B\n\u00b7 \u2016J\u22121\u20162 \ufe38 \ufe37\ufe37 \ufe38\n\u22641/\u03bb\n\n  \u2264 B\n\u03bb .\nWe now estimate the pieces of Equation (2). The following series of upper bounds are independent of the concrete choice of the parameters \u03b3t, \u03b7t. However, for the reader\u2019s convenience in the last inequality of each estimation we make the bound explicit by substituting the values for \u03b3t, \u03b7t by the choices given in Theorem 3.1. We will tacitly use the following inequality to estimate sums like \u2211 T t=1 \u03b3t:\nT\n\u2211 t=1\nt\u03b1 \u2264 { T\u03b1+1+\u03b1 \u03b1+1 \u2264 T \u03b1+1\n\u03b1+1 , \u22121 < \u03b1 < 0 T\u03b1+1\u22121\n\u03b1+1 + T \u03b1 \u2264 T\u03b1+1\u03b1+1 + T\u03b1, \u03b1 > 0\nWe first estimate the regret when using the loss estimators L\u0302t. For this we use a generalized variant of EXP (see Lemma A.1), which works with arbitrary convex sets contained in the positive orthant.\nLemma 3.3.\nT\n\u2211 t=1\n(L\u0302\u22bat x\u0302t \u2212 L\u0302 \u22ba t x) \u2264 KL(a + x, a + x\u03021)\n\u03b7T + T\u22121 \u2211 t=1 \u03b3t + (e \u2212 2) T \u2211 t=1 \u03b7t n \u2211 i=1 (ai + x\u0302t,i)L\u0302 2 t,i\n\u2264 4 KL(a + x, a + x\u03021)T 2/3 min{1, 2\u03bbT1/3} + 3 4 T2/3 + (e \u2212 2) T\n\u2211 t=1\n\u03b7t n\n\u2211 i=1\n(ai + x\u0302t,i)L\u0302 2 t,i.\n(6)\nProof. This follows from Lemma A.1 with the L\u0302t as loss vectors and the a + x\u0302t as played actions. Note that a cancels on the left hand side in (L\u0302\u22bat (a + x\u0302t)\u2212 L\u0302 \u22ba t (a + x)).\nIn a next step we estimate the last term of Equation (6).\nLemma 3.4. With probability at least 1 \u2212 \u03b4\nT\n\u2211 t=1\n\u03b7t n\n\u2211 i=1\n(ai + x\u0302t,i)L\u0302 2 t,i \u2264 \u2016a\u20161 + B1 \u03bb T\n\u2211 t=1 \u03b7t \u03b3t + (\u2016a\u20161 + B1)B2 \u03bb2\n\u221a \u221a \u221a \u221a1\n2\nT\n\u2211 t=1 \u03b72t \u03b34t \u00b7 ln 1 \u03b4\n\u2264 \u2016a\u20161 + B1 \u03bb 3 4 T2/3 + (\u2016a\u20161 + B1)B2 \u03bb2\n\u221a\n1 2 T ln 1 \u03b4 .\n(7)\nProof. This is a special case of the Azuma\u2013Hoeffding inequality (recalled in Theorem B.1) using the bounds\n0 \u2264 n\n\u2211 i=1\n(ai + x\u0302t,i)L\u0302 2 t,i \u2264\nn\n\u2211 i=1 (ai + x\u0302t,i)\n( B\n\u03b3t\u03bb\n)2\n\u2264 (\u2016a\u20161 + B1)B 2\n(\u03b3t\u03bb)2\nand\nEt\n[ n\n\u2211 i=1\n(ai + x\u0302t,i)L\u0302 2 t,i\n]\n= Et\n[ n\n\u2211 i=1\n(ai + x\u0302t,i)\u2113 2 t e \u22ba i C \u22121 t xtx \u22ba t C \u22121 t ei\n]\n= n\n\u2211 i=1\n(ai + x\u0302t,i)\u2113 2 t e \u22ba i C \u22121 t CtC \u22121 t ei\n\u2264 n\n\u2211 i=1\n(ai + x\u0302t,i)e \u22ba i C \u22121 t ei \u2264 \u2016a\u20161 + B1 \u03b3t\u03bb .\nNext we bound the difference between the true loss L \u22ba t xt and the expected estimated loss L\u0302 \u22ba t x\u0302t.\nLemma 3.5. With probability at least 1 \u2212 \u03b4\nT\n\u2211 t=1\n( L\u22bat xt \u2212 L\u0302 \u22ba t x\u0302t ) \u2264\n(\n1 + B(B + \u03b5)\n\u03bb\n) T\n\u2211 t=1 \u03b3t +\n\u221a \u221a \u221a \u221a2 ( T + (3B + \u03b5)(B + \u03b5)\n\u03bb\nT\n\u2211 t=1 \u03b3t (1 \u2212 \u03b3t)2\n)\nln 1\n\u03b4\n+ 1\n3\n(\nB \u221a\n\u03b3T(1 \u2212 \u03b3T)\u03bb + 3 +\nB2\u03b5\n\u03bb\n)\nln 1\n\u03b4\n\u2264 ( 1 + B(B + \u03b5)\n\u03bb\n) T\n\u2211 t=1\n\u03b3t + B\n3 \u221a \u03b3T(1 \u2212 \u03b3T)\u03bb ln\n1 \u03b4 +\n[\n3 + B2\u03b5\n\u03bb + O\n(\nmax\n{\n1, B + \u03b5\u221a\n\u03bb\n}\u221a T )] max { 1, ln 1\n\u03b4\n}\n\u2264 ( 1 + B(B + \u03b5)\n\u03bb\n) 3\n4 T2/3 +\n\u221a 2B 3 \u221a \u03bb (T1/6 +T\u22121/6) ln 1 \u03b4 + [ 3 + B2\u03b5 \u03bb + O ( max { 1, B + \u03b5\u221a \u03bb }\u221a T )] max { 1, ln 1 \u03b4 } .\n(8)\nProof. Let x\u0303t := Ex\u223cpt [x] and xt := Et [xt] = (1 \u2212 \u03b3t)x\u0303t + \u03b3tu. As also \u2016x\u0302t \u2212 x\u0303t\u20162 \u2264 \u03b3t\u03b5, we have xt = (1 \u2212 \u03b3t)x\u0302t + \u03b3tv for v := u + 1\u2212\u03b3t\u03b3t (x\u0303t \u2212 x\u0302t) with \u2016v\u20162 \u2264 B + \u03b5(1 \u2212 \u03b3t) \u2264 B + \u03b5. We consider the martingale difference sequence\nXt := L \u22ba t xt \u2212 L\u0302 \u22ba t x\u0302t \u2212 Et [ L\u22bat xt \u2212 L\u0302 \u22ba t x\u0302t ] = L\u22bat xt \u2212 L\u0302 \u22ba t x\u0302t \u2212 L \u22ba t xt + L \u22ba t x\u0302t = L \u22ba t xt \u2212 L\u0302 \u22ba t x\u0302t + \u03b3tL \u22ba t (x\u0302t \u2212 v).\nNote that as x\u0303tx\u0303 \u22ba t = Ex\u223cpt [x]Ex\u223cpt [x] \u22ba Ex\u223cpt [xx\u22ba] = Pt \u2264 Ct/(1 \u2212 \u03b3t)\n( L\u0302\u22bat x\u0303t )2 = L\u0302\u22bat x\u0303t x\u0303 \u22ba t L\u0302t = \u2113 2 t x \u22ba t C \u22121 t x\u0303t x\u0303 \u22ba t C \u22121 t xt \u2264 x \u22ba t C \u22121 t PtC \u22121 t xt \u2264\nx \u22ba t C \u22121 t xt\n1 \u2212 \u03b3t \u2264 B\n2\n\u03b3t\u03bb(1 \u2212 \u03b3t) \u2264 B\n2\n\u03b3T\u03bb(1 \u2212 \u03b3T) ,\nand\n|L\u0302\u22bat x\u0302t \u2212 L\u0302 \u22ba t x\u0303t| \u2264 \u2016L\u0302t\u20162 \u00b7 \u2016x\u0302t \u2212 x\u0303t\u20162 \u2264 B2\n\u03b3t\u03bb \u03b3t\u03b5 =\nB2\u03b5\n\u03bb ,\nhence\n|Xt| \u2264 \u2223 \u2223L\u22bat xt \u2223 \u2223+ \u2223 \u2223L\u22bat xt \u2223 \u2223+ \u2223 \u2223L\u22bat x\u0302t \u2223 \u2223+ \u2223 \u2223L\u0302\u22bat x\u0302t \u2212 L\u0302 \u22ba t x\u0303t \u2223 \u2223+ \u2223 \u2223L\u0302\u22bat x\u0303t \u2223 \u2223 \u2264 3 + B\n2\u03b5\n\u03bb +\nB \u221a\n\u03b3T(1 \u2212 \u03b3T)\u03bb ,\nand the variance of Xt is easily bounded by:\nVart [Xt] \u2264 Et [ (L\u22bat xt \u2212 L\u0302 \u22ba t x\u0302t) 2 ] = Et [ (\u2113t \u00b7 (1 \u2212 x\u22bat C\u22121t x\u0302t))2 ]\n\u2264 Et [ (1 \u2212 x\u22bat C\u22121t x\u0302t)2 ] = Et [ 1 \u2212 2x\u22bat C\u22121t x\u0302t + x\u0302 \u22ba t C \u22121 t xtx \u22ba t C \u22121 t x\u0302t ]\n= 1\u2212 2xt\u22baC\u22121t x\u0302t + x\u0302 \u22ba t C \u22121 t x\u0302t = 1\u2212 1 \u2212 2\u03b3t (1 \u2212 \u03b3t)2 xt \u22baC\u22121t xt + \u03b32t (1 \u2212 \u03b3t)2 (v \u2212 2xt)\u22ba C\u22121t v \u2264 1+ \u03b3t(3B + \u03b5)(B + \u03b5) (1 \u2212 \u03b3t)2\u03bb .\nHence Benett\u2019s inequality (Theorem B.2, [Fan et al., 2012, (18)]) applied to the martingale difference sequence Xt provides\nT\n\u2211 t=1\n( L\u22bat xt \u2212 L\u0302 \u22ba t x\u0302t + \u03b3tL \u22ba t (x\u0302t \u2212 v) ) \u2264 1\n3\n(\nB \u221a\n\u03b3T(1 \u2212 \u03b3T)\u03bb + 3 +\nB2\u03b5\n\u03bb\n)\nln 1\n\u03b4\n+ \u221a \u221a \u221a \u221a2 ( T + (3B + \u03b5)(B + \u03b5)\n\u03bb\nT\n\u2211 t=1 \u03b3t (1 \u2212 \u03b3t)2\n)\nln 1\n\u03b4 .\nThe claim follows by using |L\u22bat (x\u0302t \u2212 v)| \u2264 1 + B(B + \u03b5)/\u03bb.\nFinally, we bound the difference between the true loss L\u22bat x and the estimated loss L\u0302 \u22ba t x for any point x \u2208 P.\nLemma 3.6. For all 0 < \u03b4 < 1 with probability at least 1 \u2212 \u03b4 for every x \u2208 Rn simultaneously\nT\n\u2211 t=1\n( L\u0302 \u22ba\nt x \u2212 L \u22ba t x ) \u2264 \u2016x\u20161\n3\n( B\n\u03bb +\n1\n\u03b3T\u03bb\n)\nln 2n\n\u03b4 + \u2016x\u20161\n\u221a \u221a \u221a \u221a 2\n\u03bb\nT\n\u2211 t=1\n1\n\u03b3t ln\n2n\n\u03b4 ."}, {"heading": "In particular, with probability at least 1 \u2212 \u03b4, for all x \u2208 P simultaneously", "text": "T\n\u2211 t=1\n( L\u0302 \u22ba\nt x \u2212 L \u22ba t x ) \u2264 B1\n3\u03bb\n( B + 2T1/3 ) ln 2n\n\u03b4 + B1T\n2/3\n\u221a\n1 + 4\n3T\n\u221a\n3 \u03bb ln 2n \u03b4 . (9)\nRemark 3.7. Restricting the statement for all x \u2265 0, the ln(2n/\u03b4) can be replaced by ln(n/\u03b4).\nProof. Let x = \u00b1ei be a coordinate vector or its negation. Then\nVart [ L\u0302\u22bat x \u2212 L \u22ba t x ] \u2264 Et\n[ (L\u0302\u22bat x) 2 ] \u2264 Et [ x\u22baC\u22121t xtx \u22ba t C \u22121 t x ] = x\u22baC\u22121t x \u2264 1\n\u03b3t\u03bb ,\nand\n|L\u0302\u22bat x \u2212 L \u22ba t x| \u2264 B\n\u03bb +\n1\n\u03b3t\u03bb .\nHence by Benett\u2019s inequality (Theorem B.2, [Fan et al., 2012, (18)]) the claim follows for a fixed vector x = \u00b1ei with probability at least 1 \u2212 \u03b4/(2n). Hence by the union bound, it holds for all x = \u00b1ei simultaneously with probability at least 1 \u2212 \u03b4. Finally, the inequality for a general x follows by taking linear combinations with the absolute values of the coefficients of x.\nSumming up (6), (7), (8) (substituting \u03b4/(2n + 2) for \u03b4 in the latter two) and (9) (substituting 2n\u03b4/(2n + 2) for \u03b4), with probability at least 1 \u2212 \u03b4 yields (1) of Theorem 3.1.\n4 A high-probability regret bound for ComBand\nIn this section we will show that ComBand of Cesa-Bianchia and Lugosi [2012] achieves a high-probability regret bound of O(T2/3) without any modifications. While this is worse than the optimal regret of O( \u221a\nT) obtained by GeometricHedge in Bartlett et al. [2008], it shows that already Algorithm 2, the vanilla version of ComBandwithout any correction terms suffices to achieve a high-probability regret bound.\nTheorem 4.1. With the choice\n\u03b7t := \u03b3t\u03bb\nB2 and \u03b3t :=\nt\u22121/3\n2\nAlgorithm 2 ComBand\nRequire: Losses Lt, action set A \u2286 Rn, positive parameters \u03b71 \u2265 \u03b72 \u2265 . . ., 1/2 \u2265 \u03b31 \u2265 \u03b32 \u2265 . . . Ensure: actions xt \u2208 A 1: for t = 1 to T do 2: wt(x) \u2190 \u2211t\u22121i=1 L\u0302 \u22ba\ni x for all x 3: Wt \u2190 \u2211x wt(x) 4: pt(x) \u2190 wt(x)/Wt for all x 5: qt \u2190 (1 \u2212 \u03b3t)pt + \u03b3t\u00b5 6: Sample xt \u223c qt. 7: Observe loss \u2113t := L \u22ba\nt xt. 8: Ct \u2190 Ex\u223cqt [xx\u22ba] 9: L\u0302t \u2190 \u2113tC\u22121t xt 10: end for\nAlgorithm 2 achieves regret (\u221a\n3B\u221a \u03bb\n\u221a\nln N + 2\n\u03b4 + n 3(e \u2212 2)\u03bb 4B2 + 3 2\n)\nT2/3 +O\n(\nn \u03bb\nB2 +\n(\n1 + B2\n\u03bb\n)\nln N + 2\n\u03b4\n)\u221a T (10)\n\u2264 O (\nB\u221a \u03bb\n\u221a\nln N + 2\n\u03b4 + n\n\u03bb\nB2\n)\nT2/3\nwith probability at least 1 \u2212 \u03b4 for 0 < \u03b4 < 1. Remark 4.2. Similar to Theorem 3.1, it is possible to change the ln((N + 2)/\u03b4) in the coefficient of T2/3 to the possibly much smaller ln((n + 2)/\u03b4) with a suitable altering of the other constants. However, since an T1/3 ln N term will still remain in the regret bound, this does not seem to be a significant improvement.\nWe use the same notation as in Section 3.1 for CombEXP, which we recall here for the reader\u2019s convenience. Let Et [\u2212] := E [\u2212 | x1, L1, . . . , xt\u22121, Lt\u22121, Lt] denote the conditional expectation operator given the history preceding round t and also the adversary\u2019s action in round t. Let x\u0303t := Ex\u223cpt [x] and Pt := Ex\u223cpt [xx \u22ba] denote the expectation and variance of distribution pt, respectively. Note that Ct = Et [ xtx \u22ba\nt\n] = (1 \u2212 \u03b3t)Pt + \u03b3t J.\nLemma 4.3 (Basic bounds). Let x, y1, and y2 be arbitrary actions.\n(i) Bounds on size\n|y\u22ba1 C\u22121t y2| \u2264 B2\n\u03b3t\u03bb (11)\n|L\u0302\u22bat x| \u2264 B2\n\u03b3t\u03bb (12)\n(ii) Bounds on expectation\nEt\n[\nx\u22bat C \u22121 t xt\n]\n= n (13)\nProof. Equation (11) follows from the bounds \u2016y1\u20162, \u2016y2\u20162 \u2264 B and \u2016C\u22121t \u20162 \u2264 1/(\u03b3t\u03bb), as Ct \u03b3t J \u03b3t\u03bbI. Inequality (12) follows via\n|L\u0302\u22bat x| = |\u2113t \u00b7 x \u22ba t C \u22121 t x| \u2264\nB2\n\u03b3t\u03bb .\nTo prove (13), we use a trick using the trace function to compute the expectation:\nEt\n[\nx\u22bat C \u22121 t xt\n] = Et [ Tr(C\u22121t xtx \u22ba t ) ] = Tr(C\u22121t Ct) = Tr(I) = n.\nRemark 4.4. One can similarly prove Ey\u223cpt [ y\u22baP\u22121t y ] = n, but it will not be used in the following.\nAs in the case of CombEXP, the lemmas below are independent of the choice of the \u03b3t, \u03b7t except for the last formula in each lemma, where we particularize the bounds by substituting parameters.\nFirst instead of the real regret, we estimate the regret computed using the estimators L\u0302t.\nLemma 4.5. With probability at least 1 \u2212 \u03b4\nT\n\u2211 t=1\n(L\u0302\u22bat x\u0303t \u2212 L\u0302 \u22ba t x) \u2264 ln N\n\u03b7T + (e \u2212 2)\n n T\n\u2211 t=1 \u03b7t 1 \u2212 \u03b3t + B2 \u03bb\n\u221a \u221a \u221a \u221a1\n2\nT\n\u2211 t=1 \u03b72t \u03b32t (1 \u2212 \u03b3t)2 \u00b7 ln 1 \u03b4\n\n\n\u2264 2B 2 ln N\n\u03bb T1/3 + (e \u2212 2)\n(\nn 3\u03bb\n4B2 (T2/3 + 2T1/3) + B\u221a \u03bb\n\u221a\n2T ln 1\n\u03b4\n)\n.\nProof. By Lemma A.1,\nT\n\u2211 t=1\n( L\u0302 \u22ba\nt x\u0303t \u2212 L\u0302 \u22ba t x ) \u2264 ln N\n\u03b7T + (e \u2212 2)\nT\n\u2211 t=1\n\u03b7t Ey\u223cpt [ (L\u0302\u22bat y) 2 ] .\nTo estimate the last term, first note that\nEy\u223cpt [ (L\u0302\u22bat y) 2 ] = Ey\u223cpt [ L\u0302\u22bat yy \u22baL\u0302t ] = L\u0302\u22bat Pt L\u0302t = \u2113 2 t x T t C \u22121 t PtC \u22121 t xt \u2264 x \u22ba t C \u22121 t xt\n1 \u2212 \u03b3t .\nSo far combining our estimates provides\nT\n\u2211 t=1\n(L\u0302\u22bat x\u0303t \u2212 L\u0302 \u22ba t x) \u2264 ln N\n\u03b7T +\nT\n\u2211 t=1\n\u03b7t x\u22bat C \u22121 t xt\n1 \u2212 \u03b3t =\n2B2 ln N\n\u03bb T1/3 + (e \u2212 2)\nT\n\u2211 t=1\n\u03b7t x\u22bat C \u22121 t xt\n1 \u2212 \u03b3t . (14)\nTo estimate the last term on the right-hand side, we apply the Azuma\u2013Hoeffding inequality using (11) and (13) for bounding the summands and their expectation, which readily proves the lemma:\nT\n\u2211 t=1\n\u03b7t x\u22bat C \u22121 t xt\n1 \u2212 \u03b3t \u2264 n\nT\n\u2211 t=1 \u03b7t 1 \u2212 \u03b3t + B2 \u03bb\n\u221a \u221a \u221a \u221a1\n2\nT\n\u2211 t=1 \u03b72t \u03b32t (1 \u2212 \u03b3t)2 \u00b7 ln 1 \u03b4 .\nWe turn our attention to the difference between the real loss vectors Lt and their estimators L\u0302t. We start by comparing the loss of the played action.\nLemma 4.6. With probability at least 1 \u2212 \u03b4\nT\n\u2211 t=1\n( L \u22ba\nt xt \u2212 L\u0302 \u22ba t x\u0303t ) \u2264 2\nT\n\u2211 t=1\n\u03b3t + 1\n3\n(\n2 + B \u221a\n\u03b3T\u03bb(1 \u2212 \u03b3T)\n)\nln 1\n\u03b4 +\n\u221a \u221a \u221a \u221a2 ( T + 3B2\n\u03bb\nT\n\u2211 t=1 \u03b3t (1 \u2212 \u03b3t)2\n)\nln 1\n\u03b4\n\u2264 3 2 T2/3 + 1 3\n(\n2 + \u221a 2B\u221a \u03bb (T1/6 + T\u22121/6) ) ln 1 \u03b4 + \u221a 2 ( T + 9B2 \u03bb T2/3 ) ln 1 \u03b4 .\n(15)\nProof. Let xt := Et [xt] = (1 \u2212 \u03b3t)x\u0303t + \u03b3tu denote the conditional expectation of xt given the history before round t and loss Lt. The statement is a special case of Benett\u2019s inequality (see Theorem B.2) for the martingale\nXt := L \u22ba t xt \u2212 L\u0302 \u22ba t x\u0303t \u2212 Et [ L\u22bat xt \u2212 L\u0302 \u22ba t x\u0303t ] = L\u22bat xt \u2212 L\u0302 \u22ba t x\u0303t \u2212 L \u22ba t xt + L \u22ba t x\u0303t = L \u22ba t xt \u2212 L\u0302 \u22ba t x\u0303t + \u03b3tL \u22ba t (x\u0303t \u2212 u).\nNote that x\u0303t x\u0303t \u22ba Pt by Jensen\u2019s inequality, therefore\n( L\u0302\u22bat x\u0303t )2 = L\u0302\u22bat x\u0303tx\u0303 \u22ba t L\u0302t = \u2113 2 t x \u22ba t C \u22121 t x\u0303t x\u0303 \u22ba t C \u22121 t xt \u2264 x \u22ba t C \u22121 t PtC \u22121 t xt \u2264\nx\u22bat C \u22121 t xt\n1 \u2212 \u03b3t \u2264 B\n2\n\u03b3t\u03bb(1 \u2212 \u03b3t) ,\nhence\n|Xt| \u2264 1 + B \u221a\n\u03b3t\u03bb(1 \u2212 \u03b3t) + 2\u03b3t \u2264 2 +\nB \u221a\n\u03b3T\u03bb(1 \u2212 \u03b3T) ,\nand the variance of Xt is easily bounded by:\nVart [Xt] \u2264 Et [ (L\u22bat xt \u2212 L\u0302 \u22ba t x\u0303t) 2 ] = Et [ (\u2113t \u00b7 (1 \u2212 x\u22bat C\u22121t x\u0303t))2 ]\n\u2264 Et [ (1 \u2212 x\u22bat C\u22121t x\u0303t)2 ] = Et [ 1 \u2212 2x\u22bat C\u22121t x\u0303t + x\u0303 \u22ba t C \u22121 t xtx \u22ba t C \u22121 t x\u0303t ]\n= 1 \u2212 2xt\u22baC\u22121t x\u0303t + x\u0303 \u22ba t C \u22121 t x\u0303t = 1 \u2212 1 \u2212 2\u03b3t (1 \u2212 \u03b3t)2 xt \u22baC\u22121t xt + \u03b32t (1 \u2212 \u03b3t)2 (u \u2212 2xt)\u22ba C\u22121t u \u2264 1 + 3\u03b3tB 2 (1 \u2212 \u03b3t)2\u03bb .\nBenett\u2019s inequality provides\nT\n\u2211 t=1\n( L \u22ba\nt xt \u2212 L\u0302 \u22ba t x\u0303t + \u03b3tL \u22ba t (xt \u2212 u) ) \u2264 1\n3\n(\n2 + B \u221a\n\u03b3T\u03bb(1 \u2212 \u03b3T)\n)\nln 1\n\u03b4 +\n\u221a \u221a \u221a \u221a2 ( T + 3B2\n\u03bb\nT\n\u2211 t=1\n\u03b3t\n(1 \u2212 \u03b3t)2\n)\nln 1\n\u03b4 .\nThe claim follows by using |L\u22bat (xt \u2212 u)| \u2264 2.\nNow we compare the losses Lt with their estimator L\u0302t for all fixed actions.\nLemma 4.7. For all 0 < \u03b4 < 1 with probability at least 1 \u2212 \u03b4 for every x \u2208 A simultaneously\nT\n\u2211 t=1\n( L\u0302\u22bat x \u2212 L \u22ba t x ) \u2264 1\n3\n(\n1 + B2\n\u03b3T\u03bb\n)\nln N\n\u03b4 +\n\u221a \u221a \u221a \u221a2B 2\n\u03bb\nT\n\u2211 t=1\n1 \u03b3t \u00b7 ln N \u03b4\n\u2264 1 3\n(\n1 + 2B2\n\u03bb T1/3\n)\nln N\n\u03b4 + \u221a 3B\u221a \u03bb T2/3 \u221a 1 + 4 3T \u221a ln N \u03b4 .\n(16)\nProof. As customary for concentration inequalities, we start by a variance and size estimate:\nVart [ L\u0302 \u22ba t x \u2212 L \u22ba t x ] \u2264 Et\n[ (L\u0302\u22bat x) 2 ] \u2264 Et [ x\u22baC\u22121t xtx \u22ba t C \u22121 t x ] = x\u22baC\u22121t x \u2264 B2\n\u03b3t\u03bb ,\nand\n|L\u0302\u22bat x \u2212 L \u22ba t x| \u2264 1 + B2\n\u03b3t\u03bb .\nAlso note that L\u0302 \u22ba t x \u2212 L \u22ba\nt x is a martingale difference sequence. Hence by Benett\u2019s inequality (see Theorem B.2) the claim follows for a fixed action x with probability at least 1 \u2212 \u03b4/N. Therefore by the union bound, it holds for all x \u2208 A simultaneously with probability at least 1 \u2212 \u03b4.\nSumming up (14), (15) (substituting \u03b4/(N + 2) for \u03b4), and (16) (substituting N\u03b4/(N + 2) for \u03b4), we obtain (10) with probability at least 1 \u2212 \u03b4."}, {"heading": "5 Concluding remarks", "text": "We would like to mention that our method could be immediately strengthened to provide an optimal high-probability regret of O( \u221a T) using the correction term of GeometricHedge (see Bartlett et al. [2008]) and the identity\nEn\n\n \u2211 i\u2208[d]\nM\u0303i(n)X\u0303 2 i (n)\n\n = En [ X(n)\u22baM(n)M(n)\u22ba\u03a3+n\u22121M\u0303(n)M\u0303(n) \u22ba\u03a3+n\u22121 M(n)M(n) \u22baX(n) ] ,\nused for establishing theO( \u221a\nT) regret bound for the expected case under oblivious adversaries in [Combes et al., 2015, supplementary material, proof of Theorem 6]. However, we were unable to verify this identity 1, which is equivalent to\nEn\n\n \u2211 i\u2208[d]\nM\u0303i(n)X\u0303i(n) 2\n\n = En\n\n \n\n \u2211 i\u2208[d] M\u0303i(n)X\u0303i(n)\n\n\n2 \n  ,\nand as such we only claim the weaker bound of O(T2/3). This is the only obstacle to combining CombEXP with GeometricHedge to obtain an efficient algorithm with optimal high-probability regret O( \u221a\nT) for the adaptive case using our method.\nTo put this into context, without the above identity also for the expected regret case under oblivious adversaries we were only able to establish an O(T2/3) regret bound, matching our high-probability regret bound for adaptive adversaries."}, {"heading": "B Concentration inequalities", "text": "We will use the following concentration inequalities.\nTheorem B.1 (Azuma\u2013Hoeffding inequality). For a martingale difference sequence Xt with at \u2264 Xt \u2264 bt almost surely for constants at, bt, we have with probability at least 1 \u2212 \u03b4\nT\n\u2211 t=1\nXt \u2264\n\u221a\n\u2211 T t=1(bt \u2212 at)2 ln(1/\u03b4)\n2 .\nWhile the following inequality is stated only for b = 1 in [Fan et al., 2012, (18)] it easily generalizes via scaling to arbitrary b > 0.\nTheorem B.2 (Benett\u2019s inequality [Fan et al., 2012, (18)]). For a supermartingale difference sequence Xt bounded above by a positive constant Xt \u2264 b, for any v \u2265 0 with probability at least 1 \u2212 \u03b4:\nT\n\u2211 t=1\nVart [Xt] \u2265 v or T\n\u2211 t=1\nXt \u2264 b ln(1/\u03b4)\n3 +\n\u221a\n2v ln(1/\u03b4)."}, {"heading": "C Projection for Kullback\u2013Leibler divergence", "text": "We will now describe a generic, efficient, simple Frank\u2013Wolfe algorithm for the projection step in Line 10 of Algorithm 1. We remark that there are many possibilities for improvements, such as, e.g., employing advanced variants of the Frank\u2013Wolfe algorithm (see e.g., Lacoste-Julien and Jaggi [2015]) or using customized algorithms for specific polytopes. For example, in the case of the simplex P = {x \u2265 0 |\u2211i xi = 1}, the projection of x is simply x/ \u2211ni=1 xi and for the the permutahedron there exist very fast, specialized projection methods (see e.g., Lim and Wright [2016]).\nAlgorithm 4 Projection for KL\nRequire: linear optimization oracle over a polytope P \u2286 [\u03b1, \u03b2]n, \u03b1 > 0, upper bound B for the \u21132-diameter of P, accuracy \u03b5 > 0, point x \u2208 Rn >0 Ensure: yK \u2208 P with KL(z, yK) \u2264 KL(z, x) + \u03b5 for all z \u2208 P\ny0 \u2208 P any point K \u2190\n\u2308 4B4\u03b2 \u03b13\u03b52 \u2309\nfor k = 1 to K do s \u2208 arg minz\u2208P \u2211ni=1 zi ln(yk\u22121,i/xi) {Linear optimization oracle call} yk \u2190 ((k \u2212 1)yk\u22121 + 2s)/(k + 1) end for return yK\nProposition C.1. Given a polytope P \u2286 [\u03b1, \u03b2]n with \u03b1 > 0, an upper bound B for the \u21132-diameter of P, as well as an accuracy \u03b5 > 0, Algorithm 4 computes an approximate projection with O ( B4 \u03b2\n\u03b13\u03b52\n)\noracle calls.\nProof. As the algorithm calls the oracle once per iteration, the bound on the number of oracle calls is immediate. To prove the claimed accuracy of the returned point yK, note that the algorithm is the Frank\u2013Wolfe algorithm for the function f (z) := KL(z, x). Recall that the gradient\u2207 f (z) of f at z is given by (\u2207 f (z))i = ln(zi/xi) and the Hessian is a diagonalmatrix\u22072 f (z) = diag(1/z1, 1/z2, . . . , 1/zn). As 1/\u03b2 \u2264 1/zi \u2264 1/\u03b1 for z \u2208 P, the function f is 1/\u03b1smooth and 1/\u03b2-strongly convex on P in the \u21132-norm, and has curvature C f \u2264 B2/\u03b1. Let x\u2217 := arg minz\u2208P f (z), i.e., the Bregman projection of x to P. By [Jaggi, 2013, Theorem 1], f (yK)\u2212 f (x\u2217) \u2264 2C f /(K + 2), therefore by strong convexity\n1\n2\u03b2 \u2016yK \u2212 x\u2217\u201622 \u2264 KL(yK, x)\u2212 KL(x\u2217, x) \u2264\n2B2\n\u03b1(K + 2) .\nLet z \u2208 P be arbitrary. By the Pythagorean Theorem we have KL(z, x\u2217) \u2264 KL(z, x) and thus\nKL(z, yK)\u2212 KL(z, x) \u2264 KL(z, yK)\u2212 KL(z, x\u2217) = n\n\u2211 i=1\nzi ln x\u2217i\nyK,i \u2212\nn\n\u2211 i=1\nx\u2217i + n\n\u2211 i=1 yK,i\n\u2264 n\n\u2211 i=1 zi\n( x\u2217i\nyK,i \u2212 1\n)\n\u2212 n\n\u2211 i=1\nx\u2217i + n\n\u2211 i=1\nyK,i = n\n\u2211 i=1 zi \u2212 yK,i yK,i (x\u2217i \u2212 yK,i) \u2264 B \u03b1 \u2016x\u2217 \u2212 yK\u20162\n\u2264 B \u221a 2\u03b2\n\u03b1\n\u221a KL(yK, x)\u2212 KL(x\u2217, x) \u2264 2B2\n\u221a \u03b2\n\u03b13/2 \u221a K + 2 \u2264 \u03b5.\nPlugging in K = \u2308\n4B4\u03b2 \u03b13\u03b52\n\u2309\nas set by the algorithm provides the result."}, {"heading": "D Linear decomposition", "text": "For the convenience of the reader, we briefly recall the decomposition algorithm (Algorithm5) ofMirrokni et al. [2015] that for a polytope P approximately decomposes any point x \u2208 P into a convex combination of vertices of P, using a linear optimization oracle over P. The algorithm uses Mirror Descent (see Nemirovski [1979]) to find a convex combination.\nProposition D.1 ([Mirrokni et al., 2015, Theorem 3.5]). Given a polytope P with diameter at most 2D in \u21132-norm, and a point x \u2208 P, Algorithm 5 computes with O(D2/\u03b52) calls to a linear optimization oracle over P a multiset x1, . . . , xk of vertices for k = \u23084D2/\u03b52\u2309 such that \u2016\u2211ki=1 xi/k \u2212 x\u20162 \u2264 \u03b5.\nAlgorithm 5 Linear decomposition Require: linear optimization oracle over polytope P, an inner point x \u2208 P, precision \u03b5 Ensure: vertices x1, . . . , xk \u2208 P such that \u2016x \u2212 \u2211i \u03bbixi/k\u20162 \u2264 \u03b5\nk \u2190 \u23084D2/\u03b52\u2309 \u03b7 \u2190 4\u03b5(p \u2212 1) y1 \u2190 0; z1 \u2190 0 for t = 1 to k do Choose vertex xt \u2208 arg miny\u2208P y \u22ba t y {Linear optimization oracle call}\nzt+1 \u2190 zt \u2212 \u03b7(x \u2212 xt) if \u2016zt+1\u20162 > 1 then\nyt+1 \u2190 zt+1/\u2016zt+1\u20162 else\nyt+1 \u2190 zt+1 end if\nend for return x1, . . . , xk"}, {"heading": "E Fitness of barycentric spanners for exploration", "text": "Let \u03bbmin(\u00b5) denote the minimal eigenvalue of the covariance matrix Ex\u223c\u00b5 [xx\u22ba] of a distribution \u00b5. For exploration one wishes to find a \u00b5 with a high minimal eigenvalue \u03bbmin(\u00b5). Here we show that a uniform distribution on any approximate barycentric spanner achieves within anO(n2) factor the best possible minimal eigenvalue using any scalar product on Rn. The free choice of scalar product and hence orthonormal basis allows preserving sparse representation of a polytope P.\nLemma E.1. Let v1, . . . , vn be a C-approximate barycentric spanner of a polytope P \u2286 Rn. Then the uniform distribution \u00b5v1,...,vn on the spanner satisfies\n\u03bbmin(\u00b5v1,...,vn) \u2265 \u03bbmin(\u00b5)\nC2n2\nfor any distribution \u00b5 over P.\nProof. Using that the vi form a barycentric spanner, there are coefficients \u03bbx,i for all x \u2208 P satisfying\nx = \u2211 i\n\u03bbx,ivi, |\u03bbx,i| \u2264 C.\nIn particular, with \u03b1x := \u2211 n i=1|\u03bbx,i| \u2264 Cn by Jensen\u2019s inequality\nxx\u22ba n\n\u2211 i=1\n\u03b1x|\u03bbx,i|viv\u22bai C2n n\n\u2211 i=1\nviv \u22ba i .\nHence Ex\u223c\u00b5 [xx\u22ba] C2n \u2211ni=1 viv \u22ba i = C 2n2 Ex\u223c\u00b5v1,...,vn [xx \u22ba], from which the claim follows."}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["R. Arora", "O. Dekel", "A. Tewari"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "Siam J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computin,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2004}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["P.L. Bartlett", "V. Dani", "T. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari"], "venue": "In 21th Annual Conference on Learning Theory (COLT", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Kernel-based methods for bandit convex optimization", "author": ["S. Bubeck", "R. Eldan", "Y.T. Lee"], "venue": "arXiv preprint arXiv:1607.03084,", "citeRegEx": "Bubeck et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2016}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchia", "G. Lugosi"], "venue": "Journal of Computer and System Sciences (Special Issue: Cloud Computing", "citeRegEx": "Cesa.Bianchia and Lugosi.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchia and Lugosi.", "year": 2012}, {"title": "Combinatorial bandits revisited", "author": ["R. Combes", "M.S. Talebi Mazraeh Shahi", "A. Proutiere", "M. Lelarge"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Combes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Combes et al\\.", "year": 2015}, {"title": "How to beat the adaptive multi-armed bandit", "author": ["V. Dani", "T.P. Hayes"], "venue": "arXiv preprint,", "citeRegEx": "Dani and Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes.", "year": 2006}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "T.P. Hayes", "S. Kakade"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2007}, {"title": "Maximummatching and a polyhedronwith 0,1-vertices", "author": ["J. Edmonds"], "venue": "J. Res. Nat. Bur. Standards B,", "citeRegEx": "Edmonds.,? \\Q1965\\E", "shortCiteRegEx": "Edmonds.", "year": 1965}, {"title": "Hoeffding\u2019s inequality for supermartingales", "author": ["X. Fan", "I. Grama", "Q. Liu"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "Geometric algorithms and combinatorial optimization, volume 2 of Algorithms and Combinatorics", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": "Springer-Verlag, Berlin, second edition,", "citeRegEx": "Gr\u00f6tschel et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gr\u00f6tschel et al\\.", "year": 1993}, {"title": "An optimal algorithm for bandit convex optimization", "author": ["E. Hazan", "Y. Li"], "venue": "arXiv preprint,", "citeRegEx": "Hazan and Li.,? \\Q2016\\E", "shortCiteRegEx": "Hazan and Li.", "year": 2016}, {"title": "Volumetric spanners: an efficient exploration basis for learning", "author": ["E. Hazan", "Z. Karnin", "R.Meka"], "venue": "In JMLR:Workshop and Conference Proceedings,", "citeRegEx": "Hazan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2014}, {"title": "Revisiting Frank\u2013Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "InProceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "On the global linear convergence of Frank\u2013Wolfe optimization variants", "author": ["S. Lacoste-Julien", "M. Jaggi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lacoste.Julien and Jaggi.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.Julien and Jaggi.", "year": 2015}, {"title": "Efficient Bregman projections onto the permutahedron and related polytopes", "author": ["C.H. Lim", "S.J. Wright"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Lim and Wright.,? \\Q2016\\E", "shortCiteRegEx": "Lim and Wright.", "year": 2016}, {"title": "Tight bounds for approximate Carath\u00e9odory and beyond", "author": ["V.S. Mirrokni", "R.P. Leme", "A. Vladu", "S.C. wai Wong"], "venue": "arXiv preprint,", "citeRegEx": "Mirrokni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mirrokni et al\\.", "year": 2015}, {"title": "Efficient methods for large-scale convex optimization problems", "author": ["A. Nemirovski"], "venue": "Ekonomika i MatematicheskieMetody,", "citeRegEx": "Nemirovski.,? \\Q1979\\E", "shortCiteRegEx": "Nemirovski.", "year": 1979}, {"title": "Advances in convex optimization: Conic programming", "author": ["A. Nemirovski"], "venue": "In Proceedings of the International Congress of Mathematicians. EMS-European Mathematical Society Publishing House,", "citeRegEx": "Nemirovski.,? \\Q2006\\E", "shortCiteRegEx": "Nemirovski.", "year": 2006}, {"title": "Improved high-probability regret bounds for non-stochastic bandits", "author": ["G. Neu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "The matching polytope has exponential extension complexity", "author": ["T. Rothvo\u00df"], "venue": "Proceedings of STOC, pages 263\u2013272,", "citeRegEx": "Rothvo\u00df.,? \\Q2014\\E", "shortCiteRegEx": "Rothvo\u00df.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "For the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope.", "startOffset": 80, "endOffset": 101}, {"referenceID": 5, "context": "While this bound is weaker than the optimal O( \u221a T) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions.", "startOffset": 88, "endOffset": 111}, {"referenceID": 17, "context": "For algorithms with bandit feedback, exploration (occasionally playing random actions for learning) is a crucial feature, however it does not have to be explicit as recently shown in Neu [2015], where exploration is achieved via skewing loss estimators.", "startOffset": 183, "endOffset": 194}, {"referenceID": 2, "context": "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006].", "startOffset": 67, "endOffset": 86}, {"referenceID": 2, "context": "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n.", "startOffset": 67, "endOffset": 179}, {"referenceID": 2, "context": "The variant EXP3 for multi-armed bandit problems first appeared in Auer et al. [2002], however optimal high-probability regret bounds were first achieved in Dani and Hayes [2006]. The linear bandit setting is a generalization of the multi-armed bandit setting where, utilizing the linearity of losses, the goal is to improve the dependence on the number of actions in the regret bound, which might be exponential in the dimension n. At the same time linear losses come naturally into play when considering actions with a combinatorial structure, such as e.g., matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al.", "startOffset": 67, "endOffset": 631}, {"referenceID": 2, "context": ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion.", "startOffset": 74, "endOffset": 97}, {"referenceID": 2, "context": ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( \u221a T) expected regret, and in Bartlett et al.", "startOffset": 74, "endOffset": 241}, {"referenceID": 2, "context": ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( \u221a T) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( \u221a T) regret with high probability.", "startOffset": 74, "endOffset": 308}, {"referenceID": 2, "context": ", matchings, spanning trees, m-sets; see Cesa-Bianchia and Lugosi [2012], Audibert et al. [2013] for an extensive discussion. For the linear bandit setting, the EXP-variant ComBand (Combinatorial Bandit) from Cesa-Bianchia and Lugosi [2012] has optimal O( \u221a T) expected regret, and in Bartlett et al. [2008] the modified version GeometricHedge achieves O( \u221a T) regret with high probability. While these regret bounds practically do not depend on the number of actions, both maintain a distribution over the (possibly exponentially large) action set A, which is infeasible in general due to the large data size, even though ComBand is still efficient for many specific problems. Recently, a modification of the ComBand algorithm called CombEXP (see Algorithm 1) was derived in Combes et al. [2015], which achieves general computational efficiency by not maintaining a distribution of xt, but only the desired expectation x\u0302t of the distribution, and generating a new sparse approximate distribution at every round.", "startOffset": 74, "endOffset": 797}, {"referenceID": 12, "context": "The maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvo\u00df [2014].", "startOffset": 161, "endOffset": 176}, {"referenceID": 12, "context": "The maximal matching problem is a good example where the linear programming oracle approach is useful, as it has a polynomial time linear optimization algorithm Edmonds [1965], but no polynomial-size polyhedral description Rothvo\u00df [2014].", "startOffset": 161, "endOffset": 238}, {"referenceID": 4, "context": "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al.", "startOffset": 40, "endOffset": 72}, {"referenceID": 4, "context": "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and CombEXP appeared in Combes et al.", "startOffset": 105, "endOffset": 128}, {"referenceID": 4, "context": "The algorithm ComBand first appeared in Cesa-Bianchia and Lugosi [2012], while GeometricHedge comes from Bartlett et al. [2008], and CombEXP appeared in Combes et al. [2015]. Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al.", "startOffset": 105, "endOffset": 174}, {"referenceID": 0, "context": "Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret \u03a9(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.", "startOffset": 133, "endOffset": 157}, {"referenceID": 0, "context": "Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret \u03a9(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( \u221a T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).", "startOffset": 133, "endOffset": 462}, {"referenceID": 0, "context": "Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret \u03a9(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( \u221a T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]).", "startOffset": 133, "endOffset": 532}, {"referenceID": 0, "context": "Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret \u03a9(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( \u221a T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]). For convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al.", "startOffset": 133, "endOffset": 625}, {"referenceID": 0, "context": "Using interior point methods, an efficient algorithm with O( \u221a T) expected regret for linear bandit problems has been established in Abernethy et al. [2008]. For multiarmed bandit problems, the original version of EXP3 has high-probability regret \u03a9(T2/3) against some adaptive adversaries [Dani and Hayes, 2006, Theorem 1.2], however variants with optimal O( \u221a T) regret exists, e.g., using accountants to control the exploration rate (see Dani and Hayes [2006]), or via the recent EXP3-IX with implicit exploration (see Neu [2015]). For convex loss functions, optimal regret bounds have been obtained in Hazan and Li [2016] with running time being poly-exponential in the dimension, and in [Bubeck et al., 2016, Theorem 1] with polynomial running time provided the number of constraints of the underlying polytope is polynomial in the dimension. However the case of convex loss does not subsume the combinatorial/linear case, as with convex loss all inner points of the convex set are actions; with linear losses the actions are limited to the vertices of the underlying polytope in most cases. We refer the interested reader to the excellent survey of Bubeck and Cesa-Bianchi [2012] on bandit problems.", "startOffset": 133, "endOffset": 1185}, {"referenceID": 9, "context": "Our main contribution is a high-probability regret bound for CombEXP from Combes et al. [2015] for adaptive adversaries over actions coming from arbitrary polytopes P \u2286 Rn.", "startOffset": 74, "endOffset": 95}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation).", "startOffset": 114, "endOffset": 134}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI.", "startOffset": 114, "endOffset": 1003}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al.", "startOffset": 114, "endOffset": 1519}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A.", "startOffset": 114, "endOffset": 1705}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and \u03bb = 1/n using the scalar product on R n induced by the additional structure. John\u2019s ellipsoid can be approximately estimated with a worse lower bound \u03bb = 1/n3/2 by Gr\u00f6tschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006].", "startOffset": 114, "endOffset": 1948}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and \u03bb = 1/n using the scalar product on R n induced by the additional structure. John\u2019s ellipsoid can be approximately estimated with a worse lower bound \u03bb = 1/n3/2 by Gr\u00f6tschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, .", "startOffset": 114, "endOffset": 2021}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and \u03bb = 1/n using the scalar product on R n induced by the additional structure. John\u2019s ellipsoid can be approximately estimated with a worse lower bound \u03bb = 1/n3/2 by Gr\u00f6tschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [\u22121,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [\u2212C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P.", "startOffset": 114, "endOffset": 2542}, {"referenceID": 1, "context": "However, this interpretation is clearly incorrect against an adaptive adversary (the notion of policy regret from Arora et al. [2012] matches this interpretation). Nevertheless the above notion of regret proved to be useful in many areas. With bandit feedback the forecaster learns only the loss lt but not the actual loss vector Lt. An adaptive adversary learns the forecaster\u2019s action xt after round t, and can use it in later rounds to choose his actions. We make various standard assumptions to bound the regret. The most important one is that the per round loss is bounded, i.e., |L\u22bat x| \u2264 1 for all x \u2208 A. Under reasonably assumptions, this also implies that the set A of possible actions A is bounded and we assume that \u2016x\u20162 \u2264 B and \u2016x\u20161 \u2264 B1, with suitable positive numbers B, B1. Clearly, one can always choose B1 = nB, however we obtain finer bounds by keeping them separate. The bounds B1 and B also serve as a proxy for the sparsity of the actions. Following Cesa-Bianchia and Lugosi [2012] for ComBand, we shall use a fixed arbitrary distribution \u03bc on A for exploration, whose fitness for exploration is measured by a positive lower bound \u03bb on the smallest eigenvalue of its covariance matrix J: J := Ey\u223c\u03bc [yy] \u03bbI. Here and below we denote by M N that N \u2212 M is a positive semi-definite matrix for symmetric matrices M and N. When A is small then \u03bc is typically the uniform distribution over A. For large A, common choices are the uniform distribution on a barycentric spanner of A (see Hazan et al. [2014]), or the distribution on contact points of the maximal volume ellipsoid contained in the convex hull P of A arising from John\u2019s decomposition (John\u2019s exploration; see Dani et al. [2007]), transferred to A. In the latter two cases, J = I and \u03bb = 1/n using the scalar product on R n induced by the additional structure. John\u2019s ellipsoid can be approximately estimated with a worse lower bound \u03bb = 1/n3/2 by Gr\u00f6tschel et al. [1993], however a constant factor approximation is NP-hard by Nemirovski [2006]. Recall that a barycentric spanner is a linear basis v1, . . . , vn in P (the convex hull of A), such that every element of P is a linear combination of the vi with coefficients from [\u22121,+1]. The basis v1, . . . , vn is a C-approximate barycentric spanner for some C > 1 if every element of P is a linear combination of the vi with coefficients from [\u2212C,+C]. A Capproximate barycentric spanners can be efficiently computed by O(n2 ln n/ ln C) calls to a linear optimization oracle over P by Awerbuch and Kleinberg [2004], which actually computes a spanner consisting of vertices of P. In this paper we deliberately avoid using the scalar product induced by the structure to be able to directly use the bounds available in the original space of the problem. Fortunately, the uniform distribution on an approximate barycentric spanner has a close to optimal minimal eigenvalue even in the original space, see Lemma E.1, which allows us to preserve sparsity of the original space. As such we assume that we have access to an exploration distribution over actions with sparse support of size n, where n is the dimension of the vector space, from which we can efficiently sample. Note that for specific problems exploration distributions with better minimal eigenvalue can be explicitly given; we refer the interested reader to Cesa-Bianchia and Lugosi [2012] and follow-up work for a large set of such examples.", "startOffset": 114, "endOffset": 3376}, {"referenceID": 9, "context": "Except for the shifting vector a, these ideas already appeared in Combes et al. [2015]. The algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix.", "startOffset": 66, "endOffset": 87}, {"referenceID": 9, "context": "Except for the shifting vector a, these ideas already appeared in Combes et al. [2015]. The algorithmcontains four resource-consumingsteps: (1) projection (Line 10), (2) distributiongeneration (Line 3), (3) sampling from the distribution, and (4) computing the covariance matrix. All the other steps are fast, depending only polynomially on the dimension. Themajor factor for the running time of sampling from the distribution (3), and computing the covariancematrix (4) is the sparsity of the generated distribution, i.e., the number of possible outcomes. Sparse distributions (number of outcomes polynomial in the dimension) of sufficient accuracy can be efficiently generated by the decomposition algorithm fromMirrokni et al. [2015], which we summarize as Algorithm 5 in Section D for the reader\u2019s convenience.", "startOffset": 66, "endOffset": 737}, {"referenceID": 7, "context": "In this section we will show that ComBand of Cesa-Bianchia and Lugosi [2012] achieves a high-probability regret bound of O(T2/3) without any modifications.", "startOffset": 45, "endOffset": 77}, {"referenceID": 5, "context": "While this is worse than the optimal regret of O( \u221a T) obtained by GeometricHedge in Bartlett et al. [2008], it shows that already Algorithm 2, the vanilla version of ComBandwithout any correction terms suffices to achieve a high-probability regret bound.", "startOffset": 85, "endOffset": 108}, {"referenceID": 5, "context": "We would like to mention that our method could be immediately strengthened to provide an optimal high-probability regret of O( \u221a T) using the correction term of GeometricHedge (see Bartlett et al. [2008]) and the identity", "startOffset": 181, "endOffset": 204}], "year": 2017, "abstractText": "For the linear bandit problem, we extend the analysis of algorithm CombEXP from Combes et al. [2015] to the high-probability case against adaptive adversaries, allowing actions to come from an arbitrary polytope. We prove a high-probability regret of O(T2/3) for time horizon T. While this bound is weaker than the optimal O( \u221a T) bound achieved by GeometricHedge in Bartlett et al. [2008], CombEXP is computationally efficient, requiring only an efficient linear optimization oracle over the convex hull of the actions.", "creator": "LaTeX with hyperref package"}}}