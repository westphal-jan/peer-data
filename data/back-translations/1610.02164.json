{"id": "1610.02164", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Deep Reinforcement Learning From Raw Pixels in Doom", "abstract": "In this paper, we examine the challenges that arise in such complex environments and summarize current methods to approach them. We select a task within the Doom game that has not yet been tackled. The objective of the agent is to fight enemies in a 3D world that consists of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible strategies, but do not achieve high scores given the amount of training. We provide insights into the learned behavior that can serve as a valuable starting point for further research in the Doom field.", "histories": [["v1", "Fri, 7 Oct 2016 07:07:47 GMT  (1352kb,D)", "http://arxiv.org/abs/1610.02164v1", "Bachelor's thesis"]], "COMMENTS": "Bachelor's thesis", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["danijar hafner"], "accepted": false, "id": "1610.02164"}, "pdf": {"name": "1610.02164.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning From Raw Pixels in Doom", "authors": ["Danijar Hafner", "Tobias Friedrich"], "emails": [], "sections": [{"heading": null, "text": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTMA3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.\nContents"}, {"heading": "1 Introduction 1", "text": "1.1 The Reinforcement Learning Setting . . . . . . . . . . . . . . 1 1.2 Human-Like Artificial Intelligence . . . . . . . . . . . . . . . . 2 1.3 Relation to Supervised and Unsupervised Learning . . . . . . 2 1.4 Reinforcement Learning in Games . . . . . . . . . . . . . . . . 3"}, {"heading": "2 Reinforcement Learning Background 4", "text": "2.1 Agent and Environment . . . . . . . . . . . . . . . . . . . . . 4 2.2 Value-Based Methods . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.4 Exploration versus Exploitation . . . . . . . . . . . . . . . . . 7 2.5 Temporal Difference Learning . . . . . . . . . . . . . . . . . . 7 2.6 Eligibility Traces . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.7 Policy-Based Methods . . . . . . . . . . . . . . . . . . . . . . 10 2.8 Actor-Critic Methods . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "3 Challenges in Complex Environments 13", "text": "3.1 Large State Space . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 Partial Observability . . . . . . . . . . . . . . . . . . . . . . . 14 3.3 Stable Function Approximation . . . . . . . . . . . . . . . . . 15 3.4 Sparse and Delayed Rewards . . . . . . . . . . . . . . . . . . . 16 3.5 Efficient Exploration . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "4 Algorithms for Learning from Pixels 20", "text": "4.1 Deep Q-Network . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.2 Asynchronous Advantage Actor Critic . . . . . . . . . . . . . 21"}, {"heading": "5 Experiments in the Doom Domain 22", "text": "5.1 The Doom Domain . . . . . . . . . . . . . . . . . . . . . . . . 22 5.2 Applied Preprocessing . . . . . . . . . . . . . . . . . . . . . . 23 5.3 Methods to Stabilize Learning . . . . . . . . . . . . . . . . . . 24 5.4 Evaluation Methodology . . . . . . . . . . . . . . . . . . . . . 25 5.5 Characteristics of Learned Policies . . . . . . . . . . . . . . . . 25"}, {"heading": "6 Conclusions 28", "text": "Bibliography 28\ni\nChapter 1\nIntroduction\nUnderstanding human-like thinking and behavior is one of our biggest challenges. Scientists approach this problem from diverse disciplines, including psychology, philosophy, neuroscience, cognitive science, and computer science. The computer science community tends to model behavior farther away from the biological example. However, models are commonly evaluated on complex tasks, proving their effectivity.\nSpecifically, reinforcement learning (RL) and intelligent control, two communities within machine learning and, more generally, artificial intelligence, focus on finding strategies to behave in unknown environments. This general setting allows methods to be applied to financial trading, advertising, robotics, power plant optimization, aircraft design, and more [1, 30].\nThis thesis provides an overview of current state-of-the-art algorithms in RL and applies a selection of them to the Doom video game, a recent and challenging testbed in RL.\nThe structure of this work is as follows: We motivate and introduce the RL setting in Chapter 1, and explain the fundamentals of RL methods in Chapter 2. Next, we discuss challenges that arise in complex environments like Doom in Chapter 3, describe state-of-the-art algorithms in Chapter 4, and conduct experiments in Chapter 5. We close with a conclusion in Chapter 6."}, {"heading": "1.1 The Reinforcement Learning Setting", "text": "The RL setting defines an environment and an agent that interacts with it. The environment can be any problem that we aim to solve. For example, it could be a racing track with a car on it, an advertising network, or the stock market. Each environment reveals information to the agent, such as a camera image from the perspective of the car, the profile of a user that we want to display ads to, or the current stock prices.\nThe agent uses this information to interact with the environment. In\n1\nour examples, the agent might control the steering wheel and accelerator, choose ads to display, or buy and sell shares. Moreover, the agent receives a reward signal that depends on the outcome of its actions. The problem of RL is to learn and choose the best action sequences in an initially unknown environment. In the field of control theory, we also refer to this as a control problem because the agent tries to control the environment using the available actions."}, {"heading": "1.2 Human-Like Artificial Intelligence", "text": "RL has been used to model the behavior of humans and artificial agents. Doing so assumes that humans try to optimize a reward signal. This signal can be arbitrarily complex and could be learned both during lifetime and through evolution over the course of generations. For example, he neurotransmitter dopamine is known to play a critical role in motivation and is related to such a reward system in the human brain.\nModeling human behavior as an RL problem a with complex reward function is not completely agreed on, however. While any behavior can be modeled as following a reward function, simpler underlying principles than this could exist. These principles might be more valuable to model human behavior and build intelligent agents.\nMoreover, current RL algorithms can hardly be compared with human behavior. For example, a common approach in RL algorithms is called probability matching, where the agent tries to choose actions relative to their probabilities of reward. Humans tend to prefer the small chance of a high reward over the highest expected reward. For further details, please refer to Shteingart and Loewenstein [22]."}, {"heading": "1.3 Relation to Supervised and Unsupervised", "text": "Learning\nSupervised learning (SL) is the dominant framework in the field of machine learning. It is fueled by successes in domains like computer vision and natural language processing, and the recent break-through of deep neural networks. In SL, we learn from labeled examples that we assume are independent. The objective is either to classify unseen examples or to predict a scalar property of them.\nIn comparison to SL, RL is more general by defining sequential problems. While not always useful, we could model any SL problem as a one-step RL problem. Another connection between the two frameworks is that many RL algorithms use SL internally for function approximation (Sections 2.6.1\n2\nand 3.1). Unsupervised learning (UL) is an orthogonal framework to RL, where examples are unlabeled. Thus, unsupervised learning algorithms make sense from data by compression, reconstruction, prediction, or other unsupervised objectives. Especially in complex RL environments, we can employ methods from unsupervised learning to extract good representations to base decisions on (Section 3.1)."}, {"heading": "1.4 Reinforcement Learning in Games", "text": "The RL literature uses different testbeds to evaluate and compare its algorithms. Traditional work often focuses on simple tasks, such as balancing a pole on a cart in 2D. However, we want to build agents that can cope with the additional difficulties that arise in complex environments (Chapter 3).\nVideo games provide a convenient way to evaluate algorithms in complex environments, because their interface is clearly defined and many games define a score that we forward to the agent as the reward signal. Board games are also commonly addressed using RL approaches but are not considered in this work, because their rules are known in advance.\nMost notably, the Atari environment provided by ALE [3] consists of 57 low-resolution 2D games. The agent can learn by observing either screen pixels or the main memory used by game. 3D environments where the agent observes perspective pixel images include the driving simulator Torcs [35], several similar block-world games that we refer to as Minecraft domain, and the first-person shooter game Doom [9] (Section 5.1).\n3\nChapter 2\nReinforcement Learning Background\nThe field of reinforcement learning provides a general framework that models the interaction of an agent with an unknown environment. Over multiple time steps, the agent receives observations of the environment, responds with actions, and receives rewards. The actions affect the internal environment state.\nFor example, at each time step, the agent receives a pixel view of the Doom environment and chooses one of the available keyboard keys to press. The environment then advances the game by one time step. If the agent killed an enemy, we reward it with a positive signal of 1, and otherwise with 0. Then, the environment sends the next frame to the agent so that it can choose its next action."}, {"heading": "2.1 Agent and Environment", "text": "Formally, we define the environment as partially observable Markov decision process (POMDP), consisting of a state space S, an action space A, and an observation space X. Further, we define a transition function T : S \u00d7 A\u2192 Dist(S) that we also refer to as the dynamics, an observation function O : S \u2192 Dist(X), and a reward function R : S\u00d7A\u2192 Dist(R), where Dist(D) refers to the space of random variables overD. We denote T ass\u2032 = Pr(T (s, a) = s\u2032). The initial state is s0 \u2208 S and we model terminal states implicitly by having a recurrent transition probability of 1 and a reward of 0.\nWe use O(s) to model that the agent might not be able observe the entire state of the environment. When S = X and \u2200s \u2208 S : Pr(O(s) = s) = 1, the environment is fully observable, reducing the POMDP into a Markov decision process (MDP).\nThe agent defines a policy \u03c0 : P(X) \u2192 Dist(A) that describes how it chooses actions based on previous observations from the environment. By convention, we denote the variable of the current action given previous observations as \u03c0(xt) = \u03c0((x0, . . . , xt)) and the probability of choosing a particular\n4\naction as \u03c0(at|xt) = Pr(\u03c0(xt) = at). Initially, the agents provides an action a0 based on the observation x0 observed from O(s0).\nAt each discrete time step t \u2208 N+, the environment observes a state st from S(st\u22121, at\u22121). The agent then observes a reward rt from R(st\u22121, at\u22121) and an observation xt from O(st). The environment then observes at from the agent\u2019s policy \u03c0(xt).\nWe name a trajectory of all observations starting from t = 0 an episode, and the tuples (xt\u22121, at\u22121, rt, xt) that the agent observes, transitions. Further, we write E\u03c0 [\u00b7] as the expectation over the episodes observed under a given policy \u03c0.\nThe return Rt is a random variable describing the discounted rewards after t with discount factor \u03b3 \u2208 [0, 1). Without subscript, we assume t = 0. Note that, although possibly confusing, we stick to the common notation of using the letter R to denote both the reward function and the return from t = 0.\nRt = \u221e\u2211 i=1 \u03b3iR(st+i, at+i).\nNote that the return is finite as long as the rewards have finite bounds. When all episodes of the MDP are finite, we can also allow \u03b3 = 1, because the terminal states only add rewards of 0 to the return.\nThe agent\u2019s objective is to maximize the expected return E\u03c0 [R] under its policy. The solution to this depends on the choice of \u03b3: Values close to 1 encourage long-sighted actions while values close to 0 encourage short-sighted actions."}, {"heading": "2.2 Value-Based Methods", "text": "RL methods can roughly be separated into value-based and policy-based (Section 2.7) ones. RL theory often assumes fully-observable environments, so for now, we assume that the agent found a way to reconstruct st from the observations x0, ...xt. Of course, depending on O, this might not be completely possible. We discuss the challenge of partial observability later in Section 3.2.\nAn essential concept of value-based methods is the value function V \u03c0(st) = IE\u03c0 [Rt]. We use V \u2217 to denote the value function under an optimal policy. The value function has an interesting property, known as Bellman equation:\nV \u03c0(s) = IE\u03c0 [ R(s, a) + \u03b3\n\u2211 s\u2032\u2208S Tass\u2032V \u03c0(s\u2032)\n] . (2.1)\nHere and in the following sections, we assume s and a to be of the same time step, t, and s\u2032 and a\u2032 to be of the following time step t+ 1.\n5\nKnowing both, V \u2217 and the dynamics of the environment, allows us to act optimally. At each time step, we could greedily choose the action:\narg max a\u2208A \u2211 s\u2032\u2208S T ass\u2032V \u2217(s\u2032).\nWhile we could try to model the dynamics from observations, we focus on the prevalent approach of model-free reinforcement learning in this work. Similarly to V \u03c0(s), we now introduce the Q-function:\nQ\u03c0(s, a) = \u2211 s\u2032\u2208S T ass\u2032V \u03c0(s\u2032).\nIt is the the expected return under a policy from the Q-state (s, a), which means being in state s after having committed to action a. Analogously, Q\u2217 is the Q-function under an optimal policy. The Q-function has a similar property:\nQ\u03c0(s, a) = R(s, a) + \u03b3IE\u03c0 [Q\u03c0(s\u2032, a\u2032)] . (2.2)\nInterestingly, with Q\u2217, we do not need to know the dynamics of the environment in order to act optimally, because Q\u2217 includes the weighting by transition probabilities implicitly. The idea of the algorithms we introduce next is to approximate Q\u2217 and act greedily with respect to it."}, {"heading": "2.3 Policy Iteration", "text": "In order to approximate Q\u2217, the PolicyIteration algorithm starts from any \u03c00. In each iteration k, we perform two steps: During evaluation, we estimate Q\u03c0k from observed interactions, for example using a Monte-Carlo estimate. We denote this estimate with Q\u0302\u03c0k . During improvement, we update the policy to act greedily with respect to this estimate, constructing a new policy \u03c0k+1 with\n\u03c0k+1(a, s) =\n{ 1 if a = arg maxa\u2032\u2208A Q\u0302 \u03c0k(s, a\u2032),\n0 else. .\nWe can break ties in the arg max arbitrarily. When Q\u0302\u03c0k = Q\u03c0k , it is easy to see that this step is a monotonic improvement, because \u03c0k+1 is a valid policy and \u2200s \u2208 S, a \u2208 A : max a\u2032 \u2208 AQ\u03c0(s, a\u2032) \u2265 Q\u03c0(s, a). It is even strictly monotonic, unless \u03c0k is already optimal or did not visit all Q-states.\nIn the case of an estimation error, the update may not be a monotonic improvement, but the algorithm is known to converge to Q\u2217 as the number of visits of each Q-state approaches infinity [25].\n6"}, {"heading": "2.4 Exploration versus Exploitation", "text": "In PolicyIteration, learning may stagnate early if we do not visit all Qstates over and over again. In particular, we might never visit some states just by following \u03c0k. The problem of visiting new states is called exploration in RL, as opposed to exploitation, which means acting greedily with respect to our current Q-value estimate.\nThere is a fundamental tradeoff between exploration and exploitation in RL. At any point, we might either choose to follow the policy that we current think is best, or perform actions that we think are worse with the potential of discovering better action sequences. In complex environments, exploration is one of the biggest challenges, and we discuss advanced approaches in Section 3.5.\nThe dominant approach to exploration is the straightforward EpsilonGreedy strategy, where the agent picks a random action with probability \u03b5 \u2208 [0, 1], and the action according to its normal policy otherwise. We decay \u03b5 exponentially over the course of training to guarantee convergence."}, {"heading": "2.5 Temporal Difference Learning", "text": "While PolicyIteration combined with the EpsilonGreedy exploration strategy finds the optimal policy eventually, the Monte-Carlo estimates have a comparatively high variance so that we need to observe each Q-state often in order to converge to the optimal policy. We can improve the data efficiency by using the idea of bootstrapping, where we estimate the Q-values from from a single transition:\nQ\u0302\u03c0(st, at) =\n{ rt+1 if st is terminal,\nrt+1 + \u03b3Q\u0302 \u03c0(st+1, at+1) else.\n(2.3)\nThe approximation is of considerably less variance but introduces a bias because our initial approximated Q-values might be arbitrarily wrong. In practice, bootstrapping is very common since Monte-Carlo estimates are not tractable.\nEquation 2.3 allows us to update the estimate Q\u0302\u03c0 after each time step rather than after each episode, resulting in the online-algorithm SARSA [25]. We use a small learning rate \u03b1 \u2208 R to update a running estimate of Q\u03c0(st, at),\n7\nwhere \u03b4t is known as the temporal difference error :\n\u03b4t = { rt+1 \u2212Q\u0302\u03c0(st, at) if st is terminal, rt+1 + \u03b3Q\u0302 \u03c0(st+1, at+1) \u2212Q\u0302\u03c0(st, at) else,\nand Q\u0302\u03c0(st, at)\u2190 Q\u0302\u03c0(st, at) + \u03b1\u03b4t.\n(2.4)\nSARSA approximates expected returns using the current approximation of the Q-value under its own policy. A common modification to this is known as Q-Learning, as proposed by Watkins and Dayan [31]. Here, we bootstrap using what we think is the best action rather than the action observed under our policy. Therefore, we directly approximate Q\u2217, denoting the current approximating Q\u0302:\n\u03b4t = { rt+1 \u2212Q\u0302(st, at) if st is terminal, rt+1 + \u03b3maxa\u2032\u2208A Q\u0302(st+1, a \u2032)) \u2212Q\u0302(st, at) else,\nand Q\u0302(st, at)\u2190 Q\u0302(st, at) + \u03b1\u03b4t.\n(2.5)\nThe Q-Learning algorithm might be one of the more important breakthroughs in RL [25]. It allows us to learn about the optimal way to behave by observing transitions of an arbitrary policy. The policy still effects which Q-states we visit and update, but is only needed for exploration.\nQ-Learning converges to Q\u2217 given continued exploration [29]. This requirement is minimal: Any optimal method needs to continuously obtain information about the MDP for convergence."}, {"heading": "2.6 Eligibility Traces", "text": "One problem, temporal difference methods like SARSA and Q-Learning is that updates of the approximated Q-function only affect the Q-values of predecessor states directly. With long time gaps between good actions and the corresponding rewards, many updates of the Q-function may be required for the rewards to propagate backward to the good actions.\nThis is an instance of the fundamental credit assignment problem in machine learning. When the agent receives a positive reward, it needs to figure out what state and action led to the reward so that we can make this one more likely. The TD(\u03bb) algorithm provides an answer to this by assigning eligibilities to each Q-state.\nUpon encountering a reward rt, we apply the temporal difference update rule for each Q-state (s, a) with an eligibility trace et(s, a) \u2265 0. The update\n8\nof each Q-state uses the received reward scaled by the current eligibility of the state:\nQ\u0302(s, a)\u2190 Q\u0302(s, a) + \u03b1\u03b4tet(s, a). (2.6)\nA common way to assign eligibilities is based on the duration between visiting the Q-states (s0, a0), . . . , (st, at) and receiving the reward rt+1. The state in which we receive the reward has an eligibility of 1 and the eligibility of previous states decays exponentially over time by a factor \u03bb \u2208 [0, 1]: et(st\u2212k) = (\u03b3\u03bb)\nk. We can implement this by adjusting the eligibilities at each time step:\net+1(s, a) =\n{ 1 if (s, a) = (st, at),\n\u03b3\u03bbet(s, a) else.\nThis way of assigning eligibilities is known as replacing traces because we reset the eligibility of an encountered state to 1. Alternatives include accumulating traces and Dutch traces, shown in Figure 2.1. In both cases, we keep the existing eligibility value of the visited Q-state and increment it by 1. For Dutch traces, we additionally scale down the result of this by a factor between 0 and 1.\nUsing the SARSA update rule in Equation 2.6 yields an algorithm known as TD(\u03bb), while using the Q-Learning update rule yields Q(\u03bb).\nEligibility traces bridge the gap between Monte-Carlo methods and temporal difference methods: With \u03bb = 0, we only consider the current transition, and with \u03bb = 1, we consider the entire episode."}, {"heading": "2.6.1 Function Approximation", "text": "Until now, we did not specify how to represent Q\u0302. While we could use a lookup table, in practice, we usually employ a function approximator to address large state spaces (Section 3.1).\nIn literature, gradient-based function approximation is applied commonly. Using a derivable function approximator like linear regression or neural net-\n9\nworks that start from randomly initialized parameters \u03b80, we can perform the gradient-based update rule:\n\u03b8t+1 = \u03b8t + \u03b1\u03b4t\u2207\u03b8Q\u0302(st, at). (2.7)\nHere, \u03b4t is the scalar offset of the new estimate from the previous one given by the temporal difference error of an algorithm like SARSA or QLearning, and \u03b1 is a small learning rate. Background in function approximation using neural networks is out of the scope of this work."}, {"heading": "2.7 Policy-Based Methods", "text": "In contrast to value-based methods, policy-based methods parameterize the policy directly. Depending on the problem, finding a good policy can be easier than approximating the Q-function first. Using a parameterized function approximator (Section 2.6.1), we aim to find a good set of parameters \u03b8 such that actions sampled from the policy a \u223c \u03c0\u03b8(at, st) maximize the reward in a POMDP.\nSeveral methods for searching the space of possible policy parameters have been explored, including random search, evolutionary search , and gradientbased search [6]. In the following, we focus on gradient-based methods.\nThe PolicyGradient algorithm is the most basic gradient-based method for policy search. The idea is to use the reward signals as objective and tweak the parameters \u03b8t using gradient ascent. For this to work, we are interested in the gradient of the expected reward under the policy with respect to its parameters:\n\u2207\u03b8IE\u03c0\u03b8 [Rt] = \u2207\u03b8 \u2211 s\u2208S d\u03c0\u03b8(s) \u2211 a\u2208A \u03c0\u03b8(a|s)Rt\nwhere d\u03c0\u03b8(s) denotes the probability of being in state s when following \u03c0\u03b8. Of course, we cannot find that gradient analytically because the agent interacts with an unknown, usually non-differentiable, environment. However, it is possible to obtain an estimate of the gradient using the score-function\n10\ngradient estimator [26]: \u2207\u03b8IE\u03c0\u03b8 [Rt] = \u2207\u03b8 \u2211 st\u2208S d\u03c0\u03b8 \u2211 at\u2208A \u03c0\u03b8(a|s)Rt\n= \u2211 st\u2208S d\u03c0\u03b8 \u2211 at\u2208A \u2207\u03b8\u03c0\u03b8(at|st)Rt\n= \u2211 st\u2208S d\u03c0\u03b8 \u2211 at\u2208A \u03c0\u03b8(at|st) \u2207\u03b8\u03c0\u03b8(at|st) \u03c0\u03b8(at|st) Rt\n= \u2211 st\u2208S d\u03c0\u03b8 \u2211 at\u2208A \u03c0\u03b8(at|st)\u2207\u03b8 ln(\u03c0\u03b8(at|st))Rt = IE\u03c0\u03b8 [Rt\u2207\u03b8 ln \u03c0\u03b8(at|st)] ,\n(2.8)\nwhere we decompose the expectation into a weighted sum following the definition of the expectation and move the gradient inside the sum. We then both multiply and divide the term inside the sum by \u03c0\u03b8(a|s), apply the chain rule \u2207\u03b8 ln(f(\u03b8)) = 1f(\u03b8)\u2207\u03b8f(\u03b8), and bring the result back into the form of an expectation.\nAs shown in Equation 2.8, we only require the gradient of our policy. Using a derivable function approximator, we can then sample trajectories from the environment to obtain a Monte-Carlo estimate both over states and Equation 2.8. This yields the Reinforce algorithm proposed by Williams [33].\nAs for value-based methods, the Monte-Carlo estimate is comparatively slow because we only update our estimates at the end of each episode. To improve on that, we can combine this approach with eligibility traces (Section 2.6). We can also reduce the variance of the Monte-Carlo estimates as described in the next section."}, {"heading": "2.8 Actor-Critic Methods", "text": "In the previous section, we trained the policy along the gradient of the expected reward, that is equivalent to IE\u03c0\u03b8 [Rt\u2207\u03b8 ln\u03c0\u03b8(a|s)]. When we sample transitions from the environment to estimate this expectation, the rewards can have a high variance. Thus, Reinforce requires many transitions to obtain a sufficient estimate.\nActor-critic methods improve on the data efficiency of this algorithm by subtracting a baseline B(s) from the reward that reduces the variance of the expectation. When B(s) is an approximated function, we call its approximator critic and the approximator of the policy function actor. To not introduce bias to the gradient of the reward, the gradient of the baseline with respect to the policy must be 0 [33]:\n11\nIE\u03c0\u03b8 [\u2207\u03b8 ln \u03c0\u03b8(a|s)B(s)] = \u2211 s\u2208S d\u03c0\u03b8(s) \u2211 a\u2208A \u2207\u03b8\u03c0\u03b8(a|s)B(s)\n= \u2211 s\u2208S d\u03c0\u03b8(s)B(s)\u2207\u03b8 \u2211 a\u2208A \u03c0\u03b8(a|s)\n= 0.\nA common choice is B(s) \u2248 V \u03c0(s). In this case, we train the policy by the gradient IE\u03c0\u03b8 [\u2207\u03b8 ln\u03c0\u03b8(a|s)(Rt \u2212 Vt(st))]. Here we can train the critic to approximate V (st) using familiar temporal difference methods (Section 2.5). This algorithm is known as AdvantageActorCritic as Rt \u2212 Vt(st) estimates the advantage function Q(st, at) \u2212 V (st) that describes how good an action is compared to how good the average action is in the current state.\n12\nChapter 3\nChallenges in Complex Environments\nTraditional benchmark problems in RL include tasks like Mountain Car and Cart Pole. In those tasks, the observation and action spaces are small, and the dynamics can be described by simple formulas. Moreover, these environments are usually fully observed so that the agent could reconstruct the system dynamics perfectly, given enough transitions.\nIn contrast, 3D environments like Doom have complex underlying dynamics and large state spaces that the agent can only partially observe. Therefore, we need more advanced methods to learn successful policies (Chapter 4). We now discuss challenges that arise in complex environments and methods to approach them."}, {"heading": "3.1 Large State Space", "text": "Doom is a 3D environment where agents observe perspective 2D projections from their position in the world as pixel matrices. Having such a large state space makes tabular versions of RL algorithms intractable. We can adjust those algorithms to the use of function approximators and make them tractable. However, the agent receives tens of thousands of pixels every time step. This is a computational challenge even in the case of function approximation.\nDownsampling input images only provides a partial solution to this since we must preserve information necessary for effective control. We would like our agents to find small representations of its observations that are helpful for action selection. Therefore, abstraction from individual pixels is necessary. Convolutional neural networks (CNNs) provide a computationally effective way to learn such abstractions [13].\nIn comparison to normal fully-connected neural networks, CNNs consist of convolutional layers that learn multiple filters. Each filter is shifted over the whole input or previous layer to produce a feature map. Feature maps can optionally be followed by pooling layers that downsample each feature\n13\nmap individually, using the max or mean of neighboring pixels. Applying the filters across the whole input or previous layer means that we must only learn a small filter kernel. The number of parameters of this kernel does not depend on the input size. This allows for more efficient computation and faster learning compared to fully-connected networks where a layers adds an amount of parameters that is quadratic in the number of the layer size.\nMoreover, CNNs exploit the fact that nearby pixels in the observed images are correlated. For example, walls and other surfaces result in evenly textured regions. Pooling layers help to reduce the dimensionality while keeping information represented small details, when important. This is because each filter learns a high-resolution feature and is downsampled individually."}, {"heading": "3.2 Partial Observability", "text": "In complex environments, observations do no fully reveal the state of the environment. The perspective view that the agent observes in the Doom environment contains reduced information in multiple ways:\n\u2022 The agent can only look in forward direction and its field of view only includes a fraction of the whole environment.\n\u2022 Obstacles like walls hide the parts of the scene behind them.\n\u2022 The perspective projection loses information about the 3D geometry of the scene. Reconstructing the 3D geometry and thus positions of objects is not trivial and might not even have a unique solution.\n\u2022 Many 3D environments include basic physics simulations. While the agent can see the objects in its view, the pixels do not directly contain information about velocities. It might be possible to infer them, however.\n\u2022 Several other temporal factors are not represented in the current image, like whether an item or enemy in another room still exists.\nTo learn a good policy, the agent has to detect spatial and temporal correlations in its input. For example, it would be beneficial to know the position of objects and the agent itself in the 3D environment. Knowing the positions and velocities of enemies would certainly help aiming.\nUsing hierarchical function approximators like neural networks allows learning high-level representations like the existence of an enemy in the field of view. For high-level representations, neural networks need more than one layer because a layer can only learn linear combinations of the input or previous layer. Complex features might be impossible to construct from a\n14\nlinear combination of input pixels. Zeiler and Fergus [36] visualize the layers of CNNs and show that they actually learn more abstract features in each layer.\nTo address the temporal incompleteness of the observations, we can use frame skipping, where we collect multiple images and show this stack as one observation to the agent. The agent then decides for an action we repeat while collecting the next stack of inputs [13].\nIt is also common to use recurrent neural networks (RNNs) [8, 14] to address the problem of partial observability. Neurons in these architectures have self-connections that allow activations to span multiple time steps. The output of an RNN is a function of the current input and the previous state of the network itself. In particular, a variant called Long Short-Term Memory (LSTM) and its variations like Gated Recurrent Unit (GRU) have proven to be effective in a wide range of sequential problems [12].\nCombining CNNs and LSTMs, Oh et al. [16] were able to learn useful representations from videos, allowing them to predict up to 100 observations in the Atari domain (Figure 3.1).\nA recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17]. These approximators consist of an RNN that can write to and read from an external memory. This allows the network to clearly carry information over long durations."}, {"heading": "3.3 Stable Function Approximation", "text": "Various forms of neural networks have been successfully applied to supervised and unsupervised learning problems. In those applications, the dataset is often known prior to training and can be decorrelated and many machine learning algorithms expect independent and identically distributed data. This is problematic in the RL setting, where we want to improve the policy while collecting observations sequentially. The observations can be highly correlated due to the sequential nature underlying the MDP.\nTo decorrelate data, the agent can use a replay memory as introduced by Mnih et al. [13] to store previously encountered transitions. At each time\n15\nstep, the agent then samples a random batch of transitions from this memory and uses this for training. To initialize the memory, one can run a random policy before the training phase. Note that the transitions are still biased by the start distribution of the MDP.\nThe mentioned work first managed to learn to play several 2D Atari games [3] without the use of hand-crafted features. It has also been applied to simple tasks in the Minecraft [2] and Doom [9] domains. On the other hand, the replay memory is memory intense and can be seen as unsatisfyingly far from the way humans process information.\nIn addition, Mnih et al. [13] used the idea of a target network. When training approximators by bootstrapping (Section 2.5), the targets are estimated by the same function approximator. Thus, after each training step, the target computation changes which can prevent convergence as the approximator is trained toward a moving target. We can keep a previous version of the approximator to obtain the targets. After every few time steps, we update the target network by the current version of the training approximator.\nRecently, Mnih et al. [14] proposed an alternative to using replay memories that involves multiple versions of the agent simultaneously interacting with copies of the environment. The agents apply gradient updates to a shared set of parameters. Collecting data from multiple environments at the same time sufficiently decorrelated data to learn better policies than replay memory and target network were able to find."}, {"heading": "3.4 Sparse and Delayed Rewards", "text": "The problem of non-informative feedback is not tied to 3D environments in particular, yet constitutes a significant challenge. Sometimes, we can help and guide the agent by rewarding all actions with positive or negative rewards. But in many real-world tasks, the agent might receive zero rewards most of the time and only see a binary feedback at the end of each episode.\nRare feedback is common when hand-crafting a more informative reward signal is not straightforward, or when we do not want to bias the solutions that the agent might find. In these environments, the agent has to assign credit among all its previous actions when finally receiving a reward.\nWe can apply eligibility traces (Section 2.6) to function approximation by keeping track of all transitions since the start of the episode. When the agent receives a reward, it incorporates updates for all stored states relative to the decayed reward.\nAnother way to address sparse and delayed rewards is to employ methods of temporal abstraction as explained in Section 3.5.4\n16"}, {"heading": "3.5 Efficient Exploration", "text": "Algorithms like Q-Learning (Section 2.5) are optimal in the tabular case under the assumption that each state will be visited over and over again, eventually [29]. In complex environments, it is impossible to visit each of the many states. Since we use function approximation, the agent can already generalize between similar states. There are several paradigms to the problem of effectively finding interesting and unknown experiences that help improve the policy."}, {"heading": "3.5.1 Random Exploration", "text": "We can use a simple EpsilonGreedy strategy for exploration, where at each time step, with a probability of \u03b5 \u2208 (0, 1], we choose a completely random action, and act according to our policy otherwise. When we start at \u03b5 = 1 and exponentially decay \u03b5 over time, this strategy, in the limit, guarantees to visit each state over and over again.\nIn simple environments, EpsilonGreedy might actually visit each state often enough to derive the optimal policy. But in complex 3D environments, we do not even visit each state once in a reasonable time. Visiting novel states would be important to discover better policies.\nOne reason that random exploration still works reasonably well in complex environments [13, 14] can partly be attributed to function approximation. When the function approximator generalizes over similar states, visiting one state also improves the estimate of similar states. However, more elaborate methods exist and can result in better results."}, {"heading": "3.5.2 Optimism in the Face of Uncertainty", "text": "A simple paradigm to encourage exploration in value-based algorithms (Section 2.2) is to optimistically initialize the estimated Q-values. We can do this either by pre-training the function approximator or by adding a positive bias to its outputs. Whenever the agent visits an unknown state, it will correct its Q-value downward. Less visited states still have high values assigned so that the agent tends to visiting them when facing the choice.\nThe Bayesian approach is to count the visits of each state to compute the uncertainty of its value estimate [24]. Combined with Q-learning, this converges to the true Q-function, given enough random samples [11]. Unfortunately, it is hard to obtain truly random samples for a general MDP because of its sequential nature. Another problem of counting is that the state space may be large or continuous and we want to generalize between similar states.\n17\nBellemare et al. [4] recently suggested a sequential density estimation model to derive pseudo-counts for each state in a non-tabular setting. Their method significantly outperforms existing methods on some games of the Atari domain, where exploration is challenging.\nWe can also use uncertainty-based exploration with policy gradient methods (Section 2.7). While we usually do not estimate the Q-function here, we can add the entropy of the policy as a regularization term to the its gradient [34] with a small factor \u03b2 \u2208 R specifying the amount of regularization:\nIE\u03c0\u03b8 [R(s, a)\u2207\u03b8 log \u03c0\u03b8(a|s)] + \u03b2\u2207\u03b8H ( \u03c0\u03b8(a|s) ) ,with\nH ( \u03c0\u03b8(a|s) ) = \u2212 \u2211 a\u2032\u2208A \u03c0\u03b8(a \u2032|s) log \u03c0\u03b8(a\u2032|s). (3.1)\nThis encourages a uniformly distributed policy until the policy gradient updates outweigh the regularization. The method was successfully applied to Atari and a visual 3D domain Labyrinth by Mnih et al. [14]."}, {"heading": "3.5.3 Curiosity and Intrinsic Motivation", "text": "A related paradigm is to explore in order to attain knowledge about the environment in the absence of external rewards. This usually is a modelbased approach that directly encourages novel states based on two function approximators.\nThe first approximator, called model, tries to predict the next observation and thus approximates the transition function of the MDP. The second approximator, called controller, performs control. Its objective is both to maximize expected returns and to cause the highest reduction in prediction error of the model [19]. It therefore tries to provide new observations to the model that are novel but learnable, inspired by the way humans are bored by both known knowledge and knowledge they cannot understand [20].\nThe model-controller architecture has been extended in multiple ways. Ngo et al. [15] combined it with planning to escape known areas of the state space more effectively. Schmidhuber [21] recently proposed shared neurons between the model and controller networks in a way that allows the controller to arbitrarily exploit the model for control."}, {"heading": "3.5.4 Temporal Abstraction", "text": "While most RL algorithms directly produce an action at each time step, humans plan actions on a slower time scale. Adapting this property might be necessary for human level control in complex environments. Most of the mentioned exploration methods (Section 3.5) determine the next exploration action at each time step. Temporally extended actions could be beneficial to both exploration and exploitation [18].\n18\nThe most common framework for temporal abstraction in the RL literature is the options framework proposed by Sutton et al. [27]. The idea is to learn multiple low-level policies, named options, that interact with the world. A high-level policy observes the same inputs but has the options as actions to choose from. When the high-level policy decides for an option, the corresponding low-level policy is executed for a fixed or random number of time steps.\nWhile there are multiple ways to obtain options, two recent approaches were shown to work in complex environments. Krishnamurthy et al. [10] used spectral clustering to group states with cluster centers representing options. Instead of learning individual low-level policies, the agent greedily follows a distance measure between low-level states and cluster centers that is given by the clustering algorithm.\nAlso building on the options framework, Tessler et al. [28] trained multiple CNNs on simple tasks in the Minecraft domain. These so-called deep skill networks are added in addition to the low-level actions for the high-level policy to choose from. The authors report promising results on a navigation task.\nA limitation of the options framework is its single level of hierarchy. More realistically, multi-level hierarchical algorithms are mainly explored in the fields of cognitive science and computational neuroscience. Rasmussen and Eliasmith [18] propose one such architecture and show that it is able to learn simple visual tasks.\n19\nChapter 4\nAlgorithms for Learning from Pixels\nWe explained the background of RL methods in Chapter 2 and described the challenges that arise in complex environments in Chapter 3, where we already outlined the intuition behind some of the current algorithms. In this section, we build on this and explain two state-of-the-art algorithms that have successfully been applied to 3D domains."}, {"heading": "4.1 Deep Q-Network", "text": "The currently most common algorithm for learning in high-dimension state spaces is the Deep Q-Network (DQN) algorithm suggested by Mnih et al. [13]. It is based on traditional Q-Learning algorithm with function approximation and EpsilonGreedy exploration. In its initial form, DQN does not use eligibility traces.\nThe algorithm uses a two-layer CNN, followed by a linear fully-connected layer to approximate the Q-function. Instead of taking both state and action as input, it outputs the approximated Q-values for all actions a \u2208 A simultaneously, taking only a state as input.\nTo decorrelate transitions that the agent collects during play, it uses a large replay memory. After each time step, we select a batch of transitions from the replay memory randomly. We use the temporal difference Q-Learning rule to update the neural network. We initialize \u03b5 = 1 and start annealing it after the replay memory is filled.\nIn order to further stabilize training, DQN uses a target network to compute the temporal difference targets. We copy the weights of the primary network to the target network at initialization and after every few time steps. As initially proposed, the algorithms synchronizes the target network every frame, so that the targets are computed using the network weights at the last time step.\nDQN has been successfully applied to two tasks within the Doom domain by Kempka et al. [9] who introduced this domain. In the more challenging\n20\nHealth Gathering task, where the agent must collect health items in multiple open rooms, they used three convolutional layers, followed by max-pooling layers and a fully-connected layer. With a small learning rate of \u03b1 = 0.00001, a replay memory of 10000 entries, and one million training steps, the agent learned a successful policy.\nBarron et al. [2] applied DQN to two tasks in the Minecraft domain: Collecting as many blocks of a certain color as possible, and navigating forward on a pathway without falling. In both tasks, DQN learned successful policies. Depending on the width of the pathway, a larger convolutional network and several days of training were needed to learn successful policies."}, {"heading": "4.2 Asynchronous Advantage Actor Critic", "text": "A3C by Mnih et al. [14] is an actor-critic method that is considerable more memory-efficient than DQN, because it does not require the use of a replay memory. Instead, transitions are decorrelated by training in multiple versions of the same environment in parallel and asynchronously updating a shared actor-critic model. Entropy regularization (Section 3.5.2) is employed to encourage exploration.\nEach of the originally up to 16 threads manages a copy of the model, and interacts with an instance of the environment. Each threads collects a few transitions before computing eligibility returns (Section 2.6) and computing gradients according to the AAC algorithm (Section 2.8) based on its current copy of the model. It then applies this gradient to the shared model and updates its copy to the current version of the shared model.\nOne version of A3C uses the same network architecture as DQN, except for using a softmax activation function in the last layer to model the policy rather than. The critic model shares all convolutional layers and only adds a linear fully-connected layer of size one that is trained to estimate the value function. The authors also proposed a version named LSTM-A3C that adds one LSTM layer between the convolutional layers and the output layers to approach the problem of partial observability.\nIn addition to Atari and a continuous control domain, A3C was evaluated on a new Labyrinth domain, a 3D maze where the goal is to find and collect as many apples as possible. LSTM-A3C was able to learn a successful policy for this task that avoids to walk into walls and turns around when facing dead ends. Given the recent proposal of the algorithm, it is not likely that the algorithm was applied to other 3D domains yet.\n21\nChapter 5\nExperiments in the Doom Domain\nWe now introduce the Doom domain in detail, focusing on the task used in the experiments. We explain methods that were necessary to train the algorithms (Chapter 4) in a stable manner. While the agents do not reach particularly high scores on average, they learn a range of interesting behaviors that we examine in Section 5.5."}, {"heading": "5.1 The Doom Domain", "text": "The Doom domain [9] provides RL tasks simulated by the game mechanics of the first-person shooter Doom. This 3D game features different kinds of enemies, items, and weapons. A level editor can be used to create custom tasks. We use the DeathMatch task defined in the Gym collection [5]. We first describe the Doom domain in general.\nIn Doom, the agent observes image frames that are perspective 2D projections of the world from the agent\u2019s position. A frame also contains user interface elements at the bottom, including the amount of ammunition of the agent\u2019s selected weapon, the remaining health points of the agent, and additional game-specific information. We do not extract this information explicitly.\nEach frame is represented as a tensor of dimensions screen width, screen height, and color channel. We can choose the width and height from a set of available screen resolutions. The color channel represents the RGB values of the pixels and thus always has a size of three.\nThe actions are arbitrary combinations of the 43 available user inputs to the game. Most actions are binary values to represent whether a given key on the keyboard is in pressed or released position. Some actions represent mouse movement and take on values in the range [\u221210, 10] with 0 meaning no movement.\nThe available actions perform commands in the game, such as attack, jump, crouch, reload, run, move left, look up, select next weapon, and more.\n22\nFor a detailed list, please refer to the whitepaper by Kempka et al. [9]. We focus on the DeathMatch task, where the agent faces multiple kinds of enemies that attack it. The agent must shoot an enemy multiple times, depending on the kind of enemy, to kill it an receive a reward of +1. Being hit by enemy attacks reduces the agent\u2019s health points, eventually causing the end of the episode. The agent does not receive any reward the end of the episode. The episode also ends after exceeding 104 time steps.\nAs shown in Figure 5.1, the world consists of one large hall, where enemies appear regularly, and four small rooms to the sides. Two of the small rooms contain items for the agent to restore health points. The other two rooms contain ammunition and stronger weapons than the pistol that the agent begins with. To collect items, ammunition, or weapons, the agent must walk through them. The agent starts at a random position in the hall, facing a random direction."}, {"heading": "5.2 Applied Preprocessing", "text": "To reduce computation requirements, we choose the smallest available screen resolution of 160 \u00d7 120 pixels. We further down-sample the observations to 80 \u00d7 60 pixels, and average over the color channels to produce a grayscale image. We experiment with delta frames, where we pass the difference between the current and the last observation to the agent. Both variants are visualized in Figure 5.2.\nWe further employ history frames as originally used by DQN in the Atari domain, and further explored by Kempka et al. [9]. Namely, we collect\n23\nmultiple frames, stack them, and show them to the agent as one observation. We then repeat the agent\u2019s action choice over the next time steps while collecting a new stack of images. We perform a random action during the first stack of an episode. History frames have multiple benefits: They include temporal information, allow more efficient data processing, and cause actions to be extended for multiple time steps, resulting in more smooth behavior.\nHistory frames extend the dimensionality of the observations. We use the grayscale images to compensate for this and keep computation requirements manageable. While color information could be beneficial to the agent, we expect the temporal information contained in history frames to be more valuable, especially to the state-less DQN algorithm. Further experiments would be needed to test this hypothesis.\nWe remove unnecessary actions from the action space to speed up learning, leaving only the 7 actions attack, move left, move right, move forward, move backward, turn left, and turn right. Note that we do not include actions to rotate the view upward or downward, so that the agent does not have to learn to look upright.\nFinally, we apply normalization: We scale the observed pixels into the range [0, 1] and normalize rewards to (\u22121, 0,+1) using rt \u2190 sgn(rt) [13]. Further discussion of normalization can be found in the next section."}, {"heading": "5.3 Methods to Stabilize Learning", "text": "Both DQN and LSTM-A3C are sensitive to the choice of hyper parameters [14, 23]. Because training times lie in the order of hours or days, it is not tractable to perform an excessive hyper parameter search for most researcher. We can normalize observations and rewards as described in the previous section to make it more likely that hyper parameters can be transferred between tasks and domains.\nIt was found to be essential to clip gradients of the networks in both\n24\nDQN and LSTM-A3C. Without this step, the approximators diverges in the early phase of learning. Specifically, we set each element x of the gradient to x\u2190 max {\u221210,min {x,+10}}. The exact threshold to clip at did not seem to have a large impact on training stability or results.\nWe decay the learning rate linearly over the course of training to guarantee convergence, as done by Mnih et al. [13, 14], but not by Kempka et al. [9], Barron et al. [2]."}, {"heading": "5.4 Evaluation Methodology", "text": "DQN uses a replay memory of the last 104 transitions and samples batches of size 64 from it, following the choices by Kempka et al. [9]. We anneal \u03b5 from 1.0 to 0.1 over the course of 2 \u2217 106 time steps, which is one third of the training duration. We start both training and annealing \u03b5 after the first 104 observations. This is equivalent to initializing the replay memory from transitions collected by a random policy.\nFor LSTM-A3C, we use 16 learning threads that apply their accumulated gradients every 5 observations using shared optimizer statistics, a entropy regularization factor of \u03b2 = 0.01. Further, we scale the loss of the critic by 0.5 [14].\nBoth algorithms operate on frames of history 6 at a time and use RMSProp with a decay parameter of 0.99 for optimization [14]. The learning rate starts at \u03b1 = 2 \u2217 10\u22125, similar to Kempka et al. [9] who use a fixed learning rate of 10\u22125 in the Doom domain.\nWe train both algorithms for 20 epochs, where one epoch corresponds to 5 \u2217 104 observations for DQN and 8 \u2217 105 observations for LSTM-A3C, resulting in comparable running times of both algorithms. After each epoch, we evaluate the learned policies on 104 or 105 observations, respectively. We measure the score, that is, the sum of collected rewards, that we average over the training episodes. DQN uses \u03b5 = 0.05 during evaluation."}, {"heading": "5.5 Characteristics of Learned Policies", "text": "In the conducted experiments, both algorithms were able to learn interesting policies during the 20 epochs. We describe and discuss instances of such policies in this section. The results of DQN and LSTM-A3C are similar in many cases, suggesting that both algorithms discover common structures of the DeathMatch task.\nThe agents did not show particular changes when receiving delta frames (Section 5.2) as inputs. This might be because given history frames, neural networks can easily learn to compute differences between those frames if\n25\nuseful to solve the task. Therefore, we conclude that delta frames may not be useful in combination with history frames."}, {"heading": "5.5.1 Fighting Against Enemies", "text": "As expected, the agents develop a tendency to aim at enemies. Because an agent needs to directly look toward enemies in order to shoot them, it always faces an enemy the time step before receiving a positive reward. However, the aiming is inaccurate and the agents tend to look past their enemies to the left and right side, alternatingly. It would be interesting to conduct experiments without stacking history frames to understand whether the learned policies are limited by the action repeat.\nIn several instances, the agents lose enemies from their field of view by turning, usually toward other enemies. No trained agent was found to memorize such enemies after not seeming them anymore. As a result, the agents were often attacked from the back, causing the episode to end.\nSurprisingly, LSTM-A3C agents tend to only attack when facing an enemy, but sometimes miss out the chance to do so. In contrast, DQN agents were found to shoot regularly and more often than LSTM-A3C. Further experiments would be needed to verify, whether this behavior persists after training for more than 20 episodes."}, {"heading": "5.5.2 Navigation in the World", "text": "Most agents learn to avoid running into walls while facing them, suggesting at least a limited amount of spatial awareness. In comparison, a random agent repeatedly runs into a wall and gets stuck in this position until the end of the episode.\nIn some instances, the agent walks backward against a wall at an angle, so that it slides along the wall until reaching the entrance of a side room. This behavior overfits to the particular environment of the DeathMatch task, but represents a reasonable policy to pick up the more powerful weapons inside two of the side rooms. We did not find testing episodes in which the agent used this strategy to enter one of the rooms containing health restoring items. This could be attributed to the lack of any negative reward when the agent dies.\nNotably, in one experiment, LSTM-A3C agent collects the rocket launcher weapon using this wall-sliding strategy, and starts to fire after collecting it. The agents did not fire before collecting that weapon. During training, there might have been successful episodes, where the agent initially collected this weapon. Another reason for this could be that the first enemies only appear after a couple of time steps so there is no reason to fire in the beginning of an episode.\n26\nIn general, both tested algorithms, DQN and LSTM-A3C, were able to learn interesting policies for our task in the Doom domain in relatively short amounts of training time. On the other hand, the agents did not find ways to effectively shoot multiple enemies in a row. An evaluation over multiple choices of hyper parameters and longer training times would be a sensible next step.\n27\nChapter 6\nConclusions\nWe started by introducing reinforcement learning (RL) and summarized basic reinforcement learning methods. Compared to traditional benchmarks, complex 3D environments can be more challenging in many ways. We analyze these challenges and review current algorithms and ideas to approach these.\nFollowing this background, we select to algorithms that were successfully applied to learn from pixels in 3D environments. We apply both algorithms to a challenging task within the recently presented Doom domain, that is based on a 3D first-person-shooter game.\nApplying the selected algorithms yields policies that do not reach particularly high scores, but show interesting behavior. The agents tend to aim at enemies, avoid getting stuck in walls, and find surprising strategies within this Doom task.\nWe also assess that using delta frames does not affect the training much when combined with stacking multiple images together as input to the agent. In would be interesting to see if using fewer action repeats allows for more accurate aiming.\nThe drawn conclusions are not final because other hyper parameters or longer training might allow these algorithms to perform better. We analyze instances of learned behavior and believe that observations reported in the work provide a valuable starting point for further experiments in the Doom domain.\n28"}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 19:1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep reinforcement learning in a 3-d blockworld environment", "author": ["T. Barron", "M. Whitehead", "A. Yeung"], "venue": "Deep Reinforcement Learning: Frontiers and Challenges, IJCAI 2016", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "arXiv preprint arXiv:1606.01868", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "and W", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang"], "venue": "Zaremba. Openai gym", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "A survey on policy search for robotics. Foundations and Trends in Robotics, 2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "CoRR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical reinforcement learning using spatio-temporal abstractions and deep neural networks", "author": ["R. Krishnamurthy", "A.S. Lakshminarayanan", "P. Kumar", "B. Ravindran"], "venue": "arXiv preprint arXiv:1605.05359", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A unifying framework for computational reinforcement learning theory", "author": ["L. Li"], "venue": "PhD thesis, Rutgers, The State University of New Jersey", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A critical review of recurrent 29  neural networks for sequence learning", "author": ["Z.C. Lipton", "J. Berkowitz", "C. Elkan"], "venue": "arXiv preprint arXiv:1506.00019", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "5602", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots", "author": ["H. Ngo", "M. Luciw", "A. F\u00f6rster", "J. Schmidhuber"], "venue": "Frontiers in Psychology, 4:833", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "Advances in Neural Information Processing Systems, pages 2863\u20132871", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Control of memory", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "active perception, and action in minecraft. CoRR, abs/1605.09128", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural model of hierarchical reinforcement learning", "author": ["D. Rasmussen", "C. Eliasmith"], "venue": "Proceedings of the 36th Annual Conference of the Cognitive Science Society, pages 1252\u20131257. Cognitive Science Society Austin, TX", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "From animals to animats: proceedings of the first international conference on simulation of adaptive behavior. Citeseer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}, {"title": "Formal theory of creativity", "author": ["J. Schmidhuber"], "venue": "fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models", "author": ["J. Schmidhuber"], "venue": "arXiv preprint arXiv:1511.09249", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning and human behavior", "author": ["H. Shteingart", "Y. Loewenstein"], "venue": "Current opinion in neurobiology, 25:93\u201398", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Parameter selection for the deep q-learning algorithm", "author": ["N. Sprague"], "venue": "Proceedings of the Multidisciplinary Conference on Reinforcement Learning and Decision Making ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "ICML, pages 943\u2013950", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 1. MIT press Cambridge", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence, 112", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "arXiv preprint arXiv:1604.07255", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchronous stochastic approximation and q-learning", "author": ["J.N. Tsitsiklis"], "venue": "Machine Learning, 16", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Online actor\u2013critic algorithm to solve the continuous-time infinite horizon optimal control problem", "author": ["K.G. Vamvoudakis", "F.L. Lewis"], "venue": "Automatica, 46", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science, 3", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1991}, {"title": "TORCS", "author": ["B. Wymann", "C. Dimitrakakis", "A. Sumner", "E. Espi\u00e9", "C. Guionneau", "R. Coulom"], "venue": "the open racing car simulator, v1.3.5. http://www.torcs.org", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pages 818\u2013833. Springer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This general setting allows methods to be applied to financial trading, advertising, robotics, power plant optimization, aircraft design, and more [1, 30].", "startOffset": 147, "endOffset": 154}, {"referenceID": 29, "context": "This general setting allows methods to be applied to financial trading, advertising, robotics, power plant optimization, aircraft design, and more [1, 30].", "startOffset": 147, "endOffset": 154}, {"referenceID": 21, "context": "For further details, please refer to Shteingart and Loewenstein [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Most notably, the Atari environment provided by ALE [3] consists of 57 low-resolution 2D games.", "startOffset": 52, "endOffset": 55}, {"referenceID": 34, "context": "3D environments where the agent observes perspective pixel images include the driving simulator Torcs [35], several similar block-world games that we refer to as Minecraft domain, and the first-person shooter game Doom [9] (Section 5.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "3D environments where the agent observes perspective pixel images include the driving simulator Torcs [35], several similar block-world games that we refer to as Minecraft domain, and the first-person shooter game Doom [9] (Section 5.", "startOffset": 219, "endOffset": 222}, {"referenceID": 24, "context": "In the case of an estimation error, the update may not be a monotonic improvement, but the algorithm is known to converge to Q\u2217 as the number of visits of each Q-state approaches infinity [25].", "startOffset": 188, "endOffset": 192}, {"referenceID": 0, "context": "The dominant approach to exploration is the straightforward EpsilonGreedy strategy, where the agent picks a random action with probability \u03b5 \u2208 [0, 1], and the action according to its normal policy otherwise.", "startOffset": 143, "endOffset": 149}, {"referenceID": 24, "context": "3 allows us to update the estimate Q\u0302 after each time step rather than after each episode, resulting in the online-algorithm SARSA [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 30, "context": "A common modification to this is known as Q-Learning, as proposed by Watkins and Dayan [31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "The Q-Learning algorithm might be one of the more important breakthroughs in RL [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "Q-Learning converges to Q\u2217 given continued exploration [29].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "1: Accumulating, Dutch, and replacing eligibility traces (Sutton and Barto [25]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "The state in which we receive the reward has an eligibility of 1 and the eligibility of previous states decays exponentially over time by a factor \u03bb \u2208 [0, 1]: et(st\u2212k) = (\u03b3\u03bb) .", "startOffset": 151, "endOffset": 157}, {"referenceID": 5, "context": "Several methods for searching the space of possible policy parameters have been explored, including random search, evolutionary search , and gradientbased search [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 25, "context": "gradient estimator [26]:", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "This yields the Reinforce algorithm proposed by Williams [33].", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "To not introduce bias to the gradient of the reward, the gradient of the baseline with respect to the policy must be 0 [33]:", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) provide a computationally effective way to learn such abstractions [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "[16])", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Zeiler and Fergus [36] visualize the layers of CNNs and show that they actually learn more abstract features in each layer.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "The agent then decides for an action we repeat while collecting the next stack of inputs [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "It is also common to use recurrent neural networks (RNNs) [8, 14] to address the problem of partial observability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 13, "context": "It is also common to use recurrent neural networks (RNNs) [8, 14] to address the problem of partial observability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 11, "context": "In particular, a variant called Long Short-Term Memory (LSTM) and its variations like Gated Recurrent Unit (GRU) have proven to be effective in a wide range of sequential problems [12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "[16] were able to learn useful representations from videos, allowing them to predict up to 100 observations in the Atari domain (Figure 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 63, "endOffset": 70}, {"referenceID": 6, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 63, "endOffset": 70}, {"referenceID": 16, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "[13] to store previously encountered transitions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The mentioned work first managed to learn to play several 2D Atari games [3] without the use of hand-crafted features.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "It has also been applied to simple tasks in the Minecraft [2] and Doom [9] domains.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "It has also been applied to simple tasks in the Minecraft [2] and Doom [9] domains.", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "[13] used the idea of a target network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed an alternative to using replay memories that involves multiple versions of the agent simultaneously interacting with copies of the environment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "5) are optimal in the tabular case under the assumption that each state will be visited over and over again, eventually [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "One reason that random exploration still works reasonably well in complex environments [13, 14] can partly be attributed to function approximation.", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": "One reason that random exploration still works reasonably well in complex environments [13, 14] can partly be attributed to function approximation.", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "The Bayesian approach is to count the visits of each state to compute the uncertainty of its value estimate [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Combined with Q-learning, this converges to the true Q-function, given enough random samples [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "[4] recently suggested a sequential density estimation model to derive pseudo-counts for each state in a non-tabular setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "While we usually do not estimate the Q-function here, we can add the entropy of the policy as a regularization term to the its gradient [34] with a small factor \u03b2 \u2208 R specifying the amount of regularization:", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Its objective is both to maximize expected returns and to cause the highest reduction in prediction error of the model [19].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "It therefore tries to provide new observations to the model that are novel but learnable, inspired by the way humans are bored by both known knowledge and knowledge they cannot understand [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "[15] combined it with planning to escape known areas of the state space more effectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Schmidhuber [21] recently proposed shared neurons between the model and controller networks in a way that allows the controller to arbitrarily exploit the model for control.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "Temporally extended actions could be beneficial to both exploration and exploitation [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] used spectral clustering to group states with cluster centers representing options.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] trained multiple CNNs on simple tasks in the Minecraft domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Rasmussen and Eliasmith [18] propose one such architecture and show that it is able to learn simple visual tasks.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] who introduced this domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] applied DQN to two tasks in the Minecraft domain: Collecting as many blocks of a certain color as possible, and navigating forward on a pathway without falling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] is an actor-critic method that is considerable more memory-efficient than DQN, because it does not require the use of a replay memory.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The Doom domain [9] provides RL tasks simulated by the game mechanics of the first-person shooter Doom.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "We use the DeathMatch task defined in the Gym collection [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Finally, we apply normalization: We scale the observed pixels into the range [0, 1] and normalize rewards to (\u22121, 0,+1) using rt \u2190 sgn(rt) [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 12, "context": "Finally, we apply normalization: We scale the observed pixels into the range [0, 1] and normalize rewards to (\u22121, 0,+1) using rt \u2190 sgn(rt) [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "Both DQN and LSTM-A3C are sensitive to the choice of hyper parameters [14, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "Both DQN and LSTM-A3C are sensitive to the choice of hyper parameters [14, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "[13, 14], but not by Kempka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[13, 14], but not by Kempka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[9], Barron et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "5 [14].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "99 for optimization [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "[9] who use a fixed learning rate of 10\u22125 in the Doom domain.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTMA3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.", "creator": "LaTeX with hyperref package"}}}