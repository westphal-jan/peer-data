{"id": "1610.02348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data", "abstract": "These challenges in the online data stream are known as concept drift. In this paper, we have proposed the Adaptive Convolutional ELM Method (ACNNELM) as an extension of the Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability, which aims at concept drift handling. We have improved CNN as a feature presentation learner in combination with Elastic ELM (E $^ 2 $LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability at the classification level (ACNNELM-1) and matrix conceptual drift adaptability at the ensemble level (ACNNELM-2). Our proposed Adaptive CNNELM is flexible, which works well at the classification level (ACNNELM-1) and only at the most proposed ensemble levels.", "histories": [["v1", "Fri, 7 Oct 2016 16:53:09 GMT  (1060kb,D)", "http://arxiv.org/abs/1610.02348v1", "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems. Special Issue on Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems"]], "COMMENTS": "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems. Special Issue on Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["arif budiman", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.02348"}, "pdf": {"name": "1610.02348.pdf", "metadata": {"source": "CRF", "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data", "authors": ["Arif Budiman", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["arif.budiman21@ui.ac.id"], "sections": [{"heading": null, "text": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E2LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels.\nWe verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set.\nKeywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, online,concept drift"}, {"heading": "1 Introduction", "text": "Online data stream learning is an emerging research area that has shown its importance in big data era. The big volumes of data are continuously generated from devices, softwares, and Internet, with higher incoming rate and no time bound. Knowledge mining over them needs special machine learning techniques to learn large volumes of data in a timely fashion. The techniques also need to be scalable for deployment in the big online real-time scenario (computationally tractable for processing large data streams) and capable to overcome uncertainty in the data representations.\nThe techniques, to offer an adaptive framework which adapts to any issue in which the data concepts do not follow static assumptions (known as concept drift\n1/28\nar X\niv :1\n61 0.\n02 34\n8v 1\n[ cs\n.A I]\n7 O\nct 2\nproblem [7,29]). In concept drift (CD), the input and/or output concepts has non stationary and uncertain data distribution. The uncertainties can be perceived by the increase of class overlapping or additional comprehensive features in the feature space, which makes a deterioration of classifiers.\nThe aim of CD handling [7] is to boost the generalization accuracy when the drift occurs. Common handling methods are based on classifier ensemble [5]. Ensemble methods combined decision from each classifier members (mainly using ensemble members diversification). However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].\nOne of the recent online big stream data approaches are based on Deep Learning (DL) techniques [1]. They offer promising avenue of automated feature extraction of big data in streaming approaches. The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.\nThe traditional machine learning methods i.e. Extreme Learning Machine (ELM), Support Vector Machine (SVM), Multi-Layer Perceptron Neural Network (MLP NN), Hidden Markov Model (HMM) may not be able to handle such big stream data directly [31] although they worked successfully in classification problem in many areas. The shallow methods need a good feature representation and assume all data available. Feature engineering focused on constructing features as an essential element of machine learning. However, when the data is rapidly growing within dynamic assumptions such as CD, the handy-crafted feature engineering is very difficult and the traditional methods need to be modified. The popular approach combines Deep Learning method as unsupervised feature representation learner with any supervised classifier from traditional machine learning methods.\nIn this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data. We named it Adaptive CNN-ELM (ACNNELM). We studied ACNNELM scheme for concept drift either changes in the number of feature inputs named virtual drift (VD) or the number of classes named real drift (RD) or consecutive drift when VD and RD occurred at the same time named hybrid drift (HD) [3] in recurrent context (all concepts occur alternately).\nWe developed ACNNELM based on our previous work on adaptive ELM scheme named Adaptive OS-ELM (AOS-ELM) that works as single ELM classifier for CD handling [2, 3]. As single classifier, AOS-ELM combines simultaneously many strategies to solve many types of CD in simple platform.\nAs our main contributions in this research area, we proposed two models of adaptive hybrid CNN and ELM as follows.\n1. ACNNELM-1: the adaptive scheme for integrating CNN and ELM to handle concept drift in classifier level;\n2. ACNNELM-2: the concatenation aggregation ensemble of integrating CNN and ELM classifiers to boost the performance and to handle concept drift for adaptivity in ensemble level.\nSection 1 gives introduction and research objectives. We describe related works in Section 2. Section 3 describes our proposed methods. We focus on the empirical experiments to prove the methods in MNIST and not-MNIST image classification task in Section 4. Section 5 discusses conclusions, challenges, and future directions.\n2/28"}, {"heading": "1.1 Notations", "text": "We used the notations throughout this paper to make easier for the readers:\n\u2022 Matrix is written in uppercase bold (i.e., X).\n\u2022 Vector is written in lowercase bold (i.e., x).\n\u2022 The transpose of a matrix X is written as XT . The pseudo-inverse of a matrix H is written as H\u2020.\n\u2022 g will be used as non linear activation function, i.e., sigmoid, reLU or tanh function.\n\u2022 The amount of training data is N . Each input data x contains some d attributes. The target has m number of classes. An input matrix X can be denoted as Xd\u00d7N and the target matrix T as TN \u00d7m.\n\u2022 We denote the subscript font with parenthesis to show the time sequence number. The X(0) is the subset of input data at time k = 0 as the initialization stage. X(1),X(2),...,X(k) are the subset of input data at the next sequential time. Each subset may have different number of quantity. The corresponding label data is presented as T = [ T(0),T(1),T(2), ...,T(k) ] .\n\u2022 We denote the subscript font without parenthesis to show the concept number. S concepts (sources or contexts) is using the symbol Xs for training data and Ts for target data.\n\u2022 We denote the drift event using the symbol \u226bV D , where the subscript font shows the drift type. I.e., the Concept 1 has virtual drift event and replaced by Concept 2 (Sudden changes) symbolized as C1 \u226b V DC2. The Concept 1 has real drift event\nand replaced by Concept 1 and Concept 2 recurrently (Recurrent context) in the shuffled composition symbolized as C1 \u226b RD shuffled(C1,C2)."}, {"heading": "2 Literature Reviews", "text": ""}, {"heading": "2.1 Extreme Learning Machine (ELM)", "text": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17]. Compared with Neural Networks (NN) including CNN, ELM and NN used random value parameters. However, ELM used set and fixed random value in hidden nodes parameters and used non iterative generalized pseudoinverse optimization process, while NN used iterative gradient descent optimization process to smooth the weight parameters.\nThe ELM learning objective is to get Output weight (\u03b2), where\n\u03b2\u0302 = H\u2020T (1)\nwhich H\u2020 is Pseudoinverse (Moore-Penrose generalized inverse) of H. The ELM\nlearning is simply equivalent to find the smallest least-squares solution for \u03b2\u0302 of the linear system H\u03b2 = T when the Output weight \u03b2\u0302 = H\u2020T.\nHidden layer matrix H is computed using activation function g of the summation matrix from the hidden nodes parameter (input weight a and bias b) with training input x with size N number of training data and L number of hidden nodes g(ai \u00b7 x + bi) (feature mapping).\n3/28\nTo solve H\u2020, ELM used orthogonal projection based on ridge regression method, where a positive 1/\u03bb value is added as regularization to the auto correlation matrices HTH or HHT . We can solve Eq. 1 as follows.\n\u03b2 =\n( I\n\u03bb + HTH\n)\u22121 HTT (2)\nELM has capability to learn online stream data (data comes one by one or by block with fixed or varying quantity) by solving HTH or HHT by following two methods:\n1. Sequential series using block matrices inverse.\nA Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [26] has sequential learning phase. In sequential learning, the\n\u03b2(k) computed as the previous \u03b2(k\u22121) function. If we have \u03b2\u0302(0) from H(0) filled by the N0 as initial training data and H(1) filled by N1 incremental training data, then the output weights \u03b2\u0302(1) are approximated by solving:\n([ H(0) H(1) ]T [ H(0) H(1) ])\u22121 [ H(0) H(1) ]T [ T(0) T(1) ] (3)\nThe OS-ELM assumes no changes in the hidden nodes number. However, increasing the hidden nodes number may improve the performance named Constructive Enhancement OS-ELM [18].\n2. Parallelization using MapReduce framework.\nAnother approach is Elastic Extreme Learning Machine (E2LM) [40] or Parallel ELM [10] based on MapReduce framework to solve large sequential training data in a parallel way. First, Map is the transform process of intermediate matrix multiplications for each training data portion. Second, Reduce is the aggregate process to sum the Map result.\nIf U = HTH and V = HTT, we have decomposable matrices in k time sequences and can be written as :\nU = k=\u221e\u2211 k=0 U(k) (4)\nV = k=\u221e\u2211 k=0 V(k) (5)\nFinally, The corresponding output weights \u03b2 can be obtained with centralized computing using result from reduce/aggregate process. Therefore, E2LM learning is efficient for rapidly massive training data set [40].\n\u03b2 =\n( I\n\u03bb + U\n)\u22121 V (6)\nE2LM is more computing efficient, better performance and support parallel computation than OS-ELM [40], but E2LM did not address the possibility for hidden nodes increased during the training. OS-ELM, CEOS-ELM, E2LM, and Parallel ELM method did not address concept drift issues; i.e., when the number of attributes d in\n4/28\nXd\u00d7N or the number of classes m in TN \u00d7m in data set has changed. We categorized OS-ELM, CEOS-ELM, E2LM , and Parallel ELM as non-adaptive sequential ELM.\nIn this paper, we developed parallelization framework to integrate with CNN to solve concept drift issues in sequential learning as enhancement from our previous work Adaptive OS-ELM (AOS-ELM) [3]."}, {"heading": "2.2 Convolutional Neural Networks (CNN)", "text": "Different with SLFN in ELM, a CNN consists of some convolution and sub-sampling layers in feed forward architecture. CNN is the first successful deep architecture that keep the characteristics of the traditional NN. CNN has excellent performance for spatial visual classification [36]. The key benefit of CNN comparing with another deep learning methods are using fewer parameters [43].\nAt the end of CNN layer, it is followed by fully connected standard multilayer neural network (See Fig. 1) [43]. Many variants of CNN architectures in the literature, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [8].\nThe CNN input layer is designed to exploit the 2D structure with d\u00d7 d\u00d7 r of image where d is the height and width of image, and r is the number of channels, (i.e. gray scale image has r=1 and RGB image has r=3). The convolutional layer has c filters (or kernels) of size k \u00d7 k \u00d7 q where k < d and q can either be the same as the number of channels r or smaller and may vary for each kernel. The filters have locally connected structure which is each convolved with the image to produce c feature maps of size d\u2212 k + 1 (Convolution operations) .\nEach map from convolutional layer is then pooled using either down sampling, mean or max sampling over s\u00d7 s\u00d7 s contiguous regions (s ranges between 2 for small and up to 5 for larger inputs). An additive bias and activation function (i.e. sigmoid, tanh, or reLU) can be applied to each feature map either before or after the pooling layer. At the end of the CNN layer, there may be any densely connected NN layers for supervised learning [43]. The learning errors are propagated back to the previous layers using optimization method to finally update the kernel weight parameters and bias.\nThe convolution operations are heavy computation but inherently parallel, which getting beneficial from a hardware parallel implementation [35]. Krizhevsky et. al. showed a large, deep CNN is capable of achieving record breaking results on a highly challenging dataset (the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes) using purely supervised learning. However, the CNN network size is still limited mainly by the amount of memory available on current GPUs [19].\n5/28"}, {"heading": "2.3 CNN ELM Integration", "text": "Huang et. al. [15] explained the ELM theories are not only valid for fully connected ELM architecture but are also actually valid for local connections, named local receptive fields (LRF) or similar with Kernel in CNN term. Huang et. al. proposed ELM-LRF that has connections between input layer and hidden nodes are randomly generated following any continuous different types of probability distributions. According to Huang et. al. the random convolutional hidden nodes CNN is one type of local receptive fields. Different with CNN, ELM with LRF type hidden nodes keeps the essence of ELM for non iterative output weights calculation.\nPang et. al. [33] implemented deep convolutional ELM (DC-ELM). It uses multiple convolution layers and pooling layers for high level features abstraction from input images. Then, the abstracted features are classified by an ELM classifier. Pang et. al. did not use sequential learning approach. Their results give 98.43% for DC-ELM compared with 97.79% for ELM-LRF on MNIST regular 15K samples training data.\nGuo et. al. [9] introduced an integration model of CNN-ELM, and applied to handwritten digit recognition. Guo et. al. used CNN as an automatic feature extractor and ELM to replace the original classification layer of CNN. CNN-ELM achieved an error rate of 0.67% that is lower than CNN and ELM alone. Guo et. al. trained the original CNN until converged. The last layer of CNN was replaced by ELM to complete classification without iteration. The experiments used regular MNIST data set with size-normalized and centered in a fixed-size image 28\u00d7 28 pixels. According to Guo et. al. numbers filters of different convolution layer have a significant influence on the generalization ability.\nLee et. al. [25] proposed an integration of CNN with OS-ELM. A learning method called an orthogonal bipolar vector (OBV) was also well applied by analyzing the neural networks learned by combining both algorithms. The experimental results demonstrate that the proposed method can conduct network learning at a faster rate than conventional CNN. In addition, it can be used to solve the local minima and overfitting problems. The experiment used NORB dataset, MNIST handwritten digit standard dataset, and CIFAR-10 dataset. The recognition rate for MNIST testing data set is 93.54% ."}, {"heading": "2.4 Concept Drift", "text": "The brief explanation of concept drift (CD) has been well introduced by Gama, et. al. [7] and Minku [29] based on Bayesian decision theory for class output c and incoming data X.\n6/28\nThe concept is the whole distribution (joint distribution P(X,c) in a certain time step. The CD represents any changes in the joint distribution when P (c|X) has changed; i.e., \u2203X : P(0)(X, c) 6= P(1)(X, c), where P(0) and P(1) are respectively the joint distribution at time k(0) and k(1). The CD type has categorization as follows [6,7,12,37].\n1. Real Drift (RD) refers to changes in P (c|X). The change in P (c|X) may be caused by a change in the class boundary (the number of classes) or the class conditional probabilities (likelihood) P (X|c).\n2. Virtual Drift (VD) refers to the changes in the distribution of the incoming data (i.e., P (X) changes). These changes may be due to incomplete or partial feature representation of the current data distribution. The trained model is built with additional data from the same environment without overlapping the actual class boundaries.\nWe introduced the third category named Hybrid Drift (HD) when the RD and VD occurred consecutively. [3].\nEnsemble learning is the common approaches to tackle concept drift, in which are combined using a form of voting [38,45]. The ensemble approach can integrate the results of individual classifiers into a unified predicted result to improve the accuracy and robustness than single classifiers [42]. Yu , et. al. [41] proposed a general hybrid adaptive ensemble learning framework (See Fig. 3). Liu, et. al. [27] proposed an ensemble based ELM (EN-ELM) which uses the cross-validation scheme to build ELM classifiers ensemble.\nEach drift employed different solution strategies. The solution for RD is entirely different from VD. Certain CD requirement needs to replace entirely the outdated concept (concept replacement) either sudden or gradually, but another requirement needs to handle old and new concepts that come alternately (recurring concept). Thus,\n7/28\nit is hard to combine simultaneously many types and requirement of complex drift solutions, such as hybrid drift in a simple ensemble platform. The adaptive ensemble approach may be not practical and flexible if each member itself is not designed to be adaptive [3] and may need to recall the previous training data (not single pass learning [20]). Moreover, another simple approach is using single classifier [3, 28,30].\nMirza et. al. [30] proposed OS-ELM for imbalanced and concept drift tackling named meta-cognitive OS-ELM (MOS-ELM) that was developed based on Weighted OS-ELM (WOS-ELM) [46]. MOS-ELM used an additional weighting matrix to control the CD adaptivity, however, it works for RD with concept replacement only.\nOur previous AOS-ELM works as single unified solution for VD, RD, and HD. Also, it can be applied for concept replacement and recurring [3] by using simple matrix adjustment and multiplication. We explained AOS-ELM for each scenario as follows.\n1. Scenario 1: Virtual Drift (VD).\nAccording to interpolation theory from ELM point of view and Learning Principle I of ELM Theory [14], the input weight and bias as hidden nodes H parameters are independent of training samples and their learning environment through randomization. Their independence is not only in initial training stage but also in any sequential training stages. Thus, we can adjust the input weight and bias pair {ai,bi}Li=1 on any sequential stages and still have probability one that \u2016H\u03b2 \u2212T\u2016 < to handle additional feature inputs.\nThe example implementation is when we need to combine from different type of training data set."}, {"heading": "2.5 CNN in Concept Drift", "text": "Grachten et. al. [28] proposed some adapting strategies for single CNN classifier system based on two common adaptive approaches: 1) REUSE is to reuse the model upon the change of task and replace the old task training data with the new task training data. 2) RESET is to ignore the previous representations learned, and begin the new task learning with a randomly initialized model. Further, Grachten et. al. categorized as follows.\n1. RESET: Initialize parameters with random values;\n2. RESET PRF: Combination between the prior regularization on convolutional filters (PRF) with the RESET option. PRF improves a bit the RESET baseline sometimes, and the gains are usually moderate;\n3. REUSE ALL: Initialize all parameters from prior model (except output layer);\n4. REUSE CF: Selectively reuse the prior model by keeping previous convolutional filters (CF) from the prior model.\nGrachten et. al. divided regular MNIST into two subsets (Data Set 1 from class 0 to 4, and Data Set 2 from class 5 to 9). The training set is 50000 data, 10000 data for\n9/28\nvalidation and 10000 testing data. The classifier used a CNN with a convolutional layer (32 feature maps using kernels size 5\u00d7 5, sigmoid activations and Dropout), followed by a max-pooling layer (2\u00d7 2 pool size). The classification stage used a fully connected layer (40 sigmoid units and 10 softmax units).\nZhang et. al. [44] proposed an adaptive CNN (ACNN), whose structure automatic expansion based on the average system error and training recognition rate performance. The incremental learning for new training data is handled by new branches addition while the original network keep unchanged. ACNN used the structure global expansion until meet the average error criteria and local expansion to expand the network structure. Zhang et. al. used ORL face database. The model has recognition rate increased from 91.67% to 93.33% using local expansion. However, Zhang et. al. did not discuss any concept drift handling.\nAccording to Zhang et. al. no such theory about how CNN structure constructed, i.e. the number of layers, the number of feature maps per layer. Researchers constructed and compared each CNN candidate performance for the best one. Some studies tried to use hardware acceleration to speed up the performance comparison discovery. However, Zhang et. al. did not discuss any concept drift handling. We used the idea of global expansion in our proposed method."}, {"heading": "3 Proposed Method", "text": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig. 2). However, for final H, we used nonlinear optimal tanh (1.7159\u00d7 tanh( 23 \u00d7H) activation function [23] to have better generalization accuracy. We used also CNN global expansion structure [44] to improve the performance accuracy.\nWe used the E2LM as a parallel supervised classifier to replace fully connected NN. Compared with regular ELM method, we do not need input weight as hidden nodes\n10/28\nparameter (See Fig. 9). We deployed the integration architecture becomes two models: 1) ACNNELM-1 : This ACNNELM model works based on AOS-ELM for concept drift adaptability on classifier level. 2) ACNNELM-2 : This model combines some ACNNELMs and aggregated as matrices concatenation ensemble to handle concept drift adaptability on ensemble level.\nIn ACNNELM-1, we used matrices U and V adjustment padded by using zero block matrices before recomputing the \u03b2 (See Equation 6). In ACNNELM-2, we enhanced the model by matrices concatenation aggregation of H and \u03b2 as the result from CNN ELM individual model (See Fig. 11).\nLet\u2019s T2 is additional output classes expansion as a new concept. Thus we need to modify the ELM layer only by adjusting the column dimension of matrix V with zero block matrix for column adjustment only.\n(c) Hybrid Drift (HD).\nThe HD solution basically is a combination of VD and RD method. We introduced new CNN model and adjusting the row and column dimension of matrix V.\n2. ACNNELM-2\n(a) Matrices Concatenation Ensemble to boost the performance.\nThe idea is based on matrix multiplication H\u03b2 that actually can be decomposable. We used to re-compose ELM members to be one ensemble that seems as one big ELM. This idea only needs minimum two members and no need to have additional multi-classifier strategies (See Fig. 12).\ntraining data modification by incrementing the order of training class number.\n(d) Hybrid Drift (HD)\nVD and RD can be processed on separated CNN-ELM hybrid model parallel. Thus the HD scenario is easy with Matrices Concatenation Ensemble method.\n[4]"}, {"heading": "4 Experiment and Performance Results", "text": ""}, {"heading": "4.1 Data set", "text": "Dataset is the successful key for this research to simulate big stream data. MNIST is the common data set for big data machine learning, in fact, it accepted as standard and give an excellent result. MNIST data set is a balanced data set that contains numeric handwriting (10 target class) with size 28\u00d7 28 pixel in a gray scale image. The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [24]. However, to simulate big stream data, the regular MNIST is not adequate. For that reason, we developed extended MNIST data set with larger training examples by adding 3 types of image noises (See Fig. 16). Our extended MNIST data set finally has 240,000 examples of training data and 40,000 examples of testing data.\nWe also enhanced the 28\u00d7 28 image size as attributes with additional attributes based on Histogram of oriented gradient (HOG) of images with size 9\u00d7 9. The total\n14/28\n.\nattributes become 865 attributes. The HOG additional attributes have been used in our research [3] to simulate VD scenario.\nWe used not-MNIST large data set for additional experiments. We expect not-MNIST is a harder task than MNIST. Not-MNIST dataset has a lot of foolish images (See Fig. 17 and 18). Bulatov explained the logistic regression on top of stacked autoencoder with fine-tuning gets about 89% accuracy whereas the same approach gives got 98% on MNIST [4]. Not-MNIST has gray scale 28\u00d7 28 image size as attributes. We divided the set to be numeric (360,000 data) and alphabet (A-J) symbol (540,000) data including many foolish images. The challenge with not-MNIST numeric and not-MNIST alphabet is many similarities between class 1 with class I, class 4 with class A, and another similar foolish images."}, {"heading": "4.2 Experiment Methods", "text": "We defined the scope of works as following:\n1. We enhanced DeepLearn Toolbox [32] with Matlab parallel computing toolbox.\n15/28\n2. We used single precision than double precision for all computation in this paper. Double precision has accuracy improvement in matrix inverse computation than single precision. The ELM in Matlab using double precision by default and it has better accuracy (i.e. 4-5% higher) than single precision. However, using single precision has more CPU and memory saving especially for parallelization in big stream data. Unfortunately, most papers did not mention how their precision method used. We want to achieve accuracy improvement not because of precision computation factor.\n3. We focused on simple CNN architecture that consist of convolution layers (c), following by reLU activation layer then sample pooling layer (s) in this paper.\n4. We used extended MNIST data set and not-MNIST data set to simulate big stream data.\nWe summarized the experiment methods in the following tables 1 and 2. To verify our method, we designed some experiments to answer the following\nresearch questions:\n\u2022 How is the performance comparison of more CNN layer added?\n\u2022 How is the performance comparison of non linear optimal tanh function compared with another function?\n\u2022 How is the performance comparison between non adaptive OS-ELM, CNN, and our method CNN-ELM (ACNNELM-1 and ACNNELM-2) using the extended MNIST and not-MNIST data set?\n\u2022 How is the effectiveness of Matrices Concatenation Ensemble of ACNNELM-2 method improved the performance if we used up to 16 independent models?\n\u2022 How does the ACNNELM-1 and ACNNELM-2 handle VD, RD and HD using simulated scenario on extended MNIST data set and not-MNIST data set?\nIn VD scenario, the drift event is : MNIST1 \u226b V DMNIST2 Not\u2212MNIST1 \u226bV DNot\u2212MNIST3 In RD scenario, the drift event is : MNIST3 \u226b RD shuffled(MNIST3,4) Not\u2212MNIST1 \u226bRD shuffled(NotMNIST1,2) In HD scenario, the drift event is : MNIST3 \u226b HDMNIST2 Not\u2212MNIST1 \u226bHDNot\u2212MNIST4\n16/28"}, {"heading": "4.3 Performance Results", "text": "For concept drift capability, we expect no accuracy performance decreased after the drift event, no matter how big training data is required compared with its full batch version.\nDifferent with CNN, the performance of CNN for big sequential training data is better using a larger number of the epoch. CNN has better scalability than ELM for big sequential learning data, but it needs longer time for iteration to improve the performance (See Fig. 20). With epoch=50, the testing accuracy is 90.32%. From the learning time perspective, the time for 50 iterations is equivalent to build 50 models of individual CNN ELM sequentially.\n18/28\nWe compared above benchmark result with our CNN-ELM method:\n1. The performance of CNN-ELM can be improved by using more layers in expanded structure. In Table 3, the model 6c-2s-12c-2s has better accuracy than model 6c-2s.\n.\nusing additional zero block matrix to pad the columns to be 10 columns. Then we can concatenate with Model 7 and Model 8. We tested all models with 10 classes testing data set."}, {"heading": "5 Conclusion", "text": "The proposed method gives better adaptive capability for classifier level (ACNNELM-1) and ensemble level (ACNNELM-2). ACNNELM-2 has better computation scalability and performance accuracy than ACNNELM-1 as result of the aggregation ensemble benefit.\n23/28\nHowever, some CNN related parameters need to be further investigated, i.e. iterations, random weight for kernel assignment, error backpropagation optimization, decay parameters, and larger layers for larger feature dimension. Also, We need to investigate and implement CNN ELM for non spatial recognition, i.e. posed based human action recognition [2].\nWe think some ideas for future research:\n\u2022 We will develop the methods on another CNN framework that fully supported to CUDA GPU computing. The purpose is to increase the scale up capability and to speed up the training time in big image data set.\n\u2022 We need to investigate another optimum learning parameters, i.e., stochastic\n24/28\ngradient descent, the optimum kernel weight, dropout and dropconnect regularization, decay parameters. To improve the performance, we believe the optimum CNN parameters also work well for CNNELM."}, {"heading": "6 Acknowledgment", "text": "This work is supported by Higher Education Center of Excellence Research Grant funded Indonesia Ministry of Research and Higher Education Contract No. 1068/UN2.R12/ HKP.05.00/2016"}, {"heading": "7 Conflict of Interests", "text": "The authors declare that there is no conflict of interest regarding the publication of this paper."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., 2(1):1\u2013127, Jan.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Constructive, robust and adaptive os-elm in human action recognition", "author": ["A. Budiman", "M.I. Fanany", "C. Basaruddin"], "venue": "In Industrial Automation, Information and Communications Technology (IAICT), 2014 International Conference on, pages 39\u201345, Aug", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive online sequential elm for concept drift tackling", "author": ["A. Budiman", "M.I. Fanany", "C. Basaruddin"], "venue": "Hindawi,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A review on real time data stream classification and adapting to various concept drift scenarios", "author": ["P. Dongre", "L. Malik"], "venue": "In Advance Computing Conference (IACC), 2014 IEEE International, pages 533\u2013537, Feb", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "Neural Networks, IEEE Transactions on, 22(10):1517\u20131531, Oct", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "CoRR, abs/1512.07108,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition", "author": ["L. Guo", "S. Ding"], "venue": "page 2673\u20132680, 7", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomput., 102:52\u201358, Feb.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from streaming data with concept drift and imbalance: an overview", "author": ["T. Hoens", "R. Polikar", "N. Chawla"], "venue": "Progress in Artificial Intelligence, 1(1):89\u2013101,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Trends in extreme learning machines: A review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61(0):32 \u2013 48,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An insight into extreme learning machines: Random neurons, random features and kernels", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, 6(3),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Local receptive fields based extreme learning machine", "author": ["G.-B. Huang", "Z. Bai", "L.L.C. Kasun", "C.M. Vong"], "venue": "IEEE Computational Intelligence Magazine (accepted), 10,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, 2(2):107\u2013122,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, 70(1-3):489\u2013501,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "An enhanced online sequential extreme learning machine algorithm", "author": ["Y. Jun", "M.-J. Er"], "venue": "In Control and Decision Conference, 2008. CCDC 2008. Chinese, pages 2902\u20132907, July", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems, pages 1097\u20131105,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifier ensembles for changing environments", "author": ["L. Kuncheva"], "venue": "In Multiple Classifier Systems, volume 3077 of Lecture Notes in Computer Science, pages 1\u201315. Springer Berlin Heidelberg,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Classifier ensembles for detecting concept change in streaming data: Overview and perspectives", "author": ["L.I. Kuncheva"], "venue": "In 2nd Workshop SUEMA 2008 (ECAI 2008), pages 5\u201310,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In S. Haykin and B. Kosko, editors, Intelligent Signal Processing, pages 306\u2013351. IEEE Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCu"], "venue": "Cortes. MNIST handwritten digit database,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Image classification using fast learning convolutional neural networks", "author": ["K. Lee", "D.-C. Park"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble based extreme learning machine", "author": ["N. Liu", "H. Wang"], "venue": "IEEE Signal Processing Letters, 17(8):754\u2013757, Aug", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Grachten", "author": ["C.E.C.C. Maarte"], "venue": "Strategies for conceptual change in convolutional neural networks.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "The impact of diversity on online ensemble learning in the presence of concept drift", "author": ["L. Minku", "A. White", "X. Yao"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 22(5):730\u2013742, May", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Meta-cognitive online sequential extreme learning machine for imbalanced and concept-drifting data classification", "author": ["B. Mirza", "Z. Lin"], "venue": "Neural Networks, 80:79\u201394,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning applications and challenges in big data analytics", "author": ["M.M. Najafabadi", "F. Villanustre", "T.M. Khoshgoftaar", "N. Seliya", "R. Wald", "E. Muharemagic"], "venue": "Journal of Big Data, 2(1):1\u201321,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional extreme learning machine and its application in handwritten digit classification", "author": ["S. Pang", "X. Yang"], "venue": "Hindawi,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448\u2013455,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerating Large-Scale Convolutional Neural Networks with Parallel Graphics Multiprocessors, pages 82\u201391", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In Proceedings of the Seventh International Conference on Document Analysis and Recognition Volume 2, ICDAR \u201903, pages 958\u2013, Washington, DC, USA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Handling local concept drift with dynamic integration of classifiers: Domain of antibiotic resistance in nosocomial infections", "author": ["A. Tsymbal", "M. Pechenizkiy", "P. Cunningham", "S. Puuronen"], "venue": "In Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on, pages 679\u2013684,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic integration of classifiers for handling concept drift", "author": ["A. Tsymbal", "M. Pechenizkiy", "P. Cunningham", "S. Puuronen"], "venue": "Inf. Fusion, 9(1):56\u201368, Jan.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Elastic extreme learning machine for big data classification", "author": ["J. Xin", "Z. Wang", "L. Qu", "G. Wang"], "venue": "Neurocomputing, 149, Part A:464 \u2013 471,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid adaptive classifier ensemble", "author": ["Z. Yu", "L. Li", "J. Liu", "G. Han"], "venue": "IEEE Transactions on Cybernetics, 45(2):177\u2013190, Feb", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative study between incremental and ensemble learning on data streams: Case study", "author": ["W. Zang", "P. Zhang", "C. Zhou", "L. Guo"], "venue": "Journal Of Big Data, 1(1):1\u201316,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and Understanding Convolutional Networks, pages 818\u2013833", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Springer International Publishing, Cham,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive convolutional neural network and its application in face recognition", "author": ["Y. Zhang", "D. Zhao", "J. Sun", "G. Zou", "W. Li"], "venue": "Neural Processing Letters, 43(2):389\u2013399,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning under Concept Drift: an Overview", "author": ["I. Zliobaite"], "venue": "Computing Research Repository, abs/1010.4,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Weighted extreme learning machine for imbalance learning", "author": ["W. Zong", "G.-B. Huang", "Y. Chen"], "venue": "Neurocomput., 101:229\u2013242, Feb.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "problem [7,29]).", "startOffset": 8, "endOffset": 14}, {"referenceID": 27, "context": "problem [7,29]).", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "The aim of CD handling [7] is to boost the generalization accuracy when the drift occurs.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "Common handling methods are based on classifier ensemble [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 18, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 19, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 42, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 0, "context": "One of the recent online big stream data approaches are based on Deep Learning (DL) techniques [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 146, "endOffset": 150}, {"referenceID": 36, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 200, "endOffset": 204}, {"referenceID": 20, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 254, "endOffset": 258}, {"referenceID": 29, "context": "Extreme Learning Machine (ELM), Support Vector Machine (SVM), Multi-Layer Perceptron Neural Network (MLP NN), Hidden Markov Model (HMM) may not be able to handle such big stream data directly [31] although they worked successfully in classification problem in many areas.", "startOffset": 192, "endOffset": 196}, {"referenceID": 6, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 20, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 40, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 11, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 14, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 15, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 2, "context": "We studied ACNNELM scheme for concept drift either changes in the number of feature inputs named virtual drift (VD) or the number of classes named real drift (RD) or consecutive drift when VD and RD occurred at the same time named hybrid drift (HD) [3] in recurrent context (all concepts occur alternately).", "startOffset": 249, "endOffset": 252}, {"referenceID": 1, "context": "We developed ACNNELM based on our previous work on adaptive ELM scheme named Adaptive OS-ELM (AOS-ELM) that works as single ELM classifier for CD handling [2, 3].", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "We developed ACNNELM based on our previous work on adaptive ELM scheme named Adaptive OS-ELM (AOS-ELM) that works as single ELM classifier for CD handling [2, 3].", "startOffset": 155, "endOffset": 161}, {"referenceID": 11, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 14, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 15, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 24, "context": "A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [26] has sequential learning phase.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "However, increasing the hidden nodes number may improve the performance named Constructive Enhancement OS-ELM [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 37, "context": "Another approach is Elastic Extreme Learning Machine (ELM) [40] or Parallel ELM [10] based on MapReduce framework to solve large sequential training data in a parallel way.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "Another approach is Elastic Extreme Learning Machine (ELM) [40] or Parallel ELM [10] based on MapReduce framework to solve large sequential training data in a parallel way.", "startOffset": 80, "endOffset": 84}, {"referenceID": 37, "context": "Therefore, ELM learning is efficient for rapidly massive training data set [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "ELM is more computing efficient, better performance and support parallel computation than OS-ELM [40], but ELM did not address the possibility for hidden nodes increased during the training.", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "In this paper, we developed parallelization framework to integrate with CNN to solve concept drift issues in sequential learning as enhancement from our previous work Adaptive OS-ELM (AOS-ELM) [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 33, "context": "CNN has excellent performance for spatial visual classification [36].", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "The key benefit of CNN comparing with another deep learning methods are using fewer parameters [43].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "1) [43].", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Many variants of CNN architectures in the literature, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 40, "context": "At the end of the CNN layer, there may be any densely connected NN layers for supervised learning [43].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "The convolution operations are heavy computation but inherently parallel, which getting beneficial from a hardware parallel implementation [35].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "However, the CNN network size is still limited mainly by the amount of memory available on current GPUs [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 40, "context": "CNN Architecture from AlexNet [43] .", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "[15] explained the ELM theories are not only valid for fully connected ELM architecture but are also actually valid for local connections, named local receptive fields (LRF) or similar with Kernel in CNN term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] implemented deep convolutional ELM (DC-ELM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] introduced an integration model of CNN-ELM, and applied to handwritten digit recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Structure of the deep hybrid CNN-ELM model [9] .", "startOffset": 43, "endOffset": 46}, {"referenceID": 23, "context": "[25] proposed an integration of CNN with OS-ELM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] and Minku [29] based on Bayesian decision theory for class output c and incoming data X.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[7] and Minku [29] based on Bayesian decision theory for class output c and incoming data X.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 5, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 10, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 34, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "Ensemble learning is the common approaches to tackle concept drift, in which are combined using a form of voting [38,45].", "startOffset": 113, "endOffset": 120}, {"referenceID": 42, "context": "Ensemble learning is the common approaches to tackle concept drift, in which are combined using a form of voting [38,45].", "startOffset": 113, "endOffset": 120}, {"referenceID": 39, "context": "The ensemble approach can integrate the results of individual classifiers into a unified predicted result to improve the accuracy and robustness than single classifiers [42].", "startOffset": 169, "endOffset": 173}, {"referenceID": 38, "context": "[41] proposed a general hybrid adaptive ensemble learning framework (See Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed an ensemble based ELM (EN-ELM) which uses the cross-validation scheme to build ELM classifiers ensemble.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The adaptive ensemble approach may be not practical and flexible if each member itself is not designed to be adaptive [3] and may need to recall the previous training data (not single pass learning [20]).", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "The adaptive ensemble approach may be not practical and flexible if each member itself is not designed to be adaptive [3] and may need to recall the previous training data (not single pass learning [20]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 26, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 28, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 28, "context": "[30] proposed OS-ELM for imbalanced and concept drift tackling named meta-cognitive OS-ELM (MOS-ELM) that was developed based on Weighted OS-ELM (WOS-ELM) [46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[30] proposed OS-ELM for imbalanced and concept drift tackling named meta-cognitive OS-ELM (MOS-ELM) that was developed based on Weighted OS-ELM (WOS-ELM) [46].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "Also, it can be applied for concept replacement and recurring [3] by using simple matrix adjustment and multiplication.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "According to interpolation theory from ELM point of view and Learning Principle I of ELM Theory [14], the input weight and bias as hidden nodes H parameters are independent of training samples and their learning environment through randomization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "According to universal approximation theory and inspired by the related works [18], the AOS-ELM has real drift capability by modifying the output matrix with zero block matrix concatenation to change the size matrix dimension without changing the norm value.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "[28] proposed some adapting strategies for single CNN classifier system based on two common adaptive approaches: 1) REUSE is to reuse the model upon the change of task and replace the old task training data with the new task training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "\u2019s experiment for a single data set; training methods in grey rounded boxes, represent models in circles, data instances in document shapes, and the evaluation method in white rounded box [28] .", "startOffset": 188, "endOffset": 192}, {"referenceID": 41, "context": "[44] proposed an adaptive CNN (ACNN), whose structure automatic expansion based on the average system error and training recognition rate performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 13, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 23, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 30, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 21, "context": "7159\u00d7 tanh( 23 \u00d7H) activation function [23] to have better generalization accuracy.", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "We used also CNN global expansion structure [44] to improve the performance accuracy.", "startOffset": 44, "endOffset": 48}, {"referenceID": 41, "context": "The Adaptive CNN architecture with Global expansion (2), Local expansion (3) and Incremental Learning (4) [44] .", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "Using this simple model, we have flexibilities to use reuse or reset strategy [28] for recurrent or sudden concept drift handling.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "The HOG additional attributes have been used in our research [3] to simulate VD scenario.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "posed based human action recognition [2].", "startOffset": 37, "endOffset": 40}], "year": 2016, "abstractText": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (ELM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels. We verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set. Keywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, online,concept drift", "creator": "LaTeX with hyperref package"}}}