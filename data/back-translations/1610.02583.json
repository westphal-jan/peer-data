{"id": "1610.02583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation", "abstract": "We describe recurrent neural networks (RNNs) that have drawn a lot of attention to sequential tasks such as handwriting recognition, speech recognition, and image to text. However, compared to general feedback neural networks, RNNs have feedback loops, which makes it a little difficult to understand the step of backpropagation. Therefore, we focus on the basics, in particular the backpropagation of errors to calculate gradients in terms of model parameters. Furthermore, we detail how the algorithm for backpropagation of errors to long-term memory (LSTM) is applied by deploying the memory unit.", "histories": [["v1", "Sat, 8 Oct 2016 21:10:40 GMT  (840kb,D)", "http://arxiv.org/abs/1610.02583v1", "9 pages"], ["v2", "Tue, 17 Oct 2017 04:48:14 GMT  (842kb,D)", "http://arxiv.org/abs/1610.02583v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1610.02583"}, "pdf": {"name": "1610.02583.pdf", "metadata": {"source": "CRF", "title": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation", "authors": ["Gang Chen"], "emails": ["gangchen@buffalo.edu,"], "sections": [{"heading": "A Gentle Tutorial of Recurrent Neural Network with Error", "text": "Backpropagation\nGang Chen \u2217\nDepartment of Computer Science and Engineering, SUNY at Buffalo"}, {"heading": "1 abstract", "text": "We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit."}, {"heading": "2 Sequential data", "text": "Sequential data is common in a wide variety of domains including natural language processing, speech recognition and computational biology. In general, it is divided into time series and ordered data structures. As for the time-series data, it changes over time and keeps consistent in the adjacent clips. For example the time frames for speech or video analysis, daily prices of stocks or the rainfall measurements on successive days. There are also ordered data in the sequence, such as text and sentence for handwriting recognition, and genes. For example, successfully predicting protein-protein interactions requires knowledge of the secondary structures of the proteins and semantic analysis might involve annotating tokens with parts of speech tags.\nThe goal of this work is for sequence labeling, i.e. classify all items in a sequence [4]. For example, in handwritten word recognition we wish to label a sequence of characters given features of the characters; in part of speech tagging, we wish to label a sequence of words.\nMore specifically, given an observation sequence {x1,x2, ...,xT } and its corresponding label {y1, y2, ..., yT }, we want to learn a map f : x 7\u2192 y. In the following, we will introduce RNNs to model the sequential data and give details on backpropagation over the feedback loop.\n\u2217gangchen@buffalo.edu, SUNY at Buffalo\nar X\niv :1\n61 0.\n02 58\n3v 1\n[ cs\n.L G\n] 8\nO ct\n2 01\n6"}, {"heading": "3 Recurrent neural networks", "text": "A RNN is a kind of neural networks, which can send feedback signals (to form a directed cycle), such as Hopfield net [1] and long-short term memory (LSTM) [2]. RNN models a dynamic system, where the hidden state ht is not only dependent on the current observation xt, but also relies on the previous hidden state ht\u22121. More specifically, we can represent ht as\nht = f(ht\u22121,xt) (1)\nwhere f is a nonlinear mapping. Thus, ht contains information about the whole sequence, which can be inferred from the recursive definition in Eq. 1. In other words, RNN can use the hidden variables as a memory to capture long term information from a sequence.\nSuppose that we have the following RNN model, such that\nht = tanh(Whhht\u22121 +Wxhxt + bh) (2)\nzt = softmax(Whzht + bz) (3)\nwhere zt is the prediction at the time step t, and tanh(x) is defined as\ntanh(x) = sinh(x) cosh(x) = ex \u2212 e\u2212x ex + e\u2212x = e2x \u2212 1 e2x + 1\nTake the derivative of tanh(x) w.r.t. x\n\u2202tanh(x) \u2202(x) = \u2202 sinh(x)cosh(x) \u2202x\n= \u2202sinh(x) \u2202x cosh(x)\u2212 sinh(x) \u2202cosh(x) \u2202x( cosh(x) )2 =\n[cosh(x)]2 \u2212 [sinh(x)]2( cosh(x) )2 =1\u2212 [tanh(x)]2 (4)\nMore specifically, the RNN model above has one hidden layer as depicted in Fig. 1. Notice that it is very easy to extend the one hidden case into multiple layers, which has been discussed in deep neural network before. Considering the varying length for each sequential data, we also assume the parameters in each time step are the same across the whole sequential analysis. Otherwise it will be hard to compute the gradients. In addition, sharing the weights for any sequential length can generalize the model well. As for sequential labeling, we can use the maximum likelihood to estimate model parameters. In other words, we can minimize the negative log likelihood the objective function\nL(x,y) = \u2212 \u2211 t ytlogzt (5)\nIn the following, we will use notation L as the objective function for simplicity. And further we will use L(t+ 1) to indicate the output at the time step t+ 1, s.t. L(t+ 1) = \u2212yt+1logzt+1. Let\u2019s first take the derivative with respect to zt\n\u2202L \u2202zt = \u2212(yt \u2212 zt) (6)\nNote the weight Whz is shared across all time sequence, thus we can differentiate to it at each time step and sum all together\n\u2202L \u2202Whz = \u2211 t \u2202L \u2202zt \u2202zt \u2202Whz\n(7)\nSimilarly, we can get the gradient w.r.t. bias bz\n\u2202L \u2202bz = \u2211 t \u2202L \u2202zt \u2202zt \u2202bz\n(8)\nNow let\u2019s go through the details to derive the gradient w.r.t. Whh. Considering at the time step t\u2192 t+ 1 in Fig. 1,\n\u2202L(t+ 1) \u2202Whh = \u2202L(t+ 1) \u2202zt+1 \u2202zt+1 \u2202ht+1 \u2202ht+1 \u2202Whh\n(9)\nwhere we only consider one step t\u2192 (t+1). And because the hidden state ht+1 partially dependents on ht, so we can use backpropagation to compute the above partial derivative. Think further Whh is shared cross the whole time sequence, according to the recursive definition in Eq. 2. Thus, at the time step (t\u2212 1)\u2192 t, we can further get the partial derivative w.r.t. Whh as follows\n\u2202L(t+ 1) \u2202Whh = \u2202L(t+ 1) \u2202zt+1 \u2202zt+1 \u2202ht+1 \u2202ht+1 \u2202ht \u2202ht \u2202Whh\n(10)\nThus, at the time step t+ 1, we can compute gradient w.r.t. zt+1 and further use backpropagation through time (BPTT) from t to 0 to calculate gradient w.r.t. Whh, shown as the red chain in Fig. 1. Thus, if we only consider the output zt+1 at the time step t+ 1, we can yield the following gradient w.r.t. Whh\n\u2202L(t+ 1) \u2202Whh\n= t\u2211\nk=1\n\u2202L(t+ 1) \u2202zt+1 \u2202zt+1 \u2202ht+1 \u2202ht+1 \u2202hk \u2202ht \u2202Whh\n(11)\nAggregate the gradients w.r.t. Whh over the whole time sequence with back propagation, we can finally yield the following gradient w.r.t. Whh\n\u2202L \u2202Whh = \u2211 t t+1\u2211 k=1 \u2202L(t+ 1) \u2202zt+1 \u2202zt+1 \u2202ht+1 \u2202ht+1 \u2202hk \u2202hk \u2202Whh\n(12)\nNow we turn to derive the gradient w.r.t. Wxh. Similarly, we consider the time step t + 1 (only contribution from xt+1) and calculate the gradient w.r.t. to Wxh as follows\n\u2202L(t+ 1) \u2202Wxh = \u2202L(t+ 1) \u2202ht+1 \u2202ht+1 \u2202Wxh\nBecause ht and xt+1 both make contribution to ht+1, we need to backpropagte to ht as well. If we consider the contribution from the time step t, we can further get\n\u2202L(t+ 1) \u2202Wxh = \u2202L(t+ 1) \u2202ht+1 \u2202ht+1 \u2202Wxh + \u2202L(t+ 1) \u2202ht \u2202ht \u2202Wxh\n= \u2202L(t+ 1) \u2202ht+1 \u2202ht+1 \u2202Wxh + \u2202L(t+ 1) \u2202ht+1 \u2202ht+1 \u2202ht \u2202ht \u2202Wxh\n(13)\nThus, summing up all contributions from t to 0 via backpropagation, we can yield the gradient at the time step t+ 1\n\u2202L(t+ 1) \u2202Wxh = t+1\u2211 k=1 \u2202L(t+ 1) \u2202ht+1 \u2202ht+1 \u2202hk \u2202hk \u2202Wxh\n(14)\nFurther, we can take derivative w.r.t. Wxh over the whole sequence as\n\u2202L \u2202Wxh = \u2211 t t+1\u2211 k=1 \u2202L(t+ 1) \u2202zt+1 \u2202zt+1 \u2202ht+1 \u2202ht+1 \u2202hk \u2202hk \u2202Wxh\n(15)\nHowever, there are gradient vanishing or exploding problems to RNNs. Notice that \u2202ht+1\u2202hk in Eq. 15 indicates matrix multiplication over the sequence. Because RNNs need to backpropagate gradients over a long sequence (with small values in the matrix multiplication), gradient value will shrink layer over layer, and eventually vanish after a few time steps. Thus, the states that are far away from the current time step does not contribute to the parameters\u2019 gradient computing (or parameters that RNNs is learning). Another direction is the gradient exploding, which attributed to large values in matrix multiplication.\nConsidering the weakness of RNNs, long short term memory (LSTM) was proposed to handle gradient vanishing problem [2]. Notice that RNNs makes use of a simple tanh function to incorporate the correlation between xt and ht\u22121 and ht, while LSTM model such correlation with a memory unit. And LSTM has attracted great attention for time series data recently and yielded significant improvement over RNNs. For example, LSTM has demonstrated very promising results on handwriting recognition task. In the following part, we will introduce LSTM, which introduces memory cells for the hidden states."}, {"heading": "3.1 Long short-term memory (LSTM)", "text": "The architecture of RNNs have cycles incorporating the activations from previous trim steps as input to the network to make a decision for the current input, which makes RNNs better suited for sequential labeling tasks. However, one vital problem of RNNs is the gradient vanishing problem. LSTM [2] extends the RNNs model with two advantages: (1) introduce the memory information (or cell) (2) handle long sequential better, considering the gradient vanishing problem, refer to Fig. 2 for the unit structure. In this part, we will introduce how to forward and backward LSTM neural network, and we will derive gradients and backpropagate error in details.\nThe core of LSTM is a memory unit (or cell) ct in Fig. 2, which encodes the information of the inputs that have been observed up to that step. The memory cell ct has the same inputs (ht\u22121 and xt) and outputs ht as a normal recurrent network, but has more gating units which control the information flow. The input gate and output gate respectively control the information input to the memory unit and the information output from the unit. More specifically, the output ht of the LSTM cell can be shut off via the output gate.\nAs to the memory cell itself, it is also controlled with a forget gate, which can reset the memory unit with a sigmoid function. More specifically, given a sequence data {x1, ...,xT } we have the gate\ndefinition as follows:\nft = \u03c3(Wxfxt +Whfht\u22121 + bf ) (16)\nit = \u03c3(Wxixt +Whiht\u22121 + bi) (17)\not = tanh(Wxoxt +Whoht\u22121 + bo) (18) ct = ft \u25e6 ct\u22121 + it \u25e6 gt (19) gt = \u03c3(Wxcxt +Whcht\u22121 + bc) (20) ht = ot \u25e6 tanh(ct), zt = softmax(Whzht + bz) (21)\nwhere ft indicates forget gate, it input gate, ot output gate and gt input modulation gate. Note that the memory unit models much more information than RNNs, except Eq. 18.\nSame as RNNs to make prediction, we can add a linear model over the hidden state ht, and output the likelihood with softmax function\nzt = softmax(Whzht + bz)\nIf the groundtruth at time t is yt, we can consider minimizing least square 1 2(yt\u2212zt) 2 or cross entroy to estimate model parameters. Thus, for the top layer classification with weight Whz, we can take derivative w.r.t. zt and Whz respectively\ndzt = yt \u2212 zt (22) dWhz = \u2211 t htdzt (23)\ndhT = WhzdzT (24)\nwhere we only consider the gradient w.r.t. hT at the last time step T . For any time step t, its gradient will be a little different, which will be introduced later (refer to Eq. 34).\nthen backpropagate the LSTM at the current time step t\ndot = tanh(ct)dht (25) dct = ( 1\u2212 tanh(ct)2 ) otdht (26)\ndft = ct\u22121dct (27) dct\u22121 + = ft \u25e6 dct (28) dit = gtdct (29)\ndgt = itdct (30)\nwhere the gradient w.r.t. tanh(ct) in Eq. 26 can be derived according to Eq. 4.\nfurther, back-propagate activation functions over the whole sequence dWxo = \u2211 t (1\u2212 o2t )xtdot\ndWxi = \u2211 t it(1\u2212 it)xtdit\ndWxf = \u2211 t ft(1\u2212 ft)xtdft\ndWxc = \u2211 t gt(1\u2212 gt)xtdgt (31)\nNote that the weights Wxo, Wxi, Wxf and Wxc are shared across the whole sequence, thus we need to take the same summation over t as RNNs in Eq. 7. Similarly, we have\ndWho = \u2211 t (1\u2212 o2t )ht\u22121dot\ndWhi = \u2211 t it(1\u2212 it)ht\u22121dit\ndWhf = \u2211 t ft(1\u2212 ft)ht\u22121dft\ndWhc = \u2211 t gt(1\u2212 gt)ht\u22121dgt (32)\nand corresponding hiddens at the current time step t\u2212 1\ndht\u22121 = (1\u2212 o2t )Whodot + it(1\u2212 it)Whidit + ft(1\u2212 ft)Whfdft + gt(1\u2212 gt)Whcdgt (33)\ndht\u22121 = dhk\u22121 +Whzdzt\u22121 (34)\nwhere we consider two sources to derive dht\u22121, one is from activation function in Eq. 33 and the other is from the objective function at the time step t in Eq. 24."}, {"heading": "3.2 Error backpropagation", "text": "In this part, we will give detail information on how to derive gradients, especially Eq. 28. As we talked about RNNs before, we can take the same strategy to unfold the memory unit, shown in Fig. 3. Suppose we have the least square objective function\nL(x, \u03b8) = min \u2211 t 1 2 (yt \u2212 zt)2 (35)\nwhere \u03b8 = {Whz,Wxo,Wxi,Wxf ,Wxc,Who,Whi,Whf ,Whc} with biases ignored. To make it easy to understand in the following, we use L(t) = 12(yt \u2212 zt) 2.\nAt the time step T , we take derivative w.r.t. cT\n\u2202L(T ) \u2202cT = \u2202L(T ) \u2202hT \u2202hT \u2202cT\n(36)\nAt the time step T \u2212 1, we take derivative of L(T \u2212 1) w.r.t. cT\u22121 as\n\u2202L(T \u2212 1) \u2202cT\u22121 = \u2202L(T \u2212 1) \u2202hT\u22121 \u2202hT\u22121 \u2202cT\u22121\n(37)\nHowever, according to Fig. 3, the error is not only backpropagated via L(T \u2212 1), but also from cT , thus the final gradient w.r.t. cT\u22121\n\u2202L(T \u2212 1) \u2202cT\u22121 = \u2202L(T \u2212 1) \u2202cT\u22121 + \u2202L(T ) \u2202cT\u22121 \u2202L(T \u2212 1) \u2202cT\u22121 = \u2202L(T \u2212 1) \u2202hT\u22121 \u2202hT\u22121 \u2202cT\u22121 + \u2202L(T ) \u2202hT \u2202hT \u2202cT \u2202cT \u2202cT\u22121\n(38)\nwhere we use the chain rule in Eq. 38. Further, we can rewrite Eq. 38 as\ndcT\u22121 = dcT\u22121 + fT \u25e6 dcT (39)\nIn a similar manner, we can derive Eq. 28 at any time step."}, {"heading": "3.3 Parameters learning", "text": "Forward: we can use Eqs. 16-21 to update states as the feedforward neural network from the time step 1 to T .\nBackword: we can backpropagate the error from T to 1 via Eqs. 25-34. After we get gradients using backpropagation, the model \u03b8 can be learnt with gradient based methods, such as stochastic gradient descent and L-BFGS). If we use stochastic gradient descent (SGD) to update \u03b8 = {Whz,Wxo,Wxi,Wxf ,Wxc,Who,Whi,Whf ,Whc}, then we have\n\u03b8 = \u03b8 + \u03b7d\u03b8 (40)\nwhere \u03b7 is the learning rate. Notice that more tricks can be applied here."}, {"heading": "4 Applications", "text": "RNNs can be used to handle sequential data, such as speech recognition. RNNs can also be extended into multiple layers in a bi-directional manner. Moreover, RNNs can be combines with other neural networks [3], such as convolutional neural network to handle video to text problem, as well as combining two LSTMs for machine translation."}], "references": [{"title": "Neural Networks and Physical Systems with Emergent Collective Computational Abilities", "author": ["J.J. Hopfield"], "venue": "In Neurocomputing: Foundations of Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Comput.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Generalized K-fan Multimodal Deep Model with Shared Representations", "author": ["G. Chen", "S.N. Srihari"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Machine Learning: Basics, Models and Trends, 2016", "author": ["Gang Chen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "classify all items in a sequence [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "A RNN is a kind of neural networks, which can send feedback signals (to form a directed cycle), such as Hopfield net [1] and long-short term memory (LSTM) [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "A RNN is a kind of neural networks, which can send feedback signals (to form a directed cycle), such as Hopfield net [1] and long-short term memory (LSTM) [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "Considering the weakness of RNNs, long short term memory (LSTM) was proposed to handle gradient vanishing problem [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "LSTM [2] extends the RNNs model with two advantages: (1) introduce the memory information (or cell) (2) handle long sequential better, considering the gradient vanishing problem, refer to Fig.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "Moreover, RNNs can be combines with other neural networks [3], such as convolutional neural network to handle video to text problem, as well as combining two LSTMs for machine translation.", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.", "creator": "LaTeX with hyperref package"}}}