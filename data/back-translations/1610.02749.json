{"id": "1610.02749", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "A Dynamic Window Neural Network for CCG Supertagging", "abstract": "The Combinatory Category Grammar (CCG) function is a task for assigning lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input characteristics. However, it is obvious that different tags are usually based on different context window sizes, which motivates us to build a supertagger with a dynamic window approach that can be applied as an attention mechanism to local contexts. Applying dropouts to the dynamic filters can be seen directly as a drop-out step on words, which is superior to the regular dropout for word embedding. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.", "histories": [["v1", "Mon, 10 Oct 2016 01:47:50 GMT  (783kb,D)", "http://arxiv.org/abs/1610.02749v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huijia wu", "jiajun zhang", "chengqing zong"], "accepted": true, "id": "1610.02749"}, "pdf": {"name": "1610.02749.pdf", "metadata": {"source": "CRF", "title": "A Dynamic Window Neural Network for CCG Supertagging", "authors": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong"], "emails": ["huijia.wu@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "Introduction", "text": "Combinatory Category Grammar (CCG) provides a connection between syntax and semantics of natural language. The syntax can be specified by derivations of the lexicon based on the combinatory rules, and the semantics can be recovered from a set of predicateargument relations. CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance. All these motivate us to build a more accurate CCG supertagger.\nCCG supertagging is the task to predict the lexical categories for each word in a sentence. Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window. This fixed size window assumption is\ntoo strong to generalize. We argue this from two perspectives.\nOne perspective comes from the inputs. For a particular word, the number of its categories may vary from 1 to 130 in CCGBank 02-21 (Hockenmaier and Steedman, 2007). We need to choose different context window sizes to meet different ambiguity levels. The other perspective is for the targets. There are about 21000 different words together with 31 different Part-Of-Speech(POS) tags which have the same category N/N . Using the same context window size for each word is obviously inappropriate.\nTo overcome these problems, we notice that Xu et al. (2015) use dropout in the embedding layer to make the input contexts sparse. This method motivates us to get rid of the unnecessary information in the contexts automatically rather than use a pre-specified prior. Then we observe that the gating mechanism of long short-term memory (LSTM) blocks, especially the input gate, can determine when to enter into the block. All these inspire us to add a gate to each item in the context windows to make them sparse but informative.\nar X\niv :1\n61 0.\n02 74\n9v 1\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 6\nThis method is naturally an extension to the encoderdecoder with the attention mechanism (Bahdanau et al., 2014), which can be interpreted as focusing on parts of the memories when making decisions. From this perspective, the contexts of the current word are the memories, and the dynamic window is the attention. We focus on the contexts extracted from the attended windows to predict the corresponding lexical categories.\nFigure 1 visualize this method. We add one logistic gate to each item in the input contexts. If the gate is close to zero, the corresponding features in the window will be ignored during training. Moreover, we add a dropout mask on the gates to further improve its sparsity, which can be treated as a word-level dropout. Combining attention and dropout lead a significant performance improvement.\nWe evaluated our approach on multilayer perceptrons (MLPs) and and recurrent neural networks (RNNs), including vanilla forms (standard RNNs) and gated RNNs. The experiments show that the performance of these networks can obtain improvements using our method, and can outperform all the reported results to date."}, {"heading": "Background", "text": ""}, {"heading": "Category Notation", "text": "CCG uses a set of lexical categories to represent constituents (Steedman, 2000). In particular, a fixed finite set is used as a basis for constructing other categories, which is described in Table 1.\nThe basic categories could be used to generate an infinite set C of functional categories by applying the following recursive definition:\n\u2022 N,NP, PP, S \u2208 C\n\u2022 X/Y,X\\Y \u2208 C if X,Y \u2208 C\nEach functional category specifies some arguments. Combining the arguments can form a new category according to the orders (Steedman and Baldridge, 2011). The argument could be either basic or functional, and the orders are determined by the forward slash / and the backward slash \\. A category X/Y is a forward functor which could accept an argument Y to the right and get X , while the backward functor X\\Y should accept its argument Y to the left."}, {"heading": "Neuron Based Supertaggers", "text": "CCG supertagging is an approach for assigning lexical categories for each word in a sentence. The problem can be formulated by P (c|w;\u03b8), where w = [w1, . . . , wT ] indicates the T words in a sentence, and c = [c1, . . . , cT ] indicates the corresponding lexical categories. Notice that the length of the words and categories are the same. We denote vectors with bolded font and matrices with capital letters. Bias terms in neural networks are omitted for readability.\nNetwork Inputs. Network inputs are the representation of each token in a sequence. Our inputs include the concatenation of word representations, character representations, and capitalization representations. To reduce sparsity, all words are lower-cased, together with capitalization and character representations as inputs.\nFormally, we can represent the distributed word feature fwt using a concatenation of these embeddings:\nfwt = [Lw(wt);La(at);Lc(cw)] (1)\nwhere wt, at represent the current word and its capitalization. cw := [c1, c2, . . . , cTw ], where Tw is the length of the word and ci, i \u2208 {1, . . . , Tw} is the i-th character for the particular word. Lw(\u00b7) \u2208 R|Vw|\u00d7n, La(\u00b7) \u2208 R|Va|\u00d7m and Lc(\u00b7) \u2208 R|Vc|\u00d7r are the look-up tables for the words, capitalization and characters, respectively. fwt \u2208 Rn+m+r represents the distributed feature of wt. A context window of size d surrounding the current word is used as an input:\nxt = [fwt\u2212bd/2c ; . . . ; fwt+bd/2c ] (2)\nwhere xt \u2208 R(n+m+r)\u00d7d is the concatenation of the context features. We use it as the input of the network."}, {"heading": "Network Outputs.", "text": "Since our goal is to assign CCG categories to each word, we use a softmax activation function g(\u00b7) in the output layer:\nyt = g(W hyht) (3)\nwhere yt \u2208 RK is a probability distribution over all possible categories. yk(t) =\nexp(hk)\u2211 k\u2032 exp(hk\u2032 )\nis the k-th dimension of yt, which corresponds to the k-th lexical category in the lexicon. ht \u2208 RH is the output of the hidden layer at time t. Why \u2208 RK\u00d7H is the hidden-to-output weight."}, {"heading": "Inputs to Outputs Mappings.", "text": "Neuron based supertaggers model the inputs to outputs mappings using neural networks. Since CCG supertagging can either be treated as a point or a sequential estimation problem, which correspond to two kinds of neural\nnetworks: MLPs and RNNs, respectively. For simplicity, we will talk about gated RNNs only, which are special kinds of RNNs with logistic gates in the hidden units to control information flow. There are many kinds of gated RNNs, such as long short-term memory, (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al., 2014). We focus on LSTMs only in this work.\nLSTMs replace the hidden units in vanilla RNNs with complicated blocks, which are designed as:\nc\u0303t = \u03c3(W xcxt +W hcht\u22121) (4) ct = ft ct\u22121 + it c\u0303t (5) ht = ot f(ct) (6)\nwhere xt and ht are the input and the output of the block. it \u2208 RH , ft \u2208 RH and ot \u2208 RH denote the input gate, forget gate and output gate, respectively, which are logistic units to filter the information. Wxc \u2208 RH\u00d7I is the weight storing the input. Whc \u2208 RH\u00d7H is the recurrent weight connecting the previous block outputs to the cells. ct \u2208 RH is the short-term memory state, which is used to store the history information. Based on the three gates, the information flow in ct can be kept for a long time. f(\u00b7) is the non-linear mapping, here we use the hyperbolic tangent function f(z) = e\nz\u2212e\u2212z ez+e\u2212z ."}, {"heading": "A Dynamic Window Approach", "text": "In this section we will introduce a dynamic window approach for supertagging. Specifically, we add logistic gates to each token in the context window to filter the unnecessary information. We can modify the Eq. (2) to:\nx\u0303t = rt \u2297 xt (7) := [rt\u2212bd/2cfwt\u2212bd/2c ; . . . ; rt+bd/2cfwt+bd/2c ] (8)\nHere \u2297 denote an element-wise scalar-vector product. rt := [rt\u2212bd/2c, . . . , rt+bd/2c] \u2208 Rd is a logistic gate to filter the unnecessary contexts. ri and fi, i \u2208 {t \u2212 bd/2c, . . . , t + bd/2c} is a scalar and a vector, respectively. Their product is defined as:\nrf := [rf1, . . . , rfn]\nOne perspective for the filter gate is an attention model focusing on the necessary contexts. This effect can be visualized in Figure 1: If one component of r, say ri is 0, the corresponding word feature fwi will be removed or deactivated from the input."}, {"heading": "Design of the Gates", "text": "We use a feed-forward neural network to learn rt:\nrt = \u03c3(W xrxt) (9)\nwhere \u03c3(\u00b7) is the sigmoid function defined as \u03c3(z) = 1 1+e\u2212z . This function is to make sure the values of rt are between 0 and 1. xt is network input, as defined in Eq. (2). rt \u2208 Rd is the output of the dynamic window model, where d is the window size. Wxr \u2208 Rd\u00d7I is the weight to be learned.\nOne disadvantage of the sigmoid function is when a neuron is nearly saturated, its derivative becomes small, which makes the connecting weights change very slowly. If some neurons in rt are saturated, which states will be stable and may not generalize well. To further improve the sparsity in the context window, we add a dropout mask (Srivastava et al., 2014) on rt:\nli(t) \u223c Bernouli(p) (10) r\u0303t = lt rt (11)\nwhere lt is a vector of independent Bernouli random variables li(t), which has probability p of being 1. Since r\u0303t acts on each word feature in the context window, this dropout can be viewed as drop on words directly (Dai and Le, 2015). One minor difference is they use word dropout at the sentence level, and we use it at the dynamic window level.\nTo further explain it, let\u2019s consider a trivial case when all items in rt are set to 1:\nrt = [1, . . . , 1] (12)\nAdding a dropout mask on such a rt is equivalent to drop the words in the contexts randomly, which can be seen as a window approach with a random size."}, {"heading": "Embedded into MLPs", "text": "For MLPs with this approach, we can use the filtered context as an input:\nht = f(W xhx\u0303t) (13)\nyt = g(W hyht) (14)\nwhere x\u0303t is defined in Eq. (7). We use the filtered context as an input to the hidden layer. Wxh \u2208 RH\u00d7I and Why \u2208 RK\u00d7H is the weight parameters of MLP."}, {"heading": "Embedded into Vanilla RNNs", "text": "The similar approach can be applied to RNNs with slight modifications. For each hidden state we have two types of inputs: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986). The recurrent weight may vanish or explode if its eigenvalues are deviated from 1. To avoid these problems, we add one gate to the recurrent input to reset it:\nr\u0303t = \u03c3(W xrxt) (15) st = \u03c3(W xsxt) (16)\nwhere st \u2208 R is a scalar between 0 and 1, which is used to reset the recurrent input. Wxs \u2208 R1\u00d7I is the corresponding hidden-to-output weight. Intuitively, if st is close to zero, the recurrent input Wycyt\u22121 will be disappeared, which degenerates to a MLP.\nTaken a Jordan-type RNN as an example, we have:\nh\u0303t = f(W xhx\u0303t + stW yhyt\u22121) (17)\nyt = g(W hyh\u0303t) (18)\nwhere h\u0303t \u2208 RH is the output of the hidden layer. x\u0303t is the current input. yt\u22121 \u2208 RK is the previous output of the output layer. Wyh \u2208 RH\u00d7K is the recurrent weight from the previous output layer to the current hidden layer."}, {"heading": "Embedded into Gated RNNs", "text": "For gated RNNs, we use a two-stacked bidirectional LSTM (bi-LSTM) to model the task. The architecture can be defined as follows:\n\u2212\u2192 ht = LSTM(\u2212\u2192xt, \u2212\u2212\u2192 ht\u22121, \u2212\u2212\u2192ct\u22121) (19) \u2190\u2212 ht = LSTM(\u2190\u2212xt, \u2190\u2212\u2212 ht\u22121, \u2190\u2212\u2212ct\u22121) (20) yt = g( \u2212\u2192 ht, \u2190\u2212 ht) (21)\nwhere LSTM(\u00b7) is the LSTM computation. \u2212\u2192xt and \u2190\u2212xt are the forward and the backward input sequence, respectively. The output of the two hidden layers \u2212\u2192 ht and \u2190\u2212 ht in a birectional LSTM are stacked on top of each other:\nhlt = f l(hl\u22121t ,h l t\u22121) (22)\nwhere hlt is the t-th hidden state of the l-th layer."}, {"heading": "Discussion", "text": "The main idea of the attention mechanism is to focus on parts of the memories, which are used to store the information for prediction, such as the inputs or the hidden units. From this perspective, our dynamic window method can be seen as an attention-based system. Moreover, supertagging is a special kind of sequence-tosequence problem, in which the input and the output sequence has the same length. Thus, we do not need to use an encoder to memorize the input and use another decoder to generate the output.\nThe difference between the two attention mechanisms lies in the type of memories. In the encoder-decoder architecture, the attention model is considered through a weighted average of the output of the encoder. The reason is that they use a encoder and a decoder to model the variable-length outputs, and the memories are the encoder hidden states, while in the supertagging problem, we only use a encoder to do the task, our memories are just the inputs."}, {"heading": "Experiments", "text": "We divide our experiments into two steps: First we make comparisons with the existing approaches to test the performance of our models. The comparisons do not include any externally labeled data and POS labels. Then we describe quantitative results which validate the effectiveness of our dynamic window approach. We conduct experiments on MLPs and RNNs with and without this method for comparisons."}, {"heading": "Dataset and Pre-Processing", "text": "Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage 99.4%. We follow the standard splits, using sections 02-21 for training, section 00 for development and section 23 for the test. We use a full category set containing 1285 tags. All digits are mapped into the same digit \u20189\u2019, and all words are lowercased."}, {"heading": "Network Configuration", "text": ""}, {"heading": "Initialization.", "text": "There are two types of weights in our experiments: recurrent and non-recurrent weights. For non-recurrent weights, we initialize word embeddings with the pretrained 200-dimensional GolVe vectors (Pennington et al., 2014). Other weights are initialized with the Gaussian distributionN (0, 1\u221a\nfan-in ) scaled by a factor of 0.1, where\nfan-in is the number of units in the input layer. For recurrent weight matrices, we initialize with random orthogonal matrices through SVD (Saxe et al., 2013)to avoid unstable gradients. Orthogonal initialization for recurrent weights is important in our experiments, which takes about 2% relative performance gain than other methods such as Xavier initialization (Glorot and Bengio, 2010)."}, {"heading": "Hyperparameters.", "text": "For MLPs, we use a window size of 4, while for vanilla RNNs and gated RNNs, a window size of 1 is enough to capture the local contexts. The dimension of the word embeddings is 200. The size of character embedding and capitalization embeddings are set to 5. We set the maximum number of a word\u2019s characters to 5 to eliminate complexity, which means we concatenate the leftmost 5 characters and the rightmost 5 characters as character representations. The number of cells of the stacked bi-LSTM is set to 512. We also tried 400 cells or 600 cells and found this number did not impact performance so much. All stacked hidden layers have the same number of cells. The output layer has 1286 neurons, which equals to the number of tags in the training set with a RARE symbol."}, {"heading": "Training.", "text": "We train the networks using the back-propagation algorithm, using stochastic gradient descent (SGD) algorithm with an equal learning rate 0.02 for all layers. We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD. Gradient clipping is not used. We use on-line learning in our experiments, which means the parameters will be updated on every training sequences, one at a time.\nWe use a negative log-likelihood cost to evaluate the performance. Given a training set {(xn, tn)Nn=1}, the objective function can be written as:\nC = \u2212 1 N N\u2211 n=1 logytn (23)\nwhere tn \u2208 N is the true target for sample n, and ytn is the t-th output in the softmax layer given the inputs xn."}, {"heading": "Regularization.", "text": "Dropout is the only regularizer in our model to avoid overfitting. We add a dropout mask to our filter gate rt with a drop rate 0.5, which is helpful to improve the performance. Figure 2 shows such comparisons on a forward LSTM. The behavior of other kinds of neural networks are similar. We can see that dropout on the words directly Eq. (12) is slightly better than the dynamic window without dropout Eq. (9), while dropout on the dynamic window lead to a much better improvement. We also apply dropout to the output of the hidden layer with a 0.5 drop rate. At test time, weights are scaled with a factor 1\u2212 p."}, {"heading": "Results on Supertagging Accuracy", "text": "We report the highest 1-best supertagging accuracy on the development set for final testing. Table 2 shows the comparisons of accuracy on CCGBank. We notice that stacked bi-LSTM (with depth 2) performs the best than other neural based models. Our dynamic window approach provides the highest (+11%) relative performance gain. The character-level information (+ 6% relative accuracy) and dropout (+ 8% relative accuracy) are also helpful to improve the performance. We observe that dropout on words is superior to dropout on word embeddings."}, {"heading": "On the Usage of Dynamic Filters", "text": "We experiment with a 1\u00d7 1 convolution operation on xt. The performance is 94.05% (Table 3, line 2), which indicates that the convolution is recommended to operate on words directly, rather than on word embeddings. This can be formulated as:\nx\u0303t = rt xt (24) := [rt\u2212bd/2c fwt\u2212bd/2c ; . . . ; rt+bd/2c fwt+bd/2c ]\n(25)\nHere denote an element-wise product. We can use a MLP instead of a one-layer network to\nlearn the dynamic filters:\nut = \u03c3(W xuxt) (26) rt = \u03c3(W urut) (27)\nwhere we add a hidden layer ut \u2208 Ru to learn rt. But the performance is 94.23% (Table 3, line 3) with an extra computational cost.\nThe weighted average on the inputs \u2211d\ni=1 rifi is a standard method for attention, which leads to a poor result of 93.95% (Table 3, line 4). This shows the gated concatenation might be superior to the weighted average in the context of sequence tagging."}, {"heading": "Effects of the Dynamic Window Approach", "text": "Figure 3 shows the effectiveness of our dynamic window approach. Taken a forward LSTM as an example, we can observe at first 20 epochs the LSTM + dyn performs worse than the original LSTM model since many useful input contexts are filtered using this approach, but after 20 epochs the LSTM starts to overfitting while the performance of the LSTM + dyn continues to raise up."}, {"heading": "Visualizations", "text": "Our model uses filter gates to dynamically choose the needed contexts. To understand this mechanism, we randomly choose some words to visualize the dynamic activities during training. All the visualizations are done using an MLP on CCGBank Section 02-21.\nFigure 4 shows the different dynamic activities for the words. Each sub-figure has 9 blocks (a window size of 4), each of which shows an activation of one filter li(t), i \u2208 {1, . . . , 9}. After training to convergence, the items far away from the middle words are gradually removed from the contexts, while the attentions are gradually focused on the items nearby the central words. We can observe that for different words, their dynamic activities are different."}, {"heading": "Related Work", "text": "Clark and Curran (2007) use a log-linear model to build the supertagger, using discrete feature functions for the\ntargets based on the words and POS tags. The discrete property of the model makes the features independent of each other. Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al. (2010). Without using POS tags, they use the per-trained word embeddings with 2-character suffix and capitalization as features to represent the word. This distributed embedding encodes the word similarities and provides a better representation than log-linear models. However, MLP based supertagger ignores the sequential information, and their CRF based model can capture this but takes far more computational complexity than the MLP model due to the huge number of supertags.\nThe supertaggers based on log-linear and MLP are all point estimators, while CCG supertagging is more suitable to be treated as a sequential estimation problem due to long-range dependencies of the predicate-argument relations contained in lexical categories. Recently, Xu et al. (2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs. The recurrent matrix in RNN can restore the historical information, which makes it outperform the MLP based model. But RNNs may suffer from the gradient vanishing/exploding problems and are not good at capturing long-range dependencies in practice. Vaswani et al. (2016) and Lewis et al. (2016) shows the effectiveness of bi-LSTMs in supertagging, but they do not use a context window for the inputs. We only get 93.9% performance on the development set without using context windows. We find that a window size of 1 is needed in the stacked bi-LSTM to get a better performance.\nOur model can be treated as a marriage between atten-\ntion mechanism and dropout. The most relevant attentionbased models relating to our work is Wang et al. (2015), in which they use an attention model to find the relevant words within the context for predicting the center word. Their attention mechanism is similar to Bahdanau et al. (2014), while ours was not originally designed as a weighted average but a gated concatenation. Dropout on the dynamic window is similar to (Dai and Le, 2015), which randomly drop words in the input sentences. Gal (2015) also use dropout on words, but using a fixed mask rather a random one."}, {"heading": "Conclusion", "text": "We presented a dynamic window approach for CCG supertagging. Our model uses logistic gates to filter the context window surrounding the center word. This attention mechanism shows effectiveness on both MLPs and RNNs. We observed that using dropout on the dynamic window will greatly improve the generalization performance. We further visualized the activation of the filters, which is useful to help us understanding the dynamic activities. Although our work mainly focus on the CCG supertagging, this method can be easily applied to other sequence tagging tasks, such as POS tagging and named entity recognition (NER)."}], "references": [{"title": "Broadcoverage CCG semantic parsing with amr", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699\u20131710. Association for Computational Linguistics.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Johan Bos", "Stephen Clark", "Mark Steedman", "James R Curran", "Julia Hockenmaier."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 1240. Association for Computational Linguistics.", "citeRegEx": "Bos et al\\.,? 2004", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Towards wide-coverage semantic interpretation", "author": ["Johan Bos."], "venue": "Proceedings of Sixth International Workshop on Computational Semantics IWCS, volume 6, pages 42\u201353.", "citeRegEx": "Bos.,? 2005", "shortCiteRegEx": "Bos.", "year": 2005}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Proceedings of the 2008 Conference on Semantics in Text", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Wide-coverage efficient statistical parsing with ccg and log-linear models", "author": ["Stephen Clark", "James R Curran."], "venue": "Computational Linguistics, 33(4):493\u2013552.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3061\u20133069.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal."], "venue": "arXiv preprint arXiv:1512.05287.", "citeRegEx": "Gal.,? 2015", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats, volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Lstm can solve hard long time lag problems", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Advances in neural information processing systems, pages 473\u2013479.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Ccgbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics, 33(3):355\u2013396.", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Attractor dynamics and parallellism in a connectionist sequential machine", "author": ["Michael I Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q1986\\E", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Inducing probabilistic CCG grammars from logical form with higher-order unification", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1223\u20131233. Association", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1512\u20131523. Association for Computational", "citeRegEx": "Kwiatkowski et al\\.,? 2011", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Combining distributional and logical semantics", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 1:179\u2013192.", "citeRegEx": "Lewis and Steedman.,? 2013", "shortCiteRegEx": "Lewis and Steedman.", "year": 2013}, {"title": "Improved CCG parsing with semi-supervised supertagging", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:327\u2013338.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Lstm ccg parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir", "Ram\u00f3n Fernandez Astudillo", "Chris Dyer", "Alan W Black", "Isabel Trancoso."], "venue": "EMNLP.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u201343.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Experiments on learning by back propagation", "author": ["David C Plaut"], "venue": null, "citeRegEx": "Plaut,? \\Q1986\\E", "shortCiteRegEx": "Plaut", "year": 1986}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Combinatory categorial grammar", "author": ["Mark Steedman", "Jason Baldridge."], "venue": "Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Blackwell.", "citeRegEx": "Steedman and Baldridge.,? 2011", "shortCiteRegEx": "Steedman and Baldridge.", "year": 2011}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Supertagging with lstms", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "CCG supertagging with a recurrent neural network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Volume 2: Short Papers, page 250.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Expected f-measure training for shift-reduce parsing with recurrent neural networks", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Proceedings of NAACL-HLT, pages 210\u2013220.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Online learning of relaxed CCG grammars for parsing to logical form", "author": ["Luke S Zettlemoyer", "Michael Collins."], "venue": "EMNLP-CoNLL, pages 678\u2013687.", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}], "referenceMentions": [{"referenceID": 34, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 16, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 17, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 0, "context": "CCG provides an elegant solution for a wide range of semantic analysis, such as semantic parsing (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Artzi et al., 2015), semantic representations (Bos et al.", "startOffset": 97, "endOffset": 200}, {"referenceID": 2, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 3, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 4, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 18, "context": ", 2015), semantic representations (Bos et al., 2004; Bos, 2005; Bos, 2008; Lewis and Steedman, 2013), and semantic compositions, all of which heavily depend on the supertagging and parsing performance.", "startOffset": 34, "endOffset": 100}, {"referenceID": 6, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al.", "startOffset": 68, "endOffset": 118}, {"referenceID": 19, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al.", "startOffset": 68, "endOffset": 118}, {"referenceID": 31, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 20, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 30, "context": "Existing algorithms on CCG supertagging range from point estimation (Clark and Curran, 2007; Lewis and Steedman, 2014) to sequential estimation (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016), which predict the most probable supertag of the current word according to the context in a fixed size window.", "startOffset": 144, "endOffset": 203}, {"referenceID": 13, "context": "For a particular word, the number of its categories may vary from 1 to 130 in CCGBank 02-21 (Hockenmaier and Steedman, 2007).", "startOffset": 92, "endOffset": 124}, {"referenceID": 31, "context": "To overcome these problems, we notice that Xu et al. (2015) use dropout in the embedding layer to make the input contexts sparse.", "startOffset": 43, "endOffset": 60}, {"referenceID": 1, "context": "This method is naturally an extension to the encoderdecoder with the attention mechanism (Bahdanau et al., 2014), which can be interpreted as focusing on parts of the memories when making decisions.", "startOffset": 89, "endOffset": 112}, {"referenceID": 28, "context": "CCG uses a set of lexical categories to represent constituents (Steedman, 2000).", "startOffset": 63, "endOffset": 79}, {"referenceID": 27, "context": "Combining the arguments can form a new category according to the orders (Steedman and Baldridge, 2011).", "startOffset": 72, "endOffset": 102}, {"referenceID": 12, "context": "There are many kinds of gated RNNs, such as long short-term memory, (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al.", "startOffset": 68, "endOffset": 102}, {"referenceID": 5, "context": "There are many kinds of gated RNNs, such as long short-term memory, (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Cho et al., 2014).", "startOffset": 128, "endOffset": 146}, {"referenceID": 26, "context": "To further improve the sparsity in the context window, we add a dropout mask (Srivastava et al., 2014) on rt:", "startOffset": 77, "endOffset": 102}, {"referenceID": 8, "context": "Since r\u0303t acts on each word feature in the context window, this dropout can be viewed as drop on words directly (Dai and Le, 2015).", "startOffset": 112, "endOffset": 130}, {"referenceID": 9, "context": "For each hidden state we have two types of inputs: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986).", "startOffset": 109, "endOffset": 122}, {"referenceID": 14, "context": "For each hidden state we have two types of inputs: one is from the input layer, the other is from the hidden (Elman, 1990) or the output layer (Jordan, 1986).", "startOffset": 143, "endOffset": 157}, {"referenceID": 13, "context": "Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al.", "startOffset": 41, "endOffset": 73}, {"referenceID": 22, "context": "Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage 99.", "startOffset": 117, "endOffset": 138}, {"referenceID": 23, "context": "For non-recurrent weights, we initialize word embeddings with the pretrained 200-dimensional GolVe vectors (Pennington et al., 2014).", "startOffset": 107, "endOffset": 132}, {"referenceID": 25, "context": "For recurrent weight matrices, we initialize with random orthogonal matrices through SVD (Saxe et al., 2013)to avoid unstable gradients.", "startOffset": 89, "endOffset": 108}, {"referenceID": 11, "context": "Orthogonal initialization for recurrent weights is important in our experiments, which takes about 2% relative performance gain than other methods such as Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 177, "endOffset": 202}, {"referenceID": 33, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 94, "endOffset": 108}, {"referenceID": 15, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 118, "endOffset": 139}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al.", "startOffset": 0, "endOffset": 133}, {"referenceID": 18, "context": "Lewis and Steedman (2014) propose a semisupervised supertagging model using a multi-layer perceptron (MLP) based on Collobert (2011) and conditional random field (CRF) proposed by Turian et al. (2010). Without using POS tags, they use the per-trained word embeddings with 2-character suffix and capitalization as features to represent the word.", "startOffset": 0, "endOffset": 201}, {"referenceID": 28, "context": "Recently, Xu et al. (2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs.", "startOffset": 10, "endOffset": 27}, {"referenceID": 9, "context": "(2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs. The recurrent matrix in RNN can restore the historical information, which makes it outperform the MLP based model. But RNNs may suffer from the gradient vanishing/exploding problems and are not good at capturing long-range dependencies in practice. Vaswani et al. (2016) and Lewis et al.", "startOffset": 17, "endOffset": 385}, {"referenceID": 9, "context": "(2015) design an Elman-type RNN to capture these dependencies, and use a fixed size window for each word as MLPs. The recurrent matrix in RNN can restore the historical information, which makes it outperform the MLP based model. But RNNs may suffer from the gradient vanishing/exploding problems and are not good at capturing long-range dependencies in practice. Vaswani et al. (2016) and Lewis et al. (2016) shows the effectiveness of bi-LSTMs in supertagging, but they do not use a context window for the inputs.", "startOffset": 17, "endOffset": 409}, {"referenceID": 8, "context": "Dropout on the dynamic window is similar to (Dai and Le, 2015), which randomly drop words in the input sentences.", "startOffset": 44, "endOffset": 62}, {"referenceID": 1, "context": "Their attention mechanism is similar to Bahdanau et al. (2014), while ours was not originally designed as a weighted average but a gated concatenation.", "startOffset": 40, "endOffset": 63}, {"referenceID": 1, "context": "Their attention mechanism is similar to Bahdanau et al. (2014), while ours was not originally designed as a weighted average but a gated concatenation. Dropout on the dynamic window is similar to (Dai and Le, 2015), which randomly drop words in the input sentences. Gal (2015) also use dropout on words, but using a fixed mask rather a random one.", "startOffset": 40, "endOffset": 277}], "year": 2016, "abstractText": "Combinatory Category Grammar (CCG) supertagging is a task to assign lexical categories to each word in a sentence. Almost all previous methods use fixed context window sizes as input features. However, it is obvious that different tags usually rely on different context window sizes. These motivate us to build a supertagger with a dynamic window approach, which can be treated as an attention mechanism on the local contexts. Applying dropout on the dynamic filters can be seen as drop on words directly, which is superior to the regular dropout on word embeddings. We use this approach to demonstrate the state-of-the-art CCG supertagging performance on the standard test set.", "creator": "LaTeX with hyperref package"}}}