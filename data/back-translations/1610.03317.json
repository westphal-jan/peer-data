{"id": "1610.03317", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "abstract": "Maximum internal product search (MIPS) is an important task in many machine learning applications, such as predicting a lower matrix factorization model for a referral system. Recently, there has been some work on how MIPS can be performed in sublinear time. However, most of them do not have the flexibility to control the trade-off between search efficiency and search quality. In this paper, we examine the MIPS problem with a computational budget. By carefully examining the problem structure of MIPS, we develop a novel Greedy MIPS algorithm that can handle scheduled MIPS. While simple and intuitive, Greedy-MIPS delivers surprisingly better results than modern approaches. As a concrete example, Greedy-MIPS runs on a candidate set containing half a million vectors of dimension 200, 200x faster than the naive approach and delivers search results with a top-5 precision of more than 75%.", "histories": [["v1", "Tue, 11 Oct 2016 13:10:48 GMT  (6874kb,D)", "http://arxiv.org/abs/1610.03317v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["hsiang-fu yu", "cho-jui hsieh", "qi lei", "inderjit s dhillon"], "accepted": true, "id": "1610.03317"}, "pdf": {"name": "1610.03317.pdf", "metadata": {"source": "CRF", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "authors": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Inderjit S. Dhillon"], "emails": ["rofuyu@cs.utexas.edu", "chohsieh@cs.ucdavis.edu", "leiqi@ices.utexas.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we study the computational issue in the prediction phase for many matrix factorization based latent embedding models in recommender systems, which can be mathematically formulated as a Maximum Inner Product Search (MIPS) problem. Specifically, given a large collection of n candidate vectors\nH = { hj \u2208 Rk : 1, . . . , n }\nand a query vector w \u2208 Rk, MIPS aims to identify a subset of candidates that have top largest inner product values with w. We also denote H = [h1, . . . ,hj , . . . ,hn]\n> as the candidate matrix. A naive linear search procedure to solve MIPS for a given query w requires O(nk) operations to compute n inner products and O(n log n) operations to obtain the sorted ordering of the n candidates.1\nRecently, MIPS has drawn a lot of attention in the machine learning community. Matrix factorization (MF) based recommender system [7, 10] is one of the most important applications. In an MF based recommender system, each user i is associated with a vector wi of dimension k, while each item j is associated with a vector hj of dimension k. The interaction (such as preference) between a user and an item is modeled by the value of the inner product between wi and hj . It is clear that identifying top-ranked items in such a system for a user is exactly a MIPS problem. Because both the number of users (the number of queries) and the number of items (size of vector pool in MIPS) can easily grow to millions, a naive linear search is extremely expensive; for example, to compute the preference for all m users over n items with latent embeddings of dimension k in a recommender system requires at least O(mnk) operations. When both m and n are large, the prediction procedure is extremely time consuming; it is even slower than the training\n1When only the largest B elements are required, the sorting procedure can be reduced to O(n+ B logB) on average using a selection algorithm [6].\nar X\niv :1\n61 0.\n03 31\n7v 1\n[ cs\n.D S]\n1 1\nO ct\nprocedure used to obtain the m+ n embeddings, which costs only O(|\u2126|k) operations per iteration. Taking the yahoo-music dataset as an example, we have m = 1M , n = 0.6M , |\u2126| = 250M , and\nmn = 600B 250M = |\u2126|.\nAs a result, the development of efficient algorithms for MIPS is needed in large-scale recommender systems. In addition, MIPS can be found in many other machine learning applications, such as the prediction for a multi-class or multi-label classifier [15, 18], an object detector, a structure SVM predicator, and many others.\nThere is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313]. However, most of them do not have the flexibility to control the trade-off between search efficiency and search quality in the prediction phase. In this paper, we consider the budgeted MIPS problem, which is a generalized version of the standard MIPS with a computation budget: how to generate a set of top-ranked candidates under a given budget on the number of inner products one can perform. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which handles budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to existing state-of-the-art approaches.\nContributions. Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without\nany nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13]. \u2022 Greedy-MIPS is orders of magnitudes faster than many state-of-the-art MIPS approaches to obtain a\ndesired search performance. As a specific example, on the yahoo-music data sets with n = 624, 961 and k = 200, Greedy-MIPS runs 200x faster than the naive approach and yields search results with the top-5 precision more than 75%, while the search performance of other state-of-the-art approaches under the similar speedup drops to less than 3% precision. \u2022 Greedy-MIPS supports MIPS with a budget, which brings the ability to control of the trade-off between computation efficiency and search quality in the prediction phase. To the best of our knowledge, among existing MIPS approaches, only the sampling approaches proposed in [3, 5] support the similar flexibility under a limited situation where all the candidates and query vectors are non-negative.\nOrganization. We first review existing fast MIPS approaches in Section 2 and introduce the budgeted MIPS problem in Section 3. In Section 4, we propose a novel greedy budgeted MIPS approach called Greedy-MIPS. We then show the empirical comparison in Section 5 and conclude this paper in Section 6."}, {"heading": "2 Existing Approaches for Fast MIPS", "text": "Because of its wide applicability, several algorithms have been proposed to design efficient algorithms for MIPS. Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14]. [2] is the first MIPS work which adopts such a MIPS-to-NNS reduction. Variants MIPS-to-NNS reduction are also proposed in [13, 14]. Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].\nFast MIPS approaches with sampling schemes have become popular recently [3, 5]. Various sampling schemes have been proposed to handle MIPS problem with different constraints. We will briefly review two popular sampling schemes in Section 2.2.\nh\u0302j = [hj ;\n\u221a\nM \u2212 \u2016hj\u20162]>, \u2200j, where M = maxj \u2016hj\u20162. All the transformed vectors are in the 3-dimensional sphere with radius \u221a M . As a result, the nearest neighbor of w\u0302 in this transformed 3-dimensional NNS problem, h\u03021, corresponds to the vector h1 which yields the maximum inner product value with w in the original 2-dimensional MIPS problem."}, {"heading": "2.1 Approaches with Nearest Neighbor Search Reduction", "text": "We briefly introduce the concept of the reduction proposed in [2]. First, we consider the relationship between the Euclidean distance and the inner product:\n\u2016w \u2212 hj1\u20162 = \u2016w\u20162 + \u2016hj1\u20162 \u2212 2w>hj1 \u2016w \u2212 hj2\u20162 = \u2016w\u20162 + \u2016hj2\u20162 \u2212 2w>hj2 .\nWhen all the candidate vectors hj share the same length; that is,\n\u2016h1\u2016 = \u2016h2\u2016 = \u00b7 \u00b7 \u00b7 = \u2016hn\u2016,\nthe MIPS problem is exactly the same as the NNS problem because\n\u2016w \u2212 hj1\u2016 > \u2016w \u2212 hj2\u2016 \u21d0\u21d2 w>hj1 < w>hj2 (1)\nwhen \u2016hj1\u2016 = \u2016hj2\u2016. However, when \u2016hj1\u2016 6= \u2016hj2\u2016, (1) no longer holds. See Figure 1(a) for an example where not all the candidate vectors have the same length. We can see that h1 is the candidate vector yielding the maximum inner product with w, while h2 is the nearest neighbor candidate.\nTo handle the situation where candidates have different lengths, [2] proposes the following transform to reduce the original MIPS problem with H and w in a k dimensional space to a new NNS problem with\nH\u0302 = { h\u03021, . . . , h\u0302n } and w\u0302 in a k + 1 dimensional space:\nw\u0302 = [w; 0] > ,\nh\u0302j = [ hj ; \u221a M \u2212 \u2016hj\u20162 ]> , \u2200j = 1, . . . , n, (2)\nwhere M is the maximum squared length over the entire candidate set H:\nM = max j=1,...,n\n\u2016hj\u20162.\nFirst, we can see that with the above transform, \u2225\u2225\u2225h\u0302j \u2225\u2225\u2225 2 = M for all j:\n\u2225\u2225\u2225h\u0302j \u2225\u2225\u2225 2 = \u2016hj\u20162 +M \u2212 \u2016hj\u20162 = M, \u2200j.\nThen, for any j1 6= j2, we have \u2225\u2225\u2225w\u0302 \u2212 h\u0302j1 \u2225\u2225\u2225 < \u2225\u2225\u2225w\u0302 \u2212 h\u0302j2 \u2225\u2225\u2225\n\u21d0\u21d2M + \u2016w\u20162 \u2212 2w>hj1 < M + \u2016w\u20162 \u2212 2w>hj2 \u21d0\u21d2 w>hj1 > w>hj2 .\nWith the above relationship, the original k-dimensional MIPS problem is equivalent to the transformed k+1 dimensional NNS problem. In Figure 1(b), we show the transformed NNS problem for the original MIPS problem presented in Figure 1(a).\nIn [14], another MIPS-to-NNS reduction has been proposed. The high level idea is to apply a transformation to H such that all the candidate vectors roughly have the same length by appending additional k\u0304 dimensions. In the procedure by [14], all the hj vectors are assumed (or scaled) to have \u2016hj\u2016 \u2264 U, \u2200j, where U < 1 is a positive constant. Then the following transform is applied to reduce the original k-dimensional MIPS problem to a new NNS problem with (k + k\u0304)-dimensional vectors H\u0302 and w\u0302 defined as:\nw\u0302 = [w; 0k\u0304] >\nh\u0302j = [ hj ; 1/2\u2212 \u2016hj\u20162 1 ; 1/2\u2212 \u2016hj\u20162 2 ; . . . ; 1/2\u2212 \u2016hj\u20162 k\u0304 ]> , (3)\nwhere 0k\u0304 is a zero vector of dimension k\u0304. Because U < 1, [14] shows that with the transform (3), we\nhave \u2225\u2225\u2225h\u0302j \u2225\u2225\u2225 2 = k\u0304/4 + \u2016hj\u20162 k\u0304+1\n, with the second term vanishing as k\u0304 \u2192 \u221e. Thus, all the candidates h\u0302j approximately have the same length. We can see the idea behind (3) is similar to (2): transforming H to H\u0302 such that all the candidates have the same length. Note that (2) achieves this goal exactly while (3) achieves this goal approximately. Both transforms show a similar empirical performance in [11].\nThere are many choices to solve the transformed NNS problem after the MIPS-to-NN reduction has been applied. In [11, 13, 14], various locality sensitive hashing schemes have been considered. In [2], a PCA-tree based approach is proposed, and shows better performance than LSH-based approaches, which is consistent to the empirical observations in [1] and our experimental results shown in Section 5. In [1], a simple K-means clustering algorithm is proposed to handled the transformed NNS problem."}, {"heading": "2.2 Sampling-based Approaches", "text": "The idea of the sampling-based MIPS approach is first proposed in [5] as an approach to perform approximate matrix-matrix multiplications. Its applicability on MIPS problems is studied very recently [3]. The idea\nbehind a sampling-based approach called Sample-MSIPS, is about to design an efficient sampling procedure such that the j-th candidate is selected with probability p(j):\np(j) \u223c h>j w.\nIn particular, Sample-MSIPS is an efficient scheme to sample (j, t) \u2208 [n]\u00d7 [k] with the probability p(j, t):\np(j, t) \u223c hjtwt.\nEach time a pair (j, t) is sampled, we increase the count for the j-th item by one. By the end of the sampling process, the spectrum of the counts forms an estimation of n inner product values. Due to the nature of the sampling approach, it can only handle the situation where all the candidate vectors and query vectors are nonnegative.\nDiamond-MSIPS, a diamond sampling scheme proposed in [3], is an extension of Sample-MSIPS to handle the maximum squared inner product search problem (MSIPS) where the goal is to identify candidate vectors with largest values of ( h>j w )2 . If both w and H are nonnegative or h>j w \u2265 0, \u2200j, MSIPS can be used to generate the solutions for MIPS. However, the solutions to MSIPS can be very different from the solutions to MIPS in general. For example, if all the inner product values are negative, the ordering for MSIPS is the exactly reverse ordering induced by MIPS. Here we can see that the applicability of both Sample-MSIPS and Diamond-MSIPS to MIPS is very limited."}, {"heading": "3 Budgeted Maximum Inner Product Search", "text": "The core idea behind the fast approximate MIPS approaches is to trade the search quality for the shorter query latency: the shorter the search latency, the lower the search quality. In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14]. Such approach-specific parameters are usually required to construct approach-specific data structures before any query is given, which means that the trade-off is somewhat fixed for all the queries. Particularly, the computation cost for all the query requests is fixed. However, in many real-world scenarios, each query request might have a different computational budget, which raises the question: Can we design a fast MIPS approach which supports the dynamic adjustment of the trade-off in the query phase?\nIn this section, we formally define the budgeted MIPS problem which is an extension of the standard MIPS problem with a computational budget as a parameter given in the query phase. We first summarize the essential components for fast MIPS approaches in Section 3.1 and give the problem definition of budgeted MIPS in Section 3.2."}, {"heading": "3.1 Essential Components for Fast MIPS Approaches", "text": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request:\n\u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.g., KD-tree or PCA-tree [2]), or cluster centroids [1].\n\u2022 For each query request: \u2013 Query-dependent Pre-processing: In some approaches, a query dependent pre-processing is needed.\nFor example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13]. In addition, [2] also requires another normalization. TP is used to denote the time complexity of this stage.\n\u2013 Candidate Screening: In this stage, based on the pre-constructed data structure D, an efficient procedure is performed to filter candidates such that only a subset of candidates C(w) \u2282 H is selected. In a naive linear approach, no screening procedure is performed, so C(w) simply contains all the n candidates. For a tree-based structure, C(w) contains all the candidates stored in the leaf node of the query vector. In a sampling-based MIPS approach, an efficient sampling scheme is designed to generate highly possible candidates to form C(w). TS denotes the computational cost of the screening stage. \u2013 Candidate Ranking: An exact ranking is performed on the selected candidates in C(w) obtained from the screening stage. This involves the computation of |C(w)| inner products and the sorting procedure among these |C(w)| values. The overall time complexity TR is\nTR = O(|C(w)|k + |C(w)| log|C(w)|).\nThe per-query computational cost TQ is\nTQ = TP + TS + TR.\nIt is clear that the candidate screening stage is the key component for a fast MIPS approach. In terms of the search quality, the performance highly depends on whether the screening procedure can identify highly possible candidates. In terms of the query latency, the efficiency highly depends on the size of C(w) and how fast to generate C(w). The major difference between various fast MIPS approaches is the choice of the data structure D and the corresponding screening procedure."}, {"heading": "3.2 Budgeted MIPS: Problem Definition", "text": "Budgeted maximum inner product search is an extension of the standard approximate MIPS problem with a computation budget: how to generate top-ranked candidates under a given budget on the number of inner product operations one can perform. Budgeted MIPS has a wide applicability. For example, a real-time recommender system must provide a list of recommended items for its users in a very short response time.\nNote that the cost for the candidate ranking (TR) is inevitable in the per-query cost: TQ = TP +TS +TR. A viable approach to support budgeted MIPS must include a screening procedure which satisfies the following requirements: \u2022 the flexibility to control the size of C(w) in the candidate screening stage such that |C(w)| \u2264 B, where B is a given budget, and\n\u2022 an efficient screening procedure to obtain C(w) in O(Bk) time such that the overall per-query cost is\nTQ = O(Bk +B logB).\nAs mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.g., LSH, KD-tree, or PCA-tree) designed for NNS to index the candidates H in the query-independent pre-processing stage. As the construction of D is query independent, both the search performance and the computation cost are fixed when the construction is done. For example, the performance of a PCA-MIPS depends on the depth of the PCA-tree. Given a query vector w, there is no control to the size of C(w) in the candidate generating phase. LSH-based approaches also have the similar issue. As a result, it is not clear how to generalize PCA-MIPS and LSH-MIPS in a principled way to handle the situation with a computational budget: how to reduce the size of C(w) under a limited budget and how to improve the performance when a larger budget is given.\nUnlike other NNS-based algorithms, the design of Sample-MSIPS naturally enables it to support budgeted MIPS for a nonnegative candidate matrix H and a nonnegative query w. Recall that the core idea behind Sample-MSIPS is to draw a sample candidate j among n candidates such that\np(j) \u221d h>j w.\nThe more the number of samples, the lower the variance of the estimated frequency spectrum. Clearly, Sample-MSIPS has the flexibility to control the size of C(w), and thus is a viable approach for the budgeted MIPS problem. However, Sample-MSIPS works only on the situation where the entire H and w are nonnegative. Diamond-MSIPS has the similar issue.\n4 Greedy-MIPS: A Novel Approach for Budgeted MIPS\nIn this section, we carefully study the problem structure of MIPS and develop a simple but novel algorithm called Greedy-MIPS, which handles budgeted MIPS by design. Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem. Moreover, Greedy-MIPS is a viable approach for the budgeted MIPS problem without the non-negativity limitation inherited in the sampling approaches.\nAs mentioned earlier that the key component for a fast MIPS approach is the algorithm used in the candidate screening phase. In budgeted MIPS, for any given budget B and query w, an ideal procedure for the candidate screening phase costs O(Bk) time to generate C(w) which contains the B items with the largest B inner product values over the n candidates in H. The requirement on the time complexity O(Bk) implies that the procedure is independent from n = |H|, the number of candidates in H. One might wonder whether such an ideal procedure exists or not. In fact, designing such an ideal procedure with the requirement to generate the largest B items in O(Bk) time is even more challenging than the original budgeted MIPS problem.\n4.1 A Motivating Example for Greedy-MIPS\nAlthough the existence of an ideal procedure for a general budgeted MIPS problem seems to be impossible, we demonstrate that an ideal approach exists for budgeted MIPS when k = 1. It is not hard to observe that Property 1 holds for any given H = {h1, . . . , hn | hj \u2208 R}: Property 1. For any nonzero query w \u2208 R and any budget B > 0, there are only two possible results for that top B inner products between w and H:\nw > 0\u21d2 Largest B elements in H, w < 0\u21d2 Smallest B elements in H.\nThis property leads to the following simple approach, which is an ideal procedure for the budgeted MIPS problem when k = 1: \u2022 Query-independent data structure: a sorted list of indices of H: s[r], r = 1, . . . , n such that s[r] stores\nthe index to the r-th largest candidate. That is\nhs[1] \u2265 hs[2] \u2265 \u00b7 \u00b7 \u00b7 \u2265 hs[n],\n\u2022 Candidate screening phase: for any given w 6= 0 and B > 0,\nreturn { first B elements: {s[1], . . . , s[B]} if w > 0, last B elements: {s[n], . . . , s[n\u2212 B + 1]} if w < 0\nas the indices of the exact largest-B candidates. Note that for this simple scenario (k = 1), neither the query dependent pre-processing nor the candidate ranking is needed. Thus, the overall time complexity per query is TQ = O(B). We can see that Property 1 is the key to the correctness of the above procedure. Nevertheless, it is not clear how to generalize Property 1 for MIPS problems with k \u2265 2. Fortunately, we can directly utilize the fact that Property 1 holds for k = 1 to design an efficient greedy procedure for the candidate screening when k \u2265 2.\nZ> = diag(w)H> : zjt = hjtwt,\u2200j, t"}, {"heading": "4.2 A Greedy Procedure to Candidate Screening", "text": "To better describe the idea of the proposed algorithm Greedy-MIPS, we consider the following definition (4): Definition 1. The rank of an item x among a set of items X = { x1, . . . , x|X | } is defined as\nrank(x | X ) := |X |\u2211\nj=1\nI[xj \u2265 x], (4)\nwhere I[\u00b7] is the indicator function. A ranking induced by X is a function \u03c0(\u00b7) : X \u2192 {1, . . . , |X |} such that \u03c0(xj) = rank(xj | X ) \u2200xj \u2208 X .\nOne way to store a ranking \u03c0(\u00b7) induced by X is by a sorted index array s[r] of size |X | such that\n\u03c0(xs[1]) \u2264 \u03c0(xs[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0(xs[|X |]).\nWe can see that s[r] stores the index to the item x with \u03c0(x) = r. In order to design an efficient candidate screening procedure, we carefully study the operations required for MIPS. In the naive linear MIPS approach, nk multiplication operations are required to obtain n inner product values { h>1 w, . . . ,h > nw }\n. To understand and analyze the computation required for MIPS, we define an implicit matrix Z \u2208 Rn\u00d7k:\nZ = H diag(w),\nwhere diag(w) \u2208 Rk\u00d7k is a matrix with w as it diagonal. The (j, t) entry of Z denotes the multiplication operation zjt = hjtwt and zj = diag(w)hj denotes the j-th row of Z. In Figure 2, we use Z\n> to demonstrate the implicit matrix. The implicit matrix Z is query dependant, that is, the values of Z depend on the query vector w. Note that n inner product values can be obtained by taking the column-wise summation of Z>. In particular, we have\nh>j w =\nk\u2211\nt=1\nzjt, j = 1, . . . , n.\nThus, the ranking induced by the n inner product values can be characterized by the marginal ranking \u03c0(j|w) defined on the implicit matrix Z as follows:\n\u03c0(j|w) := rank ( k\u2211\nt=1\nzjt \u2223\u2223\u2223\u2223\u2223 { k\u2211\nt=1\nz1t, \u00b7 \u00b7 \u00b7 , k\u2211\nt=1\nznt\n}) (5)\n= rank ( h>j w | { h>1 w, . . . ,h > nw }) .\nAs mentioned earlier, it is hard to design an ideal candidate screening procedure which generates C(w) based on the marginal ranking. Because the main goal for the candidate screening phase is to quickly\nidentify candidates which are highly possible to be top-ranked items, it suffices to have an efficient procedure generating C(w) by an approximation ranking. Here we propose a greedy heuristic ranking:\n\u03c0\u0304(j|w) := rank (\nk max t=1 zjt\n\u2223\u2223\u2223 {\nk max t=1 z1t, \u00b7 \u00b7 \u00b7 , k max t=1 znt\n}) , (6)\nwhich is obtained by replacing the summation terms in (5) by max operators. The intuition behind this heuristic is that the largest element of zj multiplied by k is an upper bound of h > j w:\nh>j w =\nk\u2211\nt=1\nzjt \u2264 k (\nk max t=1 zjt\n) .\nThus, \u03c0\u0304(j|w), which is induced by such an upper bound of h>j w, could be a reasonable approximation ranking for the marginal ranking \u03c0(j|w).\nNext we design an efficient procedure which generates C(w) according to the ranking \u03c0\u0304(j|w) defined in (6). First, based on the relative orderings of {zjt}, we consider the joint ranking and the conditional ranking defined as follows: \u2022 Joint ranking: \u03c0(j, t|w) is the exact ranking over the nk entries of Z.\n\u03c0(j, t|w) := rank(zjt | {z11, . . . , znk}).\n\u2022 Conditional ranking: \u03c0t(j|w) is the exact ranking over the n entires of the t-th row of Z>.\n\u03c0t(j|w) := rank(zjt | {z1t, . . . , znt}).\nSee Figure 2 for an illustration for both rankings. Similar to the marginal ranking, both joint and conditional rankings are query dependent.\nObserve that, in (6), for each j, only a single maximum entry of Z, maxkt=1 zjt, is considered to obtain the ranking \u03c0\u0304(j|w). To generate C(w) based on \u03c0\u0304(j|w), we can iterate (j, t) entries of Z in a greedy sequence such that (j1, t1) is visited before (j2, t2) if zj1t1 > zj2t2 , which is exactly the sequence corresponding to the joint ranking \u03c0(j, t|w). Each time an entry (j, t) is visited, we can include the index j into C(w) if j /\u2208 C(w). In Theorem 1, we show that the sequence to include a newly observed j into C(w) is exactly the sequence induced by the ranking \u03c0\u0304(j|w) defined in (6).\nTheorem 1. For all j1 and j2 such that \u03c0\u0304(j1|w) < \u03c0\u0304(j2|w), j1 will be included into C(w) before j2 if we iterate (j, t) pairs following the sequence induced by the joint ranking \u03c0(j, t|w).\nProof. Let t1 = arg max k t=1 zj1t and t2 = arg max k t=1 zj2t. By the definition of t1, we have \u03c0(j1, t1|w) < \u03c0(j1, t|w), \u2200t 6= t1. Thus, (j1, t1) will be first entry among {(j1, 1), . . . , (j1, k)} to be visited in the sequence corresponding to the joint ranking \u03c0(j, t|w). Similarly, (j2, t2) will be the first visited entry among {(j2, 1, ), . . . , (j2, k)}. We also have\n\u03c0\u0304(j1|w) < \u03c0\u0304(j2|w)\u21d2 zj1t1 > zj2t2 \u21d2 \u03c0(j1, t1|w) < \u03c0(j2, t2|w).\nThus, j1 will be included into C(w) before j2.\nAt first glance, generating (j, t) in the sequence according to the joint ranking \u03c0(j, t|w) might require the access to all the nk entries of Z and cost O(nk) time. In fact, based on Property 2 of conditional rankings, we can design an efficient variant of the k-way merge algorithm [8, Chapter 5.4.1] to generate (j, t) pairs in the desired sequence iteratively.\nProperty 2. Given a fixed candidate matrix H, for any possible w with wt 6= 0, the conditional ranking \u03c0t(j|w) is either \u03c0t+(j) or \u03c0t\u2212(j): \u2022 \u03c0t+(j) = rank(hjt | {h1t, . . . , hnt}), \u2022 \u03c0t\u2212(j) = rank(\u2212hjt | {\u2212h1t, . . . ,\u2212hnt}).\nAlgorithm 1 ConditionalIterator: an iterator iterates j \u2208 {1, . . . , n} based on the conditional ranking \u03c0t(j|w). This pseudo code assumes that the k sorted index arrays st[r], r = 1, . . . , n, t = 1, . . . , k are available. class ConditionalIterator:\ndef constructor(dim idx, query val):\nt, w, ptr \u2190 dim idx, query val, 1\ndef current(): return\n{ st[ptr] if w > 0,\nst[n\u2212 ptr + 1] otherwise. def hasNext(): return (ptr < n) def getNext(): ptr\u2190 ptr + 1 and return current()\nIn particular, we have\n\u03c0t(j|w) = { \u03c0t+(j) if wt > 0,\n\u03c0t\u2212(j) if wt < 0.\nSimilar to Property 1, Property 2 enables us to characterize a query dependent conditional ranking \u03c0t(j|w) by two query independent rankings \u03c0t+(j) and \u03c0t\u2212(j). As a result, similar to the motivating example in Section 4.1,for each t, we can construct and store a sorted index array st[r], r = 1, . . . , n such that\n\u03c0t+(st[1]) \u2264 \u03c0t+(st[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0t+(st[n]), (7)\nor equivalently\n\u03c0t\u2212(st[1]) \u2265 \u03c0t\u2212(st[2]) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c0t\u2212(st[n]). (8)\nThus, in the phase of query-independent data structure construction of Greedy-MIPS, we compute and store query-independent rankings \u03c0t+(\u00b7), t = 1, . . . , k by k sorted index arrays of length n: st[r], r = 1, . . . , n, t = 1, . . . , n such that (7) holds. The entire construction costs O(kn log n) time and O(kn) space.\nNext we describe the details of the proposed Greedy-MIPS algorithm when a query w and the budget B are given. As mentioned earlier, Greedy-MIPS utilizes the idea of the k-way merge algorithm to visit (j, t) entries of Z according to the joint ranking \u03c0(j, t|w). Designed to merge k sorted sublists into a single sorted list, the k-way merge algorithm uses 1) k pointers, one for each sorted sublist, and 2) a binary tree structure (either a heap or a selection tree) containing the elements pointed by these k pointers to obtain the next element to be appended into the sorted list [8, Chapter 5.4.1.]."}, {"heading": "4.2.1 Query-dependent Pre-processing", "text": "In Greedy-MIPS, we divide nk entries of (j, t) into k groups. The t-th group contains n entries:\n{(j, t) : j = 1, . . . , n}.\nHere we need an iterator playing a similar role as the pointer which can iterate index j \u2208 {1, . . . , n} in the sorted sequence induced by the conditional ranking \u03c0t(\u00b7|w). Utilizing Property 2, the t-th pre-computed sorted arrays st[r], r = 1, . . . , n can be used to construct such an iterator, called ConditionalIterator, which iterates an index j one by one in the desired sorted sequence. ConditionalIterator needs to support current() to access the currently pointed index j and getNext() to advance the iterator. In Algorithm 1, we describe a pseudo code for ConditionalIterator, which utilizes the facts (7) and (8) such that both the construction and the index access cost O(1) space and O(1) time. For each t, we use iters[t] to denote the ConditionalIterator for the t-th conditional ranking \u03c0t(j|w).\nAlgorithm 2 Query-dependent pre-processing procedure in Greedy-MIPS.\n\u2022 Input: query w \u2208 Rk \u2022 For t = 1, . . . , k\n\u2013 iters[t]\u2190 ConditionalIterator(t, wt) \u2013 j \u2190 iters[t].current() \u2013 z \u2190 hjtwt \u2013 Q.push((z, t))\n\u2022 Output: \u2013 iters[t], t = 1, . . . , k: iterators for conditional ranking \u03c0t(\u00b7|w). \u2013 Q: a max-heap containing { (z, t) | z = maxnj=1 zjt, t = 1, . . . , k } .\nRegarding the binary tree structure used in Greedy-MIPS, we consider a max-heap Q of (z, t) pairs. z \u2208 R is the compared key used to maintain the heap property of Q, and t \u2208 {1, . . . , k} is an integer to denote the index to a entry group. Each (z, t) \u2208 Q denotes the (j, t) entry of Z where\nj = iters[t].current() and z = zjt = hjtwt.\nNote that there are most k elements in the max-heap at any time. Thus, we can implement Q by a binary heap such that it supports \u2022 Q.top(): returns the maximum pair (z, t) of Q in O(1) time, \u2022 Q.pop(): deletes the maximum pair of Q in O(log k) time, and \u2022 Q.push((z, t)): inserts a new pair in O(log k) time. Note that the entire Greedy-MIPS can also be implemented using a selection tree among the k entries pointed by the k iterators. For the simplicity of presentation, we use a max-heap to describe the idea of Greedy-MIPS first and describe the details of Greedy-MIPS with a selection tree in the end of Section 4.2.2.\nIn the query-dependent pre-processing phase of Greedy-MIPS, we need to construct iters[t], t = 1, . . . , k, one for each conditional ranking \u03c0t(j|w), and a max-heap Q which is initialized to contain\n{ (z, t) | z = nmax\nj=1 zjt, t = 1, . . . , k\n} .\nA detailed procedure is described in Algorithm 2, which costs O(k log k) time and O(k) space."}, {"heading": "4.2.2 Candidate Screening", "text": "Recall the requirements for a viable candidate screening procedure to support budgeted MIPS: 1) the flexibility to control the size |C(w)| \u2264 B; and 2) an efficient procedure runs in O(Bk). The core idea of Greedy-MIPS is to iteratively traverse (j, t) entries of Z in a greedy sequence and collect newly observed indices j into C(w) until |C(w)| = B. In particular, if r = \u03c0(j, t|w), then (j, t) entry is visited at the r-th iterate. Utilizing the max-heap Q and the k iterators: iters[t], we can design an iterator, called JointIterator, which iterates (j, t) pairs one by one in the desired greedy sequence induced by joint ranking \u03c0(j, t|w). Following the k-way merge algorithm, in Algorithm 3, we describe a detailed pseudo code for such an iterator. JointIterator costs O(k log k) time to run Algorithm 2 to construct and initialize Q and iters[t], and costs O(log k) time to advance to the next entry. In Algorithm 4, we describe our first candidate screening procedure with a budget B for Greedy-MIPS, which is a simple while-loop to iterate (j, t) entries using the JointIterator with w until |C(w)| = B.\nTo analyze the time complexity of Algorithm 4, we need to know the number of the iterations of the while-loop before the stop condition is satisfied. The following Theorem 2 gives an upper bound on this number of iterations.\nAlgorithm 3 JointIterator: an iterator generates (j, t) pairs one by one based on the joint ranking \u03c0(j, t|w). The constructor costs O(k log k) time to build a max-heap Q. The time complexity to generate a pair is O(log k).\nclass JointIterator:\ndef constructor(w): \u00b7 \u00b7 \u00b7O(k log k) Run Algorithm 2 with w to initialize Q and iters[t], t = 1, . . . , k ptr\u2190 1. def current(): \u00b7 \u00b7 \u00b7O(1) (z, t)\u2190 Q.top() j \u2190 iters[t].current() return (j, t) def hasNext(): return (ptr < nk) \u00b7 \u00b7 \u00b7O(1) def getNext(): \u00b7 \u00b7 \u00b7O(log k)\n(z, t)\u2190 Q.pop() \u00b7 \u00b7 \u00b7O(log k) if iters[t].hasNext():\nj \u2190 iters[t].getNext() z \u2190 hjtwt Q.push((z, t)) \u00b7 \u00b7 \u00b7O(log k)\nptr\u2190 ptr + 1 return current()\nAlgorithm 4 Candidate screening procedure in Greedy-MIPS.\n\u2022 Input: w and an empty C(w) \u2022 jointIter\u2190 JointIterator(w) \u00b7 \u00b7 \u00b7O(k log k) \u2022 (j, t)\u2190 jointIter.current() \u2022 while |C(w)| < B:\n\u2013 if j /\u2208 C(w): append j to C(w) \u2013 (j, t)\u2190 jointIter.getNext() \u00b7 \u00b7 \u00b7O(log k)\n\u2022 Output: C(w) = {j | \u03c0\u0304(j|w) \u2264 B}\nAlgorithm 5 An improved candidate screening procedure in Greedy-MIPS. The overall time complexity is O(Bk).\n\u2022 Input: \u2013 H, w, and the computational budget B \u2013 Q and iters[t]: output of Algorithm 2 \u2013 C(w): an empty list \u2013 visited[j] = 0, j = 1, . . . , n: a zero-initialized array of length n\n\u2022 while |C(w)| < B: \u2013 (z, t)\u2190 Q.pop() \u00b7 \u00b7 \u00b7O(log k) \u2013 j \u2190 iters[t].current() \u2013 if visited[j] = 0:\n\u2217 append j into C(w) \u2217 visited[j]\u2190 1\n\u2013 while iters[t].hasNext():\n\u2217 j \u2190 iters[t].getNext() \u2217 if visited[j] = 0: \u00b7 z \u2190 hjtwt \u00b7 Q.push((z, t)) \u00b7 \u00b7 \u00b7O(log k) \u00b7 break\n\u2022 visited[j]\u2190 0,\u2200j \u2208 C(w) \u00b7 \u00b7 \u00b7O(B) \u2022 Output: C(w) = {j | \u03c0\u0304(j|w) \u2264 B}\nTheorem 2. There are at least B distinct indices j in the first Bk entries (j, t) in terms of the joint ranking \u03c0(j, t|w) for any w; that is,\n|{j | \u2200(j, t) such that \u03c0(j, t|w) \u2264 Bk}| \u2265 B. (9)\nProof. By grouping these first Bk entries by the index t and applying the pigeonhole principle, we know that there exists a group G such that it contains at least B entries. Because each entry in the same group has a distinct j index, we know that the group G contains at least B distinct indices j.\nTheorem 1 guarantees the correctness of Algorithm 4 to generate C(w) based on \u03c0\u0304(j|w) defined in (6). By Theorem 2, the overall time complexity of Algorithm 4 is O(Bk log k) as each iteration of the while-loop costs O(log k) time.\nThe O(Bk log k) time complexity of Algorithm 4 does not satisfy the efficiency requirement of a viable budgeted MIPS approach. Here we propose an improved candidate screening procedure which reduces the overall time complexity to O(Bk). Observe that the log k term comes from the Q.push (( zjt, t )) and Q.pop() operations of the max-heap for each visited (j, t) entry. As the goal of the screening procedure is to identify j indices only, we can skip the Q.push (( zjt, t )) for an entry (j, t) with the j having been included in C(w). As a result, Q.pop() is executed at most B + k \u2212 1 times when |C(w)| = B. The extra k \u2212 1 times occurs in the situation that\niters[1].current() = iters[2].current() = \u00b7 \u00b7 \u00b7 = iters[k].current()\nat the beginning of the entire screening procedure. In Algorithm 5, we give a detailed description for this improved candidate screening procedure for GreedyMIPS. See Figure 3 for a detailed illustration of this algorithm on a toy example. Note that in Algorithm 5, we use an auxiliary zero-initialized array of length n: visited[j], j = 1, . . . , n to denote whether an index j has been included in C(w) or not. As C(w) contains at most B indices, only B elements of this auxiliary array will be modified during the screening procedure. Furthermore, the auxiliary array can be reset to zero\nusing O(B) time in the end of Algorithm 5, so this auxiliary array can be utilized again for a different query vector w.\nNotice that Algorithm 5 still iterates Bk entries of Z but at most B + k \u2212 1 entries will be pushed into or pop from the max-heap. Thus, the overall time complexity of Algorithm 5 is O(Bk +B log k) = O(Bk), which satisfies the efficiency requirement for a viable approach for budgeted MIPS.\nGreedy-MIPS with a Selection Tree. As there are at most k pairs in the max-heap Q, one from each iters[t], the max-heap can be replaced by a selection tree to achieve a slightly faster implementation as suggested in [8, Chapter 5.4.1]. In Algorithm 6, we give a pseudo code for the selection tree with a O(k) time constructor, a O(1) time maximum element look-up, and a O(log k) time updater. To apply the section tree for our Greedy-MIPS, we only need to the following modifications: \u2022 In Algorithm 2, remove Q.push((z, t)) from the for-loop and construct Q by Q\u2190 SelectionTree(w, k, iters). \u2022 In Algorithm 3 and Algorithm 5, replace Q.pop() by Q.top() and replace Q.push((z, t)) by Q.updateValue(t, z)."}, {"heading": "4.2.3 Connection to Sampling-based MIPS Approaches", "text": "Sample-MSIPS, as mentioned earlier, is essentially a sampling algorithm with replacement scheme to draw entries of Z such that (j, t) is sampled with the probability proportional to zjt. Thus, Sample-MSIPS can be thought as a traversal of (j, t) entries using in a stratified random sequence determined by a distribution of the values of {zjt}, while the core idea of Greedy-MIPS is to iterate (j, t) entries of Z in a greedy sequence induced by the ordering of {zjt}. Next, we discuss the differences between Greedy-MIPS and Sample-MSIPS in a few perspectives:\nAlgorithm 6 A pseudo code of a selection tree used for Greedy-MIPS.\nclass SelectionTree: def constructor(w, k, iters) : \u00b7 \u00b7 \u00b7O(k)\nK\u0304 \u2190 min { 2i | 2i \u2265 k } for i = 1, . . . , 2K\u0304:\nbuf[i]\u2190 (\u2212\u221e, 0) for t = 1, . . . , k:\nj \u2190 iters[t].current() buf[K\u0304 + t]\u2190 (hjtwt, t)\nfor i = K\u0304, . . . , 1:\nif buf[2i].first > buf[2i+ 1].first: buf[i]\u2190 buf[2i] else: buf[i]\u2190 buf[2i+ 1]\ndef top(): return buf[1] \u00b7 \u00b7 \u00b7O(1) def updateValue(t, z): \u00b7 \u00b7 \u00b7O(log k)\ni\u2190 K\u0304 + t buf[i]\u2190 (z, t) while i > 1:\ni\u2190 bi/2c if buf[2i].first > buf[2i+ 1].first:\nbuf[i]\u2190 buf[2i] else:\nbuf[i]\u2190 buf[2i+ 1]\nApplicability: Sample-MSIPS can be applied to the situation where bothH and w are nonnegative because of the nature of sampling scheme. In contrast, Greedy-MIPS can work on any MIPS problems as only the ordering of {zjt} matters in Greedy-MIPS. Instead of h>j w, Diamond-MSIPS is designed for the MSIPS problem which is to identify candidates with largest ( h>j w )2 or \u2223\u2223h>j w\n\u2223\u2223 values [3]. In fact, for nonnegative MIPS problems, the diamond sampling is equivalent to Sample-MSIPS. Moreover, for MSIPS problems with negative entries, when the number of samples is set to be the budget B,2 the Diamond-MSIPS is equivalent to apply Sample-MSIPS to sample (j, t) entries with the probability p(j, t) \u221d |zjt|. Thus, the applicability of the existing sampling-based approaches is still very limited for general MIPS problems.\nFlexibility to Control |C(w)|: By Theorem 2, we know that Greedy-MIPS can guarantee both the time complexity of the candidate screening procedure and the size of output |C(w)| for any H, w, and B. For a sampling-based approach, one can easily control either the time complexity of the sampling procedure or the size of C(w), but not both. Because all the existing sampling-based approaches are a sampling scheme with replacement, the same entry (j, t) could be sampled repeatedly. Thus, the time complexity to guarantee that C(w) = B depends on the distribution of values of w and H. Hence, Greedy-MIPS is more flexible than sampling-based approaches in terms of the controllability of C(w)."}, {"heading": "5 Experimental Results", "text": "In this section, we perform extensive empirical comparisons to compare Greedy-MIPS with other state-ofthe-art fast MIPS approaches on both real-world and synthetic datasets.\n2This setting is used in the experiments in [3]\n\u2022 We use netflix and yahoo-music as our real-world recommender system datasets. There are 17, 770 and 624, 961 items in netflix and yahoo-music, respectively. In particular, we obtain the low rank model (W,H) by the standard low-rank matrix factorization:\nmin W,H\n\u2211\n(i,j)\u2208\u2126\n( Aij \u2212w>i hj )2 + \u03bb\n  m\u2211\ni=1\n|\u2126i| n \u2016wi\u20162 +\nn\u2211\nj=1\n\u2223\u2223\u2126\u0304j \u2223\u2223\nm \u2016hj\u20162\n ,\nwhere Aij is the rating of the j-th item given by the i-th user, \u2126 is the set of observed ratings, \u2126i = {j | (i, j) \u2208 \u2126}, and \u2126\u0304j = {i | (i, j) \u2208 \u2126}, and \u03bb is a regularization parameter. We use the CCD++ [16] algorithm implemented in LIBPMF3 to solve the above optimization problem and obtain the user embeddings {wi} and item embeddings {hj}. We use the same \u03bb used in [4]. We also obtain (W,H) with a different k: 50, 100, and k = 200. \u2022 We also generate synthetic datasets with various n = 2{17,18,19,20} and k = 2{2,5,7,10}. For each synthetic dataset, both candidate vector hj and query w vector are drawn from the normal distribution.\n3http://www.cs.utexas.edu/~rofuyu/libpmf"}, {"heading": "5.1 Experimental Settings and Evaluation Criteria", "text": "All the experiments are performed on a Linux machine with 20 cores and 256 GB memory. We ensure that only single core/thread is used for our experiments. To have a fair comparison, all the compared approaches are implemented in C++: \u2022 Greedy-MIPS: our proposed approach in Section 4. We compare the following variants in Section 5.2:\n\u2013 The improved Greedy-MIPS in Algorithm 5 with the selection tree in Algorithm 6. \u2013 The improved Greedy-MIPS in Algorithm 5 with a max-heap. \u2013 The original Greedy-MIPS in Algorithm 4 with the selection tree in Algorithm 6.\n\u2022 NNS-based MIPS approaches: \u2013 PCA-MIPS: the approach proposed in [2], which is shown to be the state-of-the-art among tree-\nbased approaches [2]. We implement a complete PCA-Tree with the neighborhood boosting techniques described in [2]. We vary the depth of PCA tree to control the trade-off between the search quality and the search efficiency. \u2013 LSH-MIPS: the approach proposed in [11, 13]. We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11]. We also implement the standard amplification procedure with an OR-construction of b hyper LSH hash functions. Each hyper LSH function is a result of an AND-construction of a random projections. We vary the values (a, b) to control the trade-off between the search quality and the search efficiency.\n\u2022 Diamond-MSIPS: the sampling scheme proposed in [3] for the maximum squared inner product search. As it shows better performance than LSH-MIPS in [3] in terms of MIPS problems, we also include Diamond-MSIPS into our comparison. F+Tree [17] is implemented as the multinomial sampling procedure. \u2022 Naive-MIPS: the baseline approach which applies a linear search to identify the exact top-K candidates.\nEvaluation Criteria. For each dataset, the actual top-20 items for each query are regarded as the ground truth. We report the average performance on a randomly selected 2,000 query vectors. To evaluate the search quality, we use the precision on the top-K prediction (prec@K), is obtained by selecting top-K items from C(w) returned by the candidate screening procedure of a compared MIPS approach. K = 5 and K = 10 are used in our experiments. To evaluate the search efficiency, we report the relative speedups over the Naive-MIPS approach as follows:\nspeedup = prediction time required by Naive-MIPS\nprediction time by a compared approach .\nRemarks on Budgeted MIPS versus Non-Budgeted MIPS. As mentioned in Section 3, PCA-MIPS and LSH-MIPS cannot handle MIPS with a budget. Both the search computation cost and the search quality are fixed when the corresponding data structure is constructed. As a result, to understand the trade-off between search efficiency and search quality for these two approaches, we can only try various values for its parameters (such as the depth for PCA tree and the amplification parameters (a, b) for LSH). For each combination of parameters, we need to re-run the entire query-independent pre-processing procedure to construct a new data structure."}, {"heading": "5.2 Experimental Results", "text": "Results on Variants of Greedy-MIPS. In Figure 4, we shows the comparison between the three variants of Greedy-MIPS on netflix and yahoo-music. We can see that the difference between the use of a selection-tree\nand a max-heap is small, while the different between the use of Algorithm 4 and the use of Algorithm 5 is more significant. For the comparison to other MIPS approaches, we use Greedy-MIPS to denote the results obtained from the version with the combination of Algorithm 5 and Algorithm 6.\nResults on Real-World Data sets. Comparison results for netflix and yahoo-music are shown in Figure 5. The first, second, and third columns present the results with k = 50, k = 100, and k = 200, respectively. It is clearly observed that given a fixed speedup, Greedy-MIPS yields predictions with much higher search quality. In particular, on the yahoo-music data set with k = 200, Greedy-MIPS runs 200x faster than Naive-MIPS and yields search results with p5 = 70%, while none of PCA-MIPS, LSH-MIPS, and Diamond-MSIPS can achieve a p5 > 10% while maintaining the similar 200x speedups.\nResults on Synthetic Data Sets. We also perform comparisons on synthetic datasets. The comparison with various n \u2208 2{17,18,19,20} is shown in Figure 6, while the comparison with various k \u2208 2{2,5,7,10} is shown in Figure 7. We observe that the performance gap between Greedy-MIPS over other approaches remains when n increases, while the gap becomes smaller when k increases. However, Greedy-MIPS still outperforms other approaches significantly."}, {"heading": "6 Conclusions", "text": "In this paper, we study the computational issue in the prediction phase for many MF-based models: a maximum inner product search problem (MIPS) with a very large number of candidate embeddings. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%."}], "references": [{"title": "Clustering is efficient for approximate maximum inner product search, 2016", "author": ["Alex Auvolat", "Sarath Chandar", "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1507.05910", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces", "author": ["Yoram Bachrach", "Yehuda Finkelstein", "Ran Gilad-Bachrach", "Liran Katzir", "Noam Koenigstein", "Nir Nice", "Ulrich Paquet"], "venue": "In Proceedings of the 8th ACM Conference on Recommender Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Diamond sampling for approximate maximum all-pairs dot-product (MAD) search", "author": ["Grey Ballard", "Seshadhri Comandur", "Tamara Kolda", "Ali Pinar"], "venue": "In Proceedings of the IEEE International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A learning-rate schedule for stochastic gradient methods to matrix factorization", "author": ["Wei-Sheng Chin", "Yong Zhuang", "Yu-Chin Juan", "Chih-Jen Lin"], "venue": "In Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Approximating matrix multiplication for pattern recognition tasks", "author": ["Edith Cohen", "David D. Lewis"], "venue": "In Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L Rivest"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "The Yahoo! music dataset and KDD- Cup\u201911", "author": ["Gideon Dror", "Noam Koenigstein", "Yehuda Koren", "Markus Weimer"], "venue": "In JMLR Workshop and Conference Proceedings: Proceedings of KDD Cup 2011 Competition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Art of Cmoputer Programming, Volumne 3: Sorting and Searching", "author": ["Donald E. Knuth"], "venue": "Addison-Wesley, 2nd edition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["Noam Koenigstein", "Parikshit Ram", "Yuval Shavitt"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert M. Bell", "Chris Volinsky"], "venue": "IEEE Computer,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "On symmetric and asymmetric lshs for inner product search", "author": ["Behnam Neyshabur", "Nathan Srebro"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1926}, {"title": "Maximum inner-product search using cone trees", "author": ["Parikshit Ram", "Alexander G. Gray"], "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Asymmetric lsh (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Improved asymmetric locality senstive hashing lsh (ALSH) for maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S. Dhillon"], "venue": "Knowledge and Information Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A scalable asynchronous distributed algorithm for topic modeling", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Hyokun Yun", "S.V.N. Vishwanathan", "Inderjit S. Dhillon"], "venue": "In Proceedings of the International World Wide Web Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Large-scale multi-label learning with missing labels", "author": ["Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit S. Dhillon"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Matrix factorization (MF) based recommender system [7, 10] is one of the most important applications.", "startOffset": 51, "endOffset": 58}, {"referenceID": 9, "context": "Matrix factorization (MF) based recommender system [7, 10] is one of the most important applications.", "startOffset": 51, "endOffset": 58}, {"referenceID": 5, "context": "When both m and n are large, the prediction procedure is extremely time consuming; it is even slower than the training 1When only the largest B elements are required, the sorting procedure can be reduced to O(n+ B logB) on average using a selection algorithm [6].", "startOffset": 259, "endOffset": 262}, {"referenceID": 14, "context": "In addition, MIPS can be found in many other machine learning applications, such as the prediction for a multi-class or multi-label classifier [15, 18], an object detector, a structure SVM predicator, and many others.", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "In addition, MIPS can be found in many other machine learning applications, such as the prediction for a multi-class or multi-label classifier [15, 18], an object detector, a structure SVM predicator, and many others.", "startOffset": 143, "endOffset": 151}, {"referenceID": 1, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 8, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 10, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 11, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 12, "context": "There is a recent line of research on accelerating MIPS for large n, such as [2, 3, 9, 11\u201313].", "startOffset": 77, "endOffset": 93}, {"referenceID": 1, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 10, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 12, "context": "Our contributions can be summarized as follows: \u2022 We carefully study the MIPS problem and develop Greedy-MIPS, which is a novel algorithm without any nearest neighbor search reduction that is essential in many state-of-the-art approaches [2, 11, 13].", "startOffset": 238, "endOffset": 249}, {"referenceID": 2, "context": "To the best of our knowledge, among existing MIPS approaches, only the sampling approaches proposed in [3, 5] support the similar flexibility under a limited situation where all the candidates and query vectors are non-negative.", "startOffset": 103, "endOffset": 109}, {"referenceID": 4, "context": "To the best of our knowledge, among existing MIPS approaches, only the sampling approaches proposed in [3, 5] support the similar flexibility under a limited situation where all the candidates and query vectors are non-negative.", "startOffset": 103, "endOffset": 109}, {"referenceID": 0, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 1, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 10, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 12, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 13, "context": "Most of existing approaches consider to reduce the MIPS problem to the nearest neighbor search problem (NNS), where the goal is to identify the nearest candidates of the given query, and apply an existing efficient NNS algorithm to solve the reduced problem [1, 2, 11, 13, 14].", "startOffset": 258, "endOffset": 276}, {"referenceID": 1, "context": "[2] is the first MIPS work which adopts such a MIPS-to-NNS reduction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Variants MIPS-to-NNS reduction are also proposed in [13, 14].", "startOffset": 52, "endOffset": 60}, {"referenceID": 13, "context": "Variants MIPS-to-NNS reduction are also proposed in [13, 14].", "startOffset": 52, "endOffset": 60}, {"referenceID": 1, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "Experimental results in [2] show the superiority of the NNS reduction over the traditional branch-and-bound search approaches for MIPS [9, 12].", "startOffset": 135, "endOffset": 142}, {"referenceID": 2, "context": "Fast MIPS approaches with sampling schemes have become popular recently [3, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 4, "context": "Fast MIPS approaches with sampling schemes have become popular recently [3, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 1, "context": "In 1(b), the reduction proposed in [2] is applied to w and {hj}: \u0175 = [w; 0]> and \u0125j = [hj ; \u221a M \u2212 \u2016hj\u2016], \u2200j, where M = maxj \u2016hj\u2016.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "We briefly introduce the concept of the reduction proposed in [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "To handle the situation where candidates have different lengths, [2] proposes the following transform to reduce the original MIPS problem with H and w in a k dimensional space to a new NNS problem with", "startOffset": 65, "endOffset": 68}, {"referenceID": 13, "context": "In [14], another MIPS-to-NNS reduction has been proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In the procedure by [14], all the hj vectors are assumed (or scaled) to have \u2016hj\u2016 \u2264 U, \u2200j, where U < 1 is a positive constant.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Because U < 1, [14] shows that with the transform (3), we have \u2225\u2225\u0125j \u2225\u2225 2 = k\u0304/4 + \u2016hj\u2016 k\u0304+1 , with the second term vanishing as k\u0304 \u2192 \u221e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "Both transforms show a similar empirical performance in [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 12, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 13, "context": "In [11, 13, 14], various locality sensitive hashing schemes have been considered.", "startOffset": 3, "endOffset": 15}, {"referenceID": 1, "context": "In [2], a PCA-tree based approach is proposed, and shows better performance than LSH-based approaches, which is consistent to the empirical observations in [1] and our experimental results shown in Section 5.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In [2], a PCA-tree based approach is proposed, and shows better performance than LSH-based approaches, which is consistent to the empirical observations in [1] and our experimental results shown in Section 5.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "In [1], a simple K-means clustering algorithm is proposed to handled the transformed NNS problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "The idea of the sampling-based MIPS approach is first proposed in [5] as an approach to perform approximate matrix-matrix multiplications.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Its applicability on MIPS problems is studied very recently [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Diamond-MSIPS, a diamond sampling scheme proposed in [3], is an extension of Sample-MSIPS to handle the maximum squared inner product search problem (MSIPS) where the goal is to identify candidate vectors with largest values of ( hj w )2 .", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 134, "endOffset": 137}, {"referenceID": 10, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 12, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 13, "context": "In most existing fast MIPS approaches, the trade-off depends on the approach-specific parameters such as the depth of the PCA tree in [2] or the number of hash functions in [11, 13, 14].", "startOffset": 173, "endOffset": 185}, {"referenceID": 10, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 12, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 13, "context": "Before diving into the details of budgeted MIPS, we first review the essential components in fast MIPS algorithms: \u2022 Before any query request: \u2013 Query-Independent Data Structure Construction: A pre-processing procedure is performed on the entire candidate sets to construct an approach-specific data structure D to store information about H, such as the LSH hash tables [11, 13, 14], space partition trees (e.", "startOffset": 370, "endOffset": 382}, {"referenceID": 1, "context": ", KD-tree or PCA-tree [2]), or cluster centroids [1].", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": ", KD-tree or PCA-tree [2]), or cluster centroids [1].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 1, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 10, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 12, "context": "For example, a vector augmentation is required in all approaches with the MIPS-to-NNS reduction [1, 2, 11, 13].", "startOffset": 96, "endOffset": 110}, {"referenceID": 1, "context": "In addition, [2] also requires another normalization.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 13, "context": "As mentioned earlier, most recently proposed efficient algorithms such as PCA-MIPS [2] and LSH-MIPS [11, 13, 14] adopt the approach to reduce the MIPS problem to an instance of NNS problem, and apply various search space partition data structures or techniques (e.", "startOffset": 100, "endOffset": 112}, {"referenceID": 0, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 1, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 10, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 12, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 13, "context": "Unlike the most recent approaches [1, 2, 11, 13, 14], Greedy-MIPS is an approach without any reduction to a NNS problem.", "startOffset": 34, "endOffset": 52}, {"referenceID": 0, "context": "hs[1] \u2265 hs[2] \u2265 \u00b7 \u00b7 \u00b7 \u2265 hs[n], \u2022 Candidate screening phase: for any given w 6= 0 and B > 0,", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "hs[1] \u2265 hs[2] \u2265 \u00b7 \u00b7 \u00b7 \u2265 hs[n], \u2022 Candidate screening phase: for any given w 6= 0 and B > 0,", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "return { first B elements: {s[1], .", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "One way to store a ranking \u03c0(\u00b7) induced by X is by a sorted index array s[r] of size |X | such that \u03c0(xs[1]) \u2264 \u03c0(xs[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0(xs[|X |]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "One way to store a ranking \u03c0(\u00b7) induced by X is by a sorted index array s[r] of size |X | such that \u03c0(xs[1]) \u2264 \u03c0(xs[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0(xs[|X |]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": ", n such that \u03c0t+(st[1]) \u2264 \u03c0t+(st[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0t+(st[n]), (7)", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": ", n such that \u03c0t+(st[1]) \u2264 \u03c0t+(st[2]) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03c0t+(st[n]), (7)", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "\u03c0t\u2212(st[1]) \u2265 \u03c0t\u2212(st[2]) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c0t\u2212(st[n]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u03c0t\u2212(st[1]) \u2265 \u03c0t\u2212(st[2]) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c0t\u2212(st[n]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "iters[1].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "current() = iters[2].", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "(b) End of iteration-1: C(w) = [6]", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "(c) End of iteration-2: C(w) = [6,1] 5.", "startOffset": 31, "endOffset": 36}, {"referenceID": 0, "context": "(c) End of iteration-2: C(w) = [6,1] 5.", "startOffset": 31, "endOffset": 36}, {"referenceID": 5, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 0, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 6, "context": "(d) End of iteration-3: C(w) = [6, 1,7] Figure 3: Illustration of Algorithm 5 with w = [1, 1, 0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 3, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "The sorted index arrays are shown in the upper part of circles on the right plot for each sub-figure; for example, s1[4] = 7, s2[1] = 6, and s3[5] = 5.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "pop() is executed and 7 = iters[2].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "current() is appended into C(w), we need to advance iters[2] twice because the index j = 1 has been included in C(w).", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "first: buf[i]\u2190 buf[2i] else: buf[i]\u2190 buf[2i+ 1] def top(): return buf[1] \u00b7 \u00b7 \u00b7O(1) def updateValue(t, z): \u00b7 \u00b7 \u00b7O(log k) i\u2190 K\u0304 + t buf[i]\u2190 (z, t) while i > 1: i\u2190 bi/2c if buf[2i].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Instead of hj w, Diamond-MSIPS is designed for the MSIPS problem which is to identify candidates with largest ( hj w )2 or \u2223h>j w \u2223\u2223 values [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "2This setting is used in the experiments in [3]", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "We use the CCD++ [16] algorithm implemented in LIBPMF to solve the above optimization problem and obtain the user embeddings {wi} and item embeddings {hj}.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "We use the same \u03bb used in [4].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "\u2022 NNS-based MIPS approaches: \u2013 PCA-MIPS: the approach proposed in [2], which is shown to be the state-of-the-art among treebased approaches [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "\u2022 NNS-based MIPS approaches: \u2013 PCA-MIPS: the approach proposed in [2], which is shown to be the state-of-the-art among treebased approaches [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "We implement a complete PCA-Tree with the neighborhood boosting techniques described in [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "\u2013 LSH-MIPS: the approach proposed in [11, 13].", "startOffset": 37, "endOffset": 45}, {"referenceID": 12, "context": "\u2013 LSH-MIPS: the approach proposed in [11, 13].", "startOffset": 37, "endOffset": 45}, {"referenceID": 1, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 59, "endOffset": 66}, {"referenceID": 10, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 59, "endOffset": 66}, {"referenceID": 10, "context": "We use the nearest neighbor transform function proposed in [2, 11] and use the random projection scheme as the LSH function as suggested in [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "\u2022 Diamond-MSIPS: the sampling scheme proposed in [3] for the maximum squared inner product search.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "As it shows better performance than LSH-MIPS in [3] in terms of MIPS problems, we also include Diamond-MSIPS into our comparison.", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "F+Tree [17] is implemented as the multinomial sampling procedure.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.", "creator": "LaTeX with hyperref package"}}}