{"id": "1610.03342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "abstract": "We present a model of visually based language learning processes based on stacked recursive neural networks that learn to predict visual characteristics from an image description in the form of a sequence of sounds. The learning task is similar to that of learners of human language who have to discover structure and meaning from loud and ambiguous data across different modalities. We show that our model actually learns to predict characteristics of the visual context from phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of layers: lower layers in the stack are comparatively more form sensitive, while higher layers are more sensitive to meaning.", "histories": [["v1", "Tue, 11 Oct 2016 14:00:28 GMT  (252kb,D)", "http://arxiv.org/abs/1610.03342v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lieke gelderloos", "grzegorz chrupa{\\l}a"], "accepted": false, "id": "1610.03342"}, "pdf": {"name": "1610.03342.pdf", "metadata": {"source": "CRF", "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "authors": ["Lieke Gelderloos", "Grzegorz Chrupa\u0142a"], "emails": ["liekegelderloos@gmail.com", "g.chrupala@uvt.nl"], "sections": [{"heading": null, "text": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning."}, {"heading": "1 Introduction", "text": "Children acquire their native language with little and weak supervision, exploiting noisy correlations between speech, visual, and other sensory signals, as well as via feedback from interaction with their peers and parents. Understanding this process is an important scientific challenge in its own right, but it also has potential to generate insights useful in engineering efforts to design conversational agents or robots. Computationally modeling the ability to learn linguistic form\u2013meaning pairings has been the focus of much research, under scenarios simplified in a variety of ways, for example:\n\u2022 distributional learning from pure word-word co-occurrences with no perceptual grounding (Landauer et al., 1998; Kiros et al., 2015);\n\u2022 cross-situational learning with word sequences and sets of symbols representing sensory input (Siskind, 1996; Fazly et al., 2010);\n\u2022 cross-situational learning using sensory audio and visual input, but with extremely limited sets of words and objects (Roy and Pentland, 2002; Iwahashi, 2003).\nSome recent models have used more naturalistic, larger-scale inputs, for example in cross-modal distributional semantics (Lazaridou et al., 2015) or in implementations of the acquisition process trained on images paired with their descriptions (Chrupa\u0142a et al., 2015). While in these works the representation of the visual scene consists of pixel-level perceptual data, the linguistic input consists of sentences segmented into discrete word symbols. In this paper we take a step towards addressing this major limitation, by using the phonetic transcription of input utterances. While this type of input is symbolic rather than perceptual, it goes a long way toward making the setting more naturalistic, and the acquisition problem more challenging: the learner may need to discover structure corresponding to morphemes, words and phrases in an unsegmented string of phonemes, and the length of the dependencies that need to be detected grows substantially when compared to word-level models.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 0.\n03 34\n2v 1\n[ cs\n.C L\n] 1\n1 O\nct 2\n01 6\nOur contributions We design and implement a simple architecture based on stacked recurrent neural networks with Gated Recurrent Units (Chung et al., 2014): our model processes the utterance phoneme by phoneme while building a distributed low-dimensional semantic representation through a series of recurrent layers. The model learns by projecting its sentence representation to image space and comparing it to the features of the corresponding visual scene.\nWe train this model on a phonetically transcribed version of MS-COCO (Lin et al., 2014) and show that it is able to successfully learn to understand aspects of sentence meaning from the visual signal, and exploits temporal structure in the input. In a number of experiments we show that different levels in the stack of recurrent layers represent different aspects of linguistic structure. Low levels focus on local, short-time-scale spans of the input sequence, and are comparatively more sensitive to form. The top level encodes global aspects of the input sequence and is sensitive to visually salient elements of its meaning."}, {"heading": "2 Related work", "text": "A major part of learning language consists in learning its structure, but in order to be able to communicate it is also of crucial importance to learn the relation of words to entities in the world. Grounded lexical acquisition is often modeled as cross-situational learning, a process of rule-based (Siskind, 1996) or statistical (Fazly et al., 2010; Frank et al., 2007) inference of word-to-referent mappings. Cross-situational models typically work on word-level language input and symbolic representations of the context. However, infants have to learn from continuous perceptual input. Lazaridou et al. (2016) partially remedy this shortcoming and propose a model of learning word meanings from text paired with continuous image representations; the limitation of their work is the toy evaluation dataset.\nRecent experimental and computational studies have found that co-occurring visual information may help to learn word forms (Thiessen, 2010; Cunillera et al., 2010; Glicksohn and Cohen, 2013; Yurovsky et al., 2012). This suggests that acquisition of word form and meaning are interactive, rather than separate.\nThe Cross-channel Early Lexical Learning (CELL) model of Roy and Pentland (2002) and the more recent work of Ra\u0308sa\u0308nen and Rasilo (2015) take into account the continuous nature of the speech signal, and incorporate visual information as well. The CELL model learns to discover words in continuous speech through co-occurence with their visual referent, but the visual input only consists of the shape of single objects, effectively bypassing referential uncertainty. Ra\u0308sa\u0308nen and Rasilo (2015) propose a probabilistic joint model of word segmentation and meaning acquisition from raw speech and a set of possible referents that appear in the context. In both Roy and Pentland (2002) and Ra\u0308sa\u0308nen and Rasilo (2015) the visual context is considerably less noisy and ambiguous than that available to children.\nThere is an extensive line of research on image captioning (see Bernardi et al. (2016) for a recent overview). Typically, captioning models learn to recognize high-level image features and associate them with words. Inspired by both image captioning research and cross-situational human language acquisition, two recent automatic speech recognition models learn to recognize word forms from visual data. In Synnaeve et al. (2014), language input consists of single spoken words and visual data consists of image fragments, which the model learns to associate. Harwath and Glass (2015) employ two convolutional neural networks, a visual object recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. As both Harwath and Glass (2015) and Synnaeve et al. (2014) work with word-sized chunks of speech, they bypass the segmentation problem that human language learners face.\nCharacter-level input representations have recently gained attention in NLP. Ling et al. (2015) and Plank et al. (2016) use bidirectional LSTMs to compose characters into word embeddings, while Chung et al. (2016) propose machine translation model with character level output. These approaches exploit character-level information but crucially they assume that word boundaries are available in the input.\nCharacter-level neural NLP without explicit word boundaries in the input is studied in cases where fixed vocabularies are inherently problematic, e.g. with combined natural and programming language\ninput (Chrupa\u0142a, 2013) or when specifically dealing with misspelled words in automatic writing feedback (Xie et al., 2016).\nCharacter-level language models are analyzed in Hermans and Schrauwen (2013) and Karpathy et al. (2015). Both studies show that character-level recurrent neural networks are sensitive to long-range dependencies: for example by keeping track of opening and closing parentheses over stretches of text. Hermans and Schrauwen (2013) describe the hierarchical organization that emerges during training, with higher layers processing information over longer timescales. In our work we show related effects in a model of visually-grounded language learning from unsegmented phonemic strings.\nWe use phonetic transcription of full sentences as a first step towards large-scale multimodal language learning from speech co-occurring with visual scenes. The use of phonetic transcription rather than raw speech signal simplifies learning and allows us to perform experiments on the encoding of linguistic knowledge as reported in section 4 without additional annotation. Our goal is to model a multi-modal language learning process that includes the segmentation problem faced by human language learners. In contrast to character-level NLP and language modeling approaches, our input data therefore does not contain word boundaries or strong cues such as whitespace and punctuation.\nIn contrast to Roy and Pentland (2002) and Ra\u0308sa\u0308nen and Rasilo (2015), the visual input to our model consists of high-level visual features, which means it contains ambiguity and noise. In contrast to Synnaeve et al. (2014) and Harwath and Glass (2015), we consider full utterances rather than separate words.\nTo our knowledge, there is no work yet on multimodal phoneme or character-level language modeling with visual input. Although the language input in this study is low-level-symbolic rather than perceptual, the learning problem is similar to that of a human language learner: discover language structure as well as meaning, based on ambiguous and noisy data from another modality.\nChrupa\u0142a et al. (2015) simulate visually grounded human language learning in face of noise and ambiguity in the visual domain. Their model predicts visual context given a sequence of words. While the visual input consists of a continuous representation, the language input consists of a sequence of words. The aim of this study is to take their approach one step further towards multimodal language learning from raw perceptual input. Ka\u0301da\u0301r et al. (2016) develop techniques for understanding and interpretation of the representations of linguistic form and meaning in recurrent neural networks, and apply these to word-level models. In our work we share the goal of revealing the nature of emerging representations, but we do not assume words as their basic unit. Also, we are especially concerned with the emergence of a hierarchy of levels of representations in stacked recurrent networks."}, {"heading": "3 Models", "text": "Consider a learner who sees a person pointing at a scene and uttering the unfamiliar phrase Look, the monkeys\u2019re playing. We may suppose that the learner will update her language understanding model such that the subsequent utterance of this phrase will evoke in her mind something close to the impression of this visual scene. Our model is a particular instantiation of this simple idea."}, {"heading": "3.1 Phon GRU", "text": "The architecture of our main model of interest, PHON GRU is schematically depicted in Figure 1 and consists of a phoneme encoding layer, followed by a stack of K Gated Recurrent Neural nets, followed by a densely connected layer which maps the last hidden state of the top recurrent layer to a vector of visual features.\nGated Recurrent Units (GRU) were introduced in Cho et al. (2014) and Chung et al. (2014) as an attempt to alleviate the problem of vanishing gradient in standard simple recurrent nets as known since the work of Elman (1990). GRUs have a linear shortcut through timesteps which bypasses the nonlinearity and thus promotes gradient flow. Specifically, a GRU computes the hidden state at current time step, ht, as the linear combination of previous activation ht\u22121, and a new candidate activation h\u0303t:\ngru(xt,ht\u22121) = (1\u2212 zt) ht\u22121 + zt h\u0303t (1)\nwhere is elementwise multiplication, and the update gate activation zt determines the amount of new information mixed in the current state:\nzt = \u03c3s(Wzxt +Uzht\u22121) (2)\nThe candidate activation is computed as:\nh\u0303t = \u03c3(Wxt +U(rt ht\u22121)) (3)\nThe reset gate rt determines how much of the current input xt is mixed in the previous state ht\u22121 to form the candidate activation:\nrt = \u03c3s(Wrxt +Urht\u22121) (4)\nBy applying the gru function repeatedly a GRU layer maps a sequence of inputs to a sequence of states:\nGRU(X,h0) = gru(xn, . . . , gru(x2, gru(x1,h0))) (5)\nwhere X stands for the matrix composed of input column vectors x1, . . . ,xn. Two or more GRU layers can be composed into a stack:\nGRU2(GRU1(X,h10),h20). (6)\nIn our version of the Stacked GRU architecture we use residualized layers:\nGRUres(X,h0) = GRU(X,h0) +X (7)\nResidual convolutional networks were introduced by He et al. (2015), while Oord et al. (2016) showed their applicability to recurrent architectures. We adopt residualized layers here as we observed they speed up learning in stacks of several GRU layers.\nOur gated recurrent units use steep sigmoids for gate activations:\n\u03c3s(z) = 1\n1 + exp(\u22123.75z)\nand rectified linear units clipped between 0 and 5 for the unit activations:\n\u03c3(z) = clip(0.5(z + abs(z)), 0, 5)\nThere are two more components of our PHON GRU model: the phoneme encoding layer, and mapping from the final state of the top GRU layer to the image feature vector. The phoneme encoding layer is a simply a lookup table E whose columns correspond to one-hot-encoded phoneme vectors. The input\nphoneme pt of utterance p at each step t indexes into the encoding matrix and produces the input column vector:\nxt = E[:, pt]. (8)\nFinally, we map the final state of the top GRU layer hKn to the vector of image features using a fully connected layer:\ni\u0302 = IhKn (9)\nOur main interest lies in recurrent phoneme-level modeling. However, in order to put the performance of the phoneme-level PHON GRU into perspective, we compare it to two word-level models. Importantly, the word models should not be seen as baselines, as they have access to word boundary and word identity information not available to PHON GRU."}, {"heading": "3.2 Word GRU", "text": "The architecture of this model is the same as PHON GRU with the difference that we use words instead of phonemes as input symbols, use learnable word embeddings instead of fixed one-hot phoneme encodings, and reduce the number of layers in the GRU stack. See Section 4 for details."}, {"heading": "3.3 Word Sum", "text": "The second model we use for comparison is a word-based non-sequential model, consisting of a word embedding matrix, a vector sum operator, and a mapping to the image feature vector:\ni\u0302 = I n\u2211\nt=1\nE[:, wt] (10)\nwhere wt is the word at position t in the input utterance. This model simply learns word embeddings which are then summed into a single vector and projected to the target image vector. Thus this model does not have access to word sequence information, and is a distributed analog of a bag-of-words model."}, {"heading": "4 Experiments", "text": "For all experiments, the models were trained on the training set of MS-COCO. MS-COCO contains over 163,000 images accompanied by at least five captions by human annotators. With an average of 7.7 labeled object instances per image, images typically contain more objects than the captions mention, making reference to the scene ambiguous. Textual input for the PHON GRU models was transcribed automatically using the grapheme-to-phoneme functionality with the default English voice of the eSpeak speech synthesis toolkit.1 Stress and pause markers were removed, as well as word boundaries (after storing their position for use in experiments), leaving only phoneme symbols. See Figure 2 for an example transcription.\nVisual input for all models was obtained by forwarding images through the 16-layer convolutional neural network described in Simonyan and Zisserman (2014) pre-trained on Imagenet (Russakovsky et al., 2014), and recording the activation vectors of the pre-softmax layer. The z-score transformation was applied to these features to ease optimization.\nMost of the details of the three model types and training hyperparameters were adopted from related work, and adapted via a limited amount of exploratory experimentation. Exhaustive exploration of the search space was not feasible due to the large number of adjustable settings in these models and their long running time. Given the importance of depth for our purposes, we did systematically explore the number of layers in the PHON GRU and WORD GRU models. A single layer is optimal for WORD GRU. For PHON GRU, see Section 4.1 below. Other important settings were as follows:\n\u2022 All models: Implemented in Theano (Bastien et al., 2012), optimized with Adam (Kingma and Ba, 2014), initial learning rate of 0.0002, minibatch size of 64, gradient norm clipped to 5.0.\n1Available at http://espeak.sourceforge.net\n\u2022 WORD SUM: 1024-dimensional word embeddings, words with frequencies below 10 replaced by UNK token. \u2022 WORD GRU: 1024-dimensional word embeddings, a single 1024 dimensional hidden layer, words with frequencies below 10 replaced by UNK token. \u2022 PHON GRU: 1024-dimensional hidden layers."}, {"heading": "4.1 Prediction of visual features", "text": "The models are trained and evaluated on the prediction of visual feature vectors from captions. While our goal is not to develop an image retrieval method, we use this task as it reflects the ability to extract visually salient semantic information from language. For the experiments on the prediction of visual features all models were trained on the training set of MS-COCO. As validation and test data we used a random sample of 5000 images each from the MS-COCO validation set.\nFigure 3 shows the value of the validation average cosine distance between the predicted visual vector and the target vector for three random initializations of each of the model types.\nThe Phonetic GRU model is more sensitive to the initialization: one can clearly distinguish three separate trajectories. The word-level models are much less affected by random initialization. In terms of the overall performance, the PHON GRU model falls between the WORD SUM model and the WORD GRU model.\nWe also evaluated the models on how well they perform when used to search images: for each validation sentence the model was used to predict the visual vector. The image vectors in the validation data were then ranked by cosine similarity to the predicted vector, and the proportion of times the correct image was among the top 5 was reported. By correct image we mean the one which the sentence was used to describe (even though often many other images are also good matches to the sentence).\nIn Figure 4 we report the validation accuracies on this task for the two word-level models, as well as for the Phon GRU model with different number of hidden layers. We trained each model version with three random initializations for each model setting, and evaluate after each epoch. We report the score of the best epoch for each initialization. The overall ranking of the models matches the direct evaluation of the loss function above: the phoneme-level models are in between the two word-level models. PHON GRU with three hidden layers is the best of the phoneme-level models.\nIn Table 1 we show the accuracies of the best version of each of the models types on the test images; these are also the model versions used in all subsequent experiments. The scores for the WORD GRU are comparable to what Chrupa\u0142a et al. (2015) report for their multitask IMAGINET model, whose visual pathway has the same structure, and who use the same data. More recently, Vendrov et al. (2016) report\nsubstantially higher scores for image search with a word-level GRU model, with the following main differences from our setting: better image features, larger training set, and a loss function optimized for the ranking task.2"}, {"heading": "4.2 Word boundary prediction", "text": "To explore the sensitivity of the PHON GRU model to linguistic structure at the sub-word level, we investigated the encoding of information about word-boundaries in the hidden layers. Logistic regression models were trained on activation patterns of the hidden layers at all timesteps, with the objective of identifying phonemes that preceded a word boundary. For comparison, we also trained logistic regression models on n-gram data to perform the same tasks, with positional phoneme n-grams in the range 1-n. The location of the word boundaries was taken from the eSpeak transcriptions, which mostly matches the location of word boundaries according to conventional English spelling. However, eSpeak models some coarticulation effects which sometimes leads to word boundaries disappearing from the transcription. For example, bank of a river is transcribed as [baNk @v@ \u00f4Iv@].\nAll models were implemented using the LogisticRegression implementation from Scikit-learn (Pedregosa et al., 2011) with L2-regularization. The random samples of 5,000 images each that served as validation and test sets in the visual feature prediction task were used as training and test sets. For the models based on the activation patterns of the hidden layers, the z-score transformation was applied to the activation values to ease optimization. The optimal value of regularization parameter C was determined using GridSearchCV with 5-fold cross validation on the training set, after which the model with the optimal settings was trained on the full training sample.\nTable 2 reports the scores on the test set. The proportion of phonemes preceding a word boundary is 0.29, meaning that predicting no word boundary by default would be correct in 0.71 of cases. At the highest hidden layer, enough information about the word form is available for correct prediction in 0.82 of cases \u2013 substantially above the majority baseline. The lower levels allow for more accurate prediction of word boundaries: 0.86 at the middle hidden layer, and 0.88 at the bottom level. Prediction scores of the logistic regression model based on the activation patterns of the lowest hidden layer are comparable to those of a bigram logistic regression model.\nThese results indicate that information on sub-word structure is only partially encoded by PHON GRU, and is mostly absent by the time the signal from the input propagates to the top layer. The bottom layer does learn to encode a fair amount of word boundary information, but the prediction score substantially below 100% indicates that it is rather selective.\nModel Acc Prec Rec Majority 0.71 Phon GRU Layer 1 0.88 0.82 0.78\nLayer 2 0.86 0.79 0.71 Layer 3 0.82 0.74 0.60\nn-gram n = 1 0.80 0.79 0.41 n = 2 0.87 0.79 0.78 n = 3 0.93 0.86 0.90 n = 4 0.95 0.90 0.93\nTable 2: Prediction scores of logistic regression models based on activation vectors of PHON GRU and on positional n-grams"}, {"heading": "4.3 Word similarity", "text": "To understand the encoding of semantic information in PHON GRU, we analyzed the cosine similarity of activation vectors for word pairs from the MEN Test Collection (Bruni et al., 2014). The MEN\n2We have preliminary results indicating that most of the analyses in the rest of Section 4 show the same general pattern for phoneme models trained following the setting of Vendrov et al. (2016).\ndataset contains 3,000 pairs of English words with semantic similarity judgements on a 50-point scale, which were obtained through crowd-sourcing.3 For each word pair in the MEN dataset, the words were transcribed phonetically using eSpeak and then fed to PHON GRU individually. For comparison, the words were also fed to WORD GRU and WORD SUM. Word pair similarity was quantified as the cosine similarity between the activation patterns of the hidden layers at the end-of-sentence symbol. In contrast to WORD GRU and WORD SUM, PHON GRU has access to the sub-word structure. To explore the role of phonemic form in word similarity, a measure of phonemic difference was included: the Levenshtein distance between the phonetic transcriptions of the two words, normalized by the length of the longer transcription.\nTable 3 shows Spearman\u2019s rank correlation coefficient between human similarity ratings from the MEN dataset and cosine similarity at the last timestep for all hidden layers. In all layers, the cosine similarities between the activation vectors for two words are significantly correlated with human similarity judgements. The strength of the correlation differs considerably between the layers, ranging from 0.09 in the first layer to 0.28 in the highest hidden layer. The second column in Table 3 shows the correlations when only taking into account the 1283 word pairs of which both words appear at least 100 times in the training set of MS-COCO. Correlations for both WORD GRU and WORD SUM are considerably higher than for PHON GRU. This is expected given that these are word level models with explicit wordembeddings, while PHON GRU builds word representations by forwarding phoneme-level input through several layers of processing.\nTable 4 shows Spearman\u2019s rank correlation coefficient between the edit distance and the cosine similarity of activation vectors at the hidden layers of PHON GRU. As expected, edit distance and cosine similarity of the activation vectors are negatively correlated: words which are more similar in form are also more similar according to the model.4\nThe negative correlation between edit distances and cosine similarities is strongest at the lowest hidden layer and weakest, though still present and stronger than for human judgements, at the third hidden layer.\nThe correlations of cosine similarities with edit distance on the one hand, and human similarity rating on the other hand, indicate that the different hidden layers reflect increasing levels of representation: whereas at the lowest level mostly encodes information about form, the highest layer mostly encodes semantic information."}, {"heading": "4.4 Position of shared substrings", "text": "Here we quantify the time-scale at which information is retained in the different layers of PHON GRU. We looked at the location of phoneme strings shared by sentences and their nearest neighbors in the 5,000-image validation sample. We determined each sentence\u2019s nearest neighbor for each hidden layer in PHON GRU. The nearest neighbour is the sentence for which the activation vector at the end of sentence symbol has the smallest cosine distance to the activation vector of the original sentence. The\n3The MEN dataset is available at http://clic.cimec.unitn.it/\u02dcelia.bruni/MEN 4Note that in the MEN dataset, meaning and word form are also (weakly) correlated: human similarity judgements and edit\ndistance are correlated at \u22120.08 (p < 1e\u22125).\nposition of matching substrings is the average position in the original sentence of symbols in substrings that are shared by the neighbor sentences, counted from the end of the sentence. A high mean average substring position thus means that the shared substring(s) appear early in the sentence. This gives an indirect measure of the timescale at which the different layers operate. Table 5 shows an example.\nAs can be seen in Table 6, the average position of shared substrings in neighbor sentences is closest to the end for the first hidden layer and moves towards the beginning of the sentence for the second and third hidden layer. This indicates a difference between the layers with regards to the timescale they represent. Whereas in the lowest layer only information from the latest timesteps is present, the higher layers retain the input signal over longer timescales."}, {"heading": "5 Future work", "text": "Although our analyses show a clear pattern of short-timescale information in the lower layers and larger dependencies in the higher layers, the third layer still encodes information about the phonetic form: its activation patterns were predictive of word boundaries, and similarities between word pairs at this level were more strongly correlated with edit distance than human similarity judgements are. It would be interesting to investigate exactly what information that is, and to what extent it is analogous to language representation in the mind of human speakers. In humans both word phonological form and word meaning can act as primes, which is somewhat reminiscent of the behavior of our model.\nFinally, we would like to take the next step towards grounded learning of language from raw perceptual input, and apply models similar to the one described here to acoustic speech signal coupled with visual input. We expect this to be a challenging but essential endeavor."}], "references": [{"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adrian Muscat", "author": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli IkizlerCinbis", "Frank Keller"], "venue": "and Barbara Plank.", "citeRegEx": "Bernardi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Nam-Khanh Tran", "author": ["Elia Bruni"], "venue": "and Marco Baroni.", "citeRegEx": "Bruni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dzmitry Bahdanau", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Akos K\u00e1d\u00e1r", "author": ["Grzegorz Chrupa\u0142a"], "venue": "and Afra Alishahi.", "citeRegEx": "Chrupa\u0142a et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "Caglar Gulcehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kyunghyun Cho", "author": ["Junyoung Chung"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Estela C\u00e0mara", "author": ["Toni Cunillera", "Matti Laine"], "venue": "and Antoni Rodr\u0131\u0301guez-Fornells.", "citeRegEx": "Cunillera et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Afra Alishahi", "author": ["Afsaneh Fazly"], "venue": "and Suzanne Stevenson.", "citeRegEx": "Fazly et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Joshua B", "author": ["Michael C. Frank", "Noah D. Goodman"], "venue": "Tenenbaum.", "citeRegEx": "Frank et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "The role of cross-modal associations in statistical learning", "author": ["Glicksohn", "Cohen2013] Arit Glicksohn", "Asher Cohen"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "Glicksohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Glicksohn et al\\.", "year": 2013}, {"title": "Deep multimodal semantic embeddings for speech and images. arXiv:1511.03690", "author": ["Harwath", "Glass2015] David Harwath", "James Glass"], "venue": null, "citeRegEx": "Harwath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harwath et al\\.", "year": 2015}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Schrauwen2013] Michiel Hermans", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Language acquisition through a human\u2013robot interface by combining speech, visual, and behavioral information", "author": ["Naoto Iwahashi"], "venue": "Information Sciences,", "citeRegEx": "Iwahashi.,? \\Q2003\\E", "shortCiteRegEx": "Iwahashi.", "year": 2003}, {"title": "Grzegorz Chrupa\u0142a", "author": ["\u00c1kos K\u00e1d\u00e1r"], "venue": "and Afra Alishahi.", "citeRegEx": "K\u00e1d\u00e1r et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Justin Johnson", "author": ["Andrej Karpathy"], "venue": "and Fei-Fei Li.", "citeRegEx": "Karpathy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Antonio Torralba", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun"], "venue": "and Sanja Fidler.", "citeRegEx": "Kiros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Peter W Foltz", "author": ["Thomas K Landauer"], "venue": "and Darrell Laham.", "citeRegEx": "Landauer et al.1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nghia The Pham", "author": ["Angeliki Lazaridou"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Raquel Fern\u00e1ndez", "author": ["Angeliki Lazaridou", "Grzegorz Chrupa\u0142a"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Piotr Doll\u00e1r", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan"], "venue": "and C Lawrence Zitnick.", "citeRegEx": "Lin et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, Ram\u00f3n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Nal Kalchbrenner", "author": ["Aaron van den Oord"], "venue": "and Koray Kavukcuoglu.", "citeRegEx": "Oord et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Matthieu Perrot", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg", "Jake Vanderplas", "Alexandre Passos", "David Cournapeau", "Matthieu Brucher"], "venue": "and \u00c9douard Duchesnay.", "citeRegEx": "Pedregosa et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2016", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. In Proceedings of ACL", "citeRegEx": "Plank et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning words from sights and sounds: a computational model", "author": ["Roy", "Pentland2002] Deb K Roy", "Alex P Pentland"], "venue": "Cognitive Science,", "citeRegEx": "Roy et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2002}, {"title": "Berg", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C"], "venue": "and Li Fei-Fei.", "citeRegEx": "Russakovsky et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "A computational study of cross-situational techniques for learning wordto-meaning mappings. Cognition, 61(1-2):39\u201391", "author": ["Jeffrey M. Siskind"], "venue": null, "citeRegEx": "Siskind.,? \\Q1996\\E", "shortCiteRegEx": "Siskind.", "year": 1996}, {"title": "Maarten Versteegh", "author": ["Gabriel Synnaeve"], "venue": "and Emmanuel Dupoux.", "citeRegEx": "Synnaeve et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Effects of visual information on adults\u2019 and infants\u2019 auditory statistical learning", "author": ["Erik D Thiessen"], "venue": "Cognitive Science,", "citeRegEx": "Thiessen.,? \\Q2010\\E", "shortCiteRegEx": "Thiessen.", "year": 2010}, {"title": "Sanja Fidler", "author": ["Ivan Vendrov", "Ryan Kiros"], "venue": "and Raquel Urtasun.", "citeRegEx": "Vendrov et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dan Jurafsky", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan"], "venue": "and Andrew Y Ng.", "citeRegEx": "Xie et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Chen Yu", "author": ["Daniel Yurovsky"], "venue": "and Linda B Smith.", "citeRegEx": "Yurovsky et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.", "creator": "LaTeX with hyperref package"}}}