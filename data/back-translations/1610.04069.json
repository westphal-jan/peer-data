{"id": "1610.04069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Truthful Mechanisms for Matching and Clustering in an Ordinal World", "abstract": "Our model is based on the fact that, in many environments, agents cannot express the numerical values of their utility for different outcomes, but are still able to classify the results in their preferred order. In particular, we investigate problems where the basic truth exists in the form of a weighted graph of agent utilities, but the algorithm can only elicit private information from agents in the form of an order of preference for each agent generated by the underlying weights. Against this background, we design truthful algorithms to indicate the true optimal solution to the hidden weights. Our techniques provide universally truthful algorithms for a number of graph problems: a 1.76 approach algorithm for max-weight matching, 2 approach algorithms for max-k matching, a 6 approach algorithm for thought-approach situations, and a hidden 2-approach algorithm for non-compromised outcomes.", "histories": [["v1", "Thu, 13 Oct 2016 13:26:35 GMT  (58kb)", "https://arxiv.org/abs/1610.04069v1", "To appear in the Proceedings of WINE 2016. The introduction of this paper has minor overlap witharXiv:1512.05504but the results are mutually exclusive"], ["v2", "Tue, 18 Oct 2016 22:56:45 GMT  (54kb)", "http://arxiv.org/abs/1610.04069v2", "To appear in the Proceedings of WINE 2016"]], "COMMENTS": "To appear in the Proceedings of WINE 2016. The introduction of this paper has minor overlap witharXiv:1512.05504but the results are mutually exclusive", "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.DS", "authors": ["elliot anshelevich", "shreyas sekar"], "accepted": false, "id": "1610.04069"}, "pdf": {"name": "1610.04069.pdf", "metadata": {"source": "CRF", "title": "Truthful Mechanisms for Matching and Clustering in an Ordinal World", "authors": ["Elliot Anshelevich", "Shreyas Sekar"], "emails": ["eanshel@cs.rpi.edu,", "sekars@rpi.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n04 06\n9v 2\n[ cs\n.G T\n] 1\n8 O\nWe study truthful mechanisms for matching and related problems in a partial information setting, where the agents\u2019 true utilities are hidden, and the algorithm only has access to ordinal preference information. Our model is motivated by the fact that in many settings, agents cannot express the numerical values of their utility for different outcomes, but are still able to rank the outcomes in their order of preference. Specifically, we study problems where the ground truth exists in the form of a weighted graph of agent utilities, but the algorithm can only elicit the agents\u2019 private information in the form of a preference ordering for each agent induced by the underlying weights. Against this backdrop, we design truthful algorithms to approximate the true optimum solution with respect to the hidden weights. Our techniques yield universally truthful algorithms for a number of graph problems: a 1.76-approximation algorithm for Max-Weight Matching, 2-approximation algorithm for Max k-matching, a 6- approximation algorithm for Densest k-subgraph, and a 2-approximation algorithm for Max Traveling Salesman as long as the hidden weights constitute a metric. We also provide improved approximation algorithms for such problems when the agents are not able to lie about their preferences. Our results are the first non-trivial truthful approximation algorithms for these problems, and indicate that in many situations, we can design robust algorithms even when the agents may lie and only provide ordinal information instead of precise utilities."}, {"heading": "1 Introduction", "text": "In recent years, the field of algorithm design has been marked by a steady shift towards newer paradigms that take into the account the behavioral aspects and communication bottlenecks pertaining to self-interested agents. In contrast to traditional algorithms that are assumed to have complete information regarding the inputs, mechanisms that interact with autonomous individuals commonly assume that the input to the algorithm is controlled by the agents themselves. In this context, a natural constraint that governs the process by which the algorithm elicits inputs from these agents is truthfulness : agents cannot improve upon the resulting outcome by misreporting the inputs. Another constraint that has recently gained traction in optimization problems on weighted graphs (where the agents correspond to the nodes) is that of ordinality: here, each agent can only submit a preference list of their neighbors ranked in the order of the edge weights. The need for algorithms that are both truthful and ordinal arises in a number of important settings; however, it is well known that it is impossible to obtain optimum solutions even when the algorithm is required to satisfy only one of these two constraints.\nIn this work, we study the design of approximation algorithms for popular graph optimization problems including matching, clustering, and team formation with the goal of understanding the\ncombined price of truthfulness and ordinality. To be more specific, we consider the above optimization problems on a weighted graph whose vertices represent the agents, and where the edge weights (that correspond to agent utilities) are private to the agents constituting that edge, and pose the following natural question: \u201cHow does a computationally efficient, truthful algorithm that only has access to each agent\u2019s edge weights in the form of preference rankings perform in comparison to an optimal algorithm that has full knowledge of the weighted graph?\u201d.\nTruthfulness in an ordinal world Mechanisms that are either truthful or ordinal have received extensive attention across the spectrum of optimization problems. However, non-trivial algorithms that satisfy both of these considerations exist only for very specific settings [14, 2]. For instance, the price of ordinality (also referred to as distortion) is well understood for a number of applications such as voting [3, 7], matching [16, 5], facility location [14], and subset selection [5, 9]. The common thread in all of these settings where the (input) information is often held by the users is that it may be impossible or prohibitively expensive for the agents to express their full utilities to the mechanism; the same agents may incur a smaller overhead if they communicate preference lists over the other users or candidates in the system. Our main contention in this paper is that in exactly the same types of settings, it is reasonable to expect strategic agents to lie about their preferences if it improves their resulting utilities. Motivated by this, we study ordinal algorithms that are also truthful. Even though such mechanisms are clearly less powerful than their \u2018ordinal but not necessarily truthful\u2019 counterparts, our high level-level contribution is that for several wellstudied graph maximization problems, one can obtain solutions that are only a constant factor away from the (social welfare of the) optimum, omniscient solution.\nModel and Problem Statements The high-level model in this paper is the same as the one in [5], with the addition of truthfulness as a constraint. The common setting for all the problems studied in this work is an undirected, complete weighted graph G whose nodes are the set of self-interested agents N with |N | = N . We use w(x, y) to denote the weight of the edge (x, y) in the graph for x, y \u2208 N . All of the optimization problems studied in this work involve selecting a subset of edges from G that obey some condition, with the objective of maximizing the weight of the edges chosen.\nMax k-Matching Compute the maximum weight matching consisting of exactly k edges. We refer to the k = N2 case as the Weighted Perfect Matching problem.\nk-Sum Clustering Given an integer k, partition the nodes into k disjoint sets (S1, . . . , Sk) of\nequal size in order to maximize \u2211k\ni=1\n\u2211\nx,y\u2208Si w(x, y). (It is assumed that N is divisible by\nk). When k = N/2, k-sum clustering reduces to the weighted perfect matching problem.\nDensest k-subgraph Given an integer k, compute a set S \u2286 N of size k to maximize the weight of the edges inside S.\nMax TSP In the maximum traveling salesman problem, the objective is to compute a tour T (cycle that visits each node in N exactly once) to maximize \u2211(x,y)\u2208T w(x, y).\nA crucial but reasonably natural assumption that we make in this work is that the edge weights satisfy the triangle inequality, i.e., for x, y, z \u2208 N , w(x, y) \u2264 w(x, z)+w(z, y). For the specific kind of the problems that we study, the metric structure occurs in a number of well-motivated environments such as: (i) social networks, where the property captures a specific notion of friendship, (ii) Euclidean metrics: each agent is a point in a metric space which denotes her skills or beliefs, and (iii) edit distances: each agent could be represented by a string over a finite alphabet (for e.g., a gene sequence) and the graph weights represent the edit or Levenshtein distances [24]. The reader\nis asked to refer to [5] for additional details on these specific applications and a mathematical treatment of friendship in social networks.\nOur framework and problem set models a multitude of interesting applications, and not surprisingly, all of the problems described above (with the metric assumption) have been the subject of a dense body of algorithmic work [5, 15, 17, 19]. In many of these applications, it becomes imperative that the algorithm provide good approximation guarantees even in the absence of precise numerical information regarding the graph weights. For instance, one can imagine partitioning a set of wedding guests to form a table assignment (k-sum clustering) or selecting a diverse team of agents in order to tackle a complex task (dense subgraph)."}, {"heading": "Algorithmic Framework", "text": "In this work, we are interested in the design of algorithms that are both ordinal and truthful. Suppose that for any one of the above problems, we are given an instance described by a weighted graph; then an algorithm A for this problem is said to be ordinal if it has access only to a vector of preference orderings induced by the graph weights. That is, the input to this algorithm consists of a set ofN preference orderings reported by each of the agents, where the preference list corresponding to agent i \u2208 I is a ranking over the agents in N \u2212{i} such that \u2200j, k \u2208 N , if i prefers j to k, then w(i, j) \u2265 w(i, k).\nThe algorithm is truthful if no single agent can improve their utility by submitting a preference ordering different from the \u2018true ranking\u2019 induced by the graph weights. Here, the utility of each agent i is simply the total weight of the edges incident to i which are chosen. These utilities have a natural interpretation with respect to the problems considered in this work. For instance, for matching problems, an agent\u2019s utility corresponds to her affinity or weight to the agent to whom she is matched, and for densest subgraph as well as k-sum clustering, the utility is her aggregate weight to the agents in the same team or cluster. Our objective in this paper is to design mechanisms that maximize the overall social welfare, i.e., the sum of the utilities of all the agents. Thus, the goal is to select a maximum-weight set of edges while knowing only ordinal preferences (instead of the true weights w), with even the ordinal preferences possibly being misrepresented by the self-interested agents.\nFinally, A is said to be an ordinal \u03b1-approximation algorithm for \u03b1 \u2265 1 if for any given instance along with the graph weights, the total objective value of the maximum weight solution with respect to the instance weights is at most a factor \u03b1 times the value of the solution returned by A, when the input corresponds to the preference rankings induced by the weights. In other words, such algorithms produce solutions which are always a factor \u03b1 away from optimum, without actually knowing what the weights w are. We conclude by pointing out that despite the extensive body of work on all of the problems described previously, hardly any of the proposed mechanisms satisfy either truthfulness or ordinality (see Related Work for exceptions), motivating the need for a new line of algorithmic thinking."}, {"heading": "Our Contributions", "text": "Our main results are summarized in Table 1. All of the non-matching problems that we study are NP-Hard even in the full information setting [15, 23, 18]. Our truthful ordinal algorithms provide constant approximation factors for a variety of problems in this setting, showing that even if only ordinal information is presented to the algorithm, and even if the agents can lie about their preferences, we can still form solutions efficiently with close to optimal utility. Note that as seen in Table 1, in [5] the authors already gave ordinal approximation algorithms for matching problems: those algorithms were not truthful, however, and achieving non-trivial approximation bounds while always giving players incentive to tell the truth requires significant additional work.\nFor example, even the natural, greedy 2-approximation algorithm for Max k-matching from [5] is not truthful.\nIn addition to considering truthful mechanisms, we also develop new approximation algorithms for the setting where the agents are not able to lie, and thus the algorithm knows their true preference ordering. By dropping the truthfulness constraint, we are able to obtain better approximation factors for clustering, densest subgraph, and max TSP. The improved results are enabled by more involved algorithmic techniques that invariably sacrifice truthfulness; they establish a clear separation between the performance of an unconstrained ordinal algorithm and one that is required to be truthful. Techniques: Our proof techniques involve carefully stitching together greedy, random, and serial dictatorship based solutions. Understandably, and perhaps unavoidably for ordinal settings, the algorithmic paradigms that form the bedrock for our mechanisms are rather simple. However, beating the guarantees obtained by a naive application of these techniques involves a more intricate understanding of the interplay between the various approaches. For instance, our algorithm for the weighted perfect matching problem involves mixing between two simple 2-approximation algorithms (greedy, random) to achieve a 1.76-guarantee: towards this end, we establish new tradeoffs between greedy and random matchings showing that when one is far away from the optimum solution, the other one must provably be close to optimum."}, {"heading": "Related Work", "text": "Algorithms proposed in the vast matching literature usually belong to one of two classes: (i) Ordinal algorithms that ignore agent utilities, and focus on (unquantifiable) axiomatic properties such as stability, truthfulness, or other notions of efficiency, and (ii) Optimization algorithms where the numerical utilities are fully specified. Algorithms belonging to the former class usually do not result in good approximations for the hidden optimum utilities, while techniques used in the latter tend to heavily rely on the knowledge of the exact edge weights and are not suitable for this setting. A notable exception to the above dichotomy is the class of optimization problems studying ordinal measures of efficiency [1, 11, 6, 20], for example, the average rank of an agent\u2019s partner in the matching. Such settings usually involve the definition of \u2018new utility functions\u2019 based on given preferences, and thus are fundamentally different from our model where preexisting cardinal utilities give rise to ordinal preferences.\nBroadly speaking, the truthful mechanisms in our work fall under the umbrella of \u2018mechanism design without money\u2019 [2, 8, 13, 16, 22], a recent line of work on designing strategyproof mechanisms for settings like ours, where monetary transfers are irrelevant. A majority of the papers in this domain deal with mechanisms that elicit agent utilities, specifically for one-sided matchings, assignments and facility location problems that are somewhat different from the graph problems we are interested in. The notable exceptions are the recent papers on truthful, ordinal mechanisms\nfor one-sided matchings [16, 8] and general allocation problems [2]. While [16] looks at normalized agent utilities and shows that no ordinal algorithm can provide an approximation factor better than \u0398( \u221a N), [8] considers minimum cost metric matching under a resource augmentation framework. The main differences between our work and these two papers are (1) we consider two-sided matching instead of one-sided, as well as other clustering problems, as well as non-truthful algorithms with better approximation factors, and (2) we consider maximization objectives in which users attempt to maximize their utility instead of minimize their cost. The latter may seem like a small difference, but it completely changes the nature of these problems, allowing us to create many different truthful mechanisms and achieve constant-factor approximations. Finally, [2] looks at the problem of allocating goods to buyers in a \u2018fair fashion\u2019. In that paper, the focus is on maximizing a popular non-linear objective known as the maximin share, which is incompatible with our objective of social welfare maximization. That said, an interesting direction is to see if our techniques extend to other objectives.\nAs discussed in the Introduction, this paper improves on several results from [5]. In [5], the authors focused on the problem of maximum-weight matching for the non-truthful setting, with the main result being an ordinal 1.6-approximation algorithm. In the current paper, we greatly extend the techniques from [5] so that they may be applied to other problems in addition to matching. Moreover, we introduce several new techniques for this setting in order to create truthful algorithms; such algorithms require a somewhat different approach and make much more sense for many of the settings that we are interested in. Other than [8], these are the first known truthful algorithms for matching and clustering with metric utilities.\nOur work is similar in motivation to the growing body of research studying settings where the voter preferences are induced by a set of hidden utilities [3, 7, 10, 4, 9, 14]. The voting protocols in these papers are essentially ordinal approximation algorithms, albeit for a very specific problem of selecting the utility-maximizing candidate from a set of alternatives."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "Truthful Ordinal Mechanisms", "text": "As mentioned previously, we are interested in designing incentive-compatible mechanisms that elicit ordinal preference information from the users, i.e., mechanisms where agents are incentivized to truthfully report their preferences in order to maximize their utility. We now formally define the notions of truthfulness pertinent to our setting. Throughout the rest of this paper, we will use Pi to represent a true ordinal preference of agent i (i.e., one that is induced by the utilities u(i, j)), and si to represent the preference ordering that agent i submits to the mechanisms (which will be equal to Pi if i tells the truth).\nDefinition (Truthful Mechanism) A deterministic mechanism M is said to be truthful if for every i \u2208 N , all ~s\u2212i, s\u2032i, we have that ui(Pi, ~s\u2212i) \u2265 ui(s\u2032i, ~s\u2212i), where ui is the utility guaranteed to agent i by the mechanism.\nDefinition (Universally Truthful Mechanisms) A randomized mechanism is said to be universally truthful if it is a probability distribution over truthful deterministic mechanisms.\nInformally, in a universally truthful mechanism, a user is incentivized to be truthful even when she knows the exact realization of the random variables involved in determining the mechanism.\nDefinition (Truthful in Expectation) A randomized mechanism is said to be truthful in expectation if an agent always maximizes her expected utility by truthfully reporting her preference ranking. The expectation is taken over the different outcomes of the mechanism.\nAll of our algorithms are universally truthful, not just in expectation. The reader is asked to refer to [12] for a useful discussion on the types of randomized mechanisms, and settings where universally truthful mechanisms are strongly preferred as opposed to the mechanisms that only guarantee truthfulness in expectation."}, {"heading": "Approaches for Designing Truthful Matching Mechanisms", "text": "As a concrete first step towards designing truthful ordinal mechanisms, we introduce three highlevel algorithmic paradigms that will form the backbone of all the results in this work. These paradigms are based on the popular algorithmic notions of Greedy, Serial Dictatorship, and Uniformly Random. For each of these paradigms, we develop approaches towards designing truthful mechanisms for the maximum matching problem. In Sections 3 and 4, we develop more sophisticated truthful mechanisms that build upon the simple paradigms presented here, leading to improved approximation factors."}, {"heading": "Greedy via Undominated Edges:", "text": "Our first algorithm is the ordinal analogue of the classic greedy matching algorithm, that has been extensively applied across the matching literature. In order to better understand this algorithm, we first define the notion of an undominated edge.\nDefinition (Undominated Edge) Given a set E of edges, (x, y) \u2208 E is said to be an undominated edge if for all (x, a) and (y, b) in E, w(x, y) \u2265 w(x, a) and w(x, y) \u2265 w(y, b).\nWe make two simple observations here regarding undominated edges based on which we define Algorithm 1.\n1. Every edge set E has at least one undominated edge. In particular, any maximum weight edge in E is obviously an undominated edge.\n2. Given an edge set E, one can efficiently find at least one undominated edge using only the ordinal preference information [5].\nM := \u2205, T is the valid set of edges initialized to the complete graph on N ; while T is not empty do\npick an undominated edge e = (x, y) from T and add it to M ; remove all edges containing x or y from T ; if |M | = k, T = \u2205.\nend\nAlgorithm 1: Greedy Algorithm for Max k-Matching\nIt is not difficult to see that this algorithm gives a 2-approximation for Max-Weight Perfect Matching, and is truthful for that case. Unfortunately, for Max k-Matching with smaller k, it is no longer truthful, and thus none of the algorithms that use Greedy as a subroutine (such as the algorithms from [5]) are truthful.\nProposition 2.1. Algorithm 1 is truthful for the Max k-Matching problem only when k = N2 .\nProof. We need to prove that for any given strategy profile adopted by the other players ~s\u2212i, player i maximizes her utility when she is truthful, i.e., if Pi is the true preference ordering of agent i and s\u2212i is any set of preference orderings for the other agents, then ui(Pi, ~s\u2212i) \u2265 ui(s\u2032i, ~s\u2212i) for any s\u2032i. Our proof will proceed via contradiction and will make use of the following fundamental\nproperty: if Algorithm 1 (for some input) matches agent i to j during some iteration, then both i and j prefer each other to every other agent that is unmatched during the same round.\nWe introduce some notation: suppose that M denotes the matching output by Algorithm 1 for input (Pi, ~s\u2212i), and for every x \u2208 N , m(x) is the agent to whom x is matched to under M . Let ej be the edge added to the matching M in round j of Algorithm 1, denote the round in which i is matched to m(i) as round k. Assume to the contrary that for input (s\u2032i, ~s\u2212i), i is matched to an agent she prefers more than m(i). Let the altered matching be referred to as M \u2032, and let m\u2032(x) be the agent who x is matched with in M \u2032.\nWe begin by proving the following claim: For each j < k, we have that ej \u2208 M \u2032. In other words, all the edges which are included into M before i is matched by Algorithm 1 must appear in both matchings no matter what i does. Once we prove this claim, we are done, since ek is the highest-weight edge from i to any node not in e1, . . . , ek\u22121, so i maximizes its utility by telling the truth and receiving utility equal to the weight of ek.\nTo prove the claim above, we proceed by induction. Note that if k = 1, then i is trivially truthful, since m(i) is its top choice in the entire graph. Now suppose that we have shown the claim for edges e1, . . . , ej\u22121. Let ej = (x, y), and without loss of generality suppose that x is matched in our algorithm constructing M \u2032 before y. At the time that x is matched with m\u2032(x), it must be that m\u2032(x) is the top choice of x from all available nodes. But, by the definition of our algorithm, y is the top choice of x that is not contained in e1, . . . , ej\u22121. Since m\n\u2032(x) is not contained in e1, . . . , ej\u22121 due to our inductive hypothesis, this means that x prefers y over m\n\u2032(x), and since y is not matched yet, this means that x and y will become matched together in M \u2032. Thus, ej is in M\n\u2032 as well. This completes the proof of our claim. To see why this mechanism is not truthful for smaller k, notice that agents which would not be matched in the first k steps have incentive to lie and form undominated edges where none exist, all in order to be matched earlier. Assume that the algorithm uses a deterministic tie-breaking rule to choose between multiple undominated edges in each round. While this does not really alter the final output for the perfect matching problem, the tie-breaking rule may lead to certain undominated edges not getting selected for the final matching.\nFix k and suppose that when the input preferences are truthful, agents i, j are not present in the matching M returned by Algorithm 1. Moreover, suppose that (1) j\u2019s first preference is i, and (2) the deterministic tie-breaking always prefers (i, j) over other edges (one can design preferences so that agents favoured by the tie-breaking are not selected for truthful inputs).\nClearly i has incentive to alter its preferences to identify j as its most preferred node and receive a utility of w(i, j), which is more than its previous utility of zero.\nCan we use a similar approach to design algorithms for the other problems that we are interested in? For k-sum clustering and Densest k-subgraph, one can follow the approach taken in [17, 5], and use the above matching as an intermediate to compute 4-approximations for the above problems. For Max TSP, we can directly leverage the above algorithm by maintaining M as a (forest of) path(s) instead of a matching in order to obtain a 2-approximate Hamiltonian tour. Unfortunately, as we show in the Appendix, these approaches do not lead to truthful algorithms at all."}, {"heading": "Serial Dictatorship", "text": "Another popular approach to compute incentive compatible matchings (albeit usually for one-sided matchings [8, 16]) is serial dictatorship, which we formally define below for our two-sided matching setting.\nProposition 2.2. Algorithm 2 is universally truthful for the Max k-Matching problem for all k.\nSerial dictatorship is among the most prominent of algorithms to feature in this work: our primary approximation algorithms for Max k-matching and Max TSP involve randomized versions\nM := \u2205, T is the set of available agents initialized to N ; while T is not empty do\npick an available agent x arbitrarily from T ; let y denote x\u2019s most preferred agent in T \u2212 {x}; add (x, y) to M ; remove all edges containing x or y from T ; if |M | = k, T = \u2205.\nend\nAlgorithm 2: Serial Dictatorship for Max k-Matching\nof serial dictatorship. Randomness A much simpler approach that is completely oblivious to the input preferences involves selecting a solution uniformly at random. Such an algorithm (described in Algorithm 3) is obviously truthful. Many of the techniques in this paper rely on carefully combining these three types of algorithms in order to produce good approximation factors while retaining truthfulness.\nM := \u2205, T is the valid set of edges initialized to the complete graph on N ; while T is not empty do\npick an edge e = (x, y) from T uniformly at random and add it to M ; remove all edges containing x or y from T ; if |M | = k, T = \u2205.\nend\nAlgorithm 3: Random Algorithm for Max k-matching\nProposition 2.3. Algorithm 3 is universally truthful for the Max k-matching problem for all k."}, {"heading": "3 Truthful Mechanisms for Matching", "text": ""}, {"heading": "Weighted Perfect Matching", "text": "So far, we have looked at two simply approaches for designing truthful mechanisms (Greedy and Random) for the weighted perfect matching problem, both of which yield 2-approximations [5] to the optimum matching. Can we do any better? In [5], the authors use a complex interleaving of greedy and random approaches to extract a non-truthful 1.6-approximation algorithm. In this paper, we instead present a simpler algorithm and rather surprising result: a simple random combination of Algorithms 1 and 3 results in a 1.76-approximation to the optimum matching. The main insight driving this result is the fact that the random and greedy approaches are in some senses complementary to each other, i.e., on instances where the approximation guarantee for the greedy algorithm is close to 2, the random algorithm performs much better.\nTheorem 3.1. The following algorihm is a universally truthful mechanism for the weighted perfect matching problem that obtains a 1.7638-approximation to the optimum matching. Greedy-Random Mix Algorithm for Weighted Perfect Matching: With probability 37 , return the output of Algorithm 1 for k = N2 and with probability 4 7 , return the output of Algorithm 3 for k = N2 .\nProof. Notation\nOur proof mainly involves non-trivial lower bounds on the performance of the random matching which highlights its complementary nature to the greedy matching. As usual, we begin with notation that allows us to divide the greedy matching into several parts for easy analysis.\nDividing Greedy into Two Halves Suppose that GR is the output of the greedy algorithm for the given instance, and RD is the random matching for the same instance. We abuse notation and define T := N (GR 1\n2 ), and GR(T ) = GR 1 2 . Recall that GR 1 2 comprises of the top (max-weight)\nfifty percent of the edges in GR. We will some times refer to T as the top half and the rest of the nodes as the bottom half. Next, define B := N \\ T , and let GR(B) denote GR \\GR(T ). Observe that both T and B consist of exactly N2 nodes. Finally, suppose that w(GR(B)) = xOPT . Sub-Dividing B We will now go one step further and divide the bottom half B into two subparts, B1 and B2, which will aid us in our analysis of the random matching. Define GR(B1) to be the top xN2 edges from GR(B), i.e., GR(B1) := {GR(B)}2x since GR(B) consists only of N4 edges. Finally, GR(B2) is the final part of the greedy matching, i.e., GR(B2) = GR(B) \\GR(B1). As with our previous definitions, B1 and B2 will represent the nodes contained in GR(B1) and GR(B2) respectively.\nWe begin by highlighting some easy observations in order to get familiar with the various sub-matchings defined above.\nProposition 3.2. 1. w(GR(T )) \u2265 12OPT .\n2. B1 consists of xN nodes and B2 consists of ( 1 2 \u2212 x)N nodes.\n3. No edge in GR(B2) can have a weight larger than 2GR(B1)\nxN .\nThe first part of the Proposition comes from Lemma B.7. The last part is simply because this is the average of edge weights in GR(B1).\nThe rest of the proof involves proving new lower bounds on the weight of the random matching as a function of x. Specifically, we will fix the performance of the greedy matching (fix x) and then show that when x is small, the random matching\u2019s weight is close to 58OPT . The reminder of the proof is just basic algebra to bring out the worst-case performance. Let us first formally state our trivial lower bound on the greedy matching.\nProposition 3.3. The weight of the greedy matching is given by:\nw(GR) = w(GR(T )) + w(GR(B)) \u2265 1 2 OPT + xOPT.\nBefore developing the machinery towards our lower bound for the random matching, we will first state our end-goal, which we will prove later. Essentially, our main claim provides an unconditional lower bound for the performance of the random matching as a well as a (conditional) bound for small x, which will serve as the worst-case.\nClaim 3.4. The weight of the random matching is always at least\nE[w(RD)] \u2265 5 8 OPT \u2212 x(1 \u2212 3 2 x)OPT.\nMoreover, when x \u2264 18 , the following is a tighter lower bound for the random matching\nE[w(RD)] \u2265 5 8 OPT \u2212 x(1 \u2212 2x)OPT."}, {"heading": "Tackling the Random Matching for Different Cases", "text": "We will now prove three lemmas that will act as the main bridges to showing Claim 3.4. These lemmas provide insight on the random matching for different cases depending on the relative weights of GR(B1) and GR(B2). First, define \u03b1 := w(GR(B1)) w(GR(B)) . We begin by studying the case when \u03b1 is smaller than 12 , i.e., the weights of the edges in GR(B) are somewhat evenly distributed\nacross GR(B1) and GR(B2). Moreover, since every edge in GR(B1) is larger than every edge in GR(B2), the following is an easy lower bound on \u03b1.\nLemma 3.5. For any given instance where w(GR(B)) = xOPT , we have that \u03b1 \u2265 2x.\nProof. GR(B1) consists of xN 2 edges whereas GR(B) consists of N 4 edges.\nTherefore, the above lemma indicates that when \u03b1 \u2264 12 , x canot be larger than 14 . Now we give the first of the three lemmas.\nLemma 3.6. Suppose that for a given instance with w(GR(B)) = xOPT , w(GR(B1)) = \u03b1xOPT with \u03b1 \u2264 12 . Then, for x \u2264 14 , we have that\nw(RD) \u2265 5 8 OPT \u2212 x(1 \u2212 2x)OPT.\nProof. From Lemma B.13, we get the following generic lower bound for RD since |B| = N2 ,\nw(RD) \u2265 1 2 OPT + 1 N {w(T )\u2212 w(B)}.\nMoreover, applying Lemma B.5 to T , we also get that w(T )N \u2265 18OPT since there exists a matching (GR(T )) solely on the nodes inside T having a weight of OPT2 . Therefore, it suffices to prove an upper bound on w(B)N . Recall that B consists of exactly n = N 2 nodes, GR(B) = xOPT , and w({GR(B)}2x) = \u03b1xOPT \u2264 12xOPT . So, directly applying Lemma B.11, we get that,\n1 n w(B) = 2 N w(B) \u2264 2w(GR(B))(1 \u2212 2x).\nSo, 1Nw(B) \u2264 xOPT (1\u22122x). Putting this inside the generic lower bound for RD, we complete the proof of this lemma.\nWe now have a bound for the case when \u03b1 \u2264 12 . Next, we provide a universal bound for the other case (\u03b1 \u2265 12 ). Observe that in this case, w(GR(B1)) \u2265 w(GR(B2)). We leverage the low weight of GR(B2) to prove the following bound.\nLemma 3.7. Suppose that for a given instance with w(GR(B)) = xOPT , w(GR(B1)) = \u03b1xOPT with \u03b1 \u2265 12 . Then, we have that\nw(RD) \u2265 5 8 OPT \u2212 x(1 \u2212 3 2 x)OPT.\nProof. Once again, we begin with a generic lower bound on RD (Corollary B.14) that depends on partitioning the node set N into 3 parts (T,B1, B2). Notice that |B2|N = 12 \u2212 x.\nw(RD) \u2265 1 2 OPT \u2212 x 2 OPT + 1 N {w(T ) + 1 2 (w(T,B1)\u2212 w(B1, B2))\u2212 w(B2)}.\nAs with Lemma 3.6, we know that w(T )N \u2265 18OPT . Now for every edge e in GR(T ), note that the triangle inequality implies that for any node in B1, going to that node from an endpoint of e and coming back to the other endpoint of e is larger than the weight of e. Summing these up, we get that w(T,B1) \u2265 |B1|GR(T ). Using the fact that GR(T ) \u2265 OPT2 gives a slightly simplified version.\nw(RD) \u2265 5 8 OPT \u2212 x 4 OPT \u2212 1 N {1 2 w(B1, B2) + w(B2)}.\nSo now, it suffices to prove a lower bound on the negative quantities. From Lemma B.9, we get that w(B1, B2) \u2264 2w(GR(B1))|B2| = 2\u03b1xOPT (1/2\u2212 x)N .\nNext, we have to provide an upper bound on w(B2) in order to complete the proof. We know as per our definitions of GR(B1), GR(B2) that each edge in the latter is no larger than the smallest edge in the former. Moreover, from Proposition 3.2, we know that w\u2217 = 2GR(B1)/xN = 2\u03b1OPT N is an upper bound on the weight of every edge insideGR(B2). So, we can directly turn to Lemma B.10 applied specifically to GR(B2) to obtain\nw(B2) \u2264 2w(GR(B2))( N\n2 \u2212 xN \u2212 t),\nwhere t = w(GR(B2))w\u2217 = N(1\u2212\u03b1)xOPT 2\u03b1OPT = N(1\u2212\u03b1)x 2\u03b1 . In conclusion, we have that\n1\nN w(B2) \u2264 (1 \u2212 \u03b1)xOPT (1 \u2212 2x\u2212 1\u2212 \u03b1 \u03b1 x) \u2264 (1\u2212 \u03b1)xOPT (1 \u2212 2x). (1)\nWe are now ready to complete our (lower) bounds on the negative quantities\n1\n2N w(B1, B2) +\n1\nN w(B2) \u2264 \u03b1xOPT (1/2\u2212 x) + (1\u2212 \u03b1)xOPT (1 \u2212 2x)\n= xOPT (1/2\u2212 x)(\u03b1+ 2\u2212 2\u03b1)\n\u2264 xOPT (1/2\u2212 x)3 2\n(Since \u03b1 \u2265 1 2 )\n= 3 4 xOPT \u2212 3 2 x2OPT.\nPlugging the final inequality into the simplified generic lower bound completes the proof.\nA careful inspection of the proof of Lemma 3.7 reveals that our lower bound is a bit loose in two places where we independently replaced \u03b1 with 12 and 1 respectively to provide a worst-case bound. Unfortunately, as a result, the lower bounds for the \u03b1 \u2265 12 and \u03b1 \u2264 12 cases do not align.\nFor our purposes however, it is enough to show that the two lower bounds apply when x \u2264 18 , which we prove below in the third of our lemmas in this subsection.\nLemma 3.8. Suppose that for a given instance with w(GR(B)) = xOPT with x \u2264 18 , we have w(GR(B1)) = \u03b1xOPT with \u03b1 \u2265 12 . Then, the following lower bound is true\nw(RD) \u2265 5 8 OPT \u2212 x(1 \u2212 2x)OPT.\nProof. The proof of the lemma picks up from the previous Lemma 3.7 with only a few simple tweaks. From Lemma 3.8 (specifically using Inequality 1), we have that\n1\n2N w(B1, B2) +\n1\nN w(B2) \u2264 xOPT {\u03b1(1/2\u2212 x) + (1\u2212 \u03b1)(1 \u2212 x\u2212\nx \u03b1 )}.\nFrom Lemma B.1, we know that the expression inside the curly parenthesis attains its maximum value for \u03b1 = 12 in the given range of x. Therefore, substituting \u03b1 = 1 2 , we get\n1\n2N w(B1, B2) +\n1\nN w(B2) \u2264 OPT {\nx 4 \u2212 x\n2\n2 +\nx 2 \u2212 3 2 x2}.\nDirectly plugging this upper bound into the simplified generic lower bound from Lemma 3.7 is enough to prove the statement in the Lemma.\n(Proof of Claim 3.4) The proof is a direct consequence of Lemmas 3.6, 3.7, and 3.8."}, {"heading": "Final Leg: Proving the Actual Bound", "text": "Proposition 3.3 and Claim 3.4 are the only tools that we require to show the final bound. We prove this in two cases depending on whether or not x \u2264 18 .\nCase I: x \u2264 18 Recall that we pick the random matching with probability p = 47 and the greedy mathing with probability 1 \u2212 p = 37 . Suppose we use w(M) to denote the weight of the matching returned by our algorithm. Then,\nE[w(M)] = (1 \u2212 p)w(GR) + p \u00b7 w(RD)\n\u2265 OPT {(1\u2212 p)(1 2 + x) + (p)( 5 8 \u2212 x+ 2x2)} = OPT {1 2 + p 1 8 + x(1 \u2212 2p) + 2px2}\nSince p is fixed, it is not hard to see that the quantity x(1\u22122p)+2px2 is minimized at x = 12\u2212 14p . Substituting p = 47 , we get OPT E[w(M)] \u2264 1.7638\nCase I: x \u2265 18 In this case, we need to use a weaker lower bound for RD.\nE[w(M)] = (1 \u2212 p)w(GR) + pw(RD)\n\u2265 OPT {(1\u2212 p)(1 2 + x) + (p)( 5 8 \u2212 x+ 3 2 x2)} = OPT {1 2 + p 1 8 + x(1 \u2212 2p) + 3 2 px2}\nUsing basic calculus, we observe the expression in the final line is a non-decreasing function of x in the range [ 18 , 1 2 ] and so, its minimum value is attained at x = 1 8 . Substituting this value above, we get OPTE[w(M)] \u2264 1.7638.\nMax k-Matching\nWe now move on to the more general Max k-matching problem, where the objective is to compute a maximum weight matching consisting only of k \u2264 N2 edges. Our previous results do not carry over to this problem. While we know from [5] that the greedy algorithm is half-optimal, one can easily construct examples where this is not truthful. On the other hand, the random matching algorithm is truthful but its approximation factor can be as large as Nk . Our main result in this section is based on the Random Serial Dictatorship algorithm that in some sense combines the best of greedy and random into a single algorithm. Such algorithms have received attention for other matching problems [8, 16]; ours is the first result showing that these algorithms can approximate the optimum matching up to a small constant factor for metric settings. Specifically, while serial dictatorship is usually easy to analyze, our algorithm greatly exploits the randomness to select good edges in expectation.\nDefinition: Random Serial Dictatorship is the same algorithm as Serial Dictatorship (Algorithm 2), except the agents x from T are picked uniformly at random.\nTheorem 3.9. Random serial dictatorship is a universally truthful mechanism that provides a 2-approximation for the Max k-matching problem."}, {"heading": "4 Truthful Mechanisms for Other Problems", "text": "Densest k-Subgraph\nIn this section we present our truthful, ordinal algorithm for Densest k-subgraph, which requires techniques somewhat different from the ones outlined in Section 2. While \u201cconventional\u201d approaches such as Greedy and Serial Dictatorship do lead to good approximations for this problem, they are not truthful, whereas random approaches are truthful but result in poor worst-case approximation factors. We combat this problem with a somewhat novel approach that combines the best of both worlds by designing a semi-oblivious algorithm that has the following property: if agent i is included in the solution, then changing her preference ordering si does not affect the mechanism\u2019s output.\nS := \u2205, T is the set of available agents initialized to N ; while |S| < k do\npick an anchor agent a and another node x, both uniformly at random from T ; let b denote a\u2019s most preferred agent in T \u2212 {a, x}; with probability 12 , add a, x to S, and set T = T \u2212 {a, x}; with probability 12 , add b, x to S and set T = T \u2212 {a, b, x};\nend\nAlgorithm 4: Hybrid Algorithm for Densest k-Subgraph\nTheorem 4.1. Algorithm 4 is a universally truthful mechanism that yields a 6-approximation for the Densest k-Subgraph problem.\nTo see why this is truthful, note that for any particular choice of the anchor agent a, the only case in which a\u2019s preference ordering makes a difference is when a is definitely not added to the final team. Therefore, by lying a cannot influence her utility in the event that she is actually chosen.\nRemark on size of k Without loss of generality, we assume that k \u2264 N2 so that T does not become empty before |S| = k. When k \u2265 N2 , there is a trivial algorithm that yields a 6-approximation to the optimum densest subgraph (see Appendix D). Since we are interested in asymptotic performance bounds, we also assume that k is even. For the rest of this proof, given any set S, node x, w(S) will denote the total weight of the edges inside S, and w(x, S) := \u2211\nj\u2208S w(x, j).\nProof. Notation We begin by defining some notation pertinent to the analysis. Suppose that our algorithm proceeds in rounds such that in each round, exactly two nodes are added to our set S, and at most 3 nodes are removed from A. Therefore, S consists of 2j nodes after j rounds. For ease of notation, we will number the rounds 2, 4, 6, . . . instead of 1, 2, 3, 4, 5, . . .; thus S has r nodes at the end of round r. Further, define Sr to be the random set of selected nodes after round r, i.e., |Sr| = r for any instantiation of this random set.\nNext, let us examine the inner workings of the algorithm. Look at any round r, the algorithm works by selecting a triplet \u2206r = {a, x, b}, where a is referred to as the anchor node, x is a node selected uniformly at random, and b is a\u2019s most preferred agent in Ar \u2212 {a, x}, (let Ar be the\nrandom set of available nodes at the beginning of round r). For the rest of this proof, we will use \u2206r to denote the random triplet of nodes selected in round r. Notice that for a given (ordered) triplet {a, x, b}, the algorithm adds (a, x) to S with probability half and (b, x) to S also with the same probability.\nFinally, we use OPTr to denote the weight of the optimum solution to the Densest k-subgraph problem when k = r, and Algr to be the expected weight of the solution output by our algorithm for the same cardinality, i.e., Algr = E[w(Sr)]. Let algr+2 represent the expected increase in the weight of the solution output by our algorithm from r to r + 2, i.e., algr+2 = Algr+2 \u2212Algr. We will prove by induction on even r that OPTr \u2264 6Algr. More specifically, we will show that for each r, OPTr \u2212 OPTr\u22122 \u2264 6algr.\nProof by Induction: OPTr \u2264 6Algr Claim 4.2. (Base Case: r = 2) OPT2 \u2264 4Alg2.\nProof. The base case is quite straightforward. Suppose that w\u2217max is the heaviest edge in N . Clearly, OPT2 = w \u2217 max. Next, let a, x \u2208 N be any two agents, and let b denote a\u2019s most preferred agent in N \u2212 {x}. Then, we claim that w(a, x) + w(b, x) \u2265 12w\u2217max. The above claim can be proved in two cases: first, suppose that b is indeed a\u2019s favorite node in N . Then, as per Lemma D.3, w(a, x) + w(b, x) \u2265 w(a, b) \u2265 12w\u2217max. In the second case, if a\u2019s most preferred node in N and N \u2212 {x} do not coincide, the only possibility is that x is a\u2019s most preferred node in N , and by the same lemma w(a, x) \u2265 12w\u2217max.\nTo complete the base case, consider any instantiation of the random triplet, \u22062 = {a, b, x}. We have that S2 = {a, x} with probability 12 and S2 = {b, x} otherwise. Therefore, for this instantiation w(S2) = 1 2 (w(a, x) + w(b, x)) \u2265 14w\u2217max. Taking the expectation over every such triplet, we get the desired base claim.\nInductive Claim: To Prove OPT r+2 \u2264 6Algr+2\nRecall that Sr denotes the random set of chosen nodes at the end of round r. We know from the induction hypothesis that OPTr \u2264 6Algr = 6E[w(Sr)]. Consider some specific instantiation of Sr, call it S\u0304r, and for this instantiation, let \u2206\u0304r+2 = {a, x, b} denote some random triplet selected by the algorithm in round r+2, i.e., we have a specific instantiation of Sr and \u2206r+2 for our algorithm. As usual, for this triplet, (a, b, x), a is the anchor node, x is the random node and b is a\u2019s most preferred node in N \\ {S\u0304r \u222a {a, x}}.\nSuppose that algr+2 is the increase in the expected weight of the solution returned by our algorithm during round r + 2 for this specific instantiation of S\u0304r, \u2206\u0304r+2., i.e., algr+2 = 1 2 [w(S\u0304r \u222a {a, x})+w(S\u0304r\u222a{b, x})]\u2212w(S\u0304r). Our proof will proceed as follows: we establish an upper bound for OPTr+2 \u2212OPTr in terms of algr+2, and then take the expectation over all possible instantiations to get the actual bound.\nBefore starting with the proof of the inductive claim, we define some auxiliary notation that will allow us to process OPT as a sequence of additions in each round, so that we can compare the addition to OPT in round r+2 to that of our algorithm in the same round. Fix p, q to be any two nodes in OPTr+2 \\ S\u0304r, and let T := OPTr+2 \\ {p, q}. T will act as a proxy to OPTr in our proofs. Notice that p, q \u2208 N \\ S\u0304r. Finally, in order to avoid messy notation, assume that b is a\u2019s (most) preferred node in N \\ S\u0304r. If this is not the case (and this can happen with a small probability), then a\u2019s most preferred node in N \\ S\u0304r has to be x. We deal with this case separately in Section 4 although the proof is quite similar.\nWe begin with a nice lower bound for algr+2. Suppose that w(a, b) = w \u2217 a.\nLemma 4.3. (Lower Bound for our Algorithm)\nalgr+2 \u2265 1\n6 [w(a, S\u0304r \u2229 T ) + w(b, S\u0304r \u2229 T )] +\n1 3 |S\u0304r \u2229 T |w\u2217a + 1 2 (|S\u0304r \\ T |+ 1)w\u2217a + 1 r \u2212 1w(S\u0304r).\nProof. Recall that algr+2 = 1 2 [w(S\u0304r \u222a{a, x})+w(S\u0304r \u222a{b, x})]\u2212w(S\u0304r). Simplifying the expression, we get\nalgr+2 = 1\n2 [w(a, S\u0304r) + w(b, S\u0304r) + w(a, x) + w(b, x)] + w(x, S\u0304r) (2)\nConsider the first two terms inside the square brackets. We can divide S\u0304r into S\u0304r\u2229T and S\u0304r \\T and simplify the two parts as follows,\nw(a, S\u0304r \u2229 T ) + w(b, S\u0304r \u2229 T ) \u2265 1\n3 [w(a, S\u0304r \u2229 T ) + w(b, S\u0304r \u2229 T )] +\n2 3 |S\u0304r \u2229 T |w\u2217a.\nThe right most term in the RHS simply comes from the triangle inequality since for any i \u2208 S\u0304r \u2229 T , w(i, a) + w(i, b) \u2265 w(a, b) = w\u2217a. Now for the second part, which also follows from the triangle inequality,\nw(a, S\u0304r \\ T ) + w(b, S\u0304r \\ T ) \u2265 |S\u0304r \\ T |w\u2217a. To wrap up the proof, we apply Lemma D.4 to w(x, S\u0304r) to get w(x, S\u0304r) \u2265 1r\u22121w(S\u0304r). An\nadditional 12w \u2217 a can extracted from 1 2 [w(a, x) + w(b, x)]. Adding up the various parts completes the lemma.\nBefore showing our upper bound on OPTr+2 \u2212OPTr, we present a simple lemma that allows us to relate the weights of any given node to the members of a set in terms of w\u2217a and the weight of a to the members of that set. Recall the definitions of p, q \u2208 OPTr+2 \\ S\u0304r. Lemma 4.4. Suppose that T, p, q are as defined previously. Then,\n1. w(p, T ) \u2264 w(a, T \u2229 S\u0304r) + |T \u2229 S\u0304r|w\u2217a + 2|T \\ S\u0304r|w\u2217a.\n2. w(q, T ) \u2264 w(b, T \u2229 S\u0304r) + 2|T \u2229 S\u0304r|w\u2217a + 2|T \\ S\u0304r|w\u2217a.\nProof. (Part I) The proof proceeds as follows: remember that since b is a\u2019s most preferred node in N \\ S\u0304r, for any i /\u2208 S\u0304r, w(i, a) \u2264 w\u2217a. This includes i = p. Moreover, for any i, j /\u2208 S\u0304r, w(i, j) \u2264 2w\u2217a as per Lemma D.3.\nw(p, T ) = \u2211\nj\u2208T\u2229S\u0304r\nw(p, j) + \u2211\nj\u2208T\\S\u0304r\nw(p, j)\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\n[w(p, a) + w(a, j)] + \u2211\nj\u2208T\\S\u0304r\n2w\u2217a\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\n[w\u2217a + w(a, j)] + 2|T \\ S\u0304r|w\u2217a\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\nw(a, j) + |T \u2229 S\u0304r|w\u2217a + 2|T \\ S\u0304r|w\u2217a.\n(Part II) The proof of the second part is almost the same as the first, except that for any i /\u2208 S\u0304r, we have that w(b, i) \u2264 2w\u2217a, once again as the product of Lemma D.3.\nw(q, T ) = \u2211\nj\u2208T\u2229S\u0304r\nw(q, j) + \u2211\nj\u2208T\\S\u0304r\nw(q, j)\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\n[w(q, b) + w(b, j)] + \u2211\nj\u2208T\\S\u0304r\n2w\u2217a\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\n[2w\u2217a + w(b, j)] + 2|T \\ S\u0304r|w\u2217a\n\u2264 \u2211\nj\u2208T\u2229S\u0304r\nw(b, j) + 2|T |w\u2217a.\nUpper Bound on OPTr+2 to Complete the Inductive Claim\nNow we express OPTr+2 \u2212OPTr in terms of algr+2 for the given instantiation. Lemma 4.5.\nOPTr+2 \u2212OPTr \u2264 6algr+2 + 1 r \u2212 1OPTr \u2212 6 r \u2212 1w(S\u0304r).\nProof. Consider OPTr+2 and remember that by definition p, q /\u2208 S\u0304r. We can divide up OPTr+2 in two ways.\nOPTr+2 = w(T ) + w(p, q) + w(p, T ) + w(q, T )\n\u2264 OPTr + w(p, q) + w(p, T ) + w(q, T ). (First)\nOPTr+2 = w(T \u222a {q}) + w(p, q) + w(p, T ) \u2264 OPTr+1 + w(p, q) + w(p, T ). (Second)\nClearly, w(T ) \u2264 OPTr and w(T \u222a {q}) \u2264 OPTr+1. Further, applying Lemma D.2, we get that OPTr+1 \u2264 OPTr + 2r\u22121OPTr. Using this to simplify the second inequality, we then add the simplified inequalities above and divide by two to get:\nOPTr+2 \u2264 OPTr + 1 r \u2212 1OPTr + w(p, q) + w(p, T ) + 1 2 w(q, T ). (3)\nNext, we simplify the (two) rightmost terms in the RHS to reflect their dependence on w\u2217a, which we recall is the weight of the maximum weight edge containing a in N \\ S\u0304r. Applying Lemma 4.4 (Part 1), we get\nw(p, q) + w(p, T ) \u2264 2w\u2217a + w(a, S\u0304r \u2229 T ) + |T \u2229 S\u0304r|w\u2217a + 2|T \\ S\u0304r|w\u2217a. (4) Similarly, using the second half of Lemma 4.4, we bound w(q, T ).\n1 2 w(q, T ) \u2264 1 2 w(b, T \u2229 S\u0304r) + |T \u2229 S\u0304r|w\u2217a + |T \\ S\u0304r|w\u2217a. (5)\nAdding Equations 4 and 5, and substituting the result into Equation 3, we have that\nOPTr+2 \u2212OPTr \u2264 1\nr \u2212 1OPTr + 3(|T \\ S\u0304r|+ 1)w \u2217 a + w(a, S\u0304r \u2229 T ) + w(b, S\u0304r \u2229 T ) + 2|T \u2229 S\u0304r|w\u2217a\nRecall from Lemma 4.3 that algr+2 \u2265 16w(a, T \u2229 S\u0304r) + w(b, T \u2229 S\u0304r)] + 13 |T \u2229 S\u0304r|w\u2217a + 12 (|T \\ S\u0304r| + 1)w\u2217a + 1r\u22121w(S\u0304r). Therefore, we get our required lemma by writing the RHS of our upper bound on OPTr+2 \u2212OPTr in terms of 6algr+2.\nOPTr+2 \u2212OPTr \u2264 1 r \u2212 1OPTr + 6algr+2 \u2212 6 r \u2212 1w(S\u0304r)\nHaving wrapped up our upper bound, we are ready to prove our actual inductive claim. From our upper bound, we have that for every possible realization of Sr, \u2206r+2, we know that\nOPTr+2 \u2212OPTr \u2264 1r\u22121OPTr + 6algr+2 \u2212 6r\u22121w(S\u0304r). Now, we push to complete our proof,\nOPTr+2 \u2212OPTr \u2264 1 r \u2212 1OPTr + ESr ,\u2206r+2 [6algr+2 \u2212 6 r \u2212 1w(Sr)].\nThe term inside the expectation is clearly 6algr+2 \u2212 6r\u22121E[w(Sr)] \u2264 6algr+2 \u2212 1r\u22121OPTr; the final inequality is a result of the induction hypothesis. The term 1r\u22121OPTr cancels out, giving us our desired claim\nOPTr+2 \u2264 OPTr + 6algr+2 \u2264 6E[w(Sr)] + 6algr+2 = 6E[w(Sr+2)] = 6Algr+2.\nTherefore our hybrid algorithm for Densest k-subgraph always returns a solution that is within a sixth of the optimum densest subgraph.\nInductive Claim when w\u2217 a = w(a, x) > w(a, b)\nSuppose that x is a\u2019s most preferred node in N \\ S\u0304r. Claim for claim, the proof proceeds in the same way as above except that the role played by b in the previous proof is now played by x. We go over the proof of this case for completeness. Assume the same notation as before, and consider the following lower bound on algr+2.\nPart 1: Lower Bound\nalgr+2 \u2265 1\n6 [w(a, T \u2229 S\u0304r) + w(x, T \u2229 S\u0304r)] +\n1 3 |T \u2229 S\u0304r|w\u2217a + 1 2 (|T \\ S\u0304r|+ 1)w\u2217a + 1 r \u2212 1w(S\u0304r) (6)\nTo see why Equation 6 is true: first notice that w(a, T \u2229 S\u0304r) +w(x, T \u2229 S\u0304r) \u2265 |T \u2229 S\u0304r|w\u2217a, and w(a, T \\ S\u0304r)+w(x, T \\ S\u0304r) \u2265 |T \\ S\u0304r|w\u2217a by a direct application of the triangle inequality. Therefore, from these two inequalities we glean that\n1 2 [w(a, S\u0304r) + w(x, S\u0304r)] \u2265 1 6 [w(a, T \u2229 S\u0304r) + w(x, T \u2229 S\u0304r)] + 1 3 |T \u2229 S\u0304r|w\u2217a + 1 2 |T \\ S\u0304r|w\u2217a.\nThe remaining terms in algr+2 are 1 2 [w(x, S\u0304r)+w(b, S\u0304r)+w(a, x)+w(b, x)]: (i) From Lemma D.4, 1 2w(x, S\u0304r) \u2265 12(r\u22121)w(S\u0304r), (ii) Via the same lemma, 12w(b, S\u0304r) \u2265 12(r\u22121)w(S\u0304r), and (iii) 12 (w(a, x)+ w(b, x)) \u2265 12w\u2217a since w(a, x) = w\u2217a. Adding (i), (ii), (iii) with our lower bound for 12 [w(a, S\u0304r) + w(x, S\u0304r)] completes the proof of the first part, i.e., Equation 6.\nPart 2: Simplifying Lemma\nNow, we make the second claim for set T as defined previously and q /\u2208 S\u0304r, also as defined previously.\nw(q, T ) \u2264 w(x, T \u2229 S\u0304r) + 2|T \u2229 S\u0304r|w\u2217a + 2|T \\ S\u0304r|w\u2217a. (7) The proof is exactly the same as in Lemma 4.4 so we do not restate it. Once again, the main\nobservation here is that for any j /\u2208 S\u0304r, w(q, j) \u2264 2w\u2217a from Lemma D.3.\nPart 3: Upper Bound\nNow we are ready to complete the proof. Let us begin by restating Equation 3, which is a generic condition and does not depend on a, x or b:\nOPTr+2 \u2264 OPTr + 1 r \u2212 1OPTr + w(p, q) + w(p, T ) + 1 2 w(q, T ).\nFrom Lemma 4.4, we have that (iv) w(p, q)+w(p, T ) \u2264 w(a, T\u2229S\u0304r)+|T\u2229S\u0304r|w\u2217a+2|T \\S\u0304r+1|w\u2217a; (v) From Equation 7, 12w(q, T ) \u2264 w(x, T \u2229 S\u0304r) + |T \u2229 S\u0304r|w\u2217a + |T \\ S\u0304r|w\u2217a.\nAdding these two equations, the rest of the proof follows as from before.\nA 2-approximation algorithm for k-Sum Clustering\nIn the literature, the k-sum clustering problem has only been studied in a full information setting, sometimes amidst the class of dispersion problems [17]. The best known approximation algorithm for this is a 2-approximation that uses the optimum matching as an intermediate. Instead, we give a much simpler algorithm with the same factor that is completely oblivious to the input, and is therefore truthful. Although the analysis of the algorithm involves new upper bounds on the optimum solution, it is still not difficult, so we include this result mostly for completeness.\nRecall that in the Max k-Sum problem, we are provided as input a vector of preference lists P (N ) along with a positive integer 2 \u2264 k \u2264 N2 with the objective being to partition the set of points N into k equal-sized clusters (of size \u03b3 = Nk ; we assume that N is divisible by k) S = (S1, . . . , Sk) to maximize\n\u2211k i=1 \u2211 x,y\u2208Si w(x, y).\nTheorem 4.6. There exists an ordinal universally truthful 2-approximation algorithm for the ksum clustering problem.\nProof. We use a simple approach that picks sets (clusters) of size \u03b3 uniformly at random.\n1. For i = 1 to k\n2. Choose \u03b3 nodes uniformly at random from N .\n3. Remove these nodes from N , and add them to Si.\n4. Return the final solution S.\nLemma 4.7. (Lower Bound) The expected value of the objective function for the clustering returned by our algorithm (S1, . . . , Sk) is exactly\n\u03b3 \u2212 1 N \u2212 1\n\u2211\n(x,y)\u2208N\u00d7N\nw(x, y).\nProof. We proceed via a symmetry argument although it is not hard to verify that the same bound can be obtained via a more exhaustive counting argument. First, by linearity of expectation, we have that the value of the objective (in expectation) is \u2211\n(x,y)\u2208N\u00d7N w(x, y)Pr(x, y \u2208 Si) where the second term is the probability that x and y belong to the same cluster in S. Using a symmetry argument (since our process chooses edges uniformly at random), we claim that the probability Pr(x, y \u2208 Si) is the same for every x, y \u2208 N .\nNow, fix any arbitrary node x \u2208 N : since there \u03b3 \u2212 1 other nodes in the same cluster as x, this means that \u2211\ny 6=x Pr(x, y \u2208 Si) = \u03b3 \u2212 1. Therefore, for every (x, y), Pr(x, y \u2208 Si) = \u03b3\u22121N\u22121 . Substituting this in the expected value of the objective function gives us the desired result.\nLemma 4.8. (Upper Bound) Suppose that O = (O1, . . . , Ok) is the optimum solution for a given instance of the Max k-sum problem. Then, we have the following upper bound on the value of the optimum solution\nk \u2211\ni=1\n\u2211\nx,y\u2208Oi\nw(x, y) \u2264 2(\u03b3 \u2212 1) N \u2212 1\n\u2211\n(x,y)\u2208N\u00d7N\nw(x, y).\nProof. Suppose that x and y are two nodes belonging to the same cluster in O. Then, by the triangle inequality, we have that for every z \u2208 N (including x and y), w(x, z) + w(y, z) \u2265 w(x, y). Summing this up over all z \u2208 N , we have \u2211z\u2208N (w(x, z) + w(y, z)) \u2265 Nw(x, y). Repeating this process over all (x, y) \u2208 S and z \u2208 N , we get\nk \u2211\ni=1\n\u2211\nx,y\u2208Si\n\u2211\nz\u2208N\n(w(x, z) + w(y, z)) \u2265 N k \u2211\ni=1\n\u2211\nx,y\u2208Oi\nw(x, y)\n= NOPT.\nNow, given some edge w(x, z), how many times does this edge appear in the LHS? Without loss of generality, suppose that x \u2208 Oi and z \u2208 Oj . Then, x has \u03b3 \u2212 1 edges inside Oi and w(x, z) appears once in the LHS for each of these neighboring edges. Similarly, z has \u03b3\u22121 edges inside Oj and w(x, z) appears once in the LHS for each edge. Therefore, for every x, z \u2208 N , w(x, z) appears 2(\u03b3 \u2212 1) times in the LHS of the above equation. Substituting this, we prove the lemma,\n\u2211\nx,y\u2208N\n2(\u03b3 \u2212 1)w(x, y) \u2265 NOPT.\nThe rest of the theorem follows immediately from the two lemmas."}, {"heading": "Max Traveling Salesman Problem", "text": "The max traveling salesman problem has received a lot of attention in the literature despite not being as popular as the minimization variant, and has seen a plethora of algorithms for both the metric and the non-metric versions [18, 19]. Such algorithms usually work by looking at the optimum matching and cycle cover and cleverly interspersing the two solutions to form a Hamiltonian cycle. In adapting this approach to our setting, we would be bottlenecked by the best possible ordinal algorithms for the above two problems. Instead, we take a direct approach towards computing a tour and show that a simple algorithm based on Serial Dictatorship results in a 2-approximation factor.\nInitialize T to be a random edge from the complete graph on N ; Let S be the set of available agents initialized to N ; while S 6= \u2205 do\npick one of the end-points of T , say x ; let y denote x\u2019s most preferred agent in S; add (x, y) to T and remove y from S;\nend Complete T to form a Hamiltonian cycle;\nAlgorithm 5: Serial Dictatorship for Max TSP\nTheorem 4.9. Algorithm 5 is a universally truthful mechanism that provides a 2-approximation to the optimum tour. Moreover, the algorithm provides a (2 + \u01eb)-approximation, where \u01eb \u2192 0 as N \u2192 \u221e, even when the edge weights do not obey the metric assumption.\nIt is easy to see that this algorithm is truthful: when an agent i is asked for its preferences, the first edge of T incident to agent i has already been decided, so i cannot affect it. Thus, to form the second edge of T incident to i, it may as well specify its most-preferred edge. Note that the randomization in the first step is essential: if we had selected the first edge based on the input preferences, then the first node could improve its utility by lying, and the algorithm would no longer be strategy-proof."}, {"heading": "5 Non-Truthful Ordinal Mechanisms", "text": "In this section we consider the case when agents are not able to lie, i.e., the algorithm knows their true ordinal preferences Pi, but is still ignorant of the hidden underlying metric utilities which induce those preferences. Designing algorithms for this setting captures the true power of ordinal information, as the necessity for approximation arises from the fact that the algorithm only has limited ordinal information at its disposal, as opposed to the agents being self-interested. This can occur due to the fact that specifying ordinal preferences is much easier than specifying the full numerical utility information; in fact even in the case when such latent numerical utilities exist, it may be difficult for the agents to quantify them precisely. This is the setting studied in papers such as [3, 7, 21, 4, 9]; in [5], the authors previously gave ordinal approximation algorithms for Densest k-Subgraph and Max TSP with approximation factors of 4 and 2.14; here we improve the TSP approximation factor to 1.88 and give a new ordinal bicriteria approximation algorithm which shows that by relaxing the set size k by a small amount for Densest k-Subgraph, a much better approximation can be achieved.\nDensest k-Subgraph\n4-approximation Algorithm\nWe begin by presenting an extremely simple 4-approximation algorithm for this problem; while not explicitly mentioned there, it was alluded to in [5]. Given an input k, the algorithm essentially computes a 2-approximate maximum matching with exactly k2 edges using the algorithm from Theorem 3.9. We then show that the k nodes that make up these edges provide a 4-approximation to the optimum solution for this problem. Unfortunately, despite the truthfulness of RSD for Max k-matching, it is easy to construct examples where this mechanism is no longer truthful for Densest subgraph."}, {"heading": "Bicriteria Approximation", "text": "Among the problems that we study, Densest k-subgraph naturally lends itself to bicriteria approximation algorithms. For instance, when we construct committees, a little additional leverage on the committee size may lead to much more diverse committees. Formally, given a parameter k, an algorithm for Densest k-subgraph is said to be a bi-criteria (\u03b1, \u03b2) approximation if the objective value of the solution S output by the algorithm for every instance is at least a factor 1\u03b1 times that of the optimum solution of size k, and if |S| \u2264 \u03b2k, for \u03b1, \u03b2 \u2265 1. Here, we give bounds on how \u03b1 decreases when \u03b2 increases. In particular, we show that if we are allowed to choose a committee of size 2k, the value of our solution is equal to the optimum solution of size k. If instead we must form a committee of size exactly k, then this results in an ordinal 4-approximation algorithm.\nTheorem 5.1. We can efficiently compute an ordinal ( 4\u03b22 , \u03b2)-approximate solution for the Densest k-subgraph problem for \u03b2 \u2264 2, i.e., a solution of size \u03b2k, whose value is at least \u03b224 times that of the optimum solution of size k.\nThe algorithm that provides us the approximation factor is simple: we compute a greedy matching of size \u03b2k2 , and return its endpoints. However the analysis is quite involved. One of the salient features of this result is that a small change in \u03b2 results in a super-linear improvement in efficiency. For example, in order to obtain a 2-approximation to the Densest k-subgraph, it is enough to compute a set of size \u223c 1.4k. Proof Sketch: The proof involves carefully charging different sets of node distances in the optimal solution to node distances in our solution. So, before giving the main proof, we provide a series of very general charging lemmas. We define a new helpful tool which we call the top-intersecting matching; we are able to use this to establish various bounds which yield our result for Densest k-subgraph. We believe that this tool and the bounds we show using it may be useful in forming other ordinal approximations.\nSpecifically, given a matching M , we will use N(M) to denote the set of nodes which form the endpoints of the edges in M . Suppose that we are provided a matching M of some given size, and a set B \u2286 N(M). Now, given an integer t \u2264 |B|, define M(t, B) to be the top (i.e., highest weight) t edges in M , such each edge in M(t, B) contains at least one node from B. We refer to M(t, B) as the top-intersecting matching. In the proof, we highlight the versatility of the top-intersecting matching by charging different sets of inter-node distances to this matching. Afterwards, we use these charging lemmas to prove the main theorem. To give the flavor of these arguments, we provide some of the upper bounds below. Here we assume that M is a greedy matching of size k, initialized with the complete edge set.\nLemma 5.2. Suppose that M is a greedy matching, and suppose that B and C are two disjoint sets such that B \u2286 N(M) with |B| = 2m, and C \u2229N(M) = \u2205. Then the following bounds hold,\n\u2211\nx,y\u2208B\nw(x, y) \u2264 \u2211\nx,y\u2208N(M(m,B))\u2229B\nw(x, y) + 5r\n2 w(M(m,B))\n\u2211\nx\u2208B,y\u2208C\nw(x, y) \u2264 2|C|w(M(m,B))\n\u2211\nx,y\u2208C\nw(x, y) \u2264 (|C| 2)\n|M | \u2212mw(M \\M(m,B))\nwhere r = |B \\N(M(m,B))|."}, {"heading": "Max Traveling Salesman", "text": "We now present an ordinal (but not truthful) 1.88-approximation algorithm for Max TSP. Unlike most of the algorithms in this paper, this algorithm is rather complex, since it requires carefully balancing several different tour constructions.\nTheorem 5.3. We give an ordinal and efficient approximation algorithms for Max TSP whose approximation factor approaches 3217 \u2248 1.88 as N \u2192 \u221e. Proof Sketch: Before defining our randomized algorithm, we first present the following lemmas: one gives a relationship between matching and Hamiltonian paths and the other shows how to stitch together two paths to form a good tour using only ordinal information.\nLemma 5.4. Given any matching M with k edges, there exists an efficient ordinal algorithm that computes a Hamiltonian path Q containing M such that the weight of the Hamiltonian path in expectation is at least\n[ 3 2 \u2212 1 k ]w(M).\nLemma 5.5. Let H1 and H2 be two Hamiltonian paths on two different sets of nodes, with a and b the endpoints of H1. Then, we can form a tour T by connecting the two paths such that w(T ) \u2265 w(H1) + w(H2) + w(a, b) without knowing the edge weights.\nOur main techniques involve carefully stitching together greedy and random sub-tours, and establishing the tradeoffs between them. Our randomized algorithm returns two tours computed by two different sub-routines with equal probability: these are given by Algorithms 6 and 7.\noutput: Tour T1 Let M be a greedy matching of size k = N3 , and B be the nodes not in M ; Complete M using Lemma 5.4 to form a Hamiltonian path HT on nodes of M ; Form a Hamiltonian path HB on B using the following randomized algorithm.; Randomized Path Algorithm ; Form a random permutation on the nodes in B; Join the nodes in the same order to form the path; (i.e., join the first and second nodes, second and third, and so on.); Final Output T1 is the output formed by using Lemma 5.5 for H1 = HB and H2 = HT .\nAlgorithm 6: First Subroutine of the randomized algorithm for Max TSP\nLemma 5.6. The following is a lower bound on the weight of the tour returned by Algorithm 6\nE[w(T1)] \u2265 [ 3 8 \u2212 3 4N ]w(T \u2217) + 6 N \u2211\nx,y\u2208B\nw(x, y).\nLemma 5.7. The expected weight of the tour returned by Algorithm 7 is at least [ 1116 \u2212 34N ]w(T \u2217)\u2212 6 N \u2211 x,y\u2208B w(x, y).\nThe final bound is obtained by using E[w(T )] = 12 (E(w(T1)] + E[w(T2)])."}, {"heading": "6 Conclusion", "text": "In this paper we study ordinal algorithms, i.e., algorithms which are aware only of preference orderings instead of the hidden weights or utilities which generate such orderings. Perhaps surprisingly, our results indicate that for many problems including Matching, k-sum clustering, Densest\noutput: Tour T2 Let M be a greedy matching of size k = N3 , and B be the nodes not in M ; Select N6 edges uniformly at random from M ; Complete these edges using Lemma 5.4 to form a Hamiltonian path HT with N 3 nodes; Let A be the set of nodes in M but not in HT ; Randomized Alternating Path Algorithm; Initialize HAB = \u2205; Select one node uniformly at random from A; Select one node uniformly at random from B; Add both the nodes to HAB in the same order; Remove them from A and B respectively ; Repeat the above process until A = B = \u2205; Final Output; T2 is the output formed by using Lemma 5.5 for H1 = HAB and H2 = HT .\nAlgorithm 7: Second Subroutine of the randomized algorithm for Max TSP\nSubgraph, and Traveling Salesman, ordinal algorithms perform almost as well as algorithms which know the underlying metric weights, even when the agents involved can lie about their preferences. This indicates that for settings involving strategic agents where it is expensive, or impossible to obtain the true numerical weights or utilities, one can use ordinal mechanisms without much loss in welfare.\nHow do these algorithms stand in comparison to unconstrained ordinal algorithms that do not obey truthfulness? In the full version of this paper, we present non-truthful, ordinal algorithms for the same set of problems including a 4-approximation algorithm for Densest subgraph and a 1.88-approximation algorithm for Max TSP. In conjunction with the ordinal 1.6-approximation algorithm for perfect matching from [5], the improved approximation factors indicate a clear separation between the two classes of algorithms. On the surface, the improvement is not surprising since in many settings, truthfulness often places strong constraints on the set of allowed algorithms and techniques; indeed, all of our truthful mechanisms are derived using the three simple techniques outlined in Section 2. That said, given the absence of matching lower bounds in this work, the resolution of the gap between these two classes of algorithms is perhaps the most important question that is yet to be addressed.\nAcknowledgements This work was supported in part by NSF awards CCF-1527497 and CNS-1218374."}, {"heading": "A Appendix: Proofs from Section 2", "text": ""}, {"heading": "Greedy Algorithms for Other Problems", "text": "We now provide some intuition on why greedy approaches do not lead to truthful algorithms for any of the other problems that we study in this work. Max k-sum and Densest k-subgraph In any clustering problem, using a greedy algorithm (or even serial dictatorship for that matter) could result in agents underplaying their most preferred node if that node will anyway be chosen in a later round. As a concrete example, consider the densest k-subgraph problem with k = 4, and an instance with 6 nodes whose preferences we define partially: a\u2019s top 3 nodes are b, c, d; b\u2019s top two nodes are a and d; c\u2019s first two nodes are a, e; d\u2019s top two preferences are b and e and finally, e, f prefer each other as a first choice.\nConsider the simple algorithm that first picks a matching M with k2 -edges and returns the same set of nodes as in the matching. Moreover, suppose that the algorithm\u2019s tie-breaking involves selecting edges containing and a or b before edges containing e, f and then c, d. Now, under these preferences, we claim that node a stands to improve her utility by lying when all the other agents are being truthful. To see why, first observe that if node a truthfully reports her preferences, the algorithm returns {a, b, e, f} as the solution set. On the other hand, if a lies and points to c as her first preference, then the algorithm picks (a, c) first followed by (b, d) resulting in the set {a, b, c, d}, which is strictly preferable from a\u2019s perspective. A similar example holds for Max k-sum with fixed tie-breaking rules.\nMax TSP The negative example is a bit more subtle for Max TSP. Suppose that the greedy algorithm works by repeatedly picking undominated edges to build a forest of paths (so only edges that do not violate this property are maintained). Remember that an agent\u2019s utility for this problem is simply the sum of weights of the two edges that she is connected to. We construct a specific sub-instance where a node stands to gain by lying about her first preferences in order to increase her aggregate utility. It is not particularly hard to design a full set of preferences consistent with the sub-instance.\nNow, suppose that for a certain instance, the algorithm has already proceeded for a given number of rounds resulting in a forest of three disjoint paths: {a1, a2, a3}, {b1, b2, b3}, and {c1, c2, c3}. Our antagonist-in-chief for this instance will be a separate node x whose first four choices are a1 > a3 > b1 > t, with w(x, t) = 1 and w(x, a1) = w(x, a3) = w(x, b1) = 2. Moreover, suppose that a1\u2019s first and second choices are x > c1, for c1, it is a1 > b1, for b1: c1 > x, and finally, a3\u2019s top choice is x. Now, if x is truthful, the unfolding series of events among these nodes will result in the addition of the edges (x, a1) and (x, t) giving x a total utility of 3. On the other hand, it is preferable for x to make a3 its first preference resulting in x\u2019s two edges being (x, a3) and (x, b1), which is a strictly better solution from x\u2019s perspective.\nB Appendix: Proofs from Section 3: 1.7638-Approximation Algorithm for Weighted Perfect Matching"}, {"heading": "Useful but Generic Lemmas", "text": "We begin with some general lemmas that do not bear any obvious relation to greedy or random matchings, but will be useful in proving some of our results later on.\nLemma B.1. Consider the following function of two variables (x, \u03b1) whose domains are as follows: x \u2208 [0, 0.5] and \u03b1 \u2208 [0, 1].\nf(x, \u03b1) = \u03b1( 1 2 \u2212 x) + (1 \u2212 \u03b1)(1 \u2212 x\u2212 x \u03b1 ).\nFor any fixed x \u2208 [0, 18 ], f(x) is not increasing from \u03b1 = 12 to 1. That is, as long as x \u2208 [0, 18 ],\nmax \u03b1\u2208[0.5,1]\nf(x, \u03b1) = f(x, 1\n2 ).\nProof. For a fixed x, we can differentiate f with respect to \u03b1, and get\n\u2202f \u2202\u03b1 = x \u03b12 \u2212 1 2 .\nThe lemma follows from the observation that the derivative is not positive when \u03b12 \u2265 2x.\nOur next set of lemmas allow us to establish upper bounds on dot-products of weight vectors. For better understanding, one can imagine these vectors to be the weights of the edges in a greedy matching. Specifically, the lemmas help us identify the distribution of the weights that lead to our worst case bounds. We begin with the following trivial lemma.\nLemma B.2. Consider two vectors (w1i ) n i=1 and (w 2 i ) n i=1 which are identical (i.e., w 1 i = w 2 i ), except that \u2203r1 \u2264 r2, \u01eb > 0 such that w1r1 = w2r1 + \u01eb and w1r2 = w2r2 \u2212 \u01eb. Let ~a be any fixed vector of the same length satisfying a1 \u2265 a2 \u2265 . . . \u2265 an. Then,\nn \u2211\ni=1\nw1i ai \u2265 n \u2211\ni=1\nw2i ai.\nBy repeatedly applying the above lemma one can identify weight vectors that dominate all other weight vectors with respect to the above sum. In essence, the above lemma indicates that it is always preferable (higher dot product) to transfer the weights from the larger indices of a vector to smaller indices. By repeatedly using the lemma, one arrives at the following corollary.\nCorollary B.3. Consider two vectors (w1i ) n i=1 and (w 2 i ) n i=1 that satisfy the following conditions\n1. \u2211n i=1 w 1 i = \u2211n i=1 w 2 i\n2. \u2203 some index k such that for every r \u2264 k, w1r \u2265 w2r and for every r > k, w1r \u2264 w2r . Let ~a be any fixed vector of the same length satisfying a1 \u2265 a2 \u2265 . . . \u2265 an. Then,\nn \u2211\ni=1\nw1i ai \u2265 n \u2211\ni=1\nw2i ai.\nOur final lemma is somewhat more specific and identifies a certain condition under which it is useful to transfer weight from the lower indices to the higher indices.\nLemma B.4. Consider two vectors (w1i ) 2n+1 i=1 and (w 2 i ) 2n+1 i=1 that satisfy the following conditions\n1. \u2211n i=1 w 1 i = \u2211n i=1 w 2 i , and \u22112n+1 i=n+1 w 1 i = \u22112n+1 i=n+1 w 2 i\n2. \u2211n i=1 w 2 i = \u22112n+1 i=n+1 w 2 i .\n3. w21 = w0; for every 2 \u2264 i \u2264 2n, w2i = w\u0304 \u2264 w0, and w22n+1 = wf . 4. for every 1 \u2264 i \u2264 2n, w1i = w\u0304 + \u01eb for some \u01eb > 0 and w12n+1 = 0.\nThen, for any N \u2265 2n+ 1, 2n+1 \u2211\ni=1\nw1i (N \u2212 2i) \u2265 2n+1 \u2211\ni=1\nw2i (N \u2212 2i).\nProof. Let us begin by establishing exact relationships between w0, w\u0304, wf , \u01eb. First, we exploit Condition (2) to show that w0 \u2212 w\u0304 = wf . Substituting the values of w2i in Condition (2) gives us,\nw0 + w\u0304(n\u2212 1) = w\u0304n+ wf . Adding and subtracting w\u0304, from the LHS gives us the desired equation. Next, we derive an\nexact formula for \u01eb in terms of w0, w\u0304, wf , by adding up the two halves of Condition (1).\nw0 + (2n\u2212 1)w\u0304 + wf = 2n(w\u0304 + \u01eb). Therefore, \u01eb =\nw0\u2212w\u0304+wf 2n . Now, we can show that the lemma follows from straightforward\nalgebra. Consider the difference between the RHS and LHS of the lemma claim, this is given by,\nRHS \u2212 LHS = (w\u0304 + \u01eb\u2212 w0)(N \u2212 2) + 2n \u2211\ni=2\n\u01eb(N \u2212 2i)\u2212 wf (N \u2212 2(2n+ 1)\n=\n2n \u2211\ni=1\n\u01eb(N \u2212 2i)\u2212 {(w0 \u2212 w\u0304)(N \u2212 2) + wf (N \u2212 2(2n+ 1))}\n= \u01eb(2nN \u2212 2n(2n+ 1))\u2212 {(w0 \u2212 w\u0304)(N \u2212 2) + wf (N \u2212 2(2n+ 1))} = (w0 \u2212 w\u0304 + wf )(N \u2212 2n\u2212 1)\u2212 {(w0 \u2212 w\u0304)(N \u2212 2) + wf (N \u2212 2(2n+ 1))} = wf (2n+ 1)\u2212 (w0 \u2212 w\u0304)(2n\u2212 1)\nSubstituting w0 \u2212 w\u0304 = wf immediately tells us that RHS \u2212 LHS \u2265 0, and thus we complete the lemma."}, {"heading": "General Properties of Matchings", "text": "Here we restate a simple lemma from [5]. Since the proof is quite easy, we re-state the full proof here for completeness.\nLemma B.5. (Upper Bound) Let G = (T,E) be a complete subgraph on the set of nodes T \u2286 S with |T | = n, and let M be any perfect matching on the larger set S. Then, the following is an upper bound on the weight of M ,\nw(M) \u2264 2 n \u2211\nx\u2208T y\u2208T\nw(x, y) + 1\nn\n\u2211\nx\u2208T y\u2208S\\T\nw(x, y)\nProof. Fix an edge e = (x, y) \u2208 M . Then, by the triangle inequality, the following must hold for every node z \u2208 T : w(x, z) + w(y, z) \u2265 w(x, y). Summing this up over all z \u2208 T , we get\n\u2211\nz\u2208T\nw(x, z) + w(y, z) \u2265 nw(x, y) = n(we).\nOnce again, repeating the above process over all e \u2208 M , and then all z \u2208 T we have\nnw(M) \u2264 2 \u2211\nx\u2208T y\u2208T\nw(x, y) + \u2211\nx\u2208T y\u2208S\\T\nw(x, y)\nEach (x, y) \u2208 E appears twice in the RHS: once when we consider the edge in M containing x, and once when we consider the edge with y."}, {"heading": "Properties of Greedy Matchings", "text": ""}, {"heading": "Notation", "text": "We begin with some notation that helps us better characterize the solution returned by the greedy algorithm. Suppose that GR denotes the output (set of edges) of the greedy algorithm (Algorithm 1) for a given instance. Then, we use wGRi to represent the weight of the i\nth largest edge in GR. For the rest of this proof, we will be abusing notation when expressing the total weight of edges inside a set. Specifically, (i) for a set of edges E (for e.g., GR), w(E) (w(GR)) will denote the sum of the weights of edges inside E (total weight of solution returned by greedy algorithm), (ii) for a set of nodes S, w(S) denotes the the total weight of the edges inside the induced graph on S, and (iii) for disjoint S, T \u2286 N , w(S, T ) is the total weight of the edges induced in the (complete) bipartite graph between S, T .\nTop x-matching\nGiven an instance, the corresponding greedy matching GR, and a fraction x \u2264 1, we define the top x-matching GRx, with respect to the given greedy matching to be the set of the top (maximum weight) x fraction of edges inside GR. That is, if |GR| = n, |GRx| = xn and every edge in GRx has a weight no smaller than any edge in GR \\GRx. Finally, let N (GRx) denote the nodes that together make up GRx.\nOur first proposition highlights certain fundamental but very crucial properties that apply to all greedy matchings. These properties, especially local stability, will provide us with a much nicer platform towards a more detailed analysis of the greedy matching.\nProposition B.6. Suppose that GR (|GR| = n) denotes the output of Algorithm 1 for some instance N , GR\u2032 \u2286 GR and T = N (GR\u2032) is the set of nodes that form the edges in GR\u2032.\n1. (Local Stability) Suppose that Algorithm 1 is run on the sub-instance consisting only of the nodes in T with the same set of (sub) preferences. Then its output has to be GR\u2032.\n2. Suppose that GR\u2032 = GRx for some x \u2264 1, and let k = xn+1. Then, in the induced subgraph on N \\ T , the maximum weight of any edge is at most wGRk .\nProof. We prove both the properties by contradiction. Suppose that local stability does not hold and running the greedy algorithm on the subinstance returns a matching GR\u2032\u2032 6= GR\u2032. Consider GR\u2032 = (e1, e2, . . . , er), where the edges are ordered based on the position in which they were selected by Algorithm 1 on the full instance, i.e., e1 was first edge in GR\n\u2032 to be selected by the algorithm, e2 was the second edge and so on.\nDenote by 1 \u2264 t \u2264 r, the smallest index such that et = (x, y) /\u2208 GR\u2032\u2032. Without loss of generality, suppose that the greedy matching algorithm on the sub-instance picks an edge containing x before it picks an edge containing y. Call this edge (x, z); then we know exists some z 6= y, such that (i) (x, z) \u2208 GR\u2032\u2032, and (ii) z \u2208 et2 for some t2 > t. Notice that by then definition of t, (e1, . . . , et\u22121) \u2286 GR\u2032\u2032 and so z cannot belong to any of these edges if it is matched to x.\nConsider the round in which the greedy algorithm on the sub-instance picked (x, z): in this round (x, z) is an undominated edge and y is still available. Therefore, x prefers z to y. However, consider the round in which the original greedy algorithm picked (x, y), clearly (x, y) is once again undominated and z was still avaiable in this round as t2 > t. Therefore, with respect to this algorithm x prefers y to z, which is an overall contradiction since we assumed that the preferences between nodes in T are not altered.\nThe second part of the lemma is easier to show. Assume by contradiction that \u2203 an edge (x\u2217, y\u2217) such that w(x\u2217, y\u2217) > wGRk is the maximum weight edge in the given induced subgraph. A maximum weight edge is always undominated, and an undominated edge never ceases to be\nundominated, therefore (x\u2217, y\u2217) \u2208 GR and more specifically (x\u2217, y\u2217) \u2208 GR \\ GRx. However, this violates the fact that wGRk is the largest weight edge in GR \\GRx.\nLocal stability has powerful consequences. For a given instance, we can take a subset of the greedy matching and show that all of the properties that apply to greedy matchings in general also apply to the subset, as it can be treated as an independent greedy matching. For the rest of this proof, we will treat greedy sub-matchings as independent greedy matchings on sub-instances, when it suits our needs.\nThe next lemma proves a simple but somewhat surprising fact. It is well-known that the greedy matching algorithm provides a 12 -approximation to the optimum matching. However, we show something much stronger: in order to get the same approximation factor, it is enough to consider only the heaviest N4 edges and completely ignore the rest. This allows us to \u2018further optimize\u2019 on the remaining edges using a random matching.\nLemma B.7. Consider some instance N : let GR be the output of the greedy algorithm for this instance, and OPT is the value of the maximum weight perfect matching for this instance. Then,\nw(GR 1 2 ) \u2265 OPT 2 .\nProof. We proceed via the standard charging argument applied to prove the half-optimality of the greedy algorithm. Pick any edge in OPT , say e = (x, y), if (x, y) \u2208 GR, we charge the edge to itself. Otherwise, at least one of x or y must be matched to an edge that yields it the same or better utility, i.e., w.l.o.g, \u2203(x, z) \u2208 GR such that w(x, z) \u2265 w(x, y). In this case, we charge (x, y) to (x, z). Clearly, every edge in GR has anywhere between 0 to 2 edges (from OPT ) assigned to it.\nFor any e \u2208 GR, suppose that se is the number of edges from OPT assigned to e. By our charging argument, the following inequality must be true,\nOPT \u2264 N 2 \u2211\ni=1\nwGRi sei ,\nwhere ei is the i th largest edge belonging to GR. Observe that\n\u2211 N 2\ni=1 sei = N 2 . Consisder\nan alternative \u2018slot vector\u2019, (~q)ei such that qei = 2 if i \u2264 N4 and qei = 0 otherwise. As per Corollary B.3, we know that\nN 2\n\u2211\ni=1\nwGRi sei \u2264 N 2 \u2211\ni=1\nwGRi qei = 2w(GR 1 2 )."}, {"heading": "Greedy Matchings: Induced Distances", "text": "In the following series of lemmas, we prove upper bounds on the total weight of induced edges inside sets of nodes based on associated greedy matchings.\nLemma B.8. Let T be some set of nodes with |T | = n and let GR denote the output of the greedy algorithm for this instance. Then,\nw(T ) \u2264 w(GR) + n/2 \u2211\ni=1\n2wGRi {n\u2212 2i}.\nProof. We know that GR contains n2 edges. Let (xi, yi) denote the i th largest edge in GR, and for any x, let T\u0304x denote the set of nodes not present in GRx, i.e., T\u0304x := N (GR \\GRx). For every i \u2264 n2 , we know from Proposition B.6 that wGRi is the maximum weight edge in T\u0304 2in . Therefore, for every such i, we know that wGRi \u2265 w(xi, j) and wGRi \u2265 w(yi, j) for all j \u2208 T\u0304 2i\nn . Summing these\nup and adding a trivial inequality on both sides, we get\n2|T\u0304 2i n |wGRi + wGRi = 2wGRi (n\u2212 2i) + wGRi \u2265\n\u2211\nj\u2208T\u0304 2i n\n[w(xi, j) + w(yi, j)] + w(xi, yi).\nAdding these up for all i gives us the lemma.\nLemma B.9. Suppose that GR denotes the output of the greedy matching algorithm for an instance N . For some given x, define T := N (GRx) and T\u0304 = N \\ T . Then,\nw(T, T\u0304 ) \u2264 2w(GRx)(|T\u0304x|). The proof is very similar to that of the previous lemma, so we do not go over it again.\nLemma B.10. Consider some set of nodes T with |T | = n and let GR denote the output of the greedy algorithm for this instance. Moreover, suppose that the maximum weight edge in GR, wGR1 \u2264 w\u2217. Then,\nw(T ) \u2264 2w(GR){n\u2212 w(GR) w\u2217 }.\nProof. As a consequence of Lemma B.8, we have that\nw(T ) \u2264 w(GR) + n/2 \u2211\ni=1\n2wGRi {n\u2212 2i}.\nWe know that for all i, wGRi \u2264 w\u2217. Let t = \u230aw(GR)w\u2217 \u230b be the maximum number of w\u2217 values that fit into w(GR) and r = w(GR)\u2212 tw\u2217 be the remainder. Then, we can construct the following alternative weight vector: w1i = w\n\u2217 if i \u2264 t, w1t+1 = r, and w1i = 0, otherwise. Using Lemma B.2 repeatedly, we get a simplified inequality,\nn/2 \u2211\ni=1\n2wGRi {n\u2212 2i} \u2264 n 2 \u2211\ni=1\n2w1i (n\u2212 2i) = t \u2211\ni=1\n2w\u2217(n\u2212 2i) + 2r(n\u2212 2(t+ 1)) =\n= 2w\u2217(nt\u2212 t(t+ 1)) + 2r(n\u2212 2(t+ 1)) = 2(w\u2217t+ r)(n \u2212 t\u2212 1)\u2212 2r(t+ 1) \u2264 2w(GR)(n \u2212 t\u2212 1)\u2212 2(rt+ r2/w\u2217) = 2w(GR)(n\u2212 t\u2212 r\nw\u2217 \u2212 1) =\n= 2w(GR)(n\u2212 w(GR) w\u2217 \u2212 1).\nWe now wrap up the lemma,\nw(T ) \u2264 w(GR) + 2w(GR)(n\u2212 w(GR) w\u2217 \u2212 1) \u2264 2w(GR)(n \u2212 w(GR) w\u2217 ).\nOur next lemma is perhaps among the most crucial and technically involved of the lemmas in this matching proof.\nLemma B.11. Suppose that GR denotes the output of the greedy algorithm for a given instance described by a set T of nodes with |T | = n. Moreover, suppose that for a given x in the range [0, 14 ], we have w(GR2x) \u2264 12w(GR). Then,\n1 n w(T ) \u2264 2w(GR){1\u2212 2x}.\nProof. From Lemma B.8, we know that\nw(T ) \u2264 w(GR) + n/2 \u2211\ni=1\n2wGRi {n\u2212 2i}.\nOur goal for this lemma is to show that (over all possible distributions of greedy edge weights satisfying the condition in the lemma), the maximum value of the second term in the above inequality is obtained when the top 2xn edges of the greedy matching all have the same weight, specifically w(GR)2xn . First, define m = xn, i.e., w GR m is the weight of the smallest edge in GR2x (since GR has n/2 edges, GR2x will have xn edges). Our proof will crucially depend on the weight of mth heaviest edge in GR, so let us use w\u0304 to denote wGRm . We begin with some less ambitious sub-claims before showing the main result.\n(Sub-Claim 1)\nm \u2211\ni=1\n2wGRi {n\u2212 2i} \u2264 2w0(n\u2212 2) + m \u2211\ni=2\n2w\u0304(n\u2212 2i),\nwhere w0 is defined so that w0 + \u2211m i=2 w\u0304 = \u2211m i=1 w GR i . To see why this is the case, consider\nthe two equal-length vectors ~w1 = (w0, w\u0304, . . . , w\u0304) and ~w2 = (w GR 1 , . . . , w GR m ). Since every entity in ~w2 is at least w\u0304 (by definition) and the two vectors sum up to the same quantity, it must be the case that w0 \u2265 w\u0304. So, we can apply our general Corollary B.3 with k = 1 and get the sub-claim. Next, we state a similar claim for the second half of the edges in GR.\n(Sub-Claim 2)\nn 2\n\u2211\ni=m+1\n2wGRi {n\u2212 2i} \u2264 2m \u2211\ni=m+1\n2w\u0304(n\u2212 2i) + 2wf (n\u2212 2(2m+ 1)),\nwhere wf is defined so that \u22112m+1 i=m+1 w\u0304+wf = \u2211\nn 2 i=m+1 w GR i . As per the lemma statement, we\nhave that \u22112m+1 i=m+1 w\u0304 + wf = w(GR) \u2212 w(GR2x) \u2265 w(GR2x) = \u2211m i=1 w GR i = w0 + \u2211m i=2 w\u0304 \u2265 mw\u0304. The proof of Sub-Claim 2 comes from an easy (repeated) application of Lemma B.2 since for every i > m, wGRi \u2264 w\u0304, so we are simply transferring the weights to the smaller indices. Combining both the sub-claims, we get an intermediate inequality, from which it is more convenient to arrive at the lemma.\nn/2 \u2211\ni=1\n2wGRi {n\u2212 2i} \u2264 2w0(n\u2212 2) + 2 2m \u2211\ni=2\nw\u0304(n\u2212 2i) + 2wf (n\u2212 2(2m+ 1)).\nWithout loss of generality, we assume that w0 and wf are defined so that w0 + \u2211m\ni=1 w\u0304 = \u22112m\ni=m+1 w\u0304+wf or else we can always transfer some weight from wf to w0, which only leads to an increase in the right hand side of the above inequality. This is equivalent to saying that the worst case for our lemma is when w(GR2x) = 1 2OPT .\nIf w0 = w\u0304, then wf = 0, and we are done. So suppose that w0 > w\u0304. The following sub-claim completes our proof for the x \u2264 14 case.\n(Sub-Claim 3) 2w0(n\u2212 2) + 2 2m \u2211\ni=2\nw\u0304(n\u2212 2i) + 2wf (n\u2212 2(2m+ 1)) \u2264 2m \u2211\ni=1\n2(w\u0304 + \u01eb)(n\u2212 2i),\nwhere \u01eb > 0 is defined correspondingly in order to maintain the aggregate weight. The sub-claim is directly derived from Lemma B.4.\nFinally, we plug all of these into our original generic bound to get\nw(T ) \u2264 w(GR) + 2 2m \u2211\ni=1\n(w\u0304 + \u01eb){n\u2212 2i} \u2264 2nw(GR)(1 \u2212 2x)."}, {"heading": "Lemmas concerning Random Matchings", "text": "Having carefully laid down the foundation for a careful future analysis of greedy matchings, we now move on to showing simple generic lower bounds for random matchings that are applicable across a variety of situations. We begin with an obvious proposition that sets the stage for more involved bounds.\nProposition B.12. Suppose that RD denotes the random matching for a given instance N . Consider the following two ways of partitioning N : (i) N = T \u222a B with disjoint T,B, and (ii), N = T \u222aB1 \u222aB2, where the 3 sets are once again disjoint. Then,\n1. E[w(RD)] = 1N (w(T ) + w(T,B) + w(B)).\n2. E[w(RD)] = 1N {w(T ) + w(T,B1) + w(B1) + w(T \u222aB1, B2) + w(B2)}.\nLemma B.13. Suppose that RD denotes the random matching for a given instance N . Consider the following two ways of partitioning N : (i) N = T \u222a B with disjoint T,B, and (ii), N = T \u222aB1 \u222aB2, where the 3 sets are once again disjoint. Then, for OPT denoting the weight of the maximum-weight matching,\n1. E[w(RD)] \u2265 1N {w(T ) + |B|OPT \u2212 w(B)}.\n2. E[w(RD)] \u2265 1N {w(T ) + w(T,B1) + w(B1) + |B2|OPT \u2212 w(B2)}\nProof. The first statement of the lemma is obtained by applying Lemma B.5 with w(M) = OPT , S = N , T = B.\nThe second statement is obtained by applying Lemma B.5 with w(M) = OPT , S = N , T = B2.\nOur final corollary is obtained by adding the two parts of Lemma B.13 and dividng by two.\nCorollary B.14. Suppose that RD denotes the random matching for a given instance N . Consider the following two ways of partitioning N : (i) N = T \u222a B with disjoint T,B, and (ii), N = T \u222aB1 \u222aB2, where the 3 sets are once again disjoint. Moreover, suppose that B1, B2 \u2286 B. Then,\nE[w(RD)] \u2265 1 N {w(T ) + 1 2 (w(T,B1) + (|B|+ |B2|)OPT \u2212 w(B1, B2))\u2212 w(B2)}.\nC Appendix: Proofs from Section 3: Max k-Matching\nTheorem 3.9. Random serial dictatorship is a universally truthful mechanism that provides a 2-approximation for the Max k-matching problem.\nProof. Notation: Given any set of nodes S, we use G\u0304(S) to denote the directed first preference graph on S defined as follows: for every i \u2208 S, there is a directed edge from i to its most preferred agent in S \u2212 {i}. Our algorithm could be viewed as selecting one edge from G\u0304(T ) uniformly at random in each iteration, where T denotes the set of available agents.\nFor any set S, define S\u22121 \u2282 S to be the random set of nodes remaining in S after removing one edge uniformly at random from G\u0304(S), i.e., S\u22121 := S \u2212 {i, j} with probability 1|S| for every (i, j) \u2208 G\u0304(S). Finally, we define OPT (S, r) to denote the (weight of the) maximum weight rmatching in S (containing r edges). When it is clear from the context, we will abuse notation and use OPT (S, r) to denote the optimum r-matching itself (as opposed to its value).\nOur proof depends on the following crucial structural claim: we show that for any set S, OPT (S, r) \u2212 E[OPT (S\u22121, r \u2212 1)] is at most twice the weight of an edge chosen uniformly at random from G\u0304(S). This recursive claim implies that if at all we end up selecting a sub-optimal edge, then this does not hurt our solution by much since E[OPT (S\u22121, r\u2212 1)] is bound to be large, and we apply the algorithm recursively on S\u22121.\nClaim C.1. (Structural Claim) For any given set S \u2286 N and r \u2264 |S|2 , we have that\nOPT (S, r) \u2264 E[OPT (S\u22121, r \u2212 1)] + 2|S| \u2211\ne\u2208G\u0304(S)\nw(e)\nIn the above claim, the expectation is taken over the different realizations of S\u22121. In words, the claim bounds the change in the optima using the \u2018increase in profit\u2019 of our algorithm. We first show how this claim can be used to complete the proof of Theorem 3.9, and then detail the proof of Claim C.1.\nProposition C.2. As long as Claim C.1 is obeyed for every S, r, our algorithm provides a 2- approximation to the Max k-matching.\nProof. Suppose that the algorithm proceeds in rounds (1 to k) where in each round exactly one edge is selected from the first preference graph. Define Si to denote the random set of available nodes at the beginning of round i (S1 = N , and is deterministic). Then taking expectation over Claim C.1, we get that for every i \u2264 k,\nESi [OPT (Si, k \u2212 i+ 1)]\u2212 ESi+1 [OPT (Si+1, k \u2212 i)] \u2264 ESi [ 2 |Si| \u2211\ne\u2208G\u0304(Si)\nw(e)].\nMoreover, if we define Algi to denote the expected weight of chosen edge in round i, the term in the RHS is simply twice Algi. Therefore, we can simplify the above inequality as follows.\nESi [OPT (Si, k \u2212 i+ 1)]\u2212 ESi+1 [OPT (Si+1, k \u2212 i)] \u2264 2Algi. (8) We also know that OPT (N , k) can be written as a telescoping summation, OPT (N , k) = \u2211k i=1 E[OPT (Si, k \u2212 i + 1)] \u2212 E[OPT (Si+1, k \u2212 i)]. After bounding the terms in the right hand side of the summation using Equation 8, we complete the proof,\nOPT (N , k)\u2212 E[OPT (Sk+1, 0)] \u2264 k \u2211\ni=1\n2Algi.\nSince E[OPT (Sk+1, 0)] = 0, and the RHS of the above algorithm is exactly the expected weight of the solution returned by our algorithm, the proposition follows.\nIt only remains to prove Claim C.1, which we complete now."}, {"heading": "Proof of Claim C.1", "text": "We need to prove that OPT (S, r) \u2264 E[OPT (S\u22121, r \u2212 1)] + 2|S| \u2211 e\u2208G\u0304(S) w(e). Now, for any i \u2208 S, we use oi to denote the agent i is matched to in OPT (S, r). If the agent is unmatched in OPT (S, r), we let oi be a null element. We also extend the notion of edge weights so that w(i, \u2205) = 0 for all i. Finally, given any i \u2208 S, let si denote i\u2019s most preferred node in S, i.e., the node to which i has an outgoing edge in G\u0304(S).\nSuppose that the (random) serial dictatorship removes the edge (a, sa) from G\u0304(S). We proceed in two cases based on whether or not a is matched to a non-null agent in OPT (S, r). Let E1 denote the subset of edges in G\u0304(S) such that a is matched to an actual agent in OPT , i.e., oa 6= \u2205. Note that sa may or may not be matched in OPT . Then, for any (a, sa) \u2208 E1, we have that\nOPT (S \u2212 {a, sa}, r \u2212 1)) \u2265 OPT (S, r)\u2212 w(a, oa)\u2212 w(sa, osa) + w(oa, osa). That is, OPT (S \u2212 {a, sa}, r \u2212 1) is at least as good as the matching obtained by pairing up oa, osa and leaving the other edges of OPT (S, r). The above inequality is robust to osa being empty.\nObserve that by definition w(a, oa) \u2264 w(a, sa) and via the triangle inequality, w(sa, osa) \u2264 w(oa, sa) + w(oa, osa) \u2264 w(oa, soa) + w(oa, osa). So, we get a simplified charging argument for edges in E1,\nOPT (S \u2212 {a, sa}, r \u2212 1)) \u2265 OPT (S, r)\u2212 w(a, sa)\u2212 w(oa, soa). (9) Finally, let us denote the remaining edges in G\u0304(S) as E2, for every edge in E2; we know that\noa = \u2205, osa may or may not be empty. We claim that for both of these cases\nOPT (S \u2212 {a, sa}, r \u2212 1)) \u2265 OPT (S, r) \u2212 2w(a, sa). (10) The main idea in this case is provided by Lemma D.3, from which we infer that the weight of any edge in OPT (S, r) is at most twice w(a, sa). So, if osa = \u2205, then OPT (S \u2212 {a, sa}, r \u2212 1)) is at least OPT (S, r)\u2212 w(a, oa). In the case that both of them are null, one can simply remove any one edge from OPT (S, r) to get a lower bound for OPT (S \u2212 {a, sa}, r \u2212 1)).\nWe are now ready to complete the proof.\nE[OPT (S\u22121, r \u2212 1)] = 1|S| \u2211\n(a,sa)\u2208G\u0304(S)\nOPT (S \u2212 {a, sa}, r \u2212 1)\n\u2265 OPT (S, r) \u2212 1|S|{ \u2211\n(a,sa)\u2208E1\nw(a, sa) + w(oa, soa) + \u2211\n(a,sa)\u2208E2\n2w(a, sa)}\n\u2265 OPT (S, r) \u2212 1|S| \u2211\n(a,sa)\u2208G\u0304(S)\n2w(a, sa)\nThe crucial observation that leads us from line 2 to line 3 is that for any (a, sa) \u2208 E1, the edge (oa, soa) must also belong to E1. Therefore, in both of these cases, we are only counting w(oa, soa) twice.\nD Densest k-subgraph\nClaim D.1. (Trivial Algorithm) Suppose that k \u2265 N2 , then the algorithm that selects a set of size k uniformly at random from N is a universally truthful 6-approximation algorithm for Densest k-subgraph.\nProof. The truthfulness of this algorithm is quite obvious so we show the approximation factor. A trival upper bound for OPT is OPT \u2264 w(N ), where w(T ) denotes the total weight of the graph induced by T .\nLet S be the random set returned by the above algorithm. Then, for some i, j \u2208 N , what is the probability that i, j \u2208 S: this probability is exactly (\nN\u22122 k\u22122) (Nk) . As expected, the worst case\noccurs when k = N2 , giving us Pr(i, j \u2208 S) \u2265 N/2\u22121 2(N\u22121) \u2265 16 for N > 3. Therefore, we have that E[w(S)] \u2265 16 \u2211 i,j\u2208N w(i, j)."}, {"heading": "General Lemmas", "text": "Before proceeding with our main proof, we take a small detour and prove some generic lemmas that do not depend on our algorithm.\nLemma D.2. For a given instance, let OPTr denote the weight of the densest subgraph of size r. Then,\nOPTr+1 \u2264 OPTr + 2\nr \u2212 1OPTr.\nProof. Suppose that Or+1 denotes the optimum solution to the Densest r + 1-subgraph problem. Then, OPTr+1 = 1 2 \u2211 i\u2208Or+1 \u2211 j\u2208Or+1 w(i, j). Then by the pigeonhole principle, there must exist at least one i \u2208 Or+1, such that \u2211\nj\u2208Or+1 w(i, j) \u2264 2OPTr+1r+1 : call this node i\u0303. Then, we have that\nOPTr \u2265 w(Or+1 \u2212 {i\u0303}) = OPTr+1 \u2212 \u2211\nj\u2208Or+1\nw(\u0303i, j) \u2265 OPTr+1 \u2212 2 OPTr+1 r + 1 .\nThe rightmost term is OPTr+1 r\u22121 r+1 . Transposing the multiplicative factor gives us the lemma.\nLemma D.3. Consider any set of nodes T , and suppose that for some x \u2208 T , y denotes x\u2019s most preferred node in T . Then for any given edge (i, j) \u2208 T \u00d7 T , we have that w(i, j) \u2264 2w(x, y).\nThe lemma follows directly from an application of the triangle inequality. Specifically, w(x, y) is at least half of the weight of the heaviest edge induced in T .\nLemma D.4. Let T \u2286 N be some set of agents and let x be any given node. Then,\nw(x, T ) \u2265 1|T | \u2212 1w(T ).\nProof. The proof comes from the triangle inequality once again. For every edge (i, j) \u2208 T \u00d7 T , we have that w(i, j) \u2264 w(i, x) + w(j, x). Adding this inequality over all edges induced in T , we observe that for each i \u2208 T , w(i, x) appears in the RHS exactly |T |\u2212 1 times, i.e., there are |T |\u2212 1 edges inside T containing i. The rest of the proof follows."}, {"heading": "E Appendix: Proofs from Section 4: Truthful Mechanism", "text": "for Max Traveling Salesman Problem\nTheorem E.1. Algorithm 8 is a universally truthful mechanism that provides a 2-approximation to the optimum tour. Moreover, the algorithm provides a (2 + \u01eb)-approximation to OPT , where \u01eb \u2192 0 as N \u2192 \u221e even when the weights do not obey the metric assumption.\nInitialize T to be a random edge from the complete graph on N ; Let S be the set of available agents initialized to N ; while S 6= \u2205 do\npick one of the end-points of T , say x ; let y denote x\u2019s most preferred agent in S; add (x, y) to T and remove y from S;\nend Complete T to form a Hamiltonian cycle;\nAlgorithm 8: Path Building Serial Dictatorship for Max TSP\nProof. It is easy to see that this algorithms is truthful: when an agent i is asked for its preferences, the first edge of T incident to agent i has already been decided, so i cannot affect it. Thus, to form the second edge of T incident to i, it may as well specify its most-preferred edge.\nThe proof proceeds via a straightforward paradigm where we charge edges in T \u2217, the welfare maximizing tour, to those in T , the solution returned by our algorithm. We first introduce some notation beginning with a simple tie-breaking rule that allows for convenient analysis. Specifically, suppose that (a, b) denotes the first (random) edge added to T . Then, pick one of a or b (say a) uniformly at random, and term this node as the \u2018dead node\u2019. For the rest of algorithm, a does not get to select another edge and remains as an end-point of T . The second edge containing a is added only when the tour is completed to form a cycle. We remark that the randomization in the first step is essential: if we had selected the first edge based on the input preferences, then the first node could improve its utility by lying, and the algorithm would no longer be strategy-proof.\nNext, for any i \u2208 N , we will use t\u22171(i) and t\u22172(i) to denote the two nodes that i is connected to in T \u2217, and t1(i), t2(i) to the nodes connected to i in T . Finally, suppose that er denotes the random edge selected by the algorithm and id, the (random) dead node. In this proof, we show that for any realization of er, id, the optimum tour is at most twice the tour returned by our algorithm. Therefore, the same approximation bound also holds in expectation.\nFix some instantiation of er, id, call it e\u0303r, i\u0303d. Our charging argument comprises of two phases: in the first phase, we charge to the edges in T all of the edges in T \u2217 except the ones containing the dead node i\u0303d. While doing so, we ensure that for each edge in T , at most two edges in T \u2217 are charged to this edge. In the final phase, we carefully charge the edges in T \u2217 containing i\u0303d to certain edges in T that were charged at most once in the first phase."}, {"heading": "First Phase Charging", "text": "Suppose that we use Si to denote the set of available nodes at the instant in our algorithm (for this particular instantiation of er, id) when an edge containing i is added to T . The algorithm then proceeds to pick i\u2019s most preferred agent in Si and adds the corresponding edge to T . Suppose that for every i \u2208 N , t2(i) denotes its most preferred node in Si.\nNow consider any edge (x\u2217, y\u2217) in T \u2217 such that x\u2217, y\u2217 6= i\u0303d. Suppose that x\u2217 was removed from the set of available nodes before y\u2217 during the course of the algorithm. Then, y\u2217 \u2208 Sx\u2217 and so, w(x\u2217, t2(x\n\u2217)) \u2265 w(x\u2217, y\u2217) and we can charge the edge (x\u2217, y\u2217) \u2208 T \u2217 to (x\u2217, t2(x\u2217)) \u2208 T . After repeating this charging for every edge in T \u2217 except (\u0303id, t \u2217 1 (\u0303id)), (\u0303id, t \u2217 2 (\u0303id)), we end up\nwith the following proposition.\nProposition E.2. The following are true at the end of the first phase of charging.\n1. At most two edges in T \u2217 are charged to any one edge in T .\n2. No edges are charged to (\u0303id, t1(\u0303id)), (\u0303id, t2 (\u0303id)) \u2208 T .\n3. At most one edge in T \u2217 is charged to any of the edges in T containing t\u22171 (\u0303id), t \u2217 2 (\u0303id).\nProof. (Statement 1) Consider any edge of the form (i, t2(i)), as per our definitions, i became unavailable before t2(i). Thus, the only edges charged to (i, t2(i)) are those in T\n\u2217 containing i, and there can only be two such edges. Statement 2 Further, suppose that (\u0303id, t1(\u0303id)) denotes the random edge in T . Clearly, edges in T \u2217 containing t1 (\u0303id) are not charged to the random edge. Finally, no edge in OPT is charged to (\u0303id, t2(\u0303id)), since the latter node is the absolute last node to become unavailable. Statement 3 This is a direct consequence of the fact that we have not charged (\u0303id, t \u2217 1 (\u0303id)), (\u0303id, t \u2217 2 (\u0303id))."}, {"heading": "Second Phase Charging", "text": "We use the triangle inequality to charge the edge (\u0303id, t \u2217 1 (\u0303id)):\nw(\u0303id, t \u2217 1 (\u0303id)) \u2264 w(\u0303id, t2(\u0303id)) + w(t2 (\u0303id), t\u22171 (\u0303id)) \u2264 w(\u0303id, t2(\u0303id)) + w(t\u22171 (\u0303id), t2(t\u22171 (\u0303id))).\nThe final inequality is due to the fact that t2 (\u0303id) \u2208 St\u2217 1 (\u0303id) (since t2(\u0303id) is the absolute last node added to the tour, and so it is available during the entire runtime of the algorithm), and so w(t2 (\u0303id), t \u2217 1 (\u0303id)) \u2264 w(t\u22171 (\u0303id), t2(t\u22171 (\u0303id))). Therefore, the edge (\u0303id, t\u22171 (\u0303id)) can be charged to two edges in T , namely (\u0303id, t2(\u0303id)) and (t \u2217 1 (\u0303id), t2(t \u2217 1 (\u0303id))\nUsing exactly the same kind of argument, we can also charge the second edge containing i\u0303d in T \u2217 to two edges in T , namely (\u0303id, t2 (\u0303id)) and w(t \u2217 2 (\u0303id), t2(t \u2217 2 (\u0303id)). This concludes the second phase of charging. In conjunction with Proposition E.2, we have successfully charged every edge in OPT by using at most two edges in T . This completes our two approximation."}, {"heading": "Proof for the Non-Metric Case", "text": "The main idea that leads to the bound for the non-metric case is that the first phase of charging does not use the metric nature of the weights in any way. Therefore, at the end of first phase, we charged all of the edges in T \u2217 minus the ones containing i\u0303d by using at most two edges in T . Note that this is for a particular instantiation.\nTherefore, taking the expectation over every such instantiation, we get that\nEid [w(T \u2217)\u2212 w(id, t\u22171(id))\u2212 w(id, t\u22172(id))] \u2264 2E[w(T )]\nw(T \u2217)\u2212 2 N w(T \u2217) \u2264 2E[w(T )].\nFor the second inequality, we used the fact that for any i \u2208 N , Pr[id = i] = 1N and therefore every edge (x\u2217, y\u2217) in T \u2217 appears with the negative sign twice: once when x\u2217 is dead, and once when y\u2217 is dead.\nThis completes the proof."}, {"heading": "F Appendix: Proofs from Section 5", "text": "Theorem F.1. We can efficiently compute an ordinal ( 4\u03b22 , \u03b2)-approximate solution for the Densest k-subgraph problem for \u03b2 \u2264 2, i.e., a solution of size \u03b2k, whose value is at least \u03b224 times that of the optimum solution of size k.\nProof. The algorithm is described as follows: \u201cLet M be a greedy matching of size \u03b2 k2 . Return S, the set of nodes which are the endpoints of the edges in M\u201d.\nThe proof is somewhat complicated, and involves carefully charging different sets of node distances in S\u2217 to node distances in S. So, before giving the main proof, we provide a series of very general charging lemmas. We begin by defining a somewhat unusual \u2018device\u2019 that guides our charging arguments. For the rest of this proof, given a matching M , we will use N(M) to denote the set of nodes which form the endpoints of the edges in M .\nSuppose that we are provided a matching M of some given size, and a set B \u2286 N(M). Now, given an integer t \u2264 |B|, define M(t, B) to be the top (i.e., highest weight) t edges in M , such each edge in M(t, B) contains at least one node from B. We refer to M(t, B) as the top-intersecting matching. In the following lemmas, we will highlight the versatility of the top-intersecting matching by charging different sets of inter node distances to this matching. Later, we will show this \u2018device\u2019 can be used to prove the main theorem.\nNote that in the lemmas that follow we will assume that M is a greedy matching of size k, initialized with the complete edge set.\nLemma F.2. Suppose that M is a greedy matching, and B \u2286 N(M) is some given set of size 2m. Then the following is a upper bound for the total distances of the edges inside B,\n\u2211\nx,y\u2208B\nw(x, y) \u2264 \u2211\nx,y\u2208N(M(m,B))\u2229B\nw(x, y) + 5r\n2 w(M(m,B)),\nwhere r = |B \\ N(M(m,B))|, i.e., r is the number of nodes of B that are not inside the set N(M(m,B)).\nProof. For convenience, let us use A to denote the set N(M(m,B)) \u2229B, i.e., the nodes of B that are contained in M(m,B). By definition of the top-intersecting matching, |A| \u2265 m, and therefore r \u2264 m, since |A|+ r = |B|.\nFirst, notice that the edges inside B can be divided into three parts as follows with A serving as the virtual partition.\n\u2211\nx,y\u2208A\nw(x, y) + \u2211\nx\u2208A y\u2208B\\A\nw(x, y) + \u2211\nx,y\u2208B\\A\nw(x, y).\nThe first term above is exactly the same as the first term in the RHS of the lemma statement. Therefore, it suffices if we show that the second term plus the third term above are at most 5r2 times the weight of the matching M(m,B). We first consider edges going from A to B \\A.\nFirst Part: Suppose that (x, y) is an edge where x \u2208 A and y \u2208 B \\A. Let (x, z) be the edge in M(m,B) that contains x. Since the edges in M were chosen in a greedy fashion and also, since w(x, z) is at least as large as the edge in M containing y (by definition of M(m,B)) , we infer that w(x, z) \u2265 w(x, y). In this fashion, we get that for a fixed x \u2208 A, \u2211y\u2208B\\Aw(x, y) \u2264 rw(x, z), where r is the number of nodes in B \\A.\nSumming up over all x \u2208 A and all y \u2208 B\\A, we have \u2211x\u2208A,y\u2208B\\Aw(x, y) \u2264 2r\u00d7w(M(m,B)). Notice that for a given edge (a, b) \u2208 M(m,B), (at most) 2r edges in A\u00d7B \\A can be charged to this edge, which happens when both a and b belong to A. In summary, we have\n\u2211\nx\u2208A y\u2208B\\A\nw(x, y) \u2264 2rw(M(m,B)).\nSecond Part: Next, consider B \\A: let M\u2217(B \\A) be the optimum matching using only the nodes in B \\A, and let M(B \\A) be the (smallest) set of edges of M containing the nodes in B \\A. Observe that the edges in M(B \\A) do not belong to M(m,B) by definition.\nSince we chose the edges in M in a greedy fashion, it is not hard to see that for every edge e = (x, y) of M\u2217(B \\ A), either the edge e is also in M(B \\ A), or one of the two edges of M(B \\A) containing x and y must have higher weight than e. If this were not true, then e would have been chosen for M , since it would be preferred by both x and y. In particular, this means that the edge emax which has the highest weight of all edges in M(B \\ A), must have weight at least that of any edge in M\u2217(B \\ A). Moreover, every edge of M(m,B) has higher weight than emax, since otherwise emax would have been chosen for M(m,B) instead of the edges in it. Since |M\u2217(B \\ A)| = r2 and |M(m,B)| = m \u2265 r, we know that M(m,B) has at least twice as many edges as M\u2217(B \\ A). In conclusion, we have that w(M\u2217(B \\ A)) \u2264 12w(M(m,B)). Finally, since the weight of a max-weight matching is at least that of a random matching, we know that \u2211\nx,y\u2208B\\Aw(x, y) \u2264 r \u00d7 w(M\u2217(B \\A)), so we get \u2211\nx,y\u2208B\\A\nw(x, y) \u2264 rw(M\u2217(B \\A)) \u2264 r 2 w(M(m,B)).\nAdding the upper bounds for the two parts, we get the lemma.\nLemma F.3. Suppose that M is a greedy matching, and suppose that B and C are two disjoint sets such that B \u2286 N(M), and C \u2229N(M) = \u2205. Then the following is an upper bound for the edges going from B to C\n\u2211\nx\u2208B,y\u2208C\nw(x, y) \u2264 2|C|w(M(m,B)),\nwhere |B| = 2m.\nProof. Suppose that M(B) is the (minimal) set of edges in M containing every node in B. Clearly, M(B) contains at most |B| = 2m edges and encompasses M(m,B).\nFix some x \u2208 B: for every y \u2208 C, we have that w(x, y) \u2264 w(x, z), where the latter is the edge in M containing x. This is because otherwise the edge (x, z) would not have been undominated, and thus would not have been added to M . In this manner, we can charge \u2211\nx\u2208B,y\u2208C w(x, y) to the matching M(B), using at most 2|C| slots of each edge, and a total of 2m|C| slots. This means that we can use a slot transfer argument and transfer all the slots to the edges in M(m,B), using at most 2|C| slots of each edge. Therefore, we conclude that\n\u2211\nx\u2208B,y\u2208C\nw(x, y) \u2264 2|C|w(M(m,B)).\nThis bounds edges going from the intersecting part to the disjoint part.\nLemma F.4. Suppose that M is a greedy matching, and suppose that B and C are two disjoint sets such that B \u2286 N(M), and C \u2229N(M) = \u2205. Then the following is an upper bound for the edges contained in C.\n\u2211\nx,y\u2208C\nw(x, y) \u2264 (|C| 2)\n|M | \u2212mw(M \\M(m,B)),\nwhere |B| = 2m.\nProof. Suppose that M\u2217(C) is the optimum matching containing only the edges in C. Since the weight of a max-weight matching is at least that of a random matching, we know that \u2211\nx,y\u2208C w(x, y) \u2264 |C|w(M\u2217(C)). Now, since the nodes in C are not present in M , we know that for any edge (x, y) in M and (a, b) \u2208 M\u2217(C), it must be that w(x, y) \u2265 w(x, a) and w(x, y) \u2265 w(x, b). Applying the triangle inequality, we get that w(a, b) \u2264 2w(x, y). Thus, for every edge in M , its weight is at least half the weight of any edge in M\u2217(C). Next, consider the set of |M | \u2212m edges in M \\M(m,B): by the above argument we get that w(M\u2217(C)) \u2264 |C||M|\u2212mw(M \\M(m,B)). Therefore, in conclusion, we get\n\u2211\nx,y\u2208C\nw(x, y) \u2264 |C|w(M\u2217(C)) \u2264 w(M \\M(m,B)) |C| 2\n|M | \u2212m.\nFinally, we establish a lower bound on the distances of the edges inside N(M) once again in terms of the top-intersecting matching.\nLemma F.5. Suppose that M is a greedy matching. Given any B \u2286 N(M) with B = 2m, let Top = N(M(m,B)) and A = B\u2229Top. Then the following is a piecewise lower bound for the edges inside N(M).\n1. \u2211 x,y\u2208Aw(x, y) \u2265 \u2211 x,y\u2208Aw(x, y) (Trivial Lower Bound)\n2. \u2211 x\u2208Top,y\u2208Top\\Aw(x, y) + \u2211 x\u2208Top,y\u2208N(M)\\Top w(x, y) \u2265 (2|M | \u2212 2m+ r2 )w(M(m,B))\n3. \u2211 x,y\u2208N(M)\\Topw(x, y) \u2265 (|M | \u2212m)w(M \\M(m,B)),\nwhere r = |B \\A|. Note that when we say x \u2208 Top and y \u2208 Top \\ A, we are counting each edge only once. This\nis true for the other summations as well.\nProof. Recall that Top refers to the set N(M(m,B)). Remember that A is a subset of Top, and B \\A has no node in common with Top.\nThe quantity \u2211 x,y\u2208N(M)w(x, y) includes (at least) \u2018edges going from (A to A), (Top to Top\\A), (Top to N(M) \\ Top), and (N(M) \\ Top to N(M) \\ Top). (Part 2.1):\n\u2211\nx\u2208Top y\u2208Top\\A\nw(x, y) \u2265 r 2 w(M(m,B)).\nNote that |Top| = 2m, and |A| = (2m\u2212 r). Now, pick any edge (x, y) \u2208 M(m,B), and the r nodes z \u2208 Top \\ A and apply the triangle inequality. We get, \u2211z\u2208Top\\Aw(x, z) + w(y, z) \u2265 rw(x, y). Summing this up over all (x, y) \u2208 M(m,B), and all z \u2208 Top \\ A, and dividing by two (since we count some edges twice), we get the first statement.\n(Part 2.2)\n\u2211\nx\u2208Top y\u2208N(M)\\Top\nw(x, y) \u2265 (2|M | \u2212 2m)w(M(m,B)).\nThis follows almost directly from taking each edge in M(m,B), and every node in N(M) \\B, and applying the triangle equality.\nSumming up Parts 2.1 and 2.2, gives us the second statement of our lemma. (Part 3)\n\u2211\nx,y\u2208N(M)\\B\nw(x, y) \u2265 (|M | \u2212m)w(M \\M(m,B)).\nThis comes from applying Lemma B.5 to the set N(M) \\B."}, {"heading": "Applying the Generic Claims", "text": "Now that we have some general properties of the top-intersecting matching, we shed light on how to apply the above framework towards our main theorem. Recall that our algorithm involves choosing a greedy matching M of size \u03b2k2 , and using its endpoints to form the set S of size \u03b2k. Let S\u2217 be the optimum solution to the Densest Subgraph problem for input parameter k. Define B to be the set of nodes that are common to both S and S\u2217, and let |B| = 2m.\nNow as per our definitions M(m,B) is the set of m highest weight edges in M all of which contain at least one node from B, i.e., nodes from S\u2217. Given this framework, we can express the weight of our optimum solution w(S\u2217) as \u2211\nx,y\u2208Aw(x, y)+ \u2211 x\u2208B,y\u2208B\\Aw(x, y)+ \u2211\nx\u2208B,y\u2208S\u2217\\B w(x, y)+ \u2211\nx,y\u2208S\u2217\\B w(x, y). We take A to be the subset of B contained in M(m,B), and r = |B \\A|. Now, we are in a position to bound w(S\u2217) in terms of w(M(m,B)), and w(M \\ M(m,B)).\nSumming up Lemmas F.2, F.3, F.4 (take C = S\u2217 \\B) , we get,\nw(S\u2217) \u2264 \u2211\nx,y\u2208A\nw(x, y) +[ 5r\n2 + 2(k \u2212 2m)]w(M(m,B))\n+ 2(k \u2212 2m)2 \u03b2k \u2212 2m w(M \\M(m,B)).\nNext, applying Lemma F.5, we transform the above upper bound to a bound on w(S). Once again, in order to avoid lengthy notation, we use Top to refer to N(M(m,B)).\nw(S\u2217) \u2264 \u2211\nx,y\u2208A\nw(x, y)\n+ 2(k \u2212 2m) + 5r2 \u03b2k \u2212 2m+ r2 [ \u2211\nx\u2208Top y\u2208Top\\A\nw(x, y) + \u2211\nx\u2208Top y\u2208N(M)\\Top\nw(x, y)]\n+ 4(k \u2212 2m)2 (\u03b2k \u2212 2m)2\n\u2211\nx,y\u2208N(M)\\Top\nw(x, y)\n\u2264 max ( 1, 2(k \u2212 2m) + 5r2 \u03b2k \u2212 2m+ r2 , 4(k \u2212 2m)2 (\u03b2k \u2212 2m)2 ) w(S)\nThe rest of the proof is somewhat algebraic, so we only sketch the details here. Look at the second term inside the maximization function above. For a fixed \u03b2, and a fixed value of m, we can show using some simple calculus that the quantity 2(k\u22122m)+ 5r 2\n\u03b2k\u22122m+ r 2\nmonotonically increases with\nr, and therefore is maximized when r attains its maximum value. Moreover, we know that r \u2264 m,\nand r \u2264 \u03b2k \u2212 2m, since S only contains \u03b2k nodes and 2m of them are inside N(M(m,B)). Now, consider 1 \u2264 \u03b2 \u2264 65 , and take r = \u03b2k \u2212 2m, we get\nw(S\u2217) \u2264 max ( 4k + 5\u03b2k \u2212 18m 3\u03b2k \u2212 6m , 4(k \u2212 2m)2 (\u03b2k \u2212 2m)2 ) w(S)\nMoreover, for a fixed \u03b2, both of the terms above reach their maximum value when m is at its\nsmallest (m = 0) giving us w(S\u2217) \u2264 max (\n4+5\u03b2 3\u03b2 , 4 \u03b22\n)\nw(S) and so when \u03b2 \u2264 65 , we get w(S\u2217) \u2264 4 \u03b22w(S).\nNext, take \u03b2 \u2265 65 , and r = m in our general bound for w(S\u2217). We get,\nw(S\u2217) \u2264 max ( 2k \u2212 3m2 \u03b2k \u2212 3m2 , 4(k \u2212 2m)2 (\u03b2k \u2212 2m)2 ) w(S)\nThis time, for a fixed 65 \u2264 \u03b2 \u2264 2, the first term reaches its maximum value when m is largest (B = S\u2217 or 2m = k), and the second when m is at its smallest (m = 0), giving us w(S\u2217) \u2264 max ( 2\u2212 3 4\n\u03b2\u2212 3 4\n, 4\u03b22\n)\nw(S) and so when \u03b2 \u2265 65 , we get w(S\u2217) \u2264 4\u03b22w(S). So in both cases, we get our bound of w(S\u2217) \u2264 4\u03b22w(S), which completes the proof of the\ntheorem."}, {"heading": "G Appendix: Proofs from Section 5: 1.88 Approximation", "text": "for Max TSP\nBefore defining our randomized algorithm, we first present the following lemma, which gives a relationship between matching and Hamiltonian paths.\nLemma G.1. Given any matching M with k edges, there exists an efficient ordinal algorithm that computes a Hamiltonian path Q containing M such that the weight of the Hamiltonian path in expectation is at least\n[ 3 2 \u2212 1 k ]w(M).\nProof. We first provide the algorithm, followed by its analysis. Suppose that K is the set of nodes contained in M .\n1. Select a node i \u2208 K uniformly at random. Suppose that e(i) is the edge in M containing i.\n2. Initialize Q = M .\n3. Order the edges in M arbitrarily into (e1, e2, . . . , ek) with the constraint that e1 = e(i).\n4. For j = 2 to k,\n5. Let x be a node in ej\u22121 having degree one in Q (if j = 2 choose x 6= i) and ej = (y, z).\n6. If y >x z, add (x, y) to Q, else add (x, z) to Q.\nSuppose that Q(j) consists of the set of nodes in Q for a given value of j (at the end of that iteration of the algorithm). We claim that w(Q) \u2265 32w(M) \u2212 w(e1), which we prove using the following inductive hypothesis,\nw(Q(j)) \u2265 w(M) + j \u2211\nr=2\n1 2 w(er)\nConsider the base case when j = 2. Suppose that e1 = (i, a), and e2 = (x, y). Without loss of generality, suppose that a prefers x to y, then in that iteration, we add (a, x) to Q. By the triangle inequality, we also know that w(x, y) \u2264 w(x, a)+w(y, a) \u2264 2w(x, a). Therefore, at the end of that iteration, we have\nw(Q) = w(M) + w(x, a) \u2265 w(M) + 1 2 w(e2). (11)\nThe inductive step follows similarly. For some value of j, let x be the degree one node in ej\u22121, and ej = (y, z). Suppose that x prefers y to z, then using the same argument as above, we know that (x, y) is added to our desired set and that w(x, y) \u2265 12w(y, z). The claim follows in an almost similar fashion to Equation 11 and the inductive hypothesis.\nIn conclusion, the total weight of the path is 32w(M) \u2212 w(e1). Since the first node i is chosen uniformly at random, every edge in M has an equal probability (p = 1k ) of being e1. So, in expectation, the weight of the tour is 32w(M) \u2212 1kw(M), which completes the lemma.\nOur randomized algorithm involves returning two tours computed by two different sub-routines with equal probability. We define some pertinent notation before describing the two sub-routines. Suppose that M is the solution returned by the Greedy Matching Procedure for k = 13N , i.e., M contains 23 times the number of edges in any perfect matching. We know from Lemma 2.2 in [5] that w(M) \u2265 w(M \u2217)\n2 , where M \u2217 is the optimum perfect matching in N ; in fact this is not difficult\nto see directly by classic charging arguments comparing greedy matchings to maximum-weight matchings.\nLet Top be the set of nodes whose edges form M , and let B = N \\ Top. Finally, given any Hamiltonian path H , we use H(f) and H(l) to denote the dangling nodes of H , i.e., the two endpoints of H whose degrees are one. Before showing the algorithm, we give a simple lemma that provides a \u2018nice way\u2019 to form a tour using two Hamiltonian Paths.\nLemma G.2. Let H1 and H2 be two Hamiltonian paths on two different sets of nodes. Then, we can form a tour T by connecting the two paths such that w(T ) \u2265 w(H1)+w(H2)+w(H1(f), H1(l)) without knowing the edge weights.\nProof. For ease of notation, we refer to the 4 endpoints of the two paths in the following manner, H1(f) = a,H1(l) = b,H2(f) = x,H2(l) = y. Without loss of generality, suppose that a prefers x to y. Then, let T = H1 \u222aH2 \u222a {(a, x), (b, y)}. In this case, we have that w(a, b) \u2264 w(a, y) +w(b, y) \u2264 w(a, x)+w(b, y). Therefore, we get w(T ) = w(H1)+w(H2)+w(a, x)+w(b, y) \u2265 w(H1)+w(H2)+ w(a, b).\noutput: Tour T1 Let M be a greedy matching of size k = N3 , and B be the nodes not in M ; Complete M using Lemma G.1 to form a Hamiltonian path HT on Top; Form a Hamiltonian path HB on B using the following randomized algorithm.; Randomized Path Algorithm ; Form a random permutation on the nodes in B; Join the nodes in the same order to form the path; (i.e., join the first and second nodes, second and third, and so on.); Final Output T1 is the output formed by using Lemma G.2 for H1 = HB and H2 = HT .\nAlgorithm 9: First Subroutine of the randomized algorithm for Max TSP\nWe now show lower bounds on the weight of T1. Below T \u2217 denotes the optimum-weight tour.\nLemma G.3. The following is a lower bound on the weight of the tour returned by Algorithm 9\nE[w(T1)] \u2265 [ 3 8 \u2212 3 4N ]w(T \u2217) + 6 N \u2211\nx,y\u2208B\nw(x, y).\nProof. Using linearity of expectations, we get that,\nE[w(T1)] = E[w(HT )] + E[w(HB) + w(T1 \\ (HT \u222aHB))].\nFor any given Hamiltonian path (using only the nodes in B) HB, suppose that TB denotes the tour obtained by completing the dangling nodes to form a cycle. From Lemma G.2, we get that w(T1 \\ (HT \u222aHB))] \u2265 w(HB(f), w(HB(l)). In other words, even though the sub-routine outputs a random tour, the weight of the edges in T but not HT and HB is at least the weight of the edge between the two dangling nodes of HB . This is true because we completed the tour in a very particular fashion using Lemma G.2. Abusing notation, we get\nE[w(T1)] = E[w(HT )] + E[w(TB)],\nwhere E[w(TB)] is the expected weight of any tour formed using the nodes only in B. However, using symmetry arguments, we can conclude that E[w(TB)] \u2265 |TB ||B|(|B|\u22121)/2 \u2211\nx,y\u2208B w(x, y) \u2265 6 N \u2211 x,y\u2208B w(x, y).\nNext, applying Lemma G.1, we get that E[w(HT )] = [ 3 2 \u2212 3N ]w(M) \u2265 [ 34 \u2212 32N ]w(M\u2217) \u2265\n[ 38 \u2212 34N ]w(T \u2217). We used the facts that w(M) \u2265 w(M\u2217) 2 , and w(M \u2217) \u2265 w(T \u2217) 2 .\nSumming up the two parts gives the desired result.\nBefore describing the second sub-routine, we introduce the notion of an alternating-tour. Given two equal-sized disjoint sets A,B, we say that TAB is a alternating tour if it is a tour, and it alternates between nodes in A and B. We can similarly define the notion of an alternating Hamiltonian path or just alternating path. Notice that an alternating path (or even a tour) can be represented as a sequence of alternating nodes from A and B respectively. In the following algorithm, we form an alternating path by adding nodes sequentially to HAB. Finally, recall that M is a greedy matching of size k = N3 , and B is set of N 3 nodes not in M .\noutput: Tour T2 Let M be a greedy matching of size k = N3 , and B be the nodes not in M ; Select N6 edges uniformly at random from M ; Complete these edges using Lemma G.1 to form a Hamiltonian path HT with N 3 nodes; Let A be the set of nodes in Top but not in HT ; Randomized Alternating Path Algorithm; Initialize HAB = \u2205; Select one node uniformly at random from A; Select one node uniformly at random from B; Add both the nodes to HAB in the same order; Remove them from A and B respectively ; Repeat the above process until A = B = \u2205; Final Output; T2 is the output formed by using Lemma G.2 for H1 = HAB and H2 = HT .\nAlgorithm 10: Second Subroutine of the randomized algorithm for Max TSP"}, {"heading": "Analysis of Sub-routine 2", "text": "We begin by defining some additional notation required for the analysis of Algorithm 10. Suppose\nthat T2 represents the set of all tours that are output by the algorithm with non-zero probability (support of T2). Notice that for any T\n\u2032 \u2208 T2, we can uniquely divide T \u2032 into H \u2032T , H \u2032AB, and the connecting edges, where H \u2032T is the path containing only the edges inside Top \\A, and H \u2032AB is the alternating path between A and B. Therefore, we have\nE[w(T2)] = E[w(HT )] + E[w(HAB) + w(T2 \\ (HT \u222aHAB))]. Next, consider the two edges connecting HT and HAB, i.e., T2 \\ (HT \u222a HAB). Since these edges were chosen using the mechanism of Lemma G.2, this means that w(T2 \\ (HT \u222a HAB)) \u2265 w(HAB(f), HAB(l)), for a given alternating path HAB. And so, abusing notation, we get\nE[w(T2)] \u2265 E[w(HT )] + E[w(TAB)], where E[w(TAB)] is the expected weight of any alternating tour formed using the nodes in A,B. Moreover, this tour is specifically formed by the sequential random mechanism described in Algorithm 10 to choose a Hamiltonian path, followed by a deterministic step where the path is completed to form a cycle. In what follows, we establish a lower bound on this quantity.\nLemma G.4. The expected weight of the random alternating tour E[w(TAB)] formed using the nodes in A,B, is at least 3N \u2211 x\u2208Top,y\u2208B w(x, y).\nProof. First, notice that\nE[w(TAB)] = \u2211\nS\u2282Top |S|=N\n3\nE[w(TAB)|A = S]Pr(A = S). (12)\nThe above equation follows from the fact that for every possible tour returned by the random algorithm (say TAB = T \u2032 AB, and A = S), we have that Pr(TAB = T \u2032 AB) = Pr(TAB = T \u2032 AB|A = S)Pr(A = S). Now, fix A = S: we have that\nE[w(TAB)|A = S] = \u2211\nx\u2208S,y\u2208B\nw(x, y)Pr((x, y) \u2208 TAB).\nBy symmetry of our random process, we know that every edge (x, y) between S and B has the same probability of being included into the tour. Consider the sum of such probabilities:\n\u2211\nx\u2208S,y\u2208B Pr((x, y) \u2208 TAB) = \u2211\nx\u2208S,y\u2208B\n\u2211\ntour T\u220b(x,y)\nPr(TAB = T ) =\n\u2211\ntour T\n2N\n3 Pr(TAB = T ) =\n2N\n3\nThe sum above is over all tours T of nodes S \u222aB. Since there are N3 nodes in both S and B, then there are N 2\n9 edges between them, and thus the probability of any edge (x, y) from S to B being in the tour TAB is exactly 6 N . Thus, we have that:\nE[w(TAB)|A = S] = 6\nN\n\u2211\nx\u2208S y\u2208B\nw(x, y).\nThe rest of the proof follows from rearranging Equation 12 as a summation across the nodes in Top, and observing that every node from Top is chosen into A with probability one-half.\nOur next lemma completes the lower bound for the second sub-routine.\nLemma G.5. The expected weight of the tour returned by Algorithm 10 is at least [ 1116\u2212 34N ]w(T \u2217)\u2212 6 N \u2211 x,y\u2208B w(x, y).\nProof. Substituting the bound obtained in Lemma G.4 into the Equation for E[w(T2)], we get,\nE[w(T2)] = E(w(HT )] + 3\nN\n\u2211\nx\u2208Top,y\u2208B\nw(x, y).\nThe rest of the proof for obtaining a lower bound on E[w(T2)] is as follows. Suppose that M\u2217 is the maximum weight matching on N . By applying Lemma B.5 to the set T = B (so n = N/3), we know that w(M\u2217) \u2264 6N \u2211 x\u2208B,y\u2208B w(x, y) + 3 N \u2211 x\u2208Top,y\u2208B w(x, y). Using the fact that w(T \u2217) \u2264 2w(M\u2217), we get 3N \u2211 x\u2208Top,y\u2208B w(x, y) \u2265 w(T\u2217) 2 \u2212 6N \u2211\nx\u2208B,y\u2208B w(x, y). Note that HT is obtained by randomly taking half of the matching M and then completing it. From Lemma G.1, we thus have that E[w(HT )] \u2265 [ 34 \u2212 3N ]w(M). Since by our construction, w(M) is at least half of w(M\u2217), which is at least half of w(T \u2217), we thus have that E[w(HT )] \u2265 [ 316 \u2212 34N ]w(T \u2217).\nIn summary, we have\nE[w(T2)] = ( 3\n16 +\n1 2 \u2212 3 4N )w(T \u2217)\u2212 6 N \u2211\nx,y\u2208B\nw(x, y).\nWe also know from Lemma G.3 that\nE[w(T1)] = ( 3 8 \u2212 3 4N )w(T \u2217) + 6 N \u2211\nx,y\u2208B\nw(x, y).\nThe final bound is obtained by using E[w(T )] = 12 (E(w(T1)] + E[w(T2)])."}, {"heading": "H Lower Bounds", "text": "Densest k-Subgraph\nClaim H.1. No ordinal approximation algorithm, deterministic or randomized, can provide an approximation factor better than 2 for Densest k-subgraph.\nProof. Since randomized algorithms are more general than deterministic algorithms, it suffices to show the claim just for Randomized Algorithms.\nGiven a parameter k, consider an instance of the Densest k-Subgraph with Mk nodes for a large enough value of M (say M is much larger than k). The set of nodes in the graph can be divided into M clusters N1, N2, . . . , NM , each containing k nodes. The preference ordering is given as follows: for a given i, every node in Ni prefers all the nodes in Ni over every node outside of Ni. The exact preference ordering within Ni and outside of Ni can be arbitrary.\nNow, randomly choose one of M clusters and assign a weight of 2 to all the edges strictly inside that cluster. Assign a weight of 1 to every other edge in the graph. It is easy to see that these weights induce the given preference orderings. Now, without loss of generality, it suffices to consider only algorithms that choose k nodes within a fixed cluster. Moreover, since the clusters\nare identical from the ordinal point of view, the optimum algorithm for this instance just picks one of the M clusters uniformly at random, and therefore, its approximation ratio is 2\n1+ 1 M\nwhich\napproaches 2 as M \u2192 \u221e"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We study truthful mechanisms for matching and related problems in a partial information<lb>setting, where the agents\u2019 true utilities are hidden, and the algorithm only has access to ordi-<lb>nal preference information. Our model is motivated by the fact that in many settings, agents<lb>cannot express the numerical values of their utility for different outcomes, but are still able<lb>to rank the outcomes in their order of preference. Specifically, we study problems where the<lb>ground truth exists in the form of a weighted graph of agent utilities, but the algorithm can<lb>only elicit the agents\u2019 private information in the form of a preference ordering for each agent<lb>induced by the underlying weights. Against this backdrop, we design truthful algorithms to<lb>approximate the true optimum solution with respect to the hidden weights. Our techniques<lb>yield universally truthful algorithms for a number of graph problems: a 1.76-approximation<lb>algorithm for Max-Weight Matching, 2-approximation algorithm for Max k-matching, a 6-<lb>approximation algorithm for Densest k-subgraph, and a 2-approximation algorithm for Max<lb>Traveling Salesman as long as the hidden weights constitute a metric. We also provide im-<lb>proved approximation algorithms for such problems when the agents are not able to lie about<lb>their preferences. Our results are the first non-trivial truthful approximation algorithms for<lb>these problems, and indicate that in many situations, we can design robust algorithms even<lb>when the agents may lie and only provide ordinal information instead of precise utilities.", "creator": "LaTeX with hyperref package"}}}