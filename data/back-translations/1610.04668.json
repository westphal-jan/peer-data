{"id": "1610.04668", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "A Closed Form Solution to Multi-View Low-Rank Regression", "abstract": "Real-life data often contain information from different channels. For example, in computer vision, we can describe an image using different image characteristics such as pixel intensity, color, HOG, GIST function, SIFT characteristics, etc. These different aspects of the same objects are often referred to as multi-view data (or multi-modal data).The low-rank regression model has proven to be an effective learning mechanism by examining the low-rank structure of real data. However, the previous low-rank regression model only works on the basis of data from a single view. In this paper, we propose a multi-rank regression model by imposing low-level constraints on the multi-rank regression model. Most importantly, we provide a coherent solution for the multi-view low-rank regression model. Extensive experiments with 4 multi-rank regression sets show that the multi-rank model meets the low-rank model of a single-view regression model is very helpful.", "histories": [["v1", "Fri, 14 Oct 2016 23:43:47 GMT  (808kb,D)", "http://arxiv.org/abs/1610.04668v1", "Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI, 2015"]], "COMMENTS": "Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuai zheng 0002", "xiao cai", "chris h q ding", "feiping nie", "heng huang"], "accepted": true, "id": "1610.04668"}, "pdf": {"name": "1610.04668.pdf", "metadata": {"source": "META", "title": "A Closed Form Solution to Multi-View Low-Rank Regression", "authors": ["Shuai Zheng", "Xiao Cai", "Chris Ding", "Feiping Nie", "Heng Huang"], "emails": ["zhengs123@gmail.com,", "xiao.cai@mavs.uta.edu,", "chqding@uta.edu,", "feipingnie@gmail.com,", "heng@uta.edu"], "sections": [{"heading": "Introduction", "text": "In many tasks, a single object can be described using information from different channels (or views). For example, a 3-D object can be described using pictures from different angles; a website can be described using the words it contains, and the hyperlinks it contains; an image can be described using different features, such as SIFT feature, and HOG feature; in daily life, a person can be characterized using age, height, weight and so on. These data all comes from different aspects and channels. Multi-view problems aim to improve existing single view model by learning a model utilizing data collected from multiple channels (Ru\u0308ping and Scheffer 2005) (de Sa 2005) (Zhou and Burges 2007).\nLow-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data (Xiang et al. 2012) (Evgeniou and Pontil 2007) (Cai et al. 2013). Existing regression models only work on single view data. To be specific, linear regression finds a linear model with respect to the single view feature data to fit target class data (Seber and Lee 2012). Let matrix B \u2208 <p\u00d7c be the parameter of the linear model. Linear regression solves a problem of minB ||Y \u2212 XTB||2F , where Copyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nX = [x1,x2, ...,xn] \u2208 <p\u00d7n is the single view feature data matrix and Y \u2208 <n\u00d7c is the target class indicator matrix. Ridge regression can achieve better results by adding a Frobenius norm based regularization on linear regression loss objective (Hoerl and Kennard 1970) (Marquaridt 1970). Ridge regression solves the problem minB ||Y \u2212XTB||2F + \u03bb||B||2F , where \u03bb is the regularization weight parameter. Cai (Cai et al. 2013) showed that whenB is low-rank, regression is equivalent to linear discriminant analysis based regressions. However, all these work only works for single-view problems.\nIn this paper, we propose a multi-view low-rank regression model by imposing low-rank constraints on regression model. This model can be solved using closed-form solution directly. In linear regression, low rank parameter matrix B\u03bd is dependent on view \u03bd. Through theoretical analysis, we show that multi-view low-rank regression model is equivalent to do regression in the subspace of each view. In other words, let B\u03bd = A\u03bdB, and it is equivalent to find the shared regression parameter matrix B under the subspace transformation A\u03bd with respect to view \u03bd. Extensive experiments performed on 4 multi-view datasets show that the proposed model outperforms single-view regression model and reveals that low-rank structure can improve the classification result of a full-rank model.\nNotations. In this paper, matrices are written in uppercase letters, such as X, Y . Vectors are written in bold lower case letters, such as x, y. Tr(X) means the trace operation for matrix X ."}, {"heading": "Multi-view Low Rank Regression", "text": "Assume that there are v views and c classes, p\u03bd is the dimension of view \u03bd, nj is the sample size of the j-th class, and n is the total sample size. Let X\u03bd = [x\u03bd1 , ...,x \u03bd n] \u2208 <p\u03bd\u00d7n be the data matrix of view \u03bd, \u03bd = 1, 2, ..., v, and Y = [y1, ...,yc] \u2208 <n\u00d7c is the normalized class indicator matrix, i.e. Yij = 1/ \u221a nj if the i-th data point belongs to the j-th class and Yij = 0 otherwise.\nWe try to minimize the residual of low rank regression model in each class and in each view. Loss function of multi-\nar X\niv :1\n61 0.\n04 66\n8v 1\n[ cs\n.L G\n] 1\n4 O\nct 2\n01 6\nview low rank ridge regression can be proposed as in Eq.(1):\nJ0 = v\u2211\n\u03bd=1\nc\u2211\nk=1\n{\u2016yk \u2212 (XT\u03bd \u03b2\u03bdk + f\u03bdk e)\u201622 + \u03bb\u03bd\u2016\u03b2\u03bdk\u201622}\n= v\u2211\n\u03bd=1\n{\u2016Y \u2212 (XT\u03bd B\u03bd + EF \u03bd)\u20162F + \u03bb\u03bd\u2016B\u03bd\u20162F } (1)\nwhere projection matrix B\u03bd = [\u03b2\u03bd1 , ..., \u03b2 \u03bd c ] \u2208 <p\u03bd\u00d7c, bias F \u03bd = diag(f\u03bd1 , ..., f \u03bd c ), E = [e, ..., e] \u2208 <n\u00d7c. e is a ndimensional column vector with all elements equal to 1. \u03bb\u03bd is the regularization parameter of view \u03bd. Let\u2019s introduce low rank projection B\u03bd with rank s, s < min(p\u03bd , c),\n\u03b2\u03bdk = A\u03bdbk, or B \u03bd = A\u03bdB, (2)\nwhere A\u03bd \u2208 <p\u03bd\u00d7s, and B = (b1, ...,bc) \u2208 <s\u00d7c. Therefore, the objective function Eq.(1) can be written as:\nJ1 =\nv\u2211\n\u03bd=1\n{\u2016Y \u2212 (XT\u03bd A\u03bdB + EF \u03bd)\u20162F + \u03bb\u03bd\u2016A\u03bdB\u20162F } (3)\nIt is noteworthy that from Eq.(3), we can see that multi-view low-rank regression model is equivalent to do regression in the subspace of each view. Matrix A\u03bd is the subspace matrix of view \u03bd. Matrix B is the shared regression parameter matrix of all views."}, {"heading": "Closed form solution", "text": "We now present the closed form solution of the Multi-view Low Rank Regression. Before we talk about the closed form solution, we present Lemma 1 to simplify Eq.(3). Lemma 1. The bias f\u03bdk can be solved and eliminated from J1, which is thus simplified into\nJ1 = v\u2211\n\u03bd=1\n{\u2016Y c \u2212XcT\u03bd A\u03bdB\u20162F + \u03bb\u03bd\u2016A\u03bdB\u20162F } (4)\nwhere bias f\u03bdk relates to B as f\u03bd\u2217k = y\u0304k \u2212 x\u0304T\u03bd A\u03bdb\u03bdk (5) and Xc\u03bd = X\u03bd \u2212 x\u0304eT is centered data matrix of view \u03bd and Y c = Y \u2212 (y\u03041, ..., y\u0304c)e is centered class indicator matrix. Proof. Taking derivative of Eq.(3) w.r.t. f\u03bdk and setting it to zero, the optimal solution of f\u03bdk is given as in Eq.(5), where y\u0304k is a real number, y\u0304k = \u2211n i=1 yki/n, x\u0304\u03bd = \u2211n i=1 x \u03bd i /n \u2208 <p\u03bd\u00d71. Substituting Eq.(5) into Eq.(3), we have Eq.(4). In the rest of this paper, we focus on solving Eq.(4). For simplicity of notations, we drop c in Xc\u03bd and use X\u03bd to denote the centered X\u03bd . Similarly, we drop c in Y c and use Y to denote the centered Y .\nNow we present Theorem 1 to give the closed form solution of multi-view low-rank regression model. Theorem 1. The optimal solution of J1({A\u03bd}, B) is the following:\n1. {A\u03bd} is given by the optimal solution of the following problem:\nmax {A\u03bd}\nTr(G\u22121HY Y THT ) (6)\nwhere\nG = G({A\u03bd}) , \u2211\n\u03bd\nAT\u03bd (X\u03bdX T \u03bd + \u03bb\u03bdI)A\u03bd , (7)\nH = H({A\u03bd}) , \u2211\n\u03bd\nAT\u03bdX\u03bd (8)"}, {"heading": "2. B is given by", "text": "B\u2217 = G\u22121H. (9)\nProof. Taking derivative of Eq.(4) w.r.t. B, we have\n\u2202J \u2202B = \u22122\n\u2211\n\u03bd\nAT\u03bdX\u03bdY + 2 \u2211\n\u03bd\nAT\u03bdX\u03bdX T \u03bd A\u03bdB\n+2\u03bb\u03bd \u2211\n\u03bd\nAT\u03bd A\u03bdB. (10)\nSetting Eq.(10) to zero, we have Eq.(9). Substituting Eq.(9) in Eq.(4), we have\nJ = \u2212 min {A\u03bd} Tr(G\u22121HY Y THT ) (11)\nwhere G = G({A\u03bd}) , \u2211 \u03bd A T \u03bd (X\u03bdX T \u03bd + \u03bb\u03bdI)A\u03bd ,\nH = H({A\u03bd}) , \u2211 \u03bd A T \u03bdX\u03bd . Eq.(11) is equivalent to Eq.(6).\nFurthermore, we present Theorem 2 to give the closed form solution for Eq.(6). Let\nA =   A1 A2 ... Av   , X =   X1 X2 ... Xv   , (12)\nSb =XY Y TXT , (13)\nSt =diag(X1X T 1 + \u03bb1I, ..., XvX T v + \u03bbvI), (14)\nTheorem 2. Eq.(6) is equivalent to\nmax A\nTr[(ATStA)\u22121ATSbA], (15)\nwhere the optimal solution A\u2217 is given by eigenvectors of S\u22121t Sb that correspond to the s largest eigenvalues."}, {"heading": "Algorithm", "text": "We present Algorithm 1 to summarize the steps of multiview low-rank regression model. One of the advantages of our model is that it can be solved using closed-form solution directly. The input of this algorithm is (1) centered and normalized data matrix X\u03bd \u2208 <p\u03bd\u00d7n from view \u03bd, where \u03bd = 1, 2, ..., v , v is view number, p\u03bd is the dimension of view \u03bd and n is sample number, (2) class indicator matrix Y \u2208 <n\u00d7c, (3) regularization weight parameter \u03bb\u03bd , (4) rank s, which is less than the class number c. The output of this algorithm is matrix A\u03bd \u2208 <p\u03bd\u00d7s and B \u2208 <s\u00d7c. We can compute Sb and St using Eq.(13) and Eq.(14). In step 2, we compute A, which is those eigenvectors of S\u22121t Sb that correspond to the s largest eigenvalues. We should use Eq.(12) to restore A\u03bd from A. Finally, we compute B using Eq.(9).\nAlgorithm 1 Multi-view low-rank regression Input: Data matrix X\u03bd \u2208 <p\u03bd\u00d7n, class indicator matrix Y \u2208 <n\u00d7c, regularization weight parameter \u03bb\u03bd , rank s < c, \u03bd = 1, 2, ..., v Output: Matrix A\u03bd \u2208 <p\u03bd\u00d7s and B \u2208 <s\u00d7c, \u03bd = 1, 2, ..., v 1: Compute Sb and St using Eq.(13) and Eq.(14) 2: Compute A\u03bd using the optimal solution of Eq.(15) 3: Compute B using Eq.(9)"}, {"heading": "Multi-view Full Rank Regression", "text": "Low-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data. Will the multi-view low-rank regression model be able to capture the low-rank structure and improve the performance of a full-rank model? We will compare the performance of multi-view low-rank regression model with a full-rank model in experiment section.\nIn the case of multi-view full-rank regression, rank s = c, there is no constraint on B\u03bd in Eq.(1) and we will not use Eq.(2). To be specific, we will minimize the objective Eq.(4):\nJ1 = v\u2211\n\u03bd=1\n{\u2016Y \u2212XT\u03bd B\u03bd\u20162F + \u03bb\u03bd\u2016B\u03bd\u20162F } (16)\nEq.(16) can be solved using close form solution. Taking derivative of Eq.(16) w.r.t. B\u03bd and setting it to zero, the optimal solution of B\u03bd is given as\nB\u03bd = (X\u03bdX T \u03bd + \u03bb\u03bdI) \u22121X\u03bdY, (17)\nwhere I \u2208 <p\u03bd\u00d7p\u03bd is an identity matrix.\nConnections to other Multi-view work Various multi-view learning models have been studied and all multi-view models are expected to have better performance than single view models. Existing multi-view approaches mainly are inspired from spectral clustering and subspace learning. de Sa (de Sa 2005) developed a spectral clustering algorithm for only two views by creating a bipartite graph based on the \u201cminimizing-disagreement\u201d idea. Zhou (Zhou and Burges 2007) developed a multi-view spectral clustering model via generalizing the single view normalized cut to the multi-view case. They try to find a cut which is close to be optimal on each single-view graph by exploiting a mixture of Markov chains associated with graphs of different views. Kumar (Kumar and Daume\u0301 2011) proposed a co-training flavour spectral clustering algorithm and use spectral embedding from one view to constrain the similarity graph used for the other view. Kumar (Kumar, Rai,\nand Daume 2011) used the philosophy of co-regularization, which has been used in the past for semi-supervised learning problems, to make the clusterings in different views agree with each other.\nMulti-view learning models from the point of view of subspace learning mainly try to find a subspace for each view and then develop a learning model across views in their subspaces. Canonical-Correlation Analysis (CCA) (Hotelling 1936) was first used to study the correlation of two views in their respective subspaces. Hardoon (Hardoon, Szedmak, and Shawe-Taylor 2004) (Hardoon and Shawe-Taylor 2009) designed an Kernel Canonical-Correlation Analysis to extract patterns from two views. Chaudhuri (Chaudhuri et al. 2009) proposed a CCA-based subspace multi-view learning approach to find a subspace such that the objects of different classes are well-separated and within-class distance is minimized. Greene (Greene and Cunningham 2009) developed a Non-negative Matrix Factorization (NMF) (Lee and Seung 1999) approach to effectively identify common patterns and reconcile between-view disagreements by combining data from multiple views.\nThe proposed multi-view low-rank regression model should be categorized into the class of subspace learning multi-view. The important contribution of this paper is that we developed low-rank regression model to study multiview problems. Surprisingly, there exists closed form solution to multi-view low-rank regression model."}, {"heading": "Experiments", "text": "In this section, we perform extensive experiments on 4 multiple-view datasets. Through model learning, we systematically explore the best settings of regression bias, regularization weight parameter \u03bb\u03bd and how to do classification using multi-view regression. We compare the classification accuracy of multi-view low-rank ridge regression with singleview regression, linear regression and full rank ridge regression."}, {"heading": "Datasets", "text": "Various multi-view datasets are used. These datasets include image datasets MSRC (Lee and Grauman 2009) and Caltech (Fei-Fei, Fergus, and Perona 2007), website dataset Cornell (Craven et al. 2000) and scientific publication dataset Cora (McCallum et al. 1999). Cornell and Cora are downloaded from (Grimal 2014). Summary of the datasets attributes are presented in Table 1, where n is sample number, c is class number, v is view number and p\u03bd lists the dimensions of different views.\nMSRC is an image scene data, including trees, buildings, planes, cows, faces, cars and so on. It has 210 images from 7 classes. We extract different features from this data. The 4 views we used in this paper are CENTRIST(1302 dimensions), GIST (512 dimensions), HOG (100 dimensions) and LBP (256 dimensions).\nCaltech is a subset of Caltech 101 image data. It has images from 20 classes, including Faces, Leopards, Motorbikes, binocular, Brain, Camera, etc.. This data has 1230 images and 4 features are extracted from this data, including CENTRIST(1302 dimensions), GIST (512 dimensions), HOG (100 dimensions) and LBP (256 dimensions).\nCornell contains 195 documents over the 5 types (student, project, course, staff, faculty). There exists referral links among these documents. We use 3 views to describe the same document, including content view (107 dimensions), inbound-link view (20 dimensions) and outboundlink view (15 dimensions).\nCora consists of 2708 scientific publications classified into one of seven classes (Neural Networks, Rule Learning, Reinforcement Learning, Probabilistic Methods, Theory, Genetic Algorithms, Case Based). The citation network consists of links among those publications. The 3 views used in our experiments include content view (101 dimensions), inbound-link view (180 dimensions) and outboundlink view (75 dimensions)."}, {"heading": "Model learning", "text": "Through model learning, we systematically explore the best settings of regression bias, regularization weight parameter \u03bb\u03bd and how to do classification using multi-view regression.\nEffect of regression bias To validate that adding bias to regression will reduce fitting residual, Figure 1 compares the residual of class indicator matrix Y using two f\u03bdk values: (1). using Eq.(5), denoted by \u201cWith bias\u201d line (red circle line), (2)f\u03bdk = 0, denoted by \u201cNo bias\u201d line (blue dot line). Residual r is defined as\nr = v\u2211\n\u03bd=1\n\u2016Y \u2212XT\u03bd A\u03bdB\u20162F . (18)\nr is the summation of label matrix residuals over all views. Theoretically, adding biasF \u03bd could produce a more accurate fitting model, which means a model has smaller residual r. We examine this property by using rank s = 1, ..., c\u2212 1. As we can see from Figure 1, for all the 4 datasets, the residual using bias is always smaller than the residual without bias using all different ranks. In Figure 1a, 1c, and 1d, the residual with bias (\u201cWith bias\u201d line) is smaller than the residual without bias (\u201cNo bias\u201d line). For MSRC data, the residual with bias is about 3 less than the residual without bias; for Caltech data, Figure 1b shows that the residual with bias is less than residual without bias; for Cornell data, the residual with bias is about 2 less on all rank numbers; for Cora data, the residual with bias is about 2 less on all rank numbers. In all, our results show that multi-view regression using bias could produce more accurate fitting models with less model residuals. In the following experiments, the default setting of all experiments is using bias.\nClassification using regression In multi-view regression, there are different ways to do classification. For single-view low-rank regression (Cai et al. 2013),\nmin A,B \u2016Y \u2212XTAB\u20162F + \u03bb\u2016AB\u20162F , (19)\nwhere A \u2208 <p\u00d7s, B \u2208 <s\u00d7c and AB is the low-rank regression parameter matrix, the following decision function is applied to classify a testing point x \u2208 <p\u00d71 into one of the c classes,\narg max 1\u2264j\u2264c (y)j , (20)\nwhere vector y = xTAB \u2208 <1\u00d7c, class j corresponds to the index of the maximum value in vector y.\nIn multi-view case, we predict a class using each view and then use majority voting to decide the final class. For example, for view \u03bd, we use the following decision function to classify a testing point x\u03bd \u2208 <p\u00d71 into one of the c classes,\narg max 1\u2264j\u2264c (y\u03bd)j , (21)\nwhere vector y\u03bd = xT\u03bd A\u03bdB \u2208 <1\u00d7c, x\u03bd is the data vector of view \u03bd, \u03bd = 1, 2, ..., v. Thus we predict a class label using every view. We have v predicted classes and apply majority voting on v results. The class with most votes is assigned to this data point. If the top two classes get same number of votes, we assign them with 0.5 probability, etc.. We call this majority voting as \u201cVoting\u201d in Figure 2.\nIn this regression prediction problem, however, we can theoretically derive another voting method denoted as \u201cSum\u201d. Since our starting point is Eqs.(1-3), after obtaining A\u03bd and B through training, for a testing point x, we learn y that minimizes the difference between label vector y and projected data of each views xT\u03bd A\u03bdB:\nmin y\nv\u2211\n\u03bd=1\n\u2016y \u2212 xT\u03bd A\u03bdB\u20162F . (22)\nIt is obvious that the solution of Eq.(22) is given as\ny = ( v\u2211\n\u03bd=1\nxTA\u03bdB)/v. (23)\nOnce y is computed, we use Eq.(20) to obtain the class. The classification accuracy using the two methods, Sum and Voting, is shown in Figure 2. As we can see from the results, for data Caltech, Cornell and Cora, Sum method has better results than Voting method obviously. Overall, the Sum voting method is better for regression based classification approach for multi-view regression. In the following experiments, the default setting of every experiment is using Sum method.\nRegularization weight parameter \u03bb\u03bd Regularization weight parameter \u03bb\u03bd affects the regression model and classification accuracy directly. Many researchers tune this regularization weight parameter exponentially within a specific domain, such as from 10\u22125 to 105. It is very time consuming and misleading. In fact, regularization weight parameter \u03bb\u03bd has direct contribution to the eigenvalues of (X\u03bdXT\u03bd +\u03bb\u03bdI), as shown in Eq.(7). A large \u03bb\u03bd could change the distribution of eigenvalues of (X\u03bdXT\u03bd +\u03bb\u03bdI) significantly. While a small \u03bb\u03bd preserves the original eigenvalues distribution ofX\u03bdXT\u03bd . Thus, we constrain \u03bb\u03bd to be the following 3 cases:\n1. The summation for all the eigenvalues of X\u03bdXT\u03bd . This will change the distribution of eigenvalues of (X\u03bdXT\u03bd + \u03bb\u03bdI) more significantly. Since X\u03bd is normalized rowwisely, \u03bb\u03bd = Tr(X\u03bdXT\u03bd ) = p\u03bd , where p\u03bd is dimension of view \u03bd. In Figure 3, result using this method is denoted as \u201cp\u201d.\n2. The average of all the eigenvalues of X\u03bdXT\u03bd . So \u03bb\u03bd = Tr(X\u03bdXT\u03bd )/p\u03bd = 1, where p\u03bd is dimension of view \u03bd. In Figure 3, result using this method is denoted as \u201c1\u201d.\n3. The 90%th largest eigenvalue. For example, ifX\u03bdXT\u03bd has 200 non-zero eigenvalues sorted from large to small, we let \u03bb\u03bd be the 90% \u00d7 200 = 180th eigenvalue. This will change the distribution of eigenvalues of (X\u03bdXT\u03bd + \u03bb\u03bdI) slightly and still preserve the original eigenvalue distribution of X\u03bdXT\u03bd . In Figure 3, result using this method is denoted as \u201cp90\u201d.\nFigure 3a shows that, for MSRC data, \u03bb\u03bd = 1 and \u201cp90\u201d performs better than using the summation of all eigenvalues (\u03bb\u03bd = p\u03bd). In Figure 3b, \u03bb\u03bd = 1 can beat \u201cp90\u201d and \u03bb\u03bd = p\u03bd . In Figure 3c, \u03bb\u03bd = 1 also has the best accuracy for rank s = 2, 3, 4. For data Cora, using different \u03bb\u03bd does not affect accuracy too much. Over all, we choose \u03bb\u03bd as the average of all eigenvalues of X\u03bdXT\u03bd , which is \u03bb\u03bd = 1. In the following experiments, the default setting of every experiment is using \u03bb\u03bd = 1.\nComparison with single view Multi-view regression uses data or information from multiple channels, such as different image features, both webpage citations view and contents view. Generally, we expect that multi-view regression can produce better results by exploiting information from multiple views. In this part, we compare multi-view low-rank regression with single-view low-rank regression (see (Cai et al. 2013)). Figure 4 shows that multi-view low-rank regression produces better classification accuracy than single-view regression for different ranks (rank s is from 1 to c \u2212 1). \u201cMV\u201d denotes multiview accuracy, \u201cV1\u201d, \u201cV2\u201d, ..., denote the accuracy using\ndifferent single view. For example, Figure 4a shows that, for data MSRC, multi-view regression has much higher accuracy than all single-view low-rank regression when rank s = 2, 3, 4, 5, 6. Figure 4b shows that, when rank s > 4, multi-view regression has much higher accuracy than all the four single views. In Figure 4c, view \u201cV1\u201d has very good accuracy, but multi-view regression has better results than view \u201cV1\u201d when s = 1, 3. In Figure 4d, multi-view outperforms single-view when s = 4, 5, 6.\nComparison of ridge regression and linear regression Linear regression (when \u03bb\u03bd = 0) and ridge regression (when \u03bb\u03bd 6= 0) are closely related. Previous research (Hoerl and Kennard 1970) (Cai et al. 2013) shows that ridge regression will have better performance than linear regression. However, all existing work is based on single view. Does multiview ridge regression produce better results than multiview linear regression? We will examine the performance of multi-view linear regression and ridge regression on the 4 multi-view data with respect to different ranks. We can get linear regression by simply setting \u03bb\u03bd = 0 in our existing multi-view ridge regression model. Figure 5 shows that multi-view low-rank ridge regression (\u201cRidge\u201d line in the figure) produces better classification accuracy than multiview low-rank linear regression (when \u03bb\u03bd = 0, \u201cLinear\u201d line in the figure) in datasets MSRC, Caltech and Cornell. For dataset Cora, ridge regression get slightly better results than linear regression when rank s = 3, 4, 5, 6."}, {"heading": "Comparison of low-rank and full-rank", "text": "In real life, low-rank reveals the underlying structure of datasets and removes the noise and redundant information in the datasets. Low-rank regression model has been proved to be an effective learning mechanism by exploring the lowrank structure of real life data (Xiang et al. 2012) (Evgeniou\nand Pontil 2007) (Cai et al. 2013). For full-rank regression, there is no constraint on B\u03bd in Eq.(1). We minimize the objective function of full-rank regression Eq. (16) and use the closed-form optimal solution given by Eq.(17) to solve the full-rank objective.\nFigure 6 compares classification accuracy using low-rank multi-view regression and full-rank multi-view regression. The blue dot line is the low-rank classification accuracy for rank s = 1, ..., c\u2212 1, where c is class number. The red dash line is full-rank classification accuracy with rank s = c. The horizontal axis denotes rank of regression and the vertical axis denotes classification accuracy. As we can see, for all the 4 datasets, low-rank regression model can always beat full-rank regression model. For example, in Figure 6a, lowrank results with s = 5 and s = 6 have higher accuracy than full-rank with s = 7 (red dash line). In Figure 6b, lowrank results with s = 11 to s = 19 have higher accuracy than full-rank with s = 20. Figure 6c shows low-rank results with s = 2, 3, 4 have higher accuracy than full-rank with s = 5. Figure 6d shows low-rank results with s = 4, 5, 6 have higher accuracy than full-rank with s = 7."}, {"heading": "Conclusion", "text": "In this paper, we proposed a multi-view low-rank regression model. We provide a closed-form solution to multiview low-rank regression model. Extensive experiments conducted on 4 multi-view datasets show that multi-view low rank regression outperforms full-rank regression counterpart and single-view counterpart in terms of classification accuracy.\nAcknowledgement. Research is partially supported by NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628."}], "references": [{"title": "On the equivalent of low-rank linear regressions and linear discriminant analysis based regressions", "author": ["X. Cai", "C. Ding", "F. Nie", "H. Huang"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data mining, 1124\u20131132. ACM.", "citeRegEx": "Cai et al\\.,? 2013", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proceedings of the 26th International Conference on Machine Learning, 129\u2013136. ACM.", "citeRegEx": "Chaudhuri et al\\.,? 2009", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Learning to construct knowledge bases from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam", "S. Slattery"], "venue": "Artificial Intelligence 118(1):69\u2013113.", "citeRegEx": "Craven et al\\.,? 2000", "shortCiteRegEx": "Craven et al\\.", "year": 2000}, {"title": "Spectral clustering with two views", "author": ["V.R. de Sa"], "venue": "In ICML workshop on Learning with Multiple Views", "citeRegEx": "Sa,? \\Q2005\\E", "shortCiteRegEx": "Sa", "year": 2005}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems 19:41.", "citeRegEx": "Evgeniou and Pontil,? 2007", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2007}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding 106(1):59\u201370.", "citeRegEx": "Fei.Fei et al\\.,? 2007", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "A matrix factorization approach for integrating multiple data views", "author": ["D. Greene", "P. Cunningham"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer. 423\u2013438.", "citeRegEx": "Greene and Cunningham,? 2009", "shortCiteRegEx": "Greene and Cunningham", "year": 2009}, {"title": "Multi-view datasets", "author": ["C. Grimal"], "venue": "http://lig-membres. imag.fr/grimal/data.html. [Online; accessed 11/17/2014].", "citeRegEx": "Grimal,? 2014", "shortCiteRegEx": "Grimal", "year": 2014}, {"title": "Convergence analysis of kernel canonical correlation analysis: theory and practice", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Machine Learning 74(1):23\u201338.", "citeRegEx": "Hardoon and Shawe.Taylor,? 2009", "shortCiteRegEx": "Hardoon and Shawe.Taylor", "year": 2009}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural computation 16(12):2639\u2013 2664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics 12(1):55\u201367.", "citeRegEx": "Hoerl and Kennard,? 1970", "shortCiteRegEx": "Hoerl and Kennard", "year": 1970}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika 321\u2013377.", "citeRegEx": "Hotelling,? 1936", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "A co-training approach for multi-view spectral clustering", "author": ["A. Kumar", "H. Daum\u00e9"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 393\u2013400. ACM.", "citeRegEx": "Kumar and Daum\u00e9,? 2011", "shortCiteRegEx": "Kumar and Daum\u00e9", "year": 2011}, {"title": "Co-regularized multi-view spectral clustering", "author": ["A. Kumar", "P. Rai", "H. Daume"], "venue": "Advances in Neural Information Processing Systems, 1413\u20131421.", "citeRegEx": "Kumar et al\\.,? 2011", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Foreground focus: Unsupervised learning from partially matching images", "author": ["Y.J. Lee", "K. Grauman"], "venue": "International Journal of Computer Vision 85(2):143\u2013166.", "citeRegEx": "Lee and Grauman,? 2009", "shortCiteRegEx": "Lee and Grauman", "year": 2009}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature 401(6755):788\u2013791.", "citeRegEx": "Lee and Seung,? 1999", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation", "author": ["D.W. Marquaridt"], "venue": "Technometrics 12(3):591\u2013612.", "citeRegEx": "Marquaridt,? 1970", "shortCiteRegEx": "Marquaridt", "year": 1970}, {"title": "A machine learning approach to building domainspecific search engines", "author": ["A. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "IJCAI, volume 99, 662\u2013667. Citeseer.", "citeRegEx": "McCallum et al\\.,? 1999", "shortCiteRegEx": "McCallum et al\\.", "year": 1999}, {"title": "Learning with multiple views", "author": ["S. R\u00fcping", "T. Scheffer"], "venue": "Proc. ICML Workshop on Learning with Multiple Views.", "citeRegEx": "R\u00fcping and Scheffer,? 2005", "shortCiteRegEx": "R\u00fcping and Scheffer", "year": 2005}, {"title": "Linear regression analysis, volume 936", "author": ["G.A. Seber", "A.J. Lee"], "venue": "John Wiley & Sons.", "citeRegEx": "Seber and Lee,? 2012", "shortCiteRegEx": "Seber and Lee", "year": 2012}, {"title": "Optimal exact least squares rank minimization", "author": ["S. Xiang", "Y. Zhu", "X. Shen", "J. Ye"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data mining, 480\u2013488. ACM.", "citeRegEx": "Xiang et al\\.,? 2012", "shortCiteRegEx": "Xiang et al\\.", "year": 2012}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J. Burges"], "venue": "Proceedings of the 24th International Conference on Machine Learning, 1159\u20131166. ACM.", "citeRegEx": "Zhou and Burges,? 2007", "shortCiteRegEx": "Zhou and Burges", "year": 2007}], "referenceMentions": [{"referenceID": 18, "context": "Multi-view problems aim to improve existing single view model by learning a model utilizing data collected from multiple channels (R\u00fcping and Scheffer 2005) (de Sa 2005) (Zhou and Burges 2007).", "startOffset": 130, "endOffset": 156}, {"referenceID": 21, "context": "Multi-view problems aim to improve existing single view model by learning a model utilizing data collected from multiple channels (R\u00fcping and Scheffer 2005) (de Sa 2005) (Zhou and Burges 2007).", "startOffset": 170, "endOffset": 192}, {"referenceID": 20, "context": "Low-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data (Xiang et al. 2012) (Evgeniou and Pontil 2007) (Cai et al.", "startOffset": 134, "endOffset": 153}, {"referenceID": 4, "context": "2012) (Evgeniou and Pontil 2007) (Cai et al.", "startOffset": 6, "endOffset": 32}, {"referenceID": 0, "context": "2012) (Evgeniou and Pontil 2007) (Cai et al. 2013).", "startOffset": 33, "endOffset": 50}, {"referenceID": 19, "context": "To be specific, linear regression finds a linear model with respect to the single view feature data to fit target class data (Seber and Lee 2012).", "startOffset": 125, "endOffset": 145}, {"referenceID": 10, "context": "Ridge regression can achieve better results by adding a Frobenius norm based regularization on linear regression loss objective (Hoerl and Kennard 1970) (Marquaridt 1970).", "startOffset": 128, "endOffset": 152}, {"referenceID": 16, "context": "Ridge regression can achieve better results by adding a Frobenius norm based regularization on linear regression loss objective (Hoerl and Kennard 1970) (Marquaridt 1970).", "startOffset": 153, "endOffset": 170}, {"referenceID": 0, "context": "Cai (Cai et al. 2013) showed that whenB is low-rank, regression is equivalent to linear discriminant analysis based regressions.", "startOffset": 4, "endOffset": 21}, {"referenceID": 21, "context": "Zhou (Zhou and Burges 2007) developed a multi-view spectral clustering model via generalizing the single view normalized cut to the multi-view case.", "startOffset": 5, "endOffset": 27}, {"referenceID": 12, "context": "Kumar (Kumar and Daum\u00e9 2011) proposed a co-training flavour spectral clustering algorithm and use spectral embedding from one view to constrain the similarity graph used for the other view.", "startOffset": 6, "endOffset": 28}, {"referenceID": 11, "context": "Canonical-Correlation Analysis (CCA) (Hotelling 1936) was first used to study the correlation of two views in their respective subspaces.", "startOffset": 37, "endOffset": 53}, {"referenceID": 8, "context": "Hardoon (Hardoon, Szedmak, and Shawe-Taylor 2004) (Hardoon and Shawe-Taylor 2009) designed an Kernel Canonical-Correlation Analysis to extract patterns from two views.", "startOffset": 50, "endOffset": 81}, {"referenceID": 1, "context": "Chaudhuri (Chaudhuri et al. 2009) proposed a CCA-based subspace multi-view learning approach to find a subspace such that the objects of different classes are well-separated and within-class distance is minimized.", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": "Greene (Greene and Cunningham 2009) developed a Non-negative Matrix Factorization (NMF) (Lee and Seung 1999) approach to effectively identify common patterns and reconcile between-view disagreements by combining data from multiple views.", "startOffset": 7, "endOffset": 35}, {"referenceID": 15, "context": "Greene (Greene and Cunningham 2009) developed a Non-negative Matrix Factorization (NMF) (Lee and Seung 1999) approach to effectively identify common patterns and reconcile between-view disagreements by combining data from multiple views.", "startOffset": 88, "endOffset": 108}, {"referenceID": 14, "context": "These datasets include image datasets MSRC (Lee and Grauman 2009) and Caltech (Fei-Fei, Fergus, and Perona 2007), website dataset Cornell (Craven et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 2, "context": "These datasets include image datasets MSRC (Lee and Grauman 2009) and Caltech (Fei-Fei, Fergus, and Perona 2007), website dataset Cornell (Craven et al. 2000) and scientific publication dataset Cora (McCallum et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 17, "context": "2000) and scientific publication dataset Cora (McCallum et al. 1999).", "startOffset": 46, "endOffset": 68}, {"referenceID": 7, "context": "Cornell and Cora are downloaded from (Grimal 2014).", "startOffset": 37, "endOffset": 50}, {"referenceID": 0, "context": "For single-view low-rank regression (Cai et al. 2013),", "startOffset": 36, "endOffset": 53}, {"referenceID": 0, "context": "In this part, we compare multi-view low-rank regression with single-view low-rank regression (see (Cai et al. 2013)).", "startOffset": 98, "endOffset": 115}, {"referenceID": 10, "context": "Previous research (Hoerl and Kennard 1970) (Cai et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "Previous research (Hoerl and Kennard 1970) (Cai et al. 2013) shows that ridge regression will have better performance than linear regression.", "startOffset": 43, "endOffset": 60}, {"referenceID": 20, "context": "Low-rank regression model has been proved to be an effective learning mechanism by exploring the lowrank structure of real life data (Xiang et al. 2012) (Evgeniou 1 2 3 4 5 6 0 0.", "startOffset": 133, "endOffset": 152}, {"referenceID": 0, "context": "and Pontil 2007) (Cai et al. 2013).", "startOffset": 17, "endOffset": 34}], "year": 2016, "abstractText": "Real life data often includes information from different channels. For example, in computer vision, we can describe an image using different image features, such as pixel intensity, color, HOG, GIST feature, SIFT features, etc.. These different aspects of the same objects are often called multi-view (or multi-modal) data. Lowrank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data. But previous low-rank regression model only works on single view data. In this paper, we propose a multi-view low-rank regression model by imposing low-rank constraints on multi-view regression model. Most importantly, we provide a closed-form solution to the multi-view low-rank regression model. Extensive experiments on 4 multi-view datasets show that the multi-view low-rank regression model outperforms single-view regression model and reveals that multiview low-rank structure is very helpful. Introduction In many tasks, a single object can be described using information from different channels (or views). For example, a 3-D object can be described using pictures from different angles; a website can be described using the words it contains, and the hyperlinks it contains; an image can be described using different features, such as SIFT feature, and HOG feature; in daily life, a person can be characterized using age, height, weight and so on. These data all comes from different aspects and channels. Multi-view problems aim to improve existing single view model by learning a model utilizing data collected from multiple channels (R\u00fcping and Scheffer 2005) (de Sa 2005) (Zhou and Burges 2007). Low-rank regression model has been proved to be an effective learning mechanism by exploring the low-rank structure of real life data (Xiang et al. 2012) (Evgeniou and Pontil 2007) (Cai et al. 2013). Existing regression models only work on single view data. To be specific, linear regression finds a linear model with respect to the single view feature data to fit target class data (Seber and Lee 2012). Let matrix B \u2208 <p\u00d7c be the parameter of the linear model. Linear regression solves a problem of minB ||Y \u2212 XB||F , where Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. X = [x1,x2, ...,xn] \u2208 <p\u00d7n is the single view feature data matrix and Y \u2208 <n\u00d7c is the target class indicator matrix. Ridge regression can achieve better results by adding a Frobenius norm based regularization on linear regression loss objective (Hoerl and Kennard 1970) (Marquaridt 1970). Ridge regression solves the problem minB ||Y \u2212XB||F + \u03bb||B||F , where \u03bb is the regularization weight parameter. Cai (Cai et al. 2013) showed that whenB is low-rank, regression is equivalent to linear discriminant analysis based regressions. However, all these work only works for single-view problems. In this paper, we propose a multi-view low-rank regression model by imposing low-rank constraints on regression model. This model can be solved using closed-form solution directly. In linear regression, low rank parameter matrix B is dependent on view \u03bd. Through theoretical analysis, we show that multi-view low-rank regression model is equivalent to do regression in the subspace of each view. In other words, let B = A\u03bdB, and it is equivalent to find the shared regression parameter matrix B under the subspace transformation A\u03bd with respect to view \u03bd. Extensive experiments performed on 4 multi-view datasets show that the proposed model outperforms single-view regression model and reveals that low-rank structure can improve the classification result of a full-rank model. Notations. In this paper, matrices are written in uppercase letters, such as X, Y . Vectors are written in bold lower case letters, such as x, y. Tr(X) means the trace operation for matrix X . Multi-view Low Rank Regression Assume that there are v views and c classes, p\u03bd is the dimension of view \u03bd, nj is the sample size of the j-th class, and n is the total sample size. Let X\u03bd = [x1 , ...,x \u03bd n] \u2208 <p\u03bd\u00d7n be the data matrix of view \u03bd, \u03bd = 1, 2, ..., v, and Y = [y1, ...,yc] \u2208 <n\u00d7c is the normalized class indicator matrix, i.e. Yij = 1/ \u221a nj if the i-th data point belongs to the j-th class and Yij = 0 otherwise. We try to minimize the residual of low rank regression model in each class and in each view. Loss function of multiProceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence 1973 ar X iv :1 61 0. 04 66 8v 1 [ cs .L G ] 1 4 O ct 2 01 6 view low rank ridge regression can be proposed as in Eq.(1):", "creator": "LaTeX with hyperref package"}}}