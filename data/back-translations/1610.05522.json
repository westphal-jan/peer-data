{"id": "1610.05522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Addressing Community Question Answering in English and Arabic", "abstract": "We tested our models on two sets of data published in SemEval-2016 Task 3 on \"Community Question Answering.\" Task 3 targeted real-world web forums in both English and Arabic. Our models include sack-of-words (BoW) features, syntactic tree cores (TKs), ranking features, embedding and evaluation functions for machine translation. To the best of our knowledge, structure cores have hardly been applied to the question-ranking task, where they need to model paraphrase relationships. In the case of the English question-ranking task, we compare our learning to (L2R) algorithms dictated by the Google-generated ranking (GR). Results show that i) the flat structures used in our TKs are robust enough to handle loud data and ii) to improve GR, but effective BoW features and KR features along with the two-key TKR synasters used in the Arabic TKs are bidirectional.", "histories": [["v1", "Tue, 18 Oct 2016 10:22:46 GMT  (314kb,D)", "http://arxiv.org/abs/1610.05522v1", "presented at Second WebQA workshop, SIGIR2016 (this http URL)"]], "COMMENTS": "presented at Second WebQA workshop, SIGIR2016 (this http URL)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["giovanni da san martino", "alberto barr\\'on-cede\\~no", "salvatore romeo", "alessandro moschitti", "shafiq joty", "fahad a al obaidli", "kateryna tymoshenko", "antonio uva"], "accepted": false, "id": "1610.05522"}, "pdf": {"name": "1610.05522.pdf", "metadata": {"source": "CRF", "title": "Addressing Community Question Answering in English and Arabic", "authors": ["Giovanni Da San Martino", "Alberto Barr\u00f3n-Cede\u00f1o", "Salvatore Romeo", "Alessandro Moschitti", "Shafiq Joty", "Fahad A. Al Obaidli", "Kateryna Tymoshenko", "Antonio Uva", "Daniele Bonadiman"], "emails": ["amoschitti@qf.org.qa", "sromeo@qf.org.qa", "d.bonadiman}@unitn.it"], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, there has been a renewed interest in information retrieval for community question answering1 (cQA). This combines traditional question answering with a modern Web scenario, where users pose questions on a forum expecting to receive answers from other users. As fora are fairly open, many contributors may post comments loosely connected to the original question, whereas others are simply wrong or are not answers at all. We addressed this problem in [3, 13, 20].\n1Commercial applications are currently using advanced technology such as the one we present in this paper: http://iyas.qcri.org/qldemo\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nSIGIR \u201916, July 17 - 21, 2016, Pisa, Italy c\u00a9 2016 Copyright held by the owner/author(s).\nOne of the most critical problem arises when a user posts a new question to the forum. In this case, the retrieval system would search for relevant comments linked to other questions in order to find potentially appropriate answers. In this noisy and complex setting, also powerful search engines (e.g., Google), have hard time to retrieve comments that can correctly answer the original question. An approach to deal with this case is to divide the problem into two separate tasks: (i) retrieving a set of similar questions and their associated set of comments and (ii) assessing the usefulness of the retrieved comments with respect to the question posed by the user. In this paper we focus on first task both in English and Arabic. This shows that our models can be applied to different languages.\nThe retrieval of relevant questions requires models different from typical search engines. Firstly, we deal with short texts: questions may include descriptions or subquestions but they are not typically larger than one or two paragraphs. Thus rich similarity features are necessary to deal effectively with the sparseness of such texts. Secondly, the syntactic structure of the questions is rather important for learning to recognize paraphrases and thus selecting the right candidates. For instance, consider the following two questions :\nq1 How do I get a visa for Qatar to visit my wife?\nq2 How do I get a visa for my wife to have her visit Qatar?\nQuestion q1 has roughly the same BoW representation as question q2, but their information request \u2014therefore their answers\u2014 are totally different. The structure of the questions can be exploited to detect the difference between these difficult cases.\nIn the past, the study of cQA models has been limited by a substantial lack of manually annotated data for question\u2013question similarity, as the main annotation was provided by the user themselves and thus essentially not fully reliable. Recently, a new resource has been released for the SemEval 2016 Task 3 on Answer Selection in cQA [19].2 Given a set of existing forum questions Q, where each existing question q \u2208 Q is associated with a set of answers Cq , and a new user question q\u2032, the ultimate task is to determine whether a comment c \u2208 Cq represents a pertinent answer to q\u2032 or not. This task can be subdivided into three tasks, namely: (A) to assign a relevance (goodness) score to each answer c \u2208 Cq with respect to the existing question q; (B) to re-rank the set of questions Q according to their relevance against the new question q\u2032; and finally (C) to predict the appropriateness of the answers c \u2208 Cq against q\u2032. An adaptation of Task C was proposed for Arabic (Task D). In this pa-\n2http://alt.qcri.org/semeval2016/task3\nar X\niv :1\n61 0.\n05 52\n2v 1\n[ cs\n.C L\n] 1\n8 O\nct 2\n01 6\nper, we focus on tasks B and D: question re-ranking in both English and Arabic.\nFor task B we use (i) text similarity features, derived from the classical BoW representations, e.g., n-grams, skip-grams; (ii) syntactic/structural features injected by TKs, which have shown to achieve the state of the art in the related task of answer sentence retrieval [27]; and (iii) features for modeling the initial rank provided by a state-of-the-art search engine, which represents a strong baseline. Our extensive experimentation, produced the following results: (i) the BoW features based on similarity measures alone do not improve GR. (ii) Our TKs applied to questions alone outperform the models based on similarity measures and when jointly used with the rank features improve all the models. In particular, they outperform GR by 1.72 MAP points (95% of statistical confidence).\nFor task D we apply tree kernels on pairs of syntactic trees of Arabic sentences and define additional features derived from machine translation evaluation scores.\nOur approaches to both tasks B and D granted us the second position on SemEval 2016 Task 3.\nThe rest of our contribution is distributed as follows. Section 2 includes related work. Section 3 describes the approached problem. Section 4 describes our learning to rank models. Section 5 discusses our experiments and obtained results. Section 6 includes conclusions and further work."}, {"heading": "2. RELATED WORK", "text": "The first step for any system that aims to automatically answer questions on cQA sites is to retrieve a set of questions similar to the user\u2019s input. The set of similar questions is later used to extract possible answers for the input question. However, determining question similarity remains one of the main challenges in cQA due to problems such as the \"lexical gap\". To overcome this problem, different approaches have been proposed. Early methods used statistical machine translation techniques to compute semantic similarity between two questions. For instance, [31] applied a phrase-based translation model. Their experiments on Yahoo! Answers showed that models based on phrases are more effective than those using words, as they are able to capture contextual information. However, approaches based on SMT have the problem of requiring lots of data in order to estimate parameters.\nAlgorithms that try to go beyond simple text representation are presented in [5] and [7]. In [5] a similarity between two questions on Yahoo! Answers is computed by using a language model with a smoothing method based on the category structure of Yahoo! Answers. In [7], the authors search for semantically similar questions by identifying the topic and focus of the user\u2019s question. More specifically, they compute a similarity between the questions\u2019 topic, which represents general users interests, and the questions\u2019 focus. A different approach using topic modeling for question retrieval was introduced in [10] and [30]. Here, the authors use LDA topic modeling to learn the latent semantic topics that generate question/answer pairs and use the learned topics distribution to retrieve similar historical questions. The quality of the ranking returned by all these systems was measured on a set of test questions from Yahoo! Answers, with question relevancy judgment annotated by users, sometimes assigned automatically based on heuristics.\nIt should be noted that the methods above exploited language models or general knowledge given by Yahoo! Answers categories or LDA topics, whereas in our paper, we model the syntactic/semantic relations between pairs of questions using shallow syntactic parsing and lexical matching. The most similar work to ours is [28], where\nthe authors found semantically related questions by computing the similarity between the syntactic trees of the two questions. They used a tree similarity computed in terms of the number of substructures shared between two trees. Different from such approach, we use pairs of questions, (qo, qs), as learning instances, thus defining relational models connecting the syntactic trees of qo and qs. In this way, the learning algorithms learn transformations that suggest if questions constituted by similar words have similar (paraphrases) or different semantics."}, {"heading": "3. PROBLEM DESCRIPTION", "text": "Among the four task of the SemEval-2016 competition, we focus our attention on the two dealing with question\u2013question similarity. In task B English we consider the questions\u2019 text only. In task D Arabic we also use the information from the answer linked to the forum question. Tasks B uses English instances extracted from Qatar Living, a forum for people to pose questions on multiple aspects of daily life in Qatar.3 Task D uses Arabic instances extracted from three medical fora: webteb, altibbi, and consult islamweb.4 As these are re-ranking tasks, we evaluate our models using mean average precision (MAP); which is the official evaluation measure of the SemEval 2016 task.\n3.1 Task B: English Question\u2013Question Similarity\n3http://www.qatarliving.com/forum 4https://www.webteb.com/, http://www.altibbi.com/, and http:// consult.islamweb.net.\nConceptually, question retrieval is not much different from a standard retrieval task. Given the asked (original) question qo, a search engine seeks the Web (or a specific Web forum) for relevant webpages. In cQA, webpages are threads containing questions qs \u2208 Q, with their user comments, where the latter can provide information for answering qo. For example, Table 1 shows an original question followed by some questions retrieved by a search engine. In the SemEval dataset we used in our experiments, each question is associated with 10 related threads that are the top 10 webpages retrieved by Google. The main difference with standard document retrieval is the document scoring function. Indeed, although both question and comments are part of the candidate webpage (thread), the question text provides a more synthetic and precise information to infer whether the candidate thread is relevant for qo. Table 2 gives class-distribution statistics of the English dataset [19]. A forum question can be a perfect match , relevant , or irrelevantwith respect to the new question. For evaluation purposes, both perfect match and relevant instances are considered relevant and must be ranked on top of the irrelevant questions. The corpus is composed of 387 user questions, each of which includes 10 potentially related questions. The task organizers used the Google search engine, which represents also the strong baseline for the task, to select potentially relevant forum questions. Table 3 shows the distribution of relevant/irrelevant forum questions per ranking position. Although relevant questions tend to be concentrated towards the top of the Google Rank, they are fairly spread over the entire ranking."}, {"heading": "3.2 Task D: Arabic Question\u2013Comment Pairs Re-Ranking", "text": "A new question and a set of thirty forum question\u2013comment pairs are provided \u2014the comment is always a correct answer to the forum question. The task consists in re-ranking the question\u2013 comment pairs according to three classes: (i) direct : if it is a direct answer to the new question; (ii) relevant : if it is not a direct answer to the question but it provides information related to the topic; and (iii) irrelevant : if it is an answer to another question, not related to the topic. For evaluation purposes, both direct and relevant forum questions are considered as good . Table 4 reports statics on the class distribution of the data.\nThere are about 60% of forum questions classified as good , thus the dataset is not very unbalanced.\nAs in the case of English, the Arabic collection questions are user-generated. Since the latter was extracted from medical-domain fora, they show specific challenges: a mix of medical terminology (used by the physicians) and colloquial language (used by the patients). The texts are usually long (on average questions are 50 and comments are 120 words long). These characteristics pose specific challenges to some of the models which are effective for task B on English, i.e. the tree kernels."}, {"heading": "4. OUR L2R MODELS", "text": "The ranking function for both tasks can be implemented by a scoring function r : Q\u00d7Q \u2192 R, where Q is the set of questions. The function r can be linear: r(qo, qs)=~w \u00b7 \u03c6(qo, qs), where ~w is a linear model and \u03c6() provides a feature vector representation of the pair, qo, qs.\nWe adopt binary SVMs to learn r from examples and ee model \u03c6(qo, qs) with different feature sets, which we describe in Sections 4.1 and 4.2."}, {"heading": "4.1 Tree Kernel Models", "text": "Tree kernels are functions that measure the similarity between tree structures. We essentially used the model of [25], originally proposed to rank passages. Different from [25], our questions may contain multiple subquestions, a subject, greetings, and elaborations, thus they are composed of several sentences. We merge the whole question text in a macro-tree using a fake root node connecting the parse trees of all the sentences. In both tasks we represent pairs of questions; therefore, we connect the constituents of two macro-trees corresponding to (qo, qs), respectively. Figure 1 shows an example where the relations between two Arabic questions are exploited to build a graph. The translation of the two questions is:\nqo \u201cWhat are the symptoms of irritable bowel syndrome (IBS)?\u201d\nqs \u201cWhat are the symptoms of irritable bowel?\u201d\nWe link the two macro-trees by connecting phrases, e.g., NP, VP, PP, when there is at least lexical matching between the phrases of qo and qs. Note that such links are marked with the presence of a REL tag. Finally, we apply either a partial tree kernel (PTK) or the syntactic tree kernels (STK) [18] and obtain the following kernel:\nK((qo, q i s), (qo, q j s)) =TK(t(qo, q i s), t(qo, q j s))\n+ TK(t(qis, qo), t(q j s, qo))\nwhere t(x, y) extracts the syntactic tree from text x, enriching it with REL tags computed with respect to y. Thus t is an asymmetric function.\nFor task B we used the PTK kernel. Since the trees of the Arabic data are rather large and very noisy, we used STK, which is faster and uses less features."}, {"heading": "4.2 Feature Vectors", "text": "Our L2R approach relies on various subsets of features to derive the relationship between two texts: text similarities, PTK similarity,\nGoogle rank, embedding features and machine translation evaluation features. In task B we used text similarities, PTK similarity, and Google rank. In task D we use text embedding features and machine translation evaluation features.\nText Similarity Features. We compute a total of 20 similarities, sim(qo, qj), using word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.\nPTK Features. Another similarity is obtained by comparing syntactic trees with PTK, i.e., TK(t(qo, qis), t(qis, qo)). Note that, different from the model in Section 4.1, PTK here is applied to the members of the same pair and thus only produces one feature.\nRanking-based Features. Our ranking feature is based on the ranking generated by Google. Each forum question is located in one position in the range [1, . . . , 10]. We try to exploit this information in two ways \u201cas-is\u201d (pos) or the inverse (pos\u22121).\nEmbeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22]. More specifically, we concatenate the vectors representing a new question and an existing question in the question\u2013answer pair, which is then fed to the SVM classifier.\nMTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment."}, {"heading": "5. EXPERIMENTS", "text": "We follow the evaluation framework of SemEval 2016 Task 3 in order to be able to compare our system with the ones of the competition [19].\nWe used different tools for preprocessing the text in English: OpenNLP\u2019s tokenizer, POS-tagger and chunk annotator5, and Stanford\u2019s lemmatizer [16], all accessible through DKPro Core [8]6. For Arabic texts were first removed stop-words, keeping only content and Latin words. We used the MADAMIRA toolkit [23] for segmenting the texts. In order to split the texts into sentences, we\n5https://opennlp.apache.org/ 6https://dkpro.github.io/dkpro-core/\nused the Stanford splitter.7 For parsing Arabic texts into syntactic trees, we used the Berkeley parser [24]."}, {"heading": "5.1 Experiments on Task B", "text": "In a set of preliminary experiments, we first compared a reranker \u2014SVMrank [12]\u2014 with a standard binary SVM [11]. As the results were comparable, we employed SVMs using the KeLP toolkit8, which enables to combine our three subsets of features within different kernels; namely RBF for the similarity features, tree kernels for the parse trees, and either linear or RBF kernels for the rankingbased feature. We set the C parameter of the SVMs to 1 in all the experiments and the parameters of the TKs and RBF kernels to default values.\nWe conducted three experiments with growing complexity for assessing the effectiveness of our different feature sets (see Section 4), with respect to GR. In agreement, with the SemEval challenge, we evaluate our rankings with Mean Average Precision (MAP), average Recall (AvgRec), and Mean Reciprocal Rank (MRR).\nWe tested the performance of each of the feature sets in isolation and pair-wise. Table 5 reports the obtained performance both on the development and test sets. The baseline is computed on GR, which produces a strong value as it is a product of the Google technology and its associated knowledge bases. We have two advantages, though: (i) Google is not tuned up on the specific forum data we use and (ii) probably Google does not use syntactic structures in algorithms such as TKs.\nOur results support the hypotheses above. Indeed, the MAP of the models derived by Similarities, TK, and their combinations is below GR: without accessing to the Google resources, our models can just approach the search engine\u2019s performance. However, when using the ranking feature, our best model outperforms the MAP of GR by 2.30 and 1.66 absolute percent points on the development and test sets, respectively. The RBF kernel on the ranking feature produces a larger improvement as it can more effectively express higher similarity values when the rankings of questions are close. This cannot be done with a linear kernel.\nTo better study the result above, Table 6 reports the combinations 7http://stanfordnlp.github.io/CoreNLP 8https://github.com/SAG-KeLP\nbetween the kernel function (Linear or RBF) and the representation of the ranking feature (the ranking itself or its inverse). While all combinations improve above the baseline, there is no clear indication on the choice between pos or pos\u22121. However, the use of the RBF kernel results in the highest performance.\nUH-PRHLT-primary, the best system at competition time [19], obtained a MAP of 76.70. This value is not statistically different from our 76.47; still that system uses knowledge bases, such as BabelNet and FrameNet [2], which we do not require."}, {"heading": "5.2 Experiments on Task D", "text": "We have performed three sets of experiments for task D, which coincide with our submissions at the competition and which are reported in Table 7. In the first experiment, cont1, we applied an SVM with a linear kernel on the embedding features. A second experiment, cont2, also includes machine translation evaluation features (both features are described in Section 4.2). Finally, a third experiment, primary, used a linear kernel on the embedding features in combination with the syntactic tree kernel (STK) [18], applied as described in Section 4.1, to the constituency trees of the question texts.\nThe submission cont1, using embedding features from [4], is an average system. When we add the machine translation evaluation (MTE) features the MAP increases from 38.33 to 39.98, making us jump from the 11th to the 7th position in the competition. However, when we combine tree kernels with embedding features MAP improves by more than 7 absolute points (45.50), achieving the second position in the competition, very close to the best system, i.e. 45.83.\nThe results on the Arabic dataset give more evidence that exploit-\ning syntactical information with tree kernels is crucial for achieving state-of-the-art performance and that tree kernels can be effective for different languages."}, {"heading": "6. CONCLUSIONS", "text": "We have tackled the task of question re-ranking in English and Arabic using different sets of features and exploiting the syntactical structure of the text via tree kernels. We showed that the combination of similarity features, syntactic structures based on tree kernels and features based on the ranking of search engines is able to boost the performance of a question re-ranker on a real-world cQA dataset. In particular, our results suggest that Google uses general models that can be on par with specific models trained on specific domains. However, if we also use advanced syntactic/semantic representations for modeling the structural relations between questions, we can achieve better results. Moreover, we modeled and tested relational tree kernels for cQA, which are robust to noise and can thus boost Google\u2019s ranking. In the future, we would like to better structure the representation of the questions. Indeed, as mentioned before, there are several different sections of the question text, e.g., subquestions, subject, elaborations. These could be used to improve our shallow representation, which, at the moment, merges all the question trees in a flat macro-tree.\nIn the comment reranking task we used for the first time tree kernels on an Arabic dataset and showed that they are a key component for reaching a performance comparable to the best SemEval-2016 system. In the future we plan to combine our system with the best one of the competition to further improve our results. Furthermore, we plan to address the challenges caused by the large size of the trees in order to be able to use more powerful tree kernels."}, {"heading": "Acknowledgments", "text": "This research is developed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, Qatar Foundation in collaboration with MIT. It is part of the Interactive sYstems for Answer Search (IYAS) project."}, {"heading": "7. REFERENCES", "text": "[1] L. Allison and T. Dix. A bit-string\nlongest-common-subsequence algorithm. Inf. Process. Lett., 23(6):305\u2013310, Dec. 1986.\n[2] C. F. Baker, C. J. Fillmore, and J. B. Lowe. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL \u201998, pages 86\u201390, Stroudsburg, PA, USA, 1998. Association for Computational Linguistics.\n[3] A. Barr\u00f3n-Cede\u00f1o, S. Filice, G. Da San Martino, S. Joty, L. M\u00e0rquez, P. Nakov, and A. Moschitti. Thread-level information for comment classification in community question answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 687\u2013693, Beijing, China, July 2015. Association for Computational Linguistics.\n[4] Y. Belinkov, M. Mohtarami, S. Cyphers, and J. Glass. VectorSLU: A continuous word vector approach to answer selection in community question answering systems. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval \u201915, Denver, Colorado, USA, 2015.\n[5] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. The use of categorization information in language models for question retrieval. In CIKM, pages 265\u2013274, 2009.\n[6] G. Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT \u201902, pages 138\u2013145, 2002.\n[7] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching questions by identifying question topic and question focus. In ACL, pages 156\u2013164, 2008.\n[8] R. Eckart de Castilho and I. Gurevych. A broad-coverage collection of portable nlp components for building shareable analysis pipelines. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, Dublin, Ireland, August 2014.\n[9] P. Jaccard. \u00c9tude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles, pages 547\u2013579, 1901.\n[10] Z. Ji, F. Xu, B. Wang, and B. He. Question-answer topic model for question retrieval in community question answering. In CIKM, pages 2471\u20132474, 2012.\n[11] T. Joachims. Making Large-scale Support Vector Machine Learning Practical. In Advances in Kernel Methods. MIT Press, Cambridge, MA, USA, 1999.\n[12] T. Joachims. Optimizing search engines using clickthrough data. KDD, pages 133\u2013142, 2002.\n[13] S. Joty, A. Barr\u00f3n-Cede\u00f1o, G. Da San Martino, S. Filice, L. M\u00e0rquez, A. Moschitti, and P. Nakov. Global thread-level inference for comment classification in community question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 573\u2013578, Lisbon, Portugal, September 2015. Association for Computational Linguistics.\n[14] A. Lavie and M. Denkowski. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2\u20133):105\u2013115, 2009.\n[15] C. Lyon, J. Malcolm, and B. Dickerson. Detecting short passages of similar text in large document collections. EMNLP, pages 118\u2013125, 2001.\n[16] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360, 2014.\n[17] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT \u201913, pages 746\u2013751, Atlanta, GA, USA, 2013.\n[18] A. Moschitti. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In ECML, pages 318\u2013329. 2006.\n[19] P. Nakov, L. M\u00e0rquez, A. Moschitti, W. Magdy, H. Mubarak, A. A. Freihat, J. Glass, and B. Randeree. SemEval-2016 task 3: Community question answering. In Proceedings of SemEval \u201916. ACL, 2016.\n[20] M. Nicosia, S. Filice, A. Barr\u00f3n-Cede\u00f1o, I. Saleh, H. Mubarak, W. Gao, P. Nakov, G. Da San Martino, A. Moschitti, K. Darwish, L. M\u00e0rquez, S. Joty, and W. Magdy. QCRI: Answer selection for community question answering - experiments for Arabic and English. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval \u201915, Denver, Colorado, USA, 2015.\n[21] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Philadelphia, Pennsylvania, USA, 2002.\n[22] R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda. Arabic Gigaword Fifth Edition. Linguistic Data Consortium (LDC), Philadelphia, 2011.\n[23] A. Pasha, M. Al-Badrashiny, M. Diab, A. E. Kholy, R. Eskander, N. Habash, M. Pooleery, O. Rambow, and R. Roth. Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, may 2014.\n[24] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404\u2013411, Rochester, New York, April 2007. Association for Computational Linguistics.\n[25] A. Severyn and A. Moschitti. Structural relationships for large-scale learning of answer re-ranking. SIGIR, pages 741\u2013750, 2012.\n[26] M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA \u201906, Cambridge, Massachusetts, USA, 2006.\n[27] K. Tymoshenko and A. Moschitti. Assessing the impact of syntactic and semantic structures for answer passages reranking. In Proceedings of CIKM \u201915, pages 1451\u20131460, New York, NY, USA, 2015. ACM.\n[28] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree matching approach to finding similar questions in community-based qa services. In SIGIR, pages 187\u2013194, 2009.\n[29] M. Wise. Yap3: Improved detection of similarities in computer program and other texts. In SIGCSE, pages 130\u2013134, 1996.\n[30] K. Zhang, W. Wu, H. Wu, Z. Li, and M. Zhou. Question retrieval with high quality answers in community question answering. In CIKM, pages 371\u2013380, 2014.\n[31] G. Zhou, L. Cai, J. Zhao, and K. Liu. Phrase-based translation model for question retrieval in community question answer archives. In ACL, pages 653\u2013662, 2011."}], "references": [{"title": "A bit-string longest-common-subsequence algorithm", "author": ["L. Allison", "T. Dix"], "venue": "Inf. Process. Lett., 23(6):305\u2013310, Dec.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL \u201998, pages 86\u201390, Stroudsburg, PA, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Thread-level information for comment classification in community question answering", "author": ["A. Barr\u00f3n-Cede\u00f1o", "S. Filice", "G. Da San Martino", "S. Joty", "L. M\u00e0rquez", "P. Nakov", "A. Moschitti"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "VectorSLU: A continuous word vector approach to answer selection in community question answering systems", "author": ["Y. Belinkov", "M. Mohtarami", "S. Cyphers", "J. Glass"], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval \u201915, Denver, Colorado, USA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of categorization information in language models for question retrieval", "author": ["X. Cao", "G. Cong", "B. Cui", "C.S. Jensen", "C. Zhang"], "venue": "CIKM, pages 265\u2013274,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["G. Doddington"], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, HLT \u201902, pages 138\u2013145,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Searching questions by identifying question topic and question focus", "author": ["H. Duan", "Y. Cao", "C.-Y. Lin", "Y. Yu"], "venue": "ACL, pages 156\u2013164,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A broad-coverage collection of portable nlp components for building shareable analysis pipelines", "author": ["R. Eckart de Castilho", "I. Gurevych"], "venue": "In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "\u00c9tude comparative de la distribution florale dans une portion des Alpes et des Jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles, pages 547\u2013579,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1901}, {"title": "Question-answer topic model for question retrieval in community question answering", "author": ["Z. Ji", "F. Xu", "B. Wang", "B. He"], "venue": "CIKM, pages 2471\u20132474,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Making Large-scale Support Vector Machine Learning Practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods. MIT Press, Cambridge, MA, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "KDD, pages 133\u2013142,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Global thread-level inference for comment classification in community question answering", "author": ["S. Joty", "A. Barr\u00f3n-Cede\u00f1o", "G. Da San Martino", "S. Filice", "L. M\u00e0rquez", "A. Moschitti", "P. Nakov"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The METEOR metric for automatic evaluation of machine translation", "author": ["A. Lavie", "M. Denkowski"], "venue": "Machine Translation, 23(2\u20133):105\u2013115,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting short passages of similar text in large document collections", "author": ["C. Lyon", "J. Malcolm", "B. Dickerson"], "venue": "EMNLP, pages 118\u2013125,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human  Language Technologies, NAACL-HLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees", "author": ["A. Moschitti"], "venue": "ECML, pages 318\u2013329.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "SemEval-2016 task 3: Community question answering", "author": ["P. Nakov", "L. M\u00e0rquez", "A. Moschitti", "W. Magdy", "H. Mubarak", "A.A. Freihat", "J. Glass", "B. Randeree"], "venue": "Proceedings of SemEval \u201916. ACL,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "QCRI: Answer selection for community question answering - experiments for Arabic and English", "author": ["M. Nicosia", "S. Filice", "A. Barr\u00f3n-Cede\u00f1o", "I. Saleh", "H. Mubarak", "W. Gao", "P. Nakov", "G. Da San Martino", "A. Moschitti", "K. Darwish", "L. M\u00e0rquez", "S. Joty", "W. Magdy"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Philadelphia, Pennsylvania, USA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic Gigaword Fifth Edition", "author": ["R. Parker", "D. Graff", "K. Chen", "J. Kong", "K. Maeda"], "venue": "Linguistic Data Consortium (LDC), Philadelphia,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic", "author": ["A. Pasha", "M. Al-Badrashiny", "M. Diab", "A.E. Kholy", "R. Eskander", "N. Habash", "M. Pooleery", "O. Rambow", "R. Roth"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, may", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved inference for unlexicalized parsing", "author": ["S. Petrov", "D. Klein"], "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404\u2013411, Rochester, New York, April", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Structural relationships for large-scale learning of answer re-ranking", "author": ["A. Severyn", "A. Moschitti"], "venue": "SIGIR, pages 741\u2013750,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA \u201906, Cambridge, Massachusetts, USA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Assessing the impact of syntactic and semantic structures for answer passages reranking", "author": ["K. Tymoshenko", "A. Moschitti"], "venue": "Proceedings of CIKM \u201915, pages 1451\u20131460, New York, NY, USA,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A syntactic tree matching approach to finding similar questions in community-based qa services", "author": ["K. Wang", "Z. Ming", "T.-S. Chua"], "venue": "SIGIR, pages 187\u2013194,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Yap3: Improved detection of similarities in computer program and other texts", "author": ["M. Wise"], "venue": "SIGCSE, pages 130\u2013134,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Question retrieval with high quality answers in community question answering", "author": ["K. Zhang", "W. Wu", "H. Wu", "Z. Li", "M. Zhou"], "venue": "CIKM, pages 371\u2013380,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Phrase-based translation model for question retrieval in community question answer archives", "author": ["G. Zhou", "L. Cai", "J. Zhao", "K. Liu"], "venue": "ACL, pages 653\u2013662,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 12, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 19, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 18, "context": "Recently, a new resource has been released for the SemEval 2016 Task 3 on Answer Selection in cQA [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": ", n-grams, skip-grams; (ii) syntactic/structural features injected by TKs, which have shown to achieve the state of the art in the related task of answer sentence retrieval [27]; and (iii) features for modeling the initial rank provided by a state-of-the-art search engine, which represents a strong baseline.", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "For instance, [31] applied a phrase-based translation model.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "Algorithms that try to go beyond simple text representation are presented in [5] and [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "Algorithms that try to go beyond simple text representation are presented in [5] and [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "In [5] a similarity between two questions on Yahoo! Answers is computed by using a language model with a smoothing method based on the category structure of Yahoo! An-", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the authors search for semantically similar questions by identifying the topic and focus of the user\u2019s question.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "A different approach using topic modeling for question retrieval was introduced in [10] and [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "A different approach using topic modeling for question retrieval was introduced in [10] and [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "The most similar work to ours is [28], where Table 1: A re-ranking example for the English Question\u2013 Question Similarity dataset.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "Table 2 gives class-distribution statistics of the English dataset [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "We essentially used the model of [25], originally proposed to rank passages.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Different from [25], our questions may contain multiple subquestions, a subject, greetings, and elaborations, thus they are composed of several sentences.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Finally, we apply either a partial tree kernel (PTK) or the syntactic tree kernels (STK) [18] and obtain the following kernel:", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "We follow the evaluation framework of SemEval 2016 Task 3 in order to be able to compare our system with the ones of the competition [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "We used different tools for preprocessing the text in English: OpenNLP\u2019s tokenizer, POS-tagger and chunk annotator, and Stanford\u2019s lemmatizer [16], all accessible through DKPro Core [8].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "We used different tools for preprocessing the text in English: OpenNLP\u2019s tokenizer, POS-tagger and chunk annotator, and Stanford\u2019s lemmatizer [16], all accessible through DKPro Core [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 22, "context": "We used the MADAMIRA toolkit [23] for segmenting the texts.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "For parsing Arabic texts into syntactic trees, we used the Berkeley parser [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "In a set of preliminary experiments, we first compared a reranker \u2014SVMrank [12]\u2014 with a standard binary SVM [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "In a set of preliminary experiments, we first compared a reranker \u2014SVMrank [12]\u2014 with a standard binary SVM [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "UH-PRHLT-primary, the best system at competition time [19], obtained a MAP of 76.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "47; still that system uses knowledge bases, such as BabelNet and FrameNet [2], which we do not require.", "startOffset": 74, "endOffset": 77}, {"referenceID": 17, "context": "Finally, a third experiment, primary, used a linear kernel on the embedding features in combination with the syntactic tree kernel (STK) [18], applied as described in Section 4.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "The submission cont1, using embedding features from [4], is an average system.", "startOffset": 52, "endOffset": 55}, {"referenceID": 18, "context": "Baselines as provided reported in [19].", "startOffset": 34, "endOffset": 38}], "year": 2016, "abstractText": "This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval2016 Task 3 on \u201cCommunity Question Answering\u201d. Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that (i) the shallow structures used in our TKs are robust enough to noisy data and (ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic.", "creator": "LaTeX with hyperref package"}}}