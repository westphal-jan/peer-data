{"id": "1610.05688", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixing models (GMM) and the hidden Markov model (HMM) to obtain binary class names for DNN training. Subverbs in speech recognition systems correspond to context-dependent bound states or senons. The present work addresses some limitations of GMM-HMM senon alignments for DNN training. We assume that the senon probabilities obtained in a DNN with binary labels can provide more precise targets to learn better acoustic models. However, DNN results exhibit inaccuracies that are represented as high-dimensional unstructured noise, while the informative components are structured and low-dimensioned. We use principle component analysis (PCA) and sparse coding to characterize the sensing underspaces, which allows for improved probabilities to be derived from low and low-density DNN targets.", "histories": [["v1", "Tue, 18 Oct 2016 16:02:10 GMT  (2065kb,D)", "http://arxiv.org/abs/1610.05688v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.LG", "authors": ["pranay dighe", "afsaneh asaei", "herve bourlard"], "accepted": false, "id": "1610.05688"}, "pdf": {"name": "1610.05688.pdf", "metadata": {"source": "CRF", "title": "LOW-RANK AND SPARSE SOFT TARGETS TO LEARN BETTER DNN ACOUSTIC MODELS", "authors": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Soft targets, Principle component analysis, Sparse coding, Automatic speech recognition, Untranscribed data."}, {"heading": "1. INTRODUCTION", "text": "DNN based acoustic models have been state-of-the-art for automatic speech recognition over the past few years [1]. While DNN input consists of multiple frames of acoustic features, the target output is obtained from a frame level GMM-HMM forced alignment corresponding to the context dependent tied triphone states or senones [2]. This procedure results in inefficiency in DNN acoustic modeling [3, 4]. Unlike the conventional practice, the present work argues that the optimal DNN targets are probability distributions rather than Kronecker deltas (hard targets). Earlier studies on optimal training of a neural network for HMM decoding provides rigorous theoretical analysis that supports this idea [5]. Here, we propose a DNN based data driven framework to obtain accurate probability distributions (soft targets) for improved DNN acoustic modeling. The proposed approach relies on modeling of low-dimensional senone subspaces in DNN posterior probabilities.\nSpeech production is known as the result of activations of a few highly constrained articulatory mechanisms leading to generation of linguistic units (e.g. phones, senones) on low-dimensional non-linear manifolds [6, 7]. In the context of DNN acoustic modeling, low-dimensional structures are exhibited in the space of DNN senone posteriors [8]. Low-rank and sparse representations are found promising to characterize senone-specific subspaces [9, 10]. The senone-specific structures are superimposed with high-dimensional unstructured noise. Hence, projection of DNN posteriors on their underlying low-dimensional subspaces enhances the DNN posterior accuracies. In this work, we propose a new application of enhanced DNN posteriors to generate accurate soft targets\nfor DNN acoustic modeling. Earlier works on exploiting low-dimensionality in DNN acoustic modeling focus on exploiting low-rank and sparse representations to modify DNN architectures for small footprint implementation. In [11, 12] low-rank decomposition of the neural network\u2019s weight matrices enables reduction in DNN complexity and memory footprint. Similar goals have been achieved by exploiting sparse connections [13] and sparse activations [14] in hidden layers of DNN. In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17]. This approach relies on soft targets providing more information for DNN training than the binary hard alignments.\nWe propose to bring together the advantage of higher information content of soft targets with the accurate model of senone space provided by low-rank and sparse representations to train superior DNN acoustic models. Soft targets enable characterization of the senone-specific subspaces by quantifying the correlations between senone classes as well as sequential dependencies (details in Section 2.1). This information is manifested in the form of structures visible among a large population of training data posterior probabilities. Potential of these posteriors to be used as soft targets for DNN training is reduced due to presence of unstructured noise. Therefore, to obtain reliable soft targets, we perform lowrank and sparse reconstruction of training data posteriors to preserve the global low-dimensional structures while discarding the random high-dimensional noise. The new DNNs trained with low-rank or sparse soft targets are capable of estimating the test posteriors on a low-dimensional space which results in better ASR performance. We consider PCA (Section 2.2) and dictionary based sparse coding (Section 2.3) for generating low-rank and sparse representations respectively. Strength of PCA lies in capturing the linear regularities in the data [18] whereas an over-complete dictionary used for sparse coding learns to model the non-linear space as a union of lowdimensional subspaces. Dictionary based sparse reconstruction also reduces the rank of the senone posterior space [9].\nExperimental evaluations are conducted on AMI corpus [19], a collection of recordings of multi-party meetings for large vocabulary speech recognition. We show in Section 3 that low-rank and sparse soft targets lead to training of better DNN acoustic models. Reductions in word error rate (WER) are observed over the baseline hybrid DNN-HMM system without the need of explicit sparse coding or low-rank reconstruction of test data posteriors. Moreover, they enable effective use of out-of-domain untranscribed data by augmenting AMI training data in a knowledge transfer fashion. DNNs trained with low-rank and sparse soft targets yield upto 4.6% relative improvement in WER, whereas a DNN trained with non-enhanced soft targets fails to exploit any further knowledge provided by the untranscribed data. To the best of our knowledge, significant benefit of DNN generated soft targets for training a more accurate DNN\nar X\niv :1\n61 0.\n05 68\n8v 1\n[ cs\n.C L\n] 1\n8 O\nct 2\n01 6\nacoustic model has not been shown in the prior work. In the rest of the paper, the proposed approach is described in Section 2. Experimental analysis is carried out in Section 3. Section 4 presents the concluding remarks and directions for future work."}, {"heading": "2. LOW-RANK AND SPARSE SOFT TARGETS", "text": "This section describes the novel approach towards reliable soft target estimation. We study reasons for regularities among senone posteriors and investigate two systematic approaches to obtain more accurate probabilities as soft targets for DNN acoustic modeling."}, {"heading": "2.1. Towards Better Targets for DNN Training", "text": "Earlier works on distillation of the DNN knowledge show the potential of soft targets for model compression and the sub-optimal nature of the hard alignments [15, 20]. Although hard targets assign a particular senone label to a relatively long sequence of (\u223c10 or more) acoustic frames, senone durations are usually shorter. A long context of input frames may lead to presence of acoustic features corresponding to multiple senones in the input (Fig. 1(a)), so the assumption of binary outputs renders inaccurate.\nIn contrast, soft outputs quantify such sequential information using non-zero probabilities for multiple senone classes. Contextual senone dependencies arising in soft targets can be attributed to the ambiguities due to phonetic transitions [20]. Furthermore, the procedure of senone extraction leads to acoustic correlations among multiple classes corresponding to the same phone-HMM states [2], as they all share the same root in the decision tree (Fig. 1(b)).\nThese dependencies can be characterized by analyzing a large number of senone probabilities from the training data. The frequent dependencies are exhibited as regularities among the correlated dimensions in senone posteriors. As a result, a matrix formed by concatenation of class-specific senone posteriors has a low-rank structure. In other words, class-specific senones lie in low-dimensional subspaces with a dimension higher than unity [9], that violates the principal assumption of binary hard targets.\nIn practice, inaccuracies in DNN training lead to the presence of unstructured high-dimensional errors (Fig. 1(c)). Therefore, the initial senone posterior probabilities obtained from the forward pass of a DNN trained with hard alignments are not accurate in quantifying the senone dependency structures. Our previous work demonstrates that the erroneous estimations can be separated using low-rank or sparse representations [10, 9]. In the present study, we consider application of PCA and sparse coding to obtain more reliable soft targets for DNN acoustic model training."}, {"heading": "2.2. Low-rank Reconstruction Using Eigenposteriors", "text": "Let zt = [p(s1|xt) . . . p(sk|xt) . . . p(sK |xt)]> denote a forward pass estimate of the posterior probabilities of K senone classes {sk}Kk=1, given the acoustic feature xt at time t. DNN is trained using the initial labels obtained from GMM-HMM forced alignment. We collect N senone posteriors which are labeled as class sk in GMM-HMM forced alignment and mean-center them in the logarithmic domain as follows:\nz\u0303t = ln(zt)\u2212 \u00b5sk (1) where \u00b5sk is mean of the collected posteriors in log-domain. Due to skewed distribution of the posterior vectors, the logarithm of posteriors fits better the Gaussian assumption of PCA. We concatenate the N senone sk posterior vectors after operation shown in (??) to form a matrix Msk \u2208 R\nK\u00d7N . For the sake of brevity, the subscript sk is dropped in the subsequent expressions. However, all the calculations are performed for each of the senone classes individually.\nPrincipal components of the senone space are obtained via eigenvector decomposition [21] of covariance matrix of M . The covariance matrix is obtained as C = 1\nN\u22121MM >. We factorize the\ncovariance matrix as C = PSP> where P \u2208 RK\u00d7K identifies the eigenvectors and S is a diagonal matrix containing the sorted eigenvalues. Eigenvectors in P which correspond to the large eigenvalues in S constitute the frequent regularities in the subspace, whereas others carry the high-dimensional unstructured noise. Hence, the low-rank projection matrix is defined as DLR = Pl \u2208 RK\u00d7l (2) where Pl is truncation of P that keeps only the first l eigenvectors and discards the erroneous variability captured by other K \u2212 l components. We select l such that relatively \u03c3% variability is preserved in the low-rank reconstruction of original senone matrix M .\nThe eigenvectors stored in the low-rank projection Pl are referred to as \u201ceigenposteriors\u201d of the senone space (in the same spirit as eigenfaces are defined for low-dimensional modeling of human faces [22]).\nLow-rank reconstruction of a mean-centered log posterior z\u0303t, denoted by z\u0303LRt is estimated as\nz\u0303LRt = DLRDLR >z\u0303t (3)\nFinally, we add the mean \u00b5sk to z\u0303 LR t and take its exponent to obtain a low-rank senone posterior zLRt for the acoustic frame xt. Low-rank posteriors obtained for the training data are used as soft targets for learning better DNNs (Fig.2). We assume that \u03c3% variability, that quantifies the low-rank regularities in senone spaces, is a parameter independent of the senone class."}, {"heading": "2.3. Sparse Reconstruction Using Dictionary Learning", "text": "Unlike PCA, over-complete dictionary learning and sparse coding enables modeling of non-linear low-dimensional manifolds. Sparse modelling assumes that senone posteriors can be generated as sparse linear combination of senone space representatives, collected in a dictionary DSP. We use online dictionary learning algorithm [23] to learn an over-complete dictionary for senone sk using a collection of N training data posteriors of senone sk, such that\nDSP = arg min D,A tN\u2211 t=t1 \u2016zt \u2212D\u03b1t\u201622 + \u03bb\u2016\u03b1t\u20161 (4)\nwhere A = [\u03b1t1 . . . \u03b1tN ] and \u03bb is a regularization factor. Again we have dropped the subscript sk, but all calculations are still senonespecific. Sparse reconstruction (Fig.2) of senone posteriors is thus obtained by first estimating the sparse representation [24] as\n\u03b1t = arg min \u03b1 \u2016zt \u2212DSP \u03b1\u201622 + \u03bb\u2016\u03b1\u20161. (5)\nfollowed by reconstruction as zSPt = DSP \u03b1t \u2200t \u2208 {t1, . . . , tN}. (6)\nSparse reconstructed senone posteriors have been previously found to be more accurate acoustic models for DNN-HMM speech recognition [9]. In particular, it was shown that the rank of senonespecific matrices is much lower after sparse reconstruction. In the present work, we investigate if they could also provide more accurate soft targets for DNN training Regularization parameter \u03bb in (??)- (??) controls the level of sparsity and the level of noise being removed after sparse reconstruction. Fig. 2 summarises the low-rank and sparse reconstruction of senone posteriors."}, {"heading": "3. EXPERIMENTAL ANALYSIS", "text": "In this section we evaluate the effectiveness of low-rank and sparse soft targets to improve the performance of DNN-HMM speech recognition. We also investigate the importance of better DNN acoustic models to exploit information from untranscribed data."}, {"heading": "3.1. Database and Speech Features", "text": "Experiments are conducted on AMI corpus [19] which contains recordings of spontaneous conversations in meeting scenarios. We use recordings from individual head microphones (IHM) comprising of around 67 hours of train set, 9 hours of development, (dev) set, and 7 hours test set. 10% of training data is used for cross-validation during DNN training, whereas dev set is used for tuning regularization parameters \u03c3 and \u03bb. For experiments using untranscribed additional training data, we use ICSI meeting corpus [25] and Librispeech corpus [26]. Data from ICSI corpus consists of meeting recordings (around 70 hours). Librispeech data is read speech from audio-books and we use a 100 hour subset of it.\nKaldi toolkit [27] is used for training DNN-HMM systems. All DNNs have 9 frames of temporal context at acoustic input and 4 hidden layers with 1200 neurons each. Input features are 39 dimensional MFCC+\u2206+\u2206\u2206 (39\u00d79=351 dimensional input) and output is 4007 dimensional senone probability vector. AMI pronunciation dictionary has \u223c23K words and a bigram model for decoding. For dictionary learning and sprase coding, SPAMS toolbox [28] is used."}, {"heading": "3.2. Baseline DNN-HMM using Hard and Soft Targets", "text": "Our baseline is a hybrid DNN-HMM system trained using forced aligned targets (IHM setup in [29]). WER using baseline DNN is 32.4% on AMI test set. Another baseline is a DNN trained using non-enhanced soft targets from the baseline. This system gives a WER of 32.0%. All soft-target based DNNs are randomly initialized and trained using cross-entropy loss backpropagation."}, {"heading": "3.3. Generation of Low-rank and Sparse Soft Targets", "text": "We group DNN forward pass senone probabilities for the training data into class-specific senone matrices. For this, senone labels from the ground truth based GMM-HMM hard alignments are used. Each matrix is restricted to have N = 104 vectors of K = 4007 senone probabilities to facilitate computation of principal components and sparse dictionary learning. We found the average rank of senone matrices, defined as the number of singular values required to preserve 95% variability, to be 44. Dictionaries of size 500 columns were learned for each senone, making them nearly 10 times overcomplete. The procedure as depicted in Fig. 2 is implemented to generate low-rank and sparse soft-targets.\nWe also encountered memory issues while storing large matrices of senone probabilities for all training and cross-validation data. It requires enormous amounts of storage space (similar to [16]). Hence, we preserve precision only upto first two decimal places in soft targets, followed by normalizing the vector to sum 1 before storing on the disk. We assume that essential information might not be in dimensions with very small probabilities. Although such thresholding can be a compromise to our approach, we did some experiments with higher precision (upto 5 decimal places), but there was no significant improvement in ASR. Both low-rank and sparse reconstruction were still computed on full soft-targets without any rounding; we perform thresholding only when storing targets on the disk.\nFirst we tune the variability preserving low-rank reconstruction parameter \u03c3 and sparsity regularizer \u03bb for better ASR performance in AMI dev set. When \u03c3=80% of variability is preserved in the principal components space, the most accurate soft targets are achieved for DNN acoustic modeling resulting in the smallest WER. Likewise, \u03bb = 0.1 was found the optimal value for sparse reconstruction. It may be noted that in both low-rank and sparse reconstruction, there is an optimal amount of enhancement needed for improving ASR.\nWhile less enhancement leads to continued presence of noise in soft targets, too much of it results in loss of essential information."}, {"heading": "3.4. DNN-HMM Speech Recognition", "text": "Speech recognition using DNNs trained with the new soft targets obtained from low-rank and sparse reconstruction is compared in Table 1). System-0 is the baseline hard target based DNN. System1 is built by supervised enhancement of soft outputs obtained from system-0 on AMI training data as shown in Fig. 2. As expected, training with the soft targets yields lower WER than the baseline hard targets. We can see that both PCA and sparse reconstruction result in more accurate acoustic modeling, where sparse reconstruction achieves 0.8% absolute reduction in WER.\nSparse reconstruction is found to work better than low-rank reconstruction for ASR. It can be due to the higher accuracy of sparse model in characterizing the non-linear senone subspaces [8]. Unlike previous works [9, 10] which required two stages of DNN forward pass and explicit low-dimensional projection, a single DNN is learned here that estimates the probabilities directly on a lowdimensional space."}, {"heading": "3.5. Training with Untranscribed Data", "text": "Given an accurate DNN acoustic model and some untranscribed input speech data, we can obtain soft targets for the new data through forward pass. Assuming that the initial model can generalize well on unseen data, the additional soft targets thus generated can be used to augment our original training data. We propose to learn better DNN acoustic models using this augmented training set. This method is reminiscent of the knowledge transfer approach [15, 16] which is typically used for model compression. In this work, we use the same network architecture for all experiments.\nDNNs trained with low-rank and sparse soft targets are used to generate soft targets for ICSI corpus and Librispeech (LIB100) which are sources of untranscribed data. Table 1 shows interesting observations from various experiments using data augmentation. First, system-2 is built augmenting enhanced AMI training data with ICSI soft targets generated from system-1. We consider ICSI corpus, consisting of spontaneous speech from meeting recordings, as in-domain with AMI corpus. While PCA based DNN successfully exploits information from this additional ICSI data showing significant improvement from system-1 to system-2, the same is not observed using sparsity based DNN.\nNext, system-3 is built by augmenting enhanced AMI data with Librispeech(LIB100) soft targets obtained from system 1. Read audio book speech data from Librispeech is out-of-domain as compared to spontaneous speech in AMI. Still, system-3 achieves similar reductions in WER as observed in system-2 which was built using in-domain ICSI data.\nSystem 4 and 5 were built to further explore if we could extract even more information from the out-of-domain Librispeech data by using soft targets from system-2 instead of system-1. Note that system-2, trained using soft targets from both AMI and ICSI spontaneous speech data, is a more accurate model than system 1. Indeed, both system 4 and 5 perform better than previous systems using PCA\nbased DNNs where system 5 outperforms the hard target based baseline by 1.5% absolute reduction in WER.\nSurprisingly, DNN soft targets obtained from sparse reconstruction are not able to exploit the unseen data in all the systems. We speculate that dictionary learning for sparse coding captures the nonlinearities specific to AMI database. These nonlinear characteristics may correspond to channel and recording conditions which vary over different databases and can not be transcended. On the other hand, the local linearity assumption of PCA leads to extraction of a highly restricted basis set that captures the most important dynamics in the senone probability space. Such regularities mainly address the acoustic dependencies among senones which are generalizable to other acoustic conditions. Hence, the eigenposteriors are invariant to the exceptional effects due to channel and recording conditions.\nSparse reconstruction is able to mitigate the undesired effects as long as they have been seen in the training data. Given the superior performance of sparse reconstruction of AMI posteriors (in system1), we believe that sparse modeling might be more powerful if some labeled data from unseen acoustic conditions is made available for dictionary learning.\nIt may be noted that training with additional untranscribed data is not effective if non-enhanced soft targets are used. In fact, systems 2-5 without low-rank or sparse reconstruction, perform worse than system-1 although they have seen more training data."}, {"heading": "4. CONCLUSIONS AND FUTURE DIRECTIONS", "text": "We presented a novel approach to improve DNN acoustic model training using low-rank and sparse soft targets. PCA and sparse coding were employed to identify senone subspaces, and enhance senone probabilities through low-dimensional reconstruction. Lowrank reconstruction using PCA relies on the existance of eigenposteriors capturing the local dynamics of senone subspaces. Although, sparse reconstruction proves more effective to achieve reliable soft targets when transcribed data is provided, low-rank reconstruction is found generalizable to out-of-domain untranscribed data. DNN trained on low-rank reconstruction acheives 4.6% relative reduction in WER, whereas DNN trained using non-enhanced soft targets fails to exploit additional information from additional data. Eigenposteriors can be better estimated using robust PCA [30] and sparse PCA [31] for better modeling of senone subspaces. Furthermore, probabilistic PCA and maximum likelihood eigen decomposition can reduce the computational cost for large scale applications.\nThis study supports the use of probabilistic outputs for DNN acoustic modeling. Specifically, enhanced soft targets can be more effective in training small footprint DNNs based on model compression. In future, we plan to investigate their usage in cross-lingual knowledge transfer [32]. We will also study domain adaptation based on the notion of eigenposteriors."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "Research leading to these results has received funding from SNSF project on \u201cParsimonious Hierarchical Automatic Speech Recognition (PHASER)\u201d grant agreement number 200021-153507."}, {"heading": "6. REFERENCES", "text": "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] Steve J Young, Julian J Odell, and Philip C Woodland, \u201cTreebased state tying for high accuracy acoustic modelling,\u201d in Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994.\n[3] Navdeep Jaitly, Vincent Vanhoucke, and Geoffrey Hinton, \u201cAutoregressive product of multi-frame predictions can improve the accuracy of hybrid models,\u201d 2014.\n[4] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, \u201cGmm-free dnn acoustic model training,\u201d in IEEE ICASSP, 2014.\n[5] Herve Bourlard, Yochai Konig, and Nelson Morgan, REMAP: Recursive Estimation and Maximization of a Posteriori Probabilities; Application to Transition-based Connectionist Speech Recognition, ICSI Technical Report TR-94-064, 1994.\n[6] Li Deng, \u201cSwitching dynamic system models for speech articulation and acoustics,\u201d in Mathematical Foundations of Speech and Language Processing, pp. 115\u2013133. Springer New York, 2004.\n[7] Simon King, Joe Frankel, Karen Livescu, Erik McDermott, Korin Richmond, and Mirjam Wester, \u201cSpeech production knowledge in automatic speech recognition,\u201d The Journal of the Acoustical Society of America, 2007.\n[8] Pranay Dighe, Afsaneh Asaei, and Herve\u0301 Bourlard, \u201cSparse modeling of neural network posterior probabilities for exemplar-based speech recognition,\u201d Speech Communication, 2015.\n[9] Pranay Dighe, Gil Luyet, Afsaneh Asaei, and Herve Bourlard, \u201cExploiting low-dimensional structures to enhance dnn based acoustic modeling in speech recognition,\u201d in IEEE ICASSP, 2016.\n[10] Gil Luyet, Pranay Dighe, Afsaneh Asaei, and Herve\u0301 Bourlard, \u201cLow-rank representation of nearest neighbor phone posterior probabilities to enhance dnn acoustic modeling,\u201d in Interspeech, 2016.\n[11] Jian Xue, Jinyu Li, and Yifan Gong, \u201cRestructuring of deep neural network acoustic models with singular value decomposition.,\u201d in INTERSPEECH, 2013.\n[12] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in IEEE ICASSP, 2013.\n[13] Dong Yu, Frank Seide, Gang Li, and Li Deng, \u201cExploiting sparseness in deep neural networks for large vocabulary speech recognition,\u201d in IEEE ICASSP, 2012.\n[14] Jian Kang, Cheng Lu, Meng Cai, Wei-Qiang Zhang, and Jia Liu, \u201cNeuron sparseness versus connection sparseness in deep neural network for large vocabulary speech recognition,\u201d in ICASSP, April 2015, pp. 4954\u20134958.\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.\n[16] William Chan, Nan Rosemary Ke, and Ian Lane, \u201cTransferring knowledge from a rnn to a dnn,\u201d in Interspeech, 2015.\n[17] Ryan Price, Ken-ichi Iso, and Koichi Shinoda, \u201cWise teachers train better dnn acoustic models,\u201d EURASIP Journal on Audio, Speech, and Music Processing, , no. 1, pp. 1\u201319, 2016.\n[18] Brian Hutchinson, Mari Ostendorf, and Maryam Fazel, \u201cA sparse plus low-rank exponential language model for limited resource scenarios,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 494\u2013504, 2015.\n[19] Iain McCowan, Jean Carletta, W Kraaij, S Ashby, S Bourban, M Flynn, M Guillemot, T Hain, J Kadlec, V Karaiskos, et al., \u201cThe ami meeting corpus,\u201d in Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, 2005, vol. 88.\n[20] Dan Gillick, Larry Gillick, and Steven Wegmann, \u201cDon\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition,\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.\n[21] Jonathon Shlens, \u201cA tutorial on principal component analysis,\u201d arXiv preprint arXiv:1404.1100, 2014.\n[22] L. Sirovich and M. Kirby, \u201cLow-dimensional procedure for the characterization of human faces,\u201d J. Opt. Soc. Am. A, pp. 519\u2013524, 1987.\n[23] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro, \u201cOnline learning for matrix factorization and sparse coding,\u201d Journal of Machine Learning Research (JMLR), vol. 11, pp. 19\u201360, 2010.\n[24] Robert Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.\n[25] Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, et al., \u201cThe icsi meeting corpus,\u201d in IEEE ICASSP, 2003.\n[26] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in IEEE ICASSP, 2015.\n[27] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka\u0301s\u030c Burget, Ondr\u030cej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motl\u0131\u0301c\u030cek, Yanmin Qian, Petr Schwarz, et al., \u201cThe kaldi speech recognition toolkit,\u201d 2011.\n[28] Julien Mairal, Francis Bach, and Jean Ponce, \u201cSparse modeling for image and vision processing,\u201d arXiv preprint arXiv:1411.3230, 2014.\n[29] I. Himawan, P. Motlicek, D. Imseng, B. Potard, N. Kim, and J. Lee, \u201cLearning feature mapping using deep neural network bottleneck features for distant large vocabulary speech recognition,\u201d in IEEE ICASSP, 2015, pp. 4540\u20134544.\n[30] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma, \u201cRobust recovery of subspace structures by low-rank representation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, , no. 99, pp. 1\u20131, 2013.\n[31] Hui Zou, Trevor Hastie, and Robert Tibshirani, \u201cSparse principal component analysis,\u201d Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265\u2013286, 2006.\n[32] Pawel Swietojanski, Arnab Ghoshal, and Steve Renals, \u201cUnsupervised cross-lingual knowledge transfer in dnn-based lvcsr,\u201d in IEEE Spoken Language Technology Workshop (SLT), 2012."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Treebased state tying for high accuracy acoustic modelling", "author": ["Steve J Young", "Julian J Odell", "Philip C Woodland"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models", "author": ["Navdeep Jaitly", "Vincent Vanhoucke", "Geoffrey Hinton"], "venue": "2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Gmm-free dnn acoustic model training", "author": ["A. Senior", "G. Heigold", "M. Bacchiani", "H. Liao"], "venue": "IEEE ICASSP, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "REMAP: Recursive Estimation and Maximization of a Posteriori Probabilities; Application to Transition-based Connectionist Speech Recognition, ICSI", "author": ["Herve Bourlard", "Yochai Konig", "Nelson Morgan"], "venue": "Technical Report TR-94-064,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Switching dynamic system models for speech articulation and acoustics", "author": ["Li Deng"], "venue": "Mathematical Foundations of Speech and Language Processing, pp. 115\u2013133. Springer New York, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech production knowledge in automatic speech recognition", "author": ["Simon King", "Joe Frankel", "Karen Livescu", "Erik McDermott", "Korin Richmond", "Mirjam Wester"], "venue": "The Journal of the Acoustical Society of America, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse modeling of neural network posterior probabilities for exemplar-based speech recognition", "author": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Speech Communication, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting low-dimensional structures to enhance dnn based acoustic modeling in speech recognition", "author": ["Pranay Dighe", "Gil Luyet", "Afsaneh Asaei", "Herve Bourlard"], "venue": "IEEE ICASSP, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank representation of nearest neighbor phone posterior probabilities to enhance dnn acoustic modeling", "author": ["Gil Luyet", "Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Interspeech, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "IEEE ICASSP, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "IEEE ICASSP, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Neuron sparseness versus connection sparseness in deep neural network for large vocabulary speech recognition", "author": ["Jian Kang", "Cheng Lu", "Meng Cai", "Wei-Qiang Zhang", "Jia Liu"], "venue": "ICASSP, April 2015, pp. 4954\u20134958.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferring knowledge from a rnn to a dnn", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "Interspeech, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Wise teachers train better dnn acoustic models", "author": ["Ryan Price", "Ken-ichi Iso", "Koichi Shinoda"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, , no. 1, pp. 1\u201319, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A sparse plus low-rank exponential language model for limited resource scenarios", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 494\u2013504, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The ami meeting corpus", "author": ["Iain McCowan", "Jean Carletta", "W Kraaij", "S Ashby", "S Bourban", "M Flynn", "M Guillemot", "T Hain", "J Kadlec", "V Karaiskos"], "venue": "Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, 2005, vol. 88.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["Dan Gillick", "Larry Gillick", "Steven Wegmann"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A tutorial on principal component analysis", "author": ["Jonathon Shlens"], "venue": "arXiv preprint arXiv:1404.1100, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-dimensional procedure for the characterization of human faces", "author": ["L. Sirovich", "M. Kirby"], "venue": "J. Opt. Soc. Am. A, pp. 519\u2013524, 1987.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1987}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "Journal of Machine Learning Research (JMLR), vol. 11, pp. 19\u201360, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "The icsi meeting corpus", "author": ["Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke"], "venue": "IEEE ICASSP, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "IEEE ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Luk\u00e1\u0161 Burget", "Ond\u0159ej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motl\u0131\u0301\u010dek", "Yanmin Qian", "Petr Schwarz"], "venue": "2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse modeling for image and vision processing", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce"], "venue": "arXiv preprint arXiv:1411.3230, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning feature mapping using deep neural network bottleneck features for distant large vocabulary speech recognition", "author": ["I. Himawan", "P. Motlicek", "D. Imseng", "B. Potard", "N. Kim", "J. Lee"], "venue": "IEEE ICASSP, 2015, pp. 4540\u20134544.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, , no. 99, pp. 1\u20131, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse principal component analysis", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265\u2013286, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised cross-lingual knowledge transfer in dnn-based lvcsr", "author": ["Pawel Swietojanski", "Arnab Ghoshal", "Steve Renals"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "DNN based acoustic models have been state-of-the-art for automatic speech recognition over the past few years [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "While DNN input consists of multiple frames of acoustic features, the target output is obtained from a frame level GMM-HMM forced alignment corresponding to the context dependent tied triphone states or senones [2].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "This procedure results in inefficiency in DNN acoustic modeling [3, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 3, "context": "This procedure results in inefficiency in DNN acoustic modeling [3, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 4, "context": "Earlier studies on optimal training of a neural network for HMM decoding provides rigorous theoretical analysis that supports this idea [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "phones, senones) on low-dimensional non-linear manifolds [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "phones, senones) on low-dimensional non-linear manifolds [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 7, "context": "In the context of DNN acoustic modeling, low-dimensional structures are exhibited in the space of DNN senone posteriors [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Low-rank and sparse representations are found promising to characterize senone-specific subspaces [9, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "Low-rank and sparse representations are found promising to characterize senone-specific subspaces [9, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 10, "context": "In [11, 12] low-rank decomposition of the neural network\u2019s weight matrices enables reduction in DNN complexity and memory footprint.", "startOffset": 3, "endOffset": 11}, {"referenceID": 11, "context": "In [11, 12] low-rank decomposition of the neural network\u2019s weight matrices enables reduction in DNN complexity and memory footprint.", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "Similar goals have been achieved by exploiting sparse connections [13] and sparse activations [14] in hidden layers of DNN.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Similar goals have been achieved by exploiting sparse connections [13] and sparse activations [14] in hidden layers of DNN.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 117, "endOffset": 125}, {"referenceID": 15, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "Strength of PCA lies in capturing the linear regularities in the data [18] whereas an over-complete dictionary used for sparse coding learns to model the non-linear space as a union of lowdimensional subspaces.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Dictionary based sparse reconstruction also reduces the rank of the senone posterior space [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "Experimental evaluations are conducted on AMI corpus [19], a collection of recordings of multi-party meetings for large vocabulary speech recognition.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Earlier works on distillation of the DNN knowledge show the potential of soft targets for model compression and the sub-optimal nature of the hard alignments [15, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "Earlier works on distillation of the DNN knowledge show the potential of soft targets for model compression and the sub-optimal nature of the hard alignments [15, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "Contextual senone dependencies arising in soft targets can be attributed to the ambiguities due to phonetic transitions [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Furthermore, the procedure of senone extraction leads to acoustic correlations among multiple classes corresponding to the same phone-HMM states [2], as they all share the same root in the decision tree (Fig.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "In other words, class-specific senones lie in low-dimensional subspaces with a dimension higher than unity [9], that violates the principal assumption of binary hard targets.", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "Our previous work demonstrates that the erroneous estimations can be separated using low-rank or sparse representations [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 8, "context": "Our previous work demonstrates that the erroneous estimations can be separated using low-rank or sparse representations [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 20, "context": "Principal components of the senone space are obtained via eigenvector decomposition [21] of covariance matrix of M .", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "The eigenvectors stored in the low-rank projection Pl are referred to as \u201ceigenposteriors\u201d of the senone space (in the same spirit as eigenfaces are defined for low-dimensional modeling of human faces [22]).", "startOffset": 201, "endOffset": 205}, {"referenceID": 22, "context": "We use online dictionary learning algorithm [23] to learn an over-complete dictionary for senone sk using a collection of N training data posteriors of senone sk, such that", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "2) of senone posteriors is thus obtained by first estimating the sparse representation [24] as", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Sparse reconstructed senone posteriors have been previously found to be more accurate acoustic models for DNN-HMM speech recognition [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 18, "context": "Experiments are conducted on AMI corpus [19] which contains recordings of spontaneous conversations in meeting scenarios.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "For experiments using untranscribed additional training data, we use ICSI meeting corpus [25] and Librispeech corpus [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "For experiments using untranscribed additional training data, we use ICSI meeting corpus [25] and Librispeech corpus [26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "Kaldi toolkit [27] is used for training DNN-HMM systems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "For dictionary learning and sprase coding, SPAMS toolbox [28] is used.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "Our baseline is a hybrid DNN-HMM system trained using forced aligned targets (IHM setup in [29]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "It requires enormous amounts of storage space (similar to [16]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "It can be due to the higher accuracy of sparse model in characterizing the non-linear senone subspaces [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Unlike previous works [9, 10] which required two stages of DNN forward pass and explicit low-dimensional projection, a single DNN is learned here that estimates the probabilities directly on a lowdimensional space.", "startOffset": 22, "endOffset": 29}, {"referenceID": 9, "context": "Unlike previous works [9, 10] which required two stages of DNN forward pass and explicit low-dimensional projection, a single DNN is learned here that estimates the probabilities directly on a lowdimensional space.", "startOffset": 22, "endOffset": 29}, {"referenceID": 14, "context": "This method is reminiscent of the knowledge transfer approach [15, 16] which is typically used for model compression.", "startOffset": 62, "endOffset": 70}, {"referenceID": 15, "context": "This method is reminiscent of the knowledge transfer approach [15, 16] which is typically used for model compression.", "startOffset": 62, "endOffset": 70}, {"referenceID": 29, "context": "Eigenposteriors can be better estimated using robust PCA [30] and sparse PCA [31] for better modeling of senone subspaces.", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "Eigenposteriors can be better estimated using robust PCA [30] and sparse PCA [31] for better modeling of senone subspaces.", "startOffset": 77, "endOffset": 81}, {"referenceID": 31, "context": "In future, we plan to investigate their usage in cross-lingual knowledge transfer [32].", "startOffset": 82, "endOffset": 86}], "year": 2016, "abstractText": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and lowdimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.", "creator": "LaTeX with hyperref package"}}}