{"id": "1610.05945", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "A multi-task learning model for malware classification with useful file access pattern from API call sequence", "abstract": "Based on API call sequences, semantic-conscious and machine learning (ML) based malware classifiers can be created for the detection or classification of malware. Previous work has focused on creating and extracting various features from malware binaries, disassembled binaries or API calls using static or dynamic analysis and resorting to ML to create classifiers. However, they require too much feature engineering and do not offer interpretability. We solve these two problems with the recent advances in deep learning: 1) RNN-based autoencoders (RNN-AEs) can automatically learn a low-dimensional representation of malware from its raw API call sequence. 2) Multiple decoders can be trained under different supervision to provide more information than the class or family designation of a malware. Inspired by the work of document classification and automated sentence synthesis, each API can be considered a multi-dimensional summary of the first set of an API.", "histories": [["v1", "Wed, 19 Oct 2016 10:06:14 GMT  (2270kb,D)", "http://arxiv.org/abs/1610.05945v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CR cs.LG", "authors": ["xin wang", "siu ming yiu"], "accepted": false, "id": "1610.05945"}, "pdf": {"name": "1610.05945.pdf", "metadata": {"source": "CRF", "title": "A multi-task learning model for malware classification with useful file access pattern from API call sequence", "authors": ["Xin Wang", "Siu Ming Yiu"], "emails": ["smyiu}@cs.hku.hk"], "sections": [{"heading": "Introduction", "text": "Malware continues to be one the the big security threats for both the Internet and computing devices. It can be used for espionage, advertisements promotion, ransom demand and other unauthorised activities on your networks and systems. Due to the ubiquity of malware, automatic tools are usually deployed for malware detection or classification. So many ML algorithms have been applied to the classification problems of malwares.\nClassification problems of malwares can be divided into two types: (I) malware detection, which is a binary classification problem and decides whether a sample is benign or malicious; (II) malware classification, which is a multi-class classification problem and outputs the family label of a sample known to be malicious. We refer to them as type I and type II respectively.\nThere are generally two kind of approaches to analyze malwares that are used to build ML-based malware clas-\nsifiers: static analysis and dynamic analysis. Static analysis examines the binary executables directly or after disassembling without really executing them. Diverse static features can be used to build malware classifiers, such as PE header information, n-grams at different granularity levels (Masud, Khan, and Thuraisingham 2008), global descriptors of malware image (Nataraj et al. 2011), section entropy, etc. Usually, compositions of various static features are used to get high accuracy (Ahmadi et al. 2016), which consequently lead to high-dimensional binary sparse feature vectors. Random projections and PCA (Dahl et al. 2013) or feature selection (Lin et al. 2015) are often performed to reduce the dimensions of the input space. However, the problem is that extractions of some static features are already very time consuming and memory intensive. Even worse, various obfuscation techniques (You and Yim 2010) make static analysis difficult nowadays and evasive techniques are available to defeat it (Sikorski and Honig 2012). Dynamic analysis observes the behaviors of a malware usually by actually executing it in a sandbox environment. Malwares are then correlated based on the similarity of their behaviors. Two typical methods of dynamic analysis are control flow analysis and API call analysis. API call information can be extracted from both static analysis and dynamic analysis. Earlier works tend to use a simple frequency representation (Tian et al. 2010) (Shankarapani et al. 2010) of the API calls, whose drawback is evident that API calls in one sequence are treated individually and isolately. The sequentiality of the API calls is such a very important feature that should be considered. Some works extract API call semantics (Zhang et al. 2014) or control flow information (Christodorescu et al. 2005) into graph representations. However, it involves too much manual tweak on graph matching and sophiscated feature engineering. Feature engineering can be hard because it requires specific domain knowledge to design helpful features and involves burdensome deployment and testing. Given the huge amount and ever increasing diversity of the malwares, it is important to build scalable model that can learn features of malware automatically. In this paper, we propose a model that learns representations of malware samples in an unsupervised way.\nFurthermore, another big problem of previous works on malware detection or classification is the lack of interpretability. For malware detection, a type I classifier marks\nar X\niv :1\n61 0.\n05 94\n5v 1\n[ cs\n.S D\n] 1\n9 O\nct 2\n01 6\na suspicious sample as benign or malicious without give an understandable reason of the marking. As for malware classification, a type II classifier outputs a family label. However, sometimes just knowing the family label is not very helpful or even meaningless. For example, packing is often used by malwares to hide their real payload, which theoretically can be used by any malwares. Though a classifier can tell you that a malware is packed, it tells nothing about the payload of the packed malware. The payload matters because it is the payload that cause substantial harms to a system and a typical classifier is not able to tell you what the payload does. Due to the flexibility of the payload of a malware, our focus should come to the payload.\nEven worse, when it comes to zero-day (Wiki 2016c) malwares, a type II classifier will make a mistake because it can only output predefined and already known family labels. So we want to ask the question of can we provide more interpretability for classification problems of malware? As an answer to this question, we propose to generate an FAP, which is a brief description of file access related behaviors, for each sample rather than just giving the class or family label.\nWe propose a more interpretable model, multi-task malware learning model, that can be used for malware classification and FAP generation at the same time. Apart from classification, we define an FAP generation task which is combined with classification to provide interpretable information. We construct our model based on the multitask seq2seq model (Luong et al. 2015). Classification and FAP generation, though quite different, are both defined as seq2seq problems in our model.\nIn summary, this paper makes the following contributions:\n\u2022 We propose a novel multi-task malware learning model on raw malware API call sequences. First, low dimensional representations of malware API call sequences are learned by RNN-AE in an unsupervised manner. Then multiple decoders, malware classifier and FAP generator, can be trained under the corresponding supervisions. To the best of our knowledge, this is the first time that multi-task learning is applied to malware learning to provide more interpretability.\n\u2022 Apart from a malware classifier, we propose a decoder for FAP generation. Inspired by the works of automatic sentence summarization (Rush, Chopra, and Weston 2015)(Nallapati et al. 2016) in Natural Language Processing (NLP), we formulate the FAP generation problem, which can be seen as automatic summarization of API call sequences. As far as we know, this is also the first attempt.\n\u2022 In our model, we reformulate the malware classification problem with RNN as a special case of seq2seq problem whose output length equals to 1. Instead of obtaining the feature vectors of malware by training an RNN to predict the next API call (Pascanu et al. 2015), we think that learning representations with RNN-AE is more natural and intuitive because RNN-AE has been widely used for representation learning in many other tasks."}, {"heading": "Preliminaries", "text": "Recurrent Neural Nerworks Unlike traditional neural network that assume all inputs are independent of each other, a recurrent neural network (RNN) is able to deal with sequential data with variable length and has shown its power in many NLP tasks. The idea is an RNN maintains a hidden state which can be seen as the memory of the network. At each time step t, the hidden state ht is updated by:\nht = f(ht\u22121, xt),\nwhere f is an activiation function that usually provides nonlinearity, and xt is the input at time step t. At each time step, an RNN performs the same calculations on different inputs with the same shared parameters.\nAn RNN can effectively learn the conditional distribution p(Y|X) where output sequence Y = (y1, ..., yT \u2032 ) and input sequence X = (x1, ..., xT ). The length of input T and that of output T \u2032 can be different. Combined with hidden state update equation above, The conditional probability distribution p(Y|X) can be unrolled to:\np(y1, ..., yT \u2032 |x1, ..., xT ) = T \u2032\u220f t=1 p(yt|ht\u22121, y1, ..., yt\u22121)\nEach p(yt|ht\u22121, y1, ..., yt\u22121) is a softmax distribution over all the input symbols.\nAutoencoders Autoencoder (AE) is a special neural network which tries to reconstruct its input. An AE consists of an encoder and a decoder, where the encoder map the input to a low-dimensional fixed-length vector from which the decoder recover the input as output. The most important feature of AE is that it learns in an unsupervised way."}, {"heading": "Multi-task Malware Learning Model", "text": "The basic seq2seq model can be transformed into multi-task seq2seq learning model (Luong et al. 2015). The multi-task extension can improve the performance of seq2seq model on machine translation centered tasks. According to the number of encoders or decoders, three settings are available in MTL seq2seq model: one-to-many, many-to-one, many-tomany. Our model employs a one-to-many setting, consisting of one encoder for representation learning and two decoders for classification and FAP generation respectively (see Fig. 1).\nRepresentation learning on API call sequence API1 call sequences are usually collected by hooking (Wiki 2016b) while running the samples in a sandbox environment. Compared to other methods, one advantage of classifying malwares using API call sequences is that they are inherently semantic-aware since every single API call is an exact action performed by malware, e.g. creation, read, write, modification and deletion of files or registry keys. One thing we should emphasize is that the semantic information of an API call sequence does not only lies in each single API call, but also lies in the sequence itself.\n1API in this paper means Windows kernel API by default.\nRNNs are known for their ability to capture the long term features in sequential or time-series data. An RNN trained to predict next API call can be applied for malware classification (Pascanu et al. 2015). Hidden states of the model are used as feature vectors to train a separate classifier. While in our model, we reformulate the malware classification problem with RNNs as a special sequence to sequence learning (Sutskever, Vinyals, and Le 2014) (also known as seq2seq) problem whose output length equals to one. In a seq2seq model, a sequence is read by an RNN encoder into fixedlength hidden state, which then can be fed to an RNN decoder to predict the output sequence. Both the length of input and output can be variable, making the seq2seq a very general and powerful model for many applications, e.g. machine translation (Sutskever, Vinyals, and Le 2014), text summerization (Nallapati et al. 2016), sentiment analysis (Dai and Le 2015).\nRNN-AE maps an input API call sequence to a fixedlength vector C, which is usually a low-dimensional representation of an input API call sequence.\nMultiple Decoders The classification task in our model trains a type II classifier and performs malware classification on samples known to be malicious, while the FAP generation task trains an API call sequence summarizer and outputs file access summaries of samples. A decoder shares the same structure as the RNN-AE used for representation learning, and its hidden state is initialized with the accumulated internal state C (see Fig. 1). A decoder starts with feeding a start symbol \u201cGO\u201d, and generates a distribution. The symbol whose index corresponds to the highest probability in the distribution is the output symbol. Then the symbol generated is fed to the decoder iteratively until the target sequence is fully generated.\nDecoders are trained to minimize the cross-entropy over the sequences. Given two discrete probability distribution p and q, the cross entropy between them is\nH(p, q) = \u2212 \u2211 x p(x) log q(x) (1)\n. In the language model, cross entropy measures how accurate the model is in predicting the test data. p is the true distribution, where the entry corresponding to the true target equals to 1 and others equals to 0. q is the learned distribution. The closer p and q are, the smaller the cross-entropy\nis. The loss function is the full cross-entropy over the test dataset\nL(\u03b8) = \u2212 1 N N\u2211 i=1 L\u2211 j=1 log q\u03b8(yij |xi, C) (2)\nwhere N and L are the number of the test dataset and the length of the target sequence respectively, and \u03b8 corresponds to the model parameters."}, {"heading": "Evaluations", "text": "We evaluate the accuracy of our model on a public malware API call sequence dataset (Kim 2016) at different granularity levels. We select two datasets which include 7430 samples for coarse-grained evaluation and 4932 samples for fine-grained evaluation (see Tab. 1 and Tab. 6). Malwares whose families are unknown or whose corresponding families do not have enough samples for training are dropped. They are split randomly into train, validation and test datasets, containing 75%, 5%, 20% samples respectively."}, {"heading": "Preprocessing", "text": "Supervisions are necessary for the training of decoders. For classification, class labels are already available in the dataset. As to the FAP generation, we first give our definition of FAP and then briefly describe how we extract an FAP for each malware.\nDefinition of FAP : Assume S = {s1, s2, ..., sn} is a set of file access related APIs, which is called FAP set, and l \u2208 Z, then we say p = |s|l is an FAP of length l, where s \u2208 S.\nWe select seven file access related APIs from Windows kernel APIs as our FAP set. Different Windows kernel APIs that perform the same function are mapped to one API (see Table 2). For example, both CopyFile and CopyFileEx perform the function that copy a file, and the difference lies in whether the file already exists. Each function usually have two names for Unicode with W as suffix and for ANSI with A as suffix respectively. All of them are mapped to CopyFile in our FAP set.\nWe employ a simple way to extract FAPs. We first set the length of our FAP to be lp = |S|, where |S| denotes the number of elements in set S . Then we generate a binary representation for each malware, which is a binary vector v with length lp. For the i-th malware, vi[j] = 1 if the j-th API in FAP set can be extracted from the i-th malware API call sequence, otherwise vi[j] = 0, where j = 1, ..., lp. We call concatenation of the elements in Si the FAP of the i-th\nmalware, where Si is subset of S corresponding to the binary vector vi. For example, if FAP set S = {a, b, c, d} and vi = [1, 0, 1, 1], then we get Si = {a, c, d} and pi = \u201dacd\u201d after concatenation."}, {"heading": "Model setup", "text": "As our model is a multi-task model based on basic seq2seq model, we first experiment on seq2seq model for classification and FAP generation as baselines. Then we verify our multi-task malware learning model on classification and FAP generation tasks respectively. There are many variants RNN units since the advent of LSTM (Hochreiter and Schmidhuber 1997). We use GRU as our default RNN unit, which has been shown to be more computationally efficient than standard LSTM without identifiable loss of performance (Chung et al. 2014). The standard experiment (AE + Decoder) trains on the train dataset and decodes on the test dataset. We can also train on the full dataset (AE(full) + Decoder) because representation learning is unsupervised. We also evaluate on the bidirectional extension of the our GRU-based model with train dataset (bAE + Decoder) and full dataset (bAE(full) + Decoder). For the standard unidirectional RNNs, the output at each time step depends on the inputs at that time step and before. The bidirectional extension allow an RNN has access to both the inputs in the past and future."}, {"heading": "Coarse-grained Evaluations and Results", "text": "We experiment on the ground truth. All FAP candidates are mapped to indices (see Tab. 5). We first evaluate on samples from trojan-fakeav, adware, worm, which we denote\nwith F3, and then on samples from F3 and packed. The reason behind this is that we find classification performance on samples from F3 and packed decreases evidently compared to the performance on samples from F3. We visualize the feature vectors in tsne (van der Maaten and Hinton 2008), a method to visualising high-dimensional vectors (see Fig. 2). Adware samples form two clear clusters, while samples from other families are very scattered and some of them are highly interweaved. A feature vector, learned from the API call sequence, can be seen as a behavioral aggregation of a malware. In Fig. 2, we can see samples from different families may share very similar behavioral aggregation, while samples from the same family may share quite different behavioral aggregation. That is what a classifier alone can not tell. However, an FAP generator is able to extract the pattern that a malware access the file system regardless of which family the malware belongs to. Unlike classification, the performance of FAP generation does not fluctuate on different datasets. The malwares from different families exhibit obviously different dominant FAPs (see Fig. 3). In our empirical experiments, the trainings with full data or bidirectional extension of GRU do not bring evident increase in performance. As to the connections between some specific FAPs and the malware families, we refer to the following section Case Studies for the detailed analyses."}, {"heading": "Case Studies", "text": "We analyze the possible correlations between the FAP of a malware and its family in this section. Presumably, malwares from different families may exhibit different FAPs, and those who are in the same family is supposed to have similar FAPs to some extent.\nWorm A worm is a kind of malware that usually spreads by replicating itself via network. One of the typical actions a worm will perform is \u201dCopy\u201d (sophos 2016a) (sophos 2016b). We can find that the ratio of FAP p6, including \u201dCopyFile\u201d, is considerable in the worm samples, yet is ignorable in samples from other family (see Fig. 3(c) and Tab. 5).\nFAP ID CreateFile WriteFile p1 CreateFile ReadFile p2 CreateFile WriteFile ReadFile p3 CreateFile p4 CreateFile ReadFile GetTempFileName SetFileAttributes DeleteFile WriteFile p5 CreateFile WriteFile CopyFile p6\nTable 5: FAP mapping list\n(a) Distribution of FAP p6 (b) Distribution of FAP p5\nFigure 4: Significance of FAPs in Coarse-grained Evaluation\nAdware Adware (Wiki 2016a) is a form of malware that downloads and displays unwanted ads when a user is online, redirects search requests to certain advertising websites according to the collected user\u2019s information without the user\u2019s awareness. Generally, an adware will send some HTTP requests according to user\u2019s browsing history. Then store the requested files to Temp (www.askvg.com 2016) directory2 for displaying and drop them in the end (sophos 2016c) (sophos 2016d). We can find that the typical subsequence the dominant FAP for adware (see Fig. 3(a) and Tab. 5) has is GetTempFileName, SetFileAttributes, DeleteFile. GetTempFileName create a name for a temporary file and SetFileAttributes is followed to set the related attributes of the file. At last, DeleteFile is called to drop some files. This typical subsequence can be found in around 70% of the samples of adware and almost none in samples from other families. So the it is highly correlated with behaviours of adware.\n2Temp is a default environment variable in Windows system, which points to a path of folder used to store temporary files. Files in Temp folder are absolutely safe to remove.\nPacked & Trojan-fakeav Packed malware is a kind of malware that hides the real code of a program through one or more layers of compression or encryption. The top 3 FAPs of packed and trojan-fakeav are similar and do not include special FAPs like worm and adware (see Fig. 3). Packing or more general obfuscation is usually used as an evasive technique to avoid detection and analysis, which is not exclusive to some specific family of malwares. Because a malware packed does not imply specifically deterministic behaviors, so the API call sequence can be quite similar to that of other samples. The reason of the decrease of classification accuracy on F3 and packed arguably is the intersections of the behaviors of the samples. The interweaved samples from different families are not easy to classify, which necessitate the more interpretable information."}, {"heading": "Fine-grained Evaluations and Results", "text": "A dataset with 8 families (see Tab. 6) is selected for finegrained evaluation. We first evaluate the quality of learned representations by visualising with tsne. The quality of the features is quite self-explainatory, samples from different families form different clusters (see Fig. 5). From the results, we can see both the classification and the FAP generation performance are more competitive than that in coarsegrained evaluation (see Tab. 7).\nCompared to other families, feature vectors of some samples from trojan-fakeav.win32.smartfortress and packed.win32.krap are quite scattered and interweaved (see Fig. 5). This justifies the essentiality of more insightful and interpretable FAP information, because these samples can be very similar and are hard to tell apart from the API call sequences. The fine-grained evaluation also narrow the scope of FAP p6 and p5 to net-worm.win32.allaple in worm and adware.win32.megasearch in adware, respectively (see Fig. 6)."}, {"heading": "Discussion", "text": "Scalability It is not enough that ML-based malware model just give us a class label without any explainatory information. On the one hand, given a malware, a classfier can be wrong, and can be much confident or less confident even if it is right. On the other hand, malwares from the same family do not necessarily perform the same way to our system.\nIt is natural that we want to know what a malware will do to a system except for the family it belongs to.\nExperiment results show that our multi-task malware learning model is able to give FAP as well as class label of a malware. Malware representations learned by RNNautoencoder from API call sequences are robust enough to\ntrained multiple decoders with quite different objectives and output more informative results.\nRather than performing one single task at a time, our model first learn representations of malwares in an unsupervised way, and then multiple decoders can be trained very efficiently than the training of one single seq2seq task.\nLimitations and Future works So far, we only use API call sequences for representation learning. Different from classification problems of images and texts, diverse source data can be used for classification problems of malware. Apart from API call sequences, so much additional information, like arguments of API call, structure of the executables, can be leveraged for malware detection or classification. How can we merge these kinds of information into our model to improve the classification accuracy and provide more interpretability is to be done in the future.\nAnother problem is lack of supervision data, because the decoders are trained in a supervised way. In our evaluation on FAP generation, we adopt a very simple way to craft FAPs from API call sequences and prove its effectiveness. Explorations of decoders with new functions and ways to generate corresponding supervision data are very important to build a robust and informative malware learning model."}, {"heading": "Conclusion", "text": "There are two problems of previous works on malware classification: (I) Malwares evolve everyday and new unknown families keep emerging. Classifiers built to output known family labels alone are not enough; (II) Labels themselves are not very interpretable for samples from the same family may perform quite differently even if the label is right. It is more robust to give a brief description to the behaviors of a malware as well as the class label. We build a multitask malware learning model based on the proven powerful multi-task seq2seq model for classification and FAP generation. An FAP tells what a malware do to a file system, and sometimes it points directly to the family the malware belongs to. Our tentative results show that not only can seq2seq model be used for malware classification based on API call sequences, but can be used for generating more insightful information from the representations learned by RNN-AE. At the same time, unsupervised representation learning enable the model to automatically leverage huge amount of unlabelled data without further feature engineering."}], "references": [{"title": "Novel feature extraction, selection and fusion for effective malware family classification", "author": ["Ahmadi"], "venue": "In Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy,", "citeRegEx": "Ahmadi,? \\Q2016\\E", "shortCiteRegEx": "Ahmadi", "year": 2016}, {"title": "Semanticsaware malware detection", "author": ["Christodorescu"], "venue": "IEEE Symposium on Security and Privacy (S&P\u201905),", "citeRegEx": "Christodorescu,? \\Q2005\\E", "shortCiteRegEx": "Christodorescu", "year": 2005}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["Dahl"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Dahl,? \\Q2013\\E", "shortCiteRegEx": "Dahl", "year": 2013}, {"title": "Semisupervised sequence learning", "author": ["Dai", "A.M. Le 2015] Dai", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Long short-term memory. Neural computation 9(8):1735\u20131780", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Api call sequence dataset. http://ocslab.hk security.net/apimds-dataset; accessed 11August-2016", "author": ["H.K. Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "Feature selection and extraction for malware classification", "author": ["Lin"], "venue": "Journal of Information Science and Engineering", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Luong"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong,? \\Q2015\\E", "shortCiteRegEx": "Luong", "year": 2015}, {"title": "A scalable multi-level feature extraction technique to detect malicious executables. Information Systems Frontiers 10(1):33\u201345", "author": ["Khan Masud", "M.M. Thuraisingham 2008] Masud", "L. Khan", "B. Thuraisingham"], "venue": null, "citeRegEx": "Masud et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Masud et al\\.", "year": 2008}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Nallapati"], "venue": "arXiv preprint arXiv:1602.06023", "citeRegEx": "Nallapati,? \\Q2016\\E", "shortCiteRegEx": "Nallapati", "year": 2016}, {"title": "Malware images: visualization and automatic classification", "author": ["Nataraj"], "venue": "In Proceedings of the 8th international symposium on visualization for cyber security,", "citeRegEx": "Nataraj,? \\Q2011\\E", "shortCiteRegEx": "Nataraj", "year": 2011}, {"title": "Malware classification with recurrent networks", "author": ["Pascanu"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Pascanu,? \\Q2015\\E", "shortCiteRegEx": "Pascanu", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Chopra Rush", "A.M. Weston 2015] Rush", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Kernel machines for malware classification and similarity analysis", "author": ["Shankarapani"], "venue": "In The 2010 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Shankarapani,? \\Q2010\\E", "shortCiteRegEx": "Shankarapani", "year": 2010}, {"title": "Practical malware analysis: the hands-on guide to dissecting malicious software. no starch", "author": ["Sikorski", "M. Honig 2012] Sikorski", "A. Honig"], "venue": null, "citeRegEx": "Sikorski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sikorski et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, 3104\u20133112", "author": ["Vinyals Sutskever", "I. Le 2014] Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Differentiating malware from cleanware using behavioural analysis", "author": ["Tian"], "venue": "In Malicious and Unwanted Software (MALWARE),", "citeRegEx": "Tian,? \\Q2010\\E", "shortCiteRegEx": "Tian", "year": 2010}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["van der Maaten", "L. Hinton 2008] van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Malware obfuscation techniques: A brief survey", "author": ["You", "I. Yim 2010] You", "K. Yim"], "venue": "In BWCCA,", "citeRegEx": "You et al\\.,? \\Q2010\\E", "shortCiteRegEx": "You et al\\.", "year": 2010}, {"title": "Semantics-aware android malware classification using weighted contextual api dependency graphs", "author": ["Zhang"], "venue": "In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "We evaluate the accuracy of our model on a public malware API call sequence dataset (Kim 2016) at different granularity levels.", "startOffset": 84, "endOffset": 94}], "year": 2016, "abstractText": "Based on API call sequences, semantic-aware and machine learning (ML) based malware classifiers can be built for malware detection or classification. Previous works concentrate on crafting and extracting various features from malware binaries, disassembled binaries or API calls via static or dynamic analysis and resorting to ML to build classifiers. However, they tend to involve too much feature engineering and fail to provide interpretability. We solve these two problems with the recent advances in deep learning: 1) RNNbased autoencoders (RNN-AEs) can automatically learn lowdimensional representation of a malware from its raw API call sequence. 2) Multiple decoders can be trained under different supervisions to give more information, other than the class or family label of a malware. Inspired by the works of document classification and automatic sentence summarization, each API call sequence can be regarded as a sentence. In this paper, we make the first attempt to build a multi-task malware learning model based on API call sequences. The model consists of two decoders, one for malware classification and one for file access pattern (FAP) generation given the API call sequence of a malware. We base our model on the general seq2seq framework. Experiments show that our model can give competitive classification results as well as insightful FAP information.", "creator": "LaTeX with hyperref package"}}}