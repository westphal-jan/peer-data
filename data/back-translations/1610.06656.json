{"id": "1610.06656", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2016", "title": "Single Pass PCA of Matrix Products", "abstract": "In this paper, we present a new algorithm for calculating an Apache Spark approximation of the product $A ^ TB $by performing only a single pass of the two matrices $A $and $B $. The easy way to do this is to (a) first sketch $A $and $B $individually and then (b) find the top components on the sketch using PCA. Our algorithm, on the other hand, retains additional summary information about $A, B $(e.g. line and column norms, etc.) and uses this additional information to obtain improved approximation from the sketches. Our main analytical result is a comparable spectral norm guarantee for existing two-pass methods; we also provide results from an Apache Spark implementation that provides better computational and statistical performance on real and synthetic evaluation data sets.", "histories": [["v1", "Fri, 21 Oct 2016 02:45:46 GMT  (407kb)", "https://arxiv.org/abs/1610.06656v1", "24 pages, 4 figures, NIPS 2016"], ["v2", "Wed, 26 Oct 2016 13:58:24 GMT  (407kb)", "http://arxiv.org/abs/1610.06656v2", "24 pages, 4 figures, NIPS 2016"]], "COMMENTS": "24 pages, 4 figures, NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.IT cs.LG math.IT", "authors": ["shanshan wu", "srinadh bhojanapalli", "sujay sanghavi", "alexandros g dimakis"], "accepted": true, "id": "1610.06656"}, "pdf": {"name": "1610.06656.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shanshan Wu"], "emails": ["shanshan@utexas.edu", "srinadh@ttic.edu", "sanghavi@mail.utexas.edu", "dimakis@austin.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n06 65\n6v 2\n[ st\nat .M\nL ]"}, {"heading": "1 Introduction", "text": "Given two large matrices A and B we study the problem of finding a low rank approximation of their product ATB, using only one pass over the matrix elements. This problem has many applications in machine learning and statistics. For example, if A = B, then this general problem reduces to Principal Component Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix from large logs, e.g., A may be a user-by-query matrix and B may be a user-by-ad matrix, so ATB computes the joint counts for each query-ad pair. The matrices A and B can also be two large bag-of-word matrices. For this case, each entry of ATB is the number of times a pair of words co-occurred together. As a fourth example, ATB can be a cross-covariance matrix between two sets of variables, e.g., A and B may be genotype and phenotype data collected on the same set of observations. A low rank approximation of the product matrix is useful for Canonical Correlation Analysis (CCA) [8]. For all these examples, ATB captures pairwise variable interactions and a low rank approximation is a way to efficiently represent the significant pairwise interactions in sub-quadratic space.\nLet A and B be matrices of size d \u00d7 n (d \u226b n) assumed too large to fit in main memory. To obtain a rank-r approximation of ATB, a naive way is to compute ATB first, and then perform truncated singular value decomposition (SVD) of ATB. This algorithm needs O(n2d) time and O(n2) memory to compute the product, followed by an SVD of the n\u00d7n matrix. An alternative option is to directly run power method on ATB without explicitly computing the product. Such an algorithm will need to access the data matrices A and B multiple times and the disk IO overhead for loading the matrices to memory multiple times will be the major performance bottleneck.\nFor this reason, a number of recent papers introduce randomized algorithms that require only a few passes over the data, approximately linear memory, and also provide spectral norm guarantees. The key step in these algorithms is to compute a smaller representation of data. This can be achieved by two different methods: (1) dimensionality reduction, i.e., matrix sketching [29, 11, 26, 12]; (2) random sampling [14, 3]. The recent results of Cohen et al. [12] provide the strongest spectral norm guarantee of the former. They show that a sketch size of O(r\u0303/\u01eb2) suffices for the sketched matrices A\u0303T B\u0303 to achieve a spectral error of \u01eb, where r\u0303 is the maximum stable rank of A and B. Note that A\u0303T B\u0303 is not the desired rank-r approximation of ATB. On the other hand, [3] is a recent sampling method with very good performance guarantees. The authors consider entrywise sampling based on column norms, followed by a matrix completion step to compute low rank approximations. There is also a lot of interesting work on streaming PCA, but none can be directly applied to the general case when A is different from B (see Figure 4(c)). Please refer to Appendix D for more discussions on related work.\nDespite the significant volume of prior work, there is no method that computes a rank-r approximation of ATB when the entries of A and B are streaming in a single pass 1. Bhojanapalli et al. [3] consider a two-pass algorithm which computes column norms in the first pass and uses them to sample in a second pass over the matrix elements. In this paper, we combine ideas from the sketching and sampling literature to obtain the first algorithm that requires only a single pass over the data.\nContributions:\n\u2022 We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix Product PCA) that computes a rank-r approximation of ATB in time O((nnz(A) + nnz(B))\u03c1\n2r3r\u0303 \u03b72 + nr 6\u03c14r\u03033 \u03b74 ). Here\nnnz(\u00b7) is the number of non-zero entries, \u03c1 is the condition number, r\u0303 is the maximum stable rank, and \u03b7 measures the spectral norm error. Existing two-pass algorithms such as [3] typically have longer runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the simple idea that first sketches A and B separately and then performs SVD on the product of their sketches. We show that our algorithm always achieves better accuracy and can perform arbitrarily better if the column vectors of A and B come from a cone (see Figures 2, 4(b), 3(b)).\n\u2022 The central idea of our algorithm is a novel rescaled JL embedding that combines information from matrix sketches and vector norms. This allows us to get better estimates of dot products of high dimensional vectors compared to previous sketching approaches. We explain the benefit compared to a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general interest beyond low rank matrix approximations.\n\u2022 We prove that our algorithm recovers a low rank approximation of ATB up to an error that depends on \u2016ATB \u2212 (ATB)r\u2016 and \u2016ATB\u2016, decaying with increasing sketch size and number of samples (Theorem 3.1). The first term is a consequence of low rank approximation and vanishes if ATB is exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have similar dependencies as in [12].\n\u2022 We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic and real datasets. Our distributed implementation uses several design innovations described in Section 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that can handle matrices that are large in both dimensions. Our experiments show that we improve by approximately a factor of 2\u00d7 in running time compared to the previous state of the art and scale gracefully as the cluster size increases. The source code is available online [36].\n1One straightforward idea is to sketch each matrix individually and perform SVD on the product of the sketches. We compare against that scheme and show that we can perform arbitrarily better using our rescaled JL embedding.\n\u2022 In addition to better performance, our algorithm offers another advantage: It is possible to compute low-rank approximations to ATB even when the entries of the two matrices arrive in some arbitrary order (as would be the case in streaming logs). We can therefore discover significant correlations even when the original datasets cannot be stored, for example due to storage or privacy limitations."}, {"heading": "2 Problem setting and algorithms", "text": "Consider the following problem: given two matrices A \u2208 Rd\u00d7n1 and B \u2208 Rd\u00d7n2 that are stored in disk, find a rank-r approximation of their product ATB. In particular, we are interested in the setting where both A, B and ATB are too large to fit into memory. This is common for modern large scale machine learning applications. For this setting, we develop a single-pass algorithm SMP-PCA that computes the rank-r approximation without explicitly forming the entire matrix ATB.\nNotations. Throughout the paper, we use A(i, j) or Aij to denote (i, j) entry for any matrix A. Let Ai and Aj be the i-th column vector and j-th row vector. We use \u2016A\u2016F for Frobenius norm, and \u2016A\u2016 for spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar, which can be found by SVD. Given a set \u2126 \u2282 [n1]\u00d7 [n2] and a matrix A \u2208 Rn1\u00d7n2 , we define P\u2126(A) \u2208 Rn1\u00d7n2 as the projection of A on \u2126, i.e., P\u2126(A)(i, j) = A(i, j) if (i, j) \u2208 \u2126 and 0 otherwise."}, {"heading": "2.1 SMP-PCA", "text": "Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the desired rank r, number of samples m, sketch size k, and the number of iterations T . Performance guarantee involving these parameters is provided in Theorem 3.1. As illustrated in Figure 1, our algorithm has three main steps: 1) compute sketches and side information in one pass over A and B; 2) given partial information of A and B, estimate important entries of ATB; 3) compute low rank approximation given estimates of a few entries of ATB. Now we explain each step in detail.\ncompute its entry as M\u0303 (i, j) = \u2016Ai\u2016 \u00b7 \u2016Bj\u2016 \u00b7 A\u0303 T i B\u0303j\n\u2016A\u0303i\u2016\u00b7\u2016B\u0303j\u2016 . Performing matrix completion on P\u2126(M\u0303 ) gives the\ndesired rank-r approximation.\nStep 1: Compute sketches and side information in one pass over A and B. In this step we compute sketches A\u0303 := \u03a0A and B\u0303 := \u03a0B, where \u03a0 \u2208 Rk\u00d7d is a random matrix with entries being i.i.d. N (0, 1/k)\nAlgorithm 1 SMP-PCA: Streaming Matrix Product PCA\n1: Input: A \u2208 Rd\u00d7n1 , B \u2208 Rd\u00d7n2 , desired rank: r, sketch size: k, number of samples: m, number of iterations: T 2: Construct a random matrix \u03a0 \u2208 Rk\u00d7d, where \u03a0(i, j) \u223c N (0, 1/k), \u2200(i, j) \u2208 [k]\u00d7 [d]. Perform a single pass over A and B to obtain: A\u0303 = \u03a0A, B\u0303 = \u03a0B, and \u2016Ai\u2016, \u2016Bj\u2016, \u2200(i, j) \u2208 [n1]\u00d7 [n2]. 3: Sample each entry (i, j) \u2208 [n1] \u00d7 [n2] independently with probability q\u0302ij = min{1, qij}, where qij is defined in Eq.(1); maintain a set \u2126 \u2282 [n1]\u00d7 [n2] which stores all the sampled pairs (i, j). 4: Define M\u0303 \u2208 Rn1\u00d7n2 , where M\u0303(i, j) is given in Eq. (2). Calculate P\u2126(M\u0303 ) \u2208 Rn1\u00d7n2 , where P\u2126(M\u0303 ) = M\u0303(i, j) if (i, j) \u2208 \u2126 and zero otherwise.\n5: Run WAltMin(P\u2126(M\u0303 ), \u2126, r, q\u0302, T ), see Appendix A for more details. 6: Output: U\u0302 \u2208 Rn1\u00d7r and V\u0302 \u2208 Rn2\u00d7r.\nrandom variables. It is known that \u03a0 satisfies an \u201doblivious Johnson-Lindenstrauss (JL) guarantee\u201d [29][34] and it helps preserving the top row spaces of A and B [11]. Note that any sketching matrix \u03a0 that is an oblivious subspace embedding can be considered here, e.g., sparse JL transform and randomized Hadamard transform (see [12] for more discussion).\nBesides A\u0303 and B\u0303, we also compute the L2 norms for all column vectors, i.e., \u2016Ai\u2016 and \u2016Bj\u2016, for all (i, j) \u2208 [n1] \u00d7 [n2]. We use this additional information to design better estimates of ATB in the next step, and also to determine important entries of A\u0303T B\u0303 to sample. Note that this is the only step that needs one pass over data.\nStep 2: Estimate important entries of ATB by rescaled JL embedding. In this step we use partial information obtained from the previous step to compute a few important entries of A\u0303T B\u0303. We first determine what entries of A\u0303T B\u0303 to sample, and then propose a novel rescaled JL embedding for estimating those entries.\nWe sample entry (i, j) of ATB independently with probability q\u0302ij = min{1, qij}, where\nqij = m \u00b7 ( \u2016Ai\u20162\n2n2\u2016A\u20162F + \u2016Bj\u20162 2n1\u2016B\u20162F ). (1)\nLet \u2126 \u2282 [n1] \u00d7 [n2] be the set of sampled entries (i, j). Since E( \u2211\ni,j qij) = m, the expected number of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in O(n1 + m log(n2)) time; we show how to do this in Appendix C.5.\nNote that qij intuitively captures important entries of ATB by giving higher weight to heavy rows and columns. We show in Section 3 that this sampling actually generates good approximation to the matrix ATB.\nThe biased sampling distribution of Eq. (1) is first proposed by Bhojanapalli et al. [3]. However, their algorithm [3] needs a second pass to compute the sampled entries, while we propose a novel way of estimating dot products, using information obtained in the first step.\nDefine M\u0303 \u2208 Rn1\u00d7n2 as M\u0303(i, j) = \u2016Ai\u2016 \u00b7 \u2016Bj\u2016 \u00b7\nA\u0303Ti B\u0303j\n\u2016A\u0303i\u2016 \u00b7 \u2016B\u0303j\u2016 . (2)\nNote that we will not compute and store M\u0303 , instead, we only calculate M\u0303(i, j) for (i, j) \u2208 \u2126. This matrix is denoted as P\u2126(M\u0303), where P\u2126(M\u0303 )(i, j) = M\u0303(i, j) if (i, j) \u2208 \u2126 and 0 otherwise.\nWe now explain the intuition of Eq. (2), and why M\u0303 is a better estimator than A\u0303T B\u0303. To estimate the (i, j) entry of ATB, a straightforward way is to use A\u0303Ti B\u0303j = \u2016A\u0303i\u2016 \u00b7 \u2016B\u0303j\u2016 \u00b7 cos \u03b8\u0303ij , where \u03b8\u0303ij is the angle between vectors A\u0303i and B\u0303j . Since we already know the actual column norms, a potentially better estimator\nwould be \u2016Ai\u2016 \u00b7 \u2016Bj\u2016 \u00b7 cos \u03b8\u0303ij . This removes the uncertainty that comes from distorted column norms2. Figure 2(a) compares the two estimators A\u0303Ti B\u0303j (JL embedding) and M\u0303(i, j) (rescaled JL embedding) for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The vectors have dimension 1,000 and the sketching matrix has dimension 10-by-1,000. Clearly rescaling by the actual norms help reduce the estimation uncertainty. This phenomenon is more prominent when the true dot products are close to \u00b11, which makes sense because cos \u03b8 has a small slope when cos \u03b8 approaches \u00b11, and hence the uncertainty from angles may produce smaller distortion compared to that from norms. In the extreme case when cos \u03b8 = \u00b11, rescaled JL embedding can perfectly recover the true dot product.\nIn the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with angle \u03b8. Given a fixed unit-norm vector x, and a random Gaussian vector t with expected norm tan(\u03b8/2), we construct new vector y by randomly picking one from the two possible choices x+ t and \u2212(x+ t), and then renormalize it. Suppose the columns of A and B are unit vectors randomly drawn from a cone with angle \u03b8, we plot the ratio of spectral norm errors \u2016ATB \u2212 A\u0303T B\u0303\u2016/\u2016ATB \u2212 M\u0303\u2016 in Figure 2(b). We observe that M\u0303 always outperforms A\u0303T B\u0303 and can be much better when \u03b8 approaches zero, which agrees with the trend indicated in Figure 2(a).\nStep 3: Compute low rank approximation given estimates of few entries of ATB. Finally we compute the low rank approximation of ATB from the samples using alternating least squares:\nmin U,V \u2208Rn\u00d7r\n\u2211\n(i,j)\u2208\u2126\nwij(e T i UV T ej \u2212 M\u0303(i, j))2, (3)\n2We also tried using the cosine rule for computing the dot product, and another sketching method specifically designed for preserving angles [4], but empirically those methods perform worse than our current estimator.\nwhere wij = 1/q\u0302ij denotes the weights, and ei, ej are standard base vectors. This is a popular technique for low rank recovery and matrix completion (see [3] and the references therein). After T iterations, we will get a rank-r approximation of M\u0303 presented in the convenient factored form. This subroutine is quite standard, so we defer the details to Appendix A."}, {"heading": "3 Analysis", "text": "Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between the sketch size k, the sampling complexity m, the number of iterations T , and the spectral error \u2016(ATB)r \u2212 A\u0302TBr\u2016, where A\u0302TBr is the output of SMP-PCA, and (ATB)r is the optimal rank-r approximation of ATB. Note that the following theorem assumes that A and B have the same size. For the general case of n1 6= n2, Theorem 3.1 is still valid by setting n = max{n1, n2}.\nTheorem 3.1. Given matrices A \u2208 Rd\u00d7n and B \u2208 Rd\u00d7n, let (ATB)r be the optimal rank-r approximation of ATB. Define r\u0303 = max{\u2016A\u2016 2 F\n\u2016A\u20162 , \u2016B\u20162 F \u2016B\u20162 } as the maximum stable rank, and \u03c1 = \u03c3\u22171 \u03c3\u2217r as the condition number\nof (ATB)r, where \u03c3\u2217i is the i-th singular values of A TB.\nLet A\u0302TBr be the output of Algorithm SMP-PCA. If the input parameters k, m, and T satisfy\nk \u2265 C1\u2016A\u2016 2\u2016B\u20162\u03c12r3 \u2016ATB\u20162F \u00b7 max{r\u0303, 2 log(n)}+ log (3/\u03b3) \u03b72 , (4)\nm \u2265 C2r\u0303 2 \u03b3 \u00b7 (\u2016A\u20162F + \u2016B\u20162F \u2016ATB\u2016F )2 \u00b7 nr 3\u03c12 log(n)T 2 \u03b72 , (5)\nT \u2265 log(\u2016A\u2016F + \u2016B\u2016F \u03b6 ), (6)\nwhere C1 and C2 are some global constants independent of A and B. Then with probability at least 1\u2212 \u03b3, we have\n\u2016(ATB)r \u2212 A\u0302TBr\u2016 \u2264 \u03b7\u2016ATB \u2212 (ATB)r\u2016F + \u03b6 + \u03b7\u03c3\u2217r . (7)\nRemark 1. Compared to the two-pass algorithm proposed by [3], we notice that Eq. (7) contains an additional error term \u03b7\u03c3\u2217r . This extra term captures the cost incurred when we are approximating entries of ATB by Eq. (2) instead of using the actual values. The exact tradeoff between \u03b7 and k is given by Eq. (4). On one hand, we want to have a small k so that the sketched matrices can fit into memory. On the other hand, the parameter k controls how much information is lost during sketching, and a larger k gives a more accurate estimation of the inner products.\nRemark 2. The dependence on \u2016A\u2016 2 F +\u2016B\u20162 F\n\u2016ATB\u2016F captures one difficult situation for our algorithm. If\n\u2016ATB\u2016F is much smaller than \u2016A\u2016F or \u2016B\u2016F , which could happen, e.g., when many column vectors of A are orthogonal to those of B, then SMP-PCA requires many samples to work. This is reasonable. Imagine that ATB is close to an identity matrix, then it may be hard to tell it from an all-zero matrix without enough samples. Nevertheless, removing this dependence is an interesting direction for future research.\nRemark 3. For the special case of A = B, SMP-PCA computes a rank-r approximation of matrix ATA in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix (ATA)r \u2212 A\u0302TAr. Most results in the online PCA literature use Frobenius norm as performance measure. Recently, [22] provides an online PCA algorithm with spectral norm guarantee. They achieves a spectral norm bound of \u01eb\u03c3\u22171+\u03c3 \u2217 r+1, which is stronger than ours. However, their algorithm requires a target dimension\nof O(r log n/\u01eb2), i.e., the output is a matrix of size n-by-O(r log n/\u01eb2), while the output of SMP-PCA is simply n-by-r.\nRemark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2, we show that the sampled matrix provides a good approximation of the actual matrix ATB. In Appendix C.3, we show that there is a geometric decrease in the distance between the computed subspaces U\u0302 , V\u0302 and the optimal ones U\u2217, V \u2217 at each iteration of WAltMin algorithm. The spectral norm bound in Theorem 3.1 is then proved using results from the previous two steps.\nComputation Complexity. We now analyze the computation complexity of SMP-PCA. In Step 1, we compute the sketched matrices of A and B, which requires O(nnz(A)k + nnz(B)k) flops. Here nnz(\u00b7) denotes the number of non-zero entries. The main job of Step 2 is to sample a set \u2126 and calculate the corresponding inner products, which takes O(m log(n) +mk) flops. Here we define n as max{n1, n2} for simplicity. According to Eq. (4), we have log(n) = O(k), so Step 2 takes O(mk) flops. In Step 3, we run alternating least squares on the sampled matrix, which can be completed in O((mr2 + nr3)T ) flops. Since Eq. (5) indicates nr = O(m), the computation complexity of Step 3 is O(mr2T ). Therefore, SMP-PCA has a total computation complexity O(nnz(A)k + nnz(B)k +mk +mr2T )."}, {"heading": "4 Numerical Experiments", "text": "Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [37]. For the purpose of comparison, we also implement a two-pass algorithm LELA [3] in Spark3. The matrices A and B are stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel as DISK_ONLY). We implement the two passes of LELA using the treeAggregate method. For SMP-PCA, we choose the subsampled randomized Hadamard transform (SRHT) [32] as the sketching matrix 4. The biased sampling procedure is performed using binary search (see Appendix C.5 for how to sample m elements in O(m log n) time). After obtaining the sampled matrix, we run ALS (alternating least squares) to get the desired low-rank matrices. More details can be found in [36].\nDescription of datasets. We test our algorithm on synthetic datasets and three real datasets: SIFT10K [20], NIPS-BW [23], and URL-reputation [24]. For synthetic data, we generate matrices A and B as GD, where G has entries independently drawn from standard Gaussian distribution, and D is a diagonal matrix with Dii = 1/i. SIFT10K is a dataset of 10,000 images. Each image is represented by 128 features. We set A as the image-by-feature matrix. The task here is to compute a low rank approximation of ATA, which is a standard PCA task. The NIPS-BW dataset contains bag-of-words features extracted from 1,500 NIPS papers. We divide the papers into two subsets, and let A and B be the corresponding word-by-paper matrices, so ATB computes the counts of co-occurred words between two sets of papers. The original URL-reputation dataset has 2.4 million URLs. Each URL is represented by 3.2 million features, and is indicated as malicious or benign. This dataset has been used previously for CCA [25]. Here we extract two subsets of features, and let A and B be the corresponding URL-by-feature matrices. The goal is to compute a low rank approximation of ATB, the cross-covariance matrix between two subsets of features.\nSample complexity. In Figure 4(a) we present simulation results on a small synthetic data with n = d = 5, 000 and r = 5. We observe that a phase transition occurs when the sample complexity m = \u0398(nr log n). This agrees with the experimental results shown in previous papers, see, e.g., [9, 3]. For the rest experiments presented in this section, unless otherwise specified, we set r = 5, T = 10, and sampling complexity m as 4nr log n.\n3To our best knowledge, this the first distributed implementation of LELA. 4Compared to Gaussian sketch, SRHT reduces the runtime from O(ndk) to O(nd log d) and space cost from O(dk) to O(d),\nwhile maintains the same quality of the output.\nComparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA and LELA [3] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1, LELA always achieves a smaller spectral norm error than SMP-PCA, which makes sense because LELA takes two passes and hence has more chances exploring the data. Besides, we observe that as the sketch size increases, the error of SMP-PCA keeps decreasing and gets closer to that of LELA.\nIn Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on m3.2xlarge Amazon EC2 instances5 . The matrices A and B have dimension n = d = 100, 000. The sketch dimension is set as k = 2, 000. We observe that the speedup achieved by SMP-PCA is more prominent for small clusters (e.g., 56 mins versus 34 mins on a cluster of size two). This is possibly due to the increasing spark overheads at larger clusters, see [17] for more related discussion.\nComparison of SMP-PCA and SVD(A\u0303T B\u0303). In Figure 4(b) we repeat the experiment in Section 2 by generating column vectors of A and B from a cone with angle \u03b8. Here SVD(A\u0303T B\u0303) refers to computing\n5Each machine has 8 cores, 30GB memory, and 2\u00d780GB SSD.\nSVD on the sketched matrices6. We plot the ratio of the spectral norm error of SVD(A\u0303T B\u0303) over that of SMP-PCA, as a function of \u03b8. Note that this is different from Figure 2(b), as now we take the effect of random sampling and SVD into account. However, the trend in both figures are the same: SMP-PCA always outperforms SVD(A\u0303T B\u0303) and can be arbitrarily better as \u03b8 goes to zero.\nIn Figure 3(b) we compare SMP-PCA and SVD(A\u0303T B\u0303) on two real datasets SIFK10K and NIPS-BW. The y-axis represents spectral norm error, defined as ||ATB \u2212 A\u0302TBr||/||ATB||, where A\u0302TBr is the rank-r approximation found by a specific algorithm. We observe that SMP-PCA outperforms SVD(A\u0303T B\u0303) by a factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.\nNow we explain why SMP-PCA produces a more accurate result than SVD(A\u0303T B\u0303). The reasons are twofold. First, our rescaled JL embedding M\u0303 is a better estimator for ATB than A\u0303T B\u0303 (Figure 2). Second, the noise due to sampling is relatively small compared to the benefit obtained from M\u0303 , and hence the final result computed using P\u2126(M\u0303 ) still outperforms SVD(A\u0303T B\u0303).\nComparison of SMP-PCA and ATr Br. Let Ar and Br be the optimal rank-r approximation of A and B, we show that even if one could use existing methods (e.g., algorithms for streaming PCA) to estimate Ar and Br, their product ATr Br can be a very poor low rank approximation of A\nTB. This is demonstrated in Figure 4(c), where we intentionally make the top r left singular vectors of A orthogonal to those of B."}, {"heading": "5 Conclusion", "text": "We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation of a matrix product, using ideas of matrix sketching and entrywise sampling. As a subroutine of our algorithm, we propose rescaled JL for estimating entries of ATB, which has smaller error compared to the standard estimator A\u0303T B\u0303. This we believe can be extended to other applications. Moreover, SMP-PCA allows the non-zero entries of A and B to be presented in any arbitrary order, and hence can be used for steaming applications. We design a distributed implementation for SMP-PCA. Our experimental results show that\n6This can be done by standard power iteration based method, without explicitly forming the product matrix A\u0303T B\u0303, whose size is too big to fit into memory according to our assumption.\nSMP-PCA can perform arbitrarily better than SVD(A\u0303T B\u0303), and is significantly faster compared to algorithms that require two or more passes over the data.\nAcknowledgements We thank the anonymous reviewers for their valuable comments. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO YIP W911NF-14-1-0258."}, {"heading": "A Weighted alternating minimization", "text": "Algorithm 2 provides a detailed explanation of WAltMin, which follows a standard procedure for matrix completion. We use R\u2126(A) = w. \u2217 P\u2126(A) to denote the Hadamard product between w and P\u2126(A): R\u2126(A)(i, j) = w(i, j) \u2217 P\u2126(A)(i, j) for (i, j) \u2208 \u2126 and 0 otherwise, where w \u2208 Rn1\u00d7n2 = 1/q\u0302ij is the weight matrix. Similarly we define the matrix R1/2\u2126 (A) as R 1/2 \u2126 (A)(i, j) = \u221a w(i, j) \u2217 P\u2126(A)(i, j) for (i, j) \u2208 \u2126 and 0 otherwise. The algorithm contains two parts: initialization (Step 2-6) and weighted alternating minimization (Step 7-10). In the first part, we compute SVD of the weighted sampled matrix R\u2126(M\u0303) and then set row i of U (0) to be zero if its norm is larger than a threshold (Step 6). More details of this trim step can be found in [3]. In the second part, the goal is to solve the following non-convex problem by alternating minimization:\nmin U,V\n\u2211\n(i,j)\u2208\u2126\nwij(e T i UV T ej \u2212 M\u0303(i, j))2, (8)\nwhere ei, ej are standard base vectors. After running T iterations, the algorithm outputs a rank-r approximation of M\u0303 presented in the convenient factored form.\nAlgorithm 2 WAltMin [3]\n1: Input: P\u2126(M\u0303) \u2208 Rn1\u00d7n2 , \u2126, r, q\u0302, and T 2: wij = 1/q\u0302ij when q\u0302ij > 0, 0 else, \u2200i, j 3: Divide \u2126 in 2T + 1 equal uniformly random subsets, i.e., \u2126 = {\u21260, . . . ,\u21262T } 4: R\u21260(M\u0303 ) = w. \u2217 P\u21260(M\u0303) 5: U (0)\u03a3(0)(V (0))T = SVD(R\u21260(M\u0303 ), r) 6: Trim U (0) and let U\u0302 (0) be the output 7: for t = 0 to T \u2212 1 do 8: V\u0302 (t+1) = argminV \u2016R1/2\u21262t+1(M\u0303 \u2212 U\u0302\n(t)V T )\u20162F 9: U\u0302 (t+1) = argminU \u2016R1/2\u21262t+2(M\u0303 \u2212 U(V\u0302\n(t+1))T )\u20162F 10: end for 11: Output: U\u0302 (T ) \u2208 Rn1\u00d7r and V\u0302 (T ) \u2208 Rn2\u00d7r."}, {"heading": "B Technical Lemmas", "text": "We will frequently use the following concentration inequality in the proof.\nLemma B.1. (Matrix Bernstein\u2019s Inequality [33]). Consider p independent random matrices X1, ....,Xp in Rn\u00d7n, where each matrix has bounded deviation from its mean:\n||Xi \u2212 E[Xi]|| \u2264 L, \u2200i."}, {"heading": "Let the norm of the covariance matrix be", "text": "\u03c32 = max {\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223E [ p\u2211\ni=1\n(Xi \u2212 E[Xi])(Xi \u2212 E[Xi])T ]\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223E [ p\u2211\ni=1\n(Xi \u2212 E[Xi])T (Xi \u2212 E[Xi]) ]\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 }"}, {"heading": "Then the following holds for all t \u2265 0:", "text": "P [\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 p\u2211\ni=1\n(Xi \u2212 E[Xi]) \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 ] \u2264 2n exp( \u2212t 2/2 \u03c32 + Lt/3 ).\nA formal definition of JL transform is given below [29][34].\nDefinition B.2. A random matrix \u03a0 \u2208 Rk\u00d7d forms a JL transform with parameters \u01eb, \u03b4, f or JLT(\u01eb, \u03b4, f ) for short, if with probability at least 1\u2212 \u03b4, for any f -element subset V \u2282 Rd, for all v, v\u2032 \u2208 V it holds that |\u3008\u03a0v,\u03a0v\u2032\u3009 \u2212 \u3008v, v\u2032\u3009| \u2264 \u01eb||v|| \u00b7 ||v\u2032||.\nThe following lemma [34] characterizes the tradeoff between the reduced dimension k and the error level \u01eb.\nLemma B.3. Let 0 < \u01eb, \u03b4 < 1, and \u03a0 \u2208 Rk\u00d7d be a random matrix where the entries \u03a0(i, j) are i.i.d. N (0, 1/k) random variables. If k = \u2126(log(f/\u03b4)\u01eb\u22122), then \u03a0 is a JLT(\u01eb, \u03b4, f ).\nWe now present two lemmas that connect A\u0303 \u2208 Rk\u00d7n and B\u0303 \u2208 Rd\u00d7n with A \u2208 Rd\u00d7n and B \u2208 Rd\u00d7n.\nLemma B.4. Let 0 < \u01eb, \u03b4 < 1, if k = \u2126( log(2n/\u03b4) \u01eb2 ), then with probability at least 1\u2212 \u03b4,\n(1\u2212 \u01eb)||A||2F \u2264 ||A\u0303||2F \u2264 (1 + \u01eb)||A||2F , (1\u2212 \u01eb)||B||2F \u2264 ||B\u0303||2F \u2264 (1 + \u01eb)||B||2F ,\n||A\u0303T B\u0303 \u2212ATB||F \u2264 \u01eb||A||F ||B||F . Proof. This is again a standard result of JL transformation, e.g., see Definition 2.3 and Theorem 2.1 of [34] and Lemma 6 of [29] .\nLemma B.5. Let 0 < \u01eb, \u03b4 < 1, if k = \u0398( r\u0303+log(1/\u03b4)\u01eb2 ), where r\u0303 = max{ ||A||2 F ||A||2 , ||B||2 F ||B||2 } is the maximum stable rank, then with probability at least 1\u2212 \u03b4,\n||A\u0303T B\u0303 \u2212ATB|| \u2264 \u01eb||A||||B||.\nProof. This follows from a recent paper [12].\nUsing the above two lemmas, we can prove the following two lemmas that relate M\u0303 with ATB, for M\u0303 defined in Algorithm 1. A more compact definition of M\u0303 is DAA\u0303T B\u0303DB , where DA and DB are diagonal matrices with (DA)ii = ||Ai||/||A\u0303i|| and (DB)jj = ||Bj ||/||B\u0303j ||.\nLemma B.6. Let 0 < \u01eb < 1/14, 0 < \u03b4 < 1, if k = \u2126( log(2n/\u03b4)\u01eb2 ), then with probability at least 1\u2212 \u03b4,\n|M\u0303ij \u2212ATi Bj | \u2264 \u01eb||Ai|| \u00b7 ||Bj ||, ||M\u0303 \u2212ATB||F \u2264 \u01eb||A||F ||B||F .\nProof. Let 0 < \u01eb < 1/2, 0 < \u03b4 < 1, according to the Definition B.2 and Lemma B.3, we have that if k = \u2126( log(2n/\u03b4)\n\u01eb2 ), then with probability at least 1\u2212 \u03b4, and for all i, j\n1\u2212 \u01eb \u2264 (DA)ii \u2264 1 + \u01eb, 1\u2212 \u01eb \u2264 (DB)jj \u2264 1 + \u01eb, |A\u0303Ti B\u0303j \u2212ATi Bj | \u2264 \u01eb||Ai||||Bj ||. (9)\nWe can now bound |M\u0303ij \u2212ATi Bj| as\n|M\u0303ij \u2212ATi Bj | \u03be1 = |A\u0303Ti B\u0303j(DA)ii(DB)jj \u2212ATi Bj| \u03be2 \u2264 max{|A\u0303Ti B\u0303j(1 + \u01eb)2 \u2212ATi Bj |, |A\u0303Ti B\u0303j(1\u2212 \u01eb)2 \u2212ATi Bj |} \u03be3 \u2264 max{(1 + \u01eb)2\u01eb||Ai||||Bj ||+ ((1 + \u01eb)2 \u2212 1)|ATi Bj |, (1\u2212 \u01eb)2\u01eb||Ai||||Bj ||+ (1\u2212 (1\u2212 \u01eb)2)|ATi Bj |} \u03be4 \u2264 7\u01eb||Ai||||Bj ||, (10)\nwhere \u03be1 follows from the definition of M\u0303ij , \u03be2 follows from the bound in Eq.(9), \u03be3 follows from triangle inequality and Eq.(9), and \u03be4 follows from |ATi Bj | \u2264 ||Ai||||Bj ||. Now rescaling \u01eb as \u01eb/7 gives the desired bound in Lemma B.6.\nHence, ||M\u0303 \u2212ATB||F = \u221a\u2211 ij |M\u0303ij \u2212ATi Bj |2 \u2264 \u221a\u2211 ij \u01eb 2||Ai||2||Bj ||2 = \u01eb||A||F ||B||F .\nLemma B.7. Let 0 < \u01eb < 1/14, 0 < \u03b4 < 1, if k = \u2126( r\u0303+log(n/\u03b4)\u01eb2 ), then with probability at least 1\u2212 \u03b4,\n||M\u0303 \u2212ATB|| \u2264 \u01eb||A||||B||.\nProof. We can bound the spectral norm of the difference matrix as follows:\n||M\u0303 \u2212ATB|| \u03be1= ||DAA\u0303T B\u0303DB \u2212DAATBDB +DAATBDB \u2212DAATB +DAATB \u2212ATB|| \u2264 ||DA||||A\u0303T B\u0303 \u2212ATB||||DB ||+ ||DA||||ATB||||DB \u2212 I||+ ||DA \u2212 I||||ATB|| \u03be3 \u2264 (1 + \u01eb)2\u01eb||A||||B|| + (1 + \u01eb)\u01eb||A||||B|| + \u01eb||A||||B|| \u2264 7\u01eb||A||||B||, (11)\nwhere \u03be1 follows from the definition of M\u0303ij , and \u03be2 follows from Lemma B.5 and bound in Eq.(9). Rescaling \u01eb as \u01eb/7 gives the desired bound in Lemma B.7.\nWe will frequently use the term with high probability. Here is a formal definition.\nDefinition B.8. We say that an event E occurs with high probability (w.h.p.) in n if the probability that its complement E\u0304 happens is polynomially small, i.e., Pr(E\u0304) = O( 1n\u03b1 ) for some constant \u03b1 > 0.\nThe following two lemmas define a \u201dnice\u201d \u03a0 and when this happens with high probability.\nDefinition B.9. The random Gaussian matrix \u03a0 is \u201dnice\u201d with parameter \u01eb if for all (i, j) such that qij \u2264 1 (i.e., qij = q\u0302ij), the sketched values M\u0303ij satisfies the following two inequalities:\n|M\u0303ij | q\u0302ij \u2264 (1 + \u01eb) n m (||A||2F + ||B||2F ),\n\u2211\n{j:q\u0302ij=qij}\nM\u03032ij q\u0302ij \u2264 (1 + \u01eb)2n m (||A||2F + ||B||2F )2.\nLemma B.10. If k = \u2126( log(n) \u01eb2 ), and 0 < \u01eb < 1/14, then the random Gaussian matrix \u03a0 \u2208 Rk\u00d7d is \u201dnice\u201d w.h.p. in n.\nProof. According to Lemma B.6, if k = \u2126( log(n)\u01eb2 ), then w.h.p. in n, for all (i, j) we have |M\u0303ij \u2212ATi Bj | \u2264 \u01eb||Ai|| \u00b7 ||Bj ||. In other words, the following holds with probability at least 1\u2212 \u03b4:\n|M\u0303ij | \u2264 |ATi Bj |+ \u01eb||Ai|| \u00b7 ||Bj || \u2264 (1 + \u01eb)||Ai|| \u00b7 ||Bj ||, \u2200(i, j)\nThe above inequality is sufficient for \u03a0 to be \u201dnice\u201d:\nM\u0303ij q\u0302ij \u2264 (1 + \u01eb) ||Ai|| \u00b7 ||Bj || q\u0302ij \u2264 (1 + \u01eb) (||Ai|| 2 + ||Bj ||2)/2\nm \u00b7 ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) \u2264 (1 + \u01eb) n m (||A||2F + ||B||2F )\n\u2211 {j:q\u0302ij=qij} M\u03032ij q\u0302ij \u2264 \u2211 {j:q\u0302ij=qij} (1 + \u01eb)2||Ai||2||Bj ||2 q\u0302ij\n\u2264 (1 + \u01eb) \u2211\n{j:q\u0302ij=qij}\n||Ai||4 + ||Bj ||4\nm \u00b7 ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n)\n\u2264 (1 + \u01eb)2n m (||A||2F + ||B||2F )2.\nTherefore, we conclude that if k = \u2126( log(n)\u01eb2 ), then \u03a0 is \u201dnice\u201d w.h.p. in n."}, {"heading": "C Proofs", "text": ""}, {"heading": "C.1 Proof overview", "text": "We now present the key steps in proving Theorem 3.1. The framework is similar to that of LELA [3]. Our proof proceeds in three steps. In the first step, we show that the sampled matrix provides a good approximation of the actual matrix ATB. The result is summarized in Lemma C.1. Here R\u2126(M\u0303 ) denotes the sampled matrix weighted by the inverse of sampling probability (see Line 4 of Algorithm 2). Detailed proof can be found in Appendix C.2. For consistency, we will use Ci (i = 1, 2, ...) to denote global constant that can vary from step to step.\nLemma C.1. (Initialization) Let m and k satisfy the following conditions for sufficiently large constants C1 and C2:\nm \u2265 C1 ( ||A||2F + ||B||2F\n||ATB||F\n)2 n\n\u03b42 log(n),\nk \u2265 C2 r\u0303 + log(n) \u03b42 \u00b7 ||A|| 2||B||2 ||ATB||2F ,\nthen the following holds w.h.p. in n:\n||R\u2126(M\u0303 )\u2212ATB|| \u2264 \u03b4||ATB||F .\nIn the second step, we show that at each iteration of WAltMin algorithm, there is a geometric decrease in the distance between the computed subspaces U\u0302 , V\u0302 and the optimal ones U\u2217, V \u2217. The result is shown in Lemma C.2. Appendix C.3 provides the detailed proof. Here for any two orthonormal matrices X and Y , we define their distance as the principal angle based distance, i.e., dist(X,Y ) = ||XT\u22a5Y ||, where X\u22a5 denotes the subspace orthogonal to X.\nLemma C.2. (WAltMin Descent) Let k, m, and T satisfy the conditions stated in Theorem 3.1. Also, consider the case when ||ATB \u2212 (ATB)r||F \u2264 1576\u03c1r1.5 ||(ATB)r||F . Let U\u0302 (t) and V\u0302 (t+1) be the t-th and (t+1)-th step iterates of the WAltMin procedure. Let U (t) and V (t+1) be the corresponding orthonormal matrices. Let ||(U (t))i|| \u2264 8\u221ar\u03c1||Ai||/||A||F and dist(U (t), U\u2217) \u2264 1/2. Denote ATB as M , then the following holds with probability at least 1\u2212 \u03b3/T :\ndist(V t+1, V \u2217) \u2264 1 2 dist(U t, U\u2217) + \u03b7||M \u2212Mr||F /\u03c3\u2217r + \u03b7,\n||(V (t+1))j || \u2264 8\u221ar\u03c1||Bj ||/||B||F .\nIn the third step, we prove the spectral norm bound in Theorem 3.1 using results from the above two lemmas. Comparing Lemma C.1 and C.2 with their counterparts of LELA (see Lemma C.2 and C.3 in [3]), we notice that Lemma C.1 has the same bound as that of LELA, but the bound in Lemma C.2 contains an extra term \u03b7. This term eventually leads to an additive error term \u03b7\u03c3\u2217r in Eq.(7). Detailed proof is in Appendix C.4."}, {"heading": "C.2 Proof of Lemma C.1", "text": "We first prove the following lemma, which shows that R\u2126(M\u0303) is close to M\u0303 . For simplicity of presentation, we define CAB := (||A||2 F +||B||2 F )2\n||ATB||2 F\n.\nLemma C.3. Suppose \u03a0 is fixed and is \u201dnice\u201d. Let m \u2265 C1 \u00b7 CAB n\u03b42 log(n) for sufficiently large global constant C1, then w.h.p. in n, the following is true:\n||R\u2126(M\u0303)\u2212 M\u0303 || \u2264 \u03b4||ATB||F .\nProof. This lemma can be proved in the same way as the proof of Lemma C.2 in [3]. The key idea is to use the matrix Bernstein inequality. Let Xij = (\u03b4ij \u2212 q\u0302ij)wijM\u0303ijeieTj , where \u03b4ij is a {0, 1} random variable indicating whether the value at (i, j) has been sampled. Since \u03a0 is fixed, {Xij}ni,j=1 are independent zero mean random matrices. Furthermore, \u2211 i,j{Xij}ni,j=1 = R\u2126(M\u0303 )\u2212 M\u0303 .\nSince \u03a0 is \u201dnice\u201d with parameter 0 < \u01eb < 1/14, we can bound the 1st and 2nd moment of Xij as follows:\n||Xij || = max{|(1\u2212 q\u0302ij)wijM\u0303ij |, |q\u0302ijwijM\u0303ij|} \u2264 |M\u0303ij | q\u0302ij \u03be1 \u2264 (1 + \u01eb) n m (||A||2F + ||B||2F );\n\u03c32 = max{ \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 E  \u2211\nij\nXijX T ij   \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 E  \u2211\nij\nXTijXij   \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 } \u03be2= max i \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nj\nq\u0302ij(1\u2212 q\u0302ij)w2ijM\u03032ij \u2223\u2223\u2223\u2223\u2223\u2223\n= max i |( 1 q\u0302ij \u2212 1)M\u03032ij | \u03be3 \u2264\n\u2211\n{j:q\u0302ij=qij}\nM\u03032ij q\u0302ij \u03be1 \u2264 (1 + \u01eb)2n m (||A||2F + ||B||2F )2,\nwhere \u03be1 follows from Lemma B.10, \u03be2 follows from a direct calculation, and \u03be3 follows from the fact that q\u0302ij \u2264 1. Now we can use matrix Bernstein inequality (see Lemma B.1) with t = \u03b4||ATB||F to show that if m \u2265 (1 + \u01eb)C1CAB n\u03b42 log(n), then the desired inequality holds w.h.p. in n, where C1 is some global constant independent of A and B. Note that since 0 < \u01eb < 1/14, (1 + \u01eb) < 2. Rescaling C1 gives the desired result.\nNow we are ready to prove Lemma C.1, which is a counterpart of Lemma C.2 in [3].\nProof. We first show that ||R\u2126(M\u0303 )\u2212 M\u0303 || \u2264 \u03b4||ATB||F holds w.h.p. in n over the randomness of \u03a0. Note that in Lemma C.3, we have shown that it is true for a fixed and \u201dnice\u201d \u03a0, now we want to show that it also holds w.h.p. in n even for a random chosen \u03a0.\nLet G be the event that we desire, i.e., G = {||R\u2126(A\u0303T B\u0303) \u2212 A\u0303T B\u0303|| \u2264 \u03b4||ATB||F }. Let G\u0304 be the complimentary event. By conditioning on \u03a0, we can bound the probability of G\u0304 as\nPr(G\u0304) = Pr(G\u0304|\u03a0 is \u201dnice\u201d)Pr(\u03a0 is \u201dnice\u201d) + Pr(G\u0304|\u03a0 is not \u201dnice\u201d)Pr(\u03a0 is not \u201dnice\u201d) \u2264 Pr(G\u0304|\u03a0 is \u201dnice\u201d) + Pr(\u03a0 is not \u201dnice\u201d).\nAccording to Lemma C.3 and Lemma B.10, if m \u2265 C1 \u00b7 CAB n\u03b42 log(n), and k \u2265 C2 log(n) \u01eb2 , then both events {G|\u03a0 is \u201dnice\u201d} and Pr(\u03a0 is \u201dnice\u201d) happen w.h.p. in n. Therefore, the the probability of G\u0304 is polynomially small in n, i.e., the desired event G happens w.h.p. in n.\nNext we show that ||M\u0303 \u2212 ATB|| \u2264 \u03b4||ATB||F holds w.h.p. in n. According to Lemma B.7, if k = \u0398( r\u0303+log(n)\u01eb2 ), then w.h.p. in n, we have ||M\u0303 \u2212ATB|| \u2264 \u01eb||A||||B||. Now let \u01eb := \u03b4 ||ATB||F ||A||||B|| , we have that if k = \u0398( r\u0303+log(n) \u03b42 \u00b7 ||A||2||B||2 ||ATB||2\nF ), then ||M\u0303 \u2212ATB|| \u2264 \u03b4||ATB||F holds w.h.p. in n. By triangle inequality, we have ||R\u2126(M\u0303 ) \u2212 ATB|| \u2264 ||R\u2126(M\u0303 ) \u2212 M\u0303 || + ||M\u0303 \u2212 ATB||. We have shown that w.h.p. in n, both terms are less than \u03b4||ATB||F . By rescaling \u03b4 as \u03b4/2, we have that the desired inequality ||R\u2126(A\u0303T B\u0303)\u2212ATB|| \u2264 \u03b4||ATB||F holds w.h.p. in n, when m and k are chosen according to the statement of Lemma C.1.\nBecause the bound of Lemma C.1 has the same form as that of Lemma C.2 in [3], the corollary of Lemma C.2 also holds for R\u2126(M\u0303), which is stated here without proof: if ||ATB \u2212 (ATB)r||F \u2264\n1 576\u03bar1.5 ||(ATB)r||F , then w.h.p. in n we have\n||(U\u0302 (0))i|| \u2264 8\u221ar||Ai||/||A||F and dist(U\u0302 (0), U\u2217) \u2264 1/2,\nwhere U\u0302 (0) is the initial iterate produced by the WAltMin algorithm (see Step 6 of Algorithm 2). This corollary will be used in the proof of Lemma C.2.\nSimilar to the original proof in [3], we can now consider two cases separately: (1) ||ATB\u2212(ATB)r||F \u2265 1\n576\u03c1r1.5 ||(ATB)r||F ; (2) ||ATB \u2212 (ATB)r||F \u2264 1576\u03c1r1.5 ||(ATB)r||F . The first case is simple: use Lemma C.1 and Wely\u2019s inequality [31] already implies the desired bound in Theorem 3.1. To see why, note that Lemma C.1 and Wely\u2019s inequality imply that\n||(ATB)r \u2212 (R\u2126(M\u0303 )r|| \u03be1 \u2264 ||ATB \u2212 (ATB)r||+ ||ATB \u2212R\u2126(M\u0303)||+ ||R\u2126(M\u0303)\u2212 (R\u2126(M\u0303))r|| \u03be2 \u2264 ||ATB \u2212 (ATB)r||+ \u03b4||ATB||F + ||R\u2126(M\u0303)\u2212ATB||+ ||ATB \u2212 (ATB)r|| \u03be3 \u2264 2||ATB \u2212 (ATB)r||+ 2\u03b4||ATB||F , (12)\nwhere Mr denotes the best rank-r approximation of M , \u03be1 follows triangle inequality, \u03be2 follows from Lemma C.1 and Wely\u2019s inequality, and \u03be3 follows from Lemma C.1. If ||ATB\u2212(ATB)r||F \u2265 1576\u03c1r1.5 ||(ATB)r||F , then ||ATB||F = ||(ATB)r||F + ||ATB \u2212 (ATB)r||F \u2264 O(\u03c1r1.5)||ATB \u2212 (ATB)r||F . Setting \u03b4 = O(\u03b7/(\u03c1r1.5)) in Eq.(12) gives the desired error bound in Theorem 3.1. Therefore, in the following analysis we only need to consider the second case."}, {"heading": "C.3 Proof of Lemma C.2", "text": "We first prove the following lemma, which is a counterpart ofLemma C.5 in [3]. For simplicity of presentation, we use M to denote ATB in the following proof.\nLemma C.4. If m \u2265 C1nr log(n)T/(\u03b3\u03b42) and k \u2265 C2(r\u0303+log(n))/\u01eb2 for sufficiently large global constants C1 and C2, then the following holds with probability at least 1\u2212 \u03b3/T :\n||(U (t))TR\u2126(M\u0303 \u2212Mr)\u2212 (U (t))T (M \u2212Mr)|| \u2264 \u03b4||M \u2212Mr||F + \u03b4\u01eb||A||F ||B||F + \u01eb||A||||B||.\nProof. For a fixed \u03a0, we have that if m \u2265 C1nr log(n)T/(\u03b3\u03b42), then following holds with probability at least 1\u2212 \u03b3/T : ||(U (t))TR\u2126(M\u0303 \u2212Mr)\u2212 (U (t))T (M\u0303 \u2212Mr)|| \u2264 \u03b4||M\u0303 \u2212Mr||F . (13) The proof of Eq.(13) is exactly the same as the proof of Lemma C.5/B.6/B.2 in [3], so we omit its details here. The key idea is to define a set of zero-mean random matrices Xij such that \u2211 ij Xij = (U\n(t))TR\u2126(M\u0303\u2212 Mr) \u2212 (U (t))T (M\u0303 \u2212Mr), and then use second moment-based matrix Chebyshev inequality to obtain the desired bound.\nAccording to Lemma B.6 and Lemma B.7, if k = \u0398((r\u0303 + log(n))/\u01eb2), then w.h.p. in n, the following holds:\n||M\u0303 \u2212ATB||F \u2264 \u01eb||A||F ||B||F , ||M\u0303 \u2212ATB|| \u2264 \u01eb||A||||B||. (14) Using triangle inequality, we have that if m and k satisfy the conditions of Lemma C.4, then the follow-\ning holds with probability at least 1\u2212 \u03b3/T :\n||(U (t))TR\u2126(M\u0303 \u2212Mr)\u2212 (U (t))T (M \u2212Mr)|| \u2264 ||(U (t))TR\u2126(M\u0303 \u2212Mr)\u2212 (U (t))T (M\u0303 \u2212Mr)||+ ||(U (t))T (M \u2212 M\u0303)|| \u03be1 \u2264 \u03b4||M\u0303 \u2212Mr||F + ||M \u2212 M\u0303 || \u2264 \u03b4||M \u2212Mr||F + \u03b4||M \u2212 M\u0303 ||F + ||M \u2212 M\u0303 || \u03be2 \u2264 \u03b4||M \u2212Mr||F + \u03b4\u01eb||A||F ||B||F + \u01eb||A||||B||,\nwhere \u03be1 follows from Eq.(13), and \u03be2 follows from Eq.(14).\nNow we are ready to prove Lemma C.2. For simplicity, we focus on the rank-1 case here. Rankr proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]. Note that compared to Lemma C.5 in [3], Lemma C.4 contains two extra terms \u03b4\u01eb||A||F ||B||F +\u01eb||A||||B||. Therefore, we need to be careful for steps that involve Lemma C.4.\nIn the rank-1 case, we use u\u0302t and v\u0302t+1 to denote the t-th and (t+1)-th step iterates (which are column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.\nProof. This proof contains two parts. In the first part, we will prove that the distance dist(vt+1, v\u2217) decreases geometrically over time. In the second part, we show that the j-th entry of vt+1 satisfies |vt+1j | \u2264 c1||Bj ||/||B||F , for some constant c1.\nBounding dist(vt+1, v\u2217): In Lemma C.4, set \u01eb = ||A TB||\n2||A||||B||\u03b7 and \u03b4 = \u03b7 2r\u0303 , where 0 < \u03b7 < 1, then we have \u03b4\u01eb||A||F ||B||F \u2264\n||A||F ||B||F ||A||||B|| \u00b7 \u03b72 2r\u0303 ||ATB|| \u2264 \u03b7||ATB||/2, and \u01eb||A||||B|| \u2264 \u03b7||ATB||/2. Therefore, with probability at least 1\u2212 \u03b3/T , the following holds:\n||(ut)TR\u2126(M\u0303 \u2212M1)\u2212 (ut)T (M \u2212M1)|| \u2264 \u03b7||M \u2212M1||F /r\u0303 + \u03b7\u03c3\u22171 . (15)\nHence, we have ||(ut)TR\u2126(M\u0303 \u2212M1)|| \u2264 dist(ut, u\u2217)||M \u2212M1||+ \u03b7||M \u2212M1||F /r\u0303 + \u03b7\u03c3\u22171 . Using the explicit formula for WAltMin update (see Eq.(46) and Eq.(47) in [3]), we can bound \u3008v\u0302t+1, v\u2217\u3009 and \u3008v\u0302t+1, v\u2217\u22a5\u3009 as follows.\n||u\u0302t||\u3008v\u0302t+1, v\u2217\u3009/\u03c3\u22171 \u2265 \u3008ut, u\u2217\u3009 \u2212 \u03b41 1\u2212 \u03b41 \u221a 1\u2212 \u3008ut, u\u2217\u30092 \u2212 1 1\u2212 \u03b41 (\u03b7 ||M \u2212M1||F r\u0303\u03c3\u22171 + \u03b7).\n||u\u0302t||\u3008v\u0302t+1, v\u2217\u22a5\u3009/\u03c3\u22171 \u2264 \u03b41 1\u2212 \u03b41 \u221a 1\u2212 \u3008ut, u\u2217\u30092 + 1 1\u2212 \u03b41 (dist(ut, u\u2217) ||M \u2212M1|| \u03c3\u22171 + \u03b7 ||M \u2212M1||F r\u0303\u03c3\u22171 + \u03b7).\nAs discussed in the end of Appendix C.2, we only need to consider the case when ||ATB\u2212(ATB)r||F \u2264 1\n576\u03c1r1.5 ||(ATB)r||F , where \u03c1 = \u03c3\u22171/\u03c3\u2217r . In the rank-1 case, this condition reduces to ||M \u2212M1||F \u2264 \u03c3 \u2217 576 . For sufficiently small constants \u03b41 and \u03b7 (e.g., \u03b41 \u2264 120 , \u03b7 \u2264 120 ), and use the fact that \u3008ut, u\u2217\u3009 \u2265 \u3008u0, u\u2217\u3009 and dist(u0, u\u2217) \u2264 1/2, we can further bound \u3008v\u0302t+1, v\u2217\u3009 and \u3008v\u0302t+1, v\u2217\u22a5\u3009 as\n||u\u0302t||\u3008v\u0302t+1, v\u2217\u3009/\u03c3\u22171 \u2265 \u3008u0, u\u2217\u3009 \u2212 1\n10\n\u221a 1\u2212 \u3008u0, u\u2217\u30092 \u2212 1\n10 \u2265\n\u221a 3\n2 \u2212 2 10 \u2265 1 2 . (16)\n||u\u0302t||\u3008v\u0302t+1, v\u2217\u22a5\u3009/\u03c3\u22171 \u2264 \u03b41\n1\u2212 \u03b41 dist(ut, u\u2217) +\n1\n576(1 \u2212 \u03b41) dist(ut, u\u2217) +\n1\n1\u2212 \u03b41 (\u03b7 ||M \u2212M1||F r\u0303\u03c3\u22171 + \u03b7)\n\u03be1 \u2264 1\n4 dist(ut, u\u2217) + 2(\u03b7||M \u2212M1||F /\u03c3\u22171 + \u03b7), (17)\nwhere \u03be1 uses the fact that r\u0303 \u2265 1 and the assumption that \u03b41 is sufficiently small. Now we are ready to bound dist(vt+1, v\u2217) as\ndist(vt+1, v\u2217) = \u221a 1\u2212 \u3008vt+1, v\u2217\u30092 = \u3008v\u0302 t+1, v\u2217\u22a5\u3009\u221a \u3008v\u0302t+1, v\u2217\u22a5\u30092 + \u3008v\u0302t+1, v\u2217\u30092 \u2264 \u3008v\u0302 t+1, v\u2217\u22a5\u3009 \u3008v\u0302t+1, v\u2217\u3009\n\u03be1 \u2264 1\n2 dist(ut, u\u2217) + 4(\u03b7||M \u2212Mr||F /\u03c3\u22171 + \u03b7), (18)\nwhere \u03be1 follows from substituting Eqs. (16) and (17). Rescaling \u03b7 as \u03b7/4 gives the desired bound of Lemma C.2 for the rank-1 case. Rank-r proof can be obtained by following a similar framework.\nBounding vt+1j : In this step, we need to prove that the j-th entry of vt+1 satisfies |vt+1j | \u2264 c1 ||Bj || ||B||F for all j, under the assumption that ut satisfies the norm bound |uti| \u2264 c1 ||Ai||||A||F for all i. The proof follows very closely to the second part of proving Lemma C.3 in [3], except that an extra multiplicative term (1 + \u01eb) will show up when bounding \u2211\ni \u03b4ijwiju t iM\u0303ij using Bernstein inequality. More\nspecifically, let Xi = (\u03b4ij \u2212 q\u0302ij)wijutiM\u0303ij . Note that if q\u0302ij = 1, then \u03b4ij = 1, Xi = 0, so we only need to consider the case when q\u0302ij < 1, i.e., q\u0302ij = qij , where qij is defined in Eq.(1).\nSuppose \u03a0 is fixed and its dimension satisfies k = \u2126( log(n) \u01eb2\n), then according to Lemma B.6, we have that w.h.p. in n,\n|M\u0303ij | \u2264 |Mij |+ \u01eb||Ai|| \u00b7 ||Bj || \u2264 (1 + \u01eb)||Ai|| \u00b7 ||Bj||, \u2200(i, j). (19)\nHence, we have M\u03032ij q\u0302ij \u03be1 \u2264 (1 + \u01eb) 2||Ai||2||Bj ||2\nm \u00b7 ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) \u2264 2n(1 + \u01eb)\n2\nm \u00b7 ||Bj ||2||A||2F , (20)\n(uti) 2\nq\u0302ij\n\u03be2 \u2264 c 2 1||Ai||2/||A||2F\nm \u00b7 ( ||Ai||2 2n||A||2\nF\n+ ||Bj ||2\n2n||B||2 F\n) \u2264 2nc\n2 1\nm , (21)\nwhere \u03be1 follows from substituting Eqs.(19) and (1), and \u03be2 follows from the assumption that |uti| \u2264 c1||Ai||/||A||F .\nWe can now bound the first and second moments of Xi as\n|Xi| \u2264 |wijutiM\u0303ij | \u2264 \u221a (uti) 2\nq\u0302ij \u221a M\u03032ij q\u0302ij \u03be1 \u2264 2nc1(1 + \u01eb) m ||Bj ||||A||F .\n\u2211\ni\nV ar(Xi) = \u2211\ni\nq\u0302ij(1\u2212 q\u0302ij)w2ij(uti)2M\u03032ij \u2264 \u2211\ni\n(uti) 2\nq\u0302ij (1 + \u01eb)2||Ai||2||Bj ||2\n\u03be2 \u2264 2nc 2 1(1 + \u01eb) 2\nm ||Bj ||2||A||2F ,\nwhere \u03be1 and \u03be2 follows from substituting Eqs.(20) and (21). The rest proof involves applying Bernstein\u2019s inequality to derive a high-probability bound on \u2211 i Xi, which is almost the same as the second part of proving Lemma C.3 in [3], so we omit the details here. The only difference is that, because of the extra multiplicative term (1 + \u01eb) in the bound of the first and second moments, the lower bound on the sample complexity m should also be multiplied by an extra (1+ \u01eb)2 term. By restricting 0 < \u01eb < 1/2, this extra multiplicative term can be ignored as long as the original lower bound of m contains a large enough constant."}, {"heading": "C.4 Proof of Theorem 3.1", "text": "We now prove our main theorem for rank-1 case here. Rank-r proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]. Similar to the previous section, we use u\u0302t and v\u0302t+1 to denote the t-th and (t+1)-th step iterates (which are column vectors in this case) of the WAltMin algorithm. Let ut and vt+1 be the corresponding normalized vectors.\nThe closed form solution for WAltMin update at t+ 1 iteration is\n||u\u0302t||v\u0302t+1j = \u03c3\u22171v\u2217j \u2211 i \u03b4ijwiju t iu \u2217 i\u2211\ni \u03b4ijwij(u t i) 2 +\n\u2211 i \u03b4ijwiju\nt i(M\u0303 \u2212M1)ij\u2211\ni \u03b4ijwij(u t i) 2\n.\nWriting in matrix form, we get\n||u\u0302t||v\u0302t+1j = \u03c3\u22171\u3008u\u2217, ut\u3009v\u2217 \u2212 \u03c3\u22171B\u22121(\u3008u\u2217, ut\u3009B \u2212C)v\u2217 +B\u22121y, (22)\nwhere B and C are diagonal matrices with Bjj = \u2211 i \u03b4ijwij(u t i) 2 and Cjj = \u2211 i \u03b4ijwiju t iu \u2217 i , and y is the\nvector R\u2126(M\u0303 \u2212M1)Tut with entries yj = \u2211 i \u03b4ijwiju t i(M\u0303 \u2212M1)ij .\nEach term of Eq.(22) can be bounded as follows.\n||(\u3008u\u2217, ut\u3009B \u2212 C)v\u2217|| \u2264 dist(ut, u\u2217), ||B\u22121|| \u2264 2, (23)\n||y|| = ||R\u2126(M\u0303 \u2212M1)Tut|| \u03be1 \u2264 dist(ut, u\u2217)||M \u2212M1||+ \u03b7||M \u2212M1||F /r\u0303 + \u03b7\u03c3\u22171 , (24)\nwhere \u03be1 follows directly from Lemma C.4. The proof of Eq.(23) is exactly the same as the proof of Lemma B.3 and B.4 in [3].\nAccording to Lemma C.2, since the distance is decreasing geometrically, after O(log(1\u03b6 )) iterations we get\ndist(ut, u\u2217) \u2264 \u03b6 + 2\u03b7||M \u2212M1||F /\u03c3\u22171 + 2\u03b7. (25)\nNow we are ready to prove the spectral norm bound in Theorem 3.1:\n||M1 \u2212 u\u0302t(v\u0302t+1)T || \u2264 ||M1 \u2212 ut(ut)TM1||+ ||ut(ut)TM1 \u2212 u\u0302t(v\u0302t+1)T || \u2264 ||(I \u2212 ut(ut)T )M1||+ ||ut[(ut)TM1 \u2212 ||u\u0302t||(v\u0302t+1)T ]|| \u03be1 \u2264 \u03c3\u22171dist(ut, u\u2217) + ||\u03c31\u3008ut, u\u2217\u3009v\u2217 \u2212 ||u\u0302t||(v\u0302t+1)T || \u03be2 \u2264 \u03c3\u22171dist(ut, u\u2217) + ||\u03c3\u22171B\u22121(\u3008u\u2217, ut\u3009B \u2212 C)v\u2217||+ ||B\u22121y|| \u03be3 \u2264 \u03c3\u22171dist(ut, u\u2217) + 2\u03c3\u22171dist(ut, u\u2217) + 2dist(ut, u\u2217)||M \u2212M1||+ 2\u03b7||M \u2212M1||F /r\u0303 + 2\u03b7\u03c3\u22171 \u03be4 \u2264 5(\u03b6\u03c3\u22171 + 2\u03b7||M \u2212M1||F + 2\u03b7\u03c3\u22171) + 2\u03b7||M \u2212M1||F + 2\u03b7\u03c3\u22171 = 5\u03b6\u03c3\u22171 + 12\u03b7||M \u2212M1||F + 12\u03b7\u03c3\u22171 (26)\nwhere \u03be1 follows from the definition of dist(ut, u\u2217), the fact that ||ut|| = 1, and (ut)TM1 = \u03c31\u3008ut, u\u2217\u3009v\u2217, \u03be2 follows from substituting Eq.(22), \u03be3 follows from Eqs.(23) and (24), and \u03be4 follows from the Eq.(25), and fact that ||M \u2212M1|| \u2264 \u03c3\u22171 , r\u0303 \u2265 1. Rescaling \u03b6 to \u03b6/(5\u03c3\u22171) (this will influence the number of iterations) and also rescaling \u03b7 to \u03b7/12 gives us the desired spectral norm error bound in Eq.(7). This completes our proof of the rank-1 case. Rank-r proof follows a similar line of reasoning and can be obtained by combining the current proof with the rank-r analysis in the original proof of LELA [3]."}, {"heading": "C.5 Sampling", "text": "We describe a way to sample m elements in O(m log(n)) time using distribution qij defined in Eq. (1). Naively one can compute all the n2 entries of min{qij , 1} and toss a coin for each entry, which takes O(n2) time. Instead of this binomial sampling we can switch to row wise multinomial sampling. For this, first estimate the expected number of samples per row mi = m( ||Ai|| 2\n2||A||2 F\n+ 12n). Now sample m1 entries from row\n1 according to the multinomial distribution,\nq\u03031j = m m1 \u00b7 ( ||A1|| 2 2n||A||2F + ||Bj ||2 2n||B||2F ) =\n||A1||2\n2n||A||2 F\n+ ||Bj ||\n2\n2n||B||2 F\n||Ai||2 2||A||2 F + 12n\n.\nNote that \u2211\nj q\u03031j = 1. To sample from this distribution, we can generate a random number in the interval [0, 1], and then locate the corresponding column index by binary searching over the cumulative distribution function (CDF) of q\u03031j . This takes O(n) time for setting up the distribution and O(m1 log(n)) time to sample. For subsequent row i, we only need O(mi log(n)) time to sample mi entries. This is because for binary search to work, only O(mi log(n)) entries of the CDF vector needs to be computed and checked. Note that the specific form of q\u0303ij ensures that its CDF entries can be updated in an efficient way (since we only need to update the linear shift and scale). Hence, sampling m elements takes a total O(m log(n)) time. Furthermore, the error in this model is bounded up to a factor of 2 of the error achieved by the Binomial model [7] [21]. For more details please see our Spark implementation."}, {"heading": "D Related work", "text": ""}, {"heading": "Approximate matrix multiplication:", "text": "In the seminal work of [14], Drineas et al. give a randomized algorithm which samples few rows of A and B and computes the approximate product. The distribution depends on the row norms of the matrices and\nthe algorithm achieves an additive error proportional to ||A||F ||B||F . Later Sarlos [29] propose a sketching based algorithm, which computes sketched matrices and then outputs their product. The analysis for this algorithm is then improved by [10]. All of these results compare the error ||ATB \u2212 A\u0303T B\u0303||F in Frobenius norm.\nFor spectral norm bound of the form ||ATB \u2212 C||2 \u2264 \u01eb||A||2||B||2, the authors in [29, 11] show that the sketch size needs to satisfy O(r/\u01eb2), where r = rank(A) + rank(B). This dependence on rank is later improved to stable rank in [26], but at the cost of a weaker dependence on \u01eb. Recently, Cohen et al. [12] further improve the dependence on \u01eb and give a bound of O(r\u0303/\u01eb2), where r\u0303 is the maximum stable rank. Note that the sketching based algorithm does not output a low rank matrix. As shown in Figure 2, rescaling by the actual column norms provide a better estimator than just using the sketched matrices. Furthermore, we show that taking SVD on the sketched matrices gives higher error rate than our algorithm (see Figure 3(b)).\nLow rank approximation: [16] introduced the problem of computing low rank approximation of a given matrix using only few passes over the data. They gave an algorithm that samples few rows and columns of the matrix and computes its SVD for low rank approximation. They show that this algorithm achieves additive error guarantees in Frobenius norm. [15, 29, 19, 13] have later developed algorithms using various sketching techniques like Gaussian projection, random Hadamard transform and volume sampling that achieve relative error in Frobenius norm.[35, 28, 18, 6] improved the analysis of these algorithms and provided error guarantees in spectral norm. More recently [11] presented an algorithm based on subspace embedding that computes the sketches in the input sparsity time.\nAnother class of methods use entrywise sampling instead of sketching to compute low rank approximation. [1] considered an uniform entrywise sampling algorithm followed by SVD to compute low rank approximation. This gives an additive approximation error. More recently [3] considered biased entrywise sampling using leverage scores, followed by matrix completion to compute low rank approximation. While this algorithm achieves relative error approximation, it takes two passes over the data.\nThere is also lot of interesting work on computing PCA over streaming data under some statistical assumptions, e.g., [2, 27, 5, 30]. In contrast, our model does not put any assumptions on the input matrix. Besides, our goal here is to get a low rank matrix and not just the subspace."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper we present a new algorithm for computing a low rank approximation of the product<lb>AB by taking only a single pass of the two matrices A and B. The straightforward way to do this is to<lb>(a) first sketch A and B individually, and then (b) find the top components using PCA on the sketch. Our<lb>algorithm in contrast retains additional summary information about A,B (e.g. row and column norms<lb>etc.) and uses this additional information to obtain an improved approximation from the sketches. Our<lb>main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in<lb>addition we also provide results from an Apache Spark implementation that shows better computational<lb>and statistical performance on real-world and synthetic evaluation datasets.", "creator": "LaTeX with hyperref package"}}}