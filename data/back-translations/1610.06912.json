{"id": "1610.06912", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2016", "title": "KGEval: Estimating Accuracy of Automatically Constructed Knowledge Graphs", "abstract": "Automatically creating large KGs by extracting text records on a web scale has received considerable attention in recent years, which has led to the creation of several KGs, such as NELL, Google Knowledge Vault, etc. These KGs consist of thousands of predicate relationships (e.g. isPerson, isMayorOf) and millions of their instances (e.g. Bill de Blasio, isMayorOf, New York City). Estimating the accuracy of such automatically constructed KGs is a difficult problem because of their size and diversity. Although crowdsourcing is an obvious choice for such evaluation, standard crowdsourcing, in which each predicate in the KGs is independently evaluated, is very expensive and particularly problematic when the available budget is limited. We show that such approaches are suboptimal as they ignore dependencies between different predicates and their instances. To overcome this challenge, we propose relational tasks (taking into account the effects of crowd size and their effectiveness).", "histories": [["v1", "Fri, 21 Oct 2016 19:49:19 GMT  (1116kb,D)", "http://arxiv.org/abs/1610.06912v1", null], ["v2", "Thu, 1 Dec 2016 06:45:34 GMT  (2493kb,D)", "http://arxiv.org/abs/1610.06912v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prakhar ojha", "partha talukdar"], "accepted": false, "id": "1610.06912"}, "pdf": {"name": "1610.06912.pdf", "metadata": {"source": "CRF", "title": "Relational Crowdsourcing and its Application in Knowledge Graph Evaluation", "authors": ["Prakhar Ojha", "Partha P Talukdar"], "emails": ["prakhar.ojha@csa.iisc.ernet.in", "ppt@serc.iisc.in"], "sections": [{"heading": "1. INTRODUCTION", "text": "Over the last few years, significant advances have been made in automatically constructing knowledge graphs such as NELL [30], Yago [37], Knowledge-Vault [10] etc., by extracting information from web documents and storing them into coherent Knowledge Graphs (KGs). Such KGs contain hundreds of predicate-relations (e.g., city, stadiumLocatedInCity) and millions of their instances called beliefs (e.g., (Joe Luis Arena, stadiumLocatedInCity, Detroit)). Due to imperfections in automatic models and unreliability of source web documents, many of these beliefs in the KG may be noisy. Reliably estimating accuracy of such KGs is of great importance before they are used in downstream applications. Human judgments are necessary in this accuracy estimation process. Please note that the evaluation process can\u2019t be\ncompletely automated as any system which is highly effective at estimating belief accuracy may as well be utilized during KG construction rather than evaluation.\nCrowdsourcing marketplaces such as Amazon Mechanical Turk (AMT)1 have emerged as a convenient way to collect human judgments on a variety of tasks, ranging from document and image classification to scientific experimentation [15, 26, 6, 21]. AMT provides a platform where a requester may post a Human Intelligence Task (HIT) which is carried out by one or more workers geographically distributed all around the world. Depending on the complexity of the task and desired level of expertise, the requester provides monetary compensation against successful completion of HIT(s).\nIn standard crowdsourcing settings, HITs are available apriori, and the requester sets the per-HIT payment and number of workers subject to the available budget before publishing the HITs. More recently, algorithms have been developed to adjust these parameters adaptively [1, 14]. However, in many crowdsourcing applications of practical significance, far too many HITs are available than what can realistically be covered by limited available budget. Hence, in such cases, there is a need to select subset of HITs, either adaptively or offline, which can fit in the available budget. Estimating the accuracy of automatically constructed Knowledge Graphs (KG) is one such instance.\nIn order to get the true accuracy of the KG, we would like to determine correctness of each and every belief in the KG. A HIT in this context is determining the correctness of a single belief. However, even with nominal rates like $0.01 per belief evaluation, and 1 worker per belief, we will need $10,000 to evaluate a KG with 1 million beliefs. This is prohibitively expensive, as available budgets are usually orders of magnitude smaller and many of these KGs bigger than this. So, the crucial problem here is:\nHow can we select a subset of beliefs (HITs) to evaluate which will (1) give the best estimate of true (but unknown) accuracy of the KG; and (2) require limited budget."}, {"heading": "Motivating example", "text": "While randomly sampling a subset of beliefs (edges of the KG) is a natural approach for its evaluation, this is often suboptimal as it ignores the inherent structural information. Let us motivate this through a small KG shown in Figure 1.\nThere are six correct and two incorrect beliefs, resulting in overall true KG accuracy of 75% which we would like to estimate. Here, each belief corresponds to a triple like\n1AMT: http://www.mturk.com\nar X\niv :1\n61 0.\n06 91\n2v 1\n[ cs\n.A I]\n2 1\nO ct\n2 01\n6\n(RedWings, isA, SportsTeam) which is an edge with its participating nodes in the graph.\nFirst, we observe that the beliefs in this example are not independent \u2013 there are coupling constraints among them. Type consistency constraint between categories and the relations connecting them, is one such example. For instance, from KG ontology we may know that homeStadiumOf relation connects an entity from the Stadium category to another entity in Sports Team category. Now, if (Joe Louis Arena, homeStadiumOf, Red Wings) is sampled and evaluated to be correct, then from these constraints we can immediately infer that (Joe Louis Arena, isA, Stadium) and (Red Wings, isA, Sports Team) are correct. Similarly, by evaluating (Taj Mahal, isA, State), which turns out to be false, we can infer that (Detroit, cityInState, TajMahal) is incorrect. In addition to type coupling constraints, Horn clauses [30, 25], such as homeStadiumOf \u2227 homeCity \u2192 stadiumLocatedInCity, can also be used. By additionally evaluating (Red Wings, homeCity, Detroit) and applying this horn clause to the already evaluated facts mentioned above, we can immediately infer that (Joe Louis Arena, stadiumLocatedInCity, Detroit) is also correct. We will explore generalized forms of these constraints in Section 3.1.\nThus, by exploiting the coupling constraints among beliefs and by evaluating only three of them, we are able to exactly estimate true accuracy of 75%. In contrast, random sampling, a popular alternative, when allowed to evaluate only three beliefs, estimates an expected accuracy of 66.7%.\nIn this paper, we make the following contributions:\n\u2022 We introduce Relational Crowdsourcing (RelCrowd), a new paradigm aimed at crowdsourcing over multirelational data where the number of potential HITs far exceeds what can be accommodated within limited available budget. RelCrowd exploits dependencies among HITs to reduce the number of explicit evaluation queries to humans, resulting in more effective budget utilization.\n\u2022 We apply the RelCrowd for quality estimation of automatically constructed KGs such as NELL [30] and\nYago [37]. To the best of our knowledge, this is the first such system of its kind to work in multi-relational setting.\n\u2022 We demonstrate that the objective optimized by RelCrowd using our inference method is in fact submodular, and hence allowing for application of efficient approximation algorithms with guarantees.\n\u2022 We report extensive experimental results on multiple real-world datasets to demonstrate effectiveness of RelCrowd.\nWe next formulate our problem in Section 2 and in doing so also establish the notations used throughout this paper. We then discuss the applicability of our approach for KG evaluation in Section 3 and present experimental results in Section 5."}, {"heading": "2. RELATIONAL CROWDSOURCING", "text": ""}, {"heading": "2.1 Notations", "text": "Notations used in the paper are summarized in Table 1. In this section, we provide brief descriptions of each. We are given a set of n, categorization type, Human Intelligence Tasks (HITs) H = {h1, . . . , hn} and a total budget B. Each HIT hi \u2208 H is a binary {0, 1} classification task\nwhose crowdsourcing cost is c(hi) \u2208 R+. Mapping function l(hi) \u2208 {0, 1} returns the value of evaluation label of hi, more generally l({h1 . . . ht}) \u2192 {0, 1}t return binary labels for set. Label for hi could either be gold expert label lg(hi) or its estimate lu(hi) as returned by noisy crowd worker u.\nWe also have access to a set of coupling constraints C = {(Ci, \u03b8i)} where each coupling constraint Ci enforces consistency of judgment assignments over subset of HITs, and \u03b8i \u2208 R represents the strength of this enforcement. Further, we may write each coupling constraint as Ci : Hi \u2192 R where Hi \u2286 H is the subset of tasks over which Ci operates, also referred to as Dom(Ci). The procedure to derive these constraints involves determining relationships among HITs which we will further explain in Section 3.1.\nTo assimilate all HITs and coupling constraints at one place, we construct a graph G = (H \u222a C, E). Both, HITs as well as constraints are treated as nodes in this graph. The edges, connecting these two types of nodes, denote the participation of tasks in coupling constraints. Specific details of graph construction will be further explained in Section 3.2.\nGiven a set of already crowdsourced HITs Q \u2286 H, we define inferable set I ( G,Q,\u0398 ) \u2286 H to be the set of HITs whose categorization labels may be automatically inferred by an inference algorithm, while using the coupling constraints C. \u0398 denotes the parameters required by inference algorithm. Given a set of categorized HITs Q \u2286 H, we define \u03a6(Q) = 1|Q| \u2211 h\u2208Q lg(h)."}, {"heading": "2.2 Problem of Relational Crowdsourcing", "text": "Using the above notation, observe that \u03a6(H) is the average categorization score2 when all the HITs in H are categorized using experts. Unfortunately, computing \u03a6(H) is not feasible in most application settings as evaluation of all HITs in H may be prohibitively expensive, i.e., \u2211 h\u2208H c(h) B, primarily due to high cardinality of H. In Relational Crowdsourcing (RelCrowd), we are interested in posting a smaller subset of HITs Q \u2286 H to crowd such that the resulting inferable set I ( G,Q,\u0398 ) provides the best estimate of \u03a6(H) under all possible choices of Q, while staying within crowdsourcing budget B. This may be formulated as follows:\narg min Q\u2286H\n\u2206Q = \u2223\u2223\u2223\u03a6(H)\u2212 \u03a6(I(G,Q,\u0398))\u2223\u2223\u2223\ns.t. \u2211 h\u2208Q c(h) \u2264 B\nwhere \u2206Q measures the deviation in average categorization scores of H and I ( G,Q,\u0398 ) . Please note that while the categorization scores of HITs in Q are obtained through aggregated crowdsourcing, categorization scores of HITs in Q\u0304 = I ( G,Q,\u0398 ) \\Q are predicted by an inference algorithm. If the labels in Q\u0304 are perfectly correlated with labels of corresponding HITs in H (i.e., the inference algorithm expands binary scores {0, 1} of other HITs given seed set Q well), then the above problem may be equivalently formulated as below:\narg max Q\u2286H \u2223\u2223I(G,Q,\u0398)\u2223\u2223 , s.t. \u2211 h\u2208Q c(h) \u2264 B (1)\n2 In this paper, we restrict our \u03a6 function to average aggregation and HITs to binary categorization tasks. Extensions to more complex forms is part of ongoing work.\nIn other words, RelCrowd aims to sample and crowdsource a HIT set Q with the largest inferable set subject to budget constraint."}, {"heading": "3. EVALUATION OF KNOWLEDGE GRAPHS USING RelCrowd", "text": "In this section, we describe how KG evaluation may be posed as an instance of Relational Crowdsourcing, where each HIT tries to categorize a belief (an edge in KG) into correct or incorrect. Given a set of evaluated HITs Q, \u03a6 ( I(G,Q,\u0398) ) is accuracy of the corresponding beliefs in its inferable set, whereas \u03a6(H) is the overall gold accuracy of entire KG. We would like to estimate this unknown quantity by crowdsourcing a small subset Q identified by optimizing the RelCrowd objective shown in Equation (1).\nIn order to solve the RelCrowd objective in the context of KG evaluation, we proceed as follows:\n\u2022 We first create a new graph, the Evaluation Coupling Graph (ECG), which puts the KG and coupling constraints into one structure. Coupling constraints are described in Section 3.1, and construction of ECG is described in Section 3.2.\n\u2022 Given an ECG graph and a few evaluated HITs in it, in Section 3.3, we address the question of how this seed evaluation may be propagated to estimate evaluation of other currently non-evaluated HITs. This constitutes the Inference Mechanism.\n\u2022 Given a partially evaluated ECG, in Section 3.4, we address the question of which next HIT(s) should be evaluated using the crowd. This constitutes the Control Mechanism.\n\u2022 Then in Section 3.4 we put all the pieces together and describe the overall RelCrowd algorithm."}, {"heading": "3.1 Evaluation Coupling Constraints", "text": "Each coupling constraint Ci \u2208 C in KG evaluation setting is a First Order Logic rule (horn clause) which allows us to infer the evaluation label of a HIT based on evaluations of one or more other HITs. Examples of a few such coupling constraints are shown below.\nC1: (JoeLouisArena, homeStadiumOf, RedWings) \u2192 (JoeLouisArena, isA, stadium) C2: (JoeLouisArena, homeStadiumOf, RedWings) \u2192 (RedWings, isA, sportsTeam) C5: (JoeLouisArena, homeStadiumOf, RedWings) \u2227 (RedWings, homeCity, Detroit) \u2192 (JoeLouisArena,\nstadiumLocatedInCity, Detroit)\nFor each Ci, HIT(s) to the left of the arrow constitute its body, while the HIT to the right is its head. Horn clauses are frequently occurring patterns in the KG which capture common sense inference knowledge. For example, we know from predefined ontology that (domain, range) type signature of homeCity relation is (sportsTeam, city). Coupling constraint C2 enforces type consistency between two HITs sharing the entity RedWings. So the enforcement of this constraint helps us infer that (RedWings, isA, sportsTeam) should be correct if (JoeLouisArena, homeStadiumOf, RedWings) is evaluated to be correct.\nSimilarly, coupling constraint C5, which is automatically mined first order rule from KG, conveys that a stadium S which is home to certain team T based in city C, is itself located in city C. It allows us to infer the correctness of a HIT based on the truth labels of two other HITs. Given a KG, recent research has developed efficient techniques to automatically learn such coupling constraints [25, 12]. We want to emphasize the fact that these first-order-logic rules are automatically generated over KG and not manually curated. These are generally series of relations (paths in KG) which are good predictors of a given relation and we use such automatically learned coupling constraints for the experiments in this paper.\nOther types of constraints, like subsumption relation, mutual exclusion, etc., which have also been successfully employed during knowledge extraction in NELL [30] and during integration of such extracted knowledge [34], can also be used as coupling constraints.\nThe above rules depict how labels of evaluated HITs h \u2208 Q are coupled together to be consistent with one another. Relational Crowdsourcing exploits these rules to our benefit by crowdsourcing only a much smaller set Q and propagating their evaluation labels over I(G,Q,\u0398) with coupling weight \u03b8j ."}, {"heading": "3.2 Evaluation Coupling Graph (ECG) Construction", "text": "Given the set of HITs H and the coupling constraints C, we construct a graph with two types of nodes: (1) a node for each HIT h \u2208 H, and (2) a node for each constraint c \u2208 C. The constraint node is connected to all the HIT nodes that\nit operates over. We call this graph the Evaluation Coupling Graph (ECG), represented as G = (H\u222aC, E) with the set of undirected edges E = {(c, h) | h \u2208 H, \u2203c \u2208 C, h \u2208 Dom(c)}. Please note that the ECG is in fact a bi-partite factor graph [24] with HIT nodes corresponding to variable-nodes and coupling constraint nodes corresponding to factor-nodes.\nExample of an ECG constructed out of the KG in Figure 1 and coupling constraint set C with |C| = 8 is shown in Figure 2. In this figure, there is a separate node corresponding to each of the eight edges (beliefs or HITs) in Figure 1. Also, there are eight coupling nodes corresponding to each coupling constraint operating over subset of these HIT nodes.\nWe want to bring forward the multi-relational property of Relational Crowdsourcing into notice. Observe the each first-order constraint node Cj is assigned \u03b8j weight, which ranks it among other rules. Higher weight indicates greater chances of being true and such constraint nodes will be more influential over other lower-weight rules in case of conflicts. We pose the Relational Crowdsourcing problem as classification of HIT nodes in ECG and covering that essential subset Q which induces a larger coverage, using coupling constraints, over all tasks."}, {"heading": "3.3 Inference Mechanism", "text": "Given a set of crowd evaluated tasks, it is essential to design an inference mechanism which helps propagate their labels. Inference module, which mostly involves machine intelligence and automated computing over crowd labels, is an essential part of crowdsourcing system [7] to save on total expenditure.\nSuppose we are given labels for few HITs l(h), \u2200h \u2208 Q \u2282 H and we would like to additionally infer labels for other HITs using the available coupling constraints. As a first approach, we could obtain logical closure over first-order rules and available evidence using deterministic enumeration technique. But, as we want to highlight, our setting addresses much richer multi-relational structure, where two HIT nodes from ECG could interact in multiple ways with varying strengths, hence requiring transition to probabilistic truth values of fact. The probabilistic nature is also apparent in crowdsourcing techniques due to inherent noise in worker responses. These reasons motivate us to retain soft truth values of tasks while making all calculations.\nWe use Probabilistic Soft Logic (PSL)3 [8], as our inference engine, to implement propagation of evaluation labels. PSL is a declarative language to reason uncertainty in relational domains via first order logic statements. One of the motivations to use PSL as our evaluation propagation mechanism is that it relaxes boolean truth values over H to continuous soft values in interval [0,1], unlike discrete binary Markov Logic Networks [35].\nThe first order rules are relaxed to their soft truth values and propagation is done by finding the most likely explanation over evidences. To get a sense of soft truthness over binary, the conjunction logical operator of first-order coupling constraints are converted to R using Lukaseiwicz t-norms; for instance, (a \u2227 b \u2192 max{0,a+b-1}). Using these norm relaxations, we further define potential function \u03c8j , corresponding to each coupling constraint node Cj in ECG. For\n3 http://www.psl.umiacs.umd.edu\nexample C5 mentioned earlier will translate to, \u03c8j(C5) = ( max{0, hx + hy \u2212 1\u2212 hw} )p\n(2)\nwhere C5 = hx \u2227 hy \u2192 hw and hx denotes the evaluation score associated with the label of HIT (Joe Louis Arena, homeStadiumOf, Red Wings), hy corresponds to (Red Wings, homeCity, Detroit) and hw for (Joe Louis Arena, locatedInCity, Detroit). We can choose p \u2208 {1, 2} for penalizing constraint violations either linearly or squared (less for small and high for larger violations). In a soft sense, \u03c8j denotes the degree of satisfaction of constraint Cj . Higher value of \u03c8j represent lower fit for Cj .\nThe probability distribution over all possible assignments of evaluation to I(G,Q,\u0398) is structured such that the ones which satisfy more coupling constraints from C are more probable. Essentially, for any permutation of labels l ( I(G,Q,\u0398) ) , this assignment is made through most probable explanation of density function given by\nP ( l ( I(G,Q,\u0398) )) = 1\nZ exp [ \u2212 |C|\u2211 j=1 \u03b8j\u03c8j ( I(G,Q,\u0398) )] (3)\nwhere Z is normalizing constant and \u03c8j \u2019s correspond to above potential functions. Optimization technique searches for the most likely explanation of this joint probability distribution over all HITs hi \u2208 I(G,Q,\u0398) by maximum aposteriori inference. Formally, the final assignment of labels to HITs is obtained by solving the following optimization problem\nlMAP ( IMAP (G,Q,\u0398) ) = arg max I(G,Q,\u0398) P ( l ( I(G,Q,\u0398) )) In our experiment, to calculate the accuracy of knowledge graph, it is essential to find the total number of correct beliefs against incorrect ones. We threshold these soft label values to distinguish true from false beliefs i.e., (lMAP (h) \u2265 threshold)\u2192 (lg(h) = lMAP (h)).\nHence, given initial evaluation labels of HITs in Q, we can use above PSL-based inference mechanism and help propagate their values to a larger set of I(G,Q,\u0398)."}, {"heading": "3.4 Control Mechanism", "text": "Given a partially evaluated ECG, the control mechanism aims to select the next set of HITs which should be evaluated by the crowd. However, before going into the details of the control mechanism, we state a few properties involving RelCrowd\u2019s optimization in Equation (1)."}, {"heading": "Submodularity", "text": "Theorem 1 The function optimized by RelCrowd (Equation (1)) using the PSL-based inference mechanism as described in Section 3.3 is submodular [29].\nIntuitively, the amount of additional utility, in terms of label inference, obtained by adding a HIT to larger set is lesser than adding it to any smaller subset. This can be proved using the observation that all pairs of HITs satisfy regularity condition [16] [22] and by using the conjecture of [20] which is later proved in [31]. Refer Section 9 for detailed proof.\nAlgorithm 1 RelCrowd: Knowledge Graph Evaluation using RelCrowd\nRequire: H: HITs, C: coupling constraints, B: assigned budget, S:seed set, c(h): HIT cost function, \u03a6: HIT categorization score aggregator\n1: G = BuildECG(H, C) 2: Q0 = InitializeSeed(G,S) 3: \u0398 = LearnParameters(S) 4: Br = B 5: while (Br > 0) do 6: h\u2217 = arg maxh\u2208H\u2032 |I(G,Qt\u22121 \u222a {h},\u0398)| 7: CrowdEvaluate(h\u2217) 8: Qt = I(G,Qt\u22121 \u222a {h\u2217},\u0398) 9: Br = Br \u2212 c(h\u2217)\n10: Q = Q\u222aQt 11: if Q \u2261 H then 12: Exit 13: end if 14: end while 15: return \u03a6(Q) = 1|Q| \u2211 h\u2208Q h"}, {"heading": "NP-Hardness", "text": "Theorem 2 The problem of selecting optimal solution in RelCrowd\u2019s optimization (Equation (1)) is NP-Hard.\nThis can be proved by showing that NP-complete Set-cover Problem (SCP) can be reduced to selection of Q which covers I(G,Q,\u0398) (see Equation (1)). Detailed proof can be found in Section 9."}, {"heading": "Control Mechanism", "text": "From above, we observe that the function optimized by RelCrowd is NP-hard and submodular. Fortunately, from classic results of [33], we know that greedy hill-climbing algorithms solve such maximization problem with approximation factor of (1 \u2212 1/e) \u2248 63% of optimal solution. Hence, we adopt a greedy strategy as our control mechanism. This greedy strategy uses the PSL-based inference mechanism to iteratively select the next HIT which is likely to give the greatest increase in cumulative inferable set size. More details are presented in next section."}, {"heading": "3.5 Bringing it all together: RelCrowd-KGE", "text": "Following all the above observations, we present RelCrowdKGE in Algorithm 1, a greedy algorithm that uses Relational Crowdsourcing in the context of knowledge graph evaluation. In Algorithm 1, to start with, we are given a set of all beliefs H from KG, we have access to coupling constraints C which bind the HITs of H, evidence seed set S \u2282 H for which we have gold labels available apriori, cost function, allocated budget and a score aggregator \u03a6. In Lines 1-3, we build the Evaluation Coupling Graph G = (H\u222aC, E) and use the labels of seed set S to initialize G as well as learn certain data specific parameters of inference engine. In lines 4-14, we repetitively run our inference mechanism, until budget is exhausted or all the HITs are covered. In each iteration, the HIT with the largest inferable set is\nidentified and evaluated using crowdsourcing. The new inferable set Qt is estimated. These automatically annotated nodes are added to Q as if they were posted to the workers. Finally, average of all the evaluated HIT scores is returned as the estimate accuracy.\nFormally, for h to be added to set I(Qt, C) at time t, there should exist a common coupling constraint Cj in ECG such that Dom(Cj) \u2282 I(Qt\u22121, C) and there exists an edge e(Cj , h) \u2208 E ."}, {"heading": "4. ANALYSIS: BUDGET ALLOCATION FOR NOISY WORKERS", "text": "In the discussion so far, we have not addressed the issue that the labels provided by crowd workers may be noisy. Hence, one has to redundantly post the same task to multiple workers and estimate its accuracy by aggregating all responses. In this section, we provide upper bounds on the number of workers which should evaluate a given HIT.\nWe make a rational assumption here, which is also widely adopted in the literature, that workers\u2019 response reflect their true opinions and noise might be attributed to innocent mistakes or lack of specific expertise. Here, we do not account for strategic workers who might collude and deliberately spam. This assumption, together with the widely accepted notion of \u2018wisdom of crowds\u2019, leads us to a stronger conclusion that expectation over the responses rh(u) for a task h by worker set {u} is closer to gold truth Gh [40]. Formally,\u2223\u2223\u2223\u2223ED(h,u)(rh(u))\u2212Gh\u2223\u2223\u2223\u2223 \u2264 12 (4) with respect to distribution D of workers and tasks. We employ majority voting to aggregate and deduce the answer r\u0302h for task h.\nWhile allocating budget for redundant tasks, we want to be more certain about those HITs h which have larger inferable set |I(G,Q \u222a {h},\u0398)| and hence post them to more workers. The rationale is to be more confident about tasks which directly impact larger subset of remaining HITs. Earlier works have considered allocation of workers to varyingcost model [40], but are not directly applicable in this setting due to our preference bias for few tasks over others. We divide the entire budget B among T tasks {h1, . . . hT } and determine the number of workers {nh1 , . . . , nhT } for each task such that ht\u2019s with larger inference set I(G,Q\u222aht,\u0398) have higher nht . Suppose, the cardinality of set I(G,Q \u222a ht,\u0398) is given by it and the cost of querying each task is c, then\nwe allocate nht = \u230aB it (1\u2212\u03b3)\nc imax\n\u230b where imax is the size of\nlargest inferable set and \u03b3 is constant. The residual budget Br = B \u2212 c \u2211T j=1 nht , can be further distributed by assigning nht = nht + 1 \u2200t \u2208 {1 . . . T} to tasks iteratively until budget exhausts. The following theorem theoretically bounds the errors with such allocation scheme. Detailed proof is in Section 9.\nTheorem 3 For a task ht, the allocation scheme in Section 4 of redundantly posing it to nht workers does not exceed the total budget B and its expected estimation error is upper bounded by e\u2212O(it), while the rest of the parameters are fixed. The expected estimation error over all tasks is upper bounded by e\u2212O(B)."}, {"heading": "5. EXPERIMENTS", "text": ""}, {"heading": "HIT set #Correct #Incorrect Gold Acc.", "text": "In order to evaluate Relational Crowdsourcing (RelCrowd) in the context of knowledge graph evaluation, we evaluate the following questions in this section:\n\u2022 Compared to other competitive baselines, how effective is RelCrowd in estimating KG accuracy while utilizing limited budget? (see Section 5.3)\n\u2022 How robust is RelCrowd to noise in the belief dataset? (see Section 5.5)\n\u2022 Do more coupling constraints help improve RelCrowd performance? (see Section 5.6)"}, {"heading": "5.1 Setup", "text": ""}, {"heading": "Datasets", "text": "We experimented with the following datasets in this paper. Statistics of the dataset used, their true accuracies, and number of coupling constraints used are reported in Table 2.\n\u2022 NELL: From NELL4, we chose a relatively denser subgraph of sports related facts, mostly pertaining to athletes, coaches, teams, leagues, stadiums etc. We constructed coupling constraints set CN using a few topranked PRA inference rules [25] along with domain and range information from ontology\u2019s predicate signatures.\n\u2022 Yago2: We also selected a Yago25 sample dataset, which unlike the NELL subset above, is not domain specific. We used AMIE horn clauses [12] to construct multi-relational coupling constraints CY for Yago-ECG. To preserve consistency in both our datasets, we introduced predicate signatures in CY although they weren\u2019t originally present."}, {"heading": "Crowdsourcing of HITs", "text": "To compare algorithm predications against human evaluations, we posted all the HITs {HN \u222aHY } to MTurk. To obtain good quality responses and for the ease of workers, we used ontology to translate each triple-extraction into human readable format. For instance, (Joe Louis Arena, homeStadiumOf, Red Wings) was rendered in HITs as \u201cStadium Joe"}, {"heading": "4 http://rtw.ml.cmu.edu/rtw/resources", "text": ""}, {"heading": "5 https://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/research/", "text": "yago-naga/\nLouis Arena is home stadium of sports team Red Wings\u201d. To still capture strange cases of machine extractions, which might not make sense to an abstracted worker, we gave the option of classifying fact as \u2018Ambiguous\u2019 which later we disambiguated ourselves. Web search hyperlinks were provided to aid the workers in case they were not sure of the answer. Figure 3 shows a sample HIT posted on AMT.\nOur focus, in this work, is not to address conventional problem of truth estimation from noisy crowd workers. We resort to simple majority voting technique in our analysis of noisy workers for structurally rich Relational Crowdsourcing. For our experiments, we consider votes of master workers for {HN \u222a HY } as gold labels, which we would like our inference algorithm to be able to predict. However, we also acknowledge several sophisticated techniques that have been proposed, like Bayesian approach, weighted majority voting etc., which are expected to perform better.\nAs all the tasks are homogeneous in nature, i.e., each is of binary classification type, we consider constant cost function for the experiments in this paper. We published our HITs under \u2018classification project\u2019 category, employed high quality Master workers and paid $0.01 per HIT. Details are presented in Table 3."}, {"heading": "Inference Engine", "text": "As each HIT in our case can only be either True or False, we impose an additional functional constraint over the soft inference values to ensure that probability of label association sums to 1 i.e., P(l(h) = True) = 1\u2212 P(l(h) = False)\nWe take PRA and AMIE paths [25, 12] along with ontological domain-range information to build PSL rules. We derive rule weights \u0398 from the normalized scores provided by their respective systems. With evidence, PSL can also learn rule weights with maximum likelihood estimation. We use the initial evidence provided to RelCrowd for learning posteriors over normalized horn-clause scores.\nProblem of cold start: Algorithm 1 assumes that we are given few evaluated HITs S as initial seed set. This initial evidence helps PSL learn posterior for rule weights and tune specific parameters. In absence of seed set, we can generate S by randomly sampling few HITs from distribution. However, getting HITs h \u2208 {S} crowd evaluated also incurs cost and we must use budget judiciously to sample only enough tasks and let RelCrowd run thereafter. In all\nour experiments below, we have considered cold-start setting and randomly sampled |S| = 50 tasks to train data specific parameters.\nNote that random HITs in S are sampled from the true distribution of labels, say D. Running PSL directly over S changes this distribution D to D\u0302 due sparse inferences, leading to undesirable skewing. Hence, before calling the iterative RelCrowd routine, we do one-time normalization of the scores assigned by inference engine (PSL in our case) to establish concordance between the accuracy estimate after initial random sampling S and that of just after first iteration of PSL, i.e we try to make D \u2248 D\u0302. We applied class mass normalization as\np\u0302(c|h) = qc pc p(c|h)\u2211\n\u2200c\u2032 qc\u2032 pc\u2032 p(c\u2032|h)\nwhere p(c|h) is the probability of obtaining class c \u2208 {0, 1} for a given HIT h, pc is the current accuracy estimate by PSL inference and qc is estimate of initial random samples from S."}, {"heading": "Performance Evaluation Metrics", "text": "To quantify the performance of algorithms, we measure them against the following metrics. We define \u2206AccMicro collectively over entire KG without making any differentiation in HITs and \u2206AccMacro as the average of \u2206AccMicro over each of the R \u2018predicate-relations\u2019 of KG separately. Formally,\n\u2206AccMicro = \u03a6(H)\u2212 1 |H| \u2211 \u2200h\u2208H l(h)\n\u2206AccMacro = 1\n|R| ( \u2211 \u2200r\u2208R [ \u03a6(Hr)\u2212 1 |Hr| \u2211 \u2200h\u2208Hr l(h) ])\nwhere \u03a6(H) is the accuracy measure we want our baselines to estimate (see Section 2.1) and l(h) is the label assignment by baseline. \u2206AccMicro treats entire knowledge graph as a bag of HITs whereas \u2206AccMacro segregates tasks based on type of edge in the KG and computes mean average precision over all such relation types."}, {"heading": "5.2 Baseline Methods", "text": "Sampling from structurally rich multi-relational graphs such as KGs in our setting is a relatively unexplored problem. We present below few competitive baselines which we compared against RelCrowd."}, {"heading": "Random baseline", "text": "We randomly picked any HIT h \u2208 H and crowdsourced for its correctness. Selection of every subsequent HIT was another independent random selection and it was repeated until budget exhausted. Results were averaged over a few such random sequences of trials."}, {"heading": "Max-Degree selection", "text": "We reverse-sorted the HITs based on their degrees in G and selected the HITs from top for evaluation in order. Given the degree-based ordering, this method favors selection of more centrally connected HITs first.\nNote that there is no notion of inferable set in these two baselines. Individual HITs are chosen and their evaluations are singularly added to compute the accuracy of KG."}, {"heading": "Independent Cascade propagation", "text": "This method is based on contagion transmission model of social networks wherein nodes infect their immediate neighbors [20]. Here, all the first order logic constraints C transform to mere neighborhood relation. As there is no notion of prioritization, all constraints are given equal importance.\nAt every time iteration, t we chose ht \u2208 H which has the highest number of neighboring HITs that are not included in inferable set I(G,Qt\u22121 \u222a ht,\u0398). Its crowdsourced evaluation is added to Qt and we let it conduct its categorization (evaluation) label to adjacent HITs in ECG. This process is repeated until all the budget is exhausted, or all the nodes in ECG are covered. We can think of this baseline as a simplification of RelCrowd where all the relations are ignored, and the inferable set inference is just neighborhood propagation."}, {"heading": "RelCrowd", "text": "This is the method proposed in the paper which is described in Algorithm 1. In addition to horn-clause and typing constraints, we used two additional negation rules for each class. These rules designate that no fact should be assumed to be true (or false) unless there is some evidence to suggest for it. In other words, these rules provide regularization guard against over prediction. Their weights are learned using maximum likelihood estimation over initial evidence S, as explained above."}, {"heading": "5.3 Is RelCrowd Effective at Estimating KG Accuracy with Limited Budget?", "text": ""}, {"heading": "Micro Accuracy", "text": "Experimental results comparing all the methods in estimating micro accuracy in two datasets \u2013 NELL and Yago2 \u2013 are presented in Table 4. The number of HITs evaluated (# Queries) column is a direct indicator of budget spent by respective methods. From this table, we observe that among all methods compared, RelCrowd is able to converge closest\n5Due to the significant positive bias in the Yago dataset, Random initially converges to almost zero micro accuracy difference. But since Random doesn\u2019t provide any stopping criteria, it soon deviates with higher accuracy difference, and ultimately requires much more number of HIT evaluation to converge again to zero error.\nto the true gold accuracy while evaluating least amount of HITs. Random selection and Max-degree are naive methods which fall short of identifying any structure in our problem and ignore constraints C completely, thus taking many more queries. In Figure 4, we plot the fraction of nodes automatically evaluated by different methods for varying amounts of seed supervision. We observe that RelCrowd is able to automatically evaluate the large number of additional HITs at each supervision level. Such fast dissemination of evaluation results in lower of queries as we observed in Table 4."}, {"heading": "Macro Accuracy", "text": "Experimental results comparing all the methods using \u2206AccMacro measure is presented in Table 4. As in the micro accuracy case described above, we observe that RelCrowd significantly outperforms all other methods. These experiments show that RelCrowd expands the labels of crowdsourced tasks effectively and not just by considering them as single union set of unordered tasks. This granular information is particularly helpful in Question-Answering type of applications. To contrast between RelCrowd and random baseline, which appear to perform closely in evaluation metrics, Figure 5 shows predicate-wise errors over NELL dataset. It is evident that RelCrowd estimates accuracy better with greater coverage."}, {"heading": "Stopping Criteria", "text": "A highly desirable feature of RelCrowd is that it has a stopping criteria \u2013 the algorithm stops further evaluation once all the HITs are covered, i.e., all beliefs of KG are part of the inferable set. Independent Cascade is the only other method which also has stopping criteria. However, as we have seen, it requires much more number of HIT evaluations compared to RelCrowd. In contrast, Random and Max-degree, due to their wavy-nature of convergence and high variance (in case of Random), does not give any concrete termination step until the budget is exhausted. Because of the coverage-based stopping criteria, RelCrowd can recognize to stop further HIT evaluation even when additional budget is available."}, {"heading": "Budget Saved", "text": "Analyzing the reduction in number of questions asked to the crowd, it is observed that RelCrowd out-performs all the other baselines by huge margins. Random baseline can take between 2.5x to 4.4x, Max-degree between 2.7x to 9.1x and Independent cascade between 1.6x to 3.1x more number of crowdsourced questions to converge to their respective accuracy estimates."}, {"heading": "5.4 How Effective is the Control Mechanism in isolation?", "text": "To verify the effectiveness of our Greedy control mechanism, we ran additional experiments combining our inference mechanism with different control baselines. We experimented with Random-RelCrowd and Max-degree-RelCrowd by replacing only the greedy step of Algorithm 1 with respective selection heuristic. We observed that random selection lead to 1.1x increase in budget while max-degree converged within 1.05x. Although the final converged values were within acceptable range (\u00b10.05%), we still want to emphasize that theoretical guarantees are made only over Greedy-RelCrowd and not other baselines. One might always construct adversarial ECG which perform poor with Max-degree or Random selection."}, {"heading": "5.5 How Robust is RelCrowd to Noise in KG Beliefs?", "text": "In order to examine adaptability and robustness of various algorithms in the presence of noise, we evaluated all the methods over noisy variants of the NELL dataset. We artifi-"}, {"heading": "Constraint Set Iterations to \u2206AccMicro (%) Convergence", "text": "cially added noise to NELL facts from HN by flipping edges of (entity, relation, value) triples which were evaluated to be true by Mechanical Turkers and also where the relation was functional. Note that the functional nature of relation ensures that the example generated is indeed negative. Experimental results comparing all methods with 10% noise is shown in Table 5. We observe that RelCrowd achieves fastest convergence in this noisy setting. We also observed similar patterns at other noise levels as well. By comparing Table 4 with Table 5, we find that while the performance of other methods (e.g., Independent Cascade) degrade considerably in the presence of noise, RelCrowd is significantly more robust in such noisy environment."}, {"heading": "5.6 Do Additional Coupling Constraints Help?", "text": "Our work in this paper is largely motivated by the thesis \u2013 richer relational couplings among HITs provide greater optimization opportunities which can be exploited compared to conventional stand alone HIT evaluations. To evaluate this thesis, we ran a few ablation experiments where we evaluated performance of RelCrowd over the same NELL HITs but with successively reduced coupling constraints. \u2206AccMicro and number of iterations required until convergence are shown in Table 6. In this table, Cb2 and Cb3 represent two sets of coupling constraints, which correspond to horn clauses of length 2 and 3, are successively ablated from C. From this table, we find that performance of RelCrowd with the non-ablated constraint set C, takes least number of iterations (or HIT evaluations) to convergence and also provides best accuracy estimate (lowest \u2206AccMicro). These results validate our thesis of exploiting increased coupling constraints among HITs for more effective crowdsourcing and better utilization of budget."}, {"heading": "5.7 Scalability and Run-time", "text": "Comparisons with MLN: Markov Logic Networks (MLN) [35] is a probabilistic logic which can serve as another candidate for our Inference Mechanism. In order to compare the runtime performance of RelCrowd with PSL and MLN as inference engines, respective; we experimented with a subset of the NELL dataset with 1860 HITs and 130 constraints. While the PSL engine took only 320 seconds, the MLN implementation6 took considerably more time and it didn\u2019t even complete the graph grounding step even after 7 hours. This justifies out choice of PSL as the inference engine for RelCrowd.\nParallelism: Computing I(G,Q,\u0398) for varying Q involves solving independent optimization function. The greedy step, which is also the most computationally intensive step, can easily be parallelized by distributing calculation of I(G,Q\u222a hi,\u0398) among i different computing nodes. The final aggregator node selects arg maxi |I(G,Q\u222a hi,\u0398)|.\nComputational Optimization: Grounding of all firstorder logic rules and maximizing their posterior probabilities, as in Equation (3), is computationally expensive. PSL inference engine uses Java implementation of hinge-loss Markov random fields (hl-mrf) to find the most probable explanation[4]. It also uses relational database for efficient retrieval during rule grounding [8].\nQuick-RelCrowd: Running the inference engine (PSL) over all remaining HITs to find the best greedy candidate may not be computationally feasible, especially in case of large G. One can resort to reduction in the search space over a smaller set H\u2032. The expectation from H\u2032 is that it should contain the best HIT, i.e., h\u2217 \u2208 H\u2032, and one can further distinguish h\u2217 by explicit iterations of PSL over H\u2032. We observed that HITs connected to higher number of unfulfilled constraints in ECG propose suitable candidates for H\u2032, where unfulfilled constraints are those Ck\u2019s which have at least one adjacent HIT hk /\u2208 I(G,Q,\u0398). We also experimentally validated this heuristic by varying size of reduced space to |H\u2032| \u2208 {1, 3, 5}, which caused only negligible change in performance of baselines, indicating that the heuristic is indeed effective and stable."}, {"heading": "6. RELATED WORK", "text": "Most of the previous work on evaluation of large scale KGs has resorted to random sampling, whereas crowdsourcing research has typically considered atomic allocation of tasks wherein the requester posted HITs independently. In estimating the accuracy of knowledge bases through crowdsourcing, we find the task of knowledge corroboration [19] to be closely aligned with our motivations. This work proposes probabilistic model to utilize a fixed set of basic first-order logic rules for label propagation. However, unlike RelCrowd, it does not look into the budget feasibility aspect and does not try to reduce upon the number of queries to crowdsource.\nMost of the other research efforts along this line have gone into modeling individual workers and minimizing their required redundancy. They are mainly focused on getting a better hold on user\u2019s behavior and use it to further get better estimates of gold truth [41]. Recent improvements use Bayesian techniques [17, 36] for predicting accuracy of classification type HITs, but they operate in much simpler atomic\n6pyLMN: http://ias.cs.tum.edu/people/jain/mlns\nsetting. None of them relate the outputs of HITs to one another and do not capture the relational complexities of our Relational Crowdsourcing.\nThere have been models named Find-Fix-Verify which break large complex tasks, such as editing erroneous text, into modular chunks of simpler HITs and deal with these three inter-dependent tasks [5, 18]. The kind of inter-dependency among the three micro-tasks is very specific in the sense that output of previous stage goes as input to the next stage and cost analysis, workers Allocation and performance bounds over this model are done [39]. Our model transcends this restrictive linear dependence and is more flexible/natural. Decision theoretic approaches on constrained workflows have been employed to obtain high quality output for minimum allocated resources [23, 7]. Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.\nOur model significantly differs from previous works in marketing theory [13], outbreak detection [27] and social network analysis [20] etc., as it operates over multi-relational modes of inference and not just singular way of connecting two entities.\nWork on budget sensitive algorithm [18, 3, 40] provides performance guarantees over several cost models, but do not account for any inter-relation among tasks.\nIn large scale crowdsourcing, recent works have highlighted case for active learning [32]. However, unlike our selection based on relational dependence at instance level tasks, active learning selection is based upon ranking generated over classifiers."}, {"heading": "7. CONCLUSION", "text": "In this paper we have introduced Relational Crowdsourcing (RelCrowd), a novel framework for crowdsourcing multirelational data. RelCrowd is aimed at settings where the number of potential HITs far exceed what can be accommodated within available budget. To the best of our knowledge, this is the first such framework of its kind. We demonstrated that the objective optimized by RelCrowd is in fact NP-Hard and submodular, and hence allowing for the application of greedy algorithms with approximation guarantees. Through extensive experiments on real datasets, we successfully demonstrated RelCrowd\u2019s application to the important problem of knowledge graph evaluation. As part of future work, we hope to extend RelCrowd to incorporate varying cost, and more sophisticated evaluation aggregation. Also, we hope to apply the model to other structurally rich environments."}, {"heading": "8. REFERENCES", "text": "[1] I. Abraham, O. Alonso, V. Kandylas, and A. Slivkins. Adaptive\ncrowdsourcing algorithms for the bandit survey problem. In COLT, pages 882\u2013910, 2013.\n[2] A. Amarilli, Y. Amsterdamer, T. Milo, and P. Senellart. Top-k querying of unknown values under order constraints, 2015. [3] A. Azaria, Y. Aumann, and S. Kraus. Automated strategies for determining rewards for human work. In AAAI. Citeseer, 2012.\n[4] S. H. Bach, B. Huang, B. London, and L. Getoor. Hinge-loss Markov random fields: Convex inference for structured prediction. In UAI, 2013. [5] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, D. Crowell, and K. Panovich. Soylent: a word processor with a crowd inside. In ACM symposium on User interface software and technology, pages 313\u2013322, 2010. [6] D. C. Brabham. Crowdsourcing as a model for problem solving an introduction and cases. Convergence: the international journal of research into new media technologies, 14(1):75\u201390, 2008. [7] J. Bragg, D. S. Weld, et al. Crowdsourcing multi-label classification for taxonomy creation. In HCOMP, 2013. [8] M. Broecheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. In UAI, 2010. [9] J. Deng, O. Russakovsky, J. Krause, M. S. Bernstein, A. Berg, and L. Fei-Fei. Scalable multi-label annotation. In SIGCHI, pages 3099\u20133102, 2014.\n[10] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In SIGKDD, pages 601\u2013610, 2014. [11] L. Duan, S. Oyama, H. Sato, and M. Kurihara. Separate or joint? estimation of multiple labels from crowdsourced annotations. Expert Systems with Applications, 41(13):5723\u20135732, 2014. [12] L. A. Gala\u0301rraga, C. Teflioudi, K. Hose, and F. Suchanek. Amie: association rule mining under incomplete evidence in ontological knowledge bases. In WWW, pages 413\u2013422, 2013. [13] J. Goldenberg, B. Libai, and E. Muller. Using complex systems analysis to advance marketing theory development: Modeling heterogeneity effects on new product growth through stochastic cellular automata. Academy of Marketing Science Review, 9(3):1\u201318, 2001. [14] C.-J. Ho, S. Jabbari, and J. W. Vaughan. Adaptive task assignment for crowdsourced classification. In ICML, pages 534\u2013542, 2013. [15] P. G. Ipeirotis. Analyzing the amazon mechanical turk marketplace. XRDS: Crossroads, The ACM Magazine for Students, 17(2):16\u201321, 2010. [16] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In CVPR, pages 1897\u20131904, 2011. [17] E. Kamar, S. Hacker, and E. Horvitz. Combining human and machine intelligence in large-scale crowdsourcing. In AAMAS, pages 467\u2013474, 2012. [18] D. R. Karger, S. Oh, and D. Shah. Budget-optimal task allocation for reliable crowdsourcing systems. Operations Research, 62(1):1\u201324, 2014. [19] G. Kasneci, J. Van Gael, R. Herbrich, and T. Graepel. Bayesian knowledge corroboration with logical rules and user feedback. In Machine Learning and Knowledge Discovery in Databases, pages 1\u201318. 2010. [20] D. Kempe, J. Kleinberg, and E\u0301. Tardos. Maximizing the spread of influence through a social network. In SIGKDD, 2003. [21] A. Kittur, E. H. Chi, and B. Suh. Crowdsourcing user studies with mechanical turk. In SIGCHI, pages 453\u2013456, 2008. [22] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(2):147\u2013159, 2004. [23] A. Kolobov, D. S. Weld, et al. Joint crowdsourcing of multiple tasks. In HCOMP, 2013. [24] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor graphs and the sum-product algorithm. Information Theory, IEEE Transactions on, 47(2):498\u2013519, 2001. [25] N. Lao, T. Mitchell, and W. W. Cohen. Random walk inference and learning in a large scale knowledge base. In EMNLP, pages 529\u2013539, 2011. [26] E. Law and L. v. Ahn. Human computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(3):1\u2013121, 2011. [27] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance. Cost-effective outbreak detection in networks. In SIGKDD, pages 420\u2013429, 2007. [28] G. Little, L. B. Chilton, M. Goldman, and R. C. Miller. Turkit: human computation algorithms on mechanical turk. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 57\u201366. ACM, 2010.\n[29] L. Lova\u0301sz. Submodular functions and convexity. In Mathematical Programming The State of the Art, pages 235\u2013257. Springer, 1983. [30] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, et al. Never-ending learning. In AAAI, 2015. [31] E. Mossel and S. Roch. On the submodularity of influence in social networks. In ACM symposium on Theory of computing, pages 128\u2013134, 2007. [32] B. Mozafari, P. Sarkar, M. Franklin, M. Jordan, and S. Madden. Scaling up crowd-sourcing to very large datasets: a case for active learning. VLDB, 8(2):125\u2013136, 2014. [33] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265\u2013294, 1978. [34] J. Pujara, H. Miao, L. Getoor, and W. Cohen. Knowledge graph identification. In The Semantic Web\u2013ISWC 2013, pages 542\u2013557. Springer, 2013. [35] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62(1-2):107\u2013136, 2006. [36] E. Simpson, S. J. Roberts, A. Smith, and C. Lintott. Bayesian combination of multiple, imperfect classifiers. 2011. [37] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In WWW, 2007. [38] Y. Sun, A. Singla, D. Fox, and A. Krause. Building hierarchies of concepts via crowdsourcing. arXiv preprint arXiv:1504.07302, 2015. [39] L. Tran-Thanh, T. D. Huynh, A. Rosenfeld, S. D. Ramchurn, and N. R. Jennings. Budgetfix: budget limited crowdsourcing for interdependent task allocation with quality guarantees. In AAMAS, pages 477\u2013484, 2014. [40] L. Tran-Thanh, M. Venanzi, A. Rogers, and N. R. Jennings. Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks. In AAMAS, 2013. [41] P. Welinder, S. Branson, P. Perona, and S. J. Belongie. The multidimensional wisdom of crowds. In NIPS, pages 2424\u20132432, 2010. [42] H. Zhang, E. Law, R. Miller, K. Gajos, D. Parkes, and E. Horvitz. Human computation tasks with global constraints. In SIGCHI, pages 217\u2013226, 2012."}, {"heading": "9. APPENDIX", "text": "Submodularity: Any real valued functions f , which acts over subsets of a finite set H, is said to be submodular if \u2200R,S \u2282 H it fulfills\nf(R) + f(S) \u2265 f(R \u222a S) + f(R \u2229 S).\nA potential function \u03c8 is called pairwise regular if for all pairs of HITs {p, q} \u2208 H it satisfies\n\u03c8p,q(0, 1) + \u03c8p,q(1, 0) \u2265 \u03c8p,q(0, 0) + \u03c8p,q(1, 1) (5)\nwhere \u03c8p,q is potential function corresponding to the constraint binding HITs {p, q} and {1, 0} are their binary evaluation labels for true/false categories. Both submodular and regular properties are found to be in accord with each other and the equivalence of regular energy functions and submodular set functions has been studied earlier in realms of image segmentation [22, 16].\nProof For Theorem 1. By construction, the Evaluation Coupling Graph G = (H \u222a C, E) is conducting, in the sense that any two HITs which share a common factor node Cj are encouraged to have similar labels. Each \u03c8j of Equation (3) is a conducting potential function and also satisfies regularity property of Equation (5), making it submodular for a given Q. As \u03b8j \u2265 0, using standard non-negative summation properties of submodularity [29], we can show that\u2211 j\u2208C \u03b8j\u03c8j is also submodular. We consider a HIT h to be confidently inferred when the soft score of its label assignment in I(G,Q,\u0398) is greater than some threshold \u03c1h \u2208 [0, 1], i.e P (l(h)|Q) \u2265 \u03c1h. Choice\nof random \u03c1h mimics the uncertainty associated in quantifying exact dependence of HITs on each other. The label assignment made through most probable explanation of\nP ( l(h) ) = 1\nZ exp [ \u2212 |C\u2032|\u2211 j=1 \u03b8j\u03c8j ( h, h\u2032 )] \u2265 \u03c1h\nwhere C\u2032 is the reduced constraint set which is active over h and h\u2032 are HITs sharing C\u2032 with h.\nWe now know that P (l(h)|Q) is submodular with respect to fixed initial set Q. However we are interested in global selection of Q in the first place which maximizes the objective of Equation (1). Although max or min of submodular functions are not submodular in general, but [20] conjectured that global function of Equation (1) is submodular if local threshold function P (h|Q) \u2265 \u03c1h respected submodularity, which holds good in our case of Equation (3). This conjecture was further proved in [31] and thus making our global optimization function of Equation (1) submodular.\nProof For Theorem 2. This can be proved by showing that NP-complete Set-cover Problem (SCP) is a special case of RelCrowd-KGE and that it can be reduced to selection of Q which covers I(G,Q,\u0398). From the above discussions note that for any fixed (Q, C), PSL gives us a definitive way of creating inferable sets.\nFor the proof to remain consistent with earlier notations, we define SCP by collection of subsets I1, I2, . . . , Im from set H = {h1, h2, . . . , hn} and we want to determine if there exist k subsets whose union equals H. Without loss of generality assume that optimal solution has k 6= m and for all practical purposes k < n < m. We define a bipartite graph with m + n nodes corresponding to Ii\u2019s and hj \u2019s respectively and construct edge (Ii, hj) if hj \u2208 Ii. SCP can help us decide if there is a set Q, with cardinality k, such that |I(G,Q,\u0398)| \u2265 n+ k.\nChoosing our hit-set Q as per the solution proposed by SCP, posting them to the crowd and further infer the evaluations of other remaining HITs using PSL solves our knowledge graph evaluation (KGE) problem in hand.\nProof For Theorem 3. Allocating nht redundant workers for each task ht, \u2200t \u2208 {1 \u00b7 \u00b7 \u00b7T} with size of inferable set it, we incur cost of\nT\u2211 t=1 c nht = T\u2211 t=1 B it (1\u2212 \u03b3) c imax \u00b7 c\n= ( T\u2211 t=1 it ) \u00b7 ( B (1\u2212 \u03b3) imax )\nFor greedy control mechanism, i1 = imax. Suppose we approximate the reduction in size of inferable set by a diminishing factor of \u03b3 \u2208 (0, 1) i.e it+1 = \u03b3 it.\n=\n( imax (1\u2212 \u03b3T )\n(1\u2212 \u03b3) ) \u00b7 ( B (1\u2212 \u03b3) imax ) \u2264 B\nNote that the above geometric approximation, which practically holds true when decaying is averaged over few time steps, helps in getting an estimate of \u2211T t=1 it at iteration t \u2264 T . Such approximation would not be possible unless we already ran the entire experiment. For practical implementation, we can use an educated guess of \u03b3 = 1\u2212 imax\navgt(it) ."}, {"heading": "Error Bounds", "text": "We further show that, with our biased allocation mechanism, theoretical error estimates of any task decrease exponentially with increase in size of inferable set. Let the worker responses for ht binary classification task, denoted by rht \u2208 {0, 1}, be aggregated using majority voting over users uk by\nr\u0302ht =\n\u230a 1\nnht nht\u2211 k=1 rht(uk)\u2212 1 2\n\u230b + 1 (6)\nFor a given task ht, these nht responses are i.i.d samples from distribution of users and response D(ht, u). Let the gold truth for task ht is denoted by Ght and its error from aggregated response is given by \u2206(ht) = |r\u0302ht \u2212Ght |. From our earlier assumption in Equation (4), we approximate Ght by expectation of responses ED(h,u)(rh(u)). By general Hoeffding-Azuma bounds over nht i.i.d responses and error margin \u03b5t, we have\n\u2206(ht) = P {\u2223\u2223\u2223\u2223\u2223 1nqt nqt\u2211 k=1 rqt(uk)\u2212 E(rh(u)) \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b5t }\n\u2264 2e\u22122nqt\u03b5 2 t\n= 2 exp ( \u22122 B it (1\u2212 \u03b3)\nc imax \u03b52t ) For fixed budget B and given error margin \u03b5t, we have \u2206(ht) = e\n\u2212O(it). From our model (see Section 2.2), we know that the aggregated label of task ht is perfectly correlated with labels of its inferable set. So the probability of making error in ht is same as making error in HIT h \u2208 I(G,Q\u222a ht,\u0398).\n\u2206 ( I(G,Q\u222a ht,\u0398) ) \u2264 2 exp ( \u22122 B it (1\u2212 \u03b3)\nc imax \u03b52t ) \u2264 2 exp ( \u22122 B imin (1\u2212 \u03b3)\nc imax \u03b52min ) Summing up over all tasks t, by union bounds we get the total expected error from absolute ground truth \u2206(B) =\u2211T t=1 \u2206(qt).\n\u2206(B) \u2264 T\u2211 t=1 2 exp ( \u22122 B it (1\u2212 \u03b3) c imax \u03b52t ) \u2264 T \u00b7 2 exp ( \u22122 B imin (1\u2212 \u03b3)\nc imax \u03b52min ) The net expected estimation error exponentially decays with increase in budget, for fixed parameters."}], "references": [{"title": "Adaptive crowdsourcing algorithms for the bandit survey problem", "author": ["I. Abraham", "O. Alonso", "V. Kandylas", "A. Slivkins"], "venue": "COLT, pages 882\u2013910", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "and P", "author": ["A. Amarilli", "Y. Amsterdamer", "T. Milo"], "venue": "Senellart. Top-k querying of unknown values under order constraints", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated strategies for determining rewards for human work", "author": ["A. Azaria", "Y. Aumann", "S. Kraus"], "venue": "AAAI. Citeseer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Hinge-loss Markov random fields: Convex inference for structured prediction", "author": ["S.H. Bach", "B. Huang", "B. London", "L. Getoor"], "venue": "UAI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Soylent: a word processor with a crowd inside", "author": ["M.S. Bernstein", "G. Little", "R.C. Miller", "B. Hartmann", "M.S. Ackerman", "D.R. Karger", "D. Crowell", "K. Panovich"], "venue": "ACM symposium on User interface software and technology, pages 313\u2013322", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Crowdsourcing as a model for problem solving an introduction and cases", "author": ["D.C. Brabham"], "venue": "Convergence: the international journal of research into new media technologies, 14(1):75\u201390", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "et al", "author": ["J. Bragg", "D.S. Weld"], "venue": "Crowdsourcing multi-label classification for taxonomy creation. In HCOMP", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic similarity logic", "author": ["M. Broecheler", "L. Mihalkova", "L. Getoor"], "venue": "UAI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable multi-label annotation", "author": ["J. Deng", "O. Russakovsky", "J. Krause", "M.S. Bernstein", "A. Berg", "L. Fei-Fei"], "venue": "SIGCHI, pages 3099\u20133102", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "SIGKDD, pages 601\u2013610", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Separate or joint? estimation of multiple labels from crowdsourced annotations", "author": ["L. Duan", "S. Oyama", "H. Sato", "M. Kurihara"], "venue": "Expert Systems with Applications, 41(13):5723\u20135732", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["L.A. Gal\u00e1rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "WWW, pages 413\u2013422", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Using complex systems analysis to advance marketing theory development: Modeling heterogeneity effects on new product growth through stochastic cellular automata", "author": ["J. Goldenberg", "B. Libai", "E. Muller"], "venue": "Academy of Marketing Science Review, 9(3):1\u201318", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Adaptive task assignment for crowdsourced classification", "author": ["C.-J. Ho", "S. Jabbari", "J.W. Vaughan"], "venue": "ICML, pages 534\u2013542", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Analyzing the amazon mechanical turk marketplace", "author": ["P.G. Ipeirotis"], "venue": "XRDS: Crossroads, The ACM Magazine for Students, 17(2):16\u201321", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "CVPR, pages 1897\u20131904", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining human and machine intelligence in large-scale crowdsourcing", "author": ["E. Kamar", "S. Hacker", "E. Horvitz"], "venue": "AAMAS, pages 467\u2013474", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Budget-optimal task allocation for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah"], "venue": "Operations Research, 62(1):1\u201324", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian knowledge corroboration with logical rules and user feedback. In Machine Learning and Knowledge Discovery in Databases, pages", "author": ["G. Kasneci", "J. Van Gael", "R. Herbrich", "T. Graepel"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "\u00c9. Tardos"], "venue": "SIGKDD", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Crowdsourcing user studies with mechanical turk", "author": ["A. Kittur", "E.H. Chi", "B. Suh"], "venue": "SIGCHI, pages 453\u2013456", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "What energy functions can be minimized via graph cuts? Pattern Analysis and Machine Intelligence", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Transactions on, 26(2):147\u2013159", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "et al", "author": ["A. Kolobov", "D.S. Weld"], "venue": "Joint crowdsourcing of multiple tasks. In HCOMP", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "venue": "Information Theory, IEEE Transactions on, 47(2):498\u2013519", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "EMNLP, pages 529\u2013539", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Cost-effective outbreak detection in networks", "author": ["J. Leskovec", "A. Krause", "C. Guestrin", "C. Faloutsos", "J. VanBriesen", "N. Glance"], "venue": "SIGKDD, pages 420\u2013429", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Turkit: human computation algorithms on mechanical turk", "author": ["G. Little", "L.B. Chilton", "M. Goldman", "R.C. Miller"], "venue": "Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 57\u201366. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical Programming The State of the Art, pages 235\u2013257. Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1983}, {"title": "et al", "author": ["T. Mitchell", "W. Cohen", "E. Hruschka", "P. Talukdar", "J. Betteridge"], "venue": "Never-ending learning. In AAAI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "On the submodularity of influence in social networks", "author": ["E. Mossel", "S. Roch"], "venue": "ACM symposium on Theory of computing, pages 128\u2013134", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Scaling up crowd-sourcing to very large datasets: a case for active learning", "author": ["B. Mozafari", "P. Sarkar", "M. Franklin", "M. Jordan", "S. Madden"], "venue": "VLDB, 8(2):125\u2013136", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "An analysis of approximations for maximizing submodular set functionsi", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming, 14(1):265\u2013294", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1978}, {"title": "Knowledge graph identification", "author": ["J. Pujara", "H. Miao", "L. Getoor", "W. Cohen"], "venue": "The Semantic Web\u2013ISWC 2013, pages 542\u2013557. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 62(1-2):107\u2013136", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian combination of multiple, imperfect classifiers", "author": ["E. Simpson", "S.J. Roberts", "A. Smith", "C. Lintott"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "WWW", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Building hierarchies of concepts via crowdsourcing", "author": ["Y. Sun", "A. Singla", "D. Fox", "A. Krause"], "venue": "arXiv preprint arXiv:1504.07302", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Budgetfix: budget limited crowdsourcing for interdependent task allocation with quality guarantees", "author": ["L. Tran-Thanh", "T.D. Huynh", "A. Rosenfeld", "S.D. Ramchurn", "N.R. Jennings"], "venue": "AAMAS, pages 477\u2013484", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks", "author": ["L. Tran-Thanh", "M. Venanzi", "A. Rogers", "N.R. Jennings"], "venue": "AAMAS", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "P. Perona", "S.J. Belongie"], "venue": "NIPS, pages 2424\u20132432", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Human computation tasks with global constraints", "author": ["H. Zhang", "E. Law", "R. Miller", "K. Gajos", "D. Parkes", "E. Horvitz"], "venue": "SIGCHI, pages 217\u2013226", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 28, "context": "Over the last few years, significant advances have been made in automatically constructing knowledge graphs such as NELL [30], Yago [37], Knowledge-Vault [10] etc.", "startOffset": 121, "endOffset": 125}, {"referenceID": 35, "context": "Over the last few years, significant advances have been made in automatically constructing knowledge graphs such as NELL [30], Yago [37], Knowledge-Vault [10] etc.", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "Over the last few years, significant advances have been made in automatically constructing knowledge graphs such as NELL [30], Yago [37], Knowledge-Vault [10] etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "Crowdsourcing marketplaces such as Amazon Mechanical Turk (AMT) have emerged as a convenient way to collect human judgments on a variety of tasks, ranging from document and image classification to scientific experimentation [15, 26, 6, 21].", "startOffset": 224, "endOffset": 239}, {"referenceID": 5, "context": "Crowdsourcing marketplaces such as Amazon Mechanical Turk (AMT) have emerged as a convenient way to collect human judgments on a variety of tasks, ranging from document and image classification to scientific experimentation [15, 26, 6, 21].", "startOffset": 224, "endOffset": 239}, {"referenceID": 20, "context": "Crowdsourcing marketplaces such as Amazon Mechanical Turk (AMT) have emerged as a convenient way to collect human judgments on a variety of tasks, ranging from document and image classification to scientific experimentation [15, 26, 6, 21].", "startOffset": 224, "endOffset": 239}, {"referenceID": 0, "context": "More recently, algorithms have been developed to adjust these parameters adaptively [1, 14].", "startOffset": 84, "endOffset": 91}, {"referenceID": 13, "context": "More recently, algorithms have been developed to adjust these parameters adaptively [1, 14].", "startOffset": 84, "endOffset": 91}, {"referenceID": 28, "context": "In addition to type coupling constraints, Horn clauses [30, 25], such as homeStadiumOf \u2227 homeCity \u2192 stadiumLocatedInCity, can also be used.", "startOffset": 55, "endOffset": 63}, {"referenceID": 24, "context": "In addition to type coupling constraints, Horn clauses [30, 25], such as homeStadiumOf \u2227 homeCity \u2192 stadiumLocatedInCity, can also be used.", "startOffset": 55, "endOffset": 63}, {"referenceID": 28, "context": "\u2022 We apply the RelCrowd for quality estimation of automatically constructed KGs such as NELL [30] and Symbol Description H = {h1, .", "startOffset": 93, "endOffset": 97}, {"referenceID": 35, "context": "Yago [37].", "startOffset": 5, "endOffset": 9}, {"referenceID": 24, "context": "Given a KG, recent research has developed efficient techniques to automatically learn such coupling constraints [25, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 11, "context": "Given a KG, recent research has developed efficient techniques to automatically learn such coupling constraints [25, 12].", "startOffset": 112, "endOffset": 120}, {"referenceID": 28, "context": ", which have also been successfully employed during knowledge extraction in NELL [30] and during integration of such extracted knowledge [34], can also be used as coupling constraints.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": ", which have also been successfully employed during knowledge extraction in NELL [30] and during integration of such extracted knowledge [34], can also be used as coupling constraints.", "startOffset": 137, "endOffset": 141}, {"referenceID": 23, "context": "Please note that the ECG is in fact a bi-partite factor graph [24] with HIT nodes corresponding to variable-nodes and coupling constraint nodes corresponding to factor-nodes.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "Inference module, which mostly involves machine intelligence and automated computing over crowd labels, is an essential part of crowdsourcing system [7] to save on total expenditure.", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "We use Probabilistic Soft Logic (PSL) [8], as our inference engine, to implement propagation of evaluation labels.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "One of the motivations to use PSL as our evaluation propagation mechanism is that it relaxes boolean truth values over H to continuous soft values in interval [0,1], unlike discrete binary Markov Logic Networks [35].", "startOffset": 159, "endOffset": 164}, {"referenceID": 33, "context": "One of the motivations to use PSL as our evaluation propagation mechanism is that it relaxes boolean truth values over H to continuous soft values in interval [0,1], unlike discrete binary Markov Logic Networks [35].", "startOffset": 211, "endOffset": 215}, {"referenceID": 27, "context": "3 is submodular [29].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "This can be proved using the observation that all pairs of HITs satisfy regularity condition [16] [22] and by using the conjecture of [20] which is later proved in [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "This can be proved using the observation that all pairs of HITs satisfy regularity condition [16] [22] and by using the conjecture of [20] which is later proved in [31].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "This can be proved using the observation that all pairs of HITs satisfy regularity condition [16] [22] and by using the conjecture of [20] which is later proved in [31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "This can be proved using the observation that all pairs of HITs satisfy regularity condition [16] [22] and by using the conjecture of [20] which is later proved in [31].", "startOffset": 164, "endOffset": 168}, {"referenceID": 31, "context": "Fortunately, from classic results of [33], we know that greedy hill-climbing algorithms solve such maximization problem with approximation factor of (1 \u2212 1/e) \u2248 63% of optimal solution.", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "This assumption, together with the widely accepted notion of \u2018wisdom of crowds\u2019, leads us to a stronger conclusion that expectation over the responses rh(u) for a task h by worker set {u} is closer to gold truth Gh [40].", "startOffset": 215, "endOffset": 219}, {"referenceID": 38, "context": "Earlier works have considered allocation of workers to varyingcost model [40], but are not directly applicable in this setting due to our preference bias for few tasks over others.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "We constructed coupling constraints set CN using a few topranked PRA inference rules [25] along with domain and range information from ontology\u2019s predicate signatures.", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "We used AMIE horn clauses [12] to construct multi-relational coupling constraints CY for Yago-ECG.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": ", P(l(h) = True) = 1\u2212 P(l(h) = False) We take PRA and AMIE paths [25, 12] along with ontological domain-range information to build PSL rules.", "startOffset": 65, "endOffset": 73}, {"referenceID": 11, "context": ", P(l(h) = True) = 1\u2212 P(l(h) = False) We take PRA and AMIE paths [25, 12] along with ontological domain-range information to build PSL rules.", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "This method is based on contagion transmission model of social networks wherein nodes infect their immediate neighbors [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 33, "context": "Comparisons with MLN: Markov Logic Networks (MLN) [35] is a probabilistic logic which can serve as another candidate for our Inference Mechanism.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "PSL inference engine uses Java implementation of hinge-loss Markov random fields (hl-mrf) to find the most probable explanation[4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "It also uses relational database for efficient retrieval during rule grounding [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 18, "context": "In estimating the accuracy of knowledge bases through crowdsourcing, we find the task of knowledge corroboration [19] to be closely aligned with our motivations.", "startOffset": 113, "endOffset": 117}, {"referenceID": 39, "context": "They are mainly focused on getting a better hold on user\u2019s behavior and use it to further get better estimates of gold truth [41].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Recent improvements use Bayesian techniques [17, 36] for predicting accuracy of classification type HITs, but they operate in much simpler atomic", "startOffset": 44, "endOffset": 52}, {"referenceID": 34, "context": "Recent improvements use Bayesian techniques [17, 36] for predicting accuracy of classification type HITs, but they operate in much simpler atomic", "startOffset": 44, "endOffset": 52}, {"referenceID": 4, "context": "There have been models named Find-Fix-Verify which break large complex tasks, such as editing erroneous text, into modular chunks of simpler HITs and deal with these three inter-dependent tasks [5, 18].", "startOffset": 194, "endOffset": 201}, {"referenceID": 17, "context": "There have been models named Find-Fix-Verify which break large complex tasks, such as editing erroneous text, into modular chunks of simpler HITs and deal with these three inter-dependent tasks [5, 18].", "startOffset": 194, "endOffset": 201}, {"referenceID": 37, "context": "The kind of inter-dependency among the three micro-tasks is very specific in the sense that output of previous stage goes as input to the next stage and cost analysis, workers Allocation and performance bounds over this model are done [39].", "startOffset": 235, "endOffset": 239}, {"referenceID": 22, "context": "Decision theoretic approaches on constrained workflows have been employed to obtain high quality output for minimum allocated resources [23, 7].", "startOffset": 136, "endOffset": 143}, {"referenceID": 6, "context": "Decision theoretic approaches on constrained workflows have been employed to obtain high quality output for minimum allocated resources [23, 7].", "startOffset": 136, "endOffset": 143}, {"referenceID": 40, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 56, "endOffset": 64}, {"referenceID": 26, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 56, "endOffset": 64}, {"referenceID": 36, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 268, "endOffset": 272}, {"referenceID": 1, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 311, "endOffset": 314}, {"referenceID": 10, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 356, "endOffset": 363}, {"referenceID": 8, "context": "Crowdsourcing tasks, like collective itinerary planning [42, 28], involves handling tasks with global constraints, but our notion of inter-dependence is again very different as compared to above model More recent work on construction of hierarchy over domain concepts [38], top-k querying over noisy crowd data [2], multi-label image annotation from crowd [11, 9] involve crowdsourcing over dependent HITs but their goals and methods vary largely from ours.", "startOffset": 356, "endOffset": 363}, {"referenceID": 12, "context": "Our model significantly differs from previous works in marketing theory [13], outbreak detection [27] and social network analysis [20] etc.", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "Our model significantly differs from previous works in marketing theory [13], outbreak detection [27] and social network analysis [20] etc.", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Our model significantly differs from previous works in marketing theory [13], outbreak detection [27] and social network analysis [20] etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "Work on budget sensitive algorithm [18, 3, 40] provides performance guarantees over several cost models, but do not account for any inter-relation among tasks.", "startOffset": 35, "endOffset": 46}, {"referenceID": 2, "context": "Work on budget sensitive algorithm [18, 3, 40] provides performance guarantees over several cost models, but do not account for any inter-relation among tasks.", "startOffset": 35, "endOffset": 46}, {"referenceID": 38, "context": "Work on budget sensitive algorithm [18, 3, 40] provides performance guarantees over several cost models, but do not account for any inter-relation among tasks.", "startOffset": 35, "endOffset": 46}, {"referenceID": 30, "context": "In large scale crowdsourcing, recent works have highlighted case for active learning [32].", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention over the last few years, resulting in the construction of several KGs, such as NELL, Google Knowledge Vault, etc. These KGs consist of thousands of \u2018predicate-relations\u2019 (e.g., isPerson, isMayorOf ) and millions of their instances (e.g., (Bill de Blasio, isMayorOf, New York City)). Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. Even though crowdsourcing is an obvious choice for such evaluation, the standard single-task crowdsourcing, where each predicate in the KG is evaluated independently, is very expensive and especially problematic if the budget available is limited. We show that such approaches are sub-optimal as they ignore dependencies among various predicates and their instances. To overcome this challenge, we propose Relational Crowdsourcing (RelCrowd), where the tasks are created while taking dependencies among predicates and instances into account. We apply this framework in the context of evaluation of large-scale KGs and demonstrate its effectiveness through extensive experiments on real-world datasets.", "creator": "LaTeX with hyperref package"}}}