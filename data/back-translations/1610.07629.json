{"id": "1610.07629", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "A Learned Representation For Artistic Style", "abstract": "The diversity of painting styles represents a rich visual vocabulary for constructing an image. The degree to which this visual vocabulary can be learned and grasped sparingly measures our understanding of the overarching characteristics of images, if not images in general. In this work, we examine the construction of a single, scalable deep network that can grasp the artistic style of a variety of images sparingly. We show that such a network generalizes across a variety of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model allows the user to explore new painting styles by arbitrarily combining the styles learned from individual images. We hope that this work represents a useful step in building rich models of paintings and provides a window on the structure of the learned representation of the artistic style.", "histories": [["v1", "Mon, 24 Oct 2016 20:06:54 GMT  (47073kb,D)", "http://arxiv.org/abs/1610.07629v1", "9 pages. 15 pages of Appendix"], ["v2", "Mon, 5 Dec 2016 16:24:40 GMT  (47075kb,D)", "http://arxiv.org/abs/1610.07629v2", "9 pages. 15 pages of Appendix"], ["v3", "Fri, 9 Dec 2016 01:20:17 GMT  (49442kb,D)", "http://arxiv.org/abs/1610.07629v3", "9 pages. 15 pages of Appendix"], ["v4", "Tue, 27 Dec 2016 23:05:51 GMT  (49443kb,D)", "http://arxiv.org/abs/1610.07629v4", "9 pages. 15 pages of Appendix"], ["v5", "Thu, 9 Feb 2017 16:29:09 GMT  (49443kb,D)", "http://arxiv.org/abs/1610.07629v5", "9 pages. 15 pages of Appendix, International Conference on Learning Representations (ICLR) 2017"]], "COMMENTS": "9 pages. 15 pages of Appendix", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["vincent dumoulin", "jonathon shlens", "manjunath kudlur"], "accepted": true, "id": "1610.07629"}, "pdf": {"name": "1610.07629.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["ARTISTIC STYLE", "Vincent Dumoulin"], "emails": ["vi.dumoulin@gmail.com,", "shlens@google.com,", "keveman@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "A pastiche is an artistic work that imitates the style of another one. Computer vision and more recently machine learning have a history of trying to automate pastiche, that is, render an image in the style of another one. This task is called style transfer, and is closely related to the texture synthesis task. While the latter tries to capture the statistical relationship between the pixels of a source image which is assumed to have a stationary distribution at some scale, the former does so while also attempting to preserve some notion of content.\nOn the computer vision side, Efros & Leung (1999) and Wei & Levoy (2000) attempt to \u201cgrow\u201d textures one pixel at a time using non-parametric sampling of pixels in an examplar image. Efros & Freeman (2001) and Liang et al. (2001) extend this idea to \u201cgrowing\u201d textures one patch at a time, and Efros & Freeman (2001) uses the approach to implement \u201ctexture transfer\u201d, i.e. transfering the texture of an object onto another one. Kwatra et al. (2005) approaches the texture synthesis problem from an energy minimization perspective, progressively refining the texture using an EMlike algorithm. Hertzmann et al. (2001) introduces the concept of \u201cimage analogies\u201d: given a pair of \u201cunfiltered\u201d and \u201cfiltered\u201d versions of an examplar image, a target image is processed to create an analogous \u201cfiltered\u201d result. More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account.\nOn the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier.\nWhile very flexible, this algorithm is expensive to run due to the optimization loop being carried. Ulyanov et al. (2016a) and Johnson et al. (2016) tackle this problem by introducing a feedforward style transfer network, which is trained to go from content to pastiche image in one pass. However, in doing so some of the flexibility of the original algorithm is lost: the style transfer network is tied to a single style, which means that separate networks have to be trained for every style being\nar X\niv :1\n61 0.\n07 62\n9v 1\n[ cs\n.C V\n] 2\n4 O\nct 2\n01 6\n(a) With conditional instance normalization, a single style transfer network can capture 32 styles at the same time, five of which are shown here. All 32 styles in this single model are in the Appendix. Golden Gate Bridge photograph by Rich Niewiroski Jr.\n(b) The style representation learned via conditional instance normalization permits the arbitrary combination of artistic styles. Each pastiche in the sequence corresponds to a different step in interpolating between the \u03b3 and \u03b2 values associated with two styles the model was trained on.\nFigure 1: Pastiches produced by a style transfer network trained on 32 styles chosen for their variety.\nmodeled. Subsequent work has brought some performance improvements to style transfer networks, e.g. with respect to color preservation (Gatys et al., 2016) or style transfer quality (Ulyanov et al., 2016b), but to our knowledge the problem of the single-purpose nature of style transfer networks remains untackled.\nWe think this is an important problem that, if solved, would have both scientific and practical importance. First, style transfer has already found use in mobile applications, for which on-device processing is contingent upon the models having a reasonable memory footprint. More broadly, building a separate newtork for each style ignores the fact that individual paintings share many common visual elements and a true model that captures artistic style would be able to exploit and learn from such regularities. Furthermore, the degree to which an artistic styling model might generalize across painting styles would directly measure our ability to build systems that parsimoniously capture the higher level features and statistics of photographs and images (Simoncelli & Olshausen, 2001).\nIn this work, we show that a simple modification of the style transfer network, namely the introduction of conditional instance normalization, allows it to learn multiple styles (Figure 1a).We demonstrate that this approach is flexible yet comparable to single-purpose style transfer networks, both qualitatively and in terms of convergence properties. This model reduces each style image into a point in an embedding space. Furthermore, this model provides a generic representation for artistic styles that seems flexible enough to capture new artistic styles much faster than a single-purpose net-\nwork. Finally, we show that the embeddding space representation permits one to arbitrarily combine artistic styles in novel ways not previously observed (Figure 1b)."}, {"heading": "2 STYLE TRANSFER WITH DEEP NETWORKS", "text": "Style transfer can be defined as finding a pastiche image p whose content is similar to that of a content image c but whose style is similar to that of a style image s. This objective is by nature vaguely defined, because similarity in content and style are themselves vaguely defined.\nThe neural algorithm of artistic style proposes the following definitions:\n\u2022 Two images are similar in content if their high-level features as extracted by a trained classifier are close in Euclidian distance.\n\u2022 Two images are similar in style if their low-level features as extracted by a trained classifier share the same statistics or, more concretely, if the difference between the features\u2019 Gram matrices has a small Frobenius norm.\nThe first point is motivated by the empirical observation that high-level features in classifiers tend to correspond to higher levels of abstractions (see Zeiler & Fergus (2014) for visualizations; see Johnson et al. (2016) for style transfer features). The second point is motivated by the observation that the artistic style of a painting may be interpreted as a visual texture (Gatys et al., 2015a). A visual texture is conjectured to be spatially homogenous and consist of repeated structural motifs whose minimal sufficient statistics are captured by lower order statistical measurements (Julesz, 1962; Portilla & Simoncelli, 1999).\nIn its original formulation, the neural algorithm of artistic style proceeds as follows: starting from some initialization of p (e.g. c, or some random initialization), the algorithm adapts p to minimize the loss function L(s, c, p) = \u03bbsLs(p) + \u03bbcLc(p), (1) whereLs(p) is the style loss, Lc(p) is the content loss and \u03bbs, \u03bbc are scaling hyperparameters. Given a set of \u201cstyle layers\u201d S and a set of \u201ccontent layers\u201d C, the style and content losses are themselves defined as\nLs(p) = \u2211 i\u2208S 1 Ui || G(\u03c6i(p))\u2212G(\u03c6i(s)) ||2F (2)\nLc(p) = \u2211 j\u2208C 1 Uj || \u03c6j(p)\u2212 \u03c6j(s) ||22 (3)\nwhere \u03c6l(x) are the classifier activations at layer l, Ul is the total number of units at layer l and G(\u03c6l(x)) is the Gram matrix associated with the layer l activations.\nIn practice, we set \u03bbc = 1.0 and and leave \u03bbs as a free hyper-parameter."}, {"heading": "2.1 FEEDFORWARD STYLE TRANSFER NETWORKS", "text": "In order to speed up the procedure outlined above, a style transfer network T is introduced (Figure 2). It takes as input a content image c and outputs the pastiche image p directly. The network is trained on many content images using the same loss function as above, i.e.\nL(s, c) = \u03bbsLs(T (c)) + \u03bbcLc(T (c)). (4)\nWhile feedforward style transfer networks solve the problem of speed at test-time, they also suffer from the fact that they are tied to one specific style. This means that a separate network has to be trained for every style to be imitated. The real-world impact of this limitation is that it becomes prohibitive to implement a style transfer application on a memory-limited device, such as a smartphone."}, {"heading": "2.2 N-STYLES FEEDFORWARD STYLE TRANSFER NETWORKS", "text": "Our work stems from the intuition that many styles probably share some degree of computation, and that this sharing is thrown away by training N networks from scratch when building an N - styles style transfer system. For instance, many impressionist paintings share similar paint strokes but differ in the color palette being used. In that case, it seems very wasteful to treat a set of N impressionist paintings as completely separate styles.\nTo take this into account, we propose to train a single conditional style transfer network T (c, s) for N styles. The conditional network is given both a content image and the identity of the style to apply and produces a pastiche corresponding to that style. While the idea is straightforward on paper, there remains the open question of how conditioning should be done. In exploring this question, we found a very surprising fact about the role of normalization in style transfer networks: to model a style, it is sufficient to specialize scaling and shifting parameters after normalization to each specific style. In other words, all convolutional weights of a style transfer network can be shared across many styles, and it is sufficient to tune parameters for an affine transformation after normalization for each style.\nWe call this approach conditional instance normalization. Building off the instance normalization technique proposed in Ulyanov et al. (2016b), we augment the \u03b3 and \u03b2 parameters so that they\u2019re N \u00d7 C matrices, where N is the number of styles being modeled and C is the number of output feature maps. Conditioning on a style is achieved as follows:\nz = \u03b3s ( x\u2212 \u00b5 \u03c3 ) + \u03b2s (5)\nwhere \u00b5 and \u03c3 are x\u2019s mean and standard deviation taken across spatial axes and \u03b3s and \u03b2s are obtained by selecting the row corresponding to s in the \u03b3 and \u03b2 matrices (Figure 3). One added benefit of this approach is that one can stylize a single image into N painting styles with a single feed forward pass of the network. In constrast, a single-style network requires N feed forward passes to perform N style transfers (Johnson et al., 2016; Ulyanov et al., 2016a).\nBecause conditional instance normalization only acts on the scaling and shifting parameters, training a style transfer network on N styles requires much less parameters than the naive approach of\ntraining N separate networks. In a typical network setup, the model consists of roughly 1.6M parameters, only around 3K (or 0.2%) of which specify individual artistic styles. In fact, because the size of \u03b3 and \u03b2 grows linearly with respect to the number of feature maps in the network, this approach requiresO(N\u00d7L) parameters, where L is the total number of feature maps in the network. In addition, as is discussed in subsection 3.4, conditional instance normalization presents the advantage that integrating an N + 1th style to the network is cheap because of the very small number of parameters to train."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "3.1 METHODOLOGY", "text": "Unless noted otherwise, all style transfer networks were trained using the hyperparameters outlined in the Appendix\u2019s Table 1.\nWe used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution. The use of mirror-padding avoids border patterns sometimes caused by zero-padding in SAME-padded convolutions, while the replacement for transposed convolutions avoids checkerboard patterning, as discussed in in Odena et al. (2016). We find that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al. (2016).\nThe evaluation images used for this work were resized such that their smaller side has size 512. Their stylized versions were then center-cropped to 512x512 pixels for display."}, {"heading": "3.2 TRAINING A SINGLE NETWORK ON N STYLES PRODUCES STYLIZATIONS COMPARABLE", "text": "TO INDEPENDENTLY-TRAINED MODELS\nAs a first test, we trained a 10-styles model on stylistically similar images, namely 10 impressionist paintings from Claude Monet. Figure 4 shows the result of applying the trained network on evaluation images for a subset of the styles, with the full results being displayed in the Appendix. The model captures different color palettes and textures. We emphasize that 99.8% of the parameters are shared across all styles in contrast to 0.2% of the parameters which are unique to each painting style.\nTo get a sense of what is being traded off by folding 10 styles into a single network, we trained a separate, single-style network on each style and compared them to the 10-styles network in terms of style transfer quality and training speed (Figure 5).\nThe left column compares the learning curves for style and content losses between the single-style networks and the 10-styles network. The losses were averaged over 32 random batches of content images. By visual inspection, we observe that the 10-styles network converges as quickly as the single-style networks in terms of style loss, but lags slightly behind in terms of content loss.\nIn order to quantify this observation, we compare the final losses for 10-styles and single-style models (center column). The 10-styles network\u2019s content loss is around 8.7 \u00b1 3.9% higher than its single-style counterparts, while the difference in style losses (8.9 \u00b1 16.5% lower) is insignificant. While the N -styles network suffers from a slight decrease in content loss convergence speed, this may not be a fair comparison, given that it takes N times more parameter updates to train N singlestyle networks separately than to train them with an N -styles network.\nThe right column shows a comparison between the pastiches produced by the 10-styles network and the ones produced by the single-style networks. We see that both results are qualitatively similar."}, {"heading": "3.3 THE N-STYLES MODEL IS FLEXIBLE ENOUGH TO CAPTURE VERY DIFFERENT STYLES", "text": "We evaluated the flexibility of the N -styles model by training a style transfer network on 32 works of art chosen for their diversity. Figure 1a shows the result of applying the trained network on evaluation images for a subset of the styles. Once again, the full results are displayed in the Appendix. The model appears to be capable of modeling all 32 styles in spite of the tremendous variation in color palette and the spatial scale of the painting styles."}, {"heading": "3.4 THE TRAINED NETWORK GENERALIZES ACROSS PAINTING STYLES", "text": "Since all weights in the transformer network are shared between styles, one way to incorporate a new style to a trained network is to keep the trained weights fixed and learn a new set of \u03b3 and \u03b2 parameters. To test the efficiency of this approach, we used it to incrementally incorporate Monet\u2019s Plum Trees in Blossom painting to the network trained on 32 varied styles. Figure 6 shows that doing so is much faster than training a new network from scratch (left) while yielding comparable pastiches: even after eight times fewer parameter updates than its single-style counterpart, the finetuned model produces comparable pastiches (right)."}, {"heading": "3.5 THE TRAINED NETWORK CAN ARBITRARILY COMBINE PAINTING STYLES", "text": "The conditional instance normalization approach raises some interesting questions about style representation. In learning a different set of \u03b3 and \u03b2 parameters for every style, we are in some sense learning an embedding of styles.\nTo probe the utility of this embedding, we tried convex combinations of the \u03b3 and \u03b2 values of very distinct styles (Figure 1b; Figure 7, left column). Employing a single convex combination produces a smooth transition from one style to the other. Suppose (\u03b31, \u03b21) and (\u03b32, \u03b22) are the parameters corresponding to two different styles. We use \u03b3 = \u03b1\u00d7\u03b31+(1\u2212\u03b1)\u00d7\u03b32 and \u03b2 = \u03b1\u00d7\u03b21+(1\u2212\u03b1)\u00d7\u03b22 to stylize an image. Figure 7 (right column) shows the style loss from the transformer network for a given source image, with respect to the Bicentennial Print and Head of a Clown paintings, as we vary \u03b1 from 0 to 1. As \u03b1 increases, the style loss with respect to Bicentennial Print increases, which explains the smooth fading out of that style\u2019s artifact in the transformed image."}, {"heading": "4 DISCUSSION", "text": "It seems very surprising that such a small proportion of the network\u2019s parameters can have such an impact on the overall process of style transfer. It could be that the network architecture is overspecified for the task. Another interpretation could be that the convolutional weights of the style transfer network encode transformations that represent \u201celements of style\u201d. The scaling and shifting factors would then provide a way for each style to inhibit or enhance the expression of various elements of style to form a global identity of style. While this work does not attempt to verify this hypothesis, we think that this would constitute a very promising direction of research in understanding the computation behind style transfer networks as well as the representation of images in general.\nThe question of how predictive each style image is of its corresponding style representation is also of great interest. If it is the case that the style representation can easily be predicted from a style image, one could imagine building a transformer network which skips learning an individual conditional embedding and instead learn to produce a pastiche directly from a style and a content image, much like in the original neural algorithm of artistic style, but without any optimization loop at test time.\nFinally, the learned style representation opens the door to generative models of style: by modeling enough paintings of a given artistic movement (e.g. impressionism), one could build a collection of style embeddings upon which a generative model could be trained. At test time, a style representation would be sampled from the generative model and used in conjunction with the style transfer network to produce a random pastiche of that artistic movement.\nIn summary, we demonstrated that conditional instance normalization constitutes a simple, efficient and scalable modification of style transfer networks that allows them to model multiple styles at the same time. We showed that despite its simplicity, the method is flexible enough to capture very different styles while having very little impact on training time and final performance of the trained network. Finally, we showed that the learned representation of style is useful in arbitrarily combining artistic styles. This work suggests the existence of a learned representation for artistic styles whose vocabulary is flexible enough to capture a diversity of the painted world."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Fred Bertsch, Douglas Eck, Cinjon Resnick and the rest of the Google Magenta team for their feedback; Peyman Milanfar, Michael Elad, Feng Yang, Jon Barron, Bhavik Singh, Jennifer Daniel as well as the the Google Brain team for their crucial suggestions and advice. Finally, we would like to thank the Google Cultural Institute, whose curated collection of art photographs was very helpful in finding exciting style images to train on."}], "references": [{"title": "Image quilting for texture synthesis and transfer", "author": ["Alexei A Efros", "William T Freeman"], "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Efros and Freeman.,? \\Q2001\\E", "shortCiteRegEx": "Efros and Freeman.", "year": 2001}, {"title": "Texture synthesis by non-parametric sampling", "author": ["Alexei A Efros", "Thomas K Leung"], "venue": "In Computer Vision,", "citeRegEx": "Efros and Leung.,? \\Q1999\\E", "shortCiteRegEx": "Efros and Leung.", "year": 1999}, {"title": "Style-transfer via texture-synthesis", "author": ["Michael Elad", "Peyman Milanfar"], "venue": "arXiv preprint arXiv:1609.03057,", "citeRegEx": "Elad and Milanfar.,? \\Q2016\\E", "shortCiteRegEx": "Elad and Milanfar.", "year": 2016}, {"title": "Split and match: Example-based adaptive patch sampling for unsupervised style transfer", "author": ["Oriel Frigo", "Neus Sabater", "Julie Delon", "Pierre Hellier"], "venue": null, "citeRegEx": "Frigo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Frigo et al\\.", "year": 2016}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Preserving color in neural artistic style transfer", "author": ["Leon A Gatys", "Matthias Bethge", "Aaron Hertzmann", "Eli Shechtman"], "venue": "arXiv preprint arXiv:1606.05897,", "citeRegEx": "Gatys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2016}, {"title": "Image analogies", "author": ["Aaron Hertzmann", "Charles E Jacobs", "Nuria Oliver", "Brian Curless", "David H Salesin"], "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Hertzmann et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hertzmann et al\\.", "year": 2001}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Visual pattern discrimination", "author": ["Bela Julesz"], "venue": "IRE Trans. Info Theory,", "citeRegEx": "Julesz.,? \\Q1962\\E", "shortCiteRegEx": "Julesz.", "year": 1962}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Texture optimization for examplebased synthesis", "author": ["Vivek Kwatra", "Irfan Essa", "Aaron Bobick", "Nipun Kwatra"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "Kwatra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kwatra et al\\.", "year": 2005}, {"title": "Real-time texture synthesis by patch-based sampling", "author": ["Lin Liang", "Ce Liu", "Ying-Qing Xu", "Baining Guo", "Heung-Yeung Shum"], "venue": "ACM Transactions on Graphics (ToG),", "citeRegEx": "Liang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2001}, {"title": "Avoiding checkerboard artifacts in neural networks", "author": ["Augustus Odena", "Christopher Olah", "Vincent Dumoulin"], "venue": "Distill,", "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Javier Portilla", "Eero Simoncelli"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Portilla and Simoncelli.,? \\Q1999\\E", "shortCiteRegEx": "Portilla and Simoncelli.", "year": 1999}, {"title": "Natural image statistics and neural representation", "author": ["Eero Simoncelli", "Bruno Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "Simoncelli and Olshausen.,? \\Q2001\\E", "shortCiteRegEx": "Simoncelli and Olshausen.", "year": 2001}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Instance normalization: The missing ingredient for fast stylization", "author": ["Dmitry Ulyanov", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1607.08022,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Fast texture synthesis using tree-structured vector quantization", "author": ["Li-Yi Wei", "Marc Levoy"], "venue": "In Proceedings of the 27th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "Wei and Levoy.,? \\Q2000\\E", "shortCiteRegEx": "Wei and Levoy.", "year": 2000}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Claude Monet, V\u00e9theuil (1879)", "author": ["Claude Monet", "Three Fishing Boats"], "venue": "Claude Monet, V\u00e9theuil (1902). 13", "citeRegEx": "Monet and Boats,? 1886", "shortCiteRegEx": "Monet and Boats", "year": 1886}, {"title": "Ernst Ludwig Kirchner, Boy with Sweets (1918)", "author": ["Roy Lichtenstein", "Bicentennial Print"], "venue": "14", "citeRegEx": "Lichtenstein and Print,? 1975", "shortCiteRegEx": "Lichtenstein and Print", "year": 1975}, {"title": "Opus 196 (1889). Paul Klee, Colors from a Distance (1932)", "author": ["Paul Signac", "Cassis", "Cap Lombard"], "venue": "Frederic Edwin Church, Cotopaxi (1855)", "citeRegEx": "Signac et al\\.,? \\Q1932\\E", "shortCiteRegEx": "Signac et al\\.", "year": 1932}, {"title": "Egon Schiele, Edith with Striped Dress, Sitting (1915)", "author": ["Henri de Toulouse-Lautrec", "Divan Japonais"], "venue": "16", "citeRegEx": "Toulouse.Lautrec and Japonais,? 1893", "shortCiteRegEx": "Toulouse.Lautrec and Japonais", "year": 1893}, {"title": "Head of a Clown (ca", "author": ["Georges Rouault"], "venue": "Giorgio de Chirico, Horses on the seashore (1927/1928)", "citeRegEx": "Rouault,? \\Q1907\\E", "shortCiteRegEx": "Rouault", "year": 1907}, {"title": "Severini Gino, Ritmo plastico del 14 luglio (1913)", "author": ["Juan Gris", "Portrait of Pablo Picasso"], "venue": "19", "citeRegEx": "Gris and Picasso,? 1912", "shortCiteRegEx": "Gris and Picasso", "year": 1912}, {"title": "John Ruskin, Trees in a Lane (1847)", "author": ["Claude Monet", "Three Fishing Boats"], "venue": "Giuseppe Cades, Tullia about to Ride over the Body of Her Father in Her Chariot (about 1770-1775). 23", "citeRegEx": "Monet and Boats,? 1886", "shortCiteRegEx": "Monet and Boats", "year": 1886}], "referenceMentions": [{"referenceID": 5, "context": "Efros & Freeman (2001) and Liang et al. (2001) extend this idea to \u201cgrowing\u201d textures one patch at a time, and Efros & Freeman (2001) uses the approach to implement \u201ctexture transfer\u201d, i.", "startOffset": 27, "endOffset": 47}, {"referenceID": 5, "context": "Efros & Freeman (2001) and Liang et al. (2001) extend this idea to \u201cgrowing\u201d textures one patch at a time, and Efros & Freeman (2001) uses the approach to implement \u201ctexture transfer\u201d, i.", "startOffset": 27, "endOffset": 134}, {"referenceID": 5, "context": "Kwatra et al. (2005) approaches the texture synthesis problem from an energy minimization perspective, progressively refining the texture using an EMlike algorithm.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Hertzmann et al. (2001) introduces the concept of \u201cimage analogies\u201d: given a pair of \u201cunfiltered\u201d and \u201cfiltered\u201d versions of an examplar image, a target image is processed to create an analogous \u201cfiltered\u201d result.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account.", "startOffset": 15, "endOffset": 35}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account.", "startOffset": 15, "endOffset": 185}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture.", "startOffset": 15, "endOffset": 474}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier.", "startOffset": 15, "endOffset": 773}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier. While very flexible, this algorithm is expensive to run due to the optimization loop being carried. Ulyanov et al. (2016a) and Johnson et al.", "startOffset": 15, "endOffset": 1101}, {"referenceID": 3, "context": "More recently, Frigo et al. (2016) treats style transfer as a local texture transfer (using an adaptive patch partition) followed by a global color transfer, and Elad & Milanfar (2016) extends Kwatra\u2019s energy-based method into a style transfer algorithm by taking content similarity into account. On the machine learning side, it has been shown that a trained classifier can be used as a feature extractor to drive texture synthesis and style transfer. Gatys et al. (2015a) uses the VGG-19 network (Simonyan & Zisserman, 2014) to extract features from a texture image and a synthesized texture. The two sets of features are compared and the synthesized texture is modified by gradient descent so that the two sets of features are as close as possible. Gatys et al. (2015b) extends this idea to style transfer by adding the constraint that the synthesized image also be close to a content image with respect to another set of features extracted by the trained VGG-19 classifier. While very flexible, this algorithm is expensive to run due to the optimization loop being carried. Ulyanov et al. (2016a) and Johnson et al. (2016) tackle this problem by introducing a feedforward style transfer network, which is trained to go from content to pastiche image in one pass.", "startOffset": 15, "endOffset": 1127}, {"referenceID": 6, "context": "with respect to color preservation (Gatys et al., 2016) or style transfer quality (Ulyanov et al.", "startOffset": 35, "endOffset": 55}, {"referenceID": 9, "context": "A visual texture is conjectured to be spatially homogenous and consist of repeated structural motifs whose minimal sufficient statistics are captured by lower order statistical measurements (Julesz, 1962; Portilla & Simoncelli, 1999).", "startOffset": 190, "endOffset": 233}, {"referenceID": 5, "context": "The first point is motivated by the empirical observation that high-level features in classifiers tend to correspond to higher levels of abstractions (see Zeiler & Fergus (2014) for visualizations; see Johnson et al. (2016) for style transfer features).", "startOffset": 202, "endOffset": 224}, {"referenceID": 8, "context": "In constrast, a single-style network requires N feed forward passes to perform N style transfers (Johnson et al., 2016; Ulyanov et al., 2016a).", "startOffset": 97, "endOffset": 142}, {"referenceID": 16, "context": "Building off the instance normalization technique proposed in Ulyanov et al. (2016b), we augment the \u03b3 and \u03b2 parameters so that they\u2019re N \u00d7 C matrices, where N is the number of styles being modeled and C is the number of output feature maps.", "startOffset": 62, "endOffset": 85}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution.", "startOffset": 44, "endOffset": 66}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution. The use of mirror-padding avoids border patterns sometimes caused by zero-padding in SAME-padded convolutions, while the replacement for transposed convolutions avoids checkerboard patterning, as discussed in in Odena et al. (2016). We find that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al.", "startOffset": 44, "endOffset": 513}, {"referenceID": 8, "context": "We used the same network architecture as in Johnson et al. (2016), except for two key details: zero-padding is replaced with mirror-padding, and transposed convolutions (also sometimes called deconvolutions) are replaced with nearest-neighbor upsampling followed by a convolution. The use of mirror-padding avoids border patterns sometimes caused by zero-padding in SAME-padded convolutions, while the replacement for transposed convolutions avoids checkerboard patterning, as discussed in in Odena et al. (2016). We find that with these two improvements training the network no longer requires a total variation loss that was previously employed to remove high frequency noise as proposed in Johnson et al. (2016). The evaluation images used for this work were resized such that their smaller side has size 512.", "startOffset": 44, "endOffset": 715}], "year": 2016, "abstractText": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.", "creator": "LaTeX with hyperref package"}}}