{"id": "1610.07717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Distributed and parallel time series feature extraction for industrial big data applications", "abstract": "The all-important feature selection problem is the identification of all strongly and weakly relevant attributes. This problem is particularly difficult to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, where each label or regression target is associated with multiple time series and meta-information at the same time. At this point, we propose an efficient, scalable feature extraction algorithm that filters available features at an early stage of the machine learning pipeline in terms of their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features.", "histories": [["v1", "Tue, 25 Oct 2016 03:31:58 GMT  (587kb,D)", "http://arxiv.org/abs/1610.07717v1", "ACML 2016 Workshop on Learning on Big Data (4)"], ["v2", "Fri, 16 Dec 2016 02:07:07 GMT  (2106kb,D)", "http://arxiv.org/abs/1610.07717v2", null], ["v3", "Fri, 19 May 2017 21:20:18 GMT  (2748kb,D)", "http://arxiv.org/abs/1610.07717v3", null]], "COMMENTS": "ACML 2016 Workshop on Learning on Big Data (4)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maximilian christ", "reas w kempa-liehr", "michael feindt"], "accepted": false, "id": "1610.07717"}, "pdf": {"name": "1610.07717.pdf", "metadata": {"source": "META", "title": "Distributed and parallel time series feature extraction for industrial big data applications", "authors": ["Maximilian Christ", "Andreas W. Kempa-Liehr", "Michael Feindt"], "emails": ["maximilian.christ@blue-yonder.com", "kempa-liehr@fmf.uni-freiburg.de", "michael.feindt@blue-yonder.com"], "sections": [{"heading": null, "text": "weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features.\nThe proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics."}, {"heading": "1. Introduction", "text": "Promising fields of application for machine learning are the Internet of Things (IoT) [Gubbi et al., 2013] and Industry 4.0 [Hermann et al., 2016] environments. In these fields, machine learning models anticipate future device states by combining knowledge about device attributes with historic sensor time series. They permit the classification of devices (e.g. hard drives) into risk classes with respect to a specific defect [Mobley, 2002]. Both fields are driven by the availability of cheap sensors and advancing connectivity between devices, which increases the need for machine learning on temporally annotated data.\nIn most cases the volume of the generated time series data forbids their transport to centralized databases [Gubbi et al., 2013]. Instead, algorithms for an efficient reduction of the data volume by means of feature extraction and feature selection are needed [Bol\u00f3nCanedo et al., 2015, p. 125\u2013136]. Furthermore, for online applications of machine learning\nc\u00a9 2016 M. Christ, A.W. Kempa-Liehr & M. Feindt.\nar X\niv :1\n61 0.\n07 71\n7v 1\n[ cs\n.L G\n] 2\nit is important to continuously select relevant features in order to deal with concept drifts caused by qualitative changes of the underlying dynamics [Liu and Setiono, 1998].\nTherefore, for industrial and other applications, one needs to combine distributed feature extraction methods with a scalable feature selection, especially for problems where several time series and meta-information have to be considered per label/target [Kusiak and Li, 2011]. For time series classification, it proved to be efficient to apply comprehensive feature extraction algorithms and then filter the respective features [Fulcher and Jones, 2014].\nMotivated by industrial applications for machine learning models Christ et al. [2016] we are extending the approach of Fulcher and Jones and propose FeatuRe Extraction based on Scalable Hypothesis tests (FRESH). The algorithm characterizes time series with comprehensive and well-established feature mappings and considers additional features describing meta-information. In a second step, each feature vector is individually and independently evaluated with respect to its significance for predicting the target under investigation. The result of these tests is a vector of p-values, quantifying the significance of each feature for predicting the label/target. This vector is evaluated on basis of the Benjamini-Yekutieli procedure [Benjamini and Yekutieli, 2001] in order to decide which features to keep.\nThe proposed algorithm is evaluated on all binary classification problems of the UCR time series classification archive [Chen et al., 2015] as well as time series data from a production line optimization project and simulated time series from a stochastic process with underlying qualitative change of dynamics [Liehr, 2013]. The results are benchmarked against well-established feature selection algorithms like linear discriminant analysis [Fulcher and Jones, 2014] and the Boruta algorithm [Kursa and Rudnicki, 2011], but also against Dynamic Time Warping [Wang et al., 2013]. The analysis shows that the proposed method outperforms Boruta based feature selection approaches as well as Dynamic Time Warping based approaches for problems with large feature sets and large time series samples. This contribution closes with a summary and an outlook on future work."}, {"heading": "2. Time series feature extraction", "text": "Temporally annotated data come in three different variants [Elmasri and Lee, 1998]: Temporally invariant information (e.g. the manufacturer of a device), temporally variant information, which change irregularly (e.g. process states), and temporally variant information with regularly updated values (e.g. sensor measurements). The latter describe the continuously changing state si,j(t) of a system or device si with respect to a specific measurement of sensor j, which is repeated in intervals of length \u2206t. This sampling captures the state of the system or device under investigation as a sequence\nsi,j(t1)\u2192 si,j(t2)\u2192 . . .\u2192 si,j(t\u03bd)\u2192 . . .\u2192 si,j(tnt)\nwith t\u03bd+1 = t\u03bd + \u2206t. Such kind of sequences are called time series and are abbreviated by\nsi,j = (si,j(t1), si,j(t2), . . . , si,j(t\u03bd), . . . , si,j(tnt)) T\n= (si,j,1, si,j,2, . . . , si,j,\u03bd , . . . , si,j,nt) T.\nHere, we are considering i = 1, . . . ,m devices with j = 1, . . . , n different time series per device. Therefore, we are dealing with n \u00b7 m \u00b7 nt values describing the ensemble under\ninvestigation. In order to characterize a time series with respect to its dynamics and reduce the data volume, a mapping \u03b8k : Rnt \u2192 R is introduced, which captures a specific aspect k of the time series. One example for such mapping might be the maximum operator\n\u03b8max(si,j) = max{si,j,1, si,j,2, . . . , si,j,\u03bd , . . . , si,j,nt},\nwhich quantifies the maximal value ever recorded for time series si,j . This kind of lower dimensional representation is called a feature, which is a measurable characteristics of the considered time series. Other examples for feature mappings \u03b8k of time series might be their mean, the number of peaks with a certain steepness, their periodicity, a global trend, etc. Comprehensive collections of time series feature mappings are discussed by Fulcher and Jones [2014] and Nun et al. [2015]. Fulcher and Jones [2014] propose to use more than 9000 features from 1000 different feature generating algorithms.\nNow, consider nf different time series feature mappings, which are applied to all m \u00b7 n time series recorded from n sensors of m devices (Fig. 1). The resulting feature matrix X \u2208 Rm\u00d7n\u03c6 has m rows (one for each device) and n\u03c6 = n \u00b7nf +ni columns with ni denoting the number of features generated from device specific meta-information. Each column of X comprises a vector X \u2208 Rm capturing a specific characteristic of all considered devices."}, {"heading": "3. Feature filtering", "text": "Typically, time series are noisy and contain redundancies. Therefore, one should keep the balance between extracting meaningful but probably fragile features and robust but probably non-significant features. Some features such as the median will not be heavily influenced by outliers, others such as max will be intrinsically fragile. The choice of the right time series feature mappings is crucial to capture the right characteristics for the task at hand."}, {"heading": "3.1. Relevance of features", "text": "A meaningless feature describes a characteristic of the time series that is not useful for the classification or regression task at hand. Radivojac et al. [2004] considered a binary target Y , stating that the relevance of feature X is measured as the difference between the class conditional distributions fX|Y=0 and fX|Y=1. We adopt this definition and consider a feature X being relevant for the classification of the binary target Y if those distributions are not equal. In general, a feature X is relevant for predicting target Y if and only if\n\u2203 y1, y2 with fY (y1) > 0, fY (y2) > 0 : fX|Y=y1 6= fX|Y=y2 . (1)\nThe condition from Equation (1) is equivalent to\nX is not relevant for target Y \u21d4 \u2200y1, y2 with fY (y1) > 0, fY (y2) > 0 : fX|Y=y1 = fX|Y=y2 \u21d4 \u2200y1 with fY (y1) > 0 : fX|Y=y1 = fX \u21d4 fX,Y = fX|Y fY = fXfY \u21d4 X,Y are statistically independent\n(2)\nWe will use the statistical independence to derive a shorter definition of a relevant feature:\nDefinition 1 (A relevant feature) A feature X\u03c6 is relevant or meaningful for the prediction of Y if and only if X\u03c6 and Y are not statistically independent."}, {"heading": "3.2. Hypothesis tests", "text": "For every extracted feature X1, . . . , X\u03c6, . . . , Xn\u03c6 we will deploy a singular statistical test checking the hypotheses\nH\u03c60 = {X\u03c6 is irrelevant for predicting Y }, H \u03c6 1 = {X\u03c6 is relevant for predicting Y }. (3)\nThe result of each hypothesis test H\u03c60 is a so-called p-value p\u03c6, which quantifies the probability that feature X\u03c6 is not relevant for predicting Y . Small p-values indicate features, which are relevant for predicting the target.\nBased on the vector (p1, . . . , pn\u03c6) T of all hypothesis tests, a multiple testing approach will select the relevant features (Sec. 3.3). We propose to treat every feature uniquely by a different statistical test, depending on wether the codomains of target and feature are binary or not. The usage of one general feature test for all constellations is not recommended. Specialized hypothesis tests yield a higher statistical power due to more assumptions about the codomains that can be used during the construction of those tests. The proposed feature significance tests are based on nonparametric hypothesis tests, that do not make\nany assumptions about the distribution of the variables, which ensures robustness of the procedure.\nExact Fisher test of independence: This feature significance test can be used if both the target and the inspected feature are binary. Fisher\u2019s exact test [Fisher, 1922] is based on the contingency table formed by X\u03c6 and Y . It inspects if both variables are statistically independent, which corresponds to the hypotheses from Eq. (3).\nFisher\u2019s test belongs to the class of exact tests. For such tests, the significance of the deviation from a null hypothesis (e.g., the p-value) can be calculated exactly, rather than relying on asymptotic results.\nKolmogorov-Smirnov test (binary feature): This feature significance test assumes the feature to be binary and the target to be continuous. In general, the KolmogorovSmirnov (KS) test is a non-parametric and stable goodness-of-fit test that checks if two random variables A and B follow the same distribution [Massey, 1951]:\nH0 = {fA = fB}, H1 = {fA 6= fB}.\nBy conditionally modeling the distribution function of target Y on the two possible values x1, x2 of the feature X\u03c6 we can use the KS test to check if the distribution of Y differs given different values of X\u03c6. Setting A = Y |X\u03c6 = x1 and B = Y |X\u03c6 = x2 results in\nH\u03c60 = {fY |X\u03c6=x1 = fY |X\u03c6=x2}, H \u03c6 1 = {fY |X\u03c6=x1 6= fY |X\u03c6=x2}. (4)\nThe hypotheses from Eq. (3) and (4) are equivalent as demonstrated in the chain of Equations in (2). Hence, the KS test can address the feature relevance of X\u03c6.\nKolmogorov-Smirnov test (binary target): When the target is binary and the feature non-binary, we can deploy the Kolmogorov-Smirnov test again. We have to switch roles of target and feature variable, resulting in the testing of the following hypothesis:\nH\u03c60 = {fX\u03c6|Y=y1 = fX\u03c6|Y=y2}, H \u03c6 1 = {fX\u03c6|Y=y1 6= fX\u03c6|Y=y2}.\nThis time y1 and y2 are the two possible values of Y and fX\u03c6|Y=yj is the conditional density function of X\u03c6 given Y . This hypothesis is also equivalent to the one in Eq. (3).\nKendal rank test: This filter can be deployed if neither target nor feature are binary. Kendall\u2019s rank test [Kendall, 1938] checks if two continuous variables may be regarded as statistically dependent, hence naturally fitting our hypotheses from Eq. (3). It is a nonparametric test based on Kendall\u2019s rank statistic \u03c4 , measuring the strength of monotonic association between X\u03c6 and Y . The calculation of the rank statistic is more complex when ties are involved [Adler, 1957], i.e. feature or target are categorical."}, {"heading": "3.3. Feature significance testing", "text": "When comparing multiple hypotheses simultaneously, errors in the inference tend to accumulate [Curran-Everett, 2000]. In this context, a wrongly added feature is a feature X\u03c6 for which the null hypothesis H\u03c60 has been rejected by the respective feature significance test, even though H\u03c60 is true. If we want to control the percentage of irrelevant added features we have to control the percentage of wrongly rejected null hypothesis among all hypothesis.\nIn multiple testing, this ratio of the expected proportion of erroneous rejections among all rejections is called false discovery rate (FDR).\nThe FDR as a measure of the accumulated statistical error was suggested by Benjamini and Hochberg [1995]. Later the non-parametric Benjamini-Yekutieli procedure was proposed. Based on the p-values it tells which hypotheses to reject while still controlling the FDR under any dependency structure between those hypotheses [Benjamini and Yekutieli, 2001]. It will be the last component of our filtered feature extraction algorithm.\nThe procedure searches for the first intersection between the ordered sequence of p-values p(\u03c6) (dotted blue curves in Fig. 2) with a linear sequence (green lines in Fig. 2)\nr\u03c6 = \u03c6q\nn\u03c6 \u2211\u03c6 \u00b5=1 1 \u00b5 . (5)\nHere, n\u03c6 is the number of all null hypotheses and q is the FDR level that the procedure controls. It will reject all hypotheses belonging to p-values that have a lower value than the p-value at the intersection, see the left side of Fig. 2(b)."}, {"heading": "3.4. The proposed feature extraction algorithm", "text": "We propose FeatuRe Extraction based on Scalable Hypothesis tests (FRESH) for parameter q \u2208 [0, 1], given by the following three steps:\n1. Perform a set of n\u03c6 univariate feature mappings as introduced in Sec. 2 on m \u00b7 n different time series to create the features X\u03c6, \u03c6 = 1, . . . , n\u03c6.\n2. For each generated feature X1, . . . , Xn\u03c6 perform exactly one hypothesis test for the hypothesisH\u03c60 from Equation (3). To do so, take the corresponding feature significance test from Sec. 3.2. Calculate the p-values p1, . . . , pn\u03c6 of the tests.\n3. Perform the Benjamini-Yekutieli procedure under correction for dependent hypotheses [Benjamini and Yekutieli, 2001] for a FDR level of q on the collected p-values p1, . . . , pn\u03c6 to decide which null hypothesis H \u03c6 0 to reject (c.f. Sec. 3.3). Only return\nfeatures X\u03c6 for which the respective hypothesis H \u03c6 0 was rejected by the procedure.\n3.5. Variants of FRESH\nA problem of many filter feature methods such as the one we utilize in steps 2 and 3 of FRESH is the redundancy in the feature selection. As long as features are considered associated with the target, they will all be selected by the filter even though many of them are highly correlated to each other [Kira and Rendell, 1992]. For example median and mean are highly correlated in the absence of outliers and therefore we expect FRESH to either select or drop both median and mean at the same time. To avoid generating a group of highly correlated features we propose to add another step to FRESH:\n\u2217. Normalize the features and perform a principal performance analysis (PCA). Keep the principal components with highest eigenvalue describing p percent of the variance.\nThis step will reduce the number of features and the obtained principal components are de-correlated, orthogonal variables [Hotelling, 1933].\nOne could perform step \u2217 between steps 1 and 2 of FRESH to get rid of the correlations between the created variables early. Then the feature significance tests in step 2 of FRESH will take principal components instead of the original features as input. We will denote this variant of FRESH as FRESH_PCAb(efore). Also, one could perform step \u2217 after the FRESH algorithm, directly after step 3. This means that the PCA will only process those features, which are found relevant by the FRESH algorithm instead of processing all features. This variant of FRESH is called FRESH_PCAa(fter)."}, {"heading": "4. Evaluation", "text": "In the following, the performance of FRESH, its two variants from Sec. 3.5 and other time series feature extraction methods are compared. The evaluation is done with respect to both the meaningfulness of the extracted features as well as the time it takes to extract the features. While doing so, all feature extraction methods operate on the same feature mappings, the differences lay only in the used feature selection process."}, {"heading": "4.1. Setup", "text": "FRESH is parameterized with q = 10%, cf. Eq. (5). Its variants, which apply a PCA, are using p = 95% (Sec. 3.5). Full_X uses all the features, which are created during step 1 of FRESH (Sec. 3.4) without any subsequent filtering. Features contained in Full_X will be filtered by applying the Boruta feature selection algorithm [Kursa and Rudnicki, 2011], or by a forward selection with a linear discriminant analysis classifier denoted LDA. Further, the direct classifier DTW_NN, a nearest neighbor search under the Dynamic Time Warping distance, is considered [Wang et al., 2006]. Those six different extraction methods and DTW_NN were each picked for a reason. DTW_NN is reported to reach the highest accuracy rates among other time series classifiers, LDA was the first proposed algorithm to automatically\nextract features from time series [Fulcher and Jones, 2014] and Boruta is a promising feature selection algorithm that incorporates interactions between features.\nTo guarantee reproducibility we use all 31 time series data sets from the UCR time series archive [Chen et al., 2015] containing a binary classification problem. To compare the runtime of the different methods, time series of flexible length and sample number belonging to two classes are generated by simulating the stochastic dynamics of a dissipative soliton [Liehr, 2013, p. 164]. The last data source originates from the production of steel billets, extracted during the German research project iPRODICT. The project demonstrates a typical application of industrial time series analysis, aiming to predict the passing or failing of product specification testings based on timely annotated data. It contains 26 univariate meta-variables forming the baseline feature set extended by 20 different sensor time series having up to 44 data points for each sample. The data set contained 1554 samples of two classes \"broken\" and \"not broken\" each, in total 3108 samples.\nDue to limitations regarding the size of this paper, we do not address the regression performance of FRESH and variants yet. In a future work, we catch up on this."}, {"heading": "4.2. Accuracy", "text": "For the data sets from the UCR time series repository as well as the iPRODICT data, the underlying structure and therefore the relevant features are unknown. We cannot compare the different methods on their ability to extract meaningful features because we do not know which features are meaningful and which are not. Also, we cannot compare the extracted features to direct classifiers such as DTW_NN. Therefore, we evaluate the performance of the feature extraction algorithms by comparing the performance of a classification algorithm on the extracted features. Hereby, we assume that more meaningful features will result in a better classification result.\nTo investigate the feature extraction methods under different conditions and to make sure that the feature filtering is not tuned to a specific classifier, the classification task is solved by a collection of a one layer neural network/perceptron (NN), a logistic regression model (LR), a support vector machine (SVM), a random forest classifier (RFC) and an adaboost classifier (ABC). The hyperparameters for those methods are not optimized to get an unbiased view on the meaningfulness of the extracted features, instead the default values from the Python package scikitlearn version 0.17.1 were used [Pedregosa et al., 2011].\nFor every data set, all available samples will be used to perform the feature extraction itself. Then, one third of the samples are randomly picked for a test set, the remaining two thirds are used to train the classifiers. We define the index set for the classification algorithms asM := {NN, SVM, RFC, ABC, LR} and for the inspected feature extraction methods we define A := {FRESH, LDA, FULL_X, FRESH_PCAa, FRESH_PCAb, Boruta}. Then, the accuracy of a classifier m \u2208M on the test part of the data set d for the features generated by method a \u2208 A is denoted as accdm(a).\nFurther, we calculate the mean of the accuracy of the five classification algorithms, which is denoted by accd(a) := 15 \u2211 m\u2208M acc d m(a). DTW_NN itself does not perform any feature extraction and its accuracy is directly calculated by predicting on the test set. It is denoted by accd(DTW_NN). Now, we are able to denote the average accuracy over all 31 UCR data\nsets with binary classification tasks [Chen et al., 2015] for each method a \u2208 A \u222a {DTW_NN} by accUCR(a) = 131 \u2211 i=1,...31 acc\ndi(a), cf. columns 3 and 7 of Tab. 1. From the 3rd column of Tab. 1 we can observe that FRESH_PCAa dominated the feature based approaches on the UCR data sets but it was not able to beat DTW_NN. On the iPRODICT data, DTW_NN could only operate on one type of time series without the univariate features. This seems to be the reasons why Boruta and FRESH_PCAa beat the accuracy of DTW_NN as shown in the 7th column of Tab. 1. Here FRESH_PCAa again achieved the highest accuracy among all feature based approaches.\nFurther, we count how often a feature extraction method a\u2217 \u2208 A reaches the highest accuracy for the 155 classifier/ data set combinations on the 31 UCR data sets among all six feature extraction methods in A while a draw counts for both methods:\nnUCRbest (a \u2217) := \u2223\u2223\u2223\u2223 { (m, di) \u2223\u2223\u2223\u2223m \u2208M, i = 1, . . . , 31 : accdim(a\u2217) = max{accdim(a) | a \u2208 A} }\u2223\u2223\u2223\u2223 .\nThe evaluation metric nUCRbest is reported in the 5 th column of Tab. 1. Again FRESH_PCAa achieved the best result, for over half of the 155 inspected combinations it had the highest reported accuracy and again Boruta came in second with 74.3 combinations on average. Regarding both accuracy metrics, FRESH_PCAa seems to be favorable over the other considered feature based approaches, with Boruta coming close."}, {"heading": "4.3. Runtime", "text": "The second and sixth column of Tab. 1 contains the average pipeline runtime, which is the combined runtime of feature extraction, training of a classifier and predicting. For DTW_NN the pipeline runtime captures just the fitting and predicting steps as no features are extracted. There are tradeoffs between the number of extracted features and the time spent on feature extraction which force us to consider the pipeline runtime instead of the extraction runtime. E.g., time spent in the extraction process can be compensated by a lower number of extracted features which reduces the time spent for training the final classifier.\nAnalog to the accuracy evaluation metrics, tdm(a) denotes the pipeline runtime in seconds of classifier m \u2208 M and feature extraction method a \u2208 A on data set d. In the same way, td(a) = 15 \u2211 m\u2208M t d m(a) denotes the average pipeline runtime in seconds over all 5 classifiers and td(DTW_NN) the runtime of fitting and predicting by DTW_NN. The average pipeline runtimes over all UCR data sets for each method a \u2208 A\u222a{DTW_NN} are denoted by tUCR(a) = 131 \u2211 i=1,...31 t\ndi(a). All calculations are executed on a single computational core in order to increase comparability.\nThe full feature matrix Full_X is the fastest extraction algorithm in our comparisons on both the UCR and the iPRODICT data, as seen in the 2nd and 6th column of Tab. 1. Saved time for the fitting of the feature selection algorithm compensated for the five classifiers having to be trained on more features. Accordingly, the PCA step of FRESH_PCAa saves so much time for the fitting of the classifiers that this step is basically \u201cfree\u201d, while FRESH_PCAa has a mean pipeline runtime of 33.86 seconds. On average, FRESH takes 33.87 seconds. The same can be observed on the iPRODICT data where Full_X, FRESH and FRESH_PCAa all had similar pipeline runtimes even though the number of extracted features varied greatly. The low average pipeline runtime of just tproject = 40.17 seconds for DTW_NN in the 6th column of Tab. 1 is due to the classification algorithm only considering one type of time series while the extraction methods operate on 20 different time series.\nApart from the observed runtimes, we are interested in the feature extraction method\u2019s ability to scale with an increasing number of feature mappings, time series length and device numbers. As expected, Fig. 3 shows that all considered feature extraction methods \u2013 in contrast to DTW_NN \u2013 scale linearly with an increasing length of the time series or increasing number of samples. This is due to the considered feature mapping having a linear runtime with respect to the length of the time series. However, Fig. 4 shows that, among the feature based approaches, only FRESH and FRESH_PCAa scale linear with an increasing number of features (e.g. due to more devices, feature mappings or types of time series)."}, {"heading": "4.4. Selected features", "text": "The number of features extracted by algorithm a \u2208 A on the data set d are denoted by ndf (a). Again this can be averaged over the UCR data sets resulting in n UCR f (a) =\n1 31 \u2211 i=1,...31 n di f (a), cf. rightmost column of Tab. 1.\nDuring our simulations, FRESH_PCAa was able to reduce the number of features drastically. For the UCR data, it reduces 161 considered feature mappings to an average number of 8.2 features while on the iPRODICT data it reduces the 3246 calculated features to 33. FRESH_PCAb only selected four respective two features on average, which may explain its low accuracies. If one wishes to extract a minimal set of relevant features, we recommend to deploy FRESH_PCAa, as it extracted the second lowest number of features but achieved the highest and second highest accuracies."}, {"heading": "4.5. Resume", "text": "We proposed FRESH as a highly scalable feature extraction algorithm. Our simulations showed that, in contrast to other considered methods, FRESH is able to scale with the number of feature mappings and samples as well as with the amount of different types and length of the time series. While doing so, it is extracting meaningful features as demonstrated by competitive accuracies.\nThe relative bad performance of FRESH_PCAb seems to originate in the PCA step selecting features only based on their ability to explain the variance in the input variables and not in their significace to predict the target variable. By this, relevant information for the classification or regression task can get lost.\nOn the other hand, the combination of FRESH with a subsequent PCA filtering to reduce the number of redundant and highly correlated features, denoted as FRESH_PCAa was overall the most competitive feature based method in our evaluation. On the UCR data, it achieved\nthe second highest and on the iPRODICT data it reached the highest accuracy. Further, it had the second lowest number of extracted features."}, {"heading": "5. Discussion", "text": ""}, {"heading": "5.1. FRESH assists the acquisition of domain knowledge", "text": "It is common knowledge that the quality of feature engineering is a crucial success factor for supervised machine learning in general [Domingos, 2012, p. 82] and for time series analysis in particular [Timmer et al., 1993]. But comprehensive domain knowledge is needed in order to perform high quality feature engineering. Contrarily, it is quite common for machine learning projects that data scientists start with limited domain knowledge and improve their process understanding while continuously discussing their models with domain experts. This is basically the reason, why dedicated time series models are very hard to build from scratch.\nOur experience with data science projects in the context of IoT and Industry 4.0 applications Christ et al. [2016] showed that it is very important to identify relevant time series features in an early stage of the project in order to engineer more specialized features in discussions with domain experts. The FRESH algorithm supports this approach by applying a huge variety of established time series feature mappings to different types of time series and meta-information simultaneously and identifies relevant features in a robust manner.\nWe observe that features extracted by FRESH contribute to a deeper understanding of the investigated problem, because each feature is intrinsically related to a distinct property of the investigated system and its dynamics. This fosters the interpretation of the extracted features by domain experts and allows for the engineering of more complex, domain specific features Christ et al. [2016] including dedicated time series models, such that their predictions in return might become a future feature mapping for FRESH."}, {"heading": "5.2. FRESH is operational", "text": "We have already mentioned that FRESH has been developed in the course of IoT and Industry 4.0 projects Christ et al. [2016]. Especially for predictive maintenance applications with limited numbers of samples and high level of noise in e.g. sensor readings, it has been proven as crucial to filter irrelevant features in order to prevent overfitting. To ensure a robust and scalable filtering, we consider each feature importance individually. This causes several implications:\n\u2022 FRESH is robust in the sense of classical statistics, because the hypothesis tests and the Benjamini-Yekutieli procedure do not make any assumptions about the probability distribution or dependence structure between the features. Here, robustness refers to the insensitivity of the estimator to outliers or violations in underlying assumptions [John et al., 2013].\n\u2022 FRESH is not considering the meaningfulness of interactions between features by design. Hence, in its discussed form it will not find meaningful feature combinations such as chessboard variables [Guyon and Elisseeff, 2003, Fig. 3a]. However, in our evaluation process the feature selection algorithm Boruta, which considers feature interactions, was not able to beat the performance of FRESH. Further, it is possible for FRESH\nto incorporate combinations of features and pre-defined interactions as new features themselves.\n\u2022 FRESH is scalable due to the parallelity of the feature calculation and hypothesis tests (see the two topmost tiers in Fig. 1) and can be trivially parallelized and even distributed over several computational units. In addition, the feature filter process has computational costs compared to feature calculation and significance testing. Therefore, FRESH scales linearly with the number of extracted features, length of the time series, and number of considered time series.\n\u2022 A side effect of ensuring robustness and testing features individually is that FRESH tends to extract highly correlated features, which could result in poor classification performance. We propose to combine FRESH with a subsequent PCA, which has been discussed as FRESH_PCAa in Sec. 3.5 and indeed improved the performance significantly."}, {"heading": "5.3. Feature selection of FRESH", "text": "Nilsson et al. [2007] proposed to divide feature selection into two flavors, the minimal optimal problem is finding a set consisting of all strongly relevant attributes and a subset of weakly relevant attributes such that all remaining weakly relevant attributes contain only redundant information. The all-relevant problem is finding all strongly and weakly relevant attributes. The first problem is way harder than the second, even asymptotically intractable for strictly positive distributions [Nilsson et al., 2007]. Accordingly, FRESH solves the second, easier problem as we extract every relevant feature, even though it might be a duplicate or highly correlated to another relevant feature [cf. Kursa and Rudnicki, 2011].\nYu and Liu [2003] separated feature selection algorithms into two categories, the wrapper model and the filter model. While the selection of wrapper models is based on the performance of a learning algorithm on the selected set of features, filter models use general characteristics to derive a decision about which features to keep. Filter models are further divided into feature weighting algorithms and subset search algorithms, which evaluate the goodness of features individually or through subsets. According to this definition, the feature selection part of FRESH is a filter model, more precisely, a feature weighting algorithm.\nFRESH contains a feature selection part on basis of hypothesis tests and the BenjaminiYekutieli procedure, which of course can be used as a feature selection algorithm itself. But, due to its systematic incorporation of scalable time series feature mappings and the proposed decomposition in computing tiers (Fig. 1) it is especially applicable to the needs of mass time series feature extraction and is considered as a feature extraction algorithm.\nBy applying a multiple testing algorithm, FRESH avoids the \u201clook-elsewhere effect\u201d [Gross and Vitells, 2010] which is a statistically significant observation arising by chance due to the high number of tested hypotheses. This effect triggered a recent discussions about the use of p-values in scientific publications [Wasserstein and Lazar, 2016]."}, {"heading": "5.4. Related work", "text": "There are both structural and statistical approaches to extract patterns from time series. Many statistical approaches rely on structures that allow the usage of genetic algorithms. They express the feature pattern for example as a tree [Mierswa and Morik, 2005, Geurts,\n2001, Eads et al., 2002]. While doing so, they aim for the best pattern and the most explaining features by alternating and optimizing the used feature mappings. In contrast, FRESH extracts the best fitting of a fixed set of patterns.\nAs an example for a structured pattern extraction, in [Olszewski, 2001] the authors search for six morphology types: constant, straight, exponential, sinusoidal, triangular, and rectangular phases. Those phases are detected by structure detectors which then output a new time series whose values stand for the identified structure. Based on this structure a domain-independent structural pattern recognition system is utilized to substitute the original time series signal by a known pattern. Due to its fixed patterns, FRESH can be considered to be a structured pattern extractor.\nOf course, there are other promising approaches like the combination of nearest neighbor search with Dynamic Time Warping [Wang et al., 2006], which is specialized on considering an ensemble of exactly one dedicated time series type and cannot take meta-information into account. For binary classifications it scales with O(n2t \u00b7 mtrain \u00b7 mtest) [Penserini and others, 2006] with mtrain and mtest being the number of devices in the train and test set, respectively. This approach also has the disadvantage that all data have to be transmitted to a central computing instance.\nThe extraction algorithm most similar to ours is presented by Fulcher and Jones [2014]. It applies a linear estimator with greedy search and a constant initial model to identify the most important features, which has been considered in this paper as LDA. The evaluation has shown, that FRESH outperforms the approach of Fulcher and Jones [2014]. Also, FRESH provides a more general approach to time series feature extraction, because it is able to extract features for regression tasks and not only for classification.\nDespite the applications for time series classification and regression, the feature selection approach of FRESH could be included into kernel methods [Cho and Saul, 2009] and therefore should find broad applicability in the machine learning community."}, {"heading": "6. Summary and future work", "text": "In this work, FeatuRe Extraction based on ScalableHypothesis tests (FRESH) for time series classification and regression was introduced. It combines well established feature extraction methods with a scalable feature selection based on non-parametric hypothesis tests and the Benjamini-Yekutieli procedure. FRESH is highly parallel and suitable for distributed IoT and Industry 4.0 applications like predictive maintenance or process line optimization, because it allows to consider several different time series types per label and additionally takes metainformation into account. The latter has been demonstrated on basis of a steel billets process line optimization of project iPRODICT.\nOur evaluation for UCR time series classification tasks has shown that FRESH in combination with a subsequent PCA outperforms all other feature extraction algorithms with respect to scalability and achieved accuracy. On the iPRODICT data set, it was even able to achieve a higher accuracy than a nearest neighbor search under Dynamic Time Warping.\nThe parallel nature of FRESH with respect to both feature extraction and filtering makes it highly applicable in situations where data is fragmented over a widespread infrastructure and computations cannot be performed on centralized infrastructure. Due to its robustness\nand applicability to machine learning problems in the context of IoT and Industry 4.0, we are expecting that FRESH will find widespread application.\nAcknowlegement\nThis research was funded in part by the German Federal Ministry of Education and Research under grant number 01IS14004 (project iPRODICT)."}], "references": [{"title": "A Modification of Kendall\u2019s Tau for the Case of Arbitrary Ties in Both Rankings", "author": ["Leta McKinney Adler"], "venue": "Journal of the American Statistical Association, 52(277):33\u201335,", "citeRegEx": "Adler.,? \\Q1957\\E", "shortCiteRegEx": "Adler.", "year": 1957}, {"title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing", "author": ["Yoav Benjamini", "Yosef Hochberg"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Benjamini and Hochberg.,? \\Q1995\\E", "shortCiteRegEx": "Benjamini and Hochberg.", "year": 1995}, {"title": "The control of the false discovery rate in multiple testing under dependency", "author": ["Yoav Benjamini", "Daniel Yekutieli"], "venue": "Annals of statistics,", "citeRegEx": "Benjamini and Yekutieli.,? \\Q2001\\E", "shortCiteRegEx": "Benjamini and Yekutieli.", "year": 2001}, {"title": "Feature Selection for High-Dimensional Data. Artificial Intelligence: Foundations, Theory, and Algorithms", "author": ["Ver\u00f3nica Bol\u00f3n-Canedo", "Noelia S\u00e1nchez-Maro\u00f1o", "Amparo Alonso-Betanzos"], "venue": null, "citeRegEx": "Bol\u00f3n.Canedo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bol\u00f3n.Canedo et al\\.", "year": 2015}, {"title": "The UCR Time Series Classification Archive", "author": ["Yanping Chen", "Eamonn Keogh", "Bing Hu", "Nurjahan Begum", "Anthony Bagnall", "Abdullah Mueen", "Gustavo Batista"], "venue": "http://www. cs.ucr.edu/~eamonn/time_series_data/,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Kernel methods for deep learning", "author": ["Youngmin Cho", "Lawrence K. Saul"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cho and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Cho and Saul.", "year": 2009}, {"title": "Time Series Analysis in Industrial Applications", "author": ["Maximilian Christ", "Frank Kienle", "Andreas W. Kempa-Liehr"], "venue": "In Workshop on Extreme Value and Time Series Analysis, KIT Karlsruhe,", "citeRegEx": "Christ et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Christ et al\\.", "year": 2016}, {"title": "Multiple comparisons: philosophies and illustrations. American Journal of Physiology - Regulatory", "author": ["Douglas Curran-Everett"], "venue": "Integrative and Comparative Physiology, 279(1):R1\u2013 R8,", "citeRegEx": "Curran.Everett.,? \\Q2000\\E", "shortCiteRegEx": "Curran.Everett.", "year": 2000}, {"title": "A few useful things to know about machine learning", "author": ["Pedro Domingos"], "venue": "Communications of the ACM, 55(10):78\u201387,", "citeRegEx": "Domingos.,? \\Q2012\\E", "shortCiteRegEx": "Domingos.", "year": 2012}, {"title": "Genetic Algorithms and Support Vector Machines for Time Series Classification", "author": ["Damian R. Eads", "Daniel Hill", "Sean Davis", "Simon J. Perkins", "Junshui Ma", "Reid B. Porter", "James P. Theiler"], "venue": "In Bosacchi et al., editor, Proc. SPIE 4787, Applications and Science of Neural Networks,", "citeRegEx": "Eads et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eads et al\\.", "year": 2002}, {"title": "Implementation options for time-series data", "author": ["Ramez Elmasri", "Jae Young Lee"], "venue": "URL http: //dx.doi.org/10.1007/BFb0053700", "citeRegEx": "Elmasri and Lee.,? \\Q1998\\E", "shortCiteRegEx": "Elmasri and Lee.", "year": 1998}, {"title": "Highly comparative feature-based time-series classification", "author": ["Ben D. Fulcher", "Nick S. Jones"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Fulcher and Jones.,? \\Q2014\\E", "shortCiteRegEx": "Fulcher and Jones.", "year": 2014}, {"title": "Pattern extraction for time series classification. In Principles of Data Mining and Knowledge Discovery, pages 115\u2013127", "author": ["Pierre Geurts"], "venue": null, "citeRegEx": "Geurts.,? \\Q2001\\E", "shortCiteRegEx": "Geurts.", "year": 2001}, {"title": "Trial factors for the look elsewhere effect in high energy physics", "author": ["Eilam Gross", "Ofer Vitells"], "venue": "The European Physical Journal C,", "citeRegEx": "Gross and Vitells.,? \\Q2010\\E", "shortCiteRegEx": "Gross and Vitells.", "year": 2010}, {"title": "Internet of Things (IoT): A vision, architectural elements, and future directions", "author": ["Jayavardhana Gubbi", "Rajkumar Buyya", "Slaven Marusic", "Marimuthu Palaniswami"], "venue": "Future Generation Computer Systems, 29(7):1645,", "citeRegEx": "Gubbi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gubbi et al\\.", "year": 2013}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Design Principles for Industrie 4.0 Scenarios", "author": ["M. Hermann", "T. Pentek", "B. Otto"], "venue": "In 2016 49th Hawaii International Conference on System Sciences (HICSS), page 3928,", "citeRegEx": "Hermann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2016}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["Harold Hotelling"], "venue": "Journal of educational psychology,", "citeRegEx": "Hotelling.,? \\Q1933\\E", "shortCiteRegEx": "Hotelling.", "year": 1933}, {"title": "A robustness study of parametric and non-parametric tests in model-based multifactor dimensionality reduction for epistasis detection", "author": ["Jestinah M Mahachie John", "Fra\u0146cois Van Lishout", "Elena S Gusareva", "Kristel Van Steen"], "venue": "BioData mining,", "citeRegEx": "John et al\\.,? \\Q2013\\E", "shortCiteRegEx": "John et al\\.", "year": 2013}, {"title": "A New Measure of Rank Correlation", "author": ["M.G. Kendall"], "venue": "Biometrika, 30(1/2):81,", "citeRegEx": "Kendall.,? \\Q1938\\E", "shortCiteRegEx": "Kendall.", "year": 1938}, {"title": "A Practical Approach to Feature Selection", "author": ["Kenji Kira", "Larry A. Rendell"], "venue": "In Proceedings of the Ninth International Workshop on Machine Learning,", "citeRegEx": "Kira and Rendell.,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell.", "year": 1992}, {"title": "The all relevant feature selection using random forest", "author": ["Miron B. Kursa", "Witold R. Rudnicki"], "venue": "arXiv preprint arXiv:1106.5112,", "citeRegEx": "Kursa and Rudnicki.,? \\Q2011\\E", "shortCiteRegEx": "Kursa and Rudnicki.", "year": 2011}, {"title": "The prediction and diagnosis of wind turbine faults", "author": ["Andrew Kusiak", "Wenyan Li"], "venue": "Renewable Energy, 36(1):16\u201323,", "citeRegEx": "Kusiak and Li.,? \\Q2011\\E", "shortCiteRegEx": "Kusiak and Li.", "year": 2011}, {"title": "Dissipative Solitons in Reaction Diffusion Systems, volume 70 of Springer Series in Synergetics", "author": ["Andreas W. Liehr"], "venue": null, "citeRegEx": "Liehr.,? \\Q2013\\E", "shortCiteRegEx": "Liehr.", "year": 2013}, {"title": "Some issues on scalable feature selection", "author": ["Huan Liu", "Rudy Setiono"], "venue": "Expert Systems with Applications,", "citeRegEx": "Liu and Setiono.,? \\Q1998\\E", "shortCiteRegEx": "Liu and Setiono.", "year": 1998}, {"title": "The Kolmogorov-Smirnov Test for Goodness of Fit", "author": ["Frank J. Massey"], "venue": "Journal of the American Statistical Association, 46(253):68,", "citeRegEx": "Massey.,? \\Q1951\\E", "shortCiteRegEx": "Massey.", "year": 1951}, {"title": "Automatic feature extraction for classifying audio data", "author": ["Ingo Mierswa", "Katharina Morik"], "venue": "Machine learning,", "citeRegEx": "Mierswa and Morik.,? \\Q2005\\E", "shortCiteRegEx": "Mierswa and Morik.", "year": 2005}, {"title": "An introduction to predictive maintenance", "author": ["R Keith Mobley"], "venue": "Elsevier Inc.,", "citeRegEx": "Mobley.,? \\Q2002\\E", "shortCiteRegEx": "Mobley.", "year": 2002}, {"title": "Consistent feature selection for pattern recognition in polynomial time", "author": ["Roland Nilsson", "Jos\u00e9 M. Pe\u00f1a", "Johan Bj\u00f6rkegren", "Jesper Tegn\u00e9r"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Nilsson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "FATS: Feature Analysis for Time Series", "author": ["Isadora Nun", "Pavlos Protopapas", "Brandon Sim", "Ming Zhu", "Rahul Dave", "Nicolas Castro", "Karim Pichara"], "venue": "arXiv preprint arXiv:1506.00010,", "citeRegEx": "Nun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nun et al\\.", "year": 2015}, {"title": "Generalized feature extraction for structural pattern recognition in time-series data", "author": ["Robert T. Olszewski"], "venue": "DTIC Document CMU-CS-01-108,", "citeRegEx": "Olszewski.,? \\Q2001\\E", "shortCiteRegEx": "Olszewski.", "year": 2001}, {"title": "A comparison of two machine-learning techniques to focus the diagnosis", "author": ["L Penserini", "others"], "venue": "task. Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "Penserini and others.,? \\Q2006\\E", "shortCiteRegEx": "Penserini and others.", "year": 2006}, {"title": "Feature Selection Filters Based on the Permutation Test", "author": ["Predrag Radivojac", "Zoran Obradovic", "A. Keith Dunker", "Slobodan Vucetic"], "venue": "In Machine Learning: ECML", "citeRegEx": "Radivojac et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Radivojac et al\\.", "year": 2004}, {"title": "Characteristics of hand tremor time series", "author": ["J. Timmer", "C. Gantert", "G. Deuschl", "J. Honerkamp"], "venue": "Biological Cybernetics,", "citeRegEx": "Timmer et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Timmer et al\\.", "year": 1993}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["Xiaoyue Wang", "Abdullah Mueen", "Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Eamonn Keogh"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Characteristic-Based Clustering for Time Series Data", "author": ["Xiaozhe Wang", "Kate Smith", "Rob Hyndman"], "venue": "Data Mining and Knowledge Discovery, 13(3):335\u2013364,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "The ASA\u2019s statement on p-values: context, process, and purpose", "author": ["Ronald L. Wasserstein", "Nicole A. Lazar"], "venue": "The American Statistician,", "citeRegEx": "Wasserstein and Lazar.,? \\Q2016\\E", "shortCiteRegEx": "Wasserstein and Lazar.", "year": 2016}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["Lei Yu", "Huan Liu"], "venue": "In ICML,", "citeRegEx": "Yu and Liu.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu.", "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "Promising fields of application for machine learning are the Internet of Things (IoT) [Gubbi et al., 2013] and Industry 4.", "startOffset": 86, "endOffset": 106}, {"referenceID": 16, "context": "0 [Hermann et al., 2016] environments.", "startOffset": 2, "endOffset": 24}, {"referenceID": 27, "context": "hard drives) into risk classes with respect to a specific defect [Mobley, 2002].", "startOffset": 65, "endOffset": 79}, {"referenceID": 14, "context": "In most cases the volume of the generated time series data forbids their transport to centralized databases [Gubbi et al., 2013].", "startOffset": 108, "endOffset": 128}, {"referenceID": 24, "context": "it is important to continuously select relevant features in order to deal with concept drifts caused by qualitative changes of the underlying dynamics [Liu and Setiono, 1998].", "startOffset": 151, "endOffset": 174}, {"referenceID": 22, "context": "Therefore, for industrial and other applications, one needs to combine distributed feature extraction methods with a scalable feature selection, especially for problems where several time series and meta-information have to be considered per label/target [Kusiak and Li, 2011].", "startOffset": 255, "endOffset": 276}, {"referenceID": 11, "context": "For time series classification, it proved to be efficient to apply comprehensive feature extraction algorithms and then filter the respective features [Fulcher and Jones, 2014].", "startOffset": 151, "endOffset": 176}, {"referenceID": 2, "context": "This vector is evaluated on basis of the Benjamini-Yekutieli procedure [Benjamini and Yekutieli, 2001] in order to decide which features to keep.", "startOffset": 71, "endOffset": 102}, {"referenceID": 4, "context": "The proposed algorithm is evaluated on all binary classification problems of the UCR time series classification archive [Chen et al., 2015] as well as time series data from a production line optimization project and simulated time series from a stochastic process with underlying qualitative change of dynamics [Liehr, 2013].", "startOffset": 120, "endOffset": 139}, {"referenceID": 23, "context": ", 2015] as well as time series data from a production line optimization project and simulated time series from a stochastic process with underlying qualitative change of dynamics [Liehr, 2013].", "startOffset": 179, "endOffset": 192}, {"referenceID": 11, "context": "The results are benchmarked against well-established feature selection algorithms like linear discriminant analysis [Fulcher and Jones, 2014] and the Boruta algorithm [Kursa and Rudnicki, 2011], but also against Dynamic Time Warping [Wang et al.", "startOffset": 116, "endOffset": 141}, {"referenceID": 21, "context": "The results are benchmarked against well-established feature selection algorithms like linear discriminant analysis [Fulcher and Jones, 2014] and the Boruta algorithm [Kursa and Rudnicki, 2011], but also against Dynamic Time Warping [Wang et al.", "startOffset": 167, "endOffset": 193}, {"referenceID": 34, "context": "The results are benchmarked against well-established feature selection algorithms like linear discriminant analysis [Fulcher and Jones, 2014] and the Boruta algorithm [Kursa and Rudnicki, 2011], but also against Dynamic Time Warping [Wang et al., 2013].", "startOffset": 233, "endOffset": 252}, {"referenceID": 4, "context": "Motivated by industrial applications for machine learning models Christ et al. [2016] we are extending the approach of Fulcher and Jones and propose FeatuRe Extraction based on Scalable Hypothesis tests (FRESH).", "startOffset": 65, "endOffset": 86}, {"referenceID": 10, "context": "Temporally annotated data come in three different variants [Elmasri and Lee, 1998]: Temporally invariant information (e.", "startOffset": 59, "endOffset": 82}, {"referenceID": 11, "context": "Comprehensive collections of time series feature mappings are discussed by Fulcher and Jones [2014] and Nun et al.", "startOffset": 75, "endOffset": 100}, {"referenceID": 11, "context": "Comprehensive collections of time series feature mappings are discussed by Fulcher and Jones [2014] and Nun et al. [2015]. Fulcher and Jones [2014] propose to use more than 9000 features from 1000 different feature generating algorithms.", "startOffset": 75, "endOffset": 122}, {"referenceID": 11, "context": "Comprehensive collections of time series feature mappings are discussed by Fulcher and Jones [2014] and Nun et al. [2015]. Fulcher and Jones [2014] propose to use more than 9000 features from 1000 different feature generating algorithms.", "startOffset": 75, "endOffset": 148}, {"referenceID": 32, "context": "Radivojac et al. [2004] considered a binary target Y , stating that the relevance of feature X is measured as the difference between the class conditional distributions fX|Y=0 and fX|Y=1.", "startOffset": 0, "endOffset": 24}, {"referenceID": 25, "context": "In general, the KolmogorovSmirnov (KS) test is a non-parametric and stable goodness-of-fit test that checks if two random variables A and B follow the same distribution [Massey, 1951]:", "startOffset": 169, "endOffset": 183}, {"referenceID": 19, "context": "Kendall\u2019s rank test [Kendall, 1938] checks if two continuous variables may be regarded as statistically dependent, hence naturally fitting our hypotheses from Eq.", "startOffset": 20, "endOffset": 35}, {"referenceID": 0, "context": "The calculation of the rank statistic is more complex when ties are involved [Adler, 1957], i.", "startOffset": 77, "endOffset": 90}, {"referenceID": 7, "context": "Feature significance testing When comparing multiple hypotheses simultaneously, errors in the inference tend to accumulate [Curran-Everett, 2000].", "startOffset": 123, "endOffset": 145}, {"referenceID": 2, "context": "Based on the p-values it tells which hypotheses to reject while still controlling the FDR under any dependency structure between those hypotheses [Benjamini and Yekutieli, 2001].", "startOffset": 146, "endOffset": 177}, {"referenceID": 1, "context": "The FDR as a measure of the accumulated statistical error was suggested by Benjamini and Hochberg [1995]. Later the non-parametric Benjamini-Yekutieli procedure was proposed.", "startOffset": 75, "endOffset": 105}, {"referenceID": 2, "context": "Perform the Benjamini-Yekutieli procedure under correction for dependent hypotheses [Benjamini and Yekutieli, 2001] for a FDR level of q on the collected p-values p1, .", "startOffset": 84, "endOffset": 115}, {"referenceID": 20, "context": "As long as features are considered associated with the target, they will all be selected by the filter even though many of them are highly correlated to each other [Kira and Rendell, 1992].", "startOffset": 164, "endOffset": 188}, {"referenceID": 17, "context": "This step will reduce the number of features and the obtained principal components are de-correlated, orthogonal variables [Hotelling, 1933].", "startOffset": 123, "endOffset": 140}, {"referenceID": 21, "context": "Features contained in Full_X will be filtered by applying the Boruta feature selection algorithm [Kursa and Rudnicki, 2011], or by a forward selection with a linear discriminant analysis classifier denoted LDA.", "startOffset": 97, "endOffset": 123}, {"referenceID": 35, "context": "Further, the direct classifier DTW_NN, a nearest neighbor search under the Dynamic Time Warping distance, is considered [Wang et al., 2006].", "startOffset": 120, "endOffset": 139}, {"referenceID": 11, "context": "extract features from time series [Fulcher and Jones, 2014] and Boruta is a promising feature selection algorithm that incorporates interactions between features.", "startOffset": 34, "endOffset": 59}, {"referenceID": 4, "context": "To guarantee reproducibility we use all 31 time series data sets from the UCR time series archive [Chen et al., 2015] containing a binary classification problem.", "startOffset": 98, "endOffset": 117}, {"referenceID": 4, "context": "sets with binary classification tasks [Chen et al., 2015] for each method a \u2208 A \u222a {DTW_NN} by accUCR(a) = 1 31 \u2211 i=1,.", "startOffset": 38, "endOffset": 57}, {"referenceID": 33, "context": "82] and for time series analysis in particular [Timmer et al., 1993].", "startOffset": 47, "endOffset": 68}, {"referenceID": 6, "context": "0 applications Christ et al. [2016] showed that it is very important to identify relevant time series features in an early stage of the project in order to engineer more specialized features in discussions with domain experts.", "startOffset": 15, "endOffset": 36}, {"referenceID": 6, "context": "0 applications Christ et al. [2016] showed that it is very important to identify relevant time series features in an early stage of the project in order to engineer more specialized features in discussions with domain experts. The FRESH algorithm supports this approach by applying a huge variety of established time series feature mappings to different types of time series and meta-information simultaneously and identifies relevant features in a robust manner. We observe that features extracted by FRESH contribute to a deeper understanding of the investigated problem, because each feature is intrinsically related to a distinct property of the investigated system and its dynamics. This fosters the interpretation of the extracted features by domain experts and allows for the engineering of more complex, domain specific features Christ et al. [2016] including dedicated time series models, such that their predictions in return might become a future feature mapping for FRESH.", "startOffset": 15, "endOffset": 858}, {"referenceID": 6, "context": "0 projects Christ et al. [2016]. Especially for predictive maintenance applications with limited numbers of samples and high level of noise in e.", "startOffset": 11, "endOffset": 32}, {"referenceID": 18, "context": "Here, robustness refers to the insensitivity of the estimator to outliers or violations in underlying assumptions [John et al., 2013].", "startOffset": 114, "endOffset": 133}, {"referenceID": 28, "context": "The first problem is way harder than the second, even asymptotically intractable for strictly positive distributions [Nilsson et al., 2007].", "startOffset": 117, "endOffset": 139}, {"referenceID": 13, "context": "By applying a multiple testing algorithm, FRESH avoids the \u201clook-elsewhere effect\u201d [Gross and Vitells, 2010] which is a statistically significant observation arising by chance due to the high number of tested hypotheses.", "startOffset": 83, "endOffset": 108}, {"referenceID": 36, "context": "This effect triggered a recent discussions about the use of p-values in scientific publications [Wasserstein and Lazar, 2016].", "startOffset": 96, "endOffset": 125}, {"referenceID": 26, "context": "Feature selection of FRESH Nilsson et al. [2007] proposed to divide feature selection into two flavors, the minimal optimal problem is finding a set consisting of all strongly relevant attributes and a subset of weakly relevant attributes such that all remaining weakly relevant attributes contain only redundant information.", "startOffset": 27, "endOffset": 49}, {"referenceID": 20, "context": "Kursa and Rudnicki, 2011]. Yu and Liu [2003] separated feature selection algorithms into two categories, the wrapper model and the filter model.", "startOffset": 0, "endOffset": 45}, {"referenceID": 30, "context": "As an example for a structured pattern extraction, in [Olszewski, 2001] the authors search for six morphology types: constant, straight, exponential, sinusoidal, triangular, and rectangular phases.", "startOffset": 54, "endOffset": 71}, {"referenceID": 35, "context": "Of course, there are other promising approaches like the combination of nearest neighbor search with Dynamic Time Warping [Wang et al., 2006], which is specialized on considering an ensemble of exactly one dedicated time series type and cannot take meta-information into account.", "startOffset": 122, "endOffset": 141}, {"referenceID": 31, "context": "For binary classifications it scales with O(nt \u00b7 mtrain \u00b7 mtest) [Penserini and others, 2006] with mtrain and mtest being the number of devices in the train and test set, respectively.", "startOffset": 65, "endOffset": 93}, {"referenceID": 5, "context": "Despite the applications for time series classification and regression, the feature selection approach of FRESH could be included into kernel methods [Cho and Saul, 2009] and therefore should find broad applicability in the machine learning community.", "startOffset": 150, "endOffset": 170}, {"referenceID": 8, "context": "2001, Eads et al., 2002]. While doing so, they aim for the best pattern and the most explaining features by alternating and optimizing the used feature mappings. In contrast, FRESH extracts the best fitting of a fixed set of patterns. As an example for a structured pattern extraction, in [Olszewski, 2001] the authors search for six morphology types: constant, straight, exponential, sinusoidal, triangular, and rectangular phases. Those phases are detected by structure detectors which then output a new time series whose values stand for the identified structure. Based on this structure a domain-independent structural pattern recognition system is utilized to substitute the original time series signal by a known pattern. Due to its fixed patterns, FRESH can be considered to be a structured pattern extractor. Of course, there are other promising approaches like the combination of nearest neighbor search with Dynamic Time Warping [Wang et al., 2006], which is specialized on considering an ensemble of exactly one dedicated time series type and cannot take meta-information into account. For binary classifications it scales with O(nt \u00b7 mtrain \u00b7 mtest) [Penserini and others, 2006] with mtrain and mtest being the number of devices in the train and test set, respectively. This approach also has the disadvantage that all data have to be transmitted to a central computing instance. The extraction algorithm most similar to ours is presented by Fulcher and Jones [2014]. It applies a linear estimator with greedy search and a constant initial model to identify the most important features, which has been considered in this paper as LDA.", "startOffset": 6, "endOffset": 1479}, {"referenceID": 8, "context": "2001, Eads et al., 2002]. While doing so, they aim for the best pattern and the most explaining features by alternating and optimizing the used feature mappings. In contrast, FRESH extracts the best fitting of a fixed set of patterns. As an example for a structured pattern extraction, in [Olszewski, 2001] the authors search for six morphology types: constant, straight, exponential, sinusoidal, triangular, and rectangular phases. Those phases are detected by structure detectors which then output a new time series whose values stand for the identified structure. Based on this structure a domain-independent structural pattern recognition system is utilized to substitute the original time series signal by a known pattern. Due to its fixed patterns, FRESH can be considered to be a structured pattern extractor. Of course, there are other promising approaches like the combination of nearest neighbor search with Dynamic Time Warping [Wang et al., 2006], which is specialized on considering an ensemble of exactly one dedicated time series type and cannot take meta-information into account. For binary classifications it scales with O(nt \u00b7 mtrain \u00b7 mtest) [Penserini and others, 2006] with mtrain and mtest being the number of devices in the train and test set, respectively. This approach also has the disadvantage that all data have to be transmitted to a central computing instance. The extraction algorithm most similar to ours is presented by Fulcher and Jones [2014]. It applies a linear estimator with greedy search and a constant initial model to identify the most important features, which has been considered in this paper as LDA. The evaluation has shown, that FRESH outperforms the approach of Fulcher and Jones [2014]. Also, FRESH provides a more general approach to time series feature extraction, because it is able to extract features for regression tasks and not only for classification.", "startOffset": 6, "endOffset": 1737}], "year": 2016, "abstractText": "The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features. The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics.", "creator": "LaTeX with hyperref package"}}}