{"id": "1610.07809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "How Document Pre-processing affects Keyphrase Extraction Performance", "abstract": "The SemEval-2010 benchmark dataset has once again brought attention to the task of automatic keyphrase extraction, a set of scientific articles that have been automatically converted from PDF to plaintext and therefore require careful pre-processing to ensure that unclear spans of text do not adversely affect keyphrase extraction. Previous work has described a wide range of document pre-processing techniques, but their impact on the overall performance of keyphrase extraction models is still unexplored.", "histories": [["v1", "Tue, 25 Oct 2016 09:59:13 GMT  (28kb)", "http://arxiv.org/abs/1610.07809v1", "Accepted at the COLING 2016 Workshop on Noisy User-generated Text (WNUT)"]], "COMMENTS": "Accepted at the COLING 2016 Workshop on Noisy User-generated Text (WNUT)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["florian boudin", "hugo mougard", "damien cram"], "accepted": false, "id": "1610.07809"}, "pdf": {"name": "1610.07809.pdf", "metadata": {"source": "CRF", "title": "How Document Pre-processing affects Keyphrase Extraction Performance", "authors": ["Florian Boudin"], "emails": ["firstname.lastname@univ-nantes.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n07 80\n9v 1\n[ cs\n.C L\n] 2\n5 O\nct 2\n01 6\nThe SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing."}, {"heading": "1 Introduction", "text": "Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset (Kim et al., 2010). This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations, footnotes) that require special handling, so as to not hinder the performance of keyphrase extraction systems. In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010; Zervanou, 2010) to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar (Nguyen and Luong, 2010). Under such conditions, it may prove difficult to draw firm conclusions about which keyphrase extraction model performs best, as the impact of preprocessing on overall performance cannot be properly quantified.\nWhile previous work clearly states that efficient document preprocessing is a prerequisite for the extraction of high quality keyphrases, there is, to our best knowledge, no empirical evidence of how preprocessing affects keyphrase extraction performance. In this paper, we re-assess the performance of several state-of-the-art keyphrase extraction models at increasingly sophisticated levels of preprocessing. Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text. More precisely, our contributions are:\n\u2022 We show that performance variation across keyphrase extraction systems is, at least in part, a function of the (often unstated) effectiveness of document preprocessing.\n\u2022 We empirically show that supervised models are more resilient to noise, and point out that the performance gap between baselines and top performing systems is narrowing with the increase in preprocessing effort.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n\u2022 We compare the previously reported results of several keyphrase extraction models with that of our re-implementation, and observe that baseline performance is underestimated because of the inconsistence in document preprocessing.\n\u2022 We release both a new version of the SemEval-2010 dataset1 with preprocessed documents and our implementation of the state-of-the-art keyphrase extraction models2 using the pke toolkit (Boudin, 2016) for use by the community."}, {"heading": "2 Dataset and Preprocessing", "text": "The SemEval-2010 benchmark dataset (Kim et al., 2010) is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool3. The only preprocessing applied is a systematic dehyphenation at line breaks4 and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators.\nLong documents such as those in the SemEval-2010 benchmark dataset are notoriously difficult to handle due to the large number of keyphrase candidates (i.e. phrases that are eligible to be keyphrases) that the systems have to cope with (Hasan and Ng, 2014). Furthermore, noisy textual content, whether due to format conversion errors or to unusable elements (e.g. equations), yield many spurious keyphrase candidates that negatively affect keyphrase extraction performance. This is particularly true for systems that make use of core NLP tools to select candidates, that in turn exhibit poor performance on degraded text. Filtering out irrelevant text is therefore needed for addressing these issues.\nIn this study, we concentrate our effort on re-assessing keyphrase extraction performance on three increasingly sophisticated levels of document preprocessing described below.\nLevel 1: We process each input file with the Stanford CoreNLP suite (Manning et al., 2014)5. We use the default parameters and perform tokenization, sentence splitting and Part-Of-Speech (POS) tagging.\nLevel 2: Similarly to (Nguyen and Luong, 2010; Lopez and Romary, 2010), we retrieve6 the original PDF files from the ACM Digital Library. We then extract the enriched7 textual content of the PDF files using an Optical Character Recognition (OCR) system8, and perform document logical structure detection using ParsCit (Kan et al., 2010)9. We use the detected logical structure to remove authorassigned keyphrases and select only relevant elements: title, headers, abstract, introduction, related work, body text10 and conclusion. We finally apply a systematic dehyphenation at line breaks and run the Stanford CoreNLP suite.\nLevel 3: As pointed out by (Treeratpituk et al., 2010; Nguyen and Luong, 2010; Wang and Li, 2010; Eichler and Neumann, 2010; El-Beltagy and Rafea, 2010), considering only the keyphrase dense parts of the scientific articles allows to improve keyphrase extraction performance. Accordingly we follow previous work and further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion. Here, the idea is to achieve the best compromise between search space (number of candidates) and maximum performance (recall).\n1https://github.com/boudinfl/semeval-2010-pre 2https://github.com/boudinfl/pke 3pdftotext, http://www.foolabs.com/xpdf/ 4Valid hyphenated forms may have their hyphen removed by this process. 5Use use Stanford CoreNLP v3.6.0. 6To ensure consistency, articles were manually collected. 7Additional information such as font format or spacial layout is also extracted. 8We use Omnipage v18, http://www.nuance.com/omnipage 9We use ParsCit v110505.\n10We further filter out tables, figures, captions, equations, notes, copyright and references.\nTable 1 shows the average number of sentences and words along with the maximum possible recall for each level of preprocessing. The maximum recall is obtained by computing the fraction of the reference keyphrases that occur in the documents. We observe that the level 2 preprocessing succeeds in eliminating irrelevant text by significantly reducing the number of words (-19%) while maintaining a high maximum recall (-2%). Level 3 preprocessing drastically reduce the number of words to less than a quarter of the original amount while interestingly still preserving high recall."}, {"heading": "3 Keyphrase Extraction Models", "text": "We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task (Kim et al., 2010). We note that two of the systems are supervised and rely on the training set to build their classification models. Document frequency counts are also computed on the training set11. Stemming12 is applied to allow more robust matching. The different keyphrase extraction models are briefly described below:\nTF\u00d7IDF: we re-implemented the TF\u00d7IDF n-gram based baseline computed by the task organizers. We use 1, 2, 3-grams as keyphrase candidates and filter out those shorter than 3 characters, containing words made of only punctuation marks or one character long13.\nKea (Witten et al., 1999): keyphrase candidates are 1, 2, 3-grams that do not begin or end with a stopword14. Keyphrases are selected using a na\u0131\u0308ve bayes classifier15 with two features: TF\u00d7IDF and the relative position of first occurrence.\nTopicRank (Bougouin et al., 2013): keyphrase candidates are the longest sequences of adjacent nouns and adjectives. Lexically similar candidates are clustered into topics and ranked using TextRank (Mihalcea and Tarau, 2004). Keyphrases are produced by extracting the first occurring candidate of the highest ranked topics.\nKP-Miner (El-Beltagy and Rafea, 2010): keyphrase candidates are sequences of words that do not contain punctuation marks or stopwords. Candidates that appear less than three times or that first occur beyond a certain position are removed16. Candidates are then weighted using a modified TF\u00d7IDF formula that account for document length.\nWINGNUS (Nguyen and Luong, 2010): keyphrase candidates are simplex nouns and noun phrases detected using a set of rules described in (Kim and Kan, 2009). Keyphrases are then selected using a na\u0131\u0308ve bayes classifier with the optimal set of features found on the training set17: TF\u00d7IDF, relative position of first occurrence and candidate length in words.\n11For more reliable estimates, we rely on level 2 counts when experimenting with level 3. 12We use the Porter stemmer in nltk. 13This filtering process is also applied to the other models. 14We use the stoplist in nltk, http://www.nltk.org 15We use the Multinomial Naive Bayes classifier from scikit-learn with default parameters, http://scikit-learn.org 16To better see the impact of preprocessing, we do not consider the cutoff parameter in our experiments. The least allowable seen frequency parameter is set to 2 which is the optimal value found on the training set. 17The optimal set of features in (Nguyen and Luong, 2010) also include the term frequency of substrings, but we observed a significant drop in performance when this feature is included.\nEach model uses a distinct keyphrase candidate selection method that provides a trade-off between the highest attainable recall and the size of set of candidates. Table 2 summarizes these numbers for each model. Syntax-based selection heuristics, as used by TopicRank and WINGNUS, are better suited to prune candidates that are unlikely to be keyphrases. As for KP-miner, removing infrequent candidates may seem rather blunt, but it turns out to be a simple yet effective pruning method when dealing with long documents. For details on how candidate selection methods affect keyphrase extraction, please refer to (Wang et al., 2014).\nApart from TopicRank that groups similar candidates into topics, the other models do not have any redundancy control mechanism. Yet, recent work has shown that up to 12% of the overall error made by state-of-the-art keyphrase extraction systems were due to redundancy (Hasan and Ng, 2014; Boudin, 2015). Therefore as a post-ranking step, we remove redundant keyphrases from the ranked lists generated by all models. A keyphrase is considered redundant if it is included in another keyphrase that is ranked higher in the list."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental settings", "text": "We follow the evaluation procedure used in the SemEval-2010 competition and evaluate the performance of each model in terms of f-measure (F) at the top N keyphrases18 . We use the set of combined author- and reader-assigned keyphrases as reference keyphrases. Extracted and reference keyphrases are stemmed to reduce the number of mismatches."}, {"heading": "4.2 Results", "text": "The performances of the keyphrase extraction models at each preprocessing level are shown in Table 3. Overall, we observe a significant increase of performance for all models at levels 3, confirming that document preprocessing plays an important role in keyphrase extraction performance. Also, the difference of f-score between the models, as measured by the standard deviation \u03c31, gradually decreases with the increasing level of preprocessing. This result strengthens the assumption made in this paper, that performance variation across models is partly a function of the effectiveness of document preprocessing.\nSomewhat surprisingly, the ordering of the two best models reverses at level 3. This showcases that rankings are heavily influenced by the preprocessing stage, despite the common lack of details and analysis it suffers from in explanatory papers. We also remark that the top performing model, namely KP-Miner, is unsupervised which supports the findings of (Hasan and Ng, 2014) indicating that recent unsupervised approaches have rivaled their supervised counterparts in performance.\nIn an attempt to quantify the performance variation across preprocessing levels, we compute the standard deviation \u03c32 for each model. Here we see that unsupervised models are more sensitive to noisy input, as revealed by higher standard deviations. We found two main reasons for this. First, using multiple discriminative features to rank keyphrase candidates adds inherent robustness to the models. Second, the supervision signal helps models to disregard noise.\nIn Table 4, we compare the outputs of the five models by measuring the percentage of valid keyphrases that are retrieved by all models at once for each preprocessing level. By these additional results, we aim\n18Scores are computed using the evaluation script provided by the SemEval-2010 organizers.\nto assess whether document preprocessing smoothes differences between models. We observe that the overlap between the outputs of the different models increases along with the level of preprocessing. This suggests that document preprocessing reduces the effect that the keyphrase extraction model in itself has on overall performance. In other words, the singularity of each model fades away gradually with increase in preprocessing effort."}, {"heading": "4.3 Reproducibility", "text": "Being able to reproduce experimental results is a central aspect of the scientific method. While assessing the importance of the preprocessing stage for five approaches, we found that several results were not reproducible, as shown in Table 5.\nWe note that the trends for baselines and high ranking systems are opposite: compared to the published results, our reproduction of top systems under-performs and our reproduction of baselines over-performs. We hypothesise that this stems from a difference in hyperparameter tuning, including the ones that the preprocessing stage makes implicit. Competitors have strong incentives to correctly optimize hyperparameters, to achieve a high ranking and get more publicity for their work while competition organizers might have the opposite incentive: too strong a baseline might not be considered a baseline anymore.\nWe also observe that with this leveled preprocessing, the gap between baselines and top systems is much smaller, lessening again the importance of raw scores and rankings to interpret the shared task results and emphasizing the importance of understanding correctly the preprocessing stage."}, {"heading": "5 Additional experiments", "text": "In the previous sections, we provided empirical evidence that document preprocessing weighs heavily on the outcome of keyphrase extraction models. This raises the question of whether further improvement might be gained from a more aggressive preprocessing. To answer this question, we take another step beyond content filtering and further abridge the input text from level 3 preprocessed documents using an unsupervised summarization technique. More specifically, we keep the title and abstract intact, as they are the two most keyphrase dense parts within scientific articles (Nguyen and Luong, 2010), and select only the most content bearing sentences from the remaining contents. To do this, sentences are ordered using TextRank (Mihalcea and Tarau, 2004) and the less informative ones, as determined by their TextRank scores normalized by their lengths in words, are filtered out.\nFinding the optimal subset of sentences from already shortened documents is however no trivial task as maximum recall linearly decreases with the number of sentences discarded. Here, we simply set the reduction ratio to 0.865 so that the average maximum recall on the training set does not lose more than 5%. Table 6 shows the reduction in the average number of sentences and words compared to level 3 preprocessing.\nThe performances of the keyphrase extraction models at level 4 preprocessing are shown in Table 7. We note that two models, namely TopicRank and TF\u00d7IDF, lose on performance. These two models mainly rely on frequency counts to rank keyphrase candidates, which in turn become less reliable at level 4 because of the very short length of the documents. Other models however have their f-scores once again increased, thus indicating that further improvement is possible from more reductive document preprocessing strategies."}, {"heading": "6 Conclusion", "text": "In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.\nGiven our findings, we recommend that future works use a common preprocessing to evaluate the interest of keyphrase extraction approaches. For this reason we make the four levels of preprocessing used in this study available for the community."}], "references": [{"title": "Reducing over-generation errors for automatic keyphrase extraction using integer linear programming", "author": ["Florian Boudin"], "venue": "In Proceedings of the ACL 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction,", "citeRegEx": "Boudin.,? \\Q2015\\E", "shortCiteRegEx": "Boudin.", "year": 2015}, {"title": "pke: an open source python-based keyphrase extraction toolkit", "author": ["Florian Boudin"], "venue": "In Proceedings of COLING 2016: System Demonstrations,", "citeRegEx": "Boudin.,? \\Q2016\\E", "shortCiteRegEx": "Boudin.", "year": 2016}, {"title": "Topicrank: Graph-based topic ranking for keyphrase extraction", "author": ["Florian Boudin", "B\u00e9atrice Daille"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Bougouin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bougouin et al\\.", "year": 2013}, {"title": "Dfki keywe: Ranking keyphrases extracted from scientific articles", "author": ["Eichler", "Neumann2010] Kathrin Eichler", "G\u00fcnter Neumann"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Eichler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eichler et al\\.", "year": 2010}, {"title": "Kp-miner: Participation in semeval", "author": ["El-Beltagy", "Rafea2010] Samhaa R. El-Beltagy", "Ahmed Rafea"], "venue": null, "citeRegEx": "El.Beltagy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "El.Beltagy et al\\.", "year": 2010}, {"title": "Automatic keyphrase extraction: A survey of the state of the art", "author": ["Hasan", "Ng2014] Kazi Saidul Hasan", "Vincent Ng"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Hasan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasan et al\\.", "year": 2014}, {"title": "Logical structure recovery in scholarly articles with rich document features", "author": ["Kan et al.2010] Min-Yen Kan", "Minh-Thang Luong", "Thuy Dung Nguyen"], "venue": "Int. J. Digit. Library Syst.,", "citeRegEx": "Kan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kan et al\\.", "year": 2010}, {"title": "Re-examining automatic keyphrase extraction approaches in scientific articles", "author": ["Kim", "Kan2009] Su Nam Kim", "Min-Yen Kan"], "venue": "In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles", "author": ["Kim et al.2010] Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Kim et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Humb: Automatic key term extraction from scientific articles in grobid", "author": ["Lopez", "Romary2010] Patrice Lopez", "Laurent Romary"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Lopez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lopez et al\\.", "year": 2010}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Textrank: Bringing order into texts", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": "Proceedings of EMNLP", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Wingnus: Keyphrase extraction utilizing document logical structure", "author": ["Nguyen", "Luong2010] Thuy Dung Nguyen", "Minh-Thang Luong"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "Seerlab: A system for extracting keyphrases from scholarly documents", "author": ["Pradeep Teregowda", "Jian Huang", "C. Lee Giles"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Treeratpituk et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Treeratpituk et al\\.", "year": 2010}, {"title": "Sjtultlab: Chunk based method for keyphrase extraction", "author": ["Wang", "Li2010] Letian Wang", "Fang Li"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "How preprocessing affects unsupervised keyphrase extraction", "author": ["Wang et al.2014] Rui Wang", "Wei Liu", "Chris Mcdonald"], "venue": "In Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing - Volume 8403,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Kea: Practical automatic keyphrase extraction", "author": ["Witten et al.1999] Ian H. Witten", "Gordon W. Paynter", "Eibe Frank", "Carl Gutwin", "Craig G. Nevill-Manning"], "venue": "In Proceedings of the Fourth ACM Conference on Digital Libraries, DL", "citeRegEx": "Witten et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Uvt: The uvt term extraction system in the keyphrase extraction task", "author": ["Kalliopi Zervanou"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Zervanou.,? \\Q2010\\E", "shortCiteRegEx": "Zervanou.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset (Kim et al., 2010).", "startOffset": 143, "endOffset": 161}, {"referenceID": 13, "context": "In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010; Zervanou, 2010) to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar (Nguyen and Luong, 2010).", "startOffset": 136, "endOffset": 198}, {"referenceID": 17, "context": "In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010; Zervanou, 2010) to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar (Nguyen and Luong, 2010).", "startOffset": 136, "endOffset": 198}, {"referenceID": 1, "context": "\u2022 We release both a new version of the SemEval-2010 dataset1 with preprocessed documents and our implementation of the state-of-the-art keyphrase extraction models2 using the pke toolkit (Boudin, 2016) for use by the community.", "startOffset": 187, "endOffset": 201}, {"referenceID": 8, "context": "The SemEval-2010 benchmark dataset (Kim et al., 2010) is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers).", "startOffset": 35, "endOffset": 53}, {"referenceID": 10, "context": "Level 1: We process each input file with the Stanford CoreNLP suite (Manning et al., 2014)5.", "startOffset": 68, "endOffset": 90}, {"referenceID": 6, "context": "We then extract the enriched7 textual content of the PDF files using an Optical Character Recognition (OCR) system8, and perform document logical structure detection using ParsCit (Kan et al., 2010)9.", "startOffset": 180, "endOffset": 198}, {"referenceID": 13, "context": "Level 3: As pointed out by (Treeratpituk et al., 2010; Nguyen and Luong, 2010; Wang and Li, 2010; Eichler and Neumann, 2010; El-Beltagy and Rafea, 2010), considering only the keyphrase dense parts of the scientific articles allows to improve keyphrase extraction performance.", "startOffset": 27, "endOffset": 152}, {"referenceID": 8, "context": "We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task (Kim et al., 2010).", "startOffset": 272, "endOffset": 290}, {"referenceID": 16, "context": "Kea (Witten et al., 1999): keyphrase candidates are 1, 2, 3-grams that do not begin or end with a stopword14.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "TopicRank (Bougouin et al., 2013): keyphrase candidates are the longest sequences of adjacent nouns and adjectives.", "startOffset": 10, "endOffset": 33}, {"referenceID": 15, "context": "For details on how candidate selection methods affect keyphrase extraction, please refer to (Wang et al., 2014).", "startOffset": 92, "endOffset": 111}, {"referenceID": 0, "context": "Yet, recent work has shown that up to 12% of the overall error made by state-of-the-art keyphrase extraction systems were due to redundancy (Hasan and Ng, 2014; Boudin, 2015).", "startOffset": 140, "endOffset": 174}, {"referenceID": 2, "context": "Table 5: Difference in f-score between our re-implementation and the original scores reported in (Hasan and Ng, 2014; Bougouin et al., 2013).", "startOffset": 97, "endOffset": 140}], "year": 2016, "abstractText": "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.", "creator": "LaTeX with hyperref package"}}}