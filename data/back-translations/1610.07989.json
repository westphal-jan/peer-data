{"id": "1610.07989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Process Discovery using Inductive Miner and Decomposition", "abstract": "This report presents a submission to the Process Discovery Contest. The competition is dedicated to evaluating tools and techniques for discovering business process models from event logs. The goal is to compare the efficiency of techniques to find process models that provide an appropriate balance between \"overfitting\" and \"underfitting.\" In the Process Discovery Contest, process discovery is transformed into a classification task with a training set and a test set, with a process model having to decide whether traces fit or not. In this report, we first show how we use two discovery techniques, namely: inductive miner and decomposition, to discover process models from the training set using the ProM tool. Second, we show how we use repeat results to 1) verify the retrievability of models and 2) classify invisible traces (in test logs) as fitting or not. Then, we discuss the classification results of model validation results with the impact of the model protocols on the selection of their complexity reports.", "histories": [["v1", "Tue, 25 Oct 2016 17:58:54 GMT  (9082kb,D)", "http://arxiv.org/abs/1610.07989v1", "A Submission to the Process Discovery Contest @ BPM2016"]], "COMMENTS": "A Submission to the Process Discovery Contest @ BPM2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raji ghawi"], "accepted": false, "id": "1610.07989"}, "pdf": {"name": "1610.07989.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Raji Ghawi"], "emails": [], "sections": [{"heading": null, "text": "In this report, we first show how we use two discovery techniques, namely: Inductive Miner and Decomposition, to discover process models from the training set using ProM tool. Second, we show how we use replay results to 1) check the rediscoverability of models, and to 2) classify unseen traces (in test logs) as fitting or not. Then, we discuss the classification results of validation logs, and the complexity of discovered models, and their impact on the selection of models for submission. The report ends with the pictures of the submitted process models."}, {"heading": "1 Introduction", "text": "Process Mining is a recently emerging discipline that links data analysis with process management. One of the central tasks of Process Mining is Process Discovery, where knowledge is extracted from event logs (readily available in today\u2019s information systems) in order to discover real processes [2]. Several tools, techniques, and algorithms have been proposed to discover business process models from event logs. The quality of discovered models and the efficiency of discovery techniques vary according to several criteria including: fitness, precision, generalization and simplicity. This document is our submission to the Process Discovery Contest organized by the IEEE CIS Task Force on Process Mining1, and which is collocated with the BPM Conference in Rio de Janeiro in September 2016. The contest is dedicated to the assessment of tools and techniques that discover business process models from event logs. The objective is to compare the efficiency of techniques to discover process models that provide a proper balance between \u201coverfitting\u201d and \u201cunderfitting\u201d. A process model is overfitting (the event log) if it is too restrictive, disallowing behavior which is part of the underlying process. Conversely, it is underfitting (the reality) if it is not restrictive enough, allowing behavior which is not part of the underlying process. Within the Process-Discovery contest, 10 event logs are provided to the contestants as a training set. These event logs are generated from business process models that show different behavioral\n1http://www.win.tue.nl/ieeetfpm/doku.php\nar X\niv :1\n61 0.\n07 98\n9v 1\n[ cs\n.A I]\n2 5\nO ct\ncharacteristics. The process models are not disclosed to the contestants. The contestants should use the training event logs to discover process models that are the closest to the original process models, in term of balancing between \u201coverfitting\u201d and \u201cunderfitting\u201d. To assess this balance, the contest has a classification perspective where a test event log will be used. The test event log contains traces representing real process behavior and traces representing behavior not related to the process. A model is as good in balancing \u201coverfitting\u201d and \u201cunderfitting\u201d as it is able to correctly classify the traces in the \u201ctest\u201d event log:\n\u2022 Given a trace representing real process behavior, the model should classify it as allowed.\n\u2022 Given a trace representing a behavior not related to the process, the model should classify it as disallowed.\nWith a classification view, the winner is the contestant who can classify correct the largest number of traces in all the test event logs. Moreover, in order to help the contestants tune their discovered models, two \u201cvalidation\u201d sets are made available in April and May 2016. In our submission to the contest, we discovered process models using two techniques: Inductive Miner [3] and Decomposition based discovery [4, 5, 6]. We chose these techniques because they guarantee rediscoverability; that is, models discovered using those techniques fit (generate) all the traces in their original training event logs. We used ProM tool (http://www.promtools.org/) to apply those techniques in discovering our submitted models. In the reminder of this report, we first show in Section 2 how do we use two discovery techniques, namely: Inductive Miner and Decomposition, to discover process models from the training set using ProM tool. Second, we present in Section 3 how we replay an event log on a process model. Then, we show how we use replay results:\n1. to check the rediscoverability of models (Section 3.1), and\n2. to classify unseen traces (in validation and/or test logs) as fitting or not (Section 3.2).\nSection 4 is dedicated for a discussion about the classification results of April and May validation logs, the complexity of discovered models, and how these two points affect our selection of models for submission. In the end of the report, we show the pictures of our submitted models."}, {"heading": "2 Process Discovery", "text": "We used two well known discovery techniques: Inductive Miner [3] and Decomposed Process Mining [4, 5, 6]. We used those techniques to discover process models for each one of the 10 training event logs. However, since only one process model should be submitted for every event log, we decided to include in our submission a mix of those models. Rationally, we have chosen those techniques because they guarantee a level of rediscoverability, namely the ability of discovering again the models used to generate the traces of the event log. We consider that rediscoverability property is essential to accomplish the task of correctly classify new unseen traces. The models for processes 1, 2, 4, 8 and 9 are discovered using Inductive Miner. We used ProM Lite 2 for this group of models. Other models - for processes 3, 5, 6, 7, and 10 - are discovered using Decomposition technique. We used ProM 6.5.1 3 for this group of models. The justification of this selection of process models is given in Section 4.\n2http://www.promtools.org/doku.php?id=promlite 3http://www.promtools.org/doku.php?id=prom651\nIn the reminder of this section, we describe the step-by-step procedure to discover process models using the above mentioned techniques."}, {"heading": "2.1 Discovery using Inductive Miner", "text": "Inductive Miner [3] is a discovery approach to construct a Process Tree for a given log. The main advantages of this approach are:\n\u2022 All discovered models correspond to sound, block-structured workflow (WF) net systems.\n\u2022 The model always fits, i.e., the model can generate the traces in the log.\nThe approach works recursively with divide and conquer strategy: split log, construct part of process tree, and proceed with handling split parts of log separately. A process tree can be directly transformed into a Petri Net. However, ProM has a plugin that can mine a Petri net directly using Inductive Miner technique. To discover a model (Petri net) from an event log with Inductive Miner technique, we used ProM Lite4 and followed the following steps:\n1. First, the log (in XES format) is loaded onto ProM and used. Then, in Actions window, the plugin \u201cMine Petri net with Inductive Miner\u201d by Leemans is selected (see Figure 1).\n2. A settings page appears then (Figure 2). For Variant, we select Inductive Miner. For Event Classifier, we select Event Name.\n3. Finally, we get a Petri net discovered using Inductive Miner. The discovered model can be visualized using Graphviz Petri net visualization (Figure 3)."}, {"heading": "2.2 Discovery using Decomposition", "text": "Decomposition [4, 5, 6] is a process mining technique that discovers an accepting Petri net5 from a log using decomposition. First, the log will be split into a number of sublogs. Second, a subnet\n4The Inductive Miner plugin is also available in ProM 6.5.1. However, we get different models when we used Inductive Miner in ProM Lite and ProM 6.5.1 (apparently, due to implementation discrepancies). Therefore, we would emphasize that we used ProM Lite for the submitted models that are discovered using Inductive Miner (as mentioned above: models 1, 2, 4, 8, and 9).\n5Accepting Petri net is a labeled Petri net with an initial marking and a collection of final markings\nwill be discovered (using the given discovery algorithm) for every sublog. Third, these subnets will be merged into a single accepting Petri net. Fourth, known reduction techniques will be applied on this accepting Petri net, and the result is returned. To discover a model from an event log using Decomposition technique, we used ProM 6.5.1 (this technique is not available in ProM Lite).\n1. First, the log (in XES format) is loaded onto ProM and used. Then, in Actions window, the plugin \u201cDiscovery using Decomposition\u201d is selected (see Figure 4). Actually, there are two available plugins in ProM 6.5.1 for discovery using Decomposition: an interactive plugin that allows for custom settings, and a batch one that uses some default settings. We used the batch plugin, hence default settings are used; for instance, the underlying discovery algorithm is ILP Miner [7][5].\n2. We get an accepting Petri net discovered using Decomposition. It can be visualized using different visualizations such as Visualize Accepting Petri Net (Dot) (Figure 5).\n3. Finally, we convert the discovered accepting Petri net into a \u2018regular\u2019 Petri net using Unpack Accepting Petri Net plugin.\nOur models are all regular Petri nets that are exported as PNML files."}, {"heading": "3 Replaying Semantics", "text": "To replay an event log on a process model, we used the ProM plugin Replay a Log on Petri Net for Conformance Analysis (available in both ProM Lite and ProM 6.5.1, with identical results obtained). We need replay for two tasks:\n1. Verify that the discovered model can generate its originating event log (rediscoverability).\n2. Classify new unseen traces (in validation and test datasets) as fitting or not-fitting with respect to the discovered models.\nTo replay a log on a process model:\n1. First, in Workspace page, both the log and the model (Petri net) are selected and used.\n2. Second, in Actions page, the plugin Replay a Log on Petri Net for Conformance Analysis is selected (Figure 6).\n3. Then, a dialog appears that allow for mapping between the transitions of the Petri net and the event classes of the log (Figure 7). We need only to choose Event Name as classifier; thus each transition is matched with the event class having the same name.\n4. The next dialog allows us to select a replay algorithm. We use the following settings (as shown in Figure 8):\n\u2022 The purpose of replay \u2013 measuring fitness. \u2022 Penalize improper completion: yes. \u2022 Suggested algorithm \u2013 A* Cost-based Fitness Express, assuming at most 32767 tokens\nin each place.\n5. The last page of the dialog is for selecting the costs of particular deviation (Figure 9). Again, we keep the defaults: the cost is always 1 except for tau transitions where the cost is 0.\nThose are the required steps to replay a log on a process model. As mentioned above, we use those steps for rediscoverability checking and for classifying unseen traces."}, {"heading": "3.1 Rediscoverability Checking", "text": "Our choice of discovery techniques is mainly based on the rediscoverability property. That is, we chose Inductive Miner and Decomposition techniques because they both guarantee this property. Therefore, once we discover a model using one of those techniques, as discussed in Section 2, we replay the originating/training event log on the discovered model to make sure that all the traces in the log fit properly in the model. To do so, we follow the replay steps mentioned above using the training log and the discovered model. The result of the replay can be visualized in different ways. For instance, using Model Projected with Alignments choice, the model can be visualized with alignments on top of it, as shown in Figure 10. In this visualization, we can take a look at the Global Statistics within the inspector window. Among the displayed statistics, we are mainly interested in three statistics: Trace Fitness, MoveModel Fitness, and Move-Log Fitness. The values of those statistics give an indicator about the rediscoverability property. When those global statistics are all equal to 1, this means that all the traces fit in the model; thus the model is able to rediscover the event log. Otherwise, if anyone of those statistics has a value less than 1, then there are some traces in the log that don\u2019t fit in the model. In our submission, all the models perfectly satisfy the rediscoverability property."}, {"heading": "3.2 Classification of Unseen Traces", "text": "In the context of the Process Discovery Contest, process discovery is turned into a classification task with a training set and a test set. Moreover, two validation sets are made available to the contestants to allow them tune their discovered models. A process model needs to decide whether traces are fitting or not. For this classification task, a test or validation log is replayed on the discovered model. Again, we used the ProM plugin Replay a Log on Petri Net for Conformance Analysis according to the steps mentioned earlier. When the replay results are obtained, we visualize them using Project Alignment to Log choice. This will display log-model alignments for each case/trace, as shown in Figure 11. For each trace, a table of statistics is shown to the left; and a colored picture of the trace alignment is shown to the right. The meaning of colors is illustrated in a legend shown on the right-top of the screen (Figure 12). Among the displayed statistics, we are interested in: Trace fitness, Move-Log fitness, and MoveModel fitness; and we use them in the classification task. We classify each trace as fitting if, and only if, those three values are perfect (equal to 1). Otherwise, when anyone of those values is less than 1, then we classify the trace as not-fitting. For example, table 1 shows those values for replaying Log-1 of April validation dataset over our\nmodel-1 (discovered using Inductive Miner).\nIn the same sense, the colored picture of the trace alignment can be also used to determine fitness. If the picture consists of green and gray alignments only, then the trace is classified as fitting. Otherwise, if the picture also contains other colors (like pink and/or yellow), then the trace is classified as not-fitting."}, {"heading": "4 Discussion", "text": "In our solution to the Process Discovery Contest, we used Inductive Miner and Decomposition techniques - as presented in Section 2 - to discover process models6 from the 10 training logs. Both groups of models are then used to replay again the training logs - as presented in Section 3.1- to ensure rediscoverability. We found that all our process models satisfy the rediscoverability property. Then, we replayed April and May validation event logs over both groups of process models - processes discovered using Inductive Miner and processes discovered using Decomposition. The\n6The pictures of process models of both groups are available at: https://github.com/rajighawi/Process_ Discovery_Contest_2016\nreplay results are then used to classify the traces of the validation logs as fitting or non-fitting - as presented in section 3.2. We communicated the obtained classifications7 with the contest organizers, who replied by indicating how many traces have been correctly classified for each one of the 10 logs. Table 2 shows the classification results of April validation logs using the two groups of process models: Inductive Miner and Decomposition. Analogue classification results of May logs are shown in Table 3.\nA quick look at these classification results tells that Decomposition models classify better than Inductive Miner models. That is, out of 200 traces in April logs, 192 are correctly classified using Decomposition models versus 183 using Inductive Miner models. Similarly, in May logs, 194 are correctly classified using Decomposition models versus 179 using Inductive Miner models However, with a more careful examination of the classification results, we made the following observations:\n\u2022 Logs 1, 2, 4, and 9 are always classified correctly: 20 out of 20 traces are classified correctly.\n\u2022 Log 8 has a better classification using Inductive Miner model than using Decomposition model (April: 15 > 14, May: 18 > 16).\n7The detailed classifications are available at: https://github.com/rajighawi/Process_Discovery_Contest_ 2016\n\u2022 Other logs - namely 3, 5, 6, 7, and 10 - has a better classification using Decomposition models than using Inductive Miner models.\nNow, based on these observations, we should decide which models shall we include in our submission to the contest8. From the second and third observation, we can easily decide to consider Inductive Miner model for log: 8, and Decomposition models for logs: 3, 5, 6, 7 and 10. However, the first observation tells us that neither one of the two groups of process models has a preference over the other with respect to the classification of logs: 1, 2, 4 and 9. Therefore, we have to consider another quality criteria of process models, namely: simplicity. Actually, the Inductive Miner models are generally more simple than Decomposition ones. One can clearly see this by looking at the pictures of both groups of models (see Appendix A). However, in order to prove this claim we computed different complexity metrics of process models [8, 9]. Table 4 shows the complexity metrics for both Inductive Miner and Decomposition process models. These metrics are: ECaM : Extended Cardoso Metric, ECyM : Extended Cyclomatic Metric, |E| and |V |: number of Edges and Nodes in the reachability graph, |A|, |P | and |T |: number of Arcs, Places, and Transitions in the Net, and SM : Structuredness Metric. Based on the values listed in Table 4 we can see that the Inductive Miner models are, in general, simpler than Decomposition ones. For instance, the average structuredness of Decomposition models is roughly one order of magnitude higher than for Inductive Miner models.\nNow, given that for logs: 1, 2, 4 and 9, there is no preference of a particular group of models based on classification results; and given that Inductive Miner models are simpler than Decomposition\n8As mentioned earlier, only one model per log must be submitted.\nmodels, we decided to consider Inductive Miner models for these logs, based on Occam\u2019s Razor principle, In summary, our submitted models to the contest are: models 1, 2, 4, 8 and 9 of Inductive Miner models, and models 3, 5, 6, 7 and 10 of Decomposition models. The pictures of these process models are depicted in Appendix A Based on this selection of process models, the contest organizers replayed the June test logs. The reported classifications are shown in Table 5, whereas the classification results are shown in Table 6. Our selection of models correctly classified 192 out of 200 traces. The organizers confirmed later that this submission gave us the second place in the contest among 14 participant teams."}, {"heading": "A Process Models", "text": ""}], "references": [{"title": "Process Discovery Contest @ BPM 2016, https://www.win.tue.nl/ieeetfpm/doku.php?id=shared:process_discovery_contest", "author": ["J. Carmona", "M. de Leoni", "B. Depair", "T. Jouck"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Process Mining: Discovery, Conformance and Enhancement of Business Processes", "author": ["Van der Aalst", "W.M.P"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Discovering Block-Structured Process Models from Event Logs - a Constructive Approach", "author": ["S. Leemans", "D. Fahland", "W. van der Aalst"], "venue": "Petri Nets 2013. LNCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "W.M.P.: Decomposed Process Mining: The ILP case", "author": ["H.M.W. Verbeek", "Van der Aalst"], "venue": "BPI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Decomposed Process Mining with DivideAndConquer", "author": ["H.M.W. Verbeek"], "venue": "Proceedings of the BPM Demo Sessions 2014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "W.M.P.: Complexity Metrics for Workflow Nets", "author": ["K.B. Lassen", "Van der Aalst"], "venue": "Information and Software Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Metrics in Process Discovery", "author": ["Blum", "F. Rojas"], "venue": "Technical Report TR/DCC-2015-6,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Abstract This report presents a submission to the Process Discovery Contest [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "One of the central tasks of Process Mining is Process Discovery, where knowledge is extracted from event logs (readily available in today\u2019s information systems) in order to discover real processes [2].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "In our submission to the contest, we discovered process models using two techniques: Inductive Miner [3] and Decomposition based discovery [4, 5, 6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "In our submission to the contest, we discovered process models using two techniques: Inductive Miner [3] and Decomposition based discovery [4, 5, 6].", "startOffset": 139, "endOffset": 148}, {"referenceID": 4, "context": "In our submission to the contest, we discovered process models using two techniques: Inductive Miner [3] and Decomposition based discovery [4, 5, 6].", "startOffset": 139, "endOffset": 148}, {"referenceID": 2, "context": "We used two well known discovery techniques: Inductive Miner [3] and Decomposed Process Mining [4, 5, 6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "We used two well known discovery techniques: Inductive Miner [3] and Decomposed Process Mining [4, 5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 4, "context": "We used two well known discovery techniques: Inductive Miner [3] and Decomposed Process Mining [4, 5, 6].", "startOffset": 95, "endOffset": 104}, {"referenceID": 2, "context": "1 Discovery using Inductive Miner Inductive Miner [3] is a discovery approach to construct a Process Tree for a given log.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "2 Discovery using Decomposition Decomposition [4, 5, 6] is a process mining technique that discovers an accepting Petri net5 from a log using decomposition.", "startOffset": 46, "endOffset": 55}, {"referenceID": 4, "context": "2 Discovery using Decomposition Decomposition [4, 5, 6] is a process mining technique that discovers an accepting Petri net5 from a log using decomposition.", "startOffset": 46, "endOffset": 55}, {"referenceID": 3, "context": "We used the batch plugin, hence default settings are used; for instance, the underlying discovery algorithm is ILP Miner [7][5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "However, in order to prove this claim we computed different complexity metrics of process models [8, 9].", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "However, in order to prove this claim we computed different complexity metrics of process models [8, 9].", "startOffset": 97, "endOffset": 103}], "year": 2016, "abstractText": "This report presents a submission to the Process Discovery Contest [1]. The contest is dedicated to the assessment of tools and techniques that discover business process models from event logs. The objective is to compare the efficiency of techniques to discover process models that provide a proper balance between \u201coverfitting\u201d and \u201cunderfitting\u201d. In the context of the Process Discovery Contest, process discovery is turned into a classification task with a training set and a test set; where a process model needs to decide whether traces are fitting or not. In this report, we first show how we use two discovery techniques, namely: Inductive Miner and Decomposition, to discover process models from the training set using ProM tool. Second, we show how we use replay results to 1) check the rediscoverability of models, and to 2) classify unseen traces (in test logs) as fitting or not. Then, we discuss the classification results of validation logs, and the complexity of discovered models, and their impact on the selection of models for submission. The report ends with the pictures of the submitted process models.", "creator": "LaTeX with hyperref package"}}}