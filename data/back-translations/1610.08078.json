{"id": "1610.08078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Dis-S2V: Discourse Informed Sen2Vec", "abstract": "The vector representation of sentences is important for many word processing tasks where sentences are grouped, classified, or classified. Recently, it has been shown that the distributed representation of sentences learned from neural models from unlabeled data exceeds the traditional sack-of-words representation. However, most of these learning methods only consider the content of a sentence and largely disregard the relationships between sentences in a discourse.", "histories": [["v1", "Tue, 25 Oct 2016 20:19:35 GMT  (2195kb,D)", "http://arxiv.org/abs/1610.08078v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["tanay kumar saha", "shafiq joty", "naeemul hassan", "mohammad al hasan"], "accepted": false, "id": "1610.08078"}, "pdf": {"name": "1610.08078.pdf", "metadata": {"source": "CRF", "title": "Dis-S2V: Discourse Informed Sen2Vec", "authors": ["Tanay Kumar Saha", "Shafiq Joty", "Naeemul Hassan", "Mohammad Al Hasan"], "emails": ["tksaha@iupui.edu}", "alhasan@iupui.edu}", "{sjoty@qf.org.qa}", "{nhassan@olemiss.edu}", "permissions@acm.org."], "sections": [{"heading": null, "text": "In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that consider the content of a sentence as well as inter-sentence relations. We first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to exploit the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (i) using the adjacency relations of a node, and (ii) using a stochastic sampling method which is more flexible in sampling neighbors of a node. The second approach uses a regularizer to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach. The models leverage the computational power of multi-core CPUs to achieve fine-grained computational efficiency. We make our code publicly available upon acceptance.\nCCS Concepts \u2022Computing methodologies\u2192 Learning latent representations; \u2022Information systems \u2192 Clustering and classification; Summarization;\nKeywords Sen2Vec; distributed representation of sentences; feature learning; discourse; retrofitting; classification; ranking; clustering\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WWW \u201917 Perth, Western Australia. c\u00a9 2016 ACM. ISBN 978-1-4503-2138-9. . . $15.00\nDOI: 10.1145/1235"}, {"heading": "1. INTRODUCTION AND MOTIVATION", "text": "Many sentence-level text processing tasks rely on representing the sentences using fixed-length vectors. For example, classifying sentences into topics using a statistical classifier like Maximum Entropy would require the sentences to be represented by vectors. Similarly, for the task of ranking sentences based on their importance in the text using a ranking model like LexRank [3] or SVMRank [9], one needs to first represent the sentences with fixed-length vectors. The most common method uses a bag-of-words or a bag-of-ngrams representation, where each dimension of the vector is computed by some form of term frequency statistics (e.g., tf*idf ).\nRecently, distributed representations, in the form of dense real-valued vectors, learned by neural network models from unlabeled data, has been shown to outperform the traditional bag-of-words representation [12]. Distributed representations encode the semantics of linguistic units and yield better generalization [15, 2]. However, most existing methods to devise distributed representation for sentences consider only the content of a sentence, and disregard relations between sentences in a text by and large [12, 6]. But, sentences rarely stand of their own in a well-written text. On a finer level, sentences are connected with each other by certain logical relations (e.g., elaboration, contrast) to express the meaning as a whole [8]. On a coarser level, sentences in a text address a common topic, often covering multiple subtopics; i.e., sentences are also topically related [22]. Our main hypothesis in this paper is that distributed representation methods for sentences should not only consider the content of the sentence but also the inter-sentence relations.\nRecent work on learning distributed representations for words has shown that semantic relations between words (e.g., synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28]. Our work in this paper is reminiscent of this line of research with a couple of crucial differences. Firstly, we are interested in representation of sentences as opposed to words, for which such resources are not readily available. Secondly, our main goal is to incorporate discourse information in the form of inter-sentence relations as opposed to semantic relations between words. These differences posit a number of new research challenges: (i) how can we represent inter-sentence relations? (ii) how can we effectively exploit the inter-sentence relations in our representation learning model? and (iii) how can we evaluate our model?\nIn this paper, we propose novel models for learning distributed representations for sentences that consider not only\nar X\niv :1\n61 0.\n08 07\n8v 1\n[ cs\n.C L\n] 2\n5 O\nct 2\n01 6\ncontent of a sentence but relations among sentences. We represent inter-sentence relations using a network, where nodes represent sentences and edges represent similarity between the corresponding sentences. Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].\nWe explore two different approaches to exploit the information in the network. In our first approach, we learn sentence vectors using existing content-based models, i.e., the Sen2Vec model proposed in [12]. Then we retrofit these vectors using information encoded in network to encourage the new vectors to be similar to the vectors of related sentences and similar to their prior representations. The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5]. In our second approach, we alter the objective function of the original Sen2Vec model with a regularizer or prior that encourages related sentences in the network to have similar vector representations. Therefore, in this approach the vectors are learned from scratch by jointly modeling the content of the sentences and the relations between sentences.\nDifferent approaches to evaluate sentence representation methods have been proposed including sentence-level prediction tasks (e.g., sentiment classification, paraphrase identification) and sentence-pair similarity computation task [6, 12]. Since existing representation methods encode vectors for each sentence independently, these evaluation methodologies are trivial. In contrast, our learning methods exploit inter-sentence similarities, which can be constrained by document boundaries. Therefore, we require datasets containing documents with sentence-level annotations.\nWe evaluate our models on three different types of tasks: classification, clustering and ranking. In particular, we consider the tasks of classifying and clustering sentences into topics, and of ranking sentences in a document to create an extractive summary of the document (i.e., by selecting the top-ranked sentences). There are standard datasets with document-level topic annotations (e.g., Reuters-21578, 20 Newsgroups). However, to our knowledge, no dataset exists with topic annotations at the sentence level. We generate sentence-level topic annotations from the document-level ones by selecting subsets of sentences that can be considered as representatives of the document and label them with the same document-level topic label. We use the standard DUC 2001 and 2002 datasets to evaluate our models on the summarization task, where we compare the system-generated summaries with the human-authored summaries.\nOur experimental results on these tasks demonstrate that the models which induce information encoded in the discourse network in the form of inter-sentence relations during learning (i.e., the regularized models) consistently outperform the content-only baseline by a good margin in all tasks, whereas the retrofitted models perform comparatively better on the classification and on the clustering tasks.\n2. METHODOLOGY\nLet G = (V,E,W ) be a discourse graph, where a node v \u2208 V represents a sentence and the edge weight w(u,v) \u2208W reflects some form of similarity between sentences u and v. A sentence v is a sequence of words (v1, v2 \u00b7 \u00b7 \u00b7 vm), each coming from a vocabulary \u2126, i.e., vi \u2208 \u2126. We define N(v) as the set of neighbors of a sentence v in G. Let \u03c6 : V \u2192 Rd be the mapping function from sentences to their distributed representations, i.e., real-valued vectors of d dimensions. Our goal is to learn \u03c6 by exploiting information from two different sources: (i) the content of the sentence, v = (v1, v2 \u00b7 \u00b7 \u00b7 vm); and (ii) the neighborhood of the sentence, N(v).\nIn the following, we first describe how we construct the discourse graph G for our problem (Section 2.1). We then briefly describe existing sentence representation methods that consider only the sentence contents (Section 2.2). In Section 2.3, we demonstrate a network-based representation method that considers only the neighborhood information. Finally, we present our retrofitting (Section 2.4) and regularized methods (Section 2.5) that incorporate both content and network neighborhood information into a single model."}, {"heading": "2.1 Discourse Graph Formulation", "text": "Let D = {v1,v2, \u00b7 \u00b7 \u00b7 ,vn} be the set of sentences in our dataset, which constitutes the nodes in the discourse graph. The edge weights w(vi,vj) are computed by measuring the similarity between the corresponding sentences, \u03c3(vi,vj). Here \u03c3 denotes a similarity metric (e.g., Cosine, Jaccard).\nFor constructing the network, we distinguish between two types of edges: (i) intra-document edges, i.e., edges between sentences of the same document, and (ii) across-document edges, i.e., edges between sentences of different documents. Different thresholding parameters can be set depending on whether the edges are intra\u2013document or across-documents."}, {"heading": "2.2 Content-based Model (S2V)", "text": "We use the Sen2Vec model proposed in [12] as our baseline model, which is trained solely based on the contents of the sentences. This approach concatenates the vectors learned by the two models shown in Figure 1: (a) a distributed memory (DM) model, and (b) a distributed bag of words (DBOW) model. In the DM model, every sentence in the dataset D is represented by a d dimensional vector in a shared lookup matrix S \u2208 Rn\u00d7d. Similarly, every word in the vocabulary \u2126 is represented by a d dimensional vector in another shared lookup matrix L \u2208 R|\u2126|\u00d7d. Given an input sentence v = (v1, v2 \u00b7 \u00b7 \u00b7 vm), the corresponding sentence vector from S and the corresponding word vectors from L\nare averaged to predict the next word in a context. More formally, let \u03c6 denote the mapping from sentence and word ids to their respective vectors in S and L, the DM model minimizes the following objective (negative log likelihood):\nJ(\u03c6) = \u2212 m\u2212k\u2211 t=k logP (vt|v; vt\u2212k+1, \u00b7 \u00b7 \u00b7 , vt\u22121) (1)\n= \u2212 m\u2212k\u2211 t=k log exp(\u03c9(vt) T z)\u2211 i exp(\u03c9(vi) T z) (2)\nwhere z is the average of \u03c6(v), \u03c6(vt\u2212k+1), \u00b7 \u00b7 \u00b7 , \u03c6(vt\u22121) input vectors, and \u03c9(vt) is the output vector representation of word vt \u2208 \u2126. The sentence vector \u03c6(v) is shared across all (sliding window) contexts extracted from the same sentence, thus acts as a distributed memory.\nIn stead of predicting the next word in the context, the DBOW model predicts the words in the context independently given the sentence id as input. More formally, DBOW minimizes the following negative log likelihood objective:\nJ(\u03c6) = \u2212 m\u2212k\u2211 t=k t\u2211 j=t\u2212k+1 logP (vj |v) (3)\n= \u2212 m\u2212k\u2211 t=k t\u2211 j=t\u2212k+1 log exp(\u03c9(vj) T\u03c6(v))\u2211 i exp(\u03c9(vi) T\u03c6(v)) (4)"}, {"heading": "2.3 Network-based Model (N2V)", "text": "The network-based representation model considers only the neighborhood information of the nodes. We use the recently proposed Node2Vec method [5]. It uses the skipgram model of Word2Vec [14] with the intuition that nodes within a graph context (or neighborhood) should have similar representations. The negative log likelihood objective of the skip-gram model for graphs can be defined as:\nJ(\u03c6) = \u2212 \u2211 v\u2208V logP (N(v)|\u03c6(v)) (5)\n= \u2212 \u2211 v\u2208V \u2211 ni\u2208N(v) logP (ni|\u03c6(v)) (6)\n= \u2212 \u2211 v\u2208V \u2211 ni\u2208N(v) log exp(\u03c9(ni) T\u03c6(v))\u2211 x\u2208V exp(\u03c9(x) T\u03c6(v)) (7)\nwhere as before, \u03c6 and \u03c9 denote the input and the output vector representations of the nodes (sentences). The neighboring nodes N(v) forms the context for node v. Node2Vec uses a biased random walk which adaptively combines breadth first search (BFS) and depth first search (DFS) to find the neighborhood of a node. The walk attempts to capture two properties of a graph often used for prediction tasks in networks: (i) homophily and (ii) structural equivalence. According to homophily, nodes in the same group or community should have similar representations (e.g., sentences in a topic). Structural equivalence suggests that nodes with similar structural roles (hub, bridge) should have similar representations (e.g., central sentences). In a real-world network, nodes exhibit mixed properties.\nThe controlled random walk in Node2Vec interpolates between BFS and DFS to generate multiple samples of N(v) for each source node v. Let u be a node just traversed from p (Figure 2) in a random walk that started from a source node v. The walk is controlled by (unnormalized) transition probabilities \u03c4u,x = \u00b5r,f (p,x).wv,x, where\n\u00b5r,f (p,x) =  1 r , if \u03b4p,x = 0\n1, if \u03b4p,x = 1 1 f if \u03b4p,x = 2\n(8)\nwhere \u03b4p,x is the distance from node p to x. The return parameter r controls the likelihood of revisiting a node; large value will reduce duplicate samples and small value will keep the search local. The forward parameter f controls the distance covered by the walk; for f > 1, the walk will act as BFS and for f < 1, it will act as DFS. Given a graph, we can precompute \u03c4 by keeping three possible values corresponding to Equation 8 for each edge transition.\nWe can apply Node2Vec directly to the discourse graph to learn vector representations for the sentences. This adaptation of the model considers only inter-sentence relations defined by the graph. Since the graph in our case is formed by matching the contents of the sentences, this method indirectly takes contents of individual sentences into account.\nIn order to incorporate contents directly, we could have a variant of the Node2Vec model, where in stead of randomly initializing the vectors, we initialize them with the precomputed vectors from the content-based model described in Section 2.2. However, we hypothesize that a more effective approach is to consider both sources of information simultaneously in a controllable joint framework."}, {"heading": "2.4 Dis-S2V by Retrofitting", "text": "We explore the general idea of retrofitting [4] to incorporate information from both the content and the neighborhood of a node (or a sentence) in a joint learning framework. Let \u03c6\u2032(v) denote the vector representation for sentence v that has already been learned by our content-based method in Section 2.2. Our aim is to retrofit this vector on the discourse graph G such that the revised vector \u03c6(v): (i) is also similar to the prior vector \u03c6\u2032(v), and (ii) is similar to the vectors of its adjacent nodes \u03c6(u). To this end, we define the following objective function to minimize: J(\u03c6) = \u2211 v\u2208V \u03b1v||\u03c6(v)\u2212\u03c6\u2032(v)||2+ \u2211 (u,v)\u2208E \u03b2vWu,v||\u03c6(u)\u2212\u03c6(v)||2 (9) where \u03b1 values control the strength to which the algorithm should match the prior vectors, and \u03b2 values control the degree of smoothness based on the graph similarity. The quadratic cost in Equation 9 is convex in \u03c6, and has a closed form solution [23]. The closed form expression requires an inversion operation, which could be expensive for big graphs. A more efficient way is to use the Jacobi method, an online\nAlgorithm 1: Jacobi method for retrofitting.\nInput : - Graph G = (V,E,W ) - Prior vectors \u03c6\u2032 - Probabilities \u03b1v and \u03b2v for all v \u2208 V Output: Retrofitted vectors \u03c6\n1. \u03c6\u2190 \u03c6\u2032 // initialization 2. repeat\nfor all v \u2208 V do \u03c6(v)\u2190 \u03b1v\u03c6 \u2032(v)+ \u2211\nu(\u03b2vWv,u+\u03b2uWu,v)\u03c6(u) \u03b1v+ \u2211 u(\u03b2vWv,u+\u03b2uWu,v)\nend\nuntil convergence;\nalgorithm to solve the Equation iteratively. Algorithm 1 gives a pseudocode, which has the following update:\n\u03c6(v)\u2190 \u03b1v\u03c6\n\u2032(v) + \u2211 u (\u03b2vWv,u + \u03b2uWu,v)\u03c6(u)\n\u03b1v + \u2211 u (\u03b2vWv,u + \u03b2uWu,v)\n(10)\nIf for all v\u2208V , \u03b1v=\u03b1 and \u03b2v=\u03b2, Equation 10 simplifies to:1\n\u03c6(v)\u2190 \u03b1\u03c6\u2032(v) +\n\u2211 uWu,v \u03c6(u)\n\u03b1+ \u2211 uWu,v\n(11)\nWe can show a correspondence between this method and a biased random walk over G, where at any vertex v \u2208 V , the walk faces two options: (i) with probability \u03b1v, the walk stops and returns the prior vector \u03c6\u2032(v); and (ii) with probability \u03b2v, the walk continues to one of v\u2019s neighbors u with probability proportional to Wu,v. Formally, the random walk view has the following iterative update [23]:\n\u03c6(v)\u2190 \u03b1v\u03c6\u2032(v) + \u03b2v Wu,v\u2211 uWu,v \u03c6(u) (12)\n2.4.1 Retrofitting with Node2Vec The above retrofitting method is limited to only first-order\nproximity, i.e., only immediate neighbors are considered in the objective function (see Equation 9). However, preserving only local structures may not be sufficient for many applications [25]. For example, sentences, which are not directly connected but share neighbors, are likely to be similar, thus should have similar vector representations.\nAs described in Section 2.3, the controlled random walk in Node2Vec gives us more flexibility in exploring the network structure by combining BFS and DFS search strategies adaptively. We therefore leverage the random walk of Node2Vec to generate neighborhood samples N(v), but modify the objective in Equation 6 for retrofitting as follows: J(\u03c6) = \u2212 \u2211 v\u2208V [ \u03b1v \u2211 ni\u2208N(v) logP (\u03c6(ni)|\u03c6(v))+\u03b2v||\u03c6(v)\u2212\u03c6\u2032(v)||2 ]\n(13) The first component of the model minimizes the cross entropy (or KL divergence) with the neighbor distribution, where the second component encourages the induced vectors to be close to their prior values.\n1Under this condition, one free parameter is enough to control the relative strength of the two components, thus \u03b2 is excluded in Equation 11."}, {"heading": "2.5 Dis-S2V by Regularization", "text": "Rather than retrofitting the vectors from the contentbased model on the discourse graph as a post-processing step, we can incorporate the neighborhood information directly into the objective function of the content-based model as a regularizer, and learn the Dis-S2V vectors in a single step. We define the following objective to minimize:\nJ(\u03c6) = \u2211 v\u2208V [ L(v) + \u03b2 \u2211 (u,v)\u2208E Wu,v||\u03c6(u)\u2212 \u03c6(v)||2 ] (14)\nwhere the first component L(v) refers to the negative log likelihood loss of the content based model described in Section 2.2, i.e., Equation 1 for DM and Equation 3 for DBOW. The second component is the graph smoothing regularizer with \u03b2 being the regularization strength. In our experiments we use the DBOW model as our content-based model. Since this model learns the vectors from scratch in one shot by considering information from both sources, the two components can be better adjusted to produce better quality vectors.\n2.5.1 Incorporating Word Semantics Notice that the content-based models also train word vec-\ntors as a byproduct (i.e., \u03c6 and \u03c9). Recent work has shown that lexical semantic relations (e.g., synonymy, hypernymy) encoded in semantic lexicons (e.g., WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28]. Therefore, in addition to the discourse graph, semantic lexicons can be used as a word-level information source to refine the word vectors in the model, which should in turn refine the sentence vectors. Following previous work [27, 28], we first convert any kind of semantic lexicon into a network, where semantically similar words defined by the lexicon become neighbors. Then we induce this information as another word-level graph smoothing factor in the objective function of the DBOW model.\n2.5.2 Other Possible Extensions Notice that the regularizer in Equation 14 considers only\nfirst-order neighborhood. One possible extension would be to consider the neighborhood sampled by the stochastic sampling method of Node2Vec, which is more flexible. Another interesting extension of the model would be, rather than defining the regularizer as a weighted distance between two vectors, we can use the objective of Node2Vec as the regularizer and minimize the combined negative log likelihood."}, {"heading": "3. EXPERIMENTAL SETTINGS", "text": "In this section, we describe our experimental setup. Specifically, we explain the tasks, datasets, metrics and experimental protocol that we use to demonstrate our models\u2019 efficacy."}, {"heading": "3.1 Task Description and Datasets", "text": "We experiment on three different sentence level tasks: (i) classification, (ii) clustering, and (iii) ranking. These are the three fundamental information system tasks and good performance over the tasks will indicate the robustness of our models in a wide range of applications. For classification (or clustering), we measure the performance of our models in classifying (or grouping) sentences based on their topics, whereas in Ranking, we investigate our model\u2019s performance in ranking the most central topical sentences by evaluating it on an extractive summarization task [17].\n3.1.1 Datasets for Classification and Clustering We use 20-Newsgroups and Reuters-21578 datasets for\nthe classification and clustering tasks. These datasets are publicly available and widely used in these tasks. 20 Newsgroups: This is a collection of approximately 20, 000 news documents.2 These documents are organized into 20 different topics. Some of these topics are closely related (e.g., talk.politics.guns and talk.politics.mideast), while others are diverse in nature (e.g., misc.forsale and soc. religion.christian). We selected 8 diverse topics in our experiments from the 20 topics. The selected topics are: talk.politics. mideast, comp.graphics, soc.religion.christian, rec. autos, sci.space, talk.politics.guns, rec.sport.baseball, and sci.med. Reuters-21578 : The is a collection of articles which appeared on the Reuters newswire in 1987.3 It has 21578 documents covering 672 topics. We use \u201cModApte\u201d for trainingtest split and select documents only from the most 8 frequent topics. The topics are: acq, crude, earn, grain, interest, money-fx, ship, and trade. Table 1 shows some basic statistics on the resultant datasets. Generating Sentence-level Topic Annotations: For our evaluation on sentence-level topic classification and clustering tasks, we have to create topic annotations at the sentence-level from the document-level topic labels. One option is to assume all the sentences of a document to have the same topic label as the document. However, this naive assumption propagates a lot of noises. Although sentences in a document collectively address a common topic, not all sentences are directly linked to that topic, rather they play supporting roles. To minimize this noise, we use an extractive (unsupervised) summarizer, LexRank [3], to select the top P% sentences as representatives of the document and label them with the same topic label as the document; see Sec. 3.3 for details about the LexRank method. In our experiments, we use the S2V vectors from the content-based model (Sec. 2.2) to represent a node (sentence) in LexRank and we set P = 2. The fourth column in Table 1 shows the total number of topic annotated sentences in each dataset that we use for classification and clustering evaluation tasks.\n3.1.2 Datasets for Extractive Summarization For summarization, we use the benchmark datasets from\nDUC 2001\u221202, where the tasks were to generate a 10-words summary and a 100-words summary for each document in the datasets.4 Table 2 shows some basic statistics about the datasets. DUC-2001 has 486 documents whereas DUC-2002 has 471 documents. The average number of sentences per document is 40 and 28, respectively. For each document, 2-3 short reference (human authored) summaries are available, which we use as gold summaries in our evaluation. The human authored summaries are of approximately 100 words.\n2http://qwone.com/ jason/20Newsgroups/ 3http://kdd.ics.uci.edu/databases/reuters21578/ 4http://www-nlpir.nist.gov/projects/duc/guidelines\nOn average, the datasets have 2.17 and 2.04 human summaries per document, respectively."}, {"heading": "3.2 Discourse Network Statistics", "text": "For constructing our discourse graph, we use the vectors learned from S2V(Sec. 2.2). Every sentence is eligible to connect with any other sentence in the dataset. However, we restrict the connections to make the graph reasonably sparse for computational efficiency, while still ensuring that the graph is informative enough. We achieve this by imposing two kinds of constraints based on the similarity values.\nFirst, we restrict the edges by setting thresholds for intraand across-document connections; sentences in a document are connected only if their cosine similarity is above the intra-document threshold, similarly, sentences across documents are connected only if their similarity is above the across-document threshold. We use 0.5 and 0.8 for intraand across-document thresholds, respectively. Second, we further prune the network by keeping only the top 20 similar neighbors for each node. Table 3 shows the basic statistics of the resultant discourse graphs for all of our datasets."}, {"heading": "3.3 The Extractive Summarizer", "text": "We use the graph-based LexRank algorithm [3] to rank the sentences in a document based on their importance. To get the summary sentences of a document, we first build a weighted graph, where the nodes represent the sentences of the document and the edges represent the cosine similarity between learned representations (using one of the models in Section 2) of the two corresponding sentences. To make the graph sparse, we avoid edges with weight less than 0.10. We run the PageRank algorithm [18] on the graph to determine the rank of each sentence in the document, and thereby extract the key sentences as summary of that document. The dumping factor in PageRank was set to 0.85."}, {"heading": "3.4 Evaluation Metrics", "text": "Topic Classification: We use precision (P), recall (R), accuracy (Acc), F1 measure (F1), and Cohen\u2019s Kappa (\u03ba) as evaluation metrics for the classification task. Topic Clustering: For measuring clustering performance, we use various measures such as homogeneity score (H), completeness score (C), V-measure [20] (V), and adjusted mutual information (AMI) score. The idea of homogeneity is that the class distribution within each cluster should be skewed to a single class. Completeness score determines\nwhether all members of a given class are assigned to the same cluster. The harmonic mean of these two measures is the Vmeasure. AMI measures the agreement of two assignments, in our case the clustering and the class distribution. It is normalized against chance. All these measures are bounded by [0, 1]. Higher score means a better clustering. Summarization: We use the widely used automatic evaluation metric ROUGE [13] to evaluate the system-generated summaries. ROUGE is a recall oriented metric that computes n-gram recall between a candidate summary and a set of reference (human authored) summaries. Among the variants, ROUGE-1 (i.e., n = 1) has been shown to correlate well with human judgments for short summaries [13]. Therefore, we only report ROUGE-1 in this paper. The configuration for ROUGE in our case is -c 99 -2 -1 -r 1000 -w 1.2 -n 4 -m -s -a -l 10 (or 10). Depending on the task at hand, ROUGE collects the first 10 or 100 words from the summary after removing the stop words to compare with the corresponding reference summaries."}, {"heading": "3.5 Experiment Protocols", "text": "Model Variants: In Table 4, we briefly describe our core models and their variants. For detailed discussion, please see the referred sections. Model Settings: All the models except the (iterative) retrofitted ones (i.e., IT-w, IT-uw) were trained with stochastic gradient descent (SGD), where the gradient is obtained via backpropagation. We used subsampling of frequent words and negative sampling in the classification layer as described in [15], which give significant speed-ups in training.\nTable 5 shows the hyper-parameters of our models and the set of values we tuned with for these hyper-parameters. For each dataset, we randomly selected 20% documents from the whole set to form a held-out validation set on which we tune these parameters. To find the best parameter values, we optimize F1 for classification, AMI for clustering and ROUGE-1 for summarization on the validation set. We also choose the parameters internal to the Node2Vec in a similar\nfashion. Table 6 shows the optimal values of each hyperparameter for the four datasets. We evaluate our models on the test set with these optimal values. We run each test experiment five times and take the average to avoid any random behavior appearing in the results. We observed the standard deviation to be quite low."}, {"heading": "4. RESULTS", "text": "Tables 7, 8 and 9 present the results on topic classification, topic clustering and summarization tasks, respectively. In the tables, we organize our models into 4 blocks; see also Table 4 for a brief description of these blocks.\nBlock I shows results for the baseline S2V model described in Sec. 2.2, which is purely a content-based model. S2V model concatenates the vectors learned by DM and DBOW models. The concatenated model performed better than individual ones. We show all other models\u2019 relative performance with respect to this S2V baseline.\nBlock II presents the results for N2V and N2V-i described in Sec. 2.3. N2V-i initializes the vectors with the representation learned from the content-based model, S2V. In Block III, we present the results of \u201cDis-S2V by Retrofitting\u201d models described in Sec. 2.4. The models in this category are: N2V-r, IT-uw, and IT-w. N2V-r uses a regularizer in the original N2V objective, which restricts new vector representation not to go far away from the already learned contentbased representation. IT-uw and IT-w retrofit the vectors in such a way that the vectors become closer to the one-hop neighborhood in the discourse network. Finally, in Block IV, we show results for \u201cDis-S2V by Regularization\u201d models described in Sec. 2.5. These models induce information from the discourse network during training, DICTREG-uw and DICTREG-w additionally use the lexicon, WordNet."}, {"heading": "4.1 Classification Results", "text": "The results in Table 7 demonstrate significant performance improvement of our models in 8-class classification task compared to the baseline, S2V. For Reuters-21578 dataset, REGuw from Block IV becomes the best performer and it improves over the baseline in R, P, Acc and \u03ba metric by 3.60%, 3.80%, 3.40%, 3.42% and 4.32%, respectively. The interrater metric (\u03ba) improves over 4%, which is a good indicator that REG-uw has better agreement with the gold label assigner. For 20 Newsgroups, REG-w becomes the best performer across all the metrics. A closer observation will reveal that REG-w also performs competitively with REG-uw in Reuters-21578 dataset. This is intuitive as these models use information from both the content and the discourse network. Moreover, as expected, N2V model performs poorly as it only uses information from the content network. N2V-i performs better than N2V as it incorporates the content vector during initialization. N2V-r, IT-uw, and IT-w perform better than S2V as all of these models control the informa-\nReuters-21578 20 Newsgroups\nR P F1 Acc \u03ba R P F1 Acc \u03ba\nI S2V 84.20 84.00 83.80 84.24 79.84 79.00 79.80 79.00 79.23 75.90\nII N2V (\u2212) 1.40 (\u2212) 1.60 (\u2212) 1.40 (\u2212) 1.50 (\u2212) 1.94 (\u2212) 2.00 (\u2212) 2.80 (\u2212) 2.00 (\u2212) 2.15 (\u2212) 2.46 N2V-i (\u2212) 0.20 (\u2212) 0.20 (+) 0.00 (\u2212) 0.20 (\u2212) 0.18 (\u2212) 1.00 (\u2212) 1.80 (\u2212) 1.00 (\u2212) 1.49 (\u2212) 1.71\nIII\nN2V-r (+) 2.60 (+) 2.80 (+) 2.40 (+) 2.52 (+) 3.24 (+) 3.00 (+) 2.60 (+) 3.00 (+) 2.98 (+) 3.46\nIT-uw (+) 2.40 (+) 2.80 (+) 1.80 (+) 2.20 (+) 2.80 (+) 2.00 (+) 2.00 (+) 2.00 (+) 2.08 (+) 2.40\ntion induction into the new vector in the form of (i) being closer to the representation learned from the content-based model, or (ii) being closer to the neighbors in the discourse graph. Overall, the models which can effectively use the content and information in discourse network wins over models, which only use content or just the network."}, {"heading": "4.2 Clustering Results", "text": "The clustering results in Table 8 clearly shows that all our models outperform the baseline by a good margin, and the models from Block IV perform significantly better than the models in Block III in all the metrics. The adjusted mutual information score (AMI) is greater than 0.40 for all of our models, indicating that our models are better than the random cluster assignments. One important observation is that N2V-i becomes the best performer with a relative improvement around 13% compared to the baseline. Note that, N2V-i incorporates information from content-only representation in initializing its model, then it gets updated based on the network neighborhood. N2V, which uses only the network neighborhood, also performs better than the content-only baseline. This indicates that the neighborhood information is quite beneficial for clustering tasks. These promising results one more time indicate the efficacy of our representation model over the content-only models.\nFigure 3 shows a visualization of clustering generated using S2V and N2V-i models for only three (3) categories out of the 8 categories. From the figure, it is clear that N2V-i was able to retrieve all three clusters whereas S2V is merging two out of three clusters to form one big cluster."}, {"heading": "4.3 Ranking Results", "text": "From Table 9, we observe that over all the summarization tasks, N2V-r, REG-uw, REG-w, DICTREG-uw, and DICTREG-w consistently outshine the baseline S2V model. For DUC 2001, in 10-length summary, N2V-r wins by 2.21% over S2V, whereas for 100-length summary, N2V-i wins over S2V by a margin of 5.16%. For DUC 2002, REG-w and REG-uw perform better than all other models. For length 10, REG-w improves over S2V by 2.5%, whereas REG-uw gets around 3.26% improvement for length 100.\nIn summary, models in Block IV and N2V-r, i.e. models which harness the information content of discourse network\nduring the learning phase have better performance than the other models in the summarization task. Here to note that, N2V-r also retrofits during learning as opposed to IT-w and IT-uw, which retrofit the learned representations as a postprocessing step.\nIn conclusion, over all the three (3) fundamental information system tasks, models in Block IV consistently improve over the S2V model. The models which use both the content and the information in the discourse network win over the baseline in both the classification and the clustering tasks\nReuters-21578 20-Newsgroups\nH C V AMI H C V AMI\nI S2V 45.54 39.88 42.50 39.50 35.08 36.26 35.68 35.08\nII N2V (+) 8.70 (+) 7.98 (+) 8.36 (+) 8.90 (+) 12.80 (+) 12.84 (+) 12.80 (+) 12.80\nN2V-i (+) 13.56 (+) 12.50 (+) 13.02 (+) 12.80 (+) 13.16 (+) 13.22 (+) 13.18 (+) 13.10\nIII\nN2V-r (+) 2.30 (+) 1.86 (+) 2.10 (+) 1.98 (+) 4.06 (+) 3.60 (+) 3.84 (+) 4.04\nIT-uw (+) 3.52 (+) 2.52 (+) 2.96 (+) 2.88 (+) 5.14 (+) 4.06 (+) 4.58 (+) 5.10\n(Please see the results of Block III and IV in Tables 7 and 8)."}, {"heading": "5. RELATED WORK", "text": "Our work is closely related to two different branches of representational learning research: (i) distributed representation of textual units (e.g., words, paragraphs), and (2) latent representation of nodes in a network."}, {"heading": "5.1 Representation for Textual Units", "text": "The Word2Vec model [14] to learn distributed representation of words is very popular for text processing tasks. The model also scales well in practice due to its simple architecture. [12] extended Word2Vec to learn the representation for sentences and documents. The model maps each sentence to an unique id and learns the representation for the sentence using the contexts of words in the sentence \u2013 either by predicting the whole context or by predicting a word in the context given the rest. In our work, we extend this model to incorporate inter-sentence relations in the form of a discourse graph. We do this using a graph-smoothing regularizer in the original objective function or by retrofitting the original vectors on the discourse graph.\nRecently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models. Our overall idea of using discourse as an extra source of information is reminiscent of these studies with a number of key differences: (i) the semantic network is given in their case, whereas we construct the network using similarities between nodes; (ii) we explore a purely network-based model for learning sentence representations and combine it with the content-based model, whereas they use the network just as an extra source of information in their content-based model. The network-based model gives us the flexibility to adaptively explore the network neighborhood.\nSkip-Thought [11] and Fast-Sent [7] are the most recently proposed sentence representation methods. Skip-Thought uses an encoder-decoder approach to reconstruct neighboring sentences of an input sentence, whereas Fast-Sent predicts words from the neighboring sentences for learning representation of the current sentence. Our discourse network is more general and can connect any pair of sentences. SkipThought is computationally very expensive [7]. Our approach is fundamentally different from these models and\ngives a light-weight solution to incorporate discourse."}, {"heading": "5.2 Representation for Nodes in Networks", "text": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work. These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21]. They are also related to each other in a sense that they all use discriminative models to learn the representation. These methods differ in how they generate the neighborhood (i.e., context) of a node from the graph. DeepWalk generates contexts using truncated random-walk. Both LINE and node2vec use alias sampling for generating contexts. node2vec uses a controlled random walk to adaptively combine breadth first search and depth first search. In our work, we use node2vec as the network-based model, and we combine it with the content-based model."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have proposed a set of novel models for learning vector representation of sentences that consider not only content of a sentence but relations among sentences. The relations among sentences are encoded in a discourse graph, which is then used to build discourse informed sentence representation models. We have explored two different ways to incorporate discourse information: (i) by retrofitting the vectors learned from a content-based model on the discourse graph, and (ii) by regularizing the contentbased model with a graph smoothing factor.\nWe evaluated our models on three tasks of classification, clustering, and ranking. Our results show that our models outperform the content-only baseline, and the regularized models perform well in all three tasks whereas the retrofitting models are good for only classification and clustering tasks.\nIn future, we would like to extend our models to represent groups in social networks and evaluate on prediction tasks involving social groups. The main difference between a network of sentences and a network of social groups is that groups are sets (order is irrelevant), whereas sentences are sequences. However, considering that our content-based model (DBOW) does not take order into account, we should be able to directly apply our models to social groups."}, {"heading": "7. REFERENCES", "text": "[1] C. F. Baker, C. J. Fillmore, and J. B. Lowe. The\nberkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pages 86\u201390, 1998.\n[2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3, Mar. 2003.\n[3] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457\u2013479, Dec. 2004.\n[4] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith. Retrofitting word vectors to semantic lexicons. In Proc. of NAACL, 2015.\n[5] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864, 2016.\n[6] F. Hill, K. Cho, and A. Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1367\u20131377, San Diego, California, June 2016. Association for Computational Linguistics.\n[7] F. Hill, K. Cho, and A. Korhonen. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483, 2016.\n[8] J. R. Hobbs. Coherence and coreference. Cognitive science, 3(1):67\u201390, 1979.\n[9] T. Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 217\u2013226, 2006.\n[10] I. Jolliffe. Principal component analysis. Wiley Online Library, 2002.\n[11] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302, 2015.\n[12] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. In ICML, volume 14, pages 1188\u20131196, 2014.\n[13] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain, 2004.\n[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS\u201913, pages 3111\u20133119, 2013.\n[16] G. A. Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995.\n[17] A. Nenkova and K. McKeown. Automatic\nsummarization. Foundations and TrendsA\u0302o\u030b in Information Retrieval, 5(2a\u0302A\u0306S\u03273):103\u2013233, 2011.\n[18] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: bringing order to the web. 1999.\n[19] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM, 2014.\n[20] A. Rosenberg and J. Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. In EMNLP-CoNLL, volume 7, pages 410\u2013420, 2007.\n[21] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000.\n[22] M. Stede. Discourse processing. Morgan & Claypool Publishers, 2011.\n[23] P. P. Talukdar and K. Crammer. New regularized algorithms for transductive learning. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD \u201909, pages 442\u2013457. Springer-Verlag, 2009.\n[24] J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1165\u20131174. ACM, 2015.\n[25] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. ACM, 2015.\n[26] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319\u20132323, 2000.\n[27] C. Xu, Y. Bai, J. Bian, B. Gao, G. Wang, X. Liu, and T.-Y. Liu. Rc-net: A general framework for incorporating knowledge into word representations. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1219\u20131228. ACM, 2014.\n[28] M. Yu and M. Dredze. Improving lexical embeddings with semantic knowledge. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 545\u2013550, Baltimore, Maryland, June 2014. Association for Computational Linguistics."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pages 86\u201390", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proc. of NAACL", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "arXiv preprint arXiv:1602.03483", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Coherence and coreference", "author": ["J.R. Hobbs"], "venue": "Cognitive science, 3(1):67\u201390", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1979}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 217\u2013226", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, pages 3294\u20133302", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML, volume 14, pages 1188\u20131196", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS\u201913, pages 3111\u20133119", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u201341", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Automatic  summarization", "author": ["A. Nenkova", "K. McKeown"], "venue": "Foundations and Trends\u00c2\u0151 in Information Retrieval, 5(2\u00e2\u0102\u015e3):103\u2013233", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The pagerank citation ranking: bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "EMNLP-CoNLL, volume 7, pages 410\u2013420", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Discourse processing", "author": ["M. Stede"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD \u201909, pages 442\u2013457. Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1165\u20131174. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "science, 290(5500):2319\u20132323", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1219\u20131228. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Similarly, for the task of ranking sentences based on their importance in the text using a ranking model like LexRank [3] or SVMRank [9], one needs to first represent the sentences with fixed-length vectors.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Similarly, for the task of ranking sentences based on their importance in the text using a ranking model like LexRank [3] or SVMRank [9], one needs to first represent the sentences with fixed-length vectors.", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "Recently, distributed representations, in the form of dense real-valued vectors, learned by neural network models from unlabeled data, has been shown to outperform the traditional bag-of-words representation [12].", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "Distributed representations encode the semantics of linguistic units and yield better generalization [15, 2].", "startOffset": 101, "endOffset": 108}, {"referenceID": 1, "context": "Distributed representations encode the semantics of linguistic units and yield better generalization [15, 2].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "However, most existing methods to devise distributed representation for sentences consider only the content of a sentence, and disregard relations between sentences in a text by and large [12, 6].", "startOffset": 188, "endOffset": 195}, {"referenceID": 5, "context": "However, most existing methods to devise distributed representation for sentences consider only the content of a sentence, and disregard relations between sentences in a text by and large [12, 6].", "startOffset": 188, "endOffset": 195}, {"referenceID": 7, "context": ", elaboration, contrast) to express the meaning as a whole [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": ", sentences are also topically related [22].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 92, "endOffset": 95}, {"referenceID": 26, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 3, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 27, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 4, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 24, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 23, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 18, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 11, "context": ", the Sen2Vec model proposed in [12].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 3, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 362, "endOffset": 365}, {"referenceID": 5, "context": ", sentiment classification, paraphrase identification) and sentence-pair similarity computation task [6, 12].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": ", sentiment classification, paraphrase identification) and sentence-pair similarity computation task [6, 12].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "We use the Sen2Vec model proposed in [12] as our baseline model, which is trained solely based on the contents of the sentences.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "We use the recently proposed Node2Vec method [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "It uses the skipgram model of Word2Vec [14] with the intuition that nodes within a graph context (or neighborhood) should have similar representations.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "We explore the general idea of retrofitting [4] to incorporate information from both the content and the neighborhood of a node (or a sentence) in a joint learning framework.", "startOffset": 44, "endOffset": 47}, {"referenceID": 22, "context": "The quadratic cost in Equation 9 is convex in \u03c6, and has a closed form solution [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Formally, the random walk view has the following iterative update [23]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "However, preserving only local structures may not be sufficient for many applications [25].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 25, "endOffset": 28}, {"referenceID": 26, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 3, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 27, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 26, "context": "Following previous work [27, 28], we first convert any kind of semantic lexicon into a network, where semantically similar words defined by the lexicon become neighbors.", "startOffset": 24, "endOffset": 32}, {"referenceID": 27, "context": "Following previous work [27, 28], we first convert any kind of semantic lexicon into a network, where semantically similar words defined by the lexicon become neighbors.", "startOffset": 24, "endOffset": 32}, {"referenceID": 16, "context": "For classification (or clustering), we measure the performance of our models in classifying (or grouping) sentences based on their topics, whereas in Ranking, we investigate our model\u2019s performance in ranking the most central topical sentences by evaluating it on an extractive summarization task [17].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "To minimize this noise, we use an extractive (unsupervised) summarizer, LexRank [3], to select the top P% sentences as representatives of the document and label them with the same topic label as the document; see Sec.", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "We use the graph-based LexRank algorithm [3] to rank the sentences in a document based on their importance.", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "We run the PageRank algorithm [18] on the graph to determine the rank of each sentence in the document, and thereby extract the key sentences as summary of that document.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Topic Clustering: For measuring clustering performance, we use various measures such as homogeneity score (H), completeness score (C), V-measure [20] (V), and adjusted mutual information (AMI) score.", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 9, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 11, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 0, "context": "All these measures are bounded by [0, 1].", "startOffset": 34, "endOffset": 40}, {"referenceID": 12, "context": "Summarization: We use the widely used automatic evaluation metric ROUGE [13] to evaluate the system-generated summaries.", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": ", n = 1) has been shown to correlate well with human judgments for short summaries [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "We used subsampling of frequent words and negative sampling in the classification layer as described in [15], which give significant speed-ups in training.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "The Word2Vec model [14] to learn distributed representation of words is very popular for text processing tasks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "[12] extended Word2Vec to learn the representation for sentences and documents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 3, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 27, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 10, "context": "Skip-Thought [11] and Fast-Sent [7] are the most recently proposed sentence representation methods.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "Skip-Thought [11] and Fast-Sent [7] are the most recently proposed sentence representation methods.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "SkipThought is computationally very expensive [7].", "startOffset": 46, "endOffset": 49}, {"referenceID": 18, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 137, "endOffset": 141}], "year": 2016, "abstractText": "Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large. In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that consider the content of a sentence as well as inter-sentence relations. We first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to exploit the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (i) using the adjacency relations of a node, and (ii) using a stochastic sampling method which is more flexible in sampling neighbors of a node. The second approach uses a regularizer to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach. The models leverage the computational power of multi-core CPUs to achieve fine-grained computational efficiency. We make our code publicly available upon acceptance.", "creator": "LaTeX with hyperref package"}}}