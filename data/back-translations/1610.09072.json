{"id": "1610.09072", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Orthogonal Random Features", "abstract": "We present a fascinating discovery related to random Fourier functions: In Gaussian kernel approximation, the random Gaussian matrix is replaced by a properly scaled random orthogonal matrix, which significantly reduces the approximation error of the kernel. We call this technique Orthogonal Random Features (ORF) and provide a theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which use a class of structured discrete orthogonal matrices to speed up the calculation, which reduces the time cost from $\\ mathcal {O} (d ^ 2) $to $\\ mathcal {O} (d\\ log d) $, where $d $is the data dimensionality, with the quality of the kernel approximation compared to ORF almost uncompromisingly verifying the effectiveness of existing ORF experiments on multiple sets of data.", "histories": [["v1", "Fri, 28 Oct 2016 03:50:00 GMT  (339kb,D)", "http://arxiv.org/abs/1610.09072v1", "NIPS 2016"]], "COMMENTS": "NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["felix x yu", "ananda theertha suresh", "krzysztof marcin choromanski", "daniel holtmann-rice", "sanjiv kumar"], "accepted": true, "id": "1610.09072"}, "pdf": {"name": "1610.09072.pdf", "metadata": {"source": "CRF", "title": "Orthogonal Random Features", "authors": ["Felix Xinnan Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "emails": ["sanjivk}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets. Kernel approximation is a powerful technique to make kernel methods scalable, by mapping input features into a new space where dot products approximate the kernel well [20]. With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].\nFormally, given a kernel K(\u00b7, \u00b7) : Rd \u00d7 Rd \u2192 R, kernel approximation methods seek to find a nonlinear transformation \u03c6(\u00b7) : Rd \u2192 Rd\u2032 such that, for any x,y \u2208 Rd\nK(x,y) \u2248 K\u0302(x,y) = \u03c6(x)T\u03c6(y).\nRandom Fourier Features [20] are used widely in approximating smooth, shift-invariant kernels. This technique requires the kernel to exhibit two properties: 1) shift-invariance, i.e. K(x,y) = K(\u2206) where \u2206 = x\u2212y; and 2) positive semi-definiteness of K(\u2206) on Rd. The second property guarantees that the Fourier transform of K(\u2206) is a nonnegative function [3]. Let p(w) be the Fourier transform of K(z). Then,\nK(x\u2212 y) = \u222b Rd p(w)ejw T (x\u2212y)dw.\nThis means that one can treat p(w) as a density function and use Monte-Carlo sampling to derive the following nonlinear map for a real-valued kernel:\n\u03c6(x) = \u221a 1/D [ sin(wT1 x), \u00b7 \u00b7 \u00b7 , sin(wTDx), cos(wT1 x), \u00b7 \u00b7 \u00b7 , cos(wTDx) ]T ,\nwhere wi is sampled i.i.d. from a probability distribution with density p(w). Let W =[ w1, \u00b7 \u00b7 \u00b7 ,wD ]T . The linear transformation Wx is central to the above computation since,\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 07\n2v 1\n[ cs\n.L G\n] 2\n8 O\nct 2\n\u2022 The choice of matrix W determines how well the estimated kernel converges to the actual kernel; \u2022 The computation of Wx has space and time costs of O(Dd). This is expensive for high-\ndimensional data, especially since D is often required to be larger than d to achieve low approximation error.\nIn this work, we address both of the above issues. We first show an intriguing discovery (Figure 1): by enforcing orthogonality on the rows of W, the kernel approximation error can be significantly reduced. We call this method Orthogonal Random Features (ORF). Section 3 describes the method and provides theoretical explanation for the improved performance.\nSince both generating a d\u00d7 d orthogonal matrix (O(d3) time and O(d2) space) and computing the transformation (O(d2) time and space) are prohibitively expensive for high-dimensional data, we further propose Structured Orthogonal Random Features (SORF) in Section 4. The idea is to replace random orthogonal matrices by a class of special structured matrices consisting of products of binary diagonal matrices and Walsh-Hadamard matrices. SORF has fast computation time, O(D log d), and almost no extra memory cost (with efficient in-place implementation). We show extensive experiments in Section 5. We also provide theoretical discussions in Section 6 of applying the structured matrices in a broader range of applications where random Gaussian matrix is used."}, {"heading": "2 Related Works", "text": "Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19]. In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].\nKey to the RFF technique is Monte-Carlo sampling. It is well known that the convergence of MonteCarlo can be largely improved by carefully choosing a deterministic sequence instead of random samples [18]. Following this line of reasoning, Yang et al. [26] proposed to use low-displacement rank sequences in RFF. Yu et al. [29] studied optimizing the sequences in a data-dependent fashion to achieve more compact maps. In contrast to the above works, this paper is motivated by an intriguing new discovery that using orthogonal random samples provides much faster convergence. Compared to [26], the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs. Furthermore, unlike [29], the results in this paper are data independent.\nStructured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8]. For the kernel approximation works, in particular, the \u201cstructured randomness\u201d leads to a minor loss of accuracy, but allows faster computation since the structured matrices enable the use of FFT-like algorithms. Furthermore, these matrices provide substantial model compression since they require subquadratic (usually only linear)\nspace. In comparison with the above works, our proposed methods SORF and ORF are more effective than RFF. In particular SORF demonstrates both lower approximation error and better efficiency than RFF. Table 1 compares the space and time costs of different techniques."}, {"heading": "3 Orthogonal Random Features", "text": "Our goal is to approximate a Gaussian kernel of the form\nK(x,y) = e\u2212||x\u2212y|| 2/2\u03c32 .\nIn the paragraph below, we assume a square linear transformation matrix W \u2208 RD\u00d7d, D = d. When D < d, we simply use the first D dimensions of the result. When D > d, we use multiple independently generated random features and concatenate the results. We comment on this setting at the end of this section.\nRecall that the linear transformation matrix of RFF can be written as\nWRFF = 1\n\u03c3 G, (1)\nwhere G \u2208 Rd\u00d7d is a random Gaussian matrix, with every entry sampled independently from the standard normal distribution. Denote the approximate kernel based on the above WRFF asKRFF(x,y). For completeness, we first show the expectation and variance of KRFF(x,y). Lemma 1. (Appendix A.2) KRFF(x,y) is an unbiased estimator of the Gaussian kernel, i.e., E(KRFF(x,y)) = e\u2212||x\u2212y||\n2/2\u03c32 . Let z = ||x \u2212 y||/\u03c3. The variance of KRFF(x,y) is Var (KRFF(x,y)) = 12D ( 1\u2212 e\u2212z2 )2 .\nThe idea of Orthogonal Random Features (ORF) is to impose orthogonality on the matrix on the linear transformation matrix G. Note that one cannot achieve unbiased kernel estimation by simply replacing G by an orthogonal matrix, since the norms of the rows of G follow the \u03c7-distribution, while rows of an orthogonal matrix have the unit norm. The linear transformation matrix of ORF has the following form\nWORF = 1\n\u03c3 SQ, (2)\nwhere Q is a uniformly distributed random orthogonal matrix1. The set of rows of Q forms a bases in Rd. S is a diagonal matrix, with diagonal entries sampled i.i.d. from the \u03c7-distribution with d degrees of freedom. S makes the norms of the rows of SQ and G identically distributed.\nDenote the approximate kernel based on the above WORF as KORF(x,y). The following shows that KORF(x,y) is an unbiased estimator of the kernel, and it has lower variance in comparison to RFF. Theorem 1. KORF(x,y) is an unbiased estimator of the Gaussian kernel, i.e.,\nE(KORF(x,y)) = e\u2212||x\u2212y|| 2/2\u03c32 .\n1We first generate the random Gaussian matrix G in (1). Q is the orthogonal matrix obtained from the QR decomposition of G. Q is distributed uniformly on the Stiefel manifold (the space of all orthogonal matrices) based on the Bartlett decomposition theorem [17].\nLet D \u2264 d, and z = ||x \u2212 y||/\u03c3. There exists a function f such that for all z, the variance of KORF(x,y) is bounded by\nVar (KORF(x,y)) \u2264 1\n2D\n(( 1\u2212 e\u2212z 2 )2 \u2212 D \u2212 1\nd e\u2212z\n2 z4 ) + f(z)\nd2 .\nProof. We first show the proof of the unbiasedness. Let z = x\u2212y\u03c3 , and z = ||z||, then E(KORF (x,y)) = E ( 1 D \u2211D i=1 cos(w T i z) ) = 1D \u2211D i=1 E ( cos(wTi z) ) . Based on the definition of ORF, w1,w2, . . . ,wD are D random vectors given by wi = siui, with u1,u2, . . . ,ud a uniformly chosen random orthonormal basis for Rd, and si\u2019s are independent \u03c7-distributed random variables with d degrees of freedom. It is easy to show that for each i, wi is distributed according to N(0, Id), and hence by Bochner\u2019s theorem,\nE[cos(wT z)] = e\u2212z 2/2.\nWe now show a proof sketch of the variance. Suppose, ai = cos(wTi z).\nVar\n( 1\nD D\u2211 i=1 ai\n) = E [(\u2211D i=1 ai\nD\n)2] \u2212 E [(\u2211D i=1 ai\nD )]2 = 1\nD2 \u2211 i ( E[a2i ]\u2212 E[ai]2 ) + 1 D2 \u2211 i \u2211 j 6=i (E[aiaj ]\u2212 E[ai]E[aj ])\n=\n( 1\u2212 e\u2212z 2 )2\n2D + D(D \u2212 1) D2\n( E[a1a2]\u2212 e\u2212z 2 ) ,\nwhere the last equality follows from symmetry. The first term in the resulting expression is exactly the variance of RFF. In order to have lower variance, E[a1a2]\u2212 e\u2212z 2\nmust be negative. We use the following lemma to quantify this term.\nLemma 2. (Appendix A.3) There is a function f such that for any z,\nE[aiaj ] \u2264 e\u2212z 2 \u2212 e\u2212z 2 z4 2d + f(z) d2 .\nTherefore, for a large d, and D \u2264 d, the ratio of the variance of ORF and RFF is\nVar(KORF(x,y)) Var(KRFF(x,y)) \u2248 1\u2212 (D \u2212 1)e \u2212z2z4 d(1\u2212 e\u2212z2)2 . (3)\nFigure 2(a) shows the ratio of the variance of ORF to that of RFF when D = d and d is large. First notice that this ratio is always smaller than 1, and hence ORF always provides improvement over\nthe conventional RFF. Interestingly, we gain significantly for small values of z. In fact, when z \u2192 0 and d \u2192 \u221e, the ratio is roughly z2 (note ex \u2248 1 + x when x \u2192 0), and ORF exhibits infinitely lower error relative to RFF. Figure 2(b) shows empirical simulations of this ratio. We can see that the variance ratio is close to that of d =\u221e (3), even when d = 32, a fairly low-dimensional setting in real-world cases.\nRecall that z = ||x\u2212 y||/\u03c3. This means that ORF preserves the kernel value especially well for data points that are close, thereby retaining the local structure of the dataset. Furthermore, empirically \u03c3 is typically not set too small in order to prevent overfitting\u2014a common rule of thumb is to set \u03c3 to be the average distance of 50th-nearest neighbors in a dataset. In Figure 2(c), we plot the distribution of z for several datasets with this choice of \u03c3. These distributions are all concentrated in the regime where ORF yields substantial variance reduction.\nThe above analysis is under the assumption that D \u2264 d. Empirically, for RFF, D needs to be larger than d in order to achieve low approximation error. In that case, we independently generate and apply the transformation (2) multiple times. The next lemma bounds the variance for this case.\nCorollary 1. Let D = m \u00b7 d, for an integer m and z = ||x\u2212 y||/\u03c3. There exists a function f such that for all z, the variance of KORF(x,y) is bounded by\nVar (KORF(x,y)) \u2264 1\n2D\n(( 1\u2212 e\u2212z 2 )2 \u2212 d\u2212 1\nd e\u2212z\n2 z4 ) + f(z)\ndD ."}, {"heading": "4 Structured Orthogonal Random Features", "text": "In the previous section, we presented Orthogonal Random Features (ORF) and provided a theoretical explanation for their effectiveness. Since generating orthogonal matrices in high dimensions can be expensive, here we propose a fast version of ORF by imposing structure on the orthogonal matrices. This method can provide drastic memory and time savings with minimal compromise on kernel approximation quality. Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8]. Let us first introduce a simplified version of ORF: replace S in (2) by a scalar \u221a d. Let us call this method ORF\u2032. The transformation matrix thus has the following form:\nWORF\u2032 =\n\u221a d\n\u03c3 Q. (4)\nTheorem 2. (Appendix B) Let KORF\u2032(x,y) be the approximate kernel computed with linear transformation matrix (4). Let D \u2264 d and z = ||x\u2212 y||/\u03c3. There exists a function f such that the bias of KORF\u2032(x,y) satisfies \u2223\u2223\u2223E(KORF\u2032(x,y))\u2212 e\u2212z2/2\u2223\u2223\u2223 \u2264 e\u2212z2/2 z4\n4d + f(z) d2 ,\nand the variance satisfies\nVar (KORF\u2032(x,y)) \u2264 1\n2D\n( (1\u2212 e\u2212z 2\n)2 \u2212 D \u2212 1 d e\u2212z 2\nz4 ) + f(z)\nd2 .\nThe above implies that when d is large KORF\u2032(x,y) is a good estimation of the kernel with low variance. Figure 3(a) shows that even for relatively small d, the estimation is almost unbiased. Figure 3(c) shows that when d \u2265 32, the variance ratio is very close to that of d =\u221e. We find empirically that ORF\u2032also provides very similar MSE in comparison with ORF in real-world datasets.\nWe now introduce Structured Orthogonal Random Features (SORF). It replaces the random orthogonal matrix Q of ORF\u2032in (4) by a special type of structured matrix HD1HD2HD3:\nWSORF =\n\u221a d\n\u03c3 HD1HD2HD3, (5)\nwhere Di \u2208 Rd\u00d7d, i = 1, 2, 3 are diagonal \u201csign-flipping\u201d matrices, with each diagonal entry sampled from the Rademacher distribution. H is the normalized Walsh-Hadamard matrix.\nComputing WSORFx has the time cost O(d log d), since multiplication with D takes O(d) time and multiplication with H takes O(d log d) time using fast Hadamard transformation. The computation of SORF can also be carried out with almost no extra memory due to the fact that both sign flipping and the Walsh-Hadamard transformation can be efficiently implemented as in-place operations [10].\nFigures 3(b)(d) show the bias and variance of SORF. Note that although the curves for small d are different from those of ORF, when d is large (d > 32 in practice), the kernel estimation is almost unbiased, and the variance ratio converges to that of ORF. In other words, it is clear that SORF can provide almost identical kernel approximation quality as that of ORF. This is also confirmed by the experiments in Section 5. In Section 6, we provide theoretical discussions to show that the structure of (5) can also be generally applied to many scenarios where random Gaussian matrices are used."}, {"heading": "5 Experiments", "text": "Kernel Approximation. We first show kernel approximation performance on six datasets. The input feature dimension d is set to be power of 2 by padding zeros or subsampling. Figure 4 compares the mean squared error (MSE) of all methods. For fixed D, the kernel approximation MSE exhibits the following ordering:\nSORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29]. By imposing orthogonality on the linear transformation matrix, Orthogonal Random Features (ORF) achieves significantly lower approximation error than Random Fourier Features (RFF). The Structured Orthogonal Random Features (SORF) have almost identical MSE to that of ORF. All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE. We also include DigitalNet, the best performing method among Quasi-Monte Carlo techniques [26]. Its MSE is lower than that of RFF, but still higher than that of ORF and SORF. The order of time cost for a fixed D is\nSORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20]. Remarkably, SORF has both better computational efficiency and higher kernel approximation quality compared to other methods.\nWe also apply ORF and SORF on classification tasks. Table 2 shows classification accuracy for different kernel approximation techniques with a (linear) SVM classifier. SORF is competitive with or better than RFF, and has greatly reduced time and space costs.\nThe Role of \u03c3. Note that a very small \u03c3 will lead to overfitting, and a very large \u03c3 provides no discriminative power for classification. Throughout the experiments, \u03c3 for each dataset is chosen to be the mean distance of the 50th `2 nearest neighbor, which empirically yields good classification results [29]. As shown in Section 3, the relative improvement over RFF is positively correlated with \u03c3. Figure 5(a)(b) verify this on the mnist dataset. Notice that the proposed methods (ORF and SORF) consistently improve over RFF.\nSimplifying SORF. The SORF transformation consists of three Hadamard-Diagonal blocks. A natural question is whether using fewer computations and randomness can achieve similar empirical performance. Figure 5(c) shows that reducing the number of blocks to two (HDHD) provides similar performance, while reducing to one block (HD) leads to large error."}, {"heading": "6 Analysis and General Applicability of the Hadamard-Diagonal Structure", "text": "We provide theoretical discussions of SORF in this section. We first show that for large d, SORF is an unbiased estimator of the Gaussian kernel.\nTheorem 3. (Appendix C) Let KSORF(x,y) be the approximate kernel computed with linear transformation matrix\n\u221a dHD1HD2HD3. Let z = ||x\u2212 y||/\u03c3. Then\u2223\u2223\u2223E(KSORF(x,y))\u2212 e\u2212z2/2\u2223\u2223\u2223 \u2264 6z\u221a\nd .\nEven though SORF is nearly-unbiased, proving tight variance and concentration guarantees similar to ORF remains an open question. The following discussion provides a sketch in that direction. We first show a lemma of RFF. Lemma 3. Let W be a random Gaussian matrix as in RFF, for a given z, the distribution of Wz is N(0, ||z||2Id).\nNote that Wz in RFF can be written as Rg, where R is a scaled orthogonal matrix such that each row has norm ||z||2 and g is distributed according to N(0, Id). Hence the distribution of Rg is N(0, ||z||2Id), identical to Wz. The concentration results of RFF use the fact that the projections of a Gaussian vector g onto orthogonal directions R are independent. We show that \u221a dHD1HD2HD3z has similar properties. In particular, we show that it can be written as R\u0303g\u0303, where rows of R\u0303 are \u201cnear-orthogonal\u201d (with high probability) and have norm ||z||2, and the vector g\u0303 is close to Gaussian (g\u0303 has independent sub-Gaussian elements), and hence the projections behave \u201cnear-independently\u201d. Specifically, g\u0303 = vec(D1) (vector of diagonal entries of D1), and R\u0303 is a function of D2, D3 and z.\nTheorem 4. (Appendix D) For a given z, there exists a R\u0303 (function of D2,D3, z), such that\u221a dHD1HD2HD3z = R\u0303vec(D1). Each row of R\u0303 has norm ||z||2 and for any t \u2265 1/d, with probability 1\u2212 de\u2212c\u00b7t2/3d1/3 , the inner product between any two rows of R\u0303 is at most t||z||2, where c is a constant.\nThe above result can also be applied to settings not limited to kernel approximation. In the appendix, we show empirically that the same scheme can be successfully applied to angle estimation where the nonlinear map f is a non-smooth sign(\u00b7) function [4]. We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7]."}, {"heading": "7 Conclusions", "text": "We have demonstrated that imposing orthogonality on the transformation matrix can greatly reduce the kernel approximation MSE of Random Fourier Features when approximating Gaussian kernels. We further proposed a type of structured orthogonal matrices with substantially lower computation and memory cost. We provided theoretical insights indicating that the Hadamard-Diagonal block structure can be generally used to replace random Gaussian matrices in a broader range of applications. Our method can also be generalized to other types of kernels such as general shift-invariant kernels and polynomial kernels based on Schoenberg\u2019s characterization as in [19]."}, {"heading": "Appendix A Variance Reduction via Orthogonal Random Features", "text": "A.1 Notation\nLet z = x\u2212y\u03c3 , and z = ||z||. For a vector y, let y(i) denote its i th coordinate. Let n!! be the double factorial of n, i.e., the product of every number from n to 1 that has the same parity as n.\nA.2 Proof of Lemma 1\nLet z = (x\u2212 y)/\u03c3. Recall that in RFF, we compute the Kernel approximation as\nD\u2211 i=1 1 D cos(wTi z),\nwhere each wi is a d dimensional vector distributed N(0, Id). Let w be a d dimensional vector distributed N(0, Id). By Bochner\u2019s theorem,\nE[cos(wT z)] = e\u2212z 2/2,\nand hence RFF yields an unbiased estimate.\nWe now compute the variance of RFF approximation. Observe that\ncos2(wT z) = 1 + cos(2wT z)\n2 =\n1 + cos(wT (2z))\n2 .\nHence by Bochner\u2019s theorem\nE[cos2(wT z)] = 1 + e\u22122z\n2\n2 .\nTherefore,\nVar(cos(wT z)) = E[cos2(wT z)]\u2212 (E[cos(wT z)])2\n= 1 + e\u22122z\n2\n2 \u2212 e\u2212z\n2 = (1\u2212 e\u2212z2)2\n2 .\nIf we take D such independent random variables w1,w2, . . .wD, since variance of the sum is sum of variances,\nVar\n( 1\nD D\u2211 i=1 cos(wTi z)\n) = (1\u2212 e\u2212z2)2\n2D .\nA.3 Proof of Lemma 2\nThe proof uses the following lemma.\nLemma 4. For a set of non-negative values \u03b11, \u03b12, . . . \u03b1k and \u03b21, \u03b22, . . . \u03b2k such that for all i, \u03b2i \u2264 \u03b1i, \u2223\u2223\u2223\u2223\u2223 1\u220fk\ni (1 + \u03b1i) \u2212\n( 1\u2212\nk\u2211 i=1 \u03b1i )\u2223\u2223\u2223\u2223\u2223 \u2264 ( k\u2211 i=1 \u03b1i )2 ,\nand \u2223\u2223\u2223\u2223\u2223 k\u220f i 1 + \u03b2i 1 + \u03b1i \u2212 ( 1 + k\u2211 i=1 \u03b2i \u2212 k\u2211 i=1 \u03b1i )\u2223\u2223\u2223\u2223\u2223 \u2264 ( k\u2211 i=1 (\u03b1i \u2212 \u03b2i) )2 + k\u2211 i=1 (\u03b1i \u2212 \u03b2i)\u03b2i.\nProof. Since \u03b1is are non-negative,\n1\u220fk i (1 + \u03b1i)\n\u2212 ( 1\u2212 k\u2211 i=1 \u03b1i ) \u2264 1 1 + \u2211k i \u03b1i \u2212 ( 1\u2212 k\u2211 i=1 \u03b1i )\n= 1\u2212 (1 +\n\u2211k i=1 \u03b1i)(1\u2212 \u2211k i=1 \u03b1i)\n1 + \u2211k i \u03b1i\n= ( \u2211k i=1 \u03b1i) 2\n1 + \u2211k i \u03b1i \u2264 ( k\u2211 i=1 \u03b1i )2 .\nFurthermore, by convexity\n1\u220fk i (1 + \u03b1i) \u2265 1 (1 + \u2211k i=1 \u03b1i/k) k \u2265 e\u2212\n\u2211k i=1 \u03b1i \u2265 1\u2212 k\u2211 i=1 \u03b1i.\nCombining the above two equations results in the first part of the lemma. For the second part observe that\nk\u220f i 1 + \u03b2i 1 + \u03b1i = 1\u220fk\ni ( 1 + \u03b1i\u2212\u03b2i1+\u03b2i ) . Hence, by the first part\u2223\u2223\u2223\u2223\u2223 k\u220f i 1 + \u03b2i 1 + \u03b1i \u2212 ( 1\u2212 k\u2211 i=1 \u03b1i \u2212 \u03b2i 1 + \u03b2i )\u2223\u2223\u2223\u2223\u2223 \u2264 ( k\u2211 i=1 \u03b1i \u2212 \u03b2i 1 + \u03b2i )2 \u2264 ( k\u2211 i=1 (\u03b1i \u2212 \u03b2i) )2 .\nFurthermore, for every i \u2223\u2223\u2223\u2223 11 + \u03b2i \u2212 1 \u2223\u2223\u2223\u2223 \u2264 \u03b2i.\nCombining the above two equations yields the second part of the lemma.\nProof of Lemma 2. Observe that\ncos(wT1 z) cos(w T 2 z) =\ncos(wT1 z + w T 2 z) + cos(w T 1 z\u2212wT2 z)\n2 .\nSince the problem is rotation invariant, instead of projecting a vector z onto a randomly chosen two orthogonal vectors u1 and u2, we can choose a vector y that is uniformly distributed on a sphere of radius z and project it on to the first two dimensions. Thus,\nE[cos(wT1 z + wT2 z)] = E[cos((s1y(1) + s2y(2))z)].\nSimilarly,\nE[cos(wT1 z\u2212wT2 z)] = E[cos((s1y(1)\u2212 s2y(2))z)].\nThe kth term in the Taylor\u2019s series expansion of sum of above two terms is\n(\u22121)k\n(2k)! ((s1y(1) + s2y(2))z)\n2k +\n(\u22121)k\n(2k)! ((s1y(1)\u2212 s2y(2))z)2k\n= (\u2212z2)k\n(2k)! k\u2211 i=0 ( 2k 2i ) s2i1 y 2i(1)s2k\u22122i2 y 2k\u22122i(2).\nA way to compute a uniformly distributed random variable on a sphere with radius z is to generate d independent random variables x = (x(1), x(2), . . . , x(d)) each distributed N(0, 1) and setting\ny(i) = zx(i)/||x||. Hence,\nE [ k\u2211 i=0 ( 2k 2i ) s2i1 y 2i(1)s2k\u22122i2 y 2k\u22122i(2) ] (a) = E\n[ 2k\u2211 i=0 ( 2k 2i ) s2i1 x 2i(1)s2k\u22122i2 x 2k\u22122i(2) ||x||2k ] (b) =\nk\u2211 i=0 ( 2k 2i ) E[s2i1 ]E[s 2k\u22122i 2 ]E [ x2i(1)x2k\u22122i(2) ||x||2k ] (c) =\nk\u2211 i=0 ( 2k 2i ) E[s2i1 ]E[s 2k\u22122i 2 ] E[x2i(1)]E[x2k\u22122i(2)] E[||x||2k]\n(d) = k\u2211 i=0 ( 2k 2i ) (d+ 2i\u2212 2)!!(d+ 2k \u2212 2i\u2212 2)!! \u00b7 (2i\u2212 1)!!(2k \u2212 2i\u2212 1)!! (d+ 2k \u2212 2)!!(d\u2212 2)!!\n(e) =\n(2k)!\n2kk! k\u2211 i=0 ( k i ) (d+ 2i\u2212 2)!!(d+ 2k \u2212 2i\u2212 2)!! (d+ 2k \u2212 2)!!(d\u2212 2)!! .\n(a) follows from linearity of expectation and the observation above. (b) follows from the independence of s1, s2, and x. (d) follows from substituting the moments of chi and Gaussian distributions. (e) follows from numerical simplification. We now describe the reasoning behind (c). Let z = x||y||||x|| , where y and x are independent N(0, Id) random variables. By the properties of the Gaussian random variables z is also a N(0, Id) random variable. Thus,\nE[z2i(1)]E[z2k\u22122i(2)] = E [ x2i(1)x2k\u22122i(2)\n||x||2k\n] E[||y||2k].\nRearranging terms, we get E [ x2i(1)x2k\u22122i(2)\n||x||2k\n] =\nE[z2i(1)]E[z2k\u22122i(2)] E[||y||2k = E[x2i(1)]E[x2k\u22122i(2)] E[||x||2k] ,\nand hence (c). Substituting the above equation in the cosine expansion, we get that the expectation is\nE[cos(s1y(1) + s2y(2)]] = \u221e\u2211 k=0 (\u2212z2)k k! k\u2211 i=0 ( k i ) 1 2k (d+ 2i\u2212 2)!!(d+ 2k \u2212 2i\u2212 2)!! (d+ 2k \u2212 2)!!(d\u2212 2)!! .\nObserve that\n(d+ 2i\u2212 2)!!(d+ 2k \u2212 2i\u2212 2)!! (d+ 2k \u2212 2)!!(d\u2212 2)!! = \u220fk\u2212i\u22121 j=0 (1 + 2j/d)\u220fk\u2212i\u22121\nj=0 (1 + 2(j + i)/d) ,\nHence by Lemma 4,\u2223\u2223\u2223\u2223\u2223\u2223 \u220fk\u2212i\u22121 j=0 (1 + 2j/d)\u220fk\u2212i\u22121 j=0 (1 + 2(j + i)/d) \u2212 1 + k\u2212i\u22121\u2211 j=0 2j d \u2212 k\u2212i\u22121\u2211 j=0 2(j + i) d \u2223\u2223\u2223\u2223\u2223\u2223 \u2264\nk\u2212i\u22121\u2211 j=0 2i d 2 + k\u2212i\u22121\u2211 j=0 2i d ( 2j d ) .\nSimplifying we get,\u2223\u2223\u2223\u2223\u2223 \u220fk\u2212i\u22121 j=0 (1 + 2j/d)\u220fk\u2212i\u22121 j=0 (1 + 2(j + i)/d) \u2212 ( 1 + 2i2 \u2212 2ik d )\u2223\u2223\u2223\u2223\u2223 \u2264 4i2(k \u2212 i)2d2 + 2i(k \u2212 i)(k \u2212 i\u2212 1)d3 .\nHence summing over i,\u2223\u2223\u2223\u2223\u2223 k\u2211 i=0 ( k i ) 1 2k \u220fk\u2212i\u22121 j=0 (1 + 2j/d)\u220fk\u2212i\u22121 j=0 (1 + 2(j + i)/d) \u2212 ( 1 + k \u2212 k2 2d )\u2223\u2223\u2223\u2223\u2223 \u2264 k44d2 + k2(k \u2212 1)2d3 . Substituting,\nE[cos((s1y(1) + s2y(2))z]] + E[cos((s1y(1)\u2212 s2y(2))z]] 2 = \u221e\u2211 k=0 (\u2212z2)k k! ( 1 + k \u2212 k2 2d + ck,d ) ,\nwhere |ck,d| \u2264 k 4 4d2 + k2(k\u22121) 2d3 . Thus,\nE[cos((s1y(1) + s2y(2))z]] + E[cos((s1y(1)\u2212 s2y(2))z]] 2\n= \u221e\u2211 k=0 (\u2212z2)k k! ( 1 + k \u2212 k2 2d + ck,d )\n\u2264 \u221e\u2211 k=0 (\u2212z2)k k! ( 1 + k \u2212 k2 2d ) + \u221e\u2211 k=0 (z2)k k! ( k4 4d2 + k2(k \u2212 1) 2d3 )\n\u2264 e\u2212z 2 \u2212 e\u2212z 2 z4 2d + ez\n2\n(z8 + 6z6 + 7z4 + z2) 4d2 + ez\n2\nz4(z6 + 2z4)\n2d3 ."}, {"heading": "Appendix B Proof of Theorem 2", "text": "The proof of the theorem is similar to that of Lemma 2 and we outline some key steps. We first bound the bias in Lemma 5 and then the variance in Lemma 6. Lemma 5. If w =\n\u221a dy, where y is distributed uniformly on a unit sphere, then\u2223\u2223\u2223\u2223E[coswT z]\u2212 (e\u2212z2/2 \u2212 e\u2212z2/2 z44d )\u2223\u2223\u2223\u2223 \u2264 ez2/2z4(z4 + 8z2 + 8)16d2 . Proof. Without loss of generality, we can assume z is along the first coordinate and hence wT z =\u221a dzy(1). A way to compute a uniformly distributed random variable on a sphere with radius z is to generate d independent random variables x = (x(1), x(2), . . . , x(d)) each distributed N(0, 1) and setting y(i) = zx(i)/||x||. hence,\nE[coswT z] = E [ cos ( z \u221a dx(1)\n||x||\n)] .\nThe kth term in the Taylor\u2019s series expansion of cosine in the above equation is\n(\u22121)k\n(2k)!\n(\u221a dx(1)z\n||x|| )2k Similar to the proof of Lemma 2, it can be shown that the expectation of this term is\nE  (\u22121)k (2k)! ( z \u221a dx(1) ||x|| )2k = (\u2212z2)k 2kk!\ndk\n(d, 2k \u2212 2)!! .\nApplying Lemma 4 and simplifying,\nE[cos(dy(1))] = \u221e\u2211 k=0 (\u2212z2)k 2kk! ( 1\u2212 k(k \u2212 1) d + c\u2032k,d ) ,\nwhere |c\u2032k,d| \u2264 ( k(k\u22121) d )2 . Hence,\u2223\u2223\u2223\u2223\u2223E[cos(dy(1))]\u2212 \u221e\u2211 k=0 (\u2212z2)k 2kk! ( 1\u2212 k(k \u2212 1) d )\u2223\u2223\u2223\u2223\u2223 \u2264 \u221e\u2211 k=0 (z2)k 2kk! ( k(k \u2212 1) d )2 , and thus \u2223\u2223\u2223\u2223E[cos(dy(1))]\u2212 e\u2212z2/2 + e\u2212z2/2 z44d \u2223\u2223\u2223\u2223 \u2264 ez2/2z4(z4 + 8z2 + 8)16d2 .\nLemma 6. Let D \u2264 d. If W = \u221a dQ, where Q is a uniformly chosen random rotation, then\nVar\n( 1\nD D\u2211 i=1 cos(wTi z)\n) \u2264 1\n2D\n( (1\u2212 e\u2212z 2\n)2 \u2212 D \u2212 1 d e\u2212z 2\nz4 )\n+ O(e3z2) d2 .\nProof. Let ai = cos(wTi z). Expanding the variance we have,\nVar\n( 1\nD D\u2211 i=1 ai\n) = 1\nD2 \u2211 i ( E[a2i ]\u2212 (E[ai])2 ) + 1 D2 \u2211 i \u2211 j 6=i (E[aiaj ]\u2212 E[ai]E[aj ])\n= 1\nD\n( E[a21]\u2212 (E[a1]2) ) + D \u2212 1 D (E[a1a2]\u2212 E[a1]E[a2]) .\nFor the first term, rewriting cos2(wT z) = 1+cos(2w T z)\n2 , similar to the proof of Lemma 5 it can be shown that\n(E[a21]\u2212 (E[a1]2) \u2264 (1\u2212 e\u2212z2)2 2 + O(e3z2) d .\nSecond term can be bounded similar to Lemma 2 and here we just sketch an outline. Similar to the proof of Lemma 2, the variance boils down to computing the expectation of cos(wT1 z + w T 2 z). Using Lemma 4 and summing Taylor\u2019s series we get\u2223\u2223\u2223\u2223E[cos(wT1 z + wT2 z)]\u2212 e\u2212z2 + e\u2212z2 z4d \u2223\u2223\u2223\u2223 \u2264 ez2z4(z4 + 4z2 + 2)d2 .\nSubstituting the above bound and the expectation from Lemma 5, we get\nE[a1a2]\u2212 E[a1]E[a2] \u2264 \u2212ez 2 z4 2d + O(e3z2) d2 ,\nand hence the lemma."}, {"heading": "Appendix C Proof of Theorem 3", "text": "The proof follows from the following two technical lemmas. Lemma 7. Let z\u2032 be distributed according to N(0, ||x||22) and y\u2032 = \u2211d i=1 x(i)di, where dis are independent Rademacher random variables. For any function g such that |g\u2032| \u2264 1 and |g| \u2264 1,\n|E[g(z\u2032)]\u2212 E[g(y\u2032)]| \u2264 3 2 d\u2211 i=1 x3(i) ||x||22 .\nProof. Let z = z\u2032/||x||2, y = y\u2032/||x||2, and h(x) = g(||x||2x), for all x. Hence h(z) = g(z\u2032) and h(y) = g(y\u2032). By a lemma due to Stein [5],\n|E[g(z\u2032)]\u2212 E[g(y\u2032)]| = |E[h(z)]\u2212 E[h(y)]| \u2264 supf{|E[f \u2032(y)\u2212 yf(y)]| : |f |\u221e \u2264 ||x||2, |f \u2032|\u221e \u2264 \u221a 2/\u03c0||x||2, |f \u2032\u2032|\u221e \u2264 2||x||2}.\nWe now bound the term on the right hand side by classic Stein-type arguments.\nE[yf(y)] = d\u2211 i=1 x(i)di ||x||2 E[f(y)].\nLet yi = y \u2212 x(i)di||x||2 . Observe that\nE[dif(y)] = E[di(f(y)\u2212 f(yi))] = E[di(f(y)\u2212 f(yi))\u2212 di(y \u2212 yi)f \u2032(yi)] + E[di(y \u2212 yi)f \u2032(yi)],\nwhere the first equality follows from the fact that yi and di are independent and di has zero mean. By Taylor series approximation, the first term is bounded by\n|E[dif(y)\u2212 f(yi)\u2212 di(y \u2212 yi)f \u2032(yi)]| \u2264 1\n2 (y \u2212 yi)2|f \u2032\u2032|\u221e =\n1\n2\nx2(i) ||x||22 |f \u2032\u2032|\u221e.\nSimilarly,\nE[di(y \u2212 yi)f \u2032(yi)] = x(i)\n||x||2 f \u2032(yi).\nCombining the above four equations, we get\u2223\u2223\u2223\u2223\u2223E [ yf(y)\u2212 d\u2211 i=1 x2(i) ||x||22 f \u2032(yi) ]\u2223\u2223\u2223\u2223\u2223 \u2264 d\u2211 i=1 |x3(i)| ||x||32 |f \u2032\u2032|\u221e. Similarly, note that\u2223\u2223\u2223\u2223\u2223E [ f \u2032(y)\u2212 d\u2211 i=1 x2(i) ||x||22 f \u2032(yi) ]\u2223\u2223\u2223\u2223\u2223 \u2264 d\u2211 i=1 |f \u2032\u2032|\u221e x2(i) ||x||22 E[|y \u2212 yi|] = d\u2211 i=1 |f \u2032\u2032|\u221e |x3(i)| ||x||32 .\nCombining the above two equations, we get\n||E[yf(y)\u2212 f \u2032(y))]| \u2264 3|f \u2032\u2032|\u221e 2 d\u2211 i=1 |x3(i)| ||x||32 .\nSubstituting the bound on the second moment of f yields the result.\nLet G be a random matrix with i.i.d. N(0, 1) entries as before. Using the above lemma we show that\u221a dHD1HD2 behaves like G while computing the bias. Lemma 8. For a given x, let z = Gx and y = \u221a dHD1HD2x. For any function g such that |g\u2032| \u2264 1 and |g| \u2264 1, \u2223\u2223\u2223\u2223\u22231d d\u2211 i=1 E [g(z(i))]\u2212 1 d d\u2211 i=1 E [g(y(i))]\n\u2223\u2223\u2223\u2223\u2223 \u2264 6\u2016x\u20162\u221ad . Proof. By triangle inequality,\u2223\u2223\u2223\u2223\u22231d d\u2211 i=1 E [g(z(i))]\u2212 1 d d\u2211 i=1 E [g(y(i))] \u2223\u2223\u2223\u2223\u2223 \u2264 1d d\u2211 i=1 |E[g(z(i))]\u2212 E[g(y(i))]|.\nLet u = HD2x. Then for every i, y(i) = \u2211 j H(i, j)D2(j)u(j). Hence by Lemma 7, we can relate expectation under y to the expectation under Gaussian distribution:\n|E[g(z(i))]\u2212 E[g(y(i))]| (a)= |E[E[g(z(i))]\u2212 E[g(y(i))|u]]|\n\u2264 3 2 d\u2211 i=1 E [ |u3(i)| ||u||22 ]\n= 3\n2 d\u2211 i=1 E [ |u3(i)| ||x||22 ] ,\nwhere the last equality follows from the fact that HD2 does not change rotation and (a) follows from the law of total expectation. By Cauchy-Schwartz inequality, for each i\nE[|u(i)|3] \u2264 \u221a E[u6(i)],\nIt can be shown that\nE[u6(i)] \u2264 15||x|| 6 2\nd3 ,\nSumming over all the indices yields the lemma.\nTheorem 3 follows from the Bochner\u2019s theorem and the fact that cos(\u00b7) satisfies requirements for the above lemma. We note that Theorem 3 holds for the matrix \u221a dHD1HD2 itself and the third component HD3 is not necessary to bound the bias."}, {"heading": "Appendix D Proof of Theorem 4", "text": "To prove Theorem 4, we use the Hanson-Wright Inequality.\nLemma 9 (Hanson-Wright Inequality). Let X = (X1, ..., Xn) \u2208 Rn be a random vector with independent subgaussian components Xi which satisfy: E[Xi] = 0 and \u2016Xi\u2016sg \u2264 K for some constant K > 0. Let A \u2208 Rn\u00d7n. Then for any t > 0 the following holds:\nP[|XTAX\u2212 E[XTAX]| > t] \u2264 2e \u2212cmin( t2 K4\u2016A\u20162 F , t K2\u2016A\u20162 ) ,\nfor some universal positive constant c > 0.\nProof of Theorem 4. For a vector u, let diag(u) denote the diagonal matrix whose entries correspond to the entries of u. For a diagonal matrix D, let vec(D) denote the vector corresponding to the diagonal entries of D. Let v = HD3z and u = HD2v = Hdiag(v)vec(D2). Observe that\n\u221a dHD1HD2HD3z = \u221a dHdiag(HD2HD3z)vec(D1).\nHence R\u0303 = \u221a dHdiag(HD2HD3z). Note that all the entries of \u221a dH have magnitude 1 and HD2HD3 do not change norm of the vector. Hence, each row of R\u0303 has norm ||z||2. To prove the orthogonality of rows of R\u0303, we need to show that for any i and j 6= i,\n\u221a d d\u2211 k=1 H(i, k)H(j, k)u2(k)\nis small. We first show that the expectation of the above quantity is 0 and then use the Hanson-Wright inequality to prove concentration. Let A be a diagonal matrix with kth entry being \u221a dH(i, k)H(j, k). The above equation can be rewriten as\nd\u2211 k=1 H(i, k)H(j, k)u2(k) = vec(D2)T diag(v)HTAHdiag(v)vec(D2).\nObserve that the (l, l) entry of the HTAH is\nd\u2211 k=1 HT (l, k)A(k, k)H(k, l) = d\u2211 k=1 H(k, l)A(k, k)H(k, l)\n= 1\nd d\u2211 k=1 A(k, k)\n= d\u2211 k=1 H(i, k)H(j, k) = 0,\nwhere the last equality follows from observing that the rows of H are orthogonal to each other. Together with the fact that elements of D2 are independent of each other, we get\nE[uTAu] = E[vec(D2)T diag(v)HTAHdiag(v)vec(D2)] = 0,\nTo prove the concentration result, observe that the entries of vec(D2) are independent and subGaussian, and hence we can use the Hanson-Wright inequality. To this end, we bound the Frobenius and the spectral norm of the underlying matrix. For the Frobenius norm, observe that\n||diag(v)HTAHdiag(v)||F (a)\n\u2264 (||v||\u221e)4 ||HTAH||F (b) = (||v||\u221e)4 ||A||F (c) = d (||v||\u221e)4 ,\nwhere (a) follows by observing that each diag(v) changes the Frobenius norm by at most ||v||2\u221e, (b) follows from the fact that H does not change the Frobenius norm, and (c) follows by substituting A.\nTo bound the spectral norm, observe that\n||diag(v)HTAHdiag(v)||2 (a)\n\u2264 (||v||\u221e)2 ||HTAH||2 (b) = (||v||\u221e)2 ||A||2 (c) = (||v||\u221e)2 ,\nwhere (a) follows by observing that each diag(v) changes the spectral norm by at most ||v||\u221e, (b) follows from the fact that rotation does not change the spectral norm, and (c) follows by substituting A. Since v = HD3z, by McDiarmid\u2019s inequality, it can be shown that with probability\u2265 1\u22122de\u2212d\n2/2, ||v||\u221e \u2264 ||z||2. Hence, by the Hanson-Wright inequality, we get\nPr ( \u221a d\nd\u2211 k=1 H(i, k)H(j, k)u2(k) > t||z||2\n) \u2264 2de\u2212d 2/2 + 2e\u2212cmin(t 2/(d 4),t/ 2),\nwhere c is a constant. Choosing = (t/d)1/3 results in the theorem."}, {"heading": "Appendix E Discrete Hadamard-Diagonal Structure in Binary Embedding", "text": "Motivated by the recent advances in using structured matrices in binary embedding, we show empirically that the same type of structured discrete orthogonal matrices (three blocks of HadamardDiagonal matrices) can also be applied to approximate angular distances for high-dimensional data. Let W \u2208 RD\u00d7d be a random matrix with i.i.d. normally distributed entries. The classic Locality\nSensitive Hashing (LSH) result shows that the sign nonlinear map \u03c6 : \u03c6(x) = 1\u221a D sign(Wx) can be used to approximate the angle, i.e., for any x,y \u2208 Rd\n\u03c6(x)T\u03c6(y) \u2248 \u03b8(x,y)/\u03c0.\nWe compare random projection based Locality Sensitive Hashing (LSH) [4], Circulant Binary Embedding (CBE) [28] and Kronecker Binary Embedding (KBE) [30]. We closely follow the experimental settings of [30]. We choose to compare with [30] because it proposed to use another type of structured random orthogonal matrix (Kronecker product of orthogonal matrices). As shown in Figure 6, our result (HDHDHD) provides higher recall and lower angular MSE in comparison with other methods."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Practical and optimal lsh for angular distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Harmonic analysis and the theory of probability", "author": ["S. Bochner"], "venue": "Dover Publications,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1955}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Lecture notes on Stein\u2019s method and applications", "author": ["S. Chatterjee"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Y. Cheng", "F.X. Yu", "R.S. Feris", "S. Kumar", "A. Choudhary", "S.-F. Chang"], "venue": "ICCV,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Triplespin-a generic compact paradigm for fast machine learning computations", "author": ["K. Choromanski", "F. Fagan", "C. Gouy-Pailler", "A. Morvan", "T. Sarlos", "J. Atif"], "venue": "arXiv,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "author": ["K. Choromanski", "V. Sindhwani"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Unified matrix treatment of the fast walsh-hadamard transform", "author": ["B.J. Fino", "V.R. Algazi"], "venue": "IEEE Transactions on Computers, (11):1142\u20131146,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1976}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "AISTATS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast cross-polytope locality-sensitive hashing", "author": ["C. Kennedy", "R. Ward"], "venue": "arXiv,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "ICML,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Random fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition, pages 262\u2013271,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "ICCV,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Aspects of multivariate statistical theory, volume 197", "author": ["R.J. Muirhead"], "venue": "John Wiley & Sons,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Quasi-Monte Carlo Methods", "author": ["H. Niederreiter"], "venue": "Wiley Online Library,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F.X. Yu", "S. Kumar"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalization properties of learning with random features", "author": ["A. Rudi", "R. Camoriano", "L. Rosasco"], "venue": "arXiv:1602.04474,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM, volume = 127, year", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["V. Sreekanth", "A. Vedaldi", "A. Zisserman", "C. Jawahar"], "venue": "BMVC,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal rates for random fourier features", "author": ["B. Sriperumbudur", "Z. Szab\u00f3"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480\u2013492,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Quasi-monte carlo feature maps for shift-invariant kernels", "author": ["J. Yang", "V. Sindhwani", "H. Avron", "M. Mahoney"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Circulant binary embedding", "author": ["F.X. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Compact nonlinear maps and circulant extensions", "author": ["F.X. Yu", "S. Kumar", "H. Rowley", "S.-F. Chang"], "venue": "arXiv:1503.03893,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast orthogonal projection based on kronecker product", "author": ["X. Zhang", "F.X. Yu", "R. Guo", "S. Kumar", "S. Wang", "S.-F. Chang"], "venue": "ICCV,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction Kernel methods are widely used in nonlinear learning [9], but they are computationally expensive for large datasets.", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "Kernel approximation is a powerful technique to make kernel methods scalable, by mapping input features into a new space where dot products approximate the kernel well [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].", "startOffset": 163, "endOffset": 171}, {"referenceID": 21, "context": "With accurate kernel approximation, efficient linear classifiers can be trained in the transformed space while retaining the expressive power of nonlinear methods [11, 22].", "startOffset": 163, "endOffset": 171}, {"referenceID": 19, "context": "Random Fourier Features [20] are used widely in approximating smooth, shift-invariant kernels.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "The second property guarantees that the Fourier transform of K(\u2206) is a nonnegative function [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 232, "endOffset": 236}, {"referenceID": 11, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 261, "endOffset": 269}, {"referenceID": 18, "context": "2 Related Works Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels [16], generalized RBF kernels [23], skewed multiplicative histogram kernels [15], additive kernels [25], and polynomial kernels [12, 19].", "startOffset": 261, "endOffset": 269}, {"referenceID": 19, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 20, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 23, "context": "In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework [20], which has been extensively studied both theoretically and empirically [27, 21, 24].", "startOffset": 196, "endOffset": 208}, {"referenceID": 17, "context": "It is well known that the convergence of MonteCarlo can be largely improved by carefully choosing a deterministic sequence instead of random samples [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "[26] proposed to use low-displacement rank sequences in RFF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] studied optimizing the sequences in a data-dependent fashion to achieve more compact maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Compared to [26], the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Furthermore, unlike [29], the results in this paper are data independent.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 27, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 28, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 7, "context": "Structured matrices have been used for speeding up dimensionality reduction [1], binary embedding [28], deep neural networks [6] and kernel approximation [14, 29, 8].", "startOffset": 154, "endOffset": 165}, {"referenceID": 19, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 13, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 19, "context": "Method Extra Memory Time Lower error than RFF? Random Fourier Feature (RFF) [20] O(Dd) O(Dd) Compact Nonlinear Map (CNM) [29] O(Dd) O(Dd) Yes (data-dependent) Quasi-Monte Carlo (QMC) [26] O(Dd) O(Dd) Yes Structured (fastfood/circulant) [29, 14] O(D) O(D log d) No Orthogonal Random Feature (ORF) O(Dd) O(Dd) Yes Structured ORF (SORF) O(D) or O(1) O(D log d) Yes Table 1: Comparison of different kernel approximation methods under the framework of Random Fourier Features [20].", "startOffset": 471, "endOffset": 475}, {"referenceID": 16, "context": "Q is distributed uniformly on the Stiefel manifold (the space of all orthogonal matrices) based on the Bartlett decomposition theorem [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 28, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 7, "context": "Note that the previous works on fast kernel approximation using structured matrices do not use structured orthogonal matrices [14, 29, 8].", "startOffset": 126, "endOffset": 137}, {"referenceID": 9, "context": "The computation of SORF can also be carried out with almost no extra memory due to the fact that both sign flipping and the Walsh-Hadamard transformation can be efficiently implemented as in-place operations [10].", "startOffset": 208, "endOffset": 212}, {"referenceID": 25, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 143, "endOffset": 151}, {"referenceID": 28, "context": "For fixed D, the kernel approximation MSE exhibits the following ordering: SORF ' ORF < QMC [26] < RFF [20] < Other fast kernel approximations [14, 29].", "startOffset": 143, "endOffset": 151}, {"referenceID": 28, "context": "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "All other fast kernel approximation methods, such as circulant [29] and FastFood [14] have higher MSE.", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "We also include DigitalNet, the best performing method among Quasi-Monte Carlo techniques [26].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 80, "endOffset": 88}, {"referenceID": 28, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 80, "endOffset": 88}, {"referenceID": 25, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "The order of time cost for a fixed D is SORF ' Other fast kernel approximations [14, 29] ORF = QMC [26] = RFF [20].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Throughout the experiments, \u03c3 for each dataset is chosen to be the mean distance of the 50th `2 nearest neighbor, which empirically yields good classification results [29].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "In the appendix, we show empirically that the same scheme can be successfully applied to angle estimation where the nonlinear map f is a non-smooth sign(\u00b7) function [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 12, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 6, "context": "We note that the HD1HD2HD3 structure has also been recently used in fast cross-polytope LSH [2, 13, 7].", "startOffset": 92, "endOffset": 102}, {"referenceID": 18, "context": "Our method can also be generalized to other types of kernels such as general shift-invariant kernels and polynomial kernels based on Schoenberg\u2019s characterization as in [19].", "startOffset": 169, "endOffset": 173}], "year": 2016, "abstractText": "We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost fromO(d) toO(d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.", "creator": "LaTeX with hyperref package"}}}