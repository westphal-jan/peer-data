{"id": "1610.09372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Multi-agent projective simulation: A starting point", "abstract": "We develop an invasion game with two defenders (Alice and Bob) using the method of projective simulation. The agent, say Alice, encounters attack symbols of the right attacker, where she can learn to prevent them. However, some of these signs are invisible to her. Instead, she perceives some other signs related to Bob's task. We elaborate an example in which an agent perceives an equal share of the perceptions of both attackers. Alice can choose to focus on her work, although she loses some attacks. Alternatively, she can collaborate with Bob to get help and give. Therefore, we conclude that the maximum blocking efficiency in cooperation is only the minimal blocking efficiency in cooperation. Furthermore, Alice has the choice between two different oblivion factors for her task or help. Therefore, she can choose between herself and the other. As a main result, we come to the conclusion that if she selfishly chooses Bob, the blocking efficiency is likely to be more selective efficiency in her task, even if it deserving a higher efficiency in both areas.", "histories": [["v1", "Sat, 29 Oct 2016 10:35:53 GMT  (1360kb)", "http://arxiv.org/abs/1610.09372v1", "17 pages, 11 figures"], ["v2", "Sun, 4 Jun 2017 16:28:34 GMT  (1036kb)", "http://arxiv.org/abs/1610.09372v2", "22 pages, 13 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["rasoul kheiri"], "accepted": false, "id": "1610.09372"}, "pdf": {"name": "1610.09372.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["R. Kheiri"], "emails": ["r.kheiry@ph.iut.ac.ir"], "sections": [{"heading": null, "text": "We develop a two-defender (Alice and Bob) invasion game using the method of projective simulation. The agent, say Alice, encounters attack\u2019 symbols coming from the right attacker where she can learn to prevent. However, some of these signs are invisible for her. Instead, she perceives some other signs that are related to Bob\u2019s task. We elaborate an example in which an agent perceives an equal portion of percepts from both attackers. Alice can choose to concentrate on her job, though she loses some attacks. Alternatively, she can have some collaboration with Bob to get and give help. It is concluded that the maximum blocking efficiency in concentration is just the minimum blocking efficiency in collaboration. Furthermore, Alice has a choice to select two different forgetting factors for her task or helping. Therefore, she can choose between herself and the other. As the main result, we conclude that if Alice selects to be selfish, she probably earns more blocking in her task and also, higher efficiency in collective blocking, regardless of Bob\u2019s selection. In addition, it turns out that when the selection of both partners is selfishness, it is the highest justice on sharing individual efficiency and it is a maximum in collective blocking efficiency too. Finally, we propose some other questions that can be tracked regarding the present study."}, {"heading": "1- Introduction", "text": "In the article \u201cprojective simulation for artificial intelligence\u201d [1] to put it briefly, there is an agent that encounters some percepts and its aim is learning to select efficient actions in new conditions via a random walk in its episodic memory. The agent thinks of previously rewarded or nonrewarded actions, which build new transition probabilities between its memory clips of episodic memory where the random walk takes place. The authors apply the formulation of the projective simulation for a defender in an invasion game and check the speed of learning, maximum blocking efficiency, etc. Then, they investigate learning with the composition in which the memory itself can be changed with a change in clips or even with the creation of new clips.\nIn the current proposal, we try to expand the context of projective simulation to include interchangeable information between two defenders named Alice and Bob in an invasion game\nwhere we might have collaboration between them in a situation in which they are not completely isolated.\nConsider, for example, a situation like that illustrated in figure 1, where there are two attackers (A1, A2) and two defenders (D1= Alice, D2= Bob), with their precept spaces \ud835\udc461 and \ud835\udc462 for D1 and D2, respectively. Here, the condition is that the defenders are isolated in their action space, but are not isolated in their percept spaces. We divide the signs coming from each attacker to two portions, blue and red portions. If there is not any intersection between blue and red percepts, we can write\nA1 signs = blue signs of A1 + red signs of A1,\nA2 signs = blue signs of A2 + red signs of A2.\nThe main applied constraint is that one portion of the signs of A1 is invisible for Alice, say red signs of A1; instead, one portion of the signs of A2 is visible for her (red signs of A2). The situation could be the same for Bob too. Thus, For Alice (Bob), the blue percepts directly come from A1 (A2), where the defender can learn them and accomplish right actions for them. However, the blue percepts are less than what Alice needs to cover all attacks of A1. It means that A1 (A2) sends some signs, but not all, in blue for the right defender Alice (Bob) and sends the rest of the signs to Bob(Alice) in red. Hence, the red percepts of Alice (Bob) come from D2 (D1), where the defender does not need them for her (his) actions. However, Alice can learn the red signs for Bob and sends green signs for him and vice versa (see figure 1). Therefore, the percepts-action space for both of them is as\n\ud835\udc461 \u2261 \ud835\udc462: {\u21d2,\u21d0,\u21d2,\u21d0,\u21d2,\u21d0},\n\ud835\udc341 \u2261 \ud835\udc342: {+,\u2212,\u21d2,\u21d0}.\nHere we choose not to distinguish between green actions and green percepts. In a generic situation, there is an open question related to such choices; what is the maximum reduction we could have for (S\u00d7 \ud835\udc34) space related to a given situation? (Especially, reduction of actions would be more important so that Alice could do fewer actions and increase her dilation time (\ud835\udc37_\ud835\udc5a\ud835\udc4e\ud835\udc65) dramatically). In the other words, can we map a situation to another one with a smaller \u210e matrix? Nevertheless, we have a choice to assign a separate episodic memory and engine for the implementation of green actions that would divide the (S\u00d7 \ud835\udc34) space to two subspaces.\nSuppose that Alice sees a given fraction (\ud835\udefc) of all signs coming from A1, which are blue percepts, and she can also see some red percepts coming from A2. Alice has a choice to ignore the red signs by focusing on blue ones and learns her job to reach the maximum blocking efficiency in \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65 = \ud835\udefc/1 if her forgetting factor \ud835\udefe is zero. In this way, she will lose to perform actions for the fraction of 1 \u2212 \ud835\udefc attacks of A1. As another choice, on the other hand, she can have some collaboration with Bob. Each of them can analyzes the red signs separately and interchange green signs that contain indirect information about the reds. Then, eventually, Alice can make use of green signs coming from Bob for action against the attacker A1.\nHowever, in collaboration, the speed of learning is decreased. Since Bob himself is learning the reds, the green signs coming from Bob are not the same as the-reds perceived before. Therefore, there needs some more time for Alice to do the right actions against the green percepts. In addition, the deliberation time is increased in collaboration. Because the percept space for each partner is increased, there can be some dilation in deliberation time raised by expanding \u210e_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65. Moreover, more importantly, \ud835\udc37_\ud835\udc5a\ud835\udc4e\ud835\udc65 is decreased directly, because Alice needs to carry out more actions (\ud835\udefc \u2212 1 more actions at least, if she has a separate engine for the green signs) at a given time, in comparison with the time when she concentrates on the blues.\nEven for the problem without considering deliberation time, there can be serious questions like a comparison between maximum blocking efficiencies related to different forgetting factors for Alice and Bob, etc. We are about to answer such questions in the following example."}, {"heading": "2- A specific example", "text": "Suppose a problem in which Alice receives half of all signs come from A1 (\ud835\udefc = 0.5) in blue and she can also see half of all the signs coming from A2 in red. The situation is just the same for Bob. We derive the blocking efficiency by assigning a given \u210e matrix for both scenarios of concentration (that is ignoring the red percepts) and collaboration.\nConcentration \ud835\udc89 matrix. If Alice ignores the reds and does action just for the blue percepts, then one can write\n\u210e = (\n\u210e(\u21d2,\u2212) \u210e(\u21d2,+) \u210e(\u21d0,\u2212) \u210e(\u21d0,+)\n0 0 0 0\n) , \ud835\udc43(\ud835\udc60) = 1\n4 \u27f6 \ud835\udc5f(\ud835\udc5b) =\n1 4 [\ud835\udc43(\u2212| \u21d0) + \ud835\udc43(+| \u21d2)],\nwhere the rows are percepts and the columns are actions. We neglect writing the superscript of \ud835\udc5b for the conditional probabilities \ud835\udc43(\ud835\udc4e|\ud835\udc60) for ease. Also, in this example, Alice and Bob are in the same situation and we neglect writing the subscript for distinguishing them. In this scenario, the maximum blocking efficiency for Alice (or Bob) becomes \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65 = 0.5/1. Nevertheless, the concentration \u210e matrix might be different (for example, see section 4, proposition number 4).\nCollaboration \ud835\udc89 matrix. One can easily expand the concentration matrix to the collaboration matrix by considering the whole percept-action space (\ud835\udc46 \u00d7 \ud835\udc34), including the reds and the greens as\n\u210e =\n(\n\u210e(\u21d2,\u2212) \u210e(\u21d2,+) \u210e(\u21d0,\u2212) \u210e(\u21d0,+)\n0 0 0 0\n\u210e(\u21d2,\u2212) \u210e(\u21d2,+) \u210e(\u21d0,\u2212) \u210e(\u21d0,\u2212)\n0 0 0 0\n0 0 0 0\n\u210e(\u21d2,\u21d0) \u210e(\u21d2,\u21d2) \u210e(\u21d0,\u21d0) \u210e(\u21d0,\u21d2))\n.\nIt turns out that since this matrix is block diagonal, one can consider two separate matrices instead. One action matrix (4 \u00d7 2) for the blue and green percepts, and one help matrix (2 \u00d7 2) for the red percepts and the green actions. At first, we can notice that, by postulate, the helping matrix has no role in blocking efficiency and therefore, \ud835\udc5d(\ud835\udc60) = 1 4\u2044 . Second, the blue part of percepts has a direct portion in the blocking efficiency, whereas the green percepts have an indirect effect on it.\nIf A1 had sent all of its attack\u2019s signs to Alice, she would have a blocking efficiency as\n\ud835\udc5f(\ud835\udc5b) = 1\n4 [\ud835\udc43(\u2212| \u21d0) + \ud835\udc43(+| \u21d2) + \ud835\udc43(\u2212| \u21d0) + \ud835\udc43(+| \u21d2)]\nUnfortunately, the red signs of A1 (A2) are sent to Bob (Alice), as shown in figure 1; hence the red probabilities \ud835\udc43(\u2213|\ud835\udc5f\ud835\udc52\ud835\udc51) are related to the helping matrix of Bob(Alice), representing \ud835\udc43(\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52\ud835\udc5b|\ud835\udc5f\ud835\udc52\ud835\udc51) and green probabilities of Alice(Bob) \ud835\udc43(\u2213|\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52\ud835\udc5b) as\n\ud835\udc43(\u2212| \u21d0) = [\ud835\udc43(\u21d0 | \u21d0) \u2217 \ud835\udc43(\u2212| \u21d0)] + [ \ud835\udc43(\u21d2 | \u21d0) \u2217 \ud835\udc43(\u2212| \u21d2) ],\n\ud835\udc43(+| \u21d2) = [ \ud835\udc43(\u21d2 | \u21d2) \u2217 \ud835\udc43(+| \u21d2)] + [\ud835\udc43(\u21d0 | \u21d2) \u2217 \ud835\udc43(+| \u21d0)].\nThus\n\ud835\udc5f(\ud835\udc5b) = 1\n4 {\ud835\udc43(\u2212| \u21d0) + \ud835\udc43(+| \u21d2) + [\ud835\udc43(\u21d0 | \u21d0) \u00d7 \ud835\udc43(\u2212| \u21d0)] + [\ud835\udc43(\u21d2 | \u21d0) \u00d7 \ud835\udc43(\u2212| \u21d2)]\n+ [ \ud835\udc43(\u21d2 | \u21d2) \u00d7 \ud835\udc43(+| \u21d2)] + [ \ud835\udc43(\u21d0 | \u21d2) \u00d7 \ud835\udc43(+| \u21d0)]}.\nIn the following, we program our model and illustrate some properties in figures. Our programming has some important assumptions:\n1- There is just one blue or green percept randomly for a given agent in the given time \ud835\udc5b. 2- Moreover, the system has reached the equilibrium in the case of receiving blue and green\npercepts so that a given agent perceive an equal amount of blue or green signs in a large time. Hence, the probability that an agent encounters each of \u21d2,\u21d0,\u21d2,\u21d0 is equal (1/4). 3- It is assumed that receiving the red percepts does not interfere with receiving the blue-\ngreen ones. One can say that the red percepts are supposed to go to a different episodic memory and the chosen actions related to the reds take place in a second engine. Thus, the probability that an agent encounters each of \u21d2,\u21d0 is one-half. 4- We have modified the effect of forgetting factor (\ud835\udefe) in rewarding actions slightly\ndifferently from the referenced paper [1]. Because by using equations (4) or (6) in the\npaper [1], rewarding actions can not be forgotten completely, even in the case of \ud835\udefe = 1. Our modification is as\n{ \u210e\ud835\udc5b+1(\ud835\udc60, \ud835\udc4e) = \u210e\ud835\udc5b(\ud835\udc60, \ud835\udc4e) + 1 \u2212 \ud835\udefe\u210e\ud835\udc5b(\ud835\udc60, \ud835\udc4e), \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51,\n\u210e\ud835\udc5b(\ud835\udc60, \ud835\udc4e) = 1, \ud835\udc5b\ud835\udc5c\ud835\udc5b \u2212 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51."}, {"heading": "Results", "text": "In the following, we compare the blocking efficiency of Alice in a variety of scenarios with different forgetting factor Gammas, without using \ud835\udc37_\ud835\udc5a\ud835\udc4e\ud835\udc65. In the next step, the collective blocking efficiency of Alice and Bob is calculated in different selections.\nFigure 2 shows the blocking efficiency of one agent, say Alice, in collaboration (solid lines) in comparison with the time when Alice can see all percepts she needs to defend against A1 (dashed lines). The speed of learning is reduced slightly when Bob helps Alice in comparison with the time when Alice perceives all percepts she needs to do defense; it is because Bob needs time to learn too. However, the maximum blocking efficiency ultimately becomes equal in both scenarios.\nFigure 3 indicates a gap between blocking efficiency in concentration and collaboration for one agent, say Alice. We can see that if Alice only acts against her blue percepts, she obviously has less efficiency, even if Bob sends random signs that are not related to the real red percepts (Bob\u2019s Gamma=1). As a result, it follows that the maximum blocking efficiency in concentration is the minimum of that of collaboration.\nFigure 4 compares some different amounts of Gammas for Alice and Bob. It shows that Alice forgetting factor is much more important for her blocking efficiency than the Bob\u2019s Gamma.\nAlthough we have observed that Alice forgetting factor is much more important for her efficiency than Bob\u2019s, one can speed up the learning in a given situation, as shown in figure 5.\nSelection between self and the other\nSo far, we had one Gamma for each partner. Now we study a scenario in which each partner has two different Gammas. It means that Alice can forget what she has been learning about the blue percepts (with the forgetting factor \ud835\udefe1) more or less than her helping job (with the forgetting factor \ud835\udefe2). As a definition, we refer to \ud835\udc34(\ud835\udefe1\ud835\udc34, \ud835\udefe2\ud835\udc34) as Alice\u2019s Gammas, where the first Gamma is related to the (4 \u00d7 2) matrix, while the second one is related to the (2 \u00d7 2) matrix. The question is that if, \ud835\udefe1\ud835\udc34 + \ud835\udefe2\ud835\udc34 = \ud835\udefe1\ud835\udc35 + \ud835\udefe2\ud835\udc35 = \ud835\udc50 and 0 \u2264 \ud835\udc50 \u2264 1, where the collective efficiency of Alice and Bob becomes maximum. In this situation, clearly, Alice and Bob have infinite choices between being selfish \ud835\udefe \u2261 (0, \ud835\udc50) and sacrificing \ud835\udefe \u2261 (\ud835\udc50, 0).\nHere we choose \ud835\udc50 = 1 and have a look at the results. Figure 6 compares collective blocking efficiencies for two different fixed selections for Bob, by changing Alice\u2019s forgetting factor. In Figure 6a, Bob sacrifices himself, whereas Bob is selfish in Figure 6b. It turns out to be that the extremums happen when Alice also selects one of the choices of selfishness or sacrificing. We can notice that in the case Bob is selfish (figure 6b), the speed of reaching to the maximum collective blocking efficiency is increased when Alice scarifies herself, though both selfishness or sacrificing choices for Alice give rise to an equal maximum collective blocking efficiency.\nFigure 6a Figure 6b\nFigure 6: Comparing different choices for Alice, when Bob's forgetting factor is fixed to be sacrificing himself in figure 6a and being selfish in figure 6b. Five different pairs of \ud835\udc34(\ud835\udefe1\ud835\udc34, \ud835\udefe2\ud835\udc34) are selected for Alice\u2019s forgetting factor with the condition \ud835\udefe1\ud835\udc34 + \ud835\udefe2\ud835\udc34 = 1.\nIn the continuance, we add the blocking efficiency of two partners together as the collective blocking efficiency in a different pair of (\u03b31, \u03b32) selections. Figure 7 is exhibiting the collective blocking efficiency of Alice and Bob where both agents have an identical selection with the condition \u03b31 + \u03b32 = 1.\nIn Figures 8a and 8b, we have chosen five different selections for each pair of (\u03b31, \u03b32) that are related to each partner. Then, we\u2019ve calculated the collective blocking efficiencies in a given large time (n=5000) for all 25 of A(\u03b31, \u03b32), B(\u03b31, \u03b32) selections for Alice and Bob, which are 25 representatives of all possible scenarios with the condition \u03b31 + \u03b32 = 1. Consequently, there are three absolute maximums and one absolute minimum in all collective blocking efficiencies among all possible selections.\nFinally, figure 9 is a 3-dimension picture of figure 8a, which involves time as a variable too.\nWe can classify the 5 extremums (four of them are in figures 8 and 9 and one of them is in figure 3) of collective blocking efficiency as shown in table 1. It follows that the Selfish-Selfish class provides justice in individual blocking efficiencies better than every alternative scenario, and it also provides the highest collective blocking efficiency too.\nAfter this, we bring a key question here. Suppose that Alice, naturally, does not know about the selection of Bob. What is the best selection for Alice to have a higher efficiency for her blocking against A1 and also, provides higher collective blocking? Since Bob has his rule to the answer, our answer must be probabilistic. Hence, the question arising is; which selection is the most probable for higher individual and collective blocking efficiency?\nAt first, we analytically derive the formula of the maximum blocking efficiency. For the rewarded actions, we have\n\u210e\ud835\udc5b+1 = \u210e\ud835\udc5b(1 \u2212 \ud835\udefe) + 1, \u210e1 = 1\nTo begin with \u210e2 = 2 \u2212 \ud835\udefe, we can write\n\u210e\ud835\udc5b = (1 + \ud835\udc4e) \ud835\udc4e\ud835\udc5b\u22122 + \u2211\ud835\udc4e\ud835\udc56 \ud835\udc5b\u22123\n\ud835\udc56=0\n, \ud835\udc4e = 1 \u2212 \ud835\udefe.\nFor 0 < \ud835\udefe < 1 and large \ud835\udc5b, the sigma is a geometric series and therefore,\nlim \ud835\udc5b\u2192\u221e\n\u210e\ud835\udc5b = 1\n\ud835\udefe , 0 < \ud835\udefe < 1\nOn the other hand, for \ud835\udefe = 0,1 the results agree with 1 \ud835\udefe\u2044 in large n. Thus,\nlim \ud835\udc5b\u2192\u221e\n\u210e\ud835\udc5b = 1\n\ud835\udefe , 0 \u2264 \ud835\udefe \u2264 1.\nThen\n\u210e\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc5b = {\n1 \ud835\udefe , \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 1, \ud835\udc5b\ud835\udc5c\ud835\udc5b \u2212 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\nWe can now derive the maximum conditional probabilities. With |\ud835\udc34| = 2 (two different actions) they are\n{ \ud835\udc43\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc4e|\ud835\udc60) =\n1\n1 + \ud835\udefe , \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\n\ud835\udc43\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc4e|\ud835\udc60) = \ud835\udefe\n1 + \ud835\udefe , \ud835\udc5b\ud835\udc5c\ud835\udc5b \u2212 \ud835\udc5f\ud835\udc52\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc51 \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc52) = 1\n2 [\n1\n1 + \ud835\udefe1\ud835\udc34 + 1 + \ud835\udefe1\ud835\udc34\ud835\udefe2\ud835\udc35 (1 + \ud835\udefe1\ud835\udc34)(1 + \ud835\udefe2\ud835\udc35) ]\nIn the condition of \ud835\udefe1\ud835\udc34 + \ud835\udefe2\ud835\udc34 = \ud835\udefe1\ud835\udc35 + \ud835\udefe2\ud835\udc35 = 1, one can write\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc52) = 1\n2 [\n1\n1 + \ud835\udc65 +\n1 + \ud835\udc65\ud835\udc66\n(1 + \ud835\udc65)(1 + \ud835\udc66) ], { \ud835\udefe1\ud835\udc34 = \ud835\udc65 \ud835\udefe2\ud835\udc35 = \ud835\udc66\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc35\ud835\udc5c\ud835\udc4f) = 1\n2 [\n1\n2 \u2212 \ud835\udc66 +\n1 + (1 \u2212 \ud835\udc65)(1 \u2212 \ud835\udc66)\n(2 \u2212 \ud835\udc65)(2 \u2212 \ud835\udc66) ],\nand\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52) = \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc52) + \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc35\ud835\udc5c\ud835\udc4f).\nWe\u2019ve illustrated \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc50\ud835\udc52) in figure 10. It can be seen that regardless of Bob\u2019s selection, the highest plot is for the time when Alice is selfish. Therefore, if Alice chooses to be selfish, she probably will earn more blocking efficiency for her job, though it is the probabilistic best selection not the certain.\nMoreover, figure 11 shows the maximum collective \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52) efficiency. Again, one can see that, if Alice selects to be selfish, she probably will increase the collective blocking efficiency too.\nFurthermore, having \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65, one can easily re-plot figure 8b for both collective and individual efficiencies using continuously different selections."}, {"heading": "3- Conclusion", "text": "We studied a model of projective simulation for a two attacker-defender invasion game in which defenders (Alice and Bob) could choose between loneliness and a variety of collaborations. In our model, each defender encountered a given portion of the signs (blue percepts) that she/he needed to prevent all attacks, while the remaining percepts were invisible for her/him. However, the agents could collaborate with each other by learning what their partners needed (red percepts) as come from partner\u2019s attacker. Then, they could help each other by getting and giving green percepts, which contained indirect information about the reds.\nWe elaborated an example in which every partner received an equal portion of blue and red percepts which were half of two attackers\u2019 signs. The agent, say Alice, could choose concentration on learning her blue percepts and lose half of her blocking efficiency. Nevertheless, she could interchange indirect green percepts with Bob, that contained indirect information from the rest of attacks. We concluded that the maximum blocking efficiency in concentration was just the minimum blocking efficiency in collaboration. However, in comparison with the situation in which all the attack\u2019s signs would be visible for Alice, the speed of learning was decreased in collaboration. Something more rewarding in collaboration was that, even if Bob completely forgot what he had been learning about red signs and therefore, sent the random green signs to Alice, she could still derive a good benefit from collaboration with the maximum blocking efficiency of 0.75 out of 1. As another important property of collaboration we found that, the\namount of forgetting factor of Alice on her defense job was much more important for her blocking efficiency than Bob\u2019s forgetting factor on the reds.\nIn the continuance, we went to assign a constant additional forgetting factor of one to each partner for learning about the blue and red signs. As a conclusion, we illustrated three maxima in collective blocking efficiency that where classified as Selfish-Selfish, Parasite-Host, and HostParasite scenarios. Also, the Selfish-Selfish scenario could be considered as the best justice between two partners because each of two partners could earn an equal portion of blocking efficiency with a maximum number in 0.75/1, that was much more than Loser-Loser or concentration scenarios (with the maximum of 0.5/1). One can observe that if the blocking efficiency of one of the individuals goes upper than 0.75 by \ud835\udc65, the blocking efficiency of his/her partner definitely goes lower than 0.75 by more than \ud835\udc65; therefore, collective efficiency is decreased, unless the other partner sacrifices himself/herself dramatically and converts himself/herself to a host for his/her parasitic partner. Sacrificing is not beneficial for the host; instead it has collective benefits. This is because, although both the Selfish-Selfish and the Parasite-Host (or vice versa) scenarios have the maximum collective blocking efficiency of 1.5, the Parasite-Host scenario reaches to the maximum sooner than the Selfish-Selfish scenario.\nFinally, as the main result, it turned out to be that when Alice selected to be selfish, regardless of Bob\u2019s selection, she could probably earn more blocking efficiency for herself and collective blocking efficiency."}, {"heading": "4- Some other possible questions", "text": "1- In our solved example, we took a fifty- fifty percent (\ud835\udefc) on blue and red percepts. We\ncould study our model for the different amounts of \ud835\udefc. Moreover, it might be possible that Alice\u2019s alpha (\ud835\udefc\ud835\udc34) differed from Bob\u2019s (\ud835\udefc\ud835\udc35).\n2- Solving this problem for three agents may give us much more social properties.\n3- We can assign some weights (\ud835\udc5a) for the values of Alice\u2019s blocking and collective blocking\nto define the average of efficiency as\n\ud835\udc34\ud835\udc38 = \ud835\udc34\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc54\ud835\udc52 \ud835\udc5c\ud835\udc53 \ud835\udc52\ud835\udc53\ud835\udc53\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc50\ud835\udc66 = \ud835\udc5a\ud835\udc34\ud835\udc5f\ud835\udc34 +\ud835\udc5a\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc5f\ud835\udc50\ud835\udc5c\ud835\udc59 \ud835\udc5a\ud835\udc34 +\ud835\udc5a\ud835\udc50\ud835\udc5c\ud835\udc59 ,\nif \ud835\udc5a\ud835\udc34 = \ud835\udc58 \ud835\udc5a\ud835\udc50\ud835\udc5c\ud835\udc59, then we can illustrate the average of efficiency for different amounts of \ud835\udc58 as\n\ud835\udc34\ud835\udc38 = \ud835\udc5f\ud835\udc34 + \ud835\udc58 \ud835\udc5f\ud835\udc50\ud835\udc5c\ud835\udc59 1 + \ud835\udc58 , \ud835\udc58 = [0,\u221e).\n4- We know that if Alice does random actions for the invisible percepts, then she can close\nthe green window and earn a maximum blocking efficiency of 0.75 without any help from Bob. Therefore, we can check the model for the concentration matrix when the action space is \ud835\udc34: {+,\u2212, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a \ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b}.\n5- Studying different amounts of \ud835\udc50 other than \ud835\udc50 = 1 in \ud835\udefe1 + \ud835\udefe2 = \ud835\udc50.\n6- Investigating a scenario in which two partners are not isolated in their actions (Alice can\nhave action in part with the red percepts coming from A2 and gain more scores). So the key question is: what happens if we have an intersection between Alice\u2019s action space and Bob\u2019s action space?\n7- Studying what would be happening in the presence of \ud835\udc37_\ud835\udc5a\ud835\udc4e\ud835\udc65, in both isolated and non-\nisolated action spaces.\n8- Studying a situation in which the red percepts are also helpful for Alice to do the right\nactions against A1, but it is much more useful for Bob.\n9- Studying the effect of reflection (R) on various scenarios of 2-defenders invasion game.\n10- Comparing with other theories:\na) Is there any correspondence between such a model of projective simulation and game\ntheory? Can we formulate a game theory in terms of projective simulation?\nb) Comparison with Transactional Analysis in Psychology. For example, I can refer to\nchapter 11 of the book \u201cAn introduction to transactional analysis, Ian Stewart, Vann Joines, 2nd edition,1987\u201d where two humans acted in a winner-winner or loser-loser play corresponding to their scripts.\nc) Comparison with the notion of Selfish Gene \u201cThe Selfish Gene wrote by Richard\nDawkins\u201d, etc. as a reformulation of the theory of natural selection in Biology.\n11- Can we derive a benefit in our model in robotic sports such as tennis, scooch? Here, two\ninvaders might be reduced to one ball!\n12- Limits of projective simulation: In statistical mechanics, we are not able to consider more\nthan a few Newtonian particles for calculating thermodynamic variables. Because the time for evaluating by a computer goes upper than a sensible time. Therefore, to what extent could we add players to our game in a projective simulation?"}], "references": [{"title": "Projective simulation for artificial intelligence", "author": ["H.J. Briegel", "G. De las Cuevas"], "venue": "Sci. Rep", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1- Introduction In the article \u201cprojective simulation for artificial intelligence\u201d [1] to put it briefly, there is an agent that encounters some percepts and its aim is learning to select efficient actions in new conditions via a random walk in its episodic memory.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "4- We have modified the effect of forgetting factor (\u03b3) in rewarding actions slightly differently from the referenced paper [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 0, "context": "paper [1], rewarding actions can not be forgotten completely, even in the case of \u03b3 = 1.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "Reference [1] Briegel, H.", "startOffset": 10, "endOffset": 13}], "year": 2016, "abstractText": "We develop a two-defender (Alice and Bob) invasion game using the method of projective simulation. The agent, say Alice, encounters attack\u2019 symbols coming from the right attacker where she can learn to prevent. However, some of these signs are invisible for her. Instead, she perceives some other signs that are related to Bob\u2019s task. We elaborate an example in which an agent perceives an equal portion of percepts from both attackers. Alice can choose to concentrate on her job, though she loses some attacks. Alternatively, she can have some collaboration with Bob to get and give help. It is concluded that the maximum blocking efficiency in concentration is just the minimum blocking efficiency in collaboration. Furthermore, Alice has a choice to select two different forgetting factors for her task or helping. Therefore, she can choose between herself and the other. As the main result, we conclude that if Alice selects to be selfish, she probably earns more blocking in her task and also, higher efficiency in collective blocking, regardless of Bob\u2019s selection. In addition, it turns out that when the selection of both partners is selfishness, it is the highest justice on sharing individual efficiency and it is a maximum in collective blocking efficiency too. Finally, we propose some other questions that can be tracked regarding the present study. 1Introduction In the article \u201cprojective simulation for artificial intelligence\u201d [1] to put it briefly, there is an agent that encounters some percepts and its aim is learning to select efficient actions in new conditions via a random walk in its episodic memory. The agent thinks of previously rewarded or nonrewarded actions, which build new transition probabilities between its memory clips of episodic memory where the random walk takes place. The authors apply the formulation of the projective simulation for a defender in an invasion game and check the speed of learning, maximum blocking efficiency, etc. Then, they investigate learning with the composition in which the memory itself can be changed with a change in clips or even with the creation of new clips. In the current proposal, we try to expand the context of projective simulation to include interchangeable information between two defenders named Alice and Bob in an invasion game where we might have collaboration between them in a situation in which they are not completely isolated. Consider, for example, a situation like that illustrated in figure 1, where there are two attackers (A1, A2) and two defenders (D1= Alice, D2= Bob), with their precept spaces S1 and S2 for D1 and D2, respectively. Here, the condition is that the defenders are isolated in their action space, but are not isolated in their percept spaces. We divide the signs coming from each attacker to two portions, blue and red portions. If there is not any intersection between blue and red percepts, we can write A1 signs = blue signs of A1 + red signs of A1, A2 signs = blue signs of A2 + red signs of A2. The main applied constraint is that one portion of the signs of A1 is invisible for Alice, say red signs of A1; instead, one portion of the signs of A2 is visible for her (red signs of A2). The situation could be the same for Bob too. Thus, For Alice (Bob), the blue percepts directly come from A1 (A2), where the defender can learn them and accomplish right actions for them. However, the blue percepts are less than what Alice needs to cover all attacks of A1. It means that A1 (A2) sends some signs, but not all, in blue for the right defender Alice (Bob) and sends the rest of the signs to Bob(Alice) in red. Hence, the red percepts of Alice (Bob) come from D2 (D1), where the defender does not need them for her (his) actions. However, Alice can learn the red signs for Bob and sends green signs for him and vice versa (see figure 1). Therefore, the percepts-action space for both of them is as S1 \u2261 S2: {\u21d2,\u21d0,\u21d2,\u21d0,\u21d2,\u21d0}, A1 \u2261 A2: {+,\u2212,\u21d2,\u21d0}. Here we choose not to distinguish between green actions and green percepts. In a generic situation, there is an open question related to such choices; what is the maximum reduction we could have for (S\u00d7 A) space related to a given situation? (Especially, reduction of actions would be more important so that Alice could do fewer actions and increase her dilation time (D_max) dramatically). In the other words, can we map a situation to another one with a smaller h matrix? Nevertheless, we have a choice to assign a separate episodic memory and engine for the implementation of green actions that would divide the (S\u00d7 A) space to two subspaces. Figure 1: A model of two attacker-defender invasion game. Suppose that Alice sees a given fraction (\u03b1) of all signs coming from A1, which are blue percepts, and she can also see some red percepts coming from A2. Alice has a choice to ignore the red signs by focusing on blue ones and learns her job to reach the maximum blocking efficiency in rmax = \u03b1/1 if her forgetting factor \u03b3 is zero. In this way, she will lose to perform actions for the fraction of 1 \u2212 \u03b1 attacks of A1. As another choice, on the other hand, she can have some collaboration with Bob. Each of them can analyzes the red signs separately and interchange green signs that contain indirect information about the reds. Then, eventually, Alice can make use of green signs coming from Bob for action against the attacker A1. However, in collaboration, the speed of learning is decreased. Since Bob himself is learning the reds, the green signs coming from Bob are not the same as the-reds perceived before. Therefore, there needs some more time for Alice to do the right actions against the green percepts. In addition, the deliberation time is increased in collaboration. Because the percept space for each partner is increased, there can be some dilation in deliberation time raised by expanding h_matrix. Moreover, more importantly, D_max is decreased directly, because Alice needs to carry out more actions (\u03b1 \u2212 1 more actions at least, if she has a separate engine for the green signs) at a given time, in comparison with the time when she concentrates on the blues. Even for the problem without considering deliberation time, there can be serious questions like a comparison between maximum blocking efficiencies related to different forgetting factors for Alice and Bob, etc. We are about to answer such questions in the following example. 2A specific example Suppose a problem in which Alice receives half of all signs come from A1 (\u03b1 = 0.5) in blue and she can also see half of all the signs coming from A2 in red. The situation is just the same for Bob. We derive the blocking efficiency by assigning a given h matrix for both scenarios of concentration (that is ignoring the red percepts) and collaboration. Concentration h matrix. If Alice ignores the reds and does action just for the blue percepts, then one can write h = ( h(\u21d2,\u2212) h(\u21d2,+) h(\u21d0,\u2212) h(\u21d0,+) 0 0 0 0 ) , P(s) = 1 4 \u27f6 rn = 1 4 [P(\u2212| \u21d0) + P(+| \u21d2)], where the rows are percepts and the columns are actions. We neglect writing the superscript of n for the conditional probabilities P(a|s) for ease. Also, in this example, Alice and Bob are in the same situation and we neglect writing the subscript for distinguishing them. In this scenario, the maximum blocking efficiency for Alice (or Bob) becomes rmax = 0.5/1. Nevertheless, the concentration h matrix might be different (for example, see section 4, proposition number 4). Collaboration h matrix. One can easily expand the concentration matrix to the collaboration matrix by considering the whole percept-action space (S \u00d7 A), including the reds and the greens as", "creator": "Microsoft\u00ae Word 2013"}}}