{"id": "1610.09778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "DPPred: An Effective Prediction Framework with Concise Discriminative Patterns", "abstract": "In the literature, two sets of models are proposed to solve prediction problems, including classification and regression. Simple models, such as generalized linear models, have ordinary performance, but are easy to interpret based on a number of simple characteristics; the others, including tree-based models, organize numerical, categorical, and high-dimensional characteristics into a comprehensive structure with rich interpretable information in the data.", "histories": [["v1", "Mon, 31 Oct 2016 03:43:04 GMT  (7559kb,D)", "http://arxiv.org/abs/1610.09778v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jingbo shang", "meng jiang", "wenzhu tong", "jinfeng xiao", "jian peng", "jiawei han"], "accepted": false, "id": "1610.09778"}, "pdf": {"name": "1610.09778.pdf", "metadata": {"source": "CRF", "title": "DPPred: An Effective Prediction Framework with Concise Discriminative Patterns", "authors": ["Jingbo Shang", "Meng Jiang", "Wenzhu Tong", "Jinfeng Xiao", "Jian Peng", "Jiawei Han"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Discriminative Pattern, Generalized Linear Model, Tree-based Models, Classification, Regression\nF"}, {"heading": "1 INTRODUCTION", "text": "Accuracy and interpretability are two desired goals in predictive modeling, including both classification and regression. Previous work can be characterized into two lines. One line has ordinary performance with strong interpretability on a set of simple features, but meets a serious bottleneck when modeling complex high-order interactions between features, such as linear regression, logistic regression [15], and support vector machine [29]. The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features. However, their lower interpretability and high complexity prevent practitioners from deploying in practice [15]. In the real-world scientific and medical applications which require both intuitive understanding of the features and high accuracies, the practitioners are not satisfied with neither line of models, and thus, it is important and challenging to develop an effective prediction framework with high interpretability\n\u2217 Data used in the preparation of this article were obtained from the Pooled Resource Open-Access ALS Clinical Trials (PRO-ACT) Database. As such, the following organizations and individuals within the PRO-ACT Consortium contributed to the design and implementation of the PRO-ACT Database and/or provided data, but did not participate in the analysis of the data or the writing of this report: (1) Neurological Clinical Research Institute, MGH; (2) Northeast ALS Consortium; (2) Novartis; (3) Prize4Life; (4) Regeneron Pharmaceuticals, Inc.; (5) Sanofi; (6) Teva Pharmaceutical Industries, Ltd.\n\u2022 J. Shang, M. Jiang, W. Tong, J. Xiao, J. Peng, J. Han are with the Department of Computer Science in University of Illinois at UrbanaChampaign, IL, USA. E-mail: {shang7, mjiang89, wtong8, jxiao13, jianpeng, hanj}@illinois.edu\nwhen dealing with high-order interactions with features. Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7]. Recently, a novel series of models, the discriminative pattern-based models [3], [4], have demonstrated their advantages over the traditional models. They prune non-discriminative patterns from the whole set of frequent patterns, however, the number of discriminative patterns used in their classification or regression models is still huge (at the magnitude of thousands). How to select concise discriminative patterns for better interpretability is still an open issue.\nTo address the above challenges, in this paper, we propose a novel discriminative patterns-based learning framework (DPPRED) that extracts a concise set of discriminative patterns from high-order interactions among features for accurate classification and regression. In DPPRED, first we train tree-based models to generate a large set of high-order patterns. Second, we explore all prefix paths from root nodes to leaf nodes in the tree-based models as our discriminative patterns. Third, we compress the number of discriminative patterns by selecting the most effective pattern combinations that fit into a generalized linear model with high classification accuracy or small regression error. This component of fast and effective pattern extraction enables the strong predictability and interpretability of DPPRED.\nIntuitively speaking, DPPRED selects the robust discriminative patterns in multi-tree based models by fitting them into a generalized linear model. Our extensive exper-\nar X\niv :1\n61 0.\n09 77\n8v 1\n[ cs\n.L G\n] 3\n1 O\nct 2\n01 6\niments demonstrate that DPPRED achieves comparable or even better performance when competing with the traditional tree-based models. Besides the effectiveness, we want to highlight that our DPPRED framework is applicable in the real-world tasks where the model storage and computational cost are highly restricted.\nDiscovering robust patterns in the Prize4Life Challenge. We apply DPPRED to analyze the prognosis and perform stratification for Amyotrophic Lateral Sclerosis (ALS) patients on the public dataset from the DREAM-Phil Bowen ALS Prediction Prize4Life Challenge 2012. Our DPPRED makes the following achievements.\n\u2022 DPPRED achieves a smaller error, a RMSE of 0.5306, than the method ranked at #7 with a RMSE of 0.5664. The RMSE of DPPRED is less than 4% higher than the winner with a RMSE of 0.5113. \u2022 The robust discriminative patterns found by our DPPRED are well interpretable, while the other methods including the winner cannot interpret their performances. Note that our DPPRED selects only 40 concise discriminative patterns involving 28 clinical variables from an exponentially large set, while other models used as many as 2 to 3 times variables. \u2022 As show in Figure 1, DPPRED discovers two new important clinical factors, the Blood Urea Nitrogen (BUN) and the respiratory rate. These two factors were not found by the top teams in the Challenge but there is indirect experimental and logical evidence\nfor their being actually worth further study [35], [14], [19]. Also, from the figure we can observe that each patient cluster generates different diagnosis patterns.\nOur DPPRED accurately predicts the ALS prognosis and systematically identifies clinically-relevant features for the ALS patient stratification in an interpretable manner. The distinct diagnosis patterns can significantly benefit the treatment of the ALS and precision medicine.\nIt is worthwhile to highlight the advantages of our proposed machine learning framework DPPRED. \u2022 Interpretability. DPPRED learns a small number of\nrobust discriminative patterns involving high-order interactions among original features. \u2022 Efficiency. DPPRED compresses multi tree-based models into a low-dimensional generalized linear model, making the online prediction extremely fast. \u2022 Effectiveness. Experimental results on several realworld datasets demonstrate that DPPRED has comparable or even better performances than the state-ofthe-art models on the standard tasks of classification and regression. \u2022 Clinical pattern discovery. DPPRED has been successfully applied to discover patient clusters and crucial clinical signals for the Amyotrophic Lateral Sclerosis (ALS) disease.\nThe remaining of this paper is organized as follows. In Section 2 we survey the related work. In Section 3 we provide the problem definition and our preliminary study. Section 4 presents our proposed DPPRED framework and the details of its algorithms. Section 5 reports empirical results on synthetic and real-world datasets. Section 6 shows our discovery in the prognosis analytic for ALS patients. Section 7 concludes the study."}, {"heading": "2 RELATED WORK", "text": "In this section we review existing methods that are related to DPPRED, including pattern-based classification models, tree-based models and pattern selection approaches."}, {"heading": "2.1 Pattern-based Classification", "text": "The philosophy of frequent pattern mining has been widely adopted to study the problem of pattern-based classification. Li et al. proposed a classification method CMAR based on multiple class-association rules [22]. Yin et al. extended it to CPAR based on predictive association rules [34]. Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10]. However, these approaches have several serious issues. First, the huge number of frequent patterns leads to expensive computational cost of pattern generation and selection. Second, the number of the selected patterns can be still as large as thousands, which limits the interpretability and causes the inefficiency of the classification model. Third, these models are not capable to address the regression tasks. Moreover, the discretization of continuous variables depends too much on parameter tuning to generate robust performances.\nRecently, Dong et al. proposed to utilize patterns in a different angle, where data are partitioned based on patterns, and complex models are trained independently in different partitions [8]. Although this type of pattern aided models sheds lights on a different usage of patterns, the model still lacks of interpretablity."}, {"heading": "2.2 Tree-based Models", "text": "Tree-based models are popular in the classification tasks. Both decision tree and boosted tree models are explainable but quite sensitive to the training data. Traditional ensemble methods using multiple trees, such as random forest [2] and gradient boosting decision trees [12], alleviate the over-fitting issue. Ren et al. showed that the global refinement could provide better performance because the growth and pruning processes in different trees are independent [28]. However, the increased model size of those multi-tree based models sacrifices the interpretability. Our proposed DPPRED is different from this category of models.\nThere are post-pruning techniques for multi-tree based models to induce new feature spaces. Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24]. Vens et al. transferred the binary vectors into an inner product kernel space using a support vector machine and showed the increase of classification accuracy [32]. Furthermore, pairwise interactions have also been studied to fit a two-layer-tree model for accurate classification and regression [25]. Though the number of features is reduced by pruning, the dimension of the newly-created feature space is still high due to a large number of constructed trees. For example, in [28], after many efforts on pruning, the model size of the pruned random forest was still at megabytes and thus the prediction was too slow to support real-time applications. Our experimental results will later show that DPPRED delivers comparable results using as few as the top 20 discriminative patterns, which is substantially reduced even compared to the state-ofthe-art models."}, {"heading": "2.3 Pattern Selection", "text": "Simply selecting patterns with the highest independent heuristics such as information gain and gini index is limited to very simple tasks due to the redundancy and over-fitting problems [20]. Given the labels, i.e., the types for classification or the real numbers for regression, LASSO [30] is widely used in feature selection tasks as well as forward selection [6]. Due to the relatively large number of candidate discriminative patterns, backward selection is not suitable in our problem setting. Our proposed DPPRED framework adopts the LASSO and forward selection methods to select discriminative patterns. Their performances have been compared and discussed in the experimental section."}, {"heading": "3 PRELIMINARIES", "text": "This section defines the problem as well as the important concepts used throughout this paper."}, {"heading": "3.1 Problem Formulation", "text": "For a prediction task (classification or regression), the data is a set of n examples in a d-dimensional feature space together with their labels (x1, y1), (x2, y2), . . . , (xn, yn), for \u2200i (1\u2264i\u2264n), xi \u2208 Rd. It is worth noting that the values in the example xi can be either continuous (numerical) or discrete (categorical). As categorical features can be transformed into several binary dummy indicators, we can assume xi \u2208 Rd without the loss of generality. The label yi is either a class (type) indicator or a real number depending on the specific task. In previous patternbased models, e.g., DDPMINE [4], patterns are extracted from categorical values and thus they are only able to handle the continuous variables after careful manual discretization, which is tricky and often requires prior knowledge about the data.\nThe goal of our proposed framework DPPRED is to learn a concise model that consists of a small set of discriminative patterns from the training data, which learns and predicts the examples as accurately as possible, i.e., predict the correct class indicator in classification tasks and predict close to the true number in regression tasks. Formally, given a dataset D, DPPRED returns a set of k discriminative patterns P using a generalized linear model f(\u00b7) that minimizes \u2211n i=1 l(f(M(xi)), yi), where l(\u00b7, \u00b7) is the general loss function, M(\u00b7) is a mapping function that maps the original feature vector x to the pattern space using patterns P .\nDPPRED generates a pool of discriminative patterns within a reasonable size, and selects top-k patterns based on their learning performance on training data, using a generalized linear learning model. Since the number of selected patterns is very limited, these patterns are able to provide informative interpretability with reasonable predictive power. In addition, for the coming testing data, by evaluating only a very small set of the selected discriminative patterns, DPPRED is enabled to make predictions with a generalized linear model efficiently."}, {"heading": "3.2 Definition", "text": "First, we define a series of concepts to derive the discriminative patterns. Traditional frequent pattern mining works on categorical data and itemset data, in which discretization is required to deal with continuous variables. Instead of roughly discretizing the numerical values, we adopt the thresholding boolean function in DPPRED.\nDefinition 1: Condition is a thresholding boolean function on a specific feature dimension. The condition is in the form of (x\u00b7,j < v) or (x\u00b7,j \u2265 v), where j indicates the specific dimension and v is the threshold value. The relational operator in a condition is either < or \u2265. For any dimension j in features corresponding to binary indicators, we restrict v to be 0.5.\nNote that the threshold values in DPPRED are not specified by users beforehand. In previous pattern-based models, e.g., DDPMINE [4], the practitioners have to discretize values of continuous variables prior to pattern mining. DPPRED automatically determines these values\nin the tree model, completely based on the training data without any human interventions.\nExample 1: Suppose xi \u2208 R10, one possible condition is that x\u00b7,1 < 0.5. Another example could be x\u00b7,2 \u2265 0.8.\nWe define a pattern as a set of conditions. Formally, we use conjunctions to concatenate different conditions: it is consistent with the prefix path in the decision tree that represents the conjunction of the conditions in the nodes along the path.\nDefinition 2: Pattern is a conjunction clause of conditions on specific feature dimensions. Formally, it is defined as follows.\n(x\u00b7,j1 < v1) \u2227 (x\u00b7,j2 \u2265 v2) \u2227 . . . \u2227 (x\u00b7,jm \u2265 vm),\nwhere m is the number of conditions within this pattern. Different patterns are allowed to have different m values.\nExample 2: Suppose xi \u2208 R10, one possible pattern is that (x\u00b7,1 < 18) \u2227 (x\u00b7,3 \u2265 100) \u2227 (x\u00b7,9 < 0.5).\nNow we define discriminative patterns as follows. Definition 3: Discriminative Patterns refer to those patterns which have strong signals on the learning tasks, given the labels of data. For example, a pattern with very high information gain on the classification training data, or a pattern with very small mean square error on the regression training data, is a discriminative pattern.\nExample 3: Suppose xi \u2208 R10 and the labels are generated as follows.\nyi = [(xi,1 \u2265 1) \u2227 (xi,2 < 0)] \u2228 [(xi,1 < 18) \u2227 (xi,3 \u2265 100)].\nBoth patterns (xi,1 \u2265 1) \u2227 (xi,2 < 0) and (xi,1 < 18) \u2227 (xi,3 \u2265 100) are two of the most discriminative patterns. Similar patterns that contain or have overlaps with these two patterns are also discriminative patterns.\nDiscriminative patterns have overlapped predictive effects. Specifically, a few discriminative patterns are special cases of other patterns. For example, in the previous example, both patterns (xi,1 \u2265 1)\u2227(xi,2 < 0) and (xi,1 \u2265 1)\u2227 (xi,2 < 0)\u2227 (xi,3 < 0) indicate a positive label. However, the second pattern only encodes a subset of data points that the first pattern encodes, and thus, it does\nnot provide extra information for the learning process. This common phenomenon shows that roughly taking the top discriminative patterns based on independent heuristics wastes the budget of the number of patterns, when the linear combination of these patterns are not synergistic. Therefore, our DPPRED selects the top-k patterns by their predictive performance to make the selected patterns complementary and compact.\nDefinition 4: Top-k Patterns are formalized as a sizek subset of discriminative patterns, which has the best performance (i.e., the highest accuracy in classification tasks or the least rooted mean square error in regression tasks) based on the training data.\nHere we assume that the training and testing data share the same distribution, which is widely acknowledged in the classification and regression problems. In this case, the accuracy on the testing data is approaching the accuracy on the training data and our model is able to alleviate the over-fitting issue.\nExample 4: In the last example, the top-2 patterns are {(xi,1 \u2265 1) \u2227 (xi,2 < 0), (xi,1 < 18) \u2227 (xi,3 \u2265 100)}."}, {"heading": "4 OUR DPPRED FRAMEWORK", "text": "This section first presents the overview of DPPRED and then introduces the details of every component in this framework as well as the theoretical time complexity."}, {"heading": "4.1 The Overview of DPPRED", "text": "Figure 2 presents the overview of our DPPRED framework. First it learns a constrained multi-tree based model with the training data. By adopting every prefix path from the root of a tree to any of its non-leaf nodes as a discriminative pattern, a large pool of discriminative patterns is ready for further top-k discriminative pattern selection. Two different solutions, forward selection and LASSO, are utilized to select top-k discriminative patterns based on their performances using a generalized linear model. Both solutions have shown high accuracies in the experiments. The corresponding linear model with the selected top-k discriminative patterns is adopted to make\npredictions on new examples. Our DPPRED is extremely fast and memory-efficient."}, {"heading": "4.2 Discriminative Pattern Generation", "text": "The first component in the DPPRED framework is the generation of high-quality discriminative patterns, as shown in Algorithm 1. We use tree bag to refer the set of instances falling into a specific node in the decision tree. The random decision tree [2] introduces the randomness via bootstrapping training data, randomly selecting features and splitting values when dividing a large tree bag into two smaller ones. T random decision trees are generated, and for each tree, all prefix paths from its root to non-leaf nodes are treated as discriminative patterns. Due to the predictivity of decision trees, so-generated patterns are highly effective in the specific prediction task. Note that the decision tree is built with different loss functions in different tasks, which could be entropy gain in classification tasks or the mean square error in regression tasks.\nAlgorithm 1: Discriminative Pattern Generation Require: n training instances (xi, yi), the number of trees T , the depth threshold D, and minimum tree bag size \u03c3 Return: a set of discriminative patterns for further selection. P \u2190 \u2205 for t = 1 to T do\nBuild a random decision tree [2] with maximum depth D and minimum tree bag size \u03c3. for each non-leaf node u do P \u2190 P \u222a {root\u2192 u}\nreturn P\nIn real-world datasets, the discriminative patterns are frequently emerging, and the length of such patterns are not too long. Specifically, we assume that the number of instances satisfying a given discriminative pattern should be at least \u03c3, and the length of discriminative patterns is no more than D. The returned patterns are discriminative to ensure prediction accuracy and diverse to ensure sufficient condition coverage. As one of the most famous multi-tree based models, random forest [2] is the best fit addressing all the requirements if we treat every prefix path from the root of a tree to its nonleaf node as a discriminative pattern. First, distributions of labels of instances in a tree bag always have low entropy. Therefore, the patterns are discriminative on the training data. Second, it provides many putative patterns from various random decision trees trained on different bootstrapped datasets. Third, the depth threshold D and the minimum tree bag size \u03c3 can be naturally added as constraints during the growth of trees."}, {"heading": "4.3 Pattern Space Construction", "text": "After the pattern generation, DPPRED maps the instances in the original feature space to a new pattern space\nusing the set of discriminative patterns discovered by tree models, as shown in Algorithm 2. For each discriminative pattern, there is one corresponding binary dimension describing whether the instances satisfy the pattern or not. Because the dimension of the pattern space is equal to the number of discriminative patterns which is a very large number after the generation phase, we need to further select a limited number of patterns and thus make the pattern space small and efficient. It is also worth a mention that this mapping process is able to be fully parallelized for speedup.\nAlgorithm 2: Pattern Space Construction Require: n instances (xi), a discriminative patterns set P Return: n instances in pattern space (x\u2032i) for i = 1 to n do\nx\u2032i \u2190 0 for j-th pattern Pj in P do\nif xi satisfies pattern Pj then x\u2032i,j \u2190 1\nreturn (x\u2032i)"}, {"heading": "4.4 Top-k Pattern Selection", "text": "After a large pool of discriminative patterns is generated, further top-k selection needs to be done to identify the most informative and interpretable patterns. A naive way is to use heuristic functions, such as information gain and gini index, to evaluate the significance of different patterns on the prediction task and choose the top ranked patterns. However, the effects of top ranked patterns based on the simple heuristic scores may have a large portion of overlaps and thus their combination does not work optimally. Therefore, to achieve the best performance and find complementary patterns, we propose two effective solutions: forward selection and LASSO, which make decisions based on the effects of the pattern combinations instead of considering different patterns independently."}, {"heading": "4.4.1 Forward Pattern Selection", "text": "Instead of exhausted search of all possible combinations of k discriminative patterns, forward selection gradually adds the discriminative patterns one by one while each newly added discriminative pattern is the best choice at that time [6], which provides an efficient approximation of the exhausted search. To be more specific, when the first k\u2032 discriminative patterns are fixed, the algorithm empirically adds one more discriminative pattern so that the new set of k\u2032 + 1 patterns achieves the best training performance in the generalized linear model, as shown in Algorithm 3. As mentioned before, when assuming training and testing data have the same distribution, using training accuracy is very reasonable."}, {"heading": "4.4.2 LASSO based Pattern Selection", "text": "L1 regularization (i.e., LASSO [30]) is designed to make the weight vector sparse by tuning a nonnegative parameter \u03bb, where the features with non-zero weight will be the\nAlgorithm 3: Top-k Pattern Selection: Forward Require: n training examples (xi, yi), a set of discriminative patterns P and k Return: top-k discriminative patterns set Pk and a generalized linear model f(\u00b7) Pk \u2190 \u2205 for t = 1 to k do\nfor each pattern p in P do x\u2032 \u2190 construct pattern space(x,Pk \u222a {p}) using Algorithm 2 g(\u00b7)\u2190 a generalized linear model [29] on (x\u2032i, yi) perp \u2190 g(\u00b7)\u2019s training performance\nPk \u2190 Pk \u222a {arg maxp perp} x\u2032 \u2190 construct pattern space(x,Pk) f(\u00b7)\u2190 a generalized linear model on (x\u2032i, yi) return Pk, f(\u00b7)\nselected ones. Since we are actually selecting features in the pattern space, for a given \u03bb, we optimize the following loss function to get a subset of important patterns.\nL = n\u2211 i l(x\u2032 T i w, yi) + \u03bb \u00b7 \u2016w\u20161, (1)\nwhere x\u2032i is the mapped binary feature representation in pattern space of i-th example; w is the weight vector in the generalized linear model; l(\u00b7, \u00b7) is a general loss function such as logistic loss. To ensure there are at most k patterns having non-zero weights in the pattern space, we should carefully choose a value for \u03bb. We assume that there exists hidden importance among the features. When the weight of a feature is non-zero in a given \u03bb = v, it is also non-zero for any smaller \u03bb < v. A binary search algorithm is shown in Algorithm 4. The LASSO implementation in GLMNET [11] is adopted in this thesis, whose loss function is the cross entropy.\nAlgorithm 4: Top-k Pattern Selection: LASSO Require: n training examples (xi, yi), a set of discriminative patterns P , k, and a small value Return: top-k discriminative patterns Pi and a generalized linear model f(\u00b7) Pk \u2190 \u2205 l\u2190 0, r \u2190 +\u221e x\u2032 \u2190 construct pattern space(x,P) using Algorithm 2 while l + < r do\n\u03bb\u2190 (l + r)/2 w\u2190 arg minw Equation 1 if non-zero weighted patterns \u2264 k then Pk \u2190 {p|p\u2019s weight is non-zero} r \u2190 \u03bb else l\u2190 \u03bb\nx\u2032 \u2190 construct pattern space(x,Pk) f(\u00b7)\u2190 a generalized linear model on (x\u2032i, yi) return Pk, f(\u00b7)\nAlgorithm 5: Prediction Require: n testing examples (xi), top-k discriminative patterns set Pk, and the generalized linear model f(\u00b7) Return: predictions of testing instances y\u0302i x\u2032 \u2190 construct pattern space(x,Pk) using Algorithm 2 for i = 1 to n do\ny\u0302i \u2190 f(x\u2032i) return y\u0302"}, {"heading": "4.5 Prediction", "text": "Once the top-k discriminative patterns are determined, for any upcoming new test instance, DPPRED first maps it into the learned pattern space, and then applies the pre-trained generalized linear model to compute the prediction, as shown in Algorithm 5. As the number of patterns is limited, both the mapping into the pattern space and the prediction of the generalized linear model will be extremely fast."}, {"heading": "4.6 Time Complexity Analysis", "text": "To build up a single random decision tree with depth threshold D and minimum tree bag size \u03c3, by assuming both numbers of random features and random partitions are small and fixed constants, the time complexity is O(nD), because the total number of instances on each level of the tree is n. Therefore, the time complexity of generating T trees is O(TnD) in the generation step.\nFor the selection step, the complexity is mainly determined by the number of discriminative patterns induced by T random decision trees, which is dependent on the total number of non-leaf nodes. As the maximum depth of a single tree is D, there is an upper bound on number of leaf nodes 2D. Starting from the tree bag size, the number of leaf nodes should be no more than dn\u03c3 e. Since the trees here are all binary trees, the number of leaf nodes is one more than the number of non-leaf nodes. Therefore, the number of discriminative patterns |P| (i.e., the number of non-leaf nodes) is bounded by T \u00b7min{2D, dn\u03c3 e}\u22121. If we solve logistic regression and LASSO using (sub-)gradient descent algorithm, and thus the time complexity per gradient step is only linear to the dimension of features and the number of examples. The time complexity is proportional to O(|P| \u00b7 n \u00b7 k2) if forward selection is used, while it is proportional to O(n \u00b7 k \u00b7 |P|) if LASSO is used. By assuming the numbers of iterations to converge are similar in LASSO and forward selection, LASSO will be a little more efficient than forward selection.\nWhen predicting new test instances, one can easily figure out the bottleneck is mapping instances into the learned pattern space. Therefore, in the batch mode where examples are considered together, the time complexity is O(n \u00b7 k \u00b7D). In the streaming (or online) mode where instances come one by one, the time complexity is O(k\u00b7D), where k is the number of discriminative patterns and D\nis the maximum tree depth, which is equivalent to the maximum number of conditions in a single pattern.\nIt is worth mentioning that all modules can be fully parallelized, leading to further speedup in practice."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we conduct extensive experiments to demonstrate the interpretability, efficiency and effectiveness of our proposed DPPRED framework. We first introduce our experimental settings, discuss the efficiency and interpretability, and then give the results on classification and regression tasks as well as parameter analysis."}, {"heading": "5.1 Experimental Settings", "text": "This subsection presents the datasets, baseline methods, and learning tasks in our experiments."}, {"heading": "5.1.1 Datasets", "text": "First, we generate synthetic datasets where the features are demographics and lab test results of patients and the label is whether the patient has a disease, in order to demonstrate the interpretability of DPPRED. Assuming doctors can diagnose the disease using some rules based on these information, it can be verified whether the top discriminative patterns selected by DPPRED are consistent with the actual diagnosing rules.\nSeveral real world classification and regression datasets from UCI Machine Learning Repository are used in the experiments, as shown in Table 1 with statistics of the number of instances and the number of features. In the datasets adult, hypo and sick, the ratio of standard train/test splitting is 2 : 1. Therefore, for the other classification and regression datasets, we divide the datasets into train/test (2 : 1) by unbiased sampling as preprocessing.\nFor classification tasks, to compare with DDPMINE, we use the same datasets including adult, hypo, sick, crx, sonar, chess, waveform, and mushroom. Because both DDPMINE and DPPRED achieve almost perfect accuracy (very close to 100%) on the datasets waveform and mushroom, these two datasets are omitted. In addition, the performance of DPPRED on high-dimensional datasets (nomao, musk and madelon datasets) is also investigated, since DDPMINE performs poorly on high-dimensional data. The metric is\nthe accuracy on the testing data: higher accuracy means better performance.\nFor regression datasets, we choose general datasets such as bike and crime, as well as clinical datasets where patterns are more likely to be present, such as parkinsons. Furthermore, to make the errors in different datasets comparable, min-max normalization is adopted to scale the continuous labels into [0, 1]. The metric is the rooted mean square error (RMSE) on the testing data: a lower RMSE means better performance."}, {"heading": "5.1.2 Baseline Methods", "text": "DDPMINE [4] is a previous state-of-the-art discriminative pattern based algorithm. It first discretizes the continuous variables such that frequent pattern mining algorithm could be applied. Using frequent and discriminative patterns, new feature space is constructed and any classical classifiers could be further utilized. DDPMINE only focuses on classification tasks and it is not applicable in regression experiments.\nRandom Forest (RF) [2] is another baseline method using same parameters as those in the random forest used in DPPRED, except for D. There is no limit on the depth in RF. Moreover, we are interested in the limited-depth random forest model (LRF) built in the topk generation step of global patterns. These two tree-based methods are capable in both classification and regression tasks. It is expected if these two complex models (i.e., hard to interpret) have slightly better performance than DPPRED, because the major contributions of DPPRED are the concise interpretable patterns instead of solely the accuracy. To make a fair comparison, Decision Tree (DT) with a similar number of nodes with DPPRED is also listed as a baseline."}, {"heading": "5.1.3 Classification and Regression Tasks", "text": "In DPPRED, for the classification tasks, the default parameter setting is T = 100, D = 6, \u03c3 = 10, k = 20. For the regression tasks, because the continuous labels are more complex than those discrete class labels in classification, it is natural to incorporate more patterns. Therefore, the default setting is T = 100, D = 6, \u03c3 = 10, k = 30.\nWe will show results using both forward selection (DPPRED-F) and LASSO (DPPRED-L) to select the top-k discriminative patterns. We deeply study the impact of the parameters such as the number of selected discriminative patterns k and the number of trees in the random forest T . Therefore, we fix the other parameters as their\ndefault values and vary the parameter value to study their impacts, respectively."}, {"heading": "5.2 Efficiency and Interpretability", "text": "Efficiency. The test running time is linearly proportional to the model complexity, which is related to the number of patterns the model used. In the experiments, DDPMINE needs 100 to 1,000 patterns while DPPRED only needs 20, which indicates a significant reduction of prediction runtime. Moreover, the random forest without any constraints will contain more than 10,000 nodes (i.e., patterns), which is far more expensive. Although the evaluation of random forest for a single testing instance will traverse only a number of nodes equals to the sum of depths in different trees, it always needs more than 1,000 traverses in the experiments. Therefore, DPPRED is the most efficient model for testing new instances, compared to DDPMINE and random forest, by achieving about 20 to 50 times speedup in practice. Furthermore, DPPRED could be fully parallelized for further speedup. The empirical results are presented in Table 2.\nInterpretability: our discovery of interpretable patterns. We generate a small medical dataset for binary classification to demonstrate the interpretability. For each patient, we draw several uniformly sampled features as follows:\n1) Age (A): positive integers no more than 60. 2) Gender (G): male or female. 3) Lab Test 1 (LT1): blood types (categorical values)\nfrom {A, B, O, AB}. 4) Lab Test 2 (LT2): continuous values in [0, 1].\nTotally, there are 105 random patients for training and 5 \u00b7 104 patients for testing.\nThe positive label of the disease is assigned to a patient if at least one of the following rules holds:\n1) (A > 18) and (G = Male) and (LT1 = AB) and (LT2 \u2265 0.6), 2) (A > 18) and (G = Female) and (LT1 = O) and (LT2 \u2265 0.5), 3) (A \u2264 18) and (LT2 \u2265 0.9). To make the classification tasks more challenging, 0.1% noise is added to the training data. That is, 0.1% labels in training will be flipped.\nWe apply both DPPRED-F and DPPRED-L on this dataset. Both give the test accuracy 99.99%. The top3 discriminative patterns found in both DPPRED-F and DPPRED-L are listed as below. We observe that the found patterns are quite close to the groundtruth rules. We demonstrate that the selected discriminative patterns provide high-quality explanation:\n1) (A > 18) and (G = Female) and (LT1 = O) and (LT2 \u2265 0.496), 2) (A \u2264 18) and (LT2 \u2265 0.900), 3) (A > 18) and (G = Male) and (LT1 = AB) and (LT2 \u2265 0.601).\nWe apply DDPMINE to this dataset but its accuracy is only 95.64%, because the discretization brings too much\nnoise. The top-3 patterns mined by DDPMINE are as follows, which are quite different from expectation:\n1) (LT2 > 0.8), 2) (G = Male) and (LT1 = AB) and (LT2 \u2265 0.6) and\n(LT2 < 0.8), 3) (G = Female) and (LT1 = O) and (LT2 \u2265 0.6) and\n(LT2 < 0.8)."}, {"heading": "5.3 Effectiveness in Classification", "text": "DDPMINE is a previous state-of-the-art pattern-based classification method, which outperforms traditional classification models including decision tree and support vector machine [3][4]. We compare DPPRED, DDPMINE and RF on the same datasets used in DDPMINE. The results are shown in Table 3. DPPRED-F and DPPREDL always have higher accuracy over DDPMINE. An important reason of this advantage is that the candidate patterns generated by tree-based models in DPPRED are much more discriminative and thus more effective on the specific classification task than those frequent but less useful patterns extracted in DDPMINE. Except for sick dataset, DPPRED-F has the highest accuracy, while DPPRED-L works best on sick dataset. It seems that DPPRED-F works a little better than DPPRED-L. However, their results are quite close to each other and are both better than those of DDPMINE on most datasets.\nMore surprisingly, DPPRED demonstrates even better performance than the complex model random forest on several datasets, while its accuracies on other datasets are still comparable with RF, which is due to the effectiveness of the pattern selection module where we select the optimal pattern combination instead of selecting patterns independently. This shows that the proposed model is very effective in classification tasks while it is highly concise and interpretable."}, {"heading": "5.4 Effectiveness in Regression", "text": "Since DDPMINE is not applicable on regression tasks, we only compare DPPRED with DT, RF, and LRF. Note that these two methods are highly complicated and thus preserve very limited interpretability. The RMSE results and the average differences compared to DPPRED are shown in Table 4.\nUnlike the results in classification datasets, complex models outperform DPPRED on all datasets although the difference is not very significant. This is reasonable\nbecause, different from the discrete class labels, the real valued prediction increases the level of difficulty. Although we have raised the number of top patterns a little, bag-of-patterns feature representations based on a small number of patterns still have some limitations to predict a real value. For example, there are at most 230 different examples in the constructed pattern space, which means there are at most 230 different predicted values, but infinite real numbers are likely to be the true value for a new example. However, it is worth noting that DPPRED (both DPPRED-F and DPPRED-L) always achieves comparable performance with RF, and work better than or similar to DT and LRF, which still demonstrates the effectiveness of DPPRED to some extent while the model is more compact and interpretable than RF and LRF."}, {"heading": "5.5 Effectiveness in High Dimensions", "text": "We are interested in high-dimensional datasets (i.e., at least 100 dimensions) because DDPMINE is not effective in large dimensional data. To compare with DDPMINE, we use classification datasets whose number of dimen-\nsions is at least 100 and no regression datasets are used. As the dimension of the original feature space grows, it is reasonable to increase the depth threshold D, as well as the number of trees T , to involve higher order interactions and increase the number of candidate discriminative patterns. Therefore, we set D = 10 and T = 200. Meanwhile, the dimension of mapped pattern space may also need to be increased due to the higher complexity of problems. As a result, we set k = 50 in nomao and musk datasets. However, we kept k = 20 in madelon dataset because many features are noises.\nAs shown in Table 5, DPPRED can always outperform DDPMINE and generate comparable results to those by RF. It is worth noting that in madelon dataset, DPPRED-F and DPPRED-L outperform RF significantly. As stated before, madelon is highly noisy. As a result, many patterns generated by random forest are not that reliable, which can be very poor at test data although they are discriminative in training data. On the other hand, DPPRED compresses the patterns and only keeps the most discriminative ones, and thus alleviates this problem to some extent. This demonstrates the robustness of DPPRED especially when the features are high dimensional and noisy. It is also worth a mention that the training process of DPPRED is at least 10 times faster than DDPMINE in high dimensional datasets."}, {"heading": "5.6 Parameter Analysis", "text": "In this subsection, we deeply study the parameters including the number of top patterns k and the number of trees in the random forest T ."}, {"heading": "5.6.1 The Number of Top Discriminative Patterns", "text": "The most interesting parameter in DPPRED is k, the number of discriminative patterns used in the final generalized linear model. It controls the model size of the generalized linear model used for prediction and thus affects its efficiency. Because the default value of k is 20 for classification tasks and 30 for regression tasks and its effectiveness has been proved in previous experiments, we vary k from 1 to 40 to see the trends of both training and testing accuracies on different datasets. Three representative classification datasets (adult, hypo, and sick) and three regression datasets (bike, crime and parkinsons) are used in this experiment.\nAs illustrated in Figure 3, the performance on test data is always following the trend of performance on training data and the performance is increasing as k grows in both classification and regression tasks (accuracy is increasing on classification datasets while error is decreasing on regression datasets). The discrepancy of training and test performance is more significant in regression tasks (right two in Figure 3), which is reasonable due to the higher complexity of the problem, but the trends are quite similar. In addition, we argue that the larger difference could be caused by insufficient size of training data, because the curves always overlap on bike dataset that is much bigger than the other two. It is also worth noting that DPPRED-L performs more consistently than DPPREDF, especially in regression tasks, as a result of \u03bb which is automatically learned in DPPRED-L but is manually specified in DPPRED-F. In summary, the similar trends in training and test data justifies that our pattern selection based on training accuracy is reasonable. In real world applications, k could be determined by cross validations.\nAlthough the performance is becoming better almost all the time, it slows down much when k is greater than the default value. This is true for both classification and regression tasks. An even larger k will hurt the efficiency of both training process and online prediction, and might introduce overfitting issues in prediction (e.g., test accuracy on hypo dataset is 99.58% when k = 20 while it becomes 99.28% when k = 40 using forward selection). Therefore, we can conclude that a very small k (e.g., k = 20) is enough for these comprehensive realworld datasets, which further proves that the proposed DPPRED can compress the model into a very tiny size while its accuracy remains comparable."}, {"heading": "5.6.2 The Number of Trees in the Model", "text": "Another important parameter in DPPRED is the number of trees needed to generate the large pool of discriminative patterns. As mentioned before, a single tree is not enough to generate that many patterns, and thus there is strong motivation to try T = 1 as an extreme case. The default value 100 works well in previous experiments, and thus we vary T in {1, 10, 50, 100, 500, 1,000} to see the trends of both training and testing accuracies. As before, three datasets for classification and regression tasks are presented in the experiments.\nFigure 4 visualizes the results on classification and regression datasets respectively. When T = 1, the perfor-\nmance is much lower than others, which means only a single decision tree is not enough for a diverse patterns pool. Too few trees generally cannot guarantee high coverage of effective patterns, especially when data set is large and dimension is high. Increasing number of trees leads to better diversity of candidate patterns. According to the curves, one can easily observe and conclude that the performance remains stable as long as the number of trees is sufficiently large, and a reasonably large T is enough to achieve a satisfying result. Similar to the number of patterns k, however, many noisy patterns will be generated if T becomes too large, which fit training data better while fail to characterize testing data and are harmful to generalization of the model (e.g., test RMSE is 0.0977 on hypo dataset when T = 100 while it becomes 0.1104 when T = 1000 using LASSO). In addition, the more trees we have, the larger number of pattern candidates will be generated, which increase the time complexity of feature selection. T is by default set to 100 in our experiments, which performs consistently well on different data sets."}, {"heading": "6 NOVEL MARKER DISCOVERY FOR ALS PATIENT STRATIFICATION", "text": "In this section, we apply DPPRED to analyze the prognosis and perform stratification for ALS patients. Unlike other diseases such as many cancers, which can be clearly classified into subtypes with distinct survival rates, no significant signals have been identified to explain the diverse survival times (ranging from less than a year to over 10 years) for ALS patients. Such a wide range makes it difficult to predict disease progression and survival, and suggests rather large underlying disease heterogeneity. There may exist different subgroups of patients, each having its unique disease causes and prognosis."}, {"heading": "6.1 ALS Dataset", "text": "To solve this puzzle, the Pooled Resource Open-Access ALS Clinical Trials (PRO-ACT) platform1 was created by Prize4Life and the Neurological Clinical Research Institute at Massachusetts General Hospital to collect ALS data from existing completed ALS clinical trials. In\n1. The data in the PRO-ACT Database are contributed by members of PRO-ACT Consortium, founded in 2011 by Prize4Life and the Northeast ALS Consortium with the funding from the ALS Therapy Alliance.\n2012, a subset of PRO-ACT data was constructed with the aim to crowdsource the challenge of ALS prognosis as a data mining task, which is known as the DREAM-Phil Bowen ALS Prediction Prize4Life Challenge (\u201cthe 2012 challenge\u201d for short in this section) [19].\nThe 2012 challenge aimed at improving the prediction of ALS progression rate, which is essentially a regression task. The participants built models with a training set of 918 patients, and submitted their models to the challenge organizers. The organizers ran the models on a separate leaderboard set of 279 patients and provided feedback on model performance to the participants. Several such submission-and-feedback cycles were run in 3 months, and then the last submissions from the participants were evaluated and ranked by the organizers on another separate validation set of 627 patients.\nThis challenge attracted more than 1,000 participants and received 37 unique algorithms during the submissionand-feedback leaderboard phase. Among them, only six algorithms demonstrated improved accuracy over the baseline (developed by the challenge organizers) on the final validation data set.\nThe best prognosis model (\u201cthe Top Solution\u201d for short in this section) developed in the 2012 challenge, which uses Bayesian trees with 484 predictive features constructed from 26 clinical variables, is a profound success. It has predicted ALS progression from clinical data better than clinicians do, and can potentially reduce the cost of future ALS trials by $6-million [19]. The Top Solution is not perfect though. It is a uniform model for all patients and thus lacks the ability to make personalized diagnosis. Also, it is hard to clinically interpret the Top Solution due to the high model complexity.\nFor fair comparison, DPPRED has been trained and evaluated in such a way that mimics the 2012 challenge. Training was performed with the same training set of 918 patients and evaluation was on the same validation set of 627 patients. The leaderboard set of 279 patients was used merely for feature calibration (described later in this paper).\nThe data used in the 2012 challenge consist of 2 parts: clinical variables and the actual ALS progression rate (which serves as the golden standard for model comparison). Available clinical variables of a patient can be grouped into 5 kinds: demographic information, vital signs, lab test results, family disease history and the Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS). A detailed description of the data can be found in the supplement of [19]. Some variables are excluded from our study because their units are not consistent for some patients.\nALSFRS is a quantitative clinical score ranging from 0 to 40 for evaluating the functional status of an ALS patient. It consists of 10 assessments of motor functioning, each evaluated within the range 0 (worst status, no function) to 4 (normal function). Those 10 evaluated\nfunctions2 are: \u201c1.speech\u201d, \u201c2.salivation\u201d, \u201c3.swallowing\u201d, \u201c4.handwriting\u201d, \u201c5.cutting food and handling utensils\u201d (with or without gastrostomy), \u201c6.dressing and hygiene\u201d, \u201c7.turning in bed and adjusting bed clothes\u201d, \u201c8.walking\u201d, \u201c9.climbing stairs\u201d and \u201c10.respiratory\u201d.\nThe rate of change in ALSFRS3 with respect to time T (\u2206ALSFRS/\u2206T ) can be used as a quantitative measurement of ALS progression rate. The task is to predict \u2206ALSFRS/\u2206T within 3 to 12 months from disease onset, given the clinical variables within the first 3 months. The RMSE between the predicted \u2206ALSFRS/\u2206T and the actual value is used to evaluate the predictive performance."}, {"heading": "6.2 Data Processing", "text": "The clinical variables about a patient contain 3 data types: static categorical, static continuous and longitudinal continuous variables. Static variables are timeindependent, while longitudinal variables are measured multiple times for each patient and are likely to change over time. Any static categorical variable with k categories is replaced with k+1 binary features where the additional one indicates whether the variable is missing. A static continuous variable is simply a continuous feature.\nEach longitudinal continuous variable {x,t}, where x \u2208 Rn is the n measured values and t \u2208 Rn is the times of n measurements in ascending order, is converted to 12 continuous features by taking some statistics of {x,t} and a derivative sequence \u2206 \u2208 Rn\u22121 whose ith element is defined as \u2206i = (xi+1 \u2212 xi)/(ti+1 \u2212 ti). 6 statistics are taken from x: the average value ( \u2211n i=1 xi)/n, the first-measured value x1, the last-measured value xn, the maximum maxi{xi}, the minimum mini{xi}, and the standard deviation \u03c3(xi). Another 6 statistics are taken similarly from \u2206.\nAfter performing such variable conversion separately on the training, leaderboard and validation sets, features are calibrated across all 3 sets so that features completely missing in at least 1 of the 3 data sets are discarded. The number of features we finally feed into DPPRED is 498, converted from 78 clinical variables."}, {"heading": "6.3 Task Description", "text": "In the precision medicine setting, we assume there are some implicit groupings underlying the patients, such as the subtypes of a certain disease. Formally, we define the patient cluster as follows.\nDefinition 5: Diagnosis-Stratified Patient Clusters are G disjoint patient groups, such that patients within the same group are similar and there are different top-k patterns of clinical variables across clusters that suggests\n2. Some patients are evaluated instead with a modified version ALSFRS-R ranging from 0 to 48, where \u201c10.respiratory\u201d is replaced with \u201cR1.Dyspnea\u201d, \u201cR2.Orthopnea\u201d and \u201cR3.Respiratory insufficiency\u201d, each ranging from 0 to 4.\n3. To assure the consistency of scales across patients, for those patients with ALSFRS-R only but no ALSFRS, the sum of questions 1-9 and R1 are used in the calculation of the rate of change.\nGroup 1\nGroup 2\nGroup 3\nGlobal Local 1 Local 2 Local 3\n0 0\n00 0 0\nBag of patterns\nBag of patterns\nBag of patterns\nBag of patterns\nBag of patterns\nBag of patterns\ndistinct diagnoses. We use patient cluster for short in this paper.\nConsidering different patient sets S , we can define the global and local patterns respectively.\nDefinition 6: Global Patterns are the top-Kg patterns by using all patients as training instances. The global patterns are expected to not only capture the general properties of the specific task, but also hopefully find the way to detect implicit groups of patients. For example, suppose a disease has 3 different subtypes, we expect some global patterns can handle the general diagnosis while others can help clinicians partition patients into the 3 subtypes.\nDefinition 7: Local Patterns are the top-Kl patterns by using only the patients in a single patient clusteras training instances. Within different patient clusters (e.g., different subtypes of a disease), patients may have different root causes, and thus need different diagnoses and treatments. Therefore, we are motivated to discover local patterns.\nIn this application, our task is to first discover global patterns for all patients and then figure out the patient clusters as well as the local patterns in each patient cluster. The goal is to demonstrate that our DPPRED can not only accurately predict ALS prognosis, but also systematically identify clinically-relevant features for ALS patient stratification in an interpretable manner, which will further facilitate personalized diagnosis and therapy."}, {"heading": "6.4 DPPRED for ALS patient stratification", "text": "As shown in Figure 5, the prognosis analysis and stratification for ALS patients work as follows. \u2022 Discover Kg global patterns based on all patients; \u2022 Partition patients into G different patient clusters\nbased on the discovered global patterns; \u2022 Discover Kl local patterns inside each patient cluster; \u2022 Construct the bag-of-patterns feature representation\nfor each patient based on all global patterns and\n0 0.2 0.4 0.6 0.8 1 1.2 1.4\nTeam 10: Linear regression\nTeam 9: Multivariate regr.\nTeam 8: Linear regression\nTeam 7: Prediction of mean\nBaseline: Support vector regr.\nTeam 6: Nonpar. regression\nTeam 5: Random forest\nTeam 4: Random forest\nTeam 3: Random forest\nTeam 2: Random forest\nTeam 1: Bayesian trees\nDPMed\nbased on these Kg +Kl discriminative patterns; \u2022 Predict by the generalized linear model. We utilize DPPRED to discover global and local patterns. Since it is a regression task, similar to our previous experiments, we set T = 100, D = 6, \u03c3 = 10,Kg = 30,Kl = 10. Therefore, for each patient, we have Kg + Kl = 40 patterns. For the patient clustering, by making analogy from bag-of-words to bag-of-patterns, we adopt Latent Dirichlet Allocation (LDA) algorithm [1] and set G = 3. More specifically, observing global patterns of patients, in order to detect patient clusters, we design a generative process of the patterns incorporating patient clusters as latent variables. First, we assume the patterns in a particular patient clusterfollow a multinomial distribution, which is a random variable draws from a prior Dirichlet distribution. Inspired from bag-of-words, by making analogies between words in documents and patterns of patients, we represent the observed patterns of a patient as a bag of patterns. Therefore, the generative process can be treated as the process of LDA."}, {"heading": "6.5 Results and Discussion", "text": "DPPRED has obtained a predictive performance comparable to the Top Solution while gives interpretable discriminative patterns, as shown in Figure 6. DPPRED with 3 patient clusters achieves a RMSE of 0.5306 on the validation data set, which is only < 4% away from the RMSE of the Top Solution, 0.5113, comparable to the\nother top-ranked algorithms which are also complicated and not interpretable, and better than the baseline RMSE, 0.5664. The linear combination of discriminative patterns trained with DPPRED includes 28 clinical variables in total, which is a small subset of all 78 available variables.\nOur top 20 most frequent clinical variable list (Figure 1) reveals the importance of the blood urea nitrogen (BUN) and the respiratory rate, which are not among the most important features reported by any of the top 5 teams nor the organizers of the 2012 challenge. The other variables in our top 20 list agree well with the 2012 challenge findings. Some examples include the critical role of the onset delta (i.e.the time between the ALS onset and the first time the patient was tested in a trial), mouth-related ALSFRS assessments (including \u201c1.speech\u201d, \u201c2.salivation\u201d and \u201c3.swallowing\u201d) and vital capacity. A high degree of consistency with the 2012 challenge results proves the reliability of DPPRED, while our newly reported important variables highlights the power of feature selection in DPPRED and shed new light on ALS research.\nThere are other reasons to take our newly discovered important clinical variables seriously when designing future studies. It has been experimentally shown that the BUN level is elevated (p < 0.05) when minocycline, a drug that can delay the progression of ALS, is applied [35], [14]. Therefore the correlation between the BUN level and the ALS progression rate is likely to be true. The respiratory rate reflects respiratory muscle functioning and thus related to \u201c10.respiratory\u201d, 1 of the 10 assessments in ALSFRS. Since the importance of \u201c10.respiratory\u201d is reported by several among the top 5 teams in the 2012 challenge [19] and also by DPPRED, it should not be surprising that the respiratory rate is also in the list. Interestingly DPPRED is the only algorithm among those that simultaneously selects both the respiratory rate and \u201c10.respiratory\u201d.\nAnother point worth mentioning is the distinct local patterns of each patient cluster displayed in Figure 1, indicating different diagnosis patterns across patient clusters. For example, the mouth-functioning-related scores are important overall but not locally in Cluster 3, while the blood pressure is important in Patient Clusters 2 & 3 but plays a less significant role in Cluster 1. Such distinct diagnosis patterns may not only aid personalized medicine but also shed light on the mechanism, underlying heterogeneity and treatment of ALS. To demonstrate the predictive performance of stratification, we also trained a DPPRED model without clustering, and its RMSE, 0.5404, is worse.\nAll these results indicate that our DPPRED not only accurately predicts ALS prognosis, but also systematically identifies clinically-relevant features for ALS patient stratification in an interpretable manner, which will facilitate personalized diagnosis and therapy."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we propose an effective and concise discriminative pattern-based prediction framework (DPPRED) to address the classification and regression problems and provide high interpretability with a small number of discriminative patterns. Specifically, DPPRED first trains a constrained multi-tree model using training data and then extracts the prefix paths from root nodes to non-leaf nodes in all the trees as candidate discriminative patterns. The size of discriminative patterns is compressed by selecting the most effective pattern combinations according to their predictive performance in a generalized linear model. Instead of selecting the patterns independently using heuristics, DPPRED finds the best combination using forward selection or LASSO, which avoids the overlapping effect between similar patterns. Extensive experiments demonstrate that DPPRED is able to model high-order interactions and present a small number of interpretable patterns to help human experts understand the data. DPPRED provides comparable or even better performance than the state-of-the-art model DDPMINE and random forest model in classification and regression. DPPRED has been successfully applied to discover patient clusters and crucial clinical signals for the amyotrophic lateral sclerosis (ALS) disease."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR, 3:993\u20131022,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Using random forest to learn imbalanced data", "author": ["C. Chen", "A. Liaw", "L. Breiman"], "venue": "University of California, Berkeley,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative frequent pattern analysis for effective classification", "author": ["H. Cheng", "X. Yan", "J. Han", "C.-W. Hsu"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, pages 716\u2013 725. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Direct discriminative pattern mining for effective classification", "author": ["H. Cheng", "X. Yan", "J. Han", "P.S. Yu"], "venue": "Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on, pages 169\u2013178. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining top-k covering rule groups for gene expression data", "author": ["G. Cong", "K.-L. Tan", "A.K. Tung", "X. Xu"], "venue": "Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 670\u2013681. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Backward, forward and stepwise automated subset selection algorithms: Frequency of obtaining authentic and noise variables", "author": ["S. Derksen", "H. Keselman"], "venue": "British Journal of Mathematical and Statistical Psychology, 45(2):265\u2013282,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Frequent substructure-based approaches for classifying chemical compounds", "author": ["M. Deshpande", "M. Kuramochi", "N. Wale", "G. Karypis"], "venue": "TKDE, 17(8):1036\u20131050,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern aided classification", "author": ["G. Dong", "V. Taslimitehrani"], "venue": "Proceedings of 2016 SIAM international conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Drop: an svm domain linker predictor trained with optimal features selected by random forest", "author": ["T. Ebina", "H. Toh", "Y. Kuroda"], "venue": "Bioinformatics, 27(4):487\u2013494,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct mining of discriminative and essential frequent patterns via model-based search tree", "author": ["W. Fan", "K. Zhang", "H. Cheng", "J. Gao", "X. Yan", "J. Han", "P. Yu", "O. Verscheure"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 230\u2013238. ACM,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "glmnet: Lasso and elasticnet regularized generalized linear models", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "R package version, 1,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of statistics, pages 1189\u20131232,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Bagging gradientboosted trees for high precision, low variance ranking models", "author": ["Y. Ganjisaffar", "R. Caruana", "C.V. Lopes"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 85\u201394. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Placebo-controlled phase i/ii studies of minocycline in amyotrophic lateral sclerosis", "author": ["P. Gordon", "D. Moore", "D. Gelinas", "C. Qualls", "M. Meister", "J. Werner", "M. Mendoza", "J. Mass", "G. Kushner", "R. Miller"], "venue": "Neurology, 62(10):1845\u20131847,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Applied logistic regression", "author": ["D.W. Hosmer Jr", "S. Lemeshow"], "venue": "John Wiley & Sons,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative tree-based feature mapping", "author": ["M. Kobetski", "J. Sullivan"], "venue": "Intelligence, 34(3),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An application of boosting to graph classification", "author": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "Advances in neural information processing systems, pages 729\u2013736,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression", "author": ["R. K\u00fcffner", "N. Zach", "R. Norel", "J. Hawe", "D. Schoenfeld", "L. Wang", "G. Li", "L. Fang", "L. Mackey", "O. Hardiman"], "venue": "Nature biotechnology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Information gain and divergence-based feature selection for machine learning-based text categorization", "author": ["C. Lee", "G.G. Lee"], "venue": "Information processing & management, 42(1):155\u2013165,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "The spectrum kernel: A string kernel for svm protein classification", "author": ["C.S. Leslie", "E. Eskin", "W.S. Noble"], "venue": "Pacific symposium on biocomputing, volume 7, pages 566\u2013575. World Scientific,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Cmar: Accurate and efficient classification based on multiple class-association rules", "author": ["W. Li", "J. Han", "J. Pei"], "venue": "Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on, pages 369\u2013376. IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "JMLR, 2:419\u2013 444,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Intelligible models for classification and regression", "author": ["Y. Lou", "R. Caruana", "J. Gehrke"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Accurate intelligible models with pairwise interactions", "author": ["Y. Lou", "R. Caruana", "J. Gehrke", "G. Hooker"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 623\u2013631. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating classification and association rule mining", "author": ["B.L.W.H.Y. Ma"], "venue": "Proceedings of the fourth international conference on knowledge discovery and data mining,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast discriminative visual codebooks using randomized clustering forests", "author": ["F. Moosmann", "B. Triggs", "F. Jurie"], "venue": "Twentieth Annual Conference on Neural Information Processing Systems (NIPS\u201906), pages 985\u2013992. MIT Press,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Global refinement of random forest", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 723\u2013730,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, 9(3):293\u2013300,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Lazy associative classification", "author": ["A. Veloso", "W. Meira", "M.J. Zaki"], "venue": "Data Mining, 2006. ICDM\u201906. Sixth International Conference on, pages 645\u2013654. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Random forest based feature induction", "author": ["C. Vens", "F. Costa"], "venue": "Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 744\u2013753. IEEE,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Harmony: Efficiently mining the best rules for classification", "author": ["J. Wang", "G. Karypis"], "venue": "Proceedings of 2005 SIAM international conference on Data Mining, volume 5, pages 205\u2013216. SIAM,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Cpar: Classification based on predictive association rules", "author": ["X. Yin", "J. Han"], "venue": "Proceedings of 2003 SIAM international conference on Data Mining, volume 3, pages 369\u2013376. SIAM,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "One line has ordinary performance with strong interpretability on a set of simple features, but meets a serious bottleneck when modeling complex high-order interactions between features, such as linear regression, logistic regression [15], and support vector machine [29].", "startOffset": 234, "endOffset": 238}, {"referenceID": 28, "context": "One line has ordinary performance with strong interpretability on a set of simple features, but meets a serious bottleneck when modeling complex high-order interactions between features, such as linear regression, logistic regression [15], and support vector machine [29].", "startOffset": 267, "endOffset": 271}, {"referenceID": 1, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 142, "endOffset": 145}, {"referenceID": 12, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 215, "endOffset": 219}, {"referenceID": 14, "context": "However, their lower interpretability and high complexity prevent practitioners from deploying in practice [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 202, "endOffset": 206}, {"referenceID": 4, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 32, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 275, "endOffset": 279}, {"referenceID": 20, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 281, "endOffset": 285}, {"referenceID": 17, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 301, "endOffset": 305}, {"referenceID": 6, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 307, "endOffset": 310}, {"referenceID": 2, "context": "Recently, a novel series of models, the discriminative pattern-based models [3], [4], have demonstrated their advantages over the traditional models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Recently, a novel series of models, the discriminative pattern-based models [3], [4], have demonstrated their advantages over the traditional models.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "Among the set of important clinical variables (rows) that DPPRED discovered from the dataset of the Prize4Life Challenge 2012, two highlighted ones have later been experimentally verified that they have extremely high correlations with the ALS disease [35], [14], [19].", "startOffset": 258, "endOffset": 262}, {"referenceID": 18, "context": "Among the set of important clinical variables (rows) that DPPRED discovered from the dataset of the Prize4Life Challenge 2012, two highlighted ones have later been experimentally verified that they have extremely high correlations with the ALS disease [35], [14], [19].", "startOffset": 264, "endOffset": 268}, {"referenceID": 13, "context": "These two factors were not found by the top teams in the Challenge but there is indirect experimental and logical evidence for their being actually worth further study [35], [14], [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "These two factors were not found by the top teams in the Challenge but there is indirect experimental and logical evidence for their being actually worth further study [35], [14], [19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "proposed a classification method CMAR based on multiple class-association rules [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": "extended it to CPAR based on predictive association rules [34].", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "proposed to utilize patterns in a different angle, where data are partitioned based on patterns, and complex models are trained independently in different partitions [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Traditional ensemble methods using multiple trees, such as random forest [2] and gradient boosting decision trees [12], alleviate the over-fitting issue.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Traditional ensemble methods using multiple trees, such as random forest [2] and gradient boosting decision trees [12], alleviate the over-fitting issue.", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "showed that the global refinement could provide better performance because the growth and pruning processes in different trees are independent [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 125, "endOffset": 128}, {"referenceID": 26, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 31, "context": "transferred the binary vectors into an inner product kernel space using a support vector machine and showed the increase of classification accuracy [32].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "Furthermore, pairwise interactions have also been studied to fit a two-layer-tree model for accurate classification and regression [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "For example, in [28], after many efforts on pruning, the model size of the pruned random forest was still at megabytes and thus the prediction was too slow to support real-time applications.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Simply selecting patterns with the highest independent heuristics such as information gain and gini index is limited to very simple tasks due to the redundancy and over-fitting problems [20].", "startOffset": 186, "endOffset": 190}, {"referenceID": 29, "context": ", the types for classification or the real numbers for regression, LASSO [30] is widely used in feature selection tasks as well as forward selection [6].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": ", the types for classification or the real numbers for regression, LASSO [30] is widely used in feature selection tasks as well as forward selection [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": ", DDPMINE [4], patterns are extracted from categorical values and thus they are only able to handle the continuous variables after careful manual discretization, which is tricky and often requires prior knowledge about the data.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": ", DDPMINE [4], the practitioners have to discretize values of continuous variables prior to pattern mining.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "The random decision tree [2] introduces the randomness via bootstrapping training data, randomly selecting features and splitting values when dividing a large tree bag into two smaller ones.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "P \u2190 \u2205 for t = 1 to T do Build a random decision tree [2] with maximum depth D and minimum tree bag size \u03c3.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "As one of the most famous multi-tree based models, random forest [2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Instead of exhausted search of all possible combinations of k discriminative patterns, forward selection gradually adds the discriminative patterns one by one while each newly added discriminative pattern is the best choice at that time [6], which provides an efficient approximation of the exhausted search.", "startOffset": 237, "endOffset": 240}, {"referenceID": 29, "context": ", LASSO [30]) is designed to make the weight vector sparse by tuning a nonnegative parameter \u03bb, where the features with non-zero weight will be the", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Require: n training examples (xi, yi), a set of discriminative patterns P and k Return: top-k discriminative patterns set Pk and a generalized linear model f(\u00b7) Pk \u2190 \u2205 for t = 1 to k do for each pattern p in P do x\u2032 \u2190 construct pattern space(x,Pk \u222a {p}) using Algorithm 2 g(\u00b7)\u2190 a generalized linear model [29] on (xi, yi) perp \u2190 g(\u00b7)\u2019s training performance Pk \u2190 Pk \u222a {arg maxp perp} x\u2032 \u2190 construct pattern space(x,Pk) f(\u00b7)\u2190 a generalized linear model on (xi, yi) return Pk, f(\u00b7)", "startOffset": 305, "endOffset": 309}, {"referenceID": 10, "context": "The LASSO implementation in GLMNET [11] is adopted in this thesis, whose loss function is the cross entropy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "Furthermore, to make the errors in different datasets comparable, min-max normalization is adopted to scale the continuous labels into [0, 1].", "startOffset": 135, "endOffset": 141}, {"referenceID": 3, "context": "DDPMINE [4] is a previous state-of-the-art discriminative pattern based algorithm.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "Random Forest (RF) [2] is another baseline method using same parameters as those in the random forest used in DPPRED, except for D.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "4) Lab Test 2 (LT2): continuous values in [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 2, "context": "DDPMINE outperforms decision tree and support vector machine on all these datasets [3], [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "DDPMINE outperforms decision tree and support vector machine on all these datasets [3], [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "DDPMINE is a previous state-of-the-art pattern-based classification method, which outperforms traditional classification models including decision tree and support vector machine [3][4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "DDPMINE is a previous state-of-the-art pattern-based classification method, which outperforms traditional classification models including decision tree and support vector machine [3][4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "To make the errors in different datasets comparable, min-max normalization is adopted to scale the continuous labels into [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 18, "context": "2012, a subset of PRO-ACT data was constructed with the aim to crowdsource the challenge of ALS prognosis as a data mining task, which is known as the DREAM-Phil Bowen ALS Prediction Prize4Life Challenge (\u201cthe 2012 challenge\u201d for short in this section) [19].", "startOffset": 253, "endOffset": 257}, {"referenceID": 18, "context": "It has predicted ALS progression from clinical data better than clinicians do, and can potentially reduce the cost of future ALS trials by $6-million [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "A detailed description of the data can be found in the supplement of [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "For the patient clustering, by making analogy from bag-of-words to bag-of-patterns, we adopt Latent Dirichlet Allocation (LDA) algorithm [1]", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "05) when minocycline, a drug that can delay the progression of ALS, is applied [35], [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "respiratory\u201d is reported by several among the top 5 teams in the 2012 challenge [19] and also by DPPRED, it should not be surprising that the respiratory rate is also in the list.", "startOffset": 80, "endOffset": 84}], "year": 2016, "abstractText": "In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data. In this paper, we propose a novel Discriminative Pattern-based Prediction framework (DPPRED) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability. Specifically, DPPRED adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models. DPPRED selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models. Extensive experiments show that in many scenarios, DPPRED provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts. In particular, taking a clinical application dataset as a case study, our DPPRED outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns.", "creator": "LaTeX with hyperref package"}}}