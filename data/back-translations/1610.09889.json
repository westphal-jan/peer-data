{"id": "1610.09889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Chinese Poetry Generation with Planning based Neural Network", "abstract": "Chinese poetry production is a very challenging task in the processing of natural language. In this paper, we propose a novel two-step method of poetry generation, which first plans the subthemes of the poem according to the user's writing intention and then generates each line of the poem one after the other using a modified, recurring neural network encoder decoder framework. The proposed planning-based method can ensure that the poem produced is consistent and semantically consistent with the user's intentions. A comprehensive evaluation with human judgement shows that our proposed approach surpasses the most modern methods of poetry production and the quality of the poem is somehow comparable to human poets.", "histories": [["v1", "Mon, 31 Oct 2016 12:16:39 GMT  (250kb,D)", "http://arxiv.org/abs/1610.09889v1", "Accepted at COLING 2016"], ["v2", "Wed, 7 Dec 2016 03:56:19 GMT  (262kb,D)", "http://arxiv.org/abs/1610.09889v2", "Accepted paper at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["zhe wang", "wei he", "hua wu", "haiyang wu", "wei li", "haifeng wang", "enhong chen"], "accepted": false, "id": "1610.09889"}, "pdf": {"name": "1610.09889.pdf", "metadata": {"source": "CRF", "title": "Chinese Poetry Generation with Planning based Neural Network", "authors": ["Zhe Wang", "Wei He", "Hua Wu", "Haiyang Wu", "Wei Li", "Haifeng Wang", "Enhong Chen"], "emails": ["xiaose@mail.ustc.edu.cn,", "cheneh@ustc.edu.cn", "wanghaifeng}@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "The classical Chinese poetry is a great and important heritage of Chinese culture. During the history of more than two thousand years, millions of beautiful poems are written to praise heroic characters, beautiful scenery, love, friendship, etc. There are different kinds of Chinese classical poetry, such as Tang poetry and Song iambics. Each type of poetry has to follow some specific structural, rhythmical and tonal patterns. Table 1 shows an example of quatrain which was one of the most popular genres of poetry in China. The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty.\nIn recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user\u2019s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user\u2019s writing intent can only affect the first line, and the rest three lines may have no association with the main topic of the poem, which may lead to semantic inconsistency when generating poems. In addition, topics of poems are usually represented by the words from the collected poems in the training corpus. But as we know, the words used in poems, especially poems written in ancient time, are different from modern languages. As a consequence, the existing methods may fail to generate meaningful poems if a user wants to write a poem for a modern term (e.g., Barack Obama).\nIn this paper, we propose a novel poetry generating method which generates poems in a two-stage procedure: the contents of poems (\u201cwhat to say\u201d) are first explicitly planned, and then surface realization (\u201chow to say\u201d) is conducted. Given a user\u2019s writing intent which can be a set of keywords, a sentence or\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 0.\n09 88\n9v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 6\neven a document described by natural language, the first step is to determine a sequence of sub-topics for the poem using a poem planning model, with each line represented by a sub-topic. The poem planning model decomposes the user\u2019s writing intent into a series of sub-topics, and each sub-topic is related to the main topic and represents an aspect of the writing intent. Then the poem is generated line by line, and each line is generated according to the corresponding sub-topic and the preceding generated lines, using a recurrent neural network based encoder-decoder model (RNN enc-dec). We modify the RNN enc-dec framework to support encoding of both sub-topics and the preceding lines. The planning based mechanism has two advantages compared to the previous methods. First, every line of the generated poem has a closer connection to user\u2019s writing intent. Second, the poem planning model can learn from extra knowledge source besides the poem data, such as large-scale web data or knowledge extracted from encyclopedias. As a consequence, it can bridge the modern concepts and the set of words covered by ancient poems. Take the term \u201cBarack Obama\u201d as the example: using the knowledge from encyclopedias, the poem planning model can extend the user\u2019s query, Barack Obama, to a series of sub-topics such as outstanding, power, etc., therefore ensuring semantic consistency in the generated poems.\nThe contribution of this paper is two-fold. First, we propose a planning-based poetry generating framework, which explicitly plans the sub-topic of each line. Second, we use a modified RNN encoderdecoder framework, which supports encoding of both sub-topics and the preceding lines, to generate the poem line by line.\nThe rest of this paper is organized as follows. Section 2 describes some previous work on poetry generation and compares our work with previous methods. Section 3 describes our planning based poetry generation framework. We introduce the datasets and experimental results in Section 4. Section 5 concludes the paper."}, {"heading": "2 Related Work", "text": "Poetry generation is a challenging task in NLP. Oliveira et al. (2009; 2012; 2014) proposed a poem generation method based on semantic and grammar templates. Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation. Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry. Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially.\nRecently, deep learning methods achieve great success in poem generation. Zhang and Lapata (2014) proposed a quatrain generation model based on recurrent neural network (RNN). The approach generates the first line from the given keywords with a recurrent neural network language model (RNNLM) (Mikolov et al., 2010) and then the subsequent lines are generated sequentially by accumulating the status of the lines that have been generated so far. Wang et al. (2016) generated the Chinese Song iambics\nusing an end-to-end neural machine translation model. The iambic is generated by translating the previous line into the next line sequentially. This procedure is similar to SMT, but the semantic relevance between sentences is better. Wang et al. (2016) did not consider the generation of the first line. Therefore, the first line is provided by users and must be a well-written sentence of the poem. Yi et al. (2016) extended this approach to generate Chinese quatrains. The problem of generating the first line is resolved by a separate neural machine translation (NMT) model which takes one keyword as input and translates it into the first line. Marjan Ghazvininejad and Knight (2016) proposed a poetry generation algorithm that first generates the rhyme words related to the given keyword and then generated the whole poem according to the rhyme words with an encoder-decoder model (Sutskever et al., 2014).\nOur work differs from the previous methods as follows. First, we don\u2019t constrain the user\u2019s input. It can be some keywords, phrases, sentences or even documents. The previous methods can only support some keywords or must provide the first line. Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus. Finally, our poem generation model has a simpler structure compared with those in (Zhang and Lapata, 2014; Yi et al., 2016)."}, {"heading": "3 Approaches", "text": ""}, {"heading": "3.1 Overview", "text": "Inspired by the observation that a human poet shall make an outline first before writing a poem, we propose a planning-based poetry generation approach (PPG) that first generates an outline according to the user\u2019s writing intent and then generates the poem. Our PPG system takes user\u2019s writing intent as input which can be a word, a sentence or a document, and then generates a poem in two stages: Poem Planning and Poem Generation. The two-stage procedure of PPG is illustrated in Figure 1.\nSuppose we are writing a poem that consists of N lines with li representing the i-th line of the poem. In the Poem Planning stage, the input query is transformed into N keywords (k1, k2, ..., kN ), where ki is the i-th keyword that represents the sub-topic for the i-th line. In the Poem Generation stage, li is generated by taking ki and l1:i\u22121 as input, where l1:i\u22121 is a sequence concatenated by all the lines\ngenerated previously, from l1 to li\u22121. Then the poem can be generated sequentially, and each line is generated according to one sub-topic and all the preceding lines."}, {"heading": "3.2 Poem Planning", "text": ""}, {"heading": "3.2.1 Keyword Extraction", "text": "The user\u2019s input writing intent can be represented as a sequence of words. There is an assumption in the Poem Planning stage that the number of keywords extracted from the input query Q must be equal to the number of lines N in the poem, which can ensure each line takes just one keyword as the sub-topic. If the user\u2019s input query Q is too long, we need to extract the most important N words and keep the original order as the keywords sequence to satisfy the requirement.\nWe use TextRank algorithm (Mihalcea and Tarau, 2004) to evaluate the importance of words. It is a graph-based ranking algorithm based on PageRank (Brin and Page, 1998). Each candidate word is represented by a vertex in the graph and edges are added between two words according to their cooccurrence; the edge weight is set according to the total count of co-occurrence strength of the two words. The TextRank score S(Vi) is initialized to a default value (e.g. 1.0) and computed iteratively until convergence according to the following equation:\nS(Vi) = (1\u2212 d) + d \u2211\nVj\u2208E(Vi)\nwji\u2211 Vk\u2208E(Vj)wjk S(Vj), (1)\nwhere wij is the weight of the edge between node Vj and Vi, E(Vi) is the set of vertices connected with Vi, and d is a damping factor that usually set to 0.85 (Brin and Page, 1998), and the initial score of S(Vi) is set to 1.0."}, {"heading": "3.2.2 Keyword Expansion", "text": "If the user\u2019s input query Q is too short to extract enough keywords, we need to expand some new keywords until the requirement of keywords number is satisfied. We use two different methods for keywords expansion.\nRNNLM-based method. We use a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010) to predict the subsequent keywords according to the preceding sequence of keywords: ki = argmaxk P (k|k1:i\u22121), where ki is the i-th keyword and k1:i\u22121 is the preceding keywords sequence.\nThe training of RNNLM needs a training set consisting of keyword sequences extracted from poems, with one keyword representing the sub-topic of one line. We automatically generate the training corpus from the collected poems. Specifically, given a poem consisting of N lines, we first rank the words in each line according to the TextRank scores computed on the poem corpus. Then the word with the highest TextRank score is selected as the keyword for the line. In this way, we can extract a keyword sequence for every poem, and generate a training corpus for the RNNLM based keywords predicting model.\nKnowledge-based method. The above RNNLM-based method is only suitable for generating subtopics for those covering by the collected poems. This method does not work when the user\u2019s query contains out-of-domain keywords, for example, a named entity not covered by the training corpus.\nTo solve this problem, we propose a knowledge-based method that employs extra sources of knowledge to generate sub-topics. The extra knowledge sources can be used include encyclopedias, suggestions of search engines, lexical databases (e.g. WordNet), etc. Given a keyword ki, the key idea of the method is to find some words that can best describe or interpret ki. In this paper, we use the encyclopedia entries as the source of knowledge to expand new keywords from ki. We retrieve those satisfying all the following conditions as candidate keywords: (1) the word is in the window of [-5, 5] around ki; (2) the part-of-speech of the word is adjective or noun; (3) the word is covered by the vocabulary of the poem corpus. Then the candidate words with the highest TextRank score are selected as the keywords."}, {"heading": "3.3 Poem Generation", "text": "In the Poem Generation stage, the poem is generated line by line. Each line is generated by taking the keyword specified by the Poem Planning model and all the preceding text as input. This procedure can be\nconsidered as a sequence-to-sequence mapping problem with a slight difference that the input consists of two different kinds of sequences: the keyword specified by the Poem Planning model and the previously generated text of the poem. We modify the framework of an attention based RNN encoder-decoder (RNN enc-dec) (Bahdanau et al., 2014) to support multiple sequences as input.\nGiven a keyword k which has Tk characters, i.e. k = {a1, a2, ..., aTk}, and the preceding text x which has Tx characters, i.e. x = {x1, x2, ..., xTx}, we first encode k into a sequence of hidden states [r1 : rTk ], and x into [h1 : hTx ], with bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014) models. Then we integrate [r1 : rTk ] into a vector rc by concatenating the last forward state and the first backward state of [r1 : rTk ], where\nrc = [\u2212\u2192rTk\u2190\u2212r1 ] . (2)\nWe set h0 = rc, then the sequence of vectors h = [h0 : hTx ] represents the semantics of both k and x, as illustrated in Figure 2. Notice that when we are generating the first line, the length of the preceding text is zero, i.e. Tx = 0, then the vector sequence h only contains one vector, i.e. h = [h0], therefore, the first line is actually generated from the first keyword.\nFor the decoder, we use another GRU which maintains an internal status vector st, and for each generation step t, the most probable output yt is generated based on st, context vector ct and previous generated output yt\u22121. This can be formulated as follows:\nyt = argmax y\nP (y|st, ct, yt\u22121). (3)\nAfter each prediction, st is updated by\nst = f(st\u22121, ct\u22121, yt\u22121). (4)\nf(\u00b7) is an activation function of GRU and ct is recomputed at each step by the alignment model:\nct = Th\u2211 j=1 atjhj . (5)\nhj is the j-th hidden state in the encoder\u2019s output. The weight atj is computed by\natj = exp(etj)\u2211Th k=1 exp(etk) , (6)\nwhere etj = v T a tanh(Wast\u22121 + Uahj). (7)\netj is the attention score on hj at time step t. The probability of the next word yt can be defined as:\nP (yt|y1, ..., yt\u22121,x,k) = g(st, yt\u22121, ct), (8)\nwhere g(\u00b7) is a nonlinear function that outputs the probability of yt. The parameters of the poem generation model are trained to maximize the log-likelihood of the training corpus:\nargmax N\u2211 n=1 logP (yn|xn,kn). (9)"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "In this paper, we focus on the generation of Chinese quatrain which has 4 lines and each line has the same length of 5 or 7 characters. We collected 76,859 quatrains from the Internet and randomly chose 2,000 poems for validation, 2,000 poems for testing, and the rest for training.\nAll the poems in the training set are first segmented into words using a CRF based word segmentation system. Then we calculate the TextRank score for every word. The word with the highest TextRank score is selected as the keyword for the line. In this way, we can extract a sequence of 4 keywords for every quatrain. From the training corpus of poems, we extracted 72,859 keyword sequences, which is used to train the RNN language model for keyword expansion (see section 3.2.2). For knowledge-based expansion, we use Baidu Baike1 and Wikipedia as the extra sources of knowledge.\nAfter extracting four keywords from the lines of a quatrain, we generate four triples composed of (the keyword, the preceding text, the current line), for every poem. Take the poem in Table 1 as example, the generated triples are shown in Table 2. All the triples are used for training the RNN enc-dec model proposed in section 3.3."}, {"heading": "4.2 Training", "text": "For the proposed attention based RNN enc-dec model, we chose the 6,000 most frequently used characters as the vocabulary for both source and target sides. The word embedding dimensionality is 512 and initialized by word2vec (Mikolov et al., 2013). The recurrent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set."}, {"heading": "4.3 Evaluation", "text": ""}, {"heading": "4.3.1 Evaluation Metrics", "text": "It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such\n1A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com.\nas BLEU and METEOR, have little correlation with human evaluation. Therefore, we carry out a human study to evaluate the poem generation models. Following (He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014), we use four evaluation standards for human evaluators to judge the poems: \u201cPoeticness\u201d, \u201cFluency\u201d, \u201cCoherence\u201d, \u201cMeaning\u201d. The detailed illustration can be seen in Table 3. The score of each aspect ranges from 1 to 5 with the higher score the better. Each system generates twenty 5-character quatrains and twenty 7-character quatrains. All the generated poems are evaluated by 5 experts and the rating scores are averaged as the final score."}, {"heading": "4.3.2 Baselines", "text": "We implemented several poetry generation methods as baselines and employed the same pre-processing method for all the methods.\nSMT. A Chinese poetry generation method based on Statistical Machine Translation (He et al., 2012). A poem is generated iteratively by \u201ctranslating\u201d the previous line into the next line.\nRNNLM. A method for generating textual sequences (Graves, 2013), which is proposed by Mikolov et al. (2010). The lines of a poem are concatenated together as a character sequence which is used to train the RNNLM.\nRNNPG. In the approach of RNN-based Poem Generator (Zhang and Lapata, 2014), the first line is generated by a standard RNNLM and then all the other lines are generated iteratively based on a context vector encoded from the previous lines.\nANMT. The Attention based Neural Machine Translation method. It considers the problem as a machine translation task, which is similar to the traditional SMT approach. The main difference is that in ANMT, the machine translation system is a standard attention based RNN enc-dec framework (Bahdanau et al., 2014)."}, {"heading": "4.3.3 Results", "text": "The results of the human evaluation are shown in Table 4. We can see that our proposed method, Planning based Poetry Generation (PPG), outperforms all baseline models in average scores. The results are consistent with both settings of 5-character and 7-character poem generations.\nThe poems generated by SMT are better in Poeticness than RNNLM, which demonstrates that the translation based method can better capture the mapping relation between two adjacent lines. ANMT is a strong baseline which performs better than SMT, RNNLM and RNNPG, but lower than our approach. Both ANMT and PPG use the attention based enc-dec framework. The main difference is that our method defines the sub-topics for each line before generating the poem. The ANMT method just translates the preceding text into the next line. Without the guide of sub-topics, the system tends to generate more general but less meaningful results. In contrast, our approach explicitly considers the keywords, which\nhas better controls of the sub-topic for every line. From the results of the human evaluation, it can be seen that the proposed method obtained very close performances in Poeticness and Fluency compared with ANMT but much higher Coherence and Meaning scores, which verified the effectiveness of the sub-topic prediction model."}, {"heading": "4.4 Automatic Generation vs. Human Poet", "text": "We conducted an interesting evaluation that directly compares our automatic poem generation system with human poets, which is similar to the Turing Test (Turing, 1950). We randomly selected twenty poems from the test set, which are written by ancient Chinese poets. We used the titles of these poems as the input and generated 20 poems by our automatic generation system. Therefore, the machine-generated poems were under the same subject with human-written poems. Then we asked some human evaluators to distinguish the human-written poems from machine-generated ones. We had 40 evaluators in total. All of them were well-educated and had Bachelor or higher degree. Four of them were professional in Chinese literature and were assigned to the Expert Group. The other thirty-six evaluators were assigned to the Normal Group. In the blind test, we showed a pair of poems and their title to the evaluator at each time, and the evaluator was asked to choose from three options: (1) poem A is written by the human; (2) poem B is written by the human; (3) cannot distinguish which one is written by the human.\nThe evaluation results are shown in Table 5. We can see that 49.9% of the machine-generated poems are wrongly identified as the human-written poems or cannot be distinguished by the normal evaluators. But for expert evaluators, this number drops to 16.3%. We can draw two conclusions from the result: (1) under the standard of normal users, the quality of our machine-generated poems is very close to human poets; (2) but from the view of professional experts, the machine-generated poems still have some obvious shortages comparing to human-written poems. Table 6 gives an example for a pair of poems selected from our blind test."}, {"heading": "4.5 Generation Examples", "text": "Besides the ancient poems in Table 6, our method can generate poems based any modern terms. Table 7 shows some examples. The title of the left poem in Table 7 is\u5564\u9152(beer), the keywords given by our poem planning model are \u5564\u9152(beer), \u9999\u9187(aroma), \u6e05\u723d(cool) and \u9189(drunk). The title of the right one is a named entity\u51b0\u5fc3(Xin Bing), who was a famous writer. The poem planning system generates three keywords besides\u51b0\u5fc3(Xin Bing): \u6625\u6c34(spring river),\u7e41\u661f(stars) and\u5f80\u4e8b(the past), which are all related to the writer\u2019s works."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we proposed a novel two-stage poetry generation method which first explicitly decomposes the user\u2019s writing intent into a series of sub-topics, and then generates a poem iteratively using a modified attention based RNN encoder-decoder framework. The modified RNN enc-dec model has two encoders that can encode both the sub-topic and the preceding text. The evaluation by human experts shows that our approach outperforms all the baseline models and the poem quality is somehow comparable to human poets. We have also demonstrated that using encyclopedias as an extra source of knowledge, our approach can expand users\u2019 input into appropriate sub-topics for poem generation. In the future, we will investigate more methods for topic planning, such as PLSA, LDA or word2vec. We will also apply our approach to other forms of literary genres e.g. Song iambics, Yuan Qu etc., or poems in other languages."}, {"heading": "6 Acknowledgments", "text": "This research was supported by the National Basic Research Program of China (973 program No. 2014CB340505), the National Key Research and Development Program of China (Grant No. 2016YFB1000904), the National Science Foundation for Distinguished Young Scholars of China (Grant No. 61325010) and the Fundamental Research Funds for the Central Universities of China (Grant No. WK2350000001). We would like to thank Xuan Liu, Qi Liu, Tong Xu, Linli Xu, Biao Chang and the anonymous reviewers for their insightful comments and suggestions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["Brin", "Page1998] Sergey Brin", "Lawrence Page"], "venue": "Computer Networks,", "citeRegEx": "Brin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Brin et al\\.", "year": 1998}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Full-face poetry generation", "author": ["Colton et al.2012] Simon Colton", "Jacob Goodwin", "Tony Veale"], "venue": "In ICCC", "citeRegEx": "Colton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Colton et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Automatic analysis of rhythmic poetry with applications to generation and translation", "author": ["Greene et al.2010] Erica Greene", "Tugba Bodrumlu", "Kevin Knight"], "venue": null, "citeRegEx": "Greene et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2010}, {"title": "Generating chinese classical poems with statistical machine translation models", "author": ["He et al.2012] Jing He", "Ming Zhou", "Long Jiang"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating chinese couplets using a statistical mt approach", "author": ["Jiang", "Zhou2008] Long Jiang", "Ming Zhou"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu et al.2016] Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1603.08023", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Using genetic algorithms to create meaningful poetic text", "author": ["Graeme Ritchie", "Henry Thompson"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "Manurung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manurung et al\\.", "year": 2012}, {"title": "Generating topical poetry", "author": ["Marjan Ghazvininejad", "Xing Shi", "Kevin Knight"], "venue": null, "citeRegEx": "Ghazvininejad et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghazvininejad et al\\.", "year": 2016}, {"title": "Textrank: Bringing order into text", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Mou et al.2016] Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings the 26th International Conference on Computational Linguistics", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Netzer et al.2009] Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,", "citeRegEx": "Netzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Adapting a generic platform for poetry generation to produce spanish poems", "author": ["Raquel Herv\u00e1s", "Alberto D\u0131\u0301az", "Pablo Gerv\u00e1s"], "venue": "In ICCC", "citeRegEx": "Oliveira et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oliveira et al\\.", "year": 2014}, {"title": "Automatic generation of poetry: an overview", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2009\\E", "shortCiteRegEx": "Oliveira.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation. Computational Creativity, Concept Invention, and General Intelligence, 1:21", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2012\\E", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Kallirroi Georgila", "Steve Young"], "venue": "In 6th SIGdial Workshop on DISCOURSE and DIALOGUE", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Tosa et al.2008] Naoko Tosa", "Hideto Obara", "Michihiko Minoh"], "venue": "In Entertainment Computing-ICEC", "citeRegEx": "Tosa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tosa et al\\.", "year": 2008}, {"title": "Chinese song iambics generation with neural attention-based model. CoRR, abs/1604.06274", "author": ["Wang et al.2016] Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A summary of rhyming constraints of chinese poems", "author": ["Li Wang"], "venue": null, "citeRegEx": "Wang.,? \\Q2002\\E", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Wu et al.2009] Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu"], "venue": "In Entertainment Computing\u2013", "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "i, poet: Automatic chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Yan et al.2013] Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": "In IJCAI", "citeRegEx": "Yan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Generating chinese classical poems with rnn encoder-decoder", "author": ["Yi et al.2016] Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun"], "venue": null, "citeRegEx": "Yi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of chinese songci", "author": ["Zhou et al.2010] Cheng-Le Zhou", "Wei You", "Xiaojun Ding"], "venue": "Journal of Software,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002).", "startOffset": 302, "endOffset": 314}, {"referenceID": 21, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 24, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 15, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 17, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 18, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 29, "context": ", 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 9, "context": ", 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 25, "context": ", 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": ", 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems.", "startOffset": 52, "endOffset": 91}, {"referenceID": 22, "context": "More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 160, "endOffset": 220}, {"referenceID": 26, "context": "More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 160, "endOffset": 220}, {"referenceID": 12, "context": "The approach generates the first line from the given keywords with a recurrent neural network language model (RNNLM) (Mikolov et al., 2010) and then the subsequent lines are generated sequentially by accumulating the status of the lines that have been generated so far.", "startOffset": 117, "endOffset": 139}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al.", "startOffset": 0, "endOffset": 94}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation.", "startOffset": 0, "endOffset": 115}, {"referenceID": 4, "context": "Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints.", "startOffset": 0, "endOffset": 163}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems.", "startOffset": 0, "endOffset": 329}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines.", "startOffset": 0, "endOffset": 489}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially.", "startOffset": 0, "endOffset": 722}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially. Recently, deep learning methods achieve great success in poem generation. Zhang and Lapata (2014) proposed a quatrain generation model based on recurrent neural network (RNN).", "startOffset": 0, "endOffset": 927}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially. Recently, deep learning methods achieve great success in poem generation. Zhang and Lapata (2014) proposed a quatrain generation model based on recurrent neural network (RNN). The approach generates the first line from the given keywords with a recurrent neural network language model (RNNLM) (Mikolov et al., 2010) and then the subsequent lines are generated sequentially by accumulating the status of the lines that have been generated so far. Wang et al. (2016) generated the Chinese Song iambics", "startOffset": 0, "endOffset": 1294}, {"referenceID": 20, "context": "Marjan Ghazvininejad and Knight (2016) proposed a poetry generation algorithm that first generates the rhyme words related to the given keyword and then generated the whole poem according to the rhyme words with an encoder-decoder model (Sutskever et al., 2014).", "startOffset": 237, "endOffset": 261}, {"referenceID": 6, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 22, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 26, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 29, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 25, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 26, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 26, "context": "Finally, our poem generation model has a simpler structure compared with those in (Zhang and Lapata, 2014; Yi et al., 2016).", "startOffset": 82, "endOffset": 123}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line. Therefore, the first line is provided by users and must be a well-written sentence of the poem. Yi et al. (2016) extended this approach to generate Chinese quatrains.", "startOffset": 0, "endOffset": 183}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line. Therefore, the first line is provided by users and must be a well-written sentence of the poem. Yi et al. (2016) extended this approach to generate Chinese quatrains. The problem of generating the first line is resolved by a separate neural machine translation (NMT) model which takes one keyword as input and translates it into the first line. Marjan Ghazvininejad and Knight (2016) proposed a poetry generation algorithm that first generates the rhyme words related to the given keyword and then generated the whole poem according to the rhyme words with an encoder-decoder model (Sutskever et al.", "startOffset": 0, "endOffset": 454}, {"referenceID": 12, "context": "We use a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010) to predict the subsequent keywords according to the preceding sequence of keywords: ki = argmaxk P (k|k1:i\u22121), where ki is the i-th keyword and k1:i\u22121 is the preceding keywords sequence.", "startOffset": 57, "endOffset": 79}, {"referenceID": 0, "context": "We modify the framework of an attention based RNN encoder-decoder (RNN enc-dec) (Bahdanau et al., 2014) to support multiple sequences as input.", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": ", xTx}, we first encode k into a sequence of hidden states [r1 : rTk ], and x into [h1 : hTx ], with bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014) models.", "startOffset": 143, "endOffset": 161}, {"referenceID": 13, "context": "The word embedding dimensionality is 512 and initialized by word2vec (Mikolov et al., 2013).", "startOffset": 69, "endOffset": 91}, {"referenceID": 27, "context": "The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128.", "startOffset": 50, "endOffset": 64}, {"referenceID": 19, "context": "1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016).", "startOffset": 164, "endOffset": 231}, {"referenceID": 14, "context": "1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016).", "startOffset": 164, "endOffset": 231}, {"referenceID": 8, "context": "Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Following (He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014), we use four evaluation standards for human evaluators to judge the poems: \u201cPoeticness\u201d, \u201cFluency\u201d, \u201cCoherence\u201d, \u201cMeaning\u201d.", "startOffset": 10, "endOffset": 69}, {"referenceID": 25, "context": "Following (He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014), we use four evaluation standards for human evaluators to judge the poems: \u201cPoeticness\u201d, \u201cFluency\u201d, \u201cCoherence\u201d, \u201cMeaning\u201d.", "startOffset": 10, "endOffset": 69}, {"referenceID": 6, "context": "A Chinese poetry generation method based on Statistical Machine Translation (He et al., 2012).", "startOffset": 76, "endOffset": 93}, {"referenceID": 4, "context": "A method for generating textual sequences (Graves, 2013), which is proposed by Mikolov et al.", "startOffset": 42, "endOffset": 56}, {"referenceID": 0, "context": "The main difference is that in ANMT, the machine translation system is a standard attention based RNN enc-dec framework (Bahdanau et al., 2014).", "startOffset": 120, "endOffset": 143}, {"referenceID": 3, "context": "A method for generating textual sequences (Graves, 2013), which is proposed by Mikolov et al. (2010). The lines of a poem are concatenated together as a character sequence which is used to train the RNNLM.", "startOffset": 43, "endOffset": 101}], "year": 2016, "abstractText": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user\u2019s writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planningbased method can ensure that the generated poem is coherent and semantically consistent with the user\u2019s intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.", "creator": "LaTeX with hyperref package"}}}