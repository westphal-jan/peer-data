{"id": "1611.00354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Faster decoding for subword level Phrase-based SMT between related languages", "abstract": "A common and effective way to train translation systems between related languages is to consider basic units at the subword level, but this increases the length of sentences, leading to increased decoding time. Increase in length is also influenced by the specific choice of data format for presenting sentences as subwords. In a phrase-based SMT framework, we examine various choices of decoding parameters and data format and their impact on decoding time and translation accuracy. We suggest the best options for these settings, which significantly improve decoding time without compromising translation accuracy.", "histories": [["v1", "Tue, 1 Nov 2016 19:56:36 GMT  (20kb)", "http://arxiv.org/abs/1611.00354v1", "Accepted at VarDial3 (Third Workshop on NLP for Similar Languages, Varieties and Dialects) collocated with COLING 2016; 7 pages"]], "COMMENTS": "Accepted at VarDial3 (Third Workshop on NLP for Similar Languages, Varieties and Dialects) collocated with COLING 2016; 7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "pushpak bhattacharyya"], "accepted": false, "id": "1611.00354"}, "pdf": {"name": "1611.00354.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anoopk@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 35\n4v 1\n[ cs\n.C L\n] 1\nN ov\n2 01\n6"}, {"heading": "1 Introduction", "text": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Examples of languages related by common ancestry are Slavic and Indo-Aryan languages. Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas (Thomason, 2000). Examples of such linguistic areas are the Indian subcontinent (Emeneau, 1956), Balkan (Trubetzkoy, 1928) and Standard Average European (Haspelmath, 2001) linguistic areas. Both forms of language relatedness lead to related languages sharing vocabulary and structural features.\nThere is substantial government, commercial and cultural communication among people speaking related languages (Europe, India and South-East Asia being prominent examples and linguistic regions in Africa possibly in the future). As these regions integrate more closely and move to a digital society, translation between related languages is becoming an important requirement. In addition, translation to/from related languages to a lingua franca like English is also very important. However, in spite of significant communication between people speaking related languages, most of these languages have few parallel corpora resources. It is therefore important to leverage the relatedness of these languages to build good-quality statistical machine translation (SMT) systems given the lack of parallel corpora.\nModelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages.\nSub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees.\nThis work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/\nOriginal this is an example of data formats for segmentation Subword units thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n\nInternal Marker thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n Boundary Marker thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n Space Marker thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n\nTable 1: Formats for sentence representation with subword units (example of orthographic syllables)\nHowever, the use of subword units increases the sentence length. This increases the training, tuning and decoding time for phrase-based SMT systems by an order of magnitude. This makes experimentation costly and time-consuming and impedes faster feedback which is important for machine translation research. Higher decoding time also makes deployment of MT systems based on subword units impractical.\nIn this work, we systematically study the choice of data format for representing sentences and various decoder parameters which affect decoding time. Our studies show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality.\nThe rest of the paper is organized as follows. Section 2 discusses the factors that affect decoding time which have been studied in this paper. Section 3 discusses our experimental setup. Section 4 discusses the results of our experiments with decoder parameters. Section 5 discusses the results of our experiments with corpus formats. Section 6 discusses prior work related to optimizing decoders for phrase-based SMT. Section 7 concludes the paper."}, {"heading": "2 Factors affecting decoding time", "text": "This section describes the factors affecting the decoding time that have been studied in this paper."}, {"heading": "2.1 Unit of translation", "text": "The decoding time for a sentence is proportional to length of the sentence (in terms of the basic units). Use of subword units will obviously result in increased sentence length. Various units have been proposed for translation (character, character n-gram, orthographic syllable, morpheme, etc.). We analysed the average length of the input sentence on four language pairs (Hindi-Malayalam, Malayalam-Hindi, Bengali-Hindi, Telugu-Malayalam) on the ILCI corpus (Jha, 2012). The average length of an input sentence for character-level representation is 7 times of the word-level input, while it is 4 times the word-level input for orthographic syllable level representation. So, the decoding time will increase substantially."}, {"heading": "2.2 Format for sentence representation", "text": "The length of the sentence to be decoded also depends on how the subword units are represented. We compare three popular formats for representation, which are illustrated in Table 1:\n\u2022 Boundary Marker: The subword at the boundary of a word is augmented with a marker character. There is one boundary subword, either the first or the last chosen as per convention. Such a representation has been used in previous work, mostly related to morpheme level representation.\n\u2022 Internal Marker: Every subword internal to the word is augmented with a marker character. This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh\u2019s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016).\n\u2022 Space Marker: The subword units are not altered, but inter-word boundary is represented by a space marker. Most work on translation between related languages has used this format.\nFor boundary and internal markers, the addition of the marker character does not change the sentence length, but can create two representations for some subwords (corresponding to internal and boundary positions), thus introducing some data sparsity. On the other hand, space marker doubles the sentence length (in terms in words), but each subword has a unique representation."}, {"heading": "2.3 Decoder Parameters", "text": "Given the basic unit and the data format, some important decoder parameters used to control the search space can affect decoding time. The decoder is essentially a search algorithm, and we investigated important settings related to two search algorithms used in the Moses SMT system: (i) stack decoding, (ii) cube-pruning (Chiang, 2007). We investigated the following parameters:\n\u2022 Beam Size: This parameter controls the size of beam which maintains the best partial translation hypotheses generated at any point during stack decoding.\n\u2022 Table Limit: Every source phrase in the phrase table can have multiple translation options. This parameter controls how many of these options are considered during stack decoding.\n\u2022 Cube Pruning Pop Limit: In the case of cube pruning, the parameter limits the number of hypotheses created for each stack .\nHaving a lower value for each of these parameters reduces the search space, thus reducing the decoding time. However, reducing the search space may increase search errors and decrease translation quality. Our work studies this time-quality trade-off."}, {"heading": "3 Experimental Setup", "text": "In this section, we describe the language pairs and datasets used, the details of our experiments and evaluation methodology."}, {"heading": "3.1 Languages and Dataset", "text": "We experimented with four language pairs (Bengali-Hindi, Malayalam-Hindi, Hindi-Malayalam and Telugu-Malayalam). Telugu and Malayalam belong to the Dravidian language family which are agglutinative. Bengali and Hindi are Indo-Aryan languages with a relatively poor morphology. The language pairs chosen cover different combinations of morphological complexity between source and target languages.\nWe used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of sentences from tourism and health domains. The data split is as follows \u2013 training: 44,777, tuning: 1000, test: 500 sentences."}, {"heading": "3.2 System details", "text": "As an example of subword level representation unit, we have studied the orthographic syllable (OS) (Kunchukuttan and Bhattacharyya, 2016) in our experiments. The OS is a linguistically motivated, variable length unit of translation, which consists of one or more consonants followed by a vowel (a C+V unit). But our methodology is not specific to any subword unit. Hence, the results and observations should hold for other subword units also. We used the Indic NLP Library1 for orthographic syllabification.\nPhrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using sub-word units (Vilar et al., 2007), we trained 10-gram language models. The language model was trained on the training split of the target language corpus.\n1http://anoopkunchukuttan.github.io/indic_nlp_library 2https://github.com/moses-smt/mgiza\nThe PBSMT systems were trained and decoded on a server with Intel Xeon processors (2.5 GHz) and 256 GB RAM."}, {"heading": "3.3 Evaluation", "text": "We use BLEU (Papineni et al., 2002) for evaluating translation accuracy. We use the sum of user and system time minus the time for loading the phrase table (all reported by Moses) to determine the time taken for decoding the test set."}, {"heading": "4 Effect of decoder parameters", "text": "We observed that the decoding time for OS-level models is approximately 70 times of the word-level model. This explosion in the decoding time makes translation highly compute intensive and difficult to perform in real-time. It also makes tuning MT systems very slow since tuning typically requires multiple decoding runs over the tuning set. Hence, we experimented with some heuristics to speed up decoding.\nFor normal stack decoding, two decoder parameters which impact the decode time are: (1) beam size of the hypothesis stack, and (2) table-limit: the number of translation options for each source phrase considered by the decoder. Since the vocabulary of the OS-level model is far less than that of the wordlevel model, we hypothesize that lower values for these parameters can reduce the decoding time without significantly affecting the accuracy. Table 2 shows the results of varying these parameters. We can see that with a beam size of 10, the decoding time is now about 9 times that of word-level decoding. This is a 7x improvement in decoding time over the default parameters, while the translation accuracy drops by less than 1%. If a beam size of 10 is used while decoding too, the drop in translation accuracy is larger (2.5%). Using this beam size during decoding also slightly reduces the translation accuracy. On the other hand, reducing the table-limit significantly reduces the translation accuracy, while resulting in lesser gains in decoding time.\nWe also experimented with cube-pruning (Chiang, 2007), a faster decoding method first proposed for use with hierarchical PBSMT. The decoding time is controlled by the pop-limit parameter in the Moses implementation of cube-pruning. With a pop-limit of 1000, the decoding time is about 12 times\nthat of word-level decoding. The drop in translation accuracy is about 1% with a 6x improvement over default stack decoding, even when the model is tuned with a pop-limit of 1000. Using this pop-limit during tuning also hardly impacts the translation accuracy. However, lower values of pop-limit reduce the translation accuracy.\nWhile our experiments primarily concentrated on OS as the unit of translation, we also compared the performance of stack decoding and cube pruning for character lavel models. The results are shown in Table 3. We see that character level models are 4-5 times slower than OS level models and hundreds of time slower than word level models with the default stack decoding. In the case of character based models also, the use of cube pruning (with pop-limit=1000) substantially speeds up decoding (20x speedup) with only a small drop in BLEU score.\nTo summarize, we show that reducing the beam size for stack decoding as well as using cube pruning help to improve decoding speed significantly, with only a marginal drop in translation accuracy. Using cube-pruning while tuning only marginally impacts translation accuracy."}, {"heading": "5 Effect of corpus format", "text": "For these experiments, we used the following decoder parameters: cube-pruning with cube-pruning-poplimit=1000 for tuning as well as testing. Table 4 shows the results of our experiments with different corpus formats.\nThe internal boundary marker format has a lower translation accuracy compared to the other two formats whose translation accuracies are comparable. In terms of decoding time, no single format is better than the others across all languages. Hence, it is recommended to use the space or boundary marker format for phrase-based SMT systems. Neural MT systems based on encoder decoder architectures, particularly without attention mechanism, are more sensitive to sentence length, so we presume that the boundary marker format may be more appropriate."}, {"heading": "6 Related Work", "text": "It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007). Aligning long sentences is computationally expensive, hence most work has concentrated on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory.\nThere has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Ferna\u0301ndez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007).\nIn this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014)."}, {"heading": "7 Conclusion and Future Work", "text": "We systematically study the choice of data format for representing subword units in sentences and various decoder parameters which affect decoding time in a phrase-based SMT setting. Our studies (using OS and character as basic units) show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality. Two data formats, the space marker and the boundary marker, perform roughly equivalently in terms of translation accuracy as well as decoding time. Since the tuning step contains a decoder in the loop, these settings also reduce the tuning time. We plan to investigate reduction of the time required for alignment."}, {"heading": "Acknowledgments", "text": "We thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information Technology, Govt. of India for their support. We also thank the anonymous reviewers for their feedback."}], "references": [{"title": "Statistical machine translation between related languages", "author": ["Mitesh Khapra", "Anoop Kunchukuttan"], "venue": "In NAACL Tutorials", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Cherry", "Foster2012] Colin Cherry", "George Foster"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Cherry et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2012}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Boosting performance of a statistical machine translation system using dynamic parallelism", "author": ["Juan C Pichel", "Jos\u00e9 C Cabaleiro", "Tom\u00e1s F Pena"], "venue": "Journal of Computational Science", "citeRegEx": "Fern\u00e1ndez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2016}, {"title": "Faster phrase-based decoding by refining feature state", "author": ["Michael Kayser", "Christopher D Manning"], "venue": "In Annual Meeting-Association For Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2014}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Fast, scalable phrase-based smt decoding", "author": ["Hoang et al.2016] Hieu Hoang", "Nikolay Bogoychev", "Lane Schwartz", "Marcin Junczys-Dowmunt"], "venue": "In arXiv Pre-print arXiv:1610.04265", "citeRegEx": "Hoang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Forest rescoring: Faster decoding with integrated language models", "author": ["Huang", "Chiang2007] Liang Huang", "David Chiang"], "venue": "In Annual Meeting-Association For Computational Linguistics", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha"], "venue": "In Language Resources and Evaluation Conference", "citeRegEx": "Jha.,? \\Q2012\\E", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "A space-efficient phrase table implementation using minimal perfect hash functions", "author": ["Marcin Junczys-Dowmunt"], "venue": "In International Conference on Text, Speech and Dialogue", "citeRegEx": "Junczys.Dowmunt.,? \\Q2012\\E", "shortCiteRegEx": "Junczys.Dowmunt.", "year": 2012}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Kunchukuttan", "Pushpak Bhattacharyya"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kunchukuttan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2016}, {"title": "Combining word-level and characterlevel models for machine translation between closely-related languages", "author": ["Nakov", "Tiedemann2012] Preslav Nakov", "J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Linguistic areas and language history", "author": ["Sarah Thomason"], "venue": "In lLanguages in Contact", "citeRegEx": "Thomason.,? \\Q2000\\E", "shortCiteRegEx": "Thomason.", "year": 2000}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets. In RANLP", "author": ["Tiedemann", "Nakov2013] J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": null, "citeRegEx": "Tiedemann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2013}, {"title": "Character-based psmt for closely related languages", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 13th Conference of the European Association for Machine Translation (EAMT", "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces. In Recent advances in natural language processing", "author": ["J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Tiedemann.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Can we translate letters", "author": ["Vilar et al.2007] David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Edinburghs statistical machine translation systems for wmt16", "author": ["Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Barry Haddow", "Ondrej Bojar"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Efficient phrase-table representation for machine translation with applications to online mt and speech translation", "author": ["Zens", "Ney2007] Richard Zens", "Hermann Ney"], "venue": "In HLT-NAACL", "citeRegEx": "Zens et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zens et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016).", "startOffset": 163, "endOffset": 191}, {"referenceID": 15, "context": "Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas (Thomason, 2000).", "startOffset": 166, "endOffset": 182}, {"referenceID": 20, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 67}, {"referenceID": 8, "context": "We analysed the average length of the input sentence on four language pairs (Hindi-Malayalam, Malayalam-Hindi, Bengali-Hindi, Telugu-Malayalam) on the ILCI corpus (Jha, 2012).", "startOffset": 163, "endOffset": 174}, {"referenceID": 21, "context": "This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh\u2019s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016).", "startOffset": 166, "endOffset": 212}, {"referenceID": 14, "context": "This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh\u2019s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016).", "startOffset": 166, "endOffset": 212}, {"referenceID": 2, "context": "The decoder is essentially a search algorithm, and we investigated important settings related to two search algorithms used in the Moses SMT system: (i) stack decoding, (ii) cube-pruning (Chiang, 2007).", "startOffset": 187, "endOffset": 201}, {"referenceID": 8, "context": "We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of sentences from tourism and health domains.", "startOffset": 57, "endOffset": 68}, {"referenceID": 10, "context": "We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 25, "endOffset": 45}, {"referenceID": 20, "context": "Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using sub-word units (Vilar et al., 2007), we trained 10-gram language models.", "startOffset": 153, "endOffset": 173}, {"referenceID": 13, "context": "3 Evaluation We use BLEU (Papineni et al., 2002) for evaluating translation accuracy.", "startOffset": 25, "endOffset": 48}, {"referenceID": 2, "context": "We also experimented with cube-pruning (Chiang, 2007), a faster decoding method first proposed for use with hierarchical PBSMT.", "startOffset": 39, "endOffset": 53}, {"referenceID": 20, "context": "It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007).", "startOffset": 188, "endOffset": 208}, {"referenceID": 19, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012).", "startOffset": 25, "endOffset": 87}, {"referenceID": 5, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al.", "startOffset": 64, "endOffset": 80}, {"referenceID": 9, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al.", "startOffset": 137, "endOffset": 160}, {"referenceID": 3, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al., 2016), efficient memory allocation (Hoang et al.", "startOffset": 192, "endOffset": 216}, {"referenceID": 6, "context": ", 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 6, "context": ", 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": ", 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007).", "startOffset": 62, "endOffset": 76}, {"referenceID": 4, "context": "Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014).", "startOffset": 94, "endOffset": 141}, {"referenceID": 11, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models.", "startOffset": 6, "endOffset": 137}, {"referenceID": 11, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models.", "startOffset": 6, "endOffset": 317}, {"referenceID": 2, "context": "Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.", "creator": "LaTeX with hyperref package"}}}