{"id": "1611.00829", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Multidimensional Binary Search for Contextual Decision-Making", "abstract": "We look at a multidimensional search problem motivated by questions of context-based decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a $d $dimensional one-size-fits-all sphere and then generates a sequence of $d $dimensional directions. We get access to the instructions but no access to the state. After we get a direction, we have to guess the value of the point product between state and direction. Our goal is to minimize the number of times our guess is more than $\\ epsilon $from the true answer. We construct a polynomic time algorithm we call Projected Volume Repentance $O (d\\ log (d /\\ epsilon)) $, which is optimal up to a $\\ log d $factor. The algorithm combines a volume reduction strategy with a new geometric technique we call cylindrization.", "histories": [["v1", "Wed, 2 Nov 2016 22:38:32 GMT  (33kb)", "http://arxiv.org/abs/1611.00829v1", null], ["v2", "Wed, 26 Apr 2017 02:29:51 GMT  (34kb)", "http://arxiv.org/abs/1611.00829v2", "Appears in EC 2017"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["ilan lobel", "renato paes leme", "adrian vladu"], "accepted": false, "id": "1611.00829"}, "pdf": {"name": "1611.00829.pdf", "metadata": {"source": "CRF", "title": "Multidimensional Binary Search for Contextual Decision-Making", "authors": ["Ilan Lobel"], "emails": ["ilobel@stern.nyu.edu", "renatoppl@google.com", "avladu@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 82\n9v 1\n[ cs\n.D S]\n2 N\nov 2\n01 6"}, {"heading": "1 Introduction", "text": "Binary search is one of the most basic primitives in algorithm design. The binary search problem consists in trying to guess an unknown real number \u03b8 \u2208 [0, 1] given access to an oracle that replies for every guess xt if xt \u2264 \u03b8 or xt > \u03b8. After log(1/\u01eb) guesses, the binary search algorithm is able to estimate \u03b8 within \u01eb precision.\nWe study a multidimensional and online version of the binary search problem. The unknown quantity is a vector \u03b8 \u2208 Rd with \u2016\u03b8\u20162 \u2264 1 and in each iteration an adversary selects a direction ut \u2208 Rd such that \u2016ut\u20162 = 1. At each iteration, the algorithm is asked to guess the value of the dot product \u03b8\u22a4ut. After the algorithm makes a guess xt, it is revealed to the algorithm whether xt \u2264 \u03b8\u22a4ut or xt > \u03b8\u22a4ut. The goal of the algorithm designer is to create an algorithm that makes as few mistakes as possible, where a mistake corresponds to a guess with an error larger than \u01eb.\nThis problem has recently come up as a key building block in the design of online algorithms for contextual decision-making. In contextual decision-making, the direction ut corresponds to a context relevant for the period t decision and \u03b8\u22a4ut corresponds to the optimal period t decision. Contextual decision-making is increasingly important in an economy where decisions are ever more customized and personalized. We now mention two applications:\nPersonalized Medicine [3]: Determining the right dosage of a drug for a given patient is a well-studied problem in the medical literature. For example, for certain anticoagulant drugs, the appropriate dosage can vary by up to a factor of 10 from individual to individual. Increasingly, doctors are using not only demographic information to decide dosage levels, but are also using higher-dimensional clinical and genetic data. Bastani and Bayati [3] propose a mathematical formulation for this problem and tackle it using tools from statistical learning and contextual bandits. At its core, the problem studied is a multidimensional binary search problem: each patient is associated with a vector of features ut which describes his or her demographic, genetic and clinical data. The algorithm outputs a recommended dosage xt and then observes only whether the dosage\nwas below or above the ideal level. If the ideal dosage is a linear function of the features with unknown coefficients \u03b8 then what the algorithm observes is whether \u03b8\u22a4ut \u2265 xt or \u03b8\u22a4ut < xt. Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products. Instead of attempting to learn the market value of each product independently, it might be more sensible for the firm to build a pricing model based on features of each product. In internet advertising, for example, each impression sold by an ad exchange is unique in its combination of demographic and cookie data. While it is hopeless for the exchange to learn how to price each combination in isolation, it is reasonable for the firm to learn a model of the market value of its different products. In this setting, each product t is described by a vector ut of features. Assume the market value is a linear function with unknown coefficients \u03b8. If the firm sets a price xt for this item it will learn that \u03b8\u22a4ut \u2265 xt if the product is sold and that \u03b8\u22a4ut < xt otherwise. The goal in this setting is not minimizing guesses that are \u01eb far from the \u03b8\u22a4ut as in the personalized medicine setting, but to maximize revenue. Revenue, however, is a very asymmetric objective: if the price is above the market value we lose the sale and incur a large loss, while underpricing still leads to a sale where the loss in revenue is the difference \u03b8\u22a4ut\u2212xt. Nevertheless, Cohen et al [5] showed that an algorithm for the multidimensional binary search problem can be converted into an algorithm for the feature-based pricing problem in a black-box manner.\nThe first approach to this problem was due to Amin, Rostamizadeh and Syed [2] in the context of the pricing problem and is based on stochastic gradient descent. The stochastic gradient approach requires the features ut to be drawn from an unknown iid distribution, so that each feature can be used to obtain an unbiased estimator of a certain function. Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO. Javanmard and Nazerzadeh [8] apply a regularized maximum likelihood estimation approach and obtain an improved regret guarantee. One could also use a general purpose contextual bandit algorithm (such as Agarwal et al. [1]) to tackle the iid version of the multidimensional binary search problem, but such an algorithm would have regret that is polynomial in 1/\u01eb instead of the logarithmic regret obtained by the specialized algorithms.\nAll the previously discussed work rely on assuming that the directions ut are sampled iid. The only approach that makes no assumptions about the directions ut is by Cohen et al [5]. They do so by tackling directly the multidimensional binary search problem with adversarial feature vectors ut and describe an algorithm with a bound of O(d\n2 log(d/\u01eb)) on the number of mistakes. To obtain that bound, the paper builds on the ellipsoid method from convex optimization. The algorithm always maintains a knowledge set in the shape of an ellipsoid and then chooses an xt that cuts the ellipsoid through its center whenever there is significant uncertainty on the value of \u03b8\u22a4ut. The algorithm than replaces the resulting half-ellipsoid with the smallest ellipsoid that encloses it, and proceeds to the next iteration.\nOur Contributions: Our paper significantly improves the regret bound on the multidimensional binary search problem, providing nearly matching upper and lower bounds for this problem. In Proposition 3.1, we construct a lower bound of \u2126(d log(1/\u01eb \u221a d)) via a reduction to d one-dimensional problems, which is significantly lower than the O(d2 log(d/\u01eb)) regret bound from Cohen et al [5]. Under Cohen et al\u2019s ellipsoid-based algorithm, a fraction 1 \u2212 e\u22121/2d of the volume is removed at each iteration. This fraction is significantly less than half because the step of replacing a half-ellipsoid with its own enclosing ellipsoid is expensive in the sense that it adds back to the knowledge set most of the volume removed in the latest cut. Thus, any ellipsoid-based method requires d steps in order to remove a constant fraction of the volume. Therefore, an algorithm that removes a constant fraction of the volume at each iteration has the potential to perform significantly\nbetter than an ellipsoid-based method and, thus, might close the gap between the upper and lower bounds. We can thus conjecture that an algorithm that selects xt in each iteration so as to create two potential knowledge sets of approximately equal volume would perform nearly optimally.\nCutting a convex set into two sets of approximately equal volume is not a difficult task. In a classical result, Gru\u0308nbaum showed that cutting a set through its centroid generates two sets, each with at least a 1/e fraction of the original volume (see Theorem 5.1). Computing a centroid is a #Phard problem, but finding an approximate value of the centroid is sufficient for our purposes, and an approximation can be computed in polynomial time. An idea similar to this one was proposed by Bertsimas and Vempala [4], in a paper where they proposed a method for solving linear programs via an approximate Gru\u0308nbaum theorem.\nHowever, removing constant fractions of the volume at each iteration is not sufficient for our purposes. Even if the knowledge set has tiny volume, we might not be able to guess the value of \u03b8\u22a4u for some directions u with \u01eb accuracy. To solve our problem, we need to ensure that the knowledge set becomes small along all possible directions. An algorithm that does not keep track of the width of the knowledge set along different directions might not perform well. Perhaps surprisingly, our conjecture that an algorithm that cuts through the centroid at each iteration would have nearoptimal regret is false. We show in Theorem 8.3 that such a centroid algorithm generates a worstcase regret of \u2126(d2 log(1/\u01eb \u221a d)). This occurs precisely because the centroid algorithm does not keep track of the different widths of the knowledge set. In an ellipsoid-based algorithm, keeping tracks of the widths of a knowledge set is a relatively easy task since they correspond to the eigenvalues of the matrix that represents the ellipsoid. Keeping track of widths is a more difficult task in an algorithm that does not rely on ellipsoids. This brings us to our key algorithmic idea: cylindrification.\nCylindrification is the technique we introduce of maintaining a set of directions along which the width of the knowledge set is small and expanding the set in those directions, thus converting the set into a high-dimensional cylinder. A cylindrified set when projected onto its subspace of small directions becomes a hypercube. When projected onto its subspace of large directions, a cylindrified set looks exactly like the original set\u2019s projection onto the same subspace. Cylindrification reduces regret by significantly increasing the usefulness of each cut.\nOur main algorithm, the Projected Volume algorithm, maintains two objects at all times. It maintains a knowledge set (as the previous algorithms did), but it also maintains a set of orthogonal directions along which the knowledge set is small. At each iteration, it cylindrifies the knowledge set and then computes an approximate value of the centroid of the cylindrified set. It then chooses xt in order to cut through this approximate centroid. In Theorem 4.3, the main result of our paper, we prove that this algorithm has a near-optimal regret of O(d log(d/\u01eb)).\nThe analysis of our algorithm relies on a series of results we prove about convex bodies. We first prove a directional version of Gru\u0308nbaum\u2019s theorem (Theorem 5.3), which states that the width of the two sets along any direction u after a cut through the centroid are at least 1/(d+1) of the width along u of the original set. We also prove that Gru\u0308nbaum\u2019s theorem is robust to approximations (Lemma 5.5) and projections (Lemma 7.1). We also prove that the process of cylindrification does not add too much volume to the set (Lemma 6.1). We then use these geometric results to prove that the volume of the knowledge set projected onto its large directions serves as a potential function and show that it decreases exponentially fast, proving our main result."}, {"heading": "2 The Model", "text": "We consider an infinite horizon game between a player and nature. The game begins with nature selecting a state \u03b8 from the d-dimensional unit ball centered at the origin. We label this ball K0,\ni.e., K0 = {\u03b8 \u2208 Rd : ||\u03b8||2 \u2264 1}. The player knows K0, but does not know the value of \u03b8. 1 At every period t = 0, 1, 2, ..., nature selects a vector ut from the d-dimensional unit sphere, i.e., U = {u \u2208 Rd : ||u||2 = 1}, that we refer to as the period t direction. At every period, after nature reveals ut, the player must choose an action xt \u2208 R. The player\u2019s goal is to choose a value of xt that is close to u \u22a4 t \u03b8. Formally, we try to minimize the number of mistakes we make, where a mistake occurs whenever |xt \u2212 u\u22a4t \u03b8| > \u01eb for a given \u01eb > 0. We incur regret in period t whenever we make a mistake:\nrt =\n{\n0 if |xt \u2212 u\u22a4t \u03b8| \u2264 \u01eb ; 1 if |xt \u2212 u\u22a4t \u03b8| > \u01eb .\nAt the end of each period, nature reports to the player whether xt \u2264 u\u22a4t \u03b8 or xt > u\u22a4t \u03b8. We note that we do not learn the regret rt in each period, only whether xt \u2212 u\u22a4t \u03b8 is positive. Our goal is to find a policy that minimizes our total regret, or equivalently, the total number of mistakes we make over an infinite time horizon, i.e., R = \u2211 \u221e\nt=1 rt."}, {"heading": "3 Lower Bound", "text": "We now construct a lower bound on the regret incurred by our algorithm. The lower bound is obtained via a straightforward reduction to d one-dimensional problems. Proposition 3.1. Any algorithm will generate regret of at least \u2126(d log(1/\u01eb \u221a d)). Proof. Assume nature selects \u03b8 from within a d-dimensional cube with sides of length 1/ \u221a d. This is a valid choice since the unit ball K0 contains such a cube. Let ei represent the vector with value 1 in coordinate i \u2208 {1, ..., d} and value 0 in all other coordinates. Suppose nature selects directions that correspond to the vectors ei in round-robin fashion, i.e., ut = e(t mod d)+1. Because of the symmetry of the cube from which \u03b8 is selected, and the orthogonality of the directions ut, this problem is equivalent to d independent binary searches over one-dimensional intervals with length l = 1/ \u221a d. Our result follows since a one-dimensional binary search over an interval with length l up to precision \u01eb incurs \u2126(log(l/\u01eb)) mistakes.\nWe note that the lower bound above applies even for the iid version of the multidimensional binary search problem, as nature could be given a distribution over d orthogonal direction vectors. Making the problem offline would also not lower the regret, as having advance knowledge of the direction vectors is useless in the instance above."}, {"heading": "4 The Projected Volume Algorithm", "text": "In this section, we describe the central idea for obtaining near-optimal regret. In the standard single-dimensional binary search algorithm, the error of the algorithm at any given iteration is proportional to the length of the interval. The length of the interval thus provides a clear measure in which to make progress. In the multi-dimensional case, there is no global measure of error, but only a measure of error for each direction. To make this precise, consider a knowledge set K \u2286 Rd corresponding to the set of values of \u03b8 that are compatible with what the algorithm has observed. Given a direction u (i.e., u is a unit vector), the error incurred by the algorithm to predict the dot product u\u22a4\u03b8 corresponds to the directional width of K along u:\nw(K,u) = max x,y\u2208K\nu\u22a4(x\u2212 y) . (4.1)\n1Although we assume for simplicity that K0 is a ball throughout our paper, we could have let K0 be an arbitrary convex body contained inside the unit ball.\nwhich is a measure that is particular for direction u. Since the algorithm does not know which directions it faces in future iterations, it must decrease some measure that implies progress in a more global sense. A natural such measure is the volume of K. However, measuring volume alone might be misleading. Consider that case where our current knowledge set is the thin rectangle represented in Figure 1.\n\u01eb\nFigure 1: Decreasing volume might not lead to progress with respect to width. Both horizontal and vertical cuts remove half the volume, but only the vertical cut makes progress towards our goal.\nCutting the knowledge set along either the red horizontal or the blue vertical line and keeping one the sides would decrease the volume by half. From the perspective of our problem, however, the red cut is useless since we already have a good estimate of the width along that direction. Meanwhile, the blue cut is very useful since it decreases the width along a direction that has still a lot of uncertainty to be resolved.\nMotivated by this observation we keep track of the volume of the knowledge set projected onto a subspace for which there is still a non-trivial amount of uncertainty. Precisely, our algorithm will be parametrized by a value \u03b4 > 0 which defines the notion of \u2018small\u2019. We maintain two objects:\n1. the knowledge set Kt \u2286 Rd which will consist of all vectors \u03b8 which are consistent with the observations of the algorithm so far.\n2. a set of orthonormal vectors St = {s1, . . . , snt} spanning a subspace Ut of dimensionality nt such that the knowledge set has small width along any of those directions and has large width along any direction perpendicular to them. Formally:\nUt = span(St) s.t. w(Kt, s) \u2264 \u03b4, \u2200s \u2208 St and w(Kt, u) > \u03b4, for all u perpendicular to Ut , where span(\u00b7) denotes the span of a set of vectors. It will be useful to refer to Lt = {u|u\u22a4s = 0, \u2200s \u2208 St} as the subspace of large directions.\nOur plan will be to ignore a dimension once it becomes small enough and focus on bounding the volume of the projection of the knowledge set Kt onto the subspace of large directions Lt. To formalize this notion, let us define the notion of cylindrification of a set with respect to orthonormal vectors.\nDefinition 4.1 (Cylindrification). Given a set of orthonormal vectors S = {s1, . . . , sn}, let L = {u|u\u22a4s = 0;\u2200s \u2208 S} be the subspace orthogonal to span(S) and \u03a0L(K) be the projection 2 of K onto L. Given a convex set K \u2286 Rd and a set of orthonormal vectors S = {s1, . . . , sn} we define:\nCyl(K,S) :=\n{\nx+ n \u2211\ni=1\nyisi\n\u2223 \u2223 \u2223 \u2223\nx \u2208 \u03a0L(K) and min \u03b8\u2208K \u03b8\u22a4si \u2264 yi \u2264 max \u03b8\u2208K \u03b8\u22a4si\n}\n."}, {"heading": "Or more concisely, but less intuitively:", "text": "Cyl(K,S) = \u03a0L(K) + \u03a0span(s1)(K) + . . .+\u03a0span(sn)(K)\nwhere the sums applied to sets are Minkowski sums. 3\nInformally, the cylindrification operation is designed to create a set with the same projection onto the subspace of large directions, i.e., \u03a0LtCyl(Kt, St) = \u03a0Lt(Kt), while regularizing the projection of the set onto the subspace of small directions: \u03a0StCyl(Kt, St) is a box.\n2Formally if {\u21131, . . . , \u2113k} is an orthonormal basis of L, then \u03c0L(x) = \u2211 k\ni=1 \u2113i\u2113\n\u22a4\ni x and \u03a0L(K) = {\u03c0L(x)|x \u2208 K}. 3By Minkowski sum between two sets, we mean A+B = {a+ b : a \u2208 A, b \u2208 B}.\nWe are now ready to present our algorithm, focusing on its geometric aspects and ignoring (for now) the question on how to efficiently compute each step. The algorithm is parametrized by a constant \u03b4 > 0. It starts with K0 being the ball of radius 1 and with S0 = \u2205. In each iteration the algorithm receives a unit vector ut from nature. The algorithm then predicts xt using the centroid zt of Cyl(Kt, St), by setting xt = u \u22a4 t zt. The definition of the centroid is given below:\nDefinition 4.2. The centroid z of a convex set K is defined as\nz = 1\nvol(K)\n\u222b\nx\u2208K x dx ,\nwhere vol(\u00b7) denotes the volume of a set. Upon learning if the estimate was too small or too large, we updateKt to Kt+1 = Kt\u2229{\u03b8|\u03b8\u22a4ut \u2264 xt} or Kt+1 = Kt \u2229 {\u03b8|\u03b8\u22a4ut \u2265 xt}. The next step in our algorithm is to verify if there exists any direction v orthogonal to St such that w(Kt+1, v) \u2264 \u03b4. As long as such directions exists, we add them to St and call the resulting set St+1.\nOur main result in this paper is:\nTheorem 4.3. The Projected Volume algorithm has regret O(d log(d/\u01eb)) for the multi-dimensional binary search problem.\nOur strategy for proving Theorem 4.3 is to use the volume of the projection of Kt onto the subspace of large directions as our potential function:\n\u03a6t := vol(\u03a0LtKt) .\nIn each iteration, either the set of small directions remains the same or it grows. We first consider the case where the set of small directions remains the same, i.e., St+1 = St. In this case, we want to argue that the volume of the projection of Kt onto Lt decreases in that iteration. If St = \u2205, then \u03a0LtKt = Kt and the volume decreases by at least a constant factor. This follows from Gru\u0308nbaum\u2019s Theorem, which we review in the next section. However, if St 6= \u2205, then a decrease in the volume of Kt does not necessarily guarantee a decrease in the volume of the projection. For example, consider the example in Figure 2 where we cut through the center of a rectangular Kt. Even though the volume of Kt+1 is half the volume of Kt, the volume of the projection onto the x-axis doesn\u2019t decrease as much. We will argue that the decrease in volume due to Gru\u0308nbaum\u2019s Theorem extends to projections (with a small loss) if the width along the cut direction is much larger than the width along the directions orthogonal to the projection subspace.\nWe now consider the case where we add a new direction to St. In this case, we will measure the volume in the next iteration as projected onto a subspace of smaller dimension than in period t. In general, the volume of a projection can be arbitrarily greater than then volume of the original set. We will use, however, the fact that the Kt is \u201clarge\u201d along every direction of Lt to argue that adding a vector to St can blow up the potential by at most a factor of O(d\n2/\u03b4). While this is a non-trivial volume increase, this can happen at most d times, leading to a volume increase by a factor of at most O(d2/\u03b4)d. We can use this fact to obtain that the algorithm will take at most O(d log(d/\u03b4)) steps before Lt becomes zero-dimensional.\nAn inquisitive reader might wonder if we truly need cylindrification to obtain near-optimal regret. We could consider an algorithm that simply chooses xt = u \u22a4 t zt at each iteration, where zt is the centroid of Kt. We show in Theorem 8.3 that such an algorithm incurs regret of \u2126(d 2 log(1/\u01eb \u221a d)). Without cylindrification, nature might select directions such that most of the volume reduction corresponds to widths in directions along which the set is already small. Cutting at the centroid of the cylindrified set, instead of the centroid of the original set, is thus crucial to ensure we make progress in the large directions.\nThe Projected Volume algorithm as discussed above does not actually run in polynomial time since computing the centroid of a convex set is a #P-hard problem. Fortunately, we can turn Projected Volume into a polynomial time algorithm with a few tweaks, as we show in Theorem 9.4. The key step is to approximate the value of the centroid instead of relying on an exact computation. The polynomial time version of Projected Volume presented in Section 9 also contains a technique for efficiently finding small directions to add to the set St."}, {"heading": "5 Convex Geometry Tools", "text": "In this section, we begin to develop the technical machinery required by the plan outlined in the previous section. In the heart of the proof will be a statement relating the volume of a convex body and a volume of its cylindrification with respect to dimensions along which the body \u2018small\u2019. In order to obtain this result, we will require customized versions of Gru\u0308nbaum\u2019s Theorem. Let us start by revisiting the basic statement of the theorem:\nTheorem 5.1 (Gru\u0308nbaum). Let K be a convex set, and let z be its centroid. Given an arbitrary nonzero vector u, let K+ = K \u2229 {x|u\u22a4(x\u2212 z) \u2265 0}. Then,\n1 e \u00b7 vol(K) \u2264 vol(K+) \u2264\n(\n1\u2212 1 e\n)\n\u00b7 vol(K) .\nIn other words, any hyperplane through the centroid splits the convex set in two parts, each of which having a constant fraction of the original volume. See Gru\u0308nbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition. The first step in the proof of Gru\u0308nbaum\u2019s Theorem consists of applying Brunn\u2019s Theorem, which is an immediate consequence of the Brunn-Minkowski inequality:\nTheorem 5.2 (Brunn). Given a convex set K, and let g(t) be the (d \u2212 1)-dimensional volume of the section K(t) := K \u2229 {x|x\u22a4e1 = t}. Then the function r(t) := g(t)1/(d\u22121) is concave in t over its support.\nWe will rely on Brunn\u2019s Theorem to prove our customized versions of Gru\u0308nbaum\u2019s Theorem."}, {"heading": "5.1 Directional Gru\u0308nbaum Theorem", "text": "We begin by proving a theorem which characterizes how much directional widths of a convex body can change after a cut through the centroid. In some sense, this can be seen as a version of Gru\u0308nbaum\u2019s Theorem bounding widths rather than volumes.\nTheorem 5.3 (Directional Gru\u0308nbaum). If K is a convex body and z is its centroid, then for every unit vector u 6= 0, the set K+ = K \u2229 {x|u\u22a4(x\u2212 z) \u2265 0} satisfies\n1\nd+ 1 \u00b7 w(K, v) \u2264 w(K+, v) \u2264 w(K, v) ,\nfor all unit vectors v.\nThe first step will be to prove Theorem 5.3 when v is the direction of u itself. We prove this in the following lemma.\nLemma 5.4. Under the conditions of Theorem 5.3, w(K+, u) \u2265 1d+1 \u00b7 w(K,u). We defer the proof of this lemma to Appendix A.1. We are now ready to prove the Directional Gru\u0308nbaum Theorem: Proof of Theorem 5.3. By translating K we can assume without loss of generality that z = 0. Consider three cases:\n1. There exists a point x+v \u2208 K+ \u2229 argmaxx\u2208K v\u22a4x. In such case, we know by the previous lemma that\nw(K+, v) \u2265 v\u22a4(x+v \u2212 z) \u2265 1\nd+ 1 w(K, v) .\n2. The second case is where there exists a point x\u2212v \u2208 K+ \u2229 argminx\u2208K v\u22a4x. Then,\nw(K+, v) \u2265 v\u22a4(z \u2212 x\u2212v ) \u2265 1\nd+ 1 w(K, v) .\n3. In the remaining case, let x+v \u2208 argmaxx\u2208K v\u22a4x and x\u2212v \u2208 argminx\u2208K v\u22a4x be such that u\u22a4x+v < 0 and u \u22a4x\u2212v < 0. Also, let xu = argmaxx\u2208K u \u22a4x. In such a case, choose real\nnumbers \u03bb+, \u03bb\u2212 between zero and one such that:\nu\u22a4 ( xu + \u03bb +(x+v \u2212 xu) ) = 0 and u\u22a4 ( xu + \u03bb \u2212(x\u2212v \u2212 xu) ) = 0 .\nWe can bound \u03bb+ and \u03bb\u2212 as follows: 1\nd+ 1 w(K,u) \u2264 u\u22a4xu = \u03bb+ \u00b7 u\u22a4(xu \u2212 x+v ) \u2264 \u03bb+ \u00b7 w(K,u) .\nSo \u03bb+ \u2265 1d+1 . By the same argument \u03bb\u2212 \u2265 1d+1 . Now, the points, x\u0303+v = xu + \u03bb+(x+v \u2212 xu) and x\u0303\u2212v = xu + \u03bb\n\u2212(x\u2212v \u2212 xu) are in K+, since they are convex combinations of points in K and their dot product with u is non-negative. Now:\nw(K+, v) \u2265 v\u22a4(x\u0303+v \u2212 x\u0303\u2212v ) = \u03bb+v\u22a4(x+v \u2212 xu) + \u03bb\u2212v\u22a4(xu \u2212 x\u2212v ) \u2265 v\u22a4(x+v \u2212 x\u2212v )\nd+ 1 =\nw(K, v)\nd+ 1 ."}, {"heading": "5.2 Approximate Gru\u0308nbaum Theorem", "text": "We will use the Directional Gru\u0308nbaum Theorem to give an approximate version of the standard volumetric Gru\u0308nbaum Theorem. Essentially, we will argue that if we cut through a point sufficiently close to the centroid, then either side of the cut will still contains a constant fraction of the volume.\nLemma 5.5 (Approximate Gru\u0308nbaum). Let K be a convex body, and let z be its centroid. For an arbitrary unit vector u, and scalar \u03b4 such that 0 \u2264 \u03b4 \u2264 w(K,u)/(d + 1)2, let K\u03b4+ = {x \u2208 K|u\u22a4(x\u2212 z) \u2265 \u03b4}. Then,\nvol(K\u03b4+) \u2265 1\ne2 \u00b7 vol(K) .\nThe proof of this lemma follows from a modification of the original proof for Gru\u0308nbaum\u2019s theorem, and it can be found in Appendix A.2."}, {"heading": "6 Cylindrification", "text": "Next we study how to relate the volume of a convex body to the volume of its projection onto a subspace.\nLemma 6.1 (Cylindrification). Let K \u2282 Rd be a convex body such that w(K,u) \u2265 \u03b4 for every unit vector u, then for every (d\u2212 1) dimensional subspace L:\nvol(\u03a0LK) \u2264 d(d+ 1)\n\u03b4 \u00b7 vol(K) .\nAs one of the ingredients of the proof, we will use John\u2019s Theorem:\nTheorem 6.2 (John). If K \u2282 Rd is a bounded convex body, then there is a point z and an ellipsoid E centered at the origin such that:\nz + 1\nd E \u2286 K \u2286 z + E .\nIn particular, we will use the following consequence of John\u2019s Theorem:\nLemma 6.3. If K \u2282 Rd is a convex body such that w(K,u) \u2265 \u03b4 for every unit vector u, then K contains a ball of diameter \u03b4/d.\nProof. Applying John\u2019s theorem and translating K if necessary so that z = 0, there exists an ellipsoid E such that 1dE \u2286 K \u2286 E. Since the width of K in each direction is at least \u03b4, the width of E must be at least \u03b4 in each direction. Since E is an ellipsoid, it must contain a ball of diameter \u03b4. Thus, 1dE contains a ball of diameter \u03b4 d . Hence, K also contains such a ball.\nWe now prove our cylindrification lemma. Proof of Lemma 6.1. Our proof proceeds in two steps:\nStep 1: Squashing K. Assume without loss of generality that the (d\u22121)-dimensional subspace L is the space defined by the d\u22121 first coordinates. Represent by xL the projection of each x onto the d \u2212 1 first components and define f : Rd\u22121 \u2192 R such that f(xL) is the length of the segment in the intersection of K and the line {(xL, y) : y \u2208 R} (see the top of Figure 3). Formally:\nf(xL) =\n\u222b\ny\u2208R 1K(xL, y)dy .\nWe now argue that f is concave. Given xL \u2208 Rd\u22121 let ax, bx be such that (xL, ax), (xL, bx) \u2208 K and f(xL) = bx \u2212 ax. Let yL, ay, by be defined analogously. To see that f is concave, given 0 < tx, ty < 1 with tx + ty = 1 we have: (txxL + tyyL, txax + tyay) and (txxL + tyyL, txbx + tyby) are in K by convexity, so: f(txxL+ tyyL) \u2264 (txbx+ tyby)\u2212 (txax+ tyay) = txf(xL)+ tyf(yL) , which allows us to define the squashed version of K (depicted in the bottom of Figure 3) as: K \u2032 = {(xL, h) : xL \u2208 \u03a0LK, 0 \u2264 h \u2264 f(xL)} . By construction, vol(K \u2032) = vol(K).\nStep 2: Conification. We know by Lemma 6.3 that K contains a ball of diameter \u03b4/d so there exists xL such that f(xL) \u2265 h := \u03b4/d. Define then the cone C to be the convex hull of {(xL, 0) : xL \u2208 \u03a0LK} and (xL, h). Such cone is a subset of K \u2032, so vol(K) = vol(K \u2032) \u2265 vol(C). Since the volume of a d-dimensional cone is given by the volume of the base times the height divided by d+ 1,\nvol(K) \u2265 vol(C) = h d+ 1 \u00b7 vol(\u03a0LK) = \u03b4 d(d + 1) \u00b7 vol(\u03a0LK) ."}, {"heading": "7 Analysis of the Projected Volume Algorithm", "text": "We are now almost ready to analyze our algorithm. To do so, we first consider a version of Gru\u0308nbaum\u2019s Theorem which concerns cuts through the centroid of a cylindrified set. The set being cut is still the original set, but we focus on what happens to the volume of its projection onto the subspace of large directions. The proof of this lemma can be found in Appendix A.3.\nLemma 7.1 (Projected Gru\u0308nbaum). Let K be a convex set contained in the ball of radius 1, and let S be a set of orthonormal vectors along which w(K, s) \u2264 \u03b4 \u2264 \u01eb2 16d(d+1)2\n, for all s \u2208 S. Let L be the subspace orthogonal to S, and let \u03a0L be the projection operator onto that subspace. If u is a direction along which w(Cyl(K,S), u) \u2265 \u01eb, z is the centroid of the cylindrified body Cyl(K,S), and K+ = {x \u2208 K : u\u22a4(x\u2212 z) \u2265 0}, then:\nvol(\u03a0LK+) \u2264 ( 1\u2212 1 e2 ) \u00b7 vol(\u03a0LK) ,\nwhere vol(\u00b7) corresponds to the (n\u2212 |S|)-dimensional volume on the subspace L. We now employ the tools we have developed to analyze the regret of the Projected Volume algorithm. As outlined in Section 4, we will keep in each iteration a convex set Kt of candidate \u03b8 vectors and we will keep a orthonormal basis St of directions for which Kt is small. If Lt is the subspace of directions that are orthogonal to St then our plan is to bound the potential \u03a6t = vol(\u03a0LtKt). Notice that if Lt is empty, then St must be an orthonormal basis such that w(Kt, s) \u2264 \u03b4,\u2200s \u2208 St. In particular, for every unit vector u and any two x, y \u2208 Kt we must have:\nu\u22a4(x\u2212 y) = \u2211\ns\u2208St\nu\u22a4s \u00b7 s\u22a4(x\u2212 y) \u2264 d\u03b4 .\nIf \u03b4 \u2264 \u01eb/d, then the algorithm will be done once Lt becomes empty. Our goal then is to bound how many iterations can we have where Lt is non-empty. First we provide a lower bound on the potential. We will use in this section the symbol \u03b3d to denote the volume of the d-dimensional unit ball. The following loose bound on \u03b3d will be sufficient for our needs: \u2126(d\n\u2212d) \u2264 \u03b3d \u2264 O(1). Lemma 7.2. If Lt is non-empty then \u03a6t \u2265 \u2126( \u03b4d)2d.\nProof. Let KL = \u03a0LtKt and k be the dimension of L. Then w(KL, u) \u2265 \u03b4 for all u \u2208 L implies by Lemma 6.3 that KL contains a ball of radius \u03b4 k , so vol(KL) \u2265 \u03b3k ( \u03b4 k )k . Since ( \u03b4 k )k \u2265 ( \u03b4 d )d and \u03b3k \u2265 \u2126(1d)d we have that \u03a6t \u2265 \u2126( \u03b4d)2d.\nNow we will give an upper bound on \u03a6t as a function of t. Together with the previous lower bound, we will get a bound on the number of iterations that can happen before Lt becomes empty. The main ingredient will be a Gru\u0308nbaum-type bound on the volume of the projection that is specifically tailored to our application. For this purpose, we use Lemma 7.1, which will specifically address the issue discussed in Figure 2. We are now ready for the proof of our main theorem: Proof of Theorem 4.3. Our goal is to bound the number of steps for which the algorithm guesses with at least \u01eb error. Let Rt be the total regret after t steps. Let Nt be 1 if w(Cyl(Kt, Lt), ut) > \u01eb and zero otherwise. Since \u2223\n\u2223u\u22a4t (zt \u2212 \u03b8) \u2223 \u2223 \u2264 \u01eb whenever w(Cyl(Kt, Lt), ut) \u2264 \u01eb, Rt \u2264 \u2211t \u03c4=1 N\u03c4 .\nLet Kt and Lt be the respective set and subspace after t iterations. Setting \u03b4 \u2264 \u01eb 2\n16d(d+1)2 we\ncan apply Lemma 7.1 directly to obtain that:\nvol(\u03a0LtKt+1) \u2264 ( 1\u2212 1 e2 )Nt vol(\u03a0LtKt) .\nIf Lt+1 = Lt, then vol(\u03a0Lt+1Kt+1) = vol(\u03a0LtKt+1). If we add one new direction v \u2208 Lt to S, then we replace KL = \u03a0LtKt by its projection on the subspace L \u2032 = {x \u2208 Lt : v\u22a4x = 0}. Since\nw(Kt, u) \u2265 \u03b4,\u2200u \u2208 Lt, then by Theorem 5.3 after we cut Kt we have w(Kt+1, u) \u2265 \u03b4d+1 , so applying the Cylindrification Lemma (Lemma 6.1) we obtain:\nvol(\u03a0L\u2032Kt+1) \u2264 d(d+ 1)2\n\u03b4 vol(\u03a0LtKt+1) .\nIf we need to add r new directions to Lt the volume can blow up by at most ( d(d+1)2\n\u03b4\n)r . In\nparticular, since the initial volume is bounded by O(1), then:\n\u2126\n(\n\u03b4\nd\n)2d \u2264 \u03a6t = vol(\u03a0LtKt) \u2264 O(1) \u00b7 ( d(d + 1)2\n\u03b4\n)d\n\u00b7 ( 1\u2212 1 e2 )\n\u2211 t\n\u03c4=1 N\u03c4\n,\nwhich means that:\nRt \u2264 t \u2211\n\u03c4=1\nN\u03c4 \u2264 O ( d log d\n\u03b4\n)\n= O\n(\nd log d\n\u01eb\n)\n."}, {"heading": "8 Why Cylindrification?", "text": "At the heart of our algorithm lies the simple idea that we should cut a constant fraction of the volume at each iteration if we want to achieve a O\u0303(d log(1/\u01eb)) regret bound. Our algorithm, however, is quite a bit more complex than that. It also keeps a set of \u2018small\u2019 directions St and it cuts through the center of a cylindrified version of the knowledge set Kt at each iteration. An inquisitive reader might wonder whether this additional complexity is really necessary. In this section we argue that cylindrification is actually necessary to obtain our near-optimal regret bound. We prove there exists an instance where the algorithm that only cuts through the center of the knowledge set (without cylindrifying it first) incurs \u2126(d2 log(1/\u01eb \u221a d)) regret.\nFormally, consider the algorithm that only keeps Kt and in each iteration guesses xt = u \u22a4 t zt\nwhere zt = 1\nvol(Kt)\n\u222b\nKt xdx and updates Kt to K + t or K \u2212 t . We call this procedure the Centroid\nalgorithm. In order to construct an instance with \u2126(d2 log(1/\u01eb \u221a d)) regret for this algorithm, we first define the following set. Given s = (s1, . . . , sk) with si > 0 for all i, define:\n\u2206(s) = {x \u2208 Rk+ : \u2211 i xi si \u2264 1} = conv({0, s1e1, . . . , skek}), where conv(\u00b7) denotes the convex hull of a set of points. Lemma 8.1. The centroid of \u2206(s) is given by sk+1 .\nWe now consider how the Centroid algorithm performs on a particular set, when nature selects a specific sequence of directions. The set we start from is the product between a (d\u2212k)-dimensional hypercube and a k-dimensional set \u2206(s), where only the kth entry of s is significantly larger than \u01eb. We now argue that it nature might require us to take \u2126(k log(1/\u01eb)) into a similarly structured set with k replaced by k+1. Repeating this argument d times will lead to our negative conclusion on the performance of the Centroid algorithm.\nLemma 8.2. Let 1 \u2264 k < d, s \u2208 Rk with 0 \u2264 si \u2264 \u01eb for i < k, 14 \u2264 sk \u2264 1. If K = \u2206(y)\u00d7 [0, 1]d\u2212k\nthen there is a sequence of \u2126(k log(1/\u01eb)) directions ut such that the Centroid algorithm incurs \u2126(k log(1\u01eb )) regret and by the end of the sequence, the knowledge set has the form: K \u2032 = \u2206(s\u2032)\u00d7 [0, 1]d\u2212k\u22121 where s\u2032 \u2208 Rk+1, 0 \u2264 s\u2032i \u2264 \u01eb for i < k + 1, 14 \u2264 s\u2032i \u2264 1.\nProof sketch. Starting from K, as a first step we select \u2126(k log(1\u01eb )) vectors in the direction ek which cause the side of the k-th side of the simplex \u2206(s) to reduce to \u01eb getting one unit of regret in each step. At the end of this step we are left with the situation illustrated in Figure 8.2. In step 2 we choose a direction slightly bent towards ek+1 to carve a (k+1)-dimensional simplex out of K. The resulting shape will be, as depicted in Figure 8.2, only partially what we want. In the third step we select more directions along ek+1 to remove the \u2018leftover\u2019 and keep only the part corresponding to the (k + 1)-dimensional simplex. A complete proof is provided in Appendix A.5.\nThis is the main ingredient necessary to show that the algorithm without cylindrification can incur \u2126(d2 log(1/\u01eb \u221a d)) regret.\nTheorem 8.3. The algorithm that always chooses xt = u \u22a4 t zt where zt is the centroid of Kt can\nincur \u2126(d2 log(1/\u01eb \u221a d)) regret.\nProof. Start with the set K0 = [0, 1] d. Apply Lemma 8.2 for k = 1, 2, 3, . . . , d \u2212 1. The total regret is \u2211d\u22121\nk=1\u2126(k log( 1 \u01eb )) = \u2126(d 2 log(1\u01eb )). To construct a valid instance (one that fits within a\nball of radius 1), we replace our initial set with K0 = [ 0, 1/ \u221a d ]d , leading to an aggregate regret of \u2126(d2 log(1/\u01eb \u221a d)). We did not do our computations above using this scaled down instance instead\nof [0, 1]d in order to avoid carrying extra \u221a d terms."}, {"heading": "9 Computation", "text": "The Projected Volume algorithm described earlier, while yielding optimal regret with respect to the number of dimensions d, can\u2019t be implemented as presented in polynomial time. The reason is that it requires implementing two steps, both of which involve solving highly nontrivial problems. The first is computing the centroid, which is known to be #P-hard [13]. The second is finding a direction along which a convex bodyK is \u201cthin\u201d (i.e. finding a unit vector u such that w(K,u) \u2264 \u03b4), for which we are not aware of a polynomial time algorithm.\nIn order to make these problems tractable, we relax the requirements of our algorithm. More specifically, we will show how our algorithm is robust, in the sense that using an approximate centroid, and finding approximately thin directions does not break the analysis.\nIn the following subsections, we show how to implement both of these steps. Then, we put them together into a polynomial time version of our algorithm."}, {"heading": "9.1 Approximating the Centroid", "text": "An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]). A\nsimilar application can be found in Bertsimas and Vempala [4], where the authors use approximate centroid computation in order to solve linear programs.\nOur application faces the same issues as in [4]. Namely, in order to efficiently sample from a convex body, one requires that the body is nearly isotropic. Although the body we start with is isotropic, after cutting or projecting this property is lost. Therefore we require maintaining a linear transformation under which the body ends up being in isotropic position. The many issues encountered when approximating the centroid are carefully handled in [4], so we will restate the following result which is implicit there (see Lemma 5 and Theorem 12):\nTheorem 9.1 ([4]). Given a d-dimensional convex body K, one can compute an approximation z\u2032 to the centroid z of K in the sense that \u2016z \u2212 z\u2032\u2016 \u2264 \u03c1 in O\u0303(d4/\u03c1) steps of a random walk in K. Note that for hit-and-run sampling, one only requires determining the intersection between a line and the given convex body; in our case this only requires one iteration through the inequality constraints determining the body."}, {"heading": "9.2 Finding Approximately Thin Directions", "text": "Instead of exactly recovering directions u satisfying w(K,u) \u2264 \u03b4, we instead recover all the directions along which w(K,u) \u2264 \u03b4\u03b1 , and potentially some along which \u03b4\u03b1 \u2264 w(K,u) \u2264 \u03b4. We do this by computing an ellipsoidal approximation of K. Indeed, having access to an ellipsoid E such that E \u2286 K \u2286 \u03b1E , we can:\n1. find a direction u such that w(K,u) \u2264 \u03b4, by checking whether E has a direction u such that w(E, u) \u2264 \u03b4/\u03b1, or\n2. decide that w(K,u) \u2265 \u03b4/\u03b1 for all u simply by showing that the smallest directional width of E is greater than or equal to \u03b4/\u03b1.\nThis task can be performed simply by inspecting the eigenvalues of E. A natural notion for such an ellipsoid is the John ellipsoid. However, computing it is NP-hard. Instead, by relaxing the approximation factor, a polynomial time algorithm can be obtained. Such a result is provided in Gro\u0308tschel et al [6], which we reproduce below for completeness (see Corollary 4.6.9).\nTheorem 9.2 ([6]). Given a convex body K containing a ball of radius r, and contained inside a ball of radius R, along with access to a separation oracle for K, one can compute an ellipsoid E such that E \u2286 K \u2286 \u221a d(d+ 1)E using dO(1) \u00b7 log(R/r) oracle calls.\nThis immediately yields the following Corollary, which we will use in our algorithmic result.\nCorollary 9.3. Given a convex body K containing a ball of radius r, and contained inside a ball of radius R, along with a separation oracle for K, one can either find a direction u such that w(K,u) \u2264 \u03b4, or certify that w(K,u) \u2265 \u03b4/( \u221a d(d+ 1)) for all u using dO(1) \u00b7 log(R/r) oracle calls."}, {"heading": "9.3 Obtaining a Polynomial Time Algorithm", "text": "The polynomial time version of our algorithm is very similar to the initial one. The differences that make computation tractable are:\n1. Instead of computing the centroid exactly, we compute the centroid to within distance \u03c1 = (\u01eb/d)O(1), via Theorem 9.1.\n2. Every iteration of the algorithm the set St is updated by repeatedly computing the ellipsoidal approximation described in Corollary 9.3, and adding the direction u corresponding to the smallest eigenvalue of the ellipsoid, if it certifies that w(K,u) \u2264 \u03b4. When no such direction is found, we know that w(K,u) \u2265 \u03b4approx := \u03b4/ \u221a d(d+ 1) for all u.\nA complete description of the new algorithm, along with its analysis, can be found in Appendix B. Combining the results in this section, we obtain the following theorem:\nTheorem 9.4. There exists an algorithm that runs in time (d/\u01eb)O(1) achieving regret O(d log(d/\u01eb)) for the multi-dimensional binary search problem."}, {"heading": "A Deferred Proofs", "text": "A.1 Proof of Lemma 5.4\nSince width is invariant under rotations and translations we can assume, without loss of generality, that u = \u2212e1. Also, since scaling the convex set along the direction of u also scales the corresponding coordinate of the centroid by the same factor, we can assume that the projection of K onto the e1 axis is [0, 1]. Using the notation from Theorem 5.2, we can write the first coordinate of the centroid z as\nz\u22a4e1 = 1\nvol(K)\n\u222b\nK x\u22a4e1 dx =\n\u222b\nK x \u22a4e1 dx\n\u222b K 1 dx = \u222b 1 0 t \u00b7 r(t)d\u22121 dt \u222b 1 0 r(t) d\u22121 dt .\nOur goal is to show that z\u22a4e1 \u2265 1d+1 . We will do it in a sequence of two steps. To simplify notation, let us define V := vol(K).\nStep 1: linearize r. We prove that the linear function r\u0303 : [0, 1] \u2192 R given by\nr\u0303(t) = (V d)1/(d\u22121) \u00b7 (1\u2212 t)\nsatisfies\n\u222b 1\n0 t \u00b7 r\u0303(t)d\u22121dt \u2264\n\u222b 1\n0 t \u00b7 r(t)d\u22121dt and\n\u222b 1\n0 r\u0303(t)d\u22121dt =\n\u222b 1\n0 r(t)d\u22121dt = V .\nWe immediately see that the second condition is satisfied, simply by evaluating the integral. Next we show that r\u0303 satisfies the first condition.\nSince by definition, r is supported everywhere over [0, 1], it means that r(1) \u2265 r\u0303(1) = 0, and therefore r(0) \u2264 r\u0303(0) (since otherwise, by concavity, it would be the case that r(t) \u2265 r\u0303(t) everywhere, and the second identity could not possibly hold). Again, using the concavity of r, this implies that there exists a point p \u2208 [0, 1] such that r(t) \u2264 r\u0303(t) for all t \u2208 [0, p], and r(t) \u2265 r\u0303(t) for all t \u2208 [p, 1].\nHence, we can write\n\u222b 1\n0 t \u00b7\n( r(t)d\u22121 \u2212 r\u0303(t)d\u22121 ) dt =\n\u222b p\n0 t \u00b7\n( r(t)d\u22121 \u2212 r\u0303(t)d\u22121 ) dt+\n\u222b 1\np t \u00b7\n( r(t)d\u22121 \u2212 r\u0303(t)d\u22121 ) dt ,\nwhere all the coefficients of t from the first term are nonpositive, and all the coefficients of t from the second term are nonnegative. Therefore we can lower bound this integral by\n\u222b p\n0 p\u00b7 ( r(t)d\u22121 \u2212 r\u0303(t)d\u22121 ) dt+\n\u222b 1\np p\u00b7 ( r(t)d\u22121 \u2212 r\u0303(t)d\u22121 )\ndt = p\u00b7 ( \u222b 1\n0 r(t)d\u22121dt\u2212\n\u222b 1\n0 r\u0303(t)d\u22121dt\n)\n= 0 ,\nwhich proves that the first condition also holds. Step 2: solve for the linear function. We can explicitly compute\n\u222b 1\n0 t \u00b7 r\u0303(t)d\u22121dt = V d \u00b7\n\u222b 1\n0 t \u00b7 (1\u2212 t)d\u22121dt = V d \u00b7 1 d(d+ 1) = V d+ 1 .\nTherefore, combining the results from the two steps, we see that\n1\nd+ 1 = \u222b 1 0 t \u00b7 r\u0303(t)d\u22121dt \u222b 1 0 r\u0303(t) d\u22121dt \u2264 \u222b 1 0 t \u00b7 r(t)d\u22121dt \u222b 1 0 r(t) d\u22121dt = z\u22a4e1 ,\nwhich yields the desired conclusion.\nA.2 Proof of Lemma 5.5\nSince our problem is invariant under rotations and translations, let us assume that u = e1, and z = 0. Furthermore, notice that our problem is invariant to scaling K along the direction of u. Therefore we can assume without loss of generality that [a, 1] is the projection of K onto the e1 axis. Then, in the notation of Lemma 5.2, we have:\nvol(K+) =\n\u222b 1\n0 r(t)d\u22121dt , vol(K\u03b4+) =\n\u222b 1\n\u03b4 r(t)d\u22121dt .\nFrom Theorem 5.1, we know that vol(K+) \u2265 vol(K)/e. We will show that vol(K\u03b4+) \u2265 vol(K+)/e, which yields the sought conclusion.\nFrom Theorem 5.3 we know that w(K,u)/(d+1) \u2264 1. Hence, using our bound on \u03b4, we obtain \u03b4 \u2264 1/(d + 1). We are left to prove, using the fact that r is a nonnegative concave function, that:\n\u222b 1\n1/(d+1) r(t)d\u22121dt \u2265 1 e \u00b7 \u222b 1 0 r(t)d\u22121dt .\nTo see that this is true, it is enough to argue that the ratio between the two integrals is minimized when r is a linear function r(t) = c \u00b7(t\u22121), for any constant c; in that case, an explicit computation of the integrals produces the desired bound.\nTo see that the ratio is minimized by a linear function, we proceed in two steps. First, consider the function r\u0303 obtained from r by replacing in on the [1/(d + 1), 1] interval with a linear function starting at r(1/(d + 1)) and ending at 0:\nr\u0303(t) =\n\n\n\nr(t), if t \u2208 [\n0, 1d+1\n]\n,\nr (\n1 d+1\n) \u00b7 dd+1 \u00b7 (t\u2212 1), if t \u2208 [ 1 d+1 , 1 ] .\nNotice that this function is still concave, and its corresponding ratio of integrals can not be greater than the one for r (since the same value gets subtracted from both integrals when switching from r to r\u0303).\nNext, consider the function\nr\u0302(t) = r\n(\n1\nd+ 1\n)\n\u00b7 d d+ 1 \u00b7 (t\u2212 1), t \u2208 [0, 1] .\nSince r\u0303 is concave, it is upper bounded by r\u0302 everywhere on [0, 1/(d + 1)]. Therefore, the ratio of integrals corresponding to r\u0302 can only decrease, compared to the one for r\u0303.\nFinally, the result follows from evaluating the integrals for r(t) = t\u2212 1.\nA.3 Proof of Lemma 7.1\nSince the problem is invariant under rotations and translations, we can assume without loss of generality that z = 0, S = {e1, . . . , ek} and L = span{ek+1, . . . , en}. For every vector x we will consider the projections of x onto the two corresponding subspaces, xS = (x1, . . . , xk) and xL = (xk+1, . . . , xn). For simplicity, will also use the notation KL := \u03a0LK.\nThe proof consists of four steps. Step 1: the direction u has a large component in L. Since w(Cyl(K,S), u) \u2265 \u01eb, and z = 0 is\nthe centroid of the cylinder, there must exist y \u2208 Cyl(K,S) such that \u2223 \u2223u\u22a4y \u2223 \u2223 = \u2223 \u2223u\u22a4(y \u2212 z) \u2223 \u2223 \u2265 \u01eb2 . Therefore \u2223\n\u2223u\u22a4S yS \u2223 \u2223+ \u2223 \u2223u\u22a4LyL \u2223 \u2223 \u2265 \u01eb2 . Since the width of Cyl(K,S) is at most \u03b4 along all small directions, we have \u2016yS\u2016\u221e \u2264 \u03b4. Therefore, by Cauchy-Schwarz,\n\u2016uL\u2016 \u2016yL\u2016 \u2265 \u2223 \u2223 \u2223 u\u22a4LyL \u2223 \u2223 \u2223 \u2265 \u01eb\n2 \u2212 k\u03b4 .\nNow, remember that since y \u2208 Cyl(K,S), K is contained inside the unit ball, and all the small directions have length at most \u03b4, it must be that \u2016y\u2016 \u2264 1 + k\u03b4. Since this implies the same upper bound on \u2016yL\u2016, combining with the bound above we see that\n\u2016uL\u2016 \u2265 \u01eb/2\u2212 k\u03b4 1 + k\u03b4 \u2265 \u01eb/2\u2212 \u01eb 2/(16(d + 1)) 1 + \u01eb2/(16(d + 1)) \u2265 \u01eb 4 .\nStep 2: lower bound the width of KL along the direction of uL. Let u\u0302L = uL/ \u2016uL\u2016 be the unit vector in the direction uL. We know by the last step that\nw(KL, uL) \u2265 \u2223 \u2223 \u2223 u\u0302\u22a4LyL \u2223 \u2223 \u2223 \u2265 \u2223 \u2223 \u2223 u\u22a4LyL \u2223 \u2223 \u2223 \u2265 \u01eb 2 \u2212 k\u03b4 \u2265 \u01eb 4 .\nStep 3: show that for all x \u2208 K+, one has u\u0302\u22a4LxL \u2265 \u2212\u01eb/(4(d + 1)2). If x \u2208 K+, then u\u22a4LxL + u\u22a4S xS \u2265 0. Since \u2016xS\u2016\u221e \u2264 \u03b4, we have u\u22a4LxL \u2265 \u2212k\u03b4. Hence\nu\u0302\u22a4LxL \u2265 \u2212 k\u03b4 \u2016uL\u2016 \u2265 \u22124d\u03b4 \u01eb \u2265 \u2212 \u01eb 4(d+ 1)2 ,\nwhere we used the fact that \u03b4 \u2264 \u01eb2/(16d(d + 1)2). Step 4: upper bound the volume of \u03a0LK+. From the previous step, we know that if x \u2208 \u03a0L(K+), then xL \u2208 {xL \u2208 KL|u\u0302\u22a4LxL \u2265 \u2212\u01eb/(4(d + 1))}. Therefore:\nvol(\u03a0LK+) \u2264 vol(KL)\u2212 vol ({ xL \u2208 KL \u2223 \u2223 \u2223\n\u2223\n(\u2212u\u0302L)\u22a4xL \u2265 \u01eb\n4(d+ 1)2\n}) \u2264 vol(KL) \u00b7 ( 1\u2212 1 e2 ) ,\nwhere the first inequality follows from the previous step, since\n\u03a0LK+ \u2286 { xL \u2208 KL \u2223 \u2223 \u2223\n\u2223\n(\u2212u\u0302L)\u22a4xL \u2264 \u01eb\n4(d+ 1)2\n}\n.\nThe second inequality follows from Lemma 5.5, since in Step 2 we showed that in this proof, we meet the conditions of that lemma. We note that it is very important that z is the centroid of Cyl(K,S) and not the centroid of K, since the application of Lemma 5.5 relies on the fact the projection of z onto the subspace L is the centroid of KL.\nA.4 Proof of Lemma 8.1\nLet zi = 1\nvol(\u2206(s))\n\u222b\n\u2206(s) xidx be the i-th component of the centroid of \u2206(s). So if s\u2212i is the vector\nin Rk\u22121 obtained from s by removing the i-th component, then the intersection of \u2206(s) with the hyperplane xi = a can be written as: {x| xi = a, x\u2212i \u2208 \u2206( s\u2212i1\u2212a/si )}. Therefore, we can write the integral defining zi as:\nzi = 1\nvol(\u2206(s))\n\u222b si\n0 xivol\n(\n\u2206\n(\ns\u2212i 1\u2212 xisi\n))\ndxi = 1\nvol(\u2206(s))\n\u222b si\n0 xivol (\u2206 (s\u2212i)) \u00b7\n(\n1\u2212 xi si\n)k\u22121\ndxi\nsince scaling each coordinate a constant factor scales the volume by this constant powered to the number of dimensions. Solving this integral, we get:\nzi = vol(\u2206(s\u2212i)) vol(\u2206(s)) \u00b7 s\n2 i\nk(k + 1) .\nWe can apply the same trick to compute the volume:\nvol(\u2206(S)) =\n\u222b si\n0 vol (\u2206 (s\u2212i)) \u00b7\n(\n1\u2212 xi si\n)k\u22121\ndxi = vol(\u2206(s\u2212i)) \u00b7 si k .\nSubstituting the volume vol(\u2206(S)) in zi we get zi = si\nk+1 .\nA.5 Proof of Lemma 8.2\nWe break the sequence of directions chosen by nature in three parts. We will show that the first part alone has regret O(k log(1\u01eb )) and the other two parts will be used to bring the knowledge set to the desired format. We won\u2019t specify the exact value of \u03b8. We only assume that \u03b8 is an arbitrary point in the final knowledge set produced.\nStep 1 : Nature picks \u2126(k log(1\u01eb )) vectors in the direction ek, choosing the K+ side. The knowledge set is initially \u2206(s)\u00d7[0, 1]d\u2212k with centroid at (\ns k+1 , 1 2 , . . . , 1 2\n)\n. The set obtained\nby cutting through this point using a hyperplane orthogonal to ek can be described as {\nx \u2208 Rd : xk \u2265 sk\nk + 1 ,\nk \u2211\ni=1\nxi si\n\u2264 1, 0 \u2264 xi \u2264 1 } ,\nwhich is, up to translation, equal to the set \u2206((1 \u2212 1k+1)s) \u00d7 [0, 1]d\u2212k . By applying such cuts \u2126(k log(1\u01eb )) we are left with a set \u2206(s\u0302)\u00d7 [0, 1]d\u2212k where 0 \u2264 s\u0302 \u2264 \u01eb. Since we assumed that \u03b8 is in the last knowledge set while sk \u2265 2\u01eb kk+1 we must be incurring one unit of regret, so we must have incurred at least \u2126(k log(1\u01eb )) regret.\nStep 2 : Nature picks a single vector in the direction v = (\nk+1 2k \u00b7 1s\u03021 , . . . , k+1 2k \u00b7 1s\u0302k , 1, 0, . . . , 0\n)\n,\nchoosing the K\u2212 side. Since the centroid is z = ( s\u0302k+1 , 1 2 , . . . , 1 2) the half-space defining K\u2212 is given by: v\n\u22a4x \u2264 v\u22a4z = 1, therefore K\u2212 is described by:\nK\u2212 =\n{\nx \u2208 Rd : k \u2211\ni=1\nxi s\u0302i k + 1 2k + xk+1 \u2264 1,\nk \u2211\ni=1\nxi s\u0302i\n\u2264 1, 0 \u2264 xi \u2264 1 }\nTo understand the shape of K\u2212 it is useful to decompose it in two parts based on the value of xk+1. Let y = 1\u2212 12 k+1k which is a quantity between 0 and 12 .\n\u2022 for x \u2208 K\u2212 with xk+1 \u2265 y the constraint \u2211k i=1 xi s\u0302i \u2264 1 is implied by \u2211ki=1 xis\u0302i k+1 2k + xk+1 \u2264 1,\nsince we can re-write the second constraint as: \u2211k i=1 xi s\u0302i (1\u2212y) \u2264 1\u2212xk+1 \u2264 1\u2212y. This means in particular that {x \u2208 K\u2212 : xk+1 \u2265 y} is equal, up to translation to \u2206(s\u0302, 1\u2212y)\u00d7 [0, 1]d\u2212k\u22121.\n\u2022 For x \u2208 K\u2212 with xk+1 \u2264 y then the constraint \u2211k i=1 xi s\u0302i k+1 2k +xk+1 \u2264 1 is implied by \u2211k i=1 xi s\u0302i \u2264 1 since\n\u2211k i=1 xi s\u0302i (1\u2212y) \u2264 1\u2212y \u2264 1\u2212xk+1. In particular, this means that {x \u2208 K\u2212 : xk+1 \u2264 y}\nis the set \u2206(s\u0302)\u00d7 [0, y] \u00d7 [0, 1]d\u2212k\u22121. Step 3 : Nature picks r vectors in direction ek+1 choosing the K+ side, where r will be decided later. After Step 2, the set is a concatentation of \u2206(s\u0302)\u00d7 [0, y]\u00d7 [0, 1]d\u2212k\u22121 and \u2206(s\u0302, 1\u2212y)\u00d7 [0, 1]d\u2212k\u22121 as displayed in Figure 4. By cutting in the ek+1 direction, we will eventually be left only with the \u2206(s\u0302, 1 \u2212 y) \u00d7 [0, 1]d\u2212k\u22121 part of the set. Pick r to be the minimum value such that this happens. Since the volume of the sections along the xk+1 dimension are non-increasing, the set after the cut must keep at least half of the width along ek+1. Therefore, after r cuts, we must be left with \u2206(s\u2032)\u00d7 [0, 1]d\u2212k\u22121 where s\u2032 \u2208 Rk+1 and 14 \u2264 1\u2212y 2 \u2264 s\u2032k+1 \u2264 1\u2212 y."}, {"heading": "B Polynomial Time Algorithm", "text": "Correctness. Correctness of this algorithm follows from a simple modification of our original analysis. In order to tolerate the fact that the centroid produced by the sampling scheme is only\napproximate, we need to resort to the Approximate Gru\u0308nbaum Theorem (see Lemma 5.5) in order to track the decrease in volume, and also to an approximate version of the Directional Gru\u0308nbaum Theorem (see Lemma B.1 below), in order to argue that directional widths still do not decrease faster than they are supposed to.\nLemma B.1 (Approximate Directional Gru\u0308nbaum). Let K be a convex body with centroid z. Let z\u2032 be an approximate centroid in the sense that \u2016z \u2212 z\u2032\u2016 \u2264 \u03c1. Then for every vector u 6= 0, the set K+ = {x|u\u22a4(x\u2212 z\u2032) \u2265 0} satisfies\n1\nd+ 1 \u00b7 w(K, v) \u2212 \u03c1 \u00b7max\n(\n1, w(K, v)\nw(K,u)\n)\n\u2264 w(K+, v) \u2264 w(K, v)\nfor any unit vector v.\nProof sketch. The analysis follows from minor modifications in the analysis of Theorem 5.3. First we modify Lemma 5.4 in order to show that\n1\nd+ 1 w(K,u) \u2212 \u03c1 \u2264 w(K+, u) \u2264 w(K,u) .\nIndeed, since \u2016z \u2212 z\u2032\u2016 \u2264 \u03c1, taking a cut perpendicular to u that passes through z\u2032 instead of z changes the directional width along u by at most \u03c1. Therefore the bound above holds in the worst case. Second, we consider the three cases considered in the proof. In the first two cases, we have w(K+, v) \u2265 1d+1 \u00b7 w(K, v) \u2212 \u03c1 via the previous bound. In the third case, let \u03bb+ and \u03bb\u2212 defined similarly. Then we have 1d+1w(K,u) \u2212 \u03c1 \u2264 \u03bb+ \u00b7 w(K,u) and similarly for \u03bb\u2212. Therefore min{\u03bb+, \u03bb\u2212} \u2265 1d+1 \u2212 \u03c1 w(K,u) . Finally, this yields\nw(K+, v) \u2265 w(K, v) \u00b7 ( 1 d+ 1 \u2212 \u03c1 w(K,u) ) ,\nand our conclusion follows.\nSimilarly, we require a robust version of projected Gru\u0308nbaum, which we sketch below.\nLemma B.2 (Approximate Projected Gru\u0308nbaum). Let K be a convex set contained in the ball of radius 1, and let S be a set of orthonormal vectors along which w(K, s) \u2264 \u03b4 \u2264 \u01eb2 32d(d+1)2 , for all s \u2208 S. Let L be the subspace orthogonal to S, and let \u03a0L be the projection operator onto that subspace. If u is a direction along which w(Cyl(K,S), u) \u2265 \u01eb, z is the centroid of the cylindrified body Cyl(K,S), z\u2032 satisfies \u2016z \u2212 z\u2032\u2016 \u2264 \u03c1 := \u01eb\n8(d+1)2 , and K+ = {x \u2208 K : u\u22a4(x\u2212 z\u2032) \u2265 0}, then:\nvol(\u03a0LK+) \u2264 ( 1\u2212 1 e2 ) \u00b7 vol(\u03a0LK) ,\nwhere vol(\u00b7) corresponds to the (n\u2212 |S|)-dimensional volume on the subspace L.\nProof sketch. The proof follows the same steps as the proof of Lemma 7.1. Below we sketch the essential differences, and show how they affect the analysis.\nThe first two steps are identical, since they do not involve the perturbed centroid z\u2032. For the third step, we proceed identically to show that\nu\u0302\u22a4LxL \u2265 \u2212 4d\u03b4 \u01eb \u2265 \u2212 \u01eb 8(d+ 1)2 ,\nwhere we used \u03b4 \u2264 \u01eb2 32d(d+1)2 .\nFinally, for the fourth step we use the fact that if x \u2208 \u03a0L(K+) then xL \u2208 {xL \u2208 KL|u\u0302\u22a4LxL \u2265 \u2212\u01eb/(8(d + 1)2)\u2212 \u03c1}. Hence\n\u03a0LK+ \u2286 { xL \u2208 KL \u2223 \u2223 \u2223\n\u2223\n(\u2212u\u0302L)\u22a4xL \u2264 \u01eb\n4(d+ 1)2\n}\n.\nand thus we obtain the same bound on vol(\u03a0LK+).\nPutting everything together, we can show that the algorithm using these approximation primitives yields the same regret asymptotically. The constants we will use throughout the our algorithm will be \u03b4approx = \u03b4/( \u221a d(d + 1)) = \u01eb2/(16d1.5(d + 1)3), and \u03c1 = \u03b42approx/(2(d + 1)). The two key results required for our robust analysis are:\n1. If the cardinality of S does not increase, then\nvol(\u03a0LtKt+1) \u2264 ( 1\u2212 1/e2 ) vol(\u03a0LtKt) .\nThis is given by the approximate projected Gru\u0308nbaum theorem (Lemma B.2).\n2. When adding an extra direction to S, we know that w(Kt, u) \u2265 \u03b4approx, for all u \u2208 Lt. Then by Lemma B.1 after we cut Kt we have that for any vector v \u2208 Lt,\nw(Kt+1, v) \u2265 w(Kt, v)\nd+ 1 \u2212 \u03c1 \u00b7max\n(\n1, w(Kt, u)\nw(Kt, v)\n)\n\u2265 \u03b4approx d+ 1 \u2212 \u03c1 \u00b7 1 \u03b4approx \u2265 \u03b4approx 2(d+ 1) ,\nby our choice of \u03c1 = \u03b42approx/(2(d + 1)). So applying the Cylindrification Lemma (Lemma 6.1) we obtain that the volume of the convex body projected onto the new subspace of large directions L\u2032 is bounded by\nvol(\u03a0L\u2032Kt+1) \u2264 d(d+ 1)\n\u03b4approx/(2(d + 1)) vol(\u03a0LtKt+1) =\n32d1.5(d+ 1)3\n\u03b4 vol(\u03a0LtKt+1) .\nThis follows just like before from Lemma 6.1. Our method of finding thin directions based on the approximate John ellipsoid (Corollary 9.3) guarantees that all directional widths in the large subspace L are at least \u03b4approx. Therefore the blow up in volume is at most by a factor of (32d1.5(d+ 1)3)/\u03b4.\nSince all the new bounds are within polynomial factors from the ones used in the analysis using exact centroids, by plugging in the old analysis, we easily obtain the same regret, up to constant factors.\nRunning time. For the running time analysis, note that the centroid approximation can be implemented using O\u0303(d4/\u03c1) = (d/\u01eb)O(1) calls to the separation oracle for the convex body. Such a separation oracle needs to take into account both the linear inequalities added during each iteration, and the at most d projections. Such an oracle can be implemented by maximizing a linear functional over a set determined by the intersection between the initial unit ball and the linear constraints (whose number is bounded by the number of iterations of the algorithm O\u0303(d log(1/\u01eb)); therefore this step can be implemented in polynomial time, and therefore all the centroid approximation steps require time (d/\u01eb)O(1).\nThe routine for finding the thin directions will be called at least once every iteration, and will find a thin direction at most d times. Therefore this contributes dO(1) log(R/r) \u00b7 log(1/\u01eb) to the running\ntime, where r is a lower bound on the smallest ball contained in the body, while R is an upper bound. From the setup we have R = 1; also, since we are finished after O\u0303(d log(1/\u01eb)) iterations, and each iteration shrinks the smallest directional width by at most a factor of dO(1), according to Lemma 6.3, we have that at all times the body will contain a ball of radius d\u2212\u2126(d). Therefore the running time contribution of the routine required for finding thin directions is dO(1) log(1/\u01eb).\nAll the other steps require at most polynomial overhead, therefore the total running time is (d/\u01eb)O(1)."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E Schapire"], "venue": "In Proceedings of ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Repeated contextual auctions with strategic buyers", "author": ["Kareem Amin", "Afshin Rostamizadeh", "Umar Syed"], "venue": "In Proceedings of NIPS, pages 622\u2013630,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Online decision-making with high-dimensional covariates", "author": ["Hamsa Bastani", "Mohsen Bayati"], "venue": "Working paper, Stanford University,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Solving convex programs by random walks", "author": ["Dimitris Bertsimas", "Santosh Vempala"], "venue": "J. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Feature-based Dynamic Pricing", "author": ["Maxime C. Cohen", "Ilan Lobel", "Renato Paes Leme"], "venue": "In Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Geometric algorithms and combinatorial optimization, volume 2", "author": ["Martin Gr\u00f6tschel", "L\u00e1szl\u00f3 Lov\u00e1sz", "Alexander Schrijver"], "venue": "Springer Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Partitions of mass-distributions and of convex bodies by hyperplanes", "author": ["Branko Gr\u00fcnbaum"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "Dynamic pricing in high-dimensions", "author": ["Adel Javanmard", "Hamid Nazerzadeh"], "venue": "arXiv preprint arXiv:1609.07574,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Hit-and-run mixes fast", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": "Mathematical Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Faster mixing via average conductance", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz", "Ravi Kannan"], "venue": "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Efficient methods in convex programming", "author": ["Arkadi Nemirovski"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Dynamic pricing with demand covariates", "author": ["Sheng Qiang", "Mohsen Bayati"], "venue": "Available at SSRN 2765257,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The binary search problem consists in trying to guess an unknown real number \u03b8 \u2208 [0, 1] given access to an oracle that replies for every guess xt if xt \u2264 \u03b8 or xt > \u03b8.", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "We now mention two applications: Personalized Medicine [3]: Determining the right dosage of a drug for a given patient is a well-studied problem in the medical literature.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Bastani and Bayati [3] propose a mathematical formulation for this problem and tackle it using tools from statistical learning and contextual bandits.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.", "startOffset": 22, "endOffset": 35}, {"referenceID": 4, "context": "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.", "startOffset": 22, "endOffset": 35}, {"referenceID": 11, "context": "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.", "startOffset": 22, "endOffset": 35}, {"referenceID": 7, "context": "Feature-based Pricing [2, 5, 12, 8]: Consider a firm that sells a very large number of differentiated products.", "startOffset": 22, "endOffset": 35}, {"referenceID": 4, "context": "Nevertheless, Cohen et al [5] showed that an algorithm for the multidimensional binary search problem can be converted into an algorithm for the feature-based pricing problem in a black-box manner.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "The first approach to this problem was due to Amin, Rostamizadeh and Syed [2] in the context of the pricing problem and is based on stochastic gradient descent.", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO.", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "Subsequent approaches by Bastani and Bayati [3] and Qiang and Bayati [12] use techniques from statistical learning such as greedy least squares or LASSO.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Javanmard and Nazerzadeh [8] apply a regularized maximum likelihood estimation approach and obtain an improved regret guarantee.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "[1]) to tackle the iid version of the multidimensional binary search problem, but such an algorithm would have regret that is polynomial in 1/\u01eb instead of the logarithmic regret obtained by the specialized algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "The only approach that makes no assumptions about the directions ut is by Cohen et al [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "1, we construct a lower bound of \u03a9(d log(1/\u01eb \u221a d)) via a reduction to d one-dimensional problems, which is significantly lower than the O(d2 log(d/\u01eb)) regret bound from Cohen et al [5].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "An idea similar to this one was proposed by Bertsimas and Vempala [4], in a paper where they proposed a method for solving linear programs via an approximate Gr\u00fcnbaum theorem.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "See Gr\u00fcnbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition.", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "See Gr\u00fcnbaum [7] for the original proof of this theorem, or Nemirovski [11] for a more recent exposition.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "If K = \u2206(y)\u00d7 [0, 1] then there is a sequence of \u03a9(k log(1/\u01eb)) directions ut such that the Centroid algorithm incurs \u03a9(k log(1\u01eb )) regret and by the end of the sequence, the knowledge set has the form: K \u2032 = \u2206(s)\u00d7 [0, 1] where s \u2208 Rk+1, 0 \u2264 s\u2032i \u2264 \u01eb for i < k + 1, 1 4 \u2264 s\u2032i \u2264 1.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "If K = \u2206(y)\u00d7 [0, 1] then there is a sequence of \u03a9(k log(1/\u01eb)) directions ut such that the Centroid algorithm incurs \u03a9(k log(1\u01eb )) regret and by the end of the sequence, the knowledge set has the form: K \u2032 = \u2206(s)\u00d7 [0, 1] where s \u2208 Rk+1, 0 \u2264 s\u2032i \u2264 \u01eb for i < k + 1, 1 4 \u2264 s\u2032i \u2264 1.", "startOffset": 213, "endOffset": 219}, {"referenceID": 0, "context": "Start with the set K0 = [0, 1] d.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "We did not do our computations above using this scaled down instance instead of [0, 1]d in order to avoid carrying extra \u221a d terms.", "startOffset": 80, "endOffset": 86}, {"referenceID": 8, "context": "1 Approximating the Centroid An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]).", "startOffset": 199, "endOffset": 202}, {"referenceID": 9, "context": "1 Approximating the Centroid An approximation of the centroid sufficient for our purposes follows from a simple application of standard algorithms for sampling points from convex bodies (hit-and-run [9], ball-walk [10]).", "startOffset": 214, "endOffset": 218}, {"referenceID": 3, "context": "similar application can be found in Bertsimas and Vempala [4], where the authors use approximate centroid computation in order to solve linear programs.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "Our application faces the same issues as in [4].", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "The many issues encountered when approximating the centroid are carefully handled in [4], so we will restate the following result which is implicit there (see Lemma 5 and Theorem 12): Theorem 9.", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "1 ([4]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Such a result is provided in Gr\u00f6tschel et al [6], which we reproduce below for completeness (see Corollary 4.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "2 ([6]).", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "We consider a multidimensional search problem that is motivated by questions in contextual decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a d-dimensional unit ball and then generates a sequence of d-dimensional directions. We are given access to the directions, but not access to the state. After receiving a direction, we have to guess the value of the dot product between the state and the direction. Our goal is to minimize the number of times when our guess is more than \u01eb away from the true answer. We construct a polynomial time algorithm that we call Projected Volume achieving regret O(d log(d/\u01eb)), which is optimal up to a log d factor. The algorithm combines a volume cutting strategy with a new geometric technique that we call cylindrification.", "creator": "LaTeX with hyperref package"}}}