{"id": "1611.01080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Probabilistic Modeling of Progressive Filtering", "abstract": "Progressive filtering is a simple method of performing hierarchical classification inspired by the behavior that most people put into practice while trying to categorize an element according to an underlying taxonomy. Any node of taxonomy that is assigned to a different category can visualize the categorization process by looking at the element down through all nodes that recognize it as belonging to the corresponding category. This essay aims to model progressive filtering technology from a probabilistic perspective in a hierarchical text categorization setting. Consequently, the task of developing, training, and testing it should be made easier for the designer of a system based on progressive filtering.", "histories": [["v1", "Thu, 3 Nov 2016 16:31:32 GMT  (116kb)", "http://arxiv.org/abs/1611.01080v1", "The article entitled Modeling Progressive Filtering, published on Fundamenta Informaticae (Vol. 138, Issue 3, pp. 285-320, July 2015), has been derived from this extended report"]], "COMMENTS": "The article entitled Modeling Progressive Filtering, published on Fundamenta Informaticae (Vol. 138, Issue 3, pp. 285-320, July 2015), has been derived from this extended report", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["giuliano armano"], "accepted": false, "id": "1611.01080"}, "pdf": {"name": "1611.01080.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Modelling of Progressive Filtering", "authors": ["Giuliano Armano"], "emails": ["armano@diee.unica.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n01 08\n0v 1\n[ cs\n.A I]"}, {"heading": "1 Introduction", "text": "Classification (or categorization) is a process of labeling data with categories taken from a predefined set, supposed to be semantically relevant to the problem at hand. The absence of an internal structure in the set of categories (or the absence of techniques able to account for this structure) leads to the so-called \u201cflat\u201d models, in which categories are dealt with independently of one another. In the event that the categories are organized in a taxonomy, typically through is-a or part-of relationships, and assuming that one wants to take into account also this information in order to improve the performance of a categorization system, the corresponding labeling process takes the name of hierarchical classification (HC). This research area has received much attention after the explosion of the World Wide Web, in which many problems and the corresponding software applications are based on an underlying taxonomy (e.g., web search with \u201cvertical\u201d search engines, online marketplaces, recommender systems).\nThis paper is aimed at modeling progressive filtering (hereinafter PF), a hierarchical technique inspired by the behavior that most humans put into practice while attempting to categorize data according to a taxonomy. PF assumes that a top-down categorization process occurs, performed in combination with a set of binary classifiers that mirror the structure of the taxonomy and are entrusted with accepting relevant inputs while rejecting the others. Starting from the root, supposed to be unique, a classifier that accepts an input passes it down to all its offspring (if any), and so on. The typical result consists of activating one or more paths within the taxonomy, i.e., those for which the corresponding classifiers have accepted the given input. While concentrating on hierarchical text categorization (HTC) problems, we will be focusing on the following issues: i) Can we predict the expected behavior of a system implemented in accordance with PF when fed with a corpus of documents whose statistical properties are known?, ii) Would it be feasible to separate the statistical information concerning inputs from the intrinsic properties of the classifiers embedded in the given taxonomy?\nTo my knowledge, no previous work has been done on the above issues, although the reasons for investigating them from a probabilistic perspective are manifold. In particular, a probabilistic model able to estimate the outcomes of a system that implements PF when applied to a real-world task can facilitate taxonomy design, optimization, and assessment. As for taxonomy design, the ability to assess in advance an update to the underlying taxonomy could be very useful for a designer. Indeed, adding or removing a node, as well as updating its characteristics, can have a substantial impact on performance, also depending on which metrics the designer wants to maximize. The importance of the model is also motivated by the fact that nowadays many e-businesses (e.g., online stores) resort to human experts to create and maintain the taxonomies considered relevant for their business activities, mainly due to the lack of automatic or semi-automatic tools for taxonomy handling. As for taxonomy optimization, let us assume that each classifier in the taxonomy has some parameters for controlling its behavior. The simplest scenario consists of focusing on the acceptance threshold, which typically falls in the range [0, 1]. According to this hypothesis, an optimization problem over the threshold space arises, characterized by high time complexity. To make this problem tractable, in our view, one should accept suboptimal solutions while looking at the above issues from a perspective based on three layers (listed from top to bottom): (i) the space of thresholds, (ii) the space of classifiers, and (iii) the space of experiments. For each layer, a source of intractability holds, which can be dealt with by means of approximate models. In particular, a \u201clight\u201d (hence, suboptimal) algorithm for threshold optimization can be used to search the space of thresholds; the mapping between the threshold and the expected behavior of a classifier can be estimated in the space of classifiers, 1 and the probabilistic model introduced in this paper can be used to predict the outcome of a test run on a corpus of documents with known statistical properties. As for taxonomy assessment, the possibility of evaluating relevant metrics\n1Many subtle problems arise in the space of classifiers when trying to reflect a change imposed on the space of thresholds. As discussion of these issues is far beyond the scope of this paper, we limit our assertion to the generic recommendations above \u2013intended to overcome the computational issues arising from the need to retrain classifiers.\ncan provide useful information about the expected behavior of a taxonomy. In particular, checking how the distribution of inputs, together with the characteristics of the embedded classifiers, affect the overall performance of a system compliant with PF can be very important while testing and maintaining a taxonomy.\nThe rest of the paper is organized as follows: Section 2 briefly recalls some related work, useful for fitting the problem within the current state-of-the-art. Section 3 introduces the concepts deemed most relevant for HTC. Section 4 defines PF, first from a probabilistic perspective and then as a linear transformation in the space of (normalized) confusion matrices. Section 5 analyzes how relevant metrics change within a taxonomy. Section 6 provides a critical assessment of PF. Conclusions and future work (Section 7) end the paper."}, {"heading": "2 Related Work", "text": "In line with the \u201cdivide and conquer\u201d philosophy, the main advantage expected from the hierarchical perspective is that the problem is partitioned into smaller subproblems, hopefully easier than the original one, so that each can be effectively and efficiently managed. Beyond this generic consideration, a number of algorithmic and architectural solutions have been experimented. A first rough division can be made between the so-called local vs. global approach. In the former case an ensemble of classifiers is generated, whereas in the latter a monolithic classifier is generated, able to account for the whole taxonomy. Local approaches seem to interpret the divide and conquer philosophy more properly, as they concentrate on (a typically small) part of the underlying taxonomy while implementing each component of the ensemble. However, the global approach does not prevent local strategies from actually being used to generate a monolithic classifier (e.g., multi-label decision trees)."}, {"heading": "2.1 Pachinko vs. Probabilistic Machines", "text": "In [20], all local approaches that rely on a sequence of top-down decisions take the esoteric name of pachinko machine, as they resemble to some extent the corresponding Japanese game. This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5]. Moreover, in [20], an extended version of the Pachinko-machine approach is proposed, adding the ability to terminate the categorization process at any intermediate level of the hierarchy.\nThe so-called probabilistic machines adopt an alternative approach, in which all paths are considered simultaneously. Their probabilities are calculated as the product of individual probabilities of categories (for each path), and the leaf categories (i.e., the most probable paths) are selected according to a maximum likelihood criterion. This approach has been used in combination with probabilistic classifiers, [7], with ANNs [14], [35], and with SVMs [2].\nIt is worth pointing out that Dumais and Chen compared the two local approaches, i.e., Pachinko machine and probabilistic, and found no difference in performance, [13]."}, {"heading": "2.2 Mapping Between Classifiers and the Underlying Taxon-", "text": "omy\nAccording to the survey paper of [28], a hierarchical approach is better understood when described from two dimensions, i.e., the nature of the given problem (or class of problems) and the characteristics of the algorithm devised to cope with it (or them). The problem is described by a triple \u3008\u03a5,\u03a8,\u03a6\u3009, where: \u03a5 specifies the type of graph representing the hierarchical classes (i.e., tree or DAG), \u03a8 indicates whether a data instance is allowed to have class labels associated with a single or multiple paths in the taxonomy, and \u03a6 describes the label depth of the data instances, i.e., full or partial. The algorithm is described by a 4-tuple \u3008\u2206,\u039e,\u2126,\u0398\u3009, where: \u2206 indicates whether single or multiple path prediction is performed, \u039e specifies whether leaf-node prediction is mandatory or not, \u2126 is the taxonomy structure the algorithm can handle (i.e., tree or DAG), and \u0398 establishes the mapping between classifiers and the underlying taxonomy (i.e., local classifier per node, local classifier per parent node, local classifier per level, and global classifier).\nA simple way to categorize the various proposals made in HC is to focus on the mapping between classifiers and the underlying taxonomy. Relevant proposals are listed from fine to coarse granularity:\n\u2013 Local Classifier per Node. This approach admits only binary decisions, as each classifier is entrusted with deciding whether the input at hand can be forwarded or not to its children. [10], [13], and [29] are the first proposals in which sequential Boolean decisions are applied in combination with local classifiers per node. In [38], the idea of mirroring the taxonomy structure through binary classifiers is clearly highlighted (the authors call this technique \u201cbinarized structured label learning\u201d). In [1], the underlying taxonomy is scattered on the corresponding set of admissible paths which originate from the root (called pipelines). Each component of a pipeline embeds a binary classifier, and pipelines are independently optimized.\n\u2013 Local Classifier per Parent Node. In the seminal work by [21], a document to be classified proceeds top-down along the given taxonomy, each classifier being used to decide to which subtree(s) the document should be sent to, until one or more leaves of the taxonomy are reached. This approach, which requires the implementation of multiclass classifiers for each parent node, gave rise to a variety of actual systems, e.g., [25], [10],[36], and [26].\n\u2013 Local Classifier per Level. This approach can be considered as a boundary between local and global approaches, as the number of outputs per level grows moving down through the taxonomy, soon becoming comparable with the number required for a global classifier. Among the proposals adopting this approach, let us recall [22] and [9].\n\u2013 Global Classifier. One classifier is trained, able to discriminate among all categories. Many global approaches to HC have been proposed, e.g., [34], [33], [19], [12], [31], [4], and [20].\nAccording to [29], training systems with a global approach is computationally heavy, as they typically do not exploit different sets of features at different hierarchical levels, and are not flexible, as a classifier must be retrained each time the hierarchical structure changes. On the other hand, although computationally more efficient, local approaches have to make several correct decisions in a row to correctly classify one example, and errors made at top levels are usually not recoverable. Moreover, the categories may lack positive examples at deep levels, making the task of training reliable classifiers difficult."}, {"heading": "2.3 Further Relevant Issues for HC", "text": "Further relevant issues for HC are the way feature selection/reduction is performed and which strategy is adopted to train the classifier(s) embedded in a hierarchical system. Research efforts in this area have focused largely on HTC.\nFeatures can be selected according to a global or a local approach (a comparison between the two approaches can be found in [35]). In global approaches, the same set of features is used at any level of the taxonomy, as done with flat categorization. This solution is normally adopted in monolithic systems, where only one classifier is entrusted with distinguishing among all categories in a taxonomy [16, 19]. Variations on this theme can be found in [36] and in [24]. In local approaches, different sets of features are selected for different nodes in the taxonomy, thus taking advantage of dividing a large initial problem into subproblems, e.g., [36]. This is the default choice for Pachinko machines. In a more recent work, [15] suggest that feature selection should pay attention to the topology of the classification scheme. Among other approaches to feature selection, let us recall [23], based on \u03c7-square feature evaluation. As for feature reduction, latent semantic indexing [11] is the most commonly used technique. Based on singular value decomposition [17], it implements the principle that words used in the same contexts tend to have similar meanings.\nAs for training strategies, according to [6], training sets can be hierarchical or proper. The former include documents of the subtree rooted in a category as positive examples and documents of the sibling subtrees as negative examples. The latter include documents of a category as positive examples (while disregarding documents from its offspring), and documents of the sibling categories as negative examples. After running several experiments aimed at assessing the pros and cons of the two training strategies, the authors have shown that hierarchical training sets are more effective."}, {"heading": "3 Hierarchical Text Categorization", "text": "As our work will focus mainly on HTC, let us summarize the basic concepts and the issues considered most relevant to this research field (see also [27] and [20])."}, {"heading": "3.1 Standard Definitions for HTC", "text": "Text Categorization. Text categorization is the task of assigning a Boolean value to each pair \u3008dj, ci\u3009 \u2208 D\u00d7C, where D is a domain of documents and C = {ck | k = 1, 2, ..., N} is a set of N predefined categories.\nHierarchical Text Categorization. Hierarchical Text Categorization is a text categorization task performed according to a given taxonomy T = \u3008C,\u2264\u3009, where C = {ck | k = 1, 2, ..., N} is a set of N predefined categories and \u201c\u2264\u201d is a reflexive, anti-symmetric, and transitive binary relation.2\nIn the most general case, T can be thought of as a strict partially ordered set (strict poset), which can be graphically represented by a DAG. We assume known all ordinary definitions concerning posets. However, for the sake of readability, let us recall some relevant definitions.\nCovering Relation. Given a taxonomy T = \u3008C,\u2264\u3009, the covering relation \u201c\u227a\u201d holds between comparable elements that are immediate neighbors in the taxonomy. In symbols: b \u227a a \u21d4 b < a \u2227\u00ac \u2203 c \u2208 C s.t. b < c < a. The characteristic function f : C \u00d7C \u2192 [0, 1] for the covering relation \u201c\u227a\u201d is defined as:\nf(b, a) = { 1 if b \u227a a 0 otherwise\n(1)\nA \u201csoft\u201d version of the above definition would substitute \u201c1\u201d (used to denote full membership) with a number intended to measure to what extent the pair in question satisfies the covering relation. In a probabilistic setting, a natural choice for the characteristic function would be to let it coincide with the conditional probability p(b|a). In symbols:\n\u2200a, b \u2208 C : b \u227a a \u21d0\u21d2 f(b, a) \u2261 p(b|a) > 0 (2)\nAncestors, Offspring, and Children Sets. The notions of ancestors, offspring, and children sets, useful when dealing with taxonomies, can be easily defined for posets (hence, for DAGs and trees). Given a node r \u2208 C:\nA(r) = {a \u2208 C | r < a} Ancestors set\nO(r) = {o \u2208 C | o < r} Offspring set (3)\nH(r) = {c \u2208 C | c \u227a r} CHildren set\nRoot, internal nodes, and leaves. A category without ancestors is called root ; a category without children is called leaf, and a category with both ancestors and offspring is called internal category.\nTwo constraints must be effective for hierarchical text categorization:\n2 Some authors use \u201c<\u201d instead of \u201c\u2264\u201d as default binary relation. As the definition of \u201c=\u201d and \u201c<\u201d from \u201c\u2264\u201d is trivial, in the following we will use \u201c<\u201d when deemed useful for rendering definitions more intuitive.\n\u2013 Hierarchical Consistency. A label set Cd \u2286 C assigned to an instance d \u2208 D is said to be consistent with a given taxonomy T = \u3008C,\u2264\u3009 if it includes the complete ancestor sets for every label c \u2208 Cd. In symbols: c \u2208 Cd \u2227 b \u2208 A(c) \u2192 b \u2208 Cd.\n\u2013 Hierarchical Consistency Requirement. Any label assignments produced by a hierarchical classification system on a given categorization task has to be consistent with the underlying category taxonomy.\nThe notion of domain of a category c is also relevant, which denotes all documents that belong to c (i.e., the set of its positive instances). Domain of a category. Given a taxonomy T = \u3008C,\u2264\u3009 the domain of a category c \u2208 C is defined as: 3\ndom(c) = {i | i \u22b3 c \u2228 \u2203a \u2208 O(c) s.t. i \u22b3 a} (4)\nWe assume that each category c \u2208 C embeds a corresponding binary classifier. Given an input, the classifier is entrusted with deciding whether or not it belongs to the corresponding category. To distinguish between a category and its embedded classifier, the latter will be denoted by a circumflex (i.e., c\u0302 denotes the classifier embedded by the category c).\nThe definition of domain can also be given for classifiers. In particular, dom(c\u0302) denotes the set of inputs accepted by c\u0302. In the ideal case in which dom(c\u0302) \u2261 dom(c), we say that the classifier acts as an oracle for the given category. However, although a classifier is expected to approximate as much as possible the corresponding category, its domain typically does not coincide with that identified by the oracle (see Figure 1), i.e., dom(c\u0302) 6= dom(c).\n3Where \u201ci \u22b3 c\u201d denotes the instance-of relation that holds between the instance i and the category c.\nWithout loss of generality, we assume that the given taxonomy T = \u3008C,\u2264\u3009 has a unique root. In principle, the domains of categories that occur along a path originating from the root satisfy an inclusion relation (see Figure 2). The same kind of inclusion relation holds among the domains of the corresponding classifiers."}, {"heading": "3.2 Non-Standard Definitions for HTC", "text": "We want T to be represented by the set of all its most representative paths, i.e., those that originate from the root. Any one of these paths will be called pipeline hereinafter. Figure 3 depicts a simple source taxonomy, on the left part, and its \u201cunfolding\u201d in terms of pipelines, on the right part.\nWell-Formed Strings and Pipelines. Given a taxonomy T = \u3008C,\u2264\u3009, a pipeline \u03c0 is a well-formed string that originates from the root.\nThe definition of pipeline relies upon the concept of well-formed string, which in turn can be defined through the corresponding characteristic function F : C\u2217 \u2192 [0, 1]:\nF (w) = { 1 w \u2261 \u03bb \u2228 w \u2208 C F (\u03b1) \u00b7 f(\u03b2, \u03b1) \u00b7 F (\u03b2) w = \u03b1+ \u03b2 \u2261 \u03b1\u03b2, \u03b1, \u03b2 \u2208 C+\n(5)\nwhere:\n\u2013 the operator \u201c+\u201d denotes concatenation between two strings (it can be omitted in absence of ambiguity);\n\u2013 the constant \u03bb denotes the empty string (with the property that \u2200\u03b1 \u2208 C\u2217 : \u03b1+\u03bb \u2261 \u03bb+ \u03b1 \u2261 \u03b1);\n\u2013 f(\u03b2, \u03b1) extends the characteristic function of the covering relation to pairs of strings in C+, as follows: f(\u03b2, \u03b1) \u2261 f(head(\u03b2), tail(\u03b1)), with head and tail having the usual semantic of extracting the first and the last element of their argument, respectively.\nNote that, in a probabilistic setting, the characteristic function F represents the probability that a document will go through the corresponding pipeline under the assumption that the embedded classifiers act as oracles.\nThe set of well-formed strings WT and the set of pipelines PT in T = \u3008C,\u2264\u3009 can now be defined as follows:\nWT = {w \u2208 C \u2217 | F (w) > 0} (6)\nPT = {\u03c0 \u2208 WT \u2212 {\u03bb} | head(\u03c0) = root(T )} (7)\nFor instance, the path A \u2192 B \u2192 D shown in Figure 2 gives rise to the string ABD, which is well-formed, as D \u227a B \u227a A, and rooted, as head(ABD) = root(T ); hence, it is a pipeline. A document of category D is expected to go through the pipeline ABD if correctly classified by the corresponding taxonomic system built upon T .\nA partial order also holds for pipelines. The concept we want to capture here is that an existing pipeline typically embeds other pipelines. In symbols:\n\u03c01, \u03c02 \u2208 PT : \u03c01 \u2264 \u03c02 \u21d0\u21d2 \u2203w \u2208 WT s.t. \u03c02 = \u03c01 + w (8)\nReferring once again to Figure 3, other than the trivial assertion ABC \u2264 ABC, we can also state that AB \u2264 ABC and that A \u2264 ABC as AB + C = A +BC = ABC.\nThe reason why pipelines are considered so important lies in the fact that they facilitate the task of analyzing the corresponding taxonomy. In particular, pipelines can be extracted from both trees and DAGs, are immune from the problem of having to deal with multiple class labels, admit overlapping between class domains, and are naturally suited to deal with partial paths (a partial path originates from the root and terminates with an internal node of the taxonomy as opposed to a full path, which terminates with a leaf). Consequently, all issues that may arise depending on the characteristics of a hierarchical problem require the activation of suitable policies only\nat the moment of moving from a pipeline-oriented to a taxonomy-oriented perspective, whereas the unfolding in terms of pipelines appears to be a common task for a variety of actual policies.\nIt is worth noting that the possibility of activating multiple paths within a taxonomy implies that, for at least one internal node c \u2208 C, an overlap occurs between (at least) two of its children. In symbols:\n\u2203 a, b \u2208 H(c) : dom(a) \u2229 dom(b) 6= \u2205 (9)\nAs for the assumption of having to deal with partial paths, this implies that at least one internal node c \u2208 C has proper instances, not shared with any of its children. In symbols:\ndom(c) \u2283 \u22c3\na\u2208H(c)\ndom(a) (10)\nA further definition will be useful when discussing the main characteristics of PF. Assuming that a, b \u2208 C and that a \u2264 b, the key concept we want to capture is that the relevant inputs for a with respect to b are all positive instances of b. To this end, let us define the notion of \u201crelevance set\u201d (rset) between two categories, as follows: Relevance Set.\n\u2200a, b \u2208 C : rset(b, a) \u25b3 =\n{ dom(a) if b \u2264 a\n\u2205 otherwise (11)\nWe can now check whether an input i is relevant (rel) for a category b, with respect to a category a, according to the definition below: Relevance.\nrel(i, b|a) \u25b3 = i \u2208 rset(b, a) (12)"}, {"heading": "4 Progressive Filtering", "text": "PF is a top-down strategy that requires the underlying taxonomy to be mirrored with local classifiers per node (hereinafter LCN), the underlying assumption being that the domain of a node/classifier encompasses the domains of its children. Consequently, classifiers are trained with hierarchical training sets and propagate an input only in the event that they accept it.4 This choice, together with the \u201cpass-down\u201d strategy, preserves hierarchical consistency, which imposes that all ancestors of a category that accepts an input must also accept it. As for the hierarchical consistency requirement, it may be satisfied or not, depending on the kind of structure in question (tree or\nDAG) and on the policy adopted to deal with well-known issues, such as: (i) leaf node prediction, (ii) premature blocking, and (iii) high-level error recovering. We know that leaf-node prediction can be mandatory or not. In the latter case, for at least one input, its most specific class is not required to be a leaf node in the taxonomy, e.g., [29]. When the classification stops at an internal node while an oracle would keep propagating the current input downwards, then the blocking problem arises. Some strategies to avoid blocking are discussed, for instance, in [30]. The research community has also devoted efforts to cope with high-level error recovering. The interested reader will find several proposals aimed at tackling this issue in [8] and in [37]."}, {"heading": "4.1 Common solutions and known issues for PF", "text": "A common solution for implementing a binary classifier c\u0302 for a category c \u2208 C consists of thresholding a real-valued classifier, entrusted with estimating the probability that an input belongs to c. In so doing, an optimization problem arises, which consists of identifying the threshold that maximizes/minimizes a utility/cost function, usually a well-known metric. The simplest solution to this problem, when classifiers are framed in a taxonomy, consists of independently optimizing pipelines of binary local classifiers, in which the same classifier is allowed to have different thresholds, depending on which pipeline it is embedded by, [1]. In so doing, a sort of \u201cflattening\u201d of the underlying taxonomy is performed, while pipelines still embed information about the underlying taxonomy.\nPF can give rise to many other kinds of actual systems, depending on the given class of problems, on the choices made by the algorithm devised to solve them, and on the specific policies adopted to deal with the most well-known issues (see, for instance, [3]) encountered while trying to enforce the hierarchical consistency requirement.\nIt is worth noting that even simple scenarios may hide subtle issues. Just to give a taste of them, let us consider a case in which the given problem requires mandatory leafprediction. This implies, at least in principle, that any input accepted by an internal node c must be accepted by at least one of its children. As, by default, PF does not perform any direct action designed to enforce this property, the blocking problem may occur. Things deteriorate when one assumes that non-mandatory leaf-prediction is permitted, as nothing guarantees that stopping the acceptance of the current input at an internal node corresponds in fact to a correct categorization.\nSumming up, a complete scenario of the probabilistic behavior of a taxonomy as a whole cannot be developed because of the large number of variations in terms of feasible policies and of the issues that may arise when trying to cope with the most well-known problems ensuing from a top-down strategy based on LCNs. Any specific solution (with its pros and cons) would generate a different statistical behavior, though still based on the pipelines extracted from the given taxonomy. This is the main reason why we concentrate on pipelines, which allow to perform a preliminary analysis regardless of the combination policy adopted. In particular, the focus will be first on classifiers in\n4A dual strategy, not considered for PF, would assume that the domain of a node/classifier accounts only for its own inputs, disregarding the domains of its children. In this case, classifiers should be trained with proper training sets and propagate an input only in the event that they do not accept it.\nisolation and then on pipelines of classifiers. In both cases, the concept of \u201cnormalized\u201d confusion matrix is used to differentiate the probabilistic behavior of a classifier from the actual confusion matrices that summarize the results of specific experiments.\nIn the following, we also assume that the behavior of all classifiers is statistically significant. Under this assumption, we can model the outcome of a classifier embedded by a pipeline with two random variables, ranging over 0 (false) and 1 (true). In particular, following the choice made to distinguish oracles from actual classifiers, random variables related to oracles are denoted in plain format (e.g., X), whereas those related\nto actual classifiers have a circumflex (e.g., X\u0302). Joint or conditional probabilities in-\nvolved in the modeling activity, e.g., p(X, X\u0302) and p(X\u0302|X), are represented with 2\u00d7 2 matrices. Single random variables are also represented with 2 \u00d7 2 diagonal matrices, exploiting the fact that p(X) \u2261 p(X,X)."}, {"heading": "4.2 Analysis of a Single Classifier", "text": "Let us denote with \u039ec(p, n) the confusion matrix of a run in which a classifier c\u0302 embedded by a category c \u2208 C is fed with m instances, of which p are positive and n negative. Paying attention to keeping the same values for p and n on different runs, the joint probability p(Xc, X\u0302c) is proportional, through m, to the expected value of \u039ec(p, n). In symbols:\nE [\u039ec(p, n)] = m \u00b7 p(Xc, X\u0302c) (13)\nAssuming statistical significance, the confusion matrix obtained from a single test (or, better, averaged over multiple tests) gives us reliable information on the performance of a classifier. Hence, we can write:\n\u039ec(p, n) \u2248 m \u00b7 p(Xc, X\u0302c) = m \u00b7 p(Xc) \u00b7 p(X\u0302c|Xc) (14)\nWe assume that the transformation performed by c\u0302 can be isolated from the inputs it processes, at least from a statistical perspective. In so doing, the confusion matrix for a given set of inputs can be written as the product between a term that accounts for the number of positive and negative instances, on the one hand, and a term that represents the expected recognition / error rate of c\u0302. In symbols:\n\u039ec(p, n) = m \u00b7 [ f\u0304c 0 0 fc ]\n\ufe38 \ufe37\ufe37 \ufe38 O(c)\u2248p(Xc)\n\u00b7 [ \u03b300 \u03b301 \u03b310 \u03b311 ]\n\ufe38 \ufe37\ufe37 \ufe38 \u0393(c)\u2248p(X\u0302c|Xc)\n(15)\nwhere:\n\u2013 fc = p/m and f\u0304c = n/m denote the percent of positive and negative instances, respectively;\n\u2013 \u03b3ij \u2248 p(X\u0302c = j | Xc = i), i, j = 0, 1, denote the percent of inputs that have been correctly classified (i = j) or misclassified (i 6= j) by c\u0302. In particular, \u03b300, \u03b301, \u03b310, and \u03b311 denote the percent of true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP), respectively. It can be easily verified that \u0393(c) is normalized row-by-row, i.e., that \u03b300 + \u03b301 = \u03b310 + \u03b311 = 1. For\nthis reason, hereinafter an estimate of the conditional probability p(X\u0302c|Xc) for a classifier c\u0302 embedded by a category c will be called normalized confusion matrix.\nThe separation between inputs and the intrinsic behavior of a classifier reported in Equation (15) suggests an interpretation that recalls the concept of transfer function, where a set of inputs is applied to c\u0302. In fact, this could be interpreted alternatively as separating the optimal behavior of a classifier from the deterioration introduced by its actual filtering capabilities. In particular, O(c) \u2248 p(Xc) represents the optimal behavior obtainable when c\u0302 acts as an oracle, whereas \u0393(c) \u2248 p(X\u0302c |Xc) represents the expected deterioration caused by the actual characteristics of the classifier."}, {"heading": "4.3 Analysis of a Pipeline of Classifiers", "text": "Pipelines are in fact the \u201cbuilding blocks\u201d of the corresponding taxonomy. Without loss of generality, in the following we will adopt a naming scheme independent from the generic pipeline being investigated. In particular, the components of a pipeline \u03c0 of length L+ 1 are assumed to be the categories c0, c1, . . . , cL (where c0 represents the root), the underlying assumption being that \u2200k = 1, . . . , L : ck\u22121 \u227a ck. An example of pipeline, extracted from a taxonomy and undergone to standard renaming, is shown in Figure 4.\nLet us also assume that \u03c0k \u2264 \u03c0 denotes the \u201csubpipeline\u201d c0c1 . . . ck and that e(X, X\u0302) denotes co-occurring events involving an oracle and the corresponding classi-\nfier; in particular, eij will be used as a shorthand for e(X = i, X\u0302 = j), \u2200i, j = 0, 1. Still for the sake of readability, the domain of ck will be denoted by Ak, whereas the domain of c\u0302k will be denoted by A\u0302k. The full list of shorthands defined with the goal of\nsimplifying the notation while deriving relevant formulas is reported in Table 1. Moreover, in absence of ambiguities, not-indexed quantities are meant to denote k = L, e.g., \u2126\u03c0(D) \u2261 \u2126\u03c0L(DL).\nStudying classifiers embedded by a pipeline requires to model their interactions, which originate from the fact that the domain of a classifier c\u0302k is, by hypothesis, a proper subset of the domain of its ancestors. While the normalized confusion matrix of a classifier c\u0302 in isolation originates from p(X\u0302c |Xc), additional conditions are required for a classifier embedded by a pipeline (except for the root), which accounts for the presence of its ancestors:\n\u0393(k) \u2248 p(X\u0302k |Xk, X\u0302k\u22121 = 1, X\u0302k\u22122 = 1, . . . , X\u03020 = 1) (16)\nHowever, due to the embedding of classifiers, some tautological implications imposed by the underlying taxonomy hold for k > 0 (see also the concept of \u201cTrue Path Rule\u201d in [32]):\nXk\u22121 = 0 |= Xk = 0, Xk = 1 |= Xk\u22121 = 1 (from Ak \u2286 Ak\u22121) (17)\nX\u0302k\u22121 = 0 |= X\u0302k = 0, X\u0302k = 1 |= X\u0302k\u22121 = 1 (from A\u0302k \u2286 A\u0302k\u22121) (18)\nHence, considering that X\u0302k\u22121 = 1 |= X\u0302k\u22122 = 1 |= . . . |= X\u03020 = 1, Equation (16) can be simplified as follows:\n\u0393(k) \u2248 p(X\u0302k |Xk, X\u0302k\u22121 = 1) (19)\n4.3.1 Finding an approximation for p(Xk, X\u0302k)\nAccording to a probabilistic perspective, the starting point of our analysis is:\nE [ \u039e(k) ] = m \u00b7 p(Xk, X\u0302k) = m \u00b7 p(e (k)) (20)\nAs the process of estimating p(Xk, X\u0302k) requires approximations, let us use a specific\nnotation for the (estimation of) the joint probability p(Xk, X\u0302k):\n\u2126(k) \u2248 p(e(k)) (21)\nFrom the law of total probability, represented with the Bayes decomposition, each component of \u2126(k) can be represented as:\n\u03c9 (k) ij \u2248 p(e (k) ij ) =\n\u2211\nr,s\np(e(k\u22121)rs ) \u00b7 p(e (k) ij |e (k\u22121) rs ), \u2200i, j = 0, 1 (22)\nFor the sake of brevity, we only derive \u03c9 (k) 00 . The reader can consult APPENDIX A for\nfurther details on the derivation of \u03c9 (k) ij , \u2200i, j = 0, 1. To keep the notation simpler,\nlet us use \u201cprime\u201d to denote events or random variables that refer to the pipeline \u03c0k, whereas plain text refers to \u03c0k\u22121:\np(e\u203200) = p(e00) \u00b7 p(e \u2032 00|e00) + p(e01) \u00b7 p(e \u2032 00|e01) + p(e10) \u00b7 p(e \u2032 00|e10) + p(e11) \u00b7 p(e \u2032 00|e11)\nwhere:\np(e\u203200|e00) = p(X \u2032 = 0, X\u0302 \u2032 = 0 |X = 0, X\u0302 = 0)\n= p(X\u0302 \u2032 = 0 |X \u2032 = 0, X = 0, X\u0302 = 0)\ufe38 \ufe37\ufe37 \ufe38 =1 \u00b7 p(X \u2032 = 0 |X = 0, X\u0302 = 0)\ufe38 \ufe37\ufe37 \ufe38 =1 = 1\np(e\u203200|e01) = p(X \u2032 = 0, X\u0302 \u2032 = 0 |X = 0, X\u0302 = 1)\n= p(X\u0302 \u2032 = 0 |X \u2032 = 0, X = 0, X\u0302 = 1)\ufe38 \ufe37\ufe37 \ufe38 \u2248\u03b3\u203200 \u00b7 p(X \u2032 = 0 |X = 0, X\u0302 = 1)\ufe38 \ufe37\ufe37 \ufe38 =1 \u2248 \u03b3\u203200\np(e\u203200|e10) = p(X \u2032 = 0, X\u0302 \u2032 = 0 |X = 1, X\u0302 = 0)\n= p(X\u0302 \u2032 = 0 |X \u2032 = 0, X = 1, X\u0302 = 0)\ufe38 \ufe37\ufe37 \ufe38 =1 \u00b7 p(X \u2032 = 0 |X = 1, X\u0302 = 0)\ufe38 \ufe37\ufe37 \ufe38 \u2248f\u0304 \u2032 \u2248 f\u0304 \u2032\np(e\u203200|e11) = p(X \u2032 = 0, X\u0302 \u2032 = 0 |X = 1, X\u0302 = 1)\n= p(X\u0302 \u2032 = 0 |X \u2032 = 0, X = 1, X\u0302 = 1)\ufe38 \ufe37\ufe37 \ufe38 \u2248\u03b3\u203200 \u00b7 p(X \u2032 = 0 |X = 1, X\u0302 = 1)\ufe38 \ufe37\ufe37 \ufe38 \u2248f\u0304\u2032 \u2248 \u03b3\u203200 \u00b7 f\u0304 \u2032\nHence:\np(e\u203200) \u2248 \u03c9 \u2032 00 = \u03c900 + \u03c901 \u00b7 \u03b3 \u2032 00 + f\u0304 \u2032 \u00b7 \u03c910 + f\u0304 \u2032 \u00b7 \u03c911 \u00b7 \u03b3 \u2032 00\nBy making the derivation explicit for all \u03c9 (k) ij , i, j = 0, 1, we can approximate\np(Xk, X\u0302k) as follows (k > 0):\n\u2126(k) =    \u03c9 (k) 00 = \u03c9 (k\u22121) 00 + \u03c9 (k\u22121) 01 \u00b7 \u03b3 (k) 00 + f\u0304k \u00b7 \u03c9 (k\u22121) 10 + f\u0304k \u00b7 \u03c9 (k\u22121) 11 \u00b7 \u03b3 (k) 00 \u03c9 (k) 01 = 0 + \u03c9 (k\u22121) 01 \u00b7 \u03b3 (k) 01 + 0 + f\u0304k \u00b7 \u03c9 (k\u22121) 11 \u00b7 \u03b3 (k) 01 \u03c9 (k) 10 = 0 + 0 + fk \u00b7 \u03c9 (k\u22121) 10 + fk \u00b7 \u03c9 (k\u22121) 11 \u00b7 \u03b3 (k) 10 \u03c9 (k) 11 = 0 + 0 + 0 + fk \u00b7 \u03c9 (k\u22121) 11 \u00b7 \u03b3 (k) 11\n(23)\nTo help the reader better understand the underlying process, a graphical representation of the transformation that occurs along a pipeline from step k \u2212 1 to step k is given in Figure 5, which highlights how the elements of \u2126(k\u22121) concur to generate \u2126(k). Quantitative information, reported in Equation (23), is intentionally disregarded.\n11 is\nresponsible for \u03c9 (k) 01 .\nAs for the base case (i.e., k = 0), it can be observed that the role of the root is to forward any incoming document down to its children. In other words, the (virtual) classifier embedded by the root accepts everything as a positive instance. For this reason, the base case for \u2126(0) is:\n\u2126(0) = [ 0 0 0 1 ] (24)\nwhereas the normalized confusion matrix of the root is:\n\u0393(0) = [ 0 1 0 1 ] \u25b3 = \u00b5 (25)\nwhere \u00b5 is a constant that characterizes the neutral classifier, whose unique responsibility is to \u201cpass everything down\u201d to its children, no matter whether input documents are TP or FP.5"}, {"heading": "4.3.2 Revisiting One Step of Progressive Filtering", "text": "Looking at Equation (23), each processing step actually involves two separate actions. As sketched in Figure 6, everything goes as if the output of a classifier undergo context switching before classification.\nContext switching. Concerns the fact that only part of TP output by c\u0302k\u22121 are still TP for c\u0302k. Under the assumption of statistical significance (and recalling the definition of relevance set), the percent of relevant inputs for c\u0302k that move from TP to FP is\n5Different choices could be made to represent the normalized confusion matrix of the root, without changing the result of the transformation that occurs there. However, the adoption of the neutral classifier appears the most intuitive. We will get back to this issue in the next subsection.\napproximately f\u0304k. Conversely, only part of FN output by c\u0302k\u22121 are still FN for c\u0302k, so that the percent of inputs that move from false to TN is still f\u0304k. Hence, with \u03c7 and \u2126 representing the percent of inputs and the percent of outputs of a classifier in terms of true/false positives/negatives, we can write:\n\u03c7(k) =\n[ \u03c9 (k\u22121) 00 + f\u0304k \u00b7 \u03c9 (k\u22121) 10 \u03c9 (k\u22121) 01 + f\u0304k \u00b7 \u03c9 (k\u22121) 11\nfk \u00b7 \u03c9 (k\u22121) 10 fk \u00b7 \u03c9 (k\u22121) 11\n] = [ 1 f\u0304k 0 fk ] \u00b7 \u2126(k\u22121) (26)\nClassification. The transformation performed by c\u0302k can be better understood highlighting that two paths can be followed by a document while going through the pipeline in hand: inner and outer path. Figure 7 illustrates the different paths followed by input documents while traversing a pipeline.\nThe inner path operates on true positives (\u03c711) and false positives (\u03c701). The corresponding transformation can be represented as follows:\n\u2126(k) \u2223\u2223\u2223 inner =\n[ 0 \u03c7\n(k) 01 \u00b7 \u03b3 (k) 01\n0 \u03c7 (k) 11 \u00b7 \u03b3 (k) 11\n] = [ \u03c7 (k) 01 0\n0 \u03c7 (k) 11\n] \u00b7 [ 0 \u03b3 (k) 01\n0 \u03b3 (k) 11\n] (27)\nThe outer path operates on true negatives (\u03c700) and false negatives (\u03c710). The whole process is cumulative, and can be represented as follows (still for the classifier c\u0302k):\n\u2126(k) \u2223\u2223\u2223 outer =\n[ \u03c7 (k) 00 + \u03c7 (k) 01 \u00b7 \u03b3 (k) 00 0\n\u03c7 (k) 10 + \u03c7 (k) 11 \u00b7 \u03b3 (k) 10 0\n] = [ \u03c7 (k) 00 0\n\u03c7 (k) 10 0\n] + [ \u03c7 (k) 01 0\n0 \u03c7 (k) 11\n] \u00b7 [ \u03b3 (k) 00 0\n\u03b3 (k) 10 0\n] (28)\nPutting together Equation (27) and (28), we obtain:\n\u2126(k) =\n[ \u03c7 (k) 00 0\n\u03c7 (k) 10 0\n] + [ \u03c7 (k) 01 0\n0 \u03c7 (k) 11\n] \u00b7 \u0393(k) (29)\nFor its importance within the model, the transformation represented by Equation (29) deserves a specific definition. Operator \u2295.\nA\u2295 B = [ \u03b100 \u03b101 \u03b110 \u03b111 ] \u2295 [ \u03b200 \u03b201 \u03b210 \u03b211 ] \u25b3 = [ \u03b100 0 \u03b110 0 ] + [ \u03b101 0 0 \u03b111 ] \u00b7 [ \u03b200 \u03b201 \u03b210 \u03b211 ] (30)\nIt is now easy to obtain a compact form for the transformation that occurs along the inner and the outer path of a pipeline. In symbols:\n\u2126(k) = \u03c7(k) \u2295 \u0393(k) = ([ 1 f\u0304k 0 fk ] \u00b7 \u2126(k\u22121) )\n\ufe38 \ufe37\ufe37 \ufe38 context switching\n\u2295 \u0393(k)\ufe38\ufe37\ufe37\ufe38 classification\n(31)\nNote that Equation (31) can be applied also to the base case (k = 0), yielding:\n\u2126(0) = \u03c7(0) \u2295 \u0393(0) = ([ 1 f\u03040 0 f0 ] \u00b7 [ 0 0 0 1 ]) \u2295 \u0393(0) = [ 0 0 0 1 ] \u2295 \u00b5 = [ 0 0 0 1 ] (32)\nEquation (32) points out that neither the (virtual) context switching performed before submitting the input to the root nor the (virtual) processing of the root alter the given input \u2013upon the assumption that f0 = 1 (hence, f\u03040 = 0) and that \u0393 (0) = \u00b5.\nSummarizing, the overall transformation can be represented as follows:\n\u2013 Base case (k = 0), i.e., output of the root:\n\u2126(0) = [ 0 0 0 1 ] (33)\n\u2013 Recursive step (k > 0), which coincides with Equation (23):\n\u2126(k) = ([ 1 f\u0304k 0 fk ] \u00b7 \u2126(k\u22121) ) \u2295 \u0393(k) (34)\nFigure 8 can help the reader better understand context switching and classification. As previously done, also in this case quantitative information is intentionally disregarded.\nUnfolding the recurrence relation that defines \u2126 allows to obtain a closed formula, which accounts for the behavior of a pipeline \u03c0k (k > 0):\n\u2126(k) =    \u03c9 (k) 00 = F\u0304k \u2212 k\u2211 j=1 f\u0304j \u00b7 Fj\u22121 \u00b7 ( j\u22121\u220f r=0 \u03b3 (r) 11 ) \u00b7 ( k\u220f s=j \u03b3 (s) 01 ) \u03c9 (k) 01 = k\u2211 j=1 f\u0304j \u00b7 Fj\u22121 \u00b7 ( j\u22121\u220f r=0 \u03b3 (r) 11 ) \u00b7 ( k\u220f s=j \u03b3 (s) 01 ) \u03c9 (k) 10 = Fk \u2212 Fk \u00b7 k\u220f j=0 \u03b3 (j) 11 \u03c9 (k) 11 = Fk \u00b7 k\u220f\nj=0\n\u03b3 (j) 11\n(35)\nIt is easy to verify from Equation (35) that \u03c9 (k) 00 = F\u0304k\u2212\u03c9 (k) 01 and that \u03c9 (k) 10 = Fk\u2212\u03c9 (k) 11 ; hence let us spend a few words to clarify the underlying semantics only for \u03c9 (k) 01 and \u03c9 (k) 11 .\nAs for \u03c9 (k) 11 , it represents the core behavior of PF. In particular, given an input, each classifier along the pipeline \u03c0k accepts and forwards it with probability fj \u00b7 \u03b3 (j) 11 , j = 0, 1, . . . , k. The resulting product can be split in two terms, one that accounts for the distribution of inputs and the other that accounts for the intrinsic properties of classifiers, as follows:\nk\u220f\nj=0\n( fj \u00b7 \u03b3 (j) 11 ) =\n( k\u220f\nj=0\nfj\n) \u00b7 ( k\u220f\nj=0\n\u03b3 (j) 11 ) = Fk \u00b7 ( k\u220f\nj=0\n\u03b3 (j) 11\n) (36)\nAs for \u03c9 (k) 01 , each component of the sum denotes a different subset of inputs, recognized as positive by c\u0302k but in fact negative. As these subsets are independent from each other, let us concentrate on a generic j-th element of the sum. In symbols:\nf\u0304j \u00b7 Fj\u22121 \u00b7\n( j\u22121\u220f\nr=0\n\u03b3 (r) 11\n) \u00b7 ( k\u220f\ns=j\n\u03b3 (s) 01\n) j = 1, 2, . . . , k (37)\nTwo processing modes hold along the pipeline \u03c0k, and the switching occurs between c\u0302j\u22121 and c\u0302j. Let us analyze these modes, together with the corresponding context switching:\n(a) Processing mode before c\u0302j . This behavior reproduces the one already analyzed for\n\u03c9 (k) 11 , with the obvious difference that it is observed along the pipeline \u03c0j\u22121;\n(b) Context switching between c\u0302j\u22121 and c\u0302j. The effect of context switching is to turn TP into FP, with probability f\u0304j;\n(c) Processing mode after c\u0302j\u22121. To keep \u201csurviving\u201d as FP, an input must be (incorrectly) recognized as positive by all the remaining classifiers that occur along the\npipeline, including c\u0302j, each with probability \u03b3 (s) 01 , s = j, j + 1, . . . k.\nAccording to the ordering followed by the enumeration above, Equation (37) can be rewritten as:\nFj\u22121 \u00b7\n( j\u22121\u220f\nr=0\n\u03b3 (r) 11\n)\n\ufe38 \ufe37\ufe37 \ufe38 (a)\n\u00b7 f\u0304j\n\ufe38\ufe37\ufe37\ufe38 (b)\n\u00b7\n( k\u220f\ns=j\n\u03b3 (s) 01\n)\n\ufe38 \ufe37\ufe37 \ufe38 (c)\nj = 1, 2, . . . , k (38)\nNow that the semantics of all elements reported in \u2126(k) has been clarified, let us try to give \u2126(k) a more concise form through the following definitions:\nk\u220f\ni=0\n\u03b3 (i) 01 \u25b3 = \u03c8 (k) 01 ,\nk\u220f\ni=0\n\u03b3 (i) 11 \u25b3 = \u03c8 (k) 11 , and\n1\nF\u0304k \u00b7\nk\u2211\ni=1\nf\u0304i \u00b7 Fi\u22121 \u00b7 \u03c8\n(i\u22121) 11 \u03c8 (i\u22121) 01 \u25b3 = \u03b7(k) (39)\nAccording to these definitions, \u2126(k) can be rewritten as (k > 0):\n\u2126(k) = [ F\u0304k 0 0 Fk ]\n\ufe38 \ufe37\ufe37 \ufe38 O(k)\n\u00b7\n[ 1\u2212 \u03b7(k) \u00b7 \u03c8\n(k) 01 \u03b7 (k) \u00b7 \u03c8 (k) 01\n1\u2212 \u03c8 (k) 11 \u03c8 (k) 11\n]\n\ufe38 \ufe37\ufe37 \ufe38 \u03a6(k)\n(40)\nwhere O(k) accounts for the optimal behavior of the pipeline \u03c0k (as all its classifiers were oracles), whereas \u03a6(k) represents the expected deterioration, due to the actual behavior of \u03c0k.\nIt is easy to verify that \u03a6, which plays for pipelines the role that \u0393 plays for single classifiers, is also normalized row-by-row for each k = 1, 2, . . . , L. As for \u03a6(0), we know that the following equivalence must hold: \u03a6(0) = \u0393(0) = \u00b5. Hence, as expected, also \u03a6(0) is normalized row-by-row.\nMoreover, as \u2126(k) spans over the whole space of events, the sum over its components must be 1. While trivially true for k = 0, it is easy to show it for any k > 0. Starting from Equation (40), we can write:\n\u2211\nij\n\u03c9ij = F\u0304k \u00b7 \u03c6 (k) 00 + F\u0304k \u00b7 \u03c6 (k) 01 + Fk \u00b7 \u03c6 (k) 11 + Fk \u00b7 \u03c6 (k) 11 = F\u0304k + Fk = 1 (41)\nLet us also note that \u03b7(k) is only apparently not defined when \u03c8 (j\u22121) 01 \u2261 0, for some j > 1. For instance, assuming that an index i exists such that \u03b3 (i) 01 = 0, we have\n\u2200j > i : \u03c8 (j\u22121) 01 = 0, which in turn implies that \u2200j > i : \u03b7 (j) = \u221e. However, \u03c9 (k) 01 (and thus \u03c9 (k) 00 ) is still defined for any k \u2265 0, as:\n\u03c9 (k) 01 = lim\n\u03b3 (i) 01 \u21920\nF\u0304k \u00b7 \u03b7 (k) \u00b7 \u03c8 (k) 01 \u2261\nk\u2211\nj=1\nf\u0304j \u00b7 Fj\u22121 \u00b7 \u03c8 (k) 11 \u00b7\nk\u220f\ns=j\n\u03b3 (s) 01 =\nk\u2211\nj=i+1\nf\u0304j \u00b7 Fj\u22121 \u00b7 \u03c8 (k) 11 \u00b7\nk\u220f\ns=j\n\u03b3 (s) 01\nHence, the confusion matrix \u039e for a pipeline of classifiers \u03c0, to which m inputs with known conditional probabilities D (with reference to the categories involved in the pipeline) are applied, can always be represented as:\n\u039e\u03c0(D;m) = m \u00b7 \u2126\u03c0(D) = m \u00b7 [ F\u0304 0 0 F ]\n\ufe38 \ufe37\ufe37 \ufe38 O\u03c0(D)\n\u00b7 [ 1\u2212 \u03b7 \u00b7 \u03c801 \u03b7 \u00b7 \u03c801 1\u2212 \u03c811 \u03c811 ]\n\ufe38 \ufe37\ufe37 \ufe38 \u03a6\u03c0(D)\n(42)\nIt is now clear that \u2126 and \u03a6 depend both on the conditional probabilities that characterize the flow of inputs along the pipeline (through \u03b7) and on the characteristics of the involved classifiers (through \u03c801 and \u03c811). However, \u03c801 and \u03c811 depend only on the intrinsic properties of the classifiers involved in a pipeline, and are in fact building blocks for defining \u03a6 and \u2126. In the following subsection, we better analyze this issue."}, {"heading": "4.4 Intrinsic Properties of a Pipeline", "text": "A recursive definition for \u03c801 and \u03c811 (actually, for the matrix \u03a8) can be easily given in terms of the \u201c\u2295\u201d operator, as follows: Definition of \u03a8.\n\u03a8(k) =    \u00b5 k = 0 \u0393(1) k = 1\n\u03a8(k\u22121) \u2295 \u0393(k) k > 1\n(43)\nWhere the choice of reporting the base case with k = 1 has been introduced only for the sake of readability, as it is consistent with the base case with k = 0. In symbols:\n\u03a8(1) = \u03a8(0) \u2295 \u0393(1) = \u00b5\u2295 \u0393(1) = [ 0 0 0 0 ] + [ 1 0 0 1 ] \u00b7 \u0393(1) \u2261 \u0393(1) (44)\nNote that the row-by-row normalization property is preserved also for \u03a8, no matter how many classifiers occur in the pipeline. We can verify it by induction from Equation (43): with \u0393(k) normalized by definition and assuming that \u03a8(k\u22121) is normalized, we only need to verify that \u03a8(k)preserves this property. To show it, let us rewrite Equation (43) as follows (k > 0):\n\u03c8 (k) 00 = \u03c8 (k\u22121) 00 + \u03c8 (k\u22121) 01 \u00b7 \u03b3 (k) 00 \u03c8 (k) 01 = \u03c8 (k\u22121) 01 \u00b7 \u03b3 (k) 01 \u03c8 (k) 10 = \u03c8 (k\u22121) 10 + \u03c8 (k\u22121) 11 \u00b7 \u03b3 (k) 10 \u03c8 (k) 11 = \u03c8 (k\u22121) 11 \u00b7 \u03b3 (k) 11\nSumming up row-by-row:\n\u03c8 (k) 00 + \u03c8 (k) 01 = \u03c8 (k\u22121) 00 + \u03c8 (k\u22121) 01 \u00b7 ( \u03b3 (k) 00 + \u03b3 (k) 01 ) = \u03c8 (k\u22121) 00 + \u03c8 (k\u22121) 01 = 1\n\u03c8 (k) 10 + \u03c8 (k) 11 = \u03c8 (k\u22121) 10 + \u03c8 (k\u22121) 11 \u00b7 ( \u03b3 (k) 10 + \u03b3 (k) 11 ) = \u03c8 (k\u22121) 10 + \u03c8 (k\u22121) 11 = 1\nUnfolding the definition of \u03a8(k) and taking into account the normalization property we can write (k \u2265 0):\n\u03a8(k) =   k\u2211 j=1 \u03b3 (i) 00 \u00b7 j\u22121\u220f i=0 \u03b3 (i) 01\nk\u220f\ni=0\n\u03b3 (i) 01\nk\u2211\nj=1\n\u03b3 (j) 10 \u00b7\nj\u22121\u220f\ni=0\n\u03b3 (i) 11\nk\u220f\ni=0\n\u03b3 (i) 11\n  =   1\u2212 k\u220f j=0 \u03b3 (k) 01\nk\u220f\nj=0\n\u03b3 (k) 01\n1\u2212 k\u220f\nj=0\n\u03b3 (k) 11\nk\u220f\nj=0\n\u03b3 (k) 11\n \n(45)\nIt is worth pointing out that \u03a8, which gives the expected result for \u03c801 and \u03c811, can be seen as a relaxed form of \u03a6, in which \u2013for a pipeline \u03c0k\u2013 all negative inputs are taken outside the domain of c1 whereas all positive inputs are taken inside the domain of ck. This choice can be imposed in the model of \u2126 by setting 0 < f1 < 1 and fj = 1, j = 2, . . . , k (f0 is always equal to 1, by definition), so that \u03b7\n(k), Fk and F\u0304k reduce to 1, f1 and f\u03041, respectively. This implies that no adaption is required for classifiers in the pipeline, except for c\u03021. Under this restrictive hypothesis, \u2126\n(k) reduces to (k > 0):\n\u2126(k) = [ f\u03041 0 0 f1 ]\n\ufe38 \ufe37\ufe37 \ufe38 O(k)\n\u00b7\n[ 1\u2212 \u03c8\n(k) 01 \u03c8 (k) 01\n1\u2212 \u03c8 (k) 11 \u03c8 (k) 11\n]\n\ufe38 \ufe37\ufe37 \ufe38 \u03a6(k)\u2261\u03a8(k)\n(46)\nAs Equation (46) accounts only for the internal structure of the corresponding pipeline, one can hypothesize that \u03a8 is in fact a homomorphism which maps elements from C\u2217 to the space of normalized confusion matrices, say M \u2261 [0, 1]4. In symbols:\n\u03a8 : C\u2217 \u2192 M (47)\nIndeed, given a taxonomy T = \u3008C,\u2264\u3009, it is easy to verify that \u03a8 is a homomorphism, as:\n\u2013 the Kleene star of C yields in fact a monoid, closed under the concatenation operation (denoted with \u201c+\u201d), associative, and whose neutral element is the empty string \u03bb;\n\u2013 the space M of normalized confusion matrices is also a monoid, closed under the \u201c\u2295\u201d operation, associative, and whose neutral element is the neutral classifier \u00b5.\nThis is due to the fact that \u03a8 preserves the structure, with \u201c\u2295\u201d and \u00b5 playing in M the role that \u201c+\u201d and \u03bb play in C\u2217. The interested reader can consult APPENDIX B for further details concerning this issue.\nNote that, although \u03a8 is defined for any string in C\u2217, we are in fact interested in pipelines (see Figure 9). However, as already pointed out, they can be easily identified throughout the characteristic function F : C\u2217 \u2192 [0, 1], which is strictly greater than zero only for well-formed strings, with the additional constraint that, to be pipelines, they must originate from the root.\nMoreover, thanks to the associative property, it is also possible to give constructive definitions for M through \u03a8. In symbols (with c \u2208 C, \u03c0 \u2208 C\u2217, and \u03a8(\u03bb) \u2261 \u00b5):\n\u2013 Right recursion: \u03c0\u2032 = c + \u03c0 \u21d2 \u03a8(\u03c0\u2032) = \u03a8(c + \u03c0) = \u03a8(c)\u2295\u03a8(\u03c0) \u2261 \u0393(c)\u2295\u03a8(\u03c0)\n\u2013 Left recursion: \u03c0\u2032 = \u03c0 + c \u21d2 \u03a8(\u03c0\u2032) = \u03a8(\u03c0 + c) = \u03a8(\u03c0)\u2295\u03a8(c) \u2261 \u03a8(\u03c0)\u2295 \u0393(c)"}, {"heading": "5 Analysis on Relevant Metrics", "text": ""}, {"heading": "5.1 Taxonomic Variations of Ordinary Metrics", "text": "According to the focus of the paper, we propose straightforward definitions for precision, recall, and F1. To differentiate them from other proposals (e.g., hP, hR, and hF1 defined in [20]), they will be denoted as tP, tR, and tF1 \u2013standing for \u201ctaxonomic\u201d P, R, and F1, respectively. Their definition apply to pipelines and strictly follow the probabilistic modeling represented by the \u2126 matrix. As the elements of \u2126(k) can be used to represent the overall transformation performed by \u03c0k, calculating tP, tR, and F1 for a pipeline \u03c0k is now straightforward:\ntP (\u03c0k) = \u03c9 (k) 11\n\u03c9 (k) 11 + \u03c9 (k) 01\n= ( 1 + \u03c9 (k) 01\n\u03c9 (k) 11\n)\u22121 = ( 1 + \u03b7(k) \u00b7\nF\u0304k Fk \u00b7 \u03c8\n(k) 01 \u03c8 (k) 11\n)\u22121 (48)\ntR(\u03c0k) = \u03c9 (k) 11\n\u03c9 (k) 11 + \u03c9 (k) 10\n= Fk \u00b7 \u03c8\n(k) 11\nFk \u00b7 \u03c8 (k) 11 + Fk \u00b7 \u03c8 (k) 10\n= \u03c8 (k) 11 (49)\ntF1(\u03c0k) = 2 \u00b7\n( 1\nP +\n1\nR\n)\u22121 = 2 \u00b7 [( 1 + \u03b7(k) \u00b7\nF\u0304k Fk \u00b7 \u03c8\n(k) 01 \u03c8 (k) 11\n) + ( 1\n\u03c8 (k) 11\n)]\u22121 (50)\nAs for the taxonomic accuracy (tA), although less important for assessing the behavior of classifiers in pipeline (mainly due to the fact that usually the imbalance between\npositive and negative inputs rapidly grows with the depth level of the classifier under analysis), it can be easily defined as well:\ntA(\u03c0k) = \u03c9 (k) 00 + \u03c9 (k) 11\u2211\nij \u03c9 (k) ij\n= tr(\u2126(k)) = F\u0304k \u00b7 ( 1\u2212 \u03b7(k) \u00b7 \u03c8(k)01 ) + Fk \u00b7 \u03c8 (k) 11 (51)\nwhere tr(\u00b7) denotes the trace of a matrix, obtained by summing up the elements of its main diagonal."}, {"heading": "5.2 How Taxonomic Precision, Recall, and F1 change along a pipeline", "text": "To better assess PF, let us analyze how the most relevant metrics change along a pipeline \u03c0. Our analysis proceeds by induction, assuming of having assessed the behavior of \u03c0k\u22121, and then verifying what happens if one adds a further classifier.\n6 Again for the sake of readability, let us distinguish any relevant parameter concerning the pipelines \u03c0k and \u03c0k\u22121 with a \u201cprime\u201d and a \u201cplain\u201d notation, respectively (for instance, F \u2032 denotes Fk, whereas F denotes Fk\u22121).\n\u2013 Precision \u2013 Imposing that tP (\u03c0\u2032)\u2212 tP (\u03c0) \u2265 0, the constraint on tP that involves the relevant parameters of c\u0302k is:\n\u03b3\u203201 \u2264 F\u0304\u03b7\nF\u0304 \u2032\u03b7\u2032 \u00b7 f \u2032 \u00b7 \u03b3\u203211 (52)\nwhere:\nF\u0304 \u2032\u03b7\u2032 =\nk\u2211\nj=1\nf\u0304j \u00b7 Fj\u22121 \u00b7 \u03c8\n(k) 11\n\u03c8 (j\u22121) 01\n= F\u0304\u03b7 + f\u0304 \u2032 \u00b7 F \u00b7 \u03c811 \u03c801 > F\u0304\u03b7\nSummarizing, tP may increase or not along a pipeline depending on the constraint reported in Equation (52), which is strictly related with the behavior of the ratio F\u0304\u03b7/F\u0304 \u2032\u03b7\u2032. Note that the behavior of tP depends on the distribution of data expected to flow along the pipeline.\n\u2013 Recall \u2013 Imposing that tR(\u03c0\u2032) \u2212 tR(\u03c0) \u2265 0, the constraint on tR that involves the relevant parameters of the classifier c\u0302k is:\n\u03b3\u203211 \u2265 1 (53)\nIt is clear that the constraint on tR is satisfied only when \u03b3\u203211 = 1. Hence, tR is monotonically decreasing along a pipeline, the lower \u03b3\u203211 (i.e., the percent of TP) the greater the decrement of R. Note that the behavior of tR does not depend on the distribution of data expected to flow along the pipeline.\n6 As, by definition, P (\u03c00) = R(\u03c00) = F1(\u03c00) \u2261 1, the constraints for \u03c01 are in fact not relevant. For this reason, the analysis concerns only pipelines \u03c0k with k > 1.\n\u2013 F1 \u2013 According to the given definition, tF1 lies in between tP and tR. It is typically decreasing, unless the expected behavior of tR (monotonically decreasing) is more than counterbalanced by an increase of tP ."}, {"heading": "6 Discussion", "text": "Two main questions have been formulated at the beginning of the paper (Section 1), concerning (i) the possibility of predicting the expected behavior of a system implemented in accordance with PF when fed with a corpus of documents whose statistical properties are known and (ii) the possibility of separating the statistical information concerning inputs from the intrinsic properties of the classifiers embedded in the given taxonomy. After focusing on the above questions, the discussion will also summarize the analysis performed on relevant metrics."}, {"heading": "6.1 Predicting the Expected behavior of a PF System", "text": "We have shown that it is very difficult to analyze a taxonomy as a whole, also due to the number of variants that can be put into practice while trying to enforce the hierarchical consistency requirement. Rather, it becomes surprisingly easy upon the extraction of the corresponding set of pipelines.\nAs the process of unfolding a taxonomy can be put into practice in many different scenarios, the analysis in terms of pipelines is apparently a common step for any LCN approach, including PF. Indeed, the unfolding process does not require specific constraints to be satisfied by the problem in hand. In particular, it can be performed in presence of (i) trees or DAGs, (ii) non-overlapping or overlapping among (the domains of) categories, and (iii) mandatory or non-mandatory leaf-node prediction.\nStarting from the assumption that the confusion matrix measured after performing an experiment with a pipeline \u03c0 = c0c1 . . . cL is in fact a single realization of a probabilistic process, the following equation holds (see \u00a7 4.3):\n\u039e\u03c0(D;m) = m \u00b7 \u2126\u03c0(D) \u2248 m \u00b7 p(XL, X\u0302L) (54)\nwhere \u2126\u03c0(D) accounts for the behavior of \u03c0 from a probabilistic perspective, as it is an estimation of the joint probability p(XL, X\u0302L). An effective procedure for evaluating \u2126 has been given, according to the knowledge about the behavior of the classifiers embedded by the pipeline, represented by their normalized confusion matrices \u0393(ck), k = 0, 1, . . . , L, and about the expected distribution of inputs. Summarizing, the answer to the first question is positive, as one can easily use the analysis performed in terms of pipelines to predict the behavior of a hierarchical system compliant with PF. Note that the analysis can be performed only when the distribution of data is known, otherwise the model will not approximate well the real-world. However, for large scale data, e.g., web applications that process user queries, the hypothesis of knowing the distribution of data is not difficult to fulfill."}, {"heading": "6.2 Separating the Statistical Information Concerning Inputs", "text": "from the Intrinsic Properties of Classifiers\nWe have shown (\u00a7 4.3) that \u2126\u03c0(D) can be represented as the product between O\u03c0(D) and \u03a6\u03c0(D). Considering that \u2126\u03c0(D) approximates the joint probability p(XL, X\u0302L), we can write:\np(XL, X\u0302L) \u2248 \u2126\u03c0(D) = O\u03c0(D) \u00b7 \u03a6\u03c0(D) (55)\nwhere O\u03c0(D) \u2248 p(XL) denotes the behavior of a pipeline under the hypothesis that all classifiers it embeds were acting as oracles, whereas \u03a6\u03c0(D) \u2248 p(X\u0302L|XL) represents the expected deterioration. We have pointed out that the \u03a6 plays for pipelines the role that \u0393 plays for single classifiers. However, although the property of row-by-row normalization is satisfied for both \u03a6 and \u0393, \u03a6 still depends on the distribution of input data while \u0393 does not. Fortunately, some building blocks have been identified, characterized by the \u03a8 matrix, whose elements depend only on the intrinsic properties of the pipeline. The dependence of \u03a6 from \u03a8 is highlighted by the following formula, which is very important in the process of pipeline analysis:\n\u2126\u03c0(D) = [ F\u0304 0 0 F ]\n\ufe38 \ufe37\ufe37 \ufe38 O\u03c0(D)\n\u00b7 [ 1\u2212 \u03b7 \u00b7 \u03c801 \u03b7 \u00b7 \u03c801 1\u2212 \u03c811 \u03c811 ]\n\ufe38 \ufe37\ufe37 \ufe38 \u03a6\u03c0(D)\n(56)\nNote that, due to its independence from the distribution of data, the task of calculating \u03a8 can be done once, and requires to be repeated only in the event that the properties of one or more classifiers in the pipeline change. Summarizing, the answer here is only partially positive, as the approximated model represented by \u2126 cannot be expressed in a way that clearly separates the distribution of input data (through O) from the intrinsic behavior of the pipeline (through \u03a6). In fact, \u03a6 still embeds the information about the distribution of input data. However, one can calculate \u03a8(\u03c0) for each pipeline \u03c0 \u2208 PT , starting from the normalized confusion matrices \u0393 of the classifiers embedded by \u03c0. The \u03a8 matrices are independent from the labeling of the input data, and can be used, together with the set of conditional probabilities that characterize the inclusion relations for the given pipeline, to calculate the normalized confusion matrix of the pipeline (i.e., \u03a6) and therefore the approximated model (i.e., \u2126).\nAs a noticeable consequence of Equation (56), testing the behavior of a pipeline for a specific value of imbalance is not straightforward. The motivation lies in the fact that the same imbalance can be obtained with many different distributions of inputs. To better highlight this issue, let us assume that one wants to measure the behavior of a pipeline in presence of 10% of positive vs. 90% of negative inputs. Positive inputs refer to the last classifier in the pipeline, and their amount is fixed (in this case, 10%). On the other hand, negative inputs can be selected in a variety of ways along the pipeline. For instance, one may select only inputs that do not belong to any category but the root (which by hypothesis accepts all inputs).7 Another peculiar policy may consist of\nselecting as negative inputs only those that belong to the last but one classifier in the pipeline. However, the above policies for negative input selection are not representative enough for identifying the behavior of a pipeline in presence of imbalance. In fact, many other selection policies are feasible, provided that the constraint on imbalance is satisfied. Summarizing, while the problem of setting up test beds with statistical significance remains (no matter whether the corresponding tests are performed with a single run, averaging over multiple runs, or resorting to k -fold cross validation), a further problem arises for pipeline testing, as its behavior depends on the distribution of inputs being processed. Hence, studying the imbalance requires at least an averaging over multiple test, each run with a different distribution of (negative) inputs."}, {"heading": "6.3 Analysis Performed on Relevant Metrics", "text": "The analysis performed on relevant metrics (i.e., tP , tR, tF1) highlights that tP depends on the distribution of data while tR does not. As for tR, we have shown that it is monotonically decreasing. This phenomenon is related with the problem of high-level error recovering, which originates from the fact that errors made at higher levels of a taxonomy have great impact on the overall performance of any actual system that implements top-down processing (including those based on PF). The impact of high-level errors on the overall performance of a system can be better understood recalling the concepts of inner and outer path: the former is entrusted with performing progressive filtering, whereas the latter accumulates inputs that have been rejected by any of the classifiers embedded by the pipeline. For this reason, there is no way to recover errors performed along the outer path (FN), while errors performed by a classifier along the inner path (FP) may be recovered by subsequent processing steps. This behavior is also highlighted by the study made on relevant metrics, where the recall (related to FN) is monotonically decreasing, whereas the precision (related to FP) may be increasing or not depending on the characteristics of the involved classifiers. A simple strategy headed to limit the impact of high-level errors can be put into practice by lowering the thresholds of the embedded classifiers, the closer the classifier to the root, the lower the threshold. In so doing, FN are expected to decrease while FP are expected to increase. However, FP can be further processed by the classifiers that occur after the current one in the given pipeline, thus literally realizing \u201cprogressive filtering\u201d. This strategy affects also the training of classifiers, which are required to maintain the same discrimination capabilities on relevant and non relevant inputs that originate from their ancestors (see the definition of relevant input given in Section 3). The main consequence of relaxing the behavior of c\u0302k\u22121 (more generally, of the pipeline \u03c0k\u22121) is that the set of relevant inputs for c\u0302k is extended with FP that originate from c\u0302k\u22121 and its ancestors. Hence, the training activity should be performed taking into account this issue, with the goal of improving the robustness of c\u0302k towards non-relevant FP.\n7This issue has already been described in \u00a7 4.4, while defining the \u03a8 matrix."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, a formal modeling of the progressive filtering technique has been performed, according to a probabilistic perspective and framed within the research field of hierarchical text categorization. In particular, the focus has been on how to assess pipelines extracted from a given taxonomy. Specific care has been taken in identifying whether some building blocks exist in the model, which are independent from the underlying distribution of input data. This part of the analysis has brought to the definition of the \u03a8 matrix, which accounts only for the structural aspects of a pipeline. How to separate the expected optimal behavior of a pipeline from the deterioration introduced by the actual classifiers it embeds is another important result. The way relevant metrics change along a pipeline has also been investigated. As expected, the precision may increase or decrease depending on the characteristics of the embedded classifiers, whereas the recall is monotonically decreasing along a pipeline. To limit the impact of this latter unwanted behavior, one may relax the behavior of classifiers at higher levels, thus reducing the overall percent of FN. The results of the analysis performed in this paper should facilitate the designer of a system based on progressive filtering in the task of devising, training and testing it. In particular, some relevant scenarios have been sketched in Section 1, in which the proposed probabilistic model can be useful.\nAs for future work, we are currently investigating the problem of which policy should be applied to train classifiers embedded in a taxonomy. Moreover, we are about to use the model in a problem of threshold optimization.\nAcknowledgements. Many thanks to all people that gave me help in the task of conceptualizing and formalizing this work. Special thanks go to Eloisa Vargiu, who first conjectured the possibility that part of the FN disregarded by a classifier should be turned into TN, and to Mauro Parodi, for his wealth of ideas about linear transformations and their application to model text categorization tasks in a hierarchical setting. Many thanks also to Donato Malerba, Giorgio Valentini, and Fabio Roli, who made a preliminary review of the manuscript."}, {"heading": "Appendix B. Deriving the \u03a8 Homomorphism", "text": "Given a taxonomy T = \u3008C,\u2264\u3009, it is well known that the closure of C under the Kleene star (i.e., C\u2217) yields a monoid, with:\n1. Closure (wrt the operator \u201c+\u201d): \u2200\u03c01, \u03c02 \u2208 C \u2217 : \u03c01 + \u03c02 \u2208 C \u2217\n2. Associativity (wrt the operator \u201c+\u201d): \u2200\u03c01, \u03c02, \u03c03 \u2208 C \u2217 : (\u03c01 + \u03c02) + \u03c03 = \u03c01 + (\u03c02 + \u03c03)\n3. Neutral element (empty string \u03bb): \u2200\u03c0 \u2208 C\u2217 : \u03c0 + \u03bb = \u03bb+ \u03c0 = \u03c0\nIn the event that \u03a8 is a homomorphism, also the set of normalized confusion matrices M \u2261 [0, 1]4 is a monoid, as a homomorphism is expected to preserve the structure while mapping C\u2217 to M. Let us verify that \u03a8 is a homomorphism by checking whether the space M is a monoid, with \u201c+\u2032\u2032 \u2192 \u201c\u2295\u2032\u2032 and \u03bb \u2192 \u00b5:\n1. Closure (wrt the operator \u201c\u2295\u201d): \u2200a, b \u2208 M : a\u2295 b \u2208 M\n2. Associativity (wrt the operator \u201c\u2295\u201d): \u2200a, b, c \u2208 M : (a\u2295 b)\u2295 c = a\u2295 (b\u2295 c)\n3. Neutral element (neutral classifier \u00b5): \u2200a \u2208 C\u2217 : a\u2295 \u00b5 = \u00b5\u2295 a = a\nProof.\n1. Closure under \u201c\u2295\u201d: \u03b1, \u03b2 \u2208 M \u21d2 \u03b1\u2295 \u03b2 \u2208 M\n\u03b1\u2295 \u03b2 = [ \u03b100 + \u03b101 \u00b7 \u03b200 \u03b101 \u00b7 \u03b201 \u03b110 + \u03b111 \u00b7 \u03b210 \u03b111 \u00b7 \u03b211 ]\nwhere\n0 \u2264 (\u03b1\u2295 \u03b2)00 \u2264 \u03b100 + \u03b101 = 1, 0 \u2264 (\u03b1\u2295 \u03b2)01 \u2264 \u03b101 \u2264 1 0 \u2264 (\u03b1\u2295 \u03b2)10 \u2264 \u03b110 + \u03b111 = 1, 0 \u2264 (\u03b1\u2295 \u03b2)11 \u2264 \u03b111 \u2264 1\nMoreover:\n(\u03b1\u2295 \u03b2)00 + (\u03b1\u2295 \u03b2)01 = (\u03b100 + \u03b101 \u00b7 \u03b200) + \u03b101 \u00b7 \u03b201 = \u03b100 + \u03b101 \u00b7 (\u03b200 + \u03b201) = 1 (\u03b1\u2295 \u03b2)10 + (\u03b1\u2295 \u03b2)11 = (\u03b110 + \u03b111 \u00b7 \u03b210) + \u03b111 \u00b7 \u03b211 = \u03b110 + \u03b111 \u00b7 (\u03b210 + \u03b211) = 1\n2. Associativity under \u201c\u2295\u201d: (\u03b1\u2295 \u03b2)\u2295 \u03b3 = \u03b1\u2295 (\u03b2 \u2295 \u03b3)\n(\u03b1\u2295 \u03b2)\u2295 \u03b3 = [ \u03b100 + \u03b101 \u00b7 \u03b200 \u03b101 \u00b7 \u03b201 \u03b110 + \u03b111 \u00b7 \u03b210 \u03b111 \u00b7 \u03b211 ] \u2295 [ \u03b300 \u03b301 \u03b310 \u03b311 ]\n= [ \u03b100 + \u03b101 \u00b7 \u03b200 0 \u03b110 + \u03b111 \u00b7 \u03b210 0 ] + [ \u03b101 \u00b7 \u03b201 0 0 \u03b111 \u00b7 \u03b211 ] \u00b7 [ \u03b300 \u03b301 \u03b310 \u03b311 ]\n= [ \u03b100 + \u03b101 \u00b7 \u03b200 + \u03b101 \u00b7 \u03b201 \u00b7 \u03b300 \u03b101 \u00b7 \u03b201 \u00b7 \u03b301 \u03b110 + \u03b111 \u00b7 \u03b210 + \u03b111 \u00b7 \u03b211 \u00b7 \u03b310 \u03b111 \u00b7 \u03b211 \u00b7 \u03b311 ]\n\u03b1\u2295 (\u03b2 \u2295 \u03b3) = [ \u03b100 \u03b101 \u03b110 \u03b111 ] \u2295 [ \u03b200 + \u03b201 \u00b7 \u03b300 \u03b201 \u00b7 \u03b301 \u03b210 + \u03b211 \u00b7 \u03b310 \u03b211 \u00b7 \u03b311 ]\n= [ \u03b100 0 \u03b110 0 ] + [ \u03b101 0 0 \u03b111 ] \u00b7 [ \u03b200 + \u03b201 \u00b7 \u03b300 \u03b201 \u00b7 \u03b301 \u03b210 + \u03b211 \u00b7 \u03b310 \u03b211 \u00b7 \u03b311 ]\n= [ \u03b100 + \u03b101 \u00b7 \u03b200 + \u03b101 \u00b7 \u03b201 \u00b7 \u03b300 \u03b101 \u00b7 \u03b201 \u00b7 \u03b301 \u03b110 + \u03b111 \u00b7 \u03b210 + \u03b111 \u00b7 \u03b211 \u00b7 \u03b310 \u03b111 \u00b7 \u03b211 \u00b7 \u03b311 ]\n3. Neutral element \u00b5: \u03b1 \u2208 M \u21d2 \u03b1\u2295 \u00b5 = \u00b5\u2295 \u03b1 \u2261 \u03b1, with \u00b5 = [ 0 1 0 1 ]\nThe neutral element \u00b5 corresponds to a classifier that accepts and passes down all its input data (i.e., FP and TP). It is easy to verify that this property holds for the choice made about \u00b5:\n\u03b1\u2295 \u00b5 = [ \u03b100 \u03b101 \u03b110 \u03b111 ] \u2295 [ 0 1 0 1 ] = [ \u03b100 0 \u03b110 0 ] + [ \u03b101 0 0 \u03b111 ] \u00b7 [ 0 1 0 1 ] \u2261 \u03b1\n\u00b5\u2295 \u03b1 = [ 0 1 0 1 ] \u2295 [ \u03b100 \u03b101 \u03b110 \u03b111 ] = [ 0 0 0 0 ] + [ 1 0 0 1 ] \u00b7 [ \u03b100 \u03b101 \u03b110 \u03b111 ] \u2261 \u03b1"}], "references": [{"title": "Assessing Progressive Filtering to Perform Hierarchical Text Categorization in Presence of Input Imbalance", "author": ["A. Addis", "G. Armano", "E. Vargiu"], "venue": "Proceedings of International Conference on Knowledge Discovery and Information Retrieval (KDIR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Hierarchical multi-label prediction of gene", "author": ["Z. Barutcuoglu", "R.E. Schapire", "O.G. Troyanskaya"], "venue": "function, Bioinformatics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Refined experts: improving classification in large taxonomies, SIGIR \u201909", "author": ["P.N. Bennett", "N. Nguyen"], "venue": "Proceedings of the 32nd international ACM SIGIR con- 30  ference on Research and development in information retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Hierarchical document categorization with support vector machines, CIKM", "author": ["L. Cai", "T. Hofmann"], "venue": "Proceedings of the thirteenth ACM international conference on Information and knowledge management,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Hierarchical Text Categorization in a Transductive Setting", "author": ["M. Ceci"], "venue": "ICDMW \u201908: Proceedings of the 2008 IEEE International Conference on Data Mining Workshops,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Classifying web documents in a hierarchy of categories: a comprehensive study", "author": ["M. Ceci", "D. Malerba"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Using Taxonomy, Discriminants, and Signatures for Navigating in Text Databases", "author": ["S. Chakrabarti", "B. Dom", "R. Agrawal", "P. Raghavan"], "venue": "Proceedings of the 23rd VLDB Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Hierarchical Classification of Documents with Error Control, PAKDD 2001 ", "author": ["C. Cheng", "J. Tang", "A. Fu", "I. King"], "venue": "Proceedings of 5th Pacific-Asia Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Predicting gene function in Saccharomyces cerevisiae, Bioinformatics, 7(19 (suppl", "author": ["A. Clare", "R. King"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The Effect of Using Hierarchical Classifiers in Text Categorization", "author": ["S. D\u2019Alessio", "K. Murray", "R. Schiaffino"], "venue": "Proceedings of of the 6th International Conference on Recherche d\u2019Information Assistee par Ordinateur (RIAO),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Large Margin Hierarchical Classification", "author": ["O. Dekel", "J. Keshet", "Y. Singer"], "venue": "Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Hierarchical classification of Web content", "author": ["S.T. Dumais", "H. Chen"], "venue": "Proceedings of SIGIR-00,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "A Neural Network Approach to Topic Spotting", "author": ["Erik Wiener", "A.S.W. Jan O. Pedersen"], "venue": "Proceedings of 4th Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Boosting multi-label hierarchical text categorization", "author": ["A. Esuli", "T. Fagni", "F. Sebastiani"], "venue": "Information Retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Categorizing Web Documents in Hierarchical Catalogues", "author": ["I. Frommholz"], "venue": "Proceedings of ECIR-01,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Calculating the singular values and pseudo-inverse of a matrix", "author": ["G.H. Golub", "W. Kahan"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1965}, {"title": "Probe, count, and classify: categorizing hidden web databases", "author": ["P. Ipeirotis", "L. Gravano", "M. Sahami"], "venue": "SIGMOD Rec.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Automatic Hierarchical Email Classification Using Association Rules", "author": ["J. Itskevitch"], "venue": "Ph.D. Thesis,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Hierarchical text categorization and its application to bioinformatics", "author": ["S. Kiritchenko"], "venue": "Ph.D. Thesis, Univ. of Ottawa,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Hierarchically classifying documents using very few words", "author": ["D. Koller", "M. Sahami"], "venue": "Proceedings of ICML-97, 14th International Conference on Machine Learning (D", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Using support vector machines for classifying large sets of multi-represented objects", "author": ["H. Kriegel", "P. Kroger", "A. Pryakhin", "M. Schubert"], "venue": "Proc. of the SIAM Int. Conference on Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Chi2: Feature selection and discretization of numeric attributes", "author": ["H. Liu", "R. Setiono"], "venue": "Proc. IEEE 7th International Conference on Tools with Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Improving text classification by shrinkage in a hierarchy of classes, Proceedings of ICML-98", "author": ["A.K. McCallum", "R. Rosenfeld", "T.M. Mitchell", "A.Y. Ng"], "venue": "15th International Conference on Machine Learning (J. W", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Feature selection, perceptron learning, and a usability case study for text categorization", "author": ["H.T. Ng", "W.B. Goh", "K.L. Low"], "venue": "Proceedings of SIGIR-97,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Hierarchical Text Categorization", "author": ["M.E. Ruiz", "P. Srinivasan"], "venue": "Using Neural Networks, Information Retrieval,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "A survey of hierarchical classification across different application domains", "author": ["C.J. Silla", "A. Freitas"], "venue": "Journal of Data Mining and Knowledge Discovery,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Hierarchical Text Classification and Evaluation, ICDM \u201901", "author": ["A. Sun", "E. Lim"], "venue": "Proceedings of the 2001 IEEE International Conference on Data Mining, IEEE Computer Society,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Blocking reduction strategies in hierarchical text classification", "author": ["A. Sun", "E. Lim", "W. Ng", "J. Srivastava"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Support vector machine learning for interdependent and structured output spaces, ICML \u201904", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "True path Rule Hierarchical Ensembles for Genome-Wide Gene Prediction", "author": ["G. Valentini"], "venue": "IEEE/ACM Trans. on Comp. Biology and Bioinformatics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Hierarchical Classification of Real Life Documents", "author": ["K. Wang", "K.W. Senqiang"], "venue": "Proc. of the 1st SIAM Int. Conference on Data Mining,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Building Hierarchical Classifiers Using Class Proximity", "author": ["K. Wang", "S. Zhou", "S. Liew"], "venue": "Proceedings of the 25th VLDB Conference,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Exploiting Hierarchy in Text Categorization", "author": ["A.S. Weigend", "E.D. Wiener", "J.O. Pedersen"], "venue": "Information Retrieval,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Strategies for Minimising Errors in Hierarchical Web Categorisation", "author": ["W. Wibowo", "H. Williams"], "venue": "Proceedings of the International Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Simple and accurate feature selection for hierarchical categorisation, DocEng", "author": ["W. Wibowo", "H.E. Williams"], "venue": "Proceedings of the 2002 ACM symposium on Document engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "The simplest scenario consists of focusing on the acceptance threshold, which typically falls in the range [0, 1].", "startOffset": 107, "endOffset": 113}, {"referenceID": 19, "context": "Probabilistic Machines In [20], all local approaches that rely on a sequence of top-down decisions take the esoteric name of pachinko machine, as they resemble to some extent the corresponding Japanese game.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 179, "endOffset": 183}, {"referenceID": 25, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 219, "endOffset": 223}, {"referenceID": 28, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 256, "endOffset": 260}, {"referenceID": 4, "context": "This approach has been widely used with different learning algorithms and techniques: linear classifiers [25], [10], probabilistic classifiers [21], decision rules [18], boosting [15], artificial neural networks (ANNs) [26], support vector machines (SVMs) [29], and in a transductive setting [5].", "startOffset": 292, "endOffset": 295}, {"referenceID": 19, "context": "Moreover, in [20], an extended version of the Pachinko-machine approach is proposed, adding the ability to terminate the categorization process at any intermediate level of the hierarchy.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "This approach has been used in combination with probabilistic classifiers, [7], with ANNs [14], [35], and with SVMs [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "This approach has been used in combination with probabilistic classifiers, [7], with ANNs [14], [35], and with SVMs [2].", "startOffset": 90, "endOffset": 94}, {"referenceID": 34, "context": "This approach has been used in combination with probabilistic classifiers, [7], with ANNs [14], [35], and with SVMs [2].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "This approach has been used in combination with probabilistic classifiers, [7], with ANNs [14], [35], and with SVMs [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 12, "context": ", Pachinko machine and probabilistic, and found no difference in performance, [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "2 Mapping Between Classifiers and the Underlying Taxonomy According to the survey paper of [28], a hierarchical approach is better understood when described from two dimensions, i.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "[10], [13], and [29] are the first proposals in which sequential Boolean decisions are applied in combination with local classifiers per node.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[10], [13], and [29] are the first proposals in which sequential Boolean decisions are applied in combination with local classifiers per node.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "[10], [13], and [29] are the first proposals in which sequential Boolean decisions are applied in combination with local classifiers per node.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "In [1], the underlying taxonomy is scattered on the corresponding set of admissible paths which originate from the root (called pipelines).", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "In the seminal work by [21], a document to be classified proceeds top-down along the given taxonomy, each classifier being used to decide to which subtree(s) the document should be sent to, until one or more leaves of the taxonomy are reached.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": ", [25], [10],[36], and [26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": ", [25], [10],[36], and [26].", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": ", [25], [10],[36], and [26].", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": ", [25], [10],[36], and [26].", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Among the proposals adopting this approach, let us recall [22] and [9].", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "Among the proposals adopting this approach, let us recall [22] and [9].", "startOffset": 67, "endOffset": 70}, {"referenceID": 33, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": ", [34], [33], [19], [12], [31], [4], and [20].", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "According to [29], training systems with a global approach is computationally heavy, as they typically do not exploit different sets of features at different hierarchical levels, and are not flexible, as a classifier must be retrained each time the hierarchical structure changes.", "startOffset": 13, "endOffset": 17}, {"referenceID": 34, "context": "Features can be selected according to a global or a local approach (a comparison between the two approaches can be found in [35]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "This solution is normally adopted in monolithic systems, where only one classifier is entrusted with distinguishing among all categories in a taxonomy [16, 19].", "startOffset": 151, "endOffset": 159}, {"referenceID": 18, "context": "This solution is normally adopted in monolithic systems, where only one classifier is entrusted with distinguishing among all categories in a taxonomy [16, 19].", "startOffset": 151, "endOffset": 159}, {"referenceID": 35, "context": "Variations on this theme can be found in [36] and in [24].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Variations on this theme can be found in [36] and in [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 35, "context": ", [36].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "In a more recent work, [15] suggest that feature selection should pay attention to the topology of the classification scheme.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Among other approaches to feature selection, let us recall [23], based on \u03c7-square feature evaluation.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "As for feature reduction, latent semantic indexing [11] is the most commonly used technique.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Based on singular value decomposition [17], it implements the principle that words used in the same contexts tend to have similar meanings.", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "As for training strategies, according to [6], training sets can be hierarchical or proper.", "startOffset": 41, "endOffset": 44}, {"referenceID": 26, "context": "As our work will focus mainly on HTC, let us summarize the basic concepts and the issues considered most relevant to this research field (see also [27] and [20]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "As our work will focus mainly on HTC, let us summarize the basic concepts and the issues considered most relevant to this research field (see also [27] and [20]).", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "The characteristic function f : C \u00d7C \u2192 [0, 1] for the covering relation \u201c\u227a\u201d is defined as:", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "The definition of pipeline relies upon the concept of well-formed string, which in turn can be defined through the corresponding characteristic function F : C \u2192 [0, 1]:", "startOffset": 161, "endOffset": 167}, {"referenceID": 28, "context": ", [29].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "Some strategies to avoid blocking are discussed, for instance, in [30].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The interested reader will find several proposals aimed at tackling this issue in [8] and in [37].", "startOffset": 82, "endOffset": 85}, {"referenceID": 36, "context": "The interested reader will find several proposals aimed at tackling this issue in [8] and in [37].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The simplest solution to this problem, when classifiers are framed in a taxonomy, consists of independently optimizing pipelines of binary local classifiers, in which the same classifier is allowed to have different thresholds, depending on which pipeline it is embedded by, [1].", "startOffset": 275, "endOffset": 278}, {"referenceID": 2, "context": "PF can give rise to many other kinds of actual systems, depending on the given class of problems, on the choices made by the algorithm devised to solve them, and on the specific policies adopted to deal with the most well-known issues (see, for instance, [3]) encountered while trying to enforce the hierarchical consistency requirement.", "startOffset": 255, "endOffset": 258}, {"referenceID": 31, "context": "However, due to the embedding of classifiers, some tautological implications imposed by the underlying taxonomy hold for k > 0 (see also the concept of \u201cTrue Path Rule\u201d in [32]):", "startOffset": 172, "endOffset": 176}, {"referenceID": 0, "context": "\u03a9 = [ 0 0 0 1 ] (24)", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "\u0393 = [ 0 1 0 1 ] \u25b3 = \u03bc (25)", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "\u0393 = [ 0 1 0 1 ] \u25b3 = \u03bc (25)", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "\u03a9 = \u03c7 \u2295 \u0393 = ([ 1 f\u03040 0 f0 ] \u00b7 [ 0 0 0 1 ]) \u2295 \u0393 = [ 0 0 0 1 ] \u2295 \u03bc = [ 0 0 0 1 ] (32)", "startOffset": 30, "endOffset": 41}, {"referenceID": 0, "context": "\u03a9 = \u03c7 \u2295 \u0393 = ([ 1 f\u03040 0 f0 ] \u00b7 [ 0 0 0 1 ]) \u2295 \u0393 = [ 0 0 0 1 ] \u2295 \u03bc = [ 0 0 0 1 ] (32)", "startOffset": 49, "endOffset": 60}, {"referenceID": 0, "context": "\u03a9 = \u03c7 \u2295 \u0393 = ([ 1 f\u03040 0 f0 ] \u00b7 [ 0 0 0 1 ]) \u2295 \u0393 = [ 0 0 0 1 ] \u2295 \u03bc = [ 0 0 0 1 ] (32)", "startOffset": 67, "endOffset": 78}, {"referenceID": 0, "context": "\u03a9 = [ 0 0 0 1 ] (33)", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "\u03a8 = \u03a8 \u2295 \u0393 = \u03bc\u2295 \u0393 = [ 0 0 0 0 ] + [ 1 0 0 1 ] \u00b7 \u0393 \u2261 \u0393 (44)", "startOffset": 33, "endOffset": 44}, {"referenceID": 0, "context": "\u03a8 = \u03a8 \u2295 \u0393 = \u03bc\u2295 \u0393 = [ 0 0 0 0 ] + [ 1 0 0 1 ] \u00b7 \u0393 \u2261 \u0393 (44)", "startOffset": 33, "endOffset": 44}, {"referenceID": 0, "context": "As Equation (46) accounts only for the internal structure of the corresponding pipeline, one can hypothesize that \u03a8 is in fact a homomorphism which maps elements from C to the space of normalized confusion matrices, say M \u2261 [0, 1].", "startOffset": 224, "endOffset": 230}, {"referenceID": 0, "context": "However, as already pointed out, they can be easily identified throughout the characteristic function F : C \u2192 [0, 1], which is strictly greater than zero only for well-formed strings, with the additional constraint that, to be pipelines, they must originate from the root.", "startOffset": 110, "endOffset": 116}, {"referenceID": 19, "context": ", hP, hR, and hF1 defined in [20]), they will be denoted as tP, tR, and tF1 \u2013standing for \u201ctaxonomic\u201d P, R, and F1, respectively.", "startOffset": 29, "endOffset": 33}], "year": 2016, "abstractText": "Progressive filtering is a simple way to perform hierarchical classification, inspired by the behavior that most humans put into practice while attempting to categorize an item according to an underlying taxonomy. Each node of the taxonomy being associated with a different category, one may visualize the categorization process by looking at the item going downwards through all the nodes that accept it as belonging to the corresponding category. This paper is aimed at modeling the progressive filtering technique from a probabilistic perspective, in a hierarchical text categorization setting. As a result, the designer of a system based on progressive filtering should be facilitated in the task of devising, training, and testing it.", "creator": "LaTeX with hyperref package"}}}