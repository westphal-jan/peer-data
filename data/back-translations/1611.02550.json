{"id": "1611.02550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "abstract": "Acoustic word embeddings -- fixed vector representations of variably long spoken word segments -- are increasingly being considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned in a discriminatory manner, so that they are similar for language segments that correspond to the same word, while they are dissimilar for segments that correspond to different words. Recent work has shown that acoustic word embeddings can outperform dynamic time warping in query-by-example search and related word discrimination tasks. However, the space for embedding models and training approaches is still relatively unexplored. In this paper, we present new discriminatory embeddings based on recurring neural networks (RNNs). We consider training losses that have been successful in previous work, in particular an entropy loss for word classification and a contrast loss that explicitly aims to differentiate word embeddings and word parameters in a \"pasitic network.\"", "histories": [["v1", "Tue, 8 Nov 2016 15:13:19 GMT  (328kb,D)", "http://arxiv.org/abs/1611.02550v1", "To appear at SLT 2016"]], "COMMENTS": "To appear at SLT 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shane settle", "karen livescu"], "accepted": false, "id": "1611.02550"}, "pdf": {"name": "1611.02550.pdf", "metadata": {"source": "CRF", "title": "DISCRIMINATIVE ACOUSTIC WORD EMBEDDINGS: RECURRENT NEURAL NETWORK-BASED APPROACHES", "authors": ["Shane Settle", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014 acoustic word embeddings, recurrent neural networks, Siamese networks\n1. INTRODUCTION\nMany speech processing tasks \u2013 such as automatic speech recognition or spoken term detection \u2013 hinge on associating segments of speech signals with word labels. In most systems developed for such tasks, words are broken down into subword units such as phones, and models are built for the individual units. An alternative, which has been considered by some researchers, is to consider each entire word segment as a single unit, without assigning parts of it to sub-word units. One motivation for the use of whole-word approaches is that\nThis research was supported by a Google faculty research award and NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.\nthey avoid the need for sub-word models. This is helpful since, despite decades of work on sub-word modeling [1, 2], it still poses significant challenges. For example, speech processing systems are still hampered by differences in conversational pronunciations [3]. A second motivation is that considering whole words at once allows us to consider a more flexible set of features and reason over longer time spans.\nWhole-word approaches typically involve, at some level, template matching. For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word. In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9]. In other words, wholeword approaches often boil down to making decisions about whether two segments are examples of the same word or not.\nAn alternative to DTW that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector representations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors.\nThere has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17]. In this paper we explore new embedding models based on recurrent neural networks (RNNs), applied to a word discrimination task related to query-by-example search. RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences. We compare several types of RNN-based embeddings and analyze their properties. Compared to prior embeddings tested on the same task, our best models achieve sizable improvements in average precision.\nar X\niv :1\n61 1.\n02 55\n0v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\n6\n2. RELATED WORK\nWe next briefly describe the most closely related prior work. Maas et al. [10] and Bengio and Heigold [11] used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition. Maas et al. trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to define feature functions in a segmental conditional random field [18] rescoring system. Bengio and Heigold also developed CNN-based embeddings for lattice rescoring, but with a contrastive loss to separate embeddings of a given word from embeddings of other words.\nLevin et al. [12] developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of reference word segments. This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20]. Voinea et al. [16] developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification.\nKamper et al. [14] compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. [12] as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data [12, 21, 22, 23]. For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs.\nThe only prior work of which we are aware using RNNs for acoustic word embeddings is that of Chen et al. [17] and Chung et al. [15]. Chen et al. learned a long short-term memory (LSTM) RNN for word classification and used the resulting hidden state vectors as a word embedding in a queryby-example task. The setting was quite specific, however, with a small number of queries and speaker-dependent training. Chung et al. [15] worked in an unsupervised setting and trained single-layer RNN autoencoders to produce embeddings for a word discrimination task. In this paper we focus on the supervised setting, and compare a variety of RNNbased structures trained with different losses.\n3. APPROACH An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, X = {xt}Tt=1, where each xt is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, g(X). The basic embedding model structure we use is\nshown in Fig. 1. The model consists of a deep RNN with some number S of stacked layers, whose final hidden state vector is passed as input to a set of F of fully connected layers; the output of the final fully connected layer is the embedding g(X).\nThe RNN hidden state at each time frame can be viewed as a representation of the input seen thus far, and its value in the last time frame T could itself serve as the final word embedding. The fully connected layers are added to account for the fact that some additional transformation may improve the representation. For example, the hidden state may need to be larger than the desired word embedding dimension, in order to be able to \u201dremember\u201d all of the needed intermediate information. Some of that information may not be needed in the final embedding. In addition, the information maintained in the hidden state may not necessarily be discriminative; some additional linear or non-linear transformation may help to learn a discriminative embedding.\nWithin this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Recurrent Unit (GRU) networks [25]. These are both types of RNNs that include a mechanism for selectively retaining or discarding information at each time frame when updating the hidden state, in order to better utilize long-term context. Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].\nIn an LSTM RNN, at each time frame both the hidden state ht and an associated \u201ccell memory\u201d vector ct, are updated and passed on to the next time frame. In other words, each forward edge in Figure 1 can be viewed as carrying both the cell memory and hidden state vectors. The updates are modulated by the values of several gating vectors, which control the degree to which the cell memory and hidden state are\nupdated in light of new information in the current frame. For a single-layer LSTM network, the updates are as follows:\nit = \u03c3(Wi[xt,ht\u22121] + bi) input gate ft = \u03c3(Wf [xt,ht\u22121] + bf ) forget gate c\u0303t = tanh(Wc[xt,ht\u22121] + bc) candidate cell memory ct = it c\u0303t + ft ct\u22121 cell memory ot = \u03c3(Wo[xt,ht\u22121] + bo) output gate ht = ot tanh(ct) hidden state\nwhere ht, ct, c\u0303t, it, ft, and ot are all vectors of the same dimensionality, Wi,Wo,Wf , and Wc are learned weight matrices of the appropriate sizes, bi,bo,bf and bc are learned bias vectors, \u03c3(\u00b7) is a componentwise logistic activation, and refers to the Hadamard (componentwise) product.\nSimilarly, in a GRU network, at each time step a GRU cell determines what components of old information are retained, overwritten, or modified in light of the next step in the input sequence. The output from a GRU cell is only the hidden state vector. A GRU cell uses a reset gate rt and an update gate ut as described below for a single-layer network:\nrt = \u03c3(Wr[xt,ht\u22121] + br) reset gate ut = \u03c3(Wu[xt,ht\u22121] + bu) update gate\nh\u0303t = tanh(Wh[xt, rt ht\u22121] + bh) candidate hidden\nht = ut ht\u22121 + (1\u2212 ut) h\u0303t hidden state\nwhere rt,ut, h\u0303t, and ht are all the same dimensionality, Wr,Wu, and Wh are learned weight matrices of the appropriate size, and br, bu and bh are learned bias vectors.\nAll of the above equations refer to single-layer networks. In a deep network, with multiple stacked layers, the same update equations are used in each layer, with the state, cell, and gate vectors replaced by layer-specific vectors hlt, c l t, and so on for layer l. For all but the first layer, the input xt is replaced by the hidden state vector from the previous layer hl\u22121t .\nFor the fully connected layers, we use rectified linear unit (ReLU) [30] activation, except for the final layer which depends on the form of supervision and loss used in training."}, {"heading": "3.1. Training", "text": "We train the RNN-based embedding models using a set of pre-segmented spoken words. We use two main training approaches, inspired by prior work but with some differences in the details. As in [14, 11], our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final layer of g(X) is a log-softmax layer. Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see [14] for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model\nis trained end-to-end and is optimized with a cross entropy loss. Although labeled data is necessarily limited, the hope is that the learned models will be useful even when applied to spoken examples of words not previously seen in the training data. For words not seen in training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not.\nThe second training approach, based on earlier work of Kamper et al. [14], is to train \u201dSiamese\u201d networks [31]. In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different. The base model remains the same as before\u2014an RNN followed by a set of fully connected layers\u2014but the final layer is no longer a softmax but rather a linear activation layer of arbitrary size. In order to learn the parameters, we simultaneously feed three word segments through three copies of our model (i.e. three networks with shared weights). One input segment is an \u201canchor\u201d, xa, the second is another segment with the same word label, xs, and the third is a segment corresponding to a different word label, xd. Then, the network is trained using a \u201ccos-hinge\u201d loss:\nlcos hinge = max{0,m+ dcos(xa, xs)\u2212 dcos(xa, xd)}\nwhere dcos(x1, x2) = 1 \u2212 cos(x1, x2) is the cosine distance between x1, x2. Unlike cross entropy training, here we directly aim to optimize relative (cosine) distance between same and different word pairs. For tasks such as query-by-example search, this training loss better respects our end objective, and can use more data since neither fully labeled data nor any minimum number of examples of each word should be needed.\n4. EXPERIMENTS\nOur end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether sameand different-word pairs have the expected relationship. and that allows us to compare to a variety of prior work. Specifically, we use the word discrimination task of Carlin et al. [21], which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the distance is below a threshold. By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP.\nThe data used for this task is drawn from the Switchboard conversational English corpus [32]. The word segments range from 50 to 200 frames in length. The acoustic features in each\nframe (the input to the word embedding models xt) are 39- dimensional MFCCs+\u2206+\u2206\u2206. We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in [14], when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.1\nWhen training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the lcos hinge loss."}, {"heading": "4.1. Classification network details", "text": "Our classifier-based embeddings use LSTM or GRU networks with 2\u20134 stacked layers and 1\u20133 fully connected layers. The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. The recurrent hidden state dimensionality is fixed at 512 and dropout [33] between stacked recurrent layers is used with probability p = 0.3. The fully connected hidden layer dimensionality is fixed at 1024. Rectified linear unit (ReLU) non-linearities and dropout with p = 0.5 are used between fully-connected layers. However, between the final recurrent hidden state output and the first fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set.\nThe classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum [34]. The learning rate is initialized at 0.1 and is reduced by a factor of 10 according to the following heuristic: If 99% of the current epoch\u2019s average batch loss is greater than the running average of batch losses over the last 3 epochs, this is considered a plateau; if there are 3 consecutive plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum."}, {"heading": "4.2. Siamese network details", "text": "For experiments with Siamese networks, we initialize (warmstart) the networks with the tuned classification network, removing the final log-softmax layer and replacing it with a linear layer of size equal to the desired embedding dimensionality. We explored embeddings with dimensionalities between\n1We thank Herman Kamper for assistance with the data and evaluation.\n8 and 2048. We use a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training minibatch consists of 2B triplets. B triplets are of the form (xa, xs, xd) where xa and xs are examples of the same class (a pair from the 100k same-word pair set) and xd is a randomly sampled example from a different class. Then, for each of these B triplets (xa, xs, xd), an additional triplet (xs, xa, xd) is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work [14], which we found to improve stability in training and performance on the development set.\nIn preliminary experiments, we compared two methods for choosing the negative examples xd during training, a uniform sampling approach and a non-uniform one. In the case of uniform sampling, we sample xd uniformly at random from the full set of training examples with labels different from xa. This sampling method requires only word-pair supervision. In the case of non-uniform sampling, xd is sampled in two steps. First, we construct a distribution Py|label(xa) over word labels y and sample a different label from it. Second, we sample an example uniformly from within the subset with the chosen label. The goal of this method is to speed up training by targeting pairs that violate the margin constraint. To construct the multinomial PMF Py|label(xa), we maintain an n\u00d7 n matrix S, where n is the number of unique word labels in training. Each word label corresponds to an integer i \u2208 [1, n] and therefore a row in S. The values in a row of S are considered similarity scores, and we can retrieve the desired PMF for each row by normalizing by its sum.\nAt the start of each epoch, we initialize S with 0\u2019s along the diagonal and 1\u2019s elsewhere (which reduces to uniform sampling). For each training pair (dcos(xa, xs), dcos(xa, xd)), we update S for both (i, j) = (label(xa), label(xd)) and (i, j) = (label(xd), label(xa)):\nsi,j +=\n{ cos(xa, xd) dcos(xa, xd) \u2264 dcos(xa, xs) +m\u2217\n0 otherwise\nThe PMFs Py|label(xa) are updated after the forward pass of an entire mini-batch. The constant m\u2217 enforces a potentially stronger constraint than is used in the lcos hinge loss, in order to promote diverse sampling. In all experiments, we set m\u2217 = 0.6. This is a heuristic approach, and it would be interesting to consider various alternatives. Preliminary experiments showed that the non-uniform sampling method outperformed uniform sampling, and in the following we report results with non-uniform sampling.\nWe optimize the Siamese network model using SGD with Nesterov momentum for 15 epochs. The learning rate is initialized to 0.001 and dropped every 3 epochs until no improvement is seen on the dev set. The final model is taken from the epoch with the highest dev set AP. All models were implemented in Torch [38] and used the rnn library of [39].\nBased on development set results, our final embedding models are LSTM networks with 3 stacked layers and 3 fully connected layers, with output dimensionality of 1024 in the case of Siamese networks. Final test set results are given in Table 1. We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22]. Both classifier and Siamese LSTM embedding models outperform all prior results on this task of which we are aware.2\nWe next analyze the effects of model design choices, as well as the learned embeddings themselves."}, {"heading": "5.1. Effect of model structure", "text": "Table 2 shows the effect on development set performance of the number of stacked layers S, the number of fully connected layers F , and LSTM vs. GRU cells, for classifierbased embeddings. The best performance in this experiment is achieved by the LSTM network with S = F = 3. However, performance still seems to be improving with additional layers, suggesting that we may be able to further improve performance by adding even more layers of either type. However, we fixed the model to S = F = 3 in order to allow for more experimentation and analysis within a reasonable time.\n2Yuan et al. [40] have recently been able to improve AP on this test set even further with CNN embeddings, by using a large set of additional (crosslingual) training data. We do not consider these results to be comparable because of their reliance on additional data.\nTable 2 reveals an interesting trend. When only one fully connected layer is used, the GRU networks outperform the LSTMs given a sufficient number of stacked layers. On the other hand, once we add more fully connected layers, the LSTMs outperform the GRUs. In the first few lines of Table 2, we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding fixed the number of fully-connected layers at F = 1. There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of [14] until we begin adding fully connected layers.\nAfter exploring a variety of stacked RNNs, we fixed the stack to 3 layers and varied the number of fully connected layers. The value of each additional fully connected layer is clearly greater than that of adding stacked layers. All networks trained with 2 or 3 fully connected layers obtain more than 0.4 AP on the development set, while stacked RNNs with 1 fully connected layer are at around 0.3 AP or less. This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14], and convolutional or recurrent layers are needed to summarize arbitrarylength segments into a fixed-dimensional representation."}, {"heading": "5.2. Effect of embedding dimensionality", "text": "For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. 2. This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings [14] for all dimensionalities \u2265 16."}, {"heading": "5.3. Effect of training vocabulary", "text": "We might expect the learned embeddings to be more accurate for words that are seen in training than for ones that are not. Fig. 2 measures this effect by showing performance as a function of the number of occurrences of the dev words in the\ntraining set. Indeed, both model types are much more successful for in-vocabulary words, and their performance improves the higher the training frequency of the words. However, performance increases more quickly for the Siamese network than for the classifier as training frequency increases. This may be due to the fact that, if a word type occurs at least k times in the classifier training set, then it occurs at least 2\u00d7 ( k 2 ) times in the Siamese paired training data."}, {"heading": "5.4. Visualization of embeddings", "text": "In order to gain a better qualitative understanding of the differences between clasiffier and Siamese-based embeddings, and of the learned embedding space more generally, we plot a two-dimensional visualization of some of our learned embeddings via t-SNE [41] in Fig. 3. For both classifier and Siamese embeddings, there is a marked difference in the quality of clusters formed by embeddings of words that were previously seen vs. previously unseen in training. However, the Siamese network embeddings appear to have better relative distances between word clusters with similar and dissimilar pronunciations. For example, the word programs appears equidistant from problems and problem in the classifierbased embedding space, but in the Siamese embedding space problems falls between problem and programs. Similarly, the cluster for democracy shifts with respect to actually and especially to better respect differences in pronunciation. More study of learned embeddings, using more data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and\nbasketball) are a likely result of optimizing the model for relative distances between words.\n6. CONCLUSION\nOur main finding is that RNN-based acoustic word embeddings outperform prior approaches, as measured via a word discrimination task related to query-by-example search. Our best results are obtained with deep LSTM RNNs with a combination of several stacked layers and several fully connected layers, optimized with a contrastive Siamese loss. Siamese networks have the benefit that, for any given training data set, they are effectively trained on a much larger set, in the sense that they measure a loss and gradient for every possible pair of data points. Our experiments suggest that the models could still be improved with additional layers. In addition, we have found that, for the purposes of acoustic word embeddings, fully connected layers are very important and have a more significant effect per layer than stacked layers, particularly when trained with the cross entropy loss function.\nThese experiments represent an initial exploration of sequential neural models for acoustic word embeddings. There are a number of directions for further work. For example, while our analyses suggest that Siamese networks are better than classifier-based models at embedding previously unseen words, our best embeddings are still much poorer for unseen words. Improvements in this direction may come from larger training sets, or may require new models that better model the shared structure between words. Other directions for future work include additional forms of supervision and training, as well as application to downstream tasks.\n7. REFERENCES\n[1] ISCA, Proceedings of the International Tutorial and Research Workshop on Pronunciation Modeling and Lexicon Adaptation for Spoken Language Technology, Estes Park, Colorado, 2002.\n[2] M. Ostendorf, \u201cMoving Beyond the \u2019Beads-on-a-String\u2019 Model of Speech,\u201d in IEEE Automatic Speech Recognition & Understanding (ASRU), 1999.\n[3] Karen Livescu, Eric Fosler-Lussier, and Florian Metze, \u201cSubword modeling for automatic speech recognition: Past, present, and emerging approaches,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 44\u201357, 2012.\n[4] Mathias De Wachter, Mike Matton, Kris Demuynck, Patrick Wambacq, Ronald Cools, and Dirk Van Compernolle, \u201cTemplate-based continuous speech recognition,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.\n[5] Georg Heigold, Patrick Nguyen, Mitchel Weintraub, and Vincent Vanhoucke, \u201cInvestigations on exemplarbased features for speech recognition towards thousands of hours of unsupervised, noisy data,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 4437\u20134440.\n[6] Florian Metze, Xavier Anguera, Etienne Barnard, Marelie Davel, and Guillaume Gravier, \u201cThe spoken web search task at MediaEval 2012,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.\n[7] Xavier Anguera, \u201cSpeaker independent discriminant feature extraction for acoustic pattern-matching,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012.\n[8] Yaodong Zhang, Kiarash Adl, and James Glass, \u201cFast spoken query detection using lower-bound dynamic time warping on graphical processing units,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 5173\u20135176.\n[9] Igor Szo\u0308ke, Miroslav Ska\u0301cel, Luka\u0301s\u0306 Burget, and Jan \u201cHonza\u201d C\u0306ernocky\u0301, \u201cCoping with channel mismatch in query-by-example - BUT QUESST 2014,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[10] Andrew L Maas, Stephen D Miller, Tyler M O\u2019neil, Andrew Y Ng, and Patrick Nguyen, \u201cWord-level acoustic modeling with convolutional vector regression,\u201d in International Conference on Machine Learning (ICML), Representation Learning Workshop, 2012.\n[11] Samy Bengio and Georg Heigold, \u201cWord embeddings for speech recognition,\u201d in Interspeech, 2014.\n[12] Keith Levin, Katharine Henry, Aren Jansen, and Karen Livescu, \u201cFixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,\u201d in IEEE Automatic Speech Recognition & Understanding (ASRU), 2013.\n[13] Keith Levin, Aren Jansen, and Benjamin Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[14] Herman Kamper, Weiran Wang, and Karen Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.\n[15] Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, and Hung-Yi Lee, \u201cUnsupervised learning of audio segment representations using sequence-to-sequence recurrent neural networks,\u201d Interspeech, 2016.\n[16] Stephen Voinea, Chiyuan Zhang, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio, \u201cWord-level invariant representations from acoustic waveforms.,\u201d in Interspeech, 2014, pp. 2385\u20132389.\n[17] Guoguo Chen, Carolina Parada, and Tara N Sainath, \u201cQuery-by-example keyword spotting using long shortterm memory networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[18] Geoffrey Zweig and Patrick Nguyen, \u201cA segmental CRF approach to large vocabulary continuous speech recognition,\u201d in IEEE Automatic Speech Recognition & Understanding (ASRU), 2009, pp. 152\u2013157.\n[19] Herman Kamper, Aren Jansen, Simon King, and Sharon Goldwater, \u201cUnsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings,\u201d in IEEE Spoken Language Technology Workshop (SLT), 2014, pp. 100\u2013105.\n[20] Herman Kamper, Aren Jansen, and Sharon Goldwater, \u201cFully unsupervised small-vocabulary speech recognition using a segmental bayesian model,\u201d in Interspeech, 2015.\n[21] Michael A Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky, \u201cRapid evaluation of speech representations for spoken term discovery,\u201d in Interspeech, 2011.\n[22] H. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \u201cUnsupervised neural network based feature extraction using weak top-down constraints,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[23] Aren Jansen, Samuel Thomas, and Hynek Hermansky, \u201cWeak top-down constraints for unsupervised acoustic model training,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.\n[24] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[25] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d Neural Information Processing Systems (NIPS), 2014.\n[26] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.\n[27] Has\u0327im Sak, Andrew Senior, and Franc\u0327oise Beaufays, \u201cLong short-term memory based recurrent neural network architectures for large vocabulary speech recognition,\u201d arXiv preprint arXiv:1402.1128, 2014.\n[28] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, \u201cAttention-based models for speech recognition,\u201d in Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.\n[29] Liang Lu, Xingxing Zhang, Kyunghyun Cho, and Steve Renals, \u201cA study of the recurrent neural network encoder-decoder for large vocabulary speech recognition,\u201d in Interspeech, 2015.\n[30] Vinod Nair and Geoffrey E Hinton, \u201cRectified linear units improve restricted boltzmann machines,\u201d in International Conference on Machine Learning (ICML), 2010, pp. 807\u2013814.\n[31] Jane Bromley, James W Bentz, Le\u0301on Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard Sa\u0308ckinger, and Roopak Shah, \u201cSignature verification using a \u2018Siamese\u2019 time delay neural network,\u201d Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.\n[32] John J Godfrey, Edward C Holliman, and Jane McDaniel, \u201cSwitchboard: Telephone speech corpus for research and development,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992, vol. 1, pp. 517\u2013520.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.,\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[34] Yurii Nesterov, \u201cA method of solving a convex programming problem with convergence rate O(1/k2),\u201d .\n[35] John Duchi, Elad Hazan, and Yoram Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Tech. Rep. UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010.\n[36] Matthew D. Zeiler, \u201cADADELTA: an adaptive learning rate method,\u201d CoRR, vol. abs/1212.5701, 2012.\n[37] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d CoRR, vol. abs/1412.6980, 2014.\n[38] Ronan Collobert, Koray Kavukcuoglu, and Cle\u0301ment Farabet, \u201cTorch7: A matlab-like environment for machine learning,\u201d in BigLearn, Neural Information Processing (NIPS) Workshop, 2011, number EPFL-CONF192376.\n[39] Nicholas Le\u0301onard, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim, \u201crnn : Recurrent library for torch,\u201d CoRR, vol. abs/1511.07889, 2015.\n[40] Yougen Yuan, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li, \u201cLearning neural network representations using cross-lingual bottleneck features with word-pair information,\u201d in Interspeech, 2016.\n[41] Laurens van der Maaten and Geoffrey Hinton, \u201cVisualizing data using t-sne,\u201d Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008."}], "references": [{"title": "Moving Beyond the \u2019Beads-on-a-String\u2019 Model of Speech", "author": ["M. Ostendorf"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Subword modeling for automatic speech recognition: Past, present, and emerging approaches", "author": ["Karen Livescu", "Eric Fosler-Lussier", "Florian Metze"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 44\u201357, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Template-based continuous speech recognition", "author": ["Mathias De Wachter", "Mike Matton", "Kris Demuynck", "Patrick Wambacq", "Ronald Cools", "Dirk Van Compernolle"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Investigations on exemplarbased features for speech recognition towards thousands of hours of unsupervised, noisy data", "author": ["Georg Heigold", "Patrick Nguyen", "Mitchel Weintraub", "Vincent Vanhoucke"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 4437\u20134440.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["Florian Metze", "Xavier Anguera", "Etienne Barnard", "Marelie Davel", "Guillaume Gravier"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker independent discriminant feature extraction for acoustic pattern-matching", "author": ["Xavier Anguera"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast spoken query detection using lower-bound dynamic time warping on graphical processing units", "author": ["Yaodong Zhang", "Kiarash Adl", "James Glass"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 5173\u20135176.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Coping with channel mismatch in query-by-example - BUT QUESST 2014", "author": ["Igor Sz\u00f6ke", "Miroslav Sk\u00e1cel", "Luk\u00e1s\u0306 Burget", "Jan \u201cHonza\u201d C\u0306ernock\u00fd"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Word-level acoustic modeling with convolutional vector regression", "author": ["Andrew L Maas", "Stephen D Miller", "Tyler M O\u2019neil", "Andrew Y Ng", "Patrick Nguyen"], "venue": "International Conference on Machine Learning (ICML), Representation Learning Workshop, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Word embeddings for speech recognition", "author": ["Samy Bengio", "Georg Heigold"], "venue": "Interspeech, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["Keith Levin", "Katharine Henry", "Aren Jansen", "Karen Livescu"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of audio segment representations using sequence-to-sequence recurrent neural networks", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-Yi Lee"], "venue": "Interspeech, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Word-level invariant representations from acoustic waveforms", "author": ["Stephen Voinea", "Chiyuan Zhang", "Georgios Evangelopoulos", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "Interspeech, 2014, pp. 2385\u20132389.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long shortterm memory networks", "author": ["Guoguo Chen", "Carolina Parada", "Tara N Sainath"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A segmental CRF approach to large vocabulary continuous speech recognition", "author": ["Geoffrey Zweig", "Patrick Nguyen"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 2009, pp. 152\u2013157.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings", "author": ["Herman Kamper", "Aren Jansen", "Simon King", "Sharon Goldwater"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2014, pp. 100\u2013105.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "venue": "Interspeech, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "Interspeech, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["Aren Jansen", "Samuel Thomas", "Hynek Hermansky"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition", "author": ["Liang Lu", "Xingxing Zhang", "Kyunghyun Cho", "Steve Renals"], "venue": "Interspeech, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "International Conference on Machine Learning (ICML), 2010, pp. 807\u2013814.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Signature verification using a \u2018Siamese\u2019 time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane Mc- Daniel"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992, vol. 1, pp. 517\u2013520.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1929}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Yurii Nesterov"], "venue": ".", "citeRegEx": "34", "shortCiteRegEx": null, "year": 0}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Tech. Rep. UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, vol. abs/1212.5701, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "BigLearn, Neural Information Processing (NIPS) Workshop, 2011, number EPFL-CONF- 192376.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "rnn : Recurrent library for torch", "author": ["Nicholas L\u00e9onard", "Sagar Waghmare", "Yang Wang", "Jin-Hwa Kim"], "venue": "CoRR, vol. abs/1511.07889, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning neural network representations using cross-lingual bottleneck features with word-pair information", "author": ["Yougen Yuan", "Cheung-Chi Leung", "Lei Xie", "Bin Ma", "Haizhou Li"], "venue": "Interspeech, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This is helpful since, despite decades of work on sub-word modeling [1, 2], it still poses significant challenges.", "startOffset": 68, "endOffset": 74}, {"referenceID": 1, "context": "For example, speech processing systems are still hampered by differences in conversational pronunciations [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word.", "startOffset": 50, "endOffset": 56}, {"referenceID": 3, "context": "For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word.", "startOffset": 50, "endOffset": 56}, {"referenceID": 4, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 6, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 7, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 8, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 9, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 10, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 11, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 12, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 13, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 14, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 15, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 8, "context": "[10] and Bengio and Heigold [11] used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and Bengio and Heigold [11] used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to define feature functions in a segmental conditional random field [18] rescoring system.", "startOffset": 177, "endOffset": 181}, {"referenceID": 10, "context": "[12] developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of reference word segments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 14, "context": "[16] developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] as well as several other CNN and DNN embeddings and DTW using several feature types.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 19, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 20, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 21, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 15, "context": "[17] and Chung et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] worked in an unsupervised setting and trained single-layer RNN autoencoders to produce embeddings for a word discrimination task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Recurrent Unit (GRU) networks [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Recurrent Unit (GRU) networks [25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 25, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 26, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 27, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 28, "context": "For the fully connected layers, we use rectified linear unit (ReLU) [30] activation, except for the final layer which depends on the form of supervision and loss used in training.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "As in [14, 11], our first approach is to use the word labels of the training segments and train the networks to classify the word.", "startOffset": 6, "endOffset": 14}, {"referenceID": 9, "context": "As in [14, 11], our first approach is to use the word labels of the training segments and train the networks to classify the word.", "startOffset": 6, "endOffset": 14}, {"referenceID": 12, "context": "Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see [14] for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer).", "startOffset": 203, "endOffset": 207}, {"referenceID": 12, "context": "[14], is to train \u201dSiamese\u201d networks [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[14], is to train \u201dSiamese\u201d networks [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "[21], which is similar to a query-by-example task where the word segmentations are known.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The data used for this task is drawn from the Switchboard conversational English corpus [32].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 73, "endOffset": 81}, {"referenceID": 10, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "As in [14], when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "The recurrent hidden state dimensionality is fixed at 512 and dropout [33] between stacked recurrent layers is used with probability p = 0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "The classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum [34].", "startOffset": 137, "endOffset": 141}, {"referenceID": 33, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 34, "endOffset": 38}, {"referenceID": 34, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "This is a slight departure from earlier work [14], which we found to improve stability in training and performance on the development set.", "startOffset": 45, "endOffset": 49}, {"referenceID": 36, "context": "All models were implemented in Torch [38] and used the rnn library of [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "All models were implemented in Torch [38] and used the rnn library of [39].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "MFCCs + DTW [14] 39\u2217 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "autoencoder + DTW [22] 100\u2217 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Classifier CNN [14] 1061 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "014 Siamese CNN [14] 1024 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 265, "endOffset": 269}, {"referenceID": 38, "context": "[40] have recently been able to improve AP on this test set even further with CNN embeddings, by using a large set of additional (crosslingual) training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of [14] until we begin adding fully connected layers.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14], and convolutional or recurrent layers are needed to summarize arbitrarylength segments into a fixed-dimensional representation.", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings [14] for all dimensionalities \u2265 16.", "startOffset": 276, "endOffset": 280}, {"referenceID": 39, "context": "In order to gain a better qualitative understanding of the differences between clasiffier and Siamese-based embeddings, and of the learned embedding space more generally, we plot a two-dimensional visualization of some of our learned embeddings via t-SNE [41] in Fig.", "startOffset": 255, "endOffset": 259}], "year": 2016, "abstractText": "Acoustic word embeddings \u2014 fixed-dimensional vector representations of variable-length spoken word segments \u2014 have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a \u201dSiamese network\u201d training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.", "creator": "LaTeX with hyperref package"}}}