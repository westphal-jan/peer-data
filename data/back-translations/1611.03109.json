{"id": "1611.03109", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Energy-efficient Machine Learning in Silicon: A Communications-inspired Approach", "abstract": "This position paper advocates a communication-inspired approach to the development of machine learning systems on energy-constrained, embedded \"always-on\" platforms. The communication-inspired approach includes two versions - 1) a deterministic version, redirecting existing IC design methods to energy-saving communication, and 2) a stochastic version, known as Shannon-inspired statistical information processing, using information-based metrics, statistical error compensation (SEC), and retraining-based methods for implementing ML systems on stochastic circuits / device webs operating at the limits of energy efficiency. The communication-inspired approach has the potential to take full advantage of the opportunities offered by ML algorithms and applications to address the challenges inherent in their deployment on energy-constrained platforms.", "histories": [["v1", "Tue, 25 Oct 2016 18:45:32 GMT  (939kb,D)", "http://arxiv.org/abs/1611.03109v1", "This paper was presented at the 2016 ICML Workshop on On-Device Intelligence, June 24, 2016"]], "COMMENTS": "This paper was presented at the 2016 ICML Workshop on On-Device Intelligence, June 24, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AR", "authors": ["naresh r shanbhag"], "accepted": false, "id": "1611.03109"}, "pdf": {"name": "1611.03109.pdf", "metadata": {"source": "CRF", "title": "Energy-efficient Machine Learning in Silicon: A Communications-inspired Approach", "authors": ["Naresh Shanbhag"], "emails": ["SHANBHAG@ILLINOIS.EDU"], "sections": [{"heading": "1. Introduction", "text": "Machine learning (ML)-based systems are transforming the way we live and interact with the world around us. In many tasks, such as those in computer vision, machines have begun to exceed human performance (Silver et al., 2016). However, machines have much catching up to do when energy costs are accounted for. While it is difficult to accurately estimate the energy cost of the AlphaGo system developed by Google DeepMind when it beat the human champion recently in the ancient game of Go, one can safely assume that the machine consumed about fourorders-of-magnitude higher power (1202 CPUs and 176 GPUs (Silver et al., 2016)) as compared to the nominally quoted power of 20W for the human brain. If ML systems need to become pervasive in our lives then it is imper-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\native that this energy cost be significantly reduced. The availability of such low-energy realizations of ML systems will enable its deployment on embedded platforms such as biomedical devices, wearables, autonomous vehicles, IoT and many others. Not surprisingly, a number of integrated circuit (IC) implementations of ML kernels and algorithms have appeared recently (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) that have set energyefficiency records. However, much work still remains to be done as the energy gap between these realizations and that achieved by the human brain remains huge. In particular, the search for minimum energy realizations of ML systems needs to be done systematically. The low-energy ML design space is complex as it encompasses deeply intertwined issues at the algorithmic, architectural, circuit and the device level. The mainstream approach today is\nCopyright\u00a02015,\u00a0Naresh\u00a0Shanbhag\nnoise\nto treat the problem of energy-efficient ML implementation as yet another problem in energy-efficient computing. We believe that there are substantial gains to be made if\nThis work was supported in part by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA.\nar X\niv :1\n61 1.\n03 10\n9v 1\n[ cs\n.L G\n] 2\n5 O\nct 2\n01 6\none were to repurpose the vast body of knowledge accumulated over two and a half decades by the designers of low-power communication and signal processing systems and ICs (A.P. Chandrakasan & Brodersen, 1992; Shanbhag, 1998; Parhi, 1999). This position paper makes the case for employing a communications-inspired approach in order to explore the design of energy-efficient ML in nanoscale silicon CMOS and emerging beyond CMOS device fabrics.\nThe communications-inspired approach in based on drawing parallels between a communication receiver and an inference kernel as shown in Fig. 1. A communication receiver infers the transmitted symbols Y from the received signal X , much as a ML system infers the class label Y from the observed data X . In both systems, the process of inference needs to be accomplished in the presence of random noise and incomplete data. Both systems need an element of learning/training to be present in order to incorporate time-varying/unknown data statistics/model into the decision making process. Communication receivers commonly employ statistical estimation procedures to learn the channel parameters, which are then employed for data recovery. Furthermore, the stochastic gradient descent (SGD) (Mathews & Xie, 1993; Keuper & Pfreundt, 2015) is commonly employed in both systems due to its ease of implementation and robustness. There is one key difference between the two systems though. In communication systems, the data X\u2019s statistics can be engineered via proper coding and modulation in the transmitter. This allows such receiver to operate with well-structured signal, channel and noise models, which lowers its complexity and energy consumption, while enhancing its accuracy. This flexibility may not be present in general ML scenarios. Nevertheless, the similarities between the two are substantial enough to warrant a closer look at low-power communication receiver design techniques and see which ones might be repurposed for ML systems.\nIn the discussion above, one assumes a deterministic circuit fabric. Recent IC implementations (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) do in fact fit this model. However, this assumption can be relaxed in case of ML systems due to their inherent ability to operate in the presence of incomplete or noisy data. This ability can be leveraged to address the statistical behavior of circuit/device fabrics that arises when these operate at the limits of energy efficiency. Such ultimate low-energy fabrics is referred to as stochastic fabrics or low-SNR circuit fabrics. Indeed, statistical behavior in such fabrics can arise when:\n\u2022 operating at very low voltages (Dreslinski et al., 2010) or low area (Roy et al., 2013), both of which result in computational errors, and/or\n\u2022 designing systems with emerging devices (Roy et al., 2013; Wei et al., 2013) which tend to be intrinsically\nstatistical in nature due to nanoscale imperfections such as variations and defects, and/or\n\u2022 embedding computation into memory (in-memory computing (Kang et al., 2014)) and sensing (in-sensor computing (Hu et al., 2012)) substrates in order to drastically reduce/eliminate data movement.\nWe refer to such ultimate low-energy fabrics as stochastic fabrics. The statistical behavior of stochastic fabrics needs to be compensated for much as a communication receiver compensates for the statistical behavior of the channel. The communications-inspired view opens up the possibility of taking the connections between ML and communications to another level by treating the circuit fabric itself as a noisy channel on which to extract information from data. We refer to this second approach as Shannoninspired statistical information processing (Shanbhag et al., 2010). Statistical information processing involves the use of information-based metrics, statistical error compensation (SEC) (Hegde & Shanbhag, 2001), and retraining approaches such as data-driven hardware resiliency (DDHR) (Wang et al., 2015) to enhance robustness. One intellectually satisfying aspect of statistical information processing is the potential for developing a comprehensive foundation for reliable information processing on stochastic fabrics much as Claude Shannon (Shannon, 1948) established one for reliable communications over a noisy channel. Such a foundation needs to provide fundamental bounds on the information processing capacity, energy-efficiency, robustness, as well as practical design techniques, e.g., SEC and DDHR, to approach these bounds.\nThis paper advocates a communications-inspired approach to the design of energy-efficient ML systems on both deterministic and stochastic fabrics. Doing so will bring together methodologies such as low-power signal processing algorithms and architectures (Parhi, 1999), algorithm transforms (Shanbhag, 1998), low-power integrated circuit (IC) design (A.P. Chandrakasan & Brodersen, 1992), information-based design metrics, statistical error compensation (SEC) and others to systematically explore the design space in order to determine minimum energy realizations."}, {"heading": "2. Machine Learning on Deterministic Fabrics", "text": "The design of communication receiver ICs begins with algorithm design employing statistical signal processing techniques such as estimation and detection to meet a specific system design metric such as the bit-error rate (BER) pe = P{Y 6= Y\u0302 } (see Fig. 1). The use of an information-based metric (BER) and its intrinsically statistical nature makes it possible to reduce algorithmic com-\nplexity right from the start. Redundant algorithmic operations are eliminated or substituted with approximate ones so as to leave the BER unaltered. Machine learning systems employ an accuracy metric pdet the probability of detection, and therefore can benefit from such approximations. Indeed, \u201capproximate computing\u201d (Venkataramani et al., 2015) strives to build a methodology to systematize and repurpose these concepts which are well-known and wellpracticed for decades by communication IC designers. The result of this step is a floating-point algorithm meeting the system requirements on BER and other metrics.\nNext, fixed-point analysis is employed to minimize the precision of computation and storage. Indeed, minimizing precision (Gupta et al., 2015) is an effective approach to reduce energy. The goal of this step is to minimize the BER difference between the floating-point and a fixed-point algorithm. Precisions is typically obtained via trial-and-error. Insights on what algorithmic aspects determine the precision tend to be lost in this process. However, for communications and ML algorithms, it is possible to obtain analytical bounds on precision. For example, the bounds on the precision BWUD of the weight-update unit of the popular least mean-squared (LMS) algorithm (Goel & Shanbhag, 1998) is given by:\nBWUD \u2265 1\n2 log2\n( 1\n\u00b52\u03c32y\u03c3 2 x\n) + SNRfl(dB)\n6 (1a)\nwhere \u00b5 is the step-size, \u03c32x and \u03c3 2 y are variances of the in-\nput X and desired signal Y , respectively, and SNRfl(dB) is the SNR of the floating point algorithm in dBs. Minimum precision requirements are thus obtained without resorting to expensive simulations. In a similar fashion, it is possible to obtain bounds for other SGD-based on-line learning algorithms.\nThe fixed-point algorithm can be described using a data flow-graph (DFG) or a control and data flow graph (CDFG). An almost infinite variety of architectures can be systematically obtained from a DFG using algorithm transforms (Parhi, 1999) such as unfolding, folding, pipelining, systolization, among others. ML algorithms tend to have a regular DFG (see Fig. 2). This opens up the possibility of realizing systolic architectures (Kung, 1982) for many ML algorithms. Some work already exists (Jones et al., 1994; Kung & Hwang, 1989). Systolic architectures are regular, have local interconnections, and can be designed to minimize data movement. The process of mapping a regular DFG to a systolic architectures involves the selection of a processor vector p, the iteration vector d and the schedule vector s, satifying the constraints pTd = 0, sTd 6= 0, and implying that the DFG node v is mapped to processor pTv in the cycle sTv. Indeed, one can derive the recently proposed architectures (Chen et al., 2016; Murmann et al., 2015) by formulating the DFG of a convolutional neural\nregular\u00a0DFG systolic\u00a0architecture\nCopyright\u00a02015,\u00a0Naresh\u00a0Shanbhag 8\nnetwork (CNN) (LeCun et al., 1998) (see Fig. 2(b)), and assigning appropriate values to p, d, and s, along with the folding transform. These design methodologies for communication ICs can be repurposed for the design of energyefficient ML systems in silicon."}, {"heading": "3. Machine Learning on Stochastic Fabrics", "text": "The communications-inspired approach presents a unique opportunity when implementing ML on deeply scaled nanofabrics that operate at the limits of energy efficiency where a transition into non-determinism occurs. For example, near/subthreshold voltage (Dreslinski et al., 2010) operation in CMOS results approximately 10\u00d7 reduction in energy but at the expense of up to 20\u00d7 increase in delay variations. This variability eventually translates into observable errors in computation, storage, and communications. We refer to such circuit and device substrate as stochastic fabrics, and the errors themselves as fabric noise. ML algorithms\u2019 intrinsic robustness to data noise enables it to absorb the impact of fabric noise. This feature, referred to popularly as \u2018error-tolerance\u2019, can be exploited to some extent by approaches such as approximate\ncomputing (Venkataramani et al., 2015) as well. However, it is possible to reduce the energy consumption even further by operating the circuit fabric at a point where the intrinsic error-tolerance of the algorithm is exceeded. At this point, corrective measures, i.e., error compensation methods, need to be incorporated. Conventional fault-tolerance techniques such as N -modular redundancy are ineffective as these have a high energy-cost, and do not account for the unique attributes of ML algorithms. A Shannon-inspired approach to error compensation turns out to be most effective.\n5\ny\u0302x 1y 2y\nNy\n, ( , )eP e \n(a)\n6\nx\n oa yy\neyy oe \ny\u0302\n, \u226a \u2245\nError\u00a0detection:\u00a0 1 if\u00a0 else\u00a0\u00a0 0\nError\u00a0correction:\u00a0\u00a0 if\u00a0 1 else\u00a0\nRaw\u00a0error\u00a0rate\u00a0=\u00a0 Pr 0\ninformation\nparity\n(b)\nFigure 3. Shannon-inspired statistical error compensation (SEC): (a) a general framework, and (b) algorithmic noise-tolerance (ANT).\nIn the past, we have proposed the notion of treating the stochastic circuit fabric as a noisy communication channel (Shanbhag, 1996) and develop Shannon-inspired statistical error compensation (SEC) techniques (see Fig. 3(a))(Hegde & Shanbhag, 2001; Shim et al., 2004; Varatkar et al., 2010) to compensate for the resulting errors at the algorithmic and architectural levels. Prototype ICs (see Fig. 4) demonstrating these ideas have been implemented. These demonstrate that computational error rates, defined as the probability of an incorrect output, of 60% (Abdallah & Shanbhag, 2013) and in specific cases (see Fig. 4(b)), up to 80% (Kim et al., 2015) can be compensated for by applying techniques based on statistical estimation and detection. SEC techniques have shown to result in energy savings ranging from 3\u00d7-to-6\u00d7 over designs that work on deterministic fabrics. The ability to compensate for such high computational error rates motivates the idea of in-situ data analytics, where computation is deeply embedded into the same substrate where data is stored or being acquired, e.g., in-memory\nBIH\u2010MIT\u00a0ECG\u00a0DB:\u00a011bits,\u00a0200Hz 45nm,\u00a0IBM\u00a0process\nCopyright\u00a02015,\u00a0Naresh\u00a0Shanbhag 17\n(Kang et al., 2014) and in-sensor computing (Hu et al., 2012). Such subtrates are not particularly well-suited for deterministic von Neumann style computing but fits the Shannon-inspired style. Thus, SEC leverages Shannon theory to develop techniques to compensate for errors that cannot be absorbed by the intrinsic error-tolerance of the algorithm. This key aspect distinguishes it from techniques that seek to work within the error-tolerance envelope of the algorithm. SEC techniques can be made adaptive in order to track variations in the data and error statistics. ML-based SEC techniques can also be developed.\nAnother approach is DDHR (Wang et al., 2015) that employs retraining to obtain parameters of the algorithm to compensate for both data and fabric noise. Both SEC and DDHR leverage the statistical nature of system and application metrics, and may even be combined in a synergistic fashion."}, {"heading": "4. Summary", "text": "ML systems have unique properties that it shares with communication systems. There is much to be gained by exploiting the connections between the two when exploring energy efficient on-device implementations of ML systems."}], "references": [{"title": "An Energy-Efficient ECG Processor in 45-nm CMOS Using Statistical Error Compensation", "author": ["R.A. Abdallah", "N.R. Shanbhag"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "Abdallah and Shanbhag,? \\Q2013\\E", "shortCiteRegEx": "Abdallah and Shanbhag", "year": 2013}, {"title": "LowPower CMOS digital design", "author": ["A.P. Chandrakasan", "S. Sheng", "R.W. Brodersen"], "venue": "IEEE Journal of SolidState Circuits,", "citeRegEx": "Chandrakasan et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Chandrakasan et al\\.", "year": 1992}, {"title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "ISSCC", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Near-Threshold Computing: Reclaiming Moore\u2019s Law Through Energy Efficient Integrated Circuits", "author": ["R.G. Dreslinski", "M. Wieckowski", "D. Blaauw", "D. Sylvester", "T. Mudge"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Dreslinski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dreslinski et al\\.", "year": 2010}, {"title": "Finite-precision analysis of the pipelined strength-reduced adaptive filter", "author": ["M. Goel", "N.R. Shanbhag"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Goel and Shanbhag,? \\Q1998\\E", "shortCiteRegEx": "Goel and Shanbhag", "year": 1998}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Soft digital signal processing", "author": ["R. Hegde", "N.R. Shanbhag"], "venue": "IEEE Transactions on VLSI Systems,", "citeRegEx": "Hegde and Shanbhag,? \\Q2001\\E", "shortCiteRegEx": "Hegde and Shanbhag", "year": 2001}, {"title": "Highresolution sensing sheet for structural-health monitoring via scalable interfacing of flexible electronics with highperformance ICs", "author": ["Y. Hu", "W. Rieutort-Louis", "J. Sanz-Robinson", "K. Song", "J.C. Sturm", "S. Wagner", "N. Verma"], "venue": "In 2012 Symposium on VLSI Circuits", "citeRegEx": "Hu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2012}, {"title": "Learning in linear systolic neural network engines: analysis and implementation", "author": ["S.R. Jones", "K.M. Sammut", "J. Hunter"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Jones et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1994}, {"title": "An energy-efficient VLSI architecture for pattern recognition via deep embedding of computation in SRAM", "author": ["M. Kang", "M.S. Keel", "N.R. Shanbhag", "S. Eilert", "K. Curewitz"], "venue": "IEEE International Conference on Acoustics,", "citeRegEx": "Kang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2014}, {"title": "A 21.5M-query-vectors/s 3.37nJ/vector reconfigurable k-nearest-neighbor accelerator with adaptive precision in 14nm tri-gate CMOS", "author": ["K. R"], "venue": "In 2016 IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "R.,? \\Q2016\\E", "shortCiteRegEx": "R.", "year": 2016}, {"title": "A 3.6-mW 50-MHz PN Code Acquisition Filter via Statistical Error Compensation in 180-nm CMOS", "author": ["E.P. Kim", "D.J. Baker", "S. Narayanan", "N.R. Shanbhag", "D.L. Jones"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Why systolic architectures", "author": ["H.T. Kung"], "venue": null, "citeRegEx": "Kung,? \\Q1982\\E", "shortCiteRegEx": "Kung", "year": 1982}, {"title": "A Unified Systolic Architecture for Artificial Neural Networks", "author": ["S. Kung", "J. Hwang"], "venue": "Journal of Parallel and Distributed Computings,", "citeRegEx": "Kung and Hwang,? \\Q1989\\E", "shortCiteRegEx": "Kung and Hwang", "year": 1989}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A stochastic gradient adaptive filter with gradient adaptive step size", "author": ["V.J. Mathews", "Z. Xie"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mathews and Xie,? \\Q1993\\E", "shortCiteRegEx": "Mathews and Xie", "year": 1993}, {"title": "Mixed-signal circuits for embedded machinelearning applications", "author": ["B. Murmann", "D. Bankman", "E. Chai", "D. Miyashita", "L. Yang"], "venue": "In 2015 49th Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Murmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Murmann et al\\.", "year": 2015}, {"title": "VLSI Digital Signal Processing Systems: Design and Implementation", "author": ["K.K. Parhi"], "venue": null, "citeRegEx": "Parhi,? \\Q1999\\E", "shortCiteRegEx": "Parhi", "year": 1999}, {"title": "A 126.1mW real-time natural UI/UX processor with embedded deep-learning core for low-power smart glasses", "author": ["S. Park", "S. Choi", "J. Lee", "M. Kim", "J. Park", "H.J. Yoo"], "venue": "IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "Park et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Park et al\\.", "year": 2016}, {"title": "Beyond charge-based computation: Boolean and nonBoolean computing with spin torque devices", "author": ["K. Roy", "M. Sharad", "Fan", "Deliang", "K. Yogendra"], "venue": "In Low Power Electronics and Design (ISLPED),", "citeRegEx": "Roy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2013}, {"title": "Lower bounds on power-dissipation for DSP algorithms", "author": ["N.R. Shanbhag"], "venue": "In Low Power Electronics and Design,", "citeRegEx": "Shanbhag,? \\Q1996\\E", "shortCiteRegEx": "Shanbhag", "year": 1996}, {"title": "Algorithm transformation techniques for low-power wireless VLSI systems design", "author": ["N.R. Shanbhag"], "venue": "International Journal of Wireless Information Networks,", "citeRegEx": "Shanbhag,? \\Q1998\\E", "shortCiteRegEx": "Shanbhag", "year": 1998}, {"title": "A mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Reliable lowpower digital signal processing via reduced precision redundancy", "author": ["B. Shim", "S. Sridhara", "N.R. Shanbhag"], "venue": "IEEE Transactions on VLSI,", "citeRegEx": "Shim et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shim et al\\.", "year": 2004}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang"], "venue": null, "citeRegEx": "Silver and Huang,? \\Q2016\\E", "shortCiteRegEx": "Silver and Huang", "year": 2016}, {"title": "Stochastic networked computation", "author": ["G.V. Varatkar", "S. Narayanan", "N.R. Shanbhag", "D.L. Jones"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems,", "citeRegEx": "Varatkar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Varatkar et al\\.", "year": 2010}, {"title": "Approximate computing and the quest for computing efficiency", "author": ["S. Venkataramani", "S.T. Chakradhar", "K. Roy", "A. Raghunathan"], "venue": "In 2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC),", "citeRegEx": "Venkataramani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkataramani et al\\.", "year": 2015}, {"title": "Overcoming Computational Errors in Sensing Platforms Through Embedded Machine-Learning Kernels", "author": ["Z. Wang", "K.H. Lee", "N. Verma"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Carbon nanotube circuits: Opportunities and challenges", "author": ["H. Wei", "M. Shulaker"], "venue": "In Design, Automation Test in Europe Conference Exhibition (DATE),", "citeRegEx": "Wei and Shulaker,? \\Q2013\\E", "shortCiteRegEx": "Wei and Shulaker", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Not surprisingly, a number of integrated circuit (IC) implementations of ML kernels and algorithms have appeared recently (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) that have set energyefficiency records.", "startOffset": 122, "endOffset": 179}, {"referenceID": 18, "context": "Not surprisingly, a number of integrated circuit (IC) implementations of ML kernels and algorithms have appeared recently (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) that have set energyefficiency records.", "startOffset": 122, "endOffset": 179}, {"referenceID": 21, "context": "one were to repurpose the vast body of knowledge accumulated over two and a half decades by the designers of low-power communication and signal processing systems and ICs (A.P. Chandrakasan & Brodersen, 1992; Shanbhag, 1998; Parhi, 1999).", "startOffset": 171, "endOffset": 237}, {"referenceID": 17, "context": "one were to repurpose the vast body of knowledge accumulated over two and a half decades by the designers of low-power communication and signal processing systems and ICs (A.P. Chandrakasan & Brodersen, 1992; Shanbhag, 1998; Parhi, 1999).", "startOffset": 171, "endOffset": 237}, {"referenceID": 2, "context": "Recent IC implementations (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) do in fact fit this model.", "startOffset": 26, "endOffset": 83}, {"referenceID": 18, "context": "Recent IC implementations (Chen et al., 2016; Kaul et al., 2016; Park et al., 2016) do in fact fit this model.", "startOffset": 26, "endOffset": 83}, {"referenceID": 3, "context": "\u2022 operating at very low voltages (Dreslinski et al., 2010) or low area (Roy et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 19, "context": ", 2010) or low area (Roy et al., 2013), both of which result in computational errors, and/or \u2022 designing systems with emerging devices (Roy et al.", "startOffset": 20, "endOffset": 38}, {"referenceID": 19, "context": ", 2013), both of which result in computational errors, and/or \u2022 designing systems with emerging devices (Roy et al., 2013; Wei et al., 2013) which tend to be intrinsically statistical in nature due to nanoscale imperfections such as variations and defects, and/or", "startOffset": 104, "endOffset": 140}, {"referenceID": 9, "context": "\u2022 embedding computation into memory (in-memory computing (Kang et al., 2014)) and sensing (in-sensor computing (Hu et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 7, "context": ", 2014)) and sensing (in-sensor computing (Hu et al., 2012)) substrates in order to drastically reduce/eliminate data movement.", "startOffset": 42, "endOffset": 59}, {"referenceID": 27, "context": "Statistical information processing involves the use of information-based metrics, statistical error compensation (SEC) (Hegde & Shanbhag, 2001), and retraining approaches such as data-driven hardware resiliency (DDHR) (Wang et al., 2015) to enhance robustness.", "startOffset": 218, "endOffset": 237}, {"referenceID": 22, "context": "One intellectually satisfying aspect of statistical information processing is the potential for developing a comprehensive foundation for reliable information processing on stochastic fabrics much as Claude Shannon (Shannon, 1948) established one for reliable communications over a noisy channel.", "startOffset": 215, "endOffset": 230}, {"referenceID": 17, "context": "Doing so will bring together methodologies such as low-power signal processing algorithms and architectures (Parhi, 1999), algorithm transforms (Shanbhag, 1998), low-power integrated circuit (IC) design (A.", "startOffset": 108, "endOffset": 121}, {"referenceID": 21, "context": "Doing so will bring together methodologies such as low-power signal processing algorithms and architectures (Parhi, 1999), algorithm transforms (Shanbhag, 1998), low-power integrated circuit (IC) design (A.", "startOffset": 144, "endOffset": 160}, {"referenceID": 26, "context": "Indeed, \u201capproximate computing\u201d (Venkataramani et al., 2015) strives to build a methodology to systematize and repurpose these concepts which are well-known and wellpracticed for decades by communication IC designers.", "startOffset": 32, "endOffset": 60}, {"referenceID": 5, "context": "Indeed, minimizing precision (Gupta et al., 2015) is an effective approach to reduce energy.", "startOffset": 29, "endOffset": 49}, {"referenceID": 17, "context": "An almost infinite variety of architectures can be systematically obtained from a DFG using algorithm transforms (Parhi, 1999) such as unfolding, folding, pipelining, systolization, among others.", "startOffset": 113, "endOffset": 126}, {"referenceID": 12, "context": "This opens up the possibility of realizing systolic architectures (Kung, 1982) for many ML algorithms.", "startOffset": 66, "endOffset": 78}, {"referenceID": 8, "context": "Some work already exists (Jones et al., 1994; Kung & Hwang, 1989).", "startOffset": 25, "endOffset": 65}, {"referenceID": 2, "context": "Indeed, one can derive the recently proposed architectures (Chen et al., 2016; Murmann et al., 2015) by formulating the DFG of a convolutional neural Copyright 2015, Naresh Shanbhag 5 d=[1,0]T, p=[0,1]T, and s=[1 0]T regular DFG systolic architecture", "startOffset": 59, "endOffset": 100}, {"referenceID": 16, "context": "Indeed, one can derive the recently proposed architectures (Chen et al., 2016; Murmann et al., 2015) by formulating the DFG of a convolutional neural Copyright 2015, Naresh Shanbhag 5 d=[1,0]T, p=[0,1]T, and s=[1 0]T regular DFG systolic architecture", "startOffset": 59, "endOffset": 100}, {"referenceID": 14, "context": "network (CNN) (LeCun et al., 1998) (see Fig.", "startOffset": 14, "endOffset": 34}, {"referenceID": 3, "context": "For example, near/subthreshold voltage (Dreslinski et al., 2010) operation in CMOS results approximately 10\u00d7 reduction in energy but at the expense of up to 20\u00d7 increase in delay variations.", "startOffset": 39, "endOffset": 64}, {"referenceID": 26, "context": "computing (Venkataramani et al., 2015) as well.", "startOffset": 10, "endOffset": 38}, {"referenceID": 20, "context": "In the past, we have proposed the notion of treating the stochastic circuit fabric as a noisy communication channel (Shanbhag, 1996) and develop Shannon-inspired statistical error compensation (SEC) techniques (see Fig.", "startOffset": 116, "endOffset": 132}, {"referenceID": 23, "context": "3(a))(Hegde & Shanbhag, 2001; Shim et al., 2004; Varatkar et al., 2010) to compensate for the resulting errors at the algorithmic and architectural levels.", "startOffset": 5, "endOffset": 71}, {"referenceID": 25, "context": "3(a))(Hegde & Shanbhag, 2001; Shim et al., 2004; Varatkar et al., 2010) to compensate for the resulting errors at the algorithmic and architectural levels.", "startOffset": 5, "endOffset": 71}, {"referenceID": 11, "context": "4(b)), up to 80% (Kim et al., 2015) can be compensated for by applying techniques based on statistical estimation and detection.", "startOffset": 17, "endOffset": 35}, {"referenceID": 9, "context": "(Kang et al., 2014) and in-sensor computing (Hu et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": ", 2014) and in-sensor computing (Hu et al., 2012).", "startOffset": 32, "endOffset": 49}, {"referenceID": 27, "context": "Another approach is DDHR (Wang et al., 2015) that employs retraining to obtain parameters of the algorithm to compensate for both data and fabric noise.", "startOffset": 25, "endOffset": 44}], "year": 2016, "abstractText": "This position paper advocates a communicationsinspired approach to the design of machine learning systems on energy-constrained embedded \u2018always-on\u2019 platforms. The communicationsinspired approach has two versions 1) a deterministic version where existing low-power communication IC design methods are repurposed, and 2) a stochastic version referred to as Shannon-inspired statistical information processing employing information-based metrics, statistical error compensation (SEC), and retraining-based methods to implement ML systems on stochastic circuit/device fabrics operating at the limits of energy-efficiency. The communications-inspired approach has the potential to fully leverage the opportunities afforded by ML algorithms and applications in order to address the challenges inherent in their deployment on energy-constrained platforms.", "creator": "LaTeX with hyperref package"}}}