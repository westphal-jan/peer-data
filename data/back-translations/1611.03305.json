{"id": "1611.03305", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Getting Started with Neural Models for Semantic Matching in Web Search", "abstract": "Recent advances in language technology have led to unattended neural models for learning word representation and larger text units. Such representations enable powerful semantic matching methods. This survey is intended as an introduction to the use of neural models for semantic matching. To stay focused, we limit ourselves to web search. We explain the background and terminology required, a taxonomy grouping the rapidly growing workload in this area, and then perform reviews of neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to this area. We conclude with an assessment of the state of knowledge and suggestions for future work.", "histories": [["v1", "Tue, 8 Nov 2016 14:28:40 GMT  (75kb)", "http://arxiv.org/abs/1611.03305v1", "under review for the Information Retrieval Journal"]], "COMMENTS": "under review for the Information Retrieval Journal", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["kezban dilek onal", "ismail sengor altingovde", "pinar karagoz", "maarten de rijke"], "accepted": false, "id": "1611.03305"}, "pdf": {"name": "1611.03305.pdf", "metadata": {"source": "CRF", "title": "Getting Started with Neural Models for Semantic Matching in Web Search", "authors": ["Kezban Dilek Onal", "Pinar Karagoz", "Maarten de Rijke"], "emails": ["dilek@ceng.metu.edu.tr,", "altingovde@ceng.metu.edu.tr,", "karagoz@ceng.metu.edu.tr", "k.d.onal@uva.nl,", "derijke@uva.nl"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n03 30\n5v 1\n[ cs\n.I R\n] 8\nN ov\nKeywords Distributed representations \u00b7 Semantic matching \u00b7 Web search"}, {"heading": "1 Introduction", "text": "In web search, the vocabulary mismatch problem [28] necessitates effective semantic similarity functions for textual units of different types. According to Li and Xu [50], semantic matching is concerned with computing the relevance of a document for a query based on representations enriched by linguistic analysis that is meant to capture the semantics of the query and document. In web search, semantic matching is required not only for matching queries to documents but also for matching queries to other textual units. For instance, query suggestions require semantic matching of\nK.D. Onal \u00b7 I.S. Altingovde \u00b7 P. Karagoz Middle East Technical University E-mail: dilek@ceng.metu.edu.tr, altingovde@ceng.metu.edu.tr, karagoz@ceng.metu.edu.tr\nK.D. Onal \u00b7 M. de Rijke University of Amsterdam E-mail: k.d.onal@uva.nl, derijke@uva.nl\nqueries to queries. And retrieving relevant ads for a given query would benefit highly from effective semantic matching.\nRecent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47]. A distributed representation for a textual unit is a dense real-valued vector that somehow encodes the semantics of the textual unit [57]. Distributed representations hold the promise of aiding semantic matching: by mapping words and other textual units to their representations, semantic matches can be computed in the representation space [50]. Indeed, recent improvements in obtaining distributed representations using neural models have quickly been used for semantic matching in web search.\nThe problem of mapping words to a representation that can capture their meanings is referred as distributional semantics and has been studied for a very long time; see [82] for an overview. Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6]. Moreover, Levy et al [48] improve context-counting models by adopting lessons from context-predicting models. Bengio et al [8] seem to have been the first ones to propose a neural language model; they introduce the idea of simultaneously learning a language model that predicts a word given its context and its representation, a so-called word embedding. This idea has been since been adopted by many followup studies. The most well-known and most widely used context-predicting models, Word2Vec [59] and GloVe [73], have been used extensively in recent work on web search. The success of neural word embeddings has also given rise to work on computing context-predicting representations of larger textual units, including paragraphs and documents [39].\nIn recent years, neural network-based models have given rise to significant performance improvements in computer vision, speech processing and machine translation [25]. Although the well-known training algorithm back-propagation dates back to 1980s, training deep neural networks was not possible due to optimization issues such as vanishing gradient and local optima. Training deep neural networks to learn hierarchies of representations became possible owing both to theoretical contributions of new machine learning algorithms and parallelization of training on GPUs [25].\nInspired by the success of neural models in other areas of computer science and artificial intelligence, there has been a tremendous growth in interest in using neural models for semantic matching in information retrieval. Despite the growing literature and a set of emerging patterns, to the best of our knowledge, there is no proper introduction to this emerging area. Deng and Yu [25] devote a chapter to applications of deep learning in information retrieval. However, this chapter only covers very early work that is mostly focused on document representations for retrieval. The recent tutorial on deep learning for information retrieval by Li and Lu [49] sketches a range of potential applications of deep learning to information retrieval (IR) problems with a broader scope.\nThe purpose of this survey is to provide a systematic introduction to the emerging area of neural models for semantic matching. While the area is still growing rapidly, by now a solid body of shared methods and architectures has been established, which we believe justifies a systematic introduction. We review the literature in the area and consider both word representations and representations of larger textual units, such as sentences and paragraphs. We survey the use of neural language models and word embeddings, we detail applications of so-called neural semantic compositionality models that determine semantic representations of larger text units from smaller ones, and we present novel neural semantic compositionality models designed specifically for web search tasks. In addition to surveying relevant literature, we provide the reader with a foundation of on neural language models and with pointers to relevant resources that those who are new to the area should appreciate.\nTo remain focused we limit the scope of this survey to the use of neural models for building distributed representations of textual units in web search. Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope. Specifically, we focus on three major web search tasks, namely query suggestion, ad retrieval and document retrieval. As we will see, each task requires semantic matching of different types of textual units.\nSection 5, 6, and 7 are devoted to studies on document retrieval, query suggestion and ad retrieval in web search, respectively. Prior to this, we introduce relevant background information on neural language models in Section 2, survey available resources in Section 3, and detail the dimensions that we use for categorizing the publications we review in Section 4."}, {"heading": "2 Background and terminology", "text": "In this section we briefly review a number of relevant key concepts underlying the use of neural models for semantic matching, viz. distributional semantics, semantic compositionality, neural language models, training procedures for neural language models, Word2Vec and GloVe, and paragraph vectors.\nWe assume that the reader has a basic understanding of neural networks (including RNNs, CNNs, LSTMs), back propagation, forward propagation, and gradient descent as can be found in, e.g., Part II of [31].\n2.1 Distributional semantics\nA distributional semantic model (DSM) is a model that relies on the distributional hypothesis [38], according to which words that occur in the same contexts tend to have similar meanings, for associating words with vectors that can capture their meaning. Statistics on observed contexts of words in a corpus is quantified to derive word vectors. The most common choice of context is the set of words that co-occur in a context window.\nBaroni et al [6] classify existing DSMs into two categories: context-counting and context-predicting. The context-counting category includes earlier DSMs such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24]. In these models, low-dimensional word vectors are obtained via factorisation of a highdimensional sparse co-occurrence matrix. The context-predicting models are neural language models in which word vectors are modelled as additional parameters of a neural network that predicts co-occurrence likelihood of context-word pairs. Neural language models comprise an embedding layer that maps a word to its distributed representation. A distributed representation of a symbol is a vector of features that characterize the meaning of the symbol and are not mutually exclusive [57].\nEarly neural language models were not aimed at learning representations for words. However, it soon turned out that the embedding layer component, which addresses the curse of dimensionality caused by one-hot vectors [8], yields useful distributed word representations, so-called word embeddings. Collobert and Weston [22] are the first ones to show the benefit of word embeddings as features for NLP tasks. Subsequently, word embeddings became widespread after introduction of the shallow models Skip-Gram and CBOW in the Word2Vec framework by Mikolov et al [59, 60]; see Section 2.5.\nBaroni et al [6] report that context-predicting models outperform context-counting models on several tasks including question sets, semantic relatedness, synonym detection, concept categorization and word analogy. In contrast, Levy et al [48] point out that the success of the popular context-predicting models Word2Vec and GloVe does not originate from the neural network architecture and the training objective but from the choices of hyper-parameters for contexts. A comprehensive analysis reveals that when these hyper-parameter choices are applied to context-counting models, no consistent advantage of context-predicting models is observed over context-counting models. To sum up, it is possible to obtain word vectors that can encode semantics successfully, owing to the contributions brought by neural language models.\n2.2 Semantic compositionality\nCompositional distributional semantics or semantic compositionality (SC) is the problem of formalizing how the meaning of larger textual units such as sentences, phrases, paragraphs and documents are built from the meanings of words [62]. Work on SC studies are motivated by the Principle of Compositionality which states that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them.\nA neural SC model maps the high-dimensional representation of a textual unit into a distributed representation by forward propagation in a neural network. The neural network parameters are learned by training to optimize task-specific objectives. Both the granularity of the target textual unit and the target task play an important role for the choice of neural network type and training objective. An SC model that considers the order of words in a sentence and aims to obtain a deep understanding may fail in an application that requires representations that can encode high-level concepts in a large document. A comparison of neural SC models of sentences learned\nfrom unlabelled data is presented in [39]. Besides the models reviewed by Hill et al [39], there exist sentence-level models that are trained using task-specific labelled data. For instance, a model can be trained to encode the sentiment of a sentence using a dataset of sentences annotated with sentiment class labels [51].\nTo the best of our knowledge, there is no survey on neural SC models for distributed representations of long documents, although the representations are useful not only for document retrieval but also for document classification and recommendation. In Section 4.3.2 we review the subset of neural SC models and associated training objectives adopted specifically in web search tasks.\n2.3 Neural language models\nA language model is a function that predicts the acceptability of pieces of text in a language. Acceptability scores are useful for ranking candidates in tasks like machine translation or speech recognition. The probability of a sequence of words P (w1, w2, . . . , wn) in a language, can be computed by Equation 1, in accordance with the chain rule:\nP (w1, w2, . . . , wt\u22121, wt) = P (w1)P (w2 | w1) \u00b7 \u00b7 \u00b7P (wt | w1, w2, . . . , wt\u22121) (1)\nProbabilistic language models, mostly approximate Equation 1 by P (wt | wt\u2212n, . . . , wt\u22121), considering only a limited context of size n, that immediately precedes wt. In neural language models the probability P (w | c) of a word w to follow the context c is computed by a neural network. The neural network takes a context c and outputs the conditional probability P (w | c) of every word w in the vocabulary V of the language:\nP (w | c, \u03b8) = exp(s\u03b8(w, c)) \u2211\nw\u2032\u2208V exp(s\u03b8(w \u2032, c))\n. (2)\nHere, s\u03b8(w, c) is an unnormalized score for the compatibility of w given the context c; s\u03b8(w, c) is computed via forward propagation of the context c through a neural network defined with the set of parameters \u03b8. Note that P (w | c) is computed by the normalized exponential (softmax) function in Equation 2, over the s\u03b8(w, c) scores for the entire vocabulary.\nParameters of the neural network are learned by training on a text corpus using gradient-descent based optimization algorithms to maximize the likelihood function L in Equation 3, on a corpus:\nL(\u03b8) = \u2211\n(t,c)\u2208T\nP (t | c, \u03b8) (3)\nThe first neural language model published is the Neural Network Language Model (NNLM) [8]. The common architecture shared by neural language models is depicted in Figure 1, with example input context c = w1, w2, w3 and the word to predict being w4, extracted from the observed sequence c = w1, w2, w3, w4. Although a probability distribution over the vocabulary V is computed, the word that should have the maximum probability is shown at the output layer, for illustration purposes.\nThe neural network takes one-hot vectors w1, w2, w3 of the words in the context. The dimensionality of the one-hot vectors is 1\u00d7 |V |. The embedding layer E in Figure 1 is indeed a |V | \u00d7 d-dimensional matrix whose i-th row is the d-dimensional word embedding for the i-th word in the vocabulary. The embedding vector of the i-th word in the vocabulary is obtained by multiplying the one-hot vector of the word with the E matrix or simply extracting the ith row of the embedding matrix. Consequently, high dimensional one-hot vectors of words w1, w2, w3 are mapped to their d-dimensional embedding vectors e1, e2, e3 by the embedding layer. Note that d is usually chosen to be in the range 100\u2013500 whereas |V | can go up to millions.\nThe hidden layer in Figure 1 takes the embedding vectors e1, e2, e3 of the context words and creates a vector hc for the input context. This layer differs between neural language model architectures. In NNLM [8] it is a non-linear neural network layer whereas in the Continuous Bag of Words (CBOW) model of Word2Vec [59], it is vector addition over word embeddings. In the Recurrent Neural Network Language Model (RNNLM) [58] the hidden context representation is computed by a recurrent neural network. Besides the hidden layer, the choice of context also differs among models. In [22] and in the CBOW model [59] context is defined by the words that surround a center word in a symmetric context. In the NNLM [8] and RNNLM [58] models, the context is defined by words that precede the target word. The Skip-Gram [59] takes a single word as input and predict words from a dynamically sized symmetric context window around the input word.\nThe classifier layer in Figure 1, which is composed of a weights matrix C of dimension d \u00d7 |V | and a bias vector of dimension |V |, is used to compute s\u03b8(w, c) using Equation 4:\ns\u03b8(w, c) = hcC + b. (4)\nTo sum up, the neural network architecture for a Neural Language Model (NLM) is defined by |V |, d, the context type, the context size |c| and the function in the hidden layer. The parameter set \u03b8 to be optimized includes the embedding matrix E, parameters from the hidden layer, the weights matrix C and the bias vector b of the classifier layer. The embedding layer E is treated as an ordinary layer of the network, its weights are initialized randomly and updated with back-propagation during training of the neural network.\n2.4 Efficient training of NLMs\nAs mentioned previously in our discussion of Equation 1, the output of a NLM is a normalized probability distribution over the entire vocabulary. Therefore, for each training sample (context pair (t, c)), it is necessary to compute the softmax function in Equation 2 and consider the whole vocabulary for computing the gradients of the likelihood function in back-propagation. This makes the training procedure computationally expensive and prevents the scalability of the models to very large corpora.\nSeveral remedies for efficiently training NLMs have been introduced. Chen et al [17] present a comparison of training methods for the NNLM. Hierarchical Softmax [67] and differentiated softmax [17] propose adapted softmax layer architectures for efficient computation of the softmax function. Another solution, adopted by methods like Importance Sampling (IS) [9] and Noise Contrastive Estimation (NCE) [66], is to avoid the normalization by using modified loss functions to approximate the softmax. Collobert and Weston [22] propose the cost function in Equation 5, which does not require normalization over the vocabulary. The NLM is trained to compute higher s\u03b8 scores for observed context-word pairs (c, t) compared to the negative samples constructed by replacing t with any other word w in V . The context is defined as the words in a symmetric window around the center word t.\n\u2211\n(t,c)\u2208T\n\u2211\nw\u2208V\nmax(0, 1\u2212 s\u03b8(t, c) + s\u03b8(w, c)) (5)\nMnih and Teh [66] apply NCE [36] to NLM training. By using NCE, the probability density estimation problem is converted to a binary classification problem. A twoclass training data set is created from the training corpus by treating the observed context-word pairs (t, c) as positive samples and noisy pairs (t\u2032, c) constructed replacing t with a word t\u2032 sampled from the noise distribution q. The NCE cost function defined for k negative samples per observed sample is given in Equation 6, where conditional probabilities of classes are computed as in Equation 7:\nLNCEk = \u2211\n(t,c)\u2208T\n\nlog p(l = 1 | t, c) + \u2211\ni=1,t\u223cq\nk log p(l = 0 | t\u2032, c)\n\n (6)\np(l = 0 | w, c) = k \u00d7 q(w)\ns\u03b8(w, c) + k \u00d7 q(w) (7)\np(l = 1 | w, c) = s\u03b8(w, c)\ns\u03b8(w, c) + k \u00d7 q(w) . (8)\n2.5 Word2Vec and GloVe\nMikolov et al [59] introduce the Skip-Gram and CBOW models that follow the NLM architecture with a linear layer for computing a distributed context representation. Figure 2 illustrate the architecture of Word2Vec models with context windows of size five. The CBOW model is trained to predict the center word of a given context. In the CBOW model, the hidden context representation is computed by the sum of the word embeddings. On the contrary, the Skip-Gram model is trained to predict words that occur in a symmetric context window given the center word. The name Skip-Gram is used since the size of the symmetric context window is sampled randomly from the range [0, c] for each word. Skip-Gram embeddings are shown to outperform embeddings obtained from NNLMs and RNNLMs in capturing the semantic and syntactic relationships between the words.\nEfficient training of Skip-Gram and CBOW models is achieved by hiearchical softmax [67] with a Huffman tree Mikolov et al [59]. In follow-up work [60], Negative Sampling (NEG) is proposed for efficiently training the Skip-Gram model. NEG is a variant of NCE in which conditional probabilities are computed by Equation 9. NEG adopts the loss function in Equation 6 but the noise distribution q is assumed to be uniform and k = |V | while computing the conditional probabilities. Skip-Gram with Negative Sampling (SGNS) departs from the goal of learning a language model and is biased towards the quality word embeddings. SGNS is never used as a language model, therefore it should be considered to be a DSM rather than a language model:\np(l = 0 | w, c) = 1\ns\u03b8(w, c) + 1 (9)\np(l = 1 | w, c) = s\u03b8(w, c)\ns\u03b8(w, c) + 1 (10)\nSubsampling frequent words is another extension introduced in [60] for speeding up training and increasing the quality of embeddings. Each word wi in the corpus is\ndiscarded with probability p(wi), given in Equation 11:\np(wi) = 1\u2212\n\u221a\nt\nf(wi) . (11)\nGlobal Vectors (GloVe) [73] combines global context and local context in the training objective for learning word embeddings. In contrast to NLMs, where embeddings are optimized to maximize the likelihood of local contexts, GloVe embeddings are trained to fit the co-occurrence ratio matrix.\nLevy et al [48] discuss that diluting frequent words before training enlarges the context window size in practice. Experiments show that the hyper-parameters about context-windows like dynamic size and subsampling frequent words have a notable impact on the performance of SGNS and GloVe [48]. Levy et al show that when these choices are applied to traditional DSMs, no consistent advantage of SGNS and GloVe is observed. In contrast to the conclusions obtained in [6], the success of context-predicting models is attributed to choice of hyper-parameters, which can also be used for context-counting DSMs, rather than to the neural architecture or the training objective.\n2.6 Paragraph vector\nThe Paragraph Vector [47] extends Word2Vec in order to learn representations for socalled paragraph, textual units of any length. Similar to Word2Vec, it is composed of two separate models, namely Paragraph Vector with Distributed Memory (PV-DM) and Paragraph Vector with Distributed Bag of Words (PV-DBOW). The architectures of PV-DM and PV-DBOW are illustrated in Figure 3. The PV-DBOW model is a Skip-Gram model where the input is a paragraph instead of a word. The PV-DBOW is trained to predict a sample context given the input paragraph. In contrast, the PV-DM model is trained to predict a word that is likely to occur in the input paragraph after the sample context. The PV-DM model is a CBOW model extended with a paragraph in the input layer and a document embedding matrix. In the PV-DBOW model, only paragraph embeddings are learned whereas in the PV-DM model word embeddings and paragraph embeddings are learned, simultaneously.\nIn Figure 3, p stands for the index of the input paragraph and w1, w2, w3, w4 represent the indices of the words in a contiguous sequence of words sampled from this paragraph. A sequence of size four is selected just for illustration purposes. Also, D represents the paragraph embedding matrix and E stands for the word embedding matrix. At the lowest layer, the input paragraph p is mapped to its embedding by a lookup in the D matrix. The hidden context representation is computed by summing the embeddings of the input words and paragraph, which is the same as in the CBOW model.\nParagraph vector models are trained on unlabelled paragraph collections. An embedding for each paragraph in the collection is learned at the end of training. The embedding for an unseen paragraph can be obtained by an additional inference stage. In the inference stage, D is extended with columns for new paragraphs; D is updated using gradient descent while other parameters of the model are kept fixed.\nLe and Mikolov [47] assess vectors obtained by averaging PV-DM and PV-DBOW vectors on sentiment classification and snippet retrieval tasks. The snippet retrieval experiments are performed on a dataset of triplets created using snippets of the top 10 results retrieved by a search engine, for a set of 1 million queries. Each triplet is composed of two relevant snippets for a query and a randomly selected irrelevant snippet from the collection. Cosine similarity between paragraph vectors is shown to be an effective similarity metric for distinguishing similar snippets in such triplets. Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents."}, {"heading": "3 Resources", "text": "In this section, we present pointers to publicly available resources and tools which the reader would benefit for getting started with neural models, distributed representations, and information retrieval experiments with semantic matching.\n3.1 Introductory tutorials\nGoldberg [30] and Cho [18] provide tutorials on getting started with neural networks from the natural language understanding perspective. Goldberg [30] covers details on training neural networks and a broader set of architectures including feed-forward networks, convolutional networks, recurrent networks and recursive networks. Cho [18] focuses on language modeling and machine translation, sketches a clear picture of the encoder-decoder architectures, recurrent networks and attention modules [5].\n3.2 Word embeddings\nCorpora used Wikipedia and GigaWord5 are the corpora widely used for learning word embeddings. The latest Wikipedia dump can be obtained from Wikimedia.1 The GigaWord5 data set is accessible through the LDC.2 Several authors have learned embeddings from query logs [14, 41, 81].\nPre-trained word embeddings It is possible to obtain pre-trained GloVe embeddings3 and CBOW embeddings learned from Bing query logs [68].\nLearning word embeddings The source code for GloVe [73]4 and the models introduced in [48]5 is publicly shared by the authors. Implementations of the Word2Vec and Paragraph Vector models are included in the gensim library.6\nVisualizing word embeddings The dimensionality reduction technique t-Distributed Stochastic Neighbor Embedding (t-SNE) [56] is commonly used for visualizing word embedding spaces and for trying to understand the structures learned by neural models.7\n3.3 Test corpora used for retrieval experiments\nFor retrieval experiments regarding neural models for semantic matching, the full range of CLEF, FIRE and TREC test collections has been used. Specifically, for evaluating neural models for semantic matching in an end-to-end web search task, TREC collections such as ClueWeb [35], .GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.\n3.4 Implementing neural SC models\nTheano,8 TensorFlow9 and Torch10 are libraries that are widely used by the deep learning community for implementing neural network models. These libraries enable construction of neural network models from pre-defined high-level building blocks\n1 https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles. xml.bz2\n2 https://catalog.ldc.upenn.edu/LDC2011T07 3 http://nlp.stanford.edu/projects/glove/ 4 http://nlp.stanford.edu/projects/glove/ 5 https://bitbucket.org/omerlevy/hyperwords 6 https://radimrehurek.com/gensim/ 7 https://lvdmaaten.github.io/tsne/ 8 http://deeplearning.net/software/theano/ 9 https://www.tensorflow.org/\n10 http://torch.ch/\nsuch as hidden units and layers. It is possible to define neural network models with different choices of architectures, non-linearity functions, etc.\nGPU support and automatic differentiation [7] are crucial features required for training neural networks. Theano and TensorFlow, enable performing matrix operations efficiently, in parallel, on GPU. Training neural networks with back-propagation requires computation of derivatives of the cost function with respect to every parameter of the network. Automatic differentiation in Theano and TensorFlow relieve the users from the effort on the manual derivation of derivatives of the objective function. These libraries compute the derivatives automatically given the definition of the neural network architecture and the cost function.\nThe deep learning community has a strong tradition of sharing, in some version, the code used to produce the experimental results reported in the field\u2019s publications. The information retrieval is increasingly adopting this attitude too. Some authors of publications on neural models for semantic matching in web search have shared their code; see e.g., [81, 83, 84, 93].11\n3.5 Experimenting with neural models\nExperimenting with neural networks is sometimes thought to be a bit of a \u201cdark art.\u201d Countless blog posts12 have been devoted to this as well as many posts on community question answering sites such as Stack Overflow13 and Quora.14 These are often valuable sources of advice, with source code snippets and illustrations."}, {"heading": "4 Taxonomy", "text": "As part of this introduction to neural models for semantic matching in web search, we provide a survey of related work. To organize the material, we use a simple taxonomy. We classify related work based on the target textual unit (TTU) for which distributed representations are required, how distributed representations are built, and the intended usage of the distributed representations learned. We summarize these features in Table 1."}, {"heading": "4.1 TTU", "text": "The set of TTUs depends on the target task. Distributed representations for queries are required for all the web search tasks covered in this survey. Representations for documents and ads are additionally required for document and ad retrieval tasks, respectively.\n11 https://github.com/ielab/adcs2015-NTLM, https://github.com/ cvangysel/SERT,https://github.com/cvangysel/SERT\n12 See, e.g., http://colah.github.io/, http://sebastianruder.com/#open, http: //www.wildml.com/,http://neuralnetworksanddeeplearning.com/ to mention a few.\n13 http://stackoverflow.com/questions/tagged/deep-learning 14 https://www.quora.com/topic/Deep-Learning\n4.2 Usage\nThere are two choices of usage observed in the work that we reviewed:\nSimilarity The representations are used to compute semantic similarity of TTU pairs directly. Feature TTU representations are used as additional or standalone features in existing supervised settings, mostly in learning to rank frameworks, for web search.\n4.3 How\nThe how feature defines the method for building TTU representations. The methods can be grouped in two main classes aggregate and learn, which we discuss below."}, {"heading": "4.3.1 Aggregate", "text": "The methods in this category rely on pre-trained word embeddings as external resources for distributed TTU representations. Existing work can be split into two subcategories depending on how the embeddings are utilized:\nExplicit Word embeddings are considered as building blocks for distributed TTU representations. The works that follow this pattern treat a TTU as Bag of Embedded Words (BoEW) or a set of points in the word embedding space. The most common aggregation method is averaging or summing the vectors of the terms in the TTU. Implicit Here, one utilizes the vector similarity in the embedding space in language modeling frameworks without explicit computation of similarity based on distributed representations for TTUs. For instance, Zuccon et al [93] compute translation probabilities of word pairs in a translation language model retrieval framework with cosine similarity of Skip-Gram vectors."}, {"heading": "4.3.2 Learn", "text": "This category covers work on learning neural semantic compositionality models for distributed representations. Three separate training objectives are observed in the reviewed work. Based on these objectives, we define the three sub-categories, namely neural learn to match, learn to predict context and learn to generate context, all of which are detailed below.\nNeural learning to match Learning to match [50] is the problem of learning a matching function f(x, y) that computes the similarity degree of two objects x and y from two different spaces X and Y . Given training data T composed of triples (x, y, r), learning to match is the optimization problem in Equation 12. There, L denotes a loss function between the actual similarity score r and the score predicted by the f function:\nargmin f\u2208F\n\u2211\n(x,y,r)\u2208T\nL(r, f(x, y)). (12)\nNeural learning to match models rely on distributed representations generated by neural networks for computing the similarity between objects. They are based on siamese neural networks [13], as illustrated in Figure 4, which consist of two identical sub-networks joined at their outputs in order to compute the similarity of the inputs. Input vector representations x, y are mapped into distributed representations hx, hy by the sub-network that we call Semantic Compositionality Network (SCN) and the similarity score of the objects is computed by the similarity of the distributed representations. Usually, cosine similarity is used to compute the similarity vectors created by the SCN, as given in Equations 13 and 14:\nhx = SCN(x), hy = SCN(y) (13)\nf(x, y) = hx \u00b7 hy |hx| |hy| . (14)\nLearning to match models require similarity assessments for training. Since it is difficult to obtain large amounts of supervised data, click information in click-through\nlogs are exploited to derive similarity assessments for query-document and query-ad pairs. If a pair (x, y) is associated with clicks, the objects are accepted to be similar and dissimilar in the absence of clicks. In the case of query-query pairs, co-occurrence in a session is accepted as similarity signal that can be extracted from query logs.\nThe training objective common to the models in this category is to minimise the negative log likelihood function in Equation 15. We replace x and y with q and d to represent a query and a document object, respectively. The document object can be any textual unit that is needed to be matched against a query, such as a document, query or ad; (q, d+) denotes each (query-clicked document) pair extracted from logs andD = {d+}\u222aD\u2212 denotes the set of all documents in the collection. The likelihood of a document d given a query q, is computed by Equation 16 with a softmax over similarity scores of distributed representations.\nL = \u2212 log \u220f\n(q,d+)\nP (d+ | q) (15)\nP (d | q) = exp(\u03b3f(q, d)) \u2211\nd\u2032\u2208D exp(\u03b3f(q, d \u2032)\n(16)\nL requires computation of a probability distribution over the entire document collection D for each (query-clicked document) pair. Since this is computationally expensive, D\u2212 is approximated by a randomly selected small set of unclicked documents, similar to the softmax approximation methods for neural language models in Section 2.4. In Figure 4 the training strategy is illustrated with four randomly selected nonrelevant documents. Let d1 be a relevant document and d2, d3, d4, d5 non-relevant documents for the query q. The siamese network is applied to compute similarity scores for each pair (q, di).\nLearn to predict context The success of word-based neural language models has motivated models for learning representations of larger textual units [39, 47] from unlabelled data. As mentioned previously, neural language models are context-predicting DSMs. The learn to predict models rely on an extension of the context-prediction idea to larger linguistic units, which is that similar textual units occur in similar contexts. For instance, sentences in a paragraph and paragraphs in a document are semantically related. The organisation of textual units\u2014of different granularity\u2014in corpora created by human can be exploited to learn distributed representations. Contextual relationships of textual units are exploited to design training objectives similar to neural language models.\nWe refer to the models that rely on the extended distributional semantics principle to obtain distributed TTU representations as learn to predict context models in accordance with the context-predicting label used for neural language models. Learn to predict context models are neural network models trained using unlabelled data to maximize the likelihood of the context of a TTU. Among the models reviewed in [39], Skip-thought Vector [44] and Paragraph Vector (PV) [47] are successful representatives of the learn to predict context idea. The training objective of the Skip-Thought Vector is to maximize the likelihood of the previous and the next sentences given an input sentence. The context of the sentence is defined as its neighboring sentences. Recall that details of the paragraph vector model are provided in Section 2.\nThe main context of a textual unit is the words it contains. Containment should be seen as a form of co-occurrence and the content of a document is required to define its textual context. Two documents are similar if they contain similar words. Besides the content, temporal context is exploited in all studies in this category for defining TTU contexts. For instance, the context of the query can be defined by the other queries in the session [34]. Finally, context of a document is defined by joining its content and the neighboring documents in a document stream in [27].\nLearn to generate context This category covers recent work in which a synthetic textual unit is generated based on the distributed representation of an input TTU. Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86]. In these applications, an input textual or visual object is encoded into a distributed representation with a neural network and a target sequence of words\u2014a translation in the target language or a caption\u2014is generated by a decoder network.\nLearn to predict context models are focused on learning embeddings for a fixed collection of TTUs. In contrast, learn to generate context models are designed to generate unseen synthetic textual units. In learn to predict context models, an additional\ninference step is required for obtaining the embedding of an unobserved TTU, as in the paragraph vector model. In the learn to generate context models, distributed representations of unseen TTUs can be computed by forward propagation through the neural network model.\n4.4 Roadmap\nIn Table 2, we provide a classification of reviewed work with respect to the features. For the publications in the aggregate, implicit category, the usage feature is not applicable since an explicit distributed representation is not constructed. Besides, in [53], which falls into learn to generate context category, the generated document is not used in a quantitative experimental framework. These exceptions are indicated using the \u2013 sign.\nIn Section 5, 6, and 7 below we survey work on neural models for semantic matching in document retrieval, query suggestion, and ad retrieval, respectively."}, {"heading": "5 Document retrieval", "text": "In this section we survey work on neural models for semantic matching for document retrieval. We follow the how feature as explained in Section 4 for organizing the material discussed. Since query expansion and query re-weighting aim to improve retrieval effectiveness by query analysis, publications on these tasks are also included in this section.\n5.1 Aggregate\nIn this section, we present the works that rely on pre-trained word embeddings for document retrieval under the implicit and explicit categories. In each subsection, we mention the works on query expansion and query re-weighting, separately."}, {"heading": "5.1.1 Explicit", "text": "This category includes publications in which query and document are viewed as Bag of Embedded Words (BoEW). Each word is associated with an embedding vector. Query-document relevance is computed using BoEW representations for the query and document. Besides query-document matching, we present query expansion studies that rely on term similarities in the embedding space to find similar terms to query.\nIn Table 3, we provide a summary of the methods. The NLM column specifies the model used for learning word embeddings, as mentioned by the authors. The second and third columns list the representations adopted for query and document, respectively. The IN and OUT suffixes in these columns indicate whether input or output embeddings are utilized. As mentioned in Section 2.3, each NLM comprises two matrices E and C, of size |V | \u00d7 d. Here, E is the matrix for the embedding layer which maps the input one-hot vectors to embedding vectors; C is the weights matrix of the classifier layer. The E matrix is referred as IN embeddings whereas the C matrix as OUT embeddings, in accordance with the naming in [68]. The last two rows of Table 3 cover publications on query expansion and re-weighting. The similarity column for these publications indicates the functions utilized for computing term or term-query similarities.\nNalisnick et al [68] point out an important feature of CBOW embeddings: the neighbors of a word represented with its IN embedding vector in the OUT space are topically similar words. And within the same embedding space, either IN or OUT, the neighbors are functionally similar words. Topically similar words are likely to co-occur in a local context whereas functionally similar words are likely to occur in similar contexts. For instance, for the term harvard, faculty, alumni, graduate are topically terms and yale, stanford, cornell functionally similar terms. Motivated by this observation, Nalisnick et al [68] propose the Dual Embedding Space Model (DESM). In the DESM a query is represented with a BoEW in the IN space and the document is represented with a BoEW in OUT space. Query-document similarity is computed by aggregating cosine similarities across all the query-document word pairs [65, 68]. DESM is evaluated in a document ranking setting with both explicitly judged data sets and implicit feedback based data sets in [65]. On explicitly judged of implicit feedback based test data sets, DESM outperforms the BM25 and LSA baselines. However, when DESM is used standalone for ranking documents from a large test collection, its performance is significantly lower than that of the LDA and BM25 baselines. DESM is shown to be an effective signal for re-ranking the document set retrieved by a first-stage retrieval model.\nIn the explicit category, Vulic\u0301 and Moens [85] propose to construct query and document representations as a sum of word embeddings learned from a pseudobilingual document collection with a Skip-Gram model. Each document pair in a document-aligned translation corpus is mapped to a pseudo-bilingual document by merging source and target documents, removing sentence boundaries and shuffling the complete document. Owing to these shuffled pseudo-bilingual documents, the words from source and target language are mapped to the same embedding space. This approach for learning bilingual word embeddings is referred as Bilingual word Embeddings Skip-Gram (BWESG). Documents are ranked by the cosine similarity of their embedding vector to the query vector. The query-document representations are evaluated both on cross-lingual and mono-lingual retrieval tasks. For the monolingual experiments, ranking the proposed distributed representations outperforms ranking LDA representations. Moreover, the word embedding-based representation is shown to bring more improvements in MAP scores when combined with a unigram language model, compared to LDA. Embedding-based representations yield higher MAP scores in cross-lingual retrieval experiments too. However, when combined with the unigram language model, LDA-based representations outperformed embedding-based representations in the EN-to-NL retrieval direction. When embedding-based representations are integrated into the unigram language model combined with LDA representations, MAP scores in the EN-NL direction can be increased. Another lesson from this work is that an IDF-weighted sum of word embeddings yields representations that can produce higher MAP scores than the summation of word embeddings. Aligned English and Dutch Wikipedia articles together with EuroParl English-Dutch document pairs are used for learning word embeddings. Both monolingual and bilingual retrieval experiments are conducted using CLEF 2001\u20132003 collections.\nIn [76], documents are modelled as a mixture distribution that generates the observed terms in the document. Roy et al estimate this distribution with k-means clus-\ntering of embeddings of the terms in the document. The likelihood of a query to be generated by the document is computed by the average inter-similarity of the set of query terms to the centroids of clusters in the document, in the word embedding space. For efficiency, the global vocabulary is clustered using Word2Vec embeddings in advance and document specific clusters are created by grouping the terms according to their global cluster ids. A centroid-based query likelihood function is evaluated in combination with language modeling with Jelinek-Mercer smoothing on the TREC 6-7-8 and TREC Robust data sets. A significant improvement is observed by the inclusion of word embedding-based query-likelihood function over the standalone language model (LM) baseline.\nAlthough focused on document classification, it is worth mentioning the Word Mover\u2019s Distance metric by Kusner et al [45] in the implicit category since it proposes to integrate word embedding similarity into a document distance metric. Kusner et al propose the document distance metric Word Mover\u2019s Distance (WMD) inspired by the well-known transportation problem Earth Mover\u2019s Distance. Each document is represented as a set of points in the word embedding space. The WMD of two documents is the minimum cumulative distance that all words in the first document need to travel to exactly match document the second document. Euclidean distance is used for computing word distances. The WMD metric is evaluated with kNN document classification experiments on different data sets and different types of textual units such as recipes, news documents, medical abstracts, tweets and sentences. WMD is reported to outperform the BoW, BM25, TF-IDF, Latent Semantic Indexing (LSI), LDA, Marginalized Stacked Denoising Autoencoder, and CCG Componential Counting Grid methods in terms of test error rate.\nA different explicit aggregation method based on BoEW representations is the Fisher Kernel proposed in [21]. This work is based on LSI embeddings, not on neural word embeddings yet it is worth mentioning here since it proposes a different aggregation method. The Fisher Kernel commonly used for constructing an image representation using local descriptors extracted from image patches is used to aggregate word embeddings into a fixed length document representation. The Fisher vector representation is compared to document representations obtained LSI, LDA and Probabilistic Latent Semantic Analysis (PLSA) document representations on document clustering and ad-hoc IR tasks. For ad-hoc retrieval experiments, CLEF\u201903, TREC 1 and 2 and ROBUST data sets are leveraged. In both tasks, the Fisher vector representation outperforms LSI, LDA and PLSA. However, on the IR task it is outperformed by the IR baseline Divergence From Randomness (DFR).\nQuery expansion and re-weighting ALMasri et al [3] use Skip-Gram and CBOW embeddings for extracting most similar terms for a given query term. Neighbors of each query term in the embedding space are used to construct an expansion term set. Expansion terms are weighted in proportion to their frequency in the expanded query. The expansion method is evaluated on CLEF medical collections and compared to expansion with PRF and expansion with Mutual Information. Experiments show that expansion with word embeddings provides statistically significant improvements over the PRF method.\nThe work on query expansion [69] based on word embeddings, although proposed for result merging in federated web search, can also be noted here since it introduces the idea of an importance vector in the query re-weighting method in [92]. In [69], a query is represented by the mean vector of query term embeddings and k-nearest neighbors of the query vector are selected to expand the query. Expansion terms are re-weighted with respect to their distance to the query vector. A query term that is more distant to the query vector is assumed to contain more information and is assigned a higher weight. Query re-weighting is modelled as a linear regression problem from importance vectors to term weights in [92]. The importance vector, which is the offset between the query vector and the term vector, is used as the feature vector for the query term. A linear regression model is trained using the ground-truth term weights computed by term recall weights which is the ratio of relevant documents that contain the query term t to the total number of relevant documents to the query q. Weighted queries based on a learned regression model are evaluated in a retrieval setting and compared against the LM and BM-25 models using the data sets ROBUST04, WT10g, GOV2, ClueWeb09B. Two variants of the model DeepTR-BOW and DeepTR-SD are compared against unweighted queries, sequential dependency models and and weighted sequential dependency models. Statistically significant improvement is observed at high precision levels and throughout the rankings compared to the first two methods.\nRoy et al [77] propose a set of query expansion methods, based on selecting k nearest neighbors of the query terms in the word embedding space and ranking these terms with respect to their similarity to the whole query. The search space for neighbors either covers the entire vocabulary or is limited to terms in the top-ranking documents from an initial retrieval. All of the expansion methods yielded lower effectiveness scores than a statistical co-occurrence based feedback method, in experiments with the TREC 6, 7 and 8 and TREC Robust data set and the LM with Jelinek Mercer smoothing as the retrieval function."}, {"heading": "5.1.2 Implicit", "text": "Zuccon et al [93] compute translation probabilities of word pairs using the cosine similarity of Word2Vec vectors in a translation language model for retrieval. The translation model enriched by embedding similarity scores is called the Neural Translation Language Model (NLTM). NLTM is shown to outperform the Dirichlet LM baseline and to be comparable to translation model with Mutual Information.\nCosine similarity of Word2Vec embeddings is used in a similar way in the Generalized Language Model (GLM) [29]. In the GLM, the probability of a query term t to be generated by a document or the collection is modelled by generating an intermediate term t\u2032 followed by a noisy channel model that transforms t to t\u2032. Cosine similarity of word embeddings is used for computing the transformation probabilities between the intermediate term and the actual term. The GLM outperforms LM and LDA-smoothed LM baselines in terms of MAP score on the TREC 6, 7 and 8 and TREC Robust data sets. However, an LDA smoothed LM achieved higher recall scores.\nQuery expansion In [26, 88], word embeddings are used for defining a new query language model (QLM). A QLM specifies a probability distribution p(w | q) over all the terms in vocabulary. In query expansion with language modeling, the top m terms w that have the highest p(w | q) value are selected as expansion terms [16]. Diaz et al [26], propose a query expansion language model based on word embeddings learned from topic-constrained corpora. When word embeddings are learned from a topically-unconstrained corpora, they can be very general. Therefore, a query language model is defined based on word embeddings learned using a subset of documents sampled from a multinomial created by applying softmax on KL divergence scores of all documents in the corpus. The original query language model is interpolated with the query expansion language model which is defined by weights of terms computed by the UUT q where U is the |V | \u00d7 d dimensional embedding matrix and q is the |V | \u00d7 1 dimensional term matrix. Locally-trained embeddings are compared against global embeddings on TREC12, Robust and ClueWeb 2009 Category B Web corpus. Local embeddings are shown to yield higher NDCG@10 scores. Besides the local and global option, the authors also investigate the effect of using the target corpus versus an external corpus for learning word embeddings. A topically-constrained set of documents sampled from a general-purpose large corpus achieves the highest effectiveness scores.\nZamani and Croft [88] propose two QLMs and an extended relevance model [46] based on word embeddings. In the first QLM, p(w | q) is computed by multiplying likelihood scores p(w | t) given individual query terms whereas in the second QLM, p(w | q) is estimated by an additive model over p(w | t) scores. The p(w | t) scores are based on similarity of word embeddings. For measuring similarity of embeddings, a sigmoid function is applied on top of the cosine similarity in order to increase the discriminative ability. The reason for this choice is the observation that the cosine similarity of the 1000th closest neighbor to a word is not much lower than the similarity of the first closest neighbor. Besides the QLMs, a relevance model [46], which computes a feedback query language model using embedding similarities in addition to term matching, is introduced. The proposed query language models are compared to Maximum Likelihood Estimation (MLE), GLM [3, 29] on AP, Robust and GOV2 collections from TREC. The first QLM is shown to be more effective in query expansion experiments. Regarding the PRF experiments, the embedding based relevance model combined with expansion using the first QLM produces the highest scores."}, {"heading": "5.1.3 Reflections on evaluation", "text": "Evaluation of word embedding-based approaches has been performed on different retrieval data sets with different choices of corpora for learning word embeddings. It is difficult to derive conclusions about the comparative performance of the proposed approaches across publications due to the variety of experimental frameworks. In Table 4, we present an overview of the experimental frameworks adopted in these studies. The NLM column specifies the neural language model and the Corpus for WE column specifies the corpus used for learning word embeddings. The Retrieval\ncorpus column presents the document collection used in retrieval evaluation. Finally, the last column gives the methods against which the proposed method is compared.\nOnly a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93]. Using term similarities computed by word embeddings in a query language model or a document language model is shown to improve retrieval effectiveness scores over the standard retrieval baseline.\nAmong the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93]. In [93], word embedding similarity achieves comparable effectiveness to mutual informationbased term similarity. For query-document similarity, Nalisnick et al [68] point out that utilising relations between the IN and OUT embedding spaces learned by CBOW yields a more effective similarity function for query-document pairs. Diaz et al [26] propose to learn word embeddings from a topically constrained corpora since the word embeddings learned from an unconstrained corpus are found to be too general.\nZamani and Croft [88, 89] apply a sigmoid function on the cosine similarity scores in order to increase the discriminative power.\nThe influence of choice of hyper-parameters used to learn word embeddings on retrieval effectiveness is not systematically analyzed in all of the publications in the aggregate category.\nNeural language model The majority of the studies rely on CBOW and Skip-Gram models from the Word2Vec framework. Surprisingly, the Glove and Word2Vec models are not compared in any of the studies. The effect of the model choice is only investigated in [93]. A significant difference is not observed between Skip-gram and CBOW in terms of their effect on retrieval effectiveness in a translation language model. A noted observation is that Skip-gram model is able to yield higher effectiveness scores with lower embedding dimension and context-window size.\nCorpus Embeddings learned from a general-purpose corpus like Wikipedia (general purpose embeddings) and embeddings learned from the retrieval corpus itself (corpus-specific word embeddings) are compared in [92, 93]. A notable difference is not observed between corpus-specific embeddings and general-purpose word embeddings when used for query-reweighting [92] or in a translation language model for computing term similarities [93]. However, Diaz et al [26] highlight that query expansion with word embeddings learned from topic-constrained collection of documents, yield higher effectiveness scores compared to embeddings learned from a general-purpose corpora.\nEmbedding dimension and context-window size Zuccon et al [93] report that embeddings learned using Skip-Gram and CBOW models are robust to different choices of embedding dimensionality and context window size. A similar observation about effect of embedding size and context-size on retrieval effectiveness is shared by Vulic\u0301 and Moens [85]. The retrieval effectiveness is found to be stable with embedding dimensions greater than 300 and context window size greater than 30 in [85].\n5.2 Learn\nIn the learn category (see Section 4.3.2) we consider three sub-categories, which we discuss next."}, {"heading": "5.2.1 Learn to match", "text": "In Table 5, neural learn to match models for document retrieval are summarized with the type of TTU pairs and the type of the neural network used as the SCN.\nThe Deep Structured Semantic Model (DSSM) [40] is the earliest neural learn to match model. The SCN of the DSSM model is composed of a word hashing layer and a deep neural network with three non-linear layers. DSSM takes the term vector of a document as the input vectors for query and documents. The word hashing layer converts input term vectors into a trigram vector. The vocabulary size is reduced from\n500K to 30K by replacing each term with its letter trigrams. For instance, the word vector is mapped to {.ve, vec, ect, cto, tor ,or.} where the dot sign is used as the start and end character. The trigram vector is propagated through three non-linear neural networks.\nThe Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component. DSSM takes a term vector of the textual unit and treats it as a bag of words. In contrast, CLSM and LSTM-DSSM take a sequence of one-hot vectors of terms and treat the TTUs as a sequence of words. CLSM includes a convolutional neural network and LSTM-DSSM includes an LSTM network as SCN. Note that the word hashing layer is common to all three models.\nDSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79]. In [40], DSSM is shown to outperform the Word Translation Model, BM25, TF-IDF and Bilingual Topic Models with posterior regularization in terms of NDCG at cutoff values 1, 3 and 10. CLSM is shown to outperform DSSM in [79]. Finally, LSTM-DSSM outperforms CLSM in [72] when document titles are used instead of full document content. When document titles are used instead of the full document content, higher NDCG scores are achieved by[79]. For computational reasons, LSTM-DSSM is evaluated only with document titles [72].\nAnother interesting publication that follows DSSM is by Liu et al [54] who propose a neural model with multi-task objectives. A model that integrates a deep neural network for query classification and the DSSM model for web document ranking via shared layers is proposed. The word hashing layer and semantic representation layer of DSSM are shared between the two models. The integrated network comprises separate task-specific semantic representation layers and output layers for two different tasks. A separate cost function is defined for each task. During training, in each iteration, a task is selected randomly and the model is updated only according to the selected cost function. The proposed model is evaluated on large scale commercial search logs. Experimental results show improvements by the integrated model over both standalone deep neural networks for query classification and a standalone DSSM for web search ranking.\nLi et al [52] utilize distributed representations produced by DSSM and CLSM in order to re-rank documents based on in-session contextual information. Similarity of query-query and query-document pairs extracted from the session context is computed using DSSM and CLSM vectors. These similarity scores are included as\nadditional features to represent session context in a context-aware learning to rank framework."}, {"heading": "5.2.2 Learn to predict", "text": "Ai et al [1, 2] investigate the use of the PV-DBOW model as a document language model for retrieval. Three shortcomings of the PV-DBOW model are identified and an extended paragraph vector model is proposed with remedies for these shortcomings. First, the PV-DBOW model is found to be biased towards short documents due to overfitting in training and the training objective is updated with L2 regularization. Secondly, the PV-DBOW model trained with NEG implicitly weights terms with respect to Inverse Corpus Frequencies (ICF) which has been shown to be inferior to Inverse Document Frequency (IDF) in [75]. A document frequency based negative sampling strategy, which converts the problem into factorization of a shifted TFIDF matrix, is adopted. Thirdly, the two layer PV-DBOW architecture depicted in Figure 7a is introduced since word substitution relations, such as the relation in carvehicle, underground-subway pairs, are not captured by PV-DBOW. The Extended Paragraph Vector (EPV) is evaluated in re-ranking the set of top 2,000 documents retrieved by the QL retrieval function. Document language models based on EPV and LDA are compared on TREC Robust04 and GOV2 data sets. An EPV-based model yields higher effectiveness scores than the LDA-based model.\nThe Hierarchical Document Vector (HDV) [27] model extends the PV-DM model to predict not only words in a document but also its temporal neighbors in a document stream. The architecture of this model is depicted in Figure 7b with a word context and document context size of five. There, w1, w2, w3, w4, w5 represent a sample context of words from the input paragraph; p1, p2, p3, p4, p5 represent a set of documents that occur in the same context.\nIn HDV, the contents of the documents in the temporal context also contribute to the document representation. Similar to the PV-DM, the words and documents are mapped to d-dimensional embedding vectors. Djuric et al [27] point out that words and documents are embedded in the same space and this makes the model useful for both recommendation and retrieval tasks including document retrieval, document recommendation, document tag recommendation and keyword suggestion. Given a keyword, titles of similar documents in the embedding space are presented to give an idea of the effectiveness of the model on the ad-hoc retrieval task. However, a quantitative evaluation is not provided for the document retrieval and keyword suggestion tasks."}, {"heading": "5.2.3 Learn to generate", "text": "Lioma et al [53] ask whether it is possible to generate relevant documents given a query. A character level LSTM network is optimized to generate a synthetic document. The network is fed with a sequence of words constructed by concatenating the query and context windows around query terms in all relevant documents for the query. For each query, a separate model is generated and a synthetic document is generated for the same query with the learned model. The synthetic document\nwas evaluated in a crowdsourcing setting. Users are provided with four word clouds that belong to three known relevant documents and the synthetic document. Each word cloud is built by selection of top frequent terms from the document. Users are asked to select the most relevant word cloud. Author report that the word cloud of the synthetic document ranked the first or second for most of the queries. Experiments were performed on the TREC Disks 4, 5 test collection with title-only queries from TREC 6, 7, 8.\n5.3 Reflections on evaluation\nTo the best of our our knowledge, there are no experiments in the literature that comparison the aggregate and learn methods or that compare different training objectives for learning neural SC models. Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27]."}, {"heading": "6 Query suggestion", "text": "Next we turn to neural models for semantic matching for query suggestions. As explained in Section 4, for the query suggestion task the literature has publications in the aggregate and learn (learn to match and learn to generate context) categories.\n6.1 Aggregate\nThe work by Cai and de Rijke [14] on query auto-completion, introduces semantic features computed using Skip-Gram embeddings, for learning to rank query autocompletion candidates. Query similarity computed by sum and maximum of the embedding similarity of term-pairs from queries are used as two separate features. The maximal embedding similarity of term pairs is found to be the most important feature in a diverse feature set including popularity-based features, a lexical similarity and another semantic similarity feature based on co-occurrence of term pairs in sessions.\n6.2 Learn"}, {"heading": "6.2.1 Learn to match", "text": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63]. In both studies, CLSM representations are used to build additional features in an existing learning to rank framework for query autocompletion.\nMitra and Craswell [64] train a CLSM model on query prefix-suffix pairs extracted from query logs by segmenting each query at every word boundary. The queries that start with the last word of the prefix issued by the user are picked from query logs. A candidate suggestion query is formed by appending the suffix from each such query to the prefix. A learning to rank model is trained using n-gram features and CLSM based features, for ranking candidate queries. CLSM representations are found to improve MMR scores when used together with n-gram features in experiments with AOL query logs and Bing logs. However, n-gram features are shown to provide a bigger improvement than CLSM features.\nIn [63], a CLSM model is trained on query pairs that are observed in succession in search logs. This work provides an analysis of CLSM vectors for queries similar to the word embedding space analysis in [61]. Mitra [63] found out that offsets between CLSM query vectors can represent intent transition patterns. To illustrate, the nearest neighbour query of the vector computed by vector(university of washington) \u2212 vector(seattle) + vector(chicago) is found to be vector (university of chicago). Besides, the offset of the vectors for university of washington and seattle is similar to the offset of the vectors for chicago state university and chicago .\nMotivated by this feature of the CLSM vectors, query reformulations are represented as the offset vector from the source query to target query [63]. Clustering of query reformulations represented by the offset vectors yields clusters that contain pairs with similar intent transitions. For instance, the query reformulations in which there is an intent jump like avatar dragons \u2192 facebook, are observed grouped in a cluster.\nMitra [63] uses CLSM vectors to define features to represent the session context and rank suggestion candidates against a prefix in personalized query autocompletion. The reformulation feature is the offset vector between the previous query of the user and suggestion candidate. The similarity features are composed of the\nsimilarity score of the suggestion candidate to each previous query in the session. For learning to rank the candidate queries against a session, reformulation features and similarity features are included as additional features. The reformulation feature is shown to be provide bigger improvements in the MMR scores than the similarity features. A combination of the reformulation and similarity features yields the highest MMR scores."}, {"heading": "6.2.2 Learn to generate context", "text": "The Hieararchical Recurrent Encoder Decoder (HRED) model [81] is a neural model designed for generating the next query based on the session context vector. The model is able to learn representations for queries, words and sessions simultaneously. The model consists of two encoder RNNs and a decoder RNN. The first encoder RNN maps each query to a distributed representation and the second encoder maps the sequence of query vectors into a session context vector. The decoder generates the most likely query to follow the session, based on the distributed session representation.\nThe HRED is evaluated in two settings. First of all, likelihood scores of the model for a query to follow a session context are used as an additional feature in a learningto-rank framework to rank query candidates obtained from a co-occurrence based suggestion model. Experimental results show that when the likelihood score is included as a feature, the performance of the baseline ranker is significantly improved. As the second experimental setting, the authors conduct a user study to evaluate the synthetic suggestions generated by the HRED model. Users are asked to classify the suggested queries as useful, somewhat useful, not useful categories. A total of 64% of the queries generated by the HRED model was found to be either useful or somewhat useful by users. This score is higher than all the other baselines where the highest score for \u201cuseful or somewhat useful\u201d is about 45%."}, {"heading": "7 Ad retrieval", "text": "We turn to the third and final task considered in this survey: ad retrieval. Here we have two groups of papers: learn to match and learn to predict.\n7.1 Learn to match\nIn Table 6, neural learn to match models for ad retrieval are summarized with the type of TTU pairs and the type of the neural network utilized as the SCN.\nAzimi et al [4] use a DSSM model for ad keyword re-writing. In paid search, each ad is associated with a set of keywords called bided keywords. The ads are ranked against a query and the ads at high rank are displayed to the user. In order to overcome the vocabulary mismatch between user queries and bided keywords, bided keywords are replaced with more common keywords. A set of candidate keywords are extracted from the set of documents returned by a search engine in response to the bided keyword query. The DSSM model is leveraged to rank the candidate keywords against the original keywords.\nThe deep-intent model proposed in Zhai et al [90, 91] comprises a Bidirectional Recurrent Neural Network (BRNN) combined with an attention module as the SCN. The attention module, first introduced in [5] for neural machine translation, is referred to as attention pooling layer. This is the first work that employs an attention module for a web search task. A recurrent neural network takes a sequence of words and generates a sequence of distributed representations, so-called context-vectors, aligned with each word. Each context vector encodes the semantics of the context from the start to the corresponding word. A Bidirectional Recurrent Neural Network (BRNN) processes the input word sequence in both forward and backward directions. Context vectors generated by a BRNN encode the context after and before the associated word. The pooling strategy is merging the context vectors vectors into a single vector that encodes the semantics of the whole sequence. The sequence vector is assigned the last context vector in last pooling, whereas an element-wise max operation is applied on context vectors in max-pooling. In attention pooling, the sequence vector is computed by a weighted sum of context vectors where the weights are determined by the attention module. The attention module takes a sequence of context vectors and outputs a weight for each vector.\nThe similarity score between a query and an ad is obtained by the dot product of their distributed representations. Similar query-ad pairs for training are extracted from the click logs. Query-ad pairs that have a click are selected as training samples and for each such sample, a randomly selected set of query-ad pairs (without click) are used as negative samples.\nDistributed representations are evaluated on click-logs from a product ad search engine which 966K pairs manually labeled by human judges. In the experiments, models that are built from different choices of RNN type (RNN, Bidirectional RNN, LSTM and LSTM-RNN) and pooling strategy (max-pooling, last pooling and attention pooling) are compared. The attention layer provides a significant gain in the AUC (area-under-curve of the receiver operating characteristic) scores when used with RNN and BRNN whereas it performs on a par with last pooling when used with LSTM-based networks. This can be attributed to the power of LSTM units for capturing long-term contextual relations.\nBesides the evaluation of distributed representations for matching, the attention scores are used to extract a subset of query words. The words that have the highest attention scores are selected to rewrite the query. The (query-subquery) pairs are classified by human judges into five categories same, superset, subset, overlap, disjoint. The sub-queries obtained by the attention module are shown to be of good quality, beating competitive baselines for the majority of queries.\n7.2 Learn to predict\nIn Table 7 learn to predict models for ad retrieval are summarized with the type of TTU pairs and the type of the neural network utilized as the SCN.\nThe context-content2vec model [34], which has the same architecture as the HDV model depicted in Figure 7b, is aimed at learning query embeddings. The context of the query is defined both by its content and other queries in the same session. For the context-content2vec model, p1, p2, p4, p5 stands for queries that occur in the same session with p3. Besides the context-content2vec model, the context2vec model illustrated in Figure 8, which predicts only temporal context, is also introduced in [34].\nThe models are trained on Yahoo search logs that contain 12 billion sessions and embeddings for approximately 45 million queries are learned. Learned query embeddings are leveraged for rewriting queries in order to improve search re-targeting. The original query is expanded with its k nearest neighbor queries in the query embedding space. The learned model is evaluated on TREC Web Track 2009\u20132013 queries and an in-house data set from Yahoo. For queries in the TREC data set, the query rewrites obtained by the proposed models are editorially judged. The PV-DM model that only predicts context yields lower editorial grades than the Query Flow Graph (QFG) baseline. Rewrites by context2vec and context-content2vec embeddings outperform the baseline. The rewrites by the context-content2vecad model, which extends context-content2vec by adding the ads and links clicked in the same session to the TTU context, are assigned the highest editorial grades on average."}, {"heading": "8 Conclusion", "text": "The purpose of this survey is to offer an introduction to neural models for semantic matching in web search. To this end we reviewed and classified existing work in the area. We used a taxonomy in which we recognize different target textual units (TTUs), different types of usage of learned text representations (\u201cusage\u201d), as well as different methods for building representations (\u201chow\u201d). Within the latter identified two sub-categories: the aggregate and learn categories. The aggregate category includes methods based on pre-computed word embeddings for computing semantic similarity, while the learn category covers the neural semantic compositionality models.\nWithin the aggregate category we observed two major patterns of exploiting word embeddings. In the explicit type of use of embeddings, TTUs are associated with a representation in the word embedding space and semantic similarity of TTUs is computed based on these representations. In the implicit type of use, similarity of word embeddings is plugged in as term similarity in an existing statistical language modeling frameworks for retrieval. Several strategies for adapting word embeddings for document retrieval have been introduced, such as topically constraining the document collection, new similarity functions and the inclusion of TF-IDF weights for aggregating word embeddings. This may be understood as an indication that we need to design IR specific objectives for learning distributed representations. Are the training objective and semantic relationships encoded by the embedding vectors useful for the target retrieval task? A future direction would be to identify the types of semantic word relations that matter to semantic matching in web search, across multiple tasks.\nWe classified the neural semantic compositionality models reviewed in the learn category, into the three sub-categories neural learn to match, learn to predict context and learn to generate context, considering the training objectives optimized for learning representations. Neural learn to match models are trained using noisy relevance signals from click information in click-through logs whereas the models in the other categories are designed to predict or generate task-specific context of TTUs. Majority of the learn to match and learn to predict models are evaluated on data sets extracted from commercial search engine logs. A comparative evaluation of models from different sub-categories, on publicly available data sets, is required in order to gain a deeper understanding of semantic compositionality for matching.\nCurrently, existing learn to predict/generate context models, mostly rely on temporal context windows. In general, it would be interesting to examine other types of contextual relations in search logs, such as long term search history of users and noisy relevance signals exploited by learn to match models. Another future direction would be applications of the attention mechanism [5] for designing models that can predict where a user would attend in document, given a query.\nLooking forward, we believe there are several key directions where progress is needed. First, we presented the document retrieval, query suggestion and ad retrieval tasks as largely disjoint tasks. However, the models proposed for one task may be useful for another. For instance, the context-content2vec model of [34] was evaluated only on matching ads to queries yet the distributed query representations could also be evaluated for query suggestion or query auto completion [15]. In particular, there is\na need to compare distributed query representations and similarity/likelihood scores produced by the proposed models on query tasks. In some work, the representations were used as features in learning to rank frameworks and there are no clues about the power of these representations in capturing semantics.\nMore broadly, there is a need for systematic and broad task-based experimental surveys that focus on comparative evaluations of models from different categories, but for the same tasks and under the same experimental conditions, very much like the reliable information access (RIA) workshop that was run in the early 2000s to gain a deeper understanding of query expansion and pseudo relevance feedback [37].\nAnother observation is that recently introduced generative models\u2014mostly based on recurrent neural networks\u2014can generate unseen (synthetic) textual units. The generated textual units have been evaluated through user studies [53, 81]. For the query suggestion task, generated queries have been found to be useful; and so have word clouds of a synthetic document. The impact of these recent neural models on user satisfaction or retrieval scenarios should be investigated on real scenarios.\nFinally, over the years, web search has made tremendous progress by learning from user behavior, either by introducing, e.g., click-based rankers [74] or, more abstractly, by using models that capture behavioral notions such as examination probability and attractiveness of search results through click models [20]. How can such implicit signals be used to train neural models for semantic matching in web search? So far, we have only seen limited examples of the use of click models in training neural models for web search tasks."}, {"heading": "A Acronyms used", "text": "AI artificial intelligence BoEW Bag of Embedded Words BWESG Bilingual word Embeddings Skip-Gram CBOW Continuous Bag of Words CLSM Convolutional Latent Semantic Model DESM Dual Embedding Space Model DFR Divergence From Randomness DSM distributional semantic model DSSM Deep Structured Semantic Model EPV Extended Paragraph Vector GLM Generalized Language Model HAL Hyperspace Analog to Language HDV Hierarchical Document Vector HRED Hieararchical Recurrent Encoder Decoder IR information retrieval IS Importance Sampling LDA Latent Dirichlet Allocation LM language model LSA Latent Semantic Analysis LSI Latent Semantic Indexing NCE Noise Contrastive Estimation NEG Negative Sampling NLM Neural Language Model NLTM Neural Translation Language Model NNLM Neural Network Language Model PLSA Probabilistic Latent Semantic Analysis\nPV Paragraph Vector PV-DBOW Paragraph Vector with Distributed Bag of Words PV-DM Paragraph Vector with Distributed Memory QLM query language model RNNLM Recurrent Neural Network Language Model SC semantic compositionality SCN Semantic Compositionality Network SGNS Skip-Gram with Negative Sampling TTU target textual unit WMD Word Mover\u2019s Distance"}], "references": [{"title": "Analysis of the paragraph vector model for information retrieval", "author": ["Q Ai", "L Yang", "J Guo", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Improving language estimation with the paragraph vector model for ad-hoc retrieval", "author": ["Q Ai", "L Yang", "J Guo", "WB Croft"], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information", "author": ["M ALMasri", "C Berrut", "JP Chevallet"], "venue": "Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Ads keyword rewriting using search engine results", "author": ["J Azimi", "A Alam", "R Zhang"], "venue": "Proceedings of the 24th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201915 Companion,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D Bahdanau", "K Cho", "Y Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of contextcounting vs. context-predicting semantic vectors. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["M Baroni", "G Dinu", "G Kruszewski"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatic differentiation in machine learning: a survey", "author": ["AG Baydin", "BA Pearlmutter", "AA Radul", "JM Siskind"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y Bengio", "R Ducharme", "P Vincent", "C Janvin"], "venue": "J Mach Learn Res 3:1137\u20131155,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y Bengio", "JS Sen\u00e9cal"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Latent dirichlet allocation. Journal of machine Learning research", "author": ["DM Blei", "AY Ng", "MI Jordan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "2016) A context-aware time model for web search", "author": ["A Borisov", "I Markov", "M de Rijke", "P Serdyukov"], "venue": "SIGIR 2016: 39th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "2016) A neural click model for web search. In: WWW 2016", "author": ["A Borisov", "I Markov", "M de Rijke", "P Serdyukov"], "venue": "25th International World Wide Web Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Signature verification using a \u201csiamese\u201d time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence", "author": ["J Bromley", "JW Bentz", "L Bottou", "I Guyon", "Y LeCun", "C Moore", "E S\u00e4ckinger", "R Shah"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Learning from homologous queries and semantically related terms for query auto completion", "author": ["F Cai", "M de Rijke"], "venue": "Information Processing & Management", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "2016) A survey of query auto completion in information retrieval. Foundations and Trends in Information Retrieval 10(4):273\u2013363  Getting Started with Neural Models for Semantic Matching in Web Search", "author": ["F Cai", "M de Rijke"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A survey of automatic query expansion in information retrieval", "author": ["C Carpineto", "G Romano"], "venue": "ACM Comput Surv 44(1):1:1\u20131:50,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Strategies for training large vocabulary neural language models. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp 1975\u20131985", "author": ["W Chen", "D Grangier", "M Auli"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Natural language understanding with distributed representation", "author": ["K Cho"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K Cho", "B van Merrienboer", "\u00c7 G\u00fcl\u00e7ehre", "F Bougares", "H Schwenk", "Y Bengio"], "venue": "CoRR abs/1406.1078,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Click Models for Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services, Morgan & Claypool Publishers, DOI http://doi. org/10.2200/S00654ED1V01Y201507ICR043, URL http://clickmodels.weebly.com/ uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf", "author": ["A Chuklin", "I Markov", "M de Rijke"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Aggregating continuous word embeddings for information retrieval. In: Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, Association for Computational Linguistics, Sofia, Bulgaria, pp 100\u2013109", "author": ["S Clinchant", "F Perronnin"], "venue": "URL http: //www.aclweb.org/anthology/W13-3212", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R Collobert", "J Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Document embedding with paragraph vectors", "author": ["AM Dai", "C Olah", "QV Le", "GS Corrado"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Indexing by latent semantic analysis. Journal of the American society for information science", "author": ["S Deerwester", "ST Dumais", "GW Furnas", "TK Landauer", "R Harshman"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1990}, {"title": "Deep learning: Methods and applications. Foundations and Trends in Signal Processing 7(3\u20134):197\u2013387, URL https://www.microsoft.com/en-us/research/ publication/deep-learning-methods-and-applications", "author": ["L Deng", "D Yu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Query expansion with locally-trained word embeddings. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp 367\u2013377", "author": ["F Diaz", "B Mitra", "N Craswell"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Hierarchical neural language models for joint representation of streaming documents and their content", "author": ["N Djuric", "H Wu", "V Radosavljevic", "M Grbovic", "N Bhamidipati"], "venue": "Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "The vocabulary problem in human-system communication", "author": ["GW Furnas", "TK Landauer", "LM Gomez", "ST Dumais"], "venue": "Commun ACM 30(11):964\u2013971,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1987}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["D Ganguly", "D Roy", "M Mitra", "GJ Jones"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "A primer on neural network models for natural language processing", "author": ["Y Goldberg"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A (2016) Deep learning, URL http://www. deeplearningbook.org, book in preparation", "author": ["I Goodfellow", "Y Bengio", "Courville"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Neural networks. In: Supervised Sequence Labelling with Recurrent Neural Networks, Springer, pp", "author": ["A Graves"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Search retargeting using directed query embeddings", "author": ["M Grbovic", "N Djuric", "V Radosavljevic", "N Bhamidipati"], "venue": "Proceedings of International World Wide Web Conference (WWW)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Context- and content-aware embeddings for query rewriting in sponsored search", "author": ["M Grbovic", "N Djuric", "V Radosavljevic", "F Silvestri", "N Bhamidipati"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "2016) A deep relevance matching model for ad-hoc retrieval. In: CIKM 2016", "author": ["J Guo", "Y Fan", "Q Ai", "WB Croft"], "venue": "25th ACM Conference on Information and Knowledge Management,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "A (2012) Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann MU", "Hyv\u00e4rinen"], "venue": "J Mach Learn Res 13(1):307\u2013361,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Overview of the reliable information access workshop", "author": ["D Harman", "C Buckley"], "venue": "Inf Retr 12(6):615\u2013641,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Learning distributed representations of sentences from unlabelled data. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, San Diego, California, pp 1367\u20131377", "author": ["F Hill", "K Cho", "A Korhonen"], "venue": "URL http://www.aclweb.org/anthology/", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["PS Huang", "X He", "J Gao", "L Deng", "A Acero", "L Heck"], "venue": "Proceedings of the 22Nd ACM International Conference on Conference on Information ", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Learning dynamic classes of events using stacked multilayer perceptron networks. In: Neu-IR: The SIGIR 2016", "author": ["N Kanhabua", "H Ren", "TB Moeslund"], "venue": "Workshop on Neural Information Retrieval", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Short text similarity with word embeddings", "author": ["T Kenter", "M de Rijke"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["T Kenter", "P Huijnen", "M Wevers", "M de Rijke"], "venue": "CIKM 2015: 24th ACM Conference on Information and Knowledge Management,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Relevance based language models", "author": ["V Lavrenko", "WB Croft"], "venue": "Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["QV Le", "T Mikolov"], "venue": "Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211\u2013225", "author": ["O Levy", "Y Goldberg", "I Dagan"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Deep learning for information retrieval", "author": ["H Li", "Z Lu"], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Semantic matching in search. Foundations and Trends in Information Retrieval 7(5):343\u2013469", "author": ["H Li", "J Xu"], "venue": "DOI 10.1561/1500000035,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["J Li", "MT Luong", "D Jurafsky", "E Hovy"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Deep learning powered in-session contextual ranking using clickthrough data. In: Workshop on Personalization: Methods and Applications, at Neural Information Processing Systems (NIPS)  Getting Started with Neural Models for Semantic Matching in Web Search", "author": ["X Li", "C Guo", "W Chu", "YY Wang", "J Shavlik"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Deep learning relevance: Creating relevant information (as opposed to retrieving it)", "author": ["C Lioma", "B Larsen", "C Petersen", "JG Simonsen"], "venue": "CoRR abs/1606.07660,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["X Liu", "J Gao", "X He", "L Deng", "K Duh", "YY Wang"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "author": ["K Lund", "C Burgess"], "venue": "Behavior Research Methods, Instruments,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1996}, {"title": "Visualizing data using t-SNE", "author": ["L van der Maaten", "G Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Parallel distributed processing. Explorations in the microstructure of cognition", "author": ["JL McClelland", "DE Rumelhart", "Group PDP Research"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1986}, {"title": "Recurrent neural network based language model", "author": ["T Mikolov", "M Karafi\u00e1t", "L Burget", "J Cernock\u1ef3", "S Khudanpur"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T Mikolov", "K Chen", "G Corrado", "J Dean"], "venue": "CoRR abs/1301.3781,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J Dean"], "venue": "KQ (eds) Advances in Neural Information Processing Systems", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations. In: Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association", "author": ["T Mikolov", "Wt Yih", "G Zweig"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Composition in distributional models of semantics. Cognitive Science 34(8):1388\u20131429, DOI 10.1111/j.1551-6709.2010.01106.x, URL http://dx.doi.org/ 10.1111/j.1551-6709.2010.01106.x", "author": ["J Mitchell", "M Lapata"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["B Mitra"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2015}, {"title": "Query auto-completion for rare prefixes", "author": ["B Mitra", "N Craswell"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2015}, {"title": "2016) A dual embedding space model for document ranking", "author": ["B Mitra", "E Nalisnick", "N Craswell", "R Caruana"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A Mnih", "YW Teh"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F Morin", "Y Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Improving document ranking with dual word embeddings", "author": ["E Nalisnick", "B Mitra", "N Craswell", "R Caruana"], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201916 Companion,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2016}, {"title": "Query transformations for result merging", "author": ["S Palakodety", "J Callan"], "venue": "Proceedings of The Twenty-Third Text REtrieval Conference,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Semantic modelling with long-short-term memory for information retrieval. CoRR abs/1412.6629, URL http://arxiv", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}, {"title": "Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": "CoRR abs/1502.06922,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2015}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 24(4):694\u2013707,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J Pennington", "R Socher", "C Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Query chains: Learning to rank from implicit feedback", "author": ["F Radlinski", "T Joachims"], "venue": "Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, ACM,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2005}, {"title": "Understanding inverse document frequency: on theoretical arguments for idf. Journal of documentation", "author": ["S Robertson"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Representing documents and queries as sets of word embedded vectors for information", "author": ["D Roy", "D Ganguly", "M Mitra", "GJ Jones"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2016}, {"title": "Using word embeddings for automatic query expansion", "author": ["D Roy", "D Paul", "M Mitra", "U Garain"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2016}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y Shen", "X He", "J Gao", "L Deng", "G Mesnil"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Y Shen", "X He", "J Gao", "L Deng", "G Mesnil"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Cnu system in ntcir-11 imine task", "author": ["W Song", "W Xu", "L Liu", "H Wang"], "venue": null, "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2014}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A Sordoni", "Y Bengio", "H Vahabi", "C Lioma", "J Grue Simonsen", "JY Nie"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["PD Turney", "P Pantel"], "venue": "J Artif Intell Res (JAIR) 37:141\u2013188,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2010}, {"title": "Learning latent vector spaces for product search. In: CIKM 2016", "author": ["C Van Gysel", "M de Rijke", "E Kanoulas"], "venue": "25th ACM Conference on Information and Knowledge Management,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2016}, {"title": "Unsupervised, efficient and semantic expertise retrieval. In: WWW 2016", "author": ["C Van Gysel", "M de Rijke", "M Worring"], "venue": "25th International World Wide Web Conference,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2016}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I Vuli\u0107", "MF Moens"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K Xu", "J Ba", "R Kiros", "K Cho", "AC Courville", "R Salakhutdinov", "RS Zemel", "Y Bengio"], "venue": "CoRR abs/1502.03044,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "Selective term proximity scoring via bp-ann", "author": ["J Yang", "J Tong", "RJ Stones", "Z Zhang", "B Ye", "G Wang", "X Liu"], "venue": "Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2016}, {"title": "Embedding-based query language models", "author": ["H Zamani", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2016}, {"title": "Estimating embedding vectors for queries", "author": ["H Zamani", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2016}, {"title": "Attention based recurrent neural networks for online advertising", "author": ["S Zhai", "Kh Chang", "R Zhang", "Z Zhang"], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201916", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2016}, {"title": "Deepintent: Learning attentions for online advertising with recurrent neural networks", "author": ["S Zhai", "Kh Chang", "R Zhang", "ZM Zhang"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2016}, {"title": "Learning to reweight terms with distributed representations", "author": ["G Zheng", "J Callan"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2015}, {"title": "Integrating and evaluating neural word embeddings in information retrieval", "author": ["G Zuccon", "B Koopman", "P Bruza", "L Azzopardi"], "venue": "Proceedings of the 20th Australasian Document Computing Symposium,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "In web search, the vocabulary mismatch problem [28] necessitates effective semantic similarity functions for textual units of different types.", "startOffset": 47, "endOffset": 51}, {"referenceID": 46, "context": "According to Li and Xu [50], semantic matching is concerned with computing the relevance of a document for a query based on representations enriched by linguistic analysis that is meant to capture the semantics of the query and document.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 7, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 55, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 69, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 37, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 189, "endOffset": 197}, {"referenceID": 43, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 189, "endOffset": 197}, {"referenceID": 53, "context": "A distributed representation for a textual unit is a dense real-valued vector that somehow encodes the semantics of the textual unit [57].", "startOffset": 133, "endOffset": 137}, {"referenceID": 46, "context": "Distributed representations hold the promise of aiding semantic matching: by mapping words and other textual units to their representations, semantic matches can be computed in the representation space [50].", "startOffset": 202, "endOffset": 206}, {"referenceID": 78, "context": "The problem of mapping words to a representation that can capture their meanings is referred as distributional semantics and has been studied for a very long time; see [82] for an overview.", "startOffset": 168, "endOffset": 172}, {"referenceID": 51, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 270, "endOffset": 274}, {"referenceID": 23, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 307, "endOffset": 311}, {"referenceID": 5, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 360, "endOffset": 363}, {"referenceID": 44, "context": "Moreover, Levy et al [48] improve context-counting models by adopting lessons from context-predicting models.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "Bengio et al [8] seem to have been the first ones to propose a neural language model; they introduce the idea of simultaneously learning a language model that predicts a word given its context and its representation, a so-called word embedding.", "startOffset": 13, "endOffset": 16}, {"referenceID": 55, "context": "The most well-known and most widely used context-predicting models, Word2Vec [59] and GloVe [73], have been used extensively in recent work on web search.", "startOffset": 77, "endOffset": 81}, {"referenceID": 69, "context": "The most well-known and most widely used context-predicting models, Word2Vec [59] and GloVe [73], have been used extensively in recent work on web search.", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "The success of neural word embeddings has also given rise to work on computing context-predicting representations of larger textual units, including paragraphs and documents [39].", "startOffset": 174, "endOffset": 178}, {"referenceID": 24, "context": "In recent years, neural network-based models have given rise to significant performance improvements in computer vision, speech processing and machine translation [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 24, "context": "Training deep neural networks to learn hierarchies of representations became possible owing both to theoretical contributions of new machine learning algorithms and parallelization of training on GPUs [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Deng and Yu [25] devote a chapter to applications of deep learning in information retrieval.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "The recent tutorial on deep learning for information retrieval by Li and Lu [49] sketches a range of potential applications of deep learning to information retrieval (IR) problems with a broader scope.", "startOffset": 76, "endOffset": 80}, {"referenceID": 80, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 120, "endOffset": 124}, {"referenceID": 79, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 173, "endOffset": 181}, {"referenceID": 11, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 173, "endOffset": 181}, {"referenceID": 40, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 199, "endOffset": 207}, {"referenceID": 41, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 199, "endOffset": 207}, {"referenceID": 76, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 225, "endOffset": 229}, {"referenceID": 30, "context": ", Part II of [31].", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "Baroni et al [6] classify existing DSMs into two categories: context-counting and context-predicting.", "startOffset": 13, "endOffset": 16}, {"referenceID": 51, "context": "The context-counting category includes earlier DSMs such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "The context-counting category includes earlier DSMs such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 53, "context": "A distributed representation of a symbol is a vector of features that characterize the meaning of the symbol and are not mutually exclusive [57].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "However, it soon turned out that the embedding layer component, which addresses the curse of dimensionality caused by one-hot vectors [8], yields useful distributed word representations, so-called word embeddings.", "startOffset": 134, "endOffset": 137}, {"referenceID": 21, "context": "Collobert and Weston [22] are the first ones to show the benefit of word embeddings as features for NLP tasks.", "startOffset": 21, "endOffset": 25}, {"referenceID": 55, "context": "Subsequently, word embeddings became widespread after introduction of the shallow models Skip-Gram and CBOW in the Word2Vec framework by Mikolov et al [59, 60]; see Section 2.", "startOffset": 151, "endOffset": 159}, {"referenceID": 56, "context": "Subsequently, word embeddings became widespread after introduction of the shallow models Skip-Gram and CBOW in the Word2Vec framework by Mikolov et al [59, 60]; see Section 2.", "startOffset": 151, "endOffset": 159}, {"referenceID": 5, "context": "Baroni et al [6] report that context-predicting models outperform context-counting models on several tasks including question sets, semantic relatedness, synonym detection, concept categorization and word analogy.", "startOffset": 13, "endOffset": 16}, {"referenceID": 44, "context": "In contrast, Levy et al [48] point out that the success of the popular context-predicting models Word2Vec and GloVe does not originate from the neural network architecture and the training objective but from the choices of hyper-parameters for contexts.", "startOffset": 24, "endOffset": 28}, {"referenceID": 58, "context": "Compositional distributional semantics or semantic compositionality (SC) is the problem of formalizing how the meaning of larger textual units such as sentences, phrases, paragraphs and documents are built from the meanings of words [62].", "startOffset": 233, "endOffset": 237}, {"referenceID": 37, "context": "from unlabelled data is presented in [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "Besides the models reviewed by Hill et al [39], there exist sentence-level models that are trained using task-specific labelled data.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "For instance, a model can be trained to encode the sentiment of a sentence using a dataset of sentences annotated with sentiment class labels [51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "The first neural language model published is the Neural Network Language Model (NNLM) [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "In NNLM [8] it is a non-linear neural network layer whereas in the Continuous Bag of Words (CBOW) model of Word2Vec [59], it is vector addition over word embeddings.", "startOffset": 8, "endOffset": 11}, {"referenceID": 55, "context": "In NNLM [8] it is a non-linear neural network layer whereas in the Continuous Bag of Words (CBOW) model of Word2Vec [59], it is vector addition over word embeddings.", "startOffset": 116, "endOffset": 120}, {"referenceID": 54, "context": "In the Recurrent Neural Network Language Model (RNNLM) [58] the hidden context representation is computed by a recurrent neural network.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "In [22] and in the CBOW model [59] context is defined by the words that surround a center word in a symmetric context.", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "In [22] and in the CBOW model [59] context is defined by the words that surround a center word in a symmetric context.", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "In the NNLM [8] and RNNLM [58] models, the context is defined by words that precede the target word.", "startOffset": 12, "endOffset": 15}, {"referenceID": 54, "context": "In the NNLM [8] and RNNLM [58] models, the context is defined by words that precede the target word.", "startOffset": 26, "endOffset": 30}, {"referenceID": 55, "context": "The Skip-Gram [59] takes a single word as input and predict words from a dynamically sized symmetric context window around the input word.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Chen et al [17] present a comparison of training methods for the NNLM.", "startOffset": 11, "endOffset": 15}, {"referenceID": 63, "context": "Hierarchical Softmax [67] and differentiated softmax [17] propose adapted softmax layer architectures for efficient computation of the softmax function.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Hierarchical Softmax [67] and differentiated softmax [17] propose adapted softmax layer architectures for efficient computation of the softmax function.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Another solution, adopted by methods like Importance Sampling (IS) [9] and Noise Contrastive Estimation (NCE) [66], is to avoid the normalization by using modified loss functions to approximate the softmax.", "startOffset": 67, "endOffset": 70}, {"referenceID": 62, "context": "Another solution, adopted by methods like Importance Sampling (IS) [9] and Noise Contrastive Estimation (NCE) [66], is to avoid the normalization by using modified loss functions to approximate the softmax.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Collobert and Weston [22] propose the cost function in Equation 5, which does not require normalization over the vocabulary.", "startOffset": 21, "endOffset": 25}, {"referenceID": 62, "context": "Mnih and Teh [66] apply NCE [36] to NLM training.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "Mnih and Teh [66] apply NCE [36] to NLM training.", "startOffset": 28, "endOffset": 32}, {"referenceID": 55, "context": "Mikolov et al [59] introduce the Skip-Gram and CBOW models that follow the NLM architecture with a linear layer for computing a distributed context representation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 63, "context": "Efficient training of Skip-Gram and CBOW models is achieved by hiearchical softmax [67] with a Huffman tree Mikolov et al [59].", "startOffset": 83, "endOffset": 87}, {"referenceID": 55, "context": "Efficient training of Skip-Gram and CBOW models is achieved by hiearchical softmax [67] with a Huffman tree Mikolov et al [59].", "startOffset": 122, "endOffset": 126}, {"referenceID": 56, "context": "In follow-up work [60], Negative Sampling (NEG) is proposed for efficiently training the Skip-Gram model.", "startOffset": 18, "endOffset": 22}, {"referenceID": 56, "context": "Subsampling frequent words is another extension introduced in [60] for speeding up training and increasing the quality of embeddings.", "startOffset": 62, "endOffset": 66}, {"referenceID": 69, "context": "Global Vectors (GloVe) [73] combines global context and local context in the training objective for learning word embeddings.", "startOffset": 23, "endOffset": 27}, {"referenceID": 44, "context": "Levy et al [48] discuss that diluting frequent words before training enlarges the context window size in practice.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "Experiments show that the hyper-parameters about context-windows like dynamic size and subsampling frequent words have a notable impact on the performance of SGNS and GloVe [48].", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "In contrast to the conclusions obtained in [6], the success of context-predicting models is attributed to choice of hyper-parameters, which can also be used for context-counting DSMs, rather than to the neural architecture or the training objective.", "startOffset": 43, "endOffset": 46}, {"referenceID": 43, "context": "The Paragraph Vector [47] extends Word2Vec in order to learn representations for socalled paragraph, textual units of any length.", "startOffset": 21, "endOffset": 25}, {"referenceID": 43, "context": "Le and Mikolov [47] assess vectors obtained by averaging PV-DM and PV-DBOW vectors on sentiment classification and snippet retrieval tasks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 127, "endOffset": 131}, {"referenceID": 43, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 280, "endOffset": 284}, {"referenceID": 29, "context": "Goldberg [30] and Cho [18] provide tutorials on getting started with neural networks from the natural language understanding perspective.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Goldberg [30] and Cho [18] provide tutorials on getting started with neural networks from the natural language understanding perspective.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "Goldberg [30] covers details on training neural networks and a broader set of architectures including feed-forward networks, convolutional networks, recurrent networks and recursive networks.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Cho [18] focuses on language modeling and machine translation, sketches a clear picture of the encoder-decoder architectures, recurrent networks and attention modules [5].", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "Cho [18] focuses on language modeling and machine translation, sketches a clear picture of the encoder-decoder architectures, recurrent networks and attention modules [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 39, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 77, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 64, "context": "Pre-trained word embeddings It is possible to obtain pre-trained GloVe embeddings3 and CBOW embeddings learned from Bing query logs [68].", "startOffset": 132, "endOffset": 136}, {"referenceID": 69, "context": "Learning word embeddings The source code for GloVe [73]4 and the models introduced in [48]5 is publicly shared by the authors.", "startOffset": 51, "endOffset": 55}, {"referenceID": 44, "context": "Learning word embeddings The source code for GloVe [73]4 and the models introduced in [48]5 is publicly shared by the authors.", "startOffset": 86, "endOffset": 90}, {"referenceID": 52, "context": "Visualizing word embeddings The dimensionality reduction technique t-Distributed Stochastic Neighbor Embedding (t-SNE) [56] is commonly used for visualizing word embedding spaces and for trying to understand the structures learned by neural models.", "startOffset": 119, "endOffset": 123}, {"referenceID": 34, "context": "Specifically, for evaluating neural models for semantic matching in an end-to-end web search task, TREC collections such as ClueWeb [35], .", "startOffset": 132, "endOffset": 136}, {"referenceID": 89, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 4, "endOffset": 8}, {"referenceID": 83, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 15, "endOffset": 19}, {"referenceID": 73, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 27, "endOffset": 31}, {"referenceID": 81, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 155, "endOffset": 163}, {"referenceID": 77, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 155, "endOffset": 163}, {"referenceID": 39, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "GPU support and automatic differentiation [7] are crucial features required for training neural networks.", "startOffset": 42, "endOffset": 45}, {"referenceID": 77, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 79, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 80, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 89, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 89, "context": "For instance, Zuccon et al [93] compute translation probabilities of word pairs in a translation language model retrieval framework with cosine similarity of Skip-Gram vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 46, "context": "Neural learning to match Learning to match [50] is the problem of learning a matching function f(x, y) that computes the similarity degree of two objects x and y from two different spaces X and Y .", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "They are based on siamese neural networks [13], as illustrated in Figure 4, which consist of two identical sub-networks joined at their outputs in order to compute the similarity of the inputs.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "5: Architecture of the neural learn to match models adapted from [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "Learn to predict context The success of word-based neural language models has motivated models for learning representations of larger textual units [39, 47] from unlabelled data.", "startOffset": 148, "endOffset": 156}, {"referenceID": 43, "context": "Learn to predict context The success of word-based neural language models has motivated models for learning representations of larger textual units [39, 47] from unlabelled data.", "startOffset": 148, "endOffset": 156}, {"referenceID": 37, "context": "Among the models reviewed in [39], Skip-thought Vector [44] and Paragraph Vector (PV) [47] are successful representatives of the learn to predict context idea.", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "Among the models reviewed in [39], Skip-thought Vector [44] and Paragraph Vector (PV) [47] are successful representatives of the learn to predict context idea.", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "For instance, the context of the query can be defined by the other queries in the session [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Finally, context of a document is defined by joining its content and the neighboring documents in a document stream in [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 183, "endOffset": 187}, {"referenceID": 18, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 222, "endOffset": 226}, {"referenceID": 82, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 248, "endOffset": 252}, {"referenceID": 49, "context": "Besides, in [53], which falls into learn to generate context category, the generated document is not used in a quantitative experimental framework.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 64, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 81, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 28, "context": "1 implicit [29, 93] 5.", "startOffset": 11, "endOffset": 19}, {"referenceID": 89, "context": "1 implicit [29, 93] 5.", "startOffset": 11, "endOffset": 19}, {"referenceID": 2, "context": "Query similarity aggregate explicit [3, 92] 5.", "startOffset": 36, "endOffset": 43}, {"referenceID": 88, "context": "Query similarity aggregate explicit [3, 92] 5.", "startOffset": 36, "endOffset": 43}, {"referenceID": 25, "context": "1 \u2013 implicit [26, 88] 5.", "startOffset": 13, "endOffset": 21}, {"referenceID": 84, "context": "1 \u2013 implicit [26, 88] 5.", "startOffset": 13, "endOffset": 21}, {"referenceID": 38, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 67, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 75, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 0, "context": "1 learn-to-predict [1, 27] 5.", "startOffset": 19, "endOffset": 26}, {"referenceID": 26, "context": "1 learn-to-predict [1, 27] 5.", "startOffset": 19, "endOffset": 26}, {"referenceID": 49, "context": "Query \u2013 learn learn-to-generate [53] 5.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Query \u2013 aggregate implicit [14] 6.", "startOffset": 27, "endOffset": 31}, {"referenceID": 59, "context": "Query feature learn learn-to-match [63, 64] 6.", "startOffset": 35, "endOffset": 43}, {"referenceID": 60, "context": "Query feature learn learn-to-match [63, 64] 6.", "startOffset": 35, "endOffset": 43}, {"referenceID": 77, "context": "1 learn learn-to-generate [81] 6.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 86, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 87, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 32, "context": "Query similarity learn learn-to-predict [33, 34] 7.", "startOffset": 40, "endOffset": 48}, {"referenceID": 33, "context": "Query similarity learn learn-to-predict [33, 34] 7.", "startOffset": 40, "endOffset": 48}, {"referenceID": 81, "context": "[85] Skipgram Sum of BoEW-IN Sum over BoEW-IN Cosine similarity", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "[68] CBOW BoEW-IN BoEW-OUT Aggregation of cosine similarities of accross all querydocument pairs", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] CBOW BoEW-IN Set of clusters in IN Average inter-similarity of query to the set of centroids in the document", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "[69] CBOW Mean of BoEW-IN N/A Cosine similarity", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[77] Word2Vec Mean of BoEW-IN N/A The mean cosine similarity between expansion term and all the terms in Q", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "The E matrix is referred as IN embeddings whereas the C matrix as OUT embeddings, in accordance with the naming in [68].", "startOffset": 115, "endOffset": 119}, {"referenceID": 64, "context": "Nalisnick et al [68] point out an important feature of CBOW embeddings: the neighbors of a word represented with its IN embedding vector in the OUT space are topically similar words.", "startOffset": 16, "endOffset": 20}, {"referenceID": 64, "context": "Motivated by this observation, Nalisnick et al [68] propose the Dual Embedding Space Model (DESM).", "startOffset": 47, "endOffset": 51}, {"referenceID": 61, "context": "Query-document similarity is computed by aggregating cosine similarities across all the query-document word pairs [65, 68].", "startOffset": 114, "endOffset": 122}, {"referenceID": 64, "context": "Query-document similarity is computed by aggregating cosine similarities across all the query-document word pairs [65, 68].", "startOffset": 114, "endOffset": 122}, {"referenceID": 61, "context": "DESM is evaluated in a document ranking setting with both explicitly judged data sets and implicit feedback based data sets in [65].", "startOffset": 127, "endOffset": 131}, {"referenceID": 81, "context": "In the explicit category, Vuli\u0107 and Moens [85] propose to construct query and document representations as a sum of word embeddings learned from a pseudobilingual document collection with a Skip-Gram model.", "startOffset": 42, "endOffset": 46}, {"referenceID": 72, "context": "In [76], documents are modelled as a mixture distribution that generates the observed terms in the document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A different explicit aggregation method based on BoEW representations is the Fisher Kernel proposed in [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Query expansion and re-weighting ALMasri et al [3] use Skip-Gram and CBOW embeddings for extracting most similar terms for a given query term.", "startOffset": 47, "endOffset": 50}, {"referenceID": 65, "context": "The work on query expansion [69] based on word embeddings, although proposed for result merging in federated web search, can also be noted here since it introduces the idea of an importance vector in the query re-weighting method in [92].", "startOffset": 28, "endOffset": 32}, {"referenceID": 88, "context": "The work on query expansion [69] based on word embeddings, although proposed for result merging in federated web search, can also be noted here since it introduces the idea of an importance vector in the query re-weighting method in [92].", "startOffset": 233, "endOffset": 237}, {"referenceID": 65, "context": "In [69], a query is represented by the mean vector of query term embeddings and k-nearest neighbors of the query vector are selected to expand the query.", "startOffset": 3, "endOffset": 7}, {"referenceID": 88, "context": "Query re-weighting is modelled as a linear regression problem from importance vectors to term weights in [92].", "startOffset": 105, "endOffset": 109}, {"referenceID": 73, "context": "Roy et al [77] propose a set of query expansion methods, based on selecting k nearest neighbors of the query terms in the word embedding space and ranking these terms with respect to their similarity to the whole query.", "startOffset": 10, "endOffset": 14}, {"referenceID": 89, "context": "Zuccon et al [93] compute translation probabilities of word pairs using the cosine similarity of Word2Vec vectors in a translation language model for retrieval.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Cosine similarity of Word2Vec embeddings is used in a similar way in the Generalized Language Model (GLM) [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 25, "context": "Query expansion In [26, 88], word embeddings are used for defining a new query language model (QLM).", "startOffset": 19, "endOffset": 27}, {"referenceID": 84, "context": "Query expansion In [26, 88], word embeddings are used for defining a new query language model (QLM).", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "In query expansion with language modeling, the top m terms w that have the highest p(w | q) value are selected as expansion terms [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "Diaz et al [26], propose a query expansion language model based on word embeddings learned from topic-constrained corpora.", "startOffset": 11, "endOffset": 15}, {"referenceID": 84, "context": "Zamani and Croft [88] propose two QLMs and an extended relevance model [46] based on word embeddings.", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "Zamani and Croft [88] propose two QLMs and an extended relevance model [46] based on word embeddings.", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": "Besides the QLMs, a relevance model [46], which computes a feedback query language model using embedding similarities in addition to term matching, is introduced.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "The proposed query language models are compared to Maximum Likelihood Estimation (MLE), GLM [3, 29] on AP, Robust and GOV2 collections from TREC.", "startOffset": 92, "endOffset": 99}, {"referenceID": 28, "context": "The proposed query language models are compared to Maximum Likelihood Estimation (MLE), GLM [3, 29] on AP, Robust and GOV2 collections from TREC.", "startOffset": 92, "endOffset": 99}, {"referenceID": 81, "context": "[85] SkipGram EuroParl corpus, Aligned EnglishDutch Wikipedia Articles CLEF En-Dutch Collection Unigram LM Unigram LM+LDA Google Translate + Unigram LM +LDA", "startOffset": 0, "endOffset": 4}, {"referenceID": 89, "context": "[93] Skipgram, CBOW AP88-89, WSJ87-92, Wikipedia TREC 6-7-8, DOTGOV, TREC Medical Records Track 20112012 Dirichlet LM, Translation Language Model with Mutual Information", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] CBOW TREC Document Collection TREC 6-7-8 LM, LDA smoothed LM", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[65] CBOW Bing logs Document collection from Bing logs LSA, BM25", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 68, "endOffset": 75}, {"referenceID": 28, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 68, "endOffset": 75}, {"referenceID": 88, "context": "[92] CBOW Google News / ClueWeb09B / Retrieval Corpus ROBUST04 WT10g GOV2 ClueWeb09B Sequential dependency Model, Unweighted Queries", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] CBOW Image2009, Case2011 and Case2012 from CLEF Image2011, Image 2012, Image 2012 and Case2011 from CLEF Expansion with PRF and Mutual Information", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "[26] CBOW Wikipedia / Gigawords / Retrieval Corpus TREC12, Robust and ClueWeb 2009 Category B QL", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] CBOW Retrieval Corpus TREC 6-7-8 and Robust LM with Jelinek Mercer Smoothing", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 20, "endOffset": 24}, {"referenceID": 84, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 105, "endOffset": 109}, {"referenceID": 89, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 64, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 84, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 85, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 89, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 89, "context": "In [93], word embedding similarity achieves comparable effectiveness to mutual informationbased term similarity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 64, "context": "For query-document similarity, Nalisnick et al [68] point out that utilising relations between the IN and OUT embedding spaces learned by CBOW yields a more effective similarity function for query-document pairs.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "Diaz et al [26] propose to learn word embeddings from a topically constrained corpora since the word embeddings learned from an unconstrained corpus are found to be too general.", "startOffset": 11, "endOffset": 15}, {"referenceID": 84, "context": "Zamani and Croft [88, 89] apply a sigmoid function on the cosine similarity scores in order to increase the discriminative power.", "startOffset": 17, "endOffset": 25}, {"referenceID": 85, "context": "Zamani and Croft [88, 89] apply a sigmoid function on the cosine similarity scores in order to increase the discriminative power.", "startOffset": 17, "endOffset": 25}, {"referenceID": 89, "context": "The effect of the model choice is only investigated in [93].", "startOffset": 55, "endOffset": 59}, {"referenceID": 88, "context": "Corpus Embeddings learned from a general-purpose corpus like Wikipedia (general purpose embeddings) and embeddings learned from the retrieval corpus itself (corpus-specific word embeddings) are compared in [92, 93].", "startOffset": 206, "endOffset": 214}, {"referenceID": 89, "context": "Corpus Embeddings learned from a general-purpose corpus like Wikipedia (general purpose embeddings) and embeddings learned from the retrieval corpus itself (corpus-specific word embeddings) are compared in [92, 93].", "startOffset": 206, "endOffset": 214}, {"referenceID": 88, "context": "A notable difference is not observed between corpus-specific embeddings and general-purpose word embeddings when used for query-reweighting [92] or in a translation language model for computing term similarities [93].", "startOffset": 140, "endOffset": 144}, {"referenceID": 89, "context": "A notable difference is not observed between corpus-specific embeddings and general-purpose word embeddings when used for query-reweighting [92] or in a translation language model for computing term similarities [93].", "startOffset": 212, "endOffset": 216}, {"referenceID": 25, "context": "However, Diaz et al [26] highlight that query expansion with word embeddings learned from topic-constrained collection of documents, yield higher effectiveness scores compared to embeddings learned from a general-purpose corpora.", "startOffset": 20, "endOffset": 24}, {"referenceID": 89, "context": "Embedding dimension and context-window size Zuccon et al [93] report that embeddings learned using Skip-Gram and CBOW models are robust to different choices of embedding dimensionality and context window size.", "startOffset": 57, "endOffset": 61}, {"referenceID": 81, "context": "A similar observation about effect of embedding size and context-size on retrieval effectiveness is shared by Vuli\u0107 and Moens [85].", "startOffset": 126, "endOffset": 130}, {"referenceID": 81, "context": "The retrieval effectiveness is found to be stable with embedding dimensions greater than 300 and context window size greater than 30 in [85].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "The Deep Structured Semantic Model (DSSM) [40] is the earliest neural learn to match model.", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "DSSM [40] Query-Document Feed Forward Deep Neural Network (DNN)", "startOffset": 5, "endOffset": 9}, {"referenceID": 74, "context": "CLSM [78, 79] Query-Document (Title) Convolutional Neural Network (CNN)", "startOffset": 5, "endOffset": 13}, {"referenceID": 75, "context": "CLSM [78, 79] Query-Document (Title) Convolutional Neural Network (CNN)", "startOffset": 5, "endOffset": 13}, {"referenceID": 67, "context": "LSTMDSSM[71] Query-Document Title Long Short Term Memory (LSTM) Network", "startOffset": 8, "endOffset": 12}, {"referenceID": 74, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 47, "endOffset": 55}, {"referenceID": 75, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 47, "endOffset": 55}, {"referenceID": 66, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 70, "endOffset": 78}, {"referenceID": 68, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 70, "endOffset": 78}, {"referenceID": 38, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 74, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 75, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 38, "context": "In [40], DSSM is shown to outperform the Word Translation Model, BM25, TF-IDF and Bilingual Topic Models with posterior regularization in terms of NDCG at cutoff values 1, 3 and 10.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": "CLSM is shown to outperform DSSM in [79].", "startOffset": 36, "endOffset": 40}, {"referenceID": 68, "context": "Finally, LSTM-DSSM outperforms CLSM in [72] when document titles are used instead of full document content.", "startOffset": 39, "endOffset": 43}, {"referenceID": 75, "context": "When document titles are used instead of the full document content, higher NDCG scores are achieved by[79].", "startOffset": 102, "endOffset": 106}, {"referenceID": 68, "context": "For computational reasons, LSTM-DSSM is evaluated only with document titles [72].", "startOffset": 76, "endOffset": 80}, {"referenceID": 50, "context": "Another interesting publication that follows DSSM is by Liu et al [54] who propose a neural model with multi-task objectives.", "startOffset": 66, "endOffset": 70}, {"referenceID": 48, "context": "Li et al [52] utilize distributed representations produced by DSSM and CLSM in order to re-rank documents based on in-session contextual information.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Ai et al [1, 2] investigate the use of the PV-DBOW model as a document language model for retrieval.", "startOffset": 9, "endOffset": 15}, {"referenceID": 1, "context": "Ai et al [1, 2] investigate the use of the PV-DBOW model as a document language model for retrieval.", "startOffset": 9, "endOffset": 15}, {"referenceID": 71, "context": "Secondly, the PV-DBOW model trained with NEG implicitly weights terms with respect to Inverse Corpus Frequencies (ICF) which has been shown to be inferior to Inverse Document Frequency (IDF) in [75].", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "The Hierarchical Document Vector (HDV) [27] model extends the PV-DM model to predict not only words in a document but also its temporal neighbors in a document stream.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Djuric et al [27] point out that words and documents are embedded in the same space and this makes the model useful for both recommendation and retrieval tasks including document retrieval, document recommendation, document tag recommendation and keyword suggestion.", "startOffset": 13, "endOffset": 17}, {"referenceID": 49, "context": "Lioma et al [53] ask whether it is possible to generate relevant documents given a query.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "(a) Two Layer PV-DBOW [1] w1 w2 w4 w5 w3 p1 p2 p4 p5", "startOffset": 22, "endOffset": 25}, {"referenceID": 26, "context": "(b) HDV [27] and context-content2vec [34]", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "(b) HDV [27] and context-content2vec [34]", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 67, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 75, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 26, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 222, "endOffset": 226}, {"referenceID": 13, "context": "The work by Cai and de Rijke [14] on query auto-completion, introduces semantic features computed using Skip-Gram embeddings, for learning to rank query autocompletion candidates.", "startOffset": 29, "endOffset": 33}, {"referenceID": 75, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 9, "endOffset": 13}, {"referenceID": 60, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 76, "endOffset": 80}, {"referenceID": 59, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 106, "endOffset": 110}, {"referenceID": 60, "context": "Mitra and Craswell [64] train a CLSM model on query prefix-suffix pairs extracted from query logs by segmenting each query at every word boundary.", "startOffset": 19, "endOffset": 23}, {"referenceID": 59, "context": "In [63], a CLSM model is trained on query pairs that are observed in succession in search logs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 57, "context": "This work provides an analysis of CLSM vectors for queries similar to the word embedding space analysis in [61].", "startOffset": 107, "endOffset": 111}, {"referenceID": 59, "context": "Mitra [63] found out that offsets between CLSM query vectors can represent intent transition patterns.", "startOffset": 6, "endOffset": 10}, {"referenceID": 59, "context": "Motivated by this feature of the CLSM vectors, query reformulations are represented as the offset vector from the source query to target query [63].", "startOffset": 143, "endOffset": 147}, {"referenceID": 59, "context": "Mitra [63] uses CLSM vectors to define features to represent the session context and rank suggestion candidates against a prefix in personalized query autocompletion.", "startOffset": 6, "endOffset": 10}, {"referenceID": 77, "context": "The Hieararchical Recurrent Encoder Decoder (HRED) model [81] is a neural model designed for generating the next query based on the session context vector.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Azimi et al [4] use a DSSM model for ad keyword re-writing.", "startOffset": 12, "endOffset": 15}, {"referenceID": 86, "context": "Deep intent [90, 91] Query-Ad Bidirectional RNN/LSTM + Attention Module", "startOffset": 12, "endOffset": 20}, {"referenceID": 87, "context": "Deep intent [90, 91] Query-Ad Bidirectional RNN/LSTM + Attention Module", "startOffset": 12, "endOffset": 20}, {"referenceID": 3, "context": "[4] Ads Keyword Pair Utilizes DSSM", "startOffset": 0, "endOffset": 3}, {"referenceID": 86, "context": "The deep-intent model proposed in Zhai et al [90, 91] comprises a Bidirectional Recurrent Neural Network (BRNN) combined with an attention module as the SCN.", "startOffset": 45, "endOffset": 53}, {"referenceID": 87, "context": "The deep-intent model proposed in Zhai et al [90, 91] comprises a Bidirectional Recurrent Neural Network (BRNN) combined with an attention module as the SCN.", "startOffset": 45, "endOffset": 53}, {"referenceID": 4, "context": "The attention module, first introduced in [5] for neural machine translation, is referred to as attention pooling layer.", "startOffset": 42, "endOffset": 45}, {"referenceID": 33, "context": "context2vec [34] Query A query Other queries (and clicked ads) in the session", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "contextcontent2vec [34] Query (1) A query (2) The query prefix (1) Other queries (and clicked ads) in the session (2) The last word of the query", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "8: The context2vec model, after [34].", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "The context-content2vec model [34], which has the same architecture as the HDV model depicted in Figure 7b, is aimed at learning query embeddings.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "Besides the context-content2vec model, the context2vec model illustrated in Figure 8, which predicts only temporal context, is also introduced in [34].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Another future direction would be applications of the attention mechanism [5] for designing models that can predict where a user would attend in document, given a query.", "startOffset": 74, "endOffset": 77}, {"referenceID": 33, "context": "For instance, the context-content2vec model of [34] was evaluated only on matching ads to queries yet the distributed query representations could also be evaluated for query suggestion or query auto completion [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "For instance, the context-content2vec model of [34] was evaluated only on matching ads to queries yet the distributed query representations could also be evaluated for query suggestion or query auto completion [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 36, "context": "More broadly, there is a need for systematic and broad task-based experimental surveys that focus on comparative evaluations of models from different categories, but for the same tasks and under the same experimental conditions, very much like the reliable information access (RIA) workshop that was run in the early 2000s to gain a deeper understanding of query expansion and pseudo relevance feedback [37].", "startOffset": 403, "endOffset": 407}, {"referenceID": 49, "context": "The generated textual units have been evaluated through user studies [53, 81].", "startOffset": 69, "endOffset": 77}, {"referenceID": 77, "context": "The generated textual units have been evaluated through user studies [53, 81].", "startOffset": 69, "endOffset": 77}, {"referenceID": 70, "context": ", click-based rankers [74] or, more abstractly, by using models that capture behavioral notions such as examination probability and attractiveness of search results through click models [20].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": ", click-based rankers [74] or, more abstractly, by using models that capture behavioral notions such as examination probability and attractiveness of search results through click models [20].", "startOffset": 186, "endOffset": 190}], "year": 2016, "abstractText": "The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work.", "creator": null}}}