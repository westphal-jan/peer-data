{"id": "1611.03949", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Linguistically Regularized LSTMs for Sentiment Classification", "abstract": "Although a variety of models for neural networks have been proposed recently, previous models have either relied on costly phrase-level annotations, whose performance decreases considerably if they are trained only with sentence-level annotations; or they do not fully utilize linguistic resources (e.g. sentiment lexicographs, negations, intensity words) and are therefore unable to produce linguistically coherent representations. In this paper, we propose simple models that are trained with sentence-level annotations, but also try to create linguistically coherent representations by employing regulators that model the linguistic role of sentiment lexicographs, negation words, and intensity words. Results show that our models are effective at capturing the emotion-altering effects of sentiment, negation, and intensity words, while still achieving competitive results without sacrificing the simplicity of the models.", "histories": [["v1", "Sat, 12 Nov 2016 03:55:10 GMT  (741kb,D)", "http://arxiv.org/abs/1611.03949v1", null], ["v2", "Tue, 25 Apr 2017 18:29:58 GMT  (825kb,D)", "http://arxiv.org/abs/1611.03949v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qiao qian", "minlie huang", "jinhao lei", "xiaoyan zhu"], "accepted": true, "id": "1611.03949"}, "pdf": {"name": "1611.03949.pdf", "metadata": {"source": "CRF", "title": "Linguistically Regularized LSTMs for Sentiment Classification", "authors": ["Qiao Qian", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["qianqiaodecember29@126.com,", "aihuang@tsinghua.edu.cn,", "zxy-dcs@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Understanding sentiment has always been one of the goals of AI in the decades. As a small step toward sentiment understanding, sentiment classification aims to classify sentiment to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification (Turney 2002), and early machine learning based methods (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2005), and recently neural network models such as convolutional neural network (CNN) (Kim 2014; Kalchbrenner, Grefenstette, and Blunsom 2014), recursive autoencoders (Socher et al. 2011; Socher et al. 2013), Long Short-Term Memory (LSTM) (Mikolov 2012; Chung et al. 2014; Tai, Socher, and Manning 2015; Zhu, Sobhani, and Guo 2015), and many more.\nIn spite of the great success of these neural models, there are some defects in previous studies. First, tree-structured models such as recursive autoencoders and Tree-LSTM (Tai, Socher, and Manning 2015; Zhu, Sobhani, and Guo 2015), depend on parsing tree structures and expensive phrase-level\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nannotation, whose performance drops substantially when only trained with sentence-level annotation. Second, sequence models such as CNN and recurrent network are not easy to produce competitive results as reported in the literature (Tai, Socher, and Manning 2015). Third, linguistic knowledge has not been fully employed in neural models, though (Qian et al. 2015) shows that part-of-speech tags can be quite effective for sentence-level classification.\nThe goal of this research is to developing simple sequence models but also attempts to fully employing linguistic resources to benefit sentiment classification. Firstly, we attempts to develop simple models that do not depend on parsing trees and avoid phrase-level annotation which is too expensive in real-world applications. Secondly, in order to obtain competitive performance, simple models can benefit from linguistic resources. Three types of resources can be addressed: sentiment lexicon, negation words, and intensity words. Sentiment lexicon offers the prior polarity of a word which can be useful in determining the sentiment polarity of longer texts such as phrases and sentences. Negation words are typical sentiment shifters (Zhu et al. 2014), which constantly shift sentiment expression. Intensify words such as very, and extremely change the valence degree of sentiment, which is important for fine-grained sentiment classification.\nIn order to model the linguistic role of sentiment, negation, and intensity words, and thus to generate linguistically coherent representations, our central idea is to regularize the difference between the predicted sentiment distribution of the current position, and that of the previous or next positions. For instance, if the current position is a negation word not, the current predicted distribution should be close to the transformed distribution of the next predicted distribution parameterized by the negation transformation matrix. To summarize, our contributions lie in three folds:\n\u2022 We propose several regularizers to model the linguistic role of sentiment, negation, and intensity words in sentiment classification. Experiments show that the regularizers are quite effective.\n\u2022 Due to the complexity of sentiment shifting effect of negation and intensify words, we design word-specific transformation matrices to respect the role of each word.\n\u2022 Unlike previous models depend on parsing structures and expensive phrase-level annotation, our models are simple\nar X\niv :1\n61 1.\n03 94\n9v 1\n[ cs\n.C L\n] 1\n2 N\nov 2\n01 6\nand efficient but also obtain competitive performance."}, {"heading": "Related Work", "text": "Neural Networks for Sentiment Classification Recently, there are many neural networks proposed for sentiment classification. The most pioneering (perhaps) model may be the recursive autoencoder neural network which builds the representation of a sentence from subphrases recursively (Socher et al. 2011; Socher et al. 2013; Dong et al. 2014; Qian et al. 2015). Such recursive models usually depend on a tree structure of input text, and in order to obtain competitive results, usually require heavy annotation on each subphrase. Sequence models do not depend on a particular tree structure, for instance, convolutional neural network (CNN) is another type of widely used models for sentiment classification (Kim 2014; Kalchbrenner, Grefenstette, and Blunsom 2014). As used similarly in image processing, CNN defines convolution operations on a sequence of a text. Long short-term memory models are also common for learning sentence-level representation due to its capability of modeling the prefix or suffix context (Hochreiter and Schmidhuber 1997). LSTM can be commonly applied to sequential data but also tree-structured parsing trees (Zhu, Sobhani, and Guo 2015; Tai, Socher, and Manning 2015). A complete search for optimal structures of recurrent networks and LSTMs can be seen in (Chung et al. 2014).\nApplying Linguistic Knowledge for Sentiment Classification Linguistic knowledge and sentiment resources are very helpful for sentiment analysis such as sentiment lexicons, negation words (not, never, neither, etc.), and intensity words (very, extremely, etc.).\nSentiment lexicon, such as Hu and Liu Lexicon (Hu and Liu 2004) and MPQA lexicon (Wilson, Wiebe, and Hoffmann 2005), is widely used for sentiment classification (Pang and Lee 2008).\nNegation words play a critical role in modifying sentiment of textual expressions. Some early negation models are designed to reverse the sign of sentiment value of the modified text (Polanyi and Zaenen 2006). Since each individual negation word can affect sentiment words in different ways, the shifting hypothesis, is proposed, assuming that negators change the sentiment values by a constant amount (Taboada et al. 2011). The modified word is also an important factor on the negation effect, for instance, negation words turn positive to negative and turn negative to not negative. (Zhu et al. 2014) incorporate negation words as feature into neural network. (Kiritchenko and Mohammad 2016) incorporate negation words and other linguistic knowledge into a SVM classifier for composing opposing polarities.\nThe intensity words can change the valence degree (i.e., sentiment intensity) of the content word. Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. (Taboada et al. 2011) directly reverse the polarity of modified words or change the sentiment strength by a fixed value. (Wei, Wu, and Lin 2011) predict the valence value for content words using a linear regression model. (Malandrakis et al. 2013) introduce a kernel function\nto combine semantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, (Wang, Zhang, and Lan 2016) propose a learning-to-rank model with a pairwise strategy to predict sentiment intensity scores."}, {"heading": "Long Short-term Memory Network", "text": ""}, {"heading": "Long Short-Term Memory (LSTM)", "text": "To deal with the notorious issues of gradient explosion and vanishing in recurrent neural network (Hochreiter and Schmidhuber 1997), Long Short-term Memory network is proposed by incorporating an additional memory cell ct \u2208 Rd at each time step. The hidden states ht and memory cell ct is a function of their previous ct\u22121 and ht\u22121 and input vector xt, formally defined as follows:\nct, ht = g (LSTM)(ct\u22121, ht\u22121, xt) (1)\nThe hidden state ht \u2208 Rd denotes the representation of position t while also encoding the preceding context of the position. For more details about LSTM, we refer readers to (Chung et al. 2014)."}, {"heading": "Bidirectional LSTM", "text": "In LSTM, the representation of each position (ht) only encodes the prefix context in a forward direction while the backward context is not respected. Bidirectional LSTM (Graves, Jaitly, and Mohamed 2013) exploited two parallel passes (forward and backward) and concatenated representations of the two LSTMs as the representation of each position. The forward and backward LSTMs are respectively formulated as follows:\n\u2212\u2192c t, \u2212\u2192 h t = g (LSTM)(\u2212\u2192c t\u22121, \u2212\u2192 h t\u22121, xt) (2) \u2190\u2212c t, \u2190\u2212 h t = g (LSTM)(\u2190\u2212c t+1, \u2190\u2212 h t+1, xt) (3)\nwhere g(LSTM) is the same as that in Eq 1. Particularly, parameters in the two LSTMs are shared. The representation of the entire sentence is [ \u2212\u2192 h n, \u2190\u2212 h 1], where n is the length of the sentence. At each position t, the joint representation ht = [ \u2212\u2192 h t, \u2190\u2212 h t], which is the concatenation of hidden states of the forward LSTM and backward LSTM. In this way, the forward and backward contexts can be considered simultaneously."}, {"heading": "Linguistically Regularized LSTM", "text": "The central idea of the paper is to output linguistically coherent predictions in sentiment classification by regularizing the outputs at adjacent positions of a sentence. For example, in sentence \u201cthis movie is interesting\u201d, the predicted sentiment distributions at \u201cthis*1\u201d, \u201cthis movie*\u201d, and \u201cthis movie is*\u201d should almost be the same, while the predicted sentiment distribution at \u201cthis movie is very interesting*\u201d should be quite different from the preceeding positions since a sentiment word (\u201cinteresting\u201d) is seen.\nMore formally, the predicted sentiment distribution (pt, based on ht, see Eq. 4) at position t should be linguistically\n1The asterisk denotes the current position.\nregularized with respect to that of the preceding (t \u2212 1) or following (t + 1) positions. We propose a generic regularizer and three special regularizers based on the following linguistic observations:\n\u2022 Non-Sentiment Regularizer: if the two adjacent positions are all non-opinion words, the sentiment distributions of the two positions should be close to each other.\n\u2022 Sentiment Regularizer: if the word is a sentiment word found in a lexicon, the sentiment distribution of the current position should be significantly different from that of the next or previous positions.\n\u2022 Negation Regularizer: Negation words such as \u201cnot\u201d and \u201cnever\u201d are critical sentiment shifter (Kennedy and Inkpen 2006): usually shifts sentiment polarity from the positive side to the negative one, but sometimes highly depends on the negation word and the words they modify. The negation regularizer models this linguistic phenomena.\n\u2022 Intensity Regularizer: Intensity words such as \u201cvery\u201d and \u201cextremely\u201d change the valence degree of a sentiment expression: for instance, from positive to very positive. Modeling this effect is quite important for fine-grained sentiment classification, and the intensity regularizer is designed to formulate this effect.\nIn order to enforce the model to produce coherent predictions, we propose a new loss function as follows to incorporate these regularizers:\nE(\u03b8) = \u2212 \u2211 i yi log pi + \u03b1 \u2211 i \u2211 t Lit + \u03b2||\u03b8||2 (4)\nwhere yi is the gold distribution, pi is the predicted distribution output from a softmax layer taking the sentence representation as input, Lit is one of the above regularizers or combination of these regularizers, \u03b1 is the weight for the regularization term, and i, t is the index of sentence and position respectively."}, {"heading": "Non-Sentiment Regularizer (NSR)", "text": "This regularizer constrains that the sentiment distributions of adjacent positions should not vary much if the additional input word xt is not a sentiment word, formally as follows:\nL (NSR) t = max(0, DKL(pt, pt\u22121)\u2212M) (5)\nwhere M is a hyperparameter for margin, pt is the predicted distribution at position t whose representation is ht, and DKL(p, q) is a symmetric KL divergence defined as follows:\nDKL = 1\n2 C\u2211 l=1 p(l) log q(l) + q(l) log p(l)\nwhere p, q are distributions over sentiment labels."}, {"heading": "Sentiment Regularizer (SR)", "text": "The sentiment regularizer constrains that the sentiment distributions of adjacent positions should drift accordingly if the input word is a sentiment word. Let\u2019s revisit the example\n\u201cthis movie is interesting\u201d again. At position t = 4 we see a positive word \u201cinteresting\u201d so the predicted distribution at this position would be more positive than that at position t = 3. This is the issue of sentiment drift.\nIn order to address the sentiment drift issue, we propose a polarity shifting distribution sc \u2208 RC for each sentiment class defined in a lexicon. For instance, a sentiment lexicon may have class labels like strong positive, weakly positive, weakly negative, and strong negative, and for each class, there is a shifting distribution which will be learned by the model. The sentiment regularizer states that if the current word is a sentiment word, the sentiment distribution drift should be observed in comparison to the previous position, as formulated as follows:\np (SR) t\u22121 = pt\u22121 + sc(xt) (6)\nL (SR) t = max(0, DKL(pt, p (SR) t\u22121 )\u2212M) (7)\nwhere p(SR)t\u22121 is the drifted sentiment distribution after considering the shifting sentiment distribution corresponding to the word at position t, c(xt) is the prior sentiment class of word xt, and sc \u2208 \u03b8 is a parameter to be optimized but could also be set fixed with prior knowledge. Note that in this way all words of the same sentiment class share the same drifting distribution, but in a refined setting, we can learn a shifting distribution for each sentiment word if large-scale datasets are available."}, {"heading": "Negation Regularizer (NR)", "text": "The negation regularizer approaches how negation words shift the sentiment distribution of its modifiers. When the input word xt is a negation word, the sentiment distribution should be shifted accordingly. However, the negation role is more complex than that by sentiment words, for example, the word \u201cnot\u201d in \u201cnot good\u201d and \u201cnot bad\u201d have different roles in polarity shifting. The former changes the polarity to negative, while the latter changes to neutral instead of positive.\nTo respect such complex negation effects, we propose a transformation matrix Tm \u2208 RC\u00d7C for each negation word m, and the matrix will be learned by the model. The regularizer assumes that if the current position is a negation word, the sentiment distribution of the current position should be close to that of the next or previous position with the transformation.\np (NR) t\u22121 = softmax(Txj \u00d7 pt\u22121) (8)\np (NR) t+1 = softmax(Txj \u00d7 pt+1) (9)\nL (NR) t = min\n{ max(0, DKL(pt, p (NR) t\u22121 )\u2212M)\nmax(0, DKL(pt, p (NR) t+1 )\u2212M)\n(10)\nwhere p(NR)t\u22121 and p (NR) t+1 is the sentiment distuibution after transformation, Txj \u2208 \u03b8 is the transformation matrix for a negation word xj , a parameter to be learned during training. In total, we train m transformation matrixs for m negation words."}, {"heading": "Intensity Regularizer (IR)", "text": "The intensify regularizer models how intensity words influence the sentiment valence of a phrase or a sentence. Intensifier can change the valence degree of the content word. Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating.\nThe formulation of the intensity effect is quite the same as that in the negation regularizer, but with different parameters of course. For each intensity word, there is a transform matrix to favor the different roles of various intensifiers on sentiment shift. For brevity, we will not repeat the formulas here."}, {"heading": "Applying Linguistic Regularizers to Bidirectional LSTM", "text": "To make our model simple and elegant in a mathematical form, we do not consider the modification scope of negation and intensity word, which is a quite challenging problem in the NLP community. However, we can alleviate the problem by leveraging bidirectional LSTM.\nFor a single LSTM, we employ a backward LSTM from the end to the beginning of a sentence. This is because, at most times, the modified words of negation and intensity words are usually at the right side of the modifiers. But sometimes, the modified words are at the left side of negation and intensity words. To better address this issue, we employ bidirectional LSTM and let the model determine which side should be chosen.\nMore formally, in Bi-LSTM, we compute a transformed sentiment distribution on \u2212\u2192p t\u22121 of the forward LSTM and also that on\u2190\u2212p t+1 of the backward LSTM, and compute the minimum distance of the distribution of the current position to the two distributions. This could be formulated as follows:\n\u2212\u2192p (R)t\u22121 = softmax(Txj \u00d7 \u2212\u2192p t\u22121) (11)\n\u2190\u2212p (R)t+1 = softmax(Txj \u00d7 \u2190\u2212p t+1) (12)\nL (R) t = min\n{ max(0, DKL( \u2212\u2192p t,\u2212\u2192p (R)t\u22121)\u2212M)\nmax(0, DKL( \u2190\u2212p t,\u2190\u2212p (R)t+1)\u2212M)\n(13)\nwhere\u2212\u2192p (R)t\u22121 and \u2190\u2212p (R)t+1 are the sentiment distributions transformed from the previous state \u2212\u2192p t\u22121 and next state \u2190\u2212p t+1 respectively. Note that R \u2208 {NR, IR} indicating the formulation works for both negation and intensity regularizers.\nDue to the same consideration, we redefine L(NSR)t and L (SR) t with bidirectional LSTM similarly. The formulation is the same and omitted for brevity."}, {"heading": "Discussion", "text": "Unlike previous studies on negation and intensity words, which modulate the linguistic effect of these words by some predefined rules, our models address these factors with mathematical operations, parameterized with shifting distribution vectors and transformation matrices. In the sentiment regularizer, the sentiment shifting effect is parameterized\nwith a class-specific distribution (but could also be wordspecific if with more data). In the negation and intensity regularizers, the effect is parameterized with word-specific transformation matrices, meaning that different words have different parameters. Since the mechanism of how negation and intensity words shift sentiment expression is quite complex and highly dependent on individual words, we believe such mathematical operation will be more suitable for addressing complex linguistic roles of these words. This is a major advantage of our approach over other methods."}, {"heading": "Experiment", "text": ""}, {"heading": "Dataset and Sentiment Lexicon", "text": "Two datasets are used for evaluating the proposed models: Movie Review (MR) (Pang and Lee 2005) which has two classes as negative, positive and Stanford Sentiment Treebank (SST) (Socher et al. 2013) which has five classes. For details, we refer readers to the two papers. SST has provided phrase-level annotation on all inner nodes, but we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation.\nThe sentiment lexicon contains two parts. The first part comes from MPQA (Wilson, Wiebe, and Hoffmann 2005), which contains 5, 153 sentiment words, each with polarity rating. The second part consists of the leaf nodes of the SST dataset (i.e., all sentiment words) and there are 6, 886 polar words except neural ones. We combine the two parts and ignore those words that have conflicting sentiment labels, and produce a lexicon of 9, 750 words with 4 sentiment labels. For negation and intensity words, we collect them manually since the number is small, some of which can be seen in Table 2.\nRegarding the parameters and initialization details of the models, please refer to the \u201cSupplemental Material\u201d due to the length limit."}, {"heading": "Overall Comparison", "text": "We include several lines of baselines in the evaluation. The baselines are listed as follows: \u2022 RNN/RNTN Recursive Neural Network over parsing\ntrees, proposed by (Socher et al. 2011) and Recursive Tensor Neural Network (Socher et al. 2013) employs tensors to model correlations between different dimensions of child nodes\u2019 vectors.\n\u2022 LSTM/Bi-LSTM Long-short Term Memory (Cho et al. 2014) and the bidirectional variant as introduced previously.\n\u2022 Tree-LSTM Tree-Structured Long Short-Term Memory (Tai, Socher, and Manning 2015) introduces memory cells and gates into tree-structured neural network.\n\u2022 CNN Convolutional Neural Network (Kalchbrenner, Grefenstette, and Blunsom 2014) generates sentence representation by convolution and pooling operations. Firstly, we evaluate our model on the MR dataset and the results are shown in Table 3. As can be seen, we can make the following statements: \u2022 Both LR-LSTM and LR-Bi-LSTM outperforms their\ncounterparts substantially (81.5% vs. 77.4% and 82.1% vs. 79.3%, resp.), demonstrating the effectiveness of the linguistic regularizers.\n\u2022 LR-LSTM and LR-Bi-LSTM perform slightly better than Tree-LSTM but Tree-LSTM leverages a constituency tree structure while our model is a simple sequence model. As future work, we will apply such regularizers to treestructured models.\n\u2022 On this dataset, our model is comparable to CNN. For fine-grained sentiment classification, we evaluate our model on the SST dataset which has five sentiment classes { very negative, negative, neutral, positive, very positive} so that we can evaluate the sentiment shifting effect of intensity words. The experiment result is shown in Table 3. We have the following observations: \u2022 Similarly, linguistically regularized LSTM and Bi-LSTM\nare better than their counterparts. It\u2019s worth noting that LR-Bi-LSTM (trained with just sentence-level annotation) is even comparable to Bi-LSTM trained with phraselevel annotation. That means, LR-Bi-LSTM can avoid the heavy phrase-level annotation but still obtain competitive results.\n\u2022 Our models are comparable to Tree-LSTM but our models are not dependent on a parsing tree and more simple, and hence more efficient. Further, for Tree-LSTM, the model is heavily dependent on phrase-level annotation, otherwise the performance drops substantially (from 51% to 48.1%).\n\u2022 On this dataset, our model is apparently better than CNN."}, {"heading": "The Effect of Different Regularizers", "text": "In order to reveal the effect of each individual regularizer, we conduct ablation experiments. Each time, we remove a regularizer and observe how the performance varies. First of all,\nwe conduct this experiment on the entire datasets, and then we experiment with sub-datasets that only contain negation words or intensity words.\nThe experiment results are shown in Table 4 where we can see that the non-sentiment regularizer and sentiment regularizer play a key role2, and the negation regularizer and intensity regularizer are effective but less important than the previous two regularizers. This may be due to the fact that only 14% of sentences contains negation words in the test datasets, and 23% contains intensity words, and thus we further evaluate the models on two subsets, as shown in Table 5.\nThe experiments on the subsets show that: 1) With linguistic regularizers, LR-Bi-LSTM outperforms Bi-LSTM remarkably on these subsets; 2) When the negation regularizer is removed from the model, the performance drops significantly on both MR and SST subsets; 3) Similar observations can be made regarding the intensity regularizer.\n2Kindly note that almost all sentences contain sentiment words, see Tab. 1."}, {"heading": "The Effect of the Negation Regularizer", "text": "To further reveal the linguistic role of negation words, we compare the predicted sentiment distributions of a phrase pair with or without a negation word. The experimental results performed on MR are shown in Fig. 1. Each dot denotes a phrase pair (for example, <interesting, not interesting>), where the x-axis denotes the positive score3 of the phrase without negators (e.g., interesting), and the y-axis indicates the positive score for the phrase with negators (e.g., not interesting). The curves in the figures show this function: [1 \u2212 y, y] = softmax(Tnw \u2217 [1 \u2212 x, x]) where [1 \u2212 r, r] is a sentiment distribution on [negative, positive], x is the positive score of the phrase without negators (x-axis) and y that of the phrase with negators (y-axis), and Tnw is the transformation matrix for the negation word nw, see Eq. 8.\nWe can observe the following statements:\n\u2022 All the dots are distributed around the curve, and there is no dot at the up-right and bottom-left blocks, indicating the regularizer plays in a role in sentiment shifting of negators.\n\u2022 The dots at the up-left and bottom-right respectively indicates the negation effects: changing negative to positive and positive to negative.Typical phrases include never seems hopelessly (up-left), no good scenes (bottom-right), not interesting (bottom-right), etc. There are also some positive/negative phrases shifting to neutral sentiment such as not so good, and not too bad.\n\u2022 The dots located at the center indicate that neutral phrases maintain neutral sentiment with negators. Typical phrases include not at home, not here, where negators typically modify non-sentiment words.\n3 The score is obtained from the predicted distribution, where 1 means positive and 0 means negative."}, {"heading": "The Effect of the Intensity Regularizer", "text": "To further reveal the linguistic role of intensity words, we perform experiments on the SST dataset, as illustrated in Figure 2. We show the confusion matrix that shows how the sentiment shifts after being modified by intensifiers. The number 20 in the first matrix, for instance, means that there are 20 phrases have a sentiment class of negative (-) but shifting to very negative (- -) after being modified by an intensity word \u201cvery\u201d.\nAs can be seen from the results, for \u201cmost\u201d, there are 21/21/13/12 phrases whose sentiment is shifted from negative to very negative (eg. most irresponsible picture), positive to very positive (eg. most famous author), neutral to negative (eg. most plain), and neutral to positive (eg. most closely), respectively. Similar observations can be found with word \u201cvery\u201d.\nThere are also many phrases maintain their sentiment. No surprisingly, for very positive/negative phrases, phrases modified by intensifiers still maintain the strong sentiment. For the left phrases, they fall into three categories: first, words modified by intensifiers are non-sentiment words, such as most of us, most part; second, intensifiers are not strong enough to shift sentiment, such as most complex (from negative to negative), most traditional (from positive to positive); third, our models fail to shift sentiment with intensifiers such as most vital, most resonant film."}, {"heading": "Conclusion and Future Work", "text": "We present linguistically regularized LSTMs for sentencelevel sentiment classification. The proposed models address the sentient shifting effect of sentiment, negation, and intensity words to produce linguistically coherent representations. Furthermore, our models are sequence LSTMs which do not depend on a parsing tree-structure and do not require expensive phrase-level annotation to obtain competitve results. Results show that our models are able to address the linguistic role of sentiment, negation, and intensity words.\nIn order to maintain the simplicity of the proposed models, we do not fully consider the modification scope of negation and intensity words, though we partially address this issue by applying a minimization operartor (see Eq. 10, Eq. 13) and bi-directional LSTM. As future work, we plan to apply the linguistic regularizers to tree-LSTM to address the scope issue since the parsing tree is easier to indicate the modification scope explicitly."}], "references": [{"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung,? \\Q2014\\E", "shortCiteRegEx": "Chung", "year": 2014}, {"title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis", "author": ["Dong"], "venue": null, "citeRegEx": "Dong,? \\Q2014\\E", "shortCiteRegEx": "Dong", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Jaitly Graves", "A. Mohamed 2013] Graves", "N. Jaitly", "A.-r. Mohamed"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Liu", "author": ["M. Hu"], "venue": "B.", "citeRegEx": "Hu and Liu 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A convolutional neural network for modelling sentences", "author": ["Grefenstette Kalchbrenner", "N. Blunsom 2014] Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "and Inkpen", "author": ["A. Kennedy"], "venue": "D.", "citeRegEx": "Kennedy and Inkpen 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["S. Kiritchenko", "Mohammad"], "venue": "M.", "citeRegEx": "Kiritchenko and Mohammad 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributional semantic models for affective text analysis", "author": ["Malandrakis"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "Malandrakis,? \\Q2013\\E", "shortCiteRegEx": "Malandrakis", "year": 2013}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Lee", "author": ["B. Pang"], "venue": "L.", "citeRegEx": "Pang and Lee 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Lee Pang", "B. Vaithyanathan 2002] Pang", "L. Lee", "S. Vaithyanathan"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "and Zaenen", "author": ["L. Polanyi"], "venue": "A.", "citeRegEx": "Polanyi and Zaenen 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning tag embeddings and tag-specific composition functions in recursive neural network", "author": ["Qian"], "venue": "In ACL,", "citeRegEx": "Qian,? \\Q2015\\E", "shortCiteRegEx": "Qian", "year": 2015}, {"title": "C", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "Manning"], "venue": "D.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng"], "venue": "Y.; and Potts, C.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["Taboada"], "venue": "Computational linguistics", "citeRegEx": "Taboada,? \\Q2011\\E", "shortCiteRegEx": "Taboada", "year": 2011}, {"title": "C", "author": ["K.S. Tai", "R. Socher", "Manning"], "venue": "D.", "citeRegEx": "Tai. Socher. and Manning 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["Turney"], "venue": "D.", "citeRegEx": "Turney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ecnu at semeval-2016 task 7: An enhanced supervised learning method for lexicon sentiment intensity ranking", "author": ["Zhang Wang", "F. Lan 2016] Wang", "Z. Zhang", "M. Lan"], "venue": "Proceedings of SemEval 491\u2013496", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A regression approach to affective rating of chinese words from anew", "author": ["Wu Wei", "W.-L. Lin 2011] Wei", "C.-H. Wu", "J.C. Lin"], "venue": "In Affective Computing and Intelligent Interaction", "citeRegEx": "Wei et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2011}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Wiebe Wilson", "T. Hoffmann 2005] Wilson", "J. Wiebe", "P. Hoffmann"], "venue": null, "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Zhu"], "venue": null, "citeRegEx": "Zhu,? \\Q2014\\E", "shortCiteRegEx": "Zhu", "year": 2014}, {"title": "Long short-term memory over recursive structures", "author": ["Sobhani Zhu", "X. Guo 2015] Zhu", "P. Sobhani", "H. Guo"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Sentiment understanding has been a long-term goal of AI in the past decades. This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations. In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models\u2019 simplicity.", "creator": "LaTeX with hyperref package"}}}