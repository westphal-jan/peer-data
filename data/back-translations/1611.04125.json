{"id": "1611.04125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion", "abstract": "In this paper, we propose a novel framework for embedding words, entities, and relationships in the same continuous vector space. In this model, both entity and relationship embedding are learned using knowledge graphs and plaintext. In experiments, we evaluate the common learning model on three tasks, including entity prediction, relationship prediction, and relationship classification from text. Experimental results show that our model can significantly and consistently improve performance on the three tasks compared to other baselines.", "histories": [["v1", "Sun, 13 Nov 2016 12:32:20 GMT  (223kb,D)", "http://arxiv.org/abs/1611.04125v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu han", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1611.04125"}, "pdf": {"name": "1611.04125.pdf", "metadata": {"source": "CRF", "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion", "authors": ["Xu Han", "Zhiyuan Liu", "Maosong Sun"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "People construct various large-scale knowledge graphs (KGs) to organize structural knowledge about the world, such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), DBPedia (Auer et al., 2007) and WordNet (Miller, 1995). A typical knowledge graph is a multiple-relational directed graph with nodes corresponding to entities, and edges corresponding to relations between these entities. Knowledge graphs are playing an important role in numerous applications such as question answering and Web search.\nThe facts in knowledge graphs are usually recorded as a set of relational triples (h, r, t) with h and t indicating head and tail entities and r the relation between h and t, e.g., (Mark Twain, PlaceOfBirth, Florida).\nTypical large-scale knowledge graphs are usually far from complete. The task of knowledge graph completion aims to enrich KGs with novel facts. Based on the network structure of KGs, many graphbased methods have been proposed to find novel facts between entities (Lao et al., 2011; Lao and Cohen, 2010). Many efforts are also devoted to extract relational facts from plain text (Zeng et al., 2014; dos Santos et al., 2015). However, these approaches cannot jointly take both KGs and plain texts into consideration.\nIn recent years, neural-based knowledge representation has been proposed to encode both entities and relations into a low-dimensional space, which are capable to find novel facts (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015). More importantly, neural models enable us to conduct joint rep-\nar X\niv :1\n61 1.\n04 12\n5v 1\n[ cs\n.C L\n] 1\n3 N\nov 2\nresentation learning of text and knowledge within a unified semantic space, and perform knowledge graph completion more accurately.\nSome pioneering works have been done. For example, (Wang et al., 2014a) performs joint learning simply considering alignment between words and entities, and (Toutanova et al., 2015) extracts textual relations from plain texts using dependency parsing to enhance relation embeddings. These works either consider only partial information in plain text (entity mentions in (Wang et al., 2014a) and textual relations in (Toutanova et al., 2015)), or rely on complicated linguistic analysis (dependency parsing in (Toutanova et al., 2015)) which may bring inevitable parsing errors.\nTo address these issues, we propose a novel framework for joint representation learning. As shown in Figure 1, the framework is expected to take full advantages of both text and KGs via complicated alignments with respect to words, entities and relations. Moreover, our method applies deep neural networks instead of linguistic analysis to encode the semantics of sentences, which is especially capable of modeling large-scale and noisy Web text.\nWe conduct experiments on a real-world dataset with KG extracted from Freebase and text derived from the New York Times corpus. We evaluate our method on the tasks including entity prediction, relation prediction with embeddings and relation classification from text. Experiment results demonstrate that, our method can effectively perform joint representation learning and obtain more informative knowledge representation, which significantly outperforms other baseline methods on all three tasks."}, {"heading": "2 Related Work", "text": "The work in this paper relates to representation learning of KGs, words and textual relations. Related works are reviewed as follows.\nRepresentation Learning of KGs. A variety of approaches have been proposed to encode both entities and relations into a continuous low-dimensional space. Inspired by (Mikolov et al., 2013b), TransE (Bordes et al., 2013) regards the relation r in each (h, r, t) as a translation from h to t within the lowdimensional space, i.e., h + r = t, where h and t are entity embeddings and r is relation embed-\nding. Despite of its simplicity, TransE achieves the state-of-the-art performance of representation learning for KGs, especially for those large-scale and sparse KGs. Hence, we simply incorporate TransE in our method to handle representation learning for KGs.\nNote that, our method is also flexible to incorporate extension models of TransE, such as TransH (Wang et al., 2014b) and TransR (Lin et al., 2015), which is not the focus of this paper and will be left as our future work.\nRepresentation Learning of Textual Relations. Many works aim to extract relational facts from large-scale text corpora (Mintz et al., 2009; Riedel et al., 2010). This indicates textual relations between entities are contained in plain text. In recent years, deep neural models such as convolutional neural networks (CNN) have been proposed to encode semantics of sentences to identify relations between entities (Zeng et al., 2014; dos Santos et al., 2015). As compared to conventional models, neural models are capable to accurately capture textual relations between entities from text sequences without explicitly linguistic analysis, and further encode into continuous vector space. Hence, in this work we apply CNN to embed textual relations and conduct joint learning of text and KGs with respect to relations.\nMany neural models such as recurrent neural networks (RNN) (Zhang and Wang, 2015) and longshort term memory networks (LSTM) (Xu et al., 2015) have also been explored for relation extraction. These models can also be applied to perform representation learning for textual relations, which will be explored in future work.\nRepresentation Learning of Words. Given a text corpus, we can learn word representations without supervision. The learning objective is defined as the likelihood of predicting its context words of each word or vice versa (Mikolov et al., 2013b). Continuous bag-of-words (CBOW) (Mikolov et al., 2013a) and Skip-Gram (Mikolov et al., 2013c) are stateof-the-art methods for word representation learning. The learned word embeddings can capture both syntactic and semantic features of words derived from plain text. As reported in many previous works, deep neural network will benefit significantly if being initialized with pre-trained word embeddings (Erhan et al., 2010). In this work, we apply Skip-Gram for\nword representation learning, which serves as initialization for joint representation learning of text and KGs."}, {"heading": "3 The Framework", "text": "In this section we introduce the framework of joint representation learning, starting by notations and definitions."}, {"heading": "3.1 Notations and Definitions", "text": "We denote a knowledge graph as G = {E,R, T}, where E indicates a set of entities, R indicates a set of relation types, and T indicates a set of fact triples. Each triple (h, r, t) \u2208 T indicates there is a relation r \u2208 R between h \u2208 E and t \u2208 E.\nWe denote a text corpus as D and its vocabulary as V , containing all words, phrases and entity mentions. In the corpus D, each sentence is denoted as a word sequence s = {x1, . . . , xn}, xi \u2208 V , and the length is n.\nFor entities, relations and words, we use the bold face to indicate their corresponding lowdimensional vectors. For example, the embeddings of h, t \u2208 E, r \u2208 R and x \u2208 V are h, t, r,x \u2208 Rk of k dimension, respectively."}, {"heading": "3.2 Joint Learning Method", "text": "As mentioned in Section 2, representation learning methods have been proposed for knowledge graphs and text corpora respectively. In this work, we propose a joint learning framework for both KGs and text.\nIn this framework, we aim to learn representations of entities, relations and words jointly. Denote all these representations as model parameters \u03b8 = {\u03b8E , \u03b8R, \u03b8V }. The framework aims to find optimized parameters\n\u03b8\u0302 = argmin\u03b8L\u03b8(G,D), (1)\nwhere L\u03b8(G,D) is the loss function defined over the knowledge graph G and the text corpus D. The loss function can be further decomposed as follows,\nL\u03b8(G,D) = L\u03b8E ,\u03b8R(G) + \u03c4L\u03b8(D) + \u03bb\u2016\u03b8\u20162, (2)\nwhere \u03c4 and \u03bb are harmonic factors, and \u2016\u03b8\u20162 is the regularizer defined as L2 distance.\nL\u03b8E ,\u03b8R(G) is responsible to learn representations of both entities and relations from the knowledge graph G. This part will be introduced in detail in Section 3.3. L\u03b8(D) is responsible to learn representations of entities and relations as well as words from the text corpus T . It is straightforward to learn word representations from text as discussed in Section 2. On the contrary, since entities and relations are not explicitly shown in text, we have to identify entities and relations in text to support representation learning of entities and relations from text. The process is realized by entity-text alignment and relation-text alignment.\nEntity-Text Alignment. Many entities are mentioned in text. Due to the complex polysemy of entity mentions (e.g., an entity name Washington in a sentence could be indicating either a person or a location), it is non-trivial to build entity-text alignment. The alignment can be built via entity linking techniques or anchor text information. In this paper, we simply use the anchor text annotated in articles to build the alignment between entities in E and entity mentions in V . We will share the aligned entity representations to corresponding entity mentions.\nRelation-Text Alignment. As mentioned in Section 2, textual relations can be extracted from text. Hence, relation representation can also be learned from plain text. Inspired by the idea of distant supervision, for a relation r \u2208 R, we collect all entity pairs Pr = {(h, t)} connected by r in KG. Afterwards, for each entity pair in Pr, we extract all sentences that contain the both entities from D, and regard them as the positive instances of the relation r. We can further apply deep neural networks to encode the semantic of these sentences into the corresponding relation representation. The process will be introduced in detail in Section 3.4.\nIn summary, the framework enables joint representation learning of both entities and relations by taking full advantages of both KG and text. The learned representations are expected to be more informative and robust, which will be verified in experiments."}, {"heading": "3.3 Representation Learning of KGs", "text": "We select TransE (Bordes et al., 2013) to learn representations of entities and relations from KGs.\nFor each entity pair (h, t) in a KG G, we define their latent relation embedding rht as a translation from h to t, which can be formalized as:\nrht = t\u2212 h. (3)\nMeanwhile, each triple (h, r, t) \u2208 T has an explicit relation r between h and t. Hence, we can define the scoring function for each triple as follows:\nfr(h, t) = \u2016rht \u2212 r\u20162 = \u2016(t\u2212 h)\u2212 r\u20162. (4)\nThis indicates that, for each triple (h, r, t) in T , we expect h + r \u2248 t.\nBased on the above scoring function, we can formalize the loss function over all triples in T as follows: L(G) = \u2211\n(h,r,t)\u2208T \u2211 (h\u2032,r\u2032,t\u2032)\u2208T \u2032 [ \u03b3+fr(h, t)\u2212fr\u2032(h\u2032, t\u2032) ] + .\n(5) Here [x]+ indicates keeping the positive part of x and \u03b3 > 0 is a margin. T \u2032 is the set of incorrect triples:\nT \u2032 = {(h\u2032, r, t)} \u222a {(h, r\u2032, t)} \u222a {(h, r, t\u2032)}, (6)\nwhich is constructed by replacing the entity and relation in each triple (h, r, t) \u2208 T with other entities h\u2032, t\u2032 \u2208 E and relations r\u2032 \u2208 R."}, {"heading": "3.4 Representation Learning of Textual Relations", "text": "Given a sentence containing two entities, the words in the sentence usually expose implicit features of the textual relation between the two entities. As shown in (Zeng et al., 2014), the textual relations can be learned with deep neural networks and encoded in the low-dimensional semantic space.\nWe follow (Zeng et al., 2014) and apply convolutional neural networks (CNN) to model textual relations from text. CNN is an efficient neural model widely used in image processing, which has recently been verified to be also effective for many NLP tasks such as part-of-speech tagging, named entity recognition and semantic role labeling (Collobert et al., 2011)."}, {"heading": "3.4.1 Overall Architecture", "text": "Figure 2 depicts the overall architecture of CNN for modeling textual relations. For a sentence s containing (h, t) with a relation r, the architecture takes word embeddings s = {x1, . . . ,xn} of the sentence s as input, and after passing through two layers within CNN, outputs the embedding of the textual relation rs. Our method will further learn to minimize the loss function between r and rs, which can be formalized as:\nfr(s) = \u2016rs \u2212 r\u20162. (7)\nBased on the scoring function, we can formalize the loss function over all sentences in D as follows,\nL(D) = \u2211 s\u2208D \u2211 r\u2032 6=r [ \u03b3 + fr(s)\u2212 fr\u2032(s) ] + , (8)\nwhere the notations are identical to Eq. (5). CNN contains an input layer, a convolution layer and a pooling layer, which are introduced in detail as follows."}, {"heading": "3.4.2 Input Layer", "text": "Given a sentence s made up of n words s = {x1, . . . , xn}, the input layer transforms the words\nof s into corresponding word embeddings s = {x1, . . . ,xn}. For a word xi in the given sentence, its input embedding xi is composed of two realvalued vectors: its textual word embedding wi and its position embedding pi.\nTextual word embeddings encode the semantics of the corresponding words, which are usually pre-trained from plain text via word representation learning, as introduced in Section 3.5.\nWord position embeddings (WPE) is originally proposed in (Zeng et al., 2014). WPE is a position feature indicating the relative distances of the given word to the marked entities in the sentence. As shown in Figure 2, the relative distances of the word born to the entities Mark Twain and Florida are \u22122 and +2 respectively. We map each distance to a vector of dimension kp in the continuous latent space. Given the word xi in the sentence s, the word position embedding is pi = [phi ,p t i], where p h i and pti are vectors of distances to the head entity and tail entity respectively.\nWe simply concatenate textual word embeddings and word position embeddings to build the input for CNN:\ns = {[w1;p1], . . . , [wn;pn]}. (9)"}, {"heading": "3.4.3 Convolution Layer", "text": "By taking s as the input, the convolution layer will output y. The generation process is formalized as follows.\nWe slide a window of size m over the input word sequence. For each move, we can get an embedding x\u2032i as:\nx\u2032i = [ xi\u2212m\u22121\n2 ; . . . ;xi; . . . ;xi+m\u22121 2\n] , (10)\nwhich is obtained by concatenating m vectors in s with xi as center. For instance in Figure 2, a window slides through the input vectors s and concatenates every three word embeddings. Afterwards, we transform x\u2032i into the hidden layer vector yi\nyi = tanh(Wx \u2032 i + b), (11)\nwhere W \u2208 Rkc\u00d7mkw is the convolution kernel, b \u2208 Rkc is a bias vector, kc is the dimension of hidden layer vectors yi, kw is the dimension of input vectors xi, and m is the window size."}, {"heading": "3.4.4 Pooling Layer", "text": "In the pooling layer, a max-pooling operation over the hidden layer vectors y1, . . . ,yn is applied to get the final continuous vector as the textual relation embedding rs, which is formalized as follows:\nrs,j = max{y1,j , . . . ,yn,j}, (12)\nwhere rs,j is the j-th value of the textual relation embedding rs, and yi,j is the j-th value of the hidden layer vector yi. After the pooling operation, we can get the given sentence textual relation embedding to loss function Eq. (7)."}, {"heading": "3.5 Initialization and Implementation Details", "text": "There are a large number of parameters to be optimized for joint learning. It is thus crucial to initialize these parameters appropriately. For those aligned entities and words, we initialize their embeddings via word representation learning. We follow (Mikolov et al., 2013c) and use Skip-Gram to learn word representations from the given text corpus. For relations and other entities, we initialize their embeddings randomly.\nBoth the knowledge model TransE and textual relation model CNN are optimized simultaneously using stochastic gradient descent (SGD). The parameters of all models are trained using a batch training algorithm. Note that, the gradients of CNN parameters will be back-propagated to the input word embeddings so that the embeddings of both entities and words can also be learned from plain text via CNN."}, {"heading": "4 Experiments", "text": "We conduct experiments on entity prediction and relation prediction and evaluate the performance of our methods with various baselines."}, {"heading": "4.1 Experiment Settings", "text": ""}, {"heading": "4.1.1 Datasets", "text": "Knowledge Graph. We select Freebase (Bollacker et al., 2008) as the knowledge graph for joint learning. Freebase is a widely-used large-scale world knowledge graph. In this paper, we adopt a datasets extracted Freebase, FB15K, in our experiments. The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015). We\nlist the statistics of FB15K in Table 1, including the amount of entities, relations and triples.\nText Corpus. We select sentences from the New York Times articles to align with FB15K for joint learning. To ensure alignment accuracy, we only consider those sentences with anchor text linking to the entities in FB15K. We extract 876, 227 sentences containing both head and tail entities in FB15K triples, and annotate with the corresponding relations in triples. The sentences are labeled with 29, 252 FB15K triples, including 629 relations and 5244 entities. We name the corpus as NYT."}, {"heading": "4.1.2 Evaluation Tasks", "text": "In experiments we evaluate the joint learning model and other baselines with three tasks:\n(1) Entity Prediction. The task aims at predicting missing entities in a triple according to the embeddings of another entity and relation.\n(2) Relation Prediction. The task aims at predicting missing relations in a triple according to the embeddings of head and tail entities.\n(3) Relation Classification from Text. We are also interested in extracting relational facts between novel entities not included in knowledge graphs. Hence, we conduct relation classification from text, without taking advantages of entity embeddings learned with knowledge graph structure."}, {"heading": "4.1.3 Parameter Settings", "text": "In our joint model, we select the learning rate \u03b1k on the knowledge side among {0.1, 0.01, 0.001}, and learning rate \u03b1t on the text side among {0.01, 0.025, 0.05}. The harmonic factor \u03bb = 1 and the margin \u03b3 = 1. We select the harmonic factor \u03c4 among {0.001, 0.0001, 0.00001} to balance the learning ratio between knowledge and text. The dimension of embeddings k is selected among {50, 100, 150}. The optimal configurations are \u03b1k = 0.001, \u03b1t = 0.025, \u03c4 = 0.0001, k = 150. During the learning process, we traverse the text corpus for 10 rounds as well as triples in the knowledge graph for 3000 rounds."}, {"heading": "4.2 Results of Entity Prediction", "text": "Entity prediction has also been used for evaluation in (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015). More specifically, we need to predict the tail entity when given a triple (h, r, ?) or predict the head entity when given a triple (?, r, t). In this task, for each missing entity, the system is asked to rank all candidate entities from the knowledge graph instead of only giving one best result. For each test triple (h, r, t), we replace head and tail entities with all entities in FB15K ranked in descending order of similarity scores calculated by \u2016h+r\u2212t\u20162. The relational fact (h, r, t) is expected to have smaller score than any other corrupted triples.\nWe follow (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015) and use the proportion of correct entities in Top-10 ranked entities (Hits@10) as the evaluation metric. As mentioned in (Bordes et al., 2013), a corrupted triple may also exist in knowledge graphs, which should not be considered as incorrect. Hence, before ranking, we filter out those corrupted triples that have appeared in FB15K.\nThe relations in knowledge graphs can be divided into four classes: 1-to-1, 1-to-N, N-to-1 and N-toN relations, where a \u201c1-to-N\u201d relation indicates a head entity may correspond to multiple tail entities in knowledge graphs, and so on. For example, the relation (Country, PresidentOf, Person) is a typical \u201c1-to-N\u201d relation, because there used to be many presidents for a country in history. We report the average Hits@10 scores when predicting missing head entities and tail entities with respect to different classes of relations. We also report the overall performance by averaging the Hits@10 scores over triples and over relations.\nSince the evaluation setting is identical, we simply report the results of TransE, TransH and TransR from (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015), where \u201cunif\u201d and \u201cbern\u201d are two settings to sample negative instances for learning. We also report the results of TransE we implement for joint learning.\nThe evaluation results on entity prediction is shown in Table 2. From Table 2 we observe that:\n(1) The joint model almost achieves improvements under four classes of relations when predicting head and tail entities. This indicates the per-\nformance of joint learning is consistent and robust. Note that, our TransE version, which is implemented by ourselves, outperforms previous TransE, TransH and TranR, by simply increase the embedding dimension to k = 150, which suggests the effectiveness of TransE.\n(2) The improvements on \u201c1-to-1\u201d, \u201c1-to-N\u201d and \u201cN-to-1\u201d relations are much more significant as compared to those on \u201cN-to-N\u201d. This indicates that our joint model is more effective to embed textual relations for those deterministic relations.\n(3) Our joint model achieves improvement of more than 13% than TransE when averaging over relations. This indicates that, our joint model can take advantages of plain texts and greatly improve representation power in relation-level.\n(4) In FB15K, the relation numbers in different relation classes are comparable, but more than 80% triples are instances of \u201cN-to-N\u201d relations. Since the improvement of the joint model on \u201cN-to-N\u201d relations is not as remarkable as on other relation classes, hence the overall superiority of our joint model seems not so notable when averaging over triples as compared to averaging over relations."}, {"heading": "4.3 Results of Relation Prediction", "text": "The task aims to predict the missing relation between two entities based on their embeddings. More specifically, we need to predict the relation when given a triple (h, ?, t). In this task, for each missing relation, the system is asked to find one best result, according to similarity scores calculated by \u2016h + r \u2212 t\u20162. Because the number of relations is much smaller, compared with the number of entities, we use the accuracy of Top-1 ranked relations as the evaluation metric. Since some entities may have more than one relation between them, we also filter out those triples with corrupted relations appeared in knowledge graphs. We report the overall\nevaluation results as well as those in different relation classes.\nThe evaluation results are shown in Table 3. From Table 3 we observe that, our joint model outperforms TransE consistently in different classes of relations and in all. The joint model also achieves more significant improvements on \u201c1-to-1\u201d, \u201c1-toN\u201d and \u201cN-to-1\u201d relations. The observations are compatible with those on entity prediction."}, {"heading": "4.4 Results of Relation Classification from Text", "text": "The task aims to extract relational facts from plain text. The task has been widely studied, also named as relation extraction from text. Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers. As compared to relation prediction with embeddings, the task only uses plain text to identify relational facts, and thus is capable for novel entities not necessarily having appeared in knowledge graphs. Since there is much noise in plain text and distant supervision, it makes the task not easy. With this task, we want to investigate the effectiveness of our joint model for learning CNN models.\nWe follow (Weston et al., 2013) to conduct evaluation. The evaluation construct candidate triples combined by entity pairs in testing set and various relations, ask systems to rank these triples according to the corresponding sentences of entity pairs, and by regarding the triples in knowledge graphs\nas correct and others as incorrect, evaluate systems with precision-recall curves. Note that, the evaluation task does not consider knowledge embeddings for ranking.\nThe evaluation results on NYT test set are shown in Figure 3, where Joint-CNN indicates the CNN model learned jointly in our model, and CNN indicates the conventional CNN model learned individually from plain text. We find that, the sentence counts of different relation types vary much, and may also influence the performance of relation classification. About ninety-five percent sentences belong to the most frequent 100 relations in our dataset. In order to alleviate the influence of sentence counts, we select Top-100 relations and evaluate the classification performance among them. From Figure 3 we observe that: Joint-CNN outperforms CNN significantly over all the range. This indicates that, the joint learning model can also result in a more effective CNN model for relation classification from text. This will greatly benefit the relation extraction task, especially for those novel entities."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a model for joint learning of text and knowledge representations. Our joint model embeds entities, relations and words in the same continuous latent space. More specifically, we adopt deep neural networks CNN to encode textual relations for joint learning of relation embeddings. In experiments, we evaluate our joint model on three tasks including entity prediction, relation prediction with embeddings, and relation prediction from text.\nExperiment results show that our joint model can effectively perform representation learning from both knowledge graphs and plain text, and obtain more discriminative entity and relation embeddings for prediction. In future, we will explore the following research directions:\n(1) Distant supervision may introduce many noisy sentences with incorrect relation annotations. We will explore techniques such as multi-instance learning to reduce these noises and improve the effectiveness of joint learning. We will also explore the effectiveness of more deep neural networks like recurrent neural networks, long short-term memory other than CNN for joint learning.\n(2) Our joint model is also capable to incorporate other knowledge representation models instead of TransE, such as TransH and TransR. In future we will explore their capability in our joint model.\n(3) We will also take more rich information in our joint model, such as relation paths in knowledge graphs, and the textual relations represented by more than one sentence in a paragraph or document. These information can also be used to incorporate into knowldege graphs.\nThese future work will further improve performance over knowledge and text representation, this may let the joint model make better use of knowledge and text."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Why does unsupervised pretraining help deep learning", "author": ["Erhan et al.2010] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Pallavi Choudhury", "Michael Gamon"], "venue": "Proceedings of EMNLP", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang et al.2014a] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014b] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston et al.2013] Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "Proceedings of EMNLP", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Relation classification via recurrent neural network. arXiv preprint arXiv:1508.01006", "author": ["Zhang", "Wang2015] Dongxu Zhang", "Dong Wang"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "People construct various large-scale knowledge graphs (KGs) to organize structural knowledge about the world, such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": ", 2008), YAGO (Suchanek et al., 2007), DBPedia (Auer et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": ", 2007), DBPedia (Auer et al., 2007) and WordNet (Miller, 1995).", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": ", 2007) and WordNet (Miller, 1995).", "startOffset": 20, "endOffset": 34}, {"referenceID": 9, "context": "Based on the network structure of KGs, many graphbased methods have been proposed to find novel facts between entities (Lao et al., 2011; Lao and Cohen, 2010).", "startOffset": 119, "endOffset": 158}, {"referenceID": 24, "context": "Many efforts are also devoted to extract relational facts from plain text (Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 74, "endOffset": 118}, {"referenceID": 2, "context": "In recent years, neural-based knowledge representation has been proposed to encode both entities and relations into a low-dimensional space, which are capable to find novel facts (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 179, "endOffset": 238}, {"referenceID": 10, "context": "In recent years, neural-based knowledge representation has been proposed to encode both entities and relations into a low-dimensional space, which are capable to find novel facts (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 179, "endOffset": 238}, {"referenceID": 19, "context": ", 2014a) performs joint learning simply considering alignment between words and entities, and (Toutanova et al., 2015) extracts textual relations from plain texts using dependency parsing to enhance relation embeddings.", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": ", 2014a) and textual relations in (Toutanova et al., 2015)), or rely on complicated linguistic analysis (dependency parsing in (Toutanova et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 19, "context": ", 2015)), or rely on complicated linguistic analysis (dependency parsing in (Toutanova et al., 2015)) which may bring inevitable parsing errors.", "startOffset": 76, "endOffset": 100}, {"referenceID": 2, "context": ", 2013b), TransE (Bordes et al., 2013) regards the relation r in each (h, r, t) as a translation from h to t within the lowdimensional space, i.", "startOffset": 17, "endOffset": 38}, {"referenceID": 10, "context": ", 2014b) and TransR (Lin et al., 2015), which is not the focus of this paper and will be left as our future work.", "startOffset": 20, "endOffset": 38}, {"referenceID": 15, "context": "Many works aim to extract relational facts from large-scale text corpora (Mintz et al., 2009; Riedel et al., 2010).", "startOffset": 73, "endOffset": 114}, {"referenceID": 16, "context": "Many works aim to extract relational facts from large-scale text corpora (Mintz et al., 2009; Riedel et al., 2010).", "startOffset": 73, "endOffset": 114}, {"referenceID": 24, "context": "In recent years, deep neural models such as convolutional neural networks (CNN) have been proposed to encode semantics of sentences to identify relations between entities (Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 171, "endOffset": 215}, {"referenceID": 23, "context": "Many neural models such as recurrent neural networks (RNN) (Zhang and Wang, 2015) and longshort term memory networks (LSTM) (Xu et al., 2015) have also been explored for relation extraction.", "startOffset": 124, "endOffset": 141}, {"referenceID": 6, "context": "As reported in many previous works, deep neural network will benefit significantly if being initialized with pre-trained word embeddings (Erhan et al., 2010).", "startOffset": 137, "endOffset": 157}, {"referenceID": 2, "context": "We select TransE (Bordes et al., 2013) to learn representations of entities and relations from KGs.", "startOffset": 17, "endOffset": 38}, {"referenceID": 24, "context": "As shown in (Zeng et al., 2014), the textual relations can be learned with deep neural networks and encoded in the low-dimensional semantic space.", "startOffset": 12, "endOffset": 31}, {"referenceID": 24, "context": "We follow (Zeng et al., 2014) and apply convolutional neural networks (CNN) to model textual relations from text.", "startOffset": 10, "endOffset": 29}, {"referenceID": 4, "context": "CNN is an efficient neural model widely used in image processing, which has recently been verified to be also effective for many NLP tasks such as part-of-speech tagging, named entity recognition and semantic role labeling (Collobert et al., 2011).", "startOffset": 223, "endOffset": 247}, {"referenceID": 24, "context": "Word position embeddings (WPE) is originally proposed in (Zeng et al., 2014).", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "We select Freebase (Bollacker et al., 2008) as the knowledge graph for joint learning.", "startOffset": 19, "endOffset": 43}, {"referenceID": 2, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 3, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 10, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 2, "context": "Entity prediction has also been used for evaluation in (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 55, "endOffset": 114}, {"referenceID": 10, "context": "Entity prediction has also been used for evaluation in (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 55, "endOffset": 114}, {"referenceID": 2, "context": "We follow (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015) and use the proportion of correct entities in Top-10 ranked entities (Hits@10) as the evaluation metric.", "startOffset": 10, "endOffset": 69}, {"referenceID": 10, "context": "We follow (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015) and use the proportion of correct entities in Top-10 ranked entities (Hits@10) as the evaluation metric.", "startOffset": 10, "endOffset": 69}, {"referenceID": 2, "context": "As mentioned in (Bordes et al., 2013), a corrupted triple may also exist in knowledge graphs, which should not be considered as incorrect.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "Since the evaluation setting is identical, we simply report the results of TransE, TransH and TransR from (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015), where \u201cunif\u201d and \u201cbern\u201d are two settings to sample negative instances for learning.", "startOffset": 106, "endOffset": 165}, {"referenceID": 10, "context": "Since the evaluation setting is identical, we simply report the results of TransE, TransH and TransR from (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015), where \u201cunif\u201d and \u201cbern\u201d are two settings to sample negative instances for learning.", "startOffset": 106, "endOffset": 165}, {"referenceID": 15, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 16, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 7, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 18, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 22, "context": "We follow (Weston et al., 2013) to conduct evaluation.", "startOffset": 10, "endOffset": 31}], "year": 2016, "abstractText": "Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.", "creator": "LaTeX with hyperref package"}}}