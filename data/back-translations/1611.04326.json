{"id": "1611.04326", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "`Who would have thought of that!': A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection", "abstract": "This paper presents a simple topic model for detecting sarcasm, developed to the best of our knowledge and belief. Starting from the intuition that sarcastic tweets are likely to have a mixture of words of both emotion and tweets with literal (either positive or negative) moods, our hierarchical theme model reveals dominant themes and feelings at the topic level. Using a dataset of tweets labeled with hashtags, the model estimates the topic level and mood level distributions. Our evaluation shows that topics such as \"work,\" \"gun laws,\" \"\" weather \"are dominant themes of sarcasm. Our model is also able to detect the mixture of emotive words that exist in a text of a given sentimental label. Finally, we apply our model for predicting sarcasm in tweets. We surpass two previous papers based on statistical classifiers with specific characteristics, by about 25%.", "histories": [["v1", "Mon, 14 Nov 2016 10:40:44 GMT  (55kb,D)", "http://arxiv.org/abs/1611.04326v1", "This paper will be presented at ExPROM workshop at COLING 2016"], ["v2", "Tue, 22 Nov 2016 10:55:56 GMT  (55kb,D)", "http://arxiv.org/abs/1611.04326v2", "This version of the paper contains corrected changes, after the camera -ready submission. These changes were observed based on an issue in the output returned by SVM Perf. This paper will be presented at ExPROM workshop at COLING 2016"]], "COMMENTS": "This paper will be presented at ExPROM workshop at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aditya joshi", "prayas jain", "pushpak bhattacharyya", "mark carman"], "accepted": false, "id": "1611.04326"}, "pdf": {"name": "1611.04326.pdf", "metadata": {"source": "CRF", "title": "\u2018Who would have thought of that!\u2019: A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection", "authors": ["Aditya Joshi", "Prayas Jain", "Pushpak Bhattacharyya", "Mark James Carman"], "emails": ["pb}@cse.iitb.ac.in", "prayas.jain.cse14@iitbhu.ac.in", "mark.carman@monash.edu"], "sections": [{"heading": "1 Introduction", "text": "This paper will be presented at the 3rd ExPROM workshop at COLING 2016. http://www.cse. unt.edu/exprom2016/ Sarcasm detection is the computational task of predicting sarcasm in text. Past approaches in sarcasm detection rely on designing classifiers with specific features (to capture sentiment changes or incorporate context about the author, environment, etc.) (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al. (2016c). Approaches, in addition to this statistical classifier-based paradigm are: deep learning-based approaches as in the case of Silvio Amir et al. (2016) or rule-based approaches such as Riloff et al. (2013; Khattri et al. (2015).\nThis work employs a machine learning technique that, to the best of our knowledge, has not been used for computational sarcasm. Specifically, we introduce a topic model for extraction of sarcasm-prevalent topics and as a result, for sarcasm detection. Our model based on a supervised version of the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) is able to discover clusters of words that correspond to sarcastic topics. The goal of this work is to discover sarcasm-prevalent topics based on sentiment distribution within text, and use these topics to improve sarcasm detection. The key idea of the model is that (a) some topics are more likely to be sarcastic than others, and (b) sarcastic tweets are likely to have a different distribution of positive-negative words as compared to literal positive or negative tweets. Hence, distribution of sentiment in a tweet is the central component of our model.\nOur sarcasm topic model is learned on tweets that are labeled with three sentiment labels: literal positive, literal negative and sarcastic. In order to extract sarcasm-prevalent topics, the model uses three latent variables: a topic variable to indicate words that are prevalent in sarcastic discussions, a sentiment variable for sentiment-bearing words related to a topic, and a switch variable that switches between the two kinds of words (topic and sentiment-bearing words). Using a dataset of 166,955 tweets, our model is able to discover words corresponding to topics that are found in our corpus of positive, negative and sarcastic tweets.\nWe evaluate our model in two steps: a qualitative evaluation that ascertains sarcasm-prevalent topics based on the ones extracted, and a quantitative evaluation that evaluates sub-components of the model.\nar X\niv :1\n61 1.\n04 32\n6v 1\n[ cs\n.C L\n] 1\n4 N\nov 2\n01 6\nWe also demonstrate how it can be used for sarcasm detection. To do so, we compare our model with two prior work, and observe a significant improvement of around 25% in the F-score.\nThe rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 presents our motivation for using topic models for automatic sarcasm detection. Section 4 describes the design rationale and structure of our model. Section 5 describes the dataset and the experiment setup. Section 6 discusses the results in three steps: qualitative results, quantitative results and application of our topic model to sarcasm detection. Section 7 concludes the paper and points to future work."}, {"heading": "2 Related Work", "text": "Topic models are popular for sentiment aspect extraction. Jo and Oh (2011) present an aspect-sentiment unification model that learns different aspects of a product, and the words that are used to express sentiment towards the aspects. In terms of using two latent variables: one for aspect and one for sentiment, they are related to our model. Mukherjee and Liu (2012a) use a semi-supervised model in order to extract aspect-level sentiment. The role of the supervised sentiment label in our model is similar to their work. Finally, McAuley and Leskovec (2013a) attempt to generate rating dimensions of products using topic models. However, the topic models that have been reported in past work have been for sentiment analysis in general. They do not have any special consideration to either sarcasm as a label or sarcastic tweets as a special case of tweets. The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group.\nSarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence. The idea of distribution of sentiment that we use in our model is based on the idea of context incongruity. In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers using features such as unigrams, laughter expressions, hyperbolic expressions, etc. Liebrecht et al. (2013) experiment with unigrams, bigrams and trigrams as features. To the best of our knowledge, past approaches for sarcasm detection do not use topic modeling, which we do."}, {"heading": "3 Motivation", "text": "Topic models enable discovery of thematic structures in a large-sized corpus. The motivation behind using topic models for sarcasm detection arises from two reasons: (a) presence of sarcasm-prevalent topics, and (b) differences in sentiment distribution in sarcastic and non-sarcastic text. In context of sentiment analysis, topic models have been used for aspect-based sentiment analysis in order to discover topic and sentiment words (Jo and Oh, 2011). The general idea is that for a restaurant review, the word \u2018spicy\u2019 is more likely to describe food as against ambiance. On similar lines, the discovery that a set of words belong to a sarcasm-prevalent topic - a topic regarding which sarcastic remarks are common - can be useful as additional information to a sarcasm detection system. The key idea of our approach is that some topics are more likely to evoke sarcasm than some others. For example, a tweet about working late night at office/ doing school homework till late night is much more probable to be sarcastic than a tweet on Mother\u2019s Day. A sarcasm detection system can benefit from incorporating this information about sarcasm-prevalent topics. The second reason is the difference in sentiment distributions. A positive tweet is likely to contain only positive words, a negative tweet is likely to contain only negative words. On the other hand, a sarcastic tweet may contain a mix of the two kind of words (for example, \u2018I love being ignored\u2019 where \u2018love\u2019 is a positive word and \u2018ignored\u2019 is a negative word), except in the case of hyperbolic sarcasm (for example \u2018This is the best movie ever!\u2019 where \u2018best\u2019 is a positive word and there is no negative word). Hence, in addition to sarcasm-prevalent topics, sentiment distributions for tweets also form a critical component of our topic model."}, {"heading": "4 Sarcasm Topic Model", "text": ""}, {"heading": "4.1 Design Rationale", "text": "Our topic model requires sentiment labels of tweets, as used in Ramage et al. (2009). This sentiment can be positive or negative. However, in order to incorporate sarcasm, we re-organize the two sentiment values into three: literal positive, literal negative and sarcastic. The observed variable l in our model indicates this sentiment label. For sake of simplicity, we refer to the three values of l as positive, negative and sarcastic, in rest of the paper.\nEvery word w in a tweet is either a topic word or a sentiment word. A topic word arises due to a topic, whereas a sentiment word arises due to combination of topic and sentiment. This notion is common to several sentiment-based topic models from past work (Jo and Oh, 2011). To determine which of the two (topic or sentiment word) a given word is, our model uses three latent variables: a tweet-level topic label z, a word-level sentiment label s, and a switch variable is. Each tweet is assumed to have a single topic indicated by z. The single-topic assumption is reasonable considering the length of a tweet. At the word level, we introduce two variables is and s. For each word in the dictionary, is denotes the probability of the word being a topic word or a sentiment word. Thus, the model estimates three sets of distributions: (A) Probability of a word belonging to topic (\u03c6z) or sentiment-topic combination (\u03c7sz), (B) Sentiment distributions over label and topic (\u03c8zl), and (C) Topic distributions over label (\u03b8l). The switch variable is is sampled from \u03b7w, the probability of the word being a topic word or a sentiment word. We thus allow a word to be either a topic word or a sentiment word.1"}, {"heading": "4.2 Plate Diagram", "text": "Our sarcasm topic model to extract sarcasm-prevalent topics is based on supervised LDA (Blei et al., 2003). Figure 1 shows the plate diagram while Table 1 details the variables and distributions in the model. Every tweet consists of a set of observed words w and one tweet-level, observed sentiment label l. The label takes three values: positive, negative or sarcastic. The third label value \u2018sarcastic\u2019 indicates a scenario where a tweet appears positive on the surface but is implicitly negative (hence, sarcastic). z is a tweet-level latent variable, denoting the topic of the tweet. The number of topics, Z is experimentally determined. is is a word-level latent variable representing if a word is a topic word or a sentiment word,\n1Note that \u03b7w is not estimated during the sampling but learned from a large-scale corpus, as will be described later.\nsimilar to Mukherjee and Liu (2012c). If the word is a sentiment word, the word-level latent variable s represents the sentiment of that word. It can take S unique values. Intuitively, S is set as 2.\nAmong the distributions, \u03b7w is an observed distribution that is estimated beforehand. It denotes the probability of the word w being a topic word or a sentiment word. Distribution \u03b8l represents the distribution over z given the label of the tweet as l. \u03c8l and \u03c8zl are an hierarchical pair of distributions. \u03c8zl represents the distribution over sentiment of the word given the topic and label of the tweet and that the word is a sentiment word. \u03c7s and \u03c7sz are an hierarchical pair of distributions , where \u03c7sz represents distribution over words, given the word is a sentiment word with sentiment s and topic z. \u03c6z is a distribution over words given the word is an topic word with topic z. The generative story of our model is:\n1. For each label l, select ~\u03b8l\u223cDir(\u03b1)\n2. For each label l, select ~\u03c8l\u223cDir(\u03b21) For each topic z, select ~\u03c8l,z\u223cDir(\u03b22 ~\u03c8l) 3. For each topic z and sentiment s, select ~\u03c7s\u223cDir(\u03b41), and ~\u03c7s,z\u223cDir(\u03b42~\u03c7s)\n4. For each topic z select ~\u03c6z\u223cDir(\u03b3) 5. For each tweet k select\n(a) topic zk \u223c ~\u03b8lk (b) switch value for all words, iskj \u223c ~\u03b7j (c) sentiment for all sentiment words, skj \u223c ~\u03c8zk,lk (d) all topic words, wkj \u223c ~\u03c6zk (e) all sentiment words, wkj \u223c ~\u03c7skj ,zk\nWe estimate these distribution using Gibbs sampling. The joint probability over all variables is decomposed into these distributions, based on dependencies in the model. Estimation details have not been included due to lack of space."}, {"heading": "5 Experiment Setup", "text": "We create a dataset of English tweets for our topic model. We do not use datasets reported in past work (related to classifiers) because topic models typically require larger datasets than classifiers. The tweets are downloaded from twitter using the twitter API2 using hashtag-based supervision. Hashtag-based supervision is common in sarcasm-labeled datasets (Joshi et al., 2015). Tweets containing hashtags #happy, #excited are labeled as positive tweets. Tweets with #sad, #angry are labeled as negative tweets. Tweets with #sarcasm and #sarcastic are labeled as sarcastic tweets. The tweets are converted to lowercase, and the hashtags used for supervision are removed. Function words3, punctuation, hashtags, author names\n2https://dev.twitter.com/rest/public 3www.sequencepublishing.com\nand hyperlinks are removed from the tweets. Duplicate tweets (same tweet text repeated for multiple tweets) and re-tweets (tweet text with the \u2018RT\u2019 added in the beginning) are discarded. Finally, words which occur less than three times in the vocabulary are also removed. As a result, the tweets that have less than 3 words are removed. This results in a dataset of 166,955 tweets. Out of these, 70,934 are positive, 20,253 are negative and the remaining 75,769 are sarcastic. A total of 35398 tweets are used for testing, out of which 26,210 are of positive sentiment, 5535 are of negative sentiment and 3653 are sarcastic. We repeat that these labels are determined based on hashtags, as stated above.\nThe total number of distinct labels (L) is 3, and the total number of distinct sentiment (S) is 2. The total number of distinct topics (Z) is experimentally determined as 50. We use block-based Gibbs sampling to estimate the distributions. The block-based sampler samples all latent variables together based on their joint distributions. We set asymmetric priors based on sentiment word-list from McAuley and Leskovec (2013b).\nA key parameter of the model is \u03b7w since it drives the split of a word as a topic or a sentiment word. SentiWordNet (Baccianella et al., 2010) is used to learn the distribution \u03b7w prior to estimating the model. We average across multiple senses of a word. Based on the SentiWordNet scores to all senses of a word, we determine this probability."}, {"heading": "6.1 Qualitative Evaluation", "text": "The goal of this section is to present topics discovered by our sarcasm topic model. We do so in two steps. We first describe the topics generated when only sarcastic tweets from our corpus are used to estimate the distributions, followed by the ones when the full corpus is used. In case of the former, since only sarcastic tweets are used, the topics generated here indicate words corresponding to sarcasm-prevalent topics. In case of the latter, the sentiment-topic distributions in the model capture the prevalence of sarcasm.\nThe model estimates the \u03c6 and \u03c7 distributions corresponding to topic words and sentiment words. Top five words for a subset of topics (as estimated by \u03c6) are shown in Table 2. The headings in boldface are manually assigned4. Sarcasm-prevalent topics, as discovered by our topic model, are work, party, weather, women, etc. The corresponding sentiment topics for each of these sarcasm-prevalent topics (as estimated by \u03c7) are given in Table 3. The headings in boldface are manually assigned. For topics corresponding to \u2018party\u2019 and \u2018women\u2019, we observe that the two columns contain words from opposing sentiment polarities. An example sarcastic tweet about work is \u2018Yaay! Another night spent at office! I love working late night\u2019.\nThe previous set of topics are all from sarcastic text. We now show the topics extracted by our model from the full corpus. These topics will indicate peculiarity of topics for each of the three labels, allowing\n4This is a common practice in topics model papers, in order to interpret topics. (Mukherjee and Liu, 2012b; Joshi et al., 2016a; Kim et al., 2013)\nus to infer what topics are sarcasm-prevalent. Table 4 shows the top 5 topic words for the topics discovered (as estimated in \u03c6) from the full corpus (i.e., containing tweets of all three tweet-level sentiment labels: positive, negative and sarcastic). Table 5 shows the top 3 sentiment words for each sentiment (as estimated by \u03c7) of each of the topics discovered. Like in the previous case, the heading in boldface is manually assigned. One of the topic discovered was \u2018Music\u2019. The top 5 topic words for the topic \u2018Music\u2019 are Pop, Country, Rock, Bluegrass and Beatles. The corresponding sentiment words for Music are \u2018love\u2019, \u2018happy\u2019, \u2018good\u2019 on the positive side and \u2018sad\u2019, \u2018passion\u2019 and \u2018pain\u2019 on the negative side.\nThe remaining sections present results when the model is learned on the full corpus."}, {"heading": "6.2 Quantitative Evaluation", "text": "In this section, we answer three questions: (A) What is the likely sentiment label, if a user is talking about a particular topic? (Section 6.2.1), (B) We hypothesize that sarcastic text tends to have mixed-polarity\nwords. Does it hold in case of our model? (Section 6.2.2), and (C) How can sarcasm topic model be used for sarcasm detection? (Section 6.2.3)."}, {"heading": "6.2.1 Probability of sentiment label, given topic", "text": "We compute the probability p(l/z) based on the model. Table 6 shows these values for a subset of topics. Topics with a majority positive sentiment are Father\u2019s Day (0.9224), holidays (0.9538), etc. The topic with the highest probability of a negative sentiment is the Orlando shooting incident (0.95). Gun laws (0.5230), work and humor are where sarcasm is prevalent."}, {"heading": "6.2.2 Distribution of sentiment words for tweet-level sentiment labels", "text": "Figure 2 shows the proportion of word-level sentiment labels, for the three tweet-level sentiment labels, as estimated by our model. The X-axis indicates percentage of positive sentiment words in a tweet, while Y-axis indicates percentage of tweets which indicate a specific value of percentage. More than 60% negative tweets (bar in red) have 0% positive content words. The \u2018positive\u2019 here indicates the value of s for a word in a tweet. In other words, the said red bar indicates that 60% tweets have 0% words sampled with s as positive.\nIt follows intuition that negative tweets have low percentage of positive words (red bar on the left part of the graph) while positive tweets have high percentage of positive words (blue bar on the right part of the graph). The interesting variations are observed in case of sarcastic tweets. It must be highlighted that the sentiment labels considered for these proportions are as estimated by our topic model. Many sarcastic tweets contain very high percentage of positive sentiment words. Similarly, the proportion of tweets with around 50% positive sentiment words is around 20%, as expected. Thus, the model is able to capture the sentiment mixture as expected in the three tweet-level sentiment labels: (literal) positive, (literal) negative and sarcastic."}, {"heading": "6.2.3 Application to Sarcasm Detection", "text": "We now use our sarcasm topic model to detect sarcasm, and compare it with two prior work. The task here is to classify a tweets as either sarcastic or not. We use the topic model for sarcasm detection using two methods:\n1. Log-likelihood based: The topic model is first learned using the training corpus where the distributions in the model are estimated. Then, the topic model performs sampling for a pre-determined number of samples, in three runs - once for each label. For each run, the log-likelihood of the tweet given the estimated distributions (in the training phase) and the sampled values of the latent variables (for this tweet) is computed. The label of the tweet is returned as the one with the highest log-likelihood.\n2. Sampling-based: Like in the previous case, the topic model first estimates distributions using the training corpus. Then, the topic model is learned again where the label l is assumed to be latent, in addition to the tweet-level latent variable z, and word-level latent variables s, and is. The value of l as learned by the sampler is returned as the predicted label.\nWe compare our results with two previously existing techniques, Buschmeier et al. (2014) and Liebrecht et al. (2013). We ensure that our implementations result in performance comparable to the reported papers. The two rely on designing sarcasm-level features, and training classifiers for these features. For these classifiers, the positive and negative labels are combined as non-sarcastic. As stated above, the test set is separate from the training set. The results of these two past methods compared with the two based on topic models are shown in Table 7. The values are averaged over the two classes. Both prior work show poor F-score (around 18-19%) while our sampling based approach achieves the best F-score of 46.80%. The low values, in general, may be because our corpus is large in size, and is diverse in terms of the topics. Also, features in Liebrecht et al. (2013) are unigrams, bigrams and trigrams which may result in sparse features."}, {"heading": "7 Conclusion & Future Work", "text": "We presented a novel topic model that discovers sarcasm-prevalent topics. Our topic model uses a dataset of tweets (labeled as positive, negative and sarcastic), and estimates distributions corresponding to prevalence of a topic, prevalence of a sentiment-bearing words. We observed that topics such as work, weather, politics, etc. were discovered as sarcasm-prevalent topics. We evaluated the model in three steps: (a) Based on the distributions learned by our model, we show the most likely label, for all topics. This is to understand sarcasm-prevalence of topics when the model is learned on the full corpus. (b) We then show distribution of word-level sentiment for each tweet-level sentiment label as estimated by our model. Our intuition that sentiment distribution in a tweet is different for the three labels: positive, negative and sarcastic, holds true. (c) Finally, we show how topics from this topic model can be harnessed for sarcasm detection. We implement two approaches: one based on most likely label as per log likelihood, and another based on last sampled value during iteration. In both the cases, we are able to significantly outperform two prior work based on feature design by F-Score of around 25%.\nThe current model is limited because of its key intuition about sentiment mixture in sarcastic text. Instances such as hyperbolic sarcasm go against the intuition. The current approach relies only on bag of words which may be extended to n-grams since a lot of sarcasm is expressed through phrases with implied sentiment. This work, being an initial work in topic models for sarcasm, sets up the promise of topic models for sarcasm detection, as also demonstrated in corresponding work in aspect-based sentiment analysis."}], "references": [{"title": "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani"], "venue": "In LREC,", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "Contextualized sarcasm detection on twitter", "author": ["Bamman", "Smith2015] David Bamman", "Noah A Smith"], "venue": "In Ninth International AAAI Conference on Web and Social Media", "citeRegEx": "Bamman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Philipp Cimiano", "Roman Klinger"], "venue": "In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,", "citeRegEx": "Buschmeier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Jo", "Oh2011] Yohan Jo", "Alice H Oh"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Jo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jo et al\\.", "year": 2011}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Joshi et al.2015] Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Joshi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Political issue extraction model: A novel hierarchical topic model that uses tweets by political and non-political authors", "author": ["Joshi et al.2016a] Aditya Joshi", "Pushpak Bhattacharyya", "Mark Carman"], "venue": "In 7th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis,", "citeRegEx": "Joshi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Automatic sarcasm detection: A survey", "author": ["Joshi et al.2016b] Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "venue": "arXiv preprint arXiv:1602.03426", "citeRegEx": "Joshi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Harnessing sequence labeling for sarcasm detection in dialogue from tv series \u2018friends", "author": ["Joshi et al.2016c] Aditya Joshi", "Vaibhav Tripathi", "Pushpak Bhattacharyya", "Mark Carman"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Your sentiment precedes you: Using an author\u2019s historical tweets to predict sarcasm", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman"], "venue": "In 6TH WORKSHOP ON COMPUTATIONAL APPROACHES TO SUBJECTIVITY, SENTIMENT AND SOCIAL MEDIA ANALYSIS WASSA", "citeRegEx": "Khattri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khattri et al\\.", "year": 2015}, {"title": "A hierarchical aspect-sentiment model for online reviews", "author": ["Kim et al.2013] Suin Kim", "Jianwen Zhang", "Zheng Chen", "Alice H Oh", "Shixia Liu"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["McAuley", "Leskovec2013a] Julian McAuley", "Jure Leskovec"], "venue": "In Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews", "author": ["McAuley", "Jure Leskovec"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "Aspect extraction through semi-supervised modeling", "author": ["Mukherjee", "Liu2012a] Arjun Mukherjee", "Bing Liu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Mukherjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2012}, {"title": "Aspect extraction through semi-supervised modeling", "author": ["Mukherjee", "Liu2012b] Arjun Mukherjee", "Bing Liu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Mukherjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2012}, {"title": "Modeling review comments. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 320\u2013329", "author": ["Mukherjee", "Liu2012c] Arjun Mukherjee", "Bing Liu"], "venue": null, "citeRegEx": "Mukherjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2012}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Rajadesingan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["Ramage et al.2009] Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher D. Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1,", "citeRegEx": "Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Riloff et al.2013] Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang"], "venue": "In EMNLP,", "citeRegEx": "Riloff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "Modelling context with user embeddings for sarcasm detection in social media", "author": ["Wallace", "Hao Lyu", "Paula Carvalho M\u00e1rio J Silva"], "venue": null, "citeRegEx": "Amir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Do Kook Choe", "Laura Kertz", "Eugene Charniak"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Wallace et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}, {"title": "Twitter sarcasm detection exploiting a context-based model", "author": ["Wang et al.2015] Zelin Wang", "Zhijian Wu", "Ruimin Wang", "Yafeng Ren"], "venue": "In Web Information Systems Engineering\u2013WISE", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al.", "startOffset": 2, "endOffset": 95}, {"referenceID": 21, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al.", "startOffset": 2, "endOffset": 95}, {"referenceID": 17, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al.", "startOffset": 2, "endOffset": 95}, {"referenceID": 2, "context": "Our model based on a supervised version of the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) is able to discover clusters of words that correspond to sarcastic topics.", "startOffset": 87, "endOffset": 106}, {"referenceID": 4, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al. (2016c). Approaches, in addition to this statistical classifier-based paradigm are: deep learning-based approaches as in the case of Silvio Amir et al.", "startOffset": 3, "endOffset": 187}, {"referenceID": 4, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al. (2016c). Approaches, in addition to this statistical classifier-based paradigm are: deep learning-based approaches as in the case of Silvio Amir et al. (2016) or rule-based approaches such as Riloff et al.", "startOffset": 3, "endOffset": 338}, {"referenceID": 4, "context": ") (Joshi et al., 2015; Wallace et al., 2014; Rajadesingan et al., 2015; Bamman and Smith, 2015), or model conversations using the sequence labeling-based approach by Joshi et al. (2016c). Approaches, in addition to this statistical classifier-based paradigm are: deep learning-based approaches as in the case of Silvio Amir et al. (2016) or rule-based approaches such as Riloff et al. (2013; Khattri et al. (2015). This work employs a machine learning technique that, to the best of our knowledge, has not been used for computational sarcasm.", "startOffset": 3, "endOffset": 414}, {"referenceID": 11, "context": "Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015).", "startOffset": 65, "endOffset": 149}, {"referenceID": 22, "context": "Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015).", "startOffset": 65, "endOffset": 149}, {"referenceID": 5, "context": "Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015).", "startOffset": 65, "endOffset": 149}, {"referenceID": 4, "context": "The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets.", "startOffset": 118, "endOffset": 139}, {"referenceID": 4, "context": "The hierarchy-based structure (specifically, the chain of distributions for sentiment label) in our model is based on Joshi et al. (2016a) who extract politically relevant topics from a dataset of political tweets. The chain in their case is sentiment distribution of an individual and a group. Sarcasm detection approaches have also been reported in the past (Joshi et al., 2016b; Liebrecht et al., 2013; Wang et al., 2015; Joshi et al., 2015). Wang et al. (2015) present a contextual model for sarcasm detection that collectively models a set of tweets, using a sequence labeling algorithm - however, the goal is to detect sarcasm in the last tweet in the sequence.", "startOffset": 118, "endOffset": 465}, {"referenceID": 3, "context": "In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 3, "context": "In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al.", "startOffset": 162, "endOffset": 215}, {"referenceID": 3, "context": "In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers using features such as unigrams, laughter expressions, hyperbolic expressions, etc.", "startOffset": 162, "endOffset": 241}, {"referenceID": 3, "context": "In order to evaluate the benefit of our model to sarcasm detection, we compare two sarcasm detection approaches based on our model with two prior work, namely by Buschmeier et al. (2014) and Liebrecht et al. (2013). Buschmeier et al. (2014) train their classifiers using features such as unigrams, laughter expressions, hyperbolic expressions, etc. Liebrecht et al. (2013) experiment with unigrams, bigrams and trigrams as features.", "startOffset": 162, "endOffset": 373}, {"referenceID": 18, "context": "Our topic model requires sentiment labels of tweets, as used in Ramage et al. (2009). This sentiment can be positive or negative.", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "Our sarcasm topic model to extract sarcasm-prevalent topics is based on supervised LDA (Blei et al., 2003).", "startOffset": 87, "endOffset": 106}, {"referenceID": 5, "context": "Hashtag-based supervision is common in sarcasm-labeled datasets (Joshi et al., 2015).", "startOffset": 64, "endOffset": 84}, {"referenceID": 0, "context": "SentiWordNet (Baccianella et al., 2010) is used to learn the distribution \u03b7w prior to estimating the model.", "startOffset": 13, "endOffset": 39}, {"referenceID": 10, "context": "(Mukherjee and Liu, 2012b; Joshi et al., 2016a; Kim et al., 2013)", "startOffset": 0, "endOffset": 65}, {"referenceID": 3, "context": "We compare our results with two previously existing techniques, Buschmeier et al. (2014) and Liebrecht et al.", "startOffset": 64, "endOffset": 89}, {"referenceID": 3, "context": "We compare our results with two previously existing techniques, Buschmeier et al. (2014) and Liebrecht et al. (2013). We ensure that our implementations result in performance comparable to the reported papers.", "startOffset": 64, "endOffset": 117}, {"referenceID": 3, "context": "We compare our results with two previously existing techniques, Buschmeier et al. (2014) and Liebrecht et al. (2013). We ensure that our implementations result in performance comparable to the reported papers. The two rely on designing sarcasm-level features, and training classifiers for these features. For these classifiers, the positive and negative labels are combined as non-sarcastic. As stated above, the test set is separate from the training set. The results of these two past methods compared with the two based on topic models are shown in Table 7. The values are averaged over the two classes. Both prior work show poor F-score (around 18-19%) while our sampling based approach achieves the best F-score of 46.80%. The low values, in general, may be because our corpus is large in size, and is diverse in terms of the topics. Also, features in Liebrecht et al. (2013) are unigrams, bigrams and trigrams which may result in sparse features.", "startOffset": 64, "endOffset": 881}, {"referenceID": 3, "context": "(Buschmeier et al., 2014) 10.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "85 (Liebrecht et al., 2013) 11.", "startOffset": 3, "endOffset": 27}], "year": 2017, "abstractText": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as \u2018work\u2019, \u2018gun laws\u2019, \u2018weather\u2019 are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25%.", "creator": "LaTeX with hyperref package"}}}