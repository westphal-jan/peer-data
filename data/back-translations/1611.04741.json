{"id": "1611.04741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "abstract": "In this paper, we use recent advances in the representation process to propose a neural architecture for the problem of natural language reasoning. Our approach is aimed at mimicking a human's approach to natural language reasoning, taking into account two statements: the model uses variants of Long Short Term Memory (LSTM), attention mechanisms, and composable neural networks to perform the task. Each part of our model can be assigned to a clear functionality that humans do for performing the general task of natural language reasoning; the model is universally differentiated and enables training through stochastic gradient descent; and, based on the Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves better accuracy numbers than all models published in literature.", "histories": [["v1", "Tue, 15 Nov 2016 08:48:22 GMT  (165kb,D)", "http://arxiv.org/abs/1611.04741v1", "8 pages, 2 figures"], ["v2", "Fri, 27 Jan 2017 05:36:05 GMT  (167kb,D)", "http://arxiv.org/abs/1611.04741v2", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biswajit paria", "k m annervaz", "ambedkar dukkipati", "ankush chatterjee", "sanjay podder"], "accepted": false, "id": "1611.04741"}, "pdf": {"name": "1611.04741.pdf", "metadata": {"source": "CRF", "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "authors": ["Biswajit Paria", "Ambedkar Dukkipati", "Ankush Chatterjee", "Sanjay Podder"], "emails": ["biswajitsc@iitkgp.ac.in", "annervaz.km@csa.iisc.ernet.in", "ad@csa.iisc.ernet.in", "ankushchatterjee@iitkgp.ac.in", "sanjay.podder@accenture.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION AND MOTIVATION\nThe problem of Natural Language Inference (NLI) is to identify whether a statement (hypothesis: H) in natural language can be inferred or contradicted in the context of another statement (premise: P) in natural language. If it can neither be inferred nor contradicted, we say hypothesis is \u2018neutral\u2019 to premise. NLI is an important component for natural language understanding systems [Benthem, 2008; MacCartney and Manning, 2009]. NLI has multitude of applications including natural language question answering [Harabagiu and Hickl, 2006], semantic search, text summarization [Lacatusu et al., 2006] etc.\nConsider the three statements A: The couple is walking on the sea shore. B: The man and woman are wide awake. C: The man and woman are shopping on the busy street. Here the statement A is the premise and, B and C both are hypotheses. B can be inferred from A, where as it is reasonably clear C cannot be true if A were. A and C can be true together, in a strict sense, by arguing that there was a busy shopping option by the sea shore, which is not true generally. The problem of NLI thus falls in more \u201ccommon sense reasoning\u201d segment compared to strict logical inferencing and is subtly different from deduction in formal logical setting [MacCartney, 2009].\nUnsupervised feature learning and deep learning [Bengio, 2009; LeCun et al., 2015] based on neural networks have gained prominence in the last few years. State of the art neural networks models and appropriate algorithms to train these models have been proposed for multitude of tasks in computer vision, natural language processing, speech recognition etc. In the area of natural language processing, the recent deep\nlearning models have been proven superior to conventional rule-based or machine learning approaches in many tasks like part of speech tagging, question answering, sentiment analysis, document classification [Kumar et al., 2015] etc. Not only deep learning models hold the state of the art results for these problems, many model constructs used like attention mechanism have close alignment with human thought process. Motivated by the same, we dissect the problem of NLI into various sub tasks, similar to how human carries out NLI. We then realize each sub tasks using a deep learning construct, weave them together to create a complete end-to-end model for NLI.\nLet us first see how we can dissect the problem of NLI as humans do it. When seeing the two statements A and B as in the example above, humans first aligns information snippets between the sentences like (the couple, the man and woman) and (walking, wide awake). We notice that first pair is equivalent. From the second pair we conclude that walking is possible only in the state of being awake. From the results of these two different kinds of processing we conclude sentence B can be inferred from sentence A. Suppose in A if it were dog instead of couple, it would not have been equivalent, we could not have inferred B even though the second pair results are the same. Each pair results are important, some cases they are independent, but in most cases they are dependent as humans make use of a lot of contextual information. We analyze shopping on a street is not possible at sea shore and conclude C is contradicted by A. Note that for inferring B, we never paid attention to where the couple were walking, but to contradict C, we paid attention to the place. Humans first align the needed information according to the context, compare each pair differently by making use of the contextual information and then deduce finally by making use of each of the comparison results.\nThe main contributions of this paper are as follows. 1) A neural architecture using variants of long short term\nmemory, composable neural networks and attention mechanism is proposed for the problem of natural language inference. 2) The model is inspired from how humans carry out the task of natural language inference and hence very intuitive. Each step of the humans in performing NLI is\nar X\niv :1\n61 1.\n04 74\n1v 1\n[ cs\n.C L\n] 1\n5 N\nov 2\n01 6\nmimicked by an appropriate deep learning construct in the model. 3) We present detailed experimental results on Stanford Natural Language Inference(SNLI) Dataset [Bowman et al., 2015], and shows that proposed model outperforms all the other models"}, {"heading": "II. PRELIMINARIES AND BACKGROUND", "text": "In a deep learning framework, the natural language sentences are converted into a numerical representation by word embeddings, in the first place. This numerical representations are then encoded by using a bi-directional LSTM or a binary tree LSTM, to consider various information snippets along with the context in which they appear. Attention mechanism is used to learn the parts of the information that needs to be aligned and processed together according to the context. The generated pairs by attention mechanism are then processed separately using a set of different operators selected by soft gating. The outputs of the different process pairs are then aggregated or composed together for the final prediction task. Below we briefly describe concepts of word embeddings and LSTMs. Attention mechanism and composition, and their motivations are introduced along with the model."}, {"heading": "A. Word Embeddings", "text": "The first challenge encountered in applying deep learning models for NLP is to find a correct numerical representation for words. \u201cYou shall know a word by the company it keeps\u201d (Firth, J. R. 1957:11), is one of the most influential ideas in natural language processing. Multiple models for representing a word as a numerical vector, based on the context it appears, stem from this idea. Many vector representations for words have been proposed, including the well known latent semantic indexing [Dumais, 2004]. Vector representations for words in the context of neural networks was proposed by [Bengio et al., 2003]\nIn this paper, each word in the vocabulary is assigned a distributed word feature vector, w \u2208 Rm. The probability distribution of word sequences, P (wt|wt\u2212(n\u22121), . . . , wt\u22121), is then expressed in terms of these word feature vectors. The word feature vectors and parameters of the probability function (a neural network) are learned together by training a suitable feed-forward neural network to maximize the loglikelihood of the text corpora, considering each text snippet of fixed window size as a training sample. [Mikolov et al., 2013a] adapted this model and proposed two new models: Continuous bag of words and skip-gram model, popularly known as \u2018Word2Vec\u2019 models. Continuous bag of word models try to predict the current word given the previous and next surrounding words, discarding the word order, in a fixed context window. Skip-gram model tries to predict the surrounding words given the current word. These models have better training complexity, and thus can be used for training on large corpus. The vectors generated by these models on large corpus have shown to capture subtle semantic relationships between words, by simple vector operations on them [Mikolov\net al., 2013b]. The drawback of these models is that they mostly use local information (words in a contextual window). To effectively utilize the aggregated global information from the corpus without incurring high computational cost, \u2018GloVe\u2019 word vectors were proposed by [Pennington et al., 2014]. This model tries to create word vectors such that dot product of two vectors will closely resemble the co-occurrence statistics of the corresponding words in the full corpus. The model have shown to be more effective compared to Word2Vec models for capturing semantic regularities on smaller corpus."}, {"heading": "B. Recurrent Neural Network and Long Short Term Memory(LSTM)", "text": "The basic idea behind Recurrent Neural Networks (RNN) is to capture and encode the information present in a given sequence like text. Given a sequence of words, a numerical representation (GloVe or Word2Vec vectors) for a word is fed to a neural network and the output is computed. While computing the output for the next word, the output from the previous word (or time step) is also considered. RNNs are called recurrent because they perform the same computation for every element of a sequence using the output from previous computations. At any step RNN performs the following computation,\nRNN(ti) = f(W \u2217 xti + U \u2217 RNN(ti\u22121)),\nwhere W and U are the trainable parameters of the model, and f is a nonlinear function. The bias terms are left out here and have to be added appropriately. RNN(ti) is the output at ith timestep, which can either be utilized as is, or can be fed again to a parameterized construct such as softmax [Bishop, 2006], depending on the task at hand. The training is done by formulating a loss objective function based on the outputs at all timesteps, and trying to minimize the loss. The vanilla RNNs explained above have difficulty in learning long term dependencies in the sequence via gradient descent training [Bengio et al., 1994]. Also training vanilla RNNs is shown to be difficult because of vanishing and exploding gradient problems [Pascanu et al., 2013]. Long short term memory (LSTM) [Hochreiter and Schmidhuber, 1997], a variant of RNN is shown to be effective in capturing long-term dependencies and easier to train compared to vanilla RNNs. Multiple variants of LSTMs have been proposed in literature. One can refer to [Greff et al., 2015] for a comprehensive survey of LSTM variants.\nA LSTM module has three parameterized gates, input gate (i), forget gate (f ) and output gate (o). A gate g operates by\ngti = \u03c3(W g \u2217 xti + Ug \u2217 hti\u22121),\nwhere W g and Ug are the parameters of the gate g, ht\u22121 is the hidden state at the previous time step and \u03c3 stands for the sigmoid function. All the three gates have the same equation form and inputs, but they have different set of parameters. Along with hidden state, LSTM module also has a cell state. The updation of the hidden state and cell state at anytime step are controlled by the various gates as follows,\nCti = fti \u2217 Cti\u22121 + iti \u2217 tanh(WC \u2217 xti + UC \u2217 hti\u22121)\nand hti = oti \u2217 tanh(Cti), (1)\nwhere WC and UC are again parameters of the model. The key component of the LSTM is the cell state. The LSTM has the capability to modify and retain the content on the cell state as required by the task, using the gates and hidden states. While forward LSTM takes the input sequence as it is, a backward LSTM takes the input in the reverse order. A backward LSTM is used to capture the dependencies of a word on future words in the original sequence. A concatenation of a forward LSTM and a backward LSTM is known as bi-directional LSTM (biLSTM) [Greff et al., 2015].\n1) Binary Tree Long Short Term Memory: The LSTM or biLSTM model process the information in a sequential manner, as a linear chain. But a natural language sentence have more syntactic structure to it, and the information is represented more as a tree structure than a linear chain. To incorporate this way of processing information, tree structured LSTMs were introduced by Tai et al. 2015. In a tree structured LSTM (TreeLSTM) each node will have multiple previous time steps, one each corresponding to a child in the tree structure for the node, compared to a single previous time step of a linear chain. Different set of parameters for each child is included for the input and output gates to learn how different child information have to be processed. Using separate parameters child information is summed up to form the input and output gate values of every node, as follows:\ngti = \u03c3 W g \u2217 xti + \u2211 l\u2208child(i) Ugtl \u2217 htl  . Multiple forget gates (one for each child) are included to learn the information from each child that needs to be remembered. Forget gate update for each k \u2208 child(i) is,\nfti,tk = \u03c3 W f \u2217 xti + \u2211 l\u2208child(i) Uftk,tl \u2217 htl  . Then cell state is updated based on the forget gate values and cell state of the children is below.\nCti = \u2211\nl\u2208child(i)\nfti,tl \u2217 Ctl+\niti \u2217 tanh WC \u2217 xti + \u2211 l\u2208child(i) UCtl \u2217 htl  . Hidden states are then computed similar to normal LSTM as given in (1). The bias terms are left out in all the equations and have to be added appropriately wherever needed. All W s and Us in the above equations are model parameters, to be learned.\nThe tree structure can be formed by considering the syntactic parse of the sentence, leading to different variations of Tree LSTM [Tai et al., 2015]. If we consider the syntactic structure, each sample in the training data creates different tree structures, leading to difficulty in training the model efficiently. To work around this we considered complete binary tree structures, formed by pairing adjacent words recursively. We call this btree-LSTM in the subsequent discussion."}, {"heading": "III. THE PROPOSED MODEL", "text": "The model first encodes the sentences using a normal biLSTM or a btree-LSTM. This is to consider the different segments of the sentence along with the context, which is an essential part of human processing as explained earlier. In case of bi-LSTM, the encodings are augmented along with the corresponding word vectors to create enhanced encodings. In the case of btree-LSTM encodings this enhancement is not done since, there is no one-to-one correspondence with the number of words in the sentence after the encoding. If the bi-LSTM encodings are done there will be n encodings for a n-length sentence, where as if btree-LSTM encodings are done, there will be 2n\u2212 1 encodings. btree-LSTMs considers more possible phrasal structures(along with the context) of the input sentence compared to a bi-LSTM, as shown below.\n(v1, \u00b7 \u00b7 \u00b7 , vn)\u2190 bi\u2212 LSTM(S),\n(v1, \u00b7 \u00b7 \u00b7 , v2n\u22121)\u2190 btree\u2212 LSTM(S),\nand Se \u2190 (s1, v1, \u00b7 \u00b7 \u00b7 , sn, vn). (2)\nThe phrase encodings (vi or vi, si, i = 1, . . . , n) in (2) represents the various information snippets in the sentence S along with the context in which they appear. We do this encodings for both the sentences, hypothesis H and premise P . Next phase is to align the information snippets between the hypothesis and premise, as humans do, for which one can incorporate neural attention."}, {"heading": "A. Attention Mechanism", "text": "Attention mechanism was introduced in the context of machine translation recently [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language has to be mapped or aligned to words or phrases in another language for the purpose of translating. We use similar concept to learn this alignment for our purpose of NLI. Given two sets of vectors, a = {a1, .., an} and b = {b1, .., bn}, the attention value (a numerical quantity) vij is associated for each element of the first set ai to each element of the second set bj . Forall ai \u2208 a, attend((b1, \u00b7 \u00b7 \u00b7 , bn), ai) = (vi1, \u00b7 \u00b7 \u00b7 , vin), where,\nvij = (bj) Tai\u2211 r(br) Tai\nOne can see that for all i, \u2211\nj vij = 1. After learning, attention values will be high for elements that are mapped and low for other elements. For example with bi-LSTM or btree-LSTM encoding corresponding to the man and woman\nwill have high attention value to the encoding corresponding to the couple, and low attention values for other snippets, in the context of the given sentences. Given an element we can generate the attention values and sum up the elements of the second set, using the attention values as weights, to create a representation of the information that element is interested in or aligned with in the second set. As the attention values are high only for aligned encodings the summed up vector from the second set will be dominated by the aligned information.\nThe phrase encodings of hypothesis H are aligned with the phrase encodings of the premise P using an attention mode as given in (3). The result of the alignment is computed using a weighted sum of the phrase encodings of the premise P , using attention values as weights.\nForall Hep \u2208 He, attend(Pe,Hep) = (a1, \u00b7 \u00b7 \u00b7 , an),\nwhere\nai = Pei THep\u2211 j Pe jTHep\nand tp = \u2211 i ai \u00b7 Pei (3)\nNow that information snippets are aligned, pairs of (tp, Hep), they need to be processed. Different operators have to be applied based on the pairs and context. All the individual results have then to be aggregated to make the final decision. We use neural network composition for this purpose."}, {"heading": "B. Task Composition", "text": "Often a large task can be solved by composing the results of various different sub tasks, each computed separately. Such an approach for Question Answering was introduced by [Andreas et al., 2016]. We adapt this approach for our purpose here. After learning the alignment of encodings, we need to perform different functions or comparisons, depending on the kind of inputs and the sentence context, to see whether they contribute positively or negatively towards final prediction. In our example after aligning the encoding corresponding to the man and woman with the encoding for the couple the model has to process whether they are equivalent. Similarly after aligning walking to wide awake the model has to do a different kind of processing to verify that wide awake is followed from walking. Again if in an example, all birds are aligned with canary, model might have to check for a type of or subset of relationship. Depending on the type of input, the context of the sentence, different functions(operators or tasks) have to be applied. The operators also has to learn what it is supposed to do. Towards this purpose we introduced k number of operators, each is a two layer feed forward neural network, with different set of parameters. If a and b are the aligned encodings corresponding to two different text snippets, they are passed through k different two layer feed forward neural\nnetwork, the outputs of each weighed according to a soft gating function as\n(g1, \u00b7 \u00b7 \u00b7 , gk) = softmax(WT [a, b]) O = \u2211 i gi \u00b7 \u03c3 ( (W i2) T \u2217 \u03c3((W i1)T [a, b] ) .\nwhere W s are model parameters. The soft gating function helps to chose which operator has to be chosen to be applied, based on their types and context in the sentence in which they appear. Recall from the example different operators have to be applied to compare (the couple, the man and woman) and (walking,wide awake). This is realized by soft gating function.\nExpression for (tp,Hep) pairs from (3) are given in (4). A schematic diagram of this module is given in Figure 1.\ntaski(tp,Hep) = \u03c3 ( (W i2) T \u2217 \u03c3((W i1)T [tp,He p] )\n(g1, \u00b7 \u00b7 \u00b7 , gk) = softmax(WT [tp,Hep]) Op = \u2211 i gi \u00b7 taski(tp,Hep) (4)\nEach O denotes a certain output for an input encoding pair. Different pairs yields different Os. All the Os has to be aggregated(composed) towards the output for the final prediction. In our example after understanding the man and woman and the couple are equivalent and wide awake follows from walking, the model will have two O vectors one for each pair. Both\nthe Os have to be considered in making the final judgement. How to aggregate the various O s have to be learned by the model. There are two parts to it. One is the order in which they have to be aggregated, if there are more than two. Each ordering will give a different tree structured computation. The second being what exactly means aggregation. In the example the aggregation is an \u2019and\u2019 operator, both has to be satisfied. In another example it could be \u2019or\u2019 etc. Ideally for this, we should bring in a reinforcement learning mechanism similar to the one that is used in [Andreas et al., 2016], to learn the order of aggregation and a neural network [Socher et al., 2011] for the learning the aggregation operator. In our current model(for which results are discussed), we aggregate the operator outputs O by using a normal LSTM. The aggregation order learning which maps to tree structured computation is envisioned as a part of future work.\nThe aggregated result A is then passed through a comparison layer to do the final prediction, which is shown below,\nA = lstm(O1, \u00b7 \u00b7 \u00b7 ,On),\nlabel = softmax(WTA)),\nand loss = H(labelgold, label), (5)\nwhere W is the model parameter and H(p, q) denotes the cross-entropy between p and q. We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent [Bottou, 2012]. A schematic diagram of the complete model is given in Figure 2."}, {"heading": "C. Relevant Previous Work", "text": "NLI is a well studied problem with a rich literature using classical machine learning techniques. With the advent of deep learning, many models including LSTMs were used for NLI. Recently Stanford Natural Language Inference(SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing. Many deep learning models have been benchmarked on this dataset for NLI. Detail list is available at http://nlp.stanford. edu/projects/snli/. This recent thesis Bowman [2016] covers deep learning based works in detail.\nMany of deep learning based works relied on creating encodings of the sentences using LSTMs or convolutional neural networks or gated recurrent units or variants of recursive neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work. Bowman et al. (2016) also introduced an efficient mechanism to learn the binary parse of the tree along with creating encodings for the prediction task. Works in Rockta\u0308schel et al. [2015]; Wang and Jiang [2015] used neural attention mechanism along with LSTMs for the problem of NLI.\nThere are 3 main works in the space, which claims state of the art results. 1. To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for\nNatural Language Inference. 2. Munkhdalai and Yu (2016) introduced Neural Tree Indexers (NTI), by bringing in attention over tree structures of the sentences. 3. Parikh et al. [2016] is another very recent work, which uses the attention mechanism over words, compare them and then aggregate the results. As explained earlier we considers attention over possible sentence segment encodings(considering context), subtask division, operator selection and learning, and aggregation learning. Our model is aligned with human thought process and hence very intuitive and achieves state of the art results."}, {"heading": "IV. EXPERIMENTS AND EVALUATION", "text": "The model was implemented in TensorFlow [Abadi et al., 2015] - an open-source library for numerical computation for Deep Learning. All experiments were carried on a Dell Precision Tower 7910 server with Nvidia Titan X GPU. The models were trained using the Adam\u2019s Optimizer [Kingma and Ba, 2014] in a stochastic gradient descent [Bottou, 2012] fashion. We used batch normalization [Ioffe and Szegedy, 2015] while training. The various model parameters used are mentioned in Table I.\nWe experimented with both GloVe vectors trained1 on Common Crawl dataset as well as Word2Vec vector trained2 on Google news dataset. We used Google News trained word2vec word embeddings for the final reported results. Before matching a word in the dataset with a word in the word2vec collection, we converted all characters to lower case. The word embeddings are not trained along with the model. However before using them in our model, we transformed the embeddings using a learnable single layer neural network(\u03c3(WT \u00b7 w), where W is the model parameter and w is the word embeddings). For out of vocabulary words, we assigned them random word vectors. Each element of the word vector is randomly sampled from N (0, 0.06). This decision has been taken after observing that the word2vec vector elements are approximately distributed according to the above normal distribution.\nThere are two main datasets available in the public domain for NLI. Sentences Involving Compositional Knowledge (SICK) [Marelli et al., 2014b] dataset from the SemEval2014 task [Marelli et al., 2014a] which involves, predicting the degree of relatedness between two sentences, detecting the entailment relation holding between them. SICK consists\n1http://nlp.stanford.edu/data/glove.840B.300d.zip 2https://code.google.com/archive/p/word2vec/\nof 10000 sentence pairs manually labelled for relatedness and entailment. We have experimented with this dataset and have got very good results. The dataset being small and the model having large number of parameters, overfitting could have happened. As benchmark is not available for other state of the art models for comparison on SICK we are not including our results on this dataset. The Stanford Natural Language Inference Corpus(SNLI) [Bowman et al., 2015] dataset contains 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference. We are presenting our results on this dataset in comparison with other state of the art models.\nThe comparison results of various models on SNLI dataset is given in Table II. One can see that our model with bi-lstm encodings have better accuracy numbers compared to all published results, but fall short very close to the results reported in not yet published works [Munkhdalai and Yu, 2016; Parikh et al., 2016]. The model with btree-lstm encodings have better accuracy numbers than all the models. The class level accuracy results of various models on SNLI dataset is given in Table III."}, {"heading": "V. CONCLUSION & FUTURE WORK", "text": "We presented a complete deep learning model for the problem of natural language inference. The model used deep learning constructs like LSTM variants, attention mechanism and composable neural networks to mimic humans for natural language inference. The model is end-to-end differentiable,\nenabling training by simple stochastic gradient descent. From the initial experiments, the model have better accuracy numbers than all the published models. The model is interpretable in close alignment with human process while performing NLI, unlike other complicated deep learning models. We hope further experiments and hyper parameter tuning will improve these results further.\nThere are different enhancements for the model possible and potential future work directions. The btree-LSTM currently uses a complete binary tree structure formed by considering neighbouring encodings recursively. A binary tree learning scheme, similar to [Bowman et al., 2016] can be considered to be incorporated in the model. Tree construction based on the ordering of attention values will lead to heap like structures. We are currently working on this model which we have named Heap-LSTM. Currently the model uses soft gating for operator selection, hard selection with appropriate learning mechanism is something that has to be explored. The model aggregates the operator outputs using a simple LSTM, the aggregation tree structure learning using appropriate learning mechanism similar to [Andreas et al., 2016] is another major stream of work.\nThe alignment of the model with human thought process, already better accuracy numbers than all published models just from initial experiments, all advocate the exploration of model enhancements in these directions."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A brief history of natural logic", "author": ["Johan van Benthem"], "venue": "College Publications,", "citeRegEx": "Benthem.,? \\Q2008\\E", "shortCiteRegEx": "Benthem.", "year": 2008}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Stochastic Gradient Tricks, volume 7700, page 430445", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts"], "venue": "arXiv preprint arXiv:1603.06021,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Modeling Natural Language Semantics in Learned Representations", "author": ["Samuel Ryan Bowman"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Bowman.,? \\Q2016\\E", "shortCiteRegEx": "Bowman.", "year": 2016}, {"title": "Long shortterm memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Latent semantic analysis", "author": ["Susan T Dumais"], "venue": "Annual review of information science and technology,", "citeRegEx": "Dumais.,? \\Q2004\\E", "shortCiteRegEx": "Dumais.", "year": 2004}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "English et al\\.,? \\Q2015\\E", "shortCiteRegEx": "English et al\\.", "year": 2015}, {"title": "Lccs gistexter at duc 2006: Multi-strategy multi-document summarization", "author": ["Finley Lacatusu", "Andrew Hickl", "Kirk Roberts", "Ying Shi", "Jeremy Bensley", "Bryan Rink", "Patrick Wang", "Lara Taylor"], "venue": "In Proceedings of DUC06,", "citeRegEx": "Lacatusu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lacatusu et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Natural language inference", "author": ["Bill MacCartney"], "venue": "PhD thesis,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "An extended model of natural logic. In Proceedings of the eighth international conference on computational semantics, pages 140\u2013156", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": "Association for Computational Linguistics,", "citeRegEx": "MacCartney and Manning.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In LREC,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 1933}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "Vendrov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Learning natural language inference with long short-term memory-networks", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of NAACL.,", "citeRegEx": "Wang and Jiang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "NLI is an important component for natural language understanding systems [Benthem, 2008; MacCartney and Manning, 2009].", "startOffset": 73, "endOffset": 118}, {"referenceID": 20, "context": "NLI is an important component for natural language understanding systems [Benthem, 2008; MacCartney and Manning, 2009].", "startOffset": 73, "endOffset": 118}, {"referenceID": 17, "context": "NLI has multitude of applications including natural language question answering [Harabagiu and Hickl, 2006], semantic search, text summarization [Lacatusu et al., 2006] etc.", "startOffset": 145, "endOffset": 168}, {"referenceID": 19, "context": "compared to strict logical inferencing and is subtly different from deduction in formal logical setting [MacCartney, 2009].", "startOffset": 104, "endOffset": 122}, {"referenceID": 7, "context": "3) We present detailed experimental results on Stanford Natural Language Inference(SNLI) Dataset [Bowman et al., 2015], and shows that proposed model outperforms all the other models", "startOffset": 97, "endOffset": 118}, {"referenceID": 11, "context": "Many vector representations for words have been proposed, including the well known latent semantic indexing [Dumais, 2004].", "startOffset": 108, "endOffset": 122}, {"referenceID": 3, "context": "Vector representations for words in the context of neural networks was proposed by [Bengio et al., 2003]", "startOffset": 83, "endOffset": 104}, {"referenceID": 29, "context": "To effectively utilize the aggregated global information from the corpus without incurring high computational cost, \u2018GloVe\u2019 word vectors were proposed by [Pennington et al., 2014].", "startOffset": 154, "endOffset": 179}, {"referenceID": 5, "context": "RNN(ti) is the output at i timestep, which can either be utilized as is, or can be fed again to a parameterized construct such as softmax [Bishop, 2006], depending on the task at hand.", "startOffset": 138, "endOffset": 152}, {"referenceID": 2, "context": "The vanilla RNNs explained above have difficulty in learning long term dependencies in the sequence via gradient descent training [Bengio et al., 1994].", "startOffset": 130, "endOffset": 151}, {"referenceID": 28, "context": "Also training vanilla RNNs is shown to be difficult because of vanishing and exploding gradient problems [Pascanu et al., 2013].", "startOffset": 105, "endOffset": 127}, {"referenceID": 13, "context": "Long short term memory (LSTM) [Hochreiter and Schmidhuber, 1997], a variant of RNN is shown to be effective in capturing long-term dependencies and easier to train compared to vanilla RNNs.", "startOffset": 30, "endOffset": 64}, {"referenceID": 12, "context": "One can refer to [Greff et al., 2015] for a comprehensive survey of LSTM variants.", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "A concatenation of a forward LSTM and a backward LSTM is known as bi-directional LSTM (biLSTM) [Greff et al., 2015].", "startOffset": 95, "endOffset": 115}, {"referenceID": 32, "context": "The tree structure can be formed by considering the syntactic parse of the sentence, leading to different variations of Tree LSTM [Tai et al., 2015].", "startOffset": 130, "endOffset": 148}, {"referenceID": 1, "context": "Attention mechanism was introduced in the context of machine translation recently [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language has to be mapped or aligned to words or phrases in another", "startOffset": 82, "endOffset": 125}, {"referenceID": 18, "context": "Attention mechanism was introduced in the context of machine translation recently [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language has to be mapped or aligned to words or phrases in another", "startOffset": 82, "endOffset": 125}, {"referenceID": 0, "context": "Such an approach for Question Answering was introduced by [Andreas et al., 2016].", "startOffset": 58, "endOffset": 80}, {"referenceID": 0, "context": "Ideally for this, we should bring in a reinforcement learning mechanism similar to the one that is used in [Andreas et al., 2016], to learn the order of aggregation and a neural network [Socher et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 31, "context": ", 2016], to learn the order of aggregation and a neural network [Socher et al., 2011] for the learning the aggregation operator.", "startOffset": 64, "endOffset": 85}, {"referenceID": 6, "context": "We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent [Bottou, 2012].", "startOffset": 132, "endOffset": 146}, {"referenceID": 7, "context": "Recently Stanford Natural Language Inference(SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing.", "startOffset": 71, "endOffset": 92}, {"referenceID": 7, "context": "Recently Stanford Natural Language Inference(SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing. Many deep learning models have been benchmarked on this dataset for NLI. Detail list is available at http://nlp.stanford. edu/projects/snli/. This recent thesis Bowman [2016] covers deep learning based works in detail.", "startOffset": 72, "endOffset": 290}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work.", "startOffset": 78, "endOffset": 141}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work. Bowman et al. (2016) also introduced an efficient mechanism to learn the binary parse of the tree along with creating encodings for the prediction task.", "startOffset": 78, "endOffset": 191}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work. Bowman et al. (2016) also introduced an efficient mechanism to learn the binary parse of the tree along with creating encodings for the prediction task. Works in Rockt\u00e4schel et al. [2015]; Wang", "startOffset": 78, "endOffset": 358}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference.", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference. 2. Munkhdalai and Yu (2016) introduced Neural Tree Indexers (NTI), by bringing in attention over tree structures of the sentences.", "startOffset": 82, "endOffset": 212}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference. 2. Munkhdalai and Yu (2016) introduced Neural Tree Indexers (NTI), by bringing in attention over tree structures of the sentences. 3. Parikh et al. [2016] is another very recent work, which uses the attention mechanism over words, compare them and then aggregate the results.", "startOffset": 82, "endOffset": 339}, {"referenceID": 15, "context": "The models were trained using the Adam\u2019s Optimizer [Kingma and Ba, 2014] in a stochastic gradient descent [Bottou, 2012] fashion.", "startOffset": 51, "endOffset": 72}, {"referenceID": 6, "context": "The models were trained using the Adam\u2019s Optimizer [Kingma and Ba, 2014] in a stochastic gradient descent [Bottou, 2012] fashion.", "startOffset": 106, "endOffset": 120}, {"referenceID": 14, "context": "We used batch normalization [Ioffe and Szegedy, 2015] while training.", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "The Stanford Natural Language Inference Corpus(SNLI) [Bowman et al., 2015] dataset contains 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference.", "startOffset": 53, "endOffset": 74}, {"referenceID": 26, "context": "lished results, but fall short very close to the results reported in not yet published works [Munkhdalai and Yu, 2016; Parikh et al., 2016].", "startOffset": 93, "endOffset": 139}, {"referenceID": 8, "context": "A binary tree learning scheme, similar to [Bowman et al., 2016] can be considered to be incorporated in the model.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "the operator outputs using a simple LSTM, the aggregation tree structure learning using appropriate learning mechanism similar to [Andreas et al., 2016] is another major stream of work.", "startOffset": 130, "endOffset": 152}, {"referenceID": 7, "context": "Classifier(hand crafted features) [Bowman et al., 2015] 99.", "startOffset": 34, "endOffset": 55}, {"referenceID": 33, "context": "2 GRU encoders [Vendrov et al., 2015] 98.", "startOffset": 15, "endOffset": 37}, {"referenceID": 25, "context": "0M Tree-based CNN encoders [Mou et al., 2016] 83.", "startOffset": 27, "endOffset": 45}, {"referenceID": 8, "context": "5M SPINN-NP encoders [Bowman et al., 2016] 89.", "startOffset": 21, "endOffset": 42}, {"referenceID": 30, "context": "7M LSTM with attention [Rockt\u00e4schel et al., 2015] 85.", "startOffset": 23, "endOffset": 49}, {"referenceID": 34, "context": "5 252K mLSTM [Wang and Jiang, 2015] 92.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "9M Non Peer Reviewed Works LSTM Networks [Cheng et al., 2016] 88.", "startOffset": 41, "endOffset": 61}, {"referenceID": 26, "context": "8 582K NTI with global attention [Munkhdalai and Yu, 2016] 88.", "startOffset": 33, "endOffset": 58}, {"referenceID": 8, "context": "SPINN-NP encoders [Bowman et al., 2016] 80.", "startOffset": 18, "endOffset": 39}, {"referenceID": 34, "context": "5 mLSTM [Wang and Jiang, 2015] 81.", "startOffset": 8, "endOffset": 30}], "year": 2017, "abstractText": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.", "creator": "LaTeX with hyperref package"}}}