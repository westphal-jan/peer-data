{"id": "1611.05118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives", "abstract": "In comics, most movements in time and space are hidden in the \"gutters\" between the panels. In order to follow the story, readers connect the panels logically by inferring invisible actions through a process called \"shutter.\" While computers can now describe the content of natural images, in this essay we examine whether they understand the closure-oriented narratives conveyed by stylized artwork and dialogue in comic boards. We collect a data set, COMICS, which consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis by COMICS shows that neither text nor image alone can tell a comic story, so that a computer needs to understand both modalities in order to keep up with the action. We present three cloze-style tasks that provide models for predicting narrative and character-centric aspects of a panel as context.", "histories": [["v1", "Wed, 16 Nov 2016 02:16:09 GMT  (9072kb,D)", "http://arxiv.org/abs/1611.05118v1", null], ["v2", "Sun, 7 May 2017 20:26:24 GMT  (9230kb,D)", "http://arxiv.org/abs/1611.05118v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["mohit iyyer", "varun manjunatha", "anupam guha", "yogarshi vyas", "jordan boyd-graber", "hal daum\\'e iii", "larry davis"], "accepted": false, "id": "1611.05118"}, "pdf": {"name": "1611.05118.pdf", "metadata": {"source": "CRF", "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives", "authors": ["Mohit Iyyer", "Varun Manjunatha", "Anupam Guha", "Yogarshi Vyas", "Jordan Boyd-Graber", "Hal Daum\u00e9 III", "Larry Davis"], "emails": ["miyyer@umiacs.umd.edu", "varunm@umiacs.umd.edu", "aguha@umiacs.umd.edu", "yogarshi@umiacs.umd.edu", "hal@umiacs.umd.edu", "lsd@umiacs.umd.edu", "jordan.boyd.graber@colorado.edu"], "sections": [{"heading": "1. Introduction", "text": "Comics are fragmented scenes forged into full-fledged stories by the imagination of their readers. A comics creator can condense anything from a centuries-long intergalactic war to an ordinary family dinner into a single panel. But it is what the creator hides from their pages that makes comics truly interesting, the unspoken conversations and unseen actions that lurk in the spaces (or gutters) between adjacent panels. For example, the dialogue in Figure 1 suggests that between the second and third panels, Gilda commands her snakes to chase after a frightened Michael in some\n\u2217 Authors contributed equally\nsort of strange cult initiation. Through a process called closure [40], which involves (1) understanding individual panels and (2) making connective inferences across panels, readers form coherent storylines from seemingly disparate panels such as these. In this paper, we study whether computers can do the same by collecting a dataset of comic books (COMICS) and designing several tasks that require closure to solve.\nSection 2 describes how we create COMICS,1 which contains \u223c1.2 million panels drawn from almost 4,000 publicly-available comic books published during the \u201cGolden Age\u201d of American comics (1938\u20131954). COMICS is challenging in both style and content compared to natural images (e.g., photographs), which are the focus of most existing datasets and methods [32, 56, 55]. Much like painters, comic artists can render a single object or concept in multiple artistic styles to evoke different emotional responses from the reader. For example, the lions in Figure 2 are drawn with varying degrees of realism: the more cartoon-\n1Available at http://github.com/miyyer/comics\n1\nar X\niv :1\n61 1.\n05 11\n8v 1\n[ cs\n.C V\n] 1\n6 N\nov 2\nish lions, from humorous comics, take on human expressions (e.g., surprise, nastiness), while those from adventure comics are more photorealistic.\nComics are not just visual: creators push their stories forward through text\u2014speech balloons, thought clouds, and narrative boxes\u2014which we identify and transcribe using optical character recognition (OCR). Together, text and image are often intricately woven together to tell a story that neither could tell on its own (Section 3). To understand a story, readers must connect dialogue and narration to characters and environments; furthermore, the text must be read in the proper order, as panels often depict long scenes rather than individual moments [10]. Text plays a much larger role in COMICS than it does for existing datasets of visual stories [25].\nTo test machines\u2019 ability to perform closure, we present three novel cloze-style tasks in Section 4 that require a deep understanding of narrative and character to solve. In Section 5, we design four neural architectures to examine the impact of multimodality and contextual understanding via closure. All of these models perform significantly worse than humans on our tasks; we conclude with an error analysis (Section 6) that suggests future avenues for improvement."}, {"heading": "2. Creating a dataset of comic books", "text": "Comics, defined by cartoonist Will Eisner as sequential art [13], tell their stories in sequences of panels, or single frames that can contain both images and text. Existing comics datasets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; additionally, they lack diversity in visual style and genres. Thus,\nwe build our own dataset, COMICS, by (1) downloading comics in the public domain, (2) segmenting each page into panels, (3) extracting textbox locations from panels, and (4) running OCR on textboxes and post-processing the output. Table 1 summarizes the contents of COMICS. The rest of this section describes each step of our data creation pipeline."}, {"heading": "2.1. Where do our comics come from?", "text": "The \u201cGolden Age of Comics\u201d began during America\u2019s Great Depression and lasted through World War II, ending in the mid-1950s with the passage of strict censorship regulations. In contrast to the long, world-building story arcs popular in later eras, Golden Age comics tend to be small and self-contained; a single book usually contains multiple different stories sharing a common theme (e.g., crime or mystery). While the best-selling Golden Age comics tell of American superheroes triumphing over German and Japanese villains, a variety of other genres (such as romance, humor, and horror) also became popular [18]. The Digital Comics Museum (DCM)2 hosts user-uploaded scans of many comics by lesser-known Golden Age publishers that are now in the public domain due to copyright expiration. We download only the 4,000 highest-rated comic books from DCM to avoid off-square images and missing pages, as the scans vary in resolution and quality.3"}, {"heading": "2.2. Breaking comics into their basic elements", "text": "The DCM comics are distributed as compressed archives of JPEG page scans. To analyze closure, which occurs from panel-to-panel, we first extract panels from the page images. Next, we extract textboxes from the panels, as both location and content of textboxes are important for character and narrative understanding.\nPanel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters. Unfortunately, scanned images of eighty-year old comics do\n2http://digitalcomicmuseum.com/ 3Some of the panels in COMICS contain offensive caricatures and opin-\nions reflective of that period in American history.\nnot particularly adhere to these standards; furthermore, many DCM comics have non-standard panel layouts and/or textboxes that extend across gutters to multiple panels.\nAfter our attempts to use existing panel segmentation software failed, we turned to deep learning. We annotate 500 randomly-selected pages from our dataset with rectangular bounding boxes for panels. Each bounding box encloses both the panel artwork and the textboxes within the panel; in cases where a textbox spans multiple panels, we necessarily also include portions of the neighboring panel. After annotation, we train a region-based convolutional neural network to automatically detect panels. In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network. In Western comics, panels are usually read left-toright, top-to-bottom, so we also have to properly order all of the panels within a page after extraction. We compute the midpoint of each panel and sort them using Morton order [41], which gives incorrect orderings only for rare and complicated panel layouts.\nTextbox segmentation: Since we are particularly interested in modeling the interplay between text and artwork, we need to also convert the text in each panel to a machinereadable format.4 As with panel segmentation, existing comic textbox detection algorithms [22, 47] could not accurately localize textboxes for our data. Thus, we resort again to Faster R-CNN: we annotate 1,500 panels for textboxes,5 train a Faster-R-CNN, and sort the extracted textboxes within each panel using Morton order."}, {"heading": "2.3. OCR", "text": "The final step of our data creation pipeline is applying OCR to the extracted textbox images. We unsuccessfully experimented with two trainable open-source OCR systems, Tesseract [50] and Ocular [6], as well as Abbyy\u2019s consumergrade FineReader.6 The ineffectiveness of these systems is likely due to the considerable variation in comic fonts as well as domain mismatches with pretrained language models (comics text is always capitalized, and dialogue phenomena such as dialects may not be adequately represented in training data). Google\u2019s Cloud Vision OCR7 performs much better on comics than any other system we tried. While it sometimes struggles to detect short words or punctuation marks, the quality of the transcriptions is good con-\n4Alternatively, modules for text spotting and recognition [27] could be built into architectures for our downstream tasks, but since comic dialogues can be quite lengthy, these modules would likely perform poorly.\n5We make a distinction between narration and dialogue; the former usually occurs in strictly rectangular boxes at the top of each panel and contains text describing or introducing a new scene, while the latter is usually found in speech balloons or thought clouds.\n6http://www.abbyy.com 7http://cloud.google.com/vision\nsidering the image domain and quality. We use the Cloud Vision API to run OCR on all 2.5 million textboxes for a cost of $3,000. We post-process the transcriptions by removing systematic spelling errors (e.g., failing to recognize the first letter of a word). Finally, each book in our dataset contains three or four full-page product advertisements; since they are irrelevant for our purposes, we train a classifier on the transcriptions to remove them.8"}, {"heading": "3. Data Analysis", "text": "In this section, we explore what makes understanding narratives in COMICS difficult, focusing specifically on intrapanel behavior (how images and text interact within a panel) and interpanel transitions (how the narrative advances from one panel to the next). We characterize panels and transitions using a modified version of the annotation scheme in Scott McCloud\u2019s \u201cUnderstanding Comics\u201d [40]. Over 90% of panels rely on both text and image to convey information, as opposed to just using a single modality. Closure is also important: to understand most transitions between panels, readers must make complex inferences that often require common sense (e.g., connecting jumps in space and/or time, recognizing when new characters have been introduced to an existing scene). We conclude that any model trained to understand narrative flow in COMICS will have to effectively tie together multimodal inputs through closure.\nTo perform our analysis, we manually annotate 250 randomly-selected pairs of consecutive panels from COMICS. Each panel of a pair is annotated for intrapanel behavior, while an interpanel annotation is assigned to the transition between the panels. Two annotators independently categorize each pair, and a third annotator makes the final decision when they disagree. We use four intrapanel categories (definitions from McCloud, percentages from our annotations):\n1. Word-specific, 4.4%: The pictures illustrate, but don\u2019t significantly add to a largely complete text. 2. Picture-specific, 2.8%: The words do little more than add a soundtrack to a visually-told sequence. 3. Parallel, 0.6%: Words and pictures seem to follow very different courses without intersecting. 4. Interdependent, 92.1%: Words and pictures go handin-hand to convey an idea that neither could convey alone.\nWe group interpanel transitions into five categories: 1. Moment-to-moment, 0.4%: Almost no time passes\nbetween panels, much like adjacent frames in a video. 2. Action-to-action, 34.6%: The same subjects progress\nthrough an action within the same scene.\n8See supplementary material for specifics about our post-processing.\n3. Subject-to-subject, 32.7%: New subjects are introduced while staying within the same scene or idea. 4. Scene-to-scene, 13.8%: Significant changes in time or space between the two panels. 5. Continued conversation, 17.7%: Subjects continue a conversation across panels without any other changes.\nThe two annotators agree on 96% of the intrapanel annotations (Cohen\u2019s \u03ba = 0.657), which is unsurprising because almost every panel is interdependent. The interpanel task is significantly harder: agreement is only 68% (Cohen\u2019s \u03ba = 0.605). Panel transitions are more diverse, as all types except moment-to-moment are relatively common (Figure 3); interestingly, moment-to-moment transitions require the least amount of closure as there is almost no change in time or space between the panels. Multiple transition types may occur in the same panel, such as simultaneous changes in subjects and actions, which also contributes to the lower interpanel agreement."}, {"heading": "4. Tasks that test closure", "text": "To explore closure in COMICS, we design three novel tasks (text cloze, visual cloze, and character coherence) that test a model\u2019s ability to understand narratives and characters given a few panels of context. As shown in the previous section\u2019s analysis, a high percentage of panel transitions require non-trivial inferences from the reader; to successfully solve our proposed tasks, a model must be able to make the same kinds of connections.\nWhile their objectives are different, all three tasks\nfollow the same format: given preceding panels pi\u22121, pi\u22122, . . . , pi\u2212n as context, a model is asked to predict some aspect of panel pi. While previous work on visual storytelling focuses on generating text given some context [24], the dialogue-heavy text in COMICS makes evaluation difficult (e.g., dialects, grammatical variations, many rare words). We want our evaluations to focus specifically on closure, not generated text quality, so we instead use a cloze-style framework [53]: given c candidates\u2014with a single correct option\u2014models must use the context panels to rank the correct candidate higher than the others. The rest of this section describes each of the three tasks in detail; Table 1 provides the total instances of each task with the number of context panels n = 3.\nText Cloze: In the text cloze task, we ask the model to predict what text out of a set of candidates belongs in a particular textbox, given both context panels (text and image) as well as the current panel image. While initially we did not put any constraints on the task design, we quickly noticed two major issues. First, since the panel images include textboxes, any model trained on this task could in principle learn to crudely imitate OCR by matching text candidates to the actual image of the text. To solve this problem, we \u201cblack out\u201d the rectangle given by the bounding boxes for each textbox in a panel (see Figure 4).9 Second, panels often have multiple textboxes (e.g., conversations between characters); to focus on interpanel transitions rather\n9To reduce the chance of models trivially correlating candidate length to textbox size, we remove very short and very long candidates.\nthan intrapanel complexity, we restrict pi to panels that contain only a single textbox. Thus, nothing from the current panel matters other than the artwork; the majority of the predictive information comes from previous panels.\nVisual Cloze: We know from Section 3 that in most cases, text and image work interdependently to tell a story. In the visual cloze task, we follow the same set-up as in text cloze, but our candidates are images instead of text. A key difference is that models are not given text from the final panel; in text cloze, models are allowed to look at the final panel\u2019s artwork. This design is motivated by eyetracking studies in single-panel cartoons, which show that readers look at artwork before reading the text [7], although in many cases atypical font style and text length can invert this order [16].\nCharacter Coherence: While the previous two tasks focus mainly on narrative structure, our third task attempts to isolate character understanding through a re-ordering task. Given a jumbled set of text from the textboxes in panel pi, a model must learn to match each candidate to its corresponding textbox. We restrict this task to panels that contain exactly two dialogue boxes (narration boxes are excluded to focus the task on characters). While it is often easy to order the text based on the language alone (e.g., \u201chow\u2019s it going\u201d always comes before \u201cfine, how about you?\u201d), many cases require inferring which character is likely to utter a particular bit of dialogue based on both their previous utterances and their appearance (e.g., Figure 4, top)."}, {"heading": "4.1. Task Difficulty", "text": "For text cloze and visual cloze, we have two difficulty settings that vary in how cloze candidates are chosen. In the easy setting, we sample textboxes (or panel images) from the entire COMICS dataset at random. Most incorrect candidates in the easy setting have no relation to the provided context, as they come from completely different books and genres. This setting is thus easier for models to \u201ccheat\u201d on by relying on stylistic indicators instead of contextual information. With that said, the task is still non-trivial; for example, many bits of short dialogue can be applicable in a variety of scenarios. In the hard case, the candidates come from nearby pages, so models must rely on the context to perform well. For text cloze, all candidates are likely to mention the same character names and entities, while color schemes and textures become much less distinguishing for visual cloze."}, {"heading": "5. Models & Experiments", "text": "To measure the difficulty of these tasks for deep learning models, we adapt strong baselines for multimodal language and vision understanding tasks to the comics domain. We evaluate four different neural models, variants of which were also used to benchmark the Visual Question Answering dataset [2] and encode context for visual storytelling [25]: text-only, image-only, and two image-text models. Our best-performing model encodes panels with a hierarchical LSTM architecture (see Figure 5).\nPerformance increases when models are given images (in the form of pretrained VGG-16 features) in addition to text, which supports the insights we draw from our data analysis in Section 3. Furthermore, for the text cloze and visual cloze tasks, models perform far worse on the hard setting than the easy setting, confirming our intuition that these tasks are non-trivial when we control for stylistic dissimilarities between candidates. Finally, none of the architectures outperform human baselines, which speaks to the difficulty of understanding COMICS: image features obtained from models trained on natural images cannot capture the vast variation in artistic styles, and textual models struggle with the richness and ambiguity of colloquial dialogue highly dependent on visual contexts. In the rest of this section, we first introduce a shared notation and then use it to specify all of our models."}, {"heading": "5.1. Model definitions", "text": "In all of our tasks, we are asked to make a prediction about a particular panel given the preceding n panels as context.10 Each panel consists of three distinct elements: image, text (OCR output), and textbox bounding box coordinates. For any panel pi, we denote the corresponding image as zi. Since there can be multiple textboxes per panel, we use tix and bix to refer to individual textbox contents and bounding boxes, respectively. Each of our tasks has a different set of answer candidates A: text cloze has three\n10Test and validation instances for all tasks come from comic books that are unseen during training.\ntext candidates ta1...3 , visual cloze has three image candidates za1...3 , and character coherence has two combinations of text / bounding box pairs, {ta1/ba1 , ta2/ba2} and {ta1/ba2 , ta2/ba1}. Our architectures differ mainly in the encoding function g that they use to convert the sequence of context panels pi\u22121, pi\u22122, . . . , pi\u2212n into a fixed-length vector c. We score the answer candidates by taking their inner product with c and normalizing with the softmax function,\ns = softmax(AT c), (1)\nand we minimize the cross-entropy loss against the groundtruth labels.11\nText-only: The text-only baseline only has access to the text tix within each panel. Our g function encodes this text on multiple levels: we first compute a representation for each tix with a word embedding sum\n12 and then combine multiple textboxes within the same panel using an intrapanel LSTM [23]. Finally, we feed the panel-level representations to an interpanel LSTM and take its final hidden state as the context representation (Figure 5). For text cloze, the answer candidates are also encoded with a word embedding sum; for visual cloze, we project the 4096-d fc7 layer of VGG-16 down to the word embedding dimensionality with\n11Performance falters slightly on a development set with contrastive max-margin loss functions [51] in place of our softmax alternative.\n12As in previous work for visual question answering [57], we observe no noticeable improvement with more sophisticated encoding architectures.\na fully-connected layer.13\nImage-only: The image-only baseline is even simpler: we feed the fc7 features of each context panel to an LSTM and use the same objective function as before to score candidates. For visual cloze, we project both the context and answer representations to 512-d with additional fullyconnected layers before scoring. While the COMICS dataset is certainly large, we do not attempt learning visual features from scratch as our task-specific signals are far more complicated than simple image classification. We also try fine-tuning the lower-level layers of VGG-16 [4]; however, this substantially lowers task accuracy even with very small learning rates for the fine-tuned layers.\nImage-text: We combine the previous two models by concatenating the output of the intrapanel LSTM with the fc7 representation of the image and passing the result through a fully-connected layer before feeding it to the interpanel LSTM (Figure 5). For text cloze and character coherence, we also experiment with a variant of the imagetext baseline that has no access to the context panels, which we dub NC-image-text. In this model, the scoring function computes inner products between the image features of pi and the text candidates.14"}, {"heading": "6. Error Analysis", "text": "Table 2 contains our full experimental results, which we briefly summarize here. On all tasks except hard visual\n13For training and testing, we use three panels of context and three candidates. We use a vocabulary size of 30,000 words, restrict the maximum number of textboxes per panel to three, and set the dimensionality of word embeddings and LSTM hidden states to 256. Models are optimized using Adam [29] for ten epochs, after which we select the best-performing model on the dev set.\n14We cannot apply this model to visual cloze because we are not allowed access to the artwork in pi.\ncloze, the image-text model outperforms those trained on a single modality. However, text is much less helpful for visual cloze than it is for text cloze, suggesting that visual similarity dominates the former task. Having the context of the preceding panels helps across the board, although the improvements are lower in the hard setting. In general, there is more variation across the models in the easy setting; we hypothesize that the hard case requires moving away from pretrained image features, and transfer learning methods may prove effective here. Differences between models on character coherence are minor; we suspect that more complicated attentional architectures that leverage the bounding box locations bix are necessary to \u201cfollow\u201d speech bubble tails to the characters who speak them.\nWe also compare all models to a human baseline, for which the authors manually solve one hundred instances of each task (in the hard setting) given the same preprocessed input that is fed to the neural architectures. Most human errors are the result of poor OCR quality (e.g., misspelled words) or low image resolution. Humans comfortably outperform all models, making it worthwhile to look at where computers fail but humans succeed.\nThe top row in Figure 6 demonstrates an instance (from easy text cloze where the image helps the model make the correct prediction. The text-only model has no idea that an airplane (referred to here as a \u201cship\u201d) is present in the panel sequence, as the dialogue in the context panels make no mention of it. In contrast, the image-text model is able to use the artwork to rule out the two incorrect candidates.\nThe bottom two rows in Figure 6 show hard text cloze instances in which the image-text model is deceived by the artwork in the final panel. While the final panel of the middle row does contain what looks to be a creek, \u201ccatfish creek jail\u201d is more suited for a narrative box than a speech bubble, while the meaning of the correct candidate is obscured by the dialect and out-of-vocabulary token. Similarly, a camera films a fight scene in the last row; the model selects a candidate that describes a fight instead of focusing on the context in which the scene occurs. These examples suggest that the contextual information is overridden by strong associations between text and image, motivating architectures that go beyond similarity by leveraging external world knowledge to determine whether an utterance is truly appropriate in a given situation."}, {"heading": "7. Related Work", "text": "Our work is related to three main areas: (1) multimodal tasks that require language and vision understanding, (2) computational methods that focus on non-natural images, and (3) models that characterize language-based narratives.\nDeep learning has renewed interest in jointly reasoning about vision and language. Datasets such as MS COCO [35] and Visual Genome [31] have enabled image caption-\ning [54, 28, 56] and visual question answering [37, 36]. Similar to our character coherence task, researchers have built models that match TV show characters with their visual attributes [15] and speech patterns [21].\nClosest to our own comic book setting is the visual storytelling task, in which systems must generate [24] or re-\norder [1] stories given a dataset (SIND) of photos from Flikr galleries of \u201cstoryable\u201d events such as weddings and birthday parties. SIND\u2019s images are fundamentally different from COMICS in that they lack coherent characters and accompanying dialogue. Comics are created by skilled professionals, not crowdsourced workers, and they offer a far greater variety of character-centric stories that depend on dialogue to further the narrative; with that said, the text in COMICS is less suited for generation because of OCR errors.\nWe build here on previous work that attempts to understand non-natural images. Zitnick et al. [58] discover semantic scene properties from a clip art dataset featuring characters and objects in a limited variety of settings. Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20]. Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].\nTo the best of our knowledge, we are the first to computationally model content in comic books as opposed to just extracting their elements. We follow previous work in language-based narrative understanding; very similar to our text cloze task is the \u201cStory Cloze Test\u201d [42], in which models must predict the ending to a short (four sentences long) story. Just like our tasks, the Story Cloze Test proves difficult for computers and motivates future research into commonsense knowledge acquisition. Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels."}, {"heading": "8. Conclusion & Future Work", "text": "We present the COMICS dataset, which contains over 1.2 million panels from \u201cGolden Age\u201d comic books. We design three cloze-style tasks on COMICS to explore closure, or how readers connect disparate panels into coherent stories. Experiments with different neural architectures, along with a manual data analysis, confirm the importance of multimodal models that combine text and image for comics understanding. We additionally show that context is crucial for predicting narrative or character-centric aspects of panels.\nHowever, for computers to reach human performance, they will need to become better at leveraging context. Readers rely on commonsense knowledge to make sense of dramatic scene and camera changes; how can we inject such knowledge into our models? Another potentially intriguing direction, especially given recent advances in generative adversarial networks [17], is generating artwork given dialogue (or vice versa). Finally, COMICS presents a golden opportunity for transfer learning; can we train models that generalize across natural and non-natural images much like humans do?"}], "references": [{"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["H. Agrawal", "A. Chandrasekaran", "D. Batra", "D. Parikh", "M. Bansal"], "venue": "Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive segmentation for manga", "author": ["Y. Aramaki", "Y. Matsui", "T. Yamasaki", "K. Aizawa"], "venue": "Special Interest Group on Computer Graphics and Interactive Techniques Conference,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-modal scene networks", "author": ["Y. Aytar", "L. Castrejon", "C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A bayesian mixed effects model of literary character", "author": ["D. Bamman", "T. Underwood", "N.A. Smith"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised transcription of historical documents", "author": ["T. Berg-Kirkpatrick", "G. Durrett", "D. Klein"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual analysis of cartoons: A view from the far side", "author": ["P.J. Carroll", "J.R. Young", "M.S. Guertin"], "venue": "Eye movements and visual cognition. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The limits of time and transitions: challenges to theories of sequential image comprehension", "author": ["N. Cohn"], "venue": "Studies in Comics, 1(1),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The state of the art: Object retrieval in paintings using discriminative regions", "author": ["E. Crowley", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Face painting: querying art with photos", "author": ["E.J. Crowley", "O.M. Parkhi", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Comics & Sequential Art", "author": ["W. Eisner"], "venue": "Poorhouse Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Extracting social networks from literary fiction", "author": ["D.K. Elson", "N. Dames", "K.R. McKeown"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Hello! my name is.", "author": ["M. Everingham", "J. Sivic", "A. Zisserman"], "venue": "buffy\u201d \u2013 automatic naming of characters in TV video. In Proceedings of the British Machine Vision Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Reading without words: Eye movements in the comprehension of comic strips", "author": ["T. Foulsham", "D. Wybrow", "N. Cohn"], "venue": "Applied Cognitive Psychology, 30,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Comic Book Encyclopedia: The Ultimate Guide to Characters, Graphic Novels, Writers, and Artists in the Comic Book Universe", "author": ["R. Goulart"], "venue": "HarperCollins,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "ebdtheque: a representative database of comics", "author": ["C. Gu\u00e9rin", "C. Rigaud", "A. Mercier", "F. Ammar-Boudjelal", "K. Bertet", "A. Bouju", "J.-C. Burie", "G. Louis", "J.-M. Ogier", "A. Revel"], "venue": "International Conference on Document Analysis and Recognition,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A distorted skull lies in the bottom center: Identifying paintings from text descriptions", "author": ["A. Guha", "M. Iyyer", "J. Boyd-Graber"], "venue": "NAACL Human-Computer Question Answering Workshop,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Naming TV characters by watching and analyzing dialogs", "author": ["M. Haurilet", "M. Tapaswi", "Z. Al-Halah", "R. Stiefelhagen"], "venue": "2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7- 10, 2016,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Panel and speech balloon extraction from comic books", "author": ["A.K.N. Ho", "J.-C. Burie", "J.-M. Ogier"], "venue": "IAPR International Workshop on Document Analysis Systems,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Visual storytelling", "author": ["T.-H.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R. Girshick", "X. He", "P. Kohli", "D. Batra"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Visual storytelling", "author": ["T.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R.B. Girshick", "X. He", "P. Kohli", "D. Batra", "C.L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 1233\u20131239,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Feuding families and former friends: Unsupervised learning for dynamic fictional relationships", "author": ["M. Iyyer", "A. Guha", "S. Chaturvedi", "J. Boyd-Graber", "H. Daum\u00e9 III"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Reading text in the wild with convolutional neural networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Journal of Computer Vision, 116(1),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A layered method for determining manga text bubble reading order", "author": ["S. Kovanen", "K. Aizawa"], "venue": "International Conference on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Plot units and narrative summarization", "author": ["W.G. Lehnert"], "venue": "Cognitive Science, 5(4),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1981}, {"title": "Automatic comic page segmentation based on polygon detection", "author": ["L. Li", "Y. Wang", "Z. Tang", "L. Gao"], "venue": "Multimedia Tools and Applications, 69(1),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering, 2016", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Challenge for manga processing: Sketch-based manga retrieval", "author": ["Y. Matsui"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Sketch-based manga retrieval using manga109 dataset", "author": ["Y. Matsui", "K. Ito", "Y. Aramaki", "T. Yamasaki", "K. Aizawa"], "venue": "arXiv preprint arXiv:1510.04389,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding Comics", "author": ["S. McCloud"], "venue": "HarperCollins,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "A computer oriented geodetic data base and a new technique in file sequencing", "author": ["G.M. Morton"], "venue": "International Business Machines Co,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "author": ["N. Mostafazadeh", "N. Chambers", "X. He", "D. Parikh", "D. Batra", "L. Vanderwende", "P. Kohli", "J. Allen"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "A robust panel extraction method for manga", "author": ["X. Pang", "Y. Cao", "R.W. Lau", "A.B. Chan"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "A robust panel extraction method for manga", "author": ["X. Pang", "Y. Cao", "R.W.H. Lau", "A.B. Chan"], "venue": "Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation and indexation of complex objects in comic book images", "author": ["C. Rigaud"], "venue": "PhD thesis, University of La Rochelle, France,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "An active contour model for speech balloon detection in comics", "author": ["C. Rigaud", "J.-C. Burie", "J.-M. Ogier", "D. Karatzas", "J. Van de Weijer"], "venue": "In International Conference on Document Analysis and Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Knowledge-driven understanding of images in comic books", "author": ["C. Rigaud", "C. Gu\u00e9rin", "D. Karatzas", "J.-C. Burie", "J.-M. Ogier"], "venue": "International Journal on Document Analysis and Recognition, 18(3),", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures", "author": ["R. Schank", "R. Abelson"], "venue": "L. Erlbaum,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1977}, {"title": "An overview of the tesseract ocr engine", "author": ["R. Smith"], "venue": "International Conference on Document Analysis and Recognition,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Layout analysis of tree-structured scene frames in comic images", "author": ["T. Tanaka", "K. Shoji", "F. Toyama", "J. Miyamichi"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["W.L. Taylor"], "venue": "Journalism and Mass Communication Quarterly, 30(4),", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1953}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "Proceedings of the International Conference of Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "Proceedings of the International Conference of Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 38(4):627\u2013638,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "Through a process called closure [40], which involves (1) understanding individual panels and (2) making connective inferences across panels, readers form coherent storylines from seemingly disparate panels such as these.", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 54, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 53, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 9, "context": "To understand a story, readers must connect dialogue and narration to characters and environments; furthermore, the text must be read in the proper order, as panels often depict long scenes rather than individual moments [10].", "startOffset": 221, "endOffset": 225}, {"referenceID": 24, "context": "Text plays a much larger role in COMICS than it does for existing datasets of visual stories [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Comics, defined by cartoonist Will Eisner as sequential art [13], tell their stories in sequences of panels, or single frames that can contain both images and text.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Existing comics datasets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; additionally, they lack diversity in visual style and genres.", "startOffset": 25, "endOffset": 33}, {"referenceID": 37, "context": "Existing comics datasets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; additionally, they lack diversity in visual style and genres.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "While the best-selling Golden Age comics tell of American superheroes triumphing over German and Japanese villains, a variety of other genres (such as romance, humor, and horror) also became popular [18].", "startOffset": 199, "endOffset": 203}, {"referenceID": 33, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 72, "endOffset": 76}, {"referenceID": 50, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 41, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 46, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 43, "context": "In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network.", "startOffset": 91, "endOffset": 94}, {"referenceID": 39, "context": "We compute the midpoint of each panel and sort them using Morton order [41], which gives incorrect orderings only for rare and complicated panel layouts.", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "4 As with panel segmentation, existing comic textbox detection algorithms [22, 47] could not accurately localize textboxes for our data.", "startOffset": 74, "endOffset": 82}, {"referenceID": 45, "context": "4 As with panel segmentation, existing comic textbox detection algorithms [22, 47] could not accurately localize textboxes for our data.", "startOffset": 74, "endOffset": 82}, {"referenceID": 48, "context": "We unsuccessfully experimented with two trainable open-source OCR systems, Tesseract [50] and Ocular [6], as well as Abbyy\u2019s consumergrade FineReader.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "We unsuccessfully experimented with two trainable open-source OCR systems, Tesseract [50] and Ocular [6], as well as Abbyy\u2019s consumergrade FineReader.", "startOffset": 101, "endOffset": 104}, {"referenceID": 26, "context": "4Alternatively, modules for text spotting and recognition [27] could be built into architectures for our downstream tasks, but since comic dialogues can be quite lengthy, these modules would likely perform poorly.", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "We characterize panels and transitions using a modified version of the annotation scheme in Scott McCloud\u2019s \u201cUnderstanding Comics\u201d [40].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "While previous work on visual storytelling focuses on generating text given some context [24], the dialogue-heavy text in COMICS makes evaluation difficult (e.", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "We want our evaluations to focus specifically on closure, not generated text quality, so we instead use a cloze-style framework [53]: given c candidates\u2014with a single correct option\u2014models must use the context panels to rank the correct candidate higher than the others.", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "This design is motivated by eyetracking studies in single-panel cartoons, which show that readers look at artwork before reading the text [7], although in many cases atypical font style and text length can invert this order [16].", "startOffset": 138, "endOffset": 141}, {"referenceID": 15, "context": "This design is motivated by eyetracking studies in single-panel cartoons, which show that readers look at artwork before reading the text [7], although in many cases atypical font style and text length can invert this order [16].", "startOffset": 224, "endOffset": 228}, {"referenceID": 1, "context": "We evaluate four different neural models, variants of which were also used to benchmark the Visual Question Answering dataset [2] and encode context for visual storytelling [25]: text-only, image-only, and two image-text models.", "startOffset": 126, "endOffset": 129}, {"referenceID": 24, "context": "We evaluate four different neural models, variants of which were also used to benchmark the Visual Question Answering dataset [2] and encode context for visual storytelling [25]: text-only, image-only, and two image-text models.", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "Our g function encodes this text on multiple levels: we first compute a representation for each tix with a word embedding sum 12 and then combine multiple textboxes within the same panel using an intrapanel LSTM [23].", "startOffset": 212, "endOffset": 216}, {"referenceID": 49, "context": "11Performance falters slightly on a development set with contrastive max-margin loss functions [51] in place of our softmax alternative.", "startOffset": 95, "endOffset": 99}, {"referenceID": 55, "context": "12As in previous work for visual question answering [57], we observe no noticeable improvement with more sophisticated encoding architectures.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "We also try fine-tuning the lower-level layers of VGG-16 [4]; however, this substantially lowers task accuracy even with very small learning rates for the fine-tuned layers.", "startOffset": 57, "endOffset": 60}, {"referenceID": 28, "context": "Models are optimized using Adam [29] for ten epochs, after which we select the best-performing model on the dev set.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "Datasets such as MS COCO [35] and Visual Genome [31] have enabled image caption-", "startOffset": 48, "endOffset": 52}, {"referenceID": 52, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 27, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 54, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 35, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 47, "endOffset": 55}, {"referenceID": 34, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 47, "endOffset": 55}, {"referenceID": 14, "context": "Similar to our character coherence task, researchers have built models that match TV show characters with their visual attributes [15] and speech patterns [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Similar to our character coherence task, researchers have built models that match TV show characters with their visual attributes [15] and speech patterns [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "Closest to our own comic book setting is the visual storytelling task, in which systems must generate [24] or reorder [1] stories given a dataset (SIND) of photos from Flikr galleries of \u201cstoryable\u201d events such as weddings and birthday parties.", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Closest to our own comic book setting is the visual storytelling task, in which systems must generate [24] or reorder [1] stories given a dataset (SIND) of photos from Flikr galleries of \u201cstoryable\u201d events such as weddings and birthday parties.", "startOffset": 118, "endOffset": 121}, {"referenceID": 56, "context": "[58] discover semantic scene properties from a clip art dataset featuring characters and objects in a limited variety of settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 11, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 19, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 44, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 42, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 36, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 29, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 40, "context": "We follow previous work in language-based narrative understanding; very similar to our text cloze task is the \u201cStory Cloze Test\u201d [42], in which models must predict the ending to a short (four sentences long) story.", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 4, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 25, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 47, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 32, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 7, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 16, "context": "Readers rely on commonsense knowledge to make sense of dramatic scene and camera changes; how can we inject such knowledge into our models? Another potentially intriguing direction, especially given recent advances in generative adversarial networks [17], is generating artwork given dialogue (or vice versa).", "startOffset": 250, "endOffset": 254}], "year": 2016, "abstractText": "Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \u201cgutters\u201d between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \u201cclosure\u201d. While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.", "creator": "LaTeX with hyperref package"}}}