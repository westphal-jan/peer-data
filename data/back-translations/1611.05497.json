{"id": "1611.05497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Explicable Robot Planning as Minimizing Distance from Expected Behavior", "abstract": "In order to effectively integrate robots into human workflows, it is not enough to address the question of autonomy, but also how their actions or plans are perceived by their human counterparts. If robots create task plans without such considerations, they often show what we call inexplicable behavior from the point of view of the human who may be observing it. This problem arises from a partial or inaccurate understanding of the deliberate process and / or model (i.e. the capabilities of the robot) that influences it. This can have serious implications for the human-robot workspace, from increased cognitive strain and decreased trust in the human-robot to more serious concerns about safety in human-robot interactions. In this paper, we propose to address this problem by learning a distance function that can accurately model the notion of explainability, and develop an always available search algorithm that can use this measure in its search process to create progressively explainable plans.", "histories": [["v1", "Wed, 16 Nov 2016 23:24:38 GMT  (634kb,D)", "http://arxiv.org/abs/1611.05497v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["anagha kulkarni", "tathagata chakraborti", "yantian zha", "satya gautam vadlamudi", "yu zhang", "subbarao kambhampati"], "accepted": false, "id": "1611.05497"}, "pdf": {"name": "1611.05497.pdf", "metadata": {"source": "CRF", "title": "Explicable Robot Planning as Minimizing Distance from Expected Behavior", "authors": ["Anagha Kulkarni", "Tathagata Chakraborti", "Yantian Zha", "Satya Gautam Vadlamudi", "Yu Zhang", "Subbarao Kambhampati"], "emails": ["vsatyagautam@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nRecent advancement in the field of robotics has given us autonomous robots, vehicles, drones, etc. Typically these autonomous systems have the capability to make their own plans which help them achieve their goals. These advances have, naturally, encouraged the possibility of human-robot teaming where the autonomous robots and humans can work alongside each other. However, if the plans that are being generated by the autonomous robots are difficult to comprehend for the human observer, the unexpected behavior from the robot can raise several concerns: it may increase cognitive load, hamper the productivity of the team, and result in safety concerns and distrust towards the robot [1].\nThis mismatch between the robot\u2019s plans and the human expectations may be explained in terms of difference in the actual robot model and the human\u2019s understanding of the robot model. Thus, even with the knowledge of the robot\u2019s\n1Anagha Kulkarni, Tathagata Chakraborti, Yantian Zha, Yu Zhang, and Subbarao Kambhampati are with the Computer Science and Engineering Department at Arizona State University { akulka16, tchakra2, yantian.zha, yzhan442, rao } @ asu.edu\n2Satya Gautam Vadlamudi is with Capillary Technologies. vsatyagautam@gmail.com\ngoals, it may still not be possible for the human to make sense of the robot\u2019s plan. For example, consider a scenario with an autonomous car switching lanes on a highway. The autonomous car, in order to switch the lane, may make sharp and calculated moves, as opposed to gradually moving towards the other lane. These moves may well be optimal for the car, and backed by the car\u2019s superior sensing and steering capabilities. Nevertheless, a human passenger sitting inside may perceive this as dangerous and reckless behavior, in as much as they might be ascribing the car the sort of driving abilities they themselves have.\nTo address this issue of the differences between the robot model, MR(R), and the human mental model of the robot capabilities, MH(R), we develop an approach based on the concept of explicability. An explicable plan is a plan that is generated with the human\u2019s expectation of the robot model; the ability to synthesize explicable plans on the part of the robot thus involves the ability to take into consideration both models into its plan generation process. The intuition is that, the similarity between the robot plan that is generated from the robot model, and the plan that is generated from the human understanding of the robot model determines the explicability of the robot plan. More specifically, the smaller the explicability distance between these two plans, the more explicable the robot plan is. Of course, such a\nar X\niv :1\n61 1.\n05 49\n7v 1\n[ cs\n.A I]\n1 6\nN ov\n2 01\n6\nsimilarity metric is not readily available, which brings us to the question - How can a robot learn a distance function between plans that model the notion of explicability, and how can it use this learned similarity model to inform its own deliberative process? Keeping this in mind, we address the following questions in our paper: 1) Given a domain, can we find an approximation toMH(R), the human mental model of the robot capabilities? 2) Can the measures of distance between a robot plan, \u03c0R, and the plan expected by the human, \u03c0H, effectively capture the explicability of the robot plan? 3) Can we then integrate the explicability estimates into the robot plan generation process? The outline of our proposed approach is illustrated in Figure 1.\nTo address the first question, we start out with a robot model, MR(R) and generate different robot plans for various initial and goal states. Next, we recruit human subjects and ask them to evaluate these plans by assigning scores to them based on how well they understand them. As a part of the study, the subjects are then asked to answer a questionnaire based on the robot model, in order to elicit implicit human preferences. This questionnaire allows the domain modeler to generate MH(R), based on humans assumptions regarding the robot model in that domain. In this paper, we represent both models in PDDL [2], but they can differ in terms of their action representations, preconditions, effects, and costs.\nTo answer the second question, we explore the relationship between three existing plan distance measures: action set, causal link set and state sequence distances [3], [4] and the plan explicability distance. We use the robot plans, assigned with scores, to determine if the explicability of the plans can be modeled in terms of the aforementioned plan distance measures, in terms of a regression function. For this, we generate the plans expected by the human for the same initial and goal states using the human understanding of the robot model. We then compute the plan distances between the robot plans and human expected plans. We call the function that maps the plan distances to the explicability scores as the explicability distance.\nTo address the third question, we integrate the explicablity distance in the search process of the Fast-Downward planner [5]. We perform a cost-bounded anytime search, that can progressively generate more and more explicable plans, using the learned explicability distance as a heuristic guidance. We call this reconciliation search. Note that explicability distance exhibits non-monotonicity, i.e. a new action that gets added to a plan prefix can either increase or decrease the explicability distance depending on the context of the plan. We present an analysis on how this property affects our search. For evaluation of our system, we demonstrate the effectiveness of our system in a simulated autonomous car domain, and use human test subjects to evaluate the explicability of the generated robot plans."}, {"heading": "II. RELATED WORK", "text": "The notion of robots working alongside humans for task achievement has been a popular research direction. It is\nchallenging, mainly due to the fact that, the robot must consider the human in the loop while making its own decisions. One important requirement for achieving this, is the ability to infer about the human\u2019s intent and plan. Various plan recognition algorithms [6], [7] can be applied to perform plan recognition based on a given set of observations as a result of the agent interacting with the environment. After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11]. There is also work on performing simultaneous plan recognition and generation [12]. However, most of the prior work has only focused on how robots can make plans based on the inferred human intent.\nThe motivation for generating explicable task plans was first provided in our recent paper [13]. While that work proposes learning explicability as a labeling scheme, in this work, we consider viewing explicability more directly in terms of distances between the plans generated by the robot\u2019s own model, and the human\u2019s approximation of the robot\u2019s model. While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].\nIn most human-robot cohabitation work where robots are proactive agents, it is often assumed that the human model is provided and complete for inferring about the human intent and plan. This is often not true. Although we also assume a human model a priori, our formulation allows us to adjust this model so as to improve model incompleteness (e.g., action preference). There also exists learnable models that do not assume completeness in the first place [17]. Another note is that in [13], [14] and this work, since the model is one level deeper, which is about the robot model from the humans perspective, learning methods are adopted."}, {"heading": "III. BACKGROUND", "text": ""}, {"heading": "A. Planning", "text": "A classical planning problem can be defined as a tuple P = \u3008M, I,G\u3009, where M = \u3008F,A\u3009 is the domain model (that consists of a finite set F of fluents that define the state of the world and a set of operators or actions A), and I \u2286 F and G \u2286 F are the initial and goal states of the problem respectively. Each action a \u2208 A is a tuple of the form \u3008pre(a), eff(a), c(a)\u3009 where c(a) denotes the cost of an action, pre(a) \u2286 F is the set of preconditions for the action a and eff(a) \u2286 F is the set of the effects. The solution to the planning problem is a plan or a sequence of actions \u03c0 = \u3008a1, a2, . . . , an\u3009 such that starting from the initial state, sequentially executing the actions lands the robot in the goal state, i.e. \u0393M (I, \u03c0) |= G where \u0393M (\u00b7) is the transition function defined for the domain. The cost of the plan, denoted as c(\u03c0) = \u2211 ai\u2208\u03c0 c(ai), is given by the summation of the cost of all the actions in the plan \u03c0. Henceforth, we denote the robot plan as \u03c0R and the human expected plan as \u03c0H ."}, {"heading": "B. Plan Distance Measures", "text": "We now look at the three plan distance measures introduced in [3] and later refined in [4]. These plan distances are action, causal link and state sequence distances. Although these distance metrics do not satisfy certain mathematical properties [18], they provide a good domain independent measure of the difference between any two plans. Since the goal is to predict the differences in terms of explicability distance between the robot plans and human expected plans, the intuition is that they can be approximated using a combination of plan distance measures that capture different aspects of plans.\n1) Action Distance: We denote the set of unique actions in a plan \u03c0 as A(\u03c0) = {a | a \u2208 \u03c0}. Given the action sets A(\u03c0R) and A(\u03c0H) of two plans \u03c0R and \u03c0H respectively, the action distance, \u03b4a, is computed as the ratio of the actions that are exclusive to each plan to all the actions in the plans [4]. It is written as:\n\u03b4A(\u03c0 R, \u03c0H) = 1\u2212 |A(\u03c0 R) \u2229A(\u03c0H)| |A(\u03c0R) \u222aA(\u03c0H)|\n(1)\nThis simply means that two plans are similar (and hence their distance measure is smaller) if they contain similar actions. Note that this measure does not take the ordering of actions into account.\n2) Causal Link Distance: A causal link represents a tuple of the form \u3008ai, pi, ai+1\u3009, where pi is a predicate variable that is produced as an effect of action ai and used as a precondition for the next action ai+1. The causal link distance measure is represented similarly to the action distance, by considering the causal link sets Cl(\u03c0R) and Cl(\u03c0H) instead of action sets described above. It is written as:\n\u03b4C(\u03c0 R, \u03c0H) = 1\u2212 |Cl(\u03c0 R) \u2229 Cl(\u03c0H)| |Cl(\u03c0R) \u222a Cl(\u03c0H)|\n(2)\nAgain, plans are similar, with lower similarity scores, if they have a large number of overlapping causal links.\n3) State Sequence Distance: This distance measure, as the name suggests, takes the sequences of the states into consideration. This distance captures the context of an action in a given plan. The length of the sequences may differ and therefore there are multiple ways to define this distance measure [4]. We use the representation shown in Eq. 3. Given two state sequences (sR0 , . . . , s R n ) and (s H 0 , . . . , s H n\u2032) for \u03c0R and \u03c0H respectively, where n \u2265 n\u2032 are the lengths of the plans, the state sequence distance is written as:\n\u03b4S(\u03c0 R, \u03c0H) =\n1\nn [ n\u2032\u2211 k=1 \u2206(sRk , s H k ) + n\u2212 n\u2032 ] (3)\nwhere \u2206(sRk , s H k ) = 1 \u2212\n|sRk \u2229s H k | |sRk \u222as H k |\nrepresents the distance between two states (where sRk is overloaded to denote the set of predicate variables in state sRk ). The first term measures the normalized difference between states up to the end of the shorted plan, while the second term, in the absence of a state to compare to, assigns maximum difference possible.\nHere we illustrate the explicability distance with an example and discuss its relationship with the other distance measures. Consider the grid structure shown in Figure 2. Here we have a 5 by 5 grid. The bottom left cell is labeled as (1, 1). Some of the cells have glass floors while some others have obstacles. The robot has to find its way across the obstacles from the start cell to the goal cell. Looking at the grid structure, the human may expect the robot to take an optimal path highlighted by the green arrows. Although unbeknownst to the human, the robot has difficulty traveling across the glass floor cells because of the reflective surface and has to use special sensors while navigating across these floors. Hence, the cost of treading on these glass floors is higher than the cost of treading across normal cell. In this case, the robot\u2019s optimal plan to the goal is the one highlighted in red, which doesn\u2019t coincide with human\u2019s expectation of the robot plan.\nIn Figure 3, we provide the actions sets, causal link sets and state sequences generated for both the robot plan and human expected plan for our example illustrated in Figure 2. The corresponding plan distances are shown in Table I. These three distances capture different aspects of the plans. In this case, the explicability distance clearly has a high correlation with these other distance measures. Our goal in this paper\nInitial State: at(1, 1) Goal State: at(5, 4)\nActions: A(\u03c0R) = { move-diagonal(2, 2), move-up(2, 3), move-up(2, 4), move-diagonal(3, 5), move-diagonal(4, 4), move-right(5, 4) } A(\u03c0H ) = {move-diagonal(2, 2), move-diagonal(3, 3), move-diagonal(4, 4), move-right(5, 4) }\nCausal Links: Cl(\u03c0R) = { <move-diagonal(2, 2), at(2, 2), move-up(2, 3)>, <move-up(2, 3), at(2, 3), move-up(2, 4)>, <move-up(2, 4), at(2, 4), move-diagonal(3, 5)>, <move-diagonal(3, 5), at(3, 5), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5, 4)> } Cl(\u03c0H ) = { <move-diagonal(2, 2), at(2, 2), move-diagonal(3, 3)>, <move-diagonal(3, 3), at(3, 3), move-diagonal(4, 4)>, <move-diagonal(4, 4), at(4, 4), move-right(5, 4)> }\nState sequences: S(\u03c0R) = { {at(2, 2)}, {at(2, 3)}, {at(2, 4)}, {at(3, 5)}, {at(4, 4)} } S(\u03c0H ) = { {at(2, 2)}, {at(3, 3)}, {at(4, 4)} }\nFig. 3: The action sets, causal link sets and state sequence sets for the illustrated example in Figure 2. For the given initial and goal state, the plans illustrated in red and green in Figure 2 are used to produce respective action, causal link and state sequence sets.\nis, then, to learn to establish a general relationship between the established measures of plan distance."}, {"heading": "IV. PROPOSED METHODOLOGY", "text": ""}, {"heading": "A. Explicability Distance", "text": "Since, without the model we do not know which plan distance is most relevant in capturing explicability, we present a general formulation in this section. A more detailed formulation can be found in the following section. Let \u2206 be a 3-dimensional vector, such that for a robot plan, \u03c0R, derived from MR(R), and for an explicable plan \u03c0H , derived from MH(R), we have \u2206 = \u3008\u03b4A(\u03c0R, \u03c0H), \u03b4C(\u03c0R, \u03c0H), \u03b4S(\u03c0R, \u03c0H)\u3009T . We now define explicability distance of a robot plan, Exp(\u03c0R), as a regression based function of the three plan distances, with b as the parameter vector:\nExp(\u03c0R / \u03c0H) \u2248 f(\u2206, b) (4)\nIn order to train our regression model, we use plan traces whose actions were assigned scores by human subjects. We can then calculate the explicability score of a plan based on the average of the individual action scores."}, {"heading": "B. Plan Generation", "text": "We now present the details of our plan generation phase, where we use the explicability distance to guide our search to generate the most explicable robot plan for a given problem.\n1) Non-Monotonicity: We will now discuss the nonmonotonic behavior exhibited by explicability function and how it affects the plan generation process. The explicability distance function is non-monotonic in nature, meaning, as the partial plan grows, the explicability distance may both increase or decrease. This is because, a new action can either contribute positively or negatively to the total explicability score of the plan. As pointed out earlier, the explicability score is computed as an average of the individual action scores in the context of the plan prefix.\nObservation 1: Explicability score of a partial plan P may increase, stay equal, or even decrease when it is extended with one or more actions.\nConsider the following example, in a car domain, the goal of the car is to move to the left lane. The car squeezes leftwards in three consecutive actions and after coming to the left lane, it turns on its left indicator. Here the turning on of the left tail light after having moved left is an inexplicable action. The previous three actions were explicable to the human drivers and contribute positively to the explicability\nscore of the plan but the last action has a negative impact and decreases the score. Therefore this score and in turn the explicability distance is not a non-decreasing function. In essence, depending on the context, the explicability of an action can either improve the score or worsen it.\nObservation 2: A greedy method that expands a node with the highest explicability score of the corresponding partial plan at each step does not guarantee to find an optimal explicable plan (one of the plans with the highest explicability score) as its first solution.\nThe above observation is easy to see since, if e1 is explicability score of the first plan, then a node may exist in open list (set of unexpanded nodes) whose explicability score is less than e1, which when expanded may result in a solution plan with explicability score higher than e1.\n2) Reconciliation Search: Given the non-monotonic nature of explicability distance function, we have to generate all the candidate plans in order to find the most explicable plan. Here, we present a cost-bounded anytime greedy search algorithm called reconciliation search that generates all the valid loopless candidate solution plans up to a given cost bound, and then progressively searches for plans with better explicability scores. The value of the heuristic h(v) in a particular state v encountered during search is based entirely on the explicability distance of the robot plan prefix up to that state, given by,\nh(v) = Exp(\u03c0 / \u03c0h)\ns.t. \u0393MR(R)(I, \u03c0) = v and \u0393MH(R)(I, \u03c0h) = v\nSince we want to find explicable plans which are within a cost bound, we use the cost of the plan to prune the nodes in the search graph whenever they exceed the given maximum cost bound. We implement this search in the Fast-Downward planner [5]. The approach is described in detail in Algorithm 1.\nAt each iteration of the algorithm, the plan prefix of the robot model is compared with the explicable trace \u03c0h (these are the plans generated by the human mental model of the robot MH(R) up to the current state in the search process) for the given problem. Using the computed distances, we predict the explicability score for every candidate robot plan. The search algorithm then makes a locally optimal choice of states. After generating the first solution plan we do not stop the search but instead continue to find all the valid loopless candidate solution plans within the given cost bound or until the state space is completely explored. In the end, the candidate plan with highest explicability score is returned."}, {"heading": "V. EXPERIMENTAL ANALYSIS", "text": ""}, {"heading": "A. Autonomous Car Simulation Experiment", "text": "1) Domain Model: Autonomous cars are a topic of interest from the point of view of explicability problem. In the recent past, Google\u2019s self-driving cars [19] have been in the news for being \u201ctoo safe\u201d on the roads. These autonomous cars governed by strict traffic rules find it hard to blend\nAlgorithm 1 Reconciliation Search Input: Planning problem P = \u3008MR(R), I,G\u3009, cost bound max cost, and explicability distance function Exp Output: Robot plan with the highest explicability score \u03c0R = arg max\u03c0R Exp(\u03c0 R / \u03c0H)\n1: S \u2190 \u2205 . Candidate plan solution set 2: open\u2190 \u2205 . Open list 3: closed\u2190 \u2205 . Closed list 4: open.insert(I, 0, inf) 5: while open 6= \u2205 do 6: n\u2190 open.remove() . Node with highest h(\u00b7) 7: if n |= G then 8: S.insert(\u03c0 s.t. \u0393MR(R)(I, \u03c0) |= v) 9: end if\n10: closed.insert(n) 11: for each v \u2208 successors(n) do 12: if v /\u2208 closed then 13: if g(n) + cost(n, v) \u2264 max-cost then 14: open.insert(v, h(v)) 15: end if 16: else 17: if h(n) < h(v) then 18: closed.remove(v) 19: open.insert(v, h(v)) 20: end if 21: end if 22: end for 23: end while 24: return arg max\u03c0R\u2208S Exp(\u03c0R / \u03c0H)\nin and make judgments that would not make sense in a predominantly human environment. At four-way stops, these cars find it difficult to cross the intersection, while the human drivers keep inching forward. For a robot car, such situations, where it does not make an explicable decision can pose problems, and all the human drivers who come into contact with such cars would have to face the brunt of it.\nFor these reasons, we focused our studies on a simulated autonomous car environment, and investigated how the robot car\u2019s inexplicable behavior can be avoided by generating plans with respect to their explicability scores. In our robot car model (written in PDDL), we try to capture bad driving etiquette commonly seen on roads, such as, driving below speed limit in passing lanes, overtaking from the wrong side, turning and changing lanes without showing signal, not following the move over law, and so on. The human mental model of the robot car is defined as per test subjects assumptions of how the robot car should perform actions. From the robot model MR(R), we generated 40 plans for 16 different problems. The plans consisted of both explicable and inexplicable robot car behaviors. These plans were assessed by 20 test subjects, with each subject evaluating 8 plans. Also, each plan was evaluated by 4 different subjects, in order to get a general understanding of the assumptions of different human drivers. Therefore, the overall number of\ntraining samples was 160. The test subjects were required to have sufficient real-life driving experience. The assessment had two parts: one part involved scoring each robot car action with 1, if explicable, and 0 otherwise (the explicability score of the overall plan is calculated as the fraction of actions in the plan that were labeled as explicable); the other part involved answering a questionnaire aimed at understanding test subject\u2019s assumptions regarding the robot car. The information from this questionnaire was used to design the human mental model MH(R) of the robot car.\nThe PDDL domain of the robot car, MR(R), consists of lane and car objects as shown in Figure 4. The red car\nis the robot car in the experiments and all other cars seen in the experiments are assumed to have human drivers. The car objects are associated with predicates defining the location of a car on a lane segment, status of left and right turn lights, status of car being within speed limit, presence of a parked cop car, and so on. The actions possible in the domain are with respect to the robot car. These actions are Accelerate, Decelerate, LeftSqueeze, RightSqueeze, LeftLightOn, LeftLightOff, RightLightOn, RightLightOff, SlowDown and WaitAtStopSign, and so on. In order to change a lane, three consecutive actions of either LeftSqueeze or RightSqueeze are required to gradually move to the other lane. The PDDL domain of the human mental model, consists of same state predicates, but different action representations, preconditions, effects and action-costs. Note that even though representing the human mental model in PDDL may seem like a strong assumption, we validated the labels given by the human subjects with the PDDL human model constructed from the elicited preferences and found about 72.3% match, which indicates that MH(R) used in the evaluations is a good approximation of the human mental model of the robot\n2) Defining the Explicability Distance: For the 22 training problems, explicable plans with MH(R) were generated. Since some actions are not common to both the domains and also owing to the difference in the effects and preconditions of the actions across domains, an explicit mapping was defined between the actions over the two domains. This mapping was done in the light of the plan distance operations performed between plans in the two domains.\nThe correlation matrix in Figure 5 establishes the negative correlation of the plan distance measures to the explicability scores. From the correlation matrix it can be seen that, causal link distance has significant negative correlation with the explicability scores. After establishing the negative correlation, we proceed towards training our regression model called explicability distance.\nAt first, individual distances were used to fit the data in the regression model. This resulted in a poorly learned regression model. A linear combination of the three distances also resulted in poor results. For regression model functions 5, 6, 7 and 8, the bias, weight and accuracy values were as shown in Table II. From this table, we infer that the relationships are not necessarily linear as we speculated previously. We improve our model using Random Forest regression. Since random forests allow selection of random subset of features while splitting the decision node, the accuracy of our model improves. All the three distances have statistically significant contribution in the fitted model. We evaluate the goodness of the fit of the model, using the coefficient of determination or R2. This value determines the measure by which the fitted model can explain the variations in the target values. This value lies between 0 to 1. Higher the R2 value, better is the model fitted to the data. After training process the new regression model was found to have 0.8721 R2 value. That is to say, 87% of the variations in the features can be explained by our model. Our model predicts the explicability distance between the robot plans and human mental model plans, with a high accuracy. We call this plan distance regression model as the explicability distance.\n3) Evaluation: For evaluation of our system, we tested it on 13 different problems. We ran the algorithm with a high cost bound, in order to cover the most explicable candidate plans for all the problems. The results of this search process are as shown in Figure 6, 7 and 8. From these results, we can see that the reconciliation search is able to incrementally develop plans with better explicability scores as shown in Figure 6. In Figure 7, we see that for all the 13 problems the explicability score of the optimal plans is lesser than the final plans generated by reconciliation search. From Figure 8, we see that for the first six problems the optimal and explicable plans have same cost but our modified planner with reconciliation search produces explicable plan versions for those problems. The results also clearly show that the explicable plans can be costlier than plans that are optimal with respect to the robot\u2019s own model. This additional cost\nFig. 6: The graph shows that the search process finds plans with incrementally better explicable scores. Each color line represents the 13 different problems. The markers on the lines represent a plan solution for that problem. The y-axis gives the explicability scores of the plans and the x-axis gives the solution number. Note that the curves show the nonmonotonic nature of evaluation metric in the search process. The final output of the algorithm is, of course, the best plan found in the search process.\nFig. 7: For the test problem instances, the optimal plans generated using Fast-Downward planner and the plans generated using Reconciliation Search were compared for their explicability scores.\ncan be seen as the price the robot pays to make its behavior explicable to the human."}, {"heading": "VI. CONCLUSION", "text": "We showed how the plan distance measures play a role in determining the explicability of a robot plan. We evaluated our hypothesis in the simulated Autonomous Car PDDL domain. We generated training samples in robot\u2019s domain and assigned them with human scores. We also generated plans in the human\u2019s model to find the distances between plans in two domains. We looked at the relationships between scores and the distance measures of the plans. We learned the regression model that could best capture the explicability of the training samples. In summary, we have proved our hypothesis that using the human\u2019s mental model of the robot model we can assess the explicability of a robot plan as a function over the plan distance measures between the robot plan and the plan that the human would expect the robot to make. We also showed that the explicability distance measure can be used to bias the robots planning process to generate plans that are more in concordance with what humans expect. We are currently in the process of incorporating this theory into the behavior of a Fetch robot involved in delivery tasks, to demonstrate how it improves the explicability of the robot\u2019s behavior."}], "references": [{"title": "Adaptive aiding of human-robot teaming effects of imperfect automation on performance, trust, and workload", "author": ["E. de Visser", "R. Parasuraman"], "venue": "Journal of Cognitive Engineering and Decision Making, vol. 5, no. 2, pp. 209\u2013231, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Pddl-the planning domain definition language", "author": ["D. McDermott", "M. Ghallab", "A. Howe", "C. Knoblock", "A. Ram", "M. Veloso", "D. Weld", "D. Wilkins"], "venue": "1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Generating diverse plans to handle unknown and partially known user preferences", "author": ["T.A. Nguyen", "M. Do", "A.E. Gerevini", "I. Serina", "B. Srivastava", "S. Kambhampati"], "venue": "Artificial Intelligence, vol. 190, no. 0, pp. 1 \u2013 31, 2012. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0004370212000707", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The fast downward planning system", "author": ["M. Helmert"], "venue": "CoRR, vol. abs/1109.6051, 2011. [Online]. Available: http://arxiv.org/abs/1109.6051", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized plan recognition.", "author": ["H.A. Kautz", "J.F. Allen"], "venue": "in AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "Probabilistic plan recognition using offthe-shelf classical planners", "author": ["M. Ram\u0131rez", "H. Geffner"], "venue": "Proceedings of the Conference of the Association for the Advancement of Artificial Intelligence (AAAI 2010). Citeseer, 2010, pp. 1121\u20131126.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Planning with resource conflicts in human-robot cohabitation", "author": ["T. Chakraborti", "Y. Zhang", "D.E. Smith", "S. Kambhampati"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016, pp. 1069\u20131077.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-aware task planning for mobile robots", "author": ["M. Cirillo", "L. Karlsson", "A. Saffiotti"], "venue": "Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1\u20137.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Planning for serendipity", "author": ["T. Chakraborti", "G. Briggs", "K. Talamadupula", "Y. Zhang", "M. Scheutz", "D. Smith", "S. Kambhampati"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Coordination in human-robot teams using mental modeling and plan recognition", "author": ["K. Talamadupula", "G. Briggs", "T. Chakraborti", "M. Scheutz", "S. Kambhampati"], "venue": "Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014, pp. 2957\u20132962.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Concurrent plan recognition and execution for human-robot teams.", "author": ["S.J. Levine", "B.C. Williams"], "venue": "ICAPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Plan explainability and predictability for cobots", "author": ["Y. Zhang", "S. Sreedharan", "A. Kulkarni", "T. Chakraborti", "S.K. Hankz Hankui Zhuo"], "venue": "CoRR, vol. abs/1511.08158, 2015. [Online]. Available: http://arxiv.org/abs/1511.08158", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of socially interactive robots", "author": ["T.W. Fong", "I. Nourbakhsh", "K. Dautenhahn"], "venue": "Robotics and Autonomous Systems, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Cost-based anticipatory action selection for human\u2013robot fluency", "author": ["G. Hoffman", "C. Breazeal"], "venue": "Robotics, IEEE Transactions on, vol. 23, no. 5, pp. 952\u2013961, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Capability models and their applications in planning", "author": ["Y. Zhang", "S. Sreedharan", "S. Kambhampati"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2015, pp. 1151\u20131159.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Measuring plan diversity: Pathologies in existing approaches and a new plan distance metric.", "author": ["R.P. Goldman", "U. Kuter"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Google\u2019s driverless cars run into problem: Cars with drivers", "author": ["M. Richtel", "C. Dougherty"], "venue": "The New York Times, vol. 9, p. 1, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "from the robot can raise several concerns: it may increase cognitive load, hamper the productivity of the team, and result in safety concerns and distrust towards the robot [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "In this paper, we represent both models in PDDL [2], but they can differ in terms of their action representations, preconditions, effects, and costs.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "To answer the second question, we explore the relationship between three existing plan distance measures: action set, causal link set and state sequence distances [3], [4] and the plan explicability distance.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "To address the third question, we integrate the explicablity distance in the search process of the Fast-Downward planner [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Various plan recognition algorithms [6], [7] can be applied to perform", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Various plan recognition algorithms [6], [7] can be applied to perform", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 220, "endOffset": 224}, {"referenceID": 9, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 10, "context": "There is also work on performing simultaneous plan recognition and generation [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The motivation for generating explicable task plans was first provided in our recent paper [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 219, "endOffset": 223}, {"referenceID": 14, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "There also exists learnable models that do not assume completeness in the first place [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Another note is that in [13], [14] and this work, since the model is one level deeper, which is about the robot model from the humans perspective, learning methods are adopted.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Another note is that in [13], [14] and this work, since the model is one level deeper, which is about the robot model from the humans perspective, learning methods are adopted.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "We now look at the three plan distance measures introduced in [3] and later refined in [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Although these distance metrics do not satisfy certain mathematical properties [18], they provide a good domain independent measure of the difference between any two plans.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Given the action sets A(\u03c0) and A(\u03c0) of two plans \u03c0 and \u03c0 respectively, the action distance, \u03b4a, is computed as the ratio of the actions that are exclusive to each plan to all the actions in the plans [4].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "The length of the sequences may differ and therefore there are multiple ways to define this distance measure [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "We implement this search in the Fast-Downward planner [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 17, "context": "In the recent past, Google\u2019s self-driving cars [19] have been in the news for being \u201ctoo safe\u201d on the roads.", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "In order for robots to be integrated effectively into human work-flows, it is not enough to address the question of autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task plans without such considerations, they may often demonstrate what we refer to as inexplicable behavior from the point of view of humans who may be observing it. This problem arises due to the human observer\u2019s partial or inaccurate understanding of the robot\u2019s deliberative process and/or the model (i.e. capabilities of the robot) that informs it. This may have serious implications on the human-robot work-space, from increased cognitive load and reduced trust in the robot from the human, to more serious concerns of safety in human-robot interactions. In this paper, we propose to address this issue by learning a distance function that can accurately model the notion of explicability, and develop an anytime search algorithm that can use this measure in its search process to come up with progressively explicable plans. As the first step, robot plans are evaluated by human subjects based on how explicable they perceive the plan to be, and a scoring function called explicability distance based on the different plan distance measures is learned. We then use this explicability distance as a heuristic to guide our search in order to generate explicable robot plans, by minimizing the plan distances between the robot\u2019s plan and the human\u2019s expected plans. We conduct our experiments in a toy autonomous car domain, and provide empirical evaluations that demonstrate the usefulness of the approach in making the planning process of an autonomous agent conform to human expectations.", "creator": "TeX"}}}