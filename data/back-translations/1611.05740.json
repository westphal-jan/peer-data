{"id": "1611.05740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity", "abstract": "We present two new non-parametric statistical hypotheses tests: the first test, called the relative dependence test, allows us to determine whether a source variable is much more dependent on a first target variable or a second one. Dependence is measured using the Hilbert Schmidt Independence Criterion (HSIC); the second test, called the relative similarity test, is used to determine which of the two samples from arbitrary distributions comes much closer to a reference sample of interest and the relative degree of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we use as test statistics the difference between HSIC statistics and MMD statistics. The resulting tests are consistent and unbiased, and exhibit favorable convergence characteristics. The effectiveness of the relative dependence test is demonstrated using several real-world problems: We identify language groups from a multilingual and multilingual model, and we show that multilingual tumor locality problems are more pronounced.", "histories": [["v1", "Thu, 17 Nov 2016 15:36:31 GMT  (2676kb,D)", "http://arxiv.org/abs/1611.05740v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wacha bounliphone", "eugene belilovsky", "arthur tenenhaus", "ioannis antonoglou", "arthur gretton", "matthew b blashcko"], "accepted": false, "id": "1611.05740"}, "pdf": {"name": "1611.05740.pdf", "metadata": {"source": "CRF", "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity", "authors": ["Wacha Bounliphone", "Eugene Belilovsky", "Arthur Tenenhaus", "Ioannis Antonoglou", "Arthur Gretton", "Matthew B. Blaschko"], "emails": ["wacha.bounliphone@centralesupelec.fr", "eugene.belilovsky@inria.fr", "arthur.tenenhaus@centralesupelec.fr", "ioannisa@google.com", "arthur.gretton@gmail.com", "matthew.blaschko@esat.kuleuven.be"], "sections": [{"heading": null, "text": "We introduce two novel non-parametric statistical hypothesis tests. The first test, called the relative test of dependency, enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC). The second test, called the relative test of similarity, is use to determine which of the two samples from arbitrary distributions is significantly closer to a reference sample of interest and the relative measure of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we have used as our test statistics the difference of HSIC statistics and of MMD statistics, respectively. The resulting tests are consistent and unbiased, and (being based on U -statistics) have favorable convergence properties. The effectiveness of the relative dependency test is demonstrated on several real-world problems: we identify languages groups from a multilingual parallel corpus, and we show that tumor location is more dependent on gene expression\n\u2217. These authors contributed equally.\nc\u00a90 Bounliphone, et al..\nar X\niv :1\nthan chromosome imbalance. We also demonstrate the performance of the relative test of similarity over a broad selection of model comparisons problems in deep generative models. Open source implementations of the tests developed here are available for download from https://github.com/wbounliphone/reldep and https://github.com/eugenium/MMD."}, {"heading": "1. Introduction", "text": "This article is based upon and extends Bounliphone et al. (2015, 2016). We address two related problems using analogous tools based on estimating correlated U -statistics for dependency and similarity in a non-parametric setting.\nThe first problem (called the relative dependency test) is to compare multiple dependencies to determine which of two variables most strongly influences the third, by proposing a statistical test of the null hypothesis that a source variable is more dependent to a first target variable against the alternative hypothesis that a source variable is more dependent to a second target variable. Much recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-Euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sze\u0301kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996). For many problems in data analysis, however, the question of whether dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest. For the dependence measure, we use as our test statistic between each of the target and the source is computing using the Hilbert-Schmidt Independent Criterion (HSIC) (Gretton et al., 2005b, 2008) which is the distance between embeddings of the joint distribution and the product of the marginals in a reproducing kernel Hilbert space (RKHS). When the RKHSs are characteristic, the variables are independent iff HSIC = 0.\nThe second problem (called the relative similarity test) is to compare samples from three probability distributions by proposing a statistical test of the null hypothesis that a first candidate probability distribution is closer to a reference probability distribution against the alternative hypothesis that the second candidate probability distribution is closer. We have developed an application of this test to model selection for generative models.\nGenerative models based on deep learning techniques aim to provide sophisticated and accurate models of data, without expensive manual annotation. This is especially of interest as deep networks tend to require comparatively large training samples to achieve a good result. Model selection within this class of techniques can be a challenge, however. First, likelihoods can be difficult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015). The current best method to evaluate such models is based on Parzen-window estimates of the log likelihood (Goodfellow et al., 2014, Section 5). Second, if we are given two models with similar likelihoods, we\ntypically do not have a computationally inexpensive hypothesis test to determine whether one likelihood is significantly higher than the other. Permutation testing or other generic strategies are often computationally prohibitive, bearing in mind the relatively high computational requirements of deep networks (Krizhevsky et al., 2012). So in this work, we provide an alternative strategy for model selection, based on our non-parametric hypothesis test of relative similarity. We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set. For the metric on the space of probability distribution, we use as our test statistic the difference of two the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006, 2012a), where MMD is the difference between mean embeddings of the distributions in a RKHS. When the RKHS is equal to the unit ball in a characteristic RKHS, the probability measures are equal iff MMD = 0. For both tests, care must be taken in analyzing the asymptotic behavior of the test statistics, since the measure of dependence and the measure of similarity will themselves be correlated: they are both computed with respect to the same source. Thus, we derive the joint asymptotic distribution of both dependencies and similarities. The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (2009) to determine the asymptotic joint distributions of the statistics (see Theorems 5 & 11). Consequently, we derive the lowest variance unbiased estimator of the test statistic. We prove our approach to have greater statistical power than constructing two uncorrelated statistics on the same data by subsampling, and testing on these. Our paper is structured as follows. In Section 2, we formalize the two proposed problems and introduce the Maximum Mean Discrepancy (MMD) and the Hilbert-Schmidt Independent Criterion (HSIC). We provide unbiased estimators, as well as asymptotic distributions based on the theory of U -statistics. In Section 3 and Section 4 we derive the joint asymptotic distribution of the correlated HSICs and the correlated MMD and prove that our approach is strictly more powerful than a test that does not exploit the covariance between the correlated statistics. Finally, in Section 6, we demonstrate the performance of the relative test of dependency on problems from multilingual corpus and neurosciences and we demonstrate the performance of the relative test of similarity in different scenarios where a pair of model output is compare to a validation set over a range of training regimes and settings."}, {"heading": "2. Motivation and Background Material", "text": "In this section, we begin with a formal definition of the two problems and introduce essential background knowledge necessary for the development of our later theory. Our goal is to formulate a statistical test that answers the following questions: Let x, y and z be a random variables defined on a topological space X \u00d7 Y \u00d7 Z, with respective Borel probability measures Px, Py and Pz. Given observations Xm := {x1, ..., xm}, Ym := {y1, ..., ym} and Zm := {z1, ..., zm} such that (xi, yi, zi) are independent and identically distributed (i.i.d.) from Px \u00d7 Py \u00d7 Pz.\nProblem 1 (Relative dependency test) Is the dependency between x and y stronger than the dependency between x and z ?\nGiven observations Xm := {x1, ..., xm}, Ym := {y1, ..., ym} and Zm := {z1, ..., zm} sampled i.i.d. from Px, Py, and Pz, respectively.\nProblem 2 (Relative similarity test) Is the probability measure Px closer to Pz or to Py ?\nIn this section, we begin with a formal definition of the two problems and introduce essential background knowledge necessary for the development of our later theory. To start with, we want to determine an underlying notion of a measure for similarity and dependence. We will first explain a framework for distribution analysis via the notion of kernel mean embedding in Section 2.1, and based on this kernel mean embedding approach, we present in Section 2.2, our statistic for the relative test of dependence, the HilbertSchmidt Independence Criterion (HSIC) and our statistic for the relative test of similarity: the maximum mean discrepancy (MMD)."}, {"heading": "2.1 Kernel Mean Embedding of Distributions", "text": "This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures. The kernel mean embedding has been used to define metrics for probability distributions which is important for many problems in statistics and machine learning. First, we briefly review the properties of the Reproducing Kernel Hilbert-space (RKHS) (Aronszajn, 1950). A RKHS H with a reproducing kernel k(x, y) is a Hilbert space of functions f : X \u2192 R with inner product \u3008\u00b7, \u00b7\u3009H. Its element k(x, \u00b7) satisfies the reproducing property: \u3008f, k(x, \u00b7)\u3009H = f(x) for any f \u2208 H and consequently, \u3008k(x, \u00b7), k(y, \u00b7)\u3009H = k(x, y). We define the feature map \u03c6 : X \u2192 H by \u03c6(x) = k(x, \u00b7) and using the reproducing property, we obtain that k(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009H. We extend the notion of feature map to the mean embedding of probability measure : Suppose that a space P(X ) consists of all Borel probability measures P on some input space X . we define the mean embedding \u00b5 of P associated with a reproduction kernel k by a mapping \u00b5 : P(X ) \u2192 H, by \u00b5P = EX\u223cP [k(x, \u00b7)] = \u222b X k(\u00b7, x)dP(x). The distribution P is mapped to its expected feature map, i.e., to a point in a potentially infinite-dimensional and implicit feature space. The mean embedding \u00b5 has the property that Ex\u223cP [f(X)] = \u3008\u00b5P, f\u3009H for any f \u2208 H. The notion of universal kernels and characteristic kernels are essential to the study of kernel mean embeddings (Fukumizu et al., 2008). The kernel k is said to be universal if the corresponding RKHS H is dense in the space of bounded continuous functions on X (Steinwart, 2002). It was shown that for a universal kernel k, \u2016\u00b5P\u2212\u00b5Q\u2016H iff P = Q, i.e. the map \u00b5 is injective. The kernel k is said to be characteristic if the map \u00b5 is injective and the RKHS H is said to be characteristic if its reproducing kernel is characteristic. This notion was introduced by Fukumizu et al. (2008) and it was shown that Gaussian and Laplacian kernels are characteristic on Rd. Furthermore, the notion of mean embedding can be generalized to joint distributions of two variables using tensor product feature spaces. Let (x, y) be random variables on X \u00d7 Y and F be a RKHS with measurable kernel k on X and H be a RKHS with measurable\nkernel l on Y. We assume that Ex[k(x, x)] < \u221e and Ey[l(y, y\u2032)] < \u221e, the cross-covariance operator (see Baker (1973) and Fukumizu et al. (2004)) Cyx : H \u2192 F is defined as Cyx := Eyx [\u03c6(y)\u2297 \u03c6(x)]\u2212\u00b5Px\u2297\u00b5Py = \u00b5Pyx\u2212\u00b5Py\u2297\u00b5Px . The unique bounded operator Cyx satisfies \u3008g, Cyxf\u3009 = Cov[f(x), g(y)] for all f \u2208 F and g \u2208 H."}, {"heading": "2.2 The Maximum Mean Discrepancy and the Hilbert-Schmidt Independence Criterion", "text": "In this section, we give a formal definition of the Maximum Mean Discrepancy (MMD) and the Hilbert-Schmidt Independence Criterion (HSIC).\nDefinition 1 Let Px and Py be the marginal distributions on domains X and Y and let F be a unit ball in a characteristic RKHS H, with the continuous feature mapping \u03c6(x) \u2208 F from each x \u2208 X , such that the inner product between the features is given by the positive definite kernel function k(x, x\u2032) := \u3008\u03c6(x), \u03c6(x\u2032)\u3009. We denote the expectation of \u03c6(x) by \u00b5P := EP[\u03c6(x)] and we define the maximum mean discrepancy in a RKHS F as\nMMD [F ,Px,Py] = \u2016\u00b5Px \u2212 \u00b5Py\u2016H. (1)\nGiven x and x\u2032 independent random variables with distribution Px, and y and y\u2032 independent variables with distribution Py, the population MMD2 can be expressed in terms of expectations of kernel functions k\nMMD2 [F ,Px,Py] = Ex,x\u2032 [ k(x, x\u2032) ] \u2212 2Ex,y [k(x, y)] + Ey,y\u2032 [ k(y, y\u2032) ] . (2)\nThe following theorem describes an unbiased quadratic-time estimate of the MMD, and its asymptotic distribution when Px and Py are different.\nTheorem 2 Given observations Xm := {x1, ..., xm} and Yn := {y1, ..., yn} i.i.d. respectively from Px and Py, an unbiased empirical estimate of MMD2 [F ,Px,Py] is a sum of two U - statistics and a sample average\nMMD2u [F , Xm, Yn] = 1\nm(m\u2212 1) m\u2211 i=1 m\u2211 j 6=i k(xi, xj) + 1 n(n\u2212 1) n\u2211 i=1 n\u2211 j 6=i k(yi, yj) (3)\n\u2212 2 mn m\u2211 i=1 n\u2211 j=1 k(xi, yj).\nLet V := (v1, ..., vm) be m i.i.d. random variables, where v := (x, y) \u223c Px \u00d7 Py. When m = n, an unbiased empirical estimate of MMD2 [F ,Px,Py] is\nMMD2u [F , Xm, Ym] = 1\nm(m\u2212 1) m\u2211 i 6=j f(vi, vj) (4)\nwith f(vi, vj) = k(xi, xj) + k(yi, yj)\u2212 k(xi, yj)\u2212 k(xj , yi). We assume that E(f2) <\u221e. When Px 6= Py, as m \u2192 \u221e, MMD2u [F , X, Y ] converges in distribution to a Gaussian according to\nm1/2 ( MMD2u [F , Xm, Ym]\u2212MMD2 [F ,Px,Py] ) \u2212\u2192 N ( 0, \u03c32XY ) (5)\nwhere\n\u03c32XY = 4 ( Ev1 [(Ev2f(v1, v2))2]\u2212 [(Ev1,v2f(v1, v2))2] ) (6)\nuniformly at rate 1/ \u221a m.\nMMD determines if two samples are from different distributions: if F is a unit ball in a universal RKHS H, defined on the compact metric space X with associated kernel k(., .), then MMD2 [F ,Px,Py] = 0 if and only if Px = Py (Gretton et al., 2012a). We now present HSIC.\nDefinition 3 Let Pxy be a Borel probability measure over (X \u00d7Y,\u0393\u00d7\u039b) with \u0393 and \u039b the respective Borel sets on X and Y. Let F and G be separable RKHSs with the continuous feature mapping \u03c6(x) \u2208 F from each x \u2208 X , such that the inner product between the features is given by the kernel function k(x, x\u2032) := \u3008\u03c6(x), \u03c6(x\u2032)\u3009 and \u03d5(x) \u2208 G from each y \u2208 Y, such that l(y, y\u2032) := \u3008\u03d5(y), \u03d5(y\u2032)\u3009. When the kernels k and l are respectively associated uniquely and bounded on X and Y, the Hilbert-Schmidt Independence Criterion (HSIC) is defined as as the squared HS-norm of the associated cross-covariance operator Cyx. When the kernels k, l are associated uniquely withs respective RKHSs F and G and bounded, the population HSIC can be expressed in terms of expectations of kernel functions\nHSIC [F ,G,Pxy] : = \u2016Cyx\u20162HS = Exx\u2032yy\u2032 [ k(x, x\u2032)l(y, y\u2032) ] + Exx\u2032 [ k(x, x\u2032) ] Eyy\u2032 [ l(y, y\u2032) ] \u2212 2Exy [ Ex\u2032 [k(x, x\u2032)]Ey\u2032 [l(y, y\u2032)] ] . (7)\nThe following theorem describes an unbiased quadratic-time estimate of the HSIC, and its asymptotic property.\nTheorem 4 Given (Xm, Ym) = {(x1, y1), ..., (xm, ym)} of size m drawn i.i.d. from Pxy. An unbiased estimator HSICu [F ,G, (Xm, Ym)] is given by\nHSICu [F ,G, (Xm, Ym)] = 1\nm(m\u2212 3)\n[ Tr(K\u0303L\u0303) +\n1\u2032K\u030311\u2032L\u03031\n(m\u2212 1)(m\u2212 2) \u2212 2 m\u2212 2 1\u2032K\u0303L\u03031\n] (8)\nwhere 1 is the vector of all ones and K\u0303 and L\u0303 \u2208 Rm\u00d7m are kernel matrices related to K and L by K\u0303ij = (1\u2212 \u03b4ij)k(xi, xj) and L\u0303ij = (1\u2212 \u03b4ij)l(yi, yj). This finite sample unbiased estimator of HSIC [F ,G,Pxy] can be written as a U-statistic,\nHSICu [F ,G, (Xm, Ym)] = (m)\u221214 \u2211\n(i,j,q,r)\u2208im4\nhijqr (9)\nwhere (m)4 := m!\n(m\u2212 4)! , the index set im4 denotes the set of all 4\u2212tuples drawn without\nreplacement from the set {1, . . .m}, and the kernel h of the U-statistic is defined as\nhijqr = 1\n24 (i,j,q,r)\u2211 (s,t,u,v) kst(lst + luv \u2212 2lsu) (10)\nWe assume that E[h2] <\u221e. When Pxy 6= PxPy, as m\u2192\u221e,\nm1/2 (HSIC [F ,G,Pxy]\u2212HSICu [F ,G, (Xm, Ym)]) \u2212\u2192 N (0, \u03c32XY ) (11)\nwhere\n\u03c32XY = 16 ( Exi ( Exj ,xq ,xrhijqr )2 \u2212HSIC [F ,G,Pxy]) . (12) Its empirical estimate is \u03c3\u0302XY = 16 ( RXY \u2212 (HSICu [F ,G, (Xm, Ym)])2 ) where RXY =\n1\nm m\u2211 i=1 (m\u2212 1)\u221213 \u2211 (j,q,r)\u2208im3 \\{i} hijqr 2 and the index set im3 \\ {i} denotes the set of all 3\u2212tuples drawn without replacement from the set {1, . . .m} \\ {i}.\nHSIC determines independence: HSIC [F ,G,Pxy] = 0 if and only if Pxy = PxPy when kernels k and l are characteristic on their respective marginal domains (Gretton et al., 2006)."}, {"heading": "2.3 Statistical Hypothesis Testing", "text": "Having defined the two tests statistics (the HSIC [F ,G,Pxy], as a dependence measure and the MMD [F ,Px,Py], as a similarity measure on probability), we address the problems Pb. 1 and Pb. 2 as statistical hypothesis tests. We briefly described the framework of statistical hypothesis testing (Lehmann et al., 1986) as it applies in this context.\n(Pb. 1) - Relative HSIC We denote by (Xm, Ym, Zm) the joint sample of observations drawn i.i.d. with Borel probability measure Pxyz defined on the domain X \u00d7Y\u00d7Z and the kernels k, l and d associated uniquely with RKHSs F , G, and H, respectively. The statistical relative independence test THSIC : Xm\u00d7Xm\u00d7Xm 7\u2192 {0, 1} is use to test the null hypothesis HHSIC0 : HSIC [F ,G,Pxy] \u2264 HSIC [F ,H,Pxz] versus the alternative hypothesis HHSIC1 : HSIC [F ,G,Pxy] > HSIC [F ,H,Pxz] at a given significance level \u03b1.\n(Pb. 2) - Relative MMD We denote the observationsXm := {x1, ..., xm}, Ym := {y1, ..., ym} and Zm := {z1, ..., zm} i.i.d. with respective Borel probability measures Px, Py and Pz defined on X and the kernel k associated uniquely with the separable RKHS F . The statistical relative similarity test TMMD : Xm\u00d7Xm\u00d7Xm 7\u2192 {0, 1} is used to test the null hypothesis HMMD0 : MMD [F ,Px,Py] \u2264 MMD [F ,Px,Pz] versus the alternative hypothesis HMMD1 : MMD [F ,Px,Py] > MMD [F ,Px,Pz] at a given significance level \u03b1.\nThe tests are constructed by comparing the test statistics, in our case respectively the difference of the two empirical unbiased estimates HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)] or MMD2u [F , Xm, Ym] \u2212 MMD2u [F , Xm, Zm], with a particular threshold: if the threshold is exceeded, then the test rejects the null hypothesis at a given significance level \u03b1. The approach that we adopt here is to derive respectively the joint asymptotic distribu-\ntion of the empirical estimate of [ HSICu [F ,G, (Xm, Ym)] , HSICu [F ,H, (Xm, Zm)] ]T\nand[ MMD2u [F , Xm, Ym] , MMD2u [F , Xm, Zm] ]T ."}, {"heading": "3. A relative test of dependency", "text": "In this Section, we calculate two dependent HSIC statistics and derive the joint asymptotic distribution of these dependent quantities, which is used to construct a consistent test for the Relative HSIC problem (Pb. 1). We next construct a simpler consistent test, by computing two independent HSIC statistics on sample subsets. While the simpler strategy is superficially attractive and slightly less effort to implement, we prove the dependent strategy is strictly more powerful."}, {"heading": "3.1 Joint asymptotic distribution of HSIC and test", "text": "Given observations Xm := {x1, ..., xm}, Ym := {y1, ..., ym} and Zm := {z1, ..., zm} i.i.d. from Px, Py and Pz respectively, We denote by k, l and d the kernels associated uniquely with respective reproducing kernel Hilbert spaces F , G and H. Moreover, K, L and D \u2208 Rm\u00d7m are kernel matrices containing kij = k(xi, xj), lij = l(yi, yj) and dij = d(zi, zj). Let HSICu [F ,G, (Xm, Ym)] and HSICu [F ,H, (Xm, Zm)] be respectively the unbiased estimators of HSIC [F ,G,Pxy] and HSIC [F ,H,Pxz], written as a sum of U -statistics with respective kernels hijqr and gijqr as described in Eq. (10),\nhijqr = 1\n24 (i,j,q,r)\u2211 (s,t,u,v) kst(lst + luv \u2212 2lsu), gijqr = 1 24 (i,j,q,r)\u2211 (s,t,u,v) kst(dst + duv \u2212 2dsu). (13)\nTheorem 5 (Joint asymptotic distribution of HSIC) If E[h2] < \u221e and E[g2] < \u221e, then as m\u2192\u221e,\nm1/2 ((\nHSICu [F ,G, (Xm, Ym)] HSICu [F ,H, (Xm, Zm)]\n) \u2212 (\nHSIC [F ,G,Pxy] HSIC [F ,H,Pxz]\n)) d\u2212\u2192 N (( 0 0 ) , ( \u03c32XY \u03c3XYXZ \u03c3XYXZ \u03c3 2 XZ )) ,\n(14)\nwhere \u03c32XY and \u03c3 2 XZ are as in Eq. (12). The empirical estimate of \u03c3XYXZ is \u03c3\u0302XYXZ = 16 m (RXYXZ \u2212HSICu [F ,G, (Xm, Ym)] HSICu [F ,H, (Xm, Zm)]), where\nRXYXZ = 1\nm m\u2211 i=1 (m\u2212 1)\u221223 \u2211 (j,q,r)\u2208im3 \\{i} hijqrgijqr  . (15) Proof Eq. (15) is constructed with the definition of variance of a U-statistic as given by Serfling (2009, Ch. 5) where one variable is fixed. Eq. (14) follows from the application of Hoeffding (1963, Theorem 7.1), which gives the joint asymptotic distribution of U-statistics.\nBased on the joint asymptotic distribution of HSIC described in Theorem 5, we can now describe a statistical test to solve Pb. 1 described in Section 2.3. This is achieved by projecting the distribution to 1D using the statistic HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)], and determining where the statistic falls relative to a conservative estimate of the the 1\u2212\u03b1 quantile of the null HHSIC0 . We now derive this conservative estimate. A simple way of\nachieving this is to rotate the distribution by \u03c04 counter-clockwise about the origin, and to integrate the resulting distribution projected onto the first axis (cf. Fig. 3). Let denote the asymptotically normal distribution ofm1/2 HSICu [F ,G, (Xm, Ym)] HSICu [F ,H, (Xm, Zm)]]T as N (\u00b5,\u03a3). The distribution resulting from rotation and projection is\nN ( [Q\u00b5]1, [Q\u03a3Q T ]11 ) , (16)\nwhere Q =\n\u221a 2\n2 ( 1 \u22121 1 1 ) is the rotation matrix by \u03c04 and\n[Q\u00b5]1 =\n\u221a 2\n2 (HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)]) , (17)\n[Q\u03a3QT ]11 = 1\n2 (\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ). (18)\nFollowing the empirical distribution from Eq. (16), a test with statistic HSICu [F ,G, (Xm, Ym)]\u2212 HSICu [F ,H, (Xm, Zm)] has p-value\np \u2264 1\u2212\u03a6 (HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)])\u221a \u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ  , (19) where \u03a6 is the CDF of a standard normal distribution, and we have made the most conservative possible assumption that HSIC [F ,G,Pxy] \u2212 HSIC [F ,H,Pxz] = 0 under the null (the null also allows for the difference in population dependence measures to be negative). To implement the test in practice, the variances of \u03c32XY , \u03c3 2 XZ and \u03c3 2 XYXZ may be replaced by their empirical estimates. The test will still be consistent for a large enough sample size, since the estimates will be sufficiently well converged to ensure the test is calibrated. Eq. (15) is expensive to compute na\u0308\u0131vely, because even computing the kernels hijqr and gijqr of the U -statistic itself is a non trivial task. Following Song et al. (2012, Section 2.5), we first form a vector hXY with entries corresponding to \u2211 (j,q,r)\u2208im3 \\{i} hijqr, and a vector\nhXZ with entries corresponding to \u2211\n(j,q,r)\u2208im3 \\{i} gijqr. Collecting terms in Eq. (13) related\nto kernel matrices K\u0303 and L\u0303, hXY can be written as hXY = (m\u2212 2)2 ( K\u0303 L\u0303 ) 1\u2212m(K\u03031) (L\u03031) (20)\n+ (m\u2212 2) ( (Tr(K\u0303L\u0303))1\u2212 K\u0303(L\u03031)\u2212 L\u0303(K\u03031) )\n+ (1T L\u03031)K\u03031 + (1T K\u03031)L\u03031\u2212 ((1T K\u0303)(L\u03031))1\nwhere denotes the Hadamard product. Then RXYXZ in Eq. (15) can be computed as RXYXZ = (4m) \u22121(m \u2212 1)\u221223 hXY ThXZ. Using the order of operations implied by the parentheses in Eq. (20), the computational cost of the cross covariance term is O(m2). Combining this with the unbiased estimator of HSIC in Eq. (8) leads to a final computational complexity of O(m2). Code for performing the test is available at https://github.com/ wbounliphone/reldep. In addition to the asymptotic consistency result, we provide a finite sample bound on the deviation between the difference of two population HSIC statistics and the difference of two empirical HSIC estimates.\nTheorem 6 (Generalization bound on the difference of empirical HSIC statistics) Assume that k, l, and d are bounded almost everywhere by 1, and are non-negative. Then for m > 1 and all \u03b4 > 0 with probability at least 1\u2212\u03b4, for all Pxy and Pxz, the generalization bound on the difference of empirical HSIC statistics is\u2223\u2223 (HSIC [F ,G,Pxy]\u2212HSIC [F ,H,Pxz])\u2212 (HSICu [F ,G, (Xm, Ym)] HSICu [F ,H, (Xm, Zm)]) \u2223\u2223\n\u2264 2\n{\u221a log(6/\u03b4)\n\u03b12m + C m\n} (21)\nwhere \u03b1 > 0.24 and C are constants.\nProof In Gretton et al. (2005a) a finite sample bound is given for a single HSIC statistic. Eq. (21) is proved by using a union bound:\u2223\u2223 {HSIC [F ,G,Pxy]\u2212HSIC [F ,H,Pxz]} \u2212 {HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)]} \u2223\u2223 = \u2223\u2223 {HSIC(F ,G, Pxy)\u2212HSICu [F ,G, (Xm, Ym)]}+ {HSICu [F ,H, (Xm, Zm)]\u2212HSIC(F ,H, Pxz)} \u2223\u2223\n\u2264 \u2223\u2223HSIC(F ,G, Pxy)\u2212HSICu [F ,G, (Xm, Ym)] \u2223\u2223+ \u2223\u2223HSICu [F ,H, (Xm, Zm)]\u2212HSIC(F ,H, Pxz)\u2223\u2223\n\u2264 2\n{\u221a log(6/\u03b4)\n\u03b12m + C m\n}\nCorollary 7 HSICu [F ,G, (Xm, Ym)]\u2212HSICu [F ,H, (Xm, Zm)] converges to the population statistic at rate O(m1/2)."}, {"heading": "3.2 A simple consistent test via uncorrelated HSICs", "text": "From the result in Eq. (11), a simple, consistent test of relative dependence can be constructed as follows: split the samples from Px into two equal sized sets denoted by X \u2032 and X \u2032\u2032, and drop the second half of the sample pairs with Y and the first half of the sample pairs with Z. We will denote the remaining samples as Y \u2032 and Z \u2032\u2032. We can now estimate the joint distribution of m1/2[HSICu [ F ,G, ( Xm/2, Ym/2 )] ,HSICu [ F ,H, ( Xm/2, Zm/2 )] ]T as\nN ((\nHSIC [F ,G,Pxy] HSIC [F ,H,Pxz]\n) , ( \u03c32X\u2032Y \u2032 0\n0 \u03c32X\u2032\u2032Z\u2032\u2032\n)) , (22)\nwhich we will write as N (\u00b5\u2032,\u03a3\u2032). Given this joint distribution, we need to determine the distribution over the half space defined by HSIC [F ,G,Pxy] < HSIC [F ,H,Pxz] . As in the previous section, we achieve this by rotating the distribution by \u03c04 counter-clockwise about the origin, and integrating the resulting distribution projected onto the first axis (cf. Fig. 3). The resulting projection of the rotated distribution onto the primary axis is\nN ([ Q\u00b5\u2032 ] 1 , [ Q\u03a3\u2032QT ] 11 ) (23)\nwhere\n[Q\u00b5\u2032]1 =\n\u221a 2\n2 (HSIC [F ,G,Pxy]\u2212HSIC [F ,H,Pxz]) , [Q\u03a3\u2032QT ]11 =\n1 2 (\u03c32X\u2032Y \u2032 + \u03c3 2 X\u2032\u2032Z\u2032\u2032).\n(24)\nFrom this empirically estimated distribution, it is straightforward to construct a consistent test (cf. Eq. (19)). The power of this test varies inversely with the variance of the distribution in Eq. (23)."}, {"heading": "3.3 The dependent test is more powerful", "text": "While discarding half the samples leads to a consistent test, we might expect some loss of power over the approach in Section 3.1, due to the increase in variance with lower sample size. In this section, we prove the Section 3.1 test is more powerful than that of Section 3.2, regardless of Pxy and Pxz. We call the simple and consistent approach in Section 3.2, the independent approach, and the lower variance approach in Section 3.1, the dependent approach. The following theorem compares these approaches.\nTheorem 8 The asymptotic relative efficiency (ARE) of the independent approach relative to the dependent approach is always greater than 1.\nRemark 9 The asymptotic relative efficiency is defined in e.g. Serfling (2009, Chap. 5, Section 1.15.4). If mA and mB are the sample sizes at which tests \u201dperform equivalently\u201d (i.e. have equal power), then the ratio mAmB represents the relative efficiency. When mA and mB tend to +\u221e and the ratio mAmB \u2192 L (at equivalent performance), then the value L represents the asymptotic relative efficiency of procedure B relative to procedure A. This example is relevant to our case since we are comparing two test statistics with different asymptotically normal distributions.\nThe following lemma is used for the proof of Thm. 8.\nLemma 10 (Lower Variance) The variance of the dependent test statistic is smaller than the variance of the independent test statistic.\nProof From the convergence of moments in the application of the central limit theorem (von Bahr, 1965), we have that \u03c32X\u2032Y \u2032 = 2\u03c3 2 XY . Then the variance summary in Eq. (18) is 1 2(\u03c3 2 XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ) and the variance summary in Eq. (24) is 1 2(2\u03c3 2 XY + 2\u03c3 2 XZ) where\nin both cases the statistic is scaled by \u221a m. We have that the variance of the independent test statistic is smaller than the variance of the dependent test statistic when\n1 2 (\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ) < 1 2 (2\u03c32XY + 2\u03c3 2 XZ)\n\u21d0\u21d2 \u22122\u03c3XYXZ < \u03c32XY + \u03c32XZ (25)\nwhich is implied by the positive definiteness of \u03a3.\nProof [Proof of Thm 8] The Type II error probability of the independent test at level \u03b1 is\n\u03a6 \u03a6\u22121(1\u2212 \u03b1)\u2212 m\u22121/2(HSIC [F ,G,Pxy]\u2212HSIC [F ,H,Pxz] )\u221a \u03c32X\u2032Y \u2032 + \u03c3 2 X\u2032\u2032Z\u2032\u2032  , (26) where we again make the most conservative possible assumption that HSIC [F ,G,Pxy] \u2212 HSIC [F ,H,Pxz] = 0 under the null. The Type II error probability of the dependent test at level \u03b1 is\n\u03a6 \u03a6\u22121(1\u2212 \u03b1)\u2212 m\u22121/2(HSIC [F ,G,Pxy]\u2212HSIC [F ,H,Pxz] )\u221a \u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ  (27) where \u03a6 is the CDF of the standard normal distribution. The numerator in Eq. (26) is the same as the numerator in Eq. (27), and the denominator in Eq. (27) is smaller due to Lemma 10. The lower variance dependent test therefore has higher ARE, i.e., for a sufficient sample size m > \u03c4 for some distribution dependent \u03c4 \u2208 N+, the dependent test will be more powerful than the independent test."}, {"heading": "3.4 Generalizing to more than two HSIC statistics", "text": "The generalization of the dependence test to more than three random variables follows from the earlier derivation by applying successive rotations to a higher dimensional joint Gaussian distribution over multiple HSIC statistics. Given observations X1 := {x11, ..., x1m}, ..., Xn := {xn1 , ..., xnm} i.i.d. from respectively Px1 , ...,Pxn , we denote by f1, ..., fn the kernels associated uniquely with respective reproducing kernel Hilbert spaces F1, ...,Fn. We define a generalized statistical test, Tg : (Xm \u00d7 ... \u00d7 Xm) \u2192 {0, 1} to test the null hypothesis H0 : \u2211\n(x,y)\u2208{1,...,n}2 v(x,y) HSIC [F1, ...,Fn,Px1...xn ] \u2264 0 versus the alternative hypothesis Hn :\u2211 (x,y)\u2208{1,...,n}2 v(x,y) HSIC [F1, ...,Fn,Px1...xn ] > 0, where v is a vector of weights on each HSIC statistic. We may recover the test in the previous section by setting v(1,2) = +1 v(1,3) = \u22121 and v(i,j) = 0 for all (i, j) \u2208 {1, 2, 3}2 \\ {(1, 2), (1, 3)}. The derivation of the test follows the general strategy used in the previous section: we construct a rotation matrix so as to project the joint Gaussian distribution onto the first axis, and read the p-value from a standard normal table. To construct the rotation matrix, we simply need to rotate v such that it is aligned with the first axis. Such a rotation can be computed by composing n 2-dimensional rotation matrices as in Algorithm 1."}, {"heading": "4. A relative test of similarity", "text": "In this section, we derive our statistical test for relative similarity as measured by MMD. In order to maximize the statistical efficiency of the test, we will reuse samples from the reference distribution, denoted by Px, to compute the MMD estimates with two candidate distributions Py and Pz. We consider two MMD estimates MMD2u [F , Xm, Yn] and MMD2u [F , Xm, Zr], and as the data sample Xm is identical between them, these estimates will be correlated. We therefore first derive the joint asymptotic distribution of these two metrics and use this to construct a statistical test.\nAlgorithm 1 Successive rotation for generalized high-dimensional relative tests of dependency (cf. Section 3.4)\nRequire: v \u2208 Rn Ensure: [Qv]i = 0 \u2200i 6= 1, QTQ = I\nQ = I for i = 2 to n do\nQi = I; \u03b8 = \u2212 tan\u22121 vi[Qv]1 [Qi]11 = cos(\u03b8); [Qi]1i = \u2212 sin(\u03b8); [Qi]i1 = sin(\u03b8); [Qi]ii = cos(\u03b8) Q = QiQ\nend for\nTheorem 11 We assume that Px 6= Py, Px 6= Pz, E(k(xi, xj)) <\u221e, E(k(yi, yj)) <\u221e and E(k(xi, yj)) <\u221e, then\nm1/2 ((\nMMD2u [F , Xm, Yn] MMD2u [F , Xm, Zr]\n) \u2212 (\nMMD2 [F ,Px,Py] MMD2 [F ,Px,Pz]\n)) d\u2212\u2192 N (( 0 0 ) , ( \u03c32XY \u03c3XYXZ \u03c3XYXZ \u03c3 2 XZ )) (28)\nWe substitute the kernel MMD definition from Equation (4), expand the terms in the expectation, and determine their empirical estimates in order to compute the variances in practice. The proof and additional details of the following derivations are given in Appendix A. An empirical estimate of \u03c3XYXZ in Eq. (28), neglecting higher order terms, can be computed in O(m2):\n\u03c3\u0302XYXZ \u2248 1\nm(m\u2212 1)2 1T K\u0303xxK\u0303xx1\u2212\n( 1\nm(m\u2212 1) 1T K\u0303xx1\n)2 (29)\n\u2212 (\n1\nm(m\u2212 1)r 1T K\u0303xxKxz1\u2212\n1\nm2(m\u2212 1)r 1T K\u0303xx11 TKxz1 ) \u2212 ( 1\nm(m\u2212 1)n 1T K\u0303xxKxy1\u2212\n1\nm2(m\u2212 1)n 1T K\u0303xx11 TKxz1 ) + ( 1\nmnr 1TKyxKxz1\u2212\n1\nm2nr 1TKxy11 TKxz1 ) where 1 is a vector of all ones, while [ K\u0303xx ] ij = (1 \u2212 \u03b4ij)k(xi, xj), [Kxy]ij = k(xi, yj) and [Kxz]ij = k(xi, zj) refer to the kernel matrices. Similarly, \u03c3 2 XY in Eq. (28) is constructed as in Eq. (6). Based on the empirical distribution from Eq. (28), we now derive the RelativeMMD test in a manner similar than Section 3.1. The test statistic MMD2u [F , Xm, Yn]\u2212MMD2u [F , Xm, Zr] is used to compute the p-value p for the standard normal distribution. The test statistic is obtained by rotating the joint distribution (cf. Eq. (28)) by \u03c04 about the origin, and integrating the resulting projection on the first axis. Denote the asymptotically normal distribution of \u221a m[MMD2u [F , Xm, Yn] ; MMD2u [F , Xm, Zr]]T as N (\u00b5,\u03a3). The resulting distri-\nbution from rotating by \u03c0/4 and projecting onto the primary axis is N ( [R\u00b5]1, [R\u03a3R T ]11 )\nwhere\n[R\u00b5]1 =\n\u221a 2\n2 (MMD2u [F , Xm, Yn]\u2212MMD2u [F , Xm, Zr]) (30)\n[R\u03a3RT ]11 = 1\n2 (\u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ) (31)\nwith R is the rotation by \u03c0/4. Then, the p-values for testing H0 versus H1 are\np \u2264 \u03a6 \u2212MMD2u [F , Xm, Yn]\u2212MMD2u [F , Xm, Zr]\u221a \u03c32XY + \u03c3 2 XZ \u2212 2\u03c3XYXZ  (32) where \u03a6 is the CDF of a standard normal distribution. We have made code for performing the test publicly available. An implementation and examples are available at https:// github.com/eugenium/MMD."}, {"heading": "5. Relation of the HSIC and the Relative MMD", "text": "We propose two new statistical hypothesis tests using equivalent mathematical derivation and using two statistics that are link (as demonstrated in Gretton et al. (2012a, Section 7.3)). The MMD can be expressed as the distance in a RKHS H between mean embeddings as\nMMD [F ,Px,Py] = \u2016\u00b5Px \u2212 \u00b5Py\u2016H. (33)\nLet assume that V is an RKHS over X \u00d7 Y with kernel v ((x, y), (x\u2032, y\u2032)). If x and y are independent, then \u00b5Pxy = \u00b5Px\u00b5Py . Hence we may use \u2016\u00b5Pxy \u2212 \u00b5Px\u00b5Py\u2016V as a measure of dependence. Let assume that v ((x, y), (x\u2032, y\u2032)) = k(x, x\u2032)l(y, y\u2032), then HSIC can be expressed as the distance in V between the mean embedding of the joint distribution and the product of the marginal distributions\nHSIC [F ,G,Pxy] = \u2016\u00b5Pxy \u2212 \u00b5Px\u00b5Py\u2016V . (34)\nAlthough, the application of MMD is related to the HSIC, they lead to very different type of expressions for the variance terms of their asymptotic distributions and this is the same for the Relative MMD and the Relative HSIC. The computation of the p-value for the Relative MMD is done using the computation of the kernel matrices Kxy and Kxz, whereas for the Relative HSIC this is done using Kxx, Kyy and Kzz, showing the need of having two different implementations for each hypothesis statistical test. However, we can compute Kxy as a transformation of Kxx and Kyy such that [Kxy]jj = [Kxx (\u03a0Kyy\u03a0T )]jj where \u03a0 is a permutation matrix. But, because of this permutation matrix, we have that as the sample size n increases, the computation complexity become O(n4) instead of O(n2)."}, {"heading": "6. Experiments", "text": "We analyze the Relative HSIC and the Relative MMD in an extensive set of experiments. The section contains results on synthetics experiments and real data."}, {"heading": "6.1 Experiments for the relative dependency test", "text": "We apply our estimates of statistical dependence to three challenging problems. The first is a synthetic data experiment, in which we can directly control the relative degree of functional dependence between variates. The second experiment uses a multilingual corpus to determine the relative relations between European languages. The last experiment is a 3-block dataset which combines gene expression, comparative genomic hybridization, and a qualitative phenotype measured on a sample of Glioma patients."}, {"heading": "6.1.1 Synthetic experiments", "text": "We constructed 3 distributions as defined in Eq. (35) and illustrated in Fig. 1.\nLet t \u223c U [(0, 2\u03c0)], (35) (a) x1 \u223c t+ \u03b31N (0, 1) y1 \u223c sin(t) + \u03b31N (0, 1) (b) x2 \u223c t cos(t) + \u03b32N (0, 1) y2 \u223c t sin(t) + \u03b32N (0, 1) (c) x3 \u223c t cos(t) + \u03b33N (0, 1) y3 \u223c t sin(t) + \u03b33N (0, 1)\nThese distributions are specified so that we can control the relative degree of functional dependence between the variates by varying the relative size of noise scaling parameters \u03b31, \u03b32 and \u03b33. The question is then whether the dependence between (a) and (b) is larger than the dependence between (a) and (c). In these experiments, we fixed \u03b31 = \u03b32 = 0.3, while we varied \u03b33, and used a Gaussian kernel with bandwidth \u03c3 selected as the median pairwise distance between data points. This kernel is sufficient to obtain good performance, although other choices exist (Gretton et al., 2012b). Fig. 2 shows the power of the dependent and the independent tests as we vary \u03b33. It is clear from these results that the dependent test is far more powerful than the independent test over the great majority of \u03b33 values considered. Fig. 3 demonstrates that this superior test power arises due to the tighter and more concentrated distribution of the dependent statistic."}, {"heading": "6.1.2 Multilingual data", "text": "In this section, we demonstrate dependence testing to predict the relative similarity of different languages. We use a real world dataset taken from the parallel European Parliament\ncorpus Koehn (2005). We choose 3000 random documents in common written in: Finnish (fi), Italian (it), French (fr), Spanish (es), Portuguese (pt), English (en), Dutch (nl), German (de), Danish (da) and Swedish (sv). These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray and Atkinson, 2003). In this dataset, we considered each language as a random variable and each document as an observation.\nOur first goal is to test if the statistical dependence between two languages in the same group is greater than the statistical dependence between languages in different groups. For pre-processing, we removed stop-words (http://www.nltk.org) and performed stemming (http://snowball.tartarus.org). We applied the TF-IDF model as a feature representation and used a Gaussian kernel with the bandwidth \u03c3 set per language as the median pairwise distance between documents.\nIn Tab. 1, a selection of tests between language groups (Germanic, Romance, and Uralic) is given: all p-values strongly support that our relative dependence test finds the different language groups with very high significance.\nFurther, if we focus on the Romance family, our test enables one to answer more fine-grained questions about the relative similarity of languages within the same group. As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community Gray and Atkinson (2003); Bouckaert et al. (2012) as illustrated in Fig. 4 for the Romance group. We have run the test on all triplets from the corpus for which the topology of the tree specifies a correct ordering of the dependencies. In a fraction of a second (excluding kernel computation), we are able to recover certain features of the subtree of relationships between languages present in the Romance language group (Tab. 3). The test always indicates the correct relative similarity of languages when nearby languages (e.g. Portuguese and Spanish: pt, es) are compared with those further away\n(e.g. Portuguese and Danish: pt, da), however errors are sometimes made when comparing triplets of languages for which the nearest common ancestor is more than one link removed.\nIn our next tests, we evaluate our more general framework for testing relative dependencies with more than two HSIC statistics. We chose four languages, and tested whether the average dependence between languages in the same group is higher than the dependence between groups. The results of these tests are in Tab. 2. As before, our test is able to distinguish language groups with high significance."}, {"heading": "6.1.3 Pediatric Glioma Data", "text": "Brain tumors are the most common solid tumors in children and have the highest mortality rate of all pediatric cancers. Despite advances in multimodality therapy, children with pediatric high-grade gliomas (pHGG) invariably have an overall survival of around 20% at 5 years. Depending on their location (e.g. brainstem, central nuclei, or supratentorial), pHGG present different characteristics in terms of radiological appearance, histology, and prognosis. The hypothesis is that pHGG have different genetic origins and oncogenic pathways depending on their location. Thus, the biological processes involved in the development of the tumor may be different from one location to another.\nIn order to evaluate such hypotheses, pre-treatment frozen tumor samples were obtained from 53 children with newly diagnosed pHGG from Necker Enfants Malades (Paris, France) from Puget et al. (2012). The 53 tumors are divided into 3 locations: supratentorial (HEMI), central nuclei (MIDL), and brain stem (DIPG). The final dataset is organized in 3 blocks of variables defined for the 53 tumors: X is a block of indicator variables describing the\nlocation category, the second data matrix Y provides the expression of 15 702 genes (GE). The third data matrix Z contains the imbalances of 1229 segments (CGH) of chromosomes. For X, we use a linear kernel, which is characteristic for indicator variables, and for Y and Z, the kernel was chosen to be the Gaussian kernel with \u03c3 selected as the median of pairwise distances. The p-value of our relative dependency test is < 10\u22125. This shows that the tumor location in the brain is more dependent on gene expression than on chromosomal imbalances. By contrast with Section 6.1.1, the independent test was also able to find the same ordering of dependence, but with a p-value that is three orders of magnitude larger (p = 0.005). Fig. 5 shows iso-curves of the Gaussian distributions estimated in the independent and dependent tests. The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012)."}, {"heading": "6.2 Experiments for the relative similarity test", "text": ""}, {"heading": "6.2.1 Synthetic experiments", "text": "We verify the validity of the hypothesis test described above using a synthetic data set in which we can directly control the relative similarity between distributions. We constructed three Gaussian distributions as illustrated in Fig. 6. These Gaussian distributions are specified with different means so that we can control the degree of relative similarity between them. The question is whether the similarity between X and Z is greater than the similarity between X and Y . In these experiments, we used a Gaussian kernel with bandwidth selected as the median pairwise distance between data points, and we fixed \u00b5Y = [\u221220,\u221220], \u00b5Z = [20, 20] and varied \u00b5X such that \u00b5X = (1 \u2212 \u03b3)\u00b5Y + \u03b3\u00b5Z , for 41 regularly spaced values of \u03b3 \u2208 [0.1, 0.9] (avoiding the degenerate cases Px = Py or Px = Pz). Fig. 8 shows the p-values of the relative similarity test for different distribution. When \u03b3 is varying around 0.5, i.e., when MMD2u [F , X, Y ] is almost equal to MMD2u [F , X, Z],\nthe p-values quickly transition from 1 to 0, indicating strong discrimination of the test. In Fig. 7, we compare the power of our test to the power for both the dependent and independent approaches describes in Section 3.1 and in Section 3.2 respectively. Fig. 9 shows an empirical scatter plot of the pairs of MMD statistics along with a 2\u03c3 iso-curve of the estimated distribution, demonstrating that the parametric Gaussian distribution is well calibrated to the empirical values."}, {"heading": "6.2.2 Model selection for deep unsupervised neural networks", "text": "An important potential application of the RelativeMMD problem (Pb. 2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014). As noted by several authors, the evaluation of generative models is a challenging open problem (Li et al., 2015; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difficult to evaluate. The RelativeMMD performance can be used to compare different model settings, or even model families, in a statistically valid framework. To compare two models using our test, we generate samples from both, and compare these to a set of real target data samples that were not used to train either model.\nIn the experiments in the sequel we focus on the recently introduced variational autoencoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al., 2015). The former trains an encoder and decoder network jointly\nminimizing a regularized variational lower bound (Kingma and Welling, 2014). While the latter class of models is purely generative minimizing an MMD based objective, this model works best when coupled with a separate auto-encoder which reduces the dimensionality of the data. An architectural schematic for both classes of models is provided in Fig. 10. Both these models can be trained using standard backpropagation (Rumelhart et al., 1988). Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009).\nWe use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014). We first demonstrate the effectiveness of our test in a setting where we have a theoretical basis for expecting superiority of one unsupervised model versus another. Specifically, we use a setup where more training samples were used to create one model versus the other. We find that the RelativeMMD framework agrees with the expected results (models trained with more data generalize better). We then demonstrate how the RelativeMMD can be used in evaluating network architecture choices, and we show that our test strongly agrees with other established metrics, but in contrast can provide significance results using just the validation data while other methods may require an additional test set.\nSeveral practical matters must be considered when applying the RelativeMMD test. The selection of kernel can affect the quality of results, particularly more suitable kernels can give a faster convergence. In this work we extend the logic of the median heuristic (Gretton et al., 2012b) for bandwidth selection by computing the median pairwise distance between samples from Px and Py and averaging that with the median pairwise distance between samples from Px and Pz, which helps to maximize the difference between the two MMD statistics. Although the derivations for the variance of our statistic hold for all cases, the estimates require asymptotic arguments and thus a sufficiently large m. Selecting the kernel bandwidth in an appropriate range can therefore substantially increase the power of the test at a fixed sample size. While we observed the median heuristic to work well in our experiments, there are cases where alternative choices of kernel can provide greater power: for instance, the kernel can be chosen to maximize the expected test power on a held-out dataset (Gretton et al., 2012b).\nVariational Auto-Encoder Sample Size and Architecture Experiments We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder and decoder and a latent variable layer as shown in Fig. 10a. We use sigmoidal activation for the hidden layers of encoder and decoder. For the FreyFace data, we use a Gaussian prior on the latent space and data space. For MNIST, we used a Bernoulli prior for the data space. We fix the training set size of the second auto-encoder to 300 images for the FreyFace data and 1500 images for the MNIST data. We vary the number of training samples for the first auto-encoder. We then generate samples from both auto-encoders and compare them using Relative MMD to a held out set of data. We use 1500 FreyFace samples as the target in Relative MMD and 15000 images from MNIST. Since a single sample of the data might lead to better generalization performance by chance, we repeat this experiment multiple times and record whether the relative similarity test indicated a network is preferred or if it failed to reject the null hypothesis. The results are shown in Fig. 11 which demonstrates that we are closely following the expected model preferences. Additionally for MNIST we use\nanother separate set of supervised training and test data. We encode this data using both auto-encoders and use logistic regression to obtain a classification accuracy. The indicated accuracies closely match the results of the relative similarity test, further validating the test. We consider model selection between networks using different architectures. We train two encoders, one a fixed reference model (400 hidden units and 20 latent variables), and the other varying as specified in Tab. 4. 25000 images from the MNIST data set were used for training. We use another 20000 images as the target data in Relative MMD. Finally, we use a set of 10000 training and 10000 test images for a supervised task experiment. We use the labels in the MNIST data and perform training and classification using an `2-regularized logistic regression on the encoded features. In addition we use the supervised task test data to evaluate the variational lower bound of the data under the two models (Kingma and Welling, 2014). We show the result of this experiment in Tab. 4. For each comparison we take a different subset of training data which helps demonstrate the variation in lower bound and accuracy when re-training the reference architecture. We use a significance value of 5% and indicate when the test favors one auto-encoder over another or fails to reject the null hypothesis. We find that Relative MMD evaluation of the models closely matches performance on the supervised task and the test set variational lower bound.\nGenerative Moment Matching Networks Architecture Experiments We demonstrate our hypothesis test on a different class of deep generative models called Generative Moment Matching Networks (GMMN) (Li et al., 2015). This recently introduced model has shown competitive performance in terms of test set likelihood on the MNIST data. Further-\nA cc\nu ra\ncy\n0.3 0.5 0.8 0.9 1.0 1.5 2.0 2.5 3.0 4.0 0.89\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\nEncoder 2 Accuracy Encoder 1 Accuracy Unencoded Accuracy\nRatio Training Samples for Encoder 2 and Training Samples Encoder 1\n(c) Average Accuracy Using Autoencoder Features (25 trials)\nmore the training of this model is based on the MMD criterion. Li et al. (2015) proposes to use that model along with an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is trained on the data to obtain a low dimensional representation, then a GMMN network is trained on the latent representations (Fig. 10).\nWe use the relative similarity test to evaluate various architectural choices in this new class of models. We start from the baseline model specified in Li et al. (2015) and associated software. The details of the reference model are specified in Fig. 10.\nWe vary the number of auto-encoder hidden layers (1 to 4), generative model layers(1, 4, or 5), the number of network nodes (all or 50% of the reference model), and use of drop-out on the auto-encoder. We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015; Goodfellow et al., 2014). In total we train 48 models. We use these to compare 4 simplified binary network architecture choices using the Relative MMD: using dropout on the auto-encoder, few (1) or more (4 or 5) GMMN layers, few (1 or 2) or more (3 or 4) auto-encoder layers, and the number of network nodes. We use our test to compare these model settings using the validation set as the target in the relative similarity test, and samples from the models as the two sources. To validate our results we compare it to likelihoods computed on the test set. The results are shown in Tab. 5. We see that the likelihood results computed on a separate test set follow the conclusions obtained from MMD on the validation set. Particularly, we find that using fewer hidden layers for the GMMN and more hidden nodes generally produces better models."}, {"heading": "6.2.3 Discussion", "text": "In these experiments we have seen that the RelativeMMD test can be used to compare deep generative models obtaining judgments aligned with other metrics. Comparisons to other metrics are important for verifying our test is sensible, but it can occlude the fact that MMD is a valid evaluation technique on its own. When evaluating only sample generating models where likelihood computation is not possible, MMD is an appropriate and tractable\nmetric to consider in addition to Parzen-Window log likelihoods and visual appearance of the samples. In several ways it is potentially more appropriate than Parzen-windows as it allows one to consider directly the discrepancy between the test data samples and the model samples while allowing for significance results. In such a situation, comparing the performance of several models using the MMD against a single set of test samples, the RelativeMMD test can provide an automatic significance value without expensive crossvalidation procedures.\nGaussian kernels are closely related to Parzen-window estimates, thus computing an MMD in this case can be considered related to comparing Parzen window log-likelihoods. The MMD gives several advantages, however. First, the asymptotics of MMD are quite different to Parzen-windows, since the Parzen-window bandwidth shrinks as m grows. Asymptotics of relative tests with shrinking bandwidth are unknown: even for two samples this is challenging (Krishnamurthy et al., 2015). Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002). This is because the tests above rely on graph edge counting or nearest neighbor-type statistics, and null distributions are obtained via combinatorial arguments which are not easily extended from two to three samples. MMD is a U -statistic, hence its asymptotic behavior is much more easily generalized to multiple dependent statistics.\nThere are two primary advantages of the MMD over the variational lower bound, where it is known (Kingma and Welling, 2014): first, we have a characterization of the asymptotic behavior, which allows us to determine when the difference in performance is significant; second, comparing two lower bounds produced from two different models is unreliable, as we do not know how conservative either lower bound is."}, {"heading": "7. Conclusion", "text": "We have described two novel non-parametric statistical hypothesis tests using analogous mathematical derivation based on the estimation of two correlated U -statistics. The first test of relative dependency determines whether a source random variable is significantly more strongly dependent on one target random variable or another. The test is based on\nthe Hilbert-Schmidt Independent Criterion. And the second test of relative similarity determines whether one model generates samples significantly closer to the reference distribution than the other. The criterion is based on the Maximum Mean Discrepancy. We have shown that both test are consistent, are strictly more powerful than a test with uncorrelated statistics, and the computation requirements of the tests is quadratic in the sample size. We have applied the test of relative dependency to the problem of identifying relative dependencies between languages using a multilingual corpus, and for discovering the relative relationships between gliomas and genetic information. Additionally, we have shown the application of relative test of similarity to the problem of model selection in deep generative models, and currently an important question in machine learning. Code for our methods is available."}, {"heading": "Acknowledgments", "text": "This work is funded by Internal Funds KU Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the Newton Alumni Scheme, and DIGITEO 2013-0788D-SOPRANO. WB is supported in part by a CentraleSupe\u0301lec fellowship."}, {"heading": "Appendix A. Detailed Derivations of the MMD Variance and Covariance", "text": "The variance and the covariance for a U -statistic is described in Hoeffding (1948, Eq. 5.13) and Serfling (2009, Chap. 5). Let V := (v1, ..., vm) be m i.i.d. random variables where v := (x, y) \u223c Px\u00d7Py. An unbiased estimator of MMD2 [F ,Px,Py] is\nMMD2u [F , Xm, Ym] = 1\nm(m\u2212 1) m\u2211 i 6=j f(vi, vj) (36)\nwith f(vi, vj) = k(xi, xj) + k(yi, yj)\u2212 k(xi, yj)\u2212 k(xj , yi). Similarly, let W := (w1, ..., wm) be m i.i.d. random variables where w := (x, z) \u223c Px \u00d7 Pz. An unbiased estimator of MMD2 [F ,Px,Pz] is\nMMD2u [F , Xm, Zm] = 1\nm(m\u2212 1) m\u2211 i 6=j g(wi, wj) (37)\nwith g(wi, wj) = k(xi, xj) + k(zi, zj)\u2212 k(xi, zj)\u2212 k(xj , zi) Then the variance/covariance for a U -statistic with a kernel of order 2 is given by\nV ar(MMD2u) = 4(m\u2212 2) m(m\u2212 1) \u03b61 + 2 m(m\u2212 1) \u03b62 (38)\nEq. (38), neglecting higher order terms, can be written as\nV ar(MMD2u) = 4(m\u2212 2) m(m\u2212 1) \u03b61 +O(m\u22122) (39)\nwhere for the variance term, \u03b61 = Var [Ev1 [f(v1, V2)]] and for the covariance term \u03b61 = Var [Ev1,w1 [f(v1, V2)g(w1,W2)]].\nNotation [K\u0303xx]ij = [Kxx]ij for all i 6= j and [K\u0303xx\u2032 ]ij = 0 for j = i. Same for K\u0303yy and K\u0303zz. We will also make use of the fact that k(xi, xj) = \u3008\u03c6(xi), \u03c6(xj)\u3009 for an appropriately chosen inner product, and function \u03c6. We then denote \u00b5x := \u222b \u03c6(x)dPx.\nA.1 Variance of MMD\nWe note that many terms in expansion of the squares above cancel out due to independence. For example Ex1,y1 [\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(x1), \u00b5y\u3009]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009]Ex1 [\u3008\u03c6(x1), \u00b5y\u3009] = 0. We can thus simplify to the following expression for \u03b61\n\u03b61 = Ex1,y1 [ (Ex2,y2 [h(x1, y1)]) 2 ] \u2212 ( MMD2 [F ,PX ,PY ] )2 (40)\n= Ex1,y1 [ (\u3008\u03c6(x1), \u00b5x\u3009+ \u3008\u03c6(y1), \u00b5y\u3009 \u2212 \u3008\u03c6(x1), \u00b5y\u3009 \u2212 \u3008\u00b5x, \u03c6(y1)\u3009)2 ] \u2212 ( MMD2 [F ,PX ,PY ] )2 = Ex1,y1\n[ \u3008\u03c6(x1), \u00b5x\u30092 + 2\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(y1), \u00b5y\u3009 \u2212 2\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(x1), \u00b5y\u3009 \u2212 2\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(y1), \u00b5x\u3009+ \u3008\u03c6(y1), \u00b5y\u30092\n\u2212 2\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(x1), \u00b5y\u3009 \u2212 2\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(y1), \u00b5x\u3009 + \u3008\u03c6(x1), \u00b5y\u30092 + 2\u3008\u03c6(x1), \u00b5y\u3009\u3008\u03c6(y1), \u00b5x\u3009\n+ \u3008\u03c6(y1), \u00b5x\u30092 ] \u2212 ( MMD2 [F ,PX ,PY ] )2 = Ex1 [\u3008\u03c6(x1), \u00b5x\u30092]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009]2\n\u2212 2(Ex1 [\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(x1), \u00b5y\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009] Ex1 [\u3008\u03c6(x1), \u00b5y\u3009]) + Ey1 [\u3008\u03c6(y1), \u00b5y\u30092]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009]2 \u2212 2(Ey1 [\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(y1), \u00b5x\u3009]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009]Ey1 [\u3008\u03c6(y1), \u00b5x\u3009]) + Ex1 [\u3008\u03c6(x1), \u00b5y\u30092]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5y\u3009]2 + Ey1 [\u3008\u03c6(y1), \u00b5x\u30092]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5x\u3009]2\nSubstituting empirical expectations over the data sample for the population expectations in Eq. (41) gives\n\u03b61 \u2248 1\nm(m\u2212 1)2 1T K\u0303xxK\u0303xx1\u2212\n( 1\nm(m\u2212 1) 1T K\u0303xx1\n)2 (41)\n\u2212 2 (\n1\nm(m\u2212 1)n 1T K\u0303xxKxy1\u2212\n1\nm2(m\u2212 1)n 1T K\u0303xx11 TKxy1 ) + 1\nn(n\u2212 1)2 1T K\u0303yyK\u0303yy1\u2212\n( 1\nn(n\u2212 1) 1T K\u0303yy1 )2 \u2212 2 ( 1\nn(n\u2212 1)m 1T K\u0303yyKyx1\u2212\n1\nn2(n\u2212 1)m 1T K\u0303yy11 TKxy1 ) + 1\nn2m 1TKyxKxy1\u2212 2\n( 1\nnm 1TKxy1\n)2 + 1\nm2n 1TKxyKyx1\nDerivation of the first term for example\nEx1 [\u3008x1, \u00b5x\u30092] \u2248 1\nm m\u2211 i=1 \u3008\u03c6(xi), 1 m\u2212 1 m\u2211 j=1 j 6=i \u03c6(xj)\u3009\u3008\u03c6(xi), 1 m\u2212 1 m\u2211 k=1 k 6=i \u03c6(xk)\u3009 (42)\n= 1 m(m\u2212 1)2 m\u2211 i=1 m\u2211 j=1 j 6=i m\u2211 k=1 k 6=i k(xi, xj)k(xi, xk) = 1\nm(m\u2212 1)2 eT K\u0303xxK\u0303xxe\nA.2 Covariance of MMD\nWe note many terms in expansion of the squares above cancel out due to independence. For example Ex1,z1 [\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(z1), \u00b5z\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009]Ez1 [\u3008\u03c6(z1), \u00b5z\u3009] = 0. We can thus simplify to the following expression for \u03b61\n\u03b61 = Ex1,y1,z1 [Ex2,y2,z2 [h(x1, y1)g(x1, z1)]]\u2212 ( MMD2 [F ,PX ,PY ] MMD2 [F ,PX ,PZ ] ) (43)\n= Ex1,y1,z1 [(\u3008\u03c6(x1), \u00b5x\u3009+ \u3008\u03c6(y1), \u00b5y\u3009 \u2212 \u3008\u03c6(x1), \u00b5y\u3009 \u2212 \u3008\u03c6(x1), \u00b5y)\u3009) (\u3008\u03c6(x1), \u00b5x)\u3009+ \u3008\u03c6(z1), \u00b5z\u3009 \u2212 \u3008\u03c6(x1), \u00b5z\u3009 \u2212 \u3008\u03c6(x1), \u00b5z\u3009)] \u2212MMD2 [F ,PX ,PY ] MMD2 [F ,PX ,PZ ] = Ex1 [ \u3008\u03c6(x1), \u00b5x\u30092 ] \u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009] 2\n\u2212 (Ex1 [\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(x1), \u00b5z\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009] Ex1 [\u3008\u03c6(x1), \u00b5z\u3009]) \u2212 (Ex1 [\u3008\u03c6(x1), \u00b5x\u3009\u3008\u03c6(x1), \u00b5y\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5x\u3009] Ex1 [\u3008\u03c6(x1), \u00b5y\u3009]) + Ex1 [\u3008\u03c6(x1), \u00b5y\u3009\u3008\u03c6(x1), \u00b5z\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5y\u3009] Ex1 [\u3008\u03c6(x1), \u00b5z\u3009]\n\u2248 1 m(m\u2212 1)2\n1T K\u0303xxK\u0303xx1\u2212 (\n1\nm(m\u2212 1) 1T K\u0303xx1 )2 \u2212 ( 1\nm(m\u2212 1)r 1T K\u0303xxKxz1\u2212\n1\nm2(m\u2212 1)r 1T K\u0303xx11 TKxz1 ) \u2212 ( 1\nm(m\u2212 1)n 1T K\u0303xxKxy1\u2212\n1\nm2(m\u2212 1)n 1T K\u0303xx11 TKxz1 ) + ( 1\nmnr 1TKyxKxz1\u2212\n1\nm2nr 1TKxy11 TKxz1 ) A.3 Derivation of the variance of the difference of two MMD statistics\nIn this section we propose an alternate strategy of deriving directly the variance of a ustatistic of the difference of MMDs with a joint variable. This formulation agrees with the derivation of the covariance matrix and subsequent projection, and provides extra insights. Let D := (d1, ..., dm) be m iid random variables where d := (x, y, z) \u223c Px \u00d7 Py \u00d7 Pz. Then the difference of the unbiased estimators of MMD2 [F ,Px,Py] and MMD2 [F ,Px,Pz] is given\nby\nMMD2u [F , x, y]\u2212MMD2u [F , x, z] = 1\nm(m\u2212 1) m\u2211 i 6=j f(di, dj) (44)\nwith f , the kernel of MMD2 [F ,Px,Py]\u2212MMD2 [F ,Px,Pz] of order 2 as follows\nf(d1, d2) = (k(x1, x2) + k(y1, y2)\u2212 k(x1, y2)\u2212 k(x2, y1)) (45) \u2212 (k(x1, x2) + k(z1, z2)\u2212 k(x1, z2)\u2212 k(x2, z1))\n= (k(y1, y2)\u2212 k(x1, y2)\u2212 k(x2, y1))\u2212 (k(z1, z2)\u2212 k(x1, z2)\u2212 k(x2, z1)) (46)\nEq. (44) is a U -statistic and thus we can apply Eq. (39) to obtain its variance. We first note\nEd1(f(d1, d2)) :=\u3008\u03c6(y1), \u00b5y\u3009 \u2212 \u3008\u03c6(x1), \u00b5y\u3009 \u2212 \u3008\u00b5x, \u03c6(y1)\u3009 (47) \u2212 (\u3008\u03c6(z1), \u00b5z\u3009 \u2212 \u3008\u03c6(x1), \u00b5z\u3009 \u2212 \u3008\u00b5x, \u03c6(z1)\u3009)\nEd1,d2(f(d1, d2)) := MMD 2 [F ,Px,Py]\u2212MMD2 [F ,Px,Pz]\nWe are now ready to derive the dominant leading term,\u03b61, in the variance expression (39).\nTerm \u03b61\n\u03b61 : = Var(Ed1(f(d1, d2))) (48) = Ex1,y1,z1 [(\u3008\u03c6(y1), \u00b5y\u3009 \u2212 \u3008\u03c6(x1), \u00b5y\u3009 \u2212 \u3008\u00b5x, \u03c6(y1)\u3009 \u2212 ( \u3008\u03c6(z1), \u00b5z\u3009 \u2212 \u3008\u03c6(x1), \u00b5z\u3009 \u2212 \u3008\u00b5x, \u03c6(z1)\u3009 )2 ] \u2212(MMD2 [F ,Px,Py]\u2212MMD2 [F ,Px,Pz])2\nWe note many terms in expansion of the squares above cancel out due to independence. For example Ey1,z1 [\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(z1), \u00b5z\u3009]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009]Ez1 [\u3008\u03c6(z1), \u00b5z\u3009] = 0. We can thus simplify to the following expression for \u03b61\n\u03b61 = Ey1 [\u3008\u03c6(y1), \u00b5y\u30092]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009]2 (49) + Ex1 [\u3008\u03c6(x1), \u00b5y\u30092]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5y\u3009]2\n+ Ey1 [\u3008\u00b5x, \u03c6(y1)\u30092]\u2212 Ey1 [\u3008\u00b5x, \u03c6(y1)\u3009]2 + Ez1 [\u3008\u03c6(z1), \u00b5z\u30092]\u2212 Ez1 [\u3008\u03c6(z1), \u00b5z\u3009]2 + Ex1 [\u3008\u03c6(x1), \u00b5z\u30092]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5z\u3009]2 + Ez1 [\u3008\u00b5x, \u03c6(z1)\u30092]\u2212 Ez1 [\u3008\u00b5x, \u03c6(z1)\u3009]2 \u2212 2(Ey1 [\u3008\u03c6(y1), \u00b5y\u3009\u3008\u00b5x, \u03c6(y1)\u3009]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009] Ey1 [\u3008\u00b5x, \u03c6(y1)\u3009]) \u2212 2(Ex1 [\u3008\u03c6(x1), \u00b5y\u3009\u3008\u03c6(x1), \u00b5z\u3009]\u2212 Ex1 [\u3008\u03c6(x1), \u00b5y\u3009] Ex1 [\u3008\u03c6(x1), \u00b5z\u3009]) \u2212 2(Ez1 [\u3008\u03c6(z1), \u00b5z\u3009\u3008\u00b5x, \u03c6(z1)\u3009]\u2212 Ez1 [\u3008\u03c6(z1), \u00b5z\u3009] Ez1 [\u3008\u00b5x, \u03c6(z1)\u3009])\nWe can empirically approximate these terms as follows:\n\u03b61 \u2248 1\nn(n\u2212 1)2 1T K\u0303yyK\u0303yy1\u2212\n( 1\nn(n\u2212 1) 1T K\u0303yy1\n)2 (50)\n+ 1\nn2m 1TKTxyKxy1\u2212\n( 1\nnm 1TKxy1 )2 + 1\nnm2 1TKxyK T xy1\u2212\n( 1\nnm 1TKxy1 )2 + 1\nr(r \u2212 1)2 eT K\u0303zzK\u0303zz1\u2212\n( 1\nr(r \u2212 1) 1T K\u0303zz1 )2 + 1\nrm2 1TKxzK T xz1\u2212\n( 1\nrm 1TKxz1 )2 + 1\nr2m 1TKTxzKxz1\u2212\n( 1\nrm 1TKxz1 )2 \u2212 2 ( 1\nn(n\u2212 1)m 1T K\u0303yyKyx1\u2212\n1\nn(n\u2212 1) 1T K\u0303yy1\u00d7\n1\nnm 1TKxy1 ) \u2212 2 ( 1\nnmr 1T K\u0303TxyKxz1\u2212\n1\nnm 1TKxy1\u00d7\n1\nrm 1TKxz1 ) \u2212 2 ( 1\nr(r \u2212 1)m 1T K\u0303zzK T xz1\u2212\n1\nn(n\u2212 1) 1T K\u0303yy1\u00d7\n1\nnm 1TKxy1\n)\nA.4 Equality\nIn this section, we prove that Eq. (31) is equal to the variance of the difference of 2 MMD2 [F ,Px,Py] and MMD2 [F ,Px,Pz].\n\u03c32XY + \u03c3XZ \u2212 2\u03c3XYXZ = Ey1 [ \u3008\u03c6(y1), \u00b5y\u30092 ] \u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009] 2 (51)\n+ Ez1 [ \u3008\u03c6(z1), \u00b5y\u30092 ] \u2212 Ez1 [\u3008\u03c6(z1), \u00b5y\u3009] 2 \u2212 2 (Ey1 [\u3008\u03c6(y1), \u00b5y\u3009\u3008\u03c6(y1), \u00b5x\u3009]\u2212 Ey1 [\u3008\u03c6(y1), \u00b5y\u3009] Ey1 [\u3008\u03c6(y1), \u00b5x\u3009]) \u2212 2 (Ez1 [\u3008\u03c6(z1), \u00b5z\u3009\u3008\u03c6(z1), \u00b5x\u3009]\u2212 Ez1 [\u3008\u03c6(z1), \u00b5z\u3009] Ez1 [\u3008\u03c6(z1), \u00b5x\u3009]) + Ex1 [ \u3008\u03c6(x1), \u00b5y\u30092 ] \u2212 Ex1 [\u3008\u03c6(x1), \u00b5y\u3009] 2\n+ Ey1 [ \u3008\u03c6(y1), \u00b5z\u30092 ] \u2212 Ey1 [\u3008\u03c6(y1), \u00b5z\u3009] 2\n+ Ey1 [ \u3008\u03c6(y1), \u00b5x\u30092 ] \u2212 Ey1 [\u3008\u03c6(y1), \u00b5x\u3009] 2\n+ Ez1 [ \u3008\u03c6(z1), \u00b5x\u30092 ] \u2212 Ez1 [\u3008\u03c6(z1), \u00b5x\u3009] 2 \u2212 2 (Ex1 [\u3008\u03c6(x1), \u00b5y\u3009] Ex1 [\u3008\u03c6(x1), \u00b5z\u3009])\nWe have shown that Eq. (31) is equal to Eq. (50). We use this equality to compute the p-value for the Relative MMD test."}], "references": [{"title": "Limit theorems for U-processes", "author": ["M.A. Arcones", "E. Gine"], "venue": "The Annals of Probability,", "citeRegEx": "Arcones and Gine.,? \\Q1993\\E", "shortCiteRegEx": "Arcones and Gine.", "year": 1993}, {"title": "Joint measures and cross-covariance operators", "author": ["C.R. Baker"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Baker.,? \\Q1973\\E", "shortCiteRegEx": "Baker.", "year": 1973}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": null, "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2011\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2011}, {"title": "Mapping the origins and expansion of the Indo-European language family", "author": ["R. Bouckaert", "P. Lemey", "M. Dunn", "S.J. Greenhill", "A.V. Alekseyenko", "A.J. Drummond", "R.D. Gray", "M.A. Suchard", "Q.D. Atkinson"], "venue": null, "citeRegEx": "Bouckaert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouckaert et al\\.", "year": 2012}, {"title": "A low variance consistent test of relative dependency", "author": ["W. Bounliphone", "A. Gretton", "A. Tenenhaus", "M.B. Blaschko"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Bounliphone et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2015}, {"title": "A test of relative similarity for model selection in generative models", "author": ["W. Bounliphone", "E. Belilovsky", "M.B. Blaschko", "I. Antonoglou", "A. Gretton"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bounliphone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2016}, {"title": "A geometric approach to compare variables in a regression model", "author": ["J. Bring"], "venue": "The American Statistician,", "citeRegEx": "Bring.,? \\Q1996\\E", "shortCiteRegEx": "Bring.", "year": 1996}, {"title": "Learning non-linear combinations of kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Multiple regression in psychological research and practice", "author": ["R.B. Darlington"], "venue": "Psychological Bulletin,", "citeRegEx": "Darlington.,? \\Q1968\\E", "shortCiteRegEx": "Darlington.", "year": 1968}, {"title": "Nonlinear canonical analysis and independence tests", "author": ["J. Dauxois", "G.M. Nkiet"], "venue": "Annals of Statistics,", "citeRegEx": "Dauxois and Nkiet.,? \\Q1998\\E", "shortCiteRegEx": "Dauxois and Nkiet.", "year": 1998}, {"title": "Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests", "author": ["J.H. Friedman", "L.C. Rafsky"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman and Rafsky.,? \\Q1979\\E", "shortCiteRegEx": "Friedman and Rafsky.", "year": 1979}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Kernel measures of conditional dependence", "author": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fukumizu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2008}, {"title": "Tumorigenesis in the brain: Location, location, location", "author": ["R.J. Gilbertson", "D.H. Gutmann"], "venue": "Cancer research,", "citeRegEx": "Gilbertson and Gutmann.,? \\Q2007\\E", "shortCiteRegEx": "Gilbertson and Gutmann.", "year": 2007}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Language-tree divergence times support the Anatolian theory of Indo-European origin", "author": ["R.D. Gray", "Q.D. Atkinson"], "venue": "Nature, 426(6965):435\u2013439,", "citeRegEx": "Gray and Atkinson.,? \\Q2003\\E", "shortCiteRegEx": "Gray and Atkinson.", "year": 2003}, {"title": "Consistent nonparametric tests of independence", "author": ["A. Gretton", "L. Gyorfi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton and Gyorfi.,? \\Q2010\\E", "shortCiteRegEx": "Gretton and Gyorfi.", "year": 2010}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Schoelkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A kernel method for the two-sample-problem", "author": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gretton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2006}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.-H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "A kernel twosample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu", "B.K. Sriperumbudur"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Structural modelling with sparse kernels", "author": ["S.R. Gunn", "J.S. Kandola"], "venue": "Machine Learning,", "citeRegEx": "Gunn and Kandola.,? \\Q2002\\E", "shortCiteRegEx": "Gunn and Kandola.", "year": 2002}, {"title": "Permutation tests for equality of distributions in high-dimensional settings", "author": ["P. Hall", "N. Tajvidi"], "venue": null, "citeRegEx": "Hall and Tajvidi.,? \\Q2002\\E", "shortCiteRegEx": "Hall and Tajvidi.", "year": 2002}, {"title": "A consistent multivariate test of association based on ranks of distances", "author": ["R. Heller", "Y. Heller", "M. Gorfine"], "venue": null, "citeRegEx": "Heller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "A class of statistics with asymptotically normal distribution", "author": ["W. Hoeffding"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Hoeffding.,? \\Q1948\\E", "shortCiteRegEx": "Hoeffding.", "year": 1948}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Equitability, mutual information, and the maximal information coefficient", "author": ["J.B. Kinney", "G.S. Atwal"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kinney and Atwal.,? \\Q2014\\E", "shortCiteRegEx": "Kinney and Atwal.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Testing statistical hypotheses, volume 150", "author": ["E.L. Lehmann", "J.P. Romano", "G. Casella"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1986}, {"title": "Generative moment matching networks", "author": ["Y. Li", "K. Swersky", "R. Zemel"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Expression profiling of ependymomas unravels localization and tumor grade-specific tumorigenesis", "author": ["T. Palm", "D. Figarella-Branger", "F. Chapon", "C. Lacroix", "F. Gray", "F. Scaravilli", "D.W. Ellison", "I. Salmon", "M. Vikkula", "C. Godfraind"], "venue": "Cancer, 115(17):3955\u20133968,", "citeRegEx": "Palm et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palm et al\\.", "year": 2009}, {"title": "Mesenchymal transition and PDGFRA amplification/mutation are key distinct oncogenic events in pediatric diffuse intrinsic pontine gliomas", "author": ["S. Puget", "C. Philippe", "D. Bax", "B. Job", "P. Varlet", "M.P. Junier", "F. Andreiuolo", "D. Carvalho", "R. Reis", "L. Guerrini-Rousseau"], "venue": "PloS one,", "citeRegEx": "Puget et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Puget et al\\.", "year": 2012}, {"title": "Detecting novel associations in large", "author": ["D. Reshef", "Y. Reshef", "H. Finucane", "S. Grossman", "G. McVean", "P. Turnbaugh", "E. Lander", "M. Mitzenmacher", "P. Sabeti"], "venue": null, "citeRegEx": "Reshef et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reshef et al\\.", "year": 2011}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["P.R. Rosenbaum"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Rosenbaum.,? \\Q2005\\E", "shortCiteRegEx": "Rosenbaum.", "year": 2005}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Neurocomputing: Foundations of Research,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Equivalence of distancebased and RKHS-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "Approximation theorems of mathematical statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q2009\\E", "shortCiteRegEx": "Serfling.", "year": 2009}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic learning theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Steinwart.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart.", "year": 2002}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G. Sz\u00e9kely", "M. Rizzo", "N. Bakirov"], "venue": "Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}, {"title": "On the convergence of moments in the central limit theorem", "author": ["B. von Bahr"], "venue": "The Annals of Mathematical Statistics, 36(3):808\u2013818,", "citeRegEx": "Bahr.,? \\Q1965\\E", "shortCiteRegEx": "Bahr.", "year": 1965}, {"title": "Kernel-based conditional independence test and application in causal discovery", "author": ["K. Zhang", "J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "In 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 51, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 10, "context": ", 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al.", "startOffset": 24, "endOffset": 72}, {"referenceID": 13, "context": ", 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al.", "startOffset": 24, "endOffset": 72}, {"referenceID": 49, "context": ", 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al.", "startOffset": 68, "endOffset": 115}, {"referenceID": 44, "context": ", 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al.", "startOffset": 68, "endOffset": 115}, {"referenceID": 8, "context": ", 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 24, "context": ", 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 26, "context": ", 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 17, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 40, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 31, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 15, "context": "First, likelihoods can be difficult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 37, "context": "First, likelihoods can be difficult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 5, "context": "This article is based upon and extends Bounliphone et al. (2015, 2016). We address two related problems using analogous tools based on estimating correlated U -statistics for dependency and similarity in a non-parametric setting. The first problem (called the relative dependency test) is to compare multiple dependencies to determine which of two variables most strongly influences the third, by proposing a statistical test of the null hypothesis that a source variable is more dependent to a first target variable against the alternative hypothesis that a source variable is more dependent to a second target variable. Much recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-Euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).", "startOffset": 39, "endOffset": 1518}, {"referenceID": 5, "context": "This article is based upon and extends Bounliphone et al. (2015, 2016). We address two related problems using analogous tools based on estimating correlated U -statistics for dependency and similarity in a non-parametric setting. The first problem (called the relative dependency test) is to compare multiple dependencies to determine which of two variables most strongly influences the third, by proposing a statistical test of the null hypothesis that a source variable is more dependent to a first target variable against the alternative hypothesis that a source variable is more dependent to a second target variable. Much recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-Euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996). For many problems in data analysis, however, the question of whether dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest.", "startOffset": 39, "endOffset": 1532}, {"referenceID": 33, "context": "Permutation testing or other generic strategies are often computationally prohibitive, bearing in mind the relatively high computational requirements of deep networks (Krizhevsky et al., 2012).", "startOffset": 167, "endOffset": 192}, {"referenceID": 15, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 27, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 43, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 29, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 45, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (2009) to determine the asymptotic joint distributions of the statistics (see Theorems 5 & 11).", "startOffset": 105, "endOffset": 188}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (2009) to determine the asymptotic joint distributions of the statistics (see Theorems 5 & 11).", "startOffset": 105, "endOffset": 208}, {"referenceID": 3, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures.", "startOffset": 100, "endOffset": 153}, {"referenceID": 46, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures.", "startOffset": 100, "endOffset": 153}, {"referenceID": 13, "context": "The notion of universal kernels and characteristic kernels are essential to the study of kernel mean embeddings (Fukumizu et al., 2008).", "startOffset": 112, "endOffset": 135}, {"referenceID": 48, "context": "The kernel k is said to be universal if the corresponding RKHS H is dense in the space of bounded continuous functions on X (Steinwart, 2002).", "startOffset": 124, "endOffset": 141}, {"referenceID": 3, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures. The kernel mean embedding has been used to define metrics for probability distributions which is important for many problems in statistics and machine learning. First, we briefly review the properties of the Reproducing Kernel Hilbert-space (RKHS) (Aronszajn, 1950). A RKHS H with a reproducing kernel k(x, y) is a Hilbert space of functions f : X \u2192 R with inner product \u3008\u00b7, \u00b7\u3009H. Its element k(x, \u00b7) satisfies the reproducing property: \u3008f, k(x, \u00b7)\u3009H = f(x) for any f \u2208 H and consequently, \u3008k(x, \u00b7), k(y, \u00b7)\u3009H = k(x, y). We define the feature map \u03c6 : X \u2192 H by \u03c6(x) = k(x, \u00b7) and using the reproducing property, we obtain that k(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009H. We extend the notion of feature map to the mean embedding of probability measure : Suppose that a space P(X ) consists of all Borel probability measures P on some input space X . we define the mean embedding \u03bc of P associated with a reproduction kernel k by a mapping \u03bc : P(X ) \u2192 H, by \u03bcP = EX\u223cP [k(x, \u00b7)] = \u222b X k(\u00b7, x)dP(x). The distribution P is mapped to its expected feature map, i.e., to a point in a potentially infinite-dimensional and implicit feature space. The mean embedding \u03bc has the property that Ex\u223cP [f(X)] = \u3008\u03bcP, f\u3009H for any f \u2208 H. The notion of universal kernels and characteristic kernels are essential to the study of kernel mean embeddings (Fukumizu et al., 2008). The kernel k is said to be universal if the corresponding RKHS H is dense in the space of bounded continuous functions on X (Steinwart, 2002). It was shown that for a universal kernel k, \u2016\u03bcP\u2212\u03bcQ\u2016H iff P = Q, i.e. the map \u03bc is injective. The kernel k is said to be characteristic if the map \u03bc is injective and the RKHS H is said to be characteristic if its reproducing kernel is characteristic. This notion was introduced by Fukumizu et al. (2008) and it was shown that Gaussian and Laplacian kernels are characteristic on Rd.", "startOffset": 101, "endOffset": 2076}, {"referenceID": 1, "context": "We assume that Ex[k(x, x)] < \u221e and Ey[l(y, y\u2032)] < \u221e, the cross-covariance operator (see Baker (1973) and Fukumizu et al.", "startOffset": 88, "endOffset": 101}, {"referenceID": 1, "context": "We assume that Ex[k(x, x)] < \u221e and Ey[l(y, y\u2032)] < \u221e, the cross-covariance operator (see Baker (1973) and Fukumizu et al. (2004)) Cyx : H \u2192 F is defined as Cyx := Eyx [\u03c6(y)\u2297 \u03c6(x)]\u2212\u03bcPx\u2297\u03bcPy = \u03bcPyx\u2212\u03bcPy\u2297\u03bcPx .", "startOffset": 88, "endOffset": 128}, {"referenceID": 20, "context": "HSIC determines independence: HSIC [F ,G,Pxy] = 0 if and only if Pxy = PxPy when kernels k and l are characteristic on their respective marginal domains (Gretton et al., 2006).", "startOffset": 153, "endOffset": 175}, {"referenceID": 36, "context": "We briefly described the framework of statistical hypothesis testing (Lehmann et al., 1986) as it applies in this context.", "startOffset": 69, "endOffset": 91}, {"referenceID": 18, "context": "Proof In Gretton et al. (2005a) a finite sample bound is given for a single HSIC statistic.", "startOffset": 9, "endOffset": 32}, {"referenceID": 16, "context": "These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray and Atkinson, 2003).", "startOffset": 94, "endOffset": 119}, {"referenceID": 30, "context": "corpus Koehn (2005). We choose 3000 random documents in common written in: Finnish (fi), Italian (it), French (fr), Spanish (es), Portuguese (pt), English (en), Dutch (nl), German (de), Danish (da) and Swedish (sv).", "startOffset": 7, "endOffset": 20}, {"referenceID": 15, "context": "These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray and Atkinson, 2003). In this dataset, we considered each language as a random variable and each document as an observation. Our first goal is to test if the statistical dependence between two languages in the same group is greater than the statistical dependence between languages in different groups. For pre-processing, we removed stop-words (http://www.nltk.org) and performed stemming (http://snowball.tartarus.org). We applied the TF-IDF model as a feature representation and used a Gaussian kernel with the bandwidth \u03c3 set per language as the median pairwise distance between documents. In Tab. 1, a selection of tests between language groups (Germanic, Romance, and Uralic) is given: all p-values strongly support that our relative dependence test finds the different language groups with very high significance. Further, if we focus on the Romance family, our test enables one to answer more fine-grained questions about the relative similarity of languages within the same group. As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community Gray and Atkinson (2003); Bouckaert et al.", "startOffset": 95, "endOffset": 1260}, {"referenceID": 4, "context": "As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community Gray and Atkinson (2003); Bouckaert et al. (2012) as illustrated in Fig.", "startOffset": 172, "endOffset": 196}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 192}, {"referenceID": 39, "context": "In order to evaluate such hypotheses, pre-treatment frozen tumor samples were obtained from 53 children with newly diagnosed pHGG from Necker Enfants Malades (Paris, France) from Puget et al. (2012). The 53 tumors are divided into 3 locations: supratentorial (HEMI), central nuclei (MIDL), and brain stem (DIPG).", "startOffset": 179, "endOffset": 199}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003). Figure 4: Partial tree of Romance languages adapted from Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 192}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003). Figure 4: Partial tree of Romance languages adapted from Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 275}, {"referenceID": 14, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 38, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 39, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 30, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 2, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 34, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 43, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 37, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 15, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 37, "context": "As noted by several authors, the evaluation of generative models is a challenging open problem (Li et al., 2015; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difficult to evaluate.", "startOffset": 95, "endOffset": 137}, {"referenceID": 15, "context": "As noted by several authors, the evaluation of generative models is a challenging open problem (Li et al., 2015; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difficult to evaluate.", "startOffset": 95, "endOffset": 137}, {"referenceID": 30, "context": "In the experiments in the sequel we focus on the recently introduced variational autoencoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.", "startOffset": 99, "endOffset": 125}, {"referenceID": 37, "context": "In the experiments in the sequel we focus on the recently introduced variational autoencoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al., 2015).", "startOffset": 177, "endOffset": 194}, {"referenceID": 30, "context": "minimizing a regularized variational lower bound (Kingma and Welling, 2014).", "startOffset": 49, "endOffset": 75}, {"referenceID": 42, "context": "Both these models can be trained using standard backpropagation (Rumelhart et al., 1988).", "startOffset": 64, "endOffset": 88}, {"referenceID": 27, "context": "Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009).", "startOffset": 123, "endOffset": 176}, {"referenceID": 43, "context": "Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009).", "startOffset": 123, "endOffset": 176}, {"referenceID": 35, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 30, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 15, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 30, "context": "Variational Auto-Encoder Sample Size and Architecture Experiments We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder and decoder and a latent variable layer as shown in Fig.", "startOffset": 95, "endOffset": 121}, {"referenceID": 30, "context": "In addition we use the supervised task test data to evaluate the variational lower bound of the data under the two models (Kingma and Welling, 2014).", "startOffset": 122, "endOffset": 148}, {"referenceID": 37, "context": "Generative Moment Matching Networks Architecture Experiments We demonstrate our hypothesis test on a different class of deep generative models called Generative Moment Matching Networks (GMMN) (Li et al., 2015).", "startOffset": 193, "endOffset": 210}, {"referenceID": 37, "context": "We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 90, "endOffset": 132}, {"referenceID": 15, "context": "We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 90, "endOffset": 132}, {"referenceID": 36, "context": "Li et al. (2015) proposes to use that model along with an auto-encoder, which is the setup we employ in this work.", "startOffset": 0, "endOffset": 17}, {"referenceID": 36, "context": "Li et al. (2015) proposes to use that model along with an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is trained on the data to obtain a low dimensional representation, then a GMMN network is trained on the latent representations (Fig. 10). We use the relative similarity test to evaluate various architectural choices in this new class of models. We start from the baseline model specified in Li et al. (2015) and associated software.", "startOffset": 0, "endOffset": 459}, {"referenceID": 41, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 11, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 25, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 30, "context": "There are two primary advantages of the MMD over the variational lower bound, where it is known (Kingma and Welling, 2014): first, we have a characterization of the asymptotic behavior, which allows us to determine when the difference in performance is significant; second, comparing two lower bounds produced from two different models is unreliable, as we do not know how conservative either lower bound is.", "startOffset": 96, "endOffset": 122}], "year": 2016, "abstractText": "We introduce two novel non-parametric statistical hypothesis tests. The first test, called the relative test of dependency, enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC). The second test, called the relative test of similarity, is use to determine which of the two samples from arbitrary distributions is significantly closer to a reference sample of interest and the relative measure of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we have used as our test statistics the difference of HSIC statistics and of MMD statistics, respectively. The resulting tests are consistent and unbiased, and (being based on U -statistics) have favorable convergence properties. The effectiveness of the relative dependency test is demonstrated on several real-world problems: we identify languages groups from a multilingual parallel corpus, and we show that tumor location is more dependent on gene expression \u2217. These authors contributed equally. c \u00a90 Bounliphone, et al.. ar X iv :1 61 1. 05 74 0v 1 [ cs .A I] 1 7 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}