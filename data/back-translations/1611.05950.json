{"id": "1611.05950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Analysis of a Design Pattern for Teaching with Features and Labels", "abstract": "We introduce the error-driven design pattern to classrooms with attributes and labels, where a teacher presents attributes only when they are needed. We analyze the potential risks and benefits of this learning pattern by using lesson protocols, graphic examples, and providing limits on the effort required for an optimal machine teacher by using a linear learning algorithm, the most commonly used type of learner in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs between the use of different learning algorithms and the effort required to display (create new attributes) and lettering (provide lettering for objects).", "histories": [["v1", "Fri, 18 Nov 2016 02:04:57 GMT  (264kb,D)", "http://arxiv.org/abs/1611.05950v1", "Also available atthis https URL"]], "COMMENTS": "Also available atthis https URL", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["christopher meek", "patrice simard", "xiaojin zhu"], "accepted": false, "id": "1611.05950"}, "pdf": {"name": "1611.05950.pdf", "metadata": {"source": "CRF", "title": "Analysis of a Design Pattern for Teaching with Features and Labels", "authors": ["Christopher Meek", "Patrice Simard", "Xiaojin Zhu"], "emails": [], "sections": [{"heading": "Introduction", "text": "Featuring and labeling are critical parts of the interactive machine learning process in which a person and a machine learning algorithm coordinate to build a predictive system (a classifier, entity extractor, etc.). Unlike the case of using labels alone, little is known about how to quantify the effort required to teach a machine using both features and labels. In this paper, we consider the problem of teaching a machine how to classify objects when the teacher can provide labels for objects and provide features\u2014functions from objects to values. Our aim is to understand the effort required by a teacher to find a suitable representation for objects, to teach the target classification function, and to provide guidance to teachers about how to provide features and labels when teaching.\nSimilar to previous work on active learning and teaching dimension, we take an idealized view of the cost of labeling and featuring. In particular, we ignore variability in the effort required for these respective actions. In addition, similar to the work on teaching dimension, we assume an idealized teacher with complete knowledge about the learner, target classification function and the range of possible objects that we want to classify.\nWe analyze the effort required to teach a classification function relative to a given set of feature functions. This set of features functions can be thought of as a set of teachable functions. There are several observations that motivate us to quantify teaching effort relative to a set of feature functions. It is natural to expect that the available set of teachable functions depends on the specific learner that we are teaching and the types of objects that we want to classify (e.g., images versus text documents). In addition, the teaching effort required to teach a learner is heavily dependent on the available set of functions. For instance, if the teacher could directly teach the learner the target classification function then only one function would be required, and, for wide variety of learning algorithms, the teacher would only be require to provide two labeled examples in the case of binary classification. Of course, it is unreasonable to expect that the target classification function can directly be encoded in a feature function and, in fact, if this is possible then we need not use a machine learning algorithm to build the predictor. For these reasons, we assume that there is a set of features that are teachable and define the effort relative to this set of features. In order to capture dependencies among features we consider a lattice of sets of features rather than a set of features. We use the lattice to enforce our assumption that features are taught one at a time and to capture other dependencies such as only\nar X\niv :1\n61 1.\n05 95\n0v 1\n[ cs\n.A I]\n1 8\nN ov\n2 01\nallowing features to be taught if all of the constituent features have been taught (e.g., the feature of tall and heavy can only be included in a feature set if the features of being tall and of being heavy have previously been defined). Thus, the lattice of feature sets captures the potential alternative sequences of features that the teacher can use to teach a learner.\nWe introduce the Error-Driven-Featuring (EDF) design pattern for teaching in which the teacher prefers to add features only if they are needed to fix a prediction error on the training set. In order to analyze the risks and benefits of the EDF teaching pattern we consider two teaching protocols, one which forces the teacher to use the EDF teaching pattern and the other which does not. By quantifying the featuring and labeling effort required by these protocols we can provide a deeper understanding of the risks and benefits of the EDF pattern and potential trade-offs between featuring and labeling more generally. In our analysis we consider two specific learning algorithms; a one-nearestneighbor classifier and a linear classifier. Using our measures of teaching cost we demonstrate that there are significant risks of adding features for high-capacity learning algorithms (1NN) which can be controlled by using a low-capacity learning algorithm (linear classifier). We also demonstrate that the additional labeling costs associated with using the EDF teaching pattern for both high and low capacity learning algorithms can be bounded. The combination of these results suggest that it would be valuable to empirically evaluate the EFT design pattern for teaching. In analyzing the costs of the Error-Driven-Featuring protocol we provide new results on the hypothesis specific pool-based teaching dimension of linear classifiers and pool-based exclusion dimension of linear classifiers. 1"}, {"heading": "Related Work", "text": "There has been a a variety of work aimed at understanding the labeling effort required to build classifiers. In this section we briefly review related work. First we note that this work shares a common roots with the work of Meek (2016) but there the focus is on prediction errors rather than teaching effort.\nOne closely related concept is that of teaching dimension. The primary aim of this work is to quantify the worst case minimal effort to teach a learner one classification function (typically called a concept in this literature) from among a set of alternative classification functions. There is a large body of work aimed at understanding the teaching dimension, refining teaching dimension (e.g., extended, recursive) and the relationship between these and other concepts from learning theory such as the VC-dimension (e.g., Doliwa et al 2014, Balbach 2008, Zilles et al 2011). Our work, rather than attempting to quantify the difficulty of learning among a set of classifications, is aimed at quantifying the effort required to teach any particular classification function and to understand the relationship between adding features and adding labels. The work on teaching dimension abstracts the role of the learner and rather deals directly with hypothesis classes of classification functions. Furthermore, the work on teaching dimension abstracts away the concept of features making it useless for understanding the interplay between learner, featuring and labeling. That said, several of the concepts that we use have been treated previously in this and related literature. For instance, the idea of a concept teaching set is closely related to that of a teaching sequence (Goldman and Kearns 1995) and our optimal concept specification cost is essentially the specification number of a hypothesis (Anthony et al 1992); we add concept to distinguish it from representation specification cost. Other existing concepts include the exclusion dimension (Angluin 1994) and the unique specification dimension (Hedigus 1995) and the certificate size (Hellerstein et al 1996) which are similar to our invalidation cost. In addition, Liu et al (2016) define the teaching dimension of a hypothesis which is equivalent to the specification number and our concept specification cost. They also provide bounds on the concept specification cost for linear classifiers. Their results are related to our Proposition 7 but, unlike our result, assume that the space of objects is dense. In the terms of Zhu (2015), we provide the hypothesis specific teaching dimension for pool-based teaching. For many domains such as image classification, document classification and entity extraction and associated feature sets the assumption of a dense representation is unnatural (e.g., we cannot have a fractional number of words in a document). Like other work on classical teaching dimension, this work does not consider teaching with both labels and features.\nThe other body of related work is active learning. The aim of this body of work is to develop algorithms to choose which items to label and the quality of an algorithm is measured by the number\n1This paper is an extended version of the paper by Meek et al (2016).\nof labels that are required to obtain a desirable classification function. Thus, given our interest on both labeling and featuring this body of work is perhaps better named \u201cactive labeling\u201d. In contrast to the work on teaching dimension where the teacher has access to target classification function, in active learning, the teacher must choose the item to label without knowledge of the target classification function. This makes active learning critical to many practical systems. An excellent survey of research in this area is given by Settles (2012). Not surprisingly, the work on active learning is related to work on teaching dimension (Hanneke 2007)."}, {"heading": "Features, Labels and Learning Algorithms", "text": "In this section, we define features, labels and learning algorithms. These three concepts are the core concepts needed to discuss the cost of teaching a machine to classify objects. Thus, these definitions are the foundation of the remainder of the paper. In addition to providing these definitions, we also describe two properties of learning algorithms related to machine teaching and we describe two specific learning algorithms that are used in the remainder of the paper.\nWe are interested in building a classifier of objects. We use x and xi to denote particular objects and X to denote the set of objects of interest. We use y and yi for particular labels and Y to denote the space of possible labels. For binary classification Y = {0, 1}. A classification function is a function from X to Y .2 The set of classification functions is denoted by C = X \u2192 Y . We use c\u2217 to denote the target classification function.\nCentral to this paper are features or functions which map objects to scalar values. A feature fi (or gi) is a function from objects to real numbers (i.e. fi \u2208 X \u2192 R). A feature set is a set of features and we use F, Fi and Gi to denote generic feature sets. The feature set Fi = {fi,1, . . . , fi,p} is p-dimensional. We use a p-dimensional feature set to map an object to a point in Rp. We denote the mapped object xk using feature set Fi by Fi(xk) = (fi,1(xk), . . . , fi,p(xk)) where the result is a vector of length p where the jth entry is the result of applying the jth feature function in Fi to the object.\nWe define the potential sequences of teachable features via a lattice of feature sets. Our definition of a feature lattice enforces the restriction that features are taught sequentially. We use R = {f1, f2, f3, . . .} to denote the set of all teachable feature functions for a set of objects X . A feature lattice F for a feature set R is a set of finite subsets of R (thus F \u2286 2R) such that if Fi \u2208 F then either Fi = \u2205 or there is a Fj \u2208 F such that Fj \u2282 Fi and |Fj | + 1 = |Fi|. We restrict attention to finite sets to capture the fact that teachers can only teach a finite number of features. We note that the feature lattice also allows us to represent constraints on the order in which features can be taught. Such constraints arise naturally. For instance, before teaching the concept of the area of a rectangle one needs to first teach the concepts of length and width (e.g., feature f3(x) = f1(x)\u00d7 f2(x) can be added only if both f1 and f2 have been added as features). These definitions are illustrated in Figure 1.\nIn order to define a learning algorithm we first define training sets and, because we are considering learning with alternative feature sets, featurized training sets. A training set T \u2282 X \u00d7 Y is a set of labeled examples. We consider only honest training sets, that is, T \u2282 X \u00d7 Y such that \u2200(x, y) \u2208 T it is the case that c\u2217(x) = y. We say that the training set T has n examples if |T | = n and denote the set of training examples as {(x1, y1), . . . , (xn, yn)}. A training set is unfeaturized. We use feature sets to create featurized training sets. For p-dimensional feature set Fi and an n example training set T we denote the featurized training set Fi(T ) = {(Fi(x1), y1), . . . , (Fi(xn), yn)} \u2208 {Rp \u00d7 Y }n. We call the resulting training set an Fi featurized training set or an Fi featurization of training set T .\nNow we are prepared to define a learning algorithm. First, a d-dimensional learning algorithm `d is a function that takes a p-dimensional feature set F and a training set T and outputs a function hp \u2208 Rp \u2192 Y . Thus, the output hp of a learning algorithm using Fi and training set T can be composed with the functions in the feature set to yield a classification function of objects (i.e., hp \u25e6 Fi \u2208 C). The hypothesis space of a d-dimensional learning algorithm `d is the image of the function `d and is denoted by H`d (or Hd if there is no risk of confusion). A classification function c \u2208 C is consistent with a training set T if \u2200(x, y) \u2208 T it is the case that c(x) = y. A d-dimensional\n2Note that, while we call this mapping a classification function, the definition encompasses a broad class of prediction problems including structured prediction, entity extraction, and regression.\nlearning algorithm `d is consistent if the learning algorithm outputs a hypothesis consistent with the training set whenever there is a hypothesis in Hd that is consistent with the training set. A vector learning algorithm ` = {`0, `1, . . .} is a set of d-dimensional learning algorithms one for each dimensionality. A consistent vector learning algorithm is one in which each of the d-dimensional learning algorithms is consistent. Finally, a (feature-vector) learning algorithm L takes a feature set F , a training set T , and a vector learning algorithm ` and returns a classification function in C. In particular L`(F, T ) = `|F |(F, T ) \u25e6F \u2208 C. When the vector learning algorithm is clear from context or we are discussing a generic vector learning algorithm we drop the ` and write L(F, T ). One important property of a feature set is whether it is sufficient to teach the target classification function c\u2217. A feature feature set F is sufficient for learner L and target classification function c\u2217 if there exists a training set T such that L(F, T ) = c\u2217. A natural desiderata of a learning algorithm is that adding a feature to a sufficient feature set should not make it impossible to teach a target classification function. We capture this with the following property of a learning algorithm. We say that a learning algorithm L is monotonically sufficient if it is the case that if F is sufficient then F \u2032 \u2283 F is sufficient. Many learning algorithms, in fact, have this property.\nWe distinguish two type of training sets that are central to teaching. First, a training set T is a concept teaching set for feature set F and learning algorithm L if L(F, T ) = c\u2217. Second, a training set T an invalidation set if there is an example (x, y) \u2208 T that is not correctly classified by L(Fi, T ). The following proposition demonstrates that, for consistent learning algorithms, finding an invalidation set demonstrates that a feature set is not sufficient for the target classification function.\nProposition 1 If learning algorithm L is consistent and T is an invalidation set for feature set Fi, target concept c\u2217, and L then Fi is not sufficient for c\u2217 and L.\nMeek (2016) suggests that identifying minimal invalidation sets might be helpful for teachers wanting to identify mislabeling errors and representation errors. In this paper, an invalidation set is an indication of a representation errors because we assume that the labels in the training set are correct implying that there are no mislabeling errors.\nIn the remainder of the paper we consider two binary classification algorithms (Y = {0, 1}). The first learning algorithm is a consistent one-nearest-neighbor learning algorithm L1NN . Our onenearest-neighbor algorithm is a set of d-dimensional one-nearest-neighbor learning algorithms that use a d-dimensional feature set to project the training set into Rd. The algorithm identifies the set of\nclosest points and outputs the minimal label value of points in that set. Thus, if there is more than one closest point and their labels disagree then the learned classification will output 0. By construction, this is a consistent learning algorithm.\nThe second learning algorithm is a linear learning algorithm Llin. Our consistent linear learning algorithm is a set of d-dimensional linear learning algorithms for which the decision surface is defined by a hyperplane in the Rd or, more formally, by c(x|F,w, b) = sign(w \u00b7 F (x) + b) where the hyperplane is defined in terms of weights (w, b). We consider the linear learner Llin that produces the maximum margin separating hyperplane for a training set when one exists and outputs the constant zero function otherwise. Note that the maximum margin separating hyperplane for a training set is the separating hyperplane that maximizes the minimum distance between points in the training set and the hyperplane. again, by construction, this is a consistent learning algorithm.\nNote that we say that a feature set F is linearly sufficient for the target classification function if F is sufficient for the target classification function when using a consistent linear learning algorithm.\nWe finish this section with the following proposition that demonstrates our learning algorithms are both monotonically sufficient.\nProposition 2 The learning algorithms L1NN and Llin are monotonically sufficient."}, {"heading": "Teaching Patterns, Protocols and Costs", "text": "In this section, we introduce our Error-Drive-Featuring (EDF) design pattern for teaching and two teaching protocols. We introduce the teaching protocols as a means to study the risks and benefits of our EDF teaching pattern.\nTeaching patterns are related to design patterns (Gamma et al 1995). Whereas design patterns for programming are formalized best practices that a programmer can use to design software solutions to common problems, a design pattern for teaching (or teaching pattern) is a formalized best practice that a teacher can use to teach a computer.\nWe use a pair of teaching protocols to study the risks and benefits of our EDF teaching pattern. A teaching protocol is an algorithmic description of a method by which a teacher teaches a learner. In order to study a teaching pattern, in one protocol, we force the teacher to follow the teaching pattern and, in the other, we allow the teacher full control over their actions.\nWe contrast our teaching protocols by comparing the optimal teaching costs and, in a subsequent section, bounds on optimal teaching costs. To facilitate the discussion of optimal teaching costs we next define several teaching costs associated with a feature set."}, {"heading": "Optimal Feature Set Teaching Costs", "text": "Next we define a set of costs for a feature set. The first measure is a measure of the cost of specifying the feature set. We measure the representation specification cost of a feature set F by the cardinality of the feature set |Fi|. This idealized measure does not differentiate the effort required to specify features. In practice, different features might require different effort to specify and the cost to specify different features will depend upon the interface through which features are communicated to the learner.\nThe second measure of a feature set is a measure of the cost of specifying a target classification function using the feature set and a given learning algorithm. We measure the optimal concept specification cost by the size of the minimal concept teaching set for F using learner L if F is sufficient and to be infinite otherwise.\nThe third measure of a feature set is a measure of the cost of demonstrating that the feature set is not sufficient for a given learning algorithm. We measure the optimal invalidation cost of a feature set F using learner L by the size of the minimal invalidation set if F is not sufficient and infinite otherwise.\nWe define the optimal feature set cost vector FSCost(F,L) for a feature set F and learning algorithm L. The feature set cost vector is of length three where the first component is the feature\nspecification cost, the second component is the optimal concept specification cost and the third component is the optimal invalidation cost.\nConsider the feature set F2 in Figure 1a. The training set with three objects T = {x2, x5, x8} is a minimal concept teaching set for F2 and a minimal invalidation set for F1. Thus, we can now specify the optimal feature set costs for F2: the representation specification cost is |F2| = 1, the optimal concept specification cost is |T | = 3, the optimal invalidation cost is\u221e (i.e., FSCost(F2,L1NN ) = (1, 3,\u221e)). The optimal feature set cost vectors for other feature sets are shown in Table 1a."}, {"heading": "Analysis of Teaching Protocols", "text": "Figure 2 describes two teaching protocols. In Algorithm 1, the teacher is able to choose whether to add a feature or to add a labeled example. Because the teacher can choose when to add a feature and when to add a labeled example (i.e., the teacher implements the Choose-action function) we call this teaching protocol the Open-Featuring protocol. When adding a feature (the Add-feature function), the teacher selects one of the features that can be taught given the feature lattice F and the teaching protocol adds the feature to the current feature set and retrain the current classifier. When adding a label (the Add-example function), the teacher chooses which labeled example to add to the current training set and the teaching protocol adds the example to the training set and retrains the current classifier.\nIn Algorithm 2, the teacher can only add a feature if there is a prediction error in the training set. From Proposition 1, if we are using a consistent learner we know that this implies that the feature set is not sufficient and indicates the need to add additional features. Note this assumes that the teacher provides correct labels. For a related but alternative teaching protocol that allows for mislabeling errors see Meek (2016). In this protocol, if the current feature set is not sufficient, a teacher adds labeled examples to find an invalidation set which then enables them to add a feature to improve the feature representation. This process of creating invalidation sets continues until a sufficient feature set is identified. An ideal teacher under this protocol would want to minimize the effort to invalidate feature sets that are not sufficient. The cost of doing this for a particular feature set can be measured by the invalidation cost. There is a possibility that one can reuse examples from the invalidation sets\nof previously visited smaller feature sets, but the sum of the invalidation costs along paths in the feature lattice provides an upper bound on the cost of discovering sufficient feature sets.\nGiven these two protocols is natural to compare costs by the number of features added and the number of labeled examples that are added in defining the classifier. We can then associate a teaching cost with each feature set in the feature lattice F . The teaching cost is also a function of the learning algorithm, and the featuring protocol (Open or Error-driven). The optimal teaching costs for Llin and L1NN for different feature sets is given in Table 1b. An infinite label cost indicates that the feature set cannot be used to teach the target classification function using that protocol and learning algorithm. Because our teaching cost has two components, we would need to choose method to combine these two quantities in order to discuss optimal teaching policies. Once the teacher has provided the learner a feature set that is sufficient the teacher needs to teach the concept represented by the c\u2217 classification function. The labeling cost required to do this is captured by the concept specification cost.\nThe Open-Featuring protocol affords the teacher more flexibility than the Error-Driven-Featuring protocol. In particular, assuming that the teacher is an ideal teacher then there would be no reason to prefer the Error-Driven-Featuring protocol. If, however, the teacher is not an ideal teacher, one not always able to identify features that improve the representations or one who benefits from inspecting an invalidation set to identify features, then one might prefer the Error-Driven-Featuring protocol. In particular, this is a possibility that adding a poor feature can increase the labeling cost. For instance, when using L1NN , a poor teacher who has taught the learner to use feature f1 might add feature f2 rather than feature f3 significantly increasing the concept specification cost. In the next section we demonstrate that there is, in fact, unbounded risk for L1NN . One of the short-comings of the Error-Drive-Featuring protocol is that, once the feature set is sufficient the teacher cannot add another feature. For instance, for the example in Figure 1a, F3 and F4 are inaccessible. This might mean that representations that have lower concept specification costs cannot be used to teach c\u2217. For instance, F4 has a concept specification cost of 2 whereas the concept specification cost of F2 is 3. While this difference is not large, it is easy to create an example where the costs differ significantly. In contrast, using the Open-Featuring protocol, a teacher can choose to teach either F2 or F4 trading of the costs of adding features and concept specification (adding labels).\nThe use of the Error-Driven-Featuring protocol can mitigate the risk of poor featuring but, as discussed above, does come with potential costs. An alternative approach to mitigating the risks of featuring is to use a different learning algorithm. If we use Llin, the potential for a increasing the cost for concept specification is when adding a feature is significantly limited. This is discussed in more detail in the next section."}, {"heading": "Bounding Optimal Teaching Cost and Feature Set Costs", "text": "In this section, we provide bounds on the optimal feature set teaching costs and optimal teaching costs for Llin and L1NN with the teaching protocols defined in Section . In this section, we assume that there is a finite set of realizable objects (i.e., |X| <\u221e)."}, {"heading": "Bounding Optimal Feature Set Costs", "text": "We provide a set of propositions each of which provides tight bounds for optimal concept specification costs and optimal invalidation costs for Llin and L1NN . These propositions are presented in Table 2 with their full statements with proofs presented in the full paper.\nThe fact that the optimal concept specification cost is unbounded as a function of the size of the feature set for L1NN is due to the fact that the 1NN classifier is of high capacity. Proposition 7, however, bounds the potential increase in effort required to define the concept when adding a feature for Llin. It is important to note that optimal concept specification cost for Llin can be just two labeled objects but not in general. In fact, one can construct for d > 1, a set of objects and a feature set of size d that requires d + 1 objects to specify a linear hyperplane that generalizes to all of the objects.\nSimilar to the bound on the optimal concept specification cost, the bound optimal invalidation cost for Llin (Proposition 9) is tight. This can be demonstrated by constructing, for d \u2265 0 a set of d + 2 labeled objects in Rd such that any subset of the labeled objects is linearly separable. While Proposition 9 does provide a bound on the invalidation cost L1NN , this bound for Llin is larger than that provided by Proposition 8. We suspect, however, that in practice, the invalidation cost for the linear classifier would typically be far less then d+ 2 for non-trivial d."}, {"heading": "Bounding Teaching Costs", "text": "In this section we consider bounding the cost of teaching a target classification function c\u2217 using learning algorithms L1NN and Llin. First we consider L1NN . Due to Proposition 6, we cannot bound the risk of adding a bad feature and thus cannot bound the teaching costs for our teaching protocols. We can, however, provide bounds for our teaching protocols using Llin. The following proposition provides and upper bound on the teaching cost for a feature set.\nProposition 3 The labeling cost for a sufficient feature set F using an optimal teacher and the Open-Featuring protocol with learning algorithm Llin is \u2264 |F |+ 1.\nFor the Error-driven-featuring protocol the computation of cost is more difficult as we need to account for the cost of invalidating feature sets. Proposition 4 demonstrates a useful connection between the invalidation sets for nested feature sets when using a linear classifier.\nProposition 4 If T is an invalidation set for F , target classification function c\u2217 and a consistent linear learner then T is an invalidation set for F \u2032 \u2282 F .\nFinally, the following proposition provides an upper bound on the teaching cost for a feature set for the learning algorithm Llin.\nProposition 5 The labeling cost for a minimal sufficient feature set F using an optimal teacher and the Error-Driven-Featuring protocol with learning algorithm Llin is \u2264 2(|F |+ 1)."}, {"heading": "Appendix", "text": "In this section we provide proofs for Propositions. Several proofs rely on convex geometry and we assume that the reader is familiar with basic concepts and elementary results from convex geometry. We denote the convex closure of a set of points by conv(S).\nProposition 1 If learning algorithm L is consistent and T is an invalidation set for feature set Fi, target concept c\u2217, and L then Fi is not sufficient for c\u2217 and L.\nProof Let T be an invalidation set for Fi, target concept c\u2217 and consistent learning algorithm L. Aiming for a contradiction, we assume that Fi is sufficient for c\u2217 and L. From the fact that Fi is sufficient for target concept c\u2217 and learning algorithm L then there exists a training set T \u2032 such that L(Fi, T \u2032) = c\u2217. This implies that there is a classification function in the hypothesis class of the learning algorithm that is consistent with any (honest) training set including T . This fact and the fact that T is an invalidation set implies L is not consistent and we have a contradiction. It follows that Fi is not sufficient.\nProposition 2 The learning algorithms L1NN and Llin are monotonically sufficient.\nProof For L1NN we simply node that adding features makes more distinctions between objects thus once sufficient any superset will remain sufficient.\nFor Llin, let d-dimensional feature set F be sufficient for the target classification function. This means that \u2203(w, b) for w \u2208 Rd and b \u2208 R such that c\u2217(x) = sign(w \u00b7 F (x) + b). For F \u2032 \u2283 F if we use an offset b\u2032 = b and a weight vector w\u2032 this agrees with w for any feature f \u2208 F and is zero otherwise is equivalent to the classifier defined by (w, b) (i.e., sign(w\u2032 \u00b7 F \u2032(x) + b) = sign(w \u00b7 F (x) + b)) which proves the claim.\nLemma 1 If finite sets S, T \u2282 Rd that are strictly separable then there exists a subset U \u2286 S \u222a T such that |U | \u2264 d+1 and the maximum margin separating hyperplane defined by U \u2229S and U \u2229T separates S and T .\nProof We define the set of points that are the closest points in the convex closure of S and T (i.e., CP (S, T ) = {(s, t)|s \u2208 conv(S), t \u2208 conv(T ),\u2200s\u2032 \u2208 conv(S)\u2200t\u2032 \u2208 conv(T )dist(s, t) \u2264 dist(s\u2032, t\u2032)}). The maximum margin hyperplane defined by any two points (s, t) \u2208 CP (S, T ) suffice to define a hyperplane that separate S, T (see, e.g., Liu et al 2016). Consider a pair (s, t) \u2208 CP (S, T ). Due the the construction of the set it must be the case that s belongs to some face of conv(S) and similarly t belongs to some face of conv(T ). In fact, the points are a subset of the Cartesian product a face of conv(S) and a face of conv(T ) that share one or more points that are equidistant.\nNext we choose a subset of CP (S, T ) on the basis of the faces to which each of the pair of points belongs. Let dim(x, U) be Euclidean dimension of the minimal face of conv(U) containing x or be\u221e if x is not in a face of conv(U). We define the minimal closest pairs (a subset of CP (S, T )) to be pairs whose summed face Euclidean dimension is minimal (i.e, MinCP (S, T ) = {(s, t) \u2208 CP (S, T )|\u2200s\u2032 \u2208 conv(S),\u2200t\u2032 \u2208 conv(T ), (s\u2032, t\u2032) \u2208 CP (S, T ) implies dim(s, S) + dim(t, T ) \u2264 dim(s\u2032, S) + dim(t\u2032, T )} Let (s, t) \u2208 MinCP (S, T ). Next we establish that dim(s, S) + dim(t, T ) \u2264 d \u2212 1. Suppose this is not the case, that is, ds = dim(s, S), dt = dim(t, T ) and ds + dt \u2265 d. In this case, consider the ds dimensional ball of variation around s and the dt dimensional ball of variation around t. Because , ds + dt \u2265 d there must be a parallel direction of variation. Rays in this direction starting at s and t define pairs of points in CP (S, T ). Following this common direction of variation from both s and t we must either hit a lower dimensional face of conv(S) or conv(T ) which implies that (s, t) 6\u2208MinCP (S, T ). We have a contradiction and thus ds + dt \u2264 d\u2212 1. Finally, if dim(s, S) + dim(t, T ) \u2264 d \u2212 1 then by applying Carathe\u0301odory\u2019s theorem twice we can represent s via ds + 1 point and t via dt + 1 and thus d + 1 points suffice to define a separating hyperplane for S, T using a maximum margin hyperplane.\nProposition 4 If T is an invalidation set for F , target classification function c\u2217 and a consistent linear learner then T is an invalidation set for F \u2032 \u2282 F .\nProof Let T be an invalidation set for F ,c\u2217, and consistent linear learner L. Suppose that T is not an invalidation set for F \u2032. In this case, there are parameters (w\u2032, b\u2032) such that c\u2032(x) = sign(w\u2032 \u00b7 F \u2032(x) + b\u2032) = L(F \u2032, T ) is consistent with T . This means that there are parameters (w, b) such that c(x) = sign(w \u00b7 F (x) + b) is consistent with T and thus T is not an invalidation set for F which is a contradiction. Thus T must be an invalidation set for F \u2032 proving the proposition.\nProposition 3 The labeling cost for a sufficient feature set F using an optimal teacher and the Openfeaturing protocol with learning algorithm Llin is upper-bounded by |F |+ 1.\nProof Follows immediately from Proposition 7.\nProposition 5 The labeling cost for a minimal sufficient feature set F using an optimal teacher and the Error-driven-featuring protocol with learning algorithm Llin is upper-bounded by 2(|F |+ 1).\nProof Consider the ideal teacher that first provides labels to invalidate subsets of F along some path to F in the feature lattice F and then provides labels to teach the classification function. Because F is minimally sufficient consider any subset F \u2032 \u2208 F such that F \u2032 \u2282 F and |F \u2032| + 1 = |F |. F \u2032 is not sufficient and by Proposition 9 there is an invalidation set of size |F |+ 1. Due to Proposition 4 this invalidation set is an invalidation set for all feature sets along paths in F to F \u2032 and thus the examples in this set are sufficient to allow the teacher to add the features in F . In the second phase, the teacher, by Proposition 7 need only provide at most |F |+1 additional labels to create a concept specification set. Thus, in the two phases, the optimal teacher need provide at most 2(|F |+1) labeled examples.\nProposition 6 Adding a single feature to a feature set can increase the concept specification cost variability (by O(|X|)) when using the 1NN learning algorithm.\nProof The example configuration used in the feature set F3 from the example from Figure 1a can be extended to arbitrarily many points.\nProposition 7 For any consistent linear learner, if a d-dimensional feature set F is linearly sufficient for the target classification function then the concept specification cost is at most d+ 1.\nProof Let X be our set of objects and target be our target classification function. Define S = {F (x) \u2208 Rd|x \u2208 X and c\u2217(x) = 1} and T = {F (x) \u2208 Rd|x \u2208 X and c\u2217(x) = 0}. Because F is linearly sufficient then there exists a hyperplane separating the positive X+ an negative examples X\u2212. We then apply Lemma 1 using X+ and X\u2212 to obtain the desired result.\nProposition 8 (Meek 2016) If Fi is not sufficient for the target classification function c\u2217 using learning algorithm L1NN then the invalidation cost for feature set Fi and L1NN is two.\nProposition 9 (Meek 2016) For any consistent linear learner, if d-dimensional feature set F is not linearly sufficient for the target classification function then the representation invalidation cost is at most d+ 2."}], "references": [{"title": "Queries revisited", "author": ["D. Angluin"], "venue": "Theor. Comput. Sci. 313(2):175\u2013194.", "citeRegEx": "Angluin,? 2004", "shortCiteRegEx": "Angluin", "year": 2004}, {"title": "On exact specification by examples", "author": ["M. Anthony", "G. Brightwell", "D. Cohen", "J. Shawe-Taylor"], "venue": "Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT \u201992, 311\u2013318. New York, NY, USA: ACM.", "citeRegEx": "Anthony et al\\.,? 1992", "shortCiteRegEx": "Anthony et al\\.", "year": 1992}, {"title": "Measuring teachability using variants of the teaching dimension", "author": ["F.J. Balbach"], "venue": "Theor. Comput. Sci. 397(1-3):94\u2013113.", "citeRegEx": "Balbach,? 2008", "shortCiteRegEx": "Balbach", "year": 2008}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles"], "venue": "Journal of Machine Learning Research 15:3107\u20133131.", "citeRegEx": "Doliwa et al\\.,? 2014", "shortCiteRegEx": "Doliwa et al\\.", "year": 2014}, {"title": "Design Patterns: Elements of Reusable Object-Oriented Software", "author": ["E. Gamma", "R. Helm", "R. Johnson", "J.M. Vlissides"], "venue": "Addison-Wesley.", "citeRegEx": "Gamma et al\\.,? 1995", "shortCiteRegEx": "Gamma et al\\.", "year": 1995}, {"title": "On the complexity of teaching", "author": ["S. Goldman", "M. Kearns"], "venue": "Journal of Computer and Systems Sciences 50(1):20\u201331.", "citeRegEx": "Goldman and Kearns,? 1995", "shortCiteRegEx": "Goldman and Kearns", "year": 1995}, {"title": "Teaching dimension and the complexity of active learning", "author": ["S. Hanneke"], "venue": "Proceedings of the 20th Annual Conference on Computational Learning Theory (COLT), 6681.", "citeRegEx": "Hanneke,? 2007", "shortCiteRegEx": "Hanneke", "year": 2007}, {"title": "Generalized teaching dimensions and the query complexity of learning", "author": ["T. Heged\u0171s"], "venue": "Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT \u201995, 108\u2013117. New York, NY, USA: ACM.", "citeRegEx": "Heged\u0171s,? 1995", "shortCiteRegEx": "Heged\u0171s", "year": 1995}, {"title": "How many queries are needed to learn? J", "author": ["L. Hellerstein", "K. Pillaipakkamnatt", "V. Raghavan", "D. Wilkins"], "venue": "ACM 43(5):840\u2013862.", "citeRegEx": "Hellerstein et al\\.,? 1996", "shortCiteRegEx": "Hellerstein et al\\.", "year": 1996}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu", "H. Ohannessian"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, ICML \u201916, 117\u2013126.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Analysis of a design pattern for teaching with features and labels", "author": ["C. Meek", "P. Simard", "X. Zhu"], "venue": "NIPS 2016 Future of Interactive Machine Learning Workshop.", "citeRegEx": "Meek et al\\.,? 2016", "shortCiteRegEx": "Meek et al\\.", "year": 2016}, {"title": "A characterization of prediction errors", "author": ["C. Meek"], "venue": "ArXiv.", "citeRegEx": "Meek,? 2016", "shortCiteRegEx": "Meek", "year": 2016}, {"title": "Active Learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool.", "citeRegEx": "Settles,? 2012", "shortCiteRegEx": "Settles", "year": 2012}, {"title": "Machine teaching: an inverse problem to machine learning and an approach toward optimal education", "author": ["X. Zhu"], "venue": "AAAI.", "citeRegEx": "Zhu,? 2015", "shortCiteRegEx": "Zhu", "year": 2015}, {"title": "Models of cooperative teaching and learning", "author": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"], "venue": "Journal of Machine Learning Research 12:349\u2013384.", "citeRegEx": "Zilles et al\\.,? 2011", "shortCiteRegEx": "Zilles et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "First we note that this work shares a common roots with the work of Meek (2016) but there the focus is on prediction errors rather than teaching effort.", "startOffset": 68, "endOffset": 80}, {"referenceID": 5, "context": "For instance, the idea of a concept teaching set is closely related to that of a teaching sequence (Goldman and Kearns 1995) and our optimal concept specification cost is essentially the specification number of a hypothesis (Anthony et al 1992); we add concept to distinguish it from representation specification cost.", "startOffset": 99, "endOffset": 124}, {"referenceID": 0, "context": "Other existing concepts include the exclusion dimension (Angluin 1994) and the unique specification dimension (Hedigus 1995) and the certificate size (Hellerstein et al 1996) which are similar to our invalidation cost. In addition, Liu et al (2016) define the teaching dimension of a hypothesis which is equivalent to the specification number and our concept specification cost.", "startOffset": 57, "endOffset": 249}, {"referenceID": 0, "context": "Other existing concepts include the exclusion dimension (Angluin 1994) and the unique specification dimension (Hedigus 1995) and the certificate size (Hellerstein et al 1996) which are similar to our invalidation cost. In addition, Liu et al (2016) define the teaching dimension of a hypothesis which is equivalent to the specification number and our concept specification cost. They also provide bounds on the concept specification cost for linear classifiers. Their results are related to our Proposition 7 but, unlike our result, assume that the space of objects is dense. In the terms of Zhu (2015), we provide the hypothesis specific teaching dimension for pool-based teaching.", "startOffset": 57, "endOffset": 603}, {"referenceID": 11, "context": "This paper is an extended version of the paper by Meek et al (2016).", "startOffset": 50, "endOffset": 68}, {"referenceID": 6, "context": "Not surprisingly, the work on active learning is related to work on teaching dimension (Hanneke 2007).", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "An excellent survey of research in this area is given by Settles (2012). Not surprisingly, the work on active learning is related to work on teaching dimension (Hanneke 2007).", "startOffset": 57, "endOffset": 72}, {"referenceID": 11, "context": "For a related but alternative teaching protocol that allows for mislabeling errors see Meek (2016). In this protocol, if the current feature set is not sufficient, a teacher adds labeled examples to find an invalidation set which then enables them to add a feature to improve the feature representation.", "startOffset": 87, "endOffset": 99}], "year": 2016, "abstractText": "We study the task of teaching a machine to classify objects using features and labels. We introduce the Error-Driven-Featuring design pattern for teaching using features and labels in which a teacher prefers to introduce features only if they are needed. We analyze the potential risks and benefits of this teaching pattern through the use of teaching protocols, illustrative examples, and by providing bounds on the effort required for an optimal machine teacher using a linear learning algorithm, the most commonly used type of learners in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs of using different learning algorithms and between the effort required for featuring and labeling.", "creator": "LaTeX with hyperref package"}}}