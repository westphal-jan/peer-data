{"id": "1611.06530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Prototypical Recurrent Unit", "abstract": "The difficulty in analyzing LSTM-like relapsing neural networks lies in the complex structure of the relapsing unit, which produces highly complex nonlinear dynamics. In this paper, we are designing a new single relapsing unit, which we call the Prototypical Recurrent Unit (PRU). We are experimentally verifying that PRU functions comparably to LSTM and GRU, potentially enabling the PRU to be a prototypical example of the analytical study of LSTM-like relapsing networks. In addition to these experiments, we are also investigating the memory capacity of LSTM-like networks and gaining some insights.", "histories": [["v1", "Sun, 20 Nov 2016 15:39:43 GMT  (388kb)", "http://arxiv.org/abs/1611.06530v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dingkun long", "richong zhang", "yongyi mao"], "accepted": false, "id": "1611.06530"}, "pdf": {"name": "1611.06530.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dingkun Long", "Richong Zhang", "Yongyi Mao"], "emails": ["zhangrc}@act.buaa.edu.cn,", "yymao@eecs.uottawa.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n06 53\n0v 1\n[ cs\n.L G\n] 2\n0 N"}, {"heading": "Introduction", "text": "Deep learning has demonstrated great power in the recent years and appears to have prevailed in a broad spectrum of application domains (see, e.g., [12, 17]). Despite its great successes, the effectiveness of deep neural networks has not been understood at a theoretical depth. Thus developing novel analytic tools and theoretical frameworks for studying deep neural networks is of the greatest importance at the present time, and is anticipated to be a central subject of machine learning research in the years to come.\nThis work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23]. These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21]. Recently LSTM/GRU recurrent units have also been successfully adopted for modelling other forms of data (e.g., [3] [22]). Despite these successes, the design of LSTM and GRU recurrent units was in fact heuristical; to date there is little theoretical analysis justifying their effectiveness. A particularly interesting observation regarding these networks is that they appear to possess \u201clong-term memory\u201d, namely, being able to selectively \u201cremember\u201d the information from many time steps ago [7]. As one may naturally expect such memorization capability to have played an important role in the working of these networks, this aspect has not been well studied, analytically or experimentally.\nThe difficulty in analyzing recurrent networks resides in the complex structure of the recurrent unit, which induces highly complex nonlinear dynamics. To understand LSTM-like recurrent networks, the methodology explored in this theme of research is\nto maximally simplify the structure of the recurrent unit. That is, we wish to construct an alternative recurrent unit that captures the key components LSTM and GRU but stays as simple as possible. Such a unit can then be used for the study of recurrent networks and its structural simplicity may allow easier analysis in future research.\nTowards that goal, the main objective of this present paper is to design such a recurrent unit and verify that this unit performs comparably to LSTM and GRU. To that end, we develop a new recurrent unit, which we call the Prototypical Recurrent Unit (PRU). We rationalize our design methodology from a system-theoretic perspective where a recurrent unit is understood as a causal time-invariant system in state-space representations. Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18]. This understanding is also exploited in our design of PRU.\nThe performance of PRU is verified and compared against LSTM and GRU via extensive experiments. Using these three kinds of recurrent unit, we not only experiment on constructing a standard language model for character prediction [19], but also test the recurrent units for two controlled learning tasks, the Adding Problem [13], and the Memorization Problem. The latter problem is what we propose in this work specifically for studying the memorization capability of the recurrent networks. All experimental results confirm that PRU performs comparably to LSTM and GRU, achieving the purpose of this paper.\nAs another contribution, our experiments in this work demonstrate that the intrinsic memorization capability of the recurrent units depends critically on the dimension of the state space. The amount of targeted information (for memorization), the duration of memory, and the intensity of the interfering signal also directly impact the memorization performance.\nFinally it is perhaps worth noting that although PRU is designed to be a prototype which hopefully allows for easier analysis in future research, our experiments suggest that it can also be used as a practical alternative to LSTM and GRU. A particular advantage of PRU is its time complexity. In this metric, PRU demonstrates to be superior to both LSTM and GRU."}, {"heading": "State-Space Representations", "text": "In system theory [15], a (discrete-time) system can be understood as any physical or conceptual device that responds to an input sequence x1, x2, . . . and generates an output sequence y1, y2, . . ., where the indices of the sequences are discrete time. In general, each xt and each yt at any time t may be a vector of arbitrary dimensions. We will then use X and Y to denote the vector spaces from which xt and yt take value respectively. We will call X the input space and Y the output space. The behaviour of the system is characterized by a function J that maps the space of all input sequences to the space of all output sequences. Then two systems J and J \u2032 are equivalent if J and J \u2032 are identical as functions.\nThe class of systems that are of primary interest are causal systems, namely those in which the output yt at each time t is independent of all future inputs xt+1, xt+2, . . .. The grand idea in system theory is arguably the introduction of the notion of state to\ncausal systems [15]. This makes state-space models the central topic in system theory, resulting in wide and profound impact on system analysis and design. In a nutshell, the state configuration is an quantity internal to the system, serving as a complete summary of the all past inputs so that given the current state, the current and future outputs are independent of all past inputs.\nIn this perspective, a recurrent unit can be regarded precisely as a causal timeinvariant system in a state-space representation. We now formalize such a state-space representations.\nAt each time instant t, in addition to the input variable xt and output variable yt, the representation of a recurrent unit also contains a state variable st, taking values in a vector space S , which will be referred to as the state space. Before the system is excited by the input, or at time t = 0, it is assumed that the state variable s0 takes certain initial configuration, which is assumed customarily to be the origin 0 \u2208 S .\nThe behavior of the recurrent unit is governed by two functions F : X \u00d7 S \u2192 S and G : X \u00d7 S \u2192 Y as follows. At each time instant t, function F maps the current input xt and the previous state st\u22121 to the current state st, namely, via\nst = F (xt, st\u22121), (1)\nand function G maps the current input xt and the current state st to the current output yt, namely, via\nyt = G(xt, st). (2)\nThat is, in general a recurrent unit can be specified by the tuple (X ,Y ,S, F,G) according to (1) and (2). We call such specification of the recurrent unit Type-I statespace representation of the unit, and denote it by (X ,Y ,S, F,G)I.\nAs a clarification which might be necessary for the remainder of this paper, we pause to remark that in this paper (and under a system-theoretic perspective), the notion of a recurrent unit and that of a recurrent (neural) network are synonyms. In particular, a recurrent unit that operates over n time instances may be viewed as n copies of the same recurrent unit connected in a chain-structured network as shown in Figure 1 (top). In this \u201ctime-unfolded\u201d view, the dependency structure between the variables in Type-I representation is shown in Figure 1 (middle).\nIt is remarkable that Type-I state-space representation is generic for any causal time-invariant system and hence generic for any recurrent unit. It is easy to verify that the recurrent unit in RNN [6], LSTM and GRU networks can all be expressed this way.\nSince we aim at designing a simpler recurrent unit, we now introduce another simpler representation, which we call Type-II state-space representation. This representation is identical to the Type-I representation except that the function G is made to have domain S , or alternatively put, the current output yt at each time t is made dependent only of the current state st. That is, G acts only on st and generates yt via\nyt = G(st). (3)\nUnder this representation, the recurrent unit is specified again by the tuple (X ,Y ,S, F,G), but according to (1) and (3). We denote this representation by (X ,Y ,S, F,G)II. A diagram exhibiting the dependency structure of the variables in this representation is shown in Figure 1 (bottom).\nThe following lemma suggests that Type-II representation has precisely the same expressive power as Type-I representation.\nLemma 1 Given its input and output spaces X and Y , a recurrent unit can be represented by (X ,Y ,S, F,G)I for some choice of S , F and G if and only if it can be represented by (X ,Y , S\u0303 , f, g)II for some choice of S\u0303 , f and g.\nWe now sketch the proof of this lemma. The \u201cif\u201d part of the proof is trivial, since function G in Equation (3) is a special case of function G in Equation (2). The \u201conly if\u201d part can be proved by construction, proceeded as follows. Let (X ,Y ,S, F,G)I be given. Define S\u0303 := X \u00d7S . Let function f : X \u00d7 S\u0303 \u2192 S\u0303 be defined as follows: for each (x, x\u2032, s) \u2208 X \u00d7 X \u00d7 S = X \u00d7 S\u0303 , f(x, x\u2032, s) = (x\u2217, s\u2217) \u2208 X \u00d7 S = S\u0303 , where x\u2217 = 0 \u2208 X and s\u2217 = F (x, s) \u2208 S . Define function g : S\u0303 \u2192 Y as follows: for each (x, s) \u2208 X \u00d7 S = S\u0303 , g(x, s) = G(x, s). Now the lemma can be proved by identifying that systems (X ,Y ,S, F,G)I and (X ,Y , S\u0303, f, g)II are equivalent. This latter fact can be easily established using proof by induction.\nThe significance of this lemma is that every recurrent unit can be represented using Type-II representation, in which the current output is made only dependent of the current state. In the proof of this result, we see that to convert a Type-I representation to a Type-II representation, it may require increasing the dimension of the state space. In\nthe worst case, although often unnecessary in practice, one can make the state space S\u0303 equal to the cartesian product X \u00d7 S of the input space X and the state space S in the Type-I representation."}, {"heading": "Prototypical Recurrent Unit (PRU)", "text": "Given that there is no loss of expressive power in Type-II representation, to arrive at a simplified recurrent unit, we will stay within this representation. That is, for some given choices of vector spaces X , Y , and S , we will design two functions F : X \u00d7 S \u2192 S and G : S \u2192 S for (X ,Y ,S, F,G)II. It is our hope that the designed recurrent unit captures the essence of recurrent unit in LSTM and GRU networks, but stays as simple as possible.\nFrom the previous literature [20], the following properties of LSTM and GRU appear crucial for their effectiveness.\n1. The recurrent unit behaves according to a nonlinear system, where the nonlinearity is induced by the use of nonlinear activation functions such as sigmoid, tanh, or ReLU functions.\n2. The evolution from state st to state st+1 is additive. It has been understood that such a property is critical for eliminating the problem of vanishing or blowing-up gradient in backpropagation.\nBased on this understanding, our design philosophy is to impose these two properties minimally on the recurrent unit. Our hypothesis is that if these two properties are indeed essential, the resulting recurrent unit will behave in a way similar to GRU and LSTM recurrent units and can be used as a prototypical example for in-depth understanding of LSTM/GRU-like recurrent networks.\nSuch a design philosophy naturally results in the following new recurrent unit, which we call the Prototypical Recurrent Unit (PRU) and now describe.\nWe begin with some notations. We consider X = Rm, Y = Rl and S = Rk; all vectors are taken as column vectors; the sigmoid (logistic or soft-max) function will be denoted by \u03c3; when an activation function (\u03c3, tanh, or any other function h : R \u2192 R) applies to a vector, it acts on the vector element-wise and outputs a vector of the same length.\nWith these notations, we describe the functions F and G in (X ,Y ,S, F,G)II that defines PRU. Function F : The function F is defined by the following sequence of function compositions involving two other variables ut \u2208 S and ct \u2208 Rk (we note that although here ct is a k-dimensional vector, it should not be interpreted as a state configuration in S due to its physical meaning).\nut = tanh (Usst\u22121 + Uxxt + bu) (4)\nwhere Us is a k \u00d7 k matrix, Ux is a k \u00d7m matrix, and bu is a k-dimensional vector.\nct = \u03c3 ( CTs st\u22121 + C T x xt + bc )\n(5)\nwhere Cs is a k \u00d7 k matrix, Cx is a k \u00d7m matrix, and bc is a k-dimensional vector.\nst = ct \u2299 st\u22121 + (1 \u2212 ct)\u2299 ut (6)\nwhere \u2299 is the element-wise product. Function G: The function G is defined as follows.\nyt = h(Wst + b) (7)\nwhere W is an l \u00d7 k matrix, b is a length l vector, and h is an activation function. Depending on the applications and the physical meaning of output yt, h can be chosen as \u03c3, tanh, ReLU, or even the identity function.\nAt this point, we have completely defined PRU, which is parameterized by \u03b8 := (Cx, Cs, bc, Ux, Us, bu,W, b)."}, {"heading": "Experimental Study", "text": "Our experimental study serves two purposes. First, we wish to verify that the designed PRU behaves similarly as LSTM and GRU. For this purpose, experiments need to be performed not only for real-world applications, in which one has no control over the datasets, but also for certain meaningful tasks where we have full control over the data. Such controllable tasks will allow a comparison of these recurrent units over arbitrary ranges of data parameter settings, so as to fully demonstrate the performances of the compared recurrent units and reduce the risk of being biased by the statistics of a particular dataset.\nSecond, we wish to take the opportunity to investigate a fundamental aspect of recurrent networks, namely, their memorization capabilities. It has been experimentally observed and intuitively justified that LSTM/GRU-like recurrent unit has \u201clong-term memory\u201d [11]. Motivate by such observations, we are interested in thoroughly studying the memorization capability of these recurrent units and understand what factors may influence their memorization performance.\nAs such, we consider three different learning tasks, where the recurrent networks are trained to solve three different problems: the Memorization Problem, the Adding Problem, and the Character Prediction Problem. The Character Prediction Problem is a well-known problem in the real-world application domain [19] [16]. The Adding Problem is a controllable task, first introduced in [13]. The Memorization Problem is also a controllable task that we introduce in this work, inspired by the idea of a similar task presented in [2].\nAll models in these experiments have the architecture shown in the top diagram of Figure 1. In the description of the experiments, when we speak of \u201cstate space dimension\u201d, for both PRU and GRU, it refers to the length of the vector passed between two consecutive recurrent units in the diagram. In LSTM networks, there are two vectors of the same length passed between two consecutive recurrent units. Although from a system-theoretic perspective, two times this length should be regarded as the state space dimension, this choice would put LSTM in disadvantage. This is because the output of\nthe unit depends only on one of the vectors. For this reason, for LSTM networks, the term \u201cstate space dimension\u201d refers to half of the true state-space dimension.\nExperiments on Memorization Problem and Adding Problem are performed on the computer(Intel(R) Core(TM) i5-4570 CPU @3.20Hz), whereas experiment on Character Prediction Problem is performed on a GeForce GTX 970 GPU. Time cost is evaluated in unit of second."}, {"heading": "Memorization Problem", "text": "To describe this problem, let us first imagine a \u201cmemorization machine\u201d Mmem that behaves as follows. For any given non-negative integers I and N , an input sequence x1, x2, . . . , xI+N of scalar values are fed to the machine, where for t = 1, 2, . . . , I , xt takes on value in {+1,\u22121} each with probability 1/2, and for t = I + 1, I + 2, . . . , I +N , xt is drawn independently from a Gaussian distribution with zero mean and variance \u03b42. After processing the input sequence, the machine generates an output vector (x1, x2, . . . , xI)T of dimension I . That is, as a function, the machine Mmem behaves according to\nMmem(x1, x2, . . . , xI+N ) = (x1, x2, . . . , xI) T .\nThen in the Memorization Problem, the objective is to train a model that simulates the behaviour of Mmem, namely, capable of \u201cmemorizing\u201d the \u201cI bit\u201d \u201ctargeted information\u201d in the beginning of the input sequence, after N symbols of \u201cnoise\u201d or \u201cinterfering signal\u201d enter the model. Obviously, the Memorization Problem is configured by three parameters: I , N , and \u03b4, where I represents the amount of targeted information, N represents the duration of memory, and \u03b4 represents the intensity of noise that might interfere with the memorization behaviour of the model. Modelling: Under a recurrent network model, it is natural to regard the input space X as R and the output space Y as RI , and one may freely configure the dimension k of the state space S . Except at the final time t = I + N , the output yt is discarded, and final output yI+N is used to simulate the output Mmem(x1, x2, . . . , xI+N ) of the memorization machine. Datasets: For each problem setting (I,N, \u03b42), we generate 2000 training examples and 400 testing examples according to the specification of the problem. Training: The training of each model is performed by optimizing the Mean Square Error (MSE) defined as\nEMSE(\u03b8) := E\u2016Mmem(x1, x2, . . . , xI+N )\u2212 yI+N\u2016 2 (8)\nwhere the expectation operation E is taken as averaging over the training examples. Mini-batched Stochastic Gradient Descent (SGD, in fact more precisely, mini-batched Back-Propagation Through Time) is used for this optimization. The batch size is chosen as 100, the learning rate as 10\u22123, and the number of epochs as 1000. Each component of the model parameters is initialized to random values drawn independently from the zero-mean unit-variance Gaussian distribution. Evaluation Metrics: A trained model is evaluated using MSE defined in (8), where the expectation operation E is taken as averaging over the testing examples. For experiment setting (I,N, \u03b42, k), each studied model is trained 50 times with different\nrandom initializations, and the average MSE is taken the performance metric for the experiment setting. Time complexity for the three models are also evaluated. Results: Results are obtained for LTSM, GRU and PRU under various problems settings (I,N, \u03b42) and model state-space dimensions k.\nFigure 2 shows the performance comparison of the three recurrent units. In this figure (and as well in Figures 3 and 4), it can be seen that the three units perform similarly, among which GRU\u2019s performance is superior to the other two, and PRU outperforms LSTM to a certain extent. It can also be observed that with respect to any given parameter, the performance trends of the three units are identical.\nFigure 3 shows how the performance of each unit is related to the problem parameters I , N , and \u03b42. For every unit and a fixed state space dimension k, the following performance trend can be observed.\n\u2022 The performance degrades with increasing I . That is, when the amount of targeted information increases, it becomes more difficult for the unit to memorize this information.\n\u2022 The performance degrades with increasing N . That is, over a long period of time, the units tend to forget the targeted information.\n\u2022 The performance degrades with increasing \u03b42. That is, when the interfering signal become stronger, it is more difficult to memorize the targeted information.\nFigure 4 shows how the performance of each unit varies with the state space dimension k. It is apparent from the figure that as the dimension of state space increases, the performance of each unit improves. This behaviour is sensible, since the role of the state variable in a recurrent unit may be intuitively understood as the \u201ccontainer\u201d for \u201cstoring\u201d information, and large state space would result in larger \u201cstorage capacity\u201d.\nFrom the table below (measured at k = 3 and I = 2), it can be observed that PRU has lowest time complexity, significantly below GRU and LSTM. This is a direct consequence of PRU\u2019s structural simplicity.\nN 1 2 3 4 5 LSTM 0.6374 0.8045 0.9904 1.2219 1.3505 GRU 0.5289 0.6868 0.9424 1.0281 1.1321 PRU 0.3880 0.4915 0.6085 0.7489 0.8351"}, {"heading": "Adding Problem", "text": "To describe the Adding Problem, let Madd be an \u201cadding machine\u201d, which is a function mapping a length-N sequence (x1, x2, . . . , xN ) to a real number. In particular, each xt, t = 1, 2, . . . , N , is a vector in R2, and we may write xt as (xt[1], xt[2])T . At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b42; and in the sequence (x1[2], x2[2], . . . , xN [2]), there are exactly two 1\u2019s, the locations of which are randomly assigned; the remaining values of the sequence all are equal to 0. The behaviour of the adding machine is given by\nMadd(x1, x2, . . . , xN ) :=\nN\u2211\nt=1\nxt[1] \u00b7 xt[2].\nThe objective of the Adding Problem is then to train a model that simulates the behaviour of Madd. Obviously, the Adding Problem is parametrized by the pair (N, \u03b42). Intuitively, the Adding Problem demands higher \u201cmemorization capacity\u201d than the Memorization Problem, since only counting the locations of the two 1\u2019s in the second component the input sequence, there are ( N\n2\n) possibilities.\nModelling: Under a recurrent network model, it is natural to take input space X = R2 and output space Y = R. Except at the final time t = N , the output yt is discarded, and final output yN is used to simulate the output Madd(x1, x2, . . . , xN ) of the adding machine. Datasets: For each problem setting (N, \u03b42), we generate 2000 training examples and 400 testing examples according to the specification of the problem. Training: The training of each model is performed by optimizing the MSE between the yN and Madd(x1, x2, . . . , xN ). A mini-batched SGD method is used for optimization, where we use the same set of training parameters as those in the Memorization Problem, except that the batch size is chosen as 50. Evaluation Metrics: MSE is used as the evaluation metric, and the same averaging process as that for the Memorization Problem is applied. Results: Figure 5 shows the performance comparison of LSTM, GRU and PRU in\nthe Adding Problem, and Figure 6 shows the performance trend of each of the three units with respect varying parameters. Similar to the Memorization Problem, overall the three units perform comparably, with GRU superior to the other two units. It is worth noting in Figure 5, with low state-space dimension (k = 1), PRU appears underperform LSTM. But as the state-space dimension increases, PRU catches up (at k = 2) and even out-performs LSTM (at k = 3). This may be explained as follows. First the Adding Problem demands higher \u201cmemorization capacity\u201d. But as we discussed earlier, PRU uses the Type-II representation, which may need larger state-space for the same representation power.\nThese results also suggest that the three studied units all have identical performance trends with respect to state-space dimension or any given problem parameter. Conclusions similar to those in the Memorization Problem may be obtained. The time complexity of PRU is also the lowest among the three for the Adding Problem(table below, measured at k = 3).\nN 2 4 6 8 10 LSTM 0.4678 0.8097 1.1826 1.5450 2.0387 GRU 0.4346 0.7270 0.9954 1.3106 1.5756 PRU 0.3191 0.5822 0.7367 0.9380 1.2037"}, {"heading": "Character Prediction Problem", "text": "Let Mchar be a \u201ccharacter-prediction machine\u201d, which takes an input sequence (x1, x2, . . . , xN ) of arbitrary length N and produces an output sequence of the same length. The input sequence is fed to the machine one symbol per time unit, and at each time t, the machine is characterized by a function M t\nchar defined by\nM t\nchar(x1, . . . , xt) := xt+1.\nThat is, for every input sequence, the output of the machine is the input sequence shifted in time. Here each symbol xt is a character in a K-character alphabet. Each character in\nthe alphabet is represented by a length-K one-hot vector. The objective of the Character Prediction Problem is then to train a model that simulates the behaviour of Mchar. Modelling: Naturally, both X and Y are taken as RK in the models. The output yt is computed by a soft-max classifier. Dataset: A Shakespeare drama dataset1 is used in this experiment, where each sentence is taken as an input sequence. The dataset consists of 1,115,393 occurrences of characters from an alphabet of size 64, where 90% of the sentences are used for training set and the rest is held out for testing. Training and Evaluation: The objective of this problem is to minimize the (expected) cross entropy loss (CEL)\nECEL(\u03b8) = \u2212E\n( 1\nN\nN\u2211\nt=1\nK\u2211\ni=1\nM t\nchar(x1, . . . , xt)[i] log yt[i]\n)\nwhere we have used v[i] to denote the ith component of vector v, and used N to denote the length of the input sequence. Mini-batched SGD with Adadelta dynamic learning rate [24] is used for optimization. All parameters are randomly initialized in [\u22120.1, 0.1], and the base learning rate is set to 0.8. CEL is used to evaluate the models. Results: Figure 7 plots the performances of the three units as functions of SDG epoch number. The three units show very close performances for the chosen three settings of state space dimension. The table below lists the CEL performance of the three recurrent units at the end of SDG iterations, where PRU appears slightly outperform the other\ntwo.\nState Space Dimension LSTM GRU PRU 64 1.2752 1.3015 1.2245 96 1.2211 1.2132 1.1894\n128 1.1584 1.1968 1.1410 The average training time for PRU, GRU, and LSTM per epoch are respectively 83.65, 104.65 and 188.86 seconds respectively, with PRU leading by a significant margin."}, {"heading": "Concluding Remarks", "text": "This paper presents a new recurrent unit, PRU. Having very simple structure, PRU is shown to perform similarly to LSTM and GRU. This potentially allows the use of\n1https://github.com/karpathy/char-rnn\nPRU as a prototypical example for analytic study of LSTM-like recurrent networks. Its complexity advantage may also make it a practical alternative to LSTM and GRU.\nThis work is only the beginning of a journey towards understanding recurrent networks. It is our hope that PRU may provide some convenience to this important endeavor."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Long short-term memorynetworks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers"], "venue": "PhD thesis, Universita\u0308t Hannover,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "Journal of machine learning research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["K Greff", "R.K. Srivastava", "J Koutnik", "B.R. Steunebrink", "J Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Nonlinear Systems. Prentice-Hall, Englewood Cliffs, NJ", "author": ["H.K. Khalil"], "venue": "3nd edition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Characteraware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": ", [12, 17]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 3, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 7, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 8, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 12, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 21, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 0, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 9, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 19, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 2, "context": ", [3] [22]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": ", [3] [22]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "A particularly interesting observation regarding these networks is that they appear to possess \u201clong-term memory\u201d, namely, being able to selectively \u201cremember\u201d the information from many time steps ago [7].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 13, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 16, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 17, "context": "Using these three kinds of recurrent unit, we not only experiment on constructing a standard language model for character prediction [19], but also test the recurrent units for two controlled learning tasks, the Adding Problem [13], and the Memorization Problem.", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Using these three kinds of recurrent unit, we not only experiment on constructing a standard language model for character prediction [19], but also test the recurrent units for two controlled learning tasks, the Adding Problem [13], and the Memorization Problem.", "startOffset": 227, "endOffset": 231}, {"referenceID": 14, "context": "In system theory [15], a (discrete-time) system can be understood as any physical or conceptual device that responds to an input sequence x1, x2, .", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "causal systems [15].", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "It is easy to verify that the recurrent unit in RNN [6], LSTM and GRU networks can all be expressed this way.", "startOffset": 52, "endOffset": 55}, {"referenceID": 18, "context": "From the previous literature [20], the following properties of LSTM and GRU appear crucial for their effectiveness.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "It has been experimentally observed and intuitively justified that LSTM/GRU-like recurrent unit has \u201clong-term memory\u201d [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "The Character Prediction Problem is a well-known problem in the real-world application domain [19] [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "The Character Prediction Problem is a well-known problem in the real-world application domain [19] [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "The Adding Problem is a controllable task, first introduced in [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "The Memorization Problem is also a controllable task that we introduce in this work, inspired by the idea of a similar task presented in [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": ", N , is a vector in R, and we may write xt as (xt[1], xt[2])T .", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": ", N , is a vector in R, and we may write xt as (xt[1], xt[2])T .", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": ", xN [2]), there are exactly two 1\u2019s, the locations of which are randomly assigned; the remaining values of the sequence all are equal to 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "t=1 xt[1] \u00b7 xt[2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "t=1 xt[1] \u00b7 xt[2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "Mini-batched SGD with Adadelta dynamic learning rate [24] is used for optimization.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "The difficulty in analyzing LSTM-like recurrent neural networks lies in the complex structure of the recurrent unit, which induces highly complex nonlinear dynamics. In this paper, we design a new simple recurrent unit, which we call Prototypical Recurrent Unit (PRU). We verify experimentally that PRU performs comparably to LSTM and GRU. This potentially enables PRU to be a prototypical example for analytic study of LSTM-like recurrent networks. Along these experiments, the memorization capability of LSTM-like networks is also studied and some insights are obtained.", "creator": "LaTeX with hyperref package"}}}