{"id": "1611.08568", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "Bottleneck Conditional Density Estimation", "abstract": "The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variable autoencoder (CVAE), which uses stochastic variables as a bottleneck between input x and target y, both of which are high dimension.The key to effective formation of BCDEs is the hybrid merging of the conditional generative model with a common generative model that uses unlabeled data to regulate the conditional model. We show that the BCDE significantly exceeds the CVAE benchmarks in the MNIST quadrant prediction in the fully monitored case and establishes new benchmarks in the semi-monitored case.", "histories": [["v1", "Fri, 25 Nov 2016 19:42:53 GMT  (4993kb,D)", "http://arxiv.org/abs/1611.08568v1", null], ["v2", "Mon, 27 Feb 2017 08:28:08 GMT  (6383kb,D)", "http://arxiv.org/abs/1611.08568v2", null], ["v3", "Fri, 30 Jun 2017 12:27:01 GMT  (7196kb,D)", "http://arxiv.org/abs/1611.08568v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["rui shu", "hung hai bui", "mohammad ghavamzadeh"], "accepted": true, "id": "1611.08568"}, "pdf": {"name": "1611.08568.pdf", "metadata": {"source": "CRF", "title": "Bottleneck Conditional Density Estimation", "authors": ["Rui Shu", "Hung H. Bui", "Mohammad Ghavamzadeh"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Conditional density estimation (CDE) refers to the problem of estimating a conditional density p(y|x) for the input vector x and target vector y. In contrast to classification where the target y is simply a discrete class label, y is typically continuous or high-dimensional in CDE. Furthermore, we want to estimate the full conditional density (as opposed to its conditional mean in regression), a task that becomes important when the conditional distribution has multiple modes. CDE problems where both x and y are high-dimensional have a wide range of important applications, including video prediction, cross-modality prediction (e.g., given one modality (e.g., images) predict other modalities (e.g., sounds, captions)), model estimation in model-based reinforcement learning, and so on.\nClassical non-parametric conditional density estimators typically rely directly on local Euclidean distance in the original input and target space [7]. This approach quickly becomes ineffective in high-dimensions from both a computational and a statistical point of view. Recent advances in deep generative models have led to new parametric models for high-dimensional CDE tasks, namely the conditional variational autoencoders (CVAE) [21]. CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].\nThe original CVAE suffers from two statistical deficiencies. First, they inherently do not learn the distribution of the input x. We argue that in the case of high-dimensional input x where there might exist a latent low-dimensional representation (such as a low-dimensional manifold) of the data, recovering this structure is of importance, even if the task at hand is to learn about the conditional density p(y|x). Second, for many conditional density estimation tasks, the acquisition of labeled points is costly, motivating the need for semi-supervised CDE. A purely conditional model would not be able to utilize any available unlabeled data2. We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.\nWe focus on a set of deep conditional generative models, which we call bottleneck conditional density estimators (BCDEs). In BCDEs, the input x only influences the target y via layers of bottleneck\n\u2217Work was done at Adobe Research. 2We define a \u201clabeled point\u201d within the context of CDE to be a paired (x, y) sample, and an \u201cunlabeled point\u201d to be either x or y without a paired counterpart. We therefore use labeled/unlabeled and paired/unpaired data interchangeably.\nar X\niv :1\n61 1.\n08 56\n8v 1\n[ st\nat .M\nL ]\n2 5\nN ov\n2 01\n6\nstochastic variables z = {zi} in the conditional generative path. The BCDE naturally has a joint generative sibling model which we denote the bottleneck joint density estimator (BJDE), where the bottleneck z generates x and y independently. Following [13], we propose a hybrid training framework that regularizes the conditionally-trained BCDE parameters toward the jointly-trained BJDE parameters. This is the key feature that enables semi-supervised learning for conditional density estimation in the BCDEs.\nIn addition to our core hybrid framework for training the BCDE, we propose a method to make the BJDE and BCDE recognition models more compact. We show that regularities within the BJDE and BCDE posteriors enables extensive parameter sharing within the recognition model. We call this approach factored inference. A more compact recognition model provides additional protection against over-fitting, making factored inference especially useful in the semi-supervised regime.\nUsing our BCDE hybrid training framework, we establish new benchmarks for the MNIST quadrant prediction task [21] in both the fully-supervised and semi-supervised regimes. Our experiments show that 1) hybrid training improves performance for fully-supervised CDE, 2) combining hybrid training and factored inference enables strong performance in semi-supervised CDE, and that 3) hybrid training encourages the model to learn better representations of both the input and target."}, {"heading": "2 Related work", "text": ""}, {"heading": "2.1 Variational Autoencoders", "text": "The Variational Autoencoder (VAE) is a deep generative model for density estimation. It consists of a latent variable z with unit Gaussian prior z \u223c N (0, Ik) which in turn generates a conditionally Gaussian observable vector x|z \u223c N (\u00b5\u03b8(z), diag(\u03c32\u03b8(z)) where \u00b5 and \u03c32 are non-linear neural networks and \u03b8 represents their parameters. The VAE can be seen as a non-linear generalization of the probabilistic PCA [24], and thus can recover non-linear manifolds in the data. VAE\u2019s flexibility however makes posterior inference of the latent variable intractable. This inference issue is addressed via a recognition model q\u03c6(z|x) which serves as an amortized variational approximation of the intractable posterior p(z|x). Typically q(\u00b7|\u00b7) is also a conditionally Gaussian distribution parameterized by neural networks. Learning the VAE is done by jointly optimizing the parameters of both the generative and recognition models so as to maximize an objective that resembles an autoencoder regularized reconstruction loss [11]\ninf \u03b8,\u03c6\nEx\u223cP\u0303 ( Eq(z|x) [ln p(x|z)] + DKL(q\u03c6(z|x)||p(z)) ) \u2261 Ex\u223cP\u0303Eq\u03c6(z|x) [ ln p(z)p\u03b8(x|z) q\u03c6(z|x) ] . (1)\nWe note that the above objective can be rewritten in a form that exposes its connection to the variational lower bound of the log-likelihood\nsup \u03b8 ( Ex\u223cP\u0303 ln p\u03b8(x)\u2212 inf\u03c6 Ex\u223cP\u0303DKL(q\u03c6(z|x)\u2016p\u03b8(z|x)) ) . (2)\nWe make two remarks regarding minimization of the term DKL(q\u03c6(z|x)\u2016p\u03b8(z|x)) in Eq. (2). First, when q(\u00b7|\u00b7) is a conditionally independent Gaussian, this approximation is at best only as good as the mean-field approximation which minimizes DKL(q\u2016p\u03b8(z|x)) over all independent Gaussian q. Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6]. In practice, both \u03b8 and \u03c6 are jointly optimized in Eq. (1), and the reparameterization trick [11] is used to transform the expectation over z \u223c q\u03c6(z|x) into \u223c N (0, Ik); z = \u00b5\u03c6(x) + diag(\u03c32\u03c6(x)) which leads to an easily obtained stochastic gradient."}, {"heading": "2.2 Conditional VAEs (CVAEs)", "text": "In [21], the authors introduced the conditional version of variational autoencoders. The conditional generative model is similar to the VAE, except that the latent variable z and the generating distribution of y|z are both conditioned on the input x. The conditional generative path is\nz \u223c p\u03b8(z|x) = N (z|\u00b5z,\u03b8(x), diag(\u03c32z,\u03b8(x))), (3) y \u223c p\u03b8(y|x, z) = N (y|\u00b5y,\u03b8(x, z), diag(\u03c32y,\u03b8(x, z))) or Bern(y|\u00b5y,\u03b8(x, z)). (4)\nwhere \u03b8 denotes the parameters of the neural networks used in the generative path. The CVAE is trained by maximizing a lower bound of the conditional likelihood\nln p\u03b8(y|x) \u2265 Eq\u03c6(z|x,y) [ ln p\u03b8(z|x)p\u03b8(y|x, z)\nq\u03c6(z|x, y)\n] , (5)\nusing the same technique for VAE [11, 18] but with a recognition network q\u03c6(z|x, y) = N ( z|\u00b5\u03c6(x, y), diag ( \u03c32\u03c6(x, y) )) taking both x and y as input.\nWhile [21] demonstrated that CVAE can be applied to high-dimensional conditional density estimation, CVAE suffers from two limitations. First, CVAE cannot incorporate unlabeled data. Second, CVAE does not learn the distribution of its input and is thus more susceptible to over-fitting. To resolve these limitations, we present a new approach to conditional density estimation."}, {"heading": "3 Neural bottleneck conditional density estimation", "text": "We provide a high-level overview of our approach (Fig. 1) which consists of a new architecture and a new training procedure. Our new architecture imposes a bottleneck constraint, resulting in a class of conditional density estimators that we call bottleneck conditional density estimators (BCDEs). Unlike the CVAE, the BCDE generative path prevents x from directly influencing y. Following the conditional training paradigm in [21], conditional/discriminative training of the BCDE means maximizing the lower bound of a conditional likelihood similar to Eq. (5),\nln p\u03b8(y|x) \u2265 C(\u03b8, \u03c6;x, y) = Eq\u03c6(z|x,y) [ ln p\u03b8(z|x)p\u03b8(y|z) q\u03c6(z|x, y) ] . (6)\nWhen trained over a dataset L of paired (x, y) samples, the overall conditional training objective is C(\u03b8, \u03c6) = \u2211 x,y\u2208L C(\u03b8, \u03c6;x, y). (7)\nHowever, this approach will suffer from the same limitations as CVAE in addition imposing a bottleneck that limits the flexibility of the generative model. Instead, we propose a hybrid joint and conditional training regime that takes advantage of the bottleneck architecture to avoid over-fitting and support semi-supervision.\nOne component in our hybrid training procedure tackles the problem of estimating the joint density p(x, y). To do this, we use the joint counterpart of the BCDE: the bottleneck joint density estimator (BJDE). Unlike conditional models, the BJDE allows us to incorporate unpaired x and y data during training. Thus, the BJDE can be trained in a semi-supervised fashion. We will also show that the BJDE is well-suited for factored inference (\u00a73.2)\u2014a factorization procedure that makes the parameter space of the recognition model more compact.\nThe BJDE also serves as a way to regularize the BCDE, where the regularization constraint can be viewed as soft tying between the parameters of these two models\u2019 generative and recognition networks. It is via this regularization that the BCDE can benefit from unpaired x and y for conditional density estimation."}, {"heading": "3.1 Bottleneck joint density estimation", "text": "In the BJDE, we wish to learn the joint distribution of x and y. The bottleneck is introduced in the generative path via the bottleneck variable z, which points to x and y (Figs. 2a to 2c). Thus, the variational lower bound of the joint likelihood is\nln p\u03b8\u2032(x, y) \u2265 Jxy(\u03b8\u2032, \u03c6\u2032;x, y) = Eq\u03c6\u2032 (z|x,y) [ ln p(z)p\u03b8\u2032(x|z)p\u03b8\u2032(y|z)\nq\u03c6\u2032(z|x, y)\n] . (8)\nWe use {\u03b8\u2032, \u03c6\u2032} to indicate the parameters of the BJDE networks and reserve {\u03b8, \u03c6} for the BCDE parameters. For samples where x or y is unobserved, we will need to compute the variational lower bound for the marginal likelihoods. Here, the bottleneck plays a critical role. If x were to directly influence y, any attempt to incorporate unlabeled y would require the recognition model to infer the unobserved x from the observed y\u2014a conditional density estimation problem which may be as hard as our original task. In the bottleneck architecture, the conditional independence of x and y given z implies that only the low-dimensional bottleneck needs to be marginalized. Thus the usual variational lower bounds for the marginal likelihoods yield\nln p\u03b8\u2032(x) \u2265 Jx(\u03b8\u2032, \u03c6\u2032;x) = Eq\u03c6\u2032 (z|x) [ ln p(z)p\u03b8\u2032(x|z) q\u03c6\u2032(z|x) ] , (9)\nln p\u03b8\u2032(y) \u2265 Jy(\u03b8\u2032, \u03c6\u2032; y) = Eq\u03c6\u2032 (z|x) [ ln p(z)p\u03b8\u2032(y|z) q\u03c6\u2032(z|y) ] . (10)\nSince z takes on the task of reconstructing both x and y, the BJDE is sensitive to the distributions of x and y and learns a joint manifold over the two data sources. BJDE thus provides the following benefits: 1) learning the distribution of x makes the inference of z given x robust to perturbations in the input, 2) z becomes a joint-embedding of x and y, 3) the model can leverage unlabeled data. Overall, when the observed paired and unpaired samples are i.i.d., the joint training objectives is,\nJ (\u03b8\u2032, \u03c6\u2032) = \u2211 x,y\u2208L Jxy(\u03b8\u2032, \u03c6\u2032;x, y) + \u2211 x\u2208Ux Jx(\u03b8\u2032, \u03c6\u2032;x) + \u2211 y\u2208Uy Jy(\u03b8\u2032, \u03c6\u2032; y), , (11)\nwhere L is a dataset of paired (x, y) samples, and Ux and Uy are data sets of unpaired samples."}, {"heading": "3.2 Factored inference", "text": "When training the BJDE in the semi-supervised regime, we introduce a factored inference procedure that reduce the number of parameters in the recognition model.\nIn the semi-supervised regime, the 1-layer BJDE recognition model requires approximating three posteriors: p(z|x, y) \u221d p(z)p(x, y|z), p(z|x) \u221d p(z)p(x|z), and p(z|y) \u221d p(z)p(y|z). The standard approach would be to assign one recognition network for each approximate posterior. This approach, however, does not take advantage of the fact that these posteriors share the same likelihood functions, i.e., p(x, y|z) = p(x|z)p(y|z). Rather than learning the three approximate posteriors independently, we propose to learn the approximate likelihood functions \u02c6\u0300(z;x) \u2248 p(x|z), \u02c6\u0300(z; y) \u2248 p(y|z) and let \u02c6\u0300(z;x, y) = \u02c6\u0300(z;x)\u02c6\u0300(z; y). Consequently, this factorization of the recognition model enables parameter sharing within the joint recognition model (which is beneficial for semi-supervised learning) and eliminates the need for constructing a neural network that takes both x and y as inputs. The latter property is especially useful when learning a joint model over multiple, heterogeneous data types (e.g. image, text, and audio).\nIn practice, we directly learn recognition networks for q(z|x) and \u02c6\u0300(z; y) and perform factored inference as follows\nq(z|x, y) \u221d q\u03c6\u2032(z|x)\u02c6\u0300\u03c6\u2032(z; y), q(z|y) \u221d p(z)\u02c6\u0300\u03c6\u2032(z; y), (12)\nwhere \u03c6\u2032 parameterizes the recognition networks. To ensure proper normalization in Eq. (12), it is sufficient for \u02c6\u0300 to be bounded. If the prior p(z) belongs to an exponential family with sufficient statistics T (z), we can parameterize \u02c6\u0300\u03c6\u2032(z; y) = exp (\u3008T (z), \u03b7\u03c6\u2032(y)\u3009), where \u03b7\u03c6\u2032(y) is a network such that \u03b7\u03c6\u2032(y) \u2208 {\u03b7|{\u3008T (z), \u03b7\u3009 \u2200z} is upper bounded}. Then the approximate posterior can be obtained by simple addition in the natural parameter space of the corresponding exponential family. When the prior and approximate likelihood are both Gaussians, this is exactly precision-weighted merging of the means and variances [9].\nFactored inference offers a more compact recognition model at the cost of lower flexibility. We observe that when the supervised signal is low relative to the unsupervised signal, the benefits of a compact recognition model significantly outweigh its limitations."}, {"heading": "3.3 Hybrid training", "text": "While the BJDE can be used directly for conditional density estimation, it is not expected to yield good performance. For classification, it has been observed that a generative model trained to estimate the joint distribution may yield sub-optimal performance when compared to a model that was trained discriminatively [15]. Indeed, both [12] and [14] incorporated additional discriminative training into their objective functions in order to successfully perform semi-supervised classification with deep generative models. The necessity of additional discriminative training is attributable to the joint model being mis-specified [13].\nIn other words, when the model is mis-specified, we should not expect the optimal joint model parameters to coincide with the optimal conditional model parameters. To address this, [13] proposed a principled hybrid blending of the joint and conditional models. At its core, [13]\u2019s hybrid blending procedure regularizes the parameters \u03b8 of the conditional model toward the parameters \u03b8\u2032 of the joint model by introducing a prior that softly ties \u03b8\u2032 to \u03b8. However, [13] considers models that only contain generative parameters.\nWe propose a variant of hybrid blending that extends to variational methods for conditional density estimation. Variational methods involve both generative and recognition networks. When blending is applied to models that contain both generative \u03b8 and recognition \u03c6 parameters, it may not be immediately obvious a priori how the conditional model parameters {\u03b8, \u03c6} should be tied to the joint model parameters {\u03b8\u2032, \u03c6\u2032}. We argue that it is reasonable to impose a prior that also ties some\nelements of BCDE\u2019s \u03b8 to BJDE\u2019s \u03c6\u2032. In particular, since BJDE\u2019s q\u03c6\u2032(z|x) and BCDE\u2019s p\u03b8(z|x) both serve the same function of projecting x into the space of z, it is reasonable to impose a prior that regularizes the BCDE generative network p\u03b8(z|x) toward the BJDE recognition network q\u03c6\u2032(z|x). We thus propose the following hybrid objective function,\nH(\u03b8, \u03c6, \u03b8\u2032, \u03c6\u2032) = C(\u03b8, \u03c6) + J (\u03b8\u2032, \u03c6\u2032)\u2212 \u039b(\u03b8, \u03c6, \u03b8\u2032, \u03c6\u2032). (13)\nwhere \u039b regularizes the BCDE parameters (\u03b8, \u03c6) toward the the BJDE parameters (\u03b8\u2032, \u03c6\u2032). Details about how {\u03b8, \u03c6} is tied to {\u03b8\u2032, \u03c6\u2032} are shown in Figure 2 and Table 1. When factored inference is applied, Table 1 shows additional parameter tying between BCDE and BJDE. Note that \u039b regularization can be achieved in a variety of ways. In our experiments, we implement \u039b regularization by initializing the conditional model with the joint model parameters and performing early-stopping. Note that this approach introduces one-way regularization of the BCDE by the BJDE. We are currently exploring alternative approaches that allow for two-way regularization."}, {"heading": "4 Experiments", "text": "We evaluated the performance of our hybrid training procedure on the permutation-invariant MNIST quadrant prediction task [20, 21]. The MNIST quadrant prediction task is a conditional density estimation task where MNIST digits are partially occluded. The model is given the observed region and is evaluated by its perplexity on the occluded region. The MNIST quadrant prediction task consists of three sub-tasks depending on the degree of partial observability. 1-quadrant prediction: the bottom left quadrant is observed. 2-quadrant prediction: the left side is observed. 3-quadrant prediction: the bottom right quadrant is not observed.\nIn the fully-supervised case, the original MNIST training set {x\u2032i} 50000 i=1 is converted into our CDE training set L = {xi, yi}50000i=1 by splitting each image into its observed x and unobserved y regions. Note that the training set does not contain the original digit-class label information. In the nl-label semi-supervised case, we randomly sub-sampled nl pairs evenly across all 10 digits to create our labeled training set L = {xi, yi}nli=1. The remaining nu paired samples are decoupled and put into our unlabeled training sets Ux = {xi}nui=1 , Uy = {yi} nu i=1. The MNIST digits are statically binarized by sampling from the Bernoulli distribution according their pixel values [19]. While the original validation set provides 10000 paired (x, y) samples, we always keep our labeled validation set at one-fifth the size of the labeled training set (nl). The labeled test set size is always kept at 10000. The reported performances are averaged scores over multiple runs along with the standard error.\nWhen training our models, we optimized the various training objectives with Adam [10]. Although our training objective is based on the variational lower bound, our performance scoring metric is the negative conditional log-likelihood score which we approximate using importance sampling with 50 samples [21]. The initial learning rate is set to 0.001 and dropped by a factor of 10 when the validation score plateaus. Training was terminated with early-stopping when the validation score can no longer be improved using a given training objective. We use multi-layered perceptrons (MLPs) for all neural networks in the BCDE. All MLPs are batch-normalized [8] and parameterized with two hidden layers of 500 rectified linear units. All stochastic layers have 50 hidden units. The models were implemented in Python using Theano [22] and Keras [4]."}, {"heading": "4.1 Conditional log-likelihood performance", "text": "Tables 2 to 4 show the performance comparisons between CVAE, hybridly-trained BCDE, and baseline variants of Eq. (13): conditional training (C only) and na\u00efve pre-training (without Jxy). We also evaluate the performance of the BCDE models when factored inference is applied (Table 1), as well as the performances of 2-stochastic-layer BCDE models. The 2-L BCDE has the generative model p(z1:2, y|x) = p\u03b8(z1|x)p\u03b8(z2|z1)p\u03b8(y|z2), and its 2-L BJDE counterpart has the generative model p(z1:2, x, y) = p\u03b8\u2032(z1)p\u03b8\u2032(x|z1)p\u03b8\u2032(z2|z1)p\u03b8\u2032(y|z2). To deal with the multi-layer stochasticity, the BCDE and BJDE are trained using top-down inference [9].\nIn the fully-supervised regime, hybrid-trained BCDE achieves the best performance, significantly improves upon its conditionally-trained counterpart as well as previously reported result for CVAE [21]. In the semi-supervised regime, hybrid-trained BCDE continues to significantly outperform conditionally-trained BCDE. As the labeled training size nl reduces, the benefits of having a\nmore compact recognition model becomes more apparent; hybrid + factored inference achieves the best performance, out-performing hybrid on the nl = 5000 1-quadrant task by as much as 1 nat\u2014a considerable margin [25]. Conditionally-trained BCDE performs very poorly in the semi-supervised tasks, likely due to over-fitting issues. The 2-layer BCDE generally outperforms 1-layer BCDE due to having a more expressive variational family [9, 17].\nConditionally generated samples from the hybrid and conditional models in Figure 3 shows that the models with lower perplexity tend to produce high-entropy conditional image generators that spreads the conditional probability mass over the target output space [23].\nTo investigate the behavior of regularization using the BJDE, we show an example of the validated negative conditional likelihood learning curve during training (Fig. 4) of conditional BCDE and hybrid BCDE. For hybrid BCDE, the curve is divided into two phases: joint (pre-training with Eq. (11)), and hybrid (fine-tuning with Eq. (7)). In the fully-supervised regime, model mis-specification results in BJDE initially being worse than conditionally-trained BCDE. In the semi-supervised regime, however, the benefit of incorporating unpaired data enables BJDE to outperforms BCDE even in the conditional task. In both cases, hybrid training consistently achieves the best performance."}, {"heading": "4.2 Conditional latent space walk", "text": "The 2-layer BJDE, by design, learns to use z1 as a joint manifold for x and y. Hybrid training should ideally preserve z1 as a joint manifold. To evaluate if this is the case, we propose a variant of the MNIST quadrant task called the shift-sensitive top-bottom task. In this task, the objective is to learn to predict the the bottom half of the MNIST digit (y) when given the top half (x). However, we introduce structural perturbation to both the top and bottom halves of the image in our training, validation, and test sets by randomly shifting each pair (x, y) horizontally by the same number of pixels (shift varies between {\u22124,\u22123, . . . , 3, 4}). We then train the 2-layer BCDE using either the conditional or hybrid objective in the fully-supervised regime.\nLatent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1]. To evaluate whether z1 is a manifold of y in the BCDE, we perform a conditional latent space walk as follows. Two x\u2019s are selected and deterministically projected onto the z1 latent space using the mean of p\u03b8(z1|x). Then, we linearly interpolate between the two latent representations, and deterministically decode the interpolated representations in the space of y using the means of p\u03b8(z2|z1) and then p\u03b8(y|z2). In Figure 5, we chose to interpolate between an x shifted two pixels to the left and the same x shifted two pixels to the right. This setup was chosen for easy visual evaluation. Because hybrid training regularizes the BCDE toward the BJDE, hybridly-trained BCDE\nretains the manifold properties of the BJDE, producing meaningful bottom-half digits during z1 latent space interpolation. In comparison, the conditionally-trained BCDE tends to produce illegible bottom-half digits, suggesting its failure in learning a joint manifold."}, {"heading": "4.3 Robustness of representation", "text": "Since hybrid training makes BCDE aware of the distribution of x, it enables the model to be robust against corruption of x. To demonstrate this, we investigate another variant of the MNIST quadrant task called the shift-invariant top-bottom task. This task is similar to the shift-sensitive top-bottom task, but with one key difference: we only introduce structural noise to the top half of the image in our training, validation, and test sets. The goal is thus to learn that the prediction of y (which is always centered) is invariant to the shifted position of x. To eliminate the benefit of unlabeled data, we only perform this task in the fully-supervised regime.\nTable 5 shows that hybrid training makes the BCDE performance more robust against structural corruption in x. Because of its more compact recognition model, factored + hybrid is less vulnerable to over-fitting, resulting in a smaller performance gap between performance on the corrupted and uncorrupted data.\nTo understand why hybrid training makes the BCDE robust against corruption, we compare the latent space representations of x for hybrid versus conditional (Fig. 6). We randomly select two top halves of the digit \u201c3\u201d and shifted each image horizontally between -4 and +4 pixels, for a total of two sets of nine x\u2019s. We deterministically projected each resulting x into the space of z1 and z2 using the learned mean networks, in a manner similar to the latent space walk experiment. For easy visualization, we show the PCA projections of the latent space sub-region populated by the projections of all digits 3 and color-coded all points based on the degree of shifting. The plot for hybrid-BCDE\u2019s z1 representation of x shows that the model learns to untangle shift from other features. This in turn helps the hybrid-BCDE learns a shift-invariant z2 (which only has to reconstruct the un-shifted y), possibly by collapsing the axis along which z1 was shift-sensitive. In contrast, the conditionally-trained BCDE uses both z1 and z2 to learn only a representation of y. Thus, its learned representations are not aware of the systematic shift in x, and do not yield a shift-invariant z2."}, {"heading": "5 Conclusion", "text": "We presented the BCDE, a new framework for high-dimensional conditional density estimation where multiple layers of stochastic neural networks are used as the bottleneck between the input and the output data. To train the BCDE, we proposed a new hybrid training procedure where the BCDE is regularized towards its joint counterpart, the BJDE. The bottleneck constraint implies that only the bottleneck needs to be marginalized when missing either the input x or the output y during training, thus enabling the joint model to be trained in a semi-supervised fashion. The regularization effect between the conditional and the joint model in our hybrid training procedure helps the BCDE, itself a conditional model, to become more robust and can learn from unpaired data in semi-supervised setting. To reduce the complexity of the recognition models of the joint and the conditional models during hybrid training, we introduced factored inference, a technique that leads to more parameter sharing among the recognition networks.\nOur experiments showed that the hybridly-trained BCDE established new benchmark performances on the MNIST quadrant prediction task in both the fully-superivsed and semi-supervised regime. Hybrid training enables the BCDE to learn from unpaired data, which significantly improves performance in the semi-supervised regime. When the supervisory signal is very weak, factored inference prevents over-fitting by additionally tying parameters within the BJDE recognition networks. To understand the BCDE\u2019s strong performance in the fully-supervised regime, we showed that hybrid training transfers the joint embedding properties of the BJDE to the BCDE, allowing the BCDE to learn better representations of both the input x and output y. By learning a better representation of x, the BCDE also becomes robust to perturbations in the input.\nThe success of the BCDE hybrid training framework makes it a prime candidate for other highdimensional conditional density estimation tasks, especially in semi-supervised settings. The advantages of a more compact model when using factored inference also merits further consideration; an interesting line of future work will be to apply factored inference to joint density estimation tasks which involve multiple, heterogeneous data sources."}], "references": [{"title": "Generating Sentences from a Continuous Space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv:1511.06349,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The Helmholtz Machine", "author": ["P. Dayan", "G. Hinton", "R. Neal", "R. Zemel"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Density estimation using Real NVP", "author": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"], "venue": "arXiv:1605.08803,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["C. Fran\u00e7ois"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Graca", "J. Gillenwater", "B. Taskar"], "venue": "JMLR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G. Hinton", "P. Dayan", "B. Frey", "R. Radford"], "venue": "Science,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast Nonparametric Conditional Density Estimation", "author": ["M.P. Holmes", "A.G. Gray", "C.L. Isbell"], "venue": "arXiv:1206.5278,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Ladder Variational Autoencoders", "author": ["C. Kaae S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "arXiv:1602.02282,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv:1312.6114,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-Supervised Learning with Deep Generative Models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "arXiv:1406.5298,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Principled hybrids of generative and discriminative models", "author": ["J. Lasserre", "C. Bishop", "T. Minka"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Auxiliary Deep Generative Models", "author": ["L. Maal\u00f8e", "C. Kaae S\u00f8nderby", "S. Kaae S\u00f8nderby", "O. Winther"], "venue": "arXiv:1602.05473,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "On discriminative vs", "author": ["A. Ng", "M. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes. Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Variational Models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "ArXiv e-prints, 1511.02386, Nov.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ArXiv e-prints, 1401.4082, Jan.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "L. H"], "venue": "Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "X. Yan", "H. Lee"], "venue": "Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Probabilistic Principal Component Analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "On the Quantitative Analysis of Decoder- Based Generative Models", "author": ["Y. Wu", "Y. Burda", "R. Salakhutdinov", "R. Grosse"], "venue": "arXiv:1611.04273,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "arXiv:1512.00570,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational Neural Machine Translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang"], "venue": "arXiv:1605.07869,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Classical non-parametric conditional density estimators typically rely directly on local Euclidean distance in the original input and target space [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 20, "context": "Recent advances in deep generative models have led to new parametric models for high-dimensional CDE tasks, namely the conditional variational autoencoders (CVAE) [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "CVAEs have been applied to a variety of problems, such as MNIST quadrant prediction, segmentation [21], attribute-based image generation [26], and machine translation [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 39, "endOffset": 47}, {"referenceID": 17, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 134, "endOffset": 142}, {"referenceID": 13, "context": "We note that while variational methods [11, 18] have been applied to semi-supervised classification (where y is simply a class label) [12, 14], semisupervised CDE (where y is high-dimensional) remains an open problem.", "startOffset": 134, "endOffset": 142}, {"referenceID": 12, "context": "Following [13], we propose a hybrid training framework that regularizes the conditionally-trained BCDE parameters toward the jointly-trained BJDE parameters.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "Using our BCDE hybrid training framework, we establish new benchmarks for the MNIST quadrant prediction task [21] in both the fully-supervised and semi-supervised regimes.", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "The VAE can be seen as a non-linear generalization of the probabilistic PCA [24], and thus can recover non-linear manifolds in the data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "Learning the VAE is done by jointly optimizing the parameters of both the generative and recognition models so as to maximize an objective that resembles an autoencoder regularized reconstruction loss [11]", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 4, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 5, "context": "Second, this term serves as a form of amortized posterior regularization that encourages the posterior p\u03b8(z|x) to be close to an amortized variational family [2, 5, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 10, "context": "(1), and the reparameterization trick [11] is used to transform the expectation over z \u223c q\u03c6(z|x) into \u223c N (0, Ik); z = \u03bc\u03c6(x) + diag(\u03c3 \u03c6(x)) which leads to an easily obtained stochastic gradient.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "In [21], the authors introduced the conditional version of variational autoencoders.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "using the same technique for VAE [11, 18] but with a recognition network q\u03c6(z|x, y) = N ( z|\u03bc\u03c6(x, y), diag ( \u03c3 \u03c6(x, y) )) taking both x and y as input.", "startOffset": 33, "endOffset": 41}, {"referenceID": 17, "context": "using the same technique for VAE [11, 18] but with a recognition network q\u03c6(z|x, y) = N ( z|\u03bc\u03c6(x, y), diag ( \u03c3 \u03c6(x, y) )) taking both x and y as input.", "startOffset": 33, "endOffset": 41}, {"referenceID": 20, "context": "While [21] demonstrated that CVAE can be applied to high-dimensional conditional density estimation, CVAE suffers from two limitations.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Following the conditional training paradigm in [21], conditional/discriminative training of the BCDE means maximizing the lower bound of a conditional likelihood similar to Eq.", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "When the prior and approximate likelihood are both Gaussians, this is exactly precision-weighted merging of the means and variances [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "For classification, it has been observed that a generative model trained to estimate the joint distribution may yield sub-optimal performance when compared to a model that was trained discriminatively [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "Indeed, both [12] and [14] incorporated additional discriminative training into their objective functions in order to successfully perform semi-supervised classification with deep generative models.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Indeed, both [12] and [14] incorporated additional discriminative training into their objective functions in order to successfully perform semi-supervised classification with deep generative models.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The necessity of additional discriminative training is attributable to the joint model being mis-specified [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "To address this, [13] proposed a principled hybrid blending of the joint and conditional models.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "At its core, [13]\u2019s hybrid blending procedure regularizes the parameters \u03b8 of the conditional model toward the parameters \u03b8\u2032 of the joint model by introducing a prior that softly ties \u03b8\u2032 to \u03b8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "However, [13] considers models that only contain generative parameters.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "We evaluated the performance of our hybrid training procedure on the permutation-invariant MNIST quadrant prediction task [20, 21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 20, "context": "We evaluated the performance of our hybrid training procedure on the permutation-invariant MNIST quadrant prediction task [20, 21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 18, "context": "The MNIST digits are statically binarized by sampling from the Bernoulli distribution according their pixel values [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "When training our models, we optimized the various training objectives with Adam [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Although our training objective is based on the variational lower bound, our performance scoring metric is the negative conditional log-likelihood score which we approximate using importance sampling with 50 samples [21].", "startOffset": 216, "endOffset": 220}, {"referenceID": 7, "context": "All MLPs are batch-normalized [8] and parameterized with two hidden layers of 500 rectified linear units.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "The models were implemented in Python using Theano [22] and Keras [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "To deal with the multi-layer stochasticity, the BCDE and BJDE are trained using top-down inference [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "In the fully-supervised regime, hybrid-trained BCDE achieves the best performance, significantly improves upon its conditionally-trained counterpart as well as previously reported result for CVAE [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 63.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 44.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Models nl = 50000 nl = 25000 nl = 10000 nl = 5000 CVAE [21] 20.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "We report the test set negative conditional log-likelihood scores for the MNIST quadrant prediction task [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "more compact recognition model becomes more apparent; hybrid + factored inference achieves the best performance, out-performing hybrid on the nl = 5000 1-quadrant task by as much as 1 nat\u2014a considerable margin [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 8, "context": "The 2-layer BCDE generally outperforms 1-layer BCDE due to having a more expressive variational family [9, 17].", "startOffset": 103, "endOffset": 110}, {"referenceID": 16, "context": "The 2-layer BCDE generally outperforms 1-layer BCDE due to having a more expressive variational family [9, 17].", "startOffset": 103, "endOffset": 110}, {"referenceID": 21, "context": "Conditionally generated samples from the hybrid and conditional models in Figure 3 shows that the models with lower perplexity tend to produce high-entropy conditional image generators that spreads the conditional probability mass over the target output space [23].", "startOffset": 260, "endOffset": 264}, {"referenceID": 15, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}, {"referenceID": 2, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}, {"referenceID": 0, "context": "Latent space interpolation is a popular approach for evaluating the manifold structure of the latent space [16, 3, 1].", "startOffset": 107, "endOffset": 117}], "year": 2017, "abstractText": "We propose a neural network framework for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are highdimensional. The key to effectively train BCDEs is the hybrid blending of the conditional generative model with a joint generative model that leverages unlabeled data to regularize the conditional model. We show that the BCDE significantly outperforms the CVAE in MNIST quadrant prediction benchmarks in the fully supervised case and establishes new benchmarks in the semi-supervised case.", "creator": "LaTeX with hyperref package"}}}