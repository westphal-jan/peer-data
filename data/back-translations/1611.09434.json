{"id": "1611.09434", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Input Switched Affine Networks: An RNN Architecture Designed for Interpretability", "abstract": "The computational mechanisms by which nonlinear recurring neural networks (RNNs) achieve their goals remain an open question. There are many problem areas in which the intelligibility of the network model is critical for deployment. At this point, we are introducing a recurring architecture consisting of input-driven affine transformations, that is, an RNN without any nonlinearity and with a set of weights per input. We show that this architecture achieves nearly identical performance in Wikipedia text language modeling with the same number of model parameters as conventional architectures do in speech modeling, and can achieve this performance with the potential for computer-assisted acceleration compared to existing methods by pre-composing the composite affine transformations that correspond to longer input sequences. As our architecture is able to understand the mechanisms by which it works using linear methods, we show, for example, how the network combines the present time with the past contributions.", "histories": [["v1", "Mon, 28 Nov 2016 23:48:41 GMT  (922kb,D)", "http://arxiv.org/abs/1611.09434v1", "ICLR 2107 submission:this https URL"], ["v2", "Mon, 12 Jun 2017 20:29:48 GMT  (1889kb,D)", "http://arxiv.org/abs/1611.09434v2", "ICLR 2107 submission:this https URL"]], "COMMENTS": "ICLR 2107 submission:this https URL", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG cs.NE", "authors": ["jakob n foerster", "justin gilmer", "jascha sohl-dickstein", "jan chorowski", "david sussillo"], "accepted": true, "id": "1611.09434"}, "pdf": {"name": "1611.09434.pdf", "metadata": {"source": "CRF", "title": "INTELLIGIBLE LANGUAGE MODELING WITH INPUT SWITCHED AFFINE NETWORKS", "authors": ["Jakob N. Foerster", "Justin Gilmer", "Jan Chorowski", "Jascha Sohl-Dickstein", "David Sussillo"], "emails": ["jakob.foerster@cs.ox.ac.uk,", "jan.chorowski@cs.uni.wroc.pl", "sussillo}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural networks and the general field of deep learning have made remarkable progress over the last few years in fields such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al., 2013). For all of the success of the deep learning approach however, there are certain application domains in which intelligibility of the system is an essential design requirement. One commonly used example is the necessity to understand the decisions that a self-driving vehicle makes when avoiding various obstacles in its path. Another example is the application of neural network methodologies to scientific discovery (Mante et al., 2013). Even where intelligibility is not an overt design requirement, it is fair to say that most users of neural networks would like to better understand the models they deploy.\nThere are at least two approaches to creating intelligible network models. One approach is to build networks as normal, and then apply analysis techniques after training. Often this approach yields systems that perform extremely well, and whose intelligibility is limited. A second approach is to build a neural network where intelligibility is an explicit design constraint. In this case, the typical result is a system that can be understood reasonably well, but may underperform. In this work we follow this second approach and build intelligibility into our network model, yet without sacrificing performance for the task we studied.\nDesigning intelligibility into neural networks for all application domains is a worthy, but daunting goal. Here we contribute to that larger goal by focusing on a commonly studied task, that of character based\n\u2217This work was performed as an intern at Google Brain. \u2020Work done as a member of the Google Brain Residency program (g.co/brainresidency) \u2021Work performed when author was a visiting faculty at Google Brain.\nar X\niv :1\n61 1.\n09 43\n4v 1\n[ cs\n.A I]\n2 8\nN ov\n2 01\nlanguage modeling. We develop and analyze a model trained on a one-step-ahead prediction task of the Text8 dataset, which is 10 million characters of Wikipedia text (Mahoney, 2011). The model we use is a switched affine system, where the input determines the switching behavior by selecting a transition matrix and bias as a function of that input, and there is no nonlinearity. Surprisingly, we find that this simple architecture performs as well as a vanilla RNN, Gated Recurrent Unit (GRU) (Cho et al., 2014), IRNN (Le et al., 2015), or Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) in this task, despite being a simpler and potentially far more computationally efficient architecture.\nIn what follows, we discuss related work, define our Input Switched Affine Network (ISAN) model, demonstrate its performance on the one-step-ahead prediction task, and then analyze the model in a multitude of ways, most of which would be currently difficult or impossible to accomplish with modern nonlinear recurrent architectures."}, {"heading": "2 RELATED WORK", "text": "Work by the authors of (Karpathy et al., 2015) attempted to use character-based language modeling to begin to understand how the LSTM (Hochreiter & Schmidhuber, 1997) functions. In it, they employ n-gram word models to highlight what the LSTM has \u2013 and has not \u2013 learned about the text corpus. They were able to break down LSTM language model errors into classes, such as e.g., \"rare word\" errors. The authors of (Greff et al., 2015) engaged in a large study to understand the relative importance of the various components of an LSTM. The authors of (Collins et al., 2016) performed an enormous hyperparameter study to disentangle the effects of capacity and trainability in a number of RNN architectures.\nAttempts to understand networks in more general contexts include the use of linearization and nonlinear dynamical systems theory to understand RNNs in (Sussillo & Barak, 2013). In feedforward networks the use of linear probes has been suggested by (Alain & Bengio, 2016), and there exist a host of back-propagation techniques used to infer the most important input to drive various components of the feed-forward network, e.g. (Le et al., 2012).\nThe ISAN uses an input-switched affine model. The highly related linear time-varying systems are standard material in undergraduate electrical engineering text books. Probabilistic versions of switching linear models with discrete latent variables have a history in the context of probabilistic graphical models. A recent example is the switched linear dynamical system in (Linderman et al., 2016). Finally, multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in (Sutskever et al., 2011; Martens & Sutskever, 2011). The MRNN architecture is similar to our own, in that the dynamics matrix switches as a function of the input character. However, the MRNN relied on a tanh nonlinearity, while our model is explicitly linear. It is this property of our model which makes it both amenable to analysis, and computationally efficient.\nThe Observable Operator Model (OOM) (Jaeger, 2000) is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling. Unlike the ISAN, the OOM requires that a linear projection of the hidden state corresponds to a normalized sequence probability. This imposes strong constraints on both the model parameters and the model dynamics, and restricts the choice of training algorithms. In contrast, the ISAN applies an affine readout to the hidden state to obtain logits, which are then pushed through a SoftMax to obtain probabilities. Therefore no constraints need to be imposed on the ISAN\u2019s parameters and training is easy using backprop. Lastly, the ISAN is formulated as an affine, rather than linear model. While this doesn\u2019t change the class of processes that can be modeled, it enhances the stability of training and greatly enhances interpretability. We elaborate upon these ideas in Section 5.1."}, {"heading": "3 METHODS", "text": ""}, {"heading": "3.1 MODEL DEFINITION", "text": "In what follows Wx and bx respectively denote a transition matrix and a bias vector for a specific input x, the symbol xt is the input at time t, and ht is the hidden state at time t. Our ISAN model is\ndefined as\nht = Wxt ht\u22121 + bxt . (1)\nThe network also learns an initial hidden state h0. We emphasize the intentional absence of any nonlinear activation function."}, {"heading": "3.2 CHARACTER LEVEL LANGUAGE MODELLING WITH RNNS", "text": "The RNNs are trained on the Text8 Wikipedia dataset, for one-step-ahead character prediction. The Text8 dataset consists only of the 27 characters \u2018a\u2019-\u2018z\u2019 and \u2018_\u2019 (space). Given a character sequence of x1, ...,xt, the RNNs are trained to minimize the cross-entropy between the true next character, and the output prediction. We map from the hidden state, ht, into a logit space via an affine map. The probabilities are computed as\np (xt+1) = softmax (lt) (2) lt = Wro ht + bro, (3)\nwhere Wro and bro are the readout weights and biases, and lt is the logit vector. In line with (Collins et al., 2016) we split the training data into 80%, 10%, and 10% for train, test, and evaluation set respectively. The network was trained with the same hyperparameter tuning infrastructure as in (Collins et al., 2016). Analysis in this paper is carried out on the best-performing ISAN model, which has 1, 271, 619 parameters, corresponding to 216 hidden units, and 27 dynamics matrices Wx and biases bx."}, {"heading": "4 RESULTS AND ANALYSIS", "text": ""}, {"heading": "4.1 ISAN PERFORMANCE ON THE TEXT8 TASK", "text": "The results on Text8 are shown in Figure 1a. For the largest parameter count, the ISAN matches almost exactly the performance of all other nonlinear models with the same number of maximum parameters: RNN, IRNN, GRU, LSTM. All analyses use ISAN trained with 1.28e6 maximum parameters (1.58 bpc cross entropy). Samples of generated text from this model are relatively coherent. We show two examples, after priming with \"annual reve\", at inverse temperature of 1.5, and 2.0, respectively:\n\u2022 \u201cannual revenue and producer of the telecommunications and former communist action and saving its new state house of replicas and many practical persons\u201d\n\u2022 \u201cannual revenue seven five three million one nine nine eight the rest of the country in the united states and south africa new\u201d.\nAs a preliminary, comparative analysis, we performed PCA on the state sequence over a large set of sequences for the vanilla RNN, GRU of varying sizes, and ISAN. This is shown in Figure 1b. The eigenvalue spectra, in log of variance explained, was significantly flatter for the ISAN than the other architectures."}, {"heading": "4.2 DECOMPOSITION OF CURRENT PREDICTIONS BASED ON PREVIOUS TIME STEPS", "text": "Taking advantage of the linearity of the hidden state dynamics for any sequence of inputs, we can decompose the current latent state ht into contributions originating from different timepoints s in the history of the input:\nht = t\u2211 s=0\n( t\u220f\ns\u2032=s+1\nWxs\u2032 ) bxs , (4)\nwhere the empty product when s + 1 > t is 1 by convention, and bx0 = h0 is the learned initial hidden state. This is useful because we can analyze which factors were important in the past, for determining the current character prediction.\nUsing this decomposition and the linearity of matrix multiplication we can also write the unnormalized logit-vector, lt, as a sum of terms linear in the biases,\nlt = bro + t\u2211 s=0 \u03bats (5)\n\u03bats = Wro\n( t\u220f\ns\u2032=s+1\nWxs\u2032 ) bxs , (6)\nwhere \u03bats is the contribution from timestep s to the logits at timestep t, and \u03ba t t = bxt . For notational convenience we will sometimes replace the subscript s with the corresponding input character xs at step s when referring to \u03bats \u2013 e.g. \u03ba t \u2018q\u2019 to refer to the contribution from the character \u2018q\u2019 in a string. Similarly, when discussing the summed contributions from a word or substring we will sometimes write \u03batword to mean the summed contributions of all the \u03ba t s from that source word, \u2211 s\u2208word \u03ba t s \u2013 e.g. \u03bat\u2018the\u2019 to refer to the total logit contribution from the word \u2018the\u2019.\nWhile in standard RNNs the nonlinearity causes interdependence of the bias terms across time steps, in the ISAN the bias terms can be interpreted as independent values that are propagated and transformed through time. We emphasize that \u03bats includes the multiplicative contributions from the Wxs\u2032 for s < s\n\u2032 \u2264 t. It is however independent of prior inputs, xs\u2032 for s\u2032 < s. In Figure 2 we show an example of how this decomposition allows us to understand why a particular prediction is made at a given point in time, and how previous characters influence the decoding. For example, the sequence \u2018_annual_revenue_\u2019 is processed by the ISAN: Starting with an all-zero hidden state, we use equation (6) to accumulate a sequence of \u03bat\u2018_\u2032 ,\u03ba t \u2018a\u2032 ,\u03ba t \u2018n\u2032 ,\u03ba t \u2018n\u2032 , .... These values can then be used to understand the prediction of the network at some time t, by simple addition across the s index, which is shown in Figure 2.\nIn Figure 3 we provide a detailed view of how past characters contribute to the logits predicting the next character. There are two competing options for the next letter in the word stem \u2018reve\u2019: either \u2018revenue\u2019 or \u2018reverse\u2019. We show that without the contributions from \u2018_annual\u2019 the most likely decoding of the character after the second \u2018e\u2019 is \u2018r\u2019 (to form \u2018reverse\u2019), while the contributions from \u2018_annual\u2019 tip the balance in favor of \u2018n\u2019, decoding to \u2018revenue\u2019.\nUsing the decomposition of current step predictions in to \u03bats, we can also investigate how quickly the contributions of \u03bats decay as a function of t\u2212 s. In Figure 4a and 4b, we can see that this contribution decays on two different exponential timescales. We hypothesize that the first time scale corresponds to the decay within a word, while the next corresponds to the decay of information across words and sentences. This effect is also visible in Figure 5.\nWe can also show the relevance of the \u03bats contributions to the decoding of characters at different positions in the word. For examples, we observe that \u03bat\u2018_\u2019 makes important contributions to the prediction of the next character at time t. We show that using only the \u03bat\u2018_\u2019, the model can achieve a cross entropy of < 1 / char when the position of the character is more than 3 letters from the beginning of the word."}, {"heading": "4.3 FROM CHARACTERS TO WORDS", "text": "The ISAN provides a natural means of moving from character level representation to word level. Using the linearity of the hidden state dynamics we can aggregate all of the \u03bats belonging to a given word and visualize them as a single contribution to the prediction of the letters in the next word. This allows us to understand how each preceding word impacts the decoding for the letters of later words.\nIn Figure 5 we show that the words \u2018the\u2019 and \u2018annual\u2019 make large contributions to the prediction of the characters of \u2018r\u2019, \u2018n\u2019 in \u2018revenue\u2019, as measured by the norm of the \u03bat\u2018_the\u2019 and \u03bat\u2018_annual\u2019."}, {"heading": "4.4 SINGULAR VALUES OF TRANSITION MATRICES", "text": "Because the ISAN is composed of switched affine transformations, we can directly use the transition matrices to analyze the system. The magnitudes of the singular values for each character\u2019s Wx are shown in Figure 6 a). The singular value spectra reveal that the space token and letter tokens behave in a very different way. First, the transition matrix corresponding to the space token has much smaller singular values than the matrices corresponding to non-space tokens. This suggests that the space token induces forgetting. Furthermore, with the exception of the space token, all matrices have a very\nlarge first singular value. We find that for all non-space tokens, the first right singular vector has large inner product with the logit readout vector for predicting a space token. This relation is pictured in Figure 6b."}, {"heading": "4.5 CHANGE OF BASIS", "text": "We are free to perform a change of basis on the hidden state, and then to run the affine ISAN dynamics in that new basis. Note that this change of basis is not possible for other RNN architectures, since the action of the nonlinearity depends on the choice of basis.\nIn particular we can construct a \u2018readout basis\u2019 that explicitly divides the latent space into a subspace Pro\u2016 spanned by the rows of the readout matrix Wro, and its orthogonal complement P ro \u22a5 . This representation explicitly divides the hidden state dynamics into a 27-dimensional \u2018readout\u2019 subspace that is accessed by the readout matrix to make predictions, and a \u2018computational\u2019 subspace comprising the remaining 216\u2212 27 dimensions that are orthogonal to the readout matrix. We apply this change of basis to analyze an intriguing observation about the hidden offsets bx: As shown in Figure 7, the norm of the bx is strongly correlated to the log-probability of the unigram x in the training data. Re-expressing network parameters using the \u2018readout basis\u2019 shows that this correlation is not related to reading out the next-step prediction. This is because the norm of the projection of bx into Pro\u22a5 remains strongly correlated with character frequency, while the projections into Pro\u2016 have norms that show little correlation. This indicates that the information content or surprise of a letter is encoded through the norm of the component of bx in the computational space, rather than in the readout space.\nSimilarly, in Figure 8 we illustrate that the structure in the correlations between the bx is due to their components in Pro\u2016 , while the correlation in P ro \u22a5 is relatively uniform. We can clearly see two blocks of high correlations between the vowels and consonants respectively, while b\u2018_\u2019 is uncorrelated to either.\n4.6 COMPARISON WITH n-GRAM MODEL WITH BACKOFF\nWe compared the computation performed by n-gram language models and those performed by the ISAN. An n-gram model with back-off weights expresses the conditional probability p (xt|x1...xt\u22121) as a sum of smoothed count ratios of n-grams of different lengths, with the contribution of shorter n-grams down-weighted by backoff weights. On the other hand, the computations performed by the ISAN start with the contribution of bro to the logits, which as shown in Figure 9a) corresponds to the unigram log-probabilities. The logits are then additively updated with contributions from longer n-grams, represented by \u03bats. This additive contribution to the logits corresponds to a multiplicative modification of the emission probabilities from histories of different length. For long time lags, the additive correction to log-probabilities becomes small (Figure 2), which corresponds to multiplication by a uniform distribution. Despite these differences in how n-gram history is incorporated, we nevertheless observe an agreement between empirical models estimated on the training set and model predictions for unigrams and bigrams. Figure 9 shows that the bias term bro gives the unigram probabilities of letters, while the addition of the offset terms bx accurately predict the bigram distribution of P (xt+1|xt). Shown are both an example, P (x|\u2018_\u2032), and a summary plot for all 27 letters."}, {"heading": "5 DISCUSSION", "text": "In this paper we motivated an input-switched affine recurrent network for the purpose of intelligibility. We showed that a switched affine architecture achieves the same performance, for the same number of maximum parameters, on a language modeling task as do more common RNN architectures, including GRUs and LSTMs. We performed a series of analyses, demonstrating that the simplicity of the latent dynamics makes the trained RNN far easier to understand and interpret."}, {"heading": "5.1 BENEFITS OF AFFINE TRANSITIONS OVER LINEAR", "text": "ISAN uses affine operators to model state transitions assigned to each input symbol. Following eq. (1) each transition consists of matrix multiplication and bias vector addition. An important question is whether the biases are needed and how the ISAN would be impacted if linear transition operators were used instead of affine ones. The answer is two-fold. First, affine dynamics can be exactly implemented using linear operators in a hidden space expanded by one additional dimension. Therefore, the expressivity of ISAN does not depend on choosing a linear or affine formulation. However, we found that the affine parametrization of transitions is much easier to train. We attempted to train models using only linear transitions, but achieved a loss of only 4.1 bits per character, which corresponds to the performance of a unigram character model. Second, affine operators are easier to interpret because they permit easy visualization of contributions of each input token on the final network\u2019s prediction, as demonstrated in Section 4.2."}, {"heading": "5.2 COMPUTATIONAL BENEFITS", "text": "Switched affine networks hold the potential to be massively more computationally and memory efficient for text processing than standard RNNs, as explained in the next two subsections."}, {"heading": "5.2.1 SPARSE PARAMETER ACCESS", "text": "As shown in Figure 1a, the performance for fixed parameter count is nearly identical between the ISAN and other recurrent networks. However, at each time step, only the parameters associated with a single input are used. For K possible inputs and N parameters, the computational cost per update step is O ( N K ) , a factor of K speedup over non-switched architectures. Similarly, the number of\nhidden units is O (\u221a\nN K\n) , a factor of K 1 2 memory improvement for storage of the latent state."}, {"heading": "5.2.2 COMPOSITION OF AFFINE UPDATES", "text": "The memory and computational benefits in Section 5.2.1 are shared by other switched networks. However, ISAN is unique in its ability to precompute affine transformations corresponding to input strings. This is possible because the composition of affine transformations is also an affine transformation. This property is used in Section 4.3 to evaluate the linear contributions of words, rather than characters. This means that the hidden state update corresponding to an entire input sequence can be computed with identical cost to the update for a single character (plus the dictionary lookup cost for the composed transformation). ISAN can therefore achieve very large speedups on input processing, at the cost of increased memory use, by accumulating large lookup tables of the Wx and bx corresponding to common input sequences. Of course, practical implementations will have to incorporate complexities of memory management, batching, etc."}, {"heading": "5.3 FUTURE WORK", "text": "There are some obvious future directions to this work. Currently, we define switching behavior using an input set with finite and manageable cardinality. Studying word-level language models with enormous vocabularies may require some additional logic to scale. Adapting this model to continuous-valued inputs is another important direction. One approach is to use a tensor factorization similar to that employed by the MRNN (Sutskever et al., 2014). Another is to build a language model which switches on bigrams or trigrams, rather than characters or words, targeting an intermediate number of affine transformations.\nTraining very large switched linear models has the potential to be extremely fruitful, due both to their improved computational efficiency, and our ability to better understand and manipulate their behavior."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "We would like to thank Jasmine Collins for her help and advice, and Quoc Le, David Ha and Mohammad Norouzi for helpful discussions."}], "references": [{"title": "Understanding intermediate layers using linear classifier probes", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1610.01644,", "citeRegEx": "Alain and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Alain and Bengio.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Capacity and trainability in recurrent neural networks", "author": ["Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo"], "venue": "ICLR 2017 submission,", "citeRegEx": "Collins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "Jaeger.,? \\Q2000\\E", "shortCiteRegEx": "Jaeger.", "year": 2000}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le", "Marc A. Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg S. Corrado", "J Dean", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Recurrent switching linear dynamical systems", "author": ["Scott W Linderman", "Andrew C Miller", "Ryan P Adams", "David M Blei", "Liam Paninski", "Matthew J Johnson"], "venue": "arXiv preprint arXiv:1610.08466,", "citeRegEx": "Linderman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Linderman et al\\.", "year": 2016}, {"title": "Large text compression benchmark: About the test data, 2011. URL http://mattmahoney. net/dc/textdata. [Online; accessed 15-November-2016", "author": ["Matt Mahoney"], "venue": null, "citeRegEx": "Mahoney.,? \\Q2016\\E", "shortCiteRegEx": "Mahoney.", "year": 2016}, {"title": "Context-dependent computation by recurrent dynamics in prefrontal cortex", "author": ["Valerio Mante", "David Sussillo", "Krishna V Shenoy", "William T Newsome"], "venue": null, "citeRegEx": "Mante et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mante et al\\.", "year": 2013}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Martens and Sutskever.,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2011}, {"title": "Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks", "author": ["David Sussillo", "Omri Barak"], "venue": "Neural computation,", "citeRegEx": "Sussillo and Barak.,? \\Q2013\\E", "shortCiteRegEx": "Sussillo and Barak.", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Neural networks and the general field of deep learning have made remarkable progress over the last few years in fields such as object recognition (Krizhevsky et al., 2012), language translation (Sutskever et al.", "startOffset": 146, "endOffset": 171}, {"referenceID": 17, "context": ", 2012), language translation (Sutskever et al., 2014), and speech recognition (Graves et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 3, "context": ", 2014), and speech recognition (Graves et al., 2013).", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": "Another example is the application of neural network methodologies to scientific discovery (Mante et al., 2013).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "Surprisingly, we find that this simple architecture performs as well as a vanilla RNN, Gated Recurrent Unit (GRU) (Cho et al., 2014), IRNN (Le et al.", "startOffset": 114, "endOffset": 132}, {"referenceID": 10, "context": ", 2014), IRNN (Le et al., 2015), or Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) in this task, despite being a simpler and potentially far more computationally efficient architecture.", "startOffset": 14, "endOffset": 31}, {"referenceID": 7, "context": "Work by the authors of (Karpathy et al., 2015) attempted to use character-based language modeling to begin to understand how the LSTM (Hochreiter & Schmidhuber, 1997) functions.", "startOffset": 23, "endOffset": 46}, {"referenceID": 4, "context": "The authors of (Greff et al., 2015) engaged in a large study to understand the relative importance of the various components of an LSTM.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "The authors of (Collins et al., 2016) performed an enormous hyperparameter study to disentangle the effects of capacity and trainability in a number of RNN architectures.", "startOffset": 15, "endOffset": 37}, {"referenceID": 9, "context": "(Le et al., 2012).", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "A recent example is the switched linear dynamical system in (Linderman et al., 2016).", "startOffset": 60, "endOffset": 84}, {"referenceID": 16, "context": "Finally, multiplicative neural networks (MRNNs) were proposed precisely for character based language modeling in (Sutskever et al., 2011; Martens & Sutskever, 2011).", "startOffset": 113, "endOffset": 164}, {"referenceID": 6, "context": "The Observable Operator Model (OOM) (Jaeger, 2000) is similar to the ISAN in that the OOM updates a latent state using a separate transition matrix for each input symbol and performs probabilistic sequence modeling.", "startOffset": 36, "endOffset": 50}, {"referenceID": 2, "context": "In line with (Collins et al., 2016) we split the training data into 80%, 10%, and 10% for train, test, and evaluation set respectively.", "startOffset": 13, "endOffset": 35}, {"referenceID": 2, "context": "The network was trained with the same hyperparameter tuning infrastructure as in (Collins et al., 2016).", "startOffset": 81, "endOffset": 103}, {"referenceID": 2, "context": "The values reported for all other architectures are taken from (Collins et al., 2016).", "startOffset": 63, "endOffset": 85}, {"referenceID": 17, "context": "One approach is to use a tensor factorization similar to that employed by the MRNN (Sutskever et al., 2014).", "startOffset": 83, "endOffset": 107}], "year": 2017, "abstractText": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.", "creator": "LaTeX with hyperref package"}}}