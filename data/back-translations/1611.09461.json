{"id": "1611.09461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Cost-Sensitive Reference Pair Encoding for Multi-Label Learning", "abstract": "We propose a novel cost-sensitive multi-label classification algorithm called cost-sensitive random pair encoding (CSRPE). CSRPE reduces the cost-sensitive multi-label classification problem to many cost-sensitive binary classification problems through the label powerset approach, followed by the classic one-on-one decomposition. While such a naive reduction leads to exponentially many classifiers, we solve the training challenge of building the many classification problems through random sampling and the predictive challenge of coordination among the many classifiers by decoding the one-on-one decoding as a special case of error correction code. Extensive experimental results show that CSRPE achieves stable convergence and achieves better performance than other ensemble and error correction coding algorithms for multi-label classification.", "histories": [["v1", "Tue, 29 Nov 2016 02:41:28 GMT  (2153kb,D)", "http://arxiv.org/abs/1611.09461v1", null], ["v2", "Fri, 18 Aug 2017 07:26:05 GMT  (5624kb,D)", "http://arxiv.org/abs/1611.09461v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yao-yuan yang", "kuan-hao huang", "chih-wei chang", "hsuan-tien lin"], "accepted": false, "id": "1611.09461"}, "pdf": {"name": "1611.09461.pdf", "metadata": {"source": "CRF", "title": "Cost-Sensitive Random Pair Encoding for Multi-Label Classification", "authors": ["Yao-Yuan Yang", "Chih-Wei Chang", "Hsuan-Tien Lin"], "emails": ["b01902066@ntu.edu.tw", "cwchang@cs.cmu.edu", "htlin@csie.ntu.edu.tw"], "sections": [{"heading": null, "text": "Keywords multi-label \u00b7 cost-sensitive \u00b7 classification"}, {"heading": "1 Introduction", "text": "The multi-label classification problem aims to automatically map any instance to a relevant subset of labels. The problem matches the need of many real-world applications, including tagging web pages with a list of keywords (Katakis et al, 2008), classifying songs with a subset of emotions (Trohidis et al, 2008), and associating\nYao-Yuan Yang CSIE Department, National Taiwan University E-mail: b01902066@ntu.edu.tw\nChih-Wei Chang School of Computer Science, Carnegie Mellon University E-mail: cwchang@cs.cmu.edu\nHsuan-Tien Lin CSIE Department, National Taiwan University E-mail: htlin@csie.ntu.edu.tw\nar X\niv :1\n61 1.\n09 46\n1v 1\n[ cs\n.L G\n] 2\n9 N\nov 2\n01 6\n2 Yao-Yuan Yang et al.\ngenes with some functional classes (Elisseeff and Weston, 2001). Different applications generally require evaluating the performance of multi-label classification algorithms with different criteria, such as the Hamming loss, the 0/1 loss and the F1 score (Tsoumakas et al, 2010).\nMost of the current multi-label classification algorithms are designed based on optimizing few or even none of the evaluation criteria. For instance, Binary Relevance (BR; Tsoumakas and Katakis, 2007) learns a per-label binary classifier to predict the label\u2019s relevance, and naturally targets at the Hamming loss. Classifier Chain (CC; Read et al, 2011) extends BR by using some labels as features within the binary classifier, and thus also targets at the Hamming loss while approximately and greedily optimizing the 0/1 loss (Dembczynski et al, 2010). Label Powerset (LP; Tsoumakas et al, 2010) learns a multi-class classifier that treats each subset of labels as a hyper-class, and targets at the 0/1 loss. Random k-labelsets (RAKEL; Tsoumakas and Vlahavas, 2007) learns an ensemble of LP classifiers, each only on a few labels, and can be explained as optimizing the 0/1 loss from the Multi-label Error-correcting Code framework (ML-ECC; Ferng and Lin, 2013). FastXML (Prabhu and Varma, 2014) is designed to optimize the normalized discounted cumulative gain, a popular criterion for information retrieval. These cost-insensitive algorithms match the needs of some applications, but cannot adapt to other evaluation criteria and hence may not work well for other applications.\nMulti-label classification algorithms that automatically adapt to different evaluation criteria to improve the performance are called cost-sensitive multi-label classification (CSMLC) algorithms, as defined by Li and Lin (2014). A classic algorithm called Probabilistic Classifier Chain (PCC) (Dembczynski et al, 2010), an algorithm that extends from CC, is a state-of-the-art CSMLC algorithm. PCC organizes the labels in a chain, and estimates the conditional probability of the label set by learning a (soft) classifier per label in the train. Then, PCC takes the evaluation criteria into account in the prediction stage through Bayesian inference on the conditional probability. While PCC can in priciple adapt to any evaluation criterion, the inference step can be time-consuming to carry out unless the criterion comes with an efficient inference rule. Currently, efficient inference rules for PCC have been derived for only the Hamming loss, the rank loss, and the F1 score (Dembczynski et al, 2010, 2012, 2011).\nAnother state-of-the-art CSMLC algorithm is Condense Filter Tree (CFT) (Li and Lin, 2014), which is designed by converting the CSMLC problem to a cost-sensitive multi-class classification problem with a na\u0308\u0131ve Label Powerset (LP) reduction. LP takes each distinct subset of labels as a hyperclass, and learns a multi-class classifier to predict the relevant subset (hyperclass). Such a reduction opens a route of connecting CSMLC with the rich literature of cost-sensitive multiclass classification (Domingos, 1999; Beygelzimer et al, 2009; Lin, 2014), but results in exponentially many hyperclasses with respect to the number of labels. The many hyperclasses lead to computational difficulty for most cost-sensitive multi-class classification algorithms. CFT resolves the difficulty by some careful modifications of the Filter Tree (FT) algorithm (Beygelzimer et al, 2009) for cost-sensitive multiclass classification. Similar to PCC, CFT organizes the labels in a chain and learns a classifier per label sequentially in the chain. Nevertheless, instead of taking the criterion into account during prediction like PCC, CFT directly takes the criterion when learning each classifier along with a careful estimation of other labels along\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 3\nthe chain. The resulting CFT algorithm comes with a training time quadratic to the number of labels, and the sequential nature makes it hard to parallelize CFT. The quadratic training time without parallelization can make CFT too slow for CSMLC problems with a large number of labels.\nMulti-label classification with per-label classifiers, such as BR (Tsoumakas and Katakis, 2007) and CFT (Li and Lin, 2014), are generally considered the simpler algorithms in multi-label classification. There are other more sophisticated algorithms that go beyond the per-label classifiers to improve classification performance. Multi-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) is such an algorithm. ML-ECC uses error-correcting code (ECC) to transform the original multi-label classification problem to a bigger multi-label classification problem by adding error-correcting labels. Then, classifiers on those labels, much like ECC for communication, can be used to correct prediction errors made from the original per-label classifiers and improve multi-label classification performance. The correction step is generally carried out by decoding to the nearest neighbor in the ECC-encoded space. While ML-ECC is successful in terms of the hamming loss and the 0/1 loss (Ferng and Lin, 2013), it cannot easily adapt to other evaluation criteria and hence is not a CSMLC algorithm. In fact, extending ECC-based approaches for cost-sensitive learning is not a trivial task, and there is not even any work that utilizes general ECC for cost-sensitive multi-class classification, not to mention CSMLC.\nIn this work, we study the potential of ECC for CSMLC by considering a special type of ECC, the one-versus-one (OVO) coding. OVO is a popular technique that reduces the multiclass classification to many binary classification problems, each for distinguishing a pair of classes. It is well-known that OVO can be seen as a special case of ECC (Dietterich and Bakiri, 1995). In addition, OVO has been extended to some multi-class cost-sensitive classification algorithms like CostSensitive One-versus-one (CSOVO), which is reported to perform better than FT for cost-sensitive multi-class classification (Lin, 2014). The promising performance of CSOVO, with OVO being a special type of ECC, motivates us to study whether a better CSMLC algorithm can be derived from the key ideas of CSOVO and ECC.\nMore specifically, we follow the route of LP (Li and Lin, 2014) to reduce the CSMLC problem to a cost-sensitive multi-class classification problem. Then, we adopt CSOVO to reduce the multi-class classification problem to a voting ensemble of binary classification problems, one for each pair of hyperclasses. The exponential number of hyperclasses introduce training and prediction difficulties, which is resolved by our proposed algorithm, called Cost-sensitive Random Pair Encoding (CSRPE). For the training difficulty, we randomly sample only a fraction of binary classifiers in CSOVO to train, which simply corresponds to a randomly truncated ECC.\nFor the prediction difficulty of voting on an exponential number of hyperclasses, we follow the key idea of ECC to decode to the nearest-neighbor in the ECCencoded space. Random-sampling in training and nearest-neighbor decoding in prediction makes CSRPE a promising CSMLC algorithm in practice.\nIn our experiments on ten real-world datasets, CSRPE converges stably with respect to the code length, and outperforms other encoding approaches. It is able to adapt better to different evaluation criteria when compared with cost-insensitive algorithms, and is superior to the state-of-the-art CFT algorithm for CSMLC. Furthermore, CSRPE inherits from CSOVO that all classifiers of the ensemble\n4 Yao-Yuan Yang et al.\ncan be trained independently, making it easy to parallelize CSRPE to further decrease the training time. The superior performance and the training efficiency makes CSRPE a first-hand choice for CSMLC problems."}, {"heading": "2 Preliminary", "text": "This section will introduce the background and is organized as follows. First we start by defining the classic multi-label classification problem and extend it into cost-sensitive setting forming the cost-sensitive multi-label classification problem, in section 2.1. For section 2.2, we will review related works for multi-label and costsensitive multi-label classification problem and brought up the Multi-Label ErrorCorrecting Code framework. Then we will introduce the Error Correcting Output Code and how it is linked with Multi-Label Error-Correcting Code framework in section 2.3.\n2.1 Problem Definition\nWe follow (Li and Lin, 2014) for the definition of the multi-label classification problem (MLC) and cost-sensitive multi-label classification problem (CSMLC). In the MLC, the goal is generate a hypothesis h that maps the feature vector x \u2208 X \u2286 Rd to its relevant label set Y \u2286 {1 . . .K}. The relevant label set Y is commonly represented as a label vector (bit string) y \u2208 {0, 1}K , whose k-th bit yk = 1 if and only if k \u2208 Y. The generated hypothesis is written as h : x\u2192 y.\nFor CSMLC, we assume that we are able to get how the result is evaluate beforehand. With this additional piece of information, are we able to make the hypothesis predict better under the given evaluation method. Therefore, there exists a known cost function (matrix) C : {0, 1}K \u00d7 {0, 1}K \u2192 R in both training and predicting stage The cost function evaluates each label vector independently. Let y\u0302 be the predicted label vector and y be the ground truth label vector, the function C(y, y\u0302) outputs a real value for evaluating the performance of y\u0302. Unlike traditional MLC, the cost function should not only be used in the predicting stage to evaluate the result, it is also used in the training stage to provide additional information.\nFormally, given a dataset D = {(x(n),y(n))}Nn=1, which contains N training instances (x(n),y(n)) drawn from an unknown distribution P, the goal is to design a algorithm that uses D and C to find a classifier f : X \u2192 {0, 1}K that gives a good prediction on x drawn from P. CSMLC algorithms should be able to utilize the information given by the cost function and adopt toward different cost function with ease. Some commonly used cost functions in CSMLC are defined as follows. Let y[k] be defined as the k-th bit of the label vector y, \u2229 denotes bit-wise and operation, \u222a denotes bit-wise or operation, \u2016y\u20161 returns the number of 1s in label vector y, and [\u00b7] represents the indicator function.\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 5\nHamming(y, y\u0302) = 1\nK K\u2211 k=1 [y[k] 6= y\u0302[k]]\nF1(y, y\u0302) = 2\u2016y \u2229 y\u0302\u20161 \u2016y\u20161 + \u2016y\u0302\u20161\nRankloss(y, y\u0302) = \u2211\ny[i]>y[j]\n( [y\u0302[i] < y\u0302[j]] + 1\n2 [y\u0302[i] = y\u0302[j]]\n)\nAccuracy(y, y\u0302) = \u2016y \u2229 y\u0302\u20161 \u2016y \u222a y\u0302\u20161\n2.2 Related Works\nFor MLC, there already exists some basic approaches such as Binary Relevance (BR) (Tsoumakas and Katakis, 2007) and label powerset (LP) (Tsoumakas et al, 2010). BR constructs a binary classifier for each label (total of K binary classifiers) and predicts on each label independently. LP treats each distinct label vector y as a class and reduced the problem to a 2K-class multi-class classification problem. In other words, LP performs a mapping from label vector y \u2208 {0, 1}K \u2192 {1, . . . 2K} and then we can solve the multi-label classification problem with standard multiclass classification algorithms.\nBR and LP can be thought as two extreme cases for the reduction of multilabel classification problem. BR ignores correlations between labels and treats each label the same. On the other hand, LP considers all possible combinations of label vector to encode label correlations into its reduced sub-problems. The downside of LP is that it creates 2K class which is infeasible under most settings. RAndom K-labeLsets (RAKEL) (Tsoumakas and Vlahavas, 2007) is proposed to strike a balance between these two methods. RAKEL reduces the original MLC into multiple sub-problems randomly. Each sub-problem represents a MLC problem on a subset of the original label set such that it is reduced to a size which is directly solvable for LP For prediction, RAKEL lets each sub-problem votes on each of the label in the label set.\nMulti-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) proposed a more sophisticated framework of using error correcting code (ECC). ML-ECC takes the MLC as a communication problem. It breaks the MLC into multiple sub-problems like previous approaches. Each sub-problem has limited view to the original problem thus is prone to having error. ML-ECC tackles this issues by applying the approaches from the domain of communication in dealing with error. With proper organization of sub-problems, ML-ECC is able to encode redundant information into each sub-problem and use these information to \u201dcorrect\u201d the error. RAKEL can be taken as a special case under ML-ECC framework.\nLet M be the number of bit for the encoded vector (code length). ML-ECC is consists of an ECC encoder enc(\u00b7) : {0, 1}K \u2192 {0, 1}M , an ECC decoder dec(\u00b7) : {0, 1}M \u2192 {0, 1}K , and a base multi-label learner A. The encoder encodes the label vector y \u2208 {0, 1}K to an encoded vector b \u2208 {0, 1}M . The encoded vector\n6 Yao-Yuan Yang et al.\nmay provide redundant information of the original label vector so that if we got an error in prediction of the encoded vector, we can \u201dcorrect\u201d it back during the decoding step. ML-ECC framework can be formulated as following steps:\nTraining: given training set {(x(n),y(n))}Nn=1 \u2013 encoded vectors {b(n)}Nn=1 = {enc(y(n))}Nn=1 \u2013 train {(x(n),b(n))}Nn=1 with A and return classifier h\nPrediction: given feature vector x\n\u2013 predict codeword b\u0303 = h(x) \u2013 return prediction y\u0302 = dec(b\u0303)\nIn ML-ECC\u2019s work, they have discussed and experimented with several encoding/decoding methods taken from communication. Which includes Hamming on Repetition Code (HAMR), Bose-Chaudhuri-Hocquenghem Code (BCH), Lowdensity Parity-check Code (LDPC). They have also reformulated RAKEL to a ECC view as the RAKEL repetition Code (RREP). Their results showed that applying ECC on top of the base multi-label learner can produce better performance. Also the choice of different ECC can have different effect on different evaluation criteria. Among them, HAMR performs the best in hamming loss. They also mentioned that choosing codes with stronger error correcting ability did not necessary give better performance, since it may generated a harder learning problem for the base learner to learn.\nAll the previous multi-label algorithms are not able to extend to cost-sensitive settings. Probabilistic Classifier Chain (PCC) (Dembczynski et al, 2010) is an multi-label classification algorithm that is able extend towards different cost functions. It uses Bayes-optimal decision to estimate probability of each possible label vector for different cost function. Though it requires different inference rule for different cost function and for some cost function the inference rule can not be easily derived.\nCondense Filter Tree (CFT) (Li and Lin, 2014) is the first algorithm to proposed as a CSMLC algorithm, which is able to extend to arbitrary cost function with ease. CFT first reduced the CSMLC into cost-sensitive multi-class classification through LP. Then it adopts the idea in filter tree (Beygelzimer et al, 2009) and designed a tree structure to solve the CSMLC problem and deal with the large amount of classes generated by LP. However, CFT is still not able to handle data with large number of instances nor data with large number of labels and it can not be run in parallel. These pose a serious problem when extending to larger CSMLC problems.\n2.3 Error Correcting Output Code\nLP is able provide us a way of utilizing multi-class algorithms in multi-label setting. We started by reviewing the Error Correcting Output Code (ECOC) (Allwein et al, 2001) framework in multi-class classification problems. It is a meta method that reduces multi-class classification problem into binary classification sub-problems. Assume we have a total of S classes and a code length M . A coding matrix B \u2208 {1, 0,\u22121}S\u00d7M is first generated. There will be M different subproblems and classifiers will be trained on these problems. The coding matrix\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 7\ndefines each of the sub-problems. Let B(s, j) be the element in i-th row and j-th column of the matrix B. The element represents that if the ground truth is class i, then the j-th classifier should have predicted B(s, j). More specifically, let the original dataset be the following:\n{(x(n), y(n))}Nn=1, y(n) \u2208 {1 . . . S}\nFor the m-th binary sub-problem, the training examples given are re-labeled as the following:\n{(x(n), B(y(n),m))}Nn=1\nThen apply a binary learning algorithm to learn on these examples and outputs a binary classifier f (m) : X \u2192 {1,\u22121} . If B(s,m) = 0 (s-th row andm-th column), it means we do not care about how f (m) is classifying class s and exclude all examples with class s from training set. For prediction, assume an given example x. x is given to the M sub-problems to predict independently and can be represented as a predicted code word (f (1)(x), . . . , f (M)(x)). We go through each row of B to find the \u201dclosest\u201d code word. Assume Bi is the closest, the x is predicted as class i. A typical choice of measuring the closeness of label vectors is Hamming distance.\nOne-versus-one (OVO) is a common approach used in multi-class classification. It reduces a S-class classification problem to ( S 2 ) binary classification sub-problems. Each sub-problem solves a classification problem the between a pair of class from the S-classes. OVO can be taken as a special case of ECOC with a coding matrix B with size S \u00d7 ( S 2 ) . Each column corresponds to a sub-problem and each subproblem can be represented as a class pair (u, v), where u, v \u2208 {1 . . . S} and u 6= v and the column have u-th row 1 and v-th row \u22121. During prediction, OVO lets each sub-problem votes on each of the classes independently. Assume a classifier representing class pair (u, v). Given a feature vector x, if this classifier predicts \u22121, then it would give class v a vote. In OVO setting, there is a total of ( S 2 ) votes. The one with the most vote would be the final predicted class. This voting method is essentially the same as finding the closest code word under Hamming distance in OVO\u2019s coding matrix.\nIn multi-class setting Cost-Sensitive One-versus-one (CSOVO) (Lin, 2014), extends OVO approach to cost-sensitive setting. It optimizes towards different cost function by carefully designing the relabeled process and assigning weights to each example. CSOVO enjoys both theoretical guarantee and practical stability. It reduces a cost-sensitive S-class classification problem into an ensemble of ( S 2\n) weighted binary classification problem like the usual OVO. Let N be the number of examples, S be the number of classes, and C : {1 . . . S} \u00d7 {1 . . . S} \u2192 R be the cost function. The cost-sensitive multi-class classification has a form:\n{(x(n), y(n))}Nn=1, y(n) \u2208 {1 . . . S}, C \u2208 RS\u00d7S\nLet \u03b1, \u03b2 be the pair of classes for a sub-problem. The corresponding sub-problem is formulated as follows:\n{x(n), sign(C(\u03b2, y(n))\u2212 C(\u03b1, y(n)))}Nn=1 with weights{|C(\u03b1, y(n))\u2212 C(\u03b2, y(n))|}Nn=1 (1)\n8 Yao-Yuan Yang et al.\nOur proposed approach would utilize the formulation of CSOVO as an inspiration to develop a encoding method that are able to embed the cost information into the code word.\nThrough LP reduction, ECOC approaches are able to be used in the multilabel setting. Though still have to solve the problem that it generates too much sub-problems. But there are some benefits using ECOC approaches. The nature of ECOC is to generate multiple independent binary sub-problems. This makes solving the learning problem in parallel relatively easy. ECOC approaches also have the advantage of opting to train more classifiers to fight for better performance. Unlike CC approaches like PCC and CFT, which are limited to putting the K labels into the training set.\nThere has no previous work on general ECOC that embeds the cost information into the coding matrix even in multi-class domain. Though there are some work for special ECOC like CSOVO. We want to combine the idea of CSOVO into designing an encoding method that is able to embed cost information into the encoded vector. And then use the ML-ECC framework to solve the cost-sensitive multi-label classification problem. The result of ML-ECC shows that the correlation between the error correcting ability of ECC and the hardness of the reduced learning tasks could make the design of encoding/decoding function non-trivial."}, {"heading": "3 Proposed Approach", "text": "The proposed CSMLC algorithm follows the ML-ECC framework, which includes the encoding and decoding approach. Our proposed encoding approach, CostSensitive Random Pair Encoding (CSRPE), is a novel cost-sensitive encoding method which embeds the cost information into the encoded vector. In the decoding stage, we mainly follow the traditional Hamming decoding (Allwein et al, 2001) method, but with a small modification. Unlike PCC, which utilizes cost information in the prediction stage by applying Bayes optimal decision at the end, CSRPE does not need the cost information in the decoding step. Instead, we apply the usual ECOC decoding approach in the decoding step. As a result, the prediction stage can be much more efficient and faster.\nHamming decoding enjoys theoretical guarantee in terms of 0/1 loss in multiclass. It takes the predicted encoded vector and find the \u201cclosest\u201d row in the coding matrix, where the measure of closeness is Hamming distance dH(\u00b7, \u00b7), the number of different bits between two encoded vector. Hamming distance between encoded vectors is defined as following:\ndH(u,v) = \u03a3 M i=1|ui \u2212 vi|/2, where u,v \u2208 {1, 0,\u22121}M\nThe decoding can be formulated as given an encoded vector\ndec(b) = argmin y\u2208{0,1}K dH(enc(y),b),\nwhere b \u2208 {1,\u22121}M . Finding the closest vector under hamming distance can be linked to the classic nearest neighbor problem in which various off-the-shelf algorithms are available. For example, KD Tree (Bentley, 1975) and Ball Tree (Liu et al, 2006). When M\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 9\nis larger than the normal size where KD Tree or Ball Tree can work efficiently, approximated nearest neighbor algorithms like locality-sensitive hashing (LSH) (Andoni and Indyk, 2006) can be adapted.\nAlthough efficient algorithms exist, generating 2K different encoded vectors and checking through all of them for nearest neighbor would still be computationally infeasible. Therefore, we proposed to only work on a subset of label vector for encoding: particularly, the subset of label vectors that is more likely to be the prediction. Hence, we choose a subset Z \u2286 {0, 1}M from the whole label space as candidate set for the searching of nearest neighbor. A reasonable choice of Z would be {enc(y\u0303|y\u0303 \u2208 D)}, which are the encoded vectors of all the distinct label vectors in the training set. Our experiment shows that even if we consider the label vectors that only showed up in the testing set but not in training set in Z, the result will not improve. This experiment justifies our choice. Also from a multi-class point of view, if a class (label vector) did not show up in the training set, the chance of predicting well on that class is small.\nFor encoding, we borrow the idea from LP and CSOVO. The two frameworks ML-ECC and ECOC can be bridge by LP. With each distinct label vector mapped to a unique class, the encoding function in ML-ECC can be expressed by the\ncoding matrix in ECOC with size 2K \u00d7 ( 2K\n2\n) . While CSOVO can be deemed as\na special case in ECOC we want to utilized the well designed sub-problems in CSOVO in our encoding approach. This idea is first seem counter-intuitive since the number of reduced sub-problems are growing exponentially, the problem isn\u2019t feasible even with small K. We took a different view apart from the traditional multi-class classification problems, which is that every classes is treated completely independently. Label vectors in multi-label problems have correlations especially under cost-sensitive settings. Therefore many of the sub-problems are actually redundant. CSRPE exploits this nature of multi-label problems and designed a novel encoding method.\nHere is an example to show the redundancy of directly applying CSOVO on CSMLC. Let\u2019s assume we are using Hamming loss, K = 4, \u03b1 = {1, 0, 1, 0},\u03b2 = {1, 0, 0, 1}. The classifier with pair \u03b1 and \u03b2 is able to represent multiple classifiers with different label vector pairs. By the original design of CSOVO the 4 classifiers with following pairs will share the same sub-problem: (\u03b1 = {0, 0, 1, 0}, \u03b2 = {0, 0, 0, 1}), (\u03b1 = {1, 0, 1, 0}, \u03b2 = {1, 0, 0, 1}), (\u03b1 = {1, 1, 1, 0}, \u03b2 = {1, 1, 0, 1}), (\u03b1 = {0, 1, 1, 0}, \u03b2 = {0, 1, 0, 1}). Similar phenomenon can happen under other cost functions.\nWith the idea above, CSRPE is designed as following. Let each i corresponds to a pair of label vectors \u03b1(i), \u03b2(i) \u2208 {0, 1}K drawn from 2-combination of all the elements in the label space {0, 1}K . Given a label vector y, the encoded vector b is encoded as following (bi be the i-th bit of b):\nbi =  1 if C(y, \u03b1(i)) < C(y, \u03b2(i)) 0 if C(y, \u03b1(i)) = C(y, \u03b2(i))\n\u22121 if C(y, \u03b1(i)) > C(y, \u03b2(i))\nEach encoded bit represents whether the given label vector y suffers less cost predicted as \u03b1(i) than \u03b2(i), suffers more cost predicted as \u03b1(i) than \u03b2(i), or it is a tie situation. More intuitively, each bits encodes whether an instance is \u201dcloser\u201d to \u03b1(i) or \u03b2(i) in terms of cost.\n10 Yao-Yuan Yang et al.\nAdditionally, when training base multi-label classifier on encoded vector b with BR, each training instance (x(n),y(n)) on i-th bit is given a weight |C(\u03b1(i),y(n))\u2212 C(\u03b2(i),y(n))| according to original CSOVO. Intuitively this assign of weight gives the base learner more information about whether the label vector y(n) is \u201dclose\u201d to either side by a lot or just a small difference.\nWe apply CSRPE on the ML-ECC framework and use BR as base multi-label learner to solve cost-sensitive multi-label classification problem. CSRPE directly\nplay the role of the encoder b = enc(y) and has a length of ( 2K\n2\n) . And we adopt the\nprevious modified Hamming decoding as the decoding function. During training, each bit corresponds to a sub-problem and these sub-problems are redundant as mentioned. We truncated the b to length M uniformly random before training it with BR. Experiment results shows that with the increase of code length M , the performance can converge fast, steadily and also well performed. This indicates that the codes that has been truncated can be covered by others that has been sampled."}, {"heading": "4 EXPERIMENTS", "text": "We have conducted four different experiments to evaluate different aspects of CSRPE. The experiments is evaluated with four different evaluation criteria from ten real-world datasets. Evaluation criteria consider here includes hamming loss, f1 score (F1(y, y\u0302) =\n2\u2016y\u2229y\u0302\u20161 \u2016y\u20161+\u2016y\u0302\u20161 ), rank loss (Rankloss(y, y\u0302) = \u2211 yi>yj ( [y\u0302i < y\u0302j ] + 1 2 [y\u0302i = y\u0302j ] ) )\nand accuracy score (Accuracy(y, y\u0302) = \u2016y\u2229y\u0302\u20161\u2016y\u222ay\u0302\u20161 ). The datasets are downloaded from Mulan (Tsoumakas et al, 2011). Table 1 lists the statistics of these datasets. The last two column (density, distinct) lists the average percentage of 1s in each label vector in each sample and the number of distinct label vectors respectively.\n4.1 Compare with ECC and CFT\nThe purpose of this experiment is to estimate the performance and convergence rate between CSRPE and other encoding methods under ML-ECC framework. In the experiment, datasets are split into 20 different splits. Each split has a proportion of 50-50 for training and testing. The experiments for each dataset carried out on these 20 different splits. The base multi-label chosen is BR and the base learner for BR is decision tree (Breiman et al, 1984) from Scikit-Learn (Pedregosa\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 11\net al, 2011). The error correcting codes chosen in this experiments are Repetition Code (REP), Hamming on Repetition Code (HAMR), RAKEL Repetition Code (RREP). There are multiple repeated bits in REP and HAMR, so the base learners are trained with bagging (Breiman, 1996). REP will be equivalent to training BR with random forest (Breiman, 2001), and RREP is equivalent to RAKEL.\nAdditionally we also compare the performance with CFT since we want to compare the performance with state-of-the-art cost-sensitive multi-label algorithm. Although Probabilistic Classifier Chain (PCC) (Dembczynski et al, 2010) can be optimized towards different cost function with a limited fashion. It does not have a performance better than CFT, therefore we only compare with CFT here.\nParameters: The number of encoded bits (M) for all ECC are set to 3000. Hyper parameters that are tuned for base classifier (decision tree) is the maximum depth of each decision tree, which are searched from 5 to 35 with a interval of 5 via three-fold cross-validation. RAKEL are set with k=3 and the number of iterations for CFT are set to 8. These settings are suggested by their paper. Other parameters for base learner that are not mentioned are set as default settings of Scikit-Learn. The base learner for CFT is random forest, the total amount of decision tree in the forest is the same amount of encoded bits (M) as previous previous error correcting code settings.\nThe results shown in Table 2, 3, 4 and 5 lists the mean and standard error (standard deviation divided by the square root of number of runs) of 20 different random runs. The best result of each dataset is in bold. Note that CFT do not have results on dataset Corel5k and bibtex since CFT could not finish the experiments on these datasets in 10 days.\nTable 6 lists the number of datasets that CSRPE win, tie or losses based on t-test at 95% confident level. From the result, all five algorithms do pretty well under hamming loss. CSRPE outperforms others in f1 score, rank loss, accuracy score, and strikes even with CFT on rank loss. This shows that CSRPE is able to perform better than previous encoding methods as well as state-of-the-art costsensitive multi-label classification algorithm.\nFigure 1, 2, 3, and 4 shows different cost function versus the number of encoded bits. The x-axis represents the encoded bits (M) and y-axis represents the score/loss. These figures can be used to compare between the convergence rate between different encoding methods on different datasets. We can see that HAMR and REP has oscillation with their performance versus number of encoded bits, which needs higher amount of bits to stabilize. In these figures, CSRPE shows a fast and stable convergence rate on most datasets comparing with other encoding methods. Note that not all dataset are plotted to 3000 code length since most of the dataset don\u2019t need that long of code length to converge.\n1 2\nY a o -Y u a n Y a n g et a l.\nTable 2: Experimental results on hamming loss (best (lowest) ones are marked in bold)\nhamming \u2193 CSRPE REP (BR) RRPE (RAKEL) HAMR CFT bibtex 0.0134\u00b1 0.0000 0.0124\u00b1 0.0000 0.0130\u00b1 0.0000 0.0124\u00b1 0.0000 \u2212\nCorel5k 0.0108\u00b1 0.0001 0.0095\u00b1 0.0000 0.0097\u00b1 0.0000 0.0094\u00b1 0.0000 \u2212 yeast 0.1891\u00b1 0.0006 0.1941\u00b1 0.0007 0.1933\u00b1 0.0006 0.1932\u00b1 0.0007 0.2080\u00b1 0.0010 scene 0.0821\u00b1 0.0007 0.0914\u00b1 0.0004 0.0970\u00b1 0.0005 0.2497\u00b1 0.0025 0.1031\u00b1 0.0009 enron 0.0500\u00b1 0.0003 0.0489\u00b1 0.0002 0.0499\u00b1 0.0002 0.0485\u00b1 0.0002 0.0477\u00b1 0.0002\nmedical 0.0100\u00b1 0.0002 0.0104\u00b1 0.0002 0.0107\u00b1 0.0001 0.0102\u00b1 0.0002 0.0111\u00b1 0.0002 genbase 0.0014\u00b1 0.0001 0.0012\u00b1 0.0001 0.0011\u00b1 0.0001 0.0011\u00b1 0.0001 0.0016\u00b1 0.0001 emotions 0.1994\u00b1 0.0022 0.1966\u00b1 0.0021 0.2110\u00b1 0.0022 0.1953\u00b1 0.0019 0.2207\u00b1 0.0020 CAL500 0.1651\u00b1 0.0005 0.1522\u00b1 0.0010 0.1416\u00b1 0.0003 0.1490\u00b1 0.0005 0.1422\u00b1 0.0005\nflags 0.2585\u00b1 0.0038 0.2591\u00b1 0.0037 0.2591\u00b1 0.0027 0.2599\u00b1 0.0037 0.2899\u00b1 0.0040\nTable 3: Experimental results on f1 score (best (highest) ones are marked in bold)\nf1 \u2191 CSRPE REP (BR) RRPE (RAKEL) HAMR CFT bibtex 0.4663\u00b1 0.0008 0.3636\u00b1 0.0009 0.3761\u00b1 0.0010 0.3658\u00b1 0.0008 \u2212\nCorel5k 0.2455\u00b1 0.0012 0.0683\u00b1 0.0011 0.1028\u00b1 0.0010 0.0608\u00b1 0.0008 \u2212 yeast 0.6670\u00b1 0.0012 0.6119\u00b1 0.0014 0.6130\u00b1 0.0011 0.6171\u00b1 0.0015 0.6111\u00b1 0.0024 scene 0.7860\u00b1 0.0020 0.5895\u00b1 0.0026 0.5926\u00b1 0.0019 0.5442\u00b1 0.0046 0.6592\u00b1 0.0027 enron 0.5911\u00b1 0.0014 0.5441\u00b1 0.0026 0.5336\u00b1 0.0025 0.5459\u00b1 0.0023 0.5530\u00b1 0.0013\nmedical 0.8203\u00b1 0.0023 0.7883\u00b1 0.0028 0.7757\u00b1 0.0034 0.7877\u00b1 0.0031 0.7970\u00b1 0.0031 genbase 0.9878\u00b1 0.0008 0.9897\u00b1 0.0012 0.9899\u00b1 0.0012 0.9896\u00b1 0.0012 0.9845\u00b1 0.0009 emotions 0.6655\u00b1 0.0035 0.5968\u00b1 0.0038 0.5773\u00b1 0.0047 0.6100\u00b1 0.0035 0.6015\u00b1 0.0043 CAL500 0.4083\u00b1 0.0017 0.3388\u00b1 0.0014 0.3527\u00b1 0.0011 0.3152\u00b1 0.0012 0.3815\u00b1 0.0016\nflags 0.7222\u00b1 0.0041 0.6954\u00b1 0.0045 0.6965\u00b1 0.0044 0.7005\u00b1 0.0044 0.6725\u00b1 0.0055\nC o st-S en sitiv e R a n d o m P a ir E n co d in g fo r M u lti-L a b el C la ssifi ca tio n 1 3\nTable 4: Experimental results on rank loss (best (lowest) ones are marked in bold)\nrank loss \u2193 CSRPE REP (BR) RRPE (RAKEL) HAMR CFT bibtex 104.9449\u00b1 0.3814 132.5571\u00b1 0.2981 124.1144\u00b1 0.2511 131.4786\u00b1 0.2819 \u2212 Corel5k 490.1698\u00b1 1.1959 618.0857\u00b1 0.6695 597.2300\u00b1 0.6664 623.4943\u00b1 0.6474 \u2212 yeast 8.4511\u00b1 0.0298 9.6088\u00b1 0.0358 9.5652\u00b1 0.0290 9.4433\u00b1 0.0312 9.4730\u00b1 0.0363 scene 0.6793\u00b1 0.0083 1.1359\u00b1 0.0066 1.1486\u00b1 0.0055 2.1136\u00b1 0.0203 0.8919\u00b1 0.0069 enron 34.3199\u00b1 0.1815 43.3919\u00b1 0.2919 44.0596\u00b1 0.2810 43.3969\u00b1 0.2540 27.2029\u00b1 0.1365 medical 5.3300\u00b1 0.0676 5.4536\u00b1 0.1184 5.7327\u00b1 0.1088 5.6014\u00b1 0.1232 4.1174\u00b1 0.0741 genbase 0.3863\u00b1 0.0341 0.2461\u00b1 0.0281 0.2411\u00b1 0.0307 0.2525\u00b1 0.0257 0.4686\u00b1 0.0310 emotions 1.5912\u00b1 0.0198 1.7890\u00b1 0.0182 1.9058\u00b1 0.0220 1.7643\u00b1 0.0165 1.8338\u00b1 0.0281 CAL500 1304.6118\u00b1 4.5735 1500.2173\u00b1 5.0228 1476.8390\u00b1 4.8354 1536.8850\u00b1 4.4876 1122.0536\u00b1 4.4702 flags 3.0101\u00b1 0.0470 3.1229\u00b1 0.0434 3.1387\u00b1 0.0383 3.0781\u00b1 0.0352 3.3626\u00b1 0.0504\nTable 5: Experimental results on accuracy score (best (highest) ones are marked in bold)\naccuracy \u2191 CSRPE REP (BR) RRPE (RAKEL) HAMR CFT bibtex 0.3926\u00b1 0.0011 0.3063\u00b1 0.0009 0.3103\u00b1 0.0009 0.3094\u00b1 0.0008 \u2212 Corel5k 0.1664\u00b1 0.0009 0.0471\u00b1 0.0007 0.0696\u00b1 0.0006 0.0408\u00b1 0.0009 \u2212 yeast 0.5653\u00b1 0.0012 0.5047\u00b1 0.0014 0.5065\u00b1 0.0012 0.5120\u00b1 0.0015 0.5027\u00b1 0.0019 scene 0.7620\u00b1 0.0020 0.5791\u00b1 0.0025 0.5816\u00b1 0.0020 0.4564\u00b1 0.0046 0.6467\u00b1 0.0029 enron 0.4772\u00b1 0.0016 0.4303\u00b1 0.0023 0.4215\u00b1 0.0022 0.4344\u00b1 0.0024 0.4363\u00b1 0.0018 medical 0.7939\u00b1 0.0024 0.7559\u00b1 0.0034 0.7431\u00b1 0.0033 0.7604\u00b1 0.0033 0.7570\u00b1 0.0031 genbase 0.9835\u00b1 0.0010 0.9859\u00b1 0.0014 0.9861\u00b1 0.0014 0.9856\u00b1 0.0014 0.9792\u00b1 0.0012 emotions 0.5775\u00b1 0.0037 0.5179\u00b1 0.0037 0.4959\u00b1 0.0045 0.5320\u00b1 0.0034 0.5216\u00b1 0.0036 CAL500 0.2645\u00b1 0.0013 0.2097\u00b1 0.0010 0.2179\u00b1 0.0008 0.1925\u00b1 0.0007 0.2425\u00b1 0.0015 flags 0.6056\u00b1 0.0058 0.5849\u00b1 0.0047 0.5860\u00b1 0.0046 0.5913\u00b1 0.0051 0.5616\u00b1 0.0059\n14 Yao-Yuan Yang et al.\n4.2 Empirical Time Analysis\nTo show that CSRPE can be more efficient than state-of-the-art cost sensitive algorithm in terms of speed. We compare the training and prediction of CSRPE with CFT. For prediction of CSRPE, we uses scikit-learn\u2019s ball tree algorithm to perform the nearest neighbor decoding. Experiments are run on virtual machines with 8GB of RAM and 4 processors. The setting for these two algorithms are the same as previous experiment (3000 base learners), though the maximum depth of each tree are fixed to 20. The cost function for both algorithm is f1 score.\nThe results are shown in Figure 5. the x-axis is different datasets and the yaxis is the log of running time in millisecond. CSRPE has a shorter training and prediction time than CFT in all 8 datasets. This demonstrated the strength of CSRPE that it is able to train each encoded bit in parallel. CFT is not able to exploit the parallel computing that modern computer has. Also during training, CFT increases the amount of examples send to the base learner each round, which slows down CFT.\n4.3 Compare with Cost-Sensitive Algorithm\nAlthough previously we have compared the performance of CSRPE and CFT, CFT seems to be over-fitting using random forest as base learner (their paper also mentions that CFT is prone to over-fitting). Therefore we have conducted another experiment using a more similar setting to their original experiment. Using linear support vector machines (SVM) implemented from Liblinear (Fan et al, 2008) as base learner. We use the term CSRPE-linear and CFT-linear here. The datasets used here are the same as the previous experiment. Since we are using linear SVM each feature is scaled to [0, 1] linearly to avoid numerical error.\nIn this experiment, the objective is to evaluate the performance for CSRPE with other cost-sensitive algorithms. The two state-of-the-art algorithms that are designed for to handle multiple evaluation criteria are PCC and CFT. CFT is stateof-the-art cost-sensitive multi-label classification algorithm. CFT has a competitive multi-label classification performance and usually outperforms other algorithms on the evaluation criteria that those algorithms does not designed for. CFT also has similar performance versus PCC on the evaluation criteria that PCC can derive inference rule for. Therefore we compare CSRPE with CFT in this experiment.\nParameters: The number of base learners for CSRPE are kept 3000. The only hyper parameter for base learner that is tuned is the regularization term in linear\nSVM, it is searched by three-fold cross-validation from 10\u22123 to 103, the exponent has an interval of 1, from \u22123 to 3. The number of iterations for CFT are set to 8, which is the same as previous experiment. Other Parameters that are not mentioned are set as default setting of Liblinear.\nThe experimental result and t-test result under 95% confidence are shown in Table 7, and 9. From these tables we can see that CSRPE-linear has similar performance with CFT-linear in hamming loss and rank loss. CSRPE-linear performs better in f1 score and accuracy score.\n4.4 Full Label CSRPE\nThe objective of this experiment is to demonstrate that the result of voting only on the labels shown up in training set won\u2019t effect the result too much, we have set up another experiment. With the same data sets and splits as previous experiments. In each run, CSRPE-full added the labels that only appears in testing set but not in the training set into the voting candidates Z\u0303. Therefore all the labels appears in testing set are able to be voted.\nThe results are listed in Table 8 It shows that there is no big difference in performance between CSRPE and CSRPE-full. Even in the dataset CAL500, which all the label vectors in training and testing are different, there are only a small performance difference between CSRPE and CSRPE-full. This indicates that whether we vote all possible labels or just the labels appearing in training sets does not matter too much. Using distinct label vectors in the training set as candidate set for nearest neighbor decoding is good enough."}, {"heading": "5 Conclusions", "text": "This paper designed a novel encoding method cost-sensitive random pair ensemble (CSRPE), which embeds the cost information into the encoded vector. Exploiting the redundancy of CSOVO, CSRPE is derived from the multi-class classification algorithm CSOVO. To decode CSRPE, the traditional ECOC Hamming\n20 Yao-Yuan Yang et al.\ndecoding is enough to perform well on cost-sensitive setting. Using the ML-ECC framework with CSRPE alongside with truncating the encoded vector and we create a candidate set for decoding, CSRPE is able to be applied to the cost-sensitive multi-label classification problems.\nIn the experiments on the real-world datasets, CSRPE is able to outperform other encoding method including REP, RREP, HAMR in f1 score, rank loss, accuracy score and ties in hamming loss. CSRPE also produces a more stable performance with the increasing of the length of the encoded vector. When comparing\nCost-Sensitive Random Pair Encoding for Multi-Label Classification 21\nwith state-of-the-art cost-sensitive multi-label algorithm CFT, CSRPE also has superior performance on f1 score and accuracy score and ties on hamming loss and rank loss. Also CSRPE is able to run in parallel and the efficient nearest neighbor search let CSRPE outperforms CFT in terms of training and predicting efficiency."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["EL Allwein", "RE Schapire", "Y Singer"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A Andoni", "P Indyk"], "venue": null, "citeRegEx": "Andoni and Indyk,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk", "year": 2006}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["JL Bentley"], "venue": null, "citeRegEx": "Bentley,? \\Q1975\\E", "shortCiteRegEx": "Bentley", "year": 1975}, {"title": "Bagging predictors. Machine learning", "author": ["L Breiman"], "venue": null, "citeRegEx": "Breiman,? \\Q1996\\E", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Classification and Regression Trees", "author": ["L Breiman", "JH Friedman", "RA Olshen", "CJ Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K Dembczynski", "W Cheng", "E H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2010}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["K Dembczynski", "W Kotlowski", "E H\u00fcllermeier"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "An exact algorithm for f-measure maximization", "author": ["KJ Dembczynski", "W Waegeman", "W Cheng", "E H\u00fcllermeier"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Dembczynski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2011}, {"title": "Solving multiclass learning problems via errorcorrecting output codes. Journal of artificial intelligence research", "author": ["TG Dietterich", "G Bakiri"], "venue": null, "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Metacost: A general method for making classifiers costsensitive", "author": ["PM Domingos"], "venue": null, "citeRegEx": "Domingos,? \\Q1999\\E", "shortCiteRegEx": "Domingos", "year": 1999}, {"title": "A kernel method for multi-labelled classification", "author": ["A Elisseeff", "J Weston"], "venue": null, "citeRegEx": "Elisseeff and Weston,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff and Weston", "year": 2001}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R Fan", "K Chang", "C Hsieh", "X Wang", "C Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Multilabel classification using error-correcting codes of hard or soft bits", "author": ["CS Ferng", "HT Lin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "Ferng and Lin,? \\Q2013\\E", "shortCiteRegEx": "Ferng and Lin", "year": 2013}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["I Katakis", "G Tsoumakas", "I Vlahavas"], "venue": "ECML PKDD discovery challenge", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Condensed filter tree for cost-sensitive multi-label classification", "author": ["C Li", "H Lin"], "venue": "Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Li and Lin,? \\Q2014\\E", "shortCiteRegEx": "Li and Lin", "year": 2014}, {"title": "Reduction from cost-sensitive multiclass classification to one-versusone binary classification", "author": ["H Lin"], "venue": "ACML", "citeRegEx": "Lin,? \\Q2014\\E", "shortCiteRegEx": "Lin", "year": 2014}, {"title": "New algorithms for efficient high-dimensional nonparametric classification", "author": ["T Liu", "AW Moore", "A Gray"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y Prabhu", "M Varma"], "venue": "In: International Conference on Knowledge Discovery", "citeRegEx": "Prabhu and Varma,? \\Q2014\\E", "shortCiteRegEx": "Prabhu and Varma", "year": 2014}, {"title": "Classifier chains for multi-label classification. Machine learning", "author": ["J Read", "B Pfahringer", "G Holmes", "E Frank"], "venue": null, "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Multi-label classification: An overview", "author": ["G Tsoumakas", "I Katakis"], "venue": "International Journal of Data Warehousing and Mining", "citeRegEx": "Tsoumakas and Katakis,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Katakis", "year": 2007}, {"title": "Random k -labelsets: An ensemble method for multilabel classification", "author": ["G Tsoumakas", "IP Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas and Vlahavas,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Vlahavas", "year": 2007}, {"title": "Mulan: A java library for multi-label learning", "author": ["G Tsoumakas", "E Spyromitros-Xioufis", "J Vilcek", "I Vlahavas"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Tsoumakas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "genes with some functional classes (Elisseeff and Weston, 2001).", "startOffset": 35, "endOffset": 63}, {"referenceID": 19, "context": "For instance, Binary Relevance (BR; Tsoumakas and Katakis, 2007) learns a per-label binary classifier to predict the label\u2019s relevance, and naturally targets at the Hamming loss.", "startOffset": 31, "endOffset": 64}, {"referenceID": 20, "context": "Random k-labelsets (RAKEL; Tsoumakas and Vlahavas, 2007) learns an ensemble of LP classifiers, each only on a few labels, and can be explained as optimizing the 0/1 loss from the Multi-label Error-correcting Code framework (ML-ECC; Ferng and Lin, 2013).", "startOffset": 19, "endOffset": 56}, {"referenceID": 12, "context": "Random k-labelsets (RAKEL; Tsoumakas and Vlahavas, 2007) learns an ensemble of LP classifiers, each only on a few labels, and can be explained as optimizing the 0/1 loss from the Multi-label Error-correcting Code framework (ML-ECC; Ferng and Lin, 2013).", "startOffset": 223, "endOffset": 252}, {"referenceID": 17, "context": "FastXML (Prabhu and Varma, 2014) is designed to optimize the normalized discounted cumulative gain, a popular criterion for information retrieval.", "startOffset": 8, "endOffset": 32}, {"referenceID": 14, "context": "Multi-label classification algorithms that automatically adapt to different evaluation criteria to improve the performance are called cost-sensitive multi-label classification (CSMLC) algorithms, as defined by Li and Lin (2014). A classic algorithm called Probabilistic Classifier Chain (PCC) (Dembczynski et al, 2010), an algorithm that extends from CC, is a state-of-the-art CSMLC algorithm.", "startOffset": 210, "endOffset": 228}, {"referenceID": 14, "context": "Another state-of-the-art CSMLC algorithm is Condense Filter Tree (CFT) (Li and Lin, 2014), which is designed by converting the CSMLC problem to a cost-sensitive multi-class classification problem with a n\u00e4\u0131ve Label Powerset (LP) reduction.", "startOffset": 71, "endOffset": 89}, {"referenceID": 9, "context": "opens a route of connecting CSMLC with the rich literature of cost-sensitive multiclass classification (Domingos, 1999; Beygelzimer et al, 2009; Lin, 2014), but results in exponentially many hyperclasses with respect to the number of labels.", "startOffset": 103, "endOffset": 155}, {"referenceID": 15, "context": "opens a route of connecting CSMLC with the rich literature of cost-sensitive multiclass classification (Domingos, 1999; Beygelzimer et al, 2009; Lin, 2014), but results in exponentially many hyperclasses with respect to the number of labels.", "startOffset": 103, "endOffset": 155}, {"referenceID": 19, "context": "Multi-label classification with per-label classifiers, such as BR (Tsoumakas and Katakis, 2007) and CFT (Li and Lin, 2014), are generally considered the simpler algorithms in multi-label classification.", "startOffset": 66, "endOffset": 95}, {"referenceID": 14, "context": "Multi-label classification with per-label classifiers, such as BR (Tsoumakas and Katakis, 2007) and CFT (Li and Lin, 2014), are generally considered the simpler algorithms in multi-label classification.", "startOffset": 104, "endOffset": 122}, {"referenceID": 12, "context": "Multi-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) is such an algorithm.", "startOffset": 43, "endOffset": 64}, {"referenceID": 12, "context": "While ML-ECC is successful in terms of the hamming loss and the 0/1 loss (Ferng and Lin, 2013), it cannot easily adapt to other evaluation criteria and hence is not a CSMLC algorithm.", "startOffset": 73, "endOffset": 94}, {"referenceID": 8, "context": "It is well-known that OVO can be seen as a special case of ECC (Dietterich and Bakiri, 1995).", "startOffset": 63, "endOffset": 92}, {"referenceID": 15, "context": "In addition, OVO has been extended to some multi-class cost-sensitive classification algorithms like CostSensitive One-versus-one (CSOVO), which is reported to perform better than FT for cost-sensitive multi-class classification (Lin, 2014).", "startOffset": 229, "endOffset": 240}, {"referenceID": 14, "context": "More specifically, we follow the route of LP (Li and Lin, 2014) to reduce the CSMLC problem to a cost-sensitive multi-class classification problem.", "startOffset": 45, "endOffset": 63}, {"referenceID": 14, "context": "We follow (Li and Lin, 2014) for the definition of the multi-label classification problem (MLC) and cost-sensitive multi-label classification problem (CSMLC).", "startOffset": 10, "endOffset": 28}, {"referenceID": 19, "context": "For MLC, there already exists some basic approaches such as Binary Relevance (BR) (Tsoumakas and Katakis, 2007) and label powerset (LP) (Tsoumakas et al, 2010).", "startOffset": 82, "endOffset": 111}, {"referenceID": 20, "context": "RAndom K-labeLsets (RAKEL) (Tsoumakas and Vlahavas, 2007) is proposed to strike a balance between these two methods.", "startOffset": 27, "endOffset": 57}, {"referenceID": 12, "context": "Multi-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) proposed a more sophisticated framework of using error correcting code (ECC).", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "Condense Filter Tree (CFT) (Li and Lin, 2014) is the first algorithm to proposed as a CSMLC algorithm, which is able to extend to arbitrary cost function with ease.", "startOffset": 27, "endOffset": 45}, {"referenceID": 15, "context": "In multi-class setting Cost-Sensitive One-versus-one (CSOVO) (Lin, 2014), extends OVO approach to cost-sensitive setting.", "startOffset": 61, "endOffset": 72}, {"referenceID": 2, "context": "For example, KD Tree (Bentley, 1975) and Ball Tree (Liu et al, 2006).", "startOffset": 21, "endOffset": 36}, {"referenceID": 1, "context": "is larger than the normal size where KD Tree or Ball Tree can work efficiently, approximated nearest neighbor algorithms like locality-sensitive hashing (LSH) (Andoni and Indyk, 2006) can be adapted.", "startOffset": 159, "endOffset": 183}, {"referenceID": 3, "context": "There are multiple repeated bits in REP and HAMR, so the base learners are trained with bagging (Breiman, 1996).", "startOffset": 96, "endOffset": 111}], "year": 2016, "abstractText": "We propose a novel cost-sensitive multi-label classification algorithm called cost-sensitive random pair encoding (CSRPE). CSRPE reduces the costsensitive multi-label classification problem to many cost-sensitive binary classification problems through the label powerset approach followed by the classic oneversus-one decomposition. While such a n\u00e4\u0131ve reduction results in exponentiallymany classifiers, we resolve the training challenge of building the many classifiers by random sampling, and the prediction challenge of voting from the many classifiers by nearest-neighbor decoding through casting the one-versus-one decomposition as a special case of error-correcting code. Extensive experimental results demonstrate that CSRPE achieves stable convergence and reaches better performance than other ensemble-learning and error-correcting-coding algorithms for multi-label classification. The results also justify that CSRPE is competitive with state-of-the-art cost-sensitive multi-label classification algorithms for cost-sensitive multi-label classification.", "creator": "LaTeX with hyperref package"}}}