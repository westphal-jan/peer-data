{"id": "1611.09573", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling", "abstract": "With the advent of the semantic web, various tools and techniques have been introduced to present and organize knowledge. Conceptual hierarchies are one such technique that has garnered considerable attention for their usefulness in creating domain ontologies that are considered an integral part of the semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and combine them by identifying some potential relationships between them. In this essay, we propose a novel approach to identifying relevant concepts from simple text and then learn the hierarchy of concepts by exploiting submission relationships between them. First, we model themes based on a likely theme model, and then use a lightweight linguistic process to extract semantically rich concepts. Then, we combine concepts by exploiting an \"is-one\" relationship between pairs of concepts.", "histories": [["v1", "Tue, 29 Nov 2016 11:28:59 GMT  (571kb)", "http://arxiv.org/abs/1611.09573v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["v s anoop", "s asharaf", "p deepak"], "accepted": false, "id": "1611.09573"}, "pdf": {"name": "1611.09573.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anoop.res15@iiitmk.ac.in", "asharaf.s@iiitmk.ac.in", "deepaksp@acm.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n09 57\n3v 1\n[ cs\n.A I]\n2 9\nN ov\n2 01\n6 Learning Concept Hierarchies through Probabilistic Topic Modeling\nV S Anoopa, S Asharafb, Deepak Pc\naData Engineering Lab, Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram 695 581, India, Contact: anoop.res15@iiitmk.ac.in\nbIndian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram 695 581, India, Contact: asharaf.s@iiitmk.ac.in\ncQueen\u2019s University, Belfast, UK, Contact: deepaksp@acm.org\nWith the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \u201dis-a\u201d relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.\nKeywords : Probabilistic Topic Models, Concept Extraction, Subsumption Hierarchy Learning, Natural Language Processing, Semantic Web, Text Mining."}, {"heading": "1. INTRODUCTION", "text": "Due to rapid growth of text producing and consuming applications, numerous tools and techniques were introduced in the recent past for extracting useful patterns from unstructured text. These patterns are crucial for organizations to discover knowledge out of it and aid in making intelligent decisions. As the amount of such data grows exponentially, already available algorithms performs poor on the scalability and performance aspects. But there are still a lot of avenues where text data is yet to be exploited fully and thus we need new and efficient algorithms to tackle this situation. Platforms such as social networks, e-commerce websites, blogs and research journals generate such data in the form of unstructured\ntext and it is essential to analyze, synthesis and process such data for efficient retrieval of useful information.\nIn text mining, concepts are defined as a sequence of words that constitute real or imaginary entities. Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few. This is the era of data explosion thus it is very difficult to store, process, manage and most importantly to extract knowledge out of it. To overcome this shortfall, a significant amount of research has been carried out in the recent past for leveraging underlying thematic and semantic structure from text archives. As a\n1\nresult a good number of algorithmic techniques were introduced which are proved to be efficient for the discovery of themes and semantics underlying high dimensional data.\nTopic Models are suite of text understanding algorithms which statistically generate latent themes pervade a large collection of unstructured text. Since its inception, text mining researchers and practitioners are using it extensively to analyze and organize large document collections. They are unsupervised learning algorithms thus it does not require user tagged corpus to work with. A large number of topic modeling algorithms have been reported in the past with the difference in the assumption they make for modeling topics. Models such as Probabilistic topic models [4] and Latent Dirichlet Allocation (LDA) are some such flavors of topic modeling that attained significant attention.\nContributions: This work proposes a novel unsupervised approach for learning concept hierarchies from large unstructured text corpus which is guided by a probabilistic topic modeling approach. To begin with, we model topics from the corpus using Latent Dirichlet Allocation (LDA) algorithm and then uses a lightweight linguistic process to identify concepts which are close to the real world understanding. Then we make use of a subsumption relation [5] (\u201dis-a\u201d) to connect concepts which are related thus forms a hierarchy of concepts.\nOrganization: The rest of this paper is organized as follows. We briefly review related works in Section 2. Section 3 introduces the novel approach we have proposed. Detailed explanation of the implementation details is presented in Section 4, and the evaluation of the proposed method is discussed in Section 5. and finally we draw a conclusion and discuss future work in Section 6."}, {"heading": "2. PROBLEM DEFINITION AND RELATED WORK", "text": ""}, {"heading": "2.1. Problem Definition", "text": "Here, we define the problem formally. Given a large corpus containing unstructured text documents, our problem is to automatically generate concept hierarchies which are close to human understanding. In a nutshell, this paper aims to answer the following research questions :\n1. Is it possible to automatically extract human interpretable concepts from statistically generated topics using a lightweight linguistic process ?\n2. Can our proposed method learn a hierarchy of such concepts incorporating a subsumption relation between them, which are important in automated ontology generation ?\n3. Given a large but unstructured text corpus, can our topic modeling guided method better extracts and learns concept hierarchies compared to existing algorithms ?\nMany recent works have been reported in this direction which proposed many algorithms to extract semantically rich concepts from plain text. In the following section, we due acknowledge some past literatures that discusses methods which are close to our proposed algorithm. Notations used in this paper: To help narrative, some commonly used notations are shown in Table 1 that are used in the rest of this paper."}, {"heading": "2.2. Related Work", "text": "Concept extraction is the process of extracting real or imaginary entities from plain text that has got wider recognition in the recent past. This is due to the wide variety of applications which are mainly dealing with text data such as e-commerce websites, research articles etc. Thus a significant number of research literatures are available in the field of concept extraction and mining which proposes many algorithms with varying degrees of success. In this section, we give emphasis on past literatures in automated concept extraction and hierarchy learning algorithms and briefly discuss works closely related to our proposed framework.\nPhrase discovering topic model [6] that uses pitman-yor process and TopMine [7] were two notable works that proposed algorithms for mining topical phrases from text documents. The former constructs a topic-word matrix before modeling topics but disadvantage of the approach was that creating such a matrix for large volume of text is often difficult. The latter approach makes use of a two stage process for modeling topics and mainly works with clinical documents. First it identifies phrases using some off-the-shelf tools and then trains a topic model with the identified phrases. Another work which uses topic models for generating multi-word phrases was the topical n-gram [8]. This makes use of some switching variable for identifying a new n-gram. The assumption of this method was that the words within an n-gram usually won\u2019t share same topic, which may not be true all the time.\nAutomatic Concept Extractor (ACE), a system specifically designed for extracting concepts from HTML pages and making use of the text body and some visual clues on HTML tags for identifying potential concepts was proposed by Ramirez and Mattmann [9]. Even though this method could outperform some state of the art methods, dependency with HTML was a major drawback. Turney[10] proposed another system named GenEx, which employed a genetic algorithm supported rule learning mechanism for concept extraction.\nA system which extracts concepts from user tag and query log dataset is proposed by Parameswaran et.al.[11] which uses techniques similar to association rule mining. This method uses features such as frequency of occurrences and the popularity among users for extracting core concepts and attempts to build a web of concepts. Even though this algorithm can be applied to any large dataset, a lot of additional processing is required when dealing with web pages. A bag-of-word approach was proposed by Gelfand et.al.[12] for concept extraction from plain text and used these to form a closely tied semantic relations graph for representing relationships between them. They have applied this technique specifically for some classification tasks and found that their method produces better concepts than the Naive Bayes text classifier.\nDheeraj Rajagopal et.al.[13] introduced another graph based approach for commonsense concept extraction and detection of semantic similarity among those concepts. They used a manually labeled dataset of 200 multi-word concept pairs for evaluating their parser capable of detecting semantic similarity and showed that their method was capable of effectively finding syntactically and semantically related concepts. The main disadvantage of that method is the use of manually labeled dataset and the creation of such dataset is time consuming and requires human effort. Another work reported in this domain is the method proposed by Krulwich and Burkey [14] which uses a simple heuristics rule based approach to extract key phrases from document by considering visual clues such as the usage of bold and italic characters as features. They have shown that this technique can be extended for automatic document classification experiments.\nA key phrase extraction system called Automatic Keyphrase Extraction (KEA) developed by Witten et.al[15] was reported in the concept extraction literatures which creates a Naive Bayes learning model with known key phrases extracted from training documents and uses this model for inferring key phrases from new set of documents.\nAs an extension to this KEA framework, Song et. al.[16] proposed a method which uses the information gain measure for ranking candidate key phrases based on some distance and tf-idf features which was first introduced in [15]. Another impressive and widely used method was introduced by Frantzi et. al.[17] which extracts multi-word terms from medical documents and named as C/NC method. The algorithm uses a POS tagger POS patten filter for collecting noun phrases and then uses some statistical measures for determining the termhood of candidate multi-words.\nThe proposed method in this paper is a hybrid approach incorporating statistical methods such as topic modeling and tf-itf weighting and some lightweight linguistic processes such as POS tagging and analysis for leveraging concepts from text. We expect the learnt concept hierarchy to be close to the real world understanding of concepts which we will quantify using evaluation measures such as precision, recall and f-measure."}, {"heading": "3. BACKGROUND : LATENT DIRICHLET ALLOCATION (LDA)", "text": "A good number of topic modeling algorithms are introduced in the recent past which varies in their method of working mainly with the assumptions they adopt for the statistical processing. An automated document indexing method based on a latent class model for factor analysis of count data in the latent semantic space has been introduced by Thomas Hofman [18]. This generative data model called Probabilistic Latent Semantic Indexing (PLSI), considered as an alternative to the basic Latent Semantic Indexing has a strong statistical foundation. The basic assumption of PLSI is that each word in a document corresponds to only one topic.\nLater, Blei et. al.[19] introduced a new topic modeling algorithm known as Latent Dirichlet Allocation (LDA) which is more efficient and attractive than PLSI. This model assumes that a document contain multiple topics and such topics are leveraged using a Dirichlet Prior process. In the following section, we will briefly describe the un-\nderlying principle of LDA. Even though a LDA works well on broad ranges of discrete datasets, the text is considered to be a typical example to which the model can be best applied. The process of generating a document with n words by LDA can be described as follows[19]:\n1. Choose the number of words, n, according to Poisson Distribution;\n2. Choose the distribution over topics, \u03b8, for this document by Dirichlet Distribution;\n(a) Choose a topic T (i) \u223c Multinomial(\u03b8)\n(b) Choose a word W (i) from P ( W (i)|T (i), \u03b2 )\nThus the marginal distribution of the document can be obtained from the above process as :\nP (d) =\n\u222b\n\u03b8\n(\nn \u220f\ni=1\n\u2211\nT (i)\nP (W (i)|T (i), \u03b2)P (T (i)|\u03b8)\n)\n+P (\u03b8|\u03b1)d\u03b8\nwhere, P (\u03b8|\u03b1) is derived by Dirichlet Distribution parameterized by \u03b1, and P (W (i))|T (i), \u03b2) is the probability of W (i) under topic T (i) parameterized by \u03b2. The parameter \u03b1 can be viewed as a prior observation counting on the number of times each topic is sampled in a document, before we actually seen any word from that document. The parameter \u03b2 is a hyper-parameter determining the number of times words are sampled from a topic [19], before any word of the corpus is observed. At the end, the probability of the whole corpus D can be derived by taking the product of all documents\u2019 marginal probability as given below:\nP (D) =\nM \u220f\ni=1\nP (di) (1)"}, {"heading": "4. PROPOSED APPROACH", "text": "In the area of text mining, topic models or specifically probabilistic topic models are suite of algorithms which got wider recognition for its ability to leverage hidden thematic information from huge archives of text data. Text mining\nresearchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.\nAmong all these algorithms, LDA has got lot of attention in the recent past and is widely using because of its easiness of implementation and potential applications. Even though the power of LDA algorithm has been extensively used for leveraging topics, very few studies have been reported for mapping these statistically outputted topics to semantically rich concepts. Our proposed framework is an attempt to address this issue by making use of LDA algorithm to generate topics and we leverage concepts from such topics by using a new statistical weighting scheme and some lightweight linguistic processes. The overall work flow of the proposed approach is depicted in Fig.1.\nOur framework can be divided into 2 modules (i) concept extraction and (ii) concept hierarchy learning. The concept extraction module extract concepts from topics generated by LDA algorithm and the concept hierarchy learning module learns a hierarchy of extracted concepts by inducing a subsumption hierarchy learning algorithm. Detailed explanation of these modules are given below."}, {"heading": "4.1. Concept Extraction", "text": "In this module, we introduce a topic to concept mapping procedure for leveraging potential concepts from statistically computed topics which are generated by the LDA algorithm. The first step of the proposed framework deals with the preprocessing of data which is meant for removing unwanted and irrelevant data and noises. Latent Dirichlet Allocation algorithm is executed on top of this preprocessed data which in turn generate topics through the statistical process. A total of 50 topics have been extracted by tuning the parameters of LDA algorithm. Once we got the sufficient topics for the experiment, for each\ntopic, we have created a topic - document cluster by grouping the documents which generated such a topic and the same process has been executed for all topics under consideration.\nNow, we introduce a new weighting scheme called tf \u2212 itf (term frequency - inverse topic frequency) which is used for finding out highly contributing topic word in each topic. We bring this weighting scheme to filter out the relevant candidate topic words. Term frequency (tf) is the total number of times that particular topic word comes in the topic - document clusters. Normalized term frequency, Ntf of a topic word Tw can be calculated as:\nNtf = count(Tw) in Ctd\ncount(total terms in Ctd) (2)\nInverse topic frequency Itf is calculated as:\nItf = count(total terms in Ctd)\ncount(documents with Tw) (3)\ntf\u2212itf is calculated using the following equation:\ntf \u2212 itf = Ntf \u2217 Itf (4)\nThis step is followed by a sentence extraction process in which all the sentences which contain the topic words which have high tf-itf weight are extracted. Next, we apply a parts of speech tagging on these sentences and extract only noun and adjective tags as we are only concentrating on the extraction of concepts. In linguistic preprocessing step, we take Noun + Noun, Noun + Adjective and (Adjective / Noun) + Noun combinations of words from the tagged collection. Concept identification is the last step in the process flow in which we find out the term count of all the combinations of Noun + Noun, Noun + Adjective and (Adjective / Noun) + Noun. A positive term count implies that the current multi word can be a potential \u201dconcept\u201d and if we get a zero term count, then that multi word can be ignored. The newly proposed algorithm for extracting the concepts is shown in Algorithm 1."}, {"heading": "4.2. Concept Hierarchy Learning", "text": "In this module we derive hierarchical organization of leveraged concepts using a type of co-\nAlgorithm 1 Concept Extraction\n1: procedure ExtractConcepts(tc) 2: \u2200 t, create Ctd 3: \u2200Ctd, compute tf \u2212 itf weight 4: \u2200 t, choose n words with highest tf \u2212 itf 5: S[ ] = sentences with top tf \u2212 itf words 6: POS tag(S) 7: W [ ] = (NNP,NNS,NN, JJ) 8: MWc[ ] = noun+ noun|adj + noun 9: while |MWc| 6= 0 do\n10: termCount(MW ) \u2200 MW in MWc 11: if Tc > 0 then 12: Add MW into C 13: Remove MW from MWc 14: Fetch next MW from MWc 15: else 16: Remove MW from MWc 17: Fetch next MW from MWc 18: end if 19: end while 20: end procedure\noccurrence called \u201dsubsumption\u201d relation. Subsumption relation is found to be simple but very effective way of inferring relationships between words and phrases without using any training data or clustering methods. The basic idea behind subsumption relation is very simple : for any two concepts Ca and Cb, Ca is said to be subsume Cb if 2 conditions hold. P (Ca|Cb) = 1 and P (Cb|Ca) < 1. To be more specific, Ca subsumes Cb if the documents which Cb occurs in are a subset of the documents which Ca occurs in. Because Ca subsumes Cb and because it is more frequent, in the hierarchy, Ca is the parent of Cb.\nAlgorithm 2 Concept Hierarchy Learning\n1: procedure LearnHierarchy(C) 2: Choose pair of concepts, say Ca and Cb 3: Compute P (Ca|Cb) and P (Cb|Ca) 4: if P (Ca|Cb) = 1 and P (Cb|Ca) < 1 then 5: Assign Ca as the parent of Cb 6: else 7: Fetch next concept pairs 8: end if 9: Goto step 2, repeat \u2200 Ca, Cb\n10: end procedure"}, {"heading": "5. EXPERIMENTAL SETUP", "text": "This section concentrates on the implementation details of our proposed framework and concept extraction and hierarchy learning procedures are discussed in detail."}, {"heading": "5.1. Concept Extraction", "text": "Here, concept extraction module of the framework is discussed. This module concentrates on tasks such as data collection and pre-processing, topic modeling, topic-document clustering, tf-itf weighting, sentence extraction and POS tagging, linguistic pre-processing etc for identifying concepts and a detailed explanation of each step is given below."}, {"heading": "5.1.1. Dataset Collection and Preprocessing", "text": "We are using publicly available datasets such as Reuters Corpus Volume 1 dataset[24] and BBC News Dataset[25] for the experiment. Reuters is the world\u2019s biggest international news agency and cater different news and related information through their website, video, interactive television and mobile platforms. Reuters Corpus Volume 1 is in XML format and is freely available for research purpose. Text messages are extracted by a thorough pre-processing such as removing XML tags, URLs and other special symbols and then created a new dataset exclusively for our experiment. BBC provides two benchmarked news article datasets which is freely available for machine learning research. The general BBC dataset consist of 2225 text documents directly from their website corresponding to stories in five areas such as business, entertainment, politics, sports and technology, from 2004 to 2005. A thorough preprocessing such as stemming, and removal of stop-word, URLs and special characters on this dataset and made an experiment ready copy of the original dataset."}, {"heading": "5.1.2. Topic Modeling", "text": "Latent Dirichlet Allocation (LDA) algorithm has been applied on the pre-processed dataset to leverage topics for this experiment. The number of iterations is set to 300 as Gibbs sampling method usually approaches the target distribu-\ntion after 300 iterations. The number of topics is set to 50 and a snapshot of 5 topics we have randomly chosen is shown in Table 2."}, {"heading": "5.1.3. Topic - Document Clustering", "text": "In this step, we consider each topic and then grouped and clustered top 50 documents which contributed the creation of that specific topic. This has been done for all the 50 topics of our choice. As an outcome, we have got 50 such clusters that contain documents which generated the topics."}, {"heading": "5.1.4. TF-ITF Weighting", "text": "Here, we compute the tf \u2212 itf(term frequency \u2212 inverse topic frequency) weight of each word in every topic using Eq.(3),\nEq.(4) and Eq.(5) to find out highly used topic words in the collection. Table 2 also shows topic words along with their tf-itf weight."}, {"heading": "5.1.5. Sentence Extraction & POS Tagging", "text": "In sentence extraction step, we consider topic words having highest tf-itf weight and then extract sentences containing these topic words from the topic - document clusters. Then a parts of speech tagging has been done to identify words tagged as nouns and adjectives from these sentences as our aim is to extract potential \u201dconcepts\u201d from the repository. For this experiment, Natural Language Toolkit (NLTK) [23] has been used which contains libraries for Natural Language Processing for Python programming language."}, {"heading": "5.1.6. Linguistic Processing & Concept Identification", "text": "All words which are tagged as Nouns(NN/NNP/NNS) and Adjectives (JJ) are filtered out and all possible combinations of Noun + Noun,Adjective + Noun and (Noun/Adjective) + Noun. The results are shown in Table 3. The term count for each of these multi word term is then calculated against the original corpus and a positive term count implies that the corresponding multi-word term can be a potential concept and we eliminate the term if we get a zero term count. This process\nhas been repeated for all the multi-words we have filtered out."}, {"heading": "5.2. Concept Hierarchy Learning", "text": "Concept hierarchy learning module concentrates on leveraging a subsumption hierarchy[5] depicting an \u201dis-a\u201d relation between the concepts identified by the proposed algorithm. Subsumption relation is simple but considered as an important relationship type in any ontological structure and we calculate two probability conditions for the same. For any given two concepts, we first calculate P (C1|C2) and then P (C2|C1), in order to establish a subsumption relation, the former probability must be 1 and the latter should be less than 1. In other words, C1 subsumes C2 if the documents in which C2 occurs is a subset of the documents which C1 occurs in.\nFor instance, consider two concepts dial-up internet and network connection, the proposed method computes P (dialup internet|network connection) and P (network connection|dial \u2212 up internet) and found that the number of documents in which dialup internet occurs is a subset of number of documents in which network connection occurs. That means there exists a subsumption relation between these two concepts and dialup internet concept may be subsumed by network connection concept. This process has been repeated for all concepts in the collection, and a part of such a hierarchy generated using our proposed algorithm is shown in Fig. 2."}, {"heading": "6. EVALUATION OF RESULTS", "text": "Here we evaluate the results produced by our proposed method and precision and recall measures are used for evaluating the quality of concepts leveraged. We have first created a human generated concept repository and kept for verifying against the machine generated concepts. Precision computes the fraction of machine extracted concepts that are also human generated, and recall measures concepts which are extracted by proposed algorithm that are also human authored. In information retrieval, it is estimated\nthat achieving high precision and recall at same time is difficult and using a measure called F1, we can balance these two. Here, true positive is defined as the number of overlapped concepts between human generated concepts and concepts extracted by our proposed algorithm, false positive is the number of extracted concepts that are not truly human authored concepts and false negative is the human authored concepts that are missed by the concept extraction method. Using these measures, we have compared our proposed method against some of the existing concept extraction algorithms and the result is shown in Table 4.\nFrom the performance graph shown in Figure 4, it is clear that our proposed algorithm extracts more concepts as the number of topics are increasing. The other baseline algorithms such as ACE and ICE performs poor when the number of topics are increased randomly. This shows that the proposed algorithm outperforms the baseline algorithms when extracting real-world concepts from large number of statistically generated top-\nics."}, {"heading": "7. CONCLUSIONS AND FUTURE WORK", "text": "This paper proposed a novel framework for extracting close to real world concepts from large collection of unstructured text documents which is guided by a probabilistic topic modeling algorithm. Proposed method also deals with learning a subsumption hierarchy which exploits \u201dis-a\u201d relationships among identified concepts which is extensively used in ontology generation. Experiments conducted on large datasets such as Reuters and BBC news corpus shows that the proposed method outperforms some of the already available algorithms and better concept\nidentification is possible with this framework.\nBecause of the promising end results, we are interested to work mainly on the directions of measuring the scalability of proposed framework by using more large datasets. Apart from the basic subsumption hierarchy which depicts \u201dis-a\u201d relation, our future work will be on leveraging other relations that exist between concepts we would like to so that a this framework can automate the complete ontology generation process."}, {"heading": "12 V S Anoop, S Asharaf and Deepak P", "text": "Anoop V S is a full time Ph.D Research Scholar at Data Engineering Lab, Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram, India. He received his Masters in Computer Applications (MCA) -\nfrom IGNOU and Master of Philosophy in Computer Science from Cochin University of Science and Technology (CUSAT), Kerala in 2014. He has several publications in international journals, conference proceedings and book chapters. His research interests include Information Retrieval, Text Mining and NLP.\nAsharaf S is an Associate Professor at Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram, India. He received his Ph.D and Master of Engineering degrees in Computer Science and Engineering -\nfrom Indian Institute of Science, Bangalore. His areas of interest include algorithms, business models and software systems related to data mining, data analytics, information retrieval, computational advertising, soft computing and machine learning.\nDeepak Padmanabhan is a Lecturer (Asst. Professor) in Computer Science at Queens University Belfast, UK. He completed his M.Tech and PhD from Indian Institute of\nTechnology Madras, all in Computer Science. His current research interests include data analytics, similarity search, information retrieval and natural language processing. He has published over 40 research papers across major venues in Information and Knowledge Management. He is a Senior Member of the IEEE and ACM."}], "references": [{"title": "Semi-automated Ontology Creation for Semantic Search in Business Process Exploration", "author": ["Pospiech", "Sebastian", "Martin Pelke", "Robert Mertens"], "venue": "IEEE Tenth International Conference on Semantic Computing (ICSC).,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Exploring events and distributed representations of text in multidocument summarization", "author": ["Marujo", "Lus"], "venue": "Knowledge-Based Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Aspect term extraction for sentiment analysis in large movie reviews us ing Gini Index feature selection method and SVM classifier", "author": ["AS Manek", "PD Shenoy", "MC Mohan", "KR. Venugopal"], "venue": "World Wide Web,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Probabilistic topic models", "author": ["M Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Deriving concept hierarchies from text", "author": ["M Sanderson", "B. Croft"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A phrase-discovering topic model using hierarchical pitman-yor processes", "author": ["RV Lindsey", "WP Headden III", "MJ. Stipicevic"], "venue": "InProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (pp. 214-222),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Scalable topical phrase mining from text corpora", "author": ["A El-Kishky", "Y Song", "C Wang", "CR Voss", "J. Han"], "venue": "Proceedings of the VLDB Endowment.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Topical n-grams: Phrase and topic discovery, with an application to information retrieval", "author": ["X Wang", "A McCallum", "X. Wei"], "venue": "InSeventh IEEE International Conference on Data Mining (ICDM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "ACE: improving search engines via Automatic Concept Extraction", "author": ["Ramirez PM", "Mattmann CA"], "venue": "InInformation Reuse and Integration,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Learning algorithms for keyphrase extraction", "author": ["Turney PD"], "venue": "Information Retrieval.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Towards the web of concepts: Extracting concepts from large datasets", "author": ["A Parameswaran", "H Garcia-Molina", "A. Rajaraman"], "venue": "Proceedings of the VLDB Endowment.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Automated concept extraction from plain text", "author": ["B Gelfand", "M Wulfekuler", "PunchWF"], "venue": "AAAI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "A graph-based approach to commonsense concept extraction and semantic similarity detection", "author": ["D Rajagopal", "E Cambria", "D Olsher", "K. Kwok"], "venue": "In Proceedings of the 22nd interna Learning Concept Hierarchies through Probabilistic Topic Modeling  11 tional conference on World Wide Web companion,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Learning user information interests through extraction of semantically significant phrases", "author": ["B Krulwich", "C. Burkey"], "venue": "InProceedings of the AAAI spring symposium on machine learning in information access,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "KEA: Practical automatic keyphrase extraction", "author": ["IH Witten", "GW Paynter", "E Frank", "C Gutwin", "CG. Nevill-Manning"], "venue": "InProceedings of the fourth ACM conference on Digital libraries.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "KPSpotter: a flexible information gain-based keyphrase extraction system", "author": ["M Song", "IY Song", "X. Hu"], "venue": "InProceedings of the 5th ACM international workshop on Web information and data management.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Automatic recognition of multi-word terms", "author": ["K Frantzi", "S Ananiadou", "H. Mima"], "venue": "the c-value/nc-value method. International Journal on Digital Libraries.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["DM Blei", "AY Ng", "MI. Jordan"], "venue": "Journal of machine Learning research.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Latent semantic analysis", "author": ["Dumais ST"], "venue": "Annual review of information science and technology.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["DM Blei", "AY Ng", "MI. Jordan"], "venue": "Journal of machine Learning research.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "NLTK: the natural language toolkit", "author": ["S. Bird"], "venue": "InProceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "RCV1: A New Benchmark Collection for Text Categorization Research", "author": ["Lewis D", "Y Yang", "T Rose", "F. Li"], "venue": "Journal of Machine Learn ing Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Practical solutions to the problem of diagonal dominance in kernel document clustering", "author": ["D Greene", "P. Cunningham"], "venue": "InProceedings of the 23rd international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Models such as Probabilistic topic models [4] and Latent Dirichlet Allocation (LDA) are some such flavors of topic modeling that attained significant attention.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Then we make use of a subsumption relation [5] (\u201dis-a\u201d) to connect concepts which are related thus forms a hierarchy of concepts.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Phrase discovering topic model [6] that uses pitman-yor process and TopMine [7] were two notable works that proposed algorithms for mining topical phrases from text documents.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "Phrase discovering topic model [6] that uses pitman-yor process and TopMine [7] were two notable works that proposed algorithms for mining topical phrases from text documents.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Another work which uses topic models for generating multi-word phrases was the topical n-gram [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "Automatic Concept Extractor (ACE), a system specifically designed for extracting concepts from HTML pages and making use of the text body and some visual clues on HTML tags for identifying potential concepts was proposed by Ramirez and Mattmann [9].", "startOffset": 245, "endOffset": 248}, {"referenceID": 9, "context": "Turney[10] proposed another system named GenEx, which employed a genetic algorithm supported rule learning mechanism for concept extraction.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "[11] which uses techniques similar to association rule mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] for concept extraction from plain text and used these to form a closely tied semantic relations graph for representing relationships between them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] introduced another graph based approach for commonsense concept extraction and detection of semantic similarity among those concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Another work reported in this domain is the method proposed by Krulwich and Burkey [14] which uses a simple heuristics rule based approach to extract key phrases from document by considering visual clues such as the usage of bold and italic characters as features.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "al[15] was reported in the concept extraction literatures which creates a Naive Bayes learning model with known key phrases extracted from training documents and uses this model for inferring key phrases from new set of documents.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "[16] proposed a method which uses the information gain measure for ranking candidate key phrases based on some distance and tf-idf features which was first introduced in [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] proposed a method which uses the information gain measure for ranking candidate key phrases based on some distance and tf-idf features which was first introduced in [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 16, "context": "[17] which extracts multi-word terms from medical documents and named as C/NC method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "An automated document indexing method based on a latent class model for factor analysis of count data in the latent semantic space has been introduced by Thomas Hofman [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "[19] introduced a new topic modeling algorithm known as Latent Dirichlet Allocation (LDA) which is more efficient and attractive than PLSI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The process of generating a document with n words by LDA can be described as follows[19]:", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "The parameter \u03b2 is a hyper-parameter determining the number of times words are sampled from a topic [19], before any word of the corpus is observed.", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 187, "endOffset": 191}, {"referenceID": 23, "context": "Dataset Collection and Preprocessing We are using publicly available datasets such as Reuters Corpus Volume 1 dataset[24] and BBC News Dataset[25] for the experiment.", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Dataset Collection and Preprocessing We are using publicly available datasets such as Reuters Corpus Volume 1 dataset[24] and BBC News Dataset[25] for the experiment.", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "For this experiment, Natural Language Toolkit (NLTK) [23] has been used which contains libraries for Natural Language Processing for Python programming language.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "Concept Hierarchy Learning Concept hierarchy learning module concentrates on leveraging a subsumption hierarchy[5] depicting an \u201dis-a\u201d relation between the concepts identified by the proposed algorithm.", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \u201dis-a\u201d relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.", "creator": "LaTeX with hyperref package"}}}