{"id": "1612.00394", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Definition Modeling: Learning to Define Word Embeddings in Natural Language", "abstract": "In this paper, we examine whether it is possible to use distributed representations to create dictionary definitions of words as a more direct and transparent representation of the embedding semantics. We introduce definition modeling, the task of generating a definition for a particular word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with models across multiple datasets. Our results show that a model that controls the dependencies between the defined word and the definition words performs significantly better, and that a character-level conversion layer designed to utilize the embedding of words at the word level can provide insight into the imperfections of word embedding. Finally, error analysis suggests that the errors made by a definition model can provide insight into the imperfections of word embedding.", "histories": [["v1", "Thu, 1 Dec 2016 19:42:37 GMT  (133kb)", "http://arxiv.org/abs/1612.00394v1", "To appear in AAAI Conference 2017"]], "COMMENTS": "To appear in AAAI Conference 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thanapon noraset", "chen liang", "larry birnbaum", "doug downey"], "accepted": true, "id": "1612.00394"}, "pdf": {"name": "1612.00394.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Doug Downey"], "emails": ["chenliang2013}@u.northwestern.edu,", "l-birnbaum@northwestern.edu", "d-downey@northwestern.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 2.\n00 39\n4v 1\n[ cs\n.C L\n] 1\nD ec\n2 01"}, {"heading": "1 Introduction", "text": "Distributed representations of words, or word embeddings, are a key component in many natural language processing (NLP) models (Turian, Ratinov, and Bengio 2010; Huang et al. 2014). Recently, several neural network techniques have been introduced to learn high-quality word embeddings from unlabeled textual data (Mikolov et al. 2013a; Pennington, Socher, and Manning 2014; Yogatama et al. 2015). Embeddings have been shown to capture lexical syntax and semantics. For example, it is well-known that nearby embeddings are more likely to represent synonymous words (Landauer and Dumais 1997) or words in the same class (Downey, Schoenmackers, and Etzioni 2007). More recently, the vector offsets between embeddings have been shown to reflect analogical relations (Mikolov, Yih, and Zweig 2013). However, tasks such as word similarity and analogy only evaluate an embedding\u2019s lexical information indirectly.\nIn this work, we study whether word embeddings can be used to generate natural language definitions of their corresponding words. Dictionary definitions serve as direct and\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nexplicit statements of word meaning. Thus, compared to the word similarity and analogical relation tasks, definition generation can be considered a more transparent view of the syntax and semantics captured by an embedding. We introduce definition modeling: the task of estimating the probability of a textual definition, given a word being defined and its embedding. Specifically, for a given set of word embeddings, a definition model is trained on a corpus of word and definition pairs. The models are then tested on how well they model definitions for words not seen during the training, based on each word\u2019s embedding.\nThe definition models studied in this paper are based on recurrent neural network (RNN) models (Elman 1990; Hochreiter and Schmidhuber 1997). RNN models have established a new state-of-the-art performance on many sequence prediction and natural language generation tasks (Cho et al. 2014; Sutskever, Vinyals, and Le 2014; Karpathy and Fei-Fei 2014; Wen et al. 2015a). An important characteristic of dictionary definitions is that only a subset of the words in the definition depend strongly on the word being defined. For example, the word \u201cwoman\u201d in the definition of \u201cfeminine\u201d in Table 1 depends on the word being defined than the rest. To capture the varying degree of dependency, we introduce a gated update function that\nis trained to control information of the word being defined used for generating each definition word. Furthermore, since the morphemes of the word being defined plays a vital role in the definition, we experiment with a character-level convolutional neural network (CNN) to test whether it can provide complementary information to the word embeddings. Our best model can generate fluent and accurate definitions as shown in Table 1. We note that none of the definitions in the table exactly match any definition seen during training.\nOur contributions are as follows: (1) We introduce the definition modeling task, and present a probabilistic model for the task based on RNN language models. (2) In experiments with different model architectures and word features, we show that the gate function improves the perplexity of a RNN language model on definition modeling task by \u223c10%, and the character-level CNN further improves the perplexity by \u223c5%. (3) We also show that the definition models can be use to perform the reverse dictionary task studied in previous work, in which the goal is to match a given definition to its corresponding word. Our model achieves an 11.8% absolute gain in accuracy compared to previous state-of-the-art by Hill et al. (2016). (4) Finally, our error analysis shows that a well-trained set of word embeddings pays significant role in the quality of the generated definitions, and some of error types suggest shortcomings of the information encoded in the word embeddings."}, {"heading": "2 Previous Work", "text": "Our goal is to investigate RNN models that learns to define word embeddings by training on examples of dictionary definitions. While dictionary corpora have been utilized extensively in NLP, to the best of our knowledge none of the previous work has attempted create a generative model of definitions. Early work focused on extracting semantic information from definitions. For example, Chodorow (1985), and Klavans and Whitman (2001) constructed a taxonomy of words from dictionaries. Dolan et al. (1993) and Vanderwende et al. (2005) extracting semantic representations from definitions, to populate a lexical knowledge base.\nIn distributed semantics, words are represented by a dense vector of real numbers, rather than semantic predicates. Recently, dictionary definitions have been used to learn such embeddings. For example, Wang et al. (2015) used words in definition text as a form of \u201ccontext\u201d words for the Word2Vec algorithm (Mikolov et al. 2013b). Hill et al. (2016) use dictionary definitions to model compositionality, and evaluate the models with the reverse dictionary task. While these works learn word or phrase embeddings from definitions, we only focus on generating definitions from existing (fixed) embeddings. Our experiments show that our models outperform those of Hill et al. (2016) on the reverse dictionary task.\nOur work employs embedding models for natural language generation. A similar approach has been taken in a variety of recent work on tasks distinct from ours. Dinu and Baroni (2014) present a method that uses embeddings to map individual words to longer phrases denoting the same meaning. Likewise, Li et al. (2015) study how to encode a\nparagraph or document as an embedding, and reconstruct the original text from the encoding. Other recent work such as the image caption generation (Karpathy and Fei-Fei 2014) and spoken dialog generation (Wen et al. 2015a) are also related to our work, in that a sequence of words is generated from a single input vector. Our model architectures are inspired by sequence-to-sequence models (Cho et al. 2014; Sutskever, Vinyals, and Le 2014), but definition modeling is distinct, as it is a word-to-sequence task."}, {"heading": "3 Dictionary Definitions", "text": "In this section, we first investigate definition content and structure through a study of existing dictionaries. We then describe our new data set, and define our tasks and metrics."}, {"heading": "3.1 Definition Content and Structure", "text": "In existing dictionaries, individual definitions are often comprised of genus and differentiae (Chodorow, Byrd, and Heidorn 1985; Montemagni and Vanderwende 1992). The genus is a generalized class of the word being defined, and the differentiae is what makes the word distinct from others in the same class. For instance,\nPhosphorescent: emitting light without appreciable heat as by slow oxidation of phosphorous\n\u201cemitting light\u201d is a genus, and \u201cwithout applicable heat ...\u201d is a differntiae. Furthermore, definitions tend to include common patterns such as \u201cthe act of ...\u201d or \u201cone who has ...\u201d (Markowitz, Ahlswede, and Evens 1986). However, the patterns and styles are often unique to each dictionary.\nThe genus + differentiae (G+D) structure is not the only pattern for definitions. For example, the entry below exhibits distinct structures.\nEradication: the act of plucking up by the roots; a rooting out; extirpation; utter destruction\nThis set of definitions includes a synonym (\u201cextirpation\u201d), a reverse of the G+D structure (\u201cutter destruction\u201d), and an uncategorized structure (\u201ca rooting out\u201d)."}, {"heading": "3.2 Corpus: Preprocessing and Analysis", "text": "Dictionary corpora are available in a digital format, but are designed for human consumption and require preprocessing before they can be utilized for machine learning. Dictionaries contain non-definitional text to aid human readers, e.g. the entry for \u201cgradient\u201d in Wordnik1 contains fields (\u201cMathematics\u201d) and example usage (\u201cas, the gradient line of a railroad.\u201d). Further, many entries contain multiple definitions, usually (but not always) separated by \u201c;\u201d.\nWe desire a corpus in which each entry contains only a word being defined and a single definition. We parse dictionary entries from GCIDE2 and preprocess WordNet\u2019s glosses, and the fields and usage are removed. The parsers and preprocessing scripts can be found at https://github.com/northanapon/dict-definition.\n1https://www.wordnik.com/words/gradient 2http://gcide.gnu.org.ua/\nTo create a corpus of reasonable size for machine learning experiments, we sample around 20k words from the 50k most frequent words in the Google Web 1T corpus (Brants and Franz 2006), removing function words. In addition, we limit the number of entries for each word in a dictionary to three before the splitting by \u201c;\u201d (so that each word being defined may repeat multiple times in our corpus). After cleaning and pruning, the corpus has a vocabulary size of 29k. Other corpus statistics are shown in Table 2.\nWe analyze the underlying structure of the definitions in the corpus by manually labeling each definition with one of four structures: G+D, D+G, Syn (synonym), and Misc. In total, we examine 680 definitions from 100 randomly selected words. The results are shown in Table 3. We reaffirm earlier studies showing that the G+D structure dominates in both dictionaries. However, other structures are also present, highlighting the challenge inherent in the dictionary modeling task. Further, we observe that the genus term is sometimes general (e.g., \u201cone\u201d or \u201cthat\u201d), and other times specific (e.g. \u201can advocate\u201d)."}, {"heading": "3.3 Dictionary Definition Tasks", "text": "In the definition modeling (DM) task, we are given an input wordw\u2217, and output the likelihood of any given text D being a definition of the input word. In other words, we estimate P (D|w\u2217). We assume our definition model has access to a set of word embeddings, estimated from some corpus other than the definition corpus used to train the definition model.\nDM is a special case of language modeling, and as in language modeling the performance of a definition model can be measured by using the perplexity of a test corpus. Lower perplexity suggests that the model is more accurate at capturing the definition structures and the semantics of the word\nbeing defined. Besides perplexity measurement, there are other tasks that we can use to further evaluate a dictionary definition model including definition generation, and the reverse and forward dictionary tasks. In definition generation, the model produces a definition of a test word. In our experiments, we evaluate generated definitions using both manual examination and BLEU score, an automated metric for generated text. The reverse and forward dictionary tasks are ranking tasks, in which the definition model ranks a set of test words based on how likely they are to correspond to a given definition (the reverse dictionary task) or ranks a set of test definitions for a given word (the forward dictionary task) (Hill et al. 2016). A dictionary definition model achieves this by using the predicted likelihood P (D|w\u2217) as a ranking score."}, {"heading": "4 Models", "text": "The goal of a definition model is to predict the probability of a definition (D = [w1, ..., wT ]) given a word being defined w\u2217. Our model assumes that the probability of generating the tth word wt of a definition text depends on both the previous words and the word being defined (Eq 1). The probability distribution is usually approximated by a softmax function (Eq 2)\np(D|w\u2217) =\nT\u220f\nt=1\np(wt|w1, .., wt\u22121, w \u2217) (1)\np(wt = j|w1, .., wt\u22121, w \u2217) \u221d eWjht/\u03c4 (2)\nwhere Wj is a matrix of parameters associated with word j, ht is a vector summarizing inputs so far at token t, and \u03c4 is a hyper-parameter for temperature, set to be 1 unless specified. Note that in our expression, the word being defined w\u2217 is present at all time steps as an additional conditioning variable.\nThe definition models explored in this paper are based on a recurrent neural network language model (RNNLM) (Mikolov et al. 2010). An RNNLM is comprised of RNN units, where each unit reads one word wt at every time step t and outputs a hidden representation ht for Eq 2.\nht = g(vt\u22121, ht\u22121, v \u2217) (3)\nwhere g is a recurrent nonlinear function, vt denotes the embedding (vector representation) of the word wt, and v\u2217 is likewise the embedding of the word being defined."}, {"heading": "4.1 Model Architectures", "text": "A natural method to condition an RNN language model is to provide the network with the word being defined at the first step, as a form of \u201cseed\u201d information. The seed approach has been shown to be effective in RNNs for other tasks (Kalchbrenner and Blunsom 2013; Karpathy and Fei-Fei 2014). Here, we follow the simple method of Sutskever et al., (2011), in which the seed is added at the beginning of the text. In our case, the word being defined is added to the beginning of the definition. Note\nthat we ignore the predicted probability distribution of the seed itself at test time.\nSection 3 shows that definitions exhibit common patterns. We hypothesize that the word being defined should be given relatively more important for portions of the definition that carry semantic information, and less so for patterns or structures comprised of function and stop words. Further, Wen et al. (2015b) show that providing constant seed input at each time step can worsen the overall performance of spoken dialog generators.\nThus, inspired by the GRU update gate (Cho et al. 2014), we update the output of the recurrent unit with GRU-like update function as:\nzt = \u03c3(Wz [v \u2217;ht] + bz) (4)\nrt = \u03c3(Wr [v \u2217;ht] + br) (5)\nh\u0303t = tanh(Wh[(rt \u2299 v \u2217);ht] + bh) (6)\nht = (1\u2212 zt)\u2299 ht + zt \u2299 h\u0303t (7) where \u03c3 is the sigmoid function, [a; b] denotes vector concatenation, and \u2299 denotes element-wise multiplication. ht from Eq 3 is updated as given in Eq 7. At each time step, zt is an update gate controlling how much the output from RNN unit changes, and rt is a reset gate controlling how much information from the word being defined is allowed. We name this model Gated (G).\nIn the rest of this subsection, we present three baseline model architectures that remove portions of Gated. In our experiments, we will compare the performance of Gated against the baselines in order to measure the contribution of each portion of our architecture. First, we reduce the model into a standard RNNLM, where\nht = g(vt\u22121, ht\u22121) (8) The standard model only receives information about w\u2217 at the first step (from the seed). We refer to this baseline as Seed (S).\nA straightforward way to incorporate the word being defined throughout the definition is simply to provide its embedding v\u2217 as a constant input at every time step. We refer to this model as Input (I):\nht = g([v \u2217; vt\u22121], ht\u22121) (9)\n(Mikolov and Zweig 2012). Alternatively, the model could utilize v\u2217 to update the hidden representation from the RNN unit, named Hidden (H). The update function for Hidden is:\nht = tanh(Wh[v \u2217;ht] + bh) (10)\nwhere Wh is a weight matrix, and bh is the bias. In Hidden we update ht from Eq 3 using Eq 10. This is similar to the GRU-like architecture in Eq 7 without the gates (i.e. rt and zt are always vectors of 1s)."}, {"heading": "4.2 Other Features", "text": "In addition to model architectures, we explore whether other word features derived from the word being defined can provide complementary information to the word embeddings. We focus on two different features: affixes, and hypernym embeddings. To add these features within DM, we simply concatenate the embedding v\u2217 with the additional feature vectors.\nAffixes Many words in English and other languages consist of composed morphemes. For example, a word \u201ccapitalist\u201d contains a root word \u201ccapital\u201d and a suffix \u201c-ist\u201d. A model that knows the semantics of a given root word, along with knowledge of how affixes modify meaning, could accurately define any morphological variants of the root word. However, automatically decomposing words into morphemes and deducing the semantics of affixes is not trivial.\nWe attempt to capture prefixes and suffixes in a word by using character-level information. We employ a character-level convolution network to detect affixes (LeCun et al. 1990). Specifically, w\u2217 is represented as a sequence of characters with one-hot encoding. A padding character is added to the left and the right to indicate the start and end of the word. We then apply multiple kernels of varied lengths on the character sequence, and use max pooling to create the final features (Kim et al. 2016). We hypothesize that the convolution input, denoted as CH, will allow the model to identify regularities in how affixes alter the meanings of words.\nHypernym Embeddings As we discuss in Section 3, dictionary definitions often follow a structure of genus + differentia. We attempt to exploit this structure by providing the model with knowledge of the proper genus, drawn from a database of hypernym relations. In particular, we obtain the hypernyms from WebIsA database (Seitner et al. 2016) which employs Hearst-like patterns (Hearst 1992) to extract hypernym relations from the Web. We then provide an additional input vector, referred to as HE, to the model that is equal to the weighted sum of the top k hypernyms in the database for the word being defined. In our experiments k = 5 and the weight is linearly proportional to the frequency in the resource. For example, the top 5 hypernyms and frequencies for \u201cfawn\u201d are \u201ccolor\u201d:149, \u201cdeer\u201d:135, \u201canimal\u201d: 132.0, \u201cwildlife\u201d:82.0, \u201cyoung\u201d: 68.0."}, {"heading": "5 Experiments and Results", "text": "We now present our experiments evaluating our definition models. We train multiple model architectures using the train set and evaluate the model using the test set on all of the three tasks described in Section 3.3. We use the valid set to search for the learning hyper-parameters. Note that the words being defined are mutually exclusive across the three sets, and thus our experiments evaluate how well the models generalize to new words, rather than to additional definitions or senses of the same words.\nAll of the models utilize the same set of fixed, pretrained word embeddings from the Word2Vec project,3 and a 2-layer LSTM network as an RNN component (Hochreiter and Schmidhuber 1997). The embedding and LSTM hidden layers have 300 units each. For the affix detector, the character-level CNN has kernels of length 2-6 and size {10, 30, 40, 40, 40} with a stride of 1. During training, we maximize the log-likelihood objective using Adam, a variation of stochastic gradient decent (Kingma and Ba 2014). The learning rate is 0.001, and\n3https://code.google.com/archive/p/word2vec/\nthe training stops after 4 consecutive epochs of no significant improvement in the validation performance. The source code and dataset for our experiment can be found at https://github.com/websail-nu/torch-defseq."}, {"heading": "5.1 Definition Modeling", "text": "First, we compare our different methods for utilizing the word being defined within the models. The results are shown in the first section of Table 4. We see that the gated update (S+G) improves the performance of the Seed, while the other architectures (S+I and S+H) do not. The results are consistent with our hypothesis that the word being defined is more relevant to some words in the definition than to others, and the gate update can identify this. We explore the behavior of the gate further in Section 6.\nNext, we evaluate the contribution of the linguistic features. We see that the affixes (S+G+CH) further improves the model, suggesting that character-level information can complement word embeddings learned from context. Perhaps surprisingly, the hypernym embeddings (S+G+CH+HE) have an unclear contribution to the performance. We suspect that the average of multiple embeddings of the hypernym words may be a poor representation the genus in a definition. More sophisticated methods for harnessing hypernyms are an item of future work."}, {"heading": "5.2 Definition Generation", "text": "In this experiment, we evaluate the quality of the definitions generated by our models. We compute BLEU score between the output definitions and the dictionary definitions to measure the quality of the generation. The decoding algorithm is simply sampling a token at a time from the model\u2019s predicted probability distribution of words. We sample 40 definitions for each word being defined, using a temperature (\u03c4 in Eq 2) that is close to a greedy algorithm (0.05 or 0.1, selected from the valid set) and report the average BLEU score. For help in interpreting the BLEU scores, we also report the scores for three baseline methods that output definitions found in the training or test set. The first baseline, Inter, returns the definition of the test set word from the other dictionary. Its score thus reflects that of a definition that is semantically correct, but differs stylistically from the target dictionary. The other baselines (NE-WN and NE-GC) return the definition from the training set for the embedding nearest to that of the word being defined. In case of a word having multiple definitions, we micro-average BLEU scores before averaging an overall score.\nTable 5 shows the BLEU scores of the generated definitions given different reference dictionaries. AVG and Merge in the table are two ways of aggregating the BLEU score. AVG averages the BLEU scores by using each dictionary as the ground truth. The Merge computes score by using union of the two dictionaries. First, we can see that the baselines have low BLEU scores when evaluated on definitions from the other dictionary (Inter and NE-). This shows that different dictionaries use different styles. However, despite the fact that our best model S+G+CH is unaware of which dictionary it is evaluated against, it generates definitions that strike a balance between both dictionaries, and achieves higher BLEU scores overall. As in the earlier experiments, the Gated model improves the most over the Seed model. In addition, the affixes further improves the performance while the contribution of the hypernym embeddings is unclear on this task.\nIt is worth noting that many generated definitions contain a repeating pattern (i.e. \u201ca metal, or other materials, or other materials\u201d). We take the definitions from the language model (Seed) and our full system (S+G+CH+HE), and clean the definitions by retaining only one copy of the repeated phrases. We also only output the most likely definition for each word. The BLEU score increases by 2 (Seed* and S+G+CH+HE*). We discuss about further analysis and common error types in Section 6."}, {"heading": "5.3 Reverse and Forward Dictionary", "text": "In the dictionary tasks, the models are evaluated by how well they rank words for given definitions (RVD) or definitions for words (FWD). We compare against models from previous work on the reverse dictionary task (Hill et al. 2016). The previous models read a definition and output an embedding, then use cosine similarity between the output embedding and the word embedding as a ranking score. There are two ways to compose the output embedding: BOW w2v cosine uses vector addition and linear projection, and RNN w2v\ncosine uses a single-layer LSTM with 512 hidden units. We use two standard metrics for ranked results, accuracy at top k and R-Precision (i.e. precision of the top R where R is the number of correct definitions for the test word).\nTable 6 shows that our models perform well on the dictionary tasks, even though they are trained to optimize a distinct objective (definition likelihood). However, we note that our models have more parameters than those from previous work. Furthermore, we find that RNN w2v cosine performs better than BOW w2v cosine, which differs from the previous work. The differences may arise from our distinct preprocessing described in Section 3, i.e. redundant definitions are split into multiple definitions. We omit the information retrieval approach baseline because it is not obvious how to search for unseen words in the test set."}, {"heading": "6 Discussion", "text": "In this section, we discuss our analysis of the generated definitions. We first present a qualitative evaluation, followed by an analysis on how the models behave. Finally, we discuss error types of the generated definitions and how it might reflect information captured in the word embeddings."}, {"heading": "6.1 Qualitative Evaluation and Analysis", "text": "First, we perform a qualitative evaluation of the models\u2019 output by asking 6 participants to rank a set of definitions of 50 words sampled from the test set. For each word w, we provide in random order: a ground-truth definition for w (Dictionary), a ground-truth definition of the word w\u2032 whose embedding is nearest to that of w (NE),\nthe standard language model (Seed*), and our full system (S+G+CH+HE*). Inter-annotator agreement was strong (almost all inter-annotator correlations were above 0.6). Table 7 shows that definitions from the S+G+CH+HE* are ranked second after the dictionary definitions, on average. The advantage of S+G+CH+HE* over Seed* is statistically significant (p < 0.002, t-test), and the difference between S+G+CH+HE* is and NE is borderline significant (p < 0.06, t-test).\nAll of our results suggest that the gate-based models are more effective. We investigate this advantage by plotting the average gate activation (z and r in Eq 4 and 5) in Figure 1. The r gate is split into 3 parts corresponding to the embedding, character information, and the hypernym embedding. The figure shows that the gate makes the output distribution more dependent on the word being defined when predicting content words, and less so for function words. The hypernym embedding does not contribute to the performance and its gate activation is relatively constant. Additional examples can be found in the supplementary material.\nFinally, we present a comparison of definitions generated from different models to gain a better understanding of the models. Table 8 shows the definitions of three words from Table 1. The Random Embedding method does not generate good definitions. The nearest embedding method NE returns a similar definition, but often makes important errors (e.g., \u201cfeminine\u201d vs \u201cmasculine\u201d). The models using the gated update function generate better definitions, and the characterlevel information is often informative for selecting content words (e.g. \u201cmathematics\u201d in \u201cmathematical\u201d)."}, {"heading": "6.2 Error Analysis", "text": "In our manual error analysis of 200 labeled definitions. We find that 140 of them contain some degree of error. Table 9 shows the primary error types, with examples. Types (1) to (3) are fluency problems, and likely stem from the definition\nmodel, rather than shortcomings in the embeddings. We believe the other error types stem more from semantic gaps in the embeddings than from limitations in the definition model. Our reasons for placing the blame on the embeddings rather than the definition model itself are twofold. First, we perform an ablation study in which we train S+G+CH using randomized embeddings, rather than the learned Word2Vec ones. The performance of the model is significantly worsened (the test perplexity is 100.43, and the BLEU scores are shown in Table 5), which shows that the good performance of our definition models is in significant measure due to the embeddings. Secondly, the error types (4) - (6) are plausible shortcomings of embeddings, some of which have been reported in the literature. These erroneous definitions are partially correct (often the correct part of speech), but are missing details that may not appear in contexts of the word due to reporting bias (Gordon and Van Durme 2013). For example, the word \u201ccaptain\u201d often appears near the word \u201cship\u201d, but the role (as a leader) is frequently implicit. Likewise, embeddings are well-known to struggle in capturing antonym relations (Argerich, Torre\u0301 Zaffaroni, and Cano 2016), which helps explain the opposite definitions output by our model."}, {"heading": "7 Conclusion", "text": "In this work, we introduce the definition modeling task, and investigate whether word embeddings can be used to generate definitions of the corresponding words. We evaluate different architectures based on a RNN language model on definition generation and the reverse and forward dictionary tasks. We find the gated update function that controls the influence of the word being defined on the model at each time step improves accuracy, and that a character-level convolutional layer further improves performance. Our error analysis shows a well-trained set of word embeddings is crucial to the models, and that some failure modes of the generated definitions may provide insight into shortcomings of the word embeddings. In future work, we plan to investigate\nwhether definition models can be utilized to improve word embeddings or standard language models."}, {"heading": "8 Acknowledgments", "text": "This work was supported in part by NSF Grant IIS-1351029 and the Allen Institute for Artificial Intelligence. We thank Chandra Sekhar Bhagavatula for helpful comments."}], "references": [{"title": "M", "author": ["L. Argerich", "J. Torr\u00e9 Zaffaroni", "Cano"], "venue": "J.", "citeRegEx": "Argerich. Torr\u00e9 Zaffaroni. and Cano 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Franz", "author": ["T. Brants"], "venue": "A.", "citeRegEx": "Brants and Franz 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning phrase representations using RNN encoder-decoder", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "G", "author": ["M.S. Chodorow", "R.J. Byrd", "Heidorn"], "venue": "E.", "citeRegEx": "Chodorow. Byrd. and Heidorn 1985", "shortCiteRegEx": null, "year": 1985}, {"title": "and Baroni", "author": ["G. Dinu"], "venue": "M.", "citeRegEx": "Dinu and Baroni 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["W. Dolan", "L. Vanderwende", "Richardson"], "venue": "D.", "citeRegEx": "Dolan. Vanderwende. and Richardson 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["Schoenmackers Downey", "D. Etzioni 2007] Downey", "S. Schoenmackers", "O. Etzioni"], "venue": "In ACL 2007,", "citeRegEx": "Downey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2007}, {"title": "J", "author": ["Elman"], "venue": "L.", "citeRegEx": "Elman 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "B", "author": ["J. Gordon", "Van Durme"], "venue": "2013. Reporting bias and knowledge acquisition. In AKBC workshop,", "citeRegEx": "Gordon and Van Durme 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "M", "author": ["Hearst"], "venue": "A.", "citeRegEx": "Hearst 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning to understand phrases by embedding the dictionary. 4:17\u201330", "author": ["Hill"], "venue": null, "citeRegEx": "Hill,? \\Q2016\\E", "shortCiteRegEx": "Hill", "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning representations for weakly supervised natural language processing tasks. Computational Linguistics 40(1):85\u2013120", "author": ["Huang"], "venue": null, "citeRegEx": "Huang,? \\Q2014\\E", "shortCiteRegEx": "Huang", "year": 2014}, {"title": "and Blunsom", "author": ["N. Kalchbrenner"], "venue": "P.", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Fei-Fei", "author": ["A. Karpathy"], "venue": "L.", "citeRegEx": "Karpathy and Fei.Fei 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "Rush"], "venue": "2016. Character-aware neural language models. In AAAI", "citeRegEx": "Kim et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Ba", "author": ["D.P. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Whitman", "author": ["J. Klavans"], "venue": "B.", "citeRegEx": "Klavans and Whitman 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "S", "author": ["T.K. Landauer", "Dumais"], "venue": "T.", "citeRegEx": "Landauer and Dumais 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Handwritten digit recognition with a backpropagation network", "author": ["D. L"], "venue": null, "citeRegEx": "L.,? \\Q1990\\E", "shortCiteRegEx": "L.", "year": 1990}, {"title": "D", "author": ["J. Li", "M.-T. Luong", "Jurafsky"], "venue": "2015. A hierarchical neural autoencoder for paragraphs and documents. In ACL", "citeRegEx": "Li. Luong. and Jurafsky 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically significant patterns in dictionary definitions", "author": ["Ahlswede Markowitz", "J. Evens 1986] Markowitz", "T. Ahlswede", "M. Evens"], "venue": "ACL", "citeRegEx": "Markowitz et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Markowitz et al\\.", "year": 1986}, {"title": "and Zweig", "author": ["T. Mikolov"], "venue": "G.", "citeRegEx": "Mikolov and Zweig 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov,? \\Q2010\\E", "shortCiteRegEx": "Mikolov", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Yih Mikolov", "T. Zweig 2013] Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "HLT-NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "and Vanderwende", "author": ["S. Montemagni"], "venue": "L.", "citeRegEx": "Montemagni and Vanderwende 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C. Manning"], "venue": "EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "S", "author": ["J. Seitner", "C. Bizer", "K. Eckert", "S. Faralli", "R. Meusel", "H. Paulheim", "Ponzetto"], "venue": "2016. A large database of hypernymy relations extracted from the web. In LREC", "citeRegEx": "Seitner et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "Hinton"], "venue": "E.", "citeRegEx": "Sutskever. Martens. and Hinton 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2014", "author": ["I. Sutskever", "O. Vinyals", "Q. V Le"], "venue": "Sequence to sequence learning with neural networks. In Ghahramani, Z.; Welling, M.; Cortes, C.; Lawrence, N. D.; and Weinberger, K. Q., eds., NIPS", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Ratinov Turian", "J. Bengio 2010] Turian", "L.-A. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "A", "author": ["L. Vanderwende", "G. Kacmarcik", "H. Suzuki", "Menezes"], "venue": "2005. Mindnet: an automaticallycreated lexical resource. In HLT-EMNLP", "citeRegEx": "Vanderwende et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning lexical embeddings with syntactic and lexicographic knowledge", "author": ["Mohamed Wang", "T. Hirst 2015] Wang", "A. Mohamed", "G. Hirst"], "venue": "ACL", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen"], "venue": "EMNLP", "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["Wen"], "venue": "SIGDIAL", "citeRegEx": "Wen,? \\Q2015\\E", "shortCiteRegEx": "Wen", "year": 2015}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Yogatama"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Yogatama,? \\Q2015\\E", "shortCiteRegEx": "Yogatama", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings\u2019 semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a characterlevel convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.", "creator": "LaTeX with hyperref package"}}}