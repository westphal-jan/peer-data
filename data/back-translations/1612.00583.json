{"id": "1612.00583", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Active Search for Sparse Signals with Region Sensing", "abstract": "Intuitively, search algorithms can increase efficiency by collecting aggregated measurements that summarize large contiguous regions. However, most existing search methods either ignore the possibility of such regional observations (e.g. Bayesian optimization and multi-armed bandits) or make strong assumptions about the scanning mechanism that allows each measurement to encode arbitrarily all signals in the entire environment (e.g. compressive scanning). We propose an algorithm that actively collects data to search for sparse signals, using only loud measurements of averages in rectangular regions (including individual points) based on greedy maximization of information gathering. We analyze our algorithm in 1d and show that it can tip $\\ {O} (\\ frac} {m} {m} {m} {m} -problems when searching in space, i.e. effectively searching for signal strengths relative to effective signal locations ($).", "histories": [["v1", "Fri, 2 Dec 2016 07:44:45 GMT  (2126kb,D)", "http://arxiv.org/abs/1612.00583v1", "aaai 2017 preprint; nips exhibition of rejections"]], "COMMENTS": "aaai 2017 preprint; nips exhibition of rejections", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["yifei ma", "roman garnett", "jeff g schneider"], "accepted": true, "id": "1612.00583"}, "pdf": {"name": "1612.00583.pdf", "metadata": {"source": "CRF", "title": "Active Search for Sparse Signals with Region Sensing", "authors": ["Yifei Ma", "Roman Garnett", "Jeff Schneider"], "emails": ["yifeim@cs.cmu.edu", "garnett@wustl.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": null, "text": "1 Introduction Active search describes the problem where an agent is given a target to search for in an unknown environment and actively makes data-collection decisions so as to locate the target as quickly as possible. Examples of this setting include using aerial robots to detect gas leaks, radiation sources, and human survivors of disasters. The statistical principles for efficient designs of measurements date back to Gergonne (1815), but the growing trend to apply automated search systems in a variety of environments and with a variety of constraints has drawn much research attention recently, due to the need to address the disparate aspects of new applications.\nOne possibility in such active search scenarios we aim to explore, inspired by the robotic aerial search setting but with statistical insights that we hope to generalize, is the opportunity to take aggregate measurements that summarize\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nlarge contiguous regions of space. For example, an aerial robot carrying a radiation sensor will sense a region of space whose area depends on its altitude. How can such a robot dynamically trade off the ability to make noisier observations of larger regions of space against making higher-fidelity measurements of smaller regions?\nTo simplify the discussion, we will limit such region sensing observations to reveal the average value of an underlying function on a rectangular region of space, corrupted by independent observation noise. Noisy binary search is a simple realization of active search using such an observation scheme. This mechanism turns out to be sufficiently informative in the cases that we analyze to offer insights into a variety of search problems.\nThe ability to make aggregate region measurements in noisy environments has rarely been considered in previous work. Bayesian optimization, which has been used for localization of sparse signals (Carpin et al. 2015; Ma et al. 2015; Hern\u00e1ndez-Lobato, Hoffman, and Ghahramani 2014; Jones, Schonlau, and Welch 1998), usually considers only point measurements of an objective function. Notice that point observations can be considered in our framework if the allowed region sensing actions are constrained to be arbitrarily small. On the other extreme, compressive sensing (Donoho 2006; Cand\u00e8s and Wakin 2008; Wainwright 2009), considers scenarios where every measurement can reveal information about the entire environment through linear projection with arbitrary coefficients. This is not always a realistic assumption, as for example for an aerial robot, which can only sense its immediate vicinity. Between the two extremes, Jedynak, Frazier, and Sznitman (2012); Rajan et al. (2015); Haupt et al. (2009); Carpentier and Munos (2012); Abbasi-Yadkori (2012); Yue and Guestrin (2011) considered policies for search where observations can be made on any arbitrary subset of the search space, including discontiguous subsets, which is also often incompatible with the constraints in physical search systems.\nAnother assumption we make, common for example in compressive sensing, is sparsity. We assume that there are only a small number of strong signals in the environment; our goal is to recover these signals. Sparsity is necessary for the definition of active search problems; otherwise, for dense or weak signals, there is usually no better search approach than simply exhaustively mapping the entire space.\nIn addition to applicability in real search settings, spar-\nar X\niv :1\n61 2.\n00 58\n3v 1\n[ st\nat .M\nL ]\n2 D\nec 2\n01 6\nsity has unique mathematical properties when considered alongside region sensing. In unconstrained sensing, AriasCastro, Candes, and Davenport (2013) discovered a paradox that active compressive sensing (that is, the ability to adaptively select observations based on previously collected data) does not improve detection efficiency beyond logarithmic terms over random compressive sensing. This limitation is seen also when considering theoretical detection rates for active compressive sensing methods (Abbasi-Yadkori 2012; Carpentier and Munos 2012; Haupt et al. 2009). However, we show that active learning can in fact offer significant improvements in detection rates when observations are constrained to contiguous regions.\nWe propose an algorithm we call Region Sensing Index (RSI) that actively collects data to search for sparse signals using only noisy region sensing measurements. RSI is based on greedy maximization of information gain. Although information gain is a classic principle, we believe that its use in the recovery of sparse signals is novel and a good fit for robotic applications. We show that RSI uses O\u0303(n/\u00b52 + k2) measurements to recover all of k true signal locations with small Bayes error, where \u00b5 and n are the signal strength and the size of the search space, respectively (Theorem 3). The number of measurements with RSI is comparable with the rates offered by unconstrained compressive sensing, even though our constraints seem strong (i.e., region sensing loses all spatial resolution inside the region of measurement). Furthermore, we show that all passive designs under our contiguous region sensing constraint in 1d search spaces are fundamentally worse, with efficiency no better than sequential scanning of every point location, however strong the signals are. These results provide evidence to promote the use of and research into active methods."}, {"heading": "1.1 Related Work", "text": "Arias-Castro, Candes, and Davenport (2013) proved that the minimax sample complexity1 for any (i.e., potentially adaptive) algorithm to recover k sparse signal locations is at least \u2126(n/\u00b52), analyzing the problem in terms of the mean-squared error in the recovery of the underlying signal values. The authors also showed that a passive random design, combined with a nontrivial inference algorithm, e.g., Lasso (Wainwright 2009) or the Dantzig selector (Candes and Tao 2007), can have similar recovery rates (up to O(log n) terms). This result was presented as a paradox, suggesting that the folk statement that active methods have better sample complexity is not always true. Here we show that active search can make a substantial difference in recovery rates when the measurements are subject to the physically plausible constraint of region sensing, especially if the physical space has low dimensions.\nMalloy and Nowak (2014) presented the first active search algorithm that achieves the minimax sample complexity for general k. The algorithm is called Compressive Adaptive Sense and Search (CASS) and it can be adapted to region sensing in one-dimensional physical spaces. CASS directly\n1Sample complexity is equivalent to the number of measurements.\nextends bisection search, by allocating different sensing budgets to measurements at different bisection levels so as to minimize the cumulative error rates. However, CASS may fail if the repeated measurements of the same regions do not contain perfectly independent noise. It also has the limitation that it requires knowledge of the sensing budget apriori, yet produces no signal localization results until the very last measurements at the lowest level. Our paper addresses these practical issues with a redesigned active search algorithm using the Bayesian approach, which compares evidence instead of blindly trust the assumptions, and we use Shannon-information criteria, which implies bisection search in noiseless one-sparse cases.\nBraun, Pokutta, and Xie (2015) also used Shannoninformation criteria for active search but did not analyze their sample complexity under noisy measurements. Jedynak, Frazier, and Sznitman (2012); Rajan et al. (2015) studied a similar search problem where the \u201cregions\u201d are relaxed to any unions of disjoint subsets.\n2 Problem Formulation Consider a discrete space that is the Cartesian product of one-dimensional grids, X = \u220fdi=1[ni]; [n] = {1, . . . , n}. Let n = \u220f ni be the total number of points in X (here the product symbol is the arithmetic rather than the Cartesian product). We presume there is a latent real-valued nonnegative vector \u03b2 \u2208 Rn that represents the vector of true signals at all locations in X . We further assume that \u03b2 is sparse: it has value \u00b5 > 0 on k n locations in X and has value 0 elsewhere. We consider making observations related to \u03b2 through rectangular region sensing measurements, defined by\nyt = x > t \u03b2 + \u03b5t, s.t. xtj = wt1j\u2208At , \u03b5t \u223c N (0, \u03c32t ). (1)\nHere xt \u2208 Rn is a sensing vector that has support on At \u2286 X , a rectangular subset of X . We assume that the sensing vector has equal weight wt across its support. The resulting measurement, yt, is equal to the mean value of \u03b2 on At corrupted by independent Gaussian noise with variance \u03c32t . Note that selecting At suffices to specify the measurement location.\nIn 1d search environments, At may be any interval of [n], and the corresponding design takes the form xt = (0, . . . , 0, wt, . . . , wt, 0, . . . , 0)\n>. In higher search dimensions, we consider only regions that are contained in a hierarchical spacial pyramid (Lazebnik, Schmid, and Ponce 2006), i.e., a sequence of increasingly finer grid boxes with dyadic side lengths to cover the space at multiple resolutions.\nOur goal is to choose a sequence of designs X = {xt}Tt=1 so as to discover the support of \u03b2 with high confidence. Given a particular confidence, we will measure sample complexity by assuming \u2016xt\u20162 = 1 and \u03c3t \u2261 1 for each measurement and count the total number of measurements required to achieve that confidence, T . Letting \u2016xt\u20162 = 1 implies wt = 1/ \u221a \u2016xt\u20160, which can be seen as a relaxed notion of the region average, because the signal strength of a region measurement, which is \u00b5wt, still decreases as the region size \u2016xt\u20160 increases.\nAlgorithm 1 Region Sensing Index (RSI)\nRequire: \u03c00(k, n, \u00b5), T or , and the unknown \u03b2\u2217 Ensure: S\u0302t // (5) 1: for t = 1, 2, . . . do 2: pick xt = arg maxx\u2208X I(\u03b2; y | x, \u03c0t\u22121) // (3)&(4) 3: observe yt = x>t \u03b2\n\u2217 + \u03b5t 4: update \u03c0t(\u03b2) \u221d \u03c0t\u22121(\u03b2)p(yt | \u03b2,xt\u22121) // (2) 5: find (\u0304t, S\u0302t) = arg min|S\u0302|=k 1 kE [ |S\u0302\u2206S| | \u03c0t ] // (5) 6: break if t \u2265 T or \u0304t < , if either is defined\nThe measure of T is made to be comparable with another common choice of sample complexity, the Frobenius norm of the entire design \u2016X\u20162F , when the rows of X are normalized (Arias-Castro, Candes, and Davenport 2013). However, the normalization is often overlooked in classical compressive sensing, which allows algorithms to cheat in region sensing by making an enormous number of measurements of small weight and changing the sensing locations frequently. Another measure of complexity is to measure both \u2016X\u20162F and the number of location changes simultaneously (Malloy and Nowak 2014). However, our discretized counting of measurements is conceptually simpler.\nOur analysis is Bayesian and we will analyze performance in expectation, with prior \u03b2 \u223c \u03c00(\u03b2), a uniform distribution on the model class, S\u00b5 ( n k ) , which includes all k-sparse mod-\nels with \u00b5 signal strength among n locations (i.e., it has ( n k ) possible outcomes). The Bayes risk will be measured by the expected Delta loss, \u0304T = 1kE|S\u2206S\u0302T |, where S\u0302T is the best estimator of the k signal locations after T measurements and \u2206 is the symmetric difference operator on a pair of sets.\n3 Proposed Methods We note that region sensing loses all spatial resolution inside the region of measurement. Here we borrow ideas from noisy binary search, which has a similar property, and use information gain (IG) to drive the observation process. We name our algorithm Region Sensing Index (RSI, Algorithm 1). Like other active learning algorithms, RSI is a combination of an inference subroutine that constantly updates the distribution of \u03b2 using the collected data and a design subroutine that chooses the next region to sense based on the latest information from the inference subroutine. The inference subroutine. We use exact Bayesian inference with a uniform prior \u03c00(\u03b2) on the model class S\u00b5 ( n k ) . Denote the outcome of the first t measurements as Dt = {(x\u03c4 , y\u03c4 ) : 1 \u2264 \u03c4 \u2264 t}. Even though Dt contains a dependent sequence of data collections, where x\u03c4 depends onD\u03c4\u22121,\u2200\u03c4 , Bayesian inference decomposes into a series of efficient updates:\n\u03c0(\u03b2 | Dt) \u221d \u03c0(\u03b2)p(Dt | \u03b2) = \u03c00(\u03b2) \u220ft \u03c4=1 ( p(x\u03c4 | D\u03c4\u22121)p(y\u03c4 | \u03b2,x\u03c4 ) )\n\u221d \u03c00(\u03b2) \u220ft \u03c4=1 p(y\u03c4 | \u03b2,x\u03c4 ), (2)\nwhere p(x\u03c4 | D\u03c4\u22121) is the design without knowledge of the true \u03b2 and thus dropped. Define \u03c0t(\u03b2) = \u03c0(\u03b2 | Dt); the\nupdates have the form \u03c0t(\u03b2) \u221d \u03c0t\u22121(\u03b2)p(yt | \u03b2,xt) = \u03c0t\u22121(\u03b2)\u03c6(yt \u2212 x>t \u03b2), where \u03c6 is the standard normal pdf. The design subroutine. The next sensing vector, xt+1 \u2208 X , is chosen to maximize the IG:\nI(\u03b2; y | x, \u03c0t) = H(y | x, \u03c0t)\u2212 E [ H(y | x,\u03b2) | \u03c0t ] , (3)\nwhich is the difference between the entropy of the marginal distribution, p(y | x, \u03c0t) = \u222b \u03c6(y \u2212 x>\u03b2)\u03c0t(\u03b2) d\u03b2, and the expected entropy of the conditional distribution, p(y | \u03b2;x) = \u03c6(y \u2212 x>\u03b2). The latter, i.e., the conditional distribution for any realization of \u03b2, has fixed entropy: log \u221a 2\u03c0e. Meanwhile, the marginal entropy has no closed-form solutions; instead, we use numerical integration.\nThe numerical integration is rather straightforward, because the marginal density function is analytical. From now on, we will assume that (x, A, a, wx) correspond to the same design (its sensing vector, its locations, its region size, and its sensing weight per coordinate, respectively). Define two new variables, \u03bb = \u00b5wx(= \u00b5/ \u221a a) and \u03b3 = x>\u03b2/\u03bb, and one new parameter p = (p0, . . . , pk)> in (4). The goal is to change the variable of the integration for the marginal density function of y to:\np(y | x, \u03c0t) = \u222b \u03c0t(\u03b2)\u03c6(y \u2212 x>\u03b2) d\u03b2\n= \u2211k\nc=0 pc \u03c6(y \u2212 c\u03bb) = p(y | \u03bb,p),\nwhere pc = Pr(\u03b3 = c) = \u2211\n\u03b2:x>\u03b2=c\u03bb \u03c0t(\u03b2). (4)\nNotice, \u03b3 only has a finite number of choices: \u03b3 = |A\u2229S| \u2208 {0, . . . , k}, where S is the nonzero support of \u03b2, because both x and \u03b2 are constant on their respective supports (xj = wx,\u2200j \u2208 A and \u03b2j = \u00b5,\u2200j \u2208 S). We then numerically evaluate H(y | x, \u03c0t) = H(y | \u03bb,p) with the obtained (4).\nThe Bayes estimator of signal locations. We pick the ksparse set S\u0302T to minimize the posterior risk:\nmin |S\u0302|=k\n1 k E [ |S\u0302\u2206S| | \u03c0T ] = 1 k \u2211 \u0131\u0302\u2208S\u0302 E ( 1{\u03b2\u0131\u0302=0} | \u03c0T ) , (5)\nwhere \u03b2\u0131\u0302 is the \u0131\u0302-th element of \u03b2. In other words, RSI picks the top k locations where the posterior marginal expectation is the largest. When k = 1, this is equivalent to picking \u03b2\u0302T = arg max\u03c0T (\u03b2). Otherwise, (5) yields the smallest Bayes risk \u0304(DT ) given any collected data DT ."}, {"heading": "3.1 Accelerations", "text": "In practice, holding ( n k ) models in memory can be infeasible if k is large, we can instead recover the support of \u03b2 element-wise by repeatedly applying RSI assuming k = 1. After the posterior distribution \u03c0t(\u03b2(1)) converges to a pointmass distribution at the most-likely one-sparse model with sufficient confidence, we report its location and move on by\n2In real world experiments, we additionally estimate \u00b5\u0302\u0302 using a point measurement on the inferred signal location for better modeling.\nTable 1: Conditions and conclusions for sample complexity.\nDesign Type Region Sensing Algorithm\nPrior for Bayes Risk\nMin T to Guarantee \u0304T = 1 kE|S\u2206S\u0302T | \u2264\nSample Complexity\u2217\npassive yes (any) \u03c00(\u00b5\u2192\u221e) T \u2265 n2 (1\u2212 n\u22121n\u2212k ) (Theorem 1) \u0398(n)\nPoint sensing T \u2264 n(1\u2212 n\u22121n\u2212k ) (Corollary A.2)\nactive no (any) \u03c0\u03030 T \u2265 4n\u00b52 (1\u2212 )2 (Theorem 2) \u2126( n\u00b52 )\u2020\nyes CASS [2014] max risk (incl. \u03c00)\nT \u2264 20 n\u00b52 log( 8k ) + 2k log2(nk ) O\u0303( n\u00b52 + k)\u2021\nRSI (ours) \u03c00 T\u0304 \u2264 50( n\u00b52 + k\n2\n9 ) log2( 2 ) log( n )\n(Theorem 3) O\u0303( n\u00b52 + k 2)\u2021\n\u2217 Assume = O(1) and k n. \u2020 Shown for unconstrained sensing; binary search requires \u2126(log2(n)+k) additional measurements. \u2021 log(n) terms are left out. T\u0304 is defined differently; see Section 4.2 for details.\nAlgorithm 2 Region Sensing Index-Any-k (RSI-A)\nRequire: n, \u00b5, , and the unknown \u03b2\u2217"}, {"heading": "Ensure: S\u0302", "text": "1: initialize S\u0302 = \u2205, \u03b2\u0302 = 0 2: for k = 1, 2, . . . , do 3: infer \u03c00(\u03b2(k)) \u221d \u220ft \u03c4=1 p(y\u03c4 | \u03b2(k) + \u03b2\u0302,x\u03c4 ),\n\u2200\u03b2(k) \u2208 {\u00b51j : j 6\u2208 S\u0302} 4: call S\u0302(k) = RSI (\u03c00, ,\u03b2\u2217 \u2212 \u03b2\u0302) 5: aggregate S\u0302 = \u222ac\u2264kS\u0302(c) and \u03b2\u0302 = \u2211 \u0302\u2208S\u0302 \u00b5\u0302\u03021\u0302 . 2\nremoving the reported point from the search and recomputing the posterior distributions using the uniform prior, \u03c00(\u03b2(2)), on the new class, S\u00b5 ( n\u22121 1 ) .\nWe call this alternative algorithm Region Sensing IndexAny-k (RSI-A, Algorithm 2) and use it in our simulations so that the computational cost is no longer exponential in k. Notice, our analysis is for the unmodified RSI; the statistical disadvantage of RSI-A is no more than O(k), multiplicatively.\nWhen implementing RSI-A, we also avoid unnecessary numerical integration (3), if the region is guaranteed to have inferior IG, indicated by its p vector (4), which is easier to compute. We use the fact that I(\u03b3; y | p, \u03bb) with fixed \u03bb > 0 is concave in the probability simplex \u2206k = {p \u2208 [0, 1]k+1 : p>1 = 1}. Under k = 1 approximation, the region whose marginal probability p1 = \u2211 x>\u03b2>0 \u03c0(\u03b2) is closest to 0.5 will provably have the largest IG among all regions of the same size. Thus, we find the region with the highest IG in two steps: (1) compare the p1 value for all regions for every region size and (2) evaluate the IG of only these regions with the best p1 values (closest to 0.5) in their region sizes.\n4 Theoretical Analysis in 1D The analysis is cleanest when the search space is 1d, where the regions can be any integer intervals that subset [1, n]. Without loss of generality (WLOG), assume n is a multiple of k and n \u2265 2k. Our goal is to find the smallest number\nof measurements, T , to guarantee a small Bayes risk \u0304T = 1 kE|S\u2206S\u0302T | \u2264 . Table 1 summarizes our analysis. The sample complexity is best appreciated assuming \u00b5 1, k n, and = O(1). A typical choice is = 1/2, i.e., the number of measurements to guarantee that half of the signal support can be recovered on average."}, {"heading": "4.1 Baseline Results", "text": "Here we provide lower bounds on sample complexity. We show that under region-sensing constraints, all passive methods require T \u2265 \u2126(n) measurements and active methods require T \u2265 \u2126(n/\u00b52 +k). When \u00b5 1, active methods have significant potential for improvement over passive methods using region sensing, which contradicts with the view in unconstrained compressive sensing by Arias-Castro, Candes, and Davenport (2013); Soni and Haupt (2014). Theorem 1 (Limits of any passive methods using region sensing). Assume \u03b2 has prior \u03c00 (uniform random on S\u00b5 ( n k ) ). Any passive method with T noiseless region measurements on 1d must incur Bayes risk \u0304T \u2265 n\u2212kn\u22121 (1\u2212 2Tn ). To guarantee \u0304T \u2264 , T \u2265 n2 (1\u2212 n\u22121n\u2212k ) is required.\nThe proof is due to model identifiability, neglecting observation noise. More details can be found in the appendix. It applies to any \u00b5 \u2265 0 and particularly \u00b5\u2192\u221e. Theorem 2 (Limits of any methods, (Arias-Castro, Candes, and Davenport 2013)). Assume \u03b2 has a slightly different prior, \u03c0\u03030, that includes each location in X in the support of \u03b2 independently with probability k/n. Any method (including active and non-region-sensing) must have \u0304T \u2265 1\u2212 \u00b52 \u221a T/n. To guarantee \u0304T \u2264 , T \u2265 4n\u00b52 (1\u2212 )2 is required. The proof can be found under Theorem 3 of (Arias-Castro, Candes, and Davenport 2013). Arias-Castro, Candes, and Davenport (2013) gave a minimax risk with similar terms by modifying \u03c0\u03030 to a least favorable prior on all models that are at most k-sparse. However, we only study Bayes risk for technical convenience.\nWhen using Theorem 2 for reference, notice the difference between \u03c0\u03030 and \u03c00 that the former additionally treats\nthe sparsity to be a random variable k\u0303 with expectation k. From concentration inequalities, |k\u0303 \u2212 k| \u2264 O( \u221a k), with\nhigh probability. While k\u0303 and k are not directly comparable, Theorem 2 is still a useful baseline. Under region-sensing constraints, the number of measurements must also be at least \u2126(k) to allow visits to most of the nonzero locations at least once, in a nontrivial draw of S where the signals are separated.\nWith respect to Theorem 1, the point sensing or any nonrepeating region sensing will achieve the optimal sample complexity (up to constant factors, see Appendix A for more details). For Theorem 2, the CASS method published by Malloy and Nowak (2014) for active sensing with region constraints3 acheives a nearly optimal rate in theory. Table 1 contains a detailed summary of the sample complexities of several algorithms, including our own."}, {"heading": "4.2 Main Result", "text": "For technical convenience, we directly express our main result in terms of the expected number of measurement that are actually taken so as to realize \u0304(DT ) \u2264 for a given threshold in an experiment. Taking T = T as a random variable, the expected number of actual measurements is different from the pre-determined sampling budget that an algorithm fully consumes to guarantee a desirable averaged risk (see Section 4.1). However, it is a comparable alternative in Bayesian analysis, used by e.g., Lai and Robbins (1985); Kaufmann, Korda, and Munos (2012). When the objective is constant = O(1), our result implies a deterministic budget requirement of the same order of complexity, T \u2264 \u221212 ET 2 , where 2 = 2 , by direct application of Markov\u2019s inequality.\nTheorem 3 (Sample complexity of RSI). In active search for k sparse signals with strength \u00b5 in 1d physical space of size n \u2265 2k (WLOG, assume n is a multiple of k), given any > 0 as tolerance of posterior Bayes risk, RSI using region sensing has bounded expected number of actual measurements,\nT\u0304 = E[min{T : \u0304(DT ) \u2264 }]\n\u2264 50 ( n \u00b52 + k2 9 ) log2 (2 ) log (n ) = O\u0303 ( n \u00b52 + k2 ) , (6)\nwhere the expectation is taken over the prior distribution and sensing outcomes."}, {"heading": "4.3 Proof Sketch", "text": "The proof for Theorem 3 hinges on an observation that the information gain (IG) where RSI makes measurements is consistently large, before active search terminates with minimal Bayes risk. For example, the IG of any measurement in binary search with k = 1 and noiseless observations is always O(log(2)). However, IG is harder to approximate when the observations are noisy. Therefore, we first show an intuitive lower bound for IG. Recall notations from (4).\n3The original result in Malloy and Nowak (2014) is stronger; it considers the maximum probability of support recovery mistakes, P (S 6= S\u0302) \u2264 \u03b4, for any S that are k-sparse and any signals with at least \u00b5 strength.\nProposition 4. The IG score of a region sensing design has lower bounds with respect to its design parameters (\u03bb,p), as\nI(\u03b3; y | \u03bb,p) \u2265 2qcq\u0304c ( 2\u03a6 (\u03bb\n2\n) \u2212 1 )2\n\u2265 1 12 min{qc, q\u0304c}min{\u03bb2, 32}, \u22001 \u2264 c \u2264 k, (7)\nwhere qc = Pr(\u03b3 \u2265 c) = \u2211 \u03ba\u2265c p\u03ba, q\u0304c = 1\u2212 qc, and \u03a6(u) is the standard normal cdf.\nThe proof uses Pinsker\u2019s inequality and is given in Section B in the appendix. Notice using the common choice of Jensen\u2019s inequality will give bounds in the opposite direction. To formalizes our observation that the IG is bounded: Lemma 5. WLOG, assume n is a multiple of k and n \u2265 2k. At any step, if the current Bayes risk \u0304(D) > , we can always find a region A of size at most nk , such that \u03bb 2 \u2265 \u00b52a = k\u00b52 n and 2 \u2264 E[\u03b3 | D] \u2264 1\u2212 2 (we call this Condition E), which further yields\nI(\u03b3; y | \u03bb,p) \u2265 I\u2217 =\n25k min {k2\u00b52 n , 32 } . (8)\nThe way to find the region A that satisfies Condition E is given in Lemma B.5 in the appendix. The reason that Condition E is sufficient for (8) can be derived from Proposition 4 for k = 1 and Lemma B.6 in the appendix for k > 1.\nEq (8) shows the minimum decrease in the model entropy in expectation after each measurement, starting from the maximum entropy of a uniform prior distribution, k log(n). However, the posterior entropy can never be negative, which implies a bound on the expected number of times that (8) can be applied, i.e. the expected number of measurements to reach Bayes risk is 25 log(n) (\nn \u00b52 +\nk2\n9 ). Lemma D.5 in the appendix shows some additional improvements to obtain the logarithmic dependency of in Theorem 3.\n5 Simulation Studies We evaluated RSI or its approximation RSI-A when k > 1. Other baseline algorithms include: \u2022 CASS (compressive adaptive sense and search) (Malloy and Nowak 2014): a branch-and-bound algorithm that traverses the region hierarchy from top to down using preallocated budgets per level. We count each xi as \u2016xi\u201622 region sensing measurements (rounded up to the next integer). \u2022 Point sensing: a passive design that uses exhaustive point measurements on all locations. \u2022 CS (compressive sensing) (Donoho 2006): a non-regionsensing design that draws xt \u223c N (0, I) and rescales \u2016xt\u201622 to 1. CS then solves a convex optimization problem to infer the nonzero signals, by minimizing \u2211 t \u2016yt\u2212x>t \u03b2\u201622+\u03bb\u2016\u03b2\u20161 s.t. \u03b2 \u2265 0, where \u03bb is chosen to produce exactly k nonzero coefficients using the Lasso regularization path.\nWe picked n = 1024 and various k (sparsity) and d (the dimension of the physical space) annotated below the plots. In the d = 5 case, we chose the region space to be the Cartesian product of [4]5 and allowed regions from a spatial pyramid (Lazebnik, Schmid, and Ponce 2006) of granularity\n45, 25, and 15. Each method was run with 200 repetitions to find its average performance.\nFigure 1(a) compares the recall rates of the algorithms as they progressed in a 1d search for a single true signal of strength \u00b5 = 16. RSI was the most efficient, finding the correct location in 50% of the cases with as few as T = 20 measurements. CASS was comparable only at the step points when all the allocated budgets were used, due to its rather rigid designs. We drew multiple curves for CASS to reflect this fact; the turning points were at T = 28 and 56 for = 0.5 and 0.85, respectively. CS was less effective compared with CASS with equal budgets (e.g., \u2016X\u20162F = 52 > 28 for = 0.5) which agrees with the analysis in Arias-Castro, Candes, and Davenport (2013). Point sensing was the least efficient, using T = n/2 = 512 measurements, which was worse than the other methods by a factor of \u2126\u0303(\u00b52) (ignoring logarithmic terms). Notice, due to non-identifiability, any passive designs would have equal or worse rates.\nFigure 1(b) extends the comparison on the full spectrum of SNR, 1/4 < \u00b5 < 1024, showing the minimum number of measurements T to guarantee constant Bayes risk \u0304T < 0.5. RSI led the comparison, showing a sample complexity of O\u0303(n/\u00b52) when \u00b5 is small and O\u0303(1) when \u00b5 is large. CASS also had a similar trend. CS ignores the region sensing constraints and was inferior to RSI. Notice CS also has a minimum sample complexity, but in order to meet the incoherence conditions for Lasso sparsistency (Candes and Tao 2007; Wainwright 2009; Raskutti, Wainwright, and Yu 2010), the rank of the covariance matrix of the measurements X>SXS must be at least k. Point sensing and other passive region\nsensing would always require at least \u2126(n) measurements regardless of \u00b5. Figure 1(c-d) show similar conclusions with other choices of k and d. The number of measurements was largely unaffected by k > 1 if \u00b5 is low, which supports the first term of Theorem 3, which is O\u0303(n/\u00b52). Comparisons between CS and RSI in high dimensions (d > 1) depend on how region constraints are defined. In our high-dimensional simulations, the region choices were rather limiting for RSI, giving more advantage to the unconstrained CS when \u00b5 is large.\n6 Real World Datasets Region sensing was intended to address the problems of real robotic search. Here, we took satellite images like Figure 2a and used natural blue pixels, e.g., the blue roof which we circled near the lower left of the center of the image, as a simulated target of interest. These experiments directly simulate search and rescue in open areas based on life jacket colors or communication signals and also share similarities with gas leaks or radiation detection, where real data is usually sensitive or expensive. Many assumptions were violated in these experiments, e.g., the noise was not iid and the target was a collection of neighboring pixels. For the purpose of more accurate modeling of the actual measurement powers at each region level, we used statistics from the real data to model \u00b5w(a) and \u03c3(a) as functions of the region size a(= \u2016x\u20160).\nFigure 2b shows in the background the actual scalar observations, affinely transformed from the original RGB values to filter out the target blue color. The foreground contains the rectangular regions of measurement, sequentially decided by RSI after observing the average values in previous regions. Feasible region choices were contained in a spatial pyramid (Lazebnik, Schmid, and Ponce 2006). RSI behaved similarly to sequential scanning at the optimal altitude except for occasional bisections into subregions.\nBy comparing the IG of all feasible regions, RSI usually decides to (a) sense the next region in space when the previous outcome is low, (b) investigate the subregions when the last parent region yields a large outcome (we disallow repeated actions for the lack of noise-independence,) or (c) back out from an investigation if the subsequent measurements yield low outcomes. Option (c) demonstrates the ability of error recovery, which is our advantage to CASS thanks to Bayesian modeling. The search in Figure 2b ended after 36 measurements, whereas the image contained 36 000 pixel points.\nFigure 3 compares the performances on 221 image patches of 512\u00d7 512 pixels, cropped from National Agriculture Imagery Program (NAIP).4 The other algorithms for comparison include random (point), CS, and CASS*. Here, CASS* is a modified CASS method where each measurement can only be taken once, because repeated measurements yield the same outcome. To fully represent CASS*, in addition to choosing k by the true sparsity, we added fixed choices of k = 64 and 512, yielding three different curves.\nRSI achieved the best performance, finding on average 60% blue pixels with as few as 1700 measurements (0.5% of the total number of feasible observations). CASS* performance highly depended on the parameter choices and produced results only near the end of the experiment. CS did poorly, probably due to the fact the signals were not\niid (a blue object can contain multiple pixels).\n7 Discussions Region sensing is a new setting motivated by robotic search operations where we also found statistical insights to contrast with the unconstrained sensing in Arias-Castro, Candes, and Davenport (2013). RSI performs near-optimally in 1d search domains and fundamentally faster than passive sensing. In higher dimensions, the analysis may be harder, especially for passive baselines. The number of subregions generated by intersecting the measurement regions may be harder to count, unless measurement regions are restricted to grid regions in a spatial pyramid (such that any pair of regions is either nested or disjoint). We also want to establish frequentist analysis in the future. Finally, it is interesting to generalize the measurement model beyond taking the average value of a single region at a time.\nAcknowledgments This work is partially supported by the DARPA grant FA87501220324, National Science Foundation under Award Number IIA-1355406, and ARPA-E TERRA-REF award DEAR0000594. We also appreciate suggestions and discussions from Aarti Singh, Ying Yang, and Yining Wang.\nReferences Abbasi-Yadkori, Y. 2012. Online-to-confidence-set conversions and application to sparse stochastic bandits. Arias-Castro, E.; Candes, E. J.; and Davenport, M. 2013. On the fundamental limits of adaptive sensing. Information Theory, IEEE Transactions on. Braun, G.; Pokutta, S.; and Xie, Y. 2015. Info-greedy sequential adaptive compressed sensing. IEEE Journal of Selected Topics in Signal Processing 9(4):601\u2013611.\n4https://lta.cr.usgs.gov/node/300\nCandes, E., and Tao, T. 2007. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics. Cand\u00e8s, E. J., and Wakin, M. B. 2008. An introduction to compressive sampling. Signal Processing Magazine, IEEE 25(2):21\u201330. Carpentier, A., and Munos, R. 2012. Bandit theory meets compressed sensing for high dimensional stochastic linear bandit. In AISTATS, volume 22, 190\u2013198. Carpin, M.; Rosati, S.; Khan, M. E.; and Rimoldi, B. 2015. Uavs using bayesian optimization to locate wifi devices. arXiv preprint arXiv:1510.03592. Donoho, D. L. 2006. Compressed sensing. Information Theory, IEEE Transactions on 52(4):1289\u20131306. Gergonne, J. D. 1815. Application de la m\u00e9thode des moindre quarr\u00e9s a l\u2019interpolation des suites. Annales des Math Pures et Appl. Haupt, J. D.; Baraniuk, R. G.; Castro, R. M.; and Nowak, R. D. 2009. Compressive distilled sensing: Sparse recovery using adaptivity in compressive measurements. In Signals, Systems and Computers. IEEE. Hern\u00e1ndez-Lobato, J. M.; Hoffman, M. W.; and Ghahramani, Z. 2014. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems. Jedynak, B.; Frazier, P. I.; and Sznitman, R. 2012. Twenty questions with noise: Bayes optimal policies for entropy loss. Journal of Applied Probability 49(1):114\u2013136. Jones, D. R.; Schonlau, M.; and Welch, W. J. 1998. Efficient global optimization of expensive black-box functions. Journal of Global optimization 13(4):455\u2013492. Kaufmann, E.; Korda, N.; and Munos, R. 2012. Thompson sampling: An asymptotically optimal finite-time analysis. In Algorithmic Learning Theory, 199\u2013213. Springer. Lai, T. L., and Robbins, H. 1985. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics 6(1):4\u201322. Lazebnik, S.; Schmid, C.; and Ponce, J. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Computer Vision and Pattern Recognition. IEEE. Ma, Y.; Sutherland, D.; Garnett, R.; and Schneider, J. 2015. Active pointillistic pattern search. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics. Malloy, M. L., and Nowak, R. D. 2014. Near-optimal adaptive compressed sensing. Information Theory, IEEE Transactions on. Rajan, P.; Han, W.; Sznitman, R.; Frazier, P.; and Jedynak, B. 2015. Bayesian multiple target localization. In Proceedings of the 32nd International Conference on Machine Learning (ICML). Raskutti, G.; Wainwright, M. J.; and Yu, B. 2010. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research. Soni, A., and Haupt, J. 2014. On the fundamental limits of recovering tree sparse vectors from noisy linear measurements. Information Theory, IEEE Transactions on. Wainwright, M. J. 2009. Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso). Information Theory, IEEE Transactions on. Yue, Y., and Guestrin, C. 2011. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, 2483\u20132491.\nActive Search for Sparse Signals with Region Sensing (Appendix)\nYifei Ma Carnegie Mellon University\nPittsburgh PA 15213, US yifeim@cs.cmu.edu\nRoman Garnett Washington University in St. Louis\nSt. Louis, MO, USA garnett@wustl.edu\nJeff Schneider Carnegie Mellon University\nPittsburgh PA 15213, US schneide@cs.cmu.edu\nAbstract\nThis supplementary material includes both theoretical details (Section A\u2013B) and additional empirical results (Section C). For the theoretical part, our main paper has two separate results: a hardness result showing passive methods under region sensing constraints in 1d search spaces cannot be efficient (Theorem 1 and the first row in Table 1 in the main paper) and a positive result showing that our Region Sensing Index (RSI) is sample-efficient (Theorem 3), comparable with the optimal efficiency obtained in (Arias-Castro, Candes, and Davenport 2013). The empirical part contains the choice of parameters and a demo of search results. For convenience, in the appendix, we sometimes use K to represent the total number of sparse signals in the system, whereas in the main document we always used k.\nA Theoretical Properties for Passive Sensing\nTheorem A.1 (Theorem 1 in the main document; limits of any passive methods using region sensing). Assume \u03b2 has prior \u03c00 (uniform random on S\u00b5 ( n k ) ). Any passive method with T noiseless region measurements on 1D must incur Bayes risk \u0304T \u2265 n\u2212kn\u22121 (1\u2212 2Tn ); to guarantee \u0304T \u2264 , it requires T \u2265 n2 (1\u2212 n\u22121n\u2212k ).\nProof. We count the number of non-identifiability models with T noiseless observations, particularly when T < n2 .\nAn aggregate measurement on region [ai, bi) \u2282 [1, n+ 1) cannot identify the sparse support inside [ai, bi) (or its complement), unless it intersects with another aggregate measurement. Should two measurement regions intersect, the model is still nonidentifiable inside the intersection, set differences, and the complement of the union of both. To find out the set of all disjoint subsets where the model is non-identifiable given any passive design with m region measurements, {[ai, bi) \u2282 [1, n+ 1) : i = 1, . . . ,m}, we simply sort the unique end points as c1 < \u00b7 \u00b7 \u00b7 < cp \u2208 {ai, . . . , am} \u222a {b1, . . . , bm}, where p \u2264 2m, and use the following set of p elementary subsets:\n{ [cj , cj+1)\ufe38 \ufe37\ufe37 \ufe38\nCj\n: j = 1, . . . , p\u2212 1 } \u222a {\n[cp, n+ 1) \u222a [1, c1)\ufe38 \ufe37\ufe37 \ufe38 Cp\n} , (A.1)\nwhere the last subset is created to ensure that the number of sparse supports in the full set equals k. Notice, (A.1) is also the largest set of disjoint subsets that can be created using intersections, unions, and complements on the regions of measurements.\nWe will continue our discussion assuming that the measurements are made on the subsets contained in (A.1). When the observations are noiseless, (A.1) is a superior design than the original design, whose outcomes can be inferred as\nx>[ai,bi)\u03b2 = p\u22121\u2211\nj=1\ncj+1 \u2212 cj bi \u2212 ai x>[cj ,cj+1)\u03b2. (A.2)\nAt this point, it is easy to see that the minimum sample size to guarantee that the signals can be fully identifiable in the worst case is T \u2265 n2 ; the necessary (and sufficient) condition is to have |Cj | = 1,\u2200j = 1, . . . , p, which requires 2T \u2265 p \u2265 n. For > 0, we compute the expected Delta-risk given any fixed design which yields p elementary subsets as shown in (A.1). Let nj = |Cj |, j = 1, . . . , p. If the model \u03b2 distributes kj supports in subset Cj , respectively, then on any region where Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nnj > kj > 0, the inference algorithm can only make a random guess, e.g., for the first kj elements. Let \u03b2Cj be the signal vector on subset Cj , the conditional expected error on this subset is:\nE [ |\u03b2Cj\u2206\u03b2\u0302Cj | | kj ] = kj\u2211\nej=1\n( nj\u2212kj ej )( kj kj\u2212ej )\n( nj kj\n) ej = kj\u2211\nej=1\n( nj\u2212kj\u22121 ej\u22121 )( kj kj\u2212ej )\n( nj kj ) (nj \u2212 kj)\n=\n( nj\u22121 kj\u22121 ) ( nj kj ) (nj \u2212 kj) = kj(nj \u2212 kj) nj . (A.3)\nThe total risk conditioned on all of kj : j = 1, . . . , p is:\nE [ |\u03b2\u2206\u03b2\u0302| | k1, . . . , kp ] = p\u2211\nj=1\nE [ |\u03b2Cj\u2206\u03b2\u0302Cj | | kj ] = p\u2211\nj=1\nkj(nj \u2212 kj) nj . (A.4)\nUsing the law of total expectation assuming \u03b2 to be uniformly distributed, we can compute the expected error of the given passive design as\nE|\u03b2\u2206\u03b2\u0302| = \u2211\nk1+\u00b7\u00b7\u00b7+kp=K\n(\u220fp j=1 ( nj kj ) ( n K ) )( p\u2211\nj=1\nkj(nj \u2212 kj) nj\n)\n=\np\u2211\nj=1\n\u2211\nk1+\u00b7\u00b7\u00b7+kp=K\n\u220fp j\u2032=1 (nj\u2032 kj\u2032 ) ( n K ) kj(nj \u2212 kj) nj\n=\np\u2211\nj=1\n\u2211\nk1+\u00b7\u00b7\u00b7+kp=K\n(nj \u2212 1) ( nj\u22122 kj\u22121 )\u220f j\u2032 6=j (nj\u2032 kj\u2032 ) ( n K )\n=\np\u2211\nj=1\n(nj \u2212 1) ( n\u22122 K\u22121 ) ( n K ) = (n\u2212 p) ( n\u22122 K\u22121 ) ( n K ) = K(n\u2212K) n(n\u2212 1) (n\u2212 p) \u2264 K (n\u2212K)(n\u2212 2T ) n(n\u2212 1) (A.5)\nTo guarantee E|\u03b2\u2206\u03b2\u0302| \u2264 K , by solving (A.5) \u2264 K , a passive design requires a minimal sample size of\nT \u2265 p 2 \u2265 n 2\n( 1\u2212 n\u2212 1 n\u2212K ) . (A.6)\nCorollary A.2. Using noiseless region-sensing observations, a passive design in 1D with T \u2264 n2 region measurements achieves the optimal average-case Delta-risk, if and only if it can separate the search space into 2m disjoint subsets using intersections, unions, and complements of the measurement regions. The following example is adapted from Gray code:\nt x>\n1 0 0 0 0 1 1 1 1 2 0 0 1 1 1 1 0 0 3 0 1 1 0 0 0 0 0 4 0 0 0 0 0 1 1 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (the pattern cycles)\n(A.7)\nProof. To minimize (A.5), it is sufficient to find passive designs where p = 2T , given that the region aggregate measurements are noiseless. The expected risk of (A.5) turns out to be independent of the sizes of each elementary subset Cj (which one can verify with a minimal example where n = 4, K = 2, and p = 2), which suggests that all designs that yield p = 2T have the same average-case Delta-risk with noiseless region aggregate measurements.\nNotice that the Gray-code design may not be optimal when the measurements are noisy. For this reason, we also included point sensing in Table 1 in our main paper, which yields the same order of sample complexity and performs better when the measurement noise is large.\nB Theoretical Properties for Active Sensing The main goal of this section is to show that our main algorithm, Region Sensing Index (RSI), has the sample complexity guarantees show as Theorem 3 in the main paper. The main paper includes a proof sketch with three major steps. We show their details in 3 respective subsections."}, {"heading": "B.1 Basic Properties of Information Gain (IG)", "text": "Recall that the observation model is yt = x>t \u03b2 + t, where \u03b2 \u2208 S\u00b5 ( n k ) , \u03b2 \u223c \u03c0t, and t \u223c N (0, \u03c32t ). Omitting the time index t in this subsection, the information gain (IG) to be maximized in every step is defined as I(\u03b2; y | x, \u03c0) = H(y | x, \u03c0)\u2212 E[H(y | x,\u03b2) | \u03c0]\n\u21d4 I(\u03b3; y | \u03bb,p) = H(y | \u03bb,p)\u2212H( ), (B.1)\nwhere f(y | \u03bb,p) = K\u2211\nc=0\npc\u03c6(y \u2212 c\u03bb)\n\u03bb = \u00b5wx, \u03b3 = x>\u03b2 \u03bb , pc = Pr(\u03b3 = c) = \u2211\n\u03b2:x>\u03b2=c\u03bb\n\u03c0t(\u03b2). (B.2)\nThere are two basic properties: Lemma B.1 that is both directly applied in Section 3.1 Accelerations and indirectly used in the later proof sketch; and Proposition B.2 that appears as Proposition 4 in the main paper."}, {"heading": "Basic Property 1", "text": "Lemma B.1 (Concavity and monotonicity). I(\u03b3; y | \u03bb,p) is concave in p \u2208 RK+1+ , which includes the convex simplex of \u2206K = {p \u2208 [0, 1]K+1 : p>1 = 1}, if 0 < \u03bb < \u221e remains constant. On the other hand, I(\u03b3; y | \u03bb,p) with fixed p \u2208 \u2206K is monotone-increasing as \u03bb increases.\nProof. Concavity and monotonicity can be verified using derivatives. Notice the second term in (B.1) is constant. Here are the equations for the first term as well as its first and second order derivatives, omitting the dependency on p and \u03bb for simplicity:\nH(y;\u03bb,p) = \u2212 \u222b f(y) log f(y)dy, (B.3)\n\u2202H(y;\u03bb,p) = \u2212 \u222b ( 1 + log f(y) ) \u2202f(y) dy, (B.4)\n\u22022H(y;\u03bb,p) = \u2212 \u222b ( \u2202f(y)\u2202f(y)>\nf(y) + ( 1 + log f(y) ) \u22022f(y)\n) dy (B.5)\nPart 1. To show concavity in p(\u2265 0), let \u03c6\u03bb(y) = (\u03c6(y), \u03c6(y \u2212 \u03bb), . . . , \u03c6(y \u2212 K\u03bb))> and write out the gradient and the Hessian of H(y;\u03bb,p) with respect of p as:\n\u2202H(y;\u03bb,p)\n\u2202p> = \u2212\n\u222b \u221e\n\u2212\u221e\n( 1 + log f(y) ) \u03c6\u03bb(y) dy (B.6)\n\u22022H(y;\u03bb,p)\n\u2202p\u2202p> = \u2212\n\u222b \u221e\n\u2212\u221e\n1\nf(y) \u03c6\u03bb(y)\u03c6\u03bb(y)\n>dy (B.7)\nNotice \u03c6\u03bb(y)\u03c6\u03bb(y) > is a PSD Gram matrix, which is preserved under integration. Further, the integral returns a PD matrix if the distribution is not degenerate (\u03bb > 0 and pk > 0 for at least two distinct ks)\nPart 2.1 For monotonicity in \u03bb(> 0), in the case when K = 1, the derivative with respect to \u03bb is \u2202H(y)\n\u2202\u03bb = \u2212\n\u222b ( 1 + log f(y) ) \u00b7 p1\u03c6(y \u2212 \u03bb) \u00b7 (y \u2212 \u03bb) dy\n= \u2212p1 \u222b log f(y) \u00b7 \u03c6(y \u2212 \u03bb) \u00b7 (y \u2212 \u03bb) dy = \u2212p1 \u222b log f(y + \u03bb) \u00b7 \u03c6(y) \u00b7 (y) dy, (B.8)\nwhere the first line removes constant integrals and the second shifts the variable. In order to show that (B.8) is nonnegative, pair up y and \u2212y for y > 0 and notice that, by assuming \u03bb > 0,\n\u03c6(y + \u03bb) \u2264 \u03c6(\u2212y + \u03bb)\u21d2 f(y + \u03bb) \u2264 f(\u2212y + \u03bb). The bigger \u03bb, the larger derivative it has.\nPart 2.2 In general when K \u2265 1, we can write out the derivative as \u2202H(y;\u03bb,p)\n\u2202\u03bb = \u2212\n\u222b \u221e\n\u2212\u221e\n( 1 + log f(y) ) K\u2211\nk=0\npk\u03c6(y \u2212 k\u03bb)(y \u2212 k\u03bb)k dy\n= \u2212 K\u2211\nk=1\n\u222b \u221e\n\u2212\u221e (1 + log f(y))\nK\u2211\nt=k\npt\u03c6(y \u2212 t\u03bb)(y \u2212 t\u03bb) dy (B.9)\nDefine hk(y) = \u2211K t=k pt\u03c6(y \u2212 t\u03bb); we have\n0 = \u2212hk(y) log hk(y) \u2223\u2223\u2223 \u221e\n\u2212\u221e =\n\u222b \u221e\n\u2212\u221e (1 + log hk(y))\nK\u2211\nt=k\npt\u03c6(y \u2212 t\u03bb)(y \u2212 t\u03bb) dy (B.10)\nConsider each term of k in (B.9) and add the corresponding terms from (B.10); using `k = \u2211k\u22121 s=0 ps\u03c6(y \u2212 s\u03bb), we get\n\u2202H(y;\u03bb,p)\n\u2202\u03bb = \u2212\nK\u2211\nk=1\n\u222b \u221e\n\u2212\u221e log\n( 1 + `k(y)\nhk(y)\n) K\u2211\nt=k\n\u03c6(y \u2212 t\u03bb)(y \u2212 t\u03bb) dy. (B.11)\nThe only remaining task is to show that rk(y) = `k(y) hk(y) is monotone decreasing with respect to y, which is sufficient to guarantee that (B.11) \u2265 0, due to the odd symmetry of the remaining integrand parts around y = t\u03bb. Take the derivative of rk(y) with respect to y:\nr\u2032k(y) = `\u2032k(y)hk(y)\u2212 `k(y)h\u2032k(y)\nh2k(y) =\n\u2211 s<k\u2264t pspt ( \u03c6\u2032s(y)\u03c6t(y)\u2212 \u03c6s(y)\u03c6\u2032t(y) )\nh2k(y)\n= \u2211\ns<k\u2264t pspt\n\u03c62t (y) h2k(y)\n( \u03c6s(y)\n\u03c6t(y)\n)\u2032 = \u2211\ns<k\u2264t pspt\n\u03c62t (y) h2k(y)\n( \u03c6s(y)\n\u03c6t(y)\n) \u00b7 (s\u03bb\u2212 t\u03bb) \u2264 0, (B.12)\nwhere to simplify notations, we denote the composite function \u03c6s(y) = \u03c6(y\u2212 s\u03bb). The inequality is strict if \u03bb > 0 and pk 6= 0 for at least two k \u2208 {0, . . . ,K}."}, {"heading": "Basic Property 2", "text": "Proposition B.2 (Proposition 4 in the main document; a lower bound for the IG of a design). The IG score of a region sensing design has lower bounds with respect to its design parameters (\u03bb,p), as\nI(\u03b3; y | \u03bb,p) \u2265 2qcq\u0304c(2\u03a6(\u03bb2 )\u2212 1)2 \u2265 112 min{qc, q\u0304c}min{\u03bb2, 32}, \u22001 \u2264 c \u2264 K, (B.13) where qc = P (\u03b3 \u2265 c) = \u2211 \u03ba\u2265c p\u03ba, q\u0304c = 1\u2212 qc, and \u03a6(u) is the standard normal cdf.\nProof of Proposition B.2. To show (B.13), first inequality: Pick any 1 \u2264 c \u2264 K; let v = 1\u03b3\u2265c and v\u0302 = 1y>(c\u22121/2)\u03bb be two binary truncations of the original variables, \u03b3 and y, respectively. These truncations lose information:\nI(\u03b3; y | p, \u03bb) \u2265 I(v; v\u0302 | p, \u03bb) = EvK ( (v\u0302 | v) \u2016 v\u0302 )\n\u2265 2 \u2211\nv0\u2208{0,1} P (v = v0) sup v\u03020\n\u2223\u2223P (v\u0302 = v\u03020 | v = v0)\u2212 P (v\u0302 = v\u03020) \u2223\u2223 \ufe38 \ufe37\ufe37 \ufe38 ,\u03b4\u0302(v0,v\u03020) 2 , (B.14)\nwhere K(\u00b7 \u2016 \u00b7) is the Kullback\u2013Leibler divergence and the second line comes from Pinsker\u2019s inequality. Consider any realization of v = v0 and choose v\u03020 = v0; using the rule of total probability and direct calculation,\n\u03b4\u0302(v0, v0) = \u2223\u2223\u2223P (v\u0302 = v0 | v = v0)\u2212 P (v = v0)P (v\u0302 = v0 | v = v0)\u2212 P (v 6= v0)P (v\u0302 = v0 | v 6= v0) \u2223\u2223\u2223\n= P (v 6= v0) \u2223\u2223\u2223P (v\u0302 = v0 | v = v0)\u2212 P (v\u0302 = v0 | v 6= v0) \u2223\u2223\u2223 \u2265 P (v 6= v0) [ \u03a6 (\u03bb\n2\n) \u2212 ( 1\u2212 \u03a6 (\u03bb\n2\n))] = P (v 6= v0) ( 2\u03a6 (\u03bb\n2\n) \u2212 1 ) , (B.15)\nwhere \u03a6(\u03bb2 ) is a lower bound on the probability of correct estimation, based on the worst-case draw of \u03b3 such that y cannot be more than \u03bb2 away from \u03b3 in the direction that leads to estimation errors. Taking (B.15) to (B.14) yields the first part of the result.\nTo show (B.13), second inequality: So far we have shown an analytical lower bound for I(\u03b3; y | \u03bb,p). To make the result even more interpretable, we can further numerically evaluate the Gaussian tail distribution, to find two constants, C1 and C2, such that\n\u03a6(x)\u2212 1 2 =\n\u222b x\n0\n\u03c6(u) du =\n\u222b x\n0\n1\u221a 2\u03c0 e\u2212 u2 2 du \u2265 C1 min{x,C2}. (B.16)\nSince \u03a6(x) is monotone-increasing, we can fix C2 to find the worst difference quotient, \u03a6(x)/x, \u2200x \u2208 (0, C2]. In fact, we can directly assign C1 \u2264 \u03a6(C2)/C2, because \u03c6(u) is monotone-decreasing as u increases. We choose C2 = 32 and C1 = 1\u221a12 , which yields (\n2\u03a6 (\u03bb\n2\n) \u2212 1 )2 \u2265 1\n12 min\n{ \u03bb2, 32 } . (B.17)\nProposition B.3 (An upper bound for the IG of a design). When K = 1 and WLOG p1 \u2264 12 , the upper bound of IG derived from Jensen\u2019s inequality and max-entropy principle is I(\u03b3; y | \u03bb,p) \u2264 12p1\u03bb2, which is on the same order of (B.13) when \u03bb < O(1). In the \u03bb 1 case, the IG is naturally upper-bounded by a Bernoulli experiment with noiseless observation, H(B(p1)) = \u2212p1 log(p1)\u2212 (1\u2212 p1) log(1\u2212 p1) = O\u0303(p1). Therefore, Proposition B.2 is a good approximation to the true IG in all scenarios. (See Figure 1 for an empirical visualization.) The general upper bound is not tight for general k > 1.\nProof. The upper bound can be shown by Jensen\u2019s inequality and max-entropy principle. It is also tight when k = 1. Omitting p and \u03bb, I(\u03b3; y) = I(\u03b3; \u03b3 + ) = H(\u03b3 + ) +H(\u03b3 + | \u03b3) = H(\u03b3 + )\u2212H( ). (B.18) We only need to find the largest entropy forH(\u03b3+ ) given p and \u03bb. By Jensen\u2019s inequality, under the same mean and variance, a normal distribution has the largest entropy, where we have:\nE(\u03b3 + ) = E(\u03b3) + E( ) = p1\u03bb, \u03c32mar = Var(\u03b3 + ) = Var(\u03b3) + Var( ) + 2Cov(\u03b3, ) = p1\u03bb2 + 1. (B.19)\nWe can then use a normal distribution with the above mean and variance as a upper bound to:\nI(\u03b3; y) = H(\u03b3 + )\u2212H( ) \u2264 1 2 log(2\u03c0e\u03c32mar)\u2212 1 2 log(2\u03c0e) = 1 2 log(1 + p1\u03bb 2) \u2264 1 2 p1\u03bb 2 (B.20)"}, {"heading": "B.2 Minimum Information Gain of the Chosen Region in Each Iteration", "text": "This subsection aims to formalize the main observation in our main paper, which is that the information gain of all of the chosen measurements from RSI remain consistently large, before active search terminates with minimal Bayes risk. This observation implies a constant speed at which the model uncertainty can be reduced in expectation, leading to the upper bounds on sample complexity in Section B.3.\nRecall that the Bayes risk is defined by \u0304t = min|S\u0302|=k 1 kE[|S\u0302\u2206S| | \u03c0t], where \u2206 is the symmetric set difference operator. If we include the Bayes inference rule \u03c0t(\u03b2) \u221d \u03c00(\u03b2) \u220ft \u03c4=1 p(y | x\u03c4 ,\u03b2), we can see that \u0304t is essentially a function of the collected data Dt = {(x\u03c4 , y\u03c4 ) : 1 \u2264 \u03c4 \u2264 t}. The following lemma paraphrases Lemma 5 in the main document, with the time index t omitted. Lemma B.4 (Minimum IG of the chosen regions). WLOG, assume n is a multiple of 2k. At any step, given the data collection outcomes D and the current Bayes risk \u0304(D), we can always find a region A of size at most nk , such that \u03bb 2 \u2265 \u00b52a = k\u00b52\nn and \u0304(D)\n2 \u2264 E[\u03b3A | D] \u2264 1\u2212 \u0304(D) 2 (we call it Condition E), which further yields\nI(\u03b3; y | \u03bb,p) \u2265 I\u2217\u0304 = \u0304(D)25k min{k\u03bb2, 32} \u2265 \u0304(D) 25k min{ k2\u00b52 n , 3 2}. (B.21)\nCondition E Lemma B.4 states the result in two steps: (a) the fact that the posterior model after collecting data D still has large Bayes risk implies the existence of a very informative region that satisfies Condition E and (b) sensing on this region indeed yields nontrivial information, measured in terms of IG (B.21). We will split the proof into these two steps, accordingly. Lemma B.5 (A region that satisfies Condition E). In 1d search with unit `2-norm measurements, WLOG, assume n is a multiple of 2k. At any step, given the collected data D and the current Bayes risk \u0304(D):\n1. There always is a region B of size no larger than nk , such that \u03bb 2 B \u2265 \u00b5\n2 |B| = k\u00b52 n and E[\u03b3B | D] \u2265 \u0304(D) 2\n2. There always is a subregion A \u2282 B that satisfies Condition E:\n\u03bb2A \u2265 k\u00b52\nn and\n\u0304(D)\n2 \u2264 E[\u03b3A | D] \u2264 1\u2212\n\u0304(D)\n2 (B.22)\nProof. Part 1. Suppose the current minimizer of the posterior Bayes risk is S\u0302 = S\u0302(D) = arg maxS\u2032 \u2211 \u0302\u2208S\u2032 E[\u03b2\u0302 | D]. Evenly split the domain into K disjoint and contiguous regions and take their largest disjoint and contiguous subsets that do not intersect with S\u0302. There are at most G \u2264 2K such sets; let them be B1, . . . , BG. We use \u03b3(Bg) = \u2211 j\u2208Bg 1 > j \u03b2 to denote the corresponding region latent variables in a region Bg . The region B = arg maxBg\u2032 E[\u03b3(Bg\u2032) | D] yields\nE[\u03b3(B) | D] \u2265 \u2211G g\u2032=1 E[\u03b3(Bg\u2032) | D]\n2K = K \u2212 E[\u03b3(S\u0302) | D] 2K = \u0304(D) 2 , (B.23)\ndue to the additivity, \u2211 g \u03b3(Bg) + \u03b3(S\u0302) = \u2211 j\u2208[n] 1 > j \u03b2 = K.\nPart 2. Let A \u2282 B be the smallest contiguous subset such that E[\u03b3(A) | D] \u2265 \u0304(D)2 . Notice the maximum certainty of any point in j \u2208 A \u2286 X \\ S\u0302 is\nE[\u03b2j | D] \u2264 min \u0302\u2208S\u0302 (1\u2212 E[\u03b2\u0302 | D]) \u2264 1\u2212 \u0304(D). (B.24)\nWe then use the additivity of expectation to obtain\nE[\u03b3(A) | D] \u2264 E[\u03b3(A \\ {j}) | D] + E[\u03b2j | D] < \u0304(D) 2 + (1\u2212 \u0304(D)) = 1\u2212 \u0304(D) 2 , \u2200j \u2208 A, i.e., j 6\u2208 S\u0302. (B.25)\nIG of the Chosen Region The following obtains Lemma B.6 with additive terms of K2. It provides advantages over the straight-forward calculation in the main paper (which yields results with multiplicative factors of K). Lemma B.6 (Maximum IG when the outcome expectation is bounded). For any design on K-sparse models, if there exists 0 < \u0304 < 1 and a design (x, A, \u03bb, \u03b3) such that \u03042 \u2264 E\u03b3 \u2264 1 \u2212 \u03042 , where \u03b3 = x>\u03b2 is latent variable of signal counts in the measurement region, then the information of the experiment is lower-bounded by\nI(\u03b3; y | p, \u03bb) \u2265 \u0304 25K min{K\u03bb2, 32} (B.26)\nProof. We use the fact that IG is concave in p and we only check the vertices of the simplex of feasible probabilities to find its lower bound: \n  pk \u2265 0, k = 1, . . . ,K, (Constraint H1, . . . ,HK);\u2211K k=1 pk \u2264 1, (Constraint H0);\u2211K k=1 kpk \u2265 \u03042 , (Constraint E1);\u2211K k=1 k(1\u2212 pk) \u2265 \u03042 , (Constraint E2),\n(B.27)\nwhere p0 = 1\u2212 \u2211K k=1 pk can be decided explicitly. All vertices of the simplex, including infeasible vertices, can be found by solving K linear systems constructed from the (K + 3) linear constraints. Since E1 and E2 cannot be satisfied simultaneously for any \u0304 < 1, we can enumerate all the remaining vertices and write out their respective nonzero values:\npk = 1, from \u2229k\u2032 6=k Hk\u2032 ; pk + p` = 1, kpk + `p` = \u0304 2 , from \u2229k\u2032 6=k,` Hk\u2032 \u2229 E1;\npk + p` = 1, kpk + `p` = 1\u2212 \u03042 , from \u2229k\u2032 6=k,` Hk\u2032 \u2229 E2. (B.28)\nThe first row is infeasible when \u0304 > 0. We then bound the IG for the other rows. Without loss of generality, assume ` < k. Then, all feasible cases require ` = 0 and yield min{pk, p`} \u2265 \u03042k . Using Proposition 4,\nI(v;u | p, \u03bb) \u2265 \u0304 25K min{K2\u03bb2, 32} \u2265 \u0304 25K min{K\u03bb2, 32}. (B.29)\nProof of Lemma B.4. The design from Lemma B.5 satisfies both Condition E and \u03bb \u2265 K\u00b52n , where we can then apply Lemma B.6 to obtain the conclusion."}, {"heading": "B.3 The Proof of Theorem 3", "text": "Lemma B.4 implies that the entropy in the posterior distribution, H(\u03b2 | \u03c0t) = \u2212 \u2211 \u03b2 \u03c0t(\u03b2) log \u03c0t(\u03b2), decreases at least by I \u2217 with every measurement in expectation, starting with H(\u03b2 | \u03c00) \u2264 k log n. Since the posterior entropy cannot be negative, RSI must terminates in finite times in expectation. Theorem B.7 (Theorem 3 in the main document; sample complexity of RSI). In active search of k sparse signals with strength \u00b5 in 1d physical space of size n(\u2265 2k), given any > 0 as tolerance of posterior Bayes risk, RSI using region sensing has bounded expected number of actual measurements before stopping,\nT\u0304 = E[min{T : \u0304(DT ) \u2264 }] \u2264 50 ( n \u00b52 + k2 9 ) log2 (2 ) log (n ) = O\u0303 ( n \u00b52 + k2 ) , (B.30)\nwhere the expectation is taken over the prior distribution and sensing outcomes."}, {"heading": "The Simple Approach", "text": "Definition B.8 (Stopping time). Define T = minT {\u0304(DT ) \u2264 } to be a random stopping time for an experiment to first yield less than posterior risk, \u0304(D\u03c4 ) = 1KE[S\u2206S\u0302 | D\u03c4 ] \u2264 . T = T (\u03c4) can be determined given \u03c4 . Lemma B.9 (Simple Expectations on the Number of Measurements for Small Errors). Given any 1 > 0, t0 \u2265 0, and the first t0 data collection outcomes Dt0 , the expected number of additional measurements before the RSI stops with posterior risk less than 1 is bounded in terms of H0 = H(\u03b2 | \u03c00) and I 1 defined in Lemma B.4, as\nE(T 1 \u2212 T 0 | Dt0) \u2264 H0 I 1 \u2264 25H0 max { n k\u00b52 , k 9 } . (B.31)\nRemark B.10. By taking t0 = 0 and H0 \u2264 k log n, Lemma B.9 implies\nT\u0304 \u2264 25 log(n)\n1 max { n \u00b52 , k2 9 } . (B.32)\nProof of Lemma B.9. Let t = t0 + s for any s \u2265 0 and Dt be the random variable for the data collection outcomes until step t. According to Lemma B.4,\n(T 1 | Dt) > t \u21d2 H(\u03b2 | Dt)\u2212 Ey [ H(\u03b2 | Dt \u222a {x, y}) | Dt,xt+1 ] \u2265 I 1 (B.33)\n\u21d2 H(\u03b2 | Dt) \u2265 I 1 + Ey [ H(\u03b2 | Dt \u222a {x, y}) | Dt,xt+1 ] (B.34)\nTaking expectation over {Dt : (T 1 | Dt) > t,Dt0} = {Dt : \u0304(Dt\u2032) > 1,\u2200t\u2032 \u2264 t,Dt0} (B.35)\nyields E [ H(\u03b2 | Dt) | T 1 > t,Dt0 ] \u2265 I 1 + E [ H(\u03b2 | Dt+1) | T 1 > t,Dt0 ] , (B.36)\nwhere the expectation is taken over (Dt | Dt0 , T 1 > t) and (Dt+1 | Dt0 , T 1 > t), respectively. Next, we hope to apply Lemma B.4 at step (t+ 1), but we have to make sure that the condition still holds, which is not directly implied by (B.36). To guarantee the conditions, we divide Dt+1 into two cases and use the nonnegativity of entropy to relax the second case,\nE [ Ht+1(\u03b2) | T 1 > t,Dt0 ] = P ( T 1 > t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(\u03b2) | T 1 > t+ 1, Dt0 ]\n+ P ( T 1 = t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(\u03b2) | T 1 = t+ 1, Dt0 ] \u2265 P ( T 1 > t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(\u03b2) | T 1 > t+ 1, Dt0 ] . (B.37)\nWe can then iterate beginning with t = t0 as\nE [ Ht0(\u03b2) | Dt0 ] \u2265 P ( T 1 > t0 | Dt0 ) E [ Ht0(\u03b2) | T 1 > t0, Dt0 ]\n\u2265 P ( T 1 > t0 | Dt0 ) ( I 1 + P ( T 1 > t0 + 1 | T 1 > t0, Dt0 ) E [ Ht0+1(\u03b2) | T 1 > t0 + 1, Dt0 ] )\n= P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) E [ Ht0+1(\u03b2) | T 1 > t0 + 1, Dt0 ]\n\u2265 P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) ( I 1+\n+ P ( T 1 > t0 + 2 | T 1 > t0 + 1, Dt0 ) E [ Ht0+2(\u03b2) | T 1 > t0 + 2, Dt0\n] )\n\u2265 P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) I 1\n+ P ( T 1 > t0 + 2 | Dt0 ) E [ Ht0+2(\u03b2) | T 1 > t0 + 2, Dt0 ]\n\u2265 . . .\n\u2265 I 1 \u221e\u2211\ns=0\nP ( T 1 > t0 + s | Dt0 ) = I 1E ( T 1 \u2212 T 0 | Dt0 ) , (B.38)\nwhich leads to the conclusion given E[Ht0(\u03b2) | Dt0 ] = H(\u03b2 | Dt0) = H0."}, {"heading": "The Complex Approach", "text": "Lemma B.11 (Max entropy given Bayes error). For a K-sparse model, \u03b2 \u2208 S ( n K ) , given \u0304 \u2265 1K \u2211 j\u2208S\u0302 P (\u03b2j = 0) = 1 KE|S\u2206S\u0302|, the posterior entropy is at most\nH(\u03b2) \u2264 KH(B(\u0304)) +K\u0304 log n, (B.39)\n\u2264 K 2r ( 2r log 2 + log n ) , if \u0304 \u2264 1 2r ,\u2200r = 0, 1, 2, . . . (B.40)\nwhere H(B(\u0304)) = \u2212\u0304 log \u0304\u2212 (1\u2212 \u0304) log(1\u2212 \u0304) is denoted as the entropy of a Bernoulli experiment with \u0304 success rate.\nProof. Part 1. Let S = {S1, . . . , SK} be the set of supports of the random variable \u03b2 that is modeled by the posterior distribution given the history data that leads to the current state. We can compute the expectation as\nK\u2211\nk=0\nkP (|S\u2206S\u0302| = k) = E|S\u0302\u2206S| \u2264 K\u0304. (B.41)\nDefine pk = pk(S\u0302) = P (|S\u2206S\u0302| = k); the total entropy can be bounded:\nH(\u03b2) = \u2212 K\u2211\nk=0\n\u2211\nS:|S\u2206S\u0302|=k\n\u03c0(\u03b2S) log \u03c0(\u03b2S) (B.42)\n\u2264 \u2212 K\u2211\nk=0\npk log\n( pk(\nK K\u2212k )( n\u2212K k\n) )\n(B.43)\n\u2264 \u2212 K\u2211\nk=0\npk log pk + K\u2211\nk=0\npk log\n( K\nk\n) + K\u2211\nk=0\nkpk log n (B.44)\n= \u2212 K\u2211\nk=0\npk log pk +\nK\u2211\nk=0\npk log\n( K\nk\n) +K\u0304 log n (B.45)\nwhere (B.42) separate the joint probabilities into (K+1) groups according to their values of |S\u2206S\u0302|. Inside every group, (B.43) realizes a uniform distribution, which maximizes the entropy given any value of group marginal probability, pk. We then relax the number of combination by log ( x K\u2212k ) \u2264 (K\u2212k) log x, which yields (B.44). From there, we use the condition, reformulated as (B.41), to obtain (B.45).\nThe next step uses the principle of maximum entropy to realize the optimizer for (B.45), when the moments are bounded by (B.41). The Lagrangian of the constrained optimization is\nL(p; c, \u03c1) = \u2212 K\u2211\nk=0\npk log pk +\nK\u2211\nk=0\npk log\n( K\nk\n) + c ( K\u2211\nk=0\npk \u2212 1 ) + \u03c1 ( K\u2211\nk=0\nkpk \u2212K\u0304 ) . (B.46)\nSetting the derivatives to zero yields\n0 = \u2202L\n\u2202pk = \u2212 log pk + log\n( K\nk\n) + 1 + c+ k\u03c1 \u21d2 pk \u221d ( K\nk\n) (e\u03c1)k, (B.47)\nwhich implies that pk is the probability of k outcomes in a binomial distribution with K rounds and an iid outcome probability of p = 11+e\u2212\u03c1 in each round. Since the expectation of the total outcome is K\u0304, we have p = \u0304. Given the max-entropy binomial distribution and let (X1, . . . , XK) to be the outcome of each round; the entropy of their sum is upper bounded by the sum of their marginal entropies, which is K times the entropy of H(\u0304). So, we proved (B.39).\nPart 2. To move forward to (B.40), we need an interim result when \u0304 \u2264 12 : H(\u0304) \u2264 \u22122\u0304 log \u0304 \u21d2 H(\u03b2) \u2264 \u22122K\u0304 log \u0304+K\u0304 logN, (B.48)\nTo show the interim result, let `(\u0304) = \u2212\u0304 log \u0304 + (1 \u2212 \u0304) log(1 \u2212 \u0304); its derivatives are `\u2032(\u0304) = \u2212 log \u0304 \u2212 log(1 \u2212 \u0304) \u2212 2 and `\u2032\u2032(\u0304) = \u2212 1\u0304 + 11\u2212\u0304 . The concavity of `(\u0304) in 0 \u2264 \u0304 \u2264 12 where `\u2032\u2032(\u0304) \u2264 0 and `(0) = `( 12 ) = 0 yield `(\u0304) \u2265 0, i.e., H(\u0304) \u2264 \u22122\u0304 log \u0304,\u22000 \u2264 \u0304 \u2264 12 . Finally, (B.40) trivially holds when r = 0. Otherwise, substitute \u0304 \u2264 2\u2212r with r \u2265 1 in (B.48) yields the final conclusion.\nProof of the final theorem. Let r = 2\u2212r and T r = minT {\u0304(DT ) \u2264 r}, for r = 0, 1, . . . , dlog2( 1 )e. From Lemma B.9, we have\nE(T r+1 \u2212 T r | Dt, T r \u2264 t) \u2264 H(\u03b2 | Dt) I\u2217 r+1 . (B.49)\nWe can use Lemma B.4 with r+1 = 2\u2212r\u22121 to show\nI\u2217 r+1 \u2265 r+1 25k\nmin {k2\u00b52 n , 9 } \u2265 1 50k2r min {k2\u00b52 n , 9 }\n(B.50)\nand Lemma B.11 with \u0304(Dt) \u2264 r = 2\u2212r to bound\nH(\u03b2 | Dt) \u2264 k\n2r (2r log 2 + log n). (B.51)\nPut both bounds to (B.49) to get\nE(T r+1 \u2212 T r | Dt, T r \u2264 t) \u2264 50 max { n \u00b52 , k2 9 } (2r log 2 + log n). (B.52)\nNotice the right side is independent of Dt and t, using linearity of expectations,\nE(T r+1 \u2212 T r ) \u2264 50 max { n \u00b52 , k2 9 } (2r log 2 + log n), (B.53)\nwhich further implies, using R = dlog2 1 e < 1 + log2 1 ,\nET \u2264 R\u22121\u2211\nr=0\nE ( T r+1 \u2212 T r )\n\u2264 50 max { n\n\u00b52 , k2 9\n}R\u22121\u2211\nr=0\n(2r log 2 + log n)\n\u2264 50 max { n\n\u00b52 , k2 9\n} R((R\u2212 1) log 2 + log n)\n\u2264 50 max { n\n\u00b52 , k2 9\n} log2 2 log n\n(B.54)\nC Real-world experiments\nFor the purpose of more accurate modeling of the actual measurement powers at each region level, we use statistics from the real data to model \u00b5w(a) and \u03c3(a) as functions of the region size a. The resulting SNR, \u03bb(a) = \u00b5w(a)\u03c3(a) is shown in Table 1. The optimal region size to begin under the uniform prior distribution is 32\u00d7 32. Figure 2 show examples of blue objects that are detected from the NAIP dataset.\nReferences\nArias-Castro, E.; Candes, E. J.; and Davenport, M. 2013. On the fundamental limits of adaptive sensing. Information Theory, IEEE Transactions on."}], "references": [{"title": "Example of positive discoveries in NAIP satellite", "author": ["E. Arias-Castro", "E.J. Candes", "M. Davenport"], "venue": null, "citeRegEx": "Arias.Castro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arias.Castro et al\\.", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires \u00d5(n/\u03bc2 +k) measurements to recover all of k signal locations with small Bayes error, where \u03bc and n are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.", "creator": "LaTeX with hyperref package"}}}