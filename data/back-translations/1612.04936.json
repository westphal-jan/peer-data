{"id": "1612.04936", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2016", "title": "Learning through Dialogue Interactions by Asking Questions", "abstract": "A good dialogue agent should be able to interact with users. In this work, we explore this direction by designing a simulator and a series of synthetic film tasks that allow the learner to interact with a teacher by asking and answering questions. We explore how a learner can benefit from asking questions in both the offline and online learning environments. We show that the learner improves when he asks questions. Our work is a first step in the development of consistently learned interactive dialogue agents.", "histories": [["v1", "Thu, 15 Dec 2016 05:46:27 GMT  (355kb,D)", "http://arxiv.org/abs/1612.04936v1", null], ["v2", "Thu, 29 Dec 2016 19:47:12 GMT  (356kb,D)", "http://arxiv.org/abs/1612.04936v2", null], ["v3", "Fri, 13 Jan 2017 21:07:04 GMT  (360kb,D)", "http://arxiv.org/abs/1612.04936v3", null], ["v4", "Mon, 13 Feb 2017 17:30:42 GMT  (360kb,D)", "http://arxiv.org/abs/1612.04936v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["jiwei li", "alexander h miller", "sumit chopra", "marc'aurelio ranzato", "jason weston"], "accepted": true, "id": "1612.04936"}, "pdf": {"name": "1612.04936.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra"], "emails": ["jiwel@fb.com", "ahm@fb.com", "spchopra@fb.com", "ranzato@fb.com", "jase@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "When a student is asked a question by a teacher, but is not confident about the answer, they may ask for clarification or hints. A good conversational agent (a learner/bot/student) should have this ability to interact with a dialogue partner (the teacher/user). However, recent efforts have mostly focused on learning through fixed answers provided in the training set, rather than through interactions. In that case, when a learner encounters a confusing situation such as an unknown surface form (phrase or structure), a semantically complicated sentence or an unknown word, the agent will either make a (usually poor) guess or will redirect the user to other resources (e.g., a search engine, as in Siri). Humans, in contrast, can adapt to many situations by asking questions.\nWe identify three categories of mistakes a learner can make during dialogue1: (1) the learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question; (2) the learner has a problem with reasoning, e.g. they fail to retrieve and connect the relevant knowledge to the question at hand; (3) the learner lacks the knowledge necessary to answer the question in the first place \u2013 that is, the knowledge sources the student has access to do not contain the needed information.\nAll the situations above can be potentially addressed through interaction with the dialogue partner. Such interactions can be used to learn to perform better in future dialogues. If a human student has problems understanding a teacher\u2019s question, they might ask the teacher to clarify the question. If the student doesn\u2019t know where to start, they might ask the teacher to point out which known facts are most relevant. If the student doesn\u2019t know the information needed at all, they might ask the teacher to tell them the knowledge they\u2019re missing, writing it down for future use.\nIn this work, we try to bridge the gap between how a human and an end-to-end machine learning dialogue agent deal with these situations: our student has to learn how to learn. We hence design a simulator and a set of synthetic tasks in the movie question answering domain that allow a bot to interact with a teacher to address the issues described above. Using this framework, we explore how a bot can benefit from interaction by asking questions in both offline supervised settings and online reinforcement learning settings, as well as how to choose when to ask questions in the latter setting. In both cases, we find that the learning system improves through interacting with users."}, {"heading": "2 RELATED WORK", "text": "Learning language through interaction and feedback can be traced back to the 1950s, when Wittgenstein argued that the meaning of words is best understood from their use within given language\n1This list is not exhaustive; for example, we do not address a failure in the dialogue generation stage.\nar X\niv :1\n61 2.\n04 93\n6v 1\n[ cs\n.C L\n] 1\n5 D\nec 2\n01 6\ngames (Wittgenstein, 2010). The direction of interactive language learning through language games has been explored in the early seminal work of Winograd (Winograd, 1972), and in the recent SHRDLURN system (Wang et al., 2016). In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).\nIn the context of dialogue, with the recent popularity of deep learning models, many neural dialogue systems have been proposed. These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance. It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant. Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al., 2015). As far as we know, current dialogue systems mostly focus on learning through fixed supervised signals rather than interacting with users.\nOur work is closely related to the recent work of Weston (2016), which explores the problem of learning through conducting conversations, where supervision is given naturally in the response during the conversation. Their work introduced multiple learning schemes from dialogue utterances. In particular the authors discussed Imitation Learning, where the agent tries to learn by imitating the dialogue interactions between a teacher and an expert student; Reward-Based Imitation Learning, which only learns by imitating the dialogue interactions which have have correct answers; and Forward Prediction, which learns by predicting the teacher\u2019s feedback to the student\u2019s response. Despite the fact that Forward Prediction does not uses human-labeled rewards, the authors show that it yields promising results. However, their work did not fully explore the ability of an agent to learn via questioning and interaction. Our work can be viewed as a natural extension of theirs."}, {"heading": "3 THE TASKS", "text": "In this section we describe the dialogue tasks we designed2. They are tailored for the three different situations described in Section 1 that motivate the bot to ask questions: (1) Question Clarification, in which the bot has problems understanding its dialogue partner\u2019s text; (2) Knowledge Operation, in which the bot needs to ask for help to perform reasoning steps over an existing knowledge base; and (3) Knowledge Acquisition, in which the bot\u2019s knowledge is incomplete and needs to be filled.\nFor our experiments we adapt the WikiMovies dataset (Weston et al., 2015), which consists of roughly 100k questions over 75k entities based on questions with answers in the open movie dataset (OMDb). The training/dev/test sets respectively contain 181638 / 9702 / 9698 examples. The accuracy metric corresponds to the percentage of times the student gives correct answers to the teacher\u2019s questions.\nEach dialogue takes place between a teacher and a bot, generated using a simulator. The bot is first presented with facts from the OMDb KB. This allows us to control the exact knowledge the bot is given access to. During the dialogue we include several teacher-bot question-answer pairs which can be viewed as conversational histories3 unreleated to the question the bot needs to answer. In order to explore the benefits of asking clarification questions during a conversation, for each of the three scenarios, our simulator generated data for two different settings, namely, Question-Answering (denoted by QA), and Asking-Question (denoted by AQ). For both QA and AQ, the bot needs to give an answer to the teacher\u2019s original question at the end. The details of the simulator can be found in the appendix."}, {"heading": "3.1 QUESTION CLARIFICATION.", "text": "In this setting, the bot does not understand the teacher\u2019s question. We focus on a special situation where the bot does not understand the teacher because of typo/spelling mistakes, as shown in Figure\n2 Code and data are available at https://github.com/facebook/MemNN/tree/master/AskingQuestions. 3These history QA pairs can be viewed as distractions and are used to test the bot\u2019s ability to separate the\nwheat from the chaff. For each dialogue, we incorporate 5 extra QA pairs (10 sentences).\n1. We intentionally misspell some words in the questions such as replacing the word \u201cmovie\u201d with \u201cmovvie\u201d or \u201cstar\u201d with \u201csttar\u201d.4 To make sure that the bot will have problems understanding the question, we guarantee that the bot has never encountered the misspellings before\u2014the misspellingintroducing mechanisms in the training, dev and test sets are different, so the same word will be misspelled in different ways in different sets. We present two AQ tasks: (i) Question Paraphrase where the student asks the teacher to use a paraphrase that does not contain spelling mistakes to clarify the question by asking \u201cwhat do you mean?\u201d; and (ii) Question Verification where the student asks the teacher whether the original typo-bearing question corresponds to another question without the spelling mistakes (e.g., \u201cDo you mean which film did Tom Hanks appear in?\u201d). The teacher will give feedback by giving a paraphrase of the original question without spelling mistakes (e.g., \u201cI mean which film did Tom Hanks appear in\u201d) in Question Paraphrase or positive/negative feedback in Question Verification. Next the student will give an answer and the teacher will give positive/negative feedback depending on whether the student\u2019s answer is correct. Positive and negative feedback are variants of \u201cNo, that\u2019s incorrect\u201d or \u201cYes, that\u2019s right\u201d5. In these tasks, the bot has access to all relevant entries in the KB."}, {"heading": "3.2 KNOWLEDGE OPERATION", "text": "The bot has access to all the relevant knowledge (facts) but lacks the ability to perform necessary reasoning operations over them; see Figure 2. We focus on a special case where the bot will try to understand what are the relevant facts. We explore two settings: Ask For Relevant Knowledge (Task 3) where the bot directly asks the teacher to point out the relevant KB fact and Knowledge Verification (Task 4) where the bot asks whether the teacher\u2019s question is relevant to one particular KB fact. The teacher will point out the relevant KB fact in the Ask For Relevant Knowledge setting or give a positive or negative response in the Knowledge Verification setting. Then the bot will give an answer to the teacher\u2019s original question and the teacher will give feedback on the answer."}, {"heading": "3.3 KNOWLEDGE ACQUISITION", "text": "For the tasks in this subsection, the bot has an incomplete KB and there are entities important to the dialogue missing from it, see Figure 3. For example, given the question \u201cWhich movie did Tom Hanks star in?\u201d, the missing part could either be the entity that the teacher is asking about (question entity for short, which is Tom Hanks in this example), the relation entity (starred actors), the answer to the question (Forrest Gump), or the combination of the three. In all cases, the bot has little chance of giving the correct answer due to the missing knowledge. It needs to ask the teacher the answer to acquire the missing knowledge. The teacher will give the answer and then move on to other questions (captured in the conversational history). They later will come back to reask the question. At this point, the bot needs to give an answer since the entity is not new any more.\nThough the correct answer has effectively been included in the earlier part of the dialogue as the answer to the bot\u2019s question, as we will show later, many of the tasks are not as trivial as they look when the teacher reasks the question. This is because the bot\u2019s model needs to memorize the missing entity and then construct the links between the missing entities and known ones. This is akin to the real world case where a student might make the same mistake again and again even though each time the teacher corrects them if their answer is wrong. We now detail each task in turn.\nMissing Question Entity: The entity that the teacher is asking about is missing from the knowledge base. All KB facts containing the question entity will be hidden from the bot. In the example for Task 5 in Figure 3, since the teacher\u2019s question contains the entity Tom Hanks, the KB facts that contain Tom Hanks are hidden from the bot.\nMissing Answer Entity: The answer entity to the question is unknown to the bot. All KB facts that contain the answer entity will be hidden. Hence, in Task 6 of Figure 3, all KB facts containing the answer entity Forrest Gump will be hidden from the bot.\n4Many reasons could lead to the bot not understanding the teacher\u2019s question, e.g., the teacher\u2019s question has an unknown phrase structure, rather than unknown words. We choose to use spelling mistakes because of the ease of dataset construction.\n5In the datasets we build, there are 6 templates for positive feedback and 6 templates for negative feedback.\nMissing Relation Entity: The relation type is unknown to the bot. In Task 7 of Figure 3, all KB facts that express the relation starred actors are hidden from the bot.\nMissing Triples: The triple that expresses the relation between the question entity and the answer entity is hidden from the bot. In Task 8 of Figure 3, the triple \u201cForrest Gump (question entity) starred actors Tom Hanks (answer entity)\u201d will be hidden.\nMissing Everything: The question entity, the relation entity, the answer entity are all missing from the KB. All KB facts in Task 9 of Figure 3 will be removed since they either contain the relation entity (i.e., starred actors), the question entity (i.e., Forrest Gump) or the answer entity Tom Hanks."}, {"heading": "4 TRAIN/TEST REGIME", "text": "We now discuss in detail the regime we used to train and test our models. Assuming access to the simulator, our objective was twofold. First, we wanted to test the usefulness of asking questions. Second, we wanted to train our student bot to learn when to ask questions and what questions to ask. In order to accomplish these two objectives we explored training our models using two methodologies, namely, Offline Supervised Learning and Online Reinforcement Learning."}, {"heading": "4.1 OFFLINE SUPERVISED LEARNING", "text": "The motivation behind training our student models in an offline supervised setting was primarily to test the usefulness of the ability to ask questions. The dialogues are generated as described in\nthe previous section, and the bot\u2019s role is generated with a fixed policy. We chose a policy where answers to the teacher\u2019s questions are correct answers 50% of the time, and incorrect otherwise, to add a degree of realism. Similarly, in tasks where questions can be irrelevant they are only asked correctly 50% of the time.6\nThe offline setting explores different combinations of training and testing scenarios, which mimic different situations in the real world. The aim is to understand when and how observing interactions between two agents can help the bot improve its performance for different tasks. As a result we construct training and test sets in three ways across all tasks, resulting in 9 different scenarios per task, each of which correspond to a real world scenario.\nThe three training sets we generated are referred to as TrainQA, TrainAQ, and TrainMix. TrainQA follows the QA setting discussed in the previous section: the bot never asks questions and only tries to immediately answer. TrainAQ follows the AQ setting: the student, before answering, first always asks a question in response to the teacher\u2019s original question. TrainMix is a combination of the two where 50% of time the student asks a question and 50% does not.\nThe three test sets we generated are referred to as TestQA, TestAQ, and TestModelAQ. TestQA and TestAQ are generated similarly to TrainQA and TrainAQ, but using a perfect fixed policy (rather than 50% correct) for evaluation purposes. The setting of the TestModelAQ is different from the rest. Here while deciding on the question to be asked by the student, instead of using a fixed perfect policy which always results in a relevant question, we use a learned model to generate the question. The learned model is trained using either the TrainAQ or TrainMix set, depending on the training scenario. The teacher will reply to the question, giving positive feedback if the student\u2019s question is correct and negative feedback otherwise. The student will then give the final answer. The difference between TestModelAQ and TestAQ only exists in the Question Verification and Knowledge Verification tasks; in other tasks there is only one way to ask the question and TestModelAQ and TestAQ are identical.\nTo summarize, for every task listed in Section 3 we train one model for each of the three training sets (TrainQA, TrainAQ, TrainMix) and test each of these models on the three test sets (TestQA, TestAQ, and TestModelAQ), resulting in 9 combinations. For the purpose of notation the train/test combination is denoted by \u201cTrainSetting+TestSetting\u201d. For example, TrainAQ+TestQA denotes a model which is trained using the TrainAQ dataset and tested on TestQA dataset. Each combination has a real world interpretation. For instance, TrainAQ+TestQA would refer to a scenario where a student can ask the teacher questions during learning but cannot to do so while taking an exam. Similarly, TrainQA+TestQA describes a stoic teacher that never answers a student\u2019s question at either learning or examination time. The setting TrainQA+TestAQ corresponds to the case where a lazy student never asks question at learning time but gets anxious during the examination and always asks a question."}, {"heading": "4.2 ONLINE REINFORCEMENT LEARNING (RL)", "text": "We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask. In other words, the student learns how to learn.\nAlthough it is in the interest of the student to ask questions at every step of the conversation, since the response to its question will contain extra information, we don\u2019t want our model to learn this behavior. Each time a human student asks a question, there\u2019s a cost associated with that action. This cost is a reflection of the patience of the teacher, or more generally of the users interacting with the bot in the wild: users won\u2019t find the bot engaging if it always asks clarification questions. The student should thus be judicious about asking questions and learn when and what to ask. For instance, if the student is confident about the answer, there is no need for it to ask. Or, if the teacher\u2019s question is so hard that clarification is unlikely to help enough to get the answer right, then it should also refrain from asking.\nWe now discuss how we model this problem under the Reinforcement Learning framework. The bot is presented with KB facts (some facts might be missing depending on the task) and a question. It needs to decide whether to ask a question or not at this point. The decision whether to ask is\n6This only makes sense in tasks like Question or Knowledge Verification. In tasks where the question is static such as \u2018What do you mean?\u201d there is no way to ask an irrelevant question, and we do not use this policy.\nmade by a binary policy PRLQuestion. If the student chooses to ask a question, it will be penalized by costAQ. We explored different values of costAQ ranging from [0, 2], which we consider as modeling the patience of the teacher. The goal of this setting is to find the best policy for asking/notasking questions which would lead to the highest cumulative reward. The teacher will appropriately reply if the student asks a question. The student will eventually give an answer to the teacher\u2019s initial question at the end using the policy PRLAnswer, regardless of whether it had asked a question. The student will get a reward of +1 if its final answer is correct and \u22121 otherwise. Note that the student can ask at most one question and that the type of question is always specified by the task under consideration. The final reward the student gets is the cumulative reward over the current dialogue episode. In particular the reward structure we propose is the following:\nFor each of the tasks described in Section 3, we consider three different RL scenarios. Good-Student: The student will be presented with all relevant KB facts. There are no misspellings or unknown words in the teacher\u2019s question. This represents a knowledgable student in the real world that knows as much as it needs to know (e.g., a large knowledge base, large vocabulary). This setting is identical across all missing entity tasks (5 - 9). Poor-Student: The KB facts or the questions presented to the student are flawed depending on each task. For example, for the Question Clarification tasks, the student does not understand the question due to spelling mistakes. For the Missing Question Entity task the entity that the teacher asks about is unknown by the student and all facts containing the entity will be hidden from the student. This setting is similar to a student that is underprepared for the tasks. Medium-Student: The combination of the previous two settings where for 50% of the questions, the student has access to the full KB and there are no new words or phrases or entities in the question, and 50% of the time the question and KB are taken from the Poor-Student setting."}, {"heading": "5 MODELS", "text": "For both offline supervised and online RL settings, we use the End-to-End Memory Network model (MemN2N) (Sukhbaatar et al., 2015) as a backbone. The model takes as input the last utterance of the dialogue history (the question from the teacher) as well as a set of memory contexts including short-term memories (the dialogue history between the bot and the teacher) and long-term memories (the knowledge base facts that the bot has access to), and outputs a label. We refer readers to the Appendix for more details about MemN2N.\nOffline Supervised Settings: The first learning strategy we adopt is the reward-based imitation strategy (denoted vanilla-MemN2N) described in (Weston, 2016), where at training time, the model maximizes the log likelihood probability of the correct answers the student gave (examples with incorrect final answers are discarded). Candidate answers are words that appear in the memories, which means the bot can only predict the entities that it has seen or known before.\nWe also use a variation of MemN2N called \u201ccontext MemN2N\u201d (Cont-MemN2N for short) where we replace each word\u2019s embedding with the average of its embedding (random for unseen words) and the embeddings of the other words that appear around it. We use both the preceeding and following words as context and the number of context words is a hyperparameter selected on the dev set.\nAn issue with both vanilla-MemN2N and Cont-MemN2N is that the model only makes use of the bot\u2019s answers as signals and ignores the teacher\u2019s feedback. We thus propose to use a model that jointly predicts the bot\u2019s answers and the teacher\u2019s feedback (denoted as TrainQA (+FP)). The bot\u2019s answers are predicted using a vanilla-MemN2N and the teacher\u2019s feedback is predicted using the Forward Prediction (FP) model as described in (Weston, 2016). We refer the readers to the Appendix for the FP model details. At training time, the models learn to jointly predict the teacher\u2019s feedback and the answers with positive reward. At test time, the model will only predict the bot\u2019s answer.\nFor the TestModelAQ setting described in Section 4, the model needs to decide the question to ask. Again, we use vanilla-MemN2N that takes as input the question and contexts, and outputs the question the bot will ask.\nOnline RL Settings: A binary vanilla-MemN2N (denoted as PRL(Question)) is used to decide whether the bot should or should not ask a question, with the teacher replying if the bot does ask something. A second MemN2N is then used to decide the bot\u2019s answer, denoted as PRL(Answer). PRL(Answer) for QA and AQ are two separate models, which means the bot will use different models for final-answer prediction depending on whether it chooses to ask a question or not.7\nWe use the REINFORCE algorithm (Williams, 1992) to update PRL(Question) and PRL(Answer). For each dialogue, the bot takes two sequential actions (a1, a2): to ask or not to ask a question (denoted as a1); and guessing the final answer (denoted as a2). Let r(a1, a2) denote the cumulative reward for the dialogue episode, computed using Table 1. The gradient to update the policy is given by:\np(a1, a2) = PRL(Question)(a1) \u00b7 PRL(answer)(a2) \u2207J(\u03b8) \u2248 \u2207 log p(a1, a2)[r(a1, a2)\u2212 b]\n(1)\nwhere b is the baseline value, which is estimated using another MemN2N model that takes as input the query x and memory C, and outputs a scalar b denoting the estimation of the future reward. The baseline model is trained by minimizing the mean squared loss between the estimated reward b and actual cumulative reward r, ||r \u2212 b||2. We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details. The baseline estimator model is independent from the policy models and the error is not backpropagated back to them.\nIn practice, we find the following training strategy yields better results: first train only PRL(answer), updating gradients only for the policy that predicts the final answer. After the bot\u2019s final-answer policy is sufficiently learned, train both policies in parallel8. This has a real-world analogy where the bot first learns the basics of the task, and then learns to improve its performance via a question-asking policy tailored to the user\u2019s patience (represented by costAQ) and its own ability to asnwer questions."}, {"heading": "6 EXPERIMENTS", "text": "Offline Results: Offline results are presented in Tables 2, 3 and 4. Table 2 presents results for the vanilla-MemN2N and Forward Prediction models. Table 3 presents results for Cont-MemN2N, which is better at handling unknown words. We repeat each experiment for 10 times and report the best result. Finally, Table 4 presents results for the test scenario where the bot itself chooses when to ask questions. Observations can be summarized as as follows:\n- Asking questions helps at test time, which is intuitive since it provides additional evidence: 7An alternative is to train one single model for final answer prediction in both AQ and QA cases, similar to the TrainMix setting in the supervised learning setting. But we find training AQ and QA separately for the final answer prediction yields a little better result than the single model setting.\n8 We implement this by running 16 epochs in total, updating only the model\u2019s policy for final answers in the first 8 epochs while updating both policies during the second 8 epochs. We pick the model that achieves the best reward on the dev set during the final 8 epochs. Due to relatively large variance for RL models, we repeat each task 5 times and keep the best model on each task.\nQuestion Clarification Knowledge Acquisition Task 2: Q. Verification Task 4: K. Verification\nTestModelAQ TestModelAQ TrainAQ 0.382 0.480\nTrainAQ(+FP) 0.344 0.501 TrainMix 0.352 0.469\n\u2022 The performance of TestModelAQ, where the bot relies on its model to ask questions at test time (and thus can ask irrelevant questions) is worse than asking the correct question at test time (TestAQ) but better than not asking questions (TestQA).\n- Cont-MemN2N significantly outperforms vanilla-MemN2N. One explanation is that considering context provides significant evidence distinguishing correct answers from candidates in the dialogue history, especially in cases where the model encounters unfamiliar words.\nRL Results For the RL settings, we present results for Task 2 (Question Verification) and Task 6 (Missing Answer Entities) in Figure 5. Task 2 represents scenarios where different types of student have different abilities to correctly answer questions (e.g., a poor student can still sometimes give correct answers even when they do not fully understand the question). Task 6 represents tasks where a poor learner who lacks the knowledge necessary to answer the question can hardly give a correct answer. All types of students including the good student will theoretically benefit from asking questions (asking for the correct answer) in Task 6. We show the percentage of question-asking versus the cost of AQ on the test set and the accuracy of question-answering on the test set vs the cost of AQ. Our main findings were:\n\u2022 A good student does not need to ask questions in Task 2 (Question Verification), because they already understand the question. The student will raise questions asking for the correct answer when cost is low for Task 6 (Missing Answer Entities).\n\u2022 A poor student always asks questions when the cost is low. As the cost increases, the frequency of question-asking declines.\n\u2022 As the AQ cost increases gradually, good students will stop asking questions earlier than the medium and poor students. The explanation is intuitive: poor students benefit more from asking questions than good students, so they continue asking even with higher penalties.\n\u2022 As the probability of question-asking declines, the accuracy for poor and medium students drops. Good students are more resilient to not asking questions."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we explored how an intelligent agent can benefit from interacting with users by asking questions. We developed tasks where interaction via asking questions is desired. We explore both online and offline settings that mimic different real world situations and show that in most cases, teaching a bot to interact with humans facilitates language understanding, and consequently leads to better question answering ability."}], "references": [{"title": "Interactional feedback and the impact of attitude and motivation on noticing l2 form", "author": ["Mohammad Amin Bassiri"], "venue": "English Language and Literature Studies,", "citeRegEx": "Bassiri.,? \\Q2011\\E", "shortCiteRegEx": "Bassiri.", "year": 2011}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "Bordes and Weston.,? \\Q2016\\E", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.06931,", "citeRegEx": "Dodge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dodge et al\\.", "year": 2015}, {"title": "The conscientious consumer: Reconsidering the role of assessment feedback in student learning", "author": ["Richard Higgins", "Peter Hartley", "Alan Skelton"], "venue": "Studies in higher education,", "citeRegEx": "Higgins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2002}, {"title": "Learning through feedback", "author": ["Andrew S Latham"], "venue": "Educational Leadership,", "citeRegEx": "Latham.,? \\Q1997\\E", "shortCiteRegEx": "Latham.", "year": 1997}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1510.03055,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1511.06732,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "arXiv preprint arXiv:1606.02689,", "citeRegEx": "Su et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Learning language games through interaction", "author": ["Sida I Wang", "Percy Liang", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1606.02447,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Instructive feedback: Review of parameters and effects", "author": ["Margaret G Werts", "Mark Wolery", "Ariane Holcombe", "David L Gast"], "venue": "Journal of Behavioral Education,", "citeRegEx": "Werts et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Werts et al\\.", "year": 1995}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "arXiv preprint arXiv:1604.06045,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Understanding natural language", "author": ["Terry Winograd"], "venue": "Cognitive psychology,", "citeRegEx": "Winograd.,? \\Q1972\\E", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Philosophical investigations", "author": ["Ludwig Wittgenstein"], "venue": null, "citeRegEx": "Wittgenstein.,? \\Q2010\\E", "shortCiteRegEx": "Wittgenstein.", "year": 2010}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "games (Wittgenstein, 2010).", "startOffset": 6, "endOffset": 26}, {"referenceID": 19, "context": "The direction of interactive language learning through language games has been explored in the early seminal work of Winograd (Winograd, 1972), and in the recent SHRDLURN system (Wang et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 13, "context": "The direction of interactive language learning through language games has been explored in the early seminal work of Winograd (Winograd, 1972), and in the recent SHRDLURN system (Wang et al., 2016).", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al.", "startOffset": 164, "endOffset": 179}, {"referenceID": 4, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 5, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 15, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995).", "startOffset": 205, "endOffset": 261}, {"referenceID": 6, "context": "These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance.", "startOffset": 61, "endOffset": 120}, {"referenceID": 9, "context": "These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance.", "startOffset": 61, "endOffset": 120}, {"referenceID": 14, "context": "It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant.", "startOffset": 63, "endOffset": 121}, {"referenceID": 10, "context": "It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant.", "startOffset": 63, "endOffset": 121}, {"referenceID": 3, "context": "Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al.", "startOffset": 94, "endOffset": 128}, {"referenceID": 16, "context": "Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al.", "startOffset": 94, "endOffset": 128}, {"referenceID": 2, "context": ", 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al.", "startOffset": 66, "endOffset": 108}, {"referenceID": 7, "context": ", 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al.", "startOffset": 66, "endOffset": 108}, {"referenceID": 17, "context": ", 2016) or short stories (Weston et al., 2015).", "startOffset": 25, "endOffset": 46}, {"referenceID": 0, "context": "In a broader context, the usefulness of feedback and interactions has been validated in the setting of multiple language learning, such as second language learning (Bassiri, 2011) and learning by students (Higgins et al., 2002; Latham, 1997; Werts et al., 1995). In the context of dialogue, with the recent popularity of deep learning models, many neural dialogue systems have been proposed. These include the chit-chat type end-to-end dialogue systems (Vinyals & Le, 2015; Li et al., 2015; Sordoni et al., 2015), which directly generate a response given the previous history of user utterance. It also include a collection of goal-oriented dialogue systems (Wen et al., 2016; Su et al., 2016; Bordes & Weston, 2016), which complete a certain task such as booking a ticket or making a reservation at a restaurant. Another line of research focuses on supervised learning for question answering from dialogues (Dodge et al., 2015; Weston, 2016), using either a given database of knowledge (Bordes et al., 2015; Miller et al., 2016) or short stories (Weston et al., 2015). As far as we know, current dialogue systems mostly focus on learning through fixed supervised signals rather than interacting with users. Our work is closely related to the recent work of Weston (2016), which explores the problem of learning through conducting conversations, where supervision is given naturally in the response during the conversation.", "startOffset": 165, "endOffset": 1272}, {"referenceID": 17, "context": "For our experiments we adapt the WikiMovies dataset (Weston et al., 2015), which consists of roughly 100k questions over 75k entities based on questions with answers in the open movie dataset (OMDb).", "startOffset": 52, "endOffset": 73}, {"referenceID": 11, "context": "For both offline supervised and online RL settings, we use the End-to-End Memory Network model (MemN2N) (Sukhbaatar et al., 2015) as a backbone.", "startOffset": 104, "endOffset": 129}, {"referenceID": 16, "context": "Offline Supervised Settings: The first learning strategy we adopt is the reward-based imitation strategy (denoted vanilla-MemN2N) described in (Weston, 2016), where at training time, the model maximizes the log likelihood probability of the correct answers the student gave (examples with incorrect final answers are discarded).", "startOffset": 143, "endOffset": 157}, {"referenceID": 16, "context": "The bot\u2019s answers are predicted using a vanilla-MemN2N and the teacher\u2019s feedback is predicted using the Forward Prediction (FP) model as described in (Weston, 2016).", "startOffset": 151, "endOffset": 165}, {"referenceID": 18, "context": "7 We use the REINFORCE algorithm (Williams, 1992) to update PRL(Question) and PRL(Answer).", "startOffset": 33, "endOffset": 49}, {"referenceID": 8, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba & Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 73}], "year": 2017, "abstractText": "A good dialogue agent should have the ability to interact with users. In this work, we explore this direction by designing a simulator and a set of synthetic tasks in the movie domain that allow the learner to interact with a teacher by both asking and answering questions. We investigate how a learner can benefit from asking questions in both an offline and online reinforcement learning setting. We demonstrate that the learner improves when asking questions. Our work represents a first step in developing end-to-end learned interactive dialogue agents.", "creator": "LaTeX with hyperref package"}}}