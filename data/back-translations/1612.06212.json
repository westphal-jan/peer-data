{"id": "1612.06212", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2016", "title": "A recurrent neural network without chaos", "abstract": "We are introducing an exceptionally simple gated recurrent neural network (RNN) that achieves a performance comparable to well-known gated architectures such as LSTMs and GRUs in the task of speech modelling at the word level. We are demonstrating that our model has simple, predictable, and non-chaotic dynamics, in stark contrast to more standard gated architectures whose underlying dynamic systems exhibit chaotic behavior.", "histories": [["v1", "Mon, 19 Dec 2016 14:59:14 GMT  (684kb,D)", "http://arxiv.org/abs/1612.06212v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["thomas laurent", "james von brecht"], "accepted": true, "id": "1612.06212"}, "pdf": {"name": "1612.06212.pdf", "metadata": {"source": "CRF", "title": "A RECURRENT NEURAL NETWORK WITHOUT CHAOS", "authors": ["Thomas Laurent"], "emails": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Gated recurrent neural networks, such as the Long Short Term Memory network (LSTM) introduced by Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning tasks that involve sequential data. We propose an exceptionally simple variant of these gated architectures. The basic model takes the form\nht = \u03b8t tanh(ht\u22121) + \u03b7t tanh(Wxt), (1)\nwhere stands for the Hadamard product. The horizontal/forget gate (i.e. \u03b8t) and the vertical/input gate (i.e. \u03b7t) take the usual form used in most gated RNN architectures. Specifically\n\u03b8t := \u03c3 (U\u03b8ht\u22121 + V\u03b8xt + b\u03b8) and \u03b7t := \u03c3 (U\u03b7ht\u22121 + V\u03b7xt + b\u03b7) (2)\nwhere \u03c3(x) := (1 + e\u2212x)\u22121 denotes the logistic sigmoid function. The network (1)\u2013(2) has quite intuitive dynamics. Suppose the data xt present the model with a sequence\n(Wxt)(i) = { 10 if t = T 0 otherwise,\n(3)\nwhere (Wxt)(i) stands for the ith component of the vector Wxt. In other words we consider an input sequence xt for which the learned ith feature (Wxt)(i) remains off except at time T . When initialized from h0 = 0, the corresponding response of the network to this \u201cimpulse\u201d in the ith feature is\nht(i) \u2248  0 if t < T \u03b7T if t = T \u03b1t if t > T\n(4)\nwith \u03b1t a sequence that relaxes toward zero. The forget gate \u03b8t control the rate of this relaxation. Thus ht(i) activates when presented with a strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. Overall this leads to a dynamically simple model, in which the activation patterns in the hidden states of the network have a clear cause and predictable subsequent behavior.\nDynamics of this sort do not occur in other RNN models. Instead, the three most popular recurrent neural network architectures, namely the vanilla RNN, the LSTM and the GRU, have complex, irregular, and unpredictable dynamics. Even in the absence of input data, these networks can give rise to chaotic dynamical systems. In other words, when presented with null input data the activation patterns in their hidden states do not necessarily follow a predictable path. The proposed network (1)\u2013(2) has rather dull and minimalist dynamics in comparison; its only attractor is the zero state,\nar X\niv :1\n61 2.\n06 21\n2v 1\n[ cs\n.N E\n] 1\n9 D\nec 2\n01 6\nand so it stands at the polar-opposite end of the spectrum from chaotic systems. Perhaps surprisingly, at least in the light of this comparison, the proposed network (1) performs as well as LSTMs and GRUs on the word level language modeling task. We therefore conclude that the ability of an RNN to form chaotic temporal dynamics, in the sense we describe in Section 2, cannot explain its success on word-level language modeling tasks.\nIn the next section, we review the phenomenon of chaos in RNNs via both synthetic examples and trained models. We also prove a precise, quantified description of the dynamical picture (3)\u2013(4) for the proposed network. In particular, we show that the dynamical system induced by the proposed network is never chaotic, and for this reason we refer to it as a Chaos-Free Network (CFN). The final section provides a series of experiments that demonstrate that CFN achieve results comparable to LSTM on the word-level language modeling task. All together, these observations show that an architecture as simple as (1)\u2013(2) can achieve performance comparable to the more dynamically complex LSTM."}, {"heading": "2 CHAOS IN RECURRENT NEURAL NETWORKS", "text": "The study of RNNs from a dynamical systems point-of-view has brought fruitful insights into generic features of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013). We shall pursue a brief investigation of CFN, LSTM and GRU networks using this formalism, as it allows us to identify key distinctions between them. Recall that for a given mapping \u03a6 : Rd 7\u2192 Rd, a given initial time t0 \u2208 N and a given initial state u0 \u2208 Rd, a simple repeated iteration of the mapping \u03a6\nut+1 = \u03a6(ut) t > t0,\nut0 = u0 t = t0,\ndefines a discrete-time dynamical system. The index t \u2208 N represents the current time, while the point ut \u2208 Rd represents the current state of the system. The set of all visited states O+(u0) := {ut0 , ut0+1, . . . , ut0+n, . . .} defines the forward trajectory or forward orbit through u0. An attractor for the dynamical system is a set that is invariant (any trajectory that starts in the set remains in the set) and that attracts all trajectories that start sufficiently close to it. The attractors of chaotic dynamical systems are often fractal sets, and for this reason they are referred to as strange attractors.\nMost RNNs generically take the functional form\nut = \u03a8(ut\u22121,W1xt,W2xt, . . . ,Wkxt), (5)\nwhere xt denotes the tth input data point. For example, in the case of the CFN (1)\u2013(2), we have W1 = W , W2 = V\u03b8 and W3 = V\u03b7 . To gain insight into the underlying design of the architecture of an RNN, it proves usefull to consider how trajectories behave when they are not influenced by any external input. This lead us to consider the dynamical system\nut = \u03a6(ut\u22121) \u03a6(u) := \u03a8(u, 0, 0, . . . , 0), (6)\nwhich we refer to as the dynamical system induced by the recurrent neural network. The timeinvariant system (6) is much more tractable than (5), and it offers a mean to investigate the inner working of a given architecture; it separates the influence of input data xt, which can produce essentially any possible response, from the model itself. Studying trajectories that are not influenced by external data will give us an indication on the ability of a given RNN to generate complex and sophisticated trajectories by its own. As we shall see shortly, the dynamical system induced by a CFN has excessively simple and predictable trajectories: all of them converge to the zero state. In other words, its only attractor is the zero state. This is in sharp contrast with the dynamical systems induced by LSTM or GRU, who can exhibit chaotic behaviors and have strange attractors.\nThe learned parameters Wj in (5) describe how data influence the evolution of hidden states at each time step. From a modeling perspective, (6) would occur in the scenario where a trained RNN has learned a weak coupling between a specific data point xt0 and the hidden state at that time, in the sense that the data influence is small and so all Wjxt0 \u2248 0 nearly vanish. The hidden state then transitions according to ut0 \u2248 \u03a8(ut0\u22121, 0, 0, . . . , 0) = \u03a6(ut0\u22121). We refer to Bertschinger & Natschla\u0308ger (2004) for a study of the chaotic behavior of a simplified vanilla RNN with a specific statistical model, namely an i.i.d. Bernoulli process, for the input data as well as a specific statistical model, namely i.i.d. Gaussian, for the weights of the recurrence matrix."}, {"heading": "2.1 CHAOTIC BEHAVIOR OF LSTM AND GRU IN THE ABSENCE OF INPUT DATA", "text": "In this subsection we briefly show that LSTM and GRU, in the absence of input data, can lead to dynamical systems ut = \u03a6(ut\u22121) that are chaotic in the classical sense of the term (Strogatz, 2014). Figure 1 depicts the strange attractor of the dynamical system:\nut = [ ht ct ] u 7\u2192 \u03a6(u) = [ o tanh (f c+ i g) f c+ i g ] (7)\ni := \u03c3(Wih+ bi) f := \u03c3(Wfh+ bf ) o := \u03c3(Woh+ bo) g := tanh(Wgh+ bg) (8)\ninduced by a two-unit LSTM with weight matrices\nWi = [ \u22121 \u22124 \u22123 \u22122 ] Wo = [ 4 1 \u22129 \u22127 ] Wf = [ \u22122 6 0 \u22126 ] Wg = [ \u22121 \u22126 6 \u22129 ] (9)\nand zero bias for the model parameters. These weights were randomly generated from a normal distribution with standard deviation 5 and then rounded to the nearest integer. Figure 1(a) was obtained by choosing an initial state u0 = (h0, c0) uniformly at random in [0, 1]2 \u00d7 [0, 1]2 and plotting the h-component of the iterates ut = (ht, ct) for t between 103 and 105 (so this figure should be regarded as a two dimensional projection of a four dimensional attractor, which explain its tangled appearance). Most trajectories starting in [0, 1]2 \u00d7 [0, 1]2 converge toward the depicted attractor. The resemblance between this attractor and classical strange attractors such as the He\u0301non attractor is striking (see Figure 5 in the appendix for a depiction of the He\u0301non attractor). Successive zooms on the branch of the LSTM attractor from Figure 1(a) reveal its fractal nature. Figure 1(b) is an enlargement of the red box in Figure 1(a), and Figure 1(c) is an enlargement of the magenta box in Figure 1(b). We see that the structure repeats itself as we zoom in.\nThe most practical consequence of chaos is that the long-term behavior of their forward orbits can exhibit a high degree of sensitivity to the initial states u0. Figure 2 provides an example of such behavior for the dynamical system (7)\u2013(9). An initial condition u0 was drawn uniformly at random in [0, 1]2 \u00d7 [0, 1]2. We then computed 100, 000 small amplitude perturbations u\u03020 of u0 by adding a small random number drawn uniformly from [\u221210\u22127, 10\u22127] to each component. We then iterated (7)\u2013(9) for 200 steps and plotted the h-component of the final state u\u0302200 for each of the 100, 000 trials on Figure 2(a). The collection of these 100, 000 final states essentially fills out the entire attractor, despite the fact that their initial conditions are highly localized (i.e. at distance of no more than 10\u22127) around a fixed point. In other words, the time t = 200 map of the dynamical system will map a small neighborhood around a fixed initial condition u0 to the entire attractor. Figure 2(b) additionally illustrates this sensitivity to initial conditions for points on the attractor itself. We take an initial condition u0 on the attractor and perturb it by 10\u22127 to a nearby initial condition u\u03020. We then plot the distance \u2016u\u0302t \u2212 ut\u2016 between the two corresponding trajectories for the first 200 time steps. After an initial phase of agreement, the trajectories strongly diverge.\nThe synthetic example (7)\u2013(9) illustrates the potentially chaotic nature of the LSTM architecture. We now show that chaotic behavior occurs for trained models as well, and not just for synthetically generated instances. We take the parameter values of an LSTM with 228 hidden units trained on the\nPenn Treebank corpus without dropout (c.f. the experimental section for the precise procedure). We then set all data inputs xt to zero and run the corresponding induced dynamical system. Two trajectories starting from nearby initial conditions u0 and u\u03020 were computed (as before u\u03020 was obtained by adding to each components of u0 a small random number drawn uniformly from [\u221210\u22127, 10\u22127]). Figure 3(a) plots the first component h(1) of the hidden state for both trajectories over the first 1600 time steps. After an initial phase of agreement, the forward trajectories O+(u0) and O+(u\u03020) strongly diverge. We also see that both trajectories exhibit the typical aperiodic behavior that characterizes chaotic systems. If the inputs xt do not vanish, but come from actual word-level data, then the behavior is very different. The LSTM is now no longer an autonomous system whose dynamics are driven by its hidden states, but a time dependent system whose dynamics are mostly driven by the external inputs. Figure 3(b) shows the first component h(1) of the hidden states of two trajectories that start with initial conditions u0 and u\u03020 that are far apart. The sensitivity to initial condition disappears, and instead the trajectories converge toward each other after about 70 steps. The memory of this initial difference is lost. Overall these experiments indicate that a trained LSTM, when it is not driven by external inputs, can be chaotic. In the presence of input data, the LSTM becomes a forced system whose dynamics are dominated by external forcing.\nLike LSTM networks, GRU can also lead to dynamical systems that are chaotic and they can also have strange attractors. The depiction of such an attractor, in the case of a two-unit GRU, is provided in Figure 6 of the appendix."}, {"heading": "2.2 CHAOS-FREE BEHAVIOR OF THE CFN", "text": "The dynamical behavior of the CFN is dramatically different from that of the LSTM. In this subsection we start by showing that the hidden states of the CFN activate and relax toward zero in a predictable fashion in response to input data. On one hand, this shows that the CFN cannot produce non-trivial dynamics without some influence from data. On the other, this leads to an interpretable model; any non-trivial activations in the hidden states of a CFN have a clear cause emanating from\ndata-driven activation. This follows from a precise, quantified description of the intuitive picture (3)\u2013(4) sketched in the introduction.\nWe begin with the following simple estimate that sheds light on how the hidden states of the CFN activate and then relax toward the origin. Lemma 1. For any T, k > 0 we have\n|hT+k(i)| \u2264 \u0398k |hT (i)|+ H\n1\u2212\u0398\n( max\nT\u2264t\u2264T+k |(Wxt)(i)| ) where \u0398 and H are the maximum values of the ith components of the \u03b8 and \u03b7 gate in the time interval [T, T + k], that is:\n\u0398 = max T\u2264t\u2264T+k \u03b8t(i) and H = max T\u2264t\u2264T+k \u03b7t(i).\nThis estimate shows that if during a time interval [T1, T2] one of\n(i) the embedded inputs Wxt have weak ith feature (i.e. maxT\u2264t\u2264T+k |(Wxt)(i)| is small), (ii) or the input gates \u03b7t have their ith component close to zero (i.e. H is small),\noccurs then the ith component of the hidden state ht will relaxes toward zero at a rate that depends on the value of the ith component the the forget gate. Overall this leads to the following simple picture: ht(i) activates when presented with an embedded input Wxt with strong ith feature, and then relaxes toward zero until the data present the network once again with strong ith feature. The strength of the activation and the decay rate are controlled by the ith component of the input and forget gates. The proof of Lemma 1 is elementary \u2014\nProof of Lemma 1. Using the non-expansivity of the hyperbolic tangent, i.e. | tanh(x)| \u2264 |x|, and the triangle inequality, we obtain from (1)\n|ht(i)| \u2264 \u0398 |ht\u22121(i)|+H max T\u2264t\u2264T+k |(Wxt)(i)|\nwhenever t is in the interval [T, T + k]. Iterating this inequality and summing the geometric series then gives |hT+k(i)| \u2264 \u0398k|hT (i)|+ ( 1\u2212\u0398k\n1\u2212\u0398\n) H max\nT\u2264t\u2264T+k |(Wxt)(i)|\nfrom which we easily conclude.\nWe now turn toward the analysis of the long-term behavior of the the dynamical system\nut = ht, u 7\u2192 \u03a6(u) := \u03c3 (U\u03b8u + b\u03b8) tanh(u). (10) induced by a CFN. The following lemma shows that the only attractor of this dynamical system is the zero state. Lemma 2. Starting from any initial state u0, the trajectory O+(u0) will eventually converge to the zero state. That is, limt\u2192+\u221e ut = 0 regardless of the the initial state u0.\nProof. From the definition of \u03a6 we clearly have that the sequence defined by ut+1 = \u03a6(ut) satisfies \u22121 < ut(i) < 1 for all t and all i. Since the sequence ut is bounded, so is the sequence vt := U\u03b8ut + b\u03b8. That is there exists a finite C > 0 such that (U\u03b8ut)(i) + b\u03b8(i) < C for all t and i. Using the non-expansivity of the hyperbolic tangent, we then obtain that |ut(i)| \u2264 \u03c3(C)|ut\u22121(i)|, for all t and all i. We conclude by noting that 0 < \u03c3(C) < 1.\nLemma 2 remains true for a multi-layer CFN, that is, a CFN in which the first layer is defined by (1) and the subsequent layers 2 \u2264 ` \u2264 L are defined by:\nh (`) t = \u03b8 (`) t tanh(h (`) t\u22121) + \u03b7 (`) t tanh(W (`)h (`\u22121) t ).\nAssume that Wxt = 0 for all t > T , then an extension of the arguments contained in the proof of the two previous lemmas shows that\n|h(`)T+k| \u2264 C(1 + k) (`\u22121)\u0398k (11)\nwhere 0 < \u0398 < 1 is the maximal values for the input gates involved in layer 1 to ` of the network, and C > 0 is some constant depending only on the norms \u2016W (j)\u2016\u221e of the matrices and the sizes |h(j)T | of the initial conditions at all previous 1 \u2264 j \u2264 ` levels. Estimate (11) shows that Lemma 2 remains true for multi-layer architectures.\nInequality (11) shows that higher levels (i.e. larger `) decay more slowly, and remain non-trivial, while earlier levels (i.e. smaller `) decay more quickly. We illustrate this behavior computationally with a simple experiment. We take a 2-layer, 224-unit CFN network trained on Penn Treebank and feed it the following input data: The first 1000 inputs xt are the first 1000 words of the test set of Penn Treebank; All subsequent inputs are zero. In other words, xt = 0 if t > 1000. For each of the two layers we then select the 10 units that decay the slowest after t > 1000 and plot them on Figure 4. The first layer retains information for about 10 to 20 time steps, whereas the second layer retains information for about 100 steps. This experiment conforms with the analysis (11), and indicates that adding a third or fourth layer would potentially allow a multi-layer CFN architecture to retain information for even longer periods."}, {"heading": "3 EXPERIMENTS", "text": "In this section we show that despite its simplicity, the CFN network achieves performance comparable to the much more complex LSTM network on the word level language modeling task. We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014). We consider both one-layer and two-layer CFNs and LSTMs for our experiments. We train both CFN and LSTM networks in a similar fashion and always compare models that use the same number of parameters. We compare their performance with and without dropout, and show that in both cases they obtain similar results. We also provide results published in Mikolov et al. (2014), Jozefowicz et al. (2015) and Sukhbaatar et al. (2015) for the sake of comparison.\nFor concreteness, the exact implementation for the two-layer architecture of our model is\nh (0) t = W (0)xt\nh\u0302 (0) t = Drop(h (0) t , p)\nh (1) t = \u03b8 (1) t tanh(h (1) t\u22121) + \u03b7 (1) t tanh(W (1)h\u0302 (0) t )\nh\u0302 (1) t = Drop(h (1) t , p)\nh (2) t = \u03b8 (2) t tanh(h (2) t\u22121) + \u03b7 (2) t tanh(W (2)h\u0302 (1) t )\nh\u0302 (2) t = Drop(h (2) t , p)\nyt = LogSoftmax(W (3)h\u0302 (2) t + b) (12)\nwhere Drop(z, p) denotes the dropout operator with a probability p of setting components in z to zero. We compute the gates according to\n\u03b8 (`) t := \u03c3\n( U\n(`) \u03b8 h\u0303 (`) t\u22121 + V (`) \u03b8 h\u0303 (`\u22121) t + b\u03b8 ) and \u03b7(`)t := \u03c3 ( U (`)\u03b7 h\u0303 (`) t\u22121 + V (`) \u03b7 h\u0303 (`\u22121) t + b\u03b7 ) (13)\nwhere h\u0303(`)t\u22121 = Drop(h (`) t\u22121, q) and h\u0303 (`\u22121) t = Drop(h (`\u22121) t , q), (14)\nand thus the model has two dropout hyperparameters. The parameter p controls the amount of dropout between layers; the parameter q controls the amount of dropout inside each gate. We use a similar dropout strategy for the LSTM, in that all sigmoid gates f, o and i receive the same amount q of dropout.\nTo train the CFN and LSTM networks, we use a simple online steepest descent algorithm. We update the weights w via\nw(k+1) = w(k) \u2212 lr \u00b7 ~p where ~p = \u2207wL \u2016\u2207wL\u20162 , (15)\nand \u2207wL denotes the approximate gradient of the loss with respect to the weights as estimated from a certain number of presented examples. We use the usual backpropagation through time approximation when estimating the gradient: we unroll the net T steps in the past and neglect longer dependencies. In all experiments, the CFN and LSTM networks are unrolled for T = 35 steps and we take minibatches of size 20. In the case of an exact gradient, the update (15) simply corresponds to making a step of length lr in the direction of steepest descent. As all search directions ~p have Euclidean norm \u2016~p\u20162 = 1, we perform no gradient clipping during training. We initialize all the weights in the CFN, except for the bias of the gates, uniformly at random in [\u22120.07, 0.07]. We initialize the bias b\u03b8 and b\u03b7 of the gates to 1 and \u22121, respectively, so that at the beginning of the training\n\u03b8t \u2248 \u03c3(1) \u2248 0.73 and \u03b7t \u2248 \u03c3(\u22121) \u2248 0.23.\nWe initialize the weights of the LSTM in exactly the same way; the bias for the forget and input gate are initialized to 1 and \u22121, and all the other weights are initialized uniformly in [\u22120.07, 0.07]. This initialization scheme favors the flow of information in the horizontal direction. The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models.\nDataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words). The Text8 corpus has 100 million characters and a vocabulary size of 44,000. We used the script from Mikolov et al. (2014) to\nconstruct and split the dataset into a training set (first 99M characters) and a development set (last 1M characters).\nExperiments without Dropout. Tables 1 and 2 provide a comparison of various recurrent network architectures without dropout evaluated on the Penn Treebank corpus and the Text8 corpus. The last two rows of each table provide results for LSTM and CFN networks trained and initialized in the manner described above. We have tried both one and two layer architectures, and reported only the best result. The learning rate schedules used for each network are described in the appendix.\nWe also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8. Of these four networks, only the LSTM network from Mikolov et al. (2014) has the same number of parameters than the CFN and LSTM networks we trained (46.4M parameters). The vanilla RNN, Structurally Constrained Recurrent Network (SCRN) and End-To-End Memory Network (MemN2N) all have 500 units, but less than 46.4M parameters. We nonetheless indicate their performance in Table 2 to provide some context.\nExperiments with Dropout. Table 3 provides a comparison of various recurrent network architectures with dropout evaluated on the Penn Treebank corpus. The first three rows report results published in (Jozefowicz et al., 2015) and the last four rows provide results for LSTM and CFN networks trained and initialized with the strategy previously described. The dropout rate p and q are chosen as follows: For the experiments with 20M parameters, we set p = 55% and q = 45% for the CFN and p = 60% and q = 40% for the LSTM; For the experiments with 50M parameters, we set p = 65% and q = 55% for the CFN and p = 70% and q = 50% for the LSTM."}, {"heading": "4 CONCLUSION", "text": "Despite its simple dynamics, the CFN obtains results that compare well against LSTM networks and GRUs on word-level language modeling. This indicates that it might be possible, in general, to build RNNs that perform well while avoiding the intricate, uninterpretable and potentially chaotic dynamics that can occur in LSTMs and GRUs. Of course, it remains to be seen if dynamically simple RNNs such as the proposed CFN can perform well on a wide variety of tasks, potentially requiring longer term dependencies than the one needed for word level language modeling. The experiments presented in Section 2 indicate a plausible path forward \u2014 activations in the higher layers of a multi-layer CFN decay at a slower rate than the activations in the lower layers. In theory, complexity and long-term dependencies can therefore be captured using a more \u201cfeed-forward\u201d approach (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM or a GRU.\nOverall, the CFN is a simple model and it therefore has the potential of being mathematically wellunderstood. In particular, Section 2 reveals that the dynamics of its hidden states are inherently more interpretable than those of an LSTM. The mathematical analysis here provides a few key insights into the network, in both the presence and absence of input data, but obviously more work is needed before a complete picture can emerge. We hope that this investigation opens up new avenues of inquiry, and that such an understanding will drive subsequent improvements."}], "references": [{"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["Nils Bertschinger", "Thomas Natschl\u00e4ger"], "venue": "Neural computation,", "citeRegEx": "Bertschinger and Natschl\u00e4ger.,? \\Q2004\\E", "shortCiteRegEx": "Bertschinger and Natschl\u00e4ger.", "year": 2004}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "A two-dimensional mapping with a strange attractor", "author": ["Michel H\u00e9non"], "venue": "Communications in Mathematical Physics,", "citeRegEx": "H\u00e9non.,? \\Q1976\\E", "shortCiteRegEx": "H\u00e9non.", "year": 1976}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering", "author": ["Steven H Strogatz"], "venue": "Westview press,", "citeRegEx": "Strogatz.,? \\Q2014\\E", "shortCiteRegEx": "Strogatz.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks", "author": ["David Sussillo", "Omri Barak"], "venue": "Neural computation,", "citeRegEx": "Sussillo and Barak.,? \\Q2013\\E", "shortCiteRegEx": "Sussillo and Barak.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Gated recurrent neural networks, such as the Long Short Term Memory network (LSTM) introduced by Hochreiter & Schmidhuber (1997) and the Gated Recurrent Unit (GRU) proposed by Cho et al. (2014), prove highly effective for machine learning tasks that involve sequential data.", "startOffset": 176, "endOffset": 194}, {"referenceID": 8, "context": "The study of RNNs from a dynamical systems point-of-view has brought fruitful insights into generic features of RNNs (Sussillo & Barak, 2013; Pascanu et al., 2013).", "startOffset": 117, "endOffset": 163}, {"referenceID": 9, "context": "In this subsection we briefly show that LSTM and GRU, in the absence of input data, can lead to dynamical systems ut = \u03a6(ut\u22121) that are chaotic in the classical sense of the term (Strogatz, 2014).", "startOffset": 179, "endOffset": 195}, {"referenceID": 6, "context": "We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 7, "context": ", 1993) and the Text8 corpus (Mikolov et al., 2014).", "startOffset": 29, "endOffset": 51}, {"referenceID": 5, "context": "We use two datasets for these experiments, namely the Penn Treebank corpus (Marcus et al., 1993) and the Text8 corpus (Mikolov et al., 2014). We consider both one-layer and two-layer CFNs and LSTMs for our experiments. We train both CFN and LSTM networks in a similar fashion and always compare models that use the same number of parameters. We compare their performance with and without dropout, and show that in both cases they obtain similar results. We also provide results published in Mikolov et al. (2014), Jozefowicz et al.", "startOffset": 76, "endOffset": 513}, {"referenceID": 5, "context": "(2014), Jozefowicz et al. (2015) and Sukhbaatar et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 5, "context": "(2014), Jozefowicz et al. (2015) and Sukhbaatar et al. (2015) for the sake of comparison.", "startOffset": 8, "endOffset": 62}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.", "startOffset": 26, "endOffset": 51}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.9 GRU 5M parameters Jozefowicz et al. (2015) 108.", "startOffset": 26, "endOffset": 100}, {"referenceID": 5, "context": "Vanilla RNN 5M parameters Jozefowicz et al. (2015) 122.9 GRU 5M parameters Jozefowicz et al. (2015) 108.2 LSTM 5M parameters Jozefowicz et al. (2015) 109.", "startOffset": 26, "endOffset": 150}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al. (2014) 156 MemN2N 500 hidden units Sukhbaatar et al.", "startOffset": 48, "endOffset": 166}, {"referenceID": 7, "context": "on development set Vanilla RNN 500 hidden units Mikolov et al. (2014) 184 SCRN 500 hidden units Mikolov et al. (2014) 161 LSTM 500 hidden units Mikolov et al. (2014) 156 MemN2N 500 hidden units Sukhbaatar et al. (2015) 147 LSTM (2 layers) 46.", "startOffset": 48, "endOffset": 219}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al.", "startOffset": 87, "endOffset": 106}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models.", "startOffset": 87, "endOffset": 157}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models. Dataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words).", "startOffset": 87, "endOffset": 368}, {"referenceID": 2, "context": "The importance of a careful initialization of the forget gate was first pointed out in Gers et al. (2000) and further emphasized in Jozefowicz et al. (2015). Finally, we initialize all hidden states to zero for both models. Dataset Construction. The Penn Treebank Corpus has 1 million words and a vocabulary size of 10,000. We used the code from Zaremba et al. (2014) to construct and split the dataset into a training set (929K words), a validation set (73K words) and a test set (82K words). The Text8 corpus has 100 million characters and a vocabulary size of 44,000. We used the script from Mikolov et al. (2014) to", "startOffset": 87, "endOffset": 617}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.", "startOffset": 27, "endOffset": 52}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.0 97.7 GRU 20M parameters Jozefowicz et al. (2015) 95.", "startOffset": 27, "endOffset": 107}, {"referenceID": 5, "context": "Vanilla RNN 20M parameters Jozefowicz et al. (2015) 103.0 97.7 GRU 20M parameters Jozefowicz et al. (2015) 95.5 91.7 LSTM 20M parameters Jozefowicz et al. (2015) 83.", "startOffset": 27, "endOffset": 162}, {"referenceID": 5, "context": "The first three rows report results published in (Jozefowicz et al., 2015) and the last four rows provide results for LSTM and CFN networks trained and initialized with the strategy previously described.", "startOffset": 49, "endOffset": 74}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported).", "startOffset": 36, "endOffset": 61}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al.", "startOffset": 36, "endOffset": 281}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8.", "startOffset": 36, "endOffset": 310}, {"referenceID": 5, "context": "We also report results published in Jozefowicz et al. (2015) were a vanilla RNN, a GRU and an LSTM network were trained on Penn Treebank, each of them having 5 million parameters (only the test perplexity was reported). Finally we report results published in Mikolov et al. (2014) and Sukhbaatar et al. (2015) where various networks are trained on Text8. Of these four networks, only the LSTM network from Mikolov et al. (2014) has the same number of parameters than the CFN and LSTM networks we trained (46.", "startOffset": 36, "endOffset": 428}], "year": 2016, "abstractText": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "creator": "LaTeX with hyperref package"}}}