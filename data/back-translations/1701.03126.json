{"id": "1701.03126", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Attention-Based Multimodal Fusion for Video Description", "abstract": "Recent work has shown that it is beneficial to integrate temporal and / or spatial attention mechanisms into these models, in which the decoder network predicts each word in the description by selectively adding weight to encoded features from specific time frames (temporal attention) or features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model so that it not only selectively takes into account certain times or spatial regions, but also certain input modalities such as image features, motion features and audio features. Our new modal attention mechanism, which we call multimodal attention, provides a natural way to merge multimodal information for video description. We evaluate our method using the Youtube2Text data set and achieve results that compete with the current state of the art. More importantly, our model takes into account multimodal attention in a significant way by using only temporal attention.", "histories": [["v1", "Wed, 11 Jan 2017 19:16:42 GMT  (286kb,D)", "http://arxiv.org/abs/1701.03126v1", "Submitted to CVPR 2017 for review, 8 pages, 4 figures"], ["v2", "Thu, 9 Mar 2017 22:57:10 GMT  (284kb,D)", "http://arxiv.org/abs/1701.03126v2", "Resubmitted to the rebuttal for CVPR 2017 for review, 8 pages, 4 figures"]], "COMMENTS": "Submitted to CVPR 2017 for review, 8 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["chiori hori", "takaaki hori", "teng-yok lee", "kazuhiro sumi", "john r hershey", "tim k marks"], "accepted": false, "id": "1701.03126"}, "pdf": {"name": "1701.03126.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Multimodal Fusion for Video Description", "authors": ["Chiori Hori", "Takaaki Hori", "Teng-Yok Lee", "Kazuhiro Sumi", "John R. Hershey", "Tim K. Marks"], "emails": ["tmarks}@merl.com", "sumi@it.aoyama.ac.jp"], "sections": [{"heading": "1. Introduction and Related Work", "text": "Automatic video description, also known as video captioning, refers to the automatic generation of a natural language description (e.g., a sentence) that summarizes an input video. Video description has widespread applications including video retrieval, automatic description of home movies or online uploaded video clips, and video descriptions for the visually impaired. Moreover, developing systems that can describe videos may help us to elucidate some key components of general machine intelligence. Video description research depends on the availability of videos labeled with descriptive text. A large amount of such data is\n\u2217On sabbatical from Aoyama Gakuin University, sumi@it.aoyama.ac.jp\nbecoming available in the form of audio description prepared for visually impaired users. Thus there is an opportunity to make significant progress in this area. We propose a video description method that uses an attentionbased encoder-decoder network to generate sentences from input video.\nSentence generation using an encoder-decoder architecture was originally used for neural machine translation (NMT), in which sentences in a source language are converted into sentences in a target language [25, 5]. In this paradigm, the encoder takes an input sentence in the source language and maps it to a fixed-length feature vector in an embedding space. The decoder uses this feature vector as input to generate a sentence in the target language. However, the fixed length of the feature vector limited performance, particularly on long input sentences, so [1] proposed to encode the input sentence as a sequence of feature vectors, employing a recurrent neural network (RNN)-based soft attention model to enable the decoder to pay attention to features derived from specific words of the input sentence when generating each output word.\nThe encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].\nIn image captioning, the input is a single image, and the output is a natural-language description. Recent work on RNN-based image captioning includes [17, 25]. To improve performance, [27] added an attention mechanism, to enable focusing on specific parts of the image when generating each word of the description.\nEncoder-decoder networks have also been applied to the task of video description [24]. In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12]. The decoder network takes the encoder outputs and generates word\n1\nar X\niv :1\n70 1.\n03 12\n6v 1\n[ cs\n.C V\n] 1\n1 Ja\nn 20\n17\nsequences based on language models using recurrent neural networks (RNNs) based on long short-term memory (LSTM) units [9] or gated recurrent units (GRUs) [4]. Such systems can be trained end-to-end using videos labeled with text descriptions.\nOne inherent problem in video description is that the sequence of video features and the sequence of words in the description are not synchronized. In fact, objects and actions may appear in the video in a different order than they appear in the sentence. When choosing the right words to describe something, only the features that directly correspond to that object or action are relevant, and the other features are a source of clutter. It may be possible for an LSTM to learn to selectively encode different objects into its latent features and remember them until they are retrieved. However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10]. In recent work, these attention mechanisms have been applied to video description [28, 29]. Whereas in image captioning the attention is spatial (attending to specific regions of the image), in video description the attention may be temporal (attending to specific time frames of the video) in addition to (or instead of) spatial.\nIn this work, we propose a new use of attention: to fuse information across different modalities. Here we use modality loosely to refer to different types of features derived from the video, such as appearance, motion, or depth, as well as features from different sensors such as video and audio features. Video descriptions can include a variety of descriptive styles, including abstract descriptions of the scene, descriptions focused on objects and their relations, and descriptions of action and motion, including both motion in the scene and camera motion. The soundtrack also contains audio events that provide additional information about the described scene and its context. Depending on what is being described, different modalities of input may be important for selecting appropriate words in the description. For example, the description \u201cA boy is standing on a hill\u201d refers to objects and their relations. In contrast, \u201cA boy is jumping on a hill\u201d may rely on motion features to determine the action. \u201dA boy is listening to airplanes flying overhead\u201d may require audio features to recognize the airplanes, if they do not appear in the video. Not only do the relevant modalities change from sentence to sentence, but also from word to word, as we move from action words that describe motion to nouns that define object types. Attention to the appropriate modalities, as a function of the context, may help with choosing the right words for the video description.\nOften features from different modalities can be comple-\nmentary, in that either can provide reliable cues at different times for some aspect of a scene. Multimodal fusion is thus an important longstanding strategy for robustness. However, optimally combining information requires estimating the reliability of each modality, which remains a challenging problem. In this work, we propose that this estimation be performed by the neural network, by means of an attention mechanism that operating across different modalities (in addition to any spatio-temporal attention). By training the system end-to-end to perform the desired description of the semantic content of the video, the system can learn to use attention to fuse the modalities in a contextsensitive way. We present experiments showing that incorporating multimodal attention, in addition to temporal attention, significantly outperforms a corresponding model that uses temporal attention alone."}, {"heading": "2. Encoder-decoder-based sentence generator", "text": "One basic approach to video description is based on sequence-to-sequence learning. The input sequence, i.e., image sequence, is first encoded to a fixed-dimensional semantic vector. Then the output sequence, i.e., word sequence, is generated from the semantic vector. In this case, both the encoder and the decoder (or generator) are usually modeled as Long Short-Term Memory (LSTM) networks. Figure 1 shows an example of the LSTM-based encoderdecoder architecture.\nGiven a sequence of images, X = x1, x2, . . . , xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22]. The sequence of image features, X \u2032 = x\u20321, x \u2032 2, . . . , x \u2032 L, is obtained by extracting the activation vector of a fully-connected layer of the CNN for each input image.1 The sequence of feature vectors is then fed to the LSTM encoder, and the hidden\n1In the case of C3D, multiple images are fed to the network at once to capture dynamic features in the video.\nstate of the LSTM is given by\nht = LSTM(ht\u22121, x\u2032t;\u03bbE), (1)\nwhere the LSTM function of the encoder network \u03bbE is computed as\nLSTM(ht\u22121,xt;\u03bb) = ot tanh(ct), (2) where ot = \u03c3 ( W (\u03bb)xo xt +W (\u03bb) ho ht\u22121 + b (\u03bb) o ) (3)\nct = ftct\u22121 + it tanh ( W (\u03bb)xc xt\n+W (\u03bb) hc ht\u22121 + b (\u03bb) c\n) (4)\nft = \u03c3 ( W (\u03bb) xf xt +W (\u03bb) hf ht\u22121 + b (\u03bb) f ) (5)\nit = \u03c3 ( W (\u03bb) xi xt +W (\u03bb) hi ht\u22121 + b (\u03bb) i ) , (6)\nwhere \u03c3() is the element-wise sigmoid function, and it, ft, ot and ct are, respectively, the input gate, forget gate, output gate, and cell activation vectors for the tth input vector. The weight matrices W (\u03bb)zz and the bias vectors b (\u03bb) z are identified by the subscript z \u2208 {x, h, i, f, o, c}. For example,Whi is the hidden-input gate matrix and Wxo is the input-output gate matrix. We did not use peephole connections in this work.\nThe decoder predicts the next word iteratively beginning with the start-of-sentence token, \u201c<sos>\u201d until it predicts the end-of-sentence token, \u201c<eos>.\u201d Given decoder state si\u22121, the decoder network \u03bbD infers the next word probability distribution as\nP (y|si\u22121) = softmax ( W (\u03bbD)s si\u22121 + b (\u03bbD) s ) , (7)\nand generates word yi, which has the highest probability, according to\nyi = argmax y\u2208V P (y|si\u22121), (8)\nwhere V denotes the vocabulary. The decoder state is updated using the LSTM network of the decoder as\nsi = LSTM(si\u22121, y\u2032i;\u03bbD), (9)\nwhere y\u2032i is a word-embedding vector of ym, and the initial state s0 is obtained from the final encoder state hL and y\u20320 = Embed(<sos>) as in Figure 1.\nIn the training phase, Y = y1, . . . , yM is given as the reference. However, in the test phase, the best word sequence needs to be found based on\nY\u0302 = argmax Y \u2208V \u2217 P (Y |X) (10)\n= argmax y1,...,yM\u2208V \u2217 P (y1|s0)P (y2|s1) \u00b7 \u00b7 \u00b7\nP (yM |sM\u22121)P (<eos>|sM ). (11)\nAccordingly, we use a beam search in the test phase to keep multiple states and hypotheses with the highest cumulative probabilities at eachmth step, and select the best hypothesis from those having reached the end-of-sentence token."}, {"heading": "3. Attention-based sentence generator", "text": "Another approach to video description is an attentionbased sequence generator [6], which enables the network to emphasize features from specific times or spatial regions depending on the current context, enabling the next word to be predicted more accurately. Compared to the basic approach described in Section 2, the attention-based generator can exploit input features selectively according to the input and output contexts. The efficacy of attention models has been shown in many tasks such as machine translation [1].\nFigure 2 shows an example of the attention-based sentence generator from video, which has a temporal attention mechanism over the input image sequence.\nThe input sequence of feature vectors is obtained using one or more feature extractors. Generally, attentionbased generators employ an encoder based on a bidirectional LSTM (BLSTM) or Gated Recurrent Units (GRU) to further convert the feature vector sequence so that each vector contains its contextual information. In video description tasks, however, CNN-based features are often used directly, or one more feed-forward layer is added to reduce the dimensionality.\nIf we use an BLSTM encoder following the feature extraction, then the activation vectors (i.e., encoder states) are obtained as\nht =\n[ h (f) t\nh (b) t\n] , (12)\nwhere h(f)t and h (b) t are the forward and backward hidden activation vectors:\nh (f) t = LSTM(h (f) t\u22121, x \u2032 t;\u03bb (f) E ) (13) h (b) t = LSTM(h (b) t+1, x \u2032 t;\u03bb (b) E ). (14)\nIf we use a feed-forward layer, then the activation vector is\ncalculated as\nht = tanh(Wpx \u2032 t + bp), (15)\nwhere Wp is a weight matrix and bp is a bias vector. If we use the CNN features directly, then we assume ht = x\u2032t.\nThe attention mechanism is realized by using attention weights to the hidden activation vectors throughout the input sequence. These weights enable the network to emphasize features from those time steps that are most important for predicting the next output word.\nLet \u03b1i,t be an attention weight between the ith output word and the tth input feature vector. For the ith output, the vector representing the relevant content of the input sequence is obtained as a weighted sum of hidden unit activation vectors:\nci = L\u2211 t=1 \u03b1i,tht. (16)\nThe decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [1][6] that generates an output label sequence with content vectors ci. The network also has an LSTM decoder network, where the decoder state can be updated in the same way as Equation (9).\nThen, the output label probability is computed as P (y|si\u22121, ci) = softmax ( W (\u03bbD)s si\u22121 +W (\u03bbD) c ci + b (\u03bbD) s ) , (17) and word yi is generated according to\nyi = argmax y\u2208V P (y|si\u22121, ci). (18)\nIn contrast to Equations (7) and (8) of the basic encoderdecoder, the probability distribution is conditioned on the content vector ci, which emphasizes specific features that are most relant to predicting each subsequent word. One more feed-forward layer can be inserted before the softmax layer. In this case, the probabilities are computed as follows:\ngi = tanh ( W (\u03bbD)s si\u22121 +W (\u03bbD) c ci + b (\u03bbD) s ) , (19)\nand\nP (y|si\u22121, ci) = softmax(W (\u03bbD)g gi + b(\u03bbD)g ). (20)\nThe attention weights are computed in the same manner as in [1]:\n\u03b1i,t = exp(ei,t)\u2211L \u03c4=1 exp(ei,\u03c4 )\n(21)\nand ei,t = w \u1d40 A tanh(WAsi\u22121 + VAht + bA), (22)\nwhereWA and VA are matrices, wA and bA are vectors, and ei,t is a scalar."}, {"heading": "4. Attention-based multimodal fusion", "text": "This section proposes an attention model to handle fusion of multiple modalities, where each modality has its own sequence of feature vectors. For video description, multimodal inputs such as image features, motion features, and audio features are available. Furthermore, combination of multiple features from different feature extraction methods are often effective to improve the description accuracy.\nIn [29], content vectors from VGGNet (image features) and C3D (spatiotemporal motion features) are combined into one vector, which is used to predict the next word. This is performed in the fusion layer, in which the following activation vector is computed instead of Eq. (19),\ngi = tanh ( W (\u03bbD)s si\u22121 + di + b (\u03bbD) s ) , (23)\nwhere di =W (\u03bbD) c1 c1,i +W (\u03bbD) c2 c2,i, (24)\nand c1,i and c2,i are two different content vectors obtained using different feature extractors and/or different input modalities.\nFigure 3 shows the simple feature fusion approach, in which content vectors are obtained with attention weights for individual input sequences x11, . . . , x1L and x21, . . . , x2L\u2032 , respectively. However, these content vectors are combined with weight matrices Wc1 and Wc2, which are commonly used in the sentence generation step. Consequently, the content vectors from each feature type (or one modality) are always fused using the same weights, independent of the decoder state. This architecture lacks the ability to exploit multiple types of features effectively, because it does not allow the relative weights of each feature type (of each modality) to change based on the context.\nThis paper extends the attention mechanism to multimodal fusion. Using this multimodal attention mechanism, based on the current decoder state, the decoder network can selectively attend to specific modalities of input (or specific feature types) to predict the next word. LetK be the number of modalities, i.e., the number of sequences of input feature vectors. Our attention-based feature fusion is performed using\ngi = tanh ( W (\u03bbD)s si\u22121 + K\u2211 k=1 \u03b2k,idk,i + b (\u03bbD) s ) , (25)\nwhere dk,i =W (\u03bbD) ck ck,i + b (\u03bbD) ck . (26)\nThe multimodal attention weights \u03b2k,i are obtained in a similar way to the temporal attention mechanism:\n\u03b2k,i = exp(vk,i)\u2211K \u03ba=1 exp(v\u03ba,i) , (27)\nwhere\nvk,i = w \u1d40 B tanh(WBsi\u22121 + VBkck,i + bBk), (28)\nwhere WB and VBk are matrices, wB and bBk are vectors, and vk,i is a scalar.\nFigure 4 shows the architecture of our sentence generator, including the multimodal attention mechanism. Unlike the simple multimodal fusion method in Figure 3, in Figure 4, the feature-level attention weights can change according to the decoder state and the content vectors, which enables the decoder network to pay attention to a different set of features and/or modalities when predicting each subsequent word in the description."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Dataset", "text": "We evaluated our proposed feature fusion using the Youtube2Text video corpus [8]. This corpus is well suited for training and evaluating automatic video description generation models. The dataset has 1,970 video clips with multiple natural language descriptions. Each video clip is annotated with multiple parallel sentences provided by different Mechanical Turkers. There are 80,839 sentences in total, with about 41 annotated sentences per clip. Each sentence on average contains about 8 words. The words contained in all the sentences constitute a vocabulary of 13,010 unique lexical entries. The dataset is open-domain and covers a wide range of topics including sports, animals and music. Following [38], we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips, and a test set consisting of the remaining 670 clips."}, {"heading": "5.2. Video Preprocessing", "text": "The image data are extracted from each video clip, which consist of 24 frames per second, and rescaled to 224\u00d7224 pixel images. For extracting image features, a pretrained GoogLeNet [15] CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe [11]. Features are extracted from the hidden layer pool5/7x7 s1. We select one frame out of every 16 frames from each video clip and feed them into the CNN to obtain 1024-dimensional frame-wise feature vectors.\nWe also use a VGGNet [20] that was pretrained on the ImageNet dataset [14]. The hidden activation vectors of fully connected layer fc7 are used for the image features, which produces a sequence of 4096-dimensional feature vectors. Furthermore, to model motion and short-term spatiotemporal activity, we use the pretrained C3D [22] (which was trained on the Sports-1M dataset [13]). The C3D network reads sequential frames in the video and outputs a fixed-length feature vector every 16 frames. We extracted activation vectors from fully-connected layer fc6-1, which has 4096-dimensional features."}, {"heading": "5.3. Audio Processing", "text": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method. Since YouTube2Text corpus does not contain audio track, we extracted the audio data via the original video URLs. Although a subset of the videos were no longer available on YouTube, we were able to collect the audio data for 1,649 video clips, which covers 84% of the corpus. The 44 kHz-sampled audio data are down-sampled to 16 kHz, and Mel-Frequency Cepstral Coefficients (MFCCs) are extracted from each 50 ms time window with 25 ms shift. The\nsequence of 13-dimensional MFCC features are then concatenated into one vector from every group of 20 consecutive frames, which results in a sequence of 260-dimensional vectors. The MFCC features are normalized so that the mean and variance vectors are 0 and 1 in the training set. The validation and test sets are also adjusted with the original mean and variance vectors of the training set. Unlike with the image features, we apply a BLSTM encoder network for MFCC features, which is trained jointly with the decoder network. If audio data are missing for a video clip, then we feed in a sequence of dummy MFCC features, which is simply a sequence of zero vectors."}, {"heading": "5.4. Experimental Setup", "text": "The caption generation model, i.e. the decoder network, is trained to minimize the cross entropy criterion using the training set. Image features are fed to the decoder network through one projection layer of 512 units, while audio features, i.e. MFCCs, are fed to the BLSTM encoder followed by the decoder network. The encoder network has one projection layer of 512 units and bidirectional LSTM layers of 512 cells. The decoder network has one LSTM layer with 512 cells. Each word is embedded to a 256-dimensional vector when it is fed to the LSTM layer. We apply the AdaDelta optimizer [30] to update the parameters, which is widely used for optimizing attention models. The LSTM and attention models were implemented using Chainer [21].\nThe similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23]. We used the publicly available evaluation script prepared\nfor image captioning challenge [3]."}, {"heading": "5.5. Results and Discussion", "text": "Table 1 shows the evaluation results on the Youtube2text data set. We compared the performance for our multimodal attention model (Multimodal Attention) which integrated temporal and multimodal attention mechanisms with a simple additive multimodal fusion (Simple Multimodal), unimodal models with temporal attention (Unimodal), and baseline systems that used temporal attention.\nThe Simple Multimodal model performed better than the Unimodal models. However the proposed Multimodal Attention model outperformed Simple Multimodal. The audio feature degrades the performance of the baseline because some YouTube data includes noise such as background music, which is unrelated to the video content. As expected, the Multimodal Attention model mitigated the impact of the noise of the audio features. Moreover, combining the audio features using our proposed method reached the best performance of CIDEr for all experimental conditions.\nIn contrast to the existing systems, our temporal attention system which used only static image features (Unimodal) outperformed TA using combination of static image and dynamic video features [28]. Our proposed attention mechanisms outperformed LSTM-E [18] which does not use attention mechanisms. Our Simple Multimodal system using temporal attention is the same basic structure used by h-RNN as well as the same features extracted from VGGNet [20] and C3D [22]. However, Simple Multimodal did not perform as well as h-RNN, which may indicate that it was not as well tuned as h-RNN. While we employed AdaDelta for optimizers and LSTM-based RNN for decoder\nnetworks, h-RNN employed RMSprop and Gated Recurrent Units (GRUs). Importantly, our Multimodal Attention model improved upon Simple Multimodal and achieved comparable performance with h-RNN."}, {"heading": "6. Conclusion", "text": "We proposed a new modality-dependent attention mechanism, which we call multimodal attention, for video description based on encoder-decoder sentence generation using recurrent neural networks (RNNs). In this approach, the attention model selectively attends not just to specific times, but to specific modalities of input such as image features, spatiotemporal motion features, and audio features. This approach provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state-of-the-art methods that employ temporal attention models, in which the decoder network predicts each word in the description by selectively giving more weight to encoded features from specific time frames. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention outperforms the model that uses temporal attention alone."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "pages 4945\u20134949,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1504.00325,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Encoderdecoder approaches. CoRR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 577\u2013585. Curran Associates, Inc.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M.J. Denkowski", "A. Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine  Translation, WMT@ACL 2014, June 26-27, 2014, Baltimore, Maryland, USA, pages 376\u2013380,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2712\u20132719,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Dialog state tracking with attention-based sequence-to-sequence learning", "author": ["T. Hori", "H. Wang", "C. Hori", "S. Watanabe", "B. Harsham", "J.L. Roux", "J. Hershey", "Y. Koji", "Y. Jing", "Z. Zhu", "T. Aikawa"], "venue": "2016 IEEE Spoken Language Technology Workshop, SLT 2016, San Diego, CA, USA, December 13-16,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating Natural Video Descriptions via Multimodal Processing", "author": ["Q. Jin", "J. Liang", "X. Lin"], "venue": "Interspeech,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2-4 September 2015, Prague, Czech Republic, pages 285\u2013294,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (mrnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, abs/1412.6632,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CoRR, abs/1505.01861,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311\u2013318,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a nextgeneration open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (Learn-  ingSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 4489\u20134497,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566\u20134575,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May 31 - June 5, 2015, pages 1494\u20131504,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3156\u20133164,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition, pages 3169\u20133176, Colorado Springs, United States, June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2048\u20132057,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C.J. Pal", "H. Larochelle", "A.C. Courville"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7- 13, 2015, pages 4507\u20134515,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CoRR, abs/1510.07712,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 24, "context": "Sentence generation using an encoder-decoder architecture was originally used for neural machine translation (NMT), in which sentences in a source language are converted into sentences in a target language [25, 5].", "startOffset": 206, "endOffset": 213}, {"referenceID": 4, "context": "Sentence generation using an encoder-decoder architecture was originally used for neural machine translation (NMT), in which sentences in a source language are converted into sentences in a target language [25, 5].", "startOffset": 206, "endOffset": 213}, {"referenceID": 0, "context": "However, the fixed length of the feature vector limited performance, particularly on long input sentences, so [1] proposed to encode the input sentence as a sequence of feature vectors, employing a recurrent neural network (RNN)-based soft attention model to enable the decoder to pay attention to features derived from specific words of the input sentence when generating each output word.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 171, "endOffset": 174}, {"referenceID": 24, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 193, "endOffset": 197}, {"referenceID": 15, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": "Recent work on RNN-based image captioning includes [17, 25].", "startOffset": 51, "endOffset": 59}, {"referenceID": 24, "context": "Recent work on RNN-based image captioning includes [17, 25].", "startOffset": 51, "endOffset": 59}, {"referenceID": 26, "context": "To improve performance, [27] added an attention mechanism, to enable focusing on specific parts of the image when generating each word of the description.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "Encoder-decoder networks have also been applied to the task of video description [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 239, "endOffset": 243}, {"referenceID": 25, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 264, "endOffset": 268}, {"referenceID": 11, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 303, "endOffset": 307}, {"referenceID": 8, "context": "sequences based on language models using recurrent neural networks (RNNs) based on long short-term memory (LSTM) units [9] or gated recurrent units (GRUs) [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "sequences based on language models using recurrent neural networks (RNNs) based on long short-term memory (LSTM) units [9] or gated recurrent units (GRUs) [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 221, "endOffset": 224}, {"referenceID": 26, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 9, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 271, "endOffset": 275}, {"referenceID": 27, "context": "In recent work, these attention mechanisms have been applied to video description [28, 29].", "startOffset": 82, "endOffset": 90}, {"referenceID": 28, "context": "In recent work, these attention mechanisms have been applied to video description [28, 29].", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "Another approach to video description is an attentionbased sequence generator [6], which enables the network to emphasize features from specific times or spatial regions depending on the current context, enabling the next word to be predicted more accurately.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "The efficacy of attention models has been shown in many tasks such as machine translation [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [1][6] that generates an output label sequence with content vectors ci.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "The decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [1][6] that generates an output label sequence with content vectors ci.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "The attention weights are computed in the same manner as in [1]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 28, "context": "In [29], content vectors from VGGNet (image features) and C3D (spatiotemporal motion features) are combined into one vector, which is used to predict the next word.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "We evaluated our proposed feature fusion using the Youtube2Text video corpus [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "For extracting image features, a pretrained GoogLeNet [15] CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "For extracting image features, a pretrained GoogLeNet [15] CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "We also use a VGGNet [20] that was pretrained on the ImageNet dataset [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "We also use a VGGNet [20] that was pretrained on the ImageNet dataset [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Furthermore, to model motion and short-term spatiotemporal activity, we use the pretrained C3D [22] (which was trained on the Sports-1M dataset [13]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "Furthermore, to model motion and short-term spatiotemporal activity, we use the pretrained C3D [22] (which was trained on the Sports-1M dataset [13]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 17, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 28, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 27, "context": "TA [28] Temporal GoogLeNet 3D CNN 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "517 LSTM-E [18] VGGNet C3D 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "310 h-RNN [29] Temporal VGGNet C3D 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "We apply the AdaDelta optimizer [30] to update the parameters, which is widely used for optimizing attention models.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "The LSTM and attention models were implemented using Chainer [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 152, "endOffset": 155}, {"referenceID": 22, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "We used the publicly available evaluation script prepared for image captioning challenge [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "In contrast to the existing systems, our temporal attention system which used only static image features (Unimodal) outperformed TA using combination of static image and dynamic video features [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "Our proposed attention mechanisms outperformed LSTM-E [18] which does not use attention mechanisms.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Our Simple Multimodal system using temporal attention is the same basic structure used by h-RNN as well as the same features extracted from VGGNet [20] and C3D [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "Our Simple Multimodal system using temporal attention is the same basic structure used by h-RNN as well as the same features extracted from VGGNet [20] and C3D [22].", "startOffset": 160, "endOffset": 164}], "year": 2017, "abstractText": "Currently successful methods for video description are based on encoder-decoder sentence generation using recurrent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder network predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone.", "creator": "LaTeX with hyperref package"}}}