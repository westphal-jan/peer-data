{"id": "1701.03360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition", "abstract": "This paper presents a novel architecture for a deep recursive neural network, reducing remaining LSTM layers by 3.3% compared to existing layers. A simple LSTM layer has an internal memory cell that can learn long-term dependencies of sequential data. It also provides a time shortcut path to avoid disappearing or exploding gradients in the time domain. The proposed remaining LSTM architecture provides an additional spatial shortcut path from lower layers to efficiently train deep networks with multiple LSTM layers. Compared to previous work, the remaining LSTM layer uses the output projection matrix and output gate of LSTM to control the spatial information flow instead of additional gate networks, effectively reducing more than 10% of network parameters. An experiment on remote voice recognition on the AMI-SDM corpus shows that the performance of simple STM and STM-3 networks increases by 3.0% each.", "histories": [["v1", "Tue, 10 Jan 2017 20:03:37 GMT  (1206kb,D)", "http://arxiv.org/abs/1701.03360v1", null], ["v2", "Wed, 15 Mar 2017 00:23:45 GMT  (1636kb,D)", "http://arxiv.org/abs/1701.03360v2", null], ["v3", "Mon, 5 Jun 2017 18:51:08 GMT  (1527kb,D)", "http://arxiv.org/abs/1701.03360v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["jaeyoung kim", "mostafa el-khamy", "jungwon lee"], "accepted": false, "id": "1701.03360"}, "pdf": {"name": "1701.03360.pdf", "metadata": {"source": "CRF", "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition", "authors": ["Jaeyoung Kim", "Mostafa El-Khamy", "Jungwon Lee"], "emails": [], "sections": [{"heading": null, "text": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The proposed residual LSTM architecture provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus indicates that the performance of plain and highway LSTM networks degrades with increasing network depth. For example, 10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and highway LSTM networks, respectively. Training with both the IHM and SDM corpora, the residual LSTM architecture provided larger gain from increasing depth: a 10-layer residual LSTM showed 3.0% WER reduction over the corresponding 5-layer one.\nIndex Terms\nASR, LSTM, GMM, RNN, CNN\nI. INTRODUCTION\nOver the past year, the emergence of deep neural networks has fundamentally changed the design of automatic speech recognition (ASR). Neural network-based acoustic models presented significant performance improvement over the prior state-of-the-art Gaussian mixture model (GMM) [1], [2], [3]. Advanced neural network-based architectures further improved ASR performance. For example, convolutional neural networks (CNN) which has been huge success in\nar X\niv :1\n70 1.\n03 36\n0v 1\n[ cs\n.L G\n] 1\n0 Ja\nn 20\n17\n2 image classification and detection were effective to reduce environmental and speaker variability in acoustic features [4], [5], [6], [7]. Recurrent neural networks (RNN) were successfully applied to learn long term dependencies of sequential data [8], [9], [10].\nThe recent success of a neural network based architecture mainly comes from its deep architecture. However, training a deep neural network is a difficult problem due to vanishing or exploding gradients. Furthermore, increasing depth in recurrent architectures such as gated recurrent unit (GRU) and long short-term memory (LSTM) is significantly more difficult because they already have a deep architecture in the temporal domain.\nThere have been two successful architectures for a deep feed-forward neural network: residual network and highway network. Residual network [11] was successfully applied to train more than 100 convolutional layers for image classification and detection. The key insight in the residual network is to provide a shortcut path between layers that can be used for an additional gradient path. Highway network [12] is an another way of implementing a shortcut path in a feed-forward neural network. [12] presented successful MNIST training results with 100 layers.\nHighway LSTM [13], [14] is a recurrent version of highway network. LSTM [15] has internal memory cells that provide shortcut gradient paths in the temporal direction. Highway LSTM reused them for a highway shortcut in the spatial domain. It also introduced new gate networks to control highway paths from the prior layer memory cells. [13] presented a highway LSTM for far-field speech recognition and showed improvement over plain LSTM. However, [13] also showed that highway LSTM degraded with increasing depth.\nIn this paper, a novel highway architecture, residual LSTM is introduced. The key insights of\na residual LSTM are summarized as below.\n\u2022 Highway connection between layer outputs instead of internal memory cells: LSTM internal\nmemory cells are used to deal with gradient issues in the temporal domain. Reusing it again for the spatial domain could make it more difficult to train a network in both temporal and spatial domains. The proposed residual LSTM uses a layer output for the spatial shortcut connection instead of an internal memory cell, which can less interfere with a temporal grandient flow. \u2022 Each layer output at the residual LSTM learns residual mapping not learnable from highway\npath. Therefore, each new layer does not need to waste time or resource to generate similar outputs from prior layers. \u2022 Residual LSTM reuses LSTM projection matrix as a gate network. For a usual LSTM\n3 network size, more than 10% learnable parameters can be saved from residual LSTM over highway LSTM.\nThe experimental result on the AMI SDM corpus [16] showed 10-layer plain and highway LSTMs had 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM presented the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and highway LSTMs, respectively. For an experiment with IHM and SDM corpora, 10-layer residual LSTM showed 3.0% WER reduction over 5-layer one, whereas the experiment with only SDM corpus presented 1% reduction.\nThe rest of this paper is organized as follows. Section II will review existing highway architectures. Section III will introduce residual LSTM. Section IV will explain experimental setup and provide results on AMI distant speech recognition. Finally, this paper ends with conclusion at section V."}, {"heading": "II. REVISITING HIGHWAY NETWORKS", "text": "In this section, we give a brief review of LSTM and three existing highway architectures."}, {"heading": "A. Residual Network", "text": "Residual network [11] provides an identity mapping by shortcut paths. Since the identity mapping is always on, function output only needs to learn residual mapping. Formulation of this relation can be expressed as:\ny = F (x;W ) + x (1)\ny is a layer output, x is a layer input and F (x;W ) is a function with an internal parameter W . Without a shortcut path, F (x;W ) should represent y from input x, but with an identity mapping x, F (x;W ) only needs to learn residual mapping, y \u2212 x. As layers are stacked up, if no new residual mapping is needed, a network can bypass identity mappings without training, which could greatly simplify training of a deep network."}, {"heading": "B. Highway Network", "text": "Highway network [12] provides another way of implementing a shortcut path for a deep neural-network. Layer output H(x;Wh) is multiplied by a transform gate T (x;WT ) and before\n4 going into the next layer, a highway path x \u00b7 (1\u2212T (x;WT )) is added. Formulation of a highway network can be summarized as:\ny = H(x;Wh) \u00b7 T (x;WT ) + x \u00b7 (1\u2212 T (x;WT )) (2)\nTransform gate is defined as:\nT (x;WT ) = \u03c3(WTx+ bT ) (3)\nUnlike a residual network, a highway path is not always turned on. For example, a highway network can ignore a highway path if T (x;WT ) = 1 , or bypass a layer output when T (x;WT ) = 0."}, {"heading": "C. Long Short-Term Memory (LSTM)", "text": "Long short-term memory (LSTM) [15] was proposed to resolve vanishing or exploding gradients for a recurrent neural network. LSTM has an internal memory cell that is controlled by forget and input gate networks. A forget gate in an LSTM determines how much of prior memory value should be passed into the next time step. Similarly, an input gate scales a new input to a memory cell. Depending on the states of both gates, LSTM can represent long-term or short-term dependency of sequential data. The formulation of an LSTM is as follows:\nilt = \u03c3(W l xix l t +W l hih l t\u22121 + w l cic l t\u22121 + b l i) (4)\nf lt = \u03c3(W l xfx l t +W l hfh l t\u22121 + w l cfc l t\u22121 + b l f ) (5)\nclt = f l t \u00b7 clt\u22121 + ilt \u00b7 tanh(W lxcxlt +W lhchlt\u22121 + blc) (6)\nolt = \u03c3(W l xox l t +W l hoh l t\u22121 +W l coc l t + b l o) (7)\nrlt = o l t \u00b7 tanh(clt) (8)\nhlt = W l p \u00b7 rlt (9)\nl represents layer index and ilt, f l t and o l t are input, forget and output gates respectively. They are component-wise multiplied by input, memory cell and hidden output to gradually open or close their connections. xlt is an input from (l \u2212 1) th layer (or an input to a network when l is 1), hlt\u22121 is a l th layer output at time t \u2212 1 and clt\u22121 is an internal cell state at t \u2212 1. W lp is a projection matrix to reduce dimension of hlt.\n5"}, {"heading": "D. Highway LSTM", "text": "Highway LSTM [13], [15] reused LSTM internal memory cells for spatial domain highway connections between stacked LSTM layers. Equations (4), (5), (7), (8), and (9) do not change for a highway LSTM. Equation (6) is updated to add a highway connection:\nclt = d l t \u00b7 cl\u22121t + f lt \u00b7 clt\u22121+\nilt \u00b7 tanh(W lxcxlt +W lhchlt\u22121 + blc) (10)\ndlt = \u03c3(W l xdx l t +W l cdc l t\u22121 + w l cdc l\u22121 t + b l d) (11)\nWhere dlt is a depth gate that connects c l\u22121 t in the (l \u2212 1)th layer to clt in the lth layer. [13] showed that an acoustic model based on the highway LSTM improved far-field speech recognition compared with a plain LSTM. However, [13] also showed that word error rate (WER) degraded when the number of layers in the highway LSTM increases from 3 to 8."}, {"heading": "III. RESIDUAL LSTM", "text": "In this section, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. Residual LSTM starts with an intuition that the separation of a spatial-domain shortcut path with a temporal-domain cell update may give better flexibility to deal with vanishing or exploding gradients. Unlike a highway LSTM, residual LSTM does not accumulate a highway path on an internal memory clt. Instead, a shortcut path is added to an LSTM output layer.\nFor a matched dimension, Equation (14) can be changed into:\nhlt = o l t \u00b7 (mlt + xlt) (15)\n6 Since a highway path is always turned on for a residual LSTM, there should be a scaling parameter on the main path output. For example, linear filters in the last CNN layer of a residual network are reused to scale the main path output. For a residual LSTM, a projection matrix W lp is reused in order to scale the LSTM output. Consequently, the number of paramemters for a residual LSTM does not increase compared with a plain LSTM. Simple complexity comparison between residual LSTM and highway LSTM is as follows. If the size of the internal memory cells is N and the output layer dimension after projection is N/2, the total number of reduced parameters for a residual LSTM becomes N2/2 + 4N . For example, if N is 1024 and the number of layers is more than 5, residual LSTM has approximately 10% less network parameters compared with a highway LSTM.\nOne thing to note is that a highway path should be scaled by an output gate as in Equation (14). Empirical experiments without an output gate presented significant performance loss. This is because highway paths are accumulated as the number of layers increases. Without proper scaling, the variance of an LSTM output keeps increasing as the number of layers grows.\nOutput gate is a trainable network which can learn a proper range of an LSTM output. For\nexample, if an output gate is set as 1\u221a 2 , an lth layer output becomes\nhlt = l\u2211\nk=1\n( 1\u221a 2 )(l\u2212k+1)mkt + ( 1\u221a 2 )lxt (16)\nWhere, xt is an input to LSTM at time t. If mlt and xt are independent each other for all l and have fixed variance of 1, regardless of layer index l, the variance of layer lth output becomes 1. Since variance of a layer output is variable in the real scenario, a trainable output gate will better deal with exploding variance than a fixed scaling factor.\nIV. EXPERIMENTS"}, {"heading": "A. Experimental Setup", "text": "AMI meeting corpus [16] is used to train and evaluate a residual LSTM. AMI corpus consists of 100 hours of meeting recordings. For each meeting, three to four people have free conversation in English. Frequently, overlapped speaking from multiple speakers happens and for that case, the training transcript always follows a main speaker. Multiple microphones are used to synchronously record conversations in different environments. Individual headset microphone (IHM) recorded clean close-talking coversation and single distant microphone (SDM) recorded\nfar-field noisy conversation. In this paper, SDM is used to train a residual LSTM at Section IV-B and IV-C and combined SDM and IHM corpora are used at Section IV-D.\nKaldi [17] is a toolkit for speech recognition that is used to train a context-dependent LDAMLLT-GMM-HMM system. The trained GMM-HMM generates forced aligned labels which are later used to train a neural network-based acoustic model. Three neural network-based acoustic models are trained: plain LSTM without any shortcut path, highway LSTM and residual LSTM. All three LSTM networks have 1024 memory cells and 512 output nodes for experiments at Section IV-B, IV-C and IV-D.\nThe computational network toolkit (CNTK) [18] is used to train and decode three acoustic models. Truncated back-propagation through time (BPTT) is used to train LSTM networks with 20 frames for each truncation. For parallel processing, 40 utterances are simultaneously processed to update parameters. Cross-entropy loss function is used with L2 regularization.\nFor decoding, reduced 50k-word fisher dictionary is used for lexicon and based on this lexicon, tri-gram language model is interpolated from AMI training transcript. As a decoding option, word error rate (WER) can be calculated based on non-overlapped speaking or overlapped speaking. Recognizing overlapped speaking is to decode up to 4 concurrent speeches. Decoding overlapped speaking is a big challenge considering a network is trained to only recognize a main speaker. Following sections will provide WERs for both options.\n8"}, {"heading": "B. Training Performance with increasing Depth", "text": "Figure 2 compares phone error rates (PER) with increasing depth. Cross-validation (CV) PER is measured with a separate data that is not used for regression. Both training and CV PERs are shown in the plots. Figure 2 does not include a plain LSTM due to a limited space. Its PERs were always worse than those for highway or residual LSTM.\nFigure 2a shows PERs for a highway LSTM. Increasing depth from 3 to 5 does not show much difference in both training and CV PERs. However, 10-layer highway LSTM showed significant degradation. The converged training PER is much higher than 3 or 5-layer models in spite of increased complexity. Since CV PER also degraded, loss from training PER did not come from better generalization. Therefore, training loss is purely due to increased depth. For a 10-layer highway LSTM, it has 3.6% CV PER loss and 15% training PER loss compared with 3-layer highway LSTM.\nFigure 2b shows PERs for a residual LSTM. CV PERs consistently get better with increasing depth. For a training PER, it is slightly degraded for a 10-layer network. However, it is not a training loss from increased depth because CV PER for a 10-layer network improved. Specifically, the regularization weight for a residual LSTM was set to be 2-4 times higher than that of a highway LSTM because a residual LSTM is more susceptible to overfitting. Smaller regularization weights can easily decrease training PER but generalization performance will be much worse."}, {"heading": "C. WER Evaluation with SDM corpus", "text": "Table I compares WER for LSTM, highway LSTM and residual LSTM with increasing depth. All three networks were trained by SDM AMI corpus. Both overlapped and non-overlapped WERs are shown. For each layer, internal memory cell size is set to be 1024 and output node size is fixed as 512. A plain LSTM performed worse with increasing layers. Especially, the 10- layer LSTM degraded up to 13.7% over the 3-layer LSTM for non-overlapped WER. A highway LSTM showed better performance over a plain LSTM but still could not avoid degradation with increasing depth. The 10-layer highway LSTM presented 6.2% increase in WER over the 3-layer network.\nOn the contrary, a residual LSTM improved with increasing layers. 5-layer and 10-layer residual LSTMs have 1.2% and 2.2% WER reduction over the 3-layer network, respectively.\n9 5 10 15 20 25 30 35\nEpoch\n0.4\n0.5\n0.6\n0.7 0.8 P E R\nTraining/CV PER Curve\nhighway LSTM 3l,Train highway LSTM 5l,Train highway LSTM 10l,Train highway LSTM 3l,CV highway LSTM 5l,CV highway LSTM 10l,CV\n(a)\n5 10 15 20 25 30 35\nEpoch\n0.4\n0.5\n0.6\n0.7\n0.8\nP E\nR\nTraining/CV PER Curve\nresidual LSTM 3l,Train residual LSTM 5l,Train residual LSTM 10l,Train residual LSTM 3l,CV residual LSTM 5l,CV residual LSTM 10l,CV\n(b)\nFig. 2. Training and cross validation (CV) phone error rates (PER) for each epoch on AMI SDM corpus. (a) shows phone error rates for a highway LSTM. When depth increases from 3 to 10, both training and cross validation errors severely degraded. (b) shows phone error rates for a residual LSTM. Training/CV PER does not show significant degradation. On the contrary, CV PER got improved with 10-layer model.\nThe 10-layer residual LSTM showed the lowest 41.0% WER, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and highway LSTMs."}, {"heading": "D. WER Evaluation with SDM and IHM corpora", "text": "Table II compares WER of highway and residual LSTMs trained with combined IHM and SDM corpora. With increased corpus size, the best performing configuration for a highway LSTM is changed into 5-layer with 40.7% WER. However, 10-layer highway LSTM still suffered from training loss from increased depth: 6.6% increase in WER (non-overlapped). On the contrary, 10-layer residual LSTM showed the best WER of 39.3%, which corresponds to 3.1% WER reduction (non-overlapped) over the 5-layer one, whereas the prior experiment trained only by SDM corpus presented 1% improvement. Increasing training data provides larger gain from a deeper network. Residual LSTM enabled to train a deeper LSTM network without any training loss."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a novel architecture for a deep recurrent neural network: residual LSTM. A residual LSTM provides a shortcut path between adjacent layer outputs. Unlike a highway network, a residual LSTM does not assign dedicated gate networks for a shortcut\n10\nconnection. Instead, projection matrix and output gate are reused for a shortcut connection, which provides roughly 10% reduction of network parameters compared with a highway LSTM. Experiments on AMI corpus showed that a residual LSTM improved significantly with increasing depth, meanwhile 10-layer plain and highway LSTMs severely suffered from training loss."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced.<lb>A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It<lb>also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain.<lb>The proposed residual LSTM architecture provides an additional spatial shortcut path from lower layers<lb>for efficient training of deep networks with multiple LSTM layers. Compared with the previous work,<lb>highway LSTM, residual LSTM reuses the output projection matrix and the output gate of LSTM to<lb>control the spatial information flow instead of additional gate networks, which effectively reduces more<lb>than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus<lb>indicates that the performance of plain and highway LSTM networks degrades with increasing network<lb>depth. For example, 10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in<lb>WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided<lb>the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and<lb>highway LSTM networks, respectively. Training with both the IHM and SDM corpora, the residual<lb>LSTM architecture provided larger gain from increasing depth: a 10-layer residual LSTM showed 3.0%<lb>WER reduction over the corresponding 5-layer one.", "creator": "LaTeX with hyperref package"}}}