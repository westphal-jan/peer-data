{"id": "1701.03924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "QCRI Machine Translation Systems for IWSLT 16", "abstract": "This paper describes QCRI's machine translation systems for the IWSLT 2016 evaluation campaign: We participated in the Arabic & gt; English and English & gt; Arabic translation campaign and developed both phrase-based and neural machine translation models to see if the newly created NMT framework outperforms traditional phrase systems in Arabic-English language pairs; we trained a very strong phrase system, which includes a large language model, the Operation Sequence Model, Neural Network Joint Model and class-based models, along with various domain matching techniques such as MML filtering, blending modelling and fine-tuning over the NNJM model; however, a neural MT system trained by fine-tuning and applying ensembles over 8 models by stacking data from different genres hit key EU BL2 points in English.", "histories": [["v1", "Sat, 14 Jan 2017 14:18:54 GMT  (31kb)", "http://arxiv.org/abs/1701.03924v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nadir durrani", "fahim dalvi", "hassan sajjad", "stephan vogel"], "accepted": false, "id": "1701.03924"}, "pdf": {"name": "1701.03924.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel"], "emails": ["ndurrani@qf.org.qa", "faimaduddin@qf.org.qa", "hsajjad@qf.org.qa", "svogel@qf.org.qa"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 92\n4v 1\n[ cs\n.C L\n] 1\n4 Ja\nn 20\n17"}, {"heading": "1. Introduction", "text": "We describe QCRI\u2019s phrase-based and Neural MT systems. We participated in the Arabic-to-English and English-toArabic MT tracks. Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.\nAn interesting challenge associated with the IWSLT campaign is the problem of domain adaptation. The in-domain data based on TED talks is available in very little quantity compared to the out-domain UN corpus [4], which has been found to be harmful previously when simply concatenated to the training [5]. In this year\u2019s IWSLT, two additional data resources Opus subtitles [6] and the QED corpus [7] were introduced. The latter was also used as an official test-set. Therefore apart from exploring phrase-based versus Neural MT, we geared ourselves towards adapting our system towards TED and QED talks in this multi-domain scenario. With these goals in mind we re-explored both model weighting and data filtering techniques, in these new data settings. Below we itemize the most successful attributes of our\nphrase-based system:\n\u2022 We applied MML-based data selection [8] to the UN and Open Sub-title data, with the goals of filtering out harmful data.\n\u2022 We trained OSM models [9] on separate corpora, and interpolated them [10] by optimizing perplexity on the tuning-set. We also tried this on the OSM models trained on the word classes [11].\n\u2022 We tried the fine-tuning method of training the NNJM model on the out-domain data and fine-tuning with the in-domain TED data [12].\n\u2022 We trained big language models using all the English mono data available from the WMT campaign and giga word corpus for Arabic.\nWe trained our Neural MT system using the Nematus toolkit. We used Bidirectional RNN\u2019s for the encoder, 1024 LSTM units, and a word embedding size of 500. Below we itemize what worked when training the neural MT system:\n\u2022 We trained our baseline model on all of the UN corpus, then continued training with the Open subtitles corpus, and finally fine-tuned with the in-domain data\n\u2022 We fine-tuned all of our models without freezing any layers in the network, since we had sufficient amount of data to train on.\n\u2022 We used dropout when fine-tuning with in-domain data, since it is relatively small compared to the UN and Open subtitle data.\n\u2022 We trained our final models with an ensemble of the last eight models, where each model was fine-tuned with the in-domain data.\nFinally we applied system combination over the outputs of best Neural MT and phrase-based systems using MEMT [13]. Our efforts were mainly focused towards the AR\u2192EN TED task. In the end we just replicated our best system for the EN\u2192AR direction and the QED task. For our best Neural MT system, we were unable to use an ensemble in the EN\u2192AR direction, since we could not train several comparable models to combine."}, {"heading": "2. Data Settings and Pre/Post Processing", "text": "We trained our systems using the data made available through IWSLT 2016 campaign. This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6]. The statistics are shown in Table 1. For language model we trained using the target side of the parallel corpus and all the available English data from the recent WMT campaign [15], and GigaWord and OPUS mono corpus for Arabic.\nWe segmented Arabic data using both MADAMIRA and Farasa. We found MADAMIRA [16] performed 0.1 BLEU points better than Farasa [17] (See Table 2) and decided to use it for the competition. We tokenized the English side using standard tokenizer of Moses. For English\u2192Arabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer [5]."}, {"heading": "3. Phrase-based System", "text": ""}, {"heading": "3.1. Baseline Settings", "text": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24]. We used default distortion limit, 100- best translation options, phrase-length of 5, monotone-overpunctuation heuristic, cube-pruning with a limit of 1000 during tuning 5000 during test. We used k-best batch MIRA [25] for tuning. We used cased BLEU [26] to measure progress."}, {"heading": "3.2. Data Selection", "text": "Due to our experience from previous competitions, we were wary of the fact that simply adding the UN data is harmful for the AR\u2192 MT system, we therefore selected data through MML filtering [8]. We selected 2.5%, 3.75%, 5%, 10% and 30% of the UN data and trained MT pipeline by concatenating the selected data with the in-domain data. We did not include Opus data (40 Million Sentences) and NNJM in these experiments to get the results quickly. Table 3 shows the results. We found 3.75% (\u2248685k sentences) to be the optimal threshold. Alternative to data selection, we tried training in-\nand out-domain phrase-tables separately and using the outdomain phrase-table only as a back-off. Second last row of Table 3 shows results. While it gave improvement on top of the baseline system, it was slightly behind MML filtering.\nWe then tried to find optimal cut-off on the OPUS data, and selected 20 Million sentences (half of the Opus). Our best systems used 3.75% of the UN data and half of the Opus data. Adding the selected Opus data gave an average improvement of +1.2 BLEU points."}, {"heading": "3.3. Language Model", "text": "We trained bigger language model by using all the available English data from the recent WMT campaign1 and targetside of the parallel data. A Kneser-Ney smoothed 5-gram language model was trained on each sub-corpus individually and then interpolated to minimize perplexity on the target part of the monolingual data. We were able to obtain a gain of +0.5 using bigger language model. See Table 4."}, {"heading": "3.4. Interpolation of Operation Sequence Models", "text": "The OSM model has been a regular feature of the phrasebased pipeline [27] in the competition grade systems. It is a joint sequence translation model which integrates reordering. [10] recently found that an OSM model trained on plain concatenation of data is sub-optimal and can be improved by training OSM models on each domain individually and interpolating them by minimizing perplexity on the in-domain tune-set. Table 5 shows that using interpolated OSM model (OSMi) instead of the one trained on plain concatenation (OSMc) gives an average improvement of +0.6 BLEU points."}, {"heading": "3.5. NNJM Adaptation", "text": "We also explored the award winning Neural Network Joint Model (NNJM) in our pipeline and tried to adapt it towards the in-domain data. We trained an NNJM models on the UN and Opus data for 25 epochs and then fine-tuned [12] it by\n1http://www.statmt.org/wmt16/translation-task.html\nrunning for 25 more epochs on the in-domain data. Because the data is huge, the entire training took 1.5 months of wallclock time. Table 6 shows results. The NNJM model gave significant improvement (+0.6) on top of baseline which does not include it. We found fine-tuning method to give slight gains (+0.2) when the baseline model was trained on the Opus data. On the contrary, fine-tuning did not help when the model trained was on UN."}, {"heading": "3.6. Class-based Models", "text": "We explored the use of automatic word clusters in phrasebased models [11]. We used 50 classes, obtained by running mkcls. The clusters ids were included in the phrasetable. We additionally trained in-domain language model using word-classes and interpolated OSM on word-classes. But we only saw very small improvements using word classes."}, {"heading": "3.7. Handling Unknown Words", "text": "We tried to handle OOV words using drop-oov and through transliteration [28, 29]. The former worked slightly better and was used in the best system. Of course the gains from the two methods are additive because they are addressing different OOVs, but there\u2019s no good way to automatically find which word to drop and which one to transliterate."}, {"heading": "3.8. Final System", "text": "Table 9 shows incremental progress on this Arabic\u2192English language pair. Our best system included MML selected UN and Opus corpora, big language model, interpolated OSM and fine-tuned NNJM models. We we used drop-oov option to handle unknown words."}, {"heading": "3.9. English-to-Arabic Systems", "text": "We did not do detailed experiments for the English\u2192Arabic direction because of computational limitations, but simply replicated what worked for the Arabic\u2192English direction.\nTable 10 shows progress on this language pair. The baseline system (ID) was trained on the the TED data and target side of all the permissible parallel data. In the second row, we added all the parallel data except for the UN. In the third row we additionally added the UN data that we selected in the Arabic\u2192English direction. Additional parallel data gives an average improvement of +1.4 BLEU point. Then we added an NNJM model trained on in-domain TED data on top of this system to improve it by +0.8. Adding GigaWord and monolingual OPUS data (another 20M Sentences other than the target-side of the parallel data) gave an improvement of +0.3. Finally we replaced the baseline NNJM with the one trained on OPUS data and fine-tuned with the in-domain data to get our best system."}, {"heading": "3.10. QED Systems", "text": "We simply replicated QED systems by replacing QED corpus to be in-domain data, instead of TED data. We used the same UN data that we selected for our Arabic\u2192English system, therefore our phrase-tables remain the same. The main changes are caused when training adapted OSM and NNJM models. For NNJM we simply fine tune with QED corpus instead of the TED corpus. For interpolated OSM, we concatenated TED and QED corpus and build OSM on it, which is then interpolated with the OSM models trained on the selected UN and Opus data. We used IWSLT tuning to get the interpolation weights. This way the OSM sub-model\nSystem ted-11 ted-12 ted-13 ted-14 Avg 30%+FT(TED) 27.2 31.4 30.8 27.1 29.1 30%+TED+FT(TED) 27.2 30.8 30.1 25.8 28.5\nTable 11: Fine Tuning on Out-domain versus Concatenation \u2013 Models run for 3 epochs\ncreated from TED+QED corpus gets best weights. We also retrained the language model in this similar fashion. We used the tuning weights obtained from our best TED systems and replaced the TED adapted OSM, NNJM and language models with their QED adapted variants."}, {"heading": "4. Neural Machine Translation", "text": ""}, {"heading": "4.1. Pre/Post-processing", "text": "We used a similar pre/post-processing pipeline for Neural MT as our phrase-based systems (Section 2), and additionally applied BPE [30] before training them. Our BPE models are trained separately for both the Arabic and English datasets instead of jointly training them, since the character set differs between the languages. We limited the number of operations to 59,500, as suggested in [30]. We experimented with BPE models trained on the TED data, and on the concatenation of the TED and out-domain data. We did not see any considerable difference in performance between these models. Thus we used the BPE model trained on the TED data for the experiments reported in this paper."}, {"heading": "4.2. Baseline", "text": "We used default parameters in Nematus to train our systems: a batch size of 80, source and target vocabulary of 50K entries each, 1024 LSTM units, and the embedding layer size of 500. Baseline system were trained using only TED corpus."}, {"heading": "4.3. Fine Tuning on Concatenation versus OD", "text": "The best phrase based systems are usually trained by concatenating in and out-domain data. On the other hand, deep learning systems are trained on the out-domain data first, and then fine-tuned with in-domain data. We experimented with both strategies. In the interest of time we selected 30% of the UN data using MML filtering (Table 3). We trained two systems, one by concatenating the in-domain data with the selected (30%) UN data and other just on the selected data. Then we fine-tuned both the models with the in-domain TED data after running them for 3 epochs. Table 11 shows that fine-tuning a system trained on out-domain data only, outperforms the system fine-tuned on concatenation."}, {"heading": "4.4. Fine-tuning Variants and Dropouts", "text": "The default version of Nematus applies fine-tuning by freezing the weights of embedding layer. The intuition behind freezing a layer is to not allow the weights in that layer to change with additional data. This is sometimes useful when we can learn certain layers better from out-domain data. One\nsuch layer in our case is the word embedding layer. We tried a variation in which we do not freeze any layer. This latter variant was found to outperform the default setting (See Table 12).\nDropouts are found to be useful in NN training, when the training data is small. We experimented with using dropouts in our experiments, but did not find any significant difference. Hence we decided to use it only when fine-tuning with the in-domain data (TED/QED), since both of the other datasets (UN and OPUS) were big and did not pose any risk of inducing the problem of overfitting."}, {"heading": "4.5. Data Selection", "text": "Since we found data selection useful in the phrase based system, we also trained our neural systems using 5%, 30% and 100% of the UN data. In these experiments, we concatenated the 5% and 30% of the UN data with the in-domain data. To evaluate the most promising models, we trained all of the models until the learning plateaued, and then fine-tuned these models with in-domain data.2 The results are shown in in Table 13. Using only 5% of the data proved harmful, and the system did not generalize as well as the other models. The model trained on 30% of the data performed better than the model trained on all the data, by 0.7 BLEU points.\nIn our subsequent experiments we tried to verify if this finding holds when we add the OPUS data. We therefore trained two systems by fine-tuning 30% selected UN data or full UN data using OPUS. Here the results flipped and the we found that model that used all of the UN data performed better (Compare last two rows in Table 13). Therefore, we decided to focus our efforts on the model trained on the entire UN data for all of the following experiments."}, {"heading": "4.6. Ensemble", "text": "Ensembling models has shown to give a consistent boost in performance in past best performing systems [3]. We therefore experimented with several variations. We found the best\n2Because we were running experiments in parallel, we were not aware at this point that fine-tuning on out-domain is a better strategy\nperforming combination by fine-tuning the last eight models of the UN+OPUS system, and then ensemble these eight fine-tuned models. Performance improvements from the ensemble are shown in Table 14. The second row shows systems when we fine tune our best system in Table 13 with the in-domain TED data. In the last row we perform ensemble."}, {"heading": "4.7. Final System", "text": "Our final system was trained by first using all of the UN data. We then continued training on OPUS data. Once learning had plateaued on the OPUS data, we took the last eight models which were very similar in performance, and fine-tuned each of the them using TED data. We then combined these eight fine-tuned models in an ensemble as our final system. The progress is shown in Table 15. We used the same strategy for the QED systems by fine-tuning the last eight OPUS models with QED data, and combining these in an ensemble."}, {"heading": "4.8. English-to-Arabic Systems", "text": "We used insights gained from our Arabic-to-English system experiments to train our English\u2192Arabic systems. Our final model for both TED and QED was first trained on all of the UN data, followed by the OPUS data, and finally fine-tuned with the in-domain data. The progress is shown in Table 16."}, {"heading": "5. System Combination", "text": "We combined hypotheses produced by our best Phrase-based and Neural MT systems. For this purpose we used MultiEngine MT system, or MEMT [13]. The results are shown in Table 17. We did not gain any substantial improvements using system combination. Small improvements were obtained in the Arabic\u2192English direction baring test-2012. On\nthe contrary significant improvement was obtained only in test-2013 in the English\u2192Arabic direction. Table 18 shows results on the official test-sets."}, {"heading": "6. Summary", "text": "We trained a very strong phrase-based system with SOTA features such as OSM, NNJM and big LM. The system improved greatly by applying domain adaptation. To this end we applied MML-based filtering, interpolated OSM and finetuning of NNJM models. Overall, our phrase-based system achieved a gain of 4 BLEU points on top of the baseline system. We also applied data selection for training our NMT. However, the NMT systems quickly overfit and did not perform well. Our experiments showed that the NMT system trained on the full UN data performed best, and the final NMT system made use of all the available out-of-domain data. However, the training was performed incrementally, starting with UN data for 50k iterations, fine tuned on OPUS for 25k more iterations and then fine tuned the final model using TED talks for a few iterations. We simply replicated our settings to train QED systems. Finally we applied system combination of the two systems using MEMT.\nWhile it is computationally expensive, we found training a neural MT system much simpler than a competitive phrase-based system, where a lot of sub-components need to be optimized independently to reach the best configuration. On the contrary, an NMT system requires least supervision. Secondly once a neural system is trained, the effort can be easily reused to adapt the system towards another domain, as in this case we simply fine-tuned our UN+OPUS system with the QED corpus. On the contrary, almost all the subcomponent of a phrase-based system had to be retrained to adapt the system towards QED corpus."}, {"heading": "7. References", "text": "[1] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst, \u201cMoses: Open source toolkit for statistical machine translation,\u201d in Proceedings of the Association for Computational Linguistics (ACL\u201907), Prague, Czech Republic, 2007.\n[2] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d in ICLR, 2015. [Online]. Available: http://arxiv.org/pdf/1409.0473v6.pdf\n[3] R. Sennrich, B. Haddow, and A. Birch, \u201cEdinburgh neural machine translation systems for wmt 16,\u201d in Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 371\u2013376. [Online]. Available: http://www.aclweb.org/anthology/W16-2323\n[4] M. Ziemski, M. Junczys-Dowmunt, and B. Pouliquen, \u201cThe united nations parallel corpus v1.0,\u201d in Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoroz\u030c, Slovenia, May 23-28, 2016, 2016.\n[5] H. Sajjad, F. Guzmn, P. Nakov, A. Abdelali, K. Murray, F. A. Obaidli, and S. Vogel, \u201cQCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation,\u201d in Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.\n[6] P. Lison and J. Tiedemann, \u201cOpensubtitles2016: Extracting large parallel corpora from movie and tv subtitles,\u201d in Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA), may 2016.\n[7] A. Abdelali, F. Guzman, H. Sajjad, and S. Vogel, \u201cThe AMARA corpus: Building parallel language resources for the educational domain,\u201d in Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, May 2014.\n[8] A. Axelrod, X. He, and J. Gao, \u201cDomain adaptation via pseudo in-domain data selection,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP \u201911, Edinburgh, United Kingdom, 2011.\n[9] N. Durrani, H. Schmid, and A. Fraser, \u201cA joint sequence translation model with integrated reordering,\u201d in Proceedings of the Association for Computational\nLinguistics: Human Language Technologies (ACLHLT\u201911), Portland, OR, USA, 2011.\n[10] N. Durrani, H. Sajjad, S. Joty, A. Abdelali, and S. Vogel, \u201cUsing joint models for domain adaptation in statistical machine translation,\u201d in Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV). Florida, USA: AMTA, November 2015.\n[11] N. Durrani, P. Koehn, H. Schmid, and A. Fraser, \u201cInvestigating the usefulness of generalized word representations in smt,\u201d in Proceedings of the 25th Annual Conference on Computational Linguistics, ser. COLING\u201914, Dublin, Ireland, 2014, pp. 421\u2013432.\n[12] M.-T. Luong and C. D. Manning, \u201cStanford neural machine translation systems for spoken language domain,\u201d in International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015.\n[13] K. Heafield and A. Lavie, \u201cCMU system combination in WMT 2011,\u201d in Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 145\u2013151. [Online]. Available: http://kheafield.com/professional/avenue/wmt 2011.pdf\n[14] F. Guzma\u0301n, H. Sajjad, S. Vogel, and A. Abdelali, \u201cThe AMARA corpus: Building resources for translating the web\u2019s educational content,\u201d in Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.\n[15] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri, \u201cFindings of the 2016 conference on machine translation,\u201d in Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 131\u2013198. [Online]. Available: http://www.aclweb.org/anthology/W/W16/W16-2301\n[16] A. Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy, R. Eskander, N. Habash, M. Pooleery, O. Rambow, and R. M. Roth, \u201cMADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of Arabic,\u201d in Proceedings of the Language Resources and Evaluation Conference, ser. LREC \u201914, Reykjavik, Iceland, 2014, pp. 1094\u20131101.\n[17] A. Abdelali, K. Darwish, N. Durrani, and H. Mubarak, \u201cFarasa: A fast and furious segmenter for arabic,\u201d in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. San\nDiego, California: Association for Computational Linguistics, June 2016, pp. 11\u201316. [Online]. Available: http://www.aclweb.org/anthology/N16-3003\n[18] A. Birch, M. Huck, N. Durrani, N. Bogoychev, and P. Koehn, \u201cEdinburgh SLT and MT system description for the IWSLT 2014 evaluation,\u201d in Proceedings of the 11th International Workshop on Spoken Language Translation, ser. IWSLT \u201914, Lake Tahoe, CA, USA, 2014.\n[19] C. Dyer, V. Chahuneau, and N. A. Smith, \u201cA simple, fast, and effective reparameterization of ibm model 2,\u201d in Proceedings of NAACL\u201913, 2013.\n[20] K. Heafield, \u201cKenLM: Faster and Smaller Language Model Queries,\u201d in Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 187\u2013197. [Online]. Available: http://kheafield.com/professional/avenue/kenlm.pdf\n[21] M. Galley and C. D. Manning, \u201cA Simple and Effective Hierarchical Phrase Reordering Model,\u201d in Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, October 2008, pp. 848\u2013856. [Online]. Available: http://www.aclweb.org/anthology/D08-1089\n[22] N. Durrani, H. Schmid, A. Fraser, P. Koehn, and H. Schu\u0308tze, \u201cThe Operation Sequence Model \u2013 Combining N-Gram-based and Phrase-based Statistical Machine Translation,\u201d Computational Linguistics, vol. 41, no. 2, pp. 157\u2013186, 2015.\n[23] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul, \u201cFast and robust neural network joint models for statistical machine translation,\u201d in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014.\n[24] S. Joty, H. Sajjad, N. Durrani, K. Al-Mannai, A. Abdelali, and S. Vogel, \u201cHow to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models,\u201d in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, September 2015.\n[25] C. Cherry and G. Foster, \u201cBatch tuning strategies for statistical machine translation,\u201d in Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, ser. NAACL-HLT \u201912, Montre\u0301al, Canada, 2012.\n[26] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBLEU: A Method for Automatic Evaluation of Machine Translation,\u201d in Proceedings of the 40th Annual\nMeeting on Association for Computational Linguistics, ser. ACL \u201902, Morristown, NJ, USA, 2002, pp. 311\u2013 318.\n[27] N. Durrani, A. Fraser, H. Schmid, H. Hoang, and P. Koehn, \u201cCan markov models over minimal translation units help phrase-based smt?\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sofia, Bulgaria: Association for Computational Linguistics, August 2013, pp. 399\u2013405. [Online]. Available: http://www.aclweb.org/anthology/P13-2071\n[28] H. Sajjad, A. Fraser, and H. Schmid, \u201cA statistical model for unsupervised and semi-supervised transliteration mining,\u201d in Proceedings of the Association for Computational Linguistics (ACL\u201912), Jeju, Korea, 2012.\n[29] N. Durrani, H. Sajjad, H. Hoang, and P. Koehn, \u201cIntegrating an unsupervised transliteration model into statistical machine translation,\u201d in Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014), Gothenburg, Sweden, April 2014.\n[30] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural machine translation of rare words with subword units,\u201d in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 1715\u20131725. [Online]. Available: http://www.aclweb.org/anthology/P16-1162"}], "references": [{"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the Association for Computational Linguistics (ACL\u201907), Prague, Czech Republic, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR, 2015. [Online]. Available: http://arxiv.org/pdf/1409.0473v6.pdf", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 371\u2013376. [Online]. Available: http://www.aclweb.org/anthology/W16-2323", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M. Ziemski", "M. Junczys-Dowmunt", "B. Pouliquen"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoro\u017e, Slovenia, May 23-28, 2016, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation", "author": ["H. Sajjad", "F. Guzmn", "P. Nakov", "A. Abdelali", "K. Murray", "F.A. Obaidli", "S. Vogel"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["P. Lison", "J. Tiedemann"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA), may 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The AMARA corpus: Building parallel language resources for the educational domain", "author": ["A. Abdelali", "F. Guzman", "H. Sajjad", "S. Vogel"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, May 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation via pseudo in-domain data selection", "author": ["A. Axelrod", "X. He", "J. Gao"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP \u201911, Edinburgh, United Kingdom, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A joint sequence translation model with integrated reordering", "author": ["N. Durrani", "H. Schmid", "A. Fraser"], "venue": "Proceedings of the Association for Computational  Linguistics: Human Language Technologies (ACL- HLT\u201911), Portland, OR, USA, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Using joint models for domain adaptation in statistical machine translation", "author": ["N. Durrani", "H. Sajjad", "S. Joty", "A. Abdelali", "S. Vogel"], "venue": "Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV). Florida, USA: AMTA, November 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating the usefulness of generalized word representations in smt", "author": ["N. Durrani", "P. Koehn", "H. Schmid", "A. Fraser"], "venue": "Proceedings of the 25th Annual Conference on Computational Linguistics, ser. COLING\u201914, Dublin, Ireland, 2014, pp. 421\u2013432.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Stanford neural machine translation systems for spoken language domain", "author": ["M.-T. Luong", "C.D. Manning"], "venue": "International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "CMU system combination in WMT 2011", "author": ["K. Heafield", "A. Lavie"], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 145\u2013151. [Online]. Available: http://kheafield.com/professional/avenue/wmt 2011.pdf", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "The AMARA corpus: Building resources for translating the web\u2019s educational content", "author": ["F. Guzm\u00e1n", "H. Sajjad", "S. Vogel", "A. Abdelali"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Findings of the 2016 conference on machine translation", "author": ["O. Bojar", "R. Chatterjee", "C. Federmann", "Y. Graham", "B. Haddow", "M. Huck", "A. Jimeno Yepes", "P. Koehn", "V. Logacheva", "C. Monz", "M. Negri", "A. Neveol", "M. Neves", "M. Popel", "M. Post", "R. Rubino", "C. Scarton", "L. Specia", "M. Turchi", "K. Verspoor", "M. Zampieri"], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 131\u2013198. [Online]. Available: http://www.aclweb.org/anthology/W/W16/W16-2301", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of Arabic", "author": ["A. Pasha", "M. Al-Badrashiny", "M. Diab", "A. El Kholy", "R. Eskander", "N. Habash", "M. Pooleery", "O. Rambow", "R.M. Roth"], "venue": "Proceedings of the Language Resources and Evaluation Conference, ser. LREC \u201914, Reykjavik, Iceland, 2014, pp. 1094\u20131101.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Farasa: A fast and furious segmenter for arabic", "author": ["A. Abdelali", "K. Darwish", "N. Durrani", "H. Mubarak"], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. San  Diego, California: Association for Computational Linguistics, June 2016, pp. 11\u201316. [Online]. Available: http://www.aclweb.org/anthology/N16-3003", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Edinburgh SLT and MT system description for the IWSLT 2014 evaluation", "author": ["A. Birch", "M. Huck", "N. Durrani", "N. Bogoychev", "P. Koehn"], "venue": "Proceedings of the 11th International Workshop on Spoken Language Translation, ser. IWSLT \u201914, Lake Tahoe, CA, USA, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Proceedings of NAACL\u201913, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["K. Heafield"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 187\u2013197. [Online]. Available: http://kheafield.com/professional/avenue/kenlm.pdf", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A Simple and Effective Hierarchical Phrase Reordering Model", "author": ["M. Galley", "C.D. Manning"], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, October 2008, pp. 848\u2013856. [Online]. Available: http://www.aclweb.org/anthology/D08-1089", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "The Operation Sequence Model \u2013 Combining N-Gram-based and Phrase-based Statistical Machine Translation", "author": ["N. Durrani", "H. Schmid", "A. Fraser", "P. Koehn", "H. Sch\u00fctze"], "venue": "Computational Linguistics, vol. 41, no. 2, pp. 157\u2013186, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models", "author": ["S. Joty", "H. Sajjad", "N. Durrani", "K. Al-Mannai", "A. Abdelali", "S. Vogel"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, September 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["C. Cherry", "G. Foster"], "venue": "Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, ser. NAACL-HLT \u201912, Montr\u00e9al, Canada, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th Annual  Meeting on Association for Computational Linguistics, ser. ACL \u201902, Morristown, NJ, USA, 2002, pp. 311\u2013 318.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Can markov models over minimal translation units help phrase-based smt?\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["N. Durrani", "A. Fraser", "H. Schmid", "H. Hoang", "P. Koehn"], "venue": "Sofia, Bulgaria: Association for Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A statistical model for unsupervised and semi-supervised transliteration mining", "author": ["H. Sajjad", "A. Fraser", "H. Schmid"], "venue": "Proceedings of the Association for Computational Linguistics (ACL\u201912), Jeju, Korea, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating an unsupervised transliteration model into statistical machine translation", "author": ["N. Durrani", "H. Sajjad", "H. Hoang", "P. Koehn"], "venue": "Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014), Gothenburg, Sweden, April 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 1715\u20131725. [Online]. Available: http://www.aclweb.org/anthology/P16-1162", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 226, "endOffset": 229}, {"referenceID": 2, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 288, "endOffset": 291}, {"referenceID": 3, "context": "The in-domain data based on TED talks is available in very little quantity compared to the out-domain UN corpus [4], which has been found to be harmful previously when simply concatenated to the training [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "The in-domain data based on TED talks is available in very little quantity compared to the out-domain UN corpus [4], which has been found to be harmful previously when simply concatenated to the training [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "In this year\u2019s IWSLT, two additional data resources Opus subtitles [6] and the QED corpus [7] were introduced.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "In this year\u2019s IWSLT, two additional data resources Opus subtitles [6] and the QED corpus [7] were introduced.", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "\u2022 We applied MML-based data selection [8] to the UN and Open Sub-title data, with the goals of filtering out harmful data.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "\u2022 We trained OSM models [9] on separate corpora, and interpolated them [10] by optimizing perplexity on the tuning-set.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "\u2022 We trained OSM models [9] on separate corpora, and interpolated them [10] by optimizing perplexity on the tuning-set.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "We also tried this on the OSM models trained on the word classes [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "\u2022 We tried the fine-tuning method of training the NNJM model on the out-domain data and fine-tuning with the in-domain TED data [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Finally we applied system combination over the outputs of best Neural MT and phrase-based systems using MEMT [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "For language model we trained using the target side of the parallel corpus and all the available English data from the recent WMT campaign [15], and GigaWord and OPUS mono corpus for Arabic.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "We found MADAMIRA [16] performed 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "1 BLEU points better than Farasa [17] (See Table 2) and decided to use it for the competition.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "Before scoring the output, we normalized them and reference translations using the QCRI normalizer [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 203, "endOffset": 207}, {"referenceID": 20, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 238, "endOffset": 242}, {"referenceID": 21, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 278, "endOffset": 282}, {"referenceID": 22, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 305, "endOffset": 309}, {"referenceID": 23, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 351, "endOffset": 355}, {"referenceID": 24, "context": "We used k-best batch MIRA [25] for tuning.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "We used cased BLEU [26] to measure progress.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "Due to our experience from previous competitions, we were wary of the fact that simply adding the UN data is harmful for the AR\u2192 MT system, we therefore selected data through MML filtering [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 26, "context": "The OSM model has been a regular feature of the phrasebased pipeline [27] in the competition grade systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "[10] recently found that an OSM model trained on plain concatenation of data is sub-optimal and can be improved by training OSM models on each domain individually and interpolating them by minimizing perplexity on the in-domain tune-set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We trained an NNJM models on the UN and Opus data for 25 epochs and then fine-tuned [12] it by", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "We explored the use of automatic word clusters in phrasebased models [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "We tried to handle OOV words using drop-oov and through transliteration [28, 29].", "startOffset": 72, "endOffset": 80}, {"referenceID": 28, "context": "We tried to handle OOV words using drop-oov and through transliteration [28, 29].", "startOffset": 72, "endOffset": 80}, {"referenceID": 29, "context": "We used a similar pre/post-processing pipeline for Neural MT as our phrase-based systems (Section 2), and additionally applied BPE [30] before training them.", "startOffset": 131, "endOffset": 135}, {"referenceID": 29, "context": "We limited the number of operations to 59,500, as suggested in [30].", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "Ensembling models has shown to give a consistent boost in performance in past best performing systems [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 12, "context": "For this purpose we used MultiEngine MT system, or MEMT [13].", "startOffset": 56, "endOffset": 60}], "year": 2017, "abstractText": "This paper describes QCRI\u2019s machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic\u2192English and English\u2192Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic\u2192English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.", "creator": "LaTeX with hyperref package"}}}