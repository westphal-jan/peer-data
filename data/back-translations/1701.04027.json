{"id": "1701.04027", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Neural Models for Sequence Chunking", "abstract": "Many tasks of the Natural Language Understanding (NLU), such as shallow parsing (i.e. text chunking) and semantic filling of slots, require the assignment of representative labels to significant chunks in a sentence. Most of the current methods of the Deep Neural Network (DNN) regard these tasks as a sequence labeling problem in which a word, not a chunking, is treated as the basic unit for the labeling, and these chunks are then inferred by the standard IOB labels (Inside-Outside-Beginning). In this paper, we propose an alternative approach by examining the use of DNN for sequence chunking and proposing three neural models so that each chunking chunking-chunking-chunking-chunking-chunking-chunking-chunking-chunking-chunking-chunking-chunking-chunking-models can be treated as a complete unit for the labeling task.", "histories": [["v1", "Sun, 15 Jan 2017 11:08:28 GMT  (224kb,D)", "http://arxiv.org/abs/1701.04027v1", "Accepted by AAAI 2017"]], "COMMENTS": "Accepted by AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["feifei zhai", "saloni potdar", "bing xiang", "bowen zhou"], "accepted": true, "id": "1701.04027"}, "pdf": {"name": "1701.04027.pdf", "metadata": {"source": "CRF", "title": "Neural Models for Sequence Chunking", "authors": ["Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou"], "emails": ["fzhai@us.ibm.com", "potdars@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "Introduction", "text": "Semantic slot filling and shallow parsing which are standard NLU tasks fall under the umbrella of natural language understanding (NLU), which are usually solved by labeling meaningful chunks in a sentence. This kind of task is usually treated as a sequence labeling problem, where every word in a sentence is assigned an IOB-based (InsideOutside-Beginning) label. For example, in Figure 1, in the sentence \u201cBut it could be much worse\u201d we label \u201ccould\u201d as B-VP, \u201cbe\u201d as I-VP, and \u201cit\u201d as B-NP, while \u201cBut\u201d belongs to an artificial class O. This labeling indicates that a chunk \u201ccould be\u201d is a verb phrase (VP) where the label prefix B means the beginning word of the chunk, while I refers to the other words within the same semantic chunk; and \u201cit\u201d is a single-word chunk with NP label.\nSuch sequence labeling forms the basis for many recent deep network based approaches, e.g., convolutional neural networks (CNN), recurrent neural networks (RNN) or its variation, long short-term memory networks (LSTM). RNN and LSTM are good at capturing sequential information (Yao et al. 2013; Huang, Xu, and Yu 2015; Mesnil et al. 2015; Peng and Yao 2015; Yang, Salakhutdinov, and Cohen 2016; Kurata et al. 2016; Zhu and Yu 2016), whereas CNN can extract effective features for classification (Xu and Sarikaya 2013; Vu 2016).\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nMost of the current DNN based approaches use the IOB scheme to label chunks. However, this approach of these labels has a few drawbacks. First, we don\u2019t have an explicit model to learn and identify the scope of chunks in a sentence, instead we infer them implicitly (by IOB labels). Hence the learned model might not be able to fully utilize the training data which could result in poor performance. Second, some neural networks like RNN or LSTM have the ability to encode context information but don\u2019t treat each chunk as a complete unit. If we can eliminate this drawback, it could result in more accurate labeling, especially for multi-word chunks.\nSequence chunking is a natural solution to overcome the two drawbacks mentioned before. In sequence chunking, the original sequence labeling task is divided into two sub-tasks: (1) Segmentation, to identify scope of the chunks explicitly; (2) Labeling, to label each chunk as a single unit based on the segmentation results.\nLample et al. (2016) used a stack-LSTM (Dyer et al. 2015) and a transition-based algorithm for sequence chunking. In their paper, the segmentation step is based on shiftreduce parser based actions. In this paper, we propose an alternative approach by relying only on the neural architectures for NLU. We investigate two different ways for segmentation: (1) using IOB labels; and (2) using pointer networks (Vinyals, Fortunato, and Jaitly 2015) and propose three neural sequence chunking models. Pointer network performs better than the model using IOB. In addition, it also achieves state-of-the-art performance on both text chunking and slot filling tasks.\nar X\niv :1\n70 1.\n04 02\n7v 1\n[ cs\n.C L\n] 1\n5 Ja\nn 20\n17"}, {"heading": "Basic Neural Networks", "text": ""}, {"heading": "Recurrent Neural Network", "text": "Recurrent neural network (RNN) is a neural network that is suitable for modeling sequential information. Although theoretically it is able to capture long-distance dependencies, in practice it suffers from the gradient vanishing/exploding problems (Bengio, Simard, and Frasconi 1994). Long shortterm memory networks (LSTM) were introduced to cope with these gradient problems and model long-range dependencies (Hochreiter and Schmidhuber 1997) by using a memory cell. Given an input sentence x = (x1, x2, ..., xT ) where T is the sentence length, LSTM hidden state at timestep t is computed by:\nit = \u03c3(W ixt + U iht\u22121 + b i)\nft = \u03c3(W fxt + U fht\u22121 + b f ) ot = \u03c3(W oxt + U oht\u22121 + b o) gt = tanh(W gxt + U ght\u22121 + b g)\nct = ft ct\u22121 + it gt ht = ot tanh(ct)\n(1)\nwhere \u03c3(\u00b7) and tanh(\u00b7) are the element-wise sigmoid and hyperbolic tangent functions, is the element-wise multiplication operator, and it,ft,ot are the input, forget and output gates. ht\u22121 and ct\u22121 are the hidden state and memory cell of previous timestep respectively. To simplify the notation, we use xt to denote both the word and its embedding.\nThe bi-directional LSTM (Bi-LSTM), a modification of the LSTM, consists of a forward and a backward LSTM. The forward LSTM reads the input sentence as it is (from x1 to xT ) and computes the forward hidden states ( \u2212\u2192 h1, \u2212\u2192 h2, ..., \u2212\u2192 hT ), while the backward LSTM reads the sentence in the reverse order (from xT to x1), and creates backward hidden states ( \u2190\u2212 h1, \u2190\u2212 h1, ..., \u2190\u2212 hT ). Then for each timestep t, the hidden state of the Bi-LSTM is generated by concatenating \u2212\u2192 ht and \u2190\u2212 ht ,\n\u2190\u2192 ht = [ \u2212\u2192 ht ; \u2190\u2212 ht ] (2)"}, {"heading": "Convolutional Neural Network", "text": "Convolutional Neural Networks (CNN) have been used to extract features for sentence classification (Kim 2014; Ma et al. 2015; dos Santos, Xiang, and Zhou 2015). Given a sentence, a CNN with m filters and a filter size of n extracts a m-dimension feature vector from every n-gram phrase of the sentence. A max-over-time pooling (max-pooling) layer is applied over all extracted feature vectors to create the final indicative feature vector (m-dimension) for the sentence.\nFollowing this approach, we use CNN and max-pooling layer to extract features from chunks. For each identified chunk, we first apply CNN to the embedding of its words (irrespective of it being a single-word chunk or chunk), and then use the max-pooling layer on top to get the chunk feature vector for labeling. We use CNNMax to denote the two layers hereafter."}, {"heading": "Proposed Models", "text": "In this section, we introduce the different neural models for sequence chunking and discuss the final learning objective.\nVP 8<: ADJP 8<:NP\nModel I For segmentation, the most straightforward and intuitive way is to transform it into a sequence labeling problem with 3 classes : I - inside, O - outside, B - beginning; and then understand the scope of the chunks from these labels. Building on this, we propose Model I, which is a Bi-LSTM as shown in Figure 2. In the model, we take the bi-LSTM hidden states generated by Formula (2) as features for both segmentation and labeling.\nFor example, we first classify each word into an IOB label as shown in (Figure 2). Suppose a chunk begins at word i with length l (with one B label and followed by (l \u2212 1) I labels), then we can compute a feature vector for a chunk as follows:\nChj = Average( \u2190\u2192 hi , \u2190\u2212\u2192 hi+1, ..., \u2190\u2212\u2212\u2192 hi+l\u22121) (3)\nwhere j is the chunk index of the sentence, and Average(\u00b7) computes the average of the input vectors. With Chj , we apply a softmax layer over all chunk labels for labeling. For example in Figure 2, \u201cmuch worse\u201d is identified as a chunk with length 2; and we apply Formula (3) on its hidden states, to finally get the \u201cADJP\u201d label.\nModel II A drawback of Model I is that a single Bi-LSTM may not perform well on both segmentation and labeling subtasks. To overcome this we propose Model II, which follows the encoder-decoder framework ( Figure 3) (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014). Similar to Model I, we employ a Bi-LSTM for segmentation with IOB labels1. This Bi-LSTM will also serve as an encoder and create a sentence representation [ \u2212\u2192 hT ; \u2190\u2212 h1] (by concatenating the final hidden state of the forward and backward LSTM) which is used to initialize the decoder LSTM.\nWe modify the general encoder-decoder framework and use chunks as the inputs instead of words. For example, much worse is a chunk in Figure 3, and we take it as a single input to the decoder. The input chunk representation Cj consists of several parts. We first use the CNNMax layer to extract important information from words inside the chunk:\nCxj = g(xi, xi+1, ..., xi+l\u22121) (4)\n1Note that in Model I and II, we cannot guarantee that label \u201cO\u201d is not followed by \u201cI\u201d during segmentation. If so, we just take the first \u201cI\u201d as \u201cB\u201d. In future work it is advisable to add that as a hard constraint.\nwhere g(\u00b7) is the CNNMax layer. Then we use the context word embeddings of the chunk to capture context information (Yao et al. 2013; Mesnil et al. 2015; Kurata et al. 2016). The context window size is a hyperparameter to tune. Finally, we average the hidden states from the encoder BiLSTM by Formula (3). By using these three parts, we extract different useful information for labeling, and import them all into the decoder LSTM. Thus, the decoder LSTM hidden state is updated by:\nhj = LSTM(Cxj , Chj , Cwj , hj\u22121, cj\u22121) (5)\nhere Cwj is the concatenation of context word embeddings. Note that the computation of hidden states here is similar to Formula (1), the only difference is that here we have three inputs {Cxj , Chj , Cwj}. The generated hidden states are finally used for labeling by a softmax layer.\nModel III There are two drawbacks of using IOB labels for segmentation. First, it is hard to use chunk-level features for segmentation, like the length of chunks. Also, using IOB labels cannot compare different chunks directly. The shift-reduce algorithm used in (Lample et al. 2016) has the same issue. They both transform a multi-class classification problem (we could have a lot of chunk candidates) into a 3-class classification problem, in which the chunks are inferred implicitly.\nTo resolve this problem, we further propose Model III, which is an encoder-decoder-pointer framework (Figure 4) (Nallapati et al. 2016). Model III is similar to Model II, the only difference being the method of identifying chunks.\nModel III is a greedy process of segmentation and labeling, where we first identify one chunk, and then label it. This process is repeated until all the words are processed. As all chunks are adjacent to each other 2, after one chunk is identified, the beginning point of the next one is also known, and only its ending point is to be determined. We adopt pointer network (Vinyals, Fortunato, and Jaitly 2015) to do this. For a possible chunk beginning at timestep b, we first generate a feature vector for each possible ending point candidate i:\nuij = v T 1 tanh(W1 \u2190\u2192 hi +W2xi +W3xb +W4dj)\n+ vT2 LE(i\u2212 b+ 1) i \u2208 [b, b+ lm) (6)\nwhere j is the decoder timestep (i.e., chunk index), lm is the maximum chunk length. We use the encoder hidden\n2 Here as we don\u2019t know the label of each chunk during segmentation, we need to feed all the chunks to the decoder for labeling.\nstate \u2190\u2192 hi , the ending point candidate word embedding xi, together with current beginning word embedding xb and decoder hidden state dj as features. We also use the chunk length embedding, LE(i\u2212b+1), as the chunk level feature. W1,W2,W3,W4,v1,v2 and LE are all learnable parameters. Then the probability of choosing ending point candidate i is:\np(i) = exp(uij)\u2211b+lm\u22121\nk=b exp(u k j )\n(7)\nWe use this probability to identify the scope of chunks. For example, suppose we just identified word it as a one word chunk with label NP in Figure 4. Following the line emitted from it, we will need to decide the ending point of the next chunk (the beginning point is obviously the word could after it). With the maximum chunk length 2, we have two choices, one is to stop at word could and gets a one word chunk could, and the other is to stop at word be and generates a two word chunk could be. From the figure, we can see that the model selects the second case (red circle part), and creates a two word chunk. This chunk will serve as the input of the next decoder timestep. The decoder hidden states are updated similar to Model II (Equation 5)."}, {"heading": "Learning Objective", "text": "As we described above, all the aforementioned models solve two subtasks - segmentation and labeling. We use the crossentropy loss function for both the two subtasks, and sum the two losses to form the the learning objective:\nL(\u03b8) = Lsegmentation(\u03b8) + Llabeling(\u03b8) (8)\nwhere \u03b8 denotes the learnable parameters. Alternatively, we could also use weighted sum, or do multi-task learning by considering segmentation and labeling as the two tasks. We leave these extensions as future work."}, {"heading": "Experiments", "text": ""}, {"heading": "Experimental Setup", "text": "We conduct experiments on text chunking and semantic slot filling respectively to test the performance of the neural sequence chunking models we propose in this paper. Both these tasks identify the meaningful chunks in the sentence, such as the noun phrase (NP), or the verb phrase (VP) for text chunking in Figure 1, and the \u201cdepart city\u201d for slot filling task in Figure 5.\nWe use the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz 2000) dataset for text chunking. It contains\n8,936 training and 893 test sentences. There are 12 different labels (22 with IOB prefix included). Since it doesn\u2019t have a validation set, we hold out 10% of the training data (selected at random) as the validation set.\nTo evaluate the effectiveness of our method on the semantic slot filling task, we use two different datasets. The first one is the ATIS dataset, which consists of reservation requests from the air travel domain. It contains 4,978 training and 893 testing sentences in total, with a vocabulary size of 572. There are 84 different slot labels (127 if with IOB prefix). We randomly selected 80% of the training data for model training and the rest 20% as the validation set (Mesnil et al. 2015). Following the work of (Kurata et al. 2016), we also use a larger dataset by combining the ATIS corpus with the MIT Restaurant Corpus and MIT Movie Corpus (Liu et al. 2013a; Liu et al. 2013b). This dataset has 30,229 training and 6,810 testing instances. Similar to the previous dataset, we use 80% of the training instances for training the model, and treat the rest 20% as a validation set. This dataset has a vocabulary size of 16,049 and the number of slot labels is 116 (191 with IOB prefix included). Since this dataset is considerably larger and includes 3 different domains, we use \u201cLARGE\u201d to denote it hereafter.\nThe final performance is measured in terms of F1-score, computed by the public available script conlleval.pl 3. We report the F1-score on the test set with parameters that achieves the best F1-score on the validation set. Towards the neural sequence chunking models, after we get the label for each chunk, we will assign each of its word an IOB-based label accordingly so that the script can do evaluation. We also report the segmentation F1-score to assess the segmentation performance of different models. This is also computed by the conlleval.pl script, but only considers three labels, i.e.\n3http://www.cnts.ua.ac.be/conll2000/chunking/\n{I,O,B}. To compute the segmetnation F1-score, we delete the content label for each word, for example, if a word has a label \u201cB-VP\u201d, we will delete \u201cVP\u201d and the left \u201cB\u201d is used for segmentation F1-score.\nFor the two tasks, we use hidden state size as 100 for the forward and backward LSTM respectively in Bi-LSTM, and size 200 for the LSTM decoder. We use dropout with rate 0.5 on both the input and output of all LSTMs. The mini-batch size is set to 1. The number of training epochs are limited to 200 for text chunking, and 100 for slot filling. 4 For the CNN used in Model II and III on extracting chunk features, the filter size is the same as word embedding dimension, and the filter window size as 2. We adopt SGD to train the model, and by grid search, we tune the initial learning rate in [0.01, 0.1], learning rate decay in [1e-6, 1e-4], and context window size {1,3,5}.\nFor the word embedding, following (Kurata et al. 2016), we don\u2019t use pre-trained embedding for the slot filling task, but use a randomly initialized embedding and tune the dimension in {30, 50, 75} by grid search. For text chunking, we concatenate two different embeddings. The first is SENNA embedding (Collobert et al. 2011) with dimension 50. 5 The other is a word representation generated based on its composed characters. we adopt a CNN onto the randomly initialized character embeddings, with 30 filters and filter window size 3."}, {"heading": "Text Chunking Results", "text": "Results on the text chunking task are shown in Table 1. In this, the \u201cbaseline (Bi-LSTM)\u201d refers to a Bi-LSTM model for sequence labeling (use IOB-based labels on words as in Figure 1). \u201cF1\u201d is the final evaluation metric, and \u201csegmentF1\u201d refers to the segmentation F1-score. From the table, we can see that Model I and Model II only have comparable results with the baseline on both evaluation metrics - segment-F1 and final F1 score. Hence, we infer that using IOB labels to do segmentation independently might not be a good choice. However, Model III outperforms the baseline on both segmentation and labeling.\nWe further compare our best result with the current published results in Table 2. In the table, (Collobert et al. 2011)\n4We found that while 100 epochs are enough for slot filling model to converge, we need 200 for text chunking.\n5http://ronan.collobert.com/senna/\nis the first work of using neural networks for text chunking. Huang, Xu, and Yu used a BiLSTM-CRF framework together with a lot of handcraft features. Yang, Salakhutdinov, and Cohen extend this framework and employ a GRU to incorporate the character information of words, rather than using handcrafted features. To our best knowledge, they got the current best results 94.66 on the text chunking task. 6 Different from previous work, we model the segmentation part explicitly in our neural models, and without using CRF, we get a state-of-the-art performance of 94.72."}, {"heading": "Slot Filling Results", "text": "Segmentation Results From the Table 3, we can see that the segment-F1 score on ATIS data is much better than the one on LARGE data (\u223c99% vs. \u223c80%). This is because the ATIS data is much easier for segmentation than LARGE data. As shown in Table 4, more than 97% of the chunks in ATIS data have only one or two words, while the LARGE data has much longer chunks. Also, compared to the small ATIS vocabulary (572 words), it is harder to learn a good segmentation model with a more complicated vocabulary (about 16k words) in LARGE data.\n6They also get a performance of 95.41, but this number is from joint training, which needs the training data of other tasks.\nMoreover, Model III gets the best segmentation performance over all the models (99.01% and 82.44%), confirming that our pointer network in model III is good at this task. However, Model I and II are comparable to baseline on the easy ATIS data, and are about 1% worse on LARGE data. This further confirms our analysis on text chunking experiments that using IOB labels alone for segmentation, (like in Model I and II) cannot give us a good result.\nWe further investigate the segmentation process and show the segmentation F1-score on different chunk lengths in Table 5. The results demonstrate that the poor performance on LARGE data is mainly due to the bad performance on identifying long chunks (around 55%). Our Model III improves this score by 2% over baseline (54.88% vs. 56.59%). As the absolute performance on this subset is still low, future research efforts should focus on improving this performance. In addition, Model I and II get comparable segmentation results with the baseline model on one-words chunks, while being worse on longer chunks, further supporting this analysis.\nLabeling Results From Table 3, we observe that Model III has the best F1 score as compared to the baseline and other neural chunking models. Another observation is that Model I and II get better improvements over baseline even though they are poor at segmentation in slot filling task.\nTable 6 gives some insights on this by showing the F1score on different chunk-lengths. Comparing Table 5 and 6, we can see when Model I and II achieve comparable segment-F1 with baseline, and the F-1 scores are higher. For slot filling task, the joint learning framework (Formula (8)) helps labeling while harms segmentation on model I and II.\nMoreover, the usage of encoder in Model II could also help labeling in this task (Kurata et al. 2016). Finally, our Model III could achieve better F1 score on all chunk lengths.\nComparison with Published Results We compare the ATIS results of our best model (Model III) with current published results in Table 7. As shown in the table, many researchers have done a lot of work which uses deep neural networks for slot filling. Recent work shows the ranking loss is helpful (Vu et al. 2016), and adding encoder improves the score to 95.66%. The best published result in the table is from (Zhu and Yu 2016), which is 95.79%. Compared with previous results, our Model III gets the state-of-the-art performance 95.86%.\nWe compare our approach against the only set of published results on the LARGE data from (Kurata et al. 2016), against which we compare our approach. The reported F1 score on this dataset by their encoder-decoder model is 74.41, and our best model achieves a score of 78.49 which is significantly higher."}, {"heading": "Related Work", "text": "In recent years, many deep learning approaches have been explored for resolving the sequence labeling tasks. (Collobert et al. 2011) proposed an effective window-based approach, in which they used a feed-forward neural network to classify each word and conditional random fields (CRF) to capture the sequential information. CNNs are also widely used for extracting effective classification features (Xu and Sarikaya 2013; Vu 2016).\nRNNs are a straightforward and better suited choice for these tasks as they model sequential information. (Huang, Xu, and Yu 2015) presented a BiLSTM-CRF model, and achieved state-of-the-art performance on several tasks, like named entity recognition and text chunking with the help of handcrafted features. (Chiu and Nichols 2015) used a BiLSTM for labeling and a CNN to capture character-level information, like (dos Santos and Gatti 2014) and additionally used handcrafted features to gain good performance. Many works have then been investigated to combine the advantages of the above two works and achieved state-of-the-art performance without handcrafted features. These works usu-\nally use a BiLSTM or BiGRU as the major labeling architecture, and a LSTM or GRU or CNN to capture the characterlevel information, and finally a CRF layer to model the label dependency (Lample et al. 2016; Ma and Hovy 2016; Yang, Salakhutdinov, and Cohen 2016).\nIn addition, many similar works have also been explored for slot filling, like RNN (Yao et al. 2013; Mesnil et al. 2015), LSTM (Yao et al. 2014; Jaech, Heck, and Ostendorf 2016), adding external memory (Peng and Yao 2015), adding encoder (Kurata et al. 2016), using ranking loss (Vu et al. 2016), adding attention (Zhu and Yu 2016) and so on.\nIn the other direction, people also developed neural networks to help traditional sequence processing methods, like CRF parsing (Durrett and Klein 2015) and weighted finitestate transducer (Rastogi, Cotterell, and Eisner 2016)."}, {"heading": "Conclusion", "text": "In this paper, we presented three different models for sequence chunking. Our experiments show that the segmentation results of Model I and Model II are comparable to baseline on text chunking data and ATIS data, and worse than the baseline on LARGE data, while Model III gains higher segment-F1 score than baseline, demonstrating that the use of IOB labels is not suitable for building segmentation models independently. Moreover, Model I and II do not give consistent improvements on the final F1 score - the segmentation step improves labeling on slot filling, but not on the text chunking task. Finally, Model III consistently performs better than baseline and gets state-of-the-art performance on the two tasks. We also gain insights about the datasets we use by comparing the segment-F1 scores and F1 scores of model III. For the text chunking data (95.75 vs. 94.72) and LARGE data (82.44 vs. 78.49), the scores are close to each other, indicating that segmentation is a major challenge in these two datasets compared to labeling. But for ATIS data (99.01 vs. 95.86), the segmentation score is almost 100 percent, so labeling seems like the main challenge in this dataset. We hope this insight encourages more research efforts on the similar tasks. Finally, the proposed neural sequence chunking models achieves state-of-the-art performance on both text chunking and slot filling."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks 5(2):157\u2013166", "author": ["Simard Bengio", "Y. Frasconi 1994] Bengio", "P. Simard", "P. Frasconi"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "and Nichols", "author": ["J.P. Chiu"], "venue": "E.", "citeRegEx": "Chiu and Nichols 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "C.N. Gatti 2014] dos Santos", "M. Gatti"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACL, 626\u2013634", "author": ["Xiang dos Santos", "C. Zhou 2015] dos Santos", "B. Xiang", "B. Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "and Klein", "author": ["G. Durrett"], "venue": "D.", "citeRegEx": "Durrett and Klein 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "Smith"], "venue": "A.", "citeRegEx": "Dyer et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Xu Huang", "Z. Yu 2015] Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Domain adaptation of recurrent neural networks for natural language understanding", "author": ["Heck Jaech", "A. Ostendorf 2016] Jaech", "L. Heck", "M. Ostendorf"], "venue": "arXiv preprint arXiv:1604.00117", "citeRegEx": "Jaech et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2016}, {"title": "and Matsumoto", "author": ["T. Kudo"], "venue": "Y.", "citeRegEx": "Kudo and Matsumoto 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "and Matsumoto", "author": ["T. Kudoh"], "venue": "Y.", "citeRegEx": "Kudoh and Matsumoto 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Leveraging sentence-level information with encoder lstm for semantic slot filling", "author": ["Kurata"], "venue": "arXiv preprint arXiv:1601.01530", "citeRegEx": "Kurata,? \\Q2016\\E", "shortCiteRegEx": "Kurata", "year": 2016}, {"title": "C", "author": ["G. Lample", "M. Ballesteros", "K. Kawakami", "S. Subramanian", "Dyer"], "venue": "2016. Neural architectures for named entity recognition. In In proceedings of NAACL", "citeRegEx": "Lample et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Lane", "author": ["B. Liu"], "venue": "I.", "citeRegEx": "Liu and Lane 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Asgard: A portable architecture for multilingual dialogue systems", "author": ["Liu"], "venue": "In ICASSP. IEEE", "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "Query understanding enhanced by hierarchical parsing structures", "author": ["Liu"], "venue": "In ASRU,", "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "and Hovy", "author": ["X. Ma"], "venue": "E.", "citeRegEx": "Ma and Hovy 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Ma"], "venue": "In ACL,", "citeRegEx": "Ma,? \\Q2015\\E", "shortCiteRegEx": "Ma", "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Mesnil"], "venue": null, "citeRegEx": "Mesnil,? \\Q2015\\E", "shortCiteRegEx": "Mesnil", "year": 2015}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Nallapati"], "venue": "Proceedings of CoNLL", "citeRegEx": "Nallapati,? \\Q2016\\E", "shortCiteRegEx": "Nallapati", "year": 2016}, {"title": "and Yao", "author": ["B. Peng"], "venue": "K.", "citeRegEx": "Peng and Yao 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Cotterell Rastogi", "P. Eisner 2016] Rastogi", "R. Cotterell", "J. Eisner"], "venue": "In Proceedings of NAACL", "citeRegEx": "Rastogi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "and Pereira", "author": ["F. Sha"], "venue": "F.", "citeRegEx": "Sha and Pereira 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Sarkar", "author": ["H. Shen"], "venue": "A.", "citeRegEx": "Shen and Sarkar 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Buchholz", "author": ["E.F. Tjong Kim Sang"], "venue": "S.", "citeRegEx": "Tjong Kim Sang and Buchholz 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "N", "author": ["O. Vinyals", "M. Fortunato", "Jaitly"], "venue": "2015. Pointer networks. In NIPS, 2692\u2013", "citeRegEx": "Vinyals. Fortunato. and Jaitly 2015", "shortCiteRegEx": null, "year": 2700}, {"title": "N", "author": ["Vu"], "venue": "T.; Gupta, P.; Adel, H.; Sch, H.; et al.", "citeRegEx": "Vu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "N", "author": ["Vu"], "venue": "T.", "citeRegEx": "Vu 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Sarikaya", "author": ["P. Xu"], "venue": "R.", "citeRegEx": "Xu and Sarikaya 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-task crosslingual sequence tagging from scratch. arXiv preprint arXiv:1603.06270", "author": ["Salakhutdinov Yang", "Z. Cohen 2016] Yang", "R. Salakhutdinov", "W. Cohen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao"], "venue": null, "citeRegEx": "Yao,? \\Q2013\\E", "shortCiteRegEx": "Yao", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Yao"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Yao,? \\Q2014\\E", "shortCiteRegEx": "Yao", "year": 2014}, {"title": "and Yu", "author": ["S. Zhu"], "venue": "K.", "citeRegEx": "Zhu and Yu 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-OutsideBeginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.", "creator": "LaTeX with hyperref package"}}}