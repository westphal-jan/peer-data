{"id": "1701.04056", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Dialog Context Language Modeling with Recurrent Neural Networks", "abstract": "In this paper, we propose contextual language models that incorporate discourse information at the level of dialogue into speech modeling. Previous work on the contextual language model treats previous utterances as a sequence of inputs without taking dialogue interactions into account. We design contextual language models based on recursive neural networks (RNN) that specifically track the interactions between speakers in a dialogue. Experimental results based on Switchboard Dialog Act Corpus show that the proposed model outperplexes conventional RNN speech models by 3.3%.", "histories": [["v1", "Sun, 15 Jan 2017 15:10:29 GMT  (88kb,D)", "http://arxiv.org/abs/1701.04056v1", "Accepted for publication at ICASSP 2017"]], "COMMENTS": "Accepted for publication at ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing liu", "ian lane"], "accepted": true, "id": "1701.04056"}, "pdf": {"name": "1701.04056.pdf", "metadata": {"source": "CRF", "title": "DIALOG CONTEXT LANGUAGE MODELING WITH RECURRENT NEURAL NETWORKS", "authors": ["Bing Liu", "Ian Lane"], "emails": ["liubing@cmu.edu,", "lane@cmu.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014 RNNLM, contextual language model, dialog modeling, dialog act"}, {"heading": "1. INTRODUCTION", "text": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4]. Recurrent neural network (RNN) based models [5, 6] have recently shown success in language modeling, outperforming conventional n-gram based models. Long short-term memory [7, 8] is a widely used RNN variant for language modeling due to its superior performance in capturing longer term dependencies.\nConventional RNN based language model uses a hidden state to represent the summary of the preceding words in a sentence without considering context signals. Mikolov et al. proposed a context dependent RNN language model [9] by connecting a contextual vector to the RNN hidden state. This contextual vector is produced by applying Latent Dirichlet Allocation [10] on preceding text. Several other contextual language models were later proposed by using bag-of-word [11] and RNN methods [12] to learn larger context representation that beyond the target sentence.\nThe previously proposed contextual language models treat preceding sentences as a sequence of inputs, and they are suitable for document level context modeling. In dialog modeling, however, dialog interactions between speakers play an important role. Modeling utterances in a dialog as a\nsequence of inputs might not well capture the pauses, turntaking, and grounding phenomena [13] in a dialog. In this work, we propose contextual RNN language models that specially track the interactions between speakers. We expect such models to generate better representations of the dialog context.\nThe remainder of the paper is organized as follows. In section 2, we introduce the background on contextual language modeling. In section 3, we describe the proposed dialog context language models. Section 4 discusses the evaluation procedures and results. Section 5 concludes the work."}, {"heading": "2. BACKGROUND", "text": ""}, {"heading": "2.1. RNN Language Model", "text": "A language model assigns a probability to a sequence of words w = (w1, w2, ..., wT ) following probability distribution. Using the chain rule, the likelihood of the word sequence w can be factorized as:\nP (w) = P (w1, w2, ..., wT ) = T\u220f t=1 P (wt|w<t) (1)\nAt time step t, the system input is the embedding of the word at index t, and the system output is the probability distribution of the word at index t+ 1. The RNN hidden state ht encodes the information of the word sequence up till current step:\nht = RNN(ht\u22121, wt) (2) P (wt+1|w<t+1) = softmax(Woht + bo) (3)\nwhere Wo and bo are the output layer weights and biases."}, {"heading": "2.2. Contextual RNN Language Model", "text": "A number of methods have been proposed to introduce contextual information to the language model. Mikolov and Zweig [9] proposed a topic-conditioned RNNLM by introducing a contextual real-valued vector to RNN hidden state. The contextual vector was created by performing LDA [10] on preceding text. Wang and Cho [11] studied introducing\nar X\niv :1\n70 1.\n04 05\n6v 1\n[ cs\n.C L\n] 1\n5 Ja\nn 20\n17\ncorpus-level discourse information into language modeling. A number of context representation methods were explored, including bag-of-words, sequence of bag-of-words, and sequence of bag-of-words with attention. Lin et al. [14] proposed using hierarchical RNN for document modeling. Comparing to using bag-of-words and sequence of bag-of-words for document context representation, using hierarchical RNN can better model the order of words in preceding text, at the cost of the increased computational complexity. These contextual language models focused on contextual information at the document level. Tran et al. [15] further proposed a contextual language model that consider information at interdocument level. They claimed that by utilizing the structural information from a tree-structured document set, language modeling performance was largely improved."}, {"heading": "3. METHODS", "text": "The previously proposed contextual language models focus on applying context by encoding preceding text, without considering interactions in dialogs. These models may not be well suited for dialog language modeling, as they are not designed to capture dialog interactions, such as clarifications and confirmations. By making special design in learning dialog interactions, we expect the models to generate better representations of the dialog context, and thus lower perplexity of the target dialog turn or utterance.\nIn this section, we first explain the context dependent RNN language model that operates on utterance or turn level. Following that, we describe the two proposed contextual language models that utilize the dialog level context."}, {"heading": "3.1. Context Dependent RNNLM", "text": "Let D = (U1,U2, ...,UK) be a dialog that has K turns and involves two speakers. Each turn may have one or more utterances. The kth turn Uk = (w1, w2, ..., wTk) is represented as a sequence of Tk words. Conditioning on information of the preceding text in the dialog, probability of the target turn Uk can be calculated as:\nP (Uk|U<k) = Tk\u220f t=1 P (wUkt |w Uk <t ,U<k) (4)\nwhere U<k denotes all previous turns before Uk, and wUk<t denotes all previous words before the tth word in turn Uk.\nIn context dependent RNN language model, the context vector c is connected to the RNN hidden state together with the input word embedding at each time step (Figure 1). This is similar to the context dependent RNN language model proposed in [9], other than that the context vector is not connected directly to the RNN output layer. With the additional context vector input c, the RNN state ht is updated as:\nht = RNN(ht\u22121, [wt, c]) (5)"}, {"heading": "3.2. Context Representations", "text": "In neural network based language models, the dialog context can be represented as a dense continuous vector. This context vector can be produced in a number of ways.\nOne simple approach is to use bag of word embeddings. However, bag of word embedding context representation does not take word order into consideration. An alternative approach is to use an RNN to read the preceding text. The last hidden state of the RNN encoder can be seen as the representation of the text and be used as the context vector for the next turn. To generate document level context representation, one may cascade all sentences in a document by removing the sentence boundaries. The last RNN hidden state of the previous utterance serves as the initial RNN state of the next utterance. As in [12], we refer to this model as DRNNLM. Alternatively, in the CCDCLM model proposed in [12], the last RNN hidden state of the previous utterance is fed to the RNN hidden state of the target utterance at each time step."}, {"heading": "3.3. Interactive Dialog Context LM", "text": "The previously proposed contextual language models, such as DRNNLM and CCDCLM, treat dialog history as a sequence of inputs, without modeling dialog interactions. A dialog turn from one speaker may not only be a direct response to the other speaker\u2019s query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn k in a dialog, we propose to connect the last RNN state of turn k\u22122 directly to the starting RNN state of turn k, instead of letting it to propagate through the RNN for turn k\u22121. The last RNN state of turn k\u22121 serves as the context vector to turn k, which is fed to turn k\u2019s RNN hidden state at each time step together with the word input. The model architecture is as shown in Figure 2. The context vector c and the initial RNN hidden state for the kth turn hUk0 are defined as:\nc = h Uk\u22121 Tk\u22121 , hUk0 = h Uk\u22122 Tk\u22122\n(6)\nwhere hUk\u22121Tk\u22121 represents the last RNN hidden state of turn k\u2212 1. This model also allows the context signal from previous turns to propagate through the network in fewer steps, which helps to reduce information loss along the propagation. We refer to this model as Interactive Dialog Context Language Model (IDCLM)."}, {"heading": "3.4. External State Interactive Dialog Context LM", "text": "The propagation of dialog context can be seen as a series of updates of a hidden dialog context state along the growing dialog. IDCLM models this hidden dialog context state changes implicitly in the turn level RNN state. Such dialog context state updates can also be modeled in a separated RNN. As shown in the architecture in Figure 3, we use an external RNN to model the context changes explicitly. Input to the external state RNN is the vector representation of the previous dialog turns. The external state RNN output serves as the dialog context for next turn:\nsk\u22121 = RNNES(sk\u22122, h Uk\u22121 Tk\u22121 ) (7)\nwhere sk\u22121 is the output of the external state RNN after the processing of turn k \u2212 1. The context vector c and the initial RNN hidden state for the kth turn hUk0 are then defined as:\nc = sk\u22121, h Uk 0 = h Uk\u22122 Tk\u22122\n(8)\nWe refer to this model as External State Interactive Dialog Context Language Model (ESIDCLM).\nComparing to IDCLM, ESIDCLM releases the burden of turn level RNN by using an external RNN to model dialog context state changes. One drawback of ESIDCLM is that there are additional RNN model parameters to be learned during model training, which may make the model more prone to overfitting when training data size is limited."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Data Set", "text": "We use the Switchboard Dialog Act Corpus (SwDA)1 in evaluating our contextual langauge models. The SwDA corpus extends the Switchboard-1 Telephone Speech Corpus with turn and utterance-level dialog act tags. The utterances are also tagged with part-of-speech (POS) tags. We split the data in folder sw00 to sw09 as training set, folder sw10 as test set, and folder sw11 to sw13 as validation set. The training, validation, and test sets contain 98.7K turns (190.0K utterances), 5.7K turns (11.3K utterances), and 11.9K turns (22.2K utterances) respectively. Maximum turn length is set to 160. The vocabulary is defined with the top frequent 10K words."}, {"heading": "4.2. Baselines", "text": "We compare IDCLM and ESIDCLM to several baseline methods, including n-gram based model, single turn RNNLM, and various context dependent RNNLMs.\n5-gram KN A 5-gram language model with modified Kneser-Ney smoothing [16].\nSingle-Turn-RNNLM Conventional RNNLM that operates on single turn level with no context information.\nBoW-Context-RNNLM Contextual RNNLM with BoW representation of preceding text as context.\nDRNNLM Contextual RNNLM with turn level context vector connected to initial RNN state of the target turn.\nCCDCLM Contextual RNNLM with turn level context vector connected to RNN hidden state of the target turn at each time step. We implement this model following the design in [12].\nIn order to investigate the potential performance gain that can be achieved by introducing context, we also compare the proposed methods to RNNLMs that use true dialog act tags as context. Although human labeled dialog act might not be the best option for modeling the dialog context state, it provides a reasonable estimation of the best gain that can be achieved by introducing linguistic context. The dialog act sequence is modeled by a separated RNN, similar to the external state RNN used in ESIDCLM. We refer to this model as Dialog Act Context Language Model (DACLM).\nDACLM RNNLM with true dialog act context vector connected to RNN state of the target turn at each time step."}, {"heading": "4.3. Model Configuration and Training", "text": "In this work, we use LSTM cell [7] as the basic RNN unit for its stronger capability in capturing long-range dependencies in a word sequence comparing to simple RNN. We use pretrained word vectors [17] that are trained on Google News dataset to initialize the word embeddings. These word embeddings are fine-tuned during model training. We conduct\n1http://compprag.christopherpotts.net/swda.html\nmini-batch training using Adam optimization method following the suggested parameter setup in [18]. Maximum norm is set to 5 for gradient clipping . For regularization, we apply dropout (p = 0.8) on the non-recurrent connections [19] of LSTM. In addition, we apply L2 regularization (\u03bb = 10\u22124) on the weights and biases of the RNN output layer."}, {"heading": "4.4. Results and Analysis", "text": "The experiment results on language modeling perplexity for models using different dialog turn size are shown in Table 1. K value indicates the number of turns in the dialog. Perplexity is calculated on the last turn, with preceding turns used as context to the model.\nAs can be seen from the results, all RNN based models outperform the n-gram model by large margin. The BoW-Context-RNNLM and DRNNLM beat the Single-TurnRNNLM consistently. Our implementation of the context dependent CCDCLM performs worse than Single-TurnRNNLM. This might due to fact that the target turn word prediction depends too much on the previous turn context vector, which connects directly to the hidden state of current turn RNN at each time step. The model performance on training set might not generalize well during inference given the limited size of the training set.\nThe proposed IDCLM and ESIDCLM beat the single turn RNNLM consistently under different context turn sizes. ESIDCLM shows the best language modeling performance under dialog turn size of 3 and 5, outperforming IDCLM by a small margin. IDCLM beats all baseline models when using dialog turn size of 5, and produces slightly worse perplexity than DRNNLM when using dialog turn size of 3.\nTo analyze the best potential gain that may be achieved by introducing linguistic context, we compare the proposed contextual models to DACLM, the model that uses true dialog act history for dialog context modeling. As shown in Table 1, the gap between our proposed models and DACLM is not wide. This gives a positive hint that the proposed contextual models may implicitly capture the dialog context state changes.\nFor fine-grained analyses of the model performance, we\nfurther compute the test set perplexity per POS tag and per dialog act tag. We selected the most frequent POS tags and dialog act tags in SwDA corpus, and report the tag based perplexity relative changes (%) of the proposed models comparing to Single-Turn-RNNLM. A negative number indicates performance gain.\nTable 2 shows the model perplexity per POS tag. All the three context dependent models produce consistent performance gain over the Single-Turn-RNNLM for pronouns, prepositions, and adverbs, with pronouns having the largest perplexity improvement. However, the proposed contextual models are less effective in capturing nouns. This suggests that the proposed contextual RNN language models exploit the context to achieve superior prediction on certain but not all POS types. Further exploration on the model design is required if we want to better capture words of a specific type."}, {"heading": "DA Tag IDCLM ESIDCLM DACLM", "text": "For the dialog act tag based results in Table 3, the three contextual models show consistent performance gain on Statement-non-opinion type utterances. The perplexity changes for other dialog act tags vary for different models."}, {"heading": "5. CONCLUSIONS", "text": "In this work, we propose two dialog context language models that with special design to model dialog interactions. Our evaluation results on Switchboard Dialog Act Corpus show that the proposed model outperform conventional RNN language model by 3.3%. The proposed models also illustrate advantageous performance over several competitive contextual language models. Perplexity of the proposed dialog context language models is higher than that of the model using true dialog act tags as context by a small margin. This indicates that the proposed model may implicitly capture the dialog context state for language modeling."}, {"heading": "6. REFERENCES", "text": "[1] Lawrence Rabiner and Biing-Hwang Juang, \u201cFundamentals of speech recognition,\u201d 1993.\n[2] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960\u20134964.\n[3] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek, John D Lafferty, Robert L Mercer, and Paul S Roossin, \u201cA statistical approach to machine translation,\u201d Computational linguistics, vol. 16, no. 2, pp. 79\u201385, 1990.\n[4] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.\n[5] Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur, \u201cRecurrent neural network based language model.,\u201d in Interspeech, 2010, vol. 2, p. 3.\n[6] Toma\u0301s\u030c Mikolov, Stefan Kombrink, Luka\u0301s\u030c Burget, Jan C\u030cernocky\u0300, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.\n[7] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[8] Martin Sundermeyer, Ralf Schlu\u0308ter, and Hermann Ney, \u201cLstm neural networks for language modeling.,\u201d in Interspeech, 2012, pp. 194\u2013197.\n[9] Tomas Mikolov and Geoffrey Zweig, \u201cContext dependent recurrent neural network language model.,\u201d in SLT, 2012, pp. 234\u2013239.\n[10] David M Blei, Andrew Y Ng, and Michael I Jordan, \u201cLatent dirichlet allocation,\u201d Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.\n[11] Tian Wang and Kyunghyun Cho, \u201cLarger-context language modelling,\u201d arXiv preprint arXiv:1511.03729, 2015.\n[12] Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein, \u201cDocument context language models,\u201d arXiv preprint arXiv:1511.03962, 2015.\n[13] Herbert H Clark and Susan E Brennan, \u201cGrounding in communication,\u201d Perspectives on socially shared cognition, vol. 13, no. 1991, pp. 127\u2013149, 1991.\n[14] Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li, \u201cHierarchical recurrent neural network for document modeling,\u201d in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 899\u2013907.\n[15] Quan Hung Tran, Ingrid Zukerman, and Gholamreza Haffari, \u201cInter-document contextual language model,\u201d in Proceedings of NAACL-HLT, 2016, pp. 762\u2013766.\n[16] Stanley F Chen and Joshua Goodman, \u201cAn empirical study of smoothing techniques for language modeling,\u201d in Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.\n[17] T Mikolov and J Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d Advances in neural information processing systems, 2013.\n[18] Diederik Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[19] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals, \u201cRecurrent neural network regularization,\u201d arXiv preprint arXiv:1409.2329, 2014."}], "references": [{"title": "Fundamentals of speech recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": "1993.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960\u20134964.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A statistical approach to machine translation", "author": ["Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S Roossin"], "venue": "Computational linguistics, vol. 16, no. 2, pp. 79\u201385, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Interspeech, 2010, vol. 2, p. 3.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Interspeech, 2012, pp. 194\u2013197.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "SLT, 2012, pp. 234\u2013239.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Larger-context language modelling", "author": ["Tian Wang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1511.03729, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1511.03962, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding in communication", "author": ["Herbert H Clark", "Susan E Brennan"], "venue": "Perspectives on socially shared cognition, vol. 13, no. 1991, pp. 127\u2013149, 1991.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 899\u2013907.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Inter-document contextual language model", "author": ["Quan Hung Tran", "Ingrid Zukerman", "Gholamreza Haffari"], "venue": "Proceedings of NAACL-HLT, 2016, pp. 762\u2013766.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996, pp. 310\u2013318.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": "Advances in neural information processing systems, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 163, "endOffset": 169}, {"referenceID": 3, "context": "Language model plays an important role in many natural language processing systems, such as in automatic speech recognition [1, 2] and machine translation systems [3, 4].", "startOffset": 163, "endOffset": 169}, {"referenceID": 4, "context": "Recurrent neural network (RNN) based models [5, 6] have recently shown success in language modeling, outperforming conventional n-gram based models.", "startOffset": 44, "endOffset": 50}, {"referenceID": 5, "context": "Recurrent neural network (RNN) based models [5, 6] have recently shown success in language modeling, outperforming conventional n-gram based models.", "startOffset": 44, "endOffset": 50}, {"referenceID": 6, "context": "Long short-term memory [7, 8] is a widely used RNN variant for language modeling due to its superior performance in capturing longer term dependencies.", "startOffset": 23, "endOffset": 29}, {"referenceID": 7, "context": "Long short-term memory [7, 8] is a widely used RNN variant for language modeling due to its superior performance in capturing longer term dependencies.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "proposed a context dependent RNN language model [9] by connecting a contextual vector to the RNN hidden state.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "This contextual vector is produced by applying Latent Dirichlet Allocation [10] on preceding text.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "Several other contextual language models were later proposed by using bag-of-word [11] and RNN methods [12] to learn larger context representation that beyond the target sentence.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Several other contextual language models were later proposed by using bag-of-word [11] and RNN methods [12] to learn larger context representation that beyond the target sentence.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Modeling utterances in a dialog as a sequence of inputs might not well capture the pauses, turntaking, and grounding phenomena [13] in a dialog.", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Mikolov and Zweig [9] proposed a topic-conditioned RNNLM by introducing a contextual real-valued vector to RNN hidden state.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "The contextual vector was created by performing LDA [10] on preceding text.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "Wang and Cho [11] studied introducing ar X iv :1 70 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14] proposed using hierarchical RNN for document modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] further proposed a contextual language model that consider information at interdocument level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This is similar to the context dependent RNN language model proposed in [9], other than that the context vector is not connected directly to the RNN output layer.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "As in [12], we refer to this model as DRNNLM.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Alternatively, in the CCDCLM model proposed in [12], the last RNN hidden state of the previous utterance is fed to the RNN hidden state of the target utterance at each time step.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "5-gram KN A 5-gram language model with modified Kneser-Ney smoothing [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "We implement this model following the design in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "In this work, we use LSTM cell [7] as the basic RNN unit for its stronger capability in capturing long-range dependencies in a word sequence comparing to simple RNN.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "We use pretrained word vectors [17] that are trained on Google News dataset to initialize the word embeddings.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "mini-batch training using Adam optimization method following the suggested parameter setup in [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "8) on the non-recurrent connections [19] of LSTM.", "startOffset": 36, "endOffset": 40}], "year": 2017, "abstractText": "In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.", "creator": "LaTeX with hyperref package"}}}