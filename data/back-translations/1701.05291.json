{"id": "1701.05291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Heterogeneous Information Network Embedding for Meta Path based Proximity", "abstract": "A network embedding is the representation of a large graph in a low-dimensional space in which vertices are modeled as vectors. The goal of a good embedding is to maintain the proximity between vertices in the original graph. In this way, typical search and mining methods in embedded space can be applied using commercially available multidimensional indexing approaches. Existing network embedding techniques focus on homogeneous networks in which all vertices are considered part of a single class.", "histories": [["v1", "Thu, 19 Jan 2017 04:00:46 GMT  (4375kb,D)", "http://arxiv.org/abs/1701.05291v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["zhipeng huang", "nikos mamoulis"], "accepted": false, "id": "1701.05291"}, "pdf": {"name": "1701.05291.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Information Network Embedding for Meta Path based Proximity", "authors": ["Zhipeng Huang", "Nikos Mamoulis"], "emails": ["nikos}@cs.hku.hk"], "sections": [{"heading": null, "text": "Keywords heterogeneous information network; meta path; network embedding"}, {"heading": "1. INTRODUCTION", "text": "The availability and growth of large networks, such as social networks, co-author networks, and knowledge base graphs, has given rise to numerous applications that search and analyze information in them. However, for very large graphs, common information retrieval and mining tasks such as link prediction, node classification, clustering, and recommendation are time-consuming. This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors. A good embedding preserves the proximity (i.e., similarity) between vertices in the origi-\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nnal graph. Search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces.\nHeterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types. These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20]. Figure 1 illustrates a HIN, which describes the relationships between objects (graph vertices) of different types (e.g., author, paper, venue and topic). For example, Jiawei Han (a1) has written a WWW paper (p1), which mentions topic \u201cEmbed\u201d (t1).\nCompared to homogeneous networks, the relationships between objects in a HIN are much more complex. The proximity among objects in a HIN is not just a measure of closeness or distance, but it is also based on semantics. For example, in the HIN in Figure 1, author a1 is close to both a2 and v1, but these relationships have different semantics. a2 is a co-author of a1, while v1 is a venue where a1 has a paper published.\nMeta path [29] is a recently proposed proximity model in HINs. A meta path is a sequence of object types with edge types in between modeling a particular relationship. For example, A\u2192 P \u2192 V \u2192 P \u2192 A is a meta path, which states that two authors (A) are related by their publications in the same venue (V ). Based on meta paths, several proximity measures have been proposed. For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. These measures have been shown to have better perfor-\nar X\niv :1\n70 1.\n05 29\n1v 1\n[ cs\n.A I]\n1 9\nJa n\n20 17\nmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].\nAlthough there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs. To fill this gap, in this paper, we propose HINE, which learns a transformation of the objects (i.e., vertices) in a HIN to a low-dimensional space, such that the meta path based proximities between objects are preserved. More specifically, we define an appropriate objective function that preserves the relative proximity based rankings the vertices in the original and the embedded space. As shown in [29], meta paths with too large lengths are not very informative; therefore, we only consider meta paths up to a given length threshold l. We also investigate the use of negative sampling [19] in order to accelerate the optimization process.\nWe conduct extensive experiments on four real HIN datasets to compare our proposed HINE method with state-of-the-art network embedding methods (i.e., LINE [31] and DeepWalk [22]), which do not consider meta path based proximity. Our experimental results show that our HINE method with PCRW as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used.\nThe contributions of our paper are summarized as follows:\n\u2022 This is the first work that studies the embedding of HINs for the preservation of meta path based proximities. This is an important subject, because meta path based proximity has been proved to be more effective than traditional structured based proximities.\n\u2022 We define an objective function, which explicitly aims at minimizing the the distance between two probability distributions, one modeling the meta path based proximities between the objects, the other modeling the proximities in the low-dimensional space.\n\u2022 We investigate the use of negative sampling to accelerate the process of model optimization.\n\u2022 We conduct extensive experiments on four real HINs, which demonstrate that our HINE method is the most effective approach for preserving the information of the original networks; HINE outperforms the state-of-the-art embedding methods in many data mining tasks, e.g., classification, clustering and visualization.\nThe rest of this paper is organized as follows. Section 2 discusses related work. In Section 3, we formally define the problem. Section 4 describes our HINE method. In Section 5, we report our experimental results. Section 6 concludes the paper."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1 Heterogeneous Information Networks", "text": "The heterogeneity of nodes and edges in HINs bring challenges, but also opportunities to support important applications. Lately, there has been an increasing interest in both academia and industry in the effective search and analysis of information from HINs. The problem of classifying objects in a HIN by authority propagation is studied in [12]. Follow-up work [13] investigates a collective classification problem in HINs using meta path based dependencies. PathSelClus [30] is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing\nsome examples as seeds. The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.g., in recommender systems). A related problem is entity recommendation in HINs [34], which takes advantage of the different types of relationships in HINs to provide better recommendations."}, {"heading": "2.2 Meta Path and Proximity Measures", "text": "Meta path [29] is a general model for the proximity between objects in a HIN. Several measures have been proposed for the proximity between objects w.r.t. a given meta path P . PathCount measures the number of meta path instances connecting the two objects, and PathSim is a normalized version of it [29]. Path constrained random walk (PCRW) was firstly proposed [14] for the task of relationship retrieval over bibliographic networks. Later, [17] proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW. Finally, HeteSim [25] is recently proposed as an extension of meta path based SimRank. In this paper, we focus on the two most popular proximity measures, i.e., PathCount and PCRW."}, {"heading": "2.3 Network Embedding", "text": "Network embedding aims at learning low-dimensional representations for the vertices of a network, such that the proximities among them in the original space are preserved in the low-dimensional space. Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph. Graph factorization [1] finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix. However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in [31].\nRecently, DeepWalk [22] is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network. DeepWalk combines random walk proximity with the SkipGram model [18], a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence. However, DeepWalk has certain weaknesses when applied to our problem settings. First, the random walk proximity it adopts does not consider the heterogeneity of a HIN. In this paper, we use PCRW, which extends random walk based proximity to be applied for HINs. Second, as pointed out in [31], DeepWalk can only preserve secondorder proximity, leading to poor performance in some tasks, such as link recover and classification, which require first-order proximity to be well-preserved.\nLINE [31] is a recently proposed embedding approach for largescale networks. Although it uses an explicit objective function to preserve the network structure, its performance suffers from the way it learns the vector representations. By design, LINE learns two representations separately, one preserving first-order proximity and the other preserving second-order proximity. Then, it directly concatenates the two vectors to form the final representation. In this paper, we propose a network embedding method, which does not distinguish the learning of first and second-order proximities and embeds proximities of all orders simultaneously.\nGraRep [6] further extends DeepWalk to utilize high-order proximities. GraRep does not scale well in large networks due to the expensive computation of the power of a matrix and the involvement of SVD in the learning process. SDNE [33] is a semi-supervised deep model that captures the non-linear structural information over the network. The source code of SDNE is not available, so this ap-\nproach cannot be reproduced and compared to ours. Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information. Our current work focuses on meta path based proximities, so it does not consider any other information in the HIN besides the network structure and the types of nodes and edges."}, {"heading": "3. PROBLEM DEFINITION", "text": "In this section, we formulate the problem of heterogeneous information network (HIN) embedding for meta path based proximities. We first define a HIN as follows:\nDEFINITION 1. (Heterogeneous Information Network) A heterogeneous information network (HIN) is a directed graph G = (V,E) with an object type mapping function \u03c6 : V \u2192 L and a link type mapping function \u03c8 : E \u2192 R, where each object v \u2208 V belongs to an object type \u03c6(v) \u2208 L, and each link e \u2208 E belongs to a link type \u03c8(e) \u2208 R.\nFigure 1 illustrates a small bibliographic HIN. We can see that the HIN contains two authors, three papers, three venues, two topics, and four types of links connecting them. For the ease of discussion, we assume that each object belongs to a single type; however, our technique can easily be adapted to the case where objects belong to multiple types.\nDEFINITION 2. HIN Schema. Given a HIN G = (V,E) with mappings \u03c6 : V \u2192 L and \u03c8 : E \u2192 R, the schema TG of G is a directed graph defined over object types L and link typesR, i.e., TG = (L,R).\nThe HIN schema expresses all allowable link types between object types. Figure 2(a) shows the schema of the HIN in Figure 1, where nodes A, P , T and V correspond to author, paper, topic and venue, respectively. There are also different edge types in the schema, such as \u2018publish\u2019 and \u2018write\u2019. Figure 2(b) shows the schema of a movie-related HIN, with M , A, D, P , C representing movie, actor, director, producer and composer, respectively.\nIn a HIN G, two objects o1, o2 may be connected via multiple edges or paths. Conceptually, each of these paths represents a specific direct or composite relationship between them. In Figure 1, authors a1 and a2 are connected via multiple paths. For example, a1 \u2192 p3 \u2192 a2 means that a1 and a2 are coauthors of paper p3, and a1 \u2192 p1 \u2192 p2 \u2192 a2 means that a2\u2019s paper p2 cites a1\u2019s paper p1. These two paths represent two different relationships between author a1 and a2.\nEach different type of a relationship is modeled by a meta path [29]. Specifically, a meta path P is a sequence of object types l1, . . . , ln connected by link types r1, . . . , rn\u22121 as follows:\nP = l1 r1\u2212\u2192 l2 . . . ln\u22121 rn\u22121\u2212\u2212\u2212\u2192 ln.\nFor example, a meta path A write\u2212\u2212\u2212\u2192 P write \u22121\n\u2212\u2212\u2212\u2212\u2212\u2192 A represents the coauthor relationship between two authors.\nAn instance of the meta path P is a path in the HIN, which conforms to the pattern of P . For example, path a1 \u2192 p3 \u2192 a2 in Figure 1 is an instance of meta path A write\u2212\u2212\u2212\u2192 P write \u22121 \u2212\u2212\u2212\u2212\u2212\u2192 A.\nDEFINITION 3. (Meta Path based Proximity) Given a HING = (V,E) and a meta path P , the proximity of two nodes os, ot \u2208 V with respect to P is defined as:\ns(os, ot | P) = \u2211\npos\u2192ot\u2208P\ns(os, ot | pos\u2192ot) (1)\nwhere pos\u2192ot is a meta path instance of P linking from os to ot, and s(os, ot | pos\u2192ot) is a proximity score w.r.t. the instance pos\u2192ot .\nThere are different ways to define s(os, ot | pos\u2192ot) in the literature. Here, we only list two definitions that we use as test cases in our experiments.\n\u2022 According to the PathCount (PC) model, each meta path instance is equally important and should be given equal weight. Hence, s(os, ot | pos\u2192ot) = 1 and the proximity between two objects w.r.t. P equals the number of instances of P connecting them, i.e., s(os, ot | P) = |{pos\u2192ot \u2208 P}|.1\n\u2022 Path Constrained Random Walk (PCRW) [14] is a more sophisticated way to define the proximity s(os, ot | pos\u2192ot) based on an instance pos\u2192ot . According to this definition, s(os, ot | pos\u2192ot) is the probability that a random walk restricted on P would follow the instance pos\u2192ot .\nNote that the embedding technique we introduce in this paper can also be easily adapted to other meta path based proximity measures, e.g., PathSim [29], HeteSim [25] and BPCRW [17].\nDEFINITION 4. (Proximity in HIN) For each pair of objects os, ot \u2208 V , the proximity between os and ot in G is defined as:\ns(os, ot) = \u2211 P s(os, ot | P) (2)\nwhere P is some meta path, and s(os, ot | P) is the proximity w.r.t. the meta path P as defined in Definition 3.\nAccording to Definition 4, the proximity of two objects equals the sum of the proximities w.r.t. all meta paths. Intuitively, this can capture all kinds of relationships between the two objects. We now provide a definition for the problem that we study in this paper.\nDEFINITION 5. (HIN Embedding for Meta Path based Proximity) Given a HIN G = (V,E), develop a mapping f : V \u2192 Rd that transforms each object o \u2208 V to a vector in Rd, such that the proximities between any two objects in the original HIN are preserved in Rd."}, {"heading": "4. HINE", "text": "In this section, we introduce our methodology for embedding HINs. We first discuss how to calculate the truncated estimation of meta path based proximity in Section 4.1. Then, we introduce our model and define the objective function we want to optimize in Section 4.2. Finally, we present a negative sampling approach that accelerates the optimization of the objective function in Section 4.3. 1For the ease of presentation, we overload notationP to also denote the set of instances of meta path P ."}, {"heading": "4.1 Truncated Proximity Calculation", "text": "According to Definition 4, in order to compute the proximity of two objects oi, oj \u2208 V , we need to accumulate the corresponding meta path based proximity w.r.t. each meta path P . For example, in Table 1, we list some meta paths that have instances connecting a1 and a2 in Figure 1. We can see that there is one length-2 meta path, one length-3 meta path and two length-4 meta paths that have instances connecting a1 and a2. Generally speaking, the number of possible meta paths grows exponentially with their length and can be infinite for certain HIN schema (in this case, the computation of s(oi, oj) is infeasible).\nAs pointed out in [29], shorter meta paths are more informative than longer ones, because longer meta paths link more remote objects (which are less related semantically). Therefore, we use a truncated estimation of proximity, which only considers meta paths up to a length threshold l. This is also consistent with previous works on network embedding, which aim at reserving loworder proximity (e.g., [22] reserves only second-order proximity, and [31] first-order and second-order proximities). Therefore, we define:\ns\u0302l(oi, oj) = \u2211\nlen(P)\u2264l\ns(oi, oj | P) (3)\nas the truncated proximity between two objects oi and oj . For the case of PCRW based proximity, we have the following property:\nPROPERTY 1. s\u0302l(oi, oj) = \u2211\n(oi,o\u2032)\u2208E\np \u03c8(oi,o \u2032) oi\u2192o\u2032 \u00d7 s\u0302l\u22121(o\u2032, oj)\nwhere p\u03c8(oi,o \u2032)\noi\u2192o\u2032 is the transition probability from oi to o\u2032 w.r.t. edge\ntype \u03c8(oi, o\u2032). If there are n edges from oi that belong to edge type \u03c8(oi, o \u2032), then \u03c8(oi, o\u2032) = 1n .\nPROOF. Assume that p = oi \u2192 o\u2032 \u2192 \u00b7 \u00b7 \u00b7 \u2192 oj is a meta path instance of length l. According to definition of PCRW:\ns(oi, oj | p[1 : l]) = p\u03c8(oi,o \u2032) oi\u2192o\u2032 \u00d7 s(o\u2032, oj | p[2 : l]) (4)\nwhere p[i : j] is the subsequence of path instance p from the i-th to the j-th objects, and p[2 : l] is a length l \u2212 1 path instance. Then, by summing over all the length-l path instances for Equation 4, we get Property 1.\nBased on Property 1, we develop a dynamic programming approach (Algorithm 1) to calculate the truncated proximities. Basically, we compute the proximity matrix S\u0302k for each k from 1 to l. We first initialize the proximity matrix S\u03020 (lines 1-3). Then, we use the transition function in Property 1 to update the proximity matrix for each k (lines 4-9). If we use PCRW as the meta path based proximity measure, inc(os, o\u2032, S\u0302k\u22121[o\u2032, ot]) in line 9 equals p\u03c8(os,o\n\u2032) os\u2192o\u2032 \u00d7 S\u0302k\u22121[o \u2032, ot]. The algorithm can also be used\nAlgorithm 1: Calculate Truncated Proximity Input: HIN G = (V,E), length threshold l Output: Truncated Proximity Matrix S\u0302l\n1 S\u03020 \u2190 \u2205 2 for os \u2208 V do 3 S\u03020[os, os]\u2190 1.0 4 for k \u2208 [1 \u00b7 \u00b7 \u00b7 l] do 5 S\u0302k \u2190 \u2205 6 for os \u2208 V do 7 for o\u2032 \u2208 neighbor(os) do 8 for (o\u2032, ot) \u2208 S\u0302k\u22121 do 9 S\u0302k[os, ot]\u2190\nS\u0302k[os, ot] + inc(os, o \u2032, S\u0302k\u22121[o \u2032, ot])\n10 return S\u0302l;\nfor PathCount, in which case we set inc(os, o\u2032, S\u0302k\u22121[o\u2032, ot]) = S\u0302k\u22121[o\n\u2032, ot]. We now provide a time complexity analysis for computing the truncated proximities on a HIN using Algorithm 1. For each object os \u2208 V , we need to enumerate all the meta path instances within length l. Suppose the average degree in the HIN is D; then, there are on average lD such instances. Hence, the total time complexity for proximity calculation is O(|V | \u00b7 lD), which is linear to the size of the HIN."}, {"heading": "4.2 Model", "text": "We now introduce our HINE embedding method, which preserves the meta path based proximities between objects as described above. For each pair of objects oi and oj , we use Sigmoid function to define their joint probability, i.e.,\np(oi, oj) = 1\n1 + e\u2212vi\u00b7vj (5)\nwhere vi(or vj) \u2208 Rd is the low-dimensional representation of object oi (or oj). p : V 2 \u2192 R is a probability distribution over a pair of objects in the original HIN.\nIn the original HIN G, the empirical joint probability of oi and oj can be defined as:\np\u0302(oi, oj) = s(oi, oj)\u2211 o\u2032\u2208V s(oi, o \u2032) (6)\nTo preserve the meta path based proximity s(\u00b7, \u00b7), a natural objective is to minimize the distance of these two probability distributions:\nO = dist(p\u0302, p) (7)\nIn this paper, we choose KL-divergence as the distance metric, so we have:\nO = \u2212 \u2211\noi,oj\u2208V\ns(oi, oj) log p(oi, oj) (8)"}, {"heading": "4.3 Negative Sampling", "text": "Directly optimizing the objective function in Equation 8 is problematic. First of all, there is a trivial solution: vi,d = \u221e for all i and all d. Second, it is computationally expensive to calculate the gradient, as we need to sum over all the non-zero proximity scores s(oi, oj) for a specific object oi. To address these problems, we adopt negative sampling proposed in [19], which basically samples a small number of negative objects to enhance the influence of positive objects.\nFormally, we define an objective function for each pair of objects with non-zero meta path based proximity s(oi, oj):\nT (oi, oj) = \u2212 log (1 + e\u2212vivj )\u2212 K\u2211 1 Ev\u2032\u2208Pn(oi)[log (1 + e viv \u2032 )] (9) where K is the times of sampling, and Pn(v) is some noise distribution. As suggested by [19], we set Pn(v) \u221d dout(v)3/4 where dout(v) is the out-degree of v.\nWe adopt the asynchronous stochastic gradient descent (ASGD) algorithm [23] to optimize the objective function in Equation 9. Specifically, in each round of the optimization, we sample some pairs of objects oi and oj with non-zero proximity s(oi, oj). Then, the gradient w.r.t. vi can be calculated as:\n\u2202O \u2202vi = \u2212s(oi, oj) \u00b7\ne\u2212vivj\n1 + e\u2212vivj \u00b7 vj (10)"}, {"heading": "5. EXPERIMENTS", "text": "In this section, we conduct extensive experiments in order to test the effectiveness of our proposed HIN embedding approach. We first introduce our datasets and the set of methods to be compared in Section 5.1. Then, we evaluate the effectiveness of all approaches on five important data mining tasks: network recovery (Section 5.2), classification (Section 5.3), clustering (Section 5.4), k-NN search (Section 5.5) and visualization (Section 5.6). In addition, we conduct a case study (Section 5.7) to compare the quality of top-k lists. We evaluate the influence of parameter l in Section 5.8. Finally, we assess the runtime cost of applying our proposed transformation in Section 5.9."}, {"heading": "5.1 Dataset and Configurations", "text": "Datasets. We use four real datasets in our evaluation. Table 2 shows some statistics about them.\n\u2022 DBLP. The schema of DBLP network is shown in Figure 2(a). We use a subset of DBLP, i.e., DBLP-4-Area taken of [29], which contains 5,237 papers (P), 5,915 authors (A), 18 venues (V), 4,479 topics (T). The authors are from 4 areas: database, data mining, machine learning and information retrieval.\n\u2022 MOVIE. We extracted a subset from YAGO [10], which contains knowledge about movies. The schema of MOVIE network is shown in Figure 2(b). It contains 7,332 movies (M), 10,789 actors (A), 1,741 directors (D), 3,392 producers (P) and 1,483 composers (C). The movies are divided into five genres: action, horror, adventure, sci-fi and crime.\nCompetitors. We compare the following network embedding approaches:\n\u2022 DeepWalk [22] is a recently proposed social network embedding method (see Section 2.3 for details). In our experiment settings, we ignore the heterogeneity and directly feed the HINs for embedding. We use default training settings, i.e., window size w = 5 and length of random walk t = 40.\n\u2022 LINE [31] is a method that preserves first-order and secondorder proximities between vertices (see Section 2.3 for details). For each object, it computes two vectors; one for the first-order and one for the second-order proximities separately and then concatenates them. We use equal representation lengths for the first-order and second-order proximities, and use the default training settings; i.e., number of negative samplings. K = 5, starting value of the learning rate \u03c10 = 0.025, and total number of training samplings T = 100M . Same as DeepWalk, we directly feed the HINs for embedding.\n\u2022 HINE_PC is our HINE model using PathCount as the meta path based proximity. We implemented the process of proximity computing in C++ on a 16GB memory machine with Intel(R) Core(TM) i5-3570 CPU @ 3.4 GHz. By default, we use l = 2. In Section 5.8, we study the influence of parameter l.\n\u2022 HINE_PCRW is our HINE model using PCRW as the meta path based proximity. All the other configurations are the same as those of HINE_PC.\nUnless otherwise stated, the dimensionality d of the embedded vector space equals 10."}, {"heading": "5.2 Network Recovery", "text": "We first compare the effectiveness of different network embedding methods at a task of link recovery. For each type of links (i.e., edges) in the HIN schema, we enumerate all pairs of objects (os, ot) that can be connected by such a link and calculate their proximity in the low-dimensional space after embedding os to vs and ot to vt. Finally, we use the area under ROC-curve (AUC)\nto evaluate the performance of each embedding. For example, for edge type write, we enumerate all pairs of authors ai and papers pj in DBLP and compute the proximity for each pair. Finally, using the real DBLP network as ground-truth, we compute the AUC value for each embedding method.\nThe results for d = 10 are shown in Table 3. Observe that, in general, HINE_PC and HINE_PCRW have better performance compared to LINE and DeepWalk. In order to analyze the reasons behind the bad performance of LINE, we also included two special versions of LINE: LINE_1st (LINE_2nd) is a simple optimization approach that just uses stochastic gradient descent to optimize just the first-order (second-order) proximity among the vertices. LINE_1st has much better performance than LINE_2nd because second-order proximity preservation does not facilitate link prediction. LINE, which concatenates LINE_1st and LINE_2nd vectors, has worse performance than LINE_1st because its LINE_2nd vector component harms link prediction accuracy. This is consistent with our expectation, that training proximities of multiple orders simultaneously is better than training them separately. Among all methods, HINE_PCRW has the best performance in preserving the links of HINs; in the cases where HINE_PCRW loses by other methods, its AUC is very close to them. HINE_PCRW outperforms HINE_PC, which is consistent with results in previous work that find PCRW superior to PC (e.g., [10]). The rationale is that a random walk models proximity as probability, which naturally weighs nearby objects higher compared to remote ones."}, {"heading": "5.3 Classification", "text": "We conduct a task of multi-label classification. For DBLP, we use the areas of authors as labels. For MOVIE, we use the genres of movies as labels. For YELP, we use the restaurant types as labels. For GAME, we use the type of games as labels. We first use different methods to embed the HINs. Then, we randomly partition the samples into a training and a testing set with ratio 4 : 1. Finally, we use k nearest neighbor (k\u2212NN) classifiers with k = 5 to predict the labels of the test samples. We repeat the process for 10 times and compute the average Macro-F1 and Micro-F1 scores to evaluate the performance.\nTable 5 shows the results for d = 10. We can see that all methods have better results on DBLP and YELP, than on MOVIE and GAME. This is because the average degree on MOVIE and GAME is smaller, and the HINs are sparser (See Table 2). Observe that HINE_PC and HINE_PCRW outperform DeepWalk and LINE in DBLP and MOVIE; HINE_PCRW performs the best in both datasets. On YELP, LINE has slightly better results than HINE_PCRW (about 1%), while DeepWalk has a very poor performance. On GAME, DeepWalk is the winner, while LINE and HINE_PCRW perform similarly. Overall, we can see that HINE_PCRW has the best (or close to the best) performance, and LINE has quite good performance. On the other hand, DeepWalk\u2019s performance is not stable.\nWe also measured the performance of the methods for different values of d. Figures 5(a) and 5(b) show the results on DBLP. As d becomes larger, all approaches get better in the beginning, and then converge to a certain performance level. Observe that overall, HINE_PCRW has the best performance in classification tasks."}, {"heading": "5.4 Clustering", "text": "We also conducted clustering tasks to assess the performance of the methods. In DBLP we cluster the authors, in MOVIE we cluster the movies, in YELP we cluster the restaurants, and in GAME we cluster the games. We use the same ground-truth as in Section 5.3. We use normalized mutual information (NMI) to evaluate performance.\nTable 6 shows the results for d = 10. We can see that the performance of all methods in clustering tasks is not as stable as that in classification. All the embedding methods perform well on DBLP, but they have a relatively bad performance on the other three datasets. On DBLP, DeepWalk and HINE have better performance than LINE. On MOVIE, HINE_PC and HINE_PCRW outperform all other approaches. On YELP, LINE has better performance than HINE_PCRW, and DeepWalk has very poor performance. On GAME, DeepWalk outperforms the others. Overall, we can see that HINE_PCRW outperforms all the other methods in the task of clustering.\nFigure 5(c) shows performance of the approaches for different d values on DBLP. Observe that HINE_PCRW in general outper-\nforms all other methods. Only for a narrow range of d (d = 10) DeepWalk slightly outperforms HINE_PCRW (as also shown in Table 6). Generally speaking, HINE_PCRW best preserves the proximities among authors in the task of clustering."}, {"heading": "5.5 k-NN Search", "text": "We compare the performances of three methods, i.e., LINE, DeepWalk and HINE_PCRW, on k-NN search tasks. Specifically, we conduct a case study on DBLP, to compare the quality of k nearest authors for venue WWW in the embedded space. We first evaluate the quality of k-NN search by counting the average number of papers that the authors in the k-NN result have published in WWW, when varying k from 1 to 100 (Figure 4(a)). We can see that the nearest authors found in the embedding by HINE_PCRW have more papers published in WWW compared to the ones found in the spaces of LINE and DeepWalk.\nWe then use the top-k author list for venue WWW in the original DBLP network as ground-truth. We use two different metrics to evaluate the quality of top-k lists in the embedded space, i.e., Spearman\u2019s footrule F and Kendall\u2019s tau K [9]. The results are shown in Figures 4(b) and 4(c). We can see that the top-k list of HINE_PCRW is closer to the one in the original HIN."}, {"heading": "5.6 Visualization", "text": "We compare the performances of all approaches on the task of visualization, which aims to layout an HIN on a 2-dimensional space. Specifically, we first use an embedding method to map DBLP into a vector space, then, we map the vectors of authors to a 2-D space using the t-SNE [16] package.\nThe results are shown in Figure 6. LINE can basically separate the authors from different groups (represented by the same color), but some blue points mixed with other groups. The result of DeepWalk is not very good, as many authors from different groups are mixed together. HINE_PC clearly has better performance than DeepWalk. Compared with LINE, HINE_PC can better separate different groups. Finally, HINE_PCRW\u2019s result is the best among these methods, because it clearly separates the authors from different groups, leaving a lot of empty space between the clusters. This is consistent with the fact that HINE_PCRW has the best performance in classifying the authors of DBLP."}, {"heading": "5.7 Case Study", "text": "We perform a case study, which shows the k-NN objects to a\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\n1 2 3 4 5\nM ac\nro -F\n1\nl\nHINE_PC HINE_PCRW\n(a) Macro-F1 w.r.t. l\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\n1 2 3 4 5\nM ic\nro -F\n1\nl\nHINE_PC HINE_PCRW\n(b) Micro-F1 w.r.t. l\ngiven object in DBLP. Specifically, we show in Table 4 the top-5 closest venues, authors and topics to author Jiawei Han in DBLP.\nBy looking at the results for top-5 venues, we can see that LINE does not give a good top-5 list, as it cannot rank KDD in the top publishing venues of the author. DeepWalk is slightly better, but it ranks ICDM at the 5th position, while HINE_PCRW ranks KDD and ICDM as 1st and 2nd, respectively. Looking at the results for authors, observe that HINE_PCRW gives a similar top-5 list to DeepWalk, except that HINE_PCRW prefers top researchers, e.g., Christos Faloutsos. Looking the results for topics, note that only HINE_PCRW can provide a general topic like \u201cmining\u201d."}, {"heading": "5.8 Effect of the l Threshold", "text": "We now study the effect of l on different data mining tasks, e.g., classification and clustering. Figure 7(a) and (b) shows the results of classification on DBLP. We can see that 1) HINE_PCRW has quite stable performance w.r.t. l, and it achieves its best performance when l = 2. 2) The performance of HINE_PC is best when l = 1. This is because PathCount views meta path instances of different length equally important, while PCRW assigns smaller\nweights to the longer instances when performing the random walk. This explains why HINE_PCRW outperforms HINE_PC in these tasks.\nFigure 7(c) shows the results of clustering on DBLP. The results are similar those of classification, except that HINE_PC has much better performance than HINE_PCRW when l = 1. This is natural, because when l = 1, HINE_PCRW can only capture very local proximities. When l > 1, HINE_PCRW outperforms HINE_PC."}, {"heading": "5.9 Efficiency", "text": "We show in Figure 7(d) the running time for computing all-pair truncated proximities on DBLP with the method described in Section 4.1. We can see that our proposed algorithm can run in a reasonable time and scales well with l. Specifically, in our experiments with l = 2, the time for computing all-pair proximities is less than 1000s. In addition, note that using ASGD to solve our objective function in our experiments is very efficient, taking on average 27.48s on DBLP with d = 10."}, {"heading": "6. CONCLUSION", "text": "In this paper, we study the problem of heterogeneous network embedding for meta path based proximity, which can fully utilize the heterogeneity of the network. We also define an objective function, which aims at minimizing the distance of two distributions, one modeling the meta path based proximities, the other modeling the proximities in the embedded low-dimensional space. We also investigate using negative sampling to accelerate the optimization process. As shown in our extensive experiments, our embedding methods can better recover the original network, and have better performances over several data mining tasks, e.g., classification, clustering and visualization, over the state-of-the-art network embedding methods."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and\nA. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37\u201348. ACM, 2013.\n[2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. Dbpedia: A nucleus for a web of open data. In The Semantic Web, pages 722\u2013735. Springer, 2007. [3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585\u2013591, 2001. [4] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, pages 1247\u20131250, 2008. [5] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases. In AAAI, 2011. [6] S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In CIKM, pages 891\u2013900. ACM, 2015. [7] S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In SIGKDD, pages 119\u2013128. ACM, 2015. [8] T. F. Cox and M. A. Cox. Multidimensional scaling. CRC press, 2000. [9] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. SIAM Journal on Discrete Mathematics, 17(1):134\u2013160, 2003.\n[10] Z. Huang, Y. Zheng, R. Cheng, Y. Sun, N. Mamoulis, and X. Li. Meta structure: Computing relevance in large heterogeneous information networks. In SIGKDD, pages 1595\u20131604, 2016. [11] N. Jayaram, A. Khan, C. Li, X. Yan, and R. Elmasri. Querying knowledge graphs by example entity tuples. TKDE, 27(10):2797\u20132811, 2015. [12] M. Ji, J. Han, and M. Danilevsky. Ranking-based classification of heterogeneous information networks. In SIGKDD, pages 1298\u20131306. ACM, 2011.\n[13] X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based collective classification in heterogeneous information networks. In CIKM, pages 1567\u20131571. ACM, 2012. [14] N. Lao and W. W. Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53\u201367, 2010. [15] M. Ley. The dblp computer science bibliography: Evolution, research issues, perspectives. In SPIRE, pages 1\u201310. Springer, 2002. [16] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008. [17] C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang. Discovering meta-paths in large heterogeneous information networks. In WWW, pages 754\u2013764. ACM, 2015. [18] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [19] T. Mikolov and J. Dean. Distributed representations of words and phrases and their compositionality. pages 3111\u20133119, 2013. [20] D. Mottin, M. Lissandrini, Y. Velegrakis, and T. Palpanas. Exemplar queries: Give me an example of what you need. PVLDB, 7(5):365\u2013376, 2014. [21] S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang. Tri-party deep network representation. In IJCAI, pages 1895\u20131901, 2016. [22] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701\u2013710. ACM, 2014. [23] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages 693\u2013701, 2011. [24] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000. [25] C. Shi, X. Kong, Y. Huang, S. Y. Philip, and B. Wu. Hetesim: A general framework for relevance measure in heterogeneous networks. TKDE, 26(10):2479\u20132492, 2014. [26] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In WWW, pages 697\u2013706, 2007. [27] Y. Sun, R. Barber, M. Gupta, C. C. Aggarwal, and J. Han. Co-author relationship prediction in heterogeneous bibliographic networks. In ASONAM, pages 121\u2013128. IEEE, 2011. [28] Y. Sun, J. Han, C. C. Aggarwal, and N. V. Chawla. When will it happen?: relationship prediction in heterogeneous information networks. In WSDM, pages 663\u2013672. ACM, 2012. [29] Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992\u20131003, 2011. [30] Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. TKDD, 7(3):11, 2013. [31] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067\u20131077. ACM, 2015. [32] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319\u20132323, 2000. [33] D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In SIGKDD, pages 1225\u20131234. ACM, 2016. [34] X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In WSDM, pages 283\u2013292. ACM, 2014. [35] J. Zhang, P. S. Yu, and Z.-H. Zhou. Meta-path based multi-network collective link prediction. In SIGKDD, pages 1286\u20131295. ACM, 2014.\nFigure 4: Results of Top-k lists for venue WWW\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n4 8 12 16 20\nM ac\nro -F\n1\nd\n(a) Macro-F1 w.r.t. d\nLINE DeepWalk HEMP_PC HEMP_PCRW\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n4 8 12 16 20\nM ic\nro -F\n1\nd\n(b) Micro-F1 w.r.t. d\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n4 8 12 16 20\nN M\nI\nd\n(c) NMI w.r.t. d\nFigure 5: Performance of Classification and Clustering w.r.t. d"}], "references": [{"title": "Distributed large-scale natural graph factorization", "author": ["A. Ahmed", "N. Shervashidze", "S. Narayanamurthy", "V. Josifovski", "A.J. Smola"], "venue": "WWW, pages 37\u201348. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "The Semantic Web, pages 722\u2013735. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS, volume 14, pages 585\u2013591", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "SIGMOD, pages 1247\u20131250", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "AAAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["S. Cao", "W. Lu", "Q. Xu"], "venue": "CIKM, pages 891\u2013900. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["S. Chang", "W. Han", "J. Tang", "G.-J. Qi", "C.C. Aggarwal", "T.S. Huang"], "venue": "SIGKDD, pages 119\u2013128. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multidimensional scaling", "author": ["T.F. Cox", "M.A. Cox"], "venue": "CRC press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Comparing top k lists", "author": ["R. Fagin", "R. Kumar", "D. Sivakumar"], "venue": "SIAM Journal on Discrete Mathematics, 17(1):134\u2013160", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Meta structure: Computing relevance in large heterogeneous information networks", "author": ["Z. Huang", "Y. Zheng", "R. Cheng", "Y. Sun", "N. Mamoulis", "X. Li"], "venue": "SIGKDD, pages 1595\u20131604", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Querying knowledge graphs by example entity tuples", "author": ["N. Jayaram", "A. Khan", "C. Li", "X. Yan", "R. Elmasri"], "venue": "TKDE, 27(10):2797\u20132811", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Ranking-based classification of heterogeneous information networks", "author": ["M. Ji", "J. Han", "M. Danilevsky"], "venue": "SIGKDD, pages 1298\u20131306. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Meta path-based collective classification in heterogeneous information networks", "author": ["X. Kong", "P.S. Yu", "Y. Ding", "D.J. Wild"], "venue": "CIKM, pages 1567\u20131571. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Machine learning, 81(1):53\u201367", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The dblp computer science bibliography: Evolution", "author": ["M. Ley"], "venue": "research issues, perspectives. In SPIRE, pages 1\u201310. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Discovering meta-paths in large heterogeneous information networks", "author": ["C. Meng", "R. Cheng", "S. Maniu", "P. Senellart", "W. Zhang"], "venue": "WWW, pages 754\u2013764. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "pages 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Exemplar queries: Give me an example of what you need", "author": ["D. Mottin", "M. Lissandrini", "Y. Velegrakis", "T. Palpanas"], "venue": "PVLDB, 7(5):365\u2013376", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Tri-party deep network representation", "author": ["S. Pan", "J. Wu", "X. Zhu", "C. Zhang", "Y. Wang"], "venue": "IJCAI, pages 1895\u20131901", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "SIGKDD, pages 701\u2013710. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Hetesim: A general framework for relevance measure in heterogeneous networks", "author": ["C. Shi", "X. Kong", "Y. Huang", "S.Y. Philip", "B. Wu"], "venue": "TKDE, 26(10):2479\u20132492", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "WWW, pages 697\u2013706", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Co-author relationship prediction in heterogeneous bibliographic networks", "author": ["Y. Sun", "R. Barber", "M. Gupta", "C.C. Aggarwal", "J. Han"], "venue": "ASONAM, pages 121\u2013128. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "When will it happen?: relationship prediction in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "C.C. Aggarwal", "N.V. Chawla"], "venue": "WSDM, pages 663\u2013672. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "PVLDB, 4(11):992\u20131003", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks", "author": ["Y. Sun", "B. Norick", "J. Han", "X. Yan", "P.S. Yu", "X. Yu"], "venue": "TKDD, 7(3):11", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW, pages 1067\u20131077. ACM", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Structural deep network embedding", "author": ["D. Wang", "P. Cui", "W. Zhu"], "venue": "SIGKDD, pages 1225\u20131234. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["X. Yu", "X. Ren", "Y. Sun", "Q. Gu", "B. Sturt", "U. Khandelwal", "B. Norick", "J. Han"], "venue": "WSDM, pages 283\u2013292. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Meta-path based multi-network collective link prediction", "author": ["J. Zhang", "P.S. Yu", "Z.-H. Zhou"], "venue": "SIGKDD, pages 1286\u20131295. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 21, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 30, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 14, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 13, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 16, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 19, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 28, "context": "Meta path [29] is a recently proposed proximity model in HINs.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance.", "startOffset": 139, "endOffset": 143}, {"referenceID": 28, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 27, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 34, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 33, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 182, "endOffset": 190}, {"referenceID": 12, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 182, "endOffset": 190}, {"referenceID": 29, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 206, "endOffset": 210}, {"referenceID": 6, "context": "Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.", "startOffset": 49, "endOffset": 56}, {"referenceID": 20, "context": "Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.", "startOffset": 49, "endOffset": 56}, {"referenceID": 28, "context": "As shown in [29], meta paths with too large lengths are not very informative; therefore, we only consider meta paths up to a given length threshold l.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "We also investigate the use of negative sampling [19] in order to accelerate the optimization process.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": ", LINE [31] and DeepWalk [22]), which do not consider meta path based proximity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": ", LINE [31] and DeepWalk [22]), which do not consider meta path based proximity.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "The problem of classifying objects in a HIN by authority propagation is studied in [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Follow-up work [13] investigates a collective classification problem in HINs using meta path based dependencies.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "PathSelClus [30] is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing some examples as seeds.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 27, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 34, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 33, "context": "A related problem is entity recommendation in HINs [34], which takes advantage of the different types of relationships in HINs to provide better recommendations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "Meta path [29] is a general model for the proximity between objects in a HIN.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "PathCount measures the number of meta path instances connecting the two objects, and PathSim is a normalized version of it [29].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "Path constrained random walk (PCRW) was firstly proposed [14] for the task of relationship retrieval over bibliographic networks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Later, [17] proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "Finally, HeteSim [25] is recently proposed as an extension of meta path based SimRank.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 7, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 23, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 31, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 0, "context": "Graph factorization [1] finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix.", "startOffset": 20, "endOffset": 23}, {"referenceID": 30, "context": "However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in [31].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Recently, DeepWalk [22] is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "DeepWalk combines random walk proximity with the SkipGram model [18], a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence.", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "Second, as pointed out in [31], DeepWalk can only preserve secondorder proximity, leading to poor performance in some tasks, such as link recover and classification, which require first-order proximity to be well-preserved.", "startOffset": 26, "endOffset": 30}, {"referenceID": 30, "context": "LINE [31] is a recently proposed embedding approach for largescale networks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "GraRep [6] further extends DeepWalk to utilize high-order proximities.", "startOffset": 7, "endOffset": 10}, {"referenceID": 32, "context": "SDNE [33] is a semi-supervised deep model that captures the non-linear structural information over the network.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information.", "startOffset": 11, "endOffset": 14}, {"referenceID": 20, "context": "Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "Each different type of a relationship is modeled by a meta path [29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "\u2022 Path Constrained Random Walk (PCRW) [14] is a more sophisticated way to define the proximity s(os, ot | pos\u2192ot) based on an instance pos\u2192ot .", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "As pointed out in [29], shorter meta paths are more informative than longer ones, because longer meta paths link more remote objects (which are less related semantically).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": ", [22] reserves only second-order proximity, and [31] first-order and second-order proximities).", "startOffset": 2, "endOffset": 6}, {"referenceID": 30, "context": ", [22] reserves only second-order proximity, and [31] first-order and second-order proximities).", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "To address these problems, we adopt negative sampling proposed in [19], which basically samples a small number of negative objects to enhance the influence of positive objects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "As suggested by [19], we set Pn(v) \u221d dout(v) where dout(v) is the out-degree of v.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "We adopt the asynchronous stochastic gradient descent (ASGD) algorithm [23] to optimize the objective function in Equation 9.", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": ", DBLP-4-Area taken of [29], which contains 5,237 papers (P), 5,915 authors (A), 18 venues (V), 4,479 topics (T).", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "We extracted a subset from YAGO [10], which contains knowledge about movies.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "We extracted from Freebase [4] a HIN, which is related to video games.", "startOffset": 27, "endOffset": 30}, {"referenceID": 21, "context": "\u2022 DeepWalk [22] is a recently proposed social network embedding method (see Section 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "\u2022 LINE [31] is a method that preserves first-order and secondorder proximities between vertices (see Section 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": ", Spearman\u2019s footrule F and Kendall\u2019s tau K [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Specifically, we first use an embedding method to map DBLP into a vector space, then, we map the vectors of authors to a 2-D space using the t-SNE [16] package.", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "A network embedding is a representation of a large graph in a lowdimensional space, where vertices are modeled as vectors. The objective of a good embedding is to preserve the proximity (i.e., similarity) between vertices in the original graph. This way, typical search and mining methods (e.g., similarity search, kNN retrieval, classification, clustering) can be applied in the embedded space with the help of off-the-shelf multidimensional indexing approaches. Existing network embedding techniques focus on homogeneous networks, where all vertices are considered to belong to a single class. Therefore, they are weak in supporting similarity measures for heterogeneous networks. In this paper, we present an effective heterogeneous network embedding approach for meta path based proximity measures. We define an objective function, which aims at minimizing the distance between two distributions, one modeling the meta path based proximities, the other modeling the proximities in the embedded vector space. We also investigate the use of negative sampling to accelerate the optimization process. As shown in our extensive experimental evaluation, our method creates embeddings of high quality and has superior performance in several data mining tasks compared to state-of-the-art network embedding methods.", "creator": "LaTeX with hyperref package"}}}