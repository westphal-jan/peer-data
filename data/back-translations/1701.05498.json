{"id": "1701.05498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "abstract": "We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. the translation and rotation of non-textured rigid objects. The dataset contains thirty industry-relevant objects without significant texture and without discriminatory color or reflection properties. The objects exhibit symmetries and mutual similarities in shape and / or size. Compared to other datasets, a unique feature is that some of the objects are parts of others. The dataset includes training and test images taken with three synchronized sensors, in particular a structured light and a flight-time-dependent RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images of each sensor. In addition, two types of 3D models are provided for each object, i.e. a manually generated CAD model and a semi-structurally reconstructed RGB-D model show training images with very different test objects in front of twenty test objects, with very different black scenes.", "histories": [["v1", "Thu, 19 Jan 2017 16:16:36 GMT  (4094kb,D)", "http://arxiv.org/abs/1701.05498v1", "WACV 2017"]], "COMMENTS": "WACV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["tomas hodan", "pavel haluza", "stepan obdrzalek", "jiri matas", "manolis lourakis", "xenophon zabulis"], "accepted": false, "id": "1701.05498"}, "pdf": {"name": "1701.05498.pdf", "metadata": {"source": "CRF", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "authors": ["Tom\u00e1\u0161 Hoda\u0148", "Pavel Haluza", "\u0160t\u011bp\u00e1n Obdr\u017e\u00e1lek", "Ji\u0159\u0131\u0301 Matas", "Manolis Lourakis", "Xenophon Zabulis"], "emails": [], "sections": [{"heading": null, "text": "ing the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less."}, {"heading": "1. Introduction", "text": "Texture-less rigid objects are common in human environments and the need to learn, detect and accurately localize them from images arises in a variety of applications. The pose of a rigid object has six degrees of freedom, i.e. three in translation and three in rotation, and its full knowledge is often required. In robotics, for example, the 6D object pose facilitates spatial reasoning and allows an end-effector to act upon an object. In an augmented reality scenario, ob-\nject pose can be used to enhance one\u2019s perception of reality by augmenting objects with extra information such as hints for assembly guidance.\nThe visual appearance of a texture-less object is dominated by its global shape, color, reflectance properties, and the configuration of light sources. The lack of texture implies that the object cannot be reliably recognized with traditional techniques relying on photometric local patch detectors and descriptors [9, 31]. Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or\nar X\niv :1\n70 1.\n05 49\n8v 1\n[ cs\n.C V\n] 1\n9 Ja\nn 20\n17\nglobal description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27]. Therefore, RGBD data consisting of aligned color and depth images, obtained with widely available Kinect-like sensors, have come to play an important role.\nIn this paper, we introduce a new public dataset for 6D pose estimation of texture-less rigid objects. An overview of the included objects and test scenes is provided in Fig. 2. The dataset features thirty commodity electrical parts which have no significant texture, discriminative color or distinctive reflectance properties, and often bear similarities in shape and/or size. Furthermore, a unique characteristic of the objects is that some of them are parts of others. For example, objects 7 and 8 are built up from object 6, object 9 is made of three copies of object 10 stacked next to each other, whilst the center part of objects 17 and 18 is nearly identical to object 13. Objects exhibiting similar properties are common in industrial environments.\nThe dataset includes training and test images captured with a triplet of sensors, i.e. a structured light RGB-D sensor Primesense Carmine 1.09, a time-of-flight RGB-D sensor Microsoft Kinect v2, and an RGB camera Canon IXUS 950 IS. The sensors were time-synchronized and had similar perspectives. All images were obtained with an automatic procedure that systematically sampled images from a view sphere, resulting in ~39K training and ~10K test images from each sensor. The training images depict objects in isolation with a black background, while the test images originate from twenty table-top scenes with arbitrarily arranged objects. Complexity of the test scenes varies from those with several isolated objects and a clean background to very challenging ones with multiple instances of several objects and with a high amount of occlusion and clutter. Additionally, the dataset contains two types of 3D mesh models for each object; one manually created in CAD software and one semi-automatically reconstructed from the training RGB-D images. All occurrences of the modeled objects in the training and test images are annotated with accurate ground truth 6D poses; see Fig. 1 for their qualitative and Sec. 4.1 for their quantitative evaluation.\nThe dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17]. Since images from three sensors are available, one may also study the importance of different input modalities for a given problem. Another option is to use the training images for evaluating 3D object reconstruction methods [44], where the provided CAD models can serve as the ground truth.\nOur objectives in designing T-LESS were to provide a dataset of a substantial but manageable size, with a rigorous and complete ground truth annotation that is accurate to the level of sensor resolution, and with a significant variability\nin complexity, so that it would provide different levels of difficulty and be reasonably future-proof, i.e. solvable, but not solved by the current state-of-the-art methods. The difficulty of the dataset for 6D object pose estimation is demonstrated by the relatively low performance of the method by Hodan\u030c et al. [24]. This method otherwise achieves a performance close to the state of the art on the well-established dataset of Hinterstoisser et al. [20].\nThe remainder of the paper is organized as follows. Sec. 2 reviews related datasets, Sec. 3 describes technical details of the acquisition and post-processing of the T-LESS dataset, Sec. 4 assesses the accuracy of the ground truth poses and provides initial evaluation results, and Sec. 5 concludes the paper."}, {"heading": "2. Related Datasets", "text": "First we review datasets for estimating the 6D pose of specific rigid objects, grouped by the type of provided images, then we mention a few datasets designed for similar problems. If not stated otherwise, these datasets supply ground truth annotations in the form of 6D object poses."}, {"heading": "2.1. RGB-D Datasets", "text": "Only a few public RGB-D datasets, from over one hundred reported by Firman in [15], enable the evaluation of 6D object pose estimation methods. Most of the datasets reviewed in this section were captured with Microsoft Kinect v1 or Primesense Carmine 1.09, which represent the first generation of consumer-grade RGB-D sensors operating on the structured-light principle. The dataset introduced in [17] was captured with Microsoft Kinect v2, which is based on the time-of-flight principle.\nFor texture-less objects, the dataset of Hinterstoisser et al. [20] has become a standard benchmark used in most of the recent work, e.g. [38, 4, 47, 24, 54]. It contains 15 texture-less objects represented by a color 3D mesh model. Each object is associated with a test sequence consisting of ~1200 RGB-D images, each of which includes exactly one instance of the object. The test sequences feature significant 2D and 3D clutter, but only mild occlusion, and since the objects have discriminative color, shape and/or size, their recognition is relatively easy. In the 6D localization problem (where information about the number and identity of objects present in the images is provided beforehand [23]), state-of-the-art methods achieve recognition rates that exceed 95% for most of the objects. Brachmann et al. [4] provided additional ground truth poses for all modeled objects in one of the test sequences from [20]. This extended annotation introduces challenging test cases with various levels of occlusion and allows the evaluation of multiple object localization, with each object appearing in a single instance.\nTejani et al. [47] presented a dataset with 2 texture-less and 4 textured objects. For each object, a color 3D mesh model is provided together with a test sequence of over 700 RGB-D images. The images show several object instances with no to moderate occlusion, and with 2D and 3D clutter. Doumanoglou et al. [14] provide a dataset with 183 test images of 2 textured objects from [47] that appear in multiple instances in a challenging bin-picking scenario with heavy occlusion. Furthermore, they provide color 3D mesh\nmodels of another 6 textured objects and 170 test images depicting the objects placed on a kitchen table.\nThe Challenge and Willow datasets [58], which were collected for the 2011 ICRA Solutions in Perception Challenge, share a set of 35 textured household objects. Training data for each object is given in the form of 37 RGB-D training images that show the object from different views, plus a color point cloud obtained by merging the training images. The Challenge and Willow datasets respectively contain 176 and 353 test RGB-D images of several objects in single instances placed on top of a turntable. The Willow dataset also features distractor objects and object occlusion. Similar is the TUW dataset [1] that presents 17 textured and texture-less objects appearing in 224 test RGB-D images. Instead of a turntable setup, images were obtained by moving a robot around a static cluttered environment with some objects appearing in multiple instances.\nThe Rutgers dataset [37] is focused on perception for robotic manipulation during pick-and-place tasks and comprises of images from a cluttered warehouse environment. It includes color 3D mesh models for 24 mostly textured objects from the Amazon Picking Challenge 2015 [11], that were captured in more than 10K test RGB-D images with various amounts of occlusion.\nAldoma et al. [2] provide 3D mesh models without color information of 35 household objects that are both textured and texture-less and are often symmetric and mutually similar in shape and size. There are 50 test RGB-D images of table-top scenes with multiple objects in single instances, with no clutter and various levels of occlusion.\nThe BigBIRD dataset [42] includes images of 125 mostly textured objects that were captured in isolation on a turntable with multiple calibrated RGB-D and DSLR sensors. For each object, the dataset provides 600 RGB-D point clouds, 600 high-resolution RGB images, and a color 3D mesh model reconstructed from the point clouds. Since BigBIRD was acquired under very controlled conditions, it is not concerned with occlusions, clutter, lighting changes or varying object-sensor distance. Georgakis et al. [17] provide 6735 test RGB-D images from kitchen scenes including a subset of the BigBIRD objects. Ground truth for objects in the test images is provided only in the form of 2D bounding boxes and 3D point labeling.\nLai et al. [29] created an extensive dataset with 300 common household objects captured on a turntable from three elevations. It contains 250K segmented RGB-D images and 22 annotated video sequences with a few hundred RGB-D frames in each. Ground truth is provided only in the form of approximate rotation angles for training images and in the form of 3D point labeling for test images.\nSchlette et al. [40] synthesized RGB-D images from simulated object manipulation scenarios involving 4 textureless objects from the Cranfield assembly benchmark [10].\nSeveral small datasets that were used for evaluation of the SHOT descriptor are provided by Salti et al. [39]. These datasets include synthetic data as well as data acquired with a spacetime-stereo method and an RGB-D sensor."}, {"heading": "2.2. Depth-only and RGB-only Datasets", "text": "The depth-only dataset of Mian et al. [34] includes 3D mesh models of 5 objects and 50 test depth images acquired with an industrial range scanner. The test scenes contain only the modeled objects that occlude each other. A similar dataset is provided by Taati et al. [46]. The Desk3D dataset [3] comprises of 3D mesh models for 6 objects which are captured in over 850 test depth images with occlusion, clutter and similarly looking distractor objects. The dataset was obtained with an RGB-D sensor, however only the depth images are publicly available.\nThe IKEA dataset by Lim et al. [30] provides RGB images with objects being aligned with their exactly matched 3D models. Crivellaro et al. [12] supply 3D CAD models and annotated RGB sequences with 3 highly occluded and texture-less objects. Mun\u0303oz et al. [36] provide RGB sequences of 6 texture-less objects that are each imaged in isolation against a clean background and without occlusion. Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes."}, {"heading": "2.3. Datasets for Similar Problems", "text": "The RGB-D dataset of Michel et al. [35] is focused on articulated objects, where the goal is to estimate the 6D pose of each object part, subject to the constraints introduced by their joints. There are also datasets for categorical pose estimation. For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images annotated with 6D object poses. The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes. The PASCAL3D+ [57] and the ObjectNet3D [56] provide generic 3D models and ground truth 6D poses, but only RGB images."}, {"heading": "3. The T-LESS Dataset", "text": "Compared to the reviewed datasets, T-LESS is unique in its combination of the following characteristics. It contains 1) a larger number of industry-relevant objects, 2) training images captured under controlled conditions, 3) test images with large viewpoint changes, objects in multiple instances, affected by clutter and occlusion; including test cases that are challenging even for the state-of-the-art methods, 4) images captured with a synchronized and calibrated triplet of sensors, 5) accurate ground truth 6D poses for all modeled objects, and 6) two types of 3D models for each object.\nThe rest of the section describes the process of dataset preparation, which includes image acquisition, camera calibration, depth correction, 3D object model generation and the ground truth pose annotation."}, {"heading": "3.1. Acquisition Setup", "text": "The training and test images were captured with the aid of the setup shown in Fig. 3. It consists of a turntable, where the imaged objects were placed, and a jig with adjustable tilt, to which the sensors were attached. A marker field used for camera pose estimation was affixed to the turntable. The field was extended vertically to the sides of the turntable to\nfacilitate pose estimation at lower elevations. To capture training images, the objects were placed in the middle of the turntable and in the front of a black screen, which ensured a uniform background at all elevations. To introduce a non-uniform background in the test images, a sheet of plywood with markers at its edges was placed on the top of the turntable. In some scenes, the objects were placed on the top of other objects (e.g. books) to give them different elevations and thus invalidate a ground plane assumption that might be made by an evaluated method. The depth of object surfaces in the training and test images is in the range 0.53 \u2212 0.92m, which is within the sensing ranges of the used RGB-D sensors that are 0.35\u2212 1.4m for Carmine and 0.5\u2212 4.5m for Kinect."}, {"heading": "3.2. Calibration of Sensors", "text": "Intrinsic and distortion parameters of the sensors were estimated with the standard checkerboard-based procedure using OpenCV [6]. The root mean square re-projection error calculated at corners of the calibration checkerboard squares is 0.51 px for Carmine, 0.35 px for Kinect, and 0.43 px for Canon. For the RGB-D sensors, the calibration was performed with the RGB images. The depth images were aligned to the RGB images using the factory depthto-color registration available through manufacturer\u2019s SDKs (OpenNI 2.2 and Kinect for Windows SDK 2.0). The color and aligned depth images, which are included in the dataset, are already processed to remove radial distortion. The intrinsic parameters can be found at the dataset website.\nAll sensors were synchronized and extrinsically calibrated with respect to the turntable, making it possible to register any pair of images. Synchronization was essential since the images were taken while the turntable was spinning. The extrinsic calibration was achieved using fiducial BCH-code markers from ARToolKitPlus [52]. Specifically, the detection of particular markers in an image combined with the knowledge of their physical location on the turntable provided a set of 2D-3D correspondences. These were used to estimate the camera pose in the turntable coordinate system by robustly solving the PnP problem and then refining the estimated 6D pose by non-linearly minimizing the cumulative re-projection error with the posest library from [31]. The root mean square re-projection error, which was calculated at marker corners in all test images, is 1.27 px for Carmine, 1.37 px for Kinect, and 1.50 px for Canon. This measure combines errors in sensor calibration, marker field detection and sensor pose estimation and is therefore larger than the aforementioned error in sensor intrinsic calibration."}, {"heading": "3.3. Training and Test Images", "text": "A common strategy for dealing with poorly textured objects is to adopt a template-based approach trained on object\nimages that are acquired with a dense sampling of viewpoints, e.g. [13, 20, 38, 24]. To support such approaches, T-LESS offers training images of every object in isolation from a full view sphere. These images were obtained with a systematic acquisition procedure which uniformly sampled elevation from 85\u25e6 to \u221285\u25e6 with a 10\u25e6 step and the complete azimuth range with a 5\u25e6 step. Views from the upper and lower hemispheres were captured separately, turning the object upside down in between. In total, there are 18 \u00d7 72 = 1296 training images per object from each sensor. Exceptions are objects 19 and 20, for which only views from the upper hemisphere were captured, specifically 648 images from elevation 85\u25e6 to 5\u25e6. These objects are horizontally symmetric at the pose in which they were placed on the turntable, thus the views from the upper hemisphere are sufficient to capture their appearance. Test scenes were captured from a view hemisphere with a 10\u25e6 step in elevation (ranging from 75\u25e6 to 15\u25e6) and a 5\u25e6 step in azimuth. A total of 7 \u00d7 72 = 504 test images were captured per scene by each sensor.\nTo remove irrelevant parts of the scene in the images periphery, the provided images are cropped versions of the captured ones. Resolution of the provided images is as follows: 400 \u00d7 400 px for training RGB-D images from Carmine and Kinect, 1900\u00d7 1900 px for training RGB images from Canon, 720\u00d7540 px for test RGB-D images from Carmine and Kinect and 2560 \u00d7 1920 px for test RGB images from Canon. Sample images are shown in Fig. 4.\nParts of the marker field were visible in some of the training images, especially at lower elevations. These were masked to ensure a black background everywhere around the objects. To achieve this, we identified an object mask in an image by back-projecting its CAD model and gradually darkened the image moving from the mask perimeter towards the image border."}, {"heading": "3.4. Depth Correction", "text": "Similarly to [16, 45], we observed that the depths measured by the RGB-D sensors exhibit a systematic error. To remove it, we collected depth measurements d at projections of the marker corners and computed their expected depth values de from the known marker coordinates. The measurements were collected from the depth range 0.53 \u2013 0.92 m in which the objects appear in the training and test images. We found the following linear correction models by least squares fitting: dc = 1.0247 \u00b7d\u22125.19 for Carmine, and dc = 1.0266 \u00b7 d \u2212 26.88 for Kinect (depth measured in mm). In [45], only scaling is used for the depth correction. According to Foix et al. [16], a 3-degree polynomial function suffices to correct depth in the 1 \u2013 2 m range. In our case, a narrower range is used and we found a simple linear polynomial to adequately account for the error: the correction reduced the mean absolute difference from the\nexpected depth de from 12.4 mm to 2.8 mm for Carmine and from 7.0 mm to 3.6 mm for Kinect. The estimated correction was applied to all depth images, requiring no further action from the dataset user."}, {"heading": "3.5. 3D Object Models", "text": "For each object, a manually created CAD model and a semi-automatically reconstructed model are available (Fig. 5). Both models are provided in the form of 3D meshes with surface normals at model vertices. Surface color is included only for the reconstructed models. The normals were calculated using MeshLab [7] as the angleweighted sum of face normals incident to a vertex [48].\nThe reconstructed models were created using fastfusion, a volumetric 3D mapping system by Steinbru\u0308cker et al. [44]. The input to fastfusion were the RGB-D training images from Carmine and the associated camera poses estimated using the fiducial markers (see Sec. 3.2). For each object, two partial models were first reconstructed, one for the upper and another for the lower view hemisphere. The partial models were then aligned using the iterative closest point (ICP) algorithm applied to their vertices. This was followed by manual refinement that ensured correct registration of surface details that are visible only in color. The resulting alignment was applied to the camera poses to transform them into a common reference frame, and the updated poses were used to reconstruct the full object model from all images. These models contained some minor artifacts, e.g. small spikes, which were removed manually. It is noted that some of the objects contain small shiny metal parts whose depth is not reliably captured by the current depth sensors; in general, any glossy or translucent surface is problematic. Hence, some of these parts, such as the plug poles, were not reconstructed.\nThe reconstructed models were aligned to the CAD models using the ICP algorithm and the alignment was further refined manually. Models of both types are therefore defined in the same coordinate system and the provided ground truth poses are valid for both of them. The origin of the model coordinate system coincides with the center of the bounding box of the CAD model.\nThe geometrical similarity of the two model types was assessed by calculating the average distance from vertices of the reconstructed models to the closest surface points of the corresponding CAD models. The average distance over all object models was found to be 1.01 mm, which is very low compared to the size of objects that ranges from 58.13 mm for object 13 to 217.16 mm for object 8. Distances in the opposite direction, i.e. from the CAD models to the reconstructed models, are not informative since some CAD models contain inner parts that are not represented in the reconstructed models. The Metro software by Cignoni et al. [8] was used to measure the model differences."}, {"heading": "3.6. Ground Truth Poses", "text": "To obtain ground truth 6D object poses for images of a test scene, a dense 3D model of the scene was first reconstructed with the system of Steinbru\u0308cker et al. [44]. This was accomplished using all 504 RGB-D images of the scene along with the sensor poses estimated using the turntable markers. The CAD object models were then manually aligned to the scene model. To increase accuracy, the object models were rendered into several selected high-resolution scene images from Canon, misalignments were identified and the poses were manually refined accordingly. This process was repeated until a satisfactory alignment of the renderings with the scene images was achieved. The final poses were distributed to all test images with the aid of the known camera-to-turntable coordinate transformations. The transformed poses are provided as the ground truth poses with each test image."}, {"heading": "4. Design Validation and Experiments", "text": "This section presents an accuracy assessment of the ground truth poses and examines the difficulty of T-LESS with a recent 6D localization method."}, {"heading": "4.1. Accuracy of the Ground Truth Poses", "text": "Aiming to evaluate the accuracy of the ground truth poses, we compared the captured depth images, after the correction described in Sec. 3.4, with depth images obtained by graphically rendering the 3D object models at the ground truth poses. At each pixel with a valid depth value in both images, we calculated the difference \u03b4 = dc \u2212 dr, where dc is the captured and dr is the rendered depth. Table 1 presents statistics of these differences, aggregated over all\ntraining and test depth images. Differences exceeding 5 cm and amounting to around 2.5 % of the measurements were considered to be outliers and were pruned before calculating the statistics. The outlying differences may be caused by erroneous depth measurements, or by occlusion induced by distractor objects in the case of test images.\nThe rendered depths align well with the depths captured by Carmine, as indicated by the mean difference \u00b5\u03b4 being close to zero. In the case of Kinect, we observed that the RGB and depth images are slightly misregistered, which is the cause of the positive bias in \u00b5\u03b4 . The average absolute difference \u00b5|\u03b4| is less than 5 mm for Carmine and 9 mm for Kinect, which is near the accuracy of the sensors [28] and is relatively small compared to the size of objects. The error statistics are slightly favorable for the reconstructed models (as opposed to the CAD models), as they were obtained from the captured depth images and therefore exhibit similar characteristics and artifacts. For example, the plug poles are invisible to the RGB-D sensors and are missing in the reconstructed models, but are present in the CAD models."}, {"heading": "4.2. 6D Localization", "text": "The recent template-based method of Hodan\u030c et al. [24] was evaluated on the 6D localization problem. The input is comprised of a test image together with the identities of object instances that are present in the image, and the goal is to estimate the 6D poses of these instances [23]. The method was evaluated on all test RGB-D images from the Carmine sensor. The parameters were set as described in [24], the templates were generated from the training images from Carmine, and the CAD models were employed in the pose refinement stage as detailed in [59]. Pose estimates were evaluated as in [20], using the average distance error for objects with indistinguishable views. This error measures the misalignment between the surface of model M at the ground truth pose (R\u0304, t\u0304) and at the estimated pose (R\u0302, t\u0302), and is defined as:\ne = avg x1\u2208M min x2\u2208M \u2225\u2225\u2225(R\u0304x1 + t\u0304)\u2212 (R\u0302x2 + t\u0302)\u2225\u2225\u2225 2 .\nPose estimate (R\u0302, t\u0302) is considered correct if e \u2264 k \u00b7 d, where k = 0.1 and d is the largest distance between any pair of model vertices, i.e. the object diameter. Only the ground truth poses at which at least 10% of the object surface is visible were considered for the evaluation. The visibility was estimated as in [23].\nThe performance is measured by recall, i.e. the percentage of the ground truth poses for which a correct pose was estimated. Fig. 6 presents achieved recall per object (top) and recall per scene (middle). The objects with the lowest recall are those that are similar to other objects. For example, object 1 is often confused with object 2, as are objects 20, 21 and 22. Likewise, test scenes containing similar objects are harder, with the hardest one being scene 20 that\ncontains many similar objects and severe occlusions. Bottom of Fig. 6 plots the recall, accumulated over all objects, as a function of the fraction of their image projection that is unoccluded. The recall increases proportionally with this fraction, illustrating that occlusion is one of the main challenges in T-LESS.\nThe achieved mean recall over all objects is 67.2%, which suggests a significant margin for improvement. We note that the same method achieved a mean recall of 95.4% on the dataset of Hinterstoisser et al. [20], which is close to the state of the art: [20] reports 96.6% and [5] reports 99.0% on this dataset. The latter is not directly comparable since it was calculated only over 13 out of 15 objects included in the dataset."}, {"heading": "5. Conclusion", "text": "This paper has presented T-LESS, a new dataset for evaluating 6D pose estimation of texture-less objects that can facilitate systematic comparison of pertinent methods. The dataset features industry-relevant objects and is characterized by a large number of training and test images, accurate 6D ground truth poses, multiple sensing modalities, test scenes with multiple object instances and with increasing difficulty due to occlusion and clutter. Initial evaluation results using the dataset indicate that the state of the art in 6D object pose estimation has ample room for improvement.\nThe T-LESS dataset is available online at: cmp.felk.cvut.cz/t-less"}, {"heading": "Acknowledgements", "text": "This work was supported by the Technology Agency of the Czech Republic research program TE01020415 (V3C \u2013 Visual Computing Competence Center), CTU student grant SGS15/155/OHK3/2T/13, and the European Commission FP7 DARWIN Project, Grant No. 270138. The help of Jan Pola\u0301s\u030cek and Avgousta Hatzidaki in creating the CAD models is gratefully acknowledged."}], "references": [{"title": "Automation of \u201cground truth\u201d annotation for multi-view RGB-D object instance recognition datasets", "author": ["A. Aldoma", "T. F\u00e4ulhammer", "M. Vincze"], "venue": "In IROS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A global hypotheses verification method for 3D object recognition", "author": ["A. Aldoma", "F. Tombari", "L. Di Stefano", "M. Vincze"], "venue": "In ECCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Robust instance recognition in presence of occlusion and clutter", "author": ["U. Bonde", "V. Badrinarayanan", "R. Cipolla"], "venue": "In ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning 6D object pose estimation using 3D object coordinates", "author": ["E. Brachmann", "A. Krull", "F. Michel", "S. Gumhold", "J. Shotton", "C. Rother"], "venue": "In ECCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB", "author": ["E. Brachmann", "F. Michel", "A. Krull", "M.Y. Yang", "S. Gumhold", "C. Rother"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning OpenCV: Computer vision with the OpenCV library", "author": ["G. Bradski", "A. Kaehler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Ranzuglia. MeshLab: an open-source mesh processing tool", "author": ["P. Cignoni", "M. Callieri", "M. Corsini", "M. Dellepiane", "F. Ganovelli"], "venue": "In Eurographics Italian Chapter Conf.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Metro: measuring error on simplified surfaces", "author": ["P. Cignoni", "C. Rocchini", "R. Scopigno"], "venue": "In Computer Graphics Forum,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "The MOPED framework: Object recognition and pose estimation for manipulation", "author": ["A. Collet", "M. Martinez", "S.S. Srinivasa"], "venue": "IJRR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The development of a European benchmark for the comparison of assembly robot programming systems", "author": ["K. Collins", "A. Palmer", "K. Rathmill"], "venue": "In Robot technology and applications", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Lessons from the Amazon picking challenge", "author": ["N. Correll", "K.E. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J.M. Romano", "P.R. Wurman"], "venue": "arXiv preprint arXiv:1601.05484,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A novel representation of parts for accurate 3D object detection and tracking in monocular images", "author": ["A. Crivellaro", "M. Rad", "Y. Verdie", "K.M. Yi", "P. Fua", "V. Lepetit"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Real-time learning and detection of 3D texture-less objects: A scalable approach", "author": ["D. Damen", "P. Bunnun", "A. Calway", "W. Mayol-Cuevas"], "venue": "In BMVC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Recovering 6D object pose and predicting next-best-view in the crowd", "author": ["A. Doumanoglou", "R. Kouskouridas", "S. Malassiotis", "T.-K. Kim"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "RGBD datasets: Past, present and future", "author": ["M. Firman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Lock-in time-of-flight (ToF) cameras: a survey", "author": ["S. Foix", "G. Alenya", "C. Torras"], "venue": "Sensors Journal,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Multiview RGB-D dataset for object instance detection", "author": ["G. Georgakis", "M.A. Reza", "A. Mousavian", "P.-H. Le", "J. Kosecka"], "venue": "arXiv preprint arXiv:1609.07826,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Support surface prediction in indoor scenes", "author": ["R. Guo", "D. Hoiem"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A comprehensive performance evaluation of 3D local feature descriptors", "author": ["Y. Guo", "M. Bennamoun", "F. Sohel", "M. Lu", "J. Wan", "N.M. Kwok"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes", "author": ["S. Hinterstoisser", "V. Lepetit", "S. Ilic", "S. Holzer", "G. Bradski", "K. Konolige", "N. Navab"], "venue": "In ACCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Going further with point pair features", "author": ["S. Hinterstoisser", "V. Lepetit", "N. Rajkumar", "K. Konolige"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Efficient texture-less object detection for augmented reality guidance", "author": ["T. Hoda\u0148", "D. Damen", "W. Mayol-Cuevas", "J. Matas"], "venue": "In ISMARW,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Obdr\u017e\u00e1lek. On evaluation of 6D object pose estimation", "author": ["T. Hoda\u0148", "J. Matas"], "venue": "In ECCV Workshop on Recovering 6D Object Pose,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Detection and fine 3D pose estimation of textureless objects in RGB-D images", "author": ["T. Hoda\u0148", "X. Zabulis", "M. Lourakis", "\u0160. Obdr\u017e\u00e1lek", "J. Matas"], "venue": "In IROS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Occlusion reasoning for object detection under arbitrary viewpoint", "author": ["E. Hsiao", "M. Hebert"], "venue": "TPAMI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "A category-level 3D object dataset: Putting the Kinect to work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": "In Consumer Depth Cameras for Computer Vision", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Deep learning of local RGB-D patches for 3D object detection and 6D pose estimation", "author": ["W. Kehl", "F. Milletari", "F. Tombari", "S. Ilic", "N. Navab"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Accuracy and resolution of Kinect depth data for indoor mapping applications", "author": ["K. Khoshelham", "S. Elberink"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A large-scale hierarchical multi-view RGB-D object dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "In ICRA,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Parsing IKEA Objects: Fine Pose Estimation", "author": ["J.J. Lim", "H. Pirsiavash", "A. Torralba"], "venue": "In ICCV,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Model-based pose estimation for rigid objects", "author": ["M. Lourakis", "X. Zabulis"], "venue": "In Computer Vision Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Mobile 3D object detection in clutter", "author": ["D. Meger", "J.J. Little"], "venue": "In IROS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "On the repeatability and quality of keypoints for local feature-based 3D object retrieval from cluttered scenes", "author": ["A. Mian", "M. Bennamoun", "R. Owens"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Threedimensional model-based object recognition and segmentation in cluttered scenes", "author": ["A.S. Mian", "M. Bennamoun", "R. Owens"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Pose estimation of kinematic chain instances via object coordinate regression", "author": ["F. Michel", "A. Krull", "E. Brachmann", "M.Y. Yang", "S. Gumhold", "C. Rother"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Fast 6D pose estimation for texture-less objects from a single RGB image", "author": ["E. Mu\u00f1oz", "Y. Konishi", "V. Murino", "A.D. Bue"], "venue": "In ICRA,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "A dataset for improved RGBD-based object detection and pose estimation for warehouse pickand-place", "author": ["C. Rennie", "R. Shome", "K.E. Bekris", "A.F.D. Souza"], "venue": "RA-L,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Discriminatively trained templates for 3D object detection: A real time scalable approach", "author": ["R. Rios-Cabrera", "T. Tuytelaars"], "venue": "In ICCV,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "A new benchmark for pose estimation with ground truth from virtual reality", "author": ["C. Schlette"], "venue": "Production Engineering,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Indoor segmentation and support inference from RGBD images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "BigBIRD: A large-scale 3D database of object instances", "author": ["A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel"], "venue": "In ICRA,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Sun RGB-D: A RGB-D scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "In CVPR,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Volumetric 3D mapping in real-time on a CPU", "author": ["F. Steinbr\u00fccker", "J. Sturm", "D. Cremers"], "venue": "In ICRA,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A benchmark for the evaluation of RGB-D SLAM systems", "author": ["J. Sturm", "N. Engelhard", "F. Endres", "W. Burgard", "D. Cremers"], "venue": "In IROS,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Variable dimensional local shape descriptors for object recognition in range data", "author": ["B. Taati", "M. Bondy", "P. Jasiobedzki", "M. Greenspan"], "venue": "In ICCV,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "Latent-class hough forests for 3D object detection and pose estimation", "author": ["A. Tejani", "D. Tang", "R. Kouskouridas", "T.-K. Kim"], "venue": "In ECCV,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Computing vertex normals from polygonal facets", "author": ["G. Th\u00fcrrner", "C.A. W\u00fcthrich"], "venue": "Journal of Graphics Tools,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1998}, {"title": "Online learning for automatic segmentation of 3D data", "author": ["F. Tombari", "L. Di Stefano", "S. Giardino"], "venue": "In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "BOLD features to detect texture-less objects", "author": ["F. Tombari", "A. Franchi", "L. Di Stefano"], "venue": "In ICCV,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Unique signatures of histograms for local surface description", "author": ["F. Tombari", "S. Salti", "L. Di Stefano"], "venue": "In ECCV,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "ARToolKitPlus for pose tracking on mobile devices", "author": ["D. Wagner", "D. Schmalstieg"], "venue": "CVWW,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "UoB highly occluded object challenge II, 2016", "author": ["K. Walas", "A. Leonardis"], "venue": "www.cs.bham.ac.uk/research/ projects/uob-hooc", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Learning descriptors for object recognition and 3D pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "In CVPR,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "3DNet: Large-scale object class recognition from CAD models", "author": ["W. Wohlkinger", "A. Aldoma", "R.B. Rusu", "M. Vincze"], "venue": "In ICRA,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}, {"title": "ObjectNet3D: A large scale database for 3D object recognition", "author": ["Y. Xiang"], "venue": "In ECCV,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Beyond PASCAL: A benchmark for 3D object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "In Winter Conference on Applications of Computer Vision,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Multimodal blending for high-accuracy instance recognition", "author": ["Z. Xie", "A. Singh", "J. Uang", "K.S. Narayan", "P. Abbeel"], "venue": "In IROS,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "3D object pose refinement in range images", "author": ["X. Zabulis", "M. Lourakis", "P. Koutlemanis"], "venue": "In ICVS,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The lack of texture implies that the object cannot be reliably recognized with traditional techniques relying on photometric local patch detectors and descriptors [9, 31].", "startOffset": 163, "endOffset": 170}, {"referenceID": 30, "context": "The lack of texture implies that the object cannot be reliably recognized with traditional techniques relying on photometric local patch detectors and descriptors [9, 31].", "startOffset": 163, "endOffset": 170}, {"referenceID": 32, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 49, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 18, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 19, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 23, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 52, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 4, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 13, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 20, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 26, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 22, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 157, "endOffset": 165}, {"referenceID": 21, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 157, "endOffset": 165}, {"referenceID": 47, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 16, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 42, "context": "Another option is to use the training images for evaluating 3D object reconstruction methods [44], where the provided CAD models can serve as the ground truth.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Only a few public RGB-D datasets, from over one hundred reported by Firman in [15], enable the evaluation of 6D object pose estimation methods.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "The dataset introduced in [17] was captured with Microsoft Kinect v2, which is based on the time-of-flight principle.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "[20] has become a standard benchmark used in most of the recent work, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 45, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 52, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "In the 6D localization problem (where information about the number and identity of objects present in the images is provided beforehand [23]), state-of-the-art methods achieve recognition rates that exceed 95% for most of the objects.", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "[4] provided additional ground truth poses for all modeled objects in one of the test sequences from [20].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[4] provided additional ground truth poses for all modeled objects in one of the test sequences from [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "[47] presented a dataset with 2 texture-less and 4 textured objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] provide a dataset with 183 test images of 2 textured objects from [47] that appear in multiple instances in a challenging bin-picking scenario with heavy occlusion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[14] provide a dataset with 183 test images of 2 textured objects from [47] that appear in multiple instances in a challenging bin-picking scenario with heavy occlusion.", "startOffset": 71, "endOffset": 75}, {"referenceID": 56, "context": "The Challenge and Willow datasets [58], which were collected for the 2011 ICRA Solutions in Perception Challenge, share a set of 35 textured household objects.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Similar is the TUW dataset [1] that presents 17 textured and texture-less objects appearing in 224 test RGB-D images.", "startOffset": 27, "endOffset": 30}, {"referenceID": 36, "context": "The Rutgers dataset [37] is focused on perception for robotic manipulation during pick-and-place tasks and comprises of images from a cluttered warehouse environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "It includes color 3D mesh models for 24 mostly textured objects from the Amazon Picking Challenge 2015 [11], that were captured in more than 10K test RGB-D images with various amounts of occlusion.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "[2] provide 3D mesh models without color information of 35 household objects that are both textured and texture-less and are often symmetric and mutually similar in shape and size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "The BigBIRD dataset [42] includes images of 125 mostly textured objects that were captured in isolation on a turntable with multiple calibrated RGB-D and DSLR sensors.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "[17] provide 6735 test RGB-D images from kitchen scenes including a subset of the BigBIRD objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] created an extensive dataset with 300 common household objects captured on a turntable from three elevations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] synthesized RGB-D images from simulated object manipulation scenarios involving 4 textureless objects from the Cranfield assembly benchmark [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[40] synthesized RGB-D images from simulated object manipulation scenarios involving 4 textureless objects from the Cranfield assembly benchmark [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 33, "context": "[34] includes 3D mesh models of 5 objects and 50 test depth images acquired with an industrial range scanner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The Desk3D dataset [3] comprises of 3D mesh models for 6 objects which are captured in over 850 test depth images with occlusion, clutter and similarly looking distractor objects.", "startOffset": 19, "endOffset": 22}, {"referenceID": 29, "context": "[30] provides RGB images with objects being aligned with their exactly matched 3D models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] supply 3D CAD models and annotated RGB sequences with 3 highly occluded and texture-less objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] provide RGB sequences of 6 texture-less objects that are each imaged in isolation against a clean background and without occlusion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 48, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 37, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 24, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 34, "context": "[35] is focused on articulated objects, where the goal is to estimate the 6D pose of each object part, subject to the constraints introduced by their joints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images annotated with 6D object poses.", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images annotated with 6D object poses.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 127, "endOffset": 131}, {"referenceID": 55, "context": "The PASCAL3D+ [57] and the ObjectNet3D [56] provide generic 3D models and ground truth 6D poses, but only RGB images.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "The PASCAL3D+ [57] and the ObjectNet3D [56] provide generic 3D models and ground truth 6D poses, but only RGB images.", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "Intrinsic and distortion parameters of the sensors were estimated with the standard checkerboard-based procedure using OpenCV [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 50, "context": "The extrinsic calibration was achieved using fiducial BCH-code markers from ARToolKitPlus [52].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "These were used to estimate the camera pose in the turntable coordinate system by robustly solving the PnP problem and then refining the estimated 6D pose by non-linearly minimizing the cumulative re-projection error with the posest library from [31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 19, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 37, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 23, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 15, "context": "Similarly to [16, 45], we observed that the depths measured by the RGB-D sensors exhibit a systematic error.", "startOffset": 13, "endOffset": 21}, {"referenceID": 43, "context": "Similarly to [16, 45], we observed that the depths measured by the RGB-D sensors exhibit a systematic error.", "startOffset": 13, "endOffset": 21}, {"referenceID": 43, "context": "In [45], only scaling is used for the depth correction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[16], a 3-degree polynomial function suffices to correct depth in the 1 \u2013 2 m range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The normals were calculated using MeshLab [7] as the angleweighted sum of face normals incident to a vertex [48].", "startOffset": 42, "endOffset": 45}, {"referenceID": 46, "context": "The normals were calculated using MeshLab [7] as the angleweighted sum of face normals incident to a vertex [48].", "startOffset": 108, "endOffset": 112}, {"referenceID": 42, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] was used to measure the model differences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The average absolute difference \u03bc|\u03b4| is less than 5 mm for Carmine and 9 mm for Kinect, which is near the accuracy of the sensors [28] and is relatively small compared to the size of objects.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "[24] was evaluated on the 6D localization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The input is comprised of a test image together with the identities of object instances that are present in the image, and the goal is to estimate the 6D poses of these instances [23].", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "The parameters were set as described in [24], the templates were generated from the training images from Carmine, and the CAD models were employed in the pose refinement stage as detailed in [59].", "startOffset": 40, "endOffset": 44}, {"referenceID": 57, "context": "The parameters were set as described in [24], the templates were generated from the training images from Carmine, and the CAD models were employed in the pose refinement stage as detailed in [59].", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "Pose estimates were evaluated as in [20], using the average distance error for objects with indistinguishable views.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "The visibility was estimated as in [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "[24] on the 6D localization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], which is close to the state of the art: [20] reports 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], which is close to the state of the art: [20] reports 96.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "6% and [5] reports 99.", "startOffset": 7, "endOffset": 10}], "year": 2017, "abstractText": "We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.", "creator": "TeX"}}}