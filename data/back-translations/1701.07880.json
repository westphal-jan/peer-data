{"id": "1701.07880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "emLam -- a Hungarian Language Modeling baseline", "abstract": "This work aims to compensate for the lack of documented foundations for Hungarian language modeling. Different approaches are evaluated on the basis of three publicly available Hungarian corpora. Perplexity values comparable to models of similar sized English corpora are reported. A new, freely downloadable Hungarian benchmark corpus is introduced.", "histories": [["v1", "Thu, 26 Jan 2017 21:18:32 GMT  (151kb)", "http://arxiv.org/abs/1701.07880v1", "Additional resources: - the emLam repository:this https URL- the emLam corpus:this http URL"]], "COMMENTS": "Additional resources: - the emLam repository:this https URL- the emLam corpus:this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["d\\'avid m\\'ark nemeskey"], "accepted": false, "id": "1701.07880"}, "pdf": {"name": "1701.07880.pdf", "metadata": {"source": "CRF", "title": "emLam \u2013 a Hungarian Language Modeling baseline", "authors": ["D\u00e1vid M\u00e1rk Nemeskey"], "emails": ["nemeskeyd@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Language modeling (LM) is an integral part of several NLP applications, such as speech recognition, optical character recognition and machine translation. It has been shown that the quality of the LM has a significant effect on the performance of these systems [5, 7]. Accordingly, evaluating language modeling techniques is a crucial part of research. For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8]. Lately, the One Billion Word Benchmark corpus (1B) [8] was published for the sole reason of measuring progress in statistical language modeling.\nThe last decade saw dramatic advances in the field of language modeling. Training corpora grew from a fewmillion words (e.g. the Brown corpus) to gigaword, such as 1B, while vocabulary size increased from a few 10k to several hundred thousands. Neural networks [3, 21, 19] overtook n-grams as the language model of choice. State-of-theart LSTMp networks achieve up to 55% reductions in perplexity compared to 5-gram models [14].\nSurprisingly, these developments left few traces in the Hungarian NLP literature. Aside from an interesting line of work on morphological modeling for speech recognition [23, 18], no study is known to the author that addresses issues of Hungarian language modeling. While quality works have been published in related fields, language model performance is often not reported, or is not competitive: e.g. in their otherwise state-ofthe-art system, Tarj\u00e1n et al. [28] use a 3-gram model that achieves a perplexity of 4001 on the test set \u2014 a far cry from the numbers reported in [8] and here.\nIn this paper, we mean to fill this gap in two ways. First, we report baselines for various language modeling methods on three publicly available Hungarian corpora. Hungarian poses a challenge to word-based LM because of its agglutinative nature. The proliferation of word forms inflates the vocabulary and decreases the number of contexts a word form is seen during training, making the data sparsity problem much more 1 Personal communication with the author.\npronounced than it is for English. This makes it especially interesting to see how the performance of the tested methods translate to Hungarian.\nSecond, we present a version of the Hungarian Webcorpus [11] that can be used as a benchmark for LM models. Our motivation was to create the Hungarian equivalent of the One Billion Word Benchmark corpus for English: a freely available data set that is large enough to enable the building of high-quality LMs, yet small enough not to pose a barrier to entry to researchers. We hope that the availability of the corpus will facilitate research into newer and better LM techniques for Hungarian.\nThe software components required to reproduce this work, as well as the benchmark corpus, comprise the emLam module2 of e-magyar.hu [30]. The scripts have been released as free software under the MIT license, and can be downloaded from the emLam repository3.\nThe rest of the paper is organized as follows. The benchmark corpora, as well as our solution to the data sparsity problem is described in Section 2. In Section 3 we formally define the language modeling task and introduce the methods evaluated. Results are presented in Section 4. Finally, Section 5 contains our conclusions and ideas left for future work."}, {"heading": "2 The Hungarian Datasets", "text": "We selected three publicly available Hungarian corpora for benchmarking. The corpora are of various sizes and domains, which enabled us to evaluate both small- and largevocabulary LM configurations. The corpus sizes roughly correspond to those of the English corpora commonly used for LM benchmarks, making a comparison between the two languages easier.\nThe Szeged Treebank [31] is the largest manually annotated corpus of Hungarian. The treebank consists of CoNLL-style tsv files; we used a version in which the morphological features had been converted to KR codes to keep in line with the automatic toolchain described below. At around 1.5 million tokens, it is similar in size to the Penn Treebank [16], allowing us a direct comparison of small-vocabulary LM techniques.\nThe filtered version of the Hungarian Webcorpus [11] is a semi-gigaword corpus at 589m tokens. It consists of webpages downloaded from the .hu domain that contain an \u201cacceptable number of spelling mistakes\u201d. The downloadable corpus is already tokenized; we further processed it by performing lemmatization, morphological analysis and disambiguation with Hunmorph [29]: ocamorph for the former two and hunlex for the latter.\nThe Hungarian Gigaword Corpus (MNSZ2) [25] is the largest public Hungarian corpus. At around 1G tokens, it is comparable in size to the English 1B corpus. We preprocessed the raw text with the same tools as above.\nWe decided to use the \u2018old\u2019 hun* tools because at the time of writing, the e-magyar toolchain was not yet production ready, and the version of the Szeged corpus that uses the new universal POS tags still contained conversion errors. Therefore, the results published here might be slightly different from what one can attain by running the scripts 2 http://e-magyar.hu/hu/textmodules/emlam 3 http://github.com/dlt-rilmta/emLam\nin the emLam repository, should the issues above be addressed. However, any such differences will be, most likely, insignificant."}, {"heading": "2.1 Preprocessing", "text": "As mentioned before, the main challenge of modeling an agglutinative language is the number of distinct word forms. The solution that works well for English \u2014 putting all word forms into the vocabulary \u2014 is not reasonable: on one hand, the vocabulary size would explode (see Table 1); on the other, there is a good chance the training set does not contain all possible word forms in the language.\nThe most common solution in the literature is to break up the words into smaller segments [12, 2, 4]. The two main directions are statistical and morphological word segmentation. While good results have been reported with the former, we opted for the latter: not only is it linguistically more motivated, it also ensures that the tokens we end up with are meaningful, making the LM easier to debug.\nWe ran the aforementioned pipeline on all words in the corpus, and split all inflectional prefixes (as well as some derivational ones, such as <COMPAR>, <SUPERLAT>) into separate tokens. Only inflections marked by the KR code are included; the default zero morphemes (the nominative case marker and the present-tense third person singular for verbs) are not. A few examples:\njelmondat\u00e1val \u2192 jelmondat <POSS> <CAS<INS>> akartak \u2192 akar <PAST> <PLUR>\nOne could say that by normalizing the text like this, we \u201ddeglutenized\u201d it; therefore, the resulting variants of the corpora shall be referred to as \u201dgluten-free\u201d (GLF) from now on.\nThe full preprocessing pipeline is as follows:\n1. Tokenization and normalization. The text was lowercased, converted to utf-8 and and deglutenized 2. (Webcorpus only) Duplicates sentences were removed, resulting in a 32.5% reduction in corpus size. 3. Tokens below a certain frequency count were converted into <unk> tokens. The word distribution proved different from English: with the same threshold as in the 1B corpus (3), much more distinct tokens types remained. To be able to test LMs with a vocabulary size comparable to 1B, we worked with different thresholds for the two gigaword corpora: Webcorpus was cut at 5 words, MNSZ2 at 10. An additional thresholding level was introduced at 30 (50) tokens to make RNN training tractable. 4. Sentence order was randomized 5. The data was divided into train, development and test sets; 90%\u20135%\u20135% respec-\ntively.\nTable 1 lists the main attributes of the datasets created from the three corpora.Where not explicitly marked, the default count threshold (3) is used. The corresponding English corpora are included for comparison. It is clear from comparing the raw and GLF\ndatasets that deglutenization indeed decreases the size of the vocabulary and the number of OOVs by about 50%. Although not shown in the table, this reduction ratio remains consistent among the various thresholding levels.\nAlso apparent is that, compared to the English corpora, the number of unique tokens is much bigger even in the default Hungarian GLF datasets. Preliminary inquiry into the data revealed that three phenomena account for the majority of the token types between the 3 and 30 (50) count marks: compound nouns, productive derivations and named entities (with mistyped words coming in at fourth place). Since neither the Szeged corpus, nor (consequently) the available morphological disambiguators take compounding and derivation into account, no immediate solution was available for tackling these issues. Therefore, we decided to circumvent the problem by introducing the higher frequency thresholds and concentrating on the problem of inflections in this study.\nThe preprocessing scripts are available in the emLam repository."}, {"heading": "2.2 The Benchmark Corpus", "text": "Of the three corpora above, the Hungarian Webcorpus is the only one that is freely downloadable and available under a share-alike license (Open Content). Therefore, we decided to make not only the scripts, but the preprocessed corpus as well, similarly available for researchers.\nThe corpus can be downloaded as a list of tab-separated files. The three columns are the word, lemma and disambiguated morphological features. A unigram (word and lemma) frequency dictionary is also attached, to help create count-thresholded versions. The corpus is available under the Creative Commons Share-alike (CC SA) license.\nSuch a corpus could facilitate language modeling research in two ways. First, any result published using the corpus is easily reproducible. Second, the fact that it has been\npreprocessed similarly to the English 1B corpus, makes comparisons such as those in this paper possible and meaningful."}, {"heading": "3 Language Modeling", "text": "The task of (statistical) language modeling is to assign a probability to a word sequence S = w1, ..., wN . In this paper, we only consider sentences, but other choices (paragraphs, documents, etc.) are also common. Furthermore, we only concern ourselves with generative models, where the probability of a word does not depend on subsequent tokens. The probability of S can then be decomposed using the chain rule, as\nP (S) = P (w1, ..., wN ) = N\u2211 i=1 P (wi|w1, ..., wi\u22121). (1)\nThe condition (w1, ..., wi\u22121) is called the context of wi. One of the challenges of language modeling is that the number of possible contexts is infinite, while the training set is not. Because of this, the full context is rarely used; LMs approximate it and deal with the data sparsity problem in various ways.\nIn the following, we introduce some of the state-of-the-art methods in discrete and continuous language modeling."}, {"heading": "3.1 N-grams", "text": "N-gram models work under the Markov assumption, i.e. the current word only depends on n\u2212 1 preceding words:\nP (wi|w1, ..., wi\u22121) \u2248 P (wi|wi\u2212n+1, ..., wi\u22121). (2)\nAn n-gram model is a collection of such conditional probabilities. The data sparsity problem is addressed by smoothing the probability estimation in two ways: backoff models recursively fall back to coarser (n\u22121, n\u22122, ...-gram) models when the context of a word was not seen during training, while interpolated models always incorporate the lower orders into the probability estimation.\nA variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10]. We used the implementation in the SRILM [27] library, and tested two configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned interpolated model5. All datasets described in Table 1 were evaluated; in addition, we also tested a GLF POS model, where lemmas were replaced with their respective POS tags.\n4 -kndiscount 5 -kndiscount -gt1min 1 -gt2min 1 -gt3min 1 -gt4min 1 -gt5min 1 -interpolate1 -interpolate2 -interpolate3 -interpolate4 -interpolate5"}, {"heading": "3.2 Class-based n-grams", "text": "Class-basedmodels exploit the fact that certainwords are similar to others w.r.t. meaning or syntactic function. By clustering words into classes C according to these features, a class-based n-gram model estimates the probability of the next word as\nP (wi|w1, ..., wi\u22121, c1, ..., ci\u22121) \u2248 P (wi|ci)P (ci\u2212n+1, ..., ci\u22121). (3)\nThis is a Hidden Markov Model (HMM), where the classes are the hidden states and the words are the observations. The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24]. In this paper, we chose the latter, as a full morphological analysis was already available as a by-product of deglutenization.\nIt is generally agreed that class-based models perform poorly by themselves, but improve word-based models when interpolated with them."}, {"heading": "3.3 RNN", "text": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14]. In particular, LSTM [13] models represent the state-of-the-art on the 1B dataset [14]. The power of RNNs come from two sources: first, words are projected into a continuous vector space, thereby alleviating the sparsity issue; and second, their ability to encode the whole context into their state, thereby \u201dremembering\u201d much further back than n-grams. The downside is that it can take weeks to train an RNN, whereas an n-gram model can be computed in a few hours.\nWe ran two RNN baselines:\n1. the Medium regularized LSTM setup in [32]. We used the implementation6 in Tensorflow [1] 2. LSTM-512-512, the smallest configuration described in [14], which uses LSTMs with a projection layer [26]. The model was reimplemented in Tensorflow, and is available from the emLam repository.\nDue to time and resource constraints, the first baseline was only run on the Szeged corpus, and the second only on the smallest, GLF-30 (50) datasets."}, {"heading": "3.4 Language Model Evaluation", "text": "The standard metric of language model quality is perplexity (PPL), which measures how well the model predicts the text data. Intuitively, it shows how many options the LM considers for each word; the lower the better. The perplexity of the sequencew1, ..., wN is computed as\nPPL = 2H = 2 \u2211N i=1 \u2212 1 N log2 P (wi|w1,...,wi\u22121), (4)\nwhere H is the cross-entropy. 6 https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb\nLanguage models typically perform worse when tested on a different corpus, due to the differences in vocabulary, word distribution, style, etc. To see how significant this effect is, the models were not only evaluated on the test split of their training corpus, but on the other two corpora as well."}, {"heading": "4 Evaluation", "text": "The results achieved by the n-gram models are reported in Table 2\u20135. Table 2 lists the perplexities achieved by KN 5-grams of various kinds; the odd one out is POS GLF, where the limited vocabulary enabled us to create up to 9-gram models. For MNSZ2, the reported score is from the 7-gram model, which outperformed 8- and 9-grams.\nSimilar results reported by others on the PTB and 1B are included for comparison. A glance at the table shows that while word-based 5-grams performed much worse than their counterparts in English, the GLF-based models achieved similar scores.\nWhile the perplexities of GLFmodels onWebcorpus andMNSZ2 are comparatively close, the perplexities of the word models are about 50% higher on Webcorpus. Finding the cause of this discrepancy requires further research. Two possible candidates are data sparsity (at the same vocabulary size, Webcorpus is 25% smaller) and a difference in the distribution of inflection configurations.\nTable 3 shows the best n-gram perplexities achieved by GLF models. It can be seen that interpolated, unpruned models perform much better than backoff models.\nMeasuring class-based model performance led to surprising results. As mentioned earlier, the general consensus is that interpolating class- and word-based LMs benefit the performance of the latter; however, our findings (Table 4) did not confirm this. The class-based model could only improve on the unigram model, and failed to do so for the higher orders. The most likely explanation is that as the size of the vocabulary grows larger, the emission entropy increases, which is mirrored by the perplexity. This would explain why class-based n-grams seem to work on small corpora, such as the PTB, but not on MNSZ2.\nAnother point of interest is the diminishing returns of PPL reductions as the n-gram orders grow. While we have not experimented with 6-grams or higher orders, it seems probable that performance of GLF models would peak at 6- or 7-grams on MNSZ2 (and Webcorpus). For word-based models, this saturation point arrives much earlier: while not reported in the table, the perplexity difference between 4- and 5-grammodels is only 1-2 point. This implies that GLF models are less affected by data sparsity.\nIt is a well-known fact that the performance of LMs degrade substantially when they are not evaluated on the corpus they were trained on. This effect is clearly visible in Table 5. It is also evident, however, that GLF datasets suffer from this problem to a much lesser extent: while the perplexity more than doubled for the word-based MNSZ2 LMs, it only increased by 50\u201360% for GLF models. A similar effect can be observed between the full and GLF POS models.\nInterestingly, the Webcorpus word models exhibit the smallest perplexity increase of 10-15%. Contrasting this result with Table 2 seems to suggest that there exists a trade-off between predictive power and universality. However, it is worth noting that the performance of these word models still lags well behind that of GLF models.\nFinally, Table 6 reports the perplexities achieved by the RNN models. Two conclusions can be drawn from the numbers. First, in line with what has been reported for English by many authors, RNNs clearly outperform even the best n-gram models. Second, the numbers are similar to those reported in the original papers for English. This,\ntogether with similar observations above for n-grams, proves that once the \u201dcurse of agglutination\u201d is dealt with, a GLF Hungarian is no more difficult to model than English."}, {"heading": "5 Conclusion", "text": "This work contributes to Hungarian language modeling in two ways. First, we reported state-of-the-art LM baselines for three Hungarian corpora, from million to gigaword size. We found that raw, word-level LMs performed worse than they do for English, but when the text was split into lemmas and inflectional affixes (the \u201dgluten-free\u201d format), results were comparable to those reported on similar-sized English corpora.\nSecond, we introduced a benchmark corpus for language modeling. To our knowledge, this is the first such dataset for Hungarian. This specially prepared version of the Hungarian Webcorpus is freely available, allowing researchers to easily and reproducibly experiment with new language modeling techniques. It is comparable in size to the One Billion Word Benchmark corpus of English, making comparisons between the two languages easier."}, {"heading": "5.1 Future Work", "text": "While the methods reported here can be called state-of-the-art, many similarly effective modeling approaches are missing. Evaluating them could provide additional insight into how Hungarian \u201dworks\u201d or how Hungarian and English should be modeled differently. Understanding the unusual behaviour of word models on Webcorpus also calls for further inquiry into language and corpus structure.\nThe performance of themodels here wasmeasured in isolation. Putting them into use (maybe with some adaptation) in NLP applications such as ASR or ML could answer the question of whether the reduction in perplexity translates to similar reductions in WER or BLEU.\nThe most glaring problem touched upon, but not addressed, in this paper, is the effect of compounding and derivation on vocabulary size. Away to reduce the number of words could be amore thorough deglutenization algorithm,whichwould split compound words into their parts and strip productive derivational suffixes, while leaving frozen ones such as h\u00e1z\u00b7as\u00b7s\u00e1g untouched. This could indeed be a case when a gluten free diet does make one slimmer."}, {"heading": "Acknowledgements", "text": "This work is part of the e-magyar framework and was supported by the Research Infrastructure Development Grant, Category 2, 2015 of the Hungarian Academy of Sciences."}], "references": [{"title": "TensorFlow: Large-scalemachine learning on heterogeneous systems, 2015", "author": ["Mart\u0131n Abadi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the use of morphological analysis for dialectal Arabic speech recognition.", "author": ["Mohamed Afify", "Ruhi Sarikaya", "Hong-Kwang Jeff Kuo", "Laurent Besacier", "Yuqing Gao"], "venue": "INTERSPEECH", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A Botha", "Phil Blunsom"], "venue": "In: ICML", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Large Language Models in Machine Translation", "author": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Class\u2013 based n\u2013gram models of natural language", "author": ["P.F. Brown", "V.J. Della Pietra", "P.V. de Souza", "J.C. Lai", "R.L. Mercer"], "venue": "Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "DanBikel,Maria Shugrina, PatrickNguyen, and ShankarKumar. Large Scale Language Modeling in Automatic Speech Recognition", "author": ["Ciprian Chelba"], "venue": "Tech. rep. Google,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "One billion word benchmark for measuring progress in statistical languagemodeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "In: INTERSPEECH2014, 15th Annual Conference of the International Speech Communication Association, Singapore,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Tech. rep. TR-10-98", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A bit of progress in language modeling", "author": ["Joshua T. Goodman"], "venue": "Computer Speech & Language", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Tr\u00f3n. \u201cCreating open language resources for Hungarian", "author": ["P\u00e9ter Hal\u00e1csy", "Andr\u00e1s Kornai", "L\u00e1szl\u00f3 N\u00e9meth", "Andr\u00e1s Rung", "Istv\u00e1n Szakad\u00e1t", "Viktor"], "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Morphologically Motivated Language Models in Speech Recognition", "author": ["Teemu Hirsim\u00e4ki", "Mathias Creutz", "Vesa Siivola", "Mikko Kurimo"], "venue": "Proceedings of AKRR\u201905, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning. Espoo, Finland: Helsinki University of Technology, Laboratory of Computer and Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Long Short-TermMemory\u201d. In:Neural Computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "(Nov", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Improved backing-off for m-gram language modeling", "author": ["ReinhardKneser andHermannNey"], "venue": "In: International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["Sven Martin", "J\u00f6rg Liermann", "Hermann Ney"], "venue": "Speech communication", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Improved recognition of spontaneous Hungarian speech \u2014 Morphological and acoustic modeling techniques for a less resourced task", "author": ["P\u00e9ter Mihajlik", "Zolt\u00e1n Tuske", "Bal\u00e1zs Tarj\u00e1n", "Botty\u00e1n N\u00e9meth", "Tibor Fegy\u00f3"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Cernock\u1ef3. \u201cEmpirical Evaluation and Combination of Advanced Language Modeling Techniques.", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan"], "venue": "INTERSPEECH", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3"], "venue": "Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "SLT", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Statisztikai \u00e9s szab\u00e1ly alap\u00fa morfol\u00f3giai elemz\u0151k kombin\u00e1ci\u00f3ja besz\u00e9dfelismer\u0151 alkalmaz\u00e1shoz", "author": ["Botty\u00e1n N\u00e9meth", "P\u00e9ter Mihajlik", "Domonkos Tikk", "Viktor Tr\u00f3n"], "venue": "Proceedings of MSZNY", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Comparison of part-of-speech and automatically derived category-based language models for speech recognition", "author": ["Thomas R Niesler", "Edward WD Whittaker", "Philip C Woodland"], "venue": "In: Acoustics, Speech and Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "The Hungarian Gigaword Corpus", "author": ["Csaba Oravecz", "Tam\u00e1s V\u00e1radi", "B\u00e1lint Sass"], "venue": "Proceedings of LREC", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling.", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "INTERSPEECH", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "SRILM at sixteen: Update and outlook", "author": ["Andreas Stolcke", "Jing Zheng", "WenWang", "Victor Abrash"], "venue": "In:Proceedings of IEEEAutomatic Speech Recognition and Understanding Workshop", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Magyar nyelv\u0171, \u00e9l\u0151 k\u00f6z\u00e9leti- \u00e9s h\u00edrm\u0171sorok g\u00e9pi feliratoz\u00e1sa", "author": ["Bal\u00e1zs Tarj\u00e1n", "\u00c1d\u00e1mVarga", "Zolt\u00e1n Tobler", "Gy\u00f6rgy Szasz\u00e1k", "Tibor Fegy\u00f3", "Csaba Bord\u00e1s", "P\u00e9ter Mihajlik"], "venue": "Proc. MSZNY 2016. Szegedi Tudoma\u0301nyegyetem,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Hunmorph: Open Source Word Analysis", "author": ["Viktor Tr\u00f3n", "Gy\u00f6gy Gyepesi", "P\u00e9ter Hal\u00e1csky", "Andr\u00e1s Kornai", "L\u00e1szl\u00f3 N\u00e9meth", "D\u00e1niel Varga"], "venue": "Proceedings of the ACL Workshop on Software. Ann Arbor, Michigan: Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "e-magyar: digit\u00e1lis nyelvfeldolgoz\u00f3 rendszer", "author": ["Tam\u00e1s V\u00e1radi"], "venue": "XIII. Magyar Sza\u0301m\u0131\u0301to\u0301ge\u0301pes Nyelve\u0301szeti Konferencia (MSZNY2017). Szeged,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "Szeged Corpus 2.5: Morphological Modifications in a Manually POS-tagged Hungarian Corpus", "author": ["Veronika Vincze", "Viktor Varga", "Katalin Ilona Simk\u00f3", "J\u00e1nos Zsibrita", "\u00c1goston Nagy", "Rich\u00e1rd Farkas", "J\u00e1nos Csirik"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914)", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "It has been shown that the quality of the LM has a significant effect on the performance of these systems [5, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "It has been shown that the quality of the LM has a significant effect on the performance of these systems [5, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 9, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "Lately, the One Billion Word Benchmark corpus (1B) [8] was published for the sole reason of measuring progress in statistical language modeling.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Neural networks [3, 21, 19] overtook n-grams as the language model of choice.", "startOffset": 16, "endOffset": 27}, {"referenceID": 19, "context": "Neural networks [3, 21, 19] overtook n-grams as the language model of choice.", "startOffset": 16, "endOffset": 27}, {"referenceID": 13, "context": "State-of-theart LSTMp networks achieve up to 55% reductions in perplexity compared to 5-gram models [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Aside from an interesting line of work on morphological modeling for speech recognition [23, 18], no study is known to the author that addresses issues of Hungarian language modeling.", "startOffset": 88, "endOffset": 96}, {"referenceID": 17, "context": "Aside from an interesting line of work on morphological modeling for speech recognition [23, 18], no study is known to the author that addresses issues of Hungarian language modeling.", "startOffset": 88, "endOffset": 96}, {"referenceID": 26, "context": "[28] use a 3-gram model that achieves a perplexity of 4001 on the test set \u2014 a far cry from the numbers reported in [8] and here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[28] use a 3-gram model that achieves a perplexity of 4001 on the test set \u2014 a far cry from the numbers reported in [8] and here.", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "Second, we present a version of the Hungarian Webcorpus [11] that can be used as a benchmark for LM models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "hu [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "The Szeged Treebank [31] is the largest manually annotated corpus of Hungarian.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "5 million tokens, it is similar in size to the Penn Treebank [16], allowing us a direct comparison of small-vocabulary LM techniques.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "The filtered version of the Hungarian Webcorpus [11] is a semi-gigaword corpus at 589m tokens.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "The downloadable corpus is already tokenized; we further processed it by performing lemmatization, morphological analysis and disambiguation with Hunmorph [29]: ocamorph for the former two and hunlex for the latter.", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "The Hungarian Gigaword Corpus (MNSZ2) [25] is the largest public Hungarian corpus.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 1, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 3, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 14, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 8, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 9, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "We used the implementation in the SRILM [27] library, and tested two configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned interpolated model5.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "We used the implementation in the SRILM [27] library, and tested two configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned interpolated model5.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 16, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 22, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 30, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 13, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 12, "context": "In particular, LSTM [13] models represent the state-of-the-art on the 1B dataset [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "In particular, LSTM [13] models represent the state-of-the-art on the 1B dataset [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "the Medium regularized LSTM setup in [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "We used the implementation6 in Tensorflow [1] 2.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "LSTM-512-512, the smallest configuration described in [14], which uses LSTMs with a projection layer [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "LSTM-512-512, the smallest configuration described in [14], which uses LSTMs with a projection layer [26].", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "PTB [22] N/A 141.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "2 1B [8] 3 90", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "PTB [22] 141.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "2 N/A 1B [8] 90 67.", "startOffset": 9, "endOffset": 12}, {"referenceID": 30, "context": "Medium regularized [32] PTB 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "07 LSTM-512-512 [14] 1B 54.", "startOffset": 16, "endOffset": 20}], "year": 2017, "abstractText": "This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungarian benchmark corpus is introduced.", "creator": "LaTeX with hyperref package"}}}