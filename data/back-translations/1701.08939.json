{"id": "1701.08939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Deep Submodular Functions", "abstract": "We start with an overview of a class of submodular functions called SCMMs (sums of concave, with non-negative modular functions plus a definitive, arbitrary modular one), and then define a new class of submodular functions, which we call \"deep submodular functions\" or \"DSFs.\" We show that DSFs are a flexible parametric family of submodular functions that share many of the properties and benefits of deep neural networks (DNNs). DSFs can be motivated by the consideration of a descriptive concept structure based on floor elements, and where submodular interactions within that hierarchy are to be enabled. The results in this paper show that DSFs are a strictly larger class of submodular functions than SCMMs. We show that for every integer $k & gt; 0 $, there can be a submodular interaction.", "histories": [["v1", "Tue, 31 Jan 2017 08:06:33 GMT  (3646kb,D)", "http://arxiv.org/abs/1701.08939v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jeffrey bilmes", "wenruo bai"], "accepted": false, "id": "1701.08939"}, "pdf": {"name": "1701.08939.pdf", "metadata": {"source": "META", "title": "Deep Submodular Functions", "authors": ["Jeffrey A. Bilmes", "Wenruo Bai"], "emails": [], "sections": [{"heading": null, "text": "Contents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Background and Motivation 4", "text": ""}, {"heading": "3 Sums of Concave Composed with Modular Functions (SCMMs) 4", "text": "3.1 Feature Based Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "4 Deep Submodular Functions 8", "text": "4.1 Recursively Defined DSFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 DSFs: Practical Benefits and Relation to Deep Neural Networks . . . . . . . . . . . . . . . . 11\nar X\niv :1\n70 1.\n08 93\n9v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20"}, {"heading": "5 Relevant Properties and Special Cases 11", "text": "5.1 Properties of Concave and Submodular Functions . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Antitone Maps and Superdifferentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.3 The Special Matroid Case and Deep Matroid Rank . . . . . . . . . . . . . . . . . . . . . . . . 19 5.4 Surplus and Absolute Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"}, {"heading": "6 The Family of Deep Submodular Functions 24", "text": "6.1 DSFs generalize SCMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n6.1.1 The Laminar Matroid Rank Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 6.1.2 A Non-matroid Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 6.1.3 More General Conditions on Two-Layer Functions . . . . . . . . . . . . . . . . . . . . 28\n6.2 The DSF Family Grows Strictly with the Number of Layers . . . . . . . . . . . . . . . . . . . 29 6.3 The Family of Submodular Functions is Strictly Larger than DSFs . . . . . . . . . . . . . . . 33"}, {"heading": "7 Applications in Machine Learning and Data Science 36", "text": "7.1 Learning DSFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n7.1.1 Training and Testing on Different Ground Sets, and Multimodal Submodularity . . . . 39 7.2 Deep Supermodular Functions and Deep Differences . . . . . . . . . . . . . . . . . . . . . . . 40 7.3 Deep Multivariate Submodular Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 7.4 Simultaneously Learning Hash and Submodular Functions . . . . . . . . . . . . . . . . . . . . 42"}, {"heading": "8 Conclusions and Future Work 43", "text": ""}, {"heading": "9 Acknowledgments 43", "text": "References 44"}, {"heading": "A More General Conditions on Two-Layer Functions: Proofs 52", "text": ""}, {"heading": "B Sums of Weighted Cardinality Truncations is Smaller than SCMMs 55", "text": ""}, {"heading": "1 Introduction", "text": "Submodular functions are attractive models of many physical processes primarily because they possess an inherent naturalness to a wide variety of problems (e.g., they are good models of diversity, information, and cooperative costs) while at the same time they enjoy properties sufficient for efficient optimization. For example, submodular functions can be minimized without constraints in polynomial time [46] even though they lie within a 2n-dimensional cone in R2n and are parameterized, in their most general form, with a corresponding 2n independent degrees of freedom. Moreover, while submodular function maximization is NP-hard, submodular maximization is one of the easiest of the NP-hard problems since constant factor approximation algorithms are often available \u2014 e.g., in the cardinality constrained case, the classic 1\u2212 1/e result of Nemhauser [113] via the greedy algorithm. Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].\nSubmodular functions are becoming increasingly important in the field of machine learning. In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few. There also have been significant contributions from the machine learning community purely on the mathematical and algorithmic aspects of submodularity. This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.\nOne of the critical problems associated with utilizing submodular functions in machine learning and data science contexts is selecting which submodular function to use, and given that submodular functions lie in such a vast space with 2n degrees of freedom, it is a non-trivial task to find one that works well, if not optimally. One approach is to attempt to learn the submodular function based on either queries of some form or based on data. This has led to results, mostly in the theory community, showing how learning submodularity can be harder or easier depending on how we judge what is being learnt. For example, it was shown that learning submodularity in the PMAC setting is fairly hard [10] although in some cases things are a bit easier [42]. Learning can be made easier if we restrict ourselves to learn within only a subfamily of submodular functions. For example, in [140, 92], it is shown empirically that one can effectively learn mixtures of submodular functions using a max-margin learning framework \u2014 here the components of the mixture are fixed and it is only the mixture parameters that are learnt, leading often to a convex optimization problem. In some cases, computing gradients of the convex problem can be done using submodular maximization [92], while in other cases, even a gradient requires minimizing a difference of two submodular functions [150].\nLearning over restricted families rather than over the entire cone is desirable for the same reasons that any form of regularization in machine learning is useful. By restricting the family over which learning occurs, it decreases the complexity of the learning problem, thereby increasing the chance that one finds a good model within that family. This can be seen as a classic bias-variance tradeoff, where increasing bias can reduce variance. Up to now, learning over restricted families has apparently (to the authors\u2019 knowledge) been limited to learning mixtures over fixed components. This can be limited if the components are restricted, and if not might require a very large number of components. Therefore, there is a need for a richer and more flexible parametric family of submodular functions over which learning is not only still possible but ideally relatively easy. See Section 7.1 for further discussion on learning submodular functions.\nIn this paper, we introduce a new family of submodular functions that we term \u201cdeep submodular functions,\u201d or DSFs. DSFs strictly generalize, as we show below, many of the kinds of submodular functions that are useful in machine learning contexts. These include the so-called \u201cdecomposable\u201d submodular functions, namely those that can be represented as a sum of concave composed with modular functions [141].\nWe describe the family of DSFs and place them in the context of the general submodular family. In particular, we show that DSFs strictly generalize standard decomposable functions, thus theoretically motivating the use of deeper networks as a family over which to learn. Moreover, DSFs can represent a variety of complex submodular functions such as laminar matroid rank functions. These matroid rank functions include the truncated matroid rank function [52] that is often used to show theoretical worst-case performance for many constrained submodular minimization problems. We also show, somewhat surprisingly, that like decomposable functions, DSFs are unable to represent all possible cycle matroid rank functions. This is interesting in and of itself since there are laminar matroids that can not be represented by cycle matroids. On the other hand, we show that the more general DSFs share a variety of useful properties with decomposable functions. Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set. These advantages are made apparent in Section 2.\nInterestingly, DSFs also share certain properties with deep neural networks (DNNs), which have become widely popular in the machine learning community. For example, DNNs with weights that are strictly nonnegative correspond to a DSF. This suggests, as we show in Section 7.1, that it is possible to develop a learning framework over DSFs leveraging DNN learning frameworks. Unlike standard deep neural networks, which typically are trained either in classification or regression frameworks, however, learning submodularity often takes the form of trying to adjust the parameters so that a set of \u201csummary\u201d data sets are offered a high value. We therefore extend the max-margin learning framework of [140, 92] to apply to DSFs. Our approach can be seen as a max-margin learning approach for DNNs but restricted to DSFs.\nWe offer a list of applications for DSFs in machine learning and data science in Section 7."}, {"heading": "2 Background and Motivation", "text": "Submodular functions are discrete set functions that have the property of diminishing returns. Given a finite size-n set of objects V (the ground set), where each v \u2208 V is a distinct element. A valuation set function f : 2V \u2192 R that returns a real value for any subsetX \u2286 V is said to be submodular if for allX \u2286 Y and v /\u2208 Y the following inequality holds: f(X \u222a {v}) \u2212 f(X) \u2265 f(Y \u222a {v}) \u2212 f(Y ). This means that the incremental value (or gain) of adding another sample v to a subset decreases when the context in which v is considered grows from X to Y . We can define the gain of v in the context of X as f(v|X) , f(X \u222a {v}) \u2212 f(X). Thus, f is submodular if f(v|X) \u2265 f(v|Y ). If the gain of v is identical for all different contexts i.e., f(v|X) = f(v|Y ),\u2200X,Y \u2286 V and \u2200v \u2208 V , then the function is said to be modular. A function might also have the property of being normalized (f(\u2205) = 0) and monotone non-decreasing (f(X) \u2264 f(Y ) whenever X \u2286 Y ). If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron. If the negation of f , \u2212f , is submodular, then f is called supermodular. If m is a normalized modular function, it can be written as a sum of singleton values m(X) = \u2211 x\u2208X m(x) and, moreover, is seen simply as a vector m \u2208 RV . A very simple example of a submodular function can be described using an urn containing a set of balls and a valuation function that counts the number of colors present in the urn. Such a function, therefore, measures only the diversity of ball colors in the urn, rather than ball quantity. We are motivated by applications where we wish to build models of information and diversity over data sets, in which case V is a ground set of data items. Each v \u2208 V , in such case, might be a distinct data sample \u2014 for example, either a word, n-gram, sentence, document, image, video, protein, genome, sensor reading, a machine learning system\u2019s input-output training pair, or even a highly structured irregularly sized object such as a tree or a graph. It is also desirable for V to be a set of heterogeneous data objects, such where v1 \u2208 V may be an image and v2 \u2208 V may be a document.\nThere are many useful classes of submodular functions. One of the more widely used such function are those that, for the present purposes, we refer to a \u201cgraph based,\u201d since they are parameterized by a weighted graph. Graph-based methods have a long history in many applications of machine learning and natural language processing (NLP), e.g., [103, 112, 2, 138, 144, 85, 96, 126, 159]. Work in this field is relevant to any graph-based submodular functions parameterized by a weighted graph G = (V,E,w), where V is a set of nodes (corresponding to the ground set), E is a set of edges, and w : E \u2192 R+ is a set of non-negative edge weights representing associations (e.g., affinity or similarity) between the corresponding elements. Graph-based submodular functions include the classic graph cut function f(X) =\u2211 x\u2208X,y\u2208V \\X w(x, y), but also the monotone graph cut function f(X) = \u2211 x\u2208X,y\u2208V w(x, y), the saturated\ngraph cut function [93] f(X) = \u2211 v\u2208V min(Cv(X), \u03b1Cv(V )) where \u03b1 \u2208 (0, 1) is a hyperparameter and\nwhere Cv(X) = \u2211 x\u2208X w(v, x). Another widely used graph-based function is the facility location function\n[106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75]. It is also useful and learn conic mixtures of graph based functions as done in [92]. An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity. A drawback of graph-based functions is that building a graph over n samples has complexity O(n2) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity). Moreover, it is difficult to add elements to V as it requires O(n) computation for each addition. For machine learning applications, moreover, it is difficult with these functions to train on a training set that may generalize to a test set [92]."}, {"heading": "3 Sums of Concave Composed with Modular Functions (SCMMs)", "text": "A class of submodular functions [141] used in machine learning are the so-called \u201cdecomposable functions.\u201d. Given a set of non-negative modular functions mi : V \u2192 R+, a corresponding set of non-negative monotone\n1Lov\u00e1sz in 1980 uses the same definition, but also asked for integrality which Cunningham did not require.\nnon-decreasing normalized (i.e., \u03c6(0) = 0) concave functions \u03c6i : [0,mi(V )] \u2192 R+, and a final normalized but otherwise arbitrary modular function m\u00b1 : V \u2192 R, consider the class of functions g : 2V \u2192 R+ that take the following form:\ng(A) = \u2211 i \u03c6i(mi(A)) +m\u00b1(A) = \u2211 i \u03c6i (\u2211 a\u2208A mi(a) ) +m\u00b1(A). (1)\nThis class of functions is known to be submodular [47, 46, 141]. While such functions have been called \u201cdecomposable\u201d in the past, in this work we will refer to this class of functions as \u201cSums of Concave over non-negative Modular plus Modular\u201d (or SCMMs) in order to avoid confusion with the term \u201cdecomposable\u201d used to describe certain graphical models [86, 53].2\nSCMMs have been shown to be quite flexible [141], being able to represent a surprisingly diverse set of functions. For example, consider the bipartite neighborhood function, which is defined using a bipartite graph G = (V,U,E,w) with E \u2286 V \u00d7 U being a set of edges between elements of V and U , and where w : U \u2192 R+ is a set of weights on U . For any subset Y \u2286 U we define w(Y ) = \u2211 y\u2208Y w(y) as the sum of the weights of the elements Y . The bipartite neighborhood function is then defined as g(X) = w(\u0393(X)), where the neighbors function is defined as \u0393(X) = {u \u2208 U : \u2203(x, u) \u2208 E having x \u2208 X} \u2286 U for X \u2286 V . This can be easily written as an SCMM as follows: g(X) = \u2211 u\u2208U w(u) min(|X \u2229 \u03b4u|, 1) where \u03b4u \u2286 V are the\nneighbors of u in V \u2014 hence mu(X) = |X \u2229 \u03b4u| is a modular function and \u03c6u(\u03b1) = min(1, \u03b1) is concave. When all the weights are unity, this is also equivalent to the set cover function g(X) = |\u22c3x\u2208X \u0393(x)| where the operation min |X| s.t. g(X) = |U | attempts to cover a set U by a small set of subsets {\u0393(x) : x \u2208 X}. With such functions, it is possible to represent graph cut as follows: g(X) = f(X) + f(V \\X)\u2212 f(V ), a sum of an SCMM and a complemented SCMM. It is shown in [72] that any SCMM can be represented with a graph cut function that might optionally utilize additional auxiliary variables that are first minimized over.\nSCMMs can represent other functions as well, such as multiclass queuing system functions [63, 142], functions of the form f(A) = m1(A)\u03c6(m2(A)) where m1,m2 : V \u2192 R+ are both non-negative modular functions, and \u03c6 : R \u2192 R is a non-increasing concave function. Another useful instance is the probabilistic coverage function [39] where we have a set of topics, indexed by i, and V is a set of documents. The function, for topic u, takes the form fu(A) = 1\u2212 \u220f a\u2208A(1\u2212p(u|a)) where p(u|a) is the probability of topic u for document\na according to some model. This function can be written as fu(A) = 1 \u2212 exp(\u2212 \u2211 a\u2208A log(1/(1 \u2212 p(u|a))))\nwhere \u03c6u(\u03b1) = 1 \u2212 exp(\u2212\u03b1) is a concave function and mu(A) = \u2211 a\u2208A log(1/(1 \u2212 p(u|a))) is modular. Hence, probabilistic coverage is an SCMM. Indeed, even the facility location function can be related to SCMMs. If in the facility location function we sum over a set of concepts U rather than the entire ground set V (which can be achieved, say by first clustering V into representatives U), the function takes the form g(A) = \u2211 u\u2208U maxa\u2208A w(a, u). A soft approximation to the max function (softmax) can be obtained as follows:\n\u03c6smax(\u03b3,w)(A) , 1\n\u03b3 log( \u2211 a\u2208A exp(\u03b3wa)). (2)\nWe have that maxa\u2208A wa = lim\u03b3\u2192\u221e \u03c6smax(\u03b3,w)(A) and for any finite \u03b3, \u03c6smax(\u03b3,w)(A) is a concave over modular function. Hence, a soft concept-based facility location function would take the form g\u03b3(A) =\u2211 u\u2208U \u03c6smax(\u03b3,wu)(A) which is also an SCMM. Equation (1) allows for a final arbitrary modular function m\u00b1 without which the function class would be strictly monotone non-decreasing and trivial to unconstrainedly minimize. Allowing an arbitrary modular function to apply at the end means the function class need not be monotone and hence finding the minimizing set is non-trivial. Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117]. Moreover, it appears that there is little loss of generality in handling the nonmonotonicty separately from the polymatroidality, as any non-monotone submodular function can easily be written as a sum of a totally normalized polymatroid function plus a modular function [31, 30]. To see\n2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function. Hence, without this switch of terminology, one reasonably could speak of \u201cdecomposable decomposable submodular functions.\u201d\nthis, consider any arbitrary submodular function f and write it as f(A) = ( f(A) \u2212\u2211a\u2208A f(a|V \\ {a})) +\u2211\na\u2208A f(a|V \\ {a}), the first term f(A)\u2212 \u2211 a\u2208A f(a|V \\ {a}) is a polymatroid function and the second term\nis modular."}, {"heading": "3.1 Feature Based Functions", "text": "A particularly useful way to view SCMMs for machine learning and data science applications is when data objects are embedded in a \u201cfeature\u201d space indexed by a finite set U . Suppose we have a set of (possibly multi-modal) data objects V each of which can be described by an embedding into feature space RU+ where each u \u2208 U can be thought of as a possible feature, concept, or attribute of an object. Each object v \u2208 V is represented by a non-negative feature vector mU (v) , (mu1(v),mu2(v), . . . ,mu|U|(v)) \u2208 RU+. Each feature u \u2208 U also has an associated normalized monotone non-decreasing concave function \u03c6u : [0,mu(V )] \u2192 R+ and a non-negative importance weight wu. These then yield the class of \u201cfeature based functions\u201d\nf(X) = \u2211 u\u2208U wu\u03c6u(mu(X)) +m\u00b1(X) (3)\nwhere mu(X) = \u2211 x\u2208X mu(x). A feature based function then is an SCMM.\nIn a feature-based function, mu(v) \u2265 0 is a non-negative score that measures the degree of feature u that exists in data object v and the vector mU (v) is the entirety of the object\u2019s representation in feature space. The quantity mu(X) measures the u-ness in a collection of objects X that, when the concave function \u03c6u(\u00b7) is applied, starts diminishing the contribution of this feature for that set of objects. The importance of each feature is given by the feature weight wu. From the perspective of applications, U can be any set of features.\nAs an example in NLP, let V be a set of sentences. For s \u2208 V and u \u2208 U , definemu(v) to be the count of ngram feature u in sentence s. For the sentence s = Whenever I visit New York City, I buy a New York City map.,\nm\"the\"(s) = 1 while m\"New York City\"(s) = 2. There are many different ways to produce the scores mu(s) other than raw n-gram counts. For example, they can be TFIDF-based normalized counts, or scaled in various ways depending on the nature of u. The weight wu can be the desired relative frequency of u, the length of u, and so on.\nFeature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146]. Good feature engineering allows for potentially different size and type of data items (either within or across modalities) to be embedded within the same space and hence considered on the same playing field. Proper feature representation is often therefore a crucial for many machine learning systems to perform well. In the case of NLP, for example, features requiring annotation tools (e.g., parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.g., [157, 17, 12, 84, 124]) are available. For computer vision, this includes visual bag-of-words features (e.g., [44, 158, 90, 115, 29, 147, 35]). Any type of data can have automatically learned features using representation learning via, say, deep models (e.g., [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.\nOne of the advantages of feature based submodular functions for machine learning and data science applications is that they can leverage this vast amount of available work on feature engineering. Feature transformations can be developed separately from the resulting submodularity and can still be easily incorporated into a feature based function without loosing the submodularity property.\nFigure 1 gives another illustrative but contrived example, that demonstrates how feature functions, when maximized, attempt to achieve a form of uniformity, and hence diversity, over feature space. The figure also helps to motivate deep submodular functions in the next section. We have |V | = 9 data objects each of which is an image containing a set of shapes, some number of circles, squares, and triangles. For example, Figure 1-(I) shows that object a contains nine squares while object d contains three each of squares, circles, and triangles. To the right of these shapes is the corresponding vector mU (v) for that object (e.g., mU (g) shows four squares, three triangles, and two circles). On these shapes we can define a submodular function as follows: g(A) = \u2211 u\u2208{4, ,\u00a9} \u221a mu(A) where mu(A) = \u2211 a\u2208A countu(a) counts the total number of\nobjects of type u in the set of images A. Figure 1-(II) shows g({b}) = \u221a 8 + \u221a\n1. Figure 1-(III) shows g({a,b, c}) which has a greater diversity of objects and hence is given a greater value, while Figure 1-(IV) shows g({d,h, f}) = 9 which is the maximum valued size-three set (and is also the solution to the greedy algorithm in this case), and is the set having the greatest diversity. Diversity, therefore, corresponds to uniformity and maximizing this submodular function, under a cardinality constraint, strives to find a set of objects with as even a histogram of feature counts as possible. When using non-uniform weights wu, then maximizing this submodular function attempts to find a set that closely respect the feature weights.\nIn fact, maximizing feature based functions can be seen as a form of constrained divergence minimization. Let p = {pu}u\u2208U be a given probability distribution over features (i.e., \u2211 u pu = 1 and pu \u2265 0 for all u \u2208 U). Next, create an X-dependent distribution over features:\n0 \u2264 p\u0304u(X) , mu(X)\u2211\nu\u2032\u2208U mu\u2032(X) = mu(X) m(X) \u2264 1 (4)\nwhere m(X) , \u2211 u\u2032\u2208U mu\u2032(X). Then p\u0304u(X) can also be seen as a distribution over features U since p\u0304u(X) \u2265\n0 and \u2211 u\u2208U p\u0304u(X) = 1 for any X \u2286 V . Consider the KL-divergence between these two distributions:\nD(p||p\u0304(X)) = \u2212H(p) + logm(X)\u2212 \u2211 u\u2208U pu log(mu(X)) (5)\nHence, the KL-divergence is merely a constant plus a difference of feature-based functions. Maximizing\u2211 u\u2208U pu log(mu(X)) subject to logm(X) = const (which can be seen as a data quantity constraint) therefore is identical to finding an X that minimizes the KL-divergence between p\u0304(X) and p. Alternatively, defining g(X) , logm(X)\u2212D(p||{m\u0304u(X)}) = \u2211 u\u2208U pu log(mu(X)) as done in [136], we have a submodular function g that represents a combination of its quantity of X via its features (i.e., logm(X)) and its distribution closeness to p. The concave function in the above is \u03c6(\u03b1) = log(\u03b1) which is negative for \u03b1 < 1. We can rectify this situation by defining an extra object v\u2032 /\u2208 V having mu(v\u2032) = 1 for all u. Then g(X|v\u2032) =\u2211 u\u2208U pu log(1 +mu(X)) is also a feature based function on V .\nThe KL-divergence can be generalized in various ways, one of which is known as the f -divergence, or in particular the \u03b1-divergence [137, 3]. Using the reparameteriation \u03b1 = 1\u2212 2\u03b4 [74], the \u03b1-divergence (or now \u03b4-divergence [165]) can be expressed as\nD\u03b4(p, q) = 1 \u03b4(1\u2212 \u03b4) (1\u2212 \u2211 u\u2208U p\u03b4uq 1\u2212\u03b4 u ). (6)\nFor \u03b4 \u2192 1 we recover the standard KL-divergence above. For \u03b4 \u2208 (0, 1) we see that the optimization problem minX\u2286V :m(X)\u2264bD\u03b4(p, p\u0304(X)) where b is a budget constraint is the same as the constrained submodular maximization problem maxX\u2286V :m(X)\u2264b g(X) where g(X) = \u2211 u\u2208U p \u03b4 u(mu(X))\n1\u2212\u03b4 is a feature-based function since \u03c6u(\u03b1) = \u03b11\u2212\u03b4 is concave on \u03b1 \u2208 [0, 1] for \u03b4 \u2208 (0, 1). Hence, any such constrained submodular maximization problem can be seen as a form of \u03b1-divergence minimization.\nIndeed, there are many useful concave functions one could employ in applications and that can achieve different forms of submodular function. Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y3/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3. There are also parameterized concave functions that get as close to the hard truncation functions as we wish, such as \u03c6a,c(x) = ((x\u2212a+c\u2212a)/2)\u22121/a where a \u2265 \u22121, and c > 0 are parameters \u2014 it is straightforward to show that \u03c6\u22121,c(x) is linear, that lima\u2192\u221e \u03c6a,c(x) = min(x, c), and that for \u22121 < a < \u221e we have a form of soft min. Also recall the parameterized soft max mentioned above in relationship to the facility location function. In other cases, is useful for the concave function to be linear for a while before a soft or nonsaturating concave part kicks in, for example \u03c6(\u03b1) = min( \u221a \u03b1/\u03b3, \u03b1/\u03b3) for some constant \u03b3 > 0. These all can have their uses, depending on the application, and determine the nature of how the returns of a given feature u \u2208 U should diminish. Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].\nWe mention a final advantage of SCMMs is that they do not require the construction of a pairwise graph and therefore do not have quadratic cost as would, say a facility location function (e.g., f(X) =\u2211 v\u2208V maxx\u2208X wxv), or any function based on pair-wise distances, all of which have cost O(n\n2) to evaluate. Feature functions have an evaluation cost of O(n|U |), linear in the ground set V size and therefore are more scalable to large data set sizes. Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14]. For example, the vectors mU (v) for a newly encountered object v can be computed on the fly (or in parallel) whenever the object v is available and wherever it is located on a parallel machine."}, {"heading": "4 Deep Submodular Functions", "text": "While feature-based submodular functions are indisputably useful, their weakness lies in that features themselves may not interact, although one feature u\u2032 might be partially redundant with another feature u\u2032\u2032. For example, when describing a sentence via its component n-grams features, higher-order n-grams always include lower-order n-grams, so some n-gram features can be partially redundant. For example, in a large collection of documents about \u201cNew York City\u201d, it is likely there will be some instances of \u201cChicago,\u201d so the feature functions for these two features should likely negatively covary. One way to reduce this redundancy is to subselect the features themselves, reducing them down to a subset that tends not to interact in any way. This can only work in limited cases, however, namely when the features themselves can be reduced to an \u201cindependent\u201d set that looses no information about the data objects, and this only happens when redundancy is an all-or-nothing property (as in a matroid).\nMost real-world features, however, involve partial redundancy. The presence of \u201cNew York City\u201d shouldn\u2019t completely remove the contributing of \u201cChicago\u201d, rather it should only discount its contribution. A better\nstrategy, therefore, is to allow the feature scores to interact, say, when measuring redundancy at some higher-level concept of a \u201cbig city.\u201d\nFigure 1 offers a further pictorial example. Figure 1-(IV) shows that the most diverse set of size three is {d,h, f} since it has an even distribution over the set of features, square, triangle, circle. Suppose, however, the non-smooth shapes are seen to be partially redundant with each other, so that the presence of a square should discount, to some degree, the value of a triangle, but should not discount the value of a circle. The feature based function g(A) = \u2211 u\u2208{4, ,\u00a9} \u221a mu(A) does not allow these three features to interact in any way to achieve this form of discounting. The contribution of \u201csquare\u201d is measured combinatorially independently of \u201ctriangle\u201d \u2014 feature-based functions therefore fail for features that themselves should be considered partially redundant. We can address this issue by using an additional level of concave composition\ng(A) = \u221a \u2211 u\u2208{4, } \u221a mu(A) + \u221a m\u00a9(A), (7)\nwhere the nested square-root over the two features, square and triangle, allow them to interact and discount each other. Figure 1-(V) shows the new value of the formally maximum set {d,h, f} is no longer the maximum size-three set. Figure 1-(VI) shows the new maximum sized-three set, where the number of squares and circles together is roughly the same as the number of circles.\nIn general, to allow feature scores to interact and discount each other, we can utilize an additional \u201clayer\u201d of nested concave functions as follows:\nf(X) = \u2211 s\u2208S \u03c9s\u03c6s( \u2211 u\u2208U wsu\u03c6u(mu(X))), (8)\nwhere S is a set of meta-features, \u03c9s is a meta-feature weight, \u03c6s is a non-decreasing concave function associated with meta-feature s, and ws,u is now a meta-feature specific feature weight. With this construct, \u03c6s assigns a discounted value to the set of features in U , which can be used to represent feature redundancy. Interactions between the meta-features might be needed as well, and this can be done via meta-meta-features, and so on, resulting in a hierarchy of increasingly higher-level features. Such a hierarchy could correspond to semantic hierarchies for NLP applications (e.g., WordNet [105]), or a visual hierarchy in computer vision (e.g., ImageNet [34]). Alternatively, in the spirit of modern big-data efforts in deep learning, such a hierarchy could be learnt automatically from data.\nWe propose a new class of submodular functions that we call deep submodular functions (DSFs). They may make use of a finite-length series of disjoint sets (see Figure 2-(a)): V = V (0), which is the function\u2019s ground set, and additional sets V (1), V (2), . . . , V (K). U = V (1) can be seen as a set of \u201cfeatures\u201d, V (2) as a set of meta-features, V (3) as a set of meta-meta features, etc. up to V (K). The size of V (i) is di = |V (i)|. Two successive sets (or \u201clayers\u201d) i \u2212 1 and i are connected by a matrix w(i) \u2208 Rdi\u00d7di\u22121+ , for i \u2208 {1, . . . ,K}. Hence, rows of w(i) are indexed by elements of V (i) and columns of w(i) are indexed by elements of V (i\u22121). Given vi \u2208 V (i), define w(i)vi to be the row of w(i) corresponding to element vi, and w (i) vi (v i\u22121) is the element\nof matrix w(i) at row vi and column vi\u22121. We may think of w(i)vi : V (i\u22121) \u2192 R+ as a modular function defined on set V (i\u22121). Thus, this matrix contains di such modular functions. Further, let \u03c6vk : R+ \u2192 R+ be a non-negative non-decreasing concave function. Then, a K-layer DSF f : 2V \u2192 R+ can be expressed as follows, for any A \u2286 V ,\nf(A) = f\u0304(A) +m\u00b1(A) (9)\nwhere,\nf\u0304(A) = \u03c6vK ( \u2211 vK\u22121\u2208V (K\u22121) w (K) vK (vK\u22121)\u03c6vK\u22121 ( . . . \u2211 v2\u2208V (2) w (3) v3 (v 2)\u03c6v2 ( \u2211 v1\u2208V (1) w (2) v2 (v 1)\u03c6v1 (\u2211 a\u2208A w (1) v1 (a) )))) ,\n(10)\nand where m\u00b1 : V \u2192 R is an arbitrary modular function. Equation (9) defines a class of submodular functions. Submodularity follows since a composition of a monotone non-decreasing function h and a monotone non-decreasing concave function \u03c6 (g(\u00b7) = \u03c6(h(\u00b7))) is submodular (Theorem 1 in [93] and repeated, with proof, in Theorem 5.4) \u2014 a DSF is submodular via recursive application and since submodularity is closed under conic combinations."}, {"heading": "4.1 Recursively Defined DSFs", "text": "A slightly more general way to define a DSF and that is useful for the theorems below uses recursion. This section also defines the notation that will be often used later in the paper.\nWe are given a directed acyclic graph (DAG) G = (V,E) where for any given node v \u2208 V, we say pa(v) \u2282 V are the parents of (or vertices pointing towards) v. A given size n subset of nodes V \u2282 V corresponds to the ground set of a submodular function and for any v \u2208 V , pa(v) = \u2205. A unique \u201croot\u201d noder \u2208 V \\ V has the distinction that r /\u2208 pa(q) for any q \u2208 V. Given a non-ground node v \u2208 V \\ V , we define the concave function \u03c8v : RV \u2192 R+ where\n\u03c8v(x) = \u03c6v(\u03d5v(x)), (11a)\nand\n\u03d5v(x) = \u2211\nu\u2208pa(v)\\V\nwvu\u03c8u(x) + \u3008mv, x\u3009. (11b)\nIn the above, \u03c6v : R+ \u2192 R+ is a normalized non-decreasing univariate concave function, wvu \u2208 R+ is a non-negative weight indicating the relative importance of \u03c8u to \u03d5v, and mv : Rpa(v)\u2229V \u2192 R+ is a nonnegative linear function that evaluates as \u3008mv, x\u3009 = \u2211 u\u2208pa(v)\u2229V mv(u)x(u). In other words, \u3008mv, x\u3009 is a sparse dot-product over ground elements pa(v) \u2229 V . There is no additional additive bias constant added to the end of Equation (11b) as this is assumed to be part of \u03c6v (as a shift) if needed (alternatively, for one of the u \u2208 pa(V ) \\ V , we can set \u03c8u(x) = 1 as a constant, and the bias may be specified by a weight, as is occasionally done when specifying neural networks). The base case, where pa(v) \u2286 V therefore has \u03c8v(x) = \u03c6v(\u3008mv, x\u3009), so \u03c8v(1A) is a concave composed with a modular function. The notation 1A indicates the characteristic vector of set A, meaning 1A(v) = 1 if v \u2208 A and is otherwise zero.\nA general DSF is defined as follows: for all A \u2286 V , f(A) = \u03c8r(1A) + m\u00b1(A), where m\u00b1 : V \u2192 R is an arbitrary modular function (i.e., it may include positive and negative elements). For all v \u2208 V, we also for convenience, define gv(A) = \u03c8v(1A). To be able to treat all v \u2208 V similarly, we say, for v \u2208 V , that pa(v) = \u2205, and use the identity \u03c6v(a) = a for a \u2208 R, and set mv = 1v, so that \u03c8v(x) = \u03d5v(x) = x(v) and gv(A) = 1v\u2208A which is a modular function on V .\nBy convention, we say that a zero-layer DSF function is an arbitrary modular function, a one-layer DSF is an SCMM, and a two-layer DSF is, as we will soon see, something different. By DSFk, we mean the family of DSFs with k layers.\nAs mentioned above, from the perspective of defining a submodular function, there is no loss of generality by adding the final modular function m\u00b1 to a polymatroid function [31, 30]. The degree to which DSFs\ncomprise a subclass of submodular functions corresponds to the degree to which gr comprise a subclass of all polymatroid functions.\nThe recursive form of DSF is more convenient than the layered approach mentioned above which, in the current form, would partition V = {V (0), V (1), . . . , V (K)} into layers, and where for any v \u2208 V (i), pa(v) \u2286 V (i\u22121). Figure 2-(a) corresponds to a layered graph G = (V,E) where r = v31 and V = {v01 , v02 , . . . , v06}. Figure 2-(b) uses the same partitioning but where units are allowed to skip by more than one layer at a time. More generally, we can order the vertices in V with order \u03c3 so that {\u03c31, \u03c32, . . . , \u03c3n} = V where n = |V |, \u03c3m = r = vK where m = |V| and where \u03c3i \u2208 pa(\u03c3j) iff i < j. This allows an arbitrary pattern of skipping while maintaining submodularity. The additional linear function in Equation (11b) is strictly not necessary (e.g., there could be paths of linearity along subsets of the \u03c6vv \u2208 A for some A \u2282 V thereby achieving the same result) but we include it to stress that at each layer there may be a modular function and a bias."}, {"heading": "4.2 DSFs: Practical Benefits and Relation to Deep Neural Networks", "text": "The layered definition in Equation (9) is reminiscent of feed-forward deep neural networks (DNNs) owing to its multi-layered architecture. Interestingly, if one restricts the weights of a DNN at every layer to be non-negative, then for many standard hidden-unit activation functions the DNN constitutes a submodular function when given Boolean input vectors. The result follows for any activation function that is monotone non-decreasing concave for non-negative reals, such as the sigmoid, the hyperbolic tangent, and the rectified linear functions. In the rectified linear case, however, the entire network would be linear so the model becomes interesting only with hidden activations that are strictly concave (since the weights can be arbitrarily scaled, perhaps \u03c6(x) = min(x, 1) is a reasonable concave analogy in a DSF to the rectified linear function in a DNN). More importantly, this suggests that DSFs can be trained in a fashion similar to DNNs \u2014 specifically, training DSFs and can take advantage of the many successful training techniques and software libraries for training DNNs (many of the toolkits make it easy to project weights into the positive orthant). Further discussion on this point is given in Section 7.1. The recursive definition of DSFs, in Equation (11) is useful for the analysis in Section 5.\nDSFs should be useful for many applications in machine learning. First, they retain the advantages of SCMMs in that they require neither O(n2) computation nor access to the entire ground set for a set evaluation. The underlying DSF computation is matrix-vector multiplication that, like DNNs, can be performed very quickly using modern GPU computing. Hence, DSFs can be both fast, and useful for parallel and/or streaming applications. Second, DSFs allow for a nested hierarchy of features, similar to advantages a deep model has over a shallow model. For example, a one-layer DSF must construct a valuation over a set of objects from a large number of low-level features which can lead to fewer opportunities for feature sharing while a deeper network fosters distributed representations, also analogous to DNNs [15, 16]. It can be argued that a deep neural network is more efficient, in terms of the number of possible functions represented per weight, than a shallow neural network and perhaps DSFs share this advantage. Hence, even if the DSF and SCMM families were to be found to be same (but that Theorem 6.4 shows to be false), there could be advantages to applications and learning paradigms thanks to this natural hierarchical decomposition of concepts.\nDSFs have been used occasionally in some applications. In one instance [95], a square root was applied to a subset of the right hand nodes in a bipartite neighborhood function in order to offer reduced cost for these nodes being indirectly selected in the graph. In [155] a two-layer DSF was used to introduce higher-level interaction between features, an act that yielded benefits in speech data summarization. Lastly, laminar matroid rank functions, which are instances of DSFs as shown in Section 5.3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66]."}, {"heading": "5 Relevant Properties and Special Cases", "text": "DSFs represent a family that, at the very least, contain the family of SCMMs. Above, we argued intuitively that DSFs might extend SCMMs as they allow components themselves to directly interact, and the interactions may propagate up a many-layered hierarchy. In this section, we start off (in Section 5.1) discussing preliminaries regarding concave functions. Section 5.2 then covers specific properties of the multivariate concave function associated with a DSF, in particular the antitone gradient superdifferential property which\nis a a sufficient condition for submodularity. This section also compares this condition with the negativity of the off-diagonal Hessian matrix condition for submodular functions. Section 5.3 discusses matroid rank special cases, including the laminar matroid rank function which can be seen, in the light of this paper, as a form of deep matroid rank. This section also discusses special cases of the results shown later in the paper, in particular, that: (1) cycle matroid rank functions cannot represent all partition matroid rank functions; (2) laminar matroid rank functions strictly generalize partition matroid rank functions; (3) laminar matroid rank functions cannot express all cycle matroid rank functions; (4) DSFs generalize laminar matroid rank functions; and (5) SCMMs generalize partition matroid rank functions. Lastly, section 5.4 introduces various analysis tools (in particular the \u201csurplus\u201d) that are used later in the paper."}, {"heading": "5.1 Properties of Concave and Submodular Functions", "text": "Many of the results in the sections below rely on a number of properties of concave functions. Since we wish to consider non-differentiable concave functions, the theorems below consider this more general case where we may assume only that the concave functions have superdifferentials. It is, in general, more work to show that the properties of concave functions hold in this non-differential case, but since there seem to be no consolidated published proofs of these properties, we offer them here in full.\nLet \u03c6 : R \u2192 R be a normalized (\u03c6(0) = 0) monotone non-decreasing concave function. In any such function, there may be an initial linear part where \u03c6(x) = \u03b3x for x \u2208 [0, \u03b1\u03c6] where \u03b3 > 0 and where \u03b1\u03c6 \u2265 0 is the largest point where \u03c6 is still linear. Larger than \u03b1\u03c6, there may be a middle part consisting of a series of concave curves and line segments all situated to ensure concavity. Larger than this, there finally might be a saturation point where \u03c6(x) = c for all x \u2265 \u03b1sat, where c, \u03b1sat \u2208 R+ \u222a {\u221e}. The middle region (x \u2208 [\u03b1lin, \u03b1sat]) might or might not be smooth. It is useful sometimes in applications (e.g., [71]) to formulate submodular functions from concave functions that have an initial linear part followed by either a saturation or by a smooth concave part.\nDefinition 5.1 (Superdifferential). Let \u03c6 : Rn \u2192 R be a concave function. The superdifferential of \u03c6 at x is the set of vectors defined as follows:\n\u2202\u03c6(x) = {s \u2208 Rn : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 Rn} (12)\nThe superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114]. When \u03c6 is differentiable at x, the superdifferential corresponds to the gradient, so that \u2202\u03c6(x) = {\u2207\u03c6(x)} and otherwise members of \u2202\u03c6(x) are called subgradients. In general, we have the following:\nLemma 5.2. The superdifferential of a concave function is a monotone operator, i.e.,\n\u3008u\u2212 v, x\u2212 y\u3009 \u2264 0,\u2200x, y \u2208 Rn, u \u2208 \u2202\u03c6(x), v \u2208 \u2202\u03c6(y) (13)\nProof. We have that\nf(y) \u2264 f(x) + \u3008u, y \u2212 x\u3009, and f(x) \u2264 f(y) + \u3008v, x\u2212 y\u3009 (14)\nAdding the two inequalities yields monotonicity.\nThis means in particular that, in the one-dimensional case when n = 1, if x \u2264 y then for any u \u2208 \u2202\u03c6(x) and any v \u2208 \u2202\u03c6(y), we must have u \u2265 v. In the below, we offer a number of properties of concave superdifferentials in the 1D case. While statements of these results are intuitively clear, the authors were unable to find published proofs, so they are also included herein.\nTheorem 5.3. Let \u03c6 : R \u2192 R be a continuous function. Then \u03c6 is concave if and only if for all a, b \u2208 R with a \u2264 b, and \u2206 \u2208 R+, we have that\n\u03c6(a+ \u2206)\u2212 \u03c6(a) \u2265 \u03c6(b+ \u2206)\u2212 \u03c6(b). (15)\nAlso, \u03c6 is monotone non-decreasing concave if and only if for all a, b \u2208 R with a \u2264 b, and \u2206, \u2208 R+, we have that\n\u03c6(a+ \u2206 + )\u2212 \u03c6(a) \u2265 \u03c6(b+ \u2206)\u2212 \u03c6(b) (16)\nProof. The result is vacuous if a = b, or \u2206 = 0 so assume a < b and \u2206 > 0. If part: Assume Equation (15) is true and consider\n\u03c6(a+ \u2206)\u2212 \u03c6(a) \u2206 \u2265 \u03c6(b+ \u2206)\u2212 \u03c6(b) \u2206\n(17)\nIf \u03c6 is differentiable at a and b, then taking \u2206\u2192 0 gives us \u03c6\u2032(a) \u2265 \u03c6\u2032(b) for all a \u2264 b, and this is a sufficient condition for concavity (see Nesterov 2.13, page 54, [114]). If \u03c6 is not differentiable at either a or b, we resort to its continuity. A function is concave if and only if it is continuous and midpoint concave [116] (or midconcave [127]), defined as for any x, y \u2208 R f((x+ y)/2) \u2265 (f(x) + f(y))/2). This condition is immediate from Equation (15) by setting x = a, y = b+ \u2206, and b = a+ \u2206 = (x+ y)/2.\nOnly if part: Assume \u03c6 is concave and a < b and \u2206 > 0 are given. If \u03c6 is differentiable, then by the mean value theorem, there exists an a+ with a \u2264 a+ \u2264 a+ \u2206 and a b+ with b \u2264 b+ \u2264 b+ \u2206 where\n\u03c6\u2032(a+) = \u03c6(a+ \u2206)\u2212 \u03c6(a)\n\u2206 (18)\nand\n\u03c6\u2032(b+) = \u03c6(b+ \u2206)\u2212 \u03c6(b)\n\u2206 (19)\nIf a+ \u2206 \u2264 b then a+ \u2264 b+ and hence \u03c6\u2032(a+) \u2265 \u03c6\u2032(b+) by concavity (Nesterov) which immediately gives \u03c6(a+ \u2206)\u2212 \u03c6(a) \u2265 \u03c6(b+ \u2206)\u2212 \u03c6(b). If \u03c6 is not differentiable at either a or b, then consider da \u2208 \u2202\u03c6(a) and db \u2208 \u2202\u03c6(b), so that \u2200ya, yb, \u03c6(ya) \u2264 \u03c6(a)+ \u3008da, ya\u2212a\u3009 and \u03c6(yb) \u2264 \u03c6(b)+ \u3008db, yb\u2212 b\u3009. Taking ya = a+\u2206 and yb = b+ \u2206 gives (\u03c6(a+ \u2206)\u2212 \u03c6(a))/\u2206 = da \u2265 db = (\u03c6(b+ \u03b4)\u2212 \u03c6(b))/\u2206 which follows from the monotonicity of the superdifferential operator.\nIf a + \u2206 > b then a < b < a + \u2206 < b + \u2206. Again when \u03c6 is differentiable, by the mean value theorem, there exists a+b with a \u2264 a+b \u2264 b and a+\u2206 with a+ \u2206 \u2264 a+\u2206 \u2264 b+ \u2206 with\n\u03c6\u2032(a+b ) = \u03c6(b)\u2212 \u03c6(a)\nb\u2212 a (20)\nand\n\u03c6\u2032(a+\u2206) = \u03c6(b+ \u2206)\u2212 \u03c6(a+ \u2206)\nb\u2212 a , (21)\nand since a+b < a + \u2206, \u03c6 \u2032(a+b ) \u2265 \u03c6\u2032(a+\u2206). This immediately gives \u03c6(b) \u2212 \u03c6(a) \u2265 \u03c6(b + \u2206) \u2212 \u03c6(a + \u2206) or \u03c6(a + \u2206) \u2212 \u03c6(a) \u2265 \u03c6(b + \u2206) \u2212 \u03c6(b). If \u03c6 is not differentiable, then taking supergradients da \u2208 \u2202\u03c6(a) and da+\u2206 \u2208 \u2202\u03c6(a+ \u2206) again gives the result.\nThe second part of the theorem is immediate if we take a = b, and define \u03b4 = a + \u2206 leading to \u03c6(\u03b4 + ) \u2265 \u03c6(\u03b4), i.e., monotonicity.\nThe above proof considers the smooth and non-smooth varieties separately where the non-smooth case utilizes only the existence of the superdifferential of a concave function. Since the superdifferential always exists for a concave function, smooth or otherwise, in the below we consider only the most general case where we assume only a superdifferential exists. As a result, the proofs are a bit more involved, but when constructing DSFs and considering the resultant submodular families in Section 6, we wish to allow for the most general class concave functions.\nWe next restate Theorem 1 from [93] but also provide a proof which was missing.\nTheorem 5.4. Suppose that h : 2V \u2192 R is a monotone non-decreasing submodular function and \u03c6 is a monotone non-decreasing concave function. Then g(A) = \u03c6(h(A)) is monotone non-decreasing submodular.\nProof. Consider any A \u2286 B \u2282 V and v /\u2208 B. Define quantities a, b,\u2206, so that: a = h(A) \u2264 b = h(B), a+ \u2206 + = h(A+ v), and b+ \u2206 = h(B + b). I.e., h(v|A) = \u2206 + \u2265 h(v|B) = \u2206. Then we have\n\u03c6(a+ \u2206 + )\u2212 \u03c6(a) \u2265 \u03c6(b+ \u2206)\u2212 \u03c6(b) (22)\nor\n\u03c6(h(A+ v))\u2212 \u03c6(h(A)) \u2265 \u03c6(h(B + v))\u2212 \u03c6(h(B)). (23)\nThe slope of the linear interpolation between two points on a concave function puts a connecting relationship on the corresponding superdifferentials at each of the two points, as the following result shows.\nLemma 5.5. Given a concave function \u03c6 : R \u2192 R and two points a, b with a < b that define the value dab = (\u03c6(b)\u2212 \u03c6(a))/(b\u2212 a). Then mind\u2208\u2202\u03c6(a) d > dab if and only if maxd\u2208\u2202\u03c6(b) d < dab. Proof. From the monotonicity of the supergradient [60, 114], we always have\ndmina , min d\u2208\u2202\u03c6(a) d \u2265 dab \u2265 max d\u2208\u2202\u03c6(b) d , dmaxb (24)\nsince otherwise, say if dmina < dab, then \u03c6(a) + dmina (b \u2212 a) < \u03c6(a) + dab(b \u2212 a) = \u03c6(b) which contradicts dmina being a supergradient. We must show that the inequalities in Equation (24) can be only simultaneously strict. Let dmina be given such that dmina > dab, and suppose that dmaxb = dab. Then\n\u03c6(y) \u2264 \u03c6(b) + dmaxb (y \u2212 b) (25) = \u03c6(b) + dmaxb (y \u2212 a+ a\u2212 b) (26) = \u03c6(b) + dmaxb (a\u2212 b) + dmaxb (y \u2212 a) (27) = \u03c6(a) + dmaxb (y \u2212 a) (28)\nand hence we have found a supergradient dmaxb \u2208 \u2202\u03c6(a) with dmina > dmaxb contradicting the minimality of dmina . Hence, we must have dmaxb < dab. A similar argument shows that d max b < dab and d min a = dab leads to a contradiction of the maximality of dmaxb .\nThe next result identifies a condition that, if true, tells us about the extent of the initial linear region of a monotone non-decreasing concave function.\nTheorem 5.6. Given a monotone non-decreasing concave function \u03c6 : R\u2192 R that is normalized (\u03c6(0) = 0) and any a, b \u2208 R+ with 0 < a \u2264 b. Then \u03c6(a + b) = \u03c6(a) + \u03c6(b), if and only if \u03c6(x) is linear in the region from 0 to a+ b (that is, there exists \u03b3 \u2208 R with \u03c6(x) = \u03b3x for x \u2208 [0, a+ b]. Proof. If case: immediate.\nOnly if case: Any violations of the following inequalities would violate the superdifferential property of \u2202\u03c6(y) at 0, a, b, or a+ b:\nmin d\u2208\u2202\u03c6(0) d \u2265 \u03c6(a)/a, max d\u2208\u2202\u03c6(a) d \u2264 \u03c6(a)/a, (29)\nmin d\u2208\u2202\u03c6(a) d \u2265 \u03c6(b)\u2212 \u03c6(a) b\u2212 a , maxd\u2208\u2202\u03c6(b) d \u2264 \u03c6(b)\u2212 \u03c6(a) b\u2212 a , (30)\nmin d\u2208\u2202\u03c6(b) d \u2265 \u03c6(a+ b)\u2212 \u03c6(b) (a+ b)\u2212 a = \u03c6(a)/a, maxd\u2208\u2202\u03c6(a+b) d \u2264 \u03c6(a)/a. (31)\nThis leads to the series of inequalities:\nmin d\u2208\u2202\u03c6(0)\nd (a) \u2265 \u03c6(a)/a (b) \u2265 max\nd\u2208\u2202\u03c6(a) d \u2265 min d\u2208\u2202\u03c6(a) d \u2265 \u03c6(b)\u2212 \u03c6(a) b\u2212 a \u2265 maxd\u2208\u2202\u03c6(b) d (32)\n\u2265 min d\u2208\u2202\u03c6(b)\nd (c) \u2265 \u03c6(a)/a (d) \u2265 max\nd\u2208\u2202\u03c6(a+b) d (33)\nFrom Lemma 5.5, if (a) is strict, then so is (b), leading to the contradiction \u03c6(a)/a > \u03c6(a)/a. Also from Lemma 5.5, if (d) is strict, then so is (c), leading to the same contradiction. Hence, all inequalities are\nequalities. By the monotonicity of the superdifferential of a concave function, we have that for any x < y < z and dy \u2208 \u2202\u03c6(y) that\nmin d\u2208\u2202\u03c6(x) d \u2265 dy \u2265 max d\u2208\u2202\u03c6(z) d (34)\nHence, for all y \u2208 [0, a + b], we have \u2202\u03c6(y) = {\u03c6(a)/a}, meaning that \u03c6 is linear in this region with \u03b3 = \u03c6(a)/a = \u03c6(b)/b = \u03c6(a+ b)/(a+ b).\nIt is known that any normalized submodular function is subadditive, in that for any A \u2286 V ,\u2211a\u2208A f(a) \u2265 f(A). A similar property is true of normalized monotone non-decreasing concave functions.\nTheorem 5.7 (Subadditivity). Given a normalized monotone non-decreasing concave function \u03c6, a set of non-negative points {xi}`i=1, xi \u2208 R+, then we have\u2211\ni \u03c6(xi) \u2265 \u03c6( \u2211 i xi) (35)\nand where the inequality is strict if and only if \u2211 i xi is past any linear part of \u03c6.\nProof. It is sufficient to show that it is true for x1:`\u22121 = \u2211`\u22121 i=1 xi and x` that\n\u03c6(x1:`\u22121) + \u03c6(x`) \u2265 \u03c6( \u2211 i xi) = \u03c6(x 1:`\u22121 + x`) (36)\nthen apply it inductively with x1:`\u22122 = \u2211`\u22122 i=1 xi and x`\u22121. Hence, we only need to show that \u03c6(x1)+\u03c6(x2) \u2265 \u03c6(x1 + x2), and we get this immediately setting a = 0, \u2206 = x1, b = x2 in Equation (15). The strictness part follows from Theorem 5.6, where is states that equality in \u03c6(x1:`\u22121)+\u03c6(x`) = \u03c6( \u2211 i xi)\nholds if and only if \u03c6 is linear from 0 through x1:`\u22121 + x` = \u2211 i xi.\nThe next result shows that when an SCMM has only one term, the addition of the final modular function m\u00b1 extends the family. We in show that this is the case, even when m\u00b1 is non-negative.\nTheorem 5.8. The family of an SCMM with one concave over modular term is enlarged by an additional modular term m\u00b1.\nProof. Consider a three-element ground set V = {a,b, c} and a function g,\ng(A) = min(|A|, 1) + 1c\u2208A, (37)\nthus g is monotone non-decreasing. Suppose g(A) = \u03c6(m(A)) for some non-negative modular function m and normalized non-decreasing concave function \u03c6. Then by Equation (24), we have:\nmin d\u2208\u2202\u03c6(m(a))\nd (i) \u2265 \u03c6(m(a,b))\u2212 \u03c6(m(a)) m(a,b)\u2212m(a) = 0 (ii) \u2265 max d\u2208\u2202\u03c6(m(a,b)) d (iii) \u2265 0 (38)\nwhere the (iii) follows since \u03c6 is monotone. Hence, (ii) is an equality and by Lemma 5.5 so is (i). Hence 0 \u2208 \u2202\u03c6(m(a)). Then we have that \u03c6(y) \u2264 \u03c6(m(a,b)) + 0(y \u2212 m(a,b)). This means that \u03c6(m(a,b, c)) \u2264 \u03c6(m(a,b)) = 1 < 2 = g(a,b, c), a contradiction.\nAn immediate corollary is that SCMMs are a larger class of submodular functions than just one concave over modular function. All SCMMs, however, can be represented as a sum of modular truncations as the following lemma states: Lemma 5.9 (Sums of Modular Truncations [141]). If f is an SCMM, then f may be written as f(A) =\u2211 i min(mi(A), \u03b2i) +m\u00b1(A) where for all i, mi is a non-negative modular function, \u03b2i \u2265 0 is a non-negative constant, and where the sum is over a finite number of terms.\nTruncating modular function is important, as it is not sufficient to truncate only cardinality functions. In other words, SCMMs also generalize the family of weighted cardinality truncations, as the next result shows.\nLemma 5.10 (Sums of Weighted Cardinality Truncations). We define the class of sums of weighted cardinality truncations as\nG = g : \u2200A, g(A) = \u2211 B\u2286V |B|\u22121\u2211 i=1 \u03b1B,i min(|A \u2229B|, i), where \u2200B, i, \u03b1B,i \u2265 0 . (39) Then there exists an f \u2208 SCMM that is not in G.\nLemma 5.10 is proven in Appendix B."}, {"heading": "5.2 Antitone Maps and Superdifferentials", "text": "Thanks to concave composition closure rules [19], the root function \u03c8r(x) : Rn \u2192 R in Eqn. (11) is a monotone non-decreasing multivariate concave function that, by the concave-submodular composition rule (Theorem 5.4) yields a submodular function \u03c8r(1A). It is widely known that any univariate concave function composed with non-negative modular functions yields a submodular function. However, given an arbitrary multivariate concave function this is not the case. Consider, for example, any concave function \u03c8 over R2 that offers the following evaluations: \u03c8(0, 0) = \u03c8(1, 1) = 1, \u03c8(0, 1) = \u03c8(1, 0) = 0. Then f(A) = \u03c8(1A) is not submodular, and hence the guarantee of submodularity when composing a concave with a linear function does not extend to dimensions higher than one. In this section, we discuss a limited form of such a generalization, one that ensures submodularity and that, moreover, does not even always rely on concavity in higher dimensions. Here and below, for x, y \u2208 RV , then x \u2264 y \u21d4 x(v) \u2264 y(v),\u2200v \u2208 V .\nDefinition 5.11. A concave function is said to have an antitone superdifferential if for all x \u2264 y we have that hx \u2265 hy for all hx \u2208 \u2202\u03c8(x) and hy \u2208 \u2202\u03c8(y).\nThe antitone superdifferential is an apparently straightforward multidimensional generalization of a defining characteristic of univariate concave functions. Theorem 5.12 below generalizes Theorem 5.4 when k = 1 \u2014 this is because \u03c6 : R\u2192 R being concave is, in the univariate case, synonymous with it having an antitone superdifferential (which is synonymous with monotone supergradients [60, 114]).\nTheorem 5.12. Let \u03c8 : Rk \u2192 R be a monotone non-decreasing concave function and let ~g : 2V \u2192 Rk be a vector of polymatroid functions, where ~g(A) = (g1(A), g2(A), . . . , gk(A)). Then if \u03c8 has an antitone superdifferential, then the set function f : 2V \u2192 R defined as f(A) = \u03c8(~g(A)) for all A \u2286 V is submodular.\nProof. Given two points x, y \u2208 Rn with x \u2264 y, then the fundamental theorem of calculus for line integrals states that for any smooth relative path p from x to y, the integral through the vector field \u2207\u03c8(z) yields \u03c8(y)\u2212\u03c8(x) = \u222b p \u2207\u03c8(x+z)dz. If \u03c8 is not differentiable, we may assume, with a slight abuse of notation, that \u2207\u03c8(x) is any gradient map for all x \u2208 Rn (i.e., \u2207\u03c8(x) maps from x to some element within \u2202\u03c6(x)). Given an arbitrary A \u2286 B and v /\u2208 B, and let p(t) be any relative and parametric curve from a point ~g(A) \u2208 Rk when t = 0 to a point ~g(A+ v) \u2208 Rk when t = 1. Hence, ~g(A) + p(0) = ~g(A) and ~g(A) + p(1) = ~g(A+ v). Since ~g is a vector of polymatroid functions, we have ~g(A) \u2264 ~g(B) and ~g(A) \u2264 ~g(A+ v), and hence, the path p(t) can be taken to be monotone, so that ~0 \u2264 p(t1) \u2264 p(t2) whenever 0 \u2264 t1 \u2264 t2 \u2264 1. Other than monotonicity, the path may be arbitrary. By monotonicity and submodularity, ~0 \u2264 ~g(B + v) \u2212 ~g(B) \u2264 ~g(A + v) \u2212 ~g(A), and hence we may choose the relative path that starts at ~0, and at some point t\u2032 \u2208 (0, 1), goes through the point p(t\u2032) = ~g(B + v)\u2212 ~g(B), and ends up at p(1) = ~g(A+ v)\u2212 ~g(A). Then,\nf(A+ v)\u2212 f(A) = \u03c8(~g(A+ v))\u2212 \u03c8(~g(A)) = \u222b 1\n0\n\u2207\u03c8(~g(A) + p(t)) \u00b7 dp(t) (40)\n\u2265 \u222b t\u2032\n0\n\u2207\u03c8(~g(A) + p(t)) \u00b7 dp(t) \u2265 \u222b t\u2032\n0\n\u2207\u03c8(~g(B) + p(t)) \u00b7 dp(t) (41)\n= \u03c8(~g(B + v))\u2212 \u03c8(~g(B)) = f(B + v)\u2212 f(B), (42)\nwhere the inequality follows from the monotonicity of \u03c8, the pointwise antitonicity of the gradient map, the non-negativity of the path, and by linearity of the integral. Hence, f is submodular.\nWe also fairly quickly get a partial corollary where we need not assume that \u03c6 is monotone non-decreasing. In the below, let b \u2208 RV+ be a non-negative real vector and for any set A \u2286 V , bA is a vector such that bA(v) = b(v) if v \u2208 A and otherwise bA(v) = 0 (e.g., when b = 1 then bA = 1A is the characteristic vector of set A).\nCorollary 5.12.1. Let \u03c8 : Rn \u2192 R be any concave function and b \u2208 RV+ be a non-negative real vector. Then if \u03c8 has an antitone superdifferential, then the set function f : 2V \u2192 R defined as f(A) = \u03c8(bA) for all A \u2286 V is submodular.\nProof. The proof is practically the same as that of Theorem 5.12 except we cannot use the monotonicity of \u03c8. Here the path p is any relative path from a point x \u2208 RV+ with x(v) = 0 to a point x + bv. Given an arbitrary A \u2286 B and v /\u2208 B, we then get f(A + v) \u2212 f(A) = \u03c8(bA+v) \u2212 \u03c8(bA) = \u222b p \u2207\u03c8(bA + z) \u00b7 dz \u2265\u222b\np \u2207\u03c8(bB + z) \u00b7 dz = \u03c8(bB+v)\u2212 \u03c8(bB) = f(B + v)\u2212 f(B).\nAlternatively, we can set k = n in Theorem 5.12 and for all v \u2208 V , set gv(A) = b(v)1v\u2208A which is a modular function. Then, the same relative path can be used to move from bA to bA+v as from bB to bB+v, so only antotonicity of \u03c8 is needed in the integral.\nGiven the above, the following result is not surprising.\nLemma 5.13. Let \u03c8 : Rn \u2192 R be a concave function formed by the sum of compositions of a scalar concave function and a linear function, i.e., \u03c8(x) = \u2211 i wi\u03c6i(\u3008mi, x\u3009) + \u3008m\u00b1, x\u3009 where mi \u2208 Rn+, wi \u2265 0 for all i, and m\u00b1 \u2208 Rn (i.e., an SCMM). Then \u03c8(x) has an antitone superdifferential.\nProof. From the chain rule, we get that \u2207\u03c8(x) = \u2211i wi\u03c6\u2032i(\u3008mi, x\u3009)mTi +mT\u00b1, and since \u03c6i is concave and mi is non-negative, wi\u03c6\u2032i(\u3008mi, x\u3009)mTi is monotone non-increasing in x (mT\u00b1 is constant). In the non-differentiable case, \u03c6i being monotone-concave implies that the same is true for any supergradient map. Closure over sums is immediate.\nCorollary 5.13.1. Any linear function has an antitone superdifferential.\nLemma 5.14. Composition of monotone non-decreasing scalar concave and antitone superdifferential concave functions preserves superdifferential antitonicity.\nProof. Let \u03c6 : R \u2192 R be a monotone non-decreasing concave functions and \u03c7 : Rn \u2192 R be a monotone non-decreasing concave function with an antitone superdifferential, and define \u03c8(x) = \u03c6(\u03c7(x)). Then by the chain rule, \u2207\u03c8(x) = \u03c6\u2032(\u03c7(x))\u2207\u03c7(x). Since \u03c7(x) is monotone non-decreasing in x, the first factor \u03c6\u2032(\u03c7(x)) is monotone non-increasing. The second factor is also monotone non-increasing, hence so is the product.\nCorollary 5.14.1. The root concave function \u03c8r associated with a DSF has an antitone superdifferential. Proof. The proof follows immediately from the fact that a DSF function (Equation (11)) is a recursive application of composition of monotone concave functions, non-negative sums of monotone concave functions, and the addition of a final linear function associated with m\u00b1.\nWhile having an antitone superdifferential is sufficient to yield a submodular function, it is not necessary. Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)]. This function is concave and is tight f(A) = \u03c8(1A),\u2200A at the vertices of the unit hypercube, but is not the concave closure of f [152]. The superdifferential is given by\n\u2202\u03c8(x) = { (f(v1|Sx), f(v2|Sx), . . . , f(vn|Sx)) : Sx \u2208 argmin\nS\u2286V [f(S) + \u2211 v\u2208V\nx(v)f(v|S)] }\n(43)\nand when evaluating at x = 1A we haveMA , argminS\u2286V [f(S)+ \u2211 v\u2208V 1Af(v|S)] = {A}\u222a{A\u2032 : A\u2032 = A\u2212 v,\u2200v \u2208 A}. To have an antitone supergradient, we need \u2200x \u2264 y and gx \u2208 \u2202\u03c8(x), gy \u2208 \u2202\u03c8(y), that gx \u2265 gy. Taking x = 1A and y = 1A+v for some v /\u2208 A, we can choose A \u2208MA and A\u2032 = (A+ v \u2212 v\u2032) \u2208MA+v with v\u2032 \u2208 A. In this case, we can find a monotone submodular function with f(vi|A) < f(vi|A + v \u2212 v\u2032) which violates antitonicity.\nIn order to explore this further, we consider the case where the function \u03c8 is twice differentiable. In this case, if \u03c8 is concave, then an antitone superdifferential means for all x \u2264 y, we have for all i, \u2202\u03c8\u2202xi (x) \u2265 \u2202\u03c8 \u2202xi\n(y). Setting y = x+ 1vj , we get for all i, j\n\u22022\u03c8\n\u2202xi\u2202xj (x) = lim \u21920\n\u2202\u03c8 \u2202xi (x+ 1vj )\u2212 \u2202\u03c8\u2202xi (x)\n\u2264 0, (44)\nwhich is thus also a sufficient condition for f(A) = \u03c8(1A) being submodular. The condition is stricter than necessary, however. Consider the quadratic \u03c8 : R2 \u2192 R with \u03c8(x) = xT ( 1 \u22122 \u22122 1 ) x+41Tx. Since \u03c6(0, 0) = 0, \u03c6(0, 1) = 5, \u03c6(1, 0) = 5, and \u03c6(1, 1) = 6, f(A) = \u03c6(1A) is monotone submodular. Here, we have \u2202 2\u03c8\n\u2202x1\u2202x2 = \u22124\nbut \u2202 2\u03c8 \u2202x2i = 2 for i \u2208 {1, 2}. Being submodular does not require the non-positivity of the diagonal elements of the Hessian matrix. In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established:\nTheorem 5.15. Let \u03c6 : Rn \u2192 R be a twice differentiable function. If for all i 6= j we have \u22022\u03c6/\u2202xi\u2202xj \u2264 0 then the function f : 2V \u2192 R where f(A) = \u03c6(1A) is submodular.\nThe above result is equivalent to \u2202\u03c6(x)/\u2202xj being decreasing in xi for all i 6= j. This suggests that the antitone superdifferential condition can also be weakened while still ensuring submodularity. Define d i\u03c8(x) = \u03c8(x + 1vi) \u2212 \u03c8(x). Then an antitone superdifferential is the same as, for all x \u2264 y having d i\u03c6(x) \u2265 d i\u03c8(y) for all i and > 0. This implies that d jd i\u03c8(x) \u2264 0 for all i, j. The weaker condition asks that d jd i\u03c8(x) \u2264 0 for all i 6= j, and > 0, and this is the same as\n\u03c8(x+ 1vi) + \u03c8(x+ 1vj ) \u2265 \u03c8(x+ 1vi + 1vj ) + \u03c8(x) (45)\nwhich essentially is a restatement of the property of submodularity but on the reals. Note that when i = j, this (and \u22022\u03c6/\u2202x2i \u2264 0 in the twice differentiable case) asks for the function to be concave in the direction of each axis, but submodularity, as Theorem 5.15 states, does not require this. Indeed, submodularity is a relationship between distinct variables, not a criterion on any one particular variable.\nThe weaker condition (Theorem 5.15) is also not necessary for concavity, as the aforementioned quadratic is neither concave nor convex. Concavity requires non-positive definiteness of the Hessian matrix, something that antitone maps do not ensure. A map is any function h : RV \u2192 RV and is antitone if for all x, y \u2208 RV , (x \u2212 y)T (h(x) \u2212 h(y)) \u2264 0 for all x, y. Not only does an antitone map alone not ensure concavity (a result established originally in [128, 129]), an antitone map need not be a gradient field (a property that, if true, would make it a conservative field). For an example related to submodular functions, the multilinear extension [119], defined as:\nf\u0303(x) = \u2211 S\u2286V f(V ) \u220f i\u2208S xi \u220f j\u2208V \\S (1\u2212 xj) (46)\nhas the property that f\u0303(1A) = f(A) for all A \u2286 V . It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8]. When f is submodular, it has \u22022f\u0303(x)/\u2202xi\u2202xj \u2264 0 for all i, j, not only abiding Theorem 5.15 but also for i = j it has \u2202\u03c62/\u2202x2i = 0 since it is multilinear. Hence, multilinear extension also has an antitone map, but is also neither convex nor concave and hence has neither a subdifferential nor a superdifferential. Indeed, concavity is not at all required for an extension of a submodular function, another well known example being the Lov\u00e1sz extension of f\u0306 : RV \u2192 R of f which is a convex, has f(A) = f\u0306(1A), is defined as f\u0306(x) =\u2211n i=1 x\u03c3if(\u03c3i|\u03c31, \u03c32, . . . , \u03c3i\u22121) where \u03c3 = (\u03c31, \u03c32, . . . , \u03c3n) is an x-dependent order ensuring x\u03c31 \u2265 x\u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 x\u03c3n . f\u0306 is not twice differentiable but it has a subgradient g \u2208 \u2202f\u0306(x) where g(i) = f(\u03c3i|\u03c31, \u03c32, . . . , \u03c3i\u22121). Given x \u2264 y, a decreasing order of y can be arbitrarily different than for x implying \u2202f\u0306(x) is neither antitone nor monotone, so d id j f\u0306(x) \u2264 0 is not a property of the Lov\u00e1sz extension. Also, any function defined only on the vertices of the unit hypercube has an infinite number of both concave and convex extensions [28]. The approach above shows that antitone superdifferentials involves both concavity and submodular functions. Since Theorem 5.15 does not require concavity, however, this suggests that there may be a way to define submodular functions using generalized line integrals of antitone maps without needing concavity [131].\nWe also note that Theorem 5.15 is given as a sufficient condition, but not a necessary condition, for submodularity when we consider \u03c6 as a function used to produce f(A) = \u03c6(1A). Let \u03c6 be any function satisfying Theorem 5.15 and \u03c7 be any other function having \u03c7(1A) = 0 for all A \u2286 V . Then f(A) = \u03c6(1A) + \u03c7(1A) is submodular while \u03c6(x) + \u03c7(x) need not satisfy the theorem. Theorem 5.15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined. For example, defining \u2202if(A) = f(A\u222a {i})\u2212 f(A \\ {i}) for i \u2208 V , we have that a function f : 2V \u2192 R is submodular if and only if for i 6= j, \u2202i\u2202jf(A) \u2264 0. This is in contrast to how we use it above, which to define a submodular function only on the unit hypercube vertices starting from a function defined on Rn.\nGetting back to DSFs, since the concave function associated with a DSF has an antitone superdifferential, and since this is sufficient but not necessary for submodularity, this suggests (but does not guarantee, since DSFs evaluate \u03c8 only at hypercube vertices 1A) that the family of DSFs might not comprise all submodular functions. While in Section 6 we show that DSFs generalize SCMMs, and in Section 6.2 we show that increasing the layers in a DSF increases the size of the family, Section 6.3 shows, by giving an example, that not all submodular function can be represented by DSFs.\nIn closing this section, we state an additional potential advantage of DSFs. Ordinarily the concave closure of a submodular function is computationally hard to evaluate [152] and this is disappointing since such a construct would be useful for relaxation schemes for maximizing submodular functions (and as result surrogates, such as the multilinear extension are used). In the DSF case, however, a particular concave extension is very easy to get, namely \u03c8r(x)+\u3008m\u00b1, x\u3009. This extension perhaps could be useful for maximizing DSFs, possibly constrainedly, using concave maximization followed by appropriate rounding methods."}, {"heading": "5.3 The Special Matroid Case and Deep Matroid Rank", "text": "We discuss in this section the special case of matroids and matroid ranks as they motivate and offer insight to the results later in the paper.\nA matroid M [46] is a set system M = (V, I) where I = {I1, I2, . . .} is a set of subsets Ii \u2286 V that are called independent. A matroid has the property that \u2205 \u2208 I, that I is subclusive (i.e., given I \u2208 I and I \u2032 \u2282 I then I \u2032 \u2208 I) and that all maximally independent sets have the same size (i.e., given A,B \u2208 I with |A| < |B|, there exists a b \u2208 B \\ A such that A + b \u2208 I). The rank of a matroid, a set function r : 2V \u2192 Z+ defined as r(A) = maxI\u2208I |I \u2229A|, is a powerful class of submodular functions. All matroids are defined uniquely by their rank function as I = {A : r(A) = |A|} and therefore, we can reason about if two matroids are equivalent or not based on if their ranks are equal, and vice verse. All monotone non-decreasing non-negative integral submodular functions can be exactly represented by grouping and then evaluating grouped ground elements in a matroid [46].\nA useful matroid in machine learning applications [94, 9] is the partition matroid, where a partition (V1, V2, . . . , V`) of V is formed, along with a set of capacities k1, k2, . . . , k` \u2208 Z+. It\u2019s rank function is defined as: r(X) = \u2211` i=1 min(|X \u2229 Vi|, ki) and, therefore, is an SCMM.\nA cycle matroid is a different type of matroid based on a graph G = (V,E) where the rank function r(A) for A \u2286 E is defined as the size of the maximum \u201cspanning forest\u201d (i.e., a spanning tree for each connected component) in the edge-induced subgraph GA = (V,A). From the perspective of matroids, we can consider classes of submodular functions via their rank. If a given type of matroid cannot represent another kind, their ranks lie in distinct families. To study where DSFs are situated in the space of all submodular functions, it is useful first to study results regarding matroid rank functions.\nLemma 5.16. There are partition matroids that are not cycle matroids.\nProof. Consider the partition matroid over |V | = 4 elements and consider a partition with one block and a capacity of two, so r(X) = min(|X|, 2), so any two elements has rank 2. For this matroid to be a cyclic matroid, we must have a graph with 4 edges where every set of three (out of those 4) must contain a cycle. Lets name the edges a,b, c,d, then a,b, c contains a cycle and so does a,b,d, while a,b does not contain a cycle ({a,b} has rank 2). The only way this can happen is if either c, d are parallel edges, or of c is parallel to one of a or b, and d is also parallel to one of a or b, or if c and d are loops. In any of the above\ncases, we now have two edges that are parallel, or that contain loops, but they must have rank 2, which is a contradiction.\nIn a laminar matroid, a generalization of a partition matroid, we start with a set V and a family F = {F1, F2, . . . , } of subsets Fi \u2286 V that is laminar, namely that for all i 6= j either Fi \u2229 Fj = \u2205 or Fi \u2286 Fj or Fj \u2286 Fi (i.e., sets in F are either non-intersecting or comparable). In a laminar matroid, we also have for every F \u2208 F an associated capacity kF \u2208 Z+. A set I is independent if |I \u2229 F | \u2264 kF for all F \u2208 F . A laminar family of sets can be organized in a tree, where there is one root R \u2208 F in the tree that, w.l.o.g., can be V itself. Then the immediate parents pa(F ) \u2282 F of a set F \u2208 F in the tree are the set of maximal subsets of F in F , i.e., pa(F ) = {F \u2032 \u2208 F : F \u2032 \u2282 F and 6 \u2203F \u2032\u2032 \u2208 F s.t. F \u2032 \u2282 F \u2032\u2032 \u2282 F}. We then define the following for all F \u2208 F :\nrF (A) = min( \u2211\nF \u2032\u2208pa(F )\nrF \u2032(A \u2229 F \u2032) + |A \\ \u22c3\nF \u2032\u2208pa(F )\nF |, kF ). (47)\nA laminar matroid rank has a recursive definition r(A) = rR(A) = rV (A). Hence, if the family F forms a partition of V , we have a partition matroid. More interestingly, when compared to Eqn. (11), we see that a laminar matroid rank function is an instance of a DSF with a tree-structured DAG as shown in Figure 2. Thus, within the family of DSFs lie the truncated matroid rank functions used to show information theoretic hardness for many constrained submodular optimization problems [52], i.e., start with the partition matroid rank r(A) = min(|A \u2229R|, a) + min(|A \u2229 R\u0304|, |R\u0304|) = min(|A \u2229R|, a) + |A \u2229 R\u0304| and then truncate it as follows:\nfR(A) = min {r(A), b} = min { |A|, a+ |A \u2229 R\u0304|, b } (48)\nwith a < b. This is a function where fR(R) = a and fR(A) > a for A 6= R and |A| = |R| and can be set up to have most size \u2265 |R| sets A valued at fR(A) = b. Since this function is used to show hardness for many constrained submodular minimization problems, and since DSFs generalize laminar matroid ranks, this portends poorly for algorithms of the kind found in [70, 117] to achieve fast DSF minimization.\nLaminar matroids are more general than partition matroids. From the perspective of matroid rank, we have:\nLemma 5.17. Laminar matroids strictly generalize partition matroids\nProof. Consider a simple laminar family F = {V,B} where kV = 2, B \u2282 V with kB = 1, and |B| \u2265 2 and |V | \u2265 |B|+ 2 giving rank function\nr(X) = min(min(|X \u2229B|, 1) + |X \\B|, 2). (49)\nSuppose we are given any set of subsets {Ci}i of V and corresponding integer capacities {ki}i giving the submodular function:\nrs(X) = \u2211 i min(|X \u2229 Ci|, ki). (50)\nand suppose that rs(X) = r(X) which means rs(X) must be a matroid rank function. Note that ki \u2265 1 otherwise term i is vacuous. The Ci must be disjoint, for if not let Ci \u2229 Cj 6= \u2205, i 6= j and pick v \u2208 Ci \u2229 Cj , which gives rs(v) \u2265 2 implying rs is not a matroid rank function. Hence the sets Ci must be disjoint and rs is a partition rank function over \u222aiCi. Choose two elements b1, b2 \u2208 B. If b1 \u2208 Ci and b2 \u2208 Cj for i 6= j this gives rs({b1, b2}) = 2 6= r({b1, b2}) = 1. Hence, there is a unique i such that B \u2286 Ci. Thus, ki = 1 since if not we would get rs({b1, b2}) = 2. If there exists a v \u2208 Ci \\B then for any b \u2208 B, rs(v, b) = 1 6= 2 = r(v, b). Hence, we must have Ci = B. Now take v1, v2 /\u2208 B so that r({v1, v2}) = rs({v1, v2}) = 2, but the term of rs involving B does not involve v1, v2 so that for b \u2208 B, rs({v1, v2, b}) = 3 which is a contradiction. Hence, a laminar matroid is a strict generalization of a partition matroid.\nSince a laminar matroid generalizes a partition matroid, this augurs well for DSFs generalizing SCMMs (a result we provide in Theorem 6.4). Before considering that, we already are up against some limits of laminar matroids, i.e.:\nLemma 5.18. Laminar matroid cannot represent all cycle matroids.\nProof. Consider the cycle matroid over edges on K4, hence M = (V, I) with |V | = 6, V being the set of edges, where r(X) = |X| for |X| \u2264 2, r(X) = 2 when X is any 3-cycle, r(X) = 3 for any acyclic X with |X| = 3, and r(X) = 3 for |X| > 3. Consider the form of the laminar matroid in Eqn. (47) and suppose rV (X) = r(X) for all X. W.l.o.g., we may assume kV = 3. Suppose \u2203e \u2208 V \\\u222aF\u2208pa(V )F . Then consider any 3-cycle C involving e, and rV (C \u2212 e) = 2 but since no element of pa(V ) contains e, there is no truncation, giving rV (C) = 3, a contradiction. Hence, V = \u222aF\u2208pa(V )F . Given a 3-cycle C = {a,b, c}, suppose there exists an F \u2208 pa(V ) with a \u2208 F and b /\u2208 F and c /\u2208 F . Since we must have rV ({b, c}) = 2 and rV ({a}) = 1, this implies rV ({a,b, c}) = 3, also a contraction. Hence, any three cycle must be in one element of pa(V ), and by transitive closure over the four intersecting three-cycles, all elements of V must be in only one member of pa(V ). This implies that |pa(V )| = 1 and the only way to represent the 3-cycles is within that one term, rF (X). This process then is applied recursively until we are left with the base case, where the entire recursion boils down to the form rV (X) = min(rF (X), 3) = min(min(|X|, kF ), 3) = min(|X|,min(kF , 3)). This clearly cannot represent the cycle matroid rank function for any value of kF \u2208 Z+.\nThe proof technique is reminiscent of the back propagation method used to train DNNs and hence we call it \u201cbackprop proof\u201d \u2014 it recursively backpropagates required properties from the root though each layer (in a DSF sense) of a laminar matroid rank until it boils down to a partition matroid rank function, where the base case is clear. The proof is elucidating since it motivates the proof of Theorem 6.4 showing that DSFs extend SCMMs. We also have the immediate corollary.\nCorollary 5.18.1. Partition matroids cannot represent all cycle matroids."}, {"heading": "5.4 Surplus and Absolute Redundancy", "text": "In this section, we introduce and study the notion of the surplus of a set as measured by a submodular function. The surplus is a useful concept and will be used extensively to show, in Section 6, various properties of the DSF family.\nDefinition 5.19 (Surplus and Absolute Redundancy). For a function f : 2V \u2192 R, we define Sf (A) as the surplus (or absolute redundancy) of a set A \u2286 V by f as follows:\nSf (A) = \u2211 a\u2208A f(a)\u2212 f(A) (51)\nWe call Sf (A) the surplus of A by f . We use the term \u201csurplus\u201d under an interpretation where A is a set of agents that can perform their action either independently of each other, or may perform their actions jointly and cooperatively [149]. If an agent a \u2208 A performs the action independently, the cost is f(a) with an overall cost of \u2211 a\u2208A f(a), while if the agents A perform the action cooperatively, the overall cost is f(A).\nThe difference Sg(A) = \u2211 a\u2208A f(a)\u2212f(A) is the surplus obtained by performing the actions A cooperatively rather than individually. When g is submodular, surplus is never negative. Hence, performing the actions jointly leads overall to profit.3\nThe idea of surplus has occurred before in the field of information theory but under a different name \u2014 in this case, f(A) = H(XA) is the entropy function of a set of random variables indexed by the set A. The quantity Sf (A) = \u2211 a\u2208AH(Xa) \u2212 H(XA) is the average bit-length penalty between optimally coding the random variables in A separately (as if they were independent) vs. optimally coding them jointly. This can, thus, be called the absolute redundancy of the set A. For the entropy function, this idea was first defined in [102].4 Absolute redundancy is also called \u201ctotal correlation\u201d [154] and also the \u201cmulti-information\u201d function [143]. Our notion of surplus is not the same as [118] where they define a quantity called \u201cdeficiency\u201d the negative of which may be considered a kind of surplus. Since there may neither be a statistical, information theoretic, nor economic interpretation, we actually prefer the terms \u201ctotal interaction\u201d or \u201ccombinatorial interaction.\u201d In the below, if only for the sake of brevity, we utilize the term \u201csurplus,\u201d but stress that it applies to any submodular function whatever its interpretation. We say that the function g \u201cgives surplus\u201d to a set A whenever Sg(A) > 0 and otherwise A has \u201cno surplus.\u201d\nIn the below, we explore a number of properties and introduce a number of variants of surplus, all of which are useful later in the paper.\nLemma 5.20 (Linearity of Surplus). Let f1, f2 be two functions and \u03b11, \u03b12 \u2208 R+. Then for any A \u2286 V\nS\u03b11f1+\u03b12f2(A) = \u03b11Sf1(A) + \u03b12Sf2(A) (52)\nLemma 5.21 (Surplus is Immune to Modularity). Modular functions do not change surplus, i.e., when m : V \u2192 R is a normalized modular function and f is any set function:\nSf+m(A) = Sf (A) (53)\nThat modular functions do not influence surplus is useful to be able to ignore the final modular function m\u00b1 in a DSF when studying its properties.\nLemma 5.22 (Non-negativity of Surplus). When f is normalized (f(\u2205) = 0) and submodular, then for all A \u2286 V , Sf (A) \u2265 0.\nProof. For any A \u2286 V , with A = {a1, a2, . . . , ak},\nf(A) = k\u2211 i=1 f(ai|a1, a2, . . . , ai\u22121) \u2264 k\u2211 i=1 f(ai) (54)\nThus, with a submodular function in such a context, therefore, there can never be any deficit (negative surplus) and it is always beneficial to act cooperatively. How fairly to redistribute surplus back to the individual agents is called the \u201csurplus sharing problem\u201d and is studied in [149].\nLemma 5.23 (Mixtures Preserve Surplus). Let f1, f2, . . . be a set of submodular functions and \u03b11, \u03b12, . . . be a set of positive real-valued weights, and define f = \u2211 i \u03b1ifi as their conic combination. Then we have Sf (A) > 0 if and only if \u2203i with Sfi(A) > 0. 3In [149], surplus is defined as f(A)\u2212 \u2211 a\u2208A f(a) where f is a supermodular function, but the same idea still applies.\n4Incidentally, in 1954, [102] was also the first, to the authors knowledge, to provide inequalities on the entropy function that are identical to the submodularity condition.\nProof. This follows when one considers that \u2200i,Sfi(A) \u2265 0 for all A, that \u2200i, \u03b1i > 0, and that Sf (A) =\u2211 i \u03b1iSfi(A).\nThe next theorem is particularly important for showing certain properties of DSFs, in particular, Corollary 6.23.1.\nTheorem 5.24 (Concave Composition Preserves Surplus). Let h : 2V \u2192 R be a polymatroid function and \u03c6 : R \u2192 R be a normalized monotone non-decreasing concave function that is not identically zero. Define g : 2V \u2192 R as g(A) = \u03c6(h(A)). Then Sh(A) > 0 implies Sg(A) > 0.\nProof. Since g(\u00b7) is polymatroidal (by Theorem 5.4), Sg(A) \u2265 0 for all A. Order A arbitrarily as A = {a1, a2, . . . , ak} with k = |A|. Then since \u2211k i=1 h(ai) > h(A),\nk\u2211 i=1 \u03c6(h(ai)) (a) \u2265 \u03c6( k\u2211 i=1 h(ai)) (b) \u2265 \u03c6(h(A)), (55)\nwhere (a) follows from Theorem 5.7 and (b) follows from the monotonicity of \u03c6. If \u2211k i=1 h(ai) is still in the\nlinear part of \u03c6(\u00b7) then (b) is strict, while if\u2211ki=1 h(ai) is greater than the linear part of \u03c6(\u00b7) then, from the second part of Theorem 5.7, (a) is strict. In either case, Sg(A) > 0.\nProposition 5.25 (Concave Composition Increases Surplus). Let h : 2V \u2192 R be a polymatroid function with h(v) = 1 for all v \u2208 V , and \u03c6 : R \u2192 R be a normalized monotone non-decreasing concave function that is not identically zero and where \u03c6(1) = 1. Define g : 2V \u2192 R as g(A) = \u03c6(h(A)). Then for any A, Sg(A) \u2265 Sh(A).\nDefinition 5.26 (Grouped Surplus). We define a form of grouped surplus as follows. Given a set of m disjoint sets A1, A2, . . . , Am \u2286 V , we define:\nI (m) f (A1;A2; . . . ;Am) , m\u2211 i=1 f(Ai)\u2212 f( m\u22c3 i=1 Ai) (56)\nWhen f(A) = H(XA) is the entropy function, then the pairwise surplus I (2) f (A;B) is the well-known mutual information [27] between random variable sets XA and XB . The grouped surplus can be defined in terms of standard surplus via I(m)f (A1;A2; . . . ;Am) = Sf ({A1}, {A2}, . . . , {Am}) where we treat each of the sets {Ai}i as a singleton element groups in the standard surplus. Thus, for any m, we have Imf (A1;A2; . . . ;Am) \u2265 0 for any normalized submodular function f . We also have the following:\nProposition 5.27. Given a submodular function f and a set A \u2286 V , if Sf (A) = 0 then I(m)f (A1;A2; . . . ;Am) = 0 for any m and proper m-partition A1, A2, . . . , Am \u2286 A of A. Moreover, we have:\nSf ( m\u22c3 i=1 Ai) > I (m) f (A1;A2; . . . ;Am) (57)\nFor example, if I(2)f (A;B) > 0 then Sf (A\u222aB) > 0. The converse is not true in general, i.e., we can have I\n(2) f (A;B) = 0 while still having Sf (B) > 0. Of particular interest in this paper will be pairwise surplus of the form I(2)f (e \u2032;C) where C is a three-cycle of a graphic matroid, and e\u2032 /\u2208 C. When it is clear from the context, we will drop the superscript m and state If (A1;A2; . . . ;Am) , I (m) f (A1;A2; . . . ;Am) for any m. Considering Proposition 5.27 and Definition 5.26, we immediately obtain the following:\nProposition 5.28 (Concave Composition Preserves Grouped Surplus). Let h : 2V \u2192 R be a polymatroid function and \u03c6 : R \u2192 R be a normalized monotone non-decreasing concave function that is not identically zero. Define g : 2V \u2192 R as g(A) = \u03c6(h(A)). Then for any m and any set of m disjoint sets A1, A2, . . . , Am, we have I(m)h (A1;A2; . . . ;Am) > 0 implies I (m) g (A1;A2; . . . ;Am) > 0.\nDefinition 5.29 (Modular at B). We say a function h : 2V \u2192 R is modular at B \u2286 V if h(B) = \u2211b\u2208B h(b). When h is modular at B, it does not necessarily mean that it is modular at some A \u2282 B. However, we do have the following:\nLemma 5.30. If h : 2V \u2192 R is a submodular function. Then h is modular at all A \u2286 B if and only if Sh(B) = 0.\nProof. If h is modular for all A \u2286 B, then h(A) = \u2211a\u2208A h(a), and Sh(B) = 0. Conversely, suppose h is submodular and Sh(B) = 0 and let A \u2286 B be given. Then\nh(B) = \u2211 b\u2208B h(b) \u2265 h(A) + \u2211 b\u2208B\\A h(b) \u2265 h(B) (58)\nHence, all inequalities are equalities. Subtracting \u2211 b\u2208B\\A h(b) from both sides of the first inequality gives the result.\nLemma 5.31 (Forced Separation). Let h : 2V \u2192 R be a polymatroid function and A,B,C be disjoint subsets where Ih(A;B) = Ih(B;C) = Ih(C;A) = 0. Then if h(A) = 0 then Ih(A;B;C) = 0.\nProof. Consider the following:\nh(A) + h(B) + h(C) = h(B) + h(C) = h(B \u222a C) \u2264 h(A \u222aB \u222a C) (59) \u2264 h(A) + h(B \u222a C) = h(B \u222a C), (60)\nwhere the first equality is because h(A) = 0, the next is since Ih(B;C) = 0, the next (an inequality) is due to monotonicity, the subsequent inequality is due to submodularity, and the final one is since h(A) = 0. Hence, all inequalities are equalities, and Ih(A;B;C) = 0.\nAs an example, if A = {a}, B = {b}, C = {c}, then the consequence of the lemma is that h would be modular at the set {a,b, c}.\nThe next lemma shows how we can hold the surplus of a set accountable either to the concave function of a concave composition function or to somewhere else internal in the polymatroid function.\nLemma 5.32 (When Concave Composition Is Linear). Let h : 2V \u2192 R be a polymatroid function and \u03c6 : R \u2192 R be a normalized monotone non-decreasing concave function that is not identically zero. Define g : 2V \u2192 R as g(X) = \u03c6(h(X)) for any X \u2286 V . Given two disjoint sets A,B \u2286 V where g(A) > 0, g(B) > 0, and Ig(A;B) = 0, then any surplus Sg(A) > 0 given to A is not due to any non-linearity in \u03c6(\u00b7) but rather is due entirely to h(\u00b7). Moreover, g(X) = \u03b3h(X) for all X \u2286 A \u222aB for some \u03b3 > 0.\nProof. By Theorem 5.24, Ig(A;B) = 0 implies that Ih(A;B) = 0. Then we have\n\u03c6(h(A)) + \u03c6(h(B)) = \u03c6(h(A \u222aB)) = \u03c6(h(A) + h(B)). (61)\nAlso, g(A) > 0 \u21d2 h(A) > 0 and g(B) > 0 \u21d2 h(B) > 0. Hence, by Theorem 5.6, \u03c6(\u00b7) is linear in the range [0, h(A) + h(B)]."}, {"heading": "6 The Family of Deep Submodular Functions", "text": "We have seen that SCMMs generalize partition matroid rank functions and DSFs generalize laminar matroid rank functions. We might expect, from the above results, that DSFs might strictly generalize SCMMs \u2014 this is not immediately obvious since SCMMs are significantly more capable than partition matroid rank functions because: (1) the concave functions need not be simple truncations at integers, (2) each term can have its own non-negative modular function, (3) there is no requirement to partition the ground elements over terms in an SCMM, and (4) we are allowed with SCMMs to add an additional arbitrary modular function. We also have already seen Theorem 5.8 showing that SCMMs are a larger class of submodular functions than just one concave over modular function and, in Lemma 5.10, that they generalize weighted cardinality\ntruncations. SCMMs seem therefore to be quite dexterous. The next several sections show, however, that DSFs strictly generalize SCMMs.\nMore specifically, we formally place DSFs within the context of general submodular functions. We show in Section 6.1 that DSFs strictly generalize SCMMs while preserving many of their attractive attributes (i.e., featurization, multi-modal, and amenability to learning, streaming, and parallel optimization). Then in Section 6.2, we show that the family of DSFs strictly grow with the number of layers uses. In Section 6.3, however, we show that the family of DSFs still do not comprise all submodular functions. We summarize the results of this section in Figure 4, and that includes familial relationships amongst other classes of submodular functions (e.g., various matroid rank functions mentioned in Section 5.3)."}, {"heading": "6.1 DSFs generalize SCMMs", "text": "It is clear that DSFs contain at least the class of SCMMs since any one-layer DSF is an SCMM. We next show that SCMM \u2282 DSF holds, or that DSFs strictly generalize SCMMs, thus providing justification for using DSFs over SCMMs and, moreover, generalizing Lemma 5.17 to the non matroid case. The first DSF we choose is a laminar matroid, so SCMMs are unable to represent laminar matroid rank functions even given their additional flexibility over partition matroid rank functions. Since DSFs generalize laminar matroid rank functions, the result follows.\nIt is not immediately apparent that DSFs generalize SCMMs as the following example demonstrates. Consider the DSF f : 2V \u2192 R where V = {a,b, c,d, e, f}:\nf(A) = min ( min(|A \u2229 {a,b, c}|, 1) + min(|A \u2229 {d, e, f}|, 1), 1.5 )\n(62)\nThe reader is encouraged to ponder, for a moment, how one might represent this DSF as an SCMM. Indeed, this is one case where it is possible, as seen by the following SCMM g : 2V \u2192 R\ng(A) = \u03c6(|A \u2229 {a,b, c}|) + \u03c6(|A \u2229 {d, e, f}|) + min(|A|, 0.5)\u2212 0.5|A| (63)\nwhere \u03c6 : R \u2192 R is concave, with \u03c6(\u03b1) = min(\u03b1, 0.5 + 0.5\u03b1). It can be verified that g(A) = f(A) for all A \u2286 V . In fact, an even simpler SCMM does not use a modular function at all and puts g(A) = 1 2 (min(|A\u2229{a,b, c}|, 1)+min(|A\u2229{d, e, f}|, 1)+min(|A|, 1)). From this example, one might naturally surmise that the DSFs unable to be represented by SCMMs are obscure, contrived, and complicated. In the next two sections, however, we show two fairly simple DSFs and show that no SCMM can represent them. Then in Section 6.1.3, we provide more general conditions describing when 2-layer DSF do or do not generalize SCMMs."}, {"heading": "6.1.1 The Laminar Matroid Rank Case", "text": "Our first example DSF we choose is a simple laminar matroid on six elements. We show that SCMMs cannot express this laminar rank function and since DSFs generalize laminar matroid ranks, the result follows. Consider the following function f : 2V \u2192 R where V = {a,b, c,d, e, f}:\nf(A) = min ( min(|A \u2229 {a,b, c}|, 2) + min(|A \u2229 {d, e, f}|, 2), 3 )\n(64)\nThe function is a laminar matroid rank function with F = {V, {a,b, c}, {d, e, f}} and limits kV = 3, k{a,b,c} = 2, k{d,e,f} = 2.\nIn the following results, we assume that g : 2V \u2192 R is an SCMM of the form g(A) = \u2211i\u2208M gi(A)+m\u00b1(A) where gi(A) = \u03c6i(mi(A)) is a normalized monotone non-decreasing concave function composed with a nonnegative modular function, m\u00b1(A) is an arbitrary normalized modular function, andM is an index set. Since f itself is normalized, then we must also have g(\u2205) = 0 as well. Also define B1 = {a,b, c} and B2 = {d, e, f}.\nLemma 6.1. Suppose f(A) = g(A) for all A. Then there does not exist an i \u2208 M where gi offers surplus both to B1 and B2 (i.e., there exists no i such that Sgi(B1) > 0 and Sgi(B2) > 0.\nProof. Suppose to the contrary that there exists such an i, Then both mi(B1) and mi(B2) must both be past the last linear point of \u03c6i, say \u03b1i. We have that\nmi(B1) +mi(B2) = mi({a,b, c} \u222a {d, e, f}) = mi({a,b,d} \u222a {c, e, f}) (65) = mi({a,b,d}) +mi({c, e, f}) (66)\nSince mi(B1) > \u03b1i and mi(B2) > \u03b1i we must have at least one of mi({a,b,d}) > \u03b1i or mi({c, e, f}) > \u03b1i, w.l.o.g., say {a,b,d}. This implies that Sgi({a,b,d}) > 0 giving g an unrecoverable surplus which is a contradiction since Sf ({a,b,d}) = 0.\nThe next result is our first instance of a DSF that cannot be represented by an SCMM.\nLemma 6.2. No SCMM can represent the DSF in Equation (64).\nProof. For clarity, we offer the proof as a series of numbered statement groups.\n1. Lemma 6.1 means that we can write g as follows:\ng(A) = \u2211 i\u2208M1 gi(A) + \u2211 i\u2208M2 gi(A) + \u2211 i\u2208M0 gi(A) (67)\nwhereM0,M1,M2 is a partition ofM, and where for all i \u2208 M1, gi gives surplus to B1 but not to B2, for all i \u2208 M2, gi gives surplus to B2 but not to B1, and for all i \u2208 M0, gi gives surplus neither to B1 nor B2. Hence, for all i \u2208M1 and v \u2208 B1 we have gi(v) > 0, and for all i \u2208M2 and v \u2208 B2 we have gi(v) > 0 by Lemma 5.31. Furthermore, since B1 and B2 are the only sets of size three that are given a surplus, then for all i \u2208M0, Sgi(A) = 0 for all A with |A| \u2264 3.\n2. We also need to have zero pairwise surplus such as:\nIg(e; {a,b, c}) = If (e; {a,b, c}) = 0 (68)\nThis implies that for i \u2208 M, Igi(e; {a,b, c}) = 0. Since we know that mi(B1) is past the non-linear part of \u03c6i for i \u2208M1 and mi(B2) is past the non-linear part of \u03c6i for i \u2208M2, the only way to achieve this (and corresponding values such as Ig(b; {d, e, f}) = 0) is if both: (1) for i \u2208 M1, gi(v) = 0 when v \u2208 B2; and (2) for i \u2208M2, gi(v) = 0 when v \u2208 B1. In other words, gi with i \u2208 M1 not only offers no surplus for B2 but also give zero valuation for any v \u2208 B2 (and vice verse).\n3. Consider the following set of size-four sets A = {A \u2286 V : |A \u2229B1| = |A \u2229B2| = 2}. Note that |A| = 9. For any A \u2208 A, we have\nSf (A) = Sg(A) = \u2211 i\u2208M Sgi(A) = 1. (69)\nFor i \u2208M1 \u222aM2, we have Sgi(A) = 0 since two elements of A are given zero value to every such gi. Hence, the only terms that can achieve Equation (69) are those i within M0 having Sgi(A) > 0, where mi(A) > \u03b1i, and where \u03b1i is the last linear part of \u03c6i. Also, to ensure no unrecoverable surplus occurs, we must have that mi(C) \u2264 \u03b1i for any C having the following properties: (1) any size-three set; (2) any size-four set C with |C \u2229 B1| = 3 and |C \u2229 B2| = 1 (because Sgi(C \u2229 B1) = 0 and Igi(C \u2229B2;C \u2229B1) = 0); and (3) any size-four set C with |C \u2229B1| = 1 and |C \u2229B2| = 3. For example, with A = {a,b,d, e} and C = {a,b, c,d}, we have that\nmi(a,b, c,d) \u2264 \u03b1i < mi(a,b,d, e) = mi(A) (70)\nimplying that mi(c) < mi(e). For any A \u2208 A, define A2(A) = {A\u2032 \u2208 A : |A\u20324A| = 2} and A4(A) = {A\u2032 \u2208 A : |A\u20324A| = 4}. Then |A2(A)| = 4, |A4(A)| = 4, and A = {A}\u222aA2(A)\u222aA4(A). Suppose mi(A\u2032) > \u03b1i where A\u2032 \u2208 A4(A). For example, with A = {a,b,d, e} as above, and A\u2032 = {b, c,d, f} \u2208 A4(A) , this implies that mi(d, e, f,b) \u2264 \u03b1i < mi(b, c,d, f) implying that mi(e) < mi(c), a contradiction with the above. Hence, we must have mi(A\n\u2032) \u2264 \u03b1i. More generally, gi offering surplus to more than one member of A4(A) leads to a contradiction. Also, if A\u2032 \u2208 A4(A), then \u2203A\u2032\u2032 \u2208 A4(A\u2032) with A\u2032\u2032 6= A and A\u2032\u2032 \u2208 A4(A). For example, with A and A\u2032 given as above, A\u2032\u2032 = {a, c, e, f}. No more than one of this trio {A,A\u2032, A\u2032\u2032} can be offered surplus by the same gi for i \u2208M0. This means that we may partition the indicesM0 = { M(0)0 ,M (1) 0 ,M (2) 0 ,M (3) 0 } so that\ni \u2208 M(1)0 may give surplus to A, but neither A\u2032 nor A\u2032\u2032, i \u2208 M (2) 0 may give surplus to A \u2032 but neither A nor A\u2032\u2032, i \u2208 M(3)0 may give surplus to A\u2032\u2032 but neither A nor A\u2032, and i \u2208 M (0) 0 gives no surplus any of the trio.\nWe must then have 3 = Sf (V ) = \u2211\nj\u2208{0,1,2} \u2211 i\u2208Mj Sgi(V ) (71)\n\u2265 \u2211 i\u2208M1 Sgi(B1) + \u2211 i\u2208M2 Sgi(B2) + \u2211 i\u2208M0 Sgi(V ) (72)\n= 1 + 1 + \u2211\ni\u2208M(1)0\nSgi(V ) + \u2211\ni\u2208M(2)0\nSgi(V ) + \u2211\ni\u2208M(3)0\nSgi(V ) (73)\n\u2265 2 + \u2211\ni\u2208M(1)0\nSgi(A) + \u2211\ni\u2208M(2)0\nSgi(A \u2032) + \u2211 i\u2208M(3)0 Sgi(A \u2032\u2032) (74)\n= 2 + 1 + 1 + 1 = 5 (75)\nwhich is a contradiction."}, {"heading": "6.1.2 A Non-matroid Case", "text": "Lest one thinks it is only the matroids that give difficulty to SCMMs, consider the function f : 2V \u2192 R where again V = {a,b, c,d, e, f}.\nf(A) = min ( min(|A \u2229 {a,b, c,d}|, 3) + min(|A \u2229 {c,d, e, f}|, 3), 5 )\n(76)\nHere, there is an overlap between the two sets B1 = {a,b, c,d} and B2 = {c,d, e, f}. This is not a matroid rank since, for example, f(c) = 2. Also, minimal sets of maximum value are not all the same size, e.g., f({a, c,d}) = 5 while f({a,b, c, e}) = 5. Lemma 6.3. No SCMM can represent the DSF in Equation 76.\nProof. For clarity, we offer the proof as a series of numbered statement groups. 1. Assume that for all A \u2286 V , f(A) = g(A) = \u2211i\u2208M gi(A) for some index setM. 2. Assume \u2203i \u2208M that offers surplus both to B1 and B2. Let \u03b1i be the last linear point in \u03c6i. Then we\nmust have mi(B1) > \u03b1i and mi(B2) > \u03b1i, leading to\nmi(B1) +mi(B2) = mi(B1 \u2229B2) +mi(B14B2) (77) where B14B2 = (B1 \\B2)\u222a (B2 \\B1) is the symmetric difference between B1 and B2. Hence we must have at least one of mi(B1 \u2229 B2) > \u03b1i or mi(B14B2) > \u03b1i. Either case, however, would cause an unrecoverable surplus for sets (either B1 \u2229B2 or B14B2) neither of which should be in surplus.\n3. We may partition the index setM in toM0,M1,M2 whereM1 does not give a surplus to B2,M2 does not give a surplus to B1, andM0 gives surplus neither to B1 nor to B2.\n4. This leads to too much surplus, i.e.,\n1 = Sf (V ) = \u2211 i\u2208M Sgi(V ) = \u2211 i\u2208M0 Sgi(V )n+ \u2211 i\u2208M1 Sgi(V ) + \u2211 i\u2208M2 Sgi(V ) (78)\n\u2265 \u2211 i\u2208M1 Sgi(B1) + \u2211 i\u2208M2 Sgi(B2) = 2 (79)\na contradiction.\nExercise 6.1. It is left to the reader to show that the following function can not be represented as an SCMM:\nf(A) = min ( 4\u2211 i=1 min(|A \u2229Bi|, 3), 7 )\n(80)\nwhere V = {a,b, c,d, e, f, g,h} and where B1 = {a,b, c,d}, B2 = {c,d, e, f}, B3 = {e, f, g,h}, and B4 = {g,h, a,b}.\nIt is also possible to construct a truncated matroid rank function of the kind described in Equation (48) that cannot be represented by an SCMM.\nSummarizing the results from the above sections, we have the following.\nTheorem 6.4. The DSF family is strictly larger than that of SCMMs.\nA consequence of this theorem is that in order most generally allow interaction amongst a hierarchy of concepts, as intuitively argued in Section 4, it not sufficient to use solely SCMMs."}, {"heading": "6.1.3 More General Conditions on Two-Layer Functions", "text": "In this section, we revisit again the form of DSF in Equation (64) where we saw there is no corresponding SCMM. Let us slightly generalize Equation (64) in the following.\ng(A) = \u03c6(min(|A \u2229 {a, b, c}|, 2) + min(|A \u2229 {d, e, f}|, 2)) (81) where \u03c6 is normalized monotonically non-decreasing concave function. Lemma 6.2 does not require that for all \u03c6, the corresponding DSF has no SCMM representation. Indeed, for certain functions \u03c6 it is possible. While we do not, in this paper, give a complete characterization of those DSFs that can or cannot be represented by SCMMs, we do offer the following theorem.\nTheorem 6.5. The function g(A) in Equation 81 is an SCMM if and only if \u2212\u03c6(1) + 3.5\u03c6(2) \u2212 4\u03c6(3) + 1.5\u03c6(4) \u2265 0 and 2\u03c6(1) + \u03c6(2)\u2212 4\u03c6(3) + 2\u03c6(4) \u2265 0\nThe proof of the \u201cif\u201d part of this theorem follows by considering the following expression which is clearly an SCMM as long as all of the coefficient are non-negative. The \u201cif\u201d part of the proof is fairly easy \u2014 we may simple write g(A) as the form of SCMMs as follows:\ng(A) = [2\u03c6(1) + \u03c6(2)\u2212 4\u03c6(3) + 2\u03c6(4)] min(|A \u2229 {a, b, c, d, e, f}|, 1) (82) + [\u2212\u03c6(1) + 3.5\u03c6(2)\u2212 4\u03c6(3) + 1.5\u03c6(4)] min(|A \u2229 {a, b, c, d, e, f}|, 2) (83) + [\u2212\u03c6(2) + 2\u03c6(3)\u2212 \u03c6(4)] [min(|A \u2229 {a, b, c}|, 1) + min(|A \u2229 {d, e, f}|, 1)] (84) + [\u2212\u03c6(3) + \u03c6(4)] [min(|A \u2229 {a, b, c}|, 2) + min(|A \u2229 {d, e, f}|, 2)] (85) + [\u2212\u03c6(2) + 2\u03c6(3)\u2212 \u03c6(4)] [min((1, 1, 0, 0.5, 0.5, 0.5)T (A), 1) (86)\n+ min((0, 1, 1, 0.5, 0.5, 0.5)T (A), 1) + min((1, 0, 1, 0.5, 0.5, 0.5)T (A), 1)] (87)\n+ min((0.5, 0.5, 0.5, 1, 1, 0)T (A), 1) + min((0.5, 0.5, 0.5, 1, 0, 1)T (A), 1) (88)\n+ min((0.5, 0.5, 0.5, 1, 0, 1)T (A), 1)] (89)\nwhere (xa, xb, xc, xd, xe, xf )T is a modular function with elements xa, xb, xc, xd, xe, and xf . Hence, if all coefficients are non-negative, then g is an SCMM (in fact, g is a sum of weighted cardinality truncations, defined in Lemma 5.10). The non-negativity of the coefficients holds whenever the inequalities stated in the theorem are met. The \u201conly if\u201d part of the theorem is more involved and thus is given in Appendix A."}, {"heading": "6.2 The DSF Family Grows Strictly with the Number of Layers", "text": "It is clear that a k-layer DSF can easily express a k \u2212 1 layer DSF simply by using a linear function at the final unit. Hence, if we say that DSFk is the family of all deep submodular functions with k layers, we have that DSFk\u22121 \u2286 DSFk. It is also clear that DSF0 \u2282 DSF1 since DSF0 are modular functions while DSF1 are SCMMs. In the previous section, we demonstrated by example that DSF1 \u2282 DSF2.\nIn this section, we show that DSFs become strictly more capable as the allowable number of layers increases, meaning there are k-layer functions that cannot be represented with k \u2212 1 layers, and hence DSFk\u22121 \u2282 DSFk for any k. This result is similar to some of the recent results from the DNN literature where it is shown that in some cases, it would require exponentially many hidden units to implement a network with more layers [40]. In the DSF case, however, we show that in some cases, there is no way to represent certain k-layer DSFs with a k \u2212 1 layer function, which means that the class of DSFs is strictly increasing with the number of layers. This is different than standard neural networks where it is shown that even a shallow neural network is a universal approximator [61]. In order to do this in the DSF case, however, we allow the ground set correspondingly to grow in size with the number of layers.\nWe begin with a number of definitions and prerequisite lemmas.\nDefinition 6.6 ((A,B,C)-function). We say that polymatroid function f is an (A,B,C)-function if A,B,C \u2286 V are three non-empty disjoint subsets of V and where f satisfies the following:\nf(A \u222aB \u222a C) = f(A \u222aB) = f(B \u222a C) = f(C \u222aA) (90) = f(A) + f(B) = f(B) + f(C) = f(C) + f(A) (91)\nDefinition 6.7 (strong (A,B,C)-function). We say that f is a strong (A,B,C)-function if f is an (A,B,C)function and if f(A \u222aB \u222a C) > 0.\nLemma 6.8. If f is an (A,B,C)-function, then f(A) = f(B) = f(C). If f is a strong (A,B,C)-function, then f(A) = f(B) = f(C) > 0.\nProof. f(A) + f(B) = f(B) + f(C) = f(C) + f(A) implies f(A) = f(B) = f(C). If f is strong, we have f(A \u222aB \u222a C) > 0. Therefore, f(A) = 12f(A \u222aB \u222a C) > 0.\nA simple example of such a function is a cycle matroid rank function with A = {a}, B = {b}, and C = {c}, where {a,b, c} are the edges of a 3-cycle in the cycle matroids associated graph. Note that in any (A,B,C)-function, we have If (A;B) = If (B;C) = If (C;A) = 0. In a strongly (A,B,C)-function, we have If (A;B;C) > 0. Hence, these functions have no interaction between any two groups but there is a three-way interaction amongst the three groups. Like surplus being zero, (A,B,C)-function that are mixtures force properties amongst the components.\nLemma 6.9. If f = \u2211m i=1 fi is an (A,B,C)-function, then fi is an (A,B,C)-function for all i.\nProof. First, conditioning on the pair A,B, since \u2211m i=1 fi(A|B \u222aC) = f(A|B \u222aC) = 0 and fi(A|B \u222aC) \u2265 0 for each i, we have fi(A|B \u222aC) = 0 for each i. Doing the same for pair B,C and C,A, we have fi(A\u222aB) = fi(B \u222a C) = fi(C \u222aA) = fi(A \u222aB \u222a C).\nNext, since \u2211m i=1 fi(A)+fi(B)\u2212fi(A\u222aB) = f(A)+f(B)\u2212f(A\u222aB) = 0 and fi(A)+fi(B)\u2212fi(A\u222aB) \u2265 0 for all i, we have fi(A) + fi(B) = fi(A \u222a B) for all i. Doing the same for pairs B,C and C,A yields the result.\nDefinition 6.10. Given a function f : 2V \u2192 R, and a subset V \u2032 \u2286 V , define the restricted function fV \u2032 : 2 V \u2032 \u2192 R as fV \u2032(X) = f(X) for all X \u2286 V \u2032.\nA restricted function fV \u2032(X) has a restricted ground set, and by stating fV \u2032(X) we assume X \u2286 V \u2032.\nLemma 6.11. Let h be polymatroidal, \u03c6 be normalized monotone non-decreasing concave, and define h(X) = g(X)+m\u00b1(X), where g(X) = \u03c6(h(X)). If g is a strongly (A,B,C)-function, then hD(X) = \u03b3h(X)+m\u00b1(X) for D = A \u222aB, D = B \u222a C, and D = C \u222aA.\nProof. Since g is strongly (A,B,C), we have Ig(A;B) = 0, while g(A) = g(B) > 0, which by Lemma 5.32 means that \u03b1, the last linear point of \u03c6, must be no less than h(A,B). Hence, for any X \u2286 A \u222a B, h(X) = \u03b3h(X) +m\u00b1(X) for some \u03b3 > 0. The same holds true for B \u222a C and C \u222aA.\nGiven k \u2265 1 and a ground set V where |V | = 3k, we name each element v \u2208 V as va1,a2,...,ak where ai \u2208 {1, 2, 3} for i = 1, 2, . . . , k. Define Va1,a2,...,ak = {va1,a2,...,ak} and for 1 \u2264 j \u2264 k\u2212 1, define Va1,a2,...,aj = Va1,a2,...,aj ,1 \u222a Va1,a2,...,aj ,2 \u222a Va1,a2,...,aj ,3. For example, V = V1 \u222a V2 \u222a V3, V1 = V11 \u222a V12 \u222a V13, V2 = V21 \u222a V22 \u222a V23, V11 = V111 \u222a V112 \u222a V113, and so on.\nDefinition 6.12. We define F \u2032k as the set of set functions f : 2 V \u2192 R where |V | = 3k, f(V ) > 0 and f is a (Va1,a2,...,aj ,1, Va1,a2,...,aj ,2, Va1,a2,...,aj ,3)-function for all ai \u2208 {1, 2, 3}, 1 \u2264 i \u2264 j, and 0 \u2264 j \u2264 k \u2212 1. We also define Fk as the set of set functions f : 2V \u2192 R where |V | = 3k, and f is a strongly (Va1,a2,...,aj ,1, Va1,a2,...,aj ,2, Va1,a2,...,aj ,3)-function for all ai \u2208 {1, 2, 3}, 1 \u2264 i \u2264 j, for all 0 \u2264 j \u2264 k \u2212 1.\nFigure 5 shows three examples of cycle matroids whose ranks are in Fk for k = 1, 2, 3 thus demonstrating that Fk is non-empty. To show that there are DSFs who are members of Fk, consider the following example.\nExample 6.13. Define f\u0302k : 2Vk \u2192 R, where |Vk| = 3k. Define f\u03021(X) = 12 min(|X|, 2). For k \u2265 2, Vk is partitioned into three sets Vk1, Vk2, and Vk3 where |Vk1| = |Vk2| = |Vk3| = 3k\u22121. The level-k function is defined as f\u0302k(X) = 12 min( \u2211 i=1,2,3 f\u0302k\u22121(X \u2229 Vki), 2).\nHence, f\u0302k is like a [0, 1]-normalized laminar matroid rank function with the laminar family of sets Fk = {Vk, Vk1, Vk2, Vk3, Vk11, Vk12, Vk13, Vk21, . . .}. An immediate consequence is the following.\nLemma 6.14. f\u0302k \u2208 Fk and f\u0302k can be expressed as a k-layer DSF.\nWe also note that the families Fk and F \u2032k are the same.\nLemma 6.15. F \u2032k = Fk\nProof. Immediately, we have Fk \u2286 F \u2032k To show the other direction, assume there exists f \u2208 F \u2032k and v \u2208 V such that f(v) = 0 where v is labeled as va1,a2,...,ak . Then we have f(Va1,a2,...,ak\u22121) = 2 \u00d7 f(Va1,a2,...,ak\u22121,ak) = 0, f(Va1,a2,...,ak\u22122) = 2\u00d7 f(Va1,a2,...,ak\u22122,ak\u22121) = 0, and so on until finally we have f(V ) = 0 which contradicts with the definition of F \u2032k. Hence, for all f \u2208 F \u2032k and v \u2208 V , we have f(v) > 0 and by monotonicity f(A) > 0 for all A. Therefore, f \u2208 Fk and F \u2032k \u2286 Fk.\nLemma 6.16. Given f \u2208 Fk, suppose that f = \u2211m i=1 fi. If fi(V ) > 0, then fi \u2208 Fk for all i.\nProof. This is immediate when considering lemmas 6.9 and 6.15.\nLemma 6.17. Given f \u2208 Fk, we have \u03b3f \u2208 Fk for all \u03b3 > 0. If k \u2265 2, we have fVi \u2208 Fk\u22121, for i \u2208 {1, 2, 3}, where Vi is defined in Definition 6.12.\nProof. This is immediate from the definitions.\nLemma 6.18. For all f \u2208 Fk and \u03c6 be a normalized monotone non-decreasing concave function. If f = \u03c6(f \u2032), then f \u2032Vi \u2208 Fk\u22121, for i \u2208 {1, 2, 3}.\nProof. Using Lemma 6.11, we have fVi = \u03b3f \u2032Vi , where \u03b3 > 0 is a constant. Also we have fVi \u2208 Fk\u22121 according to second part of lemma 6.17. So f \u2032Vi \u2208 Fk\u22121 according to first part of lemma 6.17.\nFor any f \u2208 Fk, we have that f(v|V \\{v}) = 0 which follows since if v = va1,a2,...,ak\u22121,1, v\u2032 = va1,a2,...,ak\u22121,2, and v\u2032\u2032 = va1,a2,...,ak\u22121,3, 0 = f(v|v\u2032, v\u2032\u2032) \u2265 f(v|V \\{v}) \u2265 0. Hence, all members of Fk are totally normalized in this sense [31, 30].\nAs mentioned in Section 4, a DSF allows for the use of an arbitrary final modular function m\u00b1 at the top layer. If it is the case that a given f \u2208 Fk is represented as a DSF, since f is totally normalized and since the polymatroidal part must have non-negative gain, the final m\u00b1 must be non-positive as otherwise we would have f(v|V \\ {v}) > 0. Hence, in order to show that a given f \u2208 Fk can not be represented by a DSF with fewer than k layers, it is sufficient to show that a function of the form f +m+, where f \u2208 Fk and m+ is a non-negative modular function, can not be expressed as a k \u2212 1 layer DSF having m\u00b1 = 0. To this end, we introduce the following class:\nDefinition 6.19. We define the class of functions Gk = {f +m+|f \u2208 Fk,m+ \u2208M+} where M+ is the set of all non-negative normalized modular functions.\nThe addition of a modular function to an f \u2208 Fk does not change any surplus. Hence, for a g \u2208 Gk with g = f + m+ with f \u2208 Fk, we have that Ig(A;B) = If (A;B) for any disjoint sets A,B, and that Sg(A) = Sf (A) for any set A.\nThe properties of total normalization [31, 30] will be further useful in the below, so we define functional operators that totally normalize a given function. Define the functional operatorM : (2V \u2192 R)\u2192 (V \u2192 R) that maps from submodular functions to a modular function as follows, for all A \u2286 V :\n(Mf)(A) = \u2211 a\u2208A f(a|V \\ {a}). (92)\nHence, Mf is a modular function consisting of elements which are the smallest possible gain given by submodular f . We also define a total normalization functional operator T : (2V \u2192 R) \u2192 (2V \u2192 R) as follows:\n(T f)(A) = f(A)\u2212 (Mf)(A). (93) Then clearly T f is a polymatroid function that is totally normalized (i.e., (T f)(v|V \\ {v}) = 0), and we have the identity f = T f +Mf , meaning that any submodular function can be decomposed into a totally normalized polymatroid function plus a modular function [31, 30]. The decomposition is unique because if f = f \u2032 + m where f \u2032 is any function having f \u2032(v|V \\ {v}) = 0, then f(v|V \\ {v}) = m(v) so we must have that m =Mf .\nThe operator M is linear, M(f1 + f2) = Mf1 +Mf2, as is T . Also, in the present case, since f is presumed polymatroidal, the modular function is non-negative, i.e., (Mf)(v) \u2265 0 for all v.\nThe next lemma states that if f is representable as a sum, then each term must either be a member of Gk or must be purely a non-negative modular function.\nLemma 6.20. Given f \u2208 Gk, suppose that f = \u2211l i=1 fi. Then fi \u2208 Gk \u222aM+ for all i. Furthermore, for at least one i, we have fi \u2208 Gk. Proof. Consider Mf = M\u2211li=1 fi = \u2211li=1Mfi and T f = T \u2211li=1 fi = \u2211li=1 T fi. For any h \u2208 Fk and m \u2208 M+,M(h+m) = m, and hence T f = f \u2212Mf \u2208 Fk. Thus, by Lemma 6.16, we have either that T fi is identically zero or is otherwise an element of Fk. Hence, when considering that fi =Mfi + T fi, if T fi is zero,Mfi + T fi \u2208M+ and if notMfi + T fi \u2208 Gk. Furthermore, since f \u2208 Gk we can not have that for all i, fi \u2208M+. Lemma 6.21. Given an f \u2208 Gk, if f = \u03c6(f \u2032), where \u03c6 is normalized non-decreasing concave, and f \u2032 is polymatroidal, then f \u2032Vi \u2208 Gk\u22121, i \u2208 {1, 2, 3}. Proof. Since f \u2208 Gk, we have that we have If (Vi;Vj) = 0, for i, j \u2208 {1, 2, 3}, i 6= j, while g(Vi) = g(Vj) > 0. This, Lemma 5.32, means that \u03b1, the last linear point of \u03c6, must be no less than f \u2032(Vi, Vj). Hence, fVi = \u03b3f \u2032Vi for i \u2208 {1, 2, 3} and for some constant \u03b3 > 0.\nSince f = Mf + T f and f \u2208 Gk, T f \u2208 Fk and Mf \u2208 M+. Thus, (T f)Vi \u2208 Fk\u22121 by Lemma 6.17, and we also have that (Mf)Vi \u2208 M+. Hence, since fX = (Mf)X + (T f)X for any X \u2286 V , we have f \u2032Vi = 1 \u03b3 ((Mf)Vi + (T f)Vi) \u2208 Gk\u22121.\nTheorem 6.22. Any f \u2208 Gk can not be expressed via a (k \u2212 1)-layer DSF having m\u00b1 = 0. Proof. We prove this by induction.\nTo establish the base case, all f \u2208 G1 can not be expressed via a 0-layer DSF since a 0-layer DSF is modular while any f \u2208 G1 is not modular since there are sets that have strictly positive surplus. Hence, the induction step assumes that any f \u2208 Gk\u22121 can not be expressed via a (k \u2212 2)-layer DSF for k \u2265 2.\nNext, suppose we find a f \u2208 Gk where f can be expressed by a (k\u2212 1)-layer DSF. Hence, we can express f = \u03c6(f \u2032) where \u03c6(\u00b7) is concave and where f \u2032 = \u2211mi=1 fi. Since f is a (k \u2212 1)-layer DSF, then for all i, fi is a (k \u2212 2)-layer DSF.\nWe may w.l.o.g., assume that fi(V ) > 0 for all i (since if for any i we have fi(V ) = 0, then it contributes nothing to the function for any A \u2286 V by monotonicity and non-negativity). By Lemma 6.21, we have that f \u2032V1 , f \u2032 V2 , f \u2032V3 \u2208 Gk\u22121. For j \u2208 {1, 2, 3}, we have that f \u2032Vj = \u2211m i=1 fi,Vj , and by Lemma 6.20, for all i = 1, 2, . . . ,m and j \u2208 {1, 2, 3}, we have that fi,Vj \u2208 Gk\u22121 \u222aM+. Also, for each j \u2208 {1, 2, 3}, there is at least one i where fi,Vj \u2208 Gk\u22121. For these instances, by the induction step, fi,Vj can not be expressed in (k \u2212 2)-layer DSF. Since fi is more complex than fi,Vj , fi also can not be expressed using a (k \u2212 2)-layer DSF, which contradicts the above statement that fi is a (k \u2212 2)-layer DSF.\nHence, we can not find an f \u2208 Gk that can be expressed as a (k \u2212 1)-layer DSF.\nThe above results immediate imply our main theorem.\nTheorem 6.23. There are k-layer DSFs that cannot be expressed using k\u2032-layer DSFs for any k\u2032 < k.\nLetting DSFk be the family of k-layer DSFs, it is interesting to consider what happens with limk\u2192\u221eDSFk. To show the above result, we needed for the ground set to grow exponentially with k which means that for the flexibility of DSFs to grow, we need an ever increasing ground set. It remains an open question to determine if, when the ground set size is constant and fixed, if DSFk comprises a larger family, or if expressing certain DSFks with k \u2212 1 layers requires an exponential number of hidden units, analogous to [40]."}, {"heading": "6.3 The Family of Submodular Functions is Strictly Larger than DSFs", "text": "Our next result shows that, while DSFs are richer than SCMMs, and the DSF family grows with the number of layers, they still do not encompass all polymatroid functions. We show this by proving that the cycle matroid rank function on K4 is not achievable with DSFs. We adopt the idea of the backpropagation style proof in Lemma 5.18 and utilize the form of DSF given in Eqn. (11) where we strip off the DSF layer-by-layer until we reach a one-layer DSF that, as is shown, is unable to represent a cycle matroid rank over K4. In particular, we backpropagate a necessary lack of surplus, a required linearity, and also a required pairwise surplus, from the root down to the very first layer. This shows that, for up to size three sets, the DSF must be similar to a mixture of concave over modular, and which then is unable to maintain a pairwise surplus necessary for the cycle matroid rank function.\nThe reader is encouraged to review the notation in Equation (11). We start with a number of lemmas that culminate in Theorem 6.26.1.\nBy applying Lemma 5.20 and Theorem 5.24 recursively according to a DSF\u2019s DAG, there are some important and powerful implications for DSF with positive weights. Firstly, if we ever find an internal network node and corresponding set in surplus, it means some surplus is preserved all the way to the root. Correspondingly, any set A not in surplus by the network as a whole must not be in surplus at any internal node. This allows us to place constraints at one part of the network to cause consequences at distant points (i.e., many layers away) elsewhere in the network. For a DSF (or SCMM), once a node is in surplus, there is no way to recover anywhere else in the network (since there are no zero weights). We formalize this in the following:\nCorollary 6.23.1 (Preservation of Surplus). If S\u03c8u(A) > 0 for some internal node u in the DSF, then S\u03c8v (A) > 0 where v is a higher node (closer to the root r). In other words, if there is no surplus at the higher node v for some A, there can be no surplus at any lower internal node in a DSF. This is also true for grouped surplus (Definition 5.26).\nThis result immediately follows Theorem 5.24. This means that zero surplus at the root S\u03c8r(A) = 0 on a set A means all internal nodes must also have zero surplus on A. For an SCMM, it means that if one term is in surplus then the sum must also be in surplus. This is a crucial result used in Theorem 6.26.1.\nCorollary 6.23.2 (Modular on 3-Cycle). Let f : 2V \u2192 R be a DSF in the above form using the above notation, and assume f(A) = r(A) where r is a cycle matroid rank function over the edges of K4. Then for any v \u2208 V and any 3-cycle C = {a,b, c} having gv(a) = \u03c8v(1a) = 0, then Sgv ({a,b, c}) = 0 (i.e., gv is modular at the cycle C).\nProof. This follows immediately from Lemma 5.31 where the three cycle consists of edges {a,b, c} with A = {a}, B = {b}, C = {c}, and h = gv which must be polymatroidal in a DSF for any v \u2208 V.\nLemma 6.24 (Linear Part of Hidden Units). Let f : 2V \u2192 R be a DSF in the above form using the above notation, and assume f(A) = r(A) where r is a cycle matroid rank function over the edges of K4. We are given any v \u2208 V, any 3-cycle C = {a, b, c}, and any e\u2032 /\u2208 C having gv(e\u2032) > 0, gv(C) > 0, and Igv (e\u2032;C) = 0. Then any surplus Sgv (C) > 0 given to C is not due to any non-linearity in \u03c6v(\u00b7) and instead is caused by \u03d5v(\u00b7). Proof. Thus, since wuv \u2265 0 for all u \u2208 pa(v) \\ V , and the modular part of \u03d5v does not change pairwise surplus, we may apply Lemma 5.32 with g(X) = gv(X), h(X) = \u03d5v(1X), A = {e\u2032}, and B = C, which means the linear range of \u03c6v must include [0, \u03d5v(1C) + \u03d5v(1e\u2032)].\nLemma 6.25 (Decomposition of sets of three-cycles). Let f : 2V \u2192 R be a DSF in the above form using the above notation, and assume f(A) = r(A) where r is a cycle matroid rank function over the edges of K4. We are given any v \u2208 V, and a subset cid(v) \u2286 {1, 2, 3, 4} of indices of the four three-cycles (C1, C2, C3, and C4) of the matroid where |cid(v)| \u2265 2 and where the following is true:\n1. For i \u2208 cid(v), gv(Ci) > 0,\n2. for e \u2208 \u222ai\u2208cid(v)Ci, gv(e) > 0,\n3. for e /\u2208 \u222ai\u2208cid(v)Ci, gv(e) = 0,\n4. and for i \u2208 cid(v), 3-cycle Ci and e \u2208 Ci, we have Igv (e;Ci \\ {e}) = gv(e)\u2212 gv(e|Ci \\ {e}) = gv(e).\nThen we may for all X of size up to three write gv(X) as\ngv(X) = \u2211 u\u2208U wugu(X) (94)\nwith wu \u2265 0 and where for all u \u2208 U = pa(v) \\ V , there is a set of cycle indices cid(u) \u2286 cid(v) having:\n1. For i \u2208 cid(u), gu(Ci) > 0,\n2. for e \u2208 \u222ai\u2208cid(u)Ci, gu(e) > 0,\n3. for e /\u2208 \u222ai\u2208cid(u)Ci, gu(e) = 0,\n4. and for i \u2208 cid(u), 3-cycle Ci and e \u2208 Ci, , we have Igu(e;Ci \\ {e}) = gu(e)\u2212 gu(e|Ci \\ {e}) = gu(e).\nIf u is a first-layer hidden unit in the DSF then |cid(u)| = 1.\nProof of Lemma 6.25. For clarity, we offer the proof as a series of numbered statements.\n1. gv(\u00b7) has to be modular on any set up to size two, as otherwise an unrecoverable surplus will occur by Corollary 6.23.1. This means that \u03c6v has to be linear up to any valuation of any size two set (i.e., \u03d5r(1X) is still in the linear part of \u03c6r(\u00b7) for any X with |X| = 2).\n2. For the same reason, the nonlinear part of \u03c6v(\u00b7) must not start before the valuation \u03d5v(1X) for any X with |X| = 3 not in surplus (i.e., with Sgv (X) = 0, any matroid independent set of size three).\n3. Since |cid(v)| \u2265 2, for any i, j \u2208 cid(v), corresponding three-cycles Ci,Cj and any element e\u2032 \u2208 Cj where e\u2032 /\u2208 Ci, we have by Corollary 6.23.1 that\nIgv (e \u2032;Ci) = gv(e \u2032)\u2212 gv(e\u2032|Ci) = 0. (95)\nTherefore, since gv(e\u2032) > 0 and gv(Ci) > 0, by Lemma 6.24 the non-linear part of \u03c6v must not start before the valuation \u03d5v(1Ci) for any i \u2208 cid(v).\n4. For any i /\u2208 cid(v), \u2203e \u2208 Ci with gv(e) = 0. By Corollary 6.23.2, this means gv is modular at Ci.\n5. Considering the two previous statements, the non-linear part of \u03c6v(\u00b7) must not start before the valuation \u03d5v(1X) for any set with |X| = 3. Since such an X is still in the linear part of \u03c6v we may write gv(\u00b7) as:\ngv(X) = \u03b1v\u03d5v(1X) = \u2211\nu\u2208pa(v)\\V\n\u03b1vwuv\u03c8u(1X) + \u03b1v\u3008mv,1X\u3009 (96)\nfor any X up to size three, for some appropriate positive constant \u03b1v \u2208 R+.\n6. We are given that for any i \u2208 cid(v), 3-cycle Ci, and any e \u2208 Ci, Igv (e;Ci \\ e) = gv(e)\u2212 gv(e|Ci \\ {e}) = gv(e) > 0. (97)\nFrom the previous statements, however, the surplus of any such 3-cycle is not addressed by any nonlinearity in \u03c6v and must instead be handled by \u03d5v which, since gv(e) > 0, means that\n0 = gv(e|Ci \\ {e}) = \u2211\nu\u2208pa(v)\\V\n\u03b1vwuv\u03c8u(1e|1Ci\\{e}) +mv(e) (98)\nSince \u03c8u(1e|1Ci\\{e}) \u2265 0, for all u \u2208 pa(v)\\V , this requires 0 = \u03c8u(1e|1Ci\\{e}) = gu(e|Ci \\{e}). Since mv(e) \u2265 0. this also implies that mv(e) = 0,\u2200e \u2208 Ci. Since gv(e) = 0 for e /\u2208 \u222ai\u2208cid(v)Ci, we have that mv(e) = 0,\u2200e \u2208 V . Hence, the above establishes that for all u \u2208 pa(v) \\ V :\nIgu(e;Ci \\ {e}) = gu(e)\u2212 gu(e|Ci \\ {e}) = gu(e) (99)\nNext we need to consider whether gu(e) = 0 or not.\n7. If there is a u \u2208 pa(v) \\ V and corresponding i \u2208 cid(v), 3-cycle Ci having \u03c8u(1e) = 0 for some e \u2208 Ci, then Lemma 6.23.2 means that \u03c8u() must be modular at Ci. But then we must have \u03c8u(1e\u2032) = 0 for e\u2032 \u2208 Ci \\ e as otherwise, by modularity, we\u2019d get \u03c8u(1e\u2032 |1Ci\\{e\u2032}) = \u03c8u(1e\u2032) > 0 violating the requirement of Equation (98).\n8. Thus, this means that for every such u and every i \u2208 cid(v) and 3-cycle Ci, we have either \u2200e \u2208 Ci, \u03c8u(1e) = 0 or alternatively \u2200e \u2208 Ci, \u03c8u(1e) > 0, and in this latter case u must give Ci a positive surplus (to satisfy Equation (98)). Any u giving no surplus to any of the 3-cycles in cid(v) thus must have \u2200e \u2208 V, \u03c8u(1e) = 0 and so can be removed from the network without effect (which we assume in the below).\n9. Hence, for all u there exists a set cid(u) \u2286 cid(v) where for all i \u2208 cid(u), three-cycle Ci, and e \u2208 Ci, we have gu(e) > 0, gu(Ci) > 0. For e /\u2208 \u222ai\u2208cid(u)Ci, gu(e) = 0,\n10. If u is one of the first layer hidden unit nodes, then g(A) = \u03c6u(wu(A)) is a simple concave over modular function wu : V \u2192 R+. Suppose that for this u, we have |cid(u)| > 1, then taking i, j \u2208 cid(u), i 6= j, i, j \u2208 cid(v), corresponding three-cycles Ci,Cj and any element e\u2032 \u2208 Cj where e\u2032 /\u2208 Ci, we require by Corollary 6.23.1 that Igu(e\u2032;Ci) = gu(e\u2032) \u2212 gu(e\u2032|Ci) = 0. By Lemma 6.24, the non-linear part of \u03c6u must not start before the valuation of wu(Ci), meaning \u03c6u(wu(Ci)) is modular on the cycle, contradicting Equation (98). Hence, we must have |cid(u)| = 1 for first layer hidden nodes.\nTheorem 6.26 (DSFs are unable to represent the cycle matroid rank function on edges of K4).\nProof. Let f : 2V \u2192 R be a DSF in the above form. We may, w.l.o.g., assume all weights are strictly positive, as the summations below will be based on u \u2208 pa(v), so we assume that for all u \u2208 pa(v), wuv > 0.\nConsider, in Eqn. (11) , the top layer concave function along with the arbitrary modular function, and suppose that f(A) = \u03c8r(1A) +m\u00b1(A) = r(A) for all A where r : 2V \u2192 Z+ is a cycle matroid rank function on K4. Hence, gr(A) = \u03c8r(1A) = r(A)\u2212m\u00b1(A) which is an assuredly polymatroidal part of f(A).\nLet C1, C2, C3, and C4 be the four three-cycles of the matroid. Note that for all i, we have Sf (Ci) > 0 for all i, and f(Ci) > 0. Also, for all e \u2208 V , f(e) > 0. Hence, define cid(r) = {1, 2, 3, 4}. By Theorem 6.25, for any set X with |X| \u2264 3, we may write gr(X) as follows:\ngr(X) = \u2211 u\u2208U wugu(X) (100)\nwhere cid(u) \u2286 cid(r), and where for all u \u2208 U , i \u2208 cid(u), we have gu(Ci) > 0, gu(e) > 0 for e \u2208 \u222ai\u2208cid(u)Ci, and gu(e) = 0 for e /\u2208 \u222ai\u2208cid(u)Ci. Hence we may write gr(X) as:\ngr(X) = \u2211\nu\u2208U :|cid(u)|=1\nwugu(X) + \u2211\nu\u2208U :|cid(u)|>1\nwugu(X) (101)\nFor any u \u2208 U with cid(u) > 1, by Theorem 6.25, we may, for any set X of size |X| \u2264 3, write it as:\ngu(X) = \u2211 u\u2032\u2208U \u2032 wu\u2032gu\u2032(X) (102)\nwhere cid(u\u2032) \u2286 cid(u). Thus, we have\ngr(X) = \u2211\nu\u2208U :|cid(u)|=1\nwugu(X) (103)\n+ \u2211\nu\u2032\u2208U \u2032:|cid(u\u2032)|=1\nwu\u2032gu\u2032(X) + \u2211\nu\u2032\u2208U \u2032:|cid(u\u2032)|>1\nwu\u2032gu\u2032(X) (104)\nThis process may continue recursively, applying Theorem 6.25 each time, until we reach all units in the bottom layer of the DSF. We are guaranteed termination since the DSF is itself finite size. Also, since the bottom layer consists of single concave composed with modular functions, all have cid(\u00b7) = 1. Hence, for X with |X| \u2264 3, the entire DSF can be expressed as:\ngr(X) = \u2211\nu\u2208U(`) wugu(X) (105)\nwhere cid(u) = 1 and where we may partition U (`) in to four disjoint sets corresponding to the four cycles, where in each index set we have surplus only of one of the cycles. This means that it is not possible to achieve, for a cycle C and e \u2208 C,\nIgr(e;C \\ {e}) = gr(e)\u2212 gr(e|C \\ {e}) = gr(e) = 1 (106)\nsince some of the terms in the sum are non-zero meaning gr(e|C \\ {e}) > 0, thus contradicting that f(X) = r(X) for all X \u2286 V .\nThe above results therefore imply the following.\nCorollary 6.26.1 (SCCMs \u2282 DSFs \u2282 Submodular Functions). The family of SCMMs is smaller than that of DSFs, and the family of DSFs is smaller than the family of all submodular functions. That is, let Cn be the set of all submodular functions over ground set V of size n and let DSFk be the family of DSFs with k layers on V , and SCCM be the family of SCCMs on V with an arbitrary number of component functions. Then, for any k, SCCM \u2282 DSFk \u2282 Cn.\nWhile DSFs do not comprise all submodular functions, a consequence of Theorem 5.12 is that the input to a DSF can be any set of polymatroid functions. Let f be a DSF with k inputs and a ground set V = {1, 2, . . . , k}. Then we can consider the standard way to utilize a DSF, in the context of Theorem 5.12, as one where the ith input is a function gk(A) = 1k\u2208A which is modular. Theorem 5.12 allows for any polymatroid function to be used as input to a DSF, not just an indicator function, and hence the DSF can be used to add interactions between and perhaps improve these functions in some way. Hence, if several of the gk are cycle matroid rank functions, and if the DSF is learnt, the resulting family is expanded to include at least those matroid ranks used as input. It remains an open question to see if there is a small finite fixed set of input polymatroid functions that can be cascaded into a DSF in order to achieve all submodular functions.\nIt is also worth noting that in [163] it is shown that the entropy function f(A) = H(XA) when seen as a set function must satisfy inequalities that are not required for an arbitrary polymatroid function, thus implying that entropy also does not comprise all submodular function. An additional open problem, therefore, is to compare the family of DSFs to that of entropy functions."}, {"heading": "7 Applications in Machine Learning and Data Science", "text": "In this section, we describe a number of possible DSF applications in machine learning and data science."}, {"heading": "7.1 Learning DSFs", "text": "As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.\nA general outline of various learning settings is given in [76, 43] \u2014 here, we give only a very brief overview. To start, learning may involve several families of functions F , H, and T members of which are mappings from 2V to R. There is some true function f \u2208 F to be learnt based on information obtained via samples of the form (A, f(A)) for A \u2286 V . One wishes to produce an approximation f\u0303 \u2208 H to f that is good in some way. Learning submodular functions has been studied under a number of possible variants. For example, there is typically a probability distribution Pr over subsets of V (i.e., Pr(S = A) \u2265 1 and \u2211A\u2286V Pr(S = A) = 1 where S is a random variable). A set of samples D = {(Ai, f(Ai)}i is obtained via this distribution. The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]). The quality of learning could be judged over all 2n points or over some fraction, say 1\u2212\u03b2, of the points, for \u03b2 \u2208 [0, 1]. In general, there is no specificity on the particular set of points, or the particular kind of points, that should be learnt as long as at least a (probability distribution measured) fraction 1\u2212 \u03b2 of them are learnt. Learning itself happens with some probability 1\u2212 \u03b4. I.e., there is some probability \u03b4 that the learning will not succeed. While learning asks for a function in f\u0303 \u2208 H that is good, we might judge f\u0303 relative only to the best function f\u0302 \u2208 T (the touchstone class). For example, in agnostic learning [76], we acknowledge that it might be difficult to show that learning is good relative to all of F (say due to noise) but still feasible to show that learning is good relative to the best within T . Also, there are a variety of ways to judge goodness. In [11], goodness is judged multiplicatively, meaning for a set A \u2286 V we wish that f\u0303(A) \u2264 f(A) \u2264 g(n)f(A) for some function g(n), and this is typically a probabilistic condition (i.e., measured by distribution Pr, goodness, or f\u0303(A) \u2264 f(A) \u2264 g(n)f(A), should happen on a fraction at least 1\u2212 \u03b2 of the points). Alternatively, goodness may also be measured by an additive approximation error, say by a norm. I.e., defining errp(f, f\u0303) = \u2016f \u2212 f\u0303\u2016p = (EA\u223cPr[|f(A)\u2212 f\u0303(A)| p ])1/p, we may wish errp(f, f\u0303) < for p = 1 or p = 2. In the PAC (probably approximately correct) model, we probably (\u03b4 > 0) approximately ( > 0 or g(n) > 1) learn (\u03b2 = 0) with a sample or algorithmic complexity that depends on \u03b4 and g(n). In the PMAC (probably mostly approximately correct) model [11], we also \u201cmostly\u201d \u03b2 > 0 learn. In agnostic learning, F \u2287 H = T . Let Cn be the space of all submodular functions. In some cases F \u2287 Cn = H so we wish to learn the best submodular approximation to a non-submodular function. In other cases, F = Cn \u2286 T \u2286 H meaning we are allowed to deviate from submodularity as long as the error is small.\nIn the machine learning community, H may be a parametric family of submodular functions. For example, given a fixed set of component submodular functions, say {fi}`i=1 one may with to learn only the weights of a mixture {wi}i to produce f = \u2211 i wifi where wi \u2265 0 for all i to ensure submodularity is preserved. What is learnt is only the coefficients of the mixture, not the components, so the flexibility of the family is determined by the diverseness and quantity of components used. Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function. This is true both for image [150] and document [92] summarization tasks. There also has been some initial work on learnability bounds in [92]. Learning just the mixture coefficients of a mixture of submodular functions, while keeping the component functions themselves fixed, is only as flexible as the set of component functions allows, however. Given a small (or indiscriminately selected and hence potentially redundant) number of components, the family over which one can learn might be limited. As a result, one might need add a very large number of components before one obtains a sufficiently powerful family.\nAn alternative approach to learning a mixture that alleviates to some extent the above problem is to learn over a richer parametric family, and this is where DSFs hold promise. An approach to learning DSFs, therefore, is to learn within its parametric family, so H = DSFk for some finite k and where fw \u2208 DSF is parameterized by the vector w that determines the topology (e.g., number and width of layers) of the network, the numeric parameters (set of matrices) within that topology, and the set of concave functions {\u03c6u}u. As shown in the present paper, DSFs represent a strictly larger family than SCMMs. Therefore, even in the mixture case above where the components may also be learnt, there are DSFs that are unachievable by SCMMs. In addition, by Theorem 5.12, a DSF rather than a mixture can be applied to a fixed set of input submodular components (e.g., some of which might be simple indicators of the form gu(A) = 1u\u2208A and others could be cycle matroid rank functions in order to reduce any chance of the unachievability mentioned\nin Theorem 6.26). Even in cases where a DSF can be represented by an SCMM, DSFs may be a far more parsimonious representation of classes of submodular functions and hence a more efficient family over which to learn, analogous to results in DNNs showing the need for exponentially many hidden units for shallow networks to implement a network with more layers [40].\nSuppose f \u2208 Cn is a target submodular function, fw \u2208 DSFk is a parameterized k-layer DSF, D = {(Si, yi)}i is a training set consisting of subsets Si \u2286 V and valuations yi = f(Si) for the target function and that is drawn from distribution Pr. An empirical risk minimization (ERM), or regression, style of learning is obtained a standard way:\nmin w\u2208W J(w) = \u2211 i L(yi, fw(Si)) + \u2016w\u2016 (107)\nwhere L(\u00b7, \u00b7) is a loss function and \u2016w\u2016 is a norm on the parameters. Obvious candidates for the loss would be squared loss, or L1 loss, and the norm can also be chosen to prefer smaller values for w. Given the objective J(w) one may proceed using, for example, projected stochastic gradient descent, where at each step we project the weights w into W which corresponds to the non-negative orthant for parameters other than m\u00b1 to ensure submodularity is retained. Under this approach, and with an appropriate regularizer, it may be feasible to obtain generalization bounds in some form [135] as is often found in statistical machine learning settings. Note that, depending on the loss L used, this approach may be tolerant of noisy estimates of the function, where, say, yi = fw(Si) + and where is noise, somewhat analogous to how it is possible to optimize a noisy submodular function [59]. Alternatively, one could analyze it under an agnostic learning setting.\nUnder many distribution assumptions, such as when Pr is the uniform distribution over 2V , then as the training set gets larger, we approach the case where there are O(2|V |) distinct samples, and the goal is to learn the function at all points. For large ground sets, certain learning settings might become infeasible in practice due to the curse of dimensionality. As mentioned above, there are learning settings that ask only for a fraction 1\u2212 \u03b2 of the points to be learnt, but without a mechanism to specify which fraction.\nIn many practical learning situations, however, access to an oracle function h(A), or training data that utilizes h\u2019s evaluations, might not be available. Even if h available, such a learning setting might be overkill for certain applications, as we might not need a submodular function fw to be accurate at all points A \u2286 V . One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size. Such a set should be diverse and high quality. In this case, one does not need fw to be an accurate surrogate for f except on sets A for which f is large. More precisely, instead of trying to learn f everywhere, we seek only to learn the parameters w of a function so that if B \u2208 argmaxA\u2286V :|A|\u2264k fw(A), then h(B) \u2265 \u03b1h(A\u2217) for some \u03b1 \u2208 [0, 1] where A\u2217 \u2208 argmaxA\u2286V :|A|\u2264k h(A). This setting puts fewer constraints on what is needing to be learnt than the regression approach and hence should correspondingly be easier. This is somewhat analogous to discriminative learning where the entire distribution over input and output variables is not needed and instead only a conditional distribution (or a deterministic mapping from input to output) is required.\nThe max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs. Given an unknown but desired non-negative submodular function f \u2208 Cn, we are given a set of representative sets S = {S1, S2, . . .}, with Si \u2286 V and where each S \u2208 S is scored highly by f(\u00b7). Unlike the regression approach, we do not need the actual evaluations f(Si). It might be, for example, that the sets are selected summaries chosen by a human annotator from a larger set. A matroid analogy is to learn a matroid using a set of independent sets of a particular size, say `. If M \u2032 = (V, I \u2032) is a matroid of rank `\u2032 > `, then M = (V, I) is also a matroid where I = {I \u2208 I \u2032 : |I| \u2264 `}.\nIn max-margin approach, we learn the parameters w of fw in an attempt to make, for all S \u2208 S, fw(S) high, while for A \u2208 2V , fw(A) is lower by a given loss. More precisely, we ask that for S \u2208 S and A \u2208 2V , fw(S) \u2265 fw(A) + `S(A). The loss is chosen so that `S(S) = 0, so that `S(A) is very small whenever A is close to S (e.g., if A is also a good summary), and so that `S(A) is large when A is considered much worse (e.g. if A is a poor summary). Achieving the above is done by maximizing the loss-dependent margin, and reduces to finding parameters so that fw(S) \u2265 maxA\u22082V [fw(A) + `S(A)] is satisfied for S \u2208 S. The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is\nNP-hard. With regularization, the optimization becomes:\nmin w\u2208W \u2211 S\u2208S L ( max A\u22082V [fw(A) + `S(A)]\u2212 fw(S) ) + \u03bb 2 ||w||22. (108)\nwhere L is a classification loss function such as the logistic (L(x) = log(1 + exp(\u2212x))) or hinge (L(x) = max(0, x)) loss. If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure. In general, however, there are several complications.\nFirstly, the LAI problem maxA\u22082V [fw(A) + `S(A)] may be hard. Given a submodular function for the loss, as was done in [92], then the greedy algorithm offers the standard 1 \u2212 1/e approximation guarantee for LAI. On the other hand, a submodular function is not always natural for the loss. Recall above that `S(A) should be large when A is considered a poor set relative to S (e.g. if A is a poor summary). If it is the case that one may get an assessment of A, say via a surrogate f\u0303 of the ground truth function f , then one may use `S(A) = \u03ba\u2212 f\u0303(A) but this, to the extent that f\u0303 needs to represent f , approaches the labeling needs of the ERM/regression approach above. If f\u0303 is submodular, then \u03ba \u2212 f\u0303 is supermodular, and in this case solving maxA\u22082V \\S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.\nSecondly, when fw is not linear in w, the above problem is not convex. Given the enormous success of deep neural networks in addressing non-convex optimization problems, however, this should not be daunting. Indeed, given an estimation to A\u0303 \u2208 argmaxA\u22082V [fw(A) + `S(A)], we can easily obtain an approximate subgradient of weights dw \u2208 \u2202w(fw(A\u0303)\u2212 fw(S) + \u03bb/2\u2016w\u201622) to be used in a projected stochastic subgradient descent procedure. For a DSF, this subgradient can be easily computed using backpropagation, similar to the approach of [121]. Like in the mixtures case, we must use projected descent to ensure w \u2208 W and submodularity is preserved. Recall, however that the weights corresponding to m\u00b1(A) may be left negative if they so choose. Preliminary experiments in learning DSFs in this fashion were reported in [37] and show encouraging results.\nAs an additional benefit, many of the concave functions mentioned in Section 3.1 are parameterized themselves, and these parameters may also be the target of stochastic gradient based learning. In such case, not only the weights but also the concave functions of a DSF may be learnt.\nGiven the ongoing research on the non-convex learning of DNNs, which have achieved remarkable results on a plethora of machine learning tasks [87, 54], and given the similarity between DSFs and DNNs, we may leverage the same DNN learning techniques to learn DSFs. This includes stochastic gradient descent, convolutional linear maps, momentum, dropout, batch normalization, unsupervised pre-training, learning rate scheduling such as AdaGrad/Adam, convolutional matrix patterns, mini-batching, and so on. In some cases these methods might need to be modified (e.g., stochastic projected gradient descent to ensure the function remains submodular). Moreover, the suitability of fast GPU computing to the matrix-matrix multiplications necessary to evaluate DSFs should also be a benefit. Lastly, the many toolkits that support DNN training (such as Tensorflow, Theano, Torch, Caffe, CNTK, and so on), and that include automatic symbolic differentiation and semi-differentiation (for non-differentiable functions) for backpropagation-style parameter learning can easily be used to train DSF. All of these techniques and software may be leveraged to DSF\u2019s benefit, and is true both for the regression and max-margin setting."}, {"heading": "7.1.1 Training and Testing on Different Ground Sets, and Multimodal Submodularity", "text": "In the training process in machine learning, one trains with a training set and then evaluates or tests on a distinct set having no overlap with the training set. When training submodular functions, this means that the training set might consist of multiple ground sets, and the test set might consist of ground sets that were not seen during training. A data set might consist of D = {(Vi, Si, yi)}i where Vi is a ground set, Si \u2286 Vi and, when available, yi = fi(Si) is an evaluation of Si by a ground-set-specific submodular function fi. Hence, there may be no instance where two ground sets are the same, so Vi 6= Vj for i 6= j, nor might there be ground set commonality between training and test data sets. The reason this occurs can be explained using a document summarization example [92]. A training set consists of pairs, each of which is pile of documents (comprised of a set of sentences) and a subset of those sentences corresponding to a summary.\nMultiple training samples consists of different piles of documents and their corresponding summaries, and then a test set consists of a different pile of documents and summaries thereof. In this section, we discuss how to addresses this problem for DSFs via a strategy that generalizes [92, 150].\nLet V be a training set where each v \u2208 V is a data object. Any particular element v \u2208 V may be represented by a vector of non-negative weights (w(0)1 (v), w (0) 2 (v), . . . , w (0) |U |). Each object v \u2208 V is hence embedded in non-negative |U |-dimensional space corresponding to low-level features U for the object. For example, if v is a sentence, w(0)u (v) might counts the number of times an n-gram u appears in sentence v. Alternatively, w(0)u (v) might be automatically obtained via representation learning in a DNN-based autoencoder, or there can be a mix of features obtained via representation learning and hand-crafting, using any of the feature-engineering methods discussed in Section 3.1. For each feature, we can define a modular function mu(A) = \u2211 a\u2208A w (0) u (a) that measures feature u\u2019s weight for any set A \u2286 V . The entire training set, therefore can be seen a matrix w(0) to be used as the first layer in DSF (e.g., w(0) in Figure 6 left (red)) that is fixed during the training of subsequent layers (Figure 6 left (green)). As long as w(0) is non-negative, submodularity is preserved and if w(0) is constant, it allows all later layers (i.e., w(2), w(3), . . . ) to be learnt generically over any heterogeneous set of objects that can be represented in the same feature space, including multimodal data objects (e.g., consisting of images, videos, and text sentences). Any training process remains ignorant that this is happening since it sees the data only post feature representation. In fact, one can view this, in light of Theorem 5.12, as a fixed layer consisting of an SCMM that embeds data objects into feature space corresponding to the components of the SCMM.\nOnce training has occurred, and if there is an analogous process to transform distinct (and possibly different types) of test data into the same feature space, it is possible to use the learnt DSF even for a different ground set. In Figure 6 right (red), we have a different transformation w\u2032(0) into the same feature space V (1) which can use the DSF (green) learnt during training. This process analogous to the \u201cshells\u201d of [92]. In that case, mixtures were learnt over fixed components, some of which were graph based (and hence required O(n2) calculation for element-pair similarity scores). Via featurization in the first layer of a DSF, however, we may learn a DSF over a training set, preserving submodularity, avoid any O(n2) cost, and test on any new data represented in the same feature space. Alternatively, one could combine the shells approach and w\u2032(0) into a vector of polymatroid functions and then apply Theorem 5.12."}, {"heading": "7.2 Deep Supermodular Functions and Deep Differences", "text": "All of the results in this paper assume that the hidden units in a DSF are concave. If we replace these concave functions in Equation (11) then we get a class we could call Deep Supermodular Functions (DSUFs). The results in this paper, hence, generalize to show that DSUFs correspond to a larger class than just sums of convex functions composed with non-negative modular functions.\nIn [110, 64] it was shown that any set function h : 2V \u2192 R can be represented as a difference between two submodular functions. If we take f1, f2 \u2208 DSF then the class of functions DDSF = {h : h = f1 \u2212 f2, f1, f2 \u2208 DSF} can be seen as a class of deep differences of submodular functions. Considering the class DSSUF = {h : h = f + g, f \u2208 DSF, g \u2208 DSUF} can be seen as a class of deep submodular plus supermodular functions. Given that DSFs do not comprise all submodular functions, it is unlikely that DSSUFs comprise all set functions. However, these can be useful classes of functions to learn over using, say, the deep learning methods mentioned in Sections 7.1. A key advantage of learning over this family is that the framework never looses the decomposition into two submodular functions or a submodular and supermodular function. For example, after learning, we can utilize submodular level-set constrained submodular optimization of the kind developed in [66] for optimization. Learning under such a decomposition, moreover, might reveal substitutive (via f) and complementary (via g) properties of the data.\nIt may also be useful to define a class of deep \u201ccooperative-competitive\u201d energy functions for use in a probabilistic model. For example, one can define probability distributions p over binary vectors with p(x) = 1 Z exp(fw1(x) \u2212 fw2(x)) where fw1 and fw2 are both deep submodular, or p(x) = 1Z exp(fw1(x) + gw2(x)) where fw1 is deep submodular and gw2 is deep supermodular. If fw1 and gw2 have decomposition properties with respect to a graph, then these could be called deep cooperative-competitive graphical models."}, {"heading": "7.3 Deep Multivariate Submodular Functions", "text": "Submodular functions have been generalized in a variety of ways to domains other than just subsets of a finite set V (i.e., binary vectors). In Section 5.2, we discussed the negativity of the off-diagonal Hessian as a way of defining submodular functions on lattices. Other ways to generalize submodularity considers discrete generalizations of properties such as midpoint convexity over integer lattices [109].\nIn this section, we consider certain submodular generalizations to multi-argument functions. For example, a set function f(A,B) with two arguments A \u2286 V and B \u2286 V is a biset function. If the domain is of the form 22V , {(A,B) : A \u2286 V, B \u2286 V }, we may define the class of functions known as simple bisubmodular:\nDefinition 7.1 (Simple Bisubmodularity [139]). f : 22V \u2192 R is simple bisubmodular iff for each (A,B) \u2208 22V , (A\u2032, B\u2032) \u2208 22V with A \u2286 A\u2032, B \u2286 B\u2032 we have for s /\u2208 A\u2032 and s /\u2208 B\u2032:\nf(A+ s,B)\u2212 f(A,B) \u2265 f(A\u2032 + s,B\u2032)\u2212 f(A\u2032, B\u2032), f(A,B + s)\u2212 f(A,B) \u2265 f(A\u2032, B\u2032 + s)\u2212 f(A\u2032, B\u2032).\nAn equivalent way to define simple bisubmodularity is as follows.\nProposition 7.2. The function f : 22V \u2192 R is simple bisubmodular whenever \u2200(A,B), (A\u2032, B\u2032) \u2208 22V ,\nf(A,B) + f(A\u2032, B\u2032) \u2265 f(A \u222aA\u2032, B \u222aB\u2032) + f(A \u2229A\u2032, B \u2229B\u2032) (109)\nIf the domain is of the form 3V , {(A,B) : A \u2286 V, B \u2286 V, A \u2229 B = \u2205}, then we can define directed bisubmodularity as follows:\nDefinition 7.3 (Directed Bisubmodularity [125]). Biset function f : 3V \u2192 R is directed bisubmodular whenever\nf(A,B) + f(A\u2032, B\u2032) \u2265 f(A \u2229A\u2032, B \u2229B\u2032) + f((A \u222aA\u2032) \\ (B \u222aB\u2032), (B \u222aB\u2032) \\ (A \u222aA\u2032)). (110)\nDirected bisubodularity functions have been generalized to what is known as k-submodular functions in [80, 62]. More recently, simple bisubmodularity [139] has been generalized to multivariate submodular functions [134]. A multivariate submodular (or what we will call a k-multi-submodular) function f : (2V )k \u2192 R is defined as a function such that for all (X1, X2, . . . , Xk), (Y1, Y2, . . . , Yk) \u2208 (2V )k, we have that:\nf(X1, X2, . . . , Xk) + f(Y1, Y2, . . . , Yk) \u2265 f(X1 \u222a Y1, X2 \u222aX2, . . . , Xk \u222a Yk) + f(X1 \u2229 Y1, X2 \u2229X2, . . . , Xk \u2229 Yk) (111)\nThese are not the same as k-submodular functions [62] but for k = 1 we obtain standard submodular functions and for k = 2 we obtain simple bisubmodular functions.\nA DSF with k\u2032 > k layers can be used to instantiate a k-multi-submodular function. Consider a layeredDSF with k\u2032 layers corresponding to sets V (0), V (1), . . . , V (k\n\u2032). Choose a size k subset of these layers, say \u03c31, \u03c32, . . . , \u03c3k where \u03c3j \u2208 [0, k\u2032 \u2212 1] for all j, \u03c31 = 0, and all of the \u03c3j \u2019s are distinct, w.l.o.g., 0 = \u03c31 < \u03c32 < \u00b7 \u00b7 \u00b7 < \u03c3k \u2264 k\u2032 \u2212 1. Given an f \u2208 DSFk\u2032 , we ordinarily obtain a valuation f(A) using a subset A \u2286 V (0) of the ground set. Now, consider f : (2V )k \u2192 R where A1 \u2286 V (\u03c31), A2 \u2286 V (\u03c32), . . . , Ak \u2286 V (\u03c3k) and the value of f(A1, A2, . . . , Ak) is obtained as:\n\u03c6vk\u2032 ( \u2211 vk\u2032\u22121\u2208V\u0304 (k\u2032\u22121) w (k\u2032) vk\u2032 (vk \u2032\u22121)\u03c6vk\u2032\u22121 ( . . . \u2211 v1\u2208V\u0304 (1) w (2) v2 (v 1)\u03c6v1 ( \u2211 v0\u2208V\u0304 (0) w (1) v1 (v 0)\u03c6v0 ( \u2211 a\u2208V\u0304 (0) w (0) v1 (a) )))) (112)\n+m (1) \u00b1 (A1) +m (2) \u00b1 (A2) + \u00b7 \u00b7 \u00b7+m(k)\u00b1 (Ak) (113)\nwhere V\u0304 (i) = V (i) \u2229 A\u03c3\u22121i whenever \u2203j \u2208 [0, k \u2032 \u2212 1] : i = \u03c3j and otherwise V\u0304 (i) = V (i), and where m(j)\u00b1 : V (\u03c3j) \u2192 R, for each j, is an arbitrary modular function. In other words, Aj acts as a set of binary triggers to activate a set of units at layer j in the DSF. If we hold all but layer j fixed, then Aj can be seen as the set of units to provide the values for the vector bAj in Corollary 5.12.1 and as a result, we get as a result that the function is submodular in Aj . k-multi-submodularity then follows from a generalization of Proposition 7.2 to k-multi-submodularity.\nDeep k-multi-submodular functions should be useful in a number of applications, for example representing information jointly in a set of features and data items (and could be useful for simultaneous feature/data subset selection)."}, {"heading": "7.4 Simultaneously Learning Hash and Submodular Functions", "text": "One of the difficulties in training DSFs is obtaining a sufficient amount of training data. It would be useful therefore to have an strategy to easily and cheaply obtain as much training data as desired. In the spirit of the empirical success of DNNs, this section suggests one strategy for doing this.\nThe goal is to learn a map from a vector x \u2208 Rd to a b-bit vector via a function h\u03b8 : Rd \u2192 {0, 1}b, anywhere h\u03b8 is parameterized by \u03b8. The reason for doing this is to take data objects (e.g., images, documents, music files, etc.) that are represented in the input space Rd and map them to binary space {\u22121, 1}b where b < d and, moreover, since the space is binary, operations such nearest neighbor search are faster. There are existing approaches that can learn this mapping automatically, sometimes using neural networks (e.g., [56]). Often, h\u03b8 : Rd \u2192 {\u22121, 1}b rather than h\u03b8 : Rd \u2192 {0, 1}b, but this should not be of any consequence.\nThis section describes a strategy for learning hash functions that utilizes DSFs, the Lov\u00e1sz extension, and the submodular Hamming metric [50]. Let f : 2V \u2192 R be a submodular function and let f\u0306 be its Lov\u00e1sz extension. Also, let df (A,B) = f(A4B) be the submodular Hamming metric between A and B parameterized by submodular function f . We are given a large (and possibly unlabeled) data set D = {xi}i\u2208D and a corresponding distance function between data pairs (d(xi, xj) is the distance between item xi \u2208 Rd and xj \u2208 Rd). The goal is to produce a mapping h\u03b8 : Rd \u2192 {0, 1}b so that distances in the ambient space d(xi, xj) are preserved in the binary space. One approach adjusts h\u03b8 to ensure that d(xi, xj) =\u2211b `=1 1h\u03b8(xi)(`)6=h\u03b8(xj)(`). That is, we adjust h\u03b8(xi) so that the Hamming distance preserves the distances in the ambient space. In general, this problem is made more difficult by the rigidity of the Hamming distance. In order to relax this constraint, we can use a submodular Hamming metric parameterized by a DSF fw (which itself is parameterized by w). Hence, the hashing problem can be seen as finding \u03b8 and w so that the following is true as much as possible.\nd(xi, xj) = dfw(h\u03b8(xi), h\u03b8(xj)) (114)\nThe function h\u03b8 maps to binary vectors, and dfw is a function on two sets. This makes it difficult to pass derivatives through these functions in a back-propagation style learning algorithm. To address this issue, we can further relax this problem in the following way:\n\u2022 Given A,B \u2286 V , the Hamming distance is |A4B| and we can represent this as (1A \u2297 (1V \u2212 1B) + 1B \u2297 (1V \u2212 1A))(V ) where \u2297 : Rn \u00d7 Rn \u2192 Rn is the vector element multiplication operator (i.e.,\n[x\u2297 y](j) = x(j)y(j)). In other words, we define a vector zA4B \u2208 {0, 1}V with\nzA4B = 1A \u2297 (1V \u2212 1B) + 1B \u2297 (1V \u2212 1A) (115) = 1A + 1B \u2212 21A \u2297 1B (116)\nand |A4B| = zA4B(V ) = \u2211 i\u2208V zA4B(i). Hence, the submodular hamming metric is f(A4B) = f\u0306(zA4B), which holds since the Lov\u00e1sz extension is tight at the vertices of the hypercube.\n\u2022 For two arbitrary vectors z1, z2 \u2208 [0, 1]V , we can define a relaxed form of metric as follows: d(z1, z2) = f\u0306(z1 + z2 \u2212 2z1 \u2297 z2), and for a DSF, this can be expressed as df\u0306w(z1, z2) = f\u0306w(z1 + z2 \u2212 2z1 \u2297 z2).\n\u2022 Let us suppose that h\u0303\u03b8 : Rd \u2192 [0, 1]b is a mapping from real vectors to vectors in the hypercube (e.g., h\u0303\u03b8 might be expressed with a deep model with a final layer of b sigmoid units at the output to ensure that each output is between zero and one). Then we can construct a distortion between xi and xj via\ndw,\u03b8(xi, xj) , df\u0306w(h\u0303\u03b8(xi), h\u0303\u03b8(xj)) = f\u0306w(h\u0303\u03b8(xi) + h\u0303\u03b8(xj)\u2212 2h\u0303\u03b8(xi)\u2297 h\u0303\u03b8(xj)) (117)\nHence, dw,\u03b8 is a parametric family of distortion functions that uses two maps, one via the DNN h\u0303\u03b8 and another via the DSF fw using the Lovasz extension f\u0306w.\n\u2022 Assuming the original unlabeled data set D is large, and the distance function in the ambient space is accurate, it may be possible to learn both w and \u03b8 by forming an objective function to minimize:\nJ(w, \u03b8) = \u2211 i,j\u2208D \u2016d(xi, xj)\u2212 dw,\u03b8(xi, xj)\u2016. (118)\nLearning (minw,\u03b8 J(w, \u03b8)) can utilize stochastic gradient steps and the entire arsenal of DNN training methods.\nThe approach learns both the mapping function h\u0303\u03b8 and the submodular function fw simultaneously in a way that preserves the original distances. It may therefore be that h\u0303\u03b8 can be used as a feature transformation (i.e. a way to map data objects x into feature space via h\u0303\u03b8), and at the same time we obtain a submodular function fw over those features that, perhaps, can useful for summarization, all without needing labeled training data as in Section 7.1."}, {"heading": "8 Conclusions and Future Work", "text": "In this paper, we have provided a full characterization of our newly-proposed class of submodular functions, DSFs. We have introduced the antitone gradient as a way of establishing subclasses of submodular functions. We have shown that DSFs constitute a strictly larger family than the family of submodular functions obtained by additively combining concave composed with modular functions (SCMMs). We have also shown that DSFs do not comprise all submodular functions. This was all done in the special context of matroid rank functions, and also in a more general context.\nAs mentioned at various points within the paper, there are several interesting open problems associated with DSFs. An immediate task is to further develop practical strategies for successfully empirically learning DSFs, as was initiated in [36]. A second task is to establish generalization bounds for learning DSFs in an ERM framework. A third task asks if there is a finite set of \u201cboot\u201d submodular functions that, when cascaded into a DSF as in Theorem 5.12, lead to a family that comprises all polymatroid functions. And lastly, it remains to compare the DSF family with the family of all entropy functions [163]."}, {"heading": "9 Acknowledgments", "text": "Thanks to Brian Dolhansky for helping with building an initial implementation of learning DSFs that was used in [36]. Thanks also to Reza Eghbali and Kai Wei for useful discussions, and to Jan Vondrak for\nsuggesting the use of surplus and deficit as an analysis strategy. This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, a Facebook, and an Intel research award. Thanks also to the Simons Institute for the Theory of Computing, Foundations of Machine Learning Program. This work was supported in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA."}, {"heading": "A More General Conditions on Two-Layer Functions: Proofs", "text": "Proof of Theorem 6.5. We begin with the \u201conly if\u201d part. In the proof, we always assume the ground set V = {a, b, c, d, e, f}. Definition A.1. Consider the bijection p : V \u2192 V . Let Ap = {p(v)|v \u2208 A}. Notationally, we may write a given p as (v1, v2, . . . , vk) \u2192 (u1, u2, . . . , uk) where ui, vi \u2208 V with ui = p(vi). Let PA be the set of all one-to-one maps that are an identity for v \u2208 V \\ A, that is p(v) = v for all v \u2208 V \\ A. Corresponding to Theorem 6.5, in the below, assume V = {a, b, c, d, e, f}. We next define a number of operators that allow us to study the partial permutation symmetry of a set function.\nDefinition A.2. For any submodular function h, let: \u2022 EB be an operator such that EBh(A) = 1|PB | ( \u2211 p\u2208PB h(Ap));\n\u2022 E\u2032 be an operator such that E\u2032h(A) = 12 [h(A) + h(A(a,b,c,d,e,f)\u2192(d,e,f,a,b,c)];\n\u2022 and E be an operator such that Eh(A) = E\u2032E{d,e,f}E{a,b,c}h(A). Immediately, we have the following lemma.\nLemma A.3. Eg(A) = g(A) for all A \u2286 V . Also, E is a linear operation, that is E(h1 +h2) = Eh1 +Eh2. Lastly, if h is an SCMM, Eh is also an SCMM.\nLemma A.4. For any A,B \u2286 V , if [|A\u2229{a, b, c}| = |B \u2229{a, b, c}| AND |A\u2229{d, e, f}| = |B \u2229{d, e, f}|] OR [|A \u2229 {a, b, c}| = |B \u2229 {d, e, f}| AND |A \u2229 {d, e, f}| = |B \u2229 {a, b, c}|], then Eh(A) = Eh(B).\nThis means that Eh(A) is fully determined by the unordered pair {|A \u2229 {a, b, c}|, |A \u2229 {d, e, f}|}. Definition A.5. For any h : 2V \u2192 R, define Eh(n1, n2) = Eh(A), where n1 = |A \u2229 {a, b, c}|, n2 = |A \u2229 {d, e, f}|, and 0 \u2264 n2 \u2264 n1 \u2264 3.\nSince this section shows the \u201conly if\u201d part of Theorem 6.5, we have g(A) is an SCMM, thus by Lemma 5.9, g(A) = \u2211 i min(mi(A), \u03b2i) + m\u00b1(A), where mi \u2265 0 is non-negative modular and \u03b2i > 0. Immediately, we have\nEg(A) = \u2211 i Emin(mi(A), \u03b2i) + Em\u00b1(A) (119)\ng(A) = \u2211 i Egi(A) + Em\u00b1(A) (120)\naccording to lemma A.3, where gi(A) = min(mi(A), \u03b2i). Moreover, we assume mi(V ) > \u03b2i > 0 for each i; otherwise gi is modular and can be merged into the final modular term. Furthermore, we assume that mi(v) \u2264 \u03b2i for all v \u2208 V and all i. If mi(v) > \u03b2i, it means that min(mi(A), \u03b2i) = \u03b2i whenever v is selected in A. In such case, we can let mi(v) = \u03b2i which have the same function value for all A. Therefore we have\nLemma A.6. gi(v|V \\ {v}) < gi(v) = mi(v) for all i and v s.t. mi(v) > 0. In other words, Igi(v;V \\ v) > 0 for all i with gi(v) > 0.\nProof. This follows since mi(V ) passes the linear part of gi but mi(v) does not.\nLemma A.7. gi(a|{b, c}) = gi(a|{b, c, d, e, f}) for all i.\nProof. We have that 0 \u2264 Ig(a;A) \u2264 Ig(a;B) for all A \u2286 B. Hence Ig(a; {b, c, d, e, f}) = 0 implies Ig(a; {b, c}) = 0. Hence, for all i, Igi(a; {b, c, d, e, f}) = Igi(a; {b, c}) = 0, implying gi(a|{b, c}) = gi(a|{b, c, d, e, f}) for all i.\nDefinition A.8. We define the following functions:\n\u2022 f0(A) = |A|;\n\u2022 f1(A) = min(|A \u2229 {a, b, c, d, e, f}|, 1);\n\u2022 f2(A) = min(|A \u2229 {a, b, c, d, e, f}|, 2);\n\u2022 f3(A) = min(|A \u2229 {a, b, c}|, 1) + min(|A \u2229 {d, e, f}|, 1);\n\u2022 f4(A) = min(|A \u2229 {a, b, c}|, 2) + min(|A \u2229 {d, e, f}|, 2);\n\u2022 and f5(A) = Emin((1, 1, 0, 0.5, 0.5, 0.5)T (A), 1), where (xa, xb, xc, xd, xe, xf )T is a modular function with elements xa, xb, xc, xd, xe, xf .\nImmediately, we notice that Efi = fi for all i.\nLemma A.9. For a normalized monotonically non-decreasing submodular h, if h({d, e, f}) = 0, then Eh is a conical combination of f0, f3, f4\nProof. Let x = 13 (h(a) + h(b) + h(c)) and y = 1 3 (h({a, b}) + h({b, c}) + h({a, c})) and z = h({a, b, c}). Then Eh can actually be written as 12 [(z \u2212 y)f0 + (2x\u2212 y)f3 + (2y \u2212 z \u2212 x)f4] where z \u2212 y, 2x\u2212 y, 2y \u2212 z \u2212 x \u2265 0 according to submodularity.\nLemma A.10. We say a function is fully curved if f(v|V \\ v) for some v. For i such that gi is not fully curved, Egi is a conical combination of f0, f3, f4.\nProof. Without lose of generality, we assume gi(a|{b, c, d, e, f}) > 0. Immediately we have mi(a) > 0 and mi({b, c, d, e, f}) < \u03b2i. According to lemma A.6 and lemma A.7, we have I(a; {b, c}) = gi(a)\u2212 gi(a|{b, c}) = gi(a) \u2212 gi(a|V \\ {a}) > 0. Thus mi({a, b, c}) \u2265 \u03b2i. Therefore 0 = gi(a|{b, c}) \u2212 gi(a|{b, c, d, e, f}) = gi({a, b, c}) \u2212 gi({b, c}) \u2212 gi({a, b, c, d, e, f}) + gi({b, c, d, e, f}) = \u03b2i \u2212mi({b, c}) \u2212 \u03b2i + mi({b, c, d, e, f}) = mi({d, e, f}). So we have that mi({d, e, f}) = 0 and gi only involves a, b, c. According to lemma A.9, Egi is a conical combination of f0, f3 and f4.\nLemma A.11. m\u00b1 is not necessary, that is if we find one SCMM expansion of g, we can also find another SCMM expansion with m\u00b1 = 0.\nProof. For some i, Egi is fully curved and for the other i, Egi = g\u2032i + m\u2032i where g\u2032i is a fully curved SCMM and m\u2032i is modular according to lemma A.10. So we can group all m\u2032i and m\u00b1 together. If a fully curved submodular function is another fully curved submodular function plus modular, the only possibility is that the modular term equals 0. So the final modular vanishes.\nSo actually, we can ignore the final modular functions at the expansion of g. g(A) = \u2211 i min(mi(A), \u03b2i) =\u2211\niEmin(mi(A), \u03b2i) = \u2211 iEgi(A), where all term are non-negative and fully curved now.\nConsider the quality gi(a|{b, c}), it is non-negative for each i and 0 for g. So for each i, we have gi(a|{b, c}) = 0. In fact, gi is fully curved on {a, b, c} and {d, e, f} Lemma A.12. For a normalized monotonically non-decreasing submodular h, if h is fully curved on {a, b, c} and {d, e, f} , then Eh is determined by 5 values, Eh(1, 0), Eh(2, 0), Eh(1, 1), Eh(2, 1) and Eh(2, 2).\nProof. According to lemma A.4 and definition A.5, Eh(A) is determined by Eh(1, 0), Eh(1, 1), Eh(2, 0), Eh(2, 1), Eh(2, 2), Eh(3, 2) and Eh(3, 3). But Eh(n1, n2) = Eh(min(n1, 2),min(n2, 2)) according to the saturate properties. So Eh(1, 0), Eh(2, 0), Eh(1, 1), Eh(2, 1) and Eh(2, 2) are the only free variables remained.\nLemma A.13. Eh(n1, n2) = Eh1(n1, n2) + Eh2(n1, n2) if h = h1 + h2.\nSo in fact Ef is a 5-dimensional-vector. Here we calculate the 5-dimensional-vector for f1, f2, f3, f4, f5, see table 1.\nLemma A.14. For all i, Egi is a conical combination of f1, f2, f3, f4, f5.\nProof. For i s.t. gi({a, b, c}) = 0 or gi({d, e, f}) = 0, Egi is a conical combination of f0, f3, f4 according to lemma A.9. Moreover, f0 is not necessary since gi is fully curved.\nFor other i, if mi(a) + mi(b) < \u03b2i, then mi(c) = 0; otherwise gi(c|{a, b}) > 0. But in this case 0 = gi(a|{b, c}) = mi(a) and 0 = gi(b|{a, c}) = mi(b) which contradicts with gi({a, b, c}) > 0. So mi({a, b}) \u2265 \u03b2i. Similarly, we have mi({b, c}),mi({c, a}),mi({d, e}),mi({e, f}),mi({d, f}) \u2265 \u03b2i.\nSo Egi(2, 0) = Egi(2, 1) = Egi(2, 2) = \u03b2i. And the undecided parameters are Egi(1, 0) and Egi({1, 1}). It is easy to check that Egi = [Egi(1, 1) + 2Egi(1, 0)\u2212 2\u03b2i]f1 + 12 [5Egi(1, 1)\u2212 2Egi(1, 0)\u2212 3\u03b2i]f2 + [6\u03b2i\u2212\n6Egi(1, 1)]f5.\nNext we will show that all coefficients are non-negative.\nLemma A.15. Given gi(A) = min(mi(A), \u03b2i), if mi(a) + mi(b),mi(b) + mi(c),mi(c) + mi(a),mi(d) + mi(e),mi(e) +mi(f),mi(f) +mi(d) \u2265 \u03b2i, we have Egi(1, 1) + 2Egi(1, 0)\u22122\u03b2i \u2265 0, 5Egi(1, 1)\u22122Egi(1, 0)\u2212 3\u03b2i \u2265 0, Egi(1, 1) \u2264 \u03b2i Proof. Let xi be the weight of each elements. Without lose of generality, we assume that \u03b2i \u2265 x1 \u2265 xb \u2265 xc \u2265 0, \u03b2i \u2265 xd \u2265 xe \u2265 xf \u2265 0 and xc \u2265 xf . So xa, xb, xd, xe \u2265 12\u03b2i.\nEgi(1, 0) = 1 6 \u2211 i xi \u2265 23\u03b2i and Egi(1, 1) = 19 [ \u2211 v\u2208{a,b,c} \u2211 u\u2208{d,e,f} gi({v, u})] = 23\u03b2i+ 19 [min(xa+xf , \u03b2i)+ min(xb + xf , \u03b2i) + min(xc + xf ), \u03b2i)] \u2265 23\u03b2i. Therefore, Egi(1, 1) + 2Egi(1, 0)\u2212 2\u03b2i \u2265 0 and Egi(1, 1) \u2264 \u03b2i. For 5Egi(1, 1) \u2212 2Egi(1, 0) \u2212 3\u03b2i \u2265 0, if xc + xf \u2265 \u03b2i, we have Egi(1, 1) = \u03b2i and Egi(1, 0) \u2264 \u03b2i. So 5Egi(1, 1)\u2212 2Egi(1, 0)\u2212 3\u03b2i \u2265 0. If xc + xf \u2264 \u03b2i, 5Egi(1, 1) + 2Egi(1, 0)\u2212 3\u03b2i is growing when xf increased. So we can let xf = 0 for the worst case. Therefore 5Egi(1, 1)+2Egi(1, 0)\u22122\u03b2i = 5( 23\u03b2i+ 19 [xa+xb+xc])\u2212 13 [xa+xb+xc+xd+xe]\u22123\u03b2i which is increasing with respect to xa, xb, xc and deceasing with respect to xd, xe. Further more, we have 3 2\u03b2i \u2264 xa + xb + xc and xd + xe \u2264 2\u03b2i. So 5Egi(1, 1) + 2Egi(1, 0)\u2212 2\u03b2i \u2265 0\nTherefore, we have shown that Egi is a conical combination of f1, f2, f3, f4, f5 for all i. Therefore g = \u2211 iEgi is a conical combination of f1, f2, f3, f4, f5.\nThe 5-vector related to Eg is (\u03c6(1), \u03c6(2), \u03c6(2), \u03c6(3), \u03c6(4)). So according to table 1, the unique expression to expand g on f1, f2, f3, f4, f5 is g(A) = [2\u03c6(1) + \u03c6(2) \u2212 4\u03c6(3) + 2\u03c6(4)]f1 + [\u2212\u03c6(1) + 3.5\u03c6(2) \u2212 4\u03c6(3) + 1.5\u03c6(4)]f2 + [\u2212\u03c6(2) + 2\u03c6(3)\u2212 \u03c6(4)]f3 + [\u2212\u03c6(3) + \u03c6(4)]f4 + 6[\u2212\u03c6(2) + 2\u03c6(3)\u2212 \u03c6(4)]f5\nThis expression is valid if and only if \u2212\u03c6(1) + 3.5\u03c6(2)\u2212 4\u03c6(3) + 1.5\u03c6(4) \u2265 0 and 2\u03c6(1) + \u03c6(2)\u2212 4\u03c6(3) + 2\u03c6(4) \u2265 0; other coefficients are always non-negative according to concavity and monotonicity.\nThe \u201cif\u201d part is straight forward according to the above expansion as we saw after the statement of the theorem."}, {"heading": "B Sums of Weighted Cardinality Truncations is Smaller than SCMMs", "text": "In this section, show Lemma 5.10, namely that G = {\u2211B\u2286V \u2211|B|\u22121i=1 \u03b1B,i min(|A\u2229B|, i), \u2200B, i, \u03b1B,i \u2265 0} \u2282 SCMM. We assume the reader is familiar with the notation in Appendix A.\nLemma B.1. f5(A) /\u2208 { \u2211 B\u2286V \u2211|B|\u22121 i=1 \u03b1B,i min(|A \u2229B|, i)|\u03b1B,i \u2265 0}\nProof. Assume that\nf5(A) = \u2211 B\u2286V |B|\u22121\u2211 i=1 \u03b1B,i min(|A \u2229B|, i) = \u2211 B\u2286V |B|\u22121\u2211 i=1 \u03b1B,iEmin(|A \u2229B|, i). (121)\nNote that f5 is fully curved on {a, b, c} and {d, e, f}, and these hold for all terms. So for B and i, if i \u2265 |B\u2229{a, b, c}| or i \u2265 |B\u2229{d, e, f}|, \u03b1B,i = 0. Therefore the remaining terms are f1, f2, f3, f4, Emin(|A\u2229 {a, b, d, e}|, 1) and Emin(|A \u2229 {a, b, c, d, e}|, 1) Therefore, for all these functions, Ef(2, 0) \u2264 98Ef(1, 1), but Ef5(2, 0) = 6 5Ef5(1, 1) (table 1). So it is impossible to find a conical combination of min(|A \u2229 B|, i) that equals f5."}], "references": [{"title": "Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering", "author": ["A. Agarwal", "Choromanska A", "K. Choromanski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Graph-based learning for statistical machine translation", "author": ["A. Alexandrescu", "K. Kirchhoff"], "venue": "Proceedings of HLT, pages 119\u2013127,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Methods of information geometry, volume 191", "author": ["Shun-ichi Amari", "Hiroshi Nagaoka"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Streaming submodular maximization: Massive data summarization on the fly", "author": ["A. Badanidiyuru", "B. Mirzasoleiman", "A. Karbasi", "A. Krause"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 671\u2013680. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast algorithms for maximizing submodular functions", "author": ["Ashwinkumar Badanidiyuru", "Jan Vondr\u00e1k"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Bipartite matching generalizations for peptide identification in tandem mass spectrometry", "author": ["Wenruo Bai", "Jeffrey Bilmes", "William S. Noble"], "venue": "In 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Learning submodular functions", "author": ["M. Balcan", "N. Harvey"], "venue": "Technical report, arXiv:1008.2159,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning submodular functions", "author": ["Maria-Florina Balcan", "Nicholas JA Harvey"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Coreference semantics from web features", "author": ["M. Bansal", "D. Klein"], "venue": "Proceedings of ACL, pages 389\u2013398,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "The power of randomization: Distributed submodular maximization on massive datasets", "author": ["Rafael Barbosa", "Alina Ene", "Huy L Nguyen", "Justin Ward"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A new framework for distributed submodular maximization", "author": ["Rafael Da Ponte Barbosa", "Alina Ene", "Huy L Nguyen", "Justin Ward"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends R \u00a9 in Machine Learning, 2(1):1\u2013127,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Creating robust supervised classifiers via web-scale n-gram data", "author": ["S. Bergsma", "E. Pitler", "D. Lin"], "venue": "Proceedings of ACL, pages 865\u2013874,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Instance selection for machine translation using feature decay algorithms", "author": ["E. Bi\u00e7ici", "D. Yuret"], "venue": "Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272\u2013283,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univ Pr,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Discover feature engineering, how to engineer features and how to get good at it, 2014", "author": ["Jason Brownlee"], "venue": "Machine Learning Process", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Submodular maximization with cardinality constraints", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Seffi Naor", "Roy Schwartz"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Maximizing a monotone submodular function subject to a matroid constraint", "author": ["Gruia Calinescu", "Chandra Chekuri", "Martin P\u00e1l", "Jan Vondr\u00e1k"], "venue": "SIAM Journal on Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Streaming algorithms for submodular function maximization", "author": ["Chandra Chekuri", "Shalmoli Gupta", "Kent Quanrud"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes", "author": ["Chandra Chekuri", "Jan Vondr\u00e1k", "Rico Zenklusen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fast approximate kNN graph construction for high dimensional data via recursive Lanczos bisection", "author": ["J. Chen", "H.-R. Fang", "Y. Saad"], "venue": "JMLR, 10:1989\u20132012,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On the uncapacitated location problem", "author": ["G. Cornu\u00e9jols", "M. Fisher", "G.L. Nemhauser"], "venue": "Annals of Discrete Mathematics, 1:163\u2013177,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Boolean functions: Theory, algorithms, and applications", "author": ["Yves Crama", "Peter L Hammer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Visual categorization with bags of keypoints", "author": ["Gabriella Csurka", "Christopher Dance", "Lixin Fan", "Jutta Willamowski", "C\u00e9dric Bray"], "venue": "In Workshop on statistical learning in computer vision, ECCV,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Testing membership in matroid polyhedra", "author": ["W.H. Cunningham"], "venue": "J Combinatorial Theory B, 36:161\u2013188,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1984}, {"title": "Decomposition of submodular functions", "author": ["William H Cunningham"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1983}, {"title": "Optimal attack and reinforcement of a network", "author": ["William H Cunningham"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1985}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["A. Das", "D. Kempe"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Bag-of-visual-words models for adult image classification and filtering", "author": ["Thomas Deselaers", "Lexi Pimenidis", "Hermann Ney"], "venue": "In Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Deep submodular functions: Definitions & learning", "author": ["Brian Dolhansky", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Deep submodular functions: Definitions and learning", "author": ["Brian Dolhansky", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "The pure theory of monopoly", "author": ["F.Y. Edgeworth"], "venue": "Giornale degli Economisti, 1887. Reprinted in EDGEWORTH, F. Y. Papers relating to political economy. London: Macmillan,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1925}, {"title": "Turning down the noise in the blogosphere", "author": ["Khalid El-Arini", "Gaurav Veda", "Dafna Shahaf", "Carlos Guestrin"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "CoRR, abs/1512.03965,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Maximizing non-monotone submodular functions", "author": ["Uriel Feige", "Vahab S Mirrokni", "Jan Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Optimal bounds on approximation of submodular and XOS functions by juntas", "author": ["V. Feldman", "J. Vondr\u00e1k"], "venue": "CoRR, abs/1307.3301,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Representation, approximation and learning of submodular functions using low-rank decision trees", "author": ["Vitaly Feldman", "Pravesh Kothari", "Jan Vondr\u00e1k"], "venue": "In COLT,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "A visual bag of words method for interactive qualitative localization and mapping", "author": ["David Filliat"], "venue": "In Robotics and Automation,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014 II", "author": ["M.L. Fisher", "G.L. Nemhauser", "L.A. Wolsey"], "venue": "Polyhedral combinatorics, pages 73\u201387,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1978}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Number 58 in Annals of Discrete Mathematics. Elsevier Science, 2nd edition,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimizing a submodular function arising from a concave function", "author": ["Satoru Fujishige", "Satoru Iwata"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "Near-optimal MAP inference for determinantal point processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "NIPS,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate Inference for Determinantal Point Processes", "author": ["Jennifer Gillenwater"], "venue": "PhD thesis, U. Penn,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Submodular hamming metrics", "author": ["Jennifer Gillenwater", "Rishabh Iyer", "Bethany Lusch", "Rahul Kidambi", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Near-optimal MAP inference for determinantal point processes", "author": ["Jennifer Gillenwater", "Alex Kulesza", "Ben Taskar"], "venue": "Advances in Neural Information", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Approximating submodular functions everywhere", "author": ["M.X. Goemans", "N.J.A. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "SODA, pages 535\u2013544,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithmic graph theory and perfect graphs, volume 57", "author": ["Martin Charles Golumbic"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Sampling from probabilistic submodular models", "author": ["Alkis Gotovos", "S. Hamed Hassani", "Andreas Krause"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Learning binary hash codes for large-scale image search. InMachine learning for computer vision, pages 49\u201387", "author": ["Kristen Grauman", "Rob Fergus"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Active semi-supervised learning using submodular functions", "author": ["Andrew Guillory", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Optimal marketing strategies over social networks", "author": ["Jason Hartline", "Vahab Mirrokni", "Mukund Sundararajan"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2008}, {"title": "Submodular optimization under noise", "author": ["Avinatan Hassidim", "Yaron Singer"], "venue": "arXiv preprint arXiv:1601.03095,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Convex analysis and minimization algorithms I: fundamentals, volume 305", "author": ["Jean-Baptiste Hiriart-Urruty", "Claude Lemar\u00e9chal"], "venue": "Springer science & business media,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1993}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1991}, {"title": "Towards minimizing k-submodular functions", "author": ["Anna Huber", "Vladimir Kolmogorov"], "venue": "CoRR, abs/1309.5469,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Computational geometric approach to submodular function minimization for multiclass queueing systems", "author": ["Toshinari Itoko", "Satoru Iwata"], "venue": "In International Conference on Integer Programming and Combinatorial Optimization,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["Rishabh Iyer", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Submodular point processes", "author": ["Rishabh Iyer", "Jeff Bilmes"], "venue": "In 18th International Conference on Artificial Intelligence and Statistics (AISTATS-2015),", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}, {"title": "Fast semidifferential-based submodular function optimization", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff A. Bilmes"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2013}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of ICML, pages 441\u2013448,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "Reflection methods for user-friendly submodular optimization", "author": ["Stefanie Jegelka", "Francis Bach", "Suvrit Sra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["Stefanie Jegelka", "Jeff A. Bilmes"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Fast approximate submodular minimization", "author": ["Stefanie Jegelka", "Hui Lin", "Jeff A. Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2011}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 655\u00e2\u0102\u015e665", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Canonical parameterizations and zero parameter-effects curvature", "author": ["Robert E Kass"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1984}, {"title": "Finding groups in data: an introduction to cluster analysis, volume 344", "author": ["Leonard Kaufman", "Peter J Rousseeuw"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Toward efficient agnostic learning", "author": ["Michael J Kearns", "Robert E Schapire", "Linda M Sellie"], "venue": "Machine Learning,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1994}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "E. Tardos"], "venue": "SIGKDD,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2003}, {"title": "Submodularity for data selection in machine translation", "author": ["K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of EMNLP, pages 131\u2013141,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2014}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE TPAMI,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2004}, {"title": "Submodularity on a tree: Unifying l\u02c6\\ natural-convex and bisubmodular functions", "author": ["Vladimir Kolmogorov"], "venue": "In International Symposium on Mathematical Foundations of Computer Science,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2011}, {"title": "Robust submodular observation selection", "author": ["Andreas Krause", "Brendan McMahan", "Carlos Guestrin", "Anupam Gupta"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2008}, {"title": "Determinantal point processes for machine learning", "author": ["Alex Kulesza", "B Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2012}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Ravi Kumar", "Benjamin Moseley", "Sergei Vassilvitskii", "Andrea Vattani"], "venue": "ACM Transactions on Parallel Computing,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2015}, {"title": "Using n-gram and word network features for native language identification", "author": ["S. Lahiri", "R. Mihalcea"], "venue": "Proceedings of NAACL-HLT Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised semantic role induction with graph partitioning", "author": ["J. Lang", "M. Lapata"], "venue": "Proceedings of EMNLP, pages 1320\u20131331,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models, volume 17", "author": ["Steffen L Lauritzen"], "venue": null, "citeRegEx": "86", "shortCiteRegEx": "86", "year": 1996}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, may", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-monotone submodular maximization under matroid and knapsack constraints", "author": ["Jon Lee", "Vahab S Mirrokni", "Viswanath Nagarajan", "Maxim Sviridenko"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2009}, {"title": "Cost-effective outbreak detection in networks", "author": ["Jure Leskovec", "Andreas Krause", "Carlos Guestrin", "Christos Faloutsos", "Jeanne VanBriesen", "Natalie Glance"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2007}, {"title": "Contextual bag-of-words for visual categorization. Circuits and Systems for Video Technology", "author": ["Teng Li", "Tao Mei", "In-So Kweon", "Xian-Sheng Hua"], "venue": "IEEE Transactions on,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2011}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "ACL, pages 510\u2013520,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "Uncertainty in Artificial Intelligence (UAI), Catalina Island, USA, July", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2012}, {"title": "A Class of Submodular Functions for Document Summarization", "author": ["Hui Lin", "Jeff Bilmes"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2011}, {"title": "Word alignment via submodular maximization over matroids. In North American chapter of the Association for Computational Linguistics/Human Language Technology", "author": ["Hui Lin", "Jeff Bilmes"], "venue": null, "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2011}, {"title": "An application of the submodular principal partition to training data subset selection", "author": ["Hui Lin", "Jeff A. Bilmes"], "venue": "In Neural Information Processing Society (NIPS) Workshop,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2010}, {"title": "A novel graph-based compact representation of word alignment", "author": ["Q. Liu", "Z. Tu", "S. Lin"], "venue": "Proceedings of ACL, pages 358\u2013363,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2013}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": "In Proceedings of NAACL,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2015}, {"title": "Submodular feature selection for high-dimensional acoustic score spaces", "author": ["Yuzong Liu", "Kai Wei", "Katrin Kirchhoff", "Yisong Song", "Jeff Bilmes"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "An inequality for rearrangements", "author": ["GG Lorentz"], "venue": "The American Mathematical Monthly, 60(3):176\u2013179,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1953}, {"title": "Matroid matching and some applications", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1980}, {"title": "Structural parse tree features for text representation", "author": ["S. Massung", "C. Zhai", "J. Hockenmaier"], "venue": "Proceedings of IEEE Seventh Conference on Semantic Computing,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Multivariate information transmission", "author": ["William J McGill"], "venue": "Psychometrika, 19(2):97\u2013116,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1954}, {"title": "Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling", "author": ["R. Mihalcea"], "venue": "Proceedings of EMNLP, pages 411\u2013418,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems, page 3111a\u0302A\u0306S\u03273119,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 1995}, {"title": "Distributed submodular cover: Succinctly summarizing massive data", "author": ["Baharan Mirzasoleiman", "Amin Karbasi", "Ashwinkumar Badanidiyuru", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2015}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2013}, {"title": "Discrete convex analysis", "author": ["Kazuo Murota"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2003}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["Mukund Narasimhan", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2005}, {"title": "Graph connectivity measures for unsupervised word sense disambiguation", "author": ["R. Navigli", "M. Lapata"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 1683\u20131688,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions i", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming, 14:265\u2013294,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 1978}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2004}, {"title": "Automatic visual bag-of-words for online robot navigation and mapping", "author": ["Tudor Nicosevici", "Rafael Garcia"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2012}, {"title": "Convex functions and their applications: a contemporary approach", "author": ["Constantin Niculescu", "Lars-Erik Persson"], "venue": "Springer Science & Business Media,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2006}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["R. Nishihara", "S Jegelka", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems, pages 640\u2013648,", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear extensions of games", "author": ["Guillermo Owen"], "venue": "Management Science,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 1972}, {"title": "Using the mutual k-nearest neighbor graphs for semi-supervised classication of natural language data", "author": ["K. Ozaki", "M. Shimbo", "M. Komachi", "Y. Matsumoto"], "venue": "Proceedings of CoNLL, pages 154\u2013162,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2011}, {"title": "Max-Margin Tensor Neural Network for Chinese Word Segmentation", "author": ["W. Pei"], "venue": "Transactions of the Association of Computational Linguistics, pages 293\u2013303,", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), page 1532a\u0302A\u0306S\u03271543,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2014}, {"title": "Explicit and implicit syntactic features for text classification", "author": ["M. Post", "S. Bergsma"], "venue": "Proceedings of ACL, pages 866\u2013872,", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised word sense disambiguation with n-gram features", "author": ["D. Preotiuc-Pietro", "F. Hristea"], "venue": "Artificial Intelligence Review, 41(2):241\u2013260,", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2014}, {"title": "Bisubmodular functions. CORE Discussion Papers 1989001", "author": ["Liqun Qi"], "venue": "Universite\u0301 catholique de Louvain, Center for Operations Research and Econometrics (CORE),", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 1989}, {"title": "Graph propagation for paraphrasing out-ofvocabulary words in statistical machine translation", "author": ["M. Razmara", "M. Siahbani", "G. Haffari", "A. Sarkar"], "venue": "Proceedings of ACL,", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2013}, {"title": "Characterization of the subdifferentials of convex functions", "author": ["Ralph Rockafellar"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 1966}, {"title": "On the maximal monotonicity of subdifferential mappings", "author": ["Ralph Rockafellar"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 1970}, {"title": "Submodular inference of diffusion networks from multiple trees", "author": ["Manuel Gomez Rodriguez", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1205.1671,", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2012}, {"title": "A potential theory for monotone multivalued operators", "author": ["G Romano", "L Rosati", "F Marotti de Sciarra", "P Bisegna"], "venue": "Quarterly of applied mathematics,", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1993}, {"title": "Foundations of Economic Analysis", "author": ["Paul A Samuelson"], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1947}, {"title": "Complementarity: An essay on the 40th anniversary of the hicks-allen revolution in demand theory", "author": ["Paul A Samuelson"], "venue": "Journal of Economic literature,", "citeRegEx": "133", "shortCiteRegEx": "133", "year": 1974}, {"title": "Multi-agent and multivariate submodular optimization", "author": ["Richard Santiago", "F. Bruce Shepherd"], "venue": null, "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2016}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2010}, {"title": "A submodular optimization approach to sentence set selection", "author": ["Yusuke Shinohara"], "venue": "In ICASSP,", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2014}, {"title": "Differential-geometrical methods in statistics, volume 28", "author": ["Amari Shun-ichi"], "venue": null, "citeRegEx": "137", "shortCiteRegEx": "137", "year": 1985}, {"title": "UHD: Cross-lingual Word Sense Disambiguation using multilingual co-occurrence graphs", "author": ["C. Silberer", "S.P. Ponzetto"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), pages 134\u2013137,", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2010}, {"title": "On bisubmodular maximization", "author": ["Ajit Singh", "Andrew Guillory", "Jeff Bilmes"], "venue": "In Fifteenth International Conference on Artificial Intelligence and Statistics (AISTAT),", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 2012}, {"title": "Large-margin learning of submodular summarization models", "author": ["R. Sipos", "P. Shivaswamy", "T. Joachims"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224\u2013233. Association for Computational Linguistics,", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient minimization of decomposable submodular functions", "author": ["P. Stobbe", "A. Krause"], "venue": "NIPS,", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex Analysis for Minimizing and Learning Submodular Set Functions", "author": ["Peter Stobbe"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 2013}, {"title": "The multiinformation function as a tool for measuring stochastic dependence", "author": ["Milan Studen\u1ef3", "Jirina Vejnarov\u00e1"], "venue": "In Learning in graphical models,", "citeRegEx": "143", "shortCiteRegEx": "143", "year": 1998}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["A. Subramanya", "S. Petrov", "F. Pereira"], "venue": "Proceedings of EMNLP, pages 167\u2013176,", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Zoya Svitkina", "Lisa Fleischer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2011}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2005}, {"title": "Language modeling for bag-of-visual words image categorization", "author": ["Pierre Tirilly", "Vincent Claveau", "Patrick Gros"], "venue": "In Proceedings of the 2008 international conference on Content-based image and video retrieval,", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2008}, {"title": "Minimizing a submodular function on a lattice", "author": ["Donald M Topkis"], "venue": "Operations research,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 1978}, {"title": "Supermodularity and complementarity", "author": ["Donald M Topkis"], "venue": "Princeton university press,", "citeRegEx": "149", "shortCiteRegEx": "149", "year": 1998}, {"title": "Learning mixtures of submodular functions for image collection summarization", "author": ["S. Tschiatschek", "R. Iyer", "H. Wei", "J. Bilmes"], "venue": "Neural Information Processing Society (NIPS), Montreal, Canada, December", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page 384a\u0302A\u0306S\u0327394,", "citeRegEx": "151", "shortCiteRegEx": "151", "year": 2010}, {"title": "Submodularity in combinatorial optimization", "author": ["J. Vondr\u00e1k"], "venue": "PhD thesis, Charles University,", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast graph construction using auction algorithm", "author": ["J. Wang", "Y. Xia"], "venue": "arXiv preprint arXiv:1210.4917,", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2012}, {"title": "Information theoretical analysis of multivariate correlation", "author": ["Satosi Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "154", "shortCiteRegEx": "154", "year": 1960}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Florence, Italy,", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributional features for text categorization", "author": ["X.B. Xue", "Z.H.Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "157", "shortCiteRegEx": "157", "year": 2009}, {"title": "Evaluating bag-of-visualwords representations in scene classification", "author": ["Jun Yang", "Yu-Gang Jiang", "Alexander G Hauptmann", "Chong-Wah Ngo"], "venue": "In Proceedings of the international workshop on Workshop on multimedia information retrieval,", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 2007}, {"title": "Sentiment classification in under-resourced languages using graph-based semi-supervised learning methods", "author": ["R. Yong", "K. Nobuhiro", "N. Yoshinaga", "M. Kitsuregawa"], "venue": "IEICE TRANSACTIONS on Information and Systems, 97(4):790\u2013797,", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning structural svms with latent variables", "author": ["Chun-Nam John Yu", "Thorsten Joachims"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "160", "shortCiteRegEx": "160", "year": 2009}, {"title": "Exploring syntactic structured features over parse trees for relation extraction using kernel methods", "author": ["M. Zhang", "G. Zhou", "A. Aw"], "venue": "Information Processing & Management, 44(2):687\u2013701,", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast kNN graph construction with locality sensitive hashing", "author": ["Y.-M. Zhang", "K. Huang", "G. Geng", "C.-L. Liu"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 660\u2013674,", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2013}, {"title": "A non-shannon-type conditional inequality of information quantities", "author": ["Zhen Zhang", "Raymond W Yeung"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "163", "shortCiteRegEx": "163", "year": 1997}, {"title": "Mastering Feature Engineering: Principles and Techniques for Data Scientists", "author": ["Alice Zheng"], "venue": "O\u2019Reilly Media,", "citeRegEx": "164", "shortCiteRegEx": "164", "year": 2016}], "referenceMentions": [{"referenceID": 44, "context": "For example, submodular functions can be minimized without constraints in polynomial time [46] even though they lie within a 2-dimensional cone in R2n and are parameterized, in their most general form, with a corresponding 2 independent degrees of freedom.", "startOffset": 90, "endOffset": 94}, {"referenceID": 108, "context": ", in the cardinality constrained case, the classic 1\u2212 1/e result of Nemhauser [113] via the greedy algorithm.", "startOffset": 78, "endOffset": 83}, {"referenceID": 20, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 19, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 85, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 63, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 65, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 88, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 169, "endOffset": 172}, {"referenceID": 46, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 79, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 64, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 52, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 76, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 288, "endOffset": 292}, {"referenceID": 95, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 302, "endOffset": 306}, {"referenceID": 31, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 332, "endOffset": 336}, {"referenceID": 55, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 354, "endOffset": 358}, {"referenceID": 74, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 401, "endOffset": 405}, {"referenceID": 86, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 428, "endOffset": 432}, {"referenceID": 123, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 456, "endOffset": 461}, {"referenceID": 54, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 521, "endOffset": 525}, {"referenceID": 106, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 78, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 68, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 61, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 62, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 190, "endOffset": 194}, {"referenceID": 63, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "For example, it was shown that learning submodularity in the PMAC setting is fairly hard [10] although in some cases things are a bit easier [42].", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "For example, it was shown that learning submodularity in the PMAC setting is fairly hard [10] although in some cases things are a bit easier [42].", "startOffset": 141, "endOffset": 145}, {"referenceID": 133, "context": "For example, in [140, 92], it is shown empirically that one can effectively learn mixtures of submodular functions using a max-margin learning framework \u2014 here the components of the mixture are fixed and it is only the mixture parameters that are learnt, leading often to a convex optimization problem.", "startOffset": 16, "endOffset": 25}, {"referenceID": 89, "context": "For example, in [140, 92], it is shown empirically that one can effectively learn mixtures of submodular functions using a max-margin learning framework \u2014 here the components of the mixture are fixed and it is only the mixture parameters that are learnt, leading often to a convex optimization problem.", "startOffset": 16, "endOffset": 25}, {"referenceID": 89, "context": "In some cases, computing gradients of the convex problem can be done using submodular maximization [92], while in other cases, even a gradient requires minimizing a difference of two submodular functions [150].", "startOffset": 99, "endOffset": 103}, {"referenceID": 143, "context": "In some cases, computing gradients of the convex problem can be done using submodular maximization [92], while in other cases, even a gradient requires minimizing a difference of two submodular functions [150].", "startOffset": 204, "endOffset": 209}, {"referenceID": 134, "context": "These include the so-called \u201cdecomposable\u201d submodular functions, namely those that can be represented as a sum of concave composed with modular functions [141].", "startOffset": 154, "endOffset": 159}, {"referenceID": 50, "context": "These matroid rank functions include the truncated matroid rank function [52] that is often used to show theoretical worst-case performance for many constrained submodular minimization problems.", "startOffset": 73, "endOffset": 77}, {"referenceID": 89, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 396, "endOffset": 400}, {"referenceID": 5, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 80, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 21, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 103, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 11, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 12, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 133, "context": "We therefore extend the max-margin learning framework of [140, 92] to apply to DSFs.", "startOffset": 57, "endOffset": 66}, {"referenceID": 89, "context": "We therefore extend the max-margin learning framework of [140, 92] to apply to DSFs.", "startOffset": 57, "endOffset": 66}, {"referenceID": 30, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 29, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 97, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 100, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 107, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 1, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 131, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 137, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 82, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 93, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 120, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 151, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 90, "context": "Graph-based submodular functions include the classic graph cut function f(X) = \u2211 x\u2208X,y\u2208V \\X w(x, y), but also the monotone graph cut function f(X) = \u2211 x\u2208X,y\u2208V w(x, y), the saturated graph cut function [93] f(X) = \u2211 v\u2208V min(Cv(X), \u03b1Cv(V )) where \u03b1 \u2208 (0, 1) is a hyperparameter and where Cv(X) = \u2211 x\u2208X w(v, x).", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 108, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 43, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 5, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 184, "endOffset": 191}, {"referenceID": 72, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 184, "endOffset": 191}, {"referenceID": 89, "context": "It is also useful and learn conic mixtures of graph based functions as done in [92].", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 79, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 46, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 0, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 47, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 23, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 66, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 23, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 114, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 146, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 154, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 89, "context": "For machine learning applications, moreover, it is difficult with these functions to train on a training set that may generalize to a test set [92].", "startOffset": 143, "endOffset": 147}, {"referenceID": 134, "context": "A class of submodular functions [141] used in machine learning are the so-called \u201cdecomposable functions.", "startOffset": 32, "endOffset": 37}, {"referenceID": 45, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 44, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 134, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 83, "context": "While such functions have been called \u201cdecomposable\u201d in the past, in this work we will refer to this class of functions as \u201cSums of Concave over non-negative Modular plus Modular\u201d (or SCMMs) in order to avoid confusion with the term \u201cdecomposable\u201d used to describe certain graphical models [86, 53].", "startOffset": 290, "endOffset": 298}, {"referenceID": 51, "context": "While such functions have been called \u201cdecomposable\u201d in the past, in this work we will refer to this class of functions as \u201cSums of Concave over non-negative Modular plus Modular\u201d (or SCMMs) in order to avoid confusion with the term \u201cdecomposable\u201d used to describe certain graphical models [86, 53].", "startOffset": 290, "endOffset": 298}, {"referenceID": 134, "context": "2 SCMMs have been shown to be quite flexible [141], being able to represent a surprisingly diverse set of functions.", "startOffset": 45, "endOffset": 50}, {"referenceID": 69, "context": "It is shown in [72] that any SCMM can be represented with a graph cut function that might optionally utilize additional auxiliary variables that are first minimized over.", "startOffset": 15, "endOffset": 19}, {"referenceID": 60, "context": "SCMMs can represent other functions as well, such as multiclass queuing system functions [63, 142], functions of the form f(A) = m1(A)\u03c6(m2(A)) where m1,m2 : V \u2192 R+ are both non-negative modular functions, and \u03c6 : R \u2192 R is a non-increasing concave function.", "startOffset": 89, "endOffset": 98}, {"referenceID": 135, "context": "SCMMs can represent other functions as well, such as multiclass queuing system functions [63, 142], functions of the form f(A) = m1(A)\u03c6(m2(A)) where m1,m2 : V \u2192 R+ are both non-negative modular functions, and \u03c6 : R \u2192 R is a non-increasing concave function.", "startOffset": 89, "endOffset": 98}, {"referenceID": 37, "context": "Another useful instance is the probabilistic coverage function [39] where we have a set of topics, indexed by i, and V is a set of documents.", "startOffset": 63, "endOffset": 67}, {"referenceID": 134, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 67, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 112, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 29, "context": "Moreover, it appears that there is little loss of generality in handling the nonmonotonicty separately from the polymatroidality, as any non-monotone submodular function can easily be written as a sum of a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 270, "endOffset": 278}, {"referenceID": 28, "context": "Moreover, it appears that there is little loss of generality in handling the nonmonotonicty separately from the polymatroidality, as any non-monotone submodular function can easily be written as a sum of a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 270, "endOffset": 278}, {"referenceID": 83, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 53, "endOffset": 61}, {"referenceID": 51, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 53, "endOffset": 61}, {"referenceID": 29, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 146, "endOffset": 150}, {"referenceID": 156, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 153, "endOffset": 167}, {"referenceID": 18, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 153, "endOffset": 167}, {"referenceID": 139, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 203, "endOffset": 208}, {"referenceID": 153, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 98, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 117, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 149, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 15, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 10, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 81, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 118, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 42, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 150, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 87, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 110, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 27, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 140, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 33, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 144, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 101, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 104, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 116, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 70, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 94, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 129, "context": "Alternatively, defining g(X) , logm(X)\u2212D(p||{m\u0304u(X)}) = \u2211 u\u2208U pu log(mu(X)) as done in [136], we have a submodular function g that represents a combination of its quantity of X via its features (i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 130, "context": "The KL-divergence can be generalized in various ways, one of which is known as the f -divergence, or in particular the \u03b1-divergence [137, 3].", "startOffset": 132, "endOffset": 140}, {"referenceID": 2, "context": "The KL-divergence can be generalized in various ways, one of which is known as the f -divergence, or in particular the \u03b1-divergence [137, 3].", "startOffset": 132, "endOffset": 140}, {"referenceID": 71, "context": "Using the reparameteriation \u03b1 = 1\u2212 2\u03b4 [74], the \u03b1-divergence (or now \u03b4-divergence [165]) can be expressed as", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "For \u03b4 \u2208 (0, 1) we see that the optimization problem minX\u2286V :m(X)\u2264bD\u03b4(p, p\u0304(X)) where b is a budget constraint is the same as the constrained submodular maximization problem maxX\u2286V :m(X)\u2264b g(X) where g(X) = \u2211 u\u2208U p \u03b4 u(mu(X)) 1\u2212\u03b4 is a feature-based function since \u03c6u(\u03b1) = \u03b11\u2212\u03b4 is concave on \u03b1 \u2208 [0, 1] for \u03b4 \u2208 (0, 1).", "startOffset": 294, "endOffset": 300}, {"referenceID": 3, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 222, "endOffset": 225}, {"referenceID": 16, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 572, "endOffset": 580}, {"referenceID": 75, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 572, "endOffset": 580}, {"referenceID": 148, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 100, "endOffset": 105}, {"referenceID": 75, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 127, "endOffset": 131}, {"referenceID": 68, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 153, "endOffset": 157}, {"referenceID": 5, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 21, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 103, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 11, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 12, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 102, "context": ", WordNet [105]), or a visual hierarchy in computer vision (e.", "startOffset": 10, "endOffset": 15}, {"referenceID": 32, "context": ", ImageNet [34]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 90, "context": "Submodularity follows since a composition of a monotone non-decreasing function h and a monotone non-decreasing concave function \u03c6 (g(\u00b7) = \u03c6(h(\u00b7))) is submodular (Theorem 1 in [93] and repeated, with proof, in Theorem 5.", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "As mentioned above, from the perspective of defining a submodular function, there is no loss of generality by adding the final modular function m\u00b1 to a polymatroid function [31, 30].", "startOffset": 173, "endOffset": 181}, {"referenceID": 28, "context": "As mentioned above, from the perspective of defining a submodular function, there is no loss of generality by adding the final modular function m\u00b1 to a polymatroid function [31, 30].", "startOffset": 173, "endOffset": 181}, {"referenceID": 13, "context": "For example, a one-layer DSF must construct a valuation over a set of objects from a large number of low-level features which can lead to fewer opportunities for feature sharing while a deeper network fosters distributed representations, also analogous to DNNs [15, 16].", "startOffset": 261, "endOffset": 269}, {"referenceID": 14, "context": "For example, a one-layer DSF must construct a valuation over a set of objects from a large number of low-level features which can lead to fewer opportunities for feature sharing while a deeper network fosters distributed representations, also analogous to DNNs [15, 16].", "startOffset": 261, "endOffset": 269}, {"referenceID": 92, "context": "In one instance [95], a square root was applied to a subset of the right hand nodes in a bipartite neighborhood function in order to offer reduced cost for these nodes being indirectly selected in the graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 148, "context": "In [155] a two-layer DSF was used to introduce higher-level interaction between features, an act that yielded benefits in speech data summarization.", "startOffset": 3, "endOffset": 8}, {"referenceID": 50, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 138, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 63, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 68, "context": ", [71]) to formulate submodular functions from concave functions that have an initial linear part followed by either a saturation or by a smooth concave part.", "startOffset": 2, "endOffset": 6}, {"referenceID": 121, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 122, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 57, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 109, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 109, "context": "13, page 54, [114]).", "startOffset": 13, "endOffset": 18}, {"referenceID": 111, "context": "A function is concave if and only if it is continuous and midpoint concave [116] (or midconcave [127]), defined as for any x, y \u2208 R f((x+ y)/2) \u2265 (f(x) + f(y))/2).", "startOffset": 75, "endOffset": 80}, {"referenceID": 90, "context": "We next restate Theorem 1 from [93] but also provide a proof which was missing.", "startOffset": 31, "endOffset": 35}, {"referenceID": 57, "context": "From the monotonicity of the supergradient [60, 114], we always have dmin a , min d\u2208\u2202\u03c6(a) d \u2265 dab \u2265 max d\u2208\u2202\u03c6(b) d , dmax b (24)", "startOffset": 43, "endOffset": 52}, {"referenceID": 109, "context": "From the monotonicity of the supergradient [60, 114], we always have dmin a , min d\u2208\u2202\u03c6(a) d \u2265 dab \u2265 max d\u2208\u2202\u03c6(b) d , dmax b (24)", "startOffset": 43, "endOffset": 52}, {"referenceID": 134, "context": "9 (Sums of Modular Truncations [141]).", "startOffset": 31, "endOffset": 36}, {"referenceID": 17, "context": "2 Antitone Maps and Superdifferentials Thanks to concave composition closure rules [19], the root function \u03c8r(x) : R \u2192 R in Eqn.", "startOffset": 83, "endOffset": 87}, {"referenceID": 57, "context": "4 when k = 1 \u2014 this is because \u03c6 : R\u2192 R being concave is, in the univariate case, synonymous with it having an antitone superdifferential (which is synonymous with monotone supergradients [60, 114]).", "startOffset": 188, "endOffset": 197}, {"referenceID": 109, "context": "4 when k = 1 \u2014 this is because \u03c6 : R\u2192 R being concave is, in the univariate case, synonymous with it having an antitone superdifferential (which is synonymous with monotone supergradients [60, 114]).", "startOffset": 188, "endOffset": 197}, {"referenceID": 145, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 108, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 43, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 145, "context": "This function is concave and is tight f(A) = \u03c8(1A),\u2200A at the vertices of the unit hypercube, but is not the concave closure of f [152].", "startOffset": 129, "endOffset": 134}, {"referenceID": 36, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 125, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 96, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 126, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 141, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 142, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 121, "context": "Not only does an antitone map alone not ensure concavity (a result established originally in [128, 129]), an antitone map need not be a gradient field (a property that, if true, would make it a conservative field).", "startOffset": 93, "endOffset": 103}, {"referenceID": 122, "context": "Not only does an antitone map alone not ensure concavity (a result established originally in [128, 129]), an antitone map need not be a gradient field (a property that, if true, would make it a conservative field).", "startOffset": 93, "endOffset": 103}, {"referenceID": 113, "context": "For an example related to submodular functions, the multilinear extension [119], defined as:", "startOffset": 74, "endOffset": 79}, {"referenceID": 39, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 22, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 6, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 26, "context": "Also, any function defined only on the vertices of the unit hypercube has an infinite number of both concave and convex extensions [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 124, "context": "15 does not require concavity, however, this suggests that there may be a way to define submodular functions using generalized line integrals of antitone maps without needing concavity [131].", "startOffset": 185, "endOffset": 190}, {"referenceID": 36, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 125, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 126, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 141, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 142, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 145, "context": "Ordinarily the concave closure of a submodular function is computationally hard to evaluate [152] and this is disappointing since such a construct would be useful for relaxation schemes for maximizing submodular functions (and as result surrogates, such as the multilinear extension are used).", "startOffset": 92, "endOffset": 97}, {"referenceID": 44, "context": "A matroid M [46] is a set system M = (V, I) where I = {I1, I2, .", "startOffset": 12, "endOffset": 16}, {"referenceID": 44, "context": "All monotone non-decreasing non-negative integral submodular functions can be exactly represented by grouping and then evaluating grouped ground elements in a matroid [46].", "startOffset": 167, "endOffset": 171}, {"referenceID": 91, "context": "A useful matroid in machine learning applications [94, 9] is the partition matroid, where a partition (V1, V2, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 7, "context": "A useful matroid in machine learning applications [94, 9] is the partition matroid, where a partition (V1, V2, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 50, "context": "Thus, within the family of DSFs lie the truncated matroid rank functions used to show information theoretic hardness for many constrained submodular optimization problems [52], i.", "startOffset": 171, "endOffset": 175}, {"referenceID": 67, "context": "Since this function is used to show hardness for many constrained submodular minimization problems, and since DSFs generalize laminar matroid ranks, this portends poorly for algorithms of the kind found in [70, 117] to achieve fast DSF minimization.", "startOffset": 206, "endOffset": 215}, {"referenceID": 112, "context": "Since this function is used to show hardness for many constrained submodular minimization problems, and since DSFs generalize laminar matroid ranks, this portends poorly for algorithms of the kind found in [70, 117] to achieve fast DSF minimization.", "startOffset": 206, "endOffset": 215}, {"referenceID": 142, "context": "We use the term \u201csurplus\u201d under an interpretation where A is a set of agents that can perform their action either independently of each other, or may perform their actions jointly and cooperatively [149].", "startOffset": 198, "endOffset": 203}, {"referenceID": 99, "context": "For the entropy function, this idea was first defined in [102].", "startOffset": 57, "endOffset": 62}, {"referenceID": 147, "context": "4 Absolute redundancy is also called \u201ctotal correlation\u201d [154] and also the \u201cmulti-information\u201d function [143].", "startOffset": 57, "endOffset": 62}, {"referenceID": 136, "context": "4 Absolute redundancy is also called \u201ctotal correlation\u201d [154] and also the \u201cmulti-information\u201d function [143].", "startOffset": 105, "endOffset": 110}, {"referenceID": 142, "context": "How fairly to redistribute surplus back to the individual agents is called the \u201csurplus sharing problem\u201d and is studied in [149].", "startOffset": 123, "endOffset": 128}, {"referenceID": 142, "context": "3In [149], surplus is defined as f(A)\u2212 \u2211 a\u2208A f(a) where f is a supermodular function, but the same idea still applies.", "startOffset": 4, "endOffset": 9}, {"referenceID": 99, "context": "4Incidentally, in 1954, [102] was also the first, to the authors knowledge, to provide inequalities on the entropy function that are identical to the submodularity condition.", "startOffset": 24, "endOffset": 29}, {"referenceID": 25, "context": "When f(A) = H(XA) is the entropy function, then the pairwise surplus I (2) f (A;B) is the well-known mutual information [27] between random variable sets XA and XB .", "startOffset": 120, "endOffset": 124}, {"referenceID": 38, "context": "This result is similar to some of the recent results from the DNN literature where it is shown that in some cases, it would require exponentially many hidden units to implement a network with more layers [40].", "startOffset": 204, "endOffset": 208}, {"referenceID": 58, "context": "This is different than standard neural networks where it is shown that even a shallow neural network is a universal approximator [61].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "Hence, f\u0302k is like a [0, 1]-normalized laminar matroid rank function with the laminar family of sets Fk = {Vk, Vk1, Vk2, Vk3, Vk11, Vk12, Vk13, Vk21, .", "startOffset": 21, "endOffset": 27}, {"referenceID": 29, "context": "Hence, all members of Fk are totally normalized in this sense [31, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 28, "context": "Hence, all members of Fk are totally normalized in this sense [31, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 29, "context": "The properties of total normalization [31, 30] will be further useful in the below, so we define functional operators that totally normalize a given function.", "startOffset": 38, "endOffset": 46}, {"referenceID": 28, "context": "The properties of total normalization [31, 30] will be further useful in the below, so we define functional operators that totally normalize a given function.", "startOffset": 38, "endOffset": 46}, {"referenceID": 29, "context": ", (T f)(v|V \\ {v}) = 0), and we have the identity f = T f +Mf , meaning that any submodular function can be decomposed into a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 190, "endOffset": 198}, {"referenceID": 28, "context": ", (T f)(v|V \\ {v}) = 0), and we have the identity f = T f +Mf , meaning that any submodular function can be decomposed into a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 190, "endOffset": 198}, {"referenceID": 38, "context": "It remains an open question to determine if, when the ground set size is constant and fixed, if DSFk comprises a larger family, or if expressing certain DSFks with k \u2212 1 layers requires an exponential number of hidden units, analogous to [40].", "startOffset": 238, "endOffset": 242}, {"referenceID": 155, "context": "It is also worth noting that in [163] it is shown that the entropy function f(A) = H(XA) when seen as a set function must satisfy inequalities that are not required for an arbitrary polymatroid function, thus implying that entropy also does not comprise all submodular function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 50, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 9, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 41, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 40, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 73, "context": "A general outline of various learning settings is given in [76, 43] \u2014 here, we give only a very brief overview.", "startOffset": 59, "endOffset": 67}, {"referenceID": 41, "context": "A general outline of various learning settings is given in [76, 43] \u2014 here, we give only a very brief overview.", "startOffset": 59, "endOffset": 67}, {"referenceID": 9, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 37, "endOffset": 41}, {"referenceID": 41, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 111, "endOffset": 119}, {"referenceID": 40, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 111, "endOffset": 119}, {"referenceID": 0, "context": "The quality of learning could be judged over all 2 points or over some fraction, say 1\u2212\u03b2, of the points, for \u03b2 \u2208 [0, 1].", "startOffset": 113, "endOffset": 119}, {"referenceID": 73, "context": "For example, in agnostic learning [76], we acknowledge that it might be difficult to show that learning is good relative to all of F (say due to noise) but still feasible to show that learning is good relative to the best within T .", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In [11], goodness is judged multiplicatively, meaning for a set A \u2286 V we wish that f\u0303(A) \u2264 f(A) \u2264 g(n)f(A) for some function g(n), and this is typically a probabilistic condition (i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In the PMAC (probably mostly approximately correct) model [11], we also \u201cmostly\u201d \u03b2 > 0 learn.", "startOffset": 58, "endOffset": 62}, {"referenceID": 133, "context": "Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function.", "startOffset": 88, "endOffset": 97}, {"referenceID": 89, "context": "Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function.", "startOffset": 88, "endOffset": 97}, {"referenceID": 143, "context": "This is true both for image [150] and document [92] summarization tasks.", "startOffset": 28, "endOffset": 33}, {"referenceID": 89, "context": "This is true both for image [150] and document [92] summarization tasks.", "startOffset": 47, "endOffset": 51}, {"referenceID": 89, "context": "There also has been some initial work on learnability bounds in [92].", "startOffset": 64, "endOffset": 68}, {"referenceID": 38, "context": "Even in cases where a DSF can be represented by an SCMM, DSFs may be a far more parsimonious representation of classes of submodular functions and hence a more efficient family over which to learn, analogous to results in DNNs showing the need for exponentially many hidden units for shallow networks to implement a network with more layers [40].", "startOffset": 341, "endOffset": 345}, {"referenceID": 128, "context": "Under this approach, and with an appropriate regularizer, it may be feasible to obtain generalization bounds in some form [135] as is often found in statistical machine learning settings.", "startOffset": 122, "endOffset": 127}, {"referenceID": 56, "context": "Note that, depending on the loss L used, this approach may be tolerant of noisy estimates of the function, where, say, yi = fw(Si) + and where is noise, somewhat analogous to how it is possible to optimize a noisy submodular function [59].", "startOffset": 234, "endOffset": 238}, {"referenceID": 89, "context": "One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size.", "startOffset": 45, "endOffset": 54}, {"referenceID": 143, "context": "One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size.", "startOffset": 45, "endOffset": 54}, {"referenceID": 0, "context": "More precisely, instead of trying to learn f everywhere, we seek only to learn the parameters w of a function so that if B \u2208 argmaxA\u2286V :|A|\u2264k fw(A), then h(B) \u2265 \u03b1h(A\u2217) for some \u03b1 \u2208 [0, 1] where A\u2217 \u2208 argmaxA\u2286V :|A|\u2264k h(A).", "startOffset": 181, "endOffset": 187}, {"referenceID": 133, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 89, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 143, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 139, "context": "The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is", "startOffset": 82, "endOffset": 92}, {"referenceID": 152, "context": "The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is", "startOffset": 82, "endOffset": 92}, {"referenceID": 133, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 89, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 143, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 89, "context": "Given a submodular function for the loss, as was done in [92], then the greedy algorithm offers the standard 1 \u2212 1/e approximation guarantee for LAI.", "startOffset": 57, "endOffset": 61}, {"referenceID": 106, "context": "If f\u0303 is submodular, then \u03ba \u2212 f\u0303 is supermodular, and in this case solving maxA\u22082V \\S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.", "startOffset": 211, "endOffset": 220}, {"referenceID": 61, "context": "If f\u0303 is submodular, then \u03ba \u2212 f\u0303 is supermodular, and in this case solving maxA\u22082V \\S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.", "startOffset": 211, "endOffset": 220}, {"referenceID": 115, "context": "For a DSF, this subgradient can be easily computed using backpropagation, similar to the approach of [121].", "startOffset": 101, "endOffset": 106}, {"referenceID": 35, "context": "Preliminary experiments in learning DSFs in this fashion were reported in [37] and show encouraging results.", "startOffset": 74, "endOffset": 78}, {"referenceID": 84, "context": "Given the ongoing research on the non-convex learning of DNNs, which have achieved remarkable results on a plethora of machine learning tasks [87, 54], and given the similarity between DSFs and DNNs, we may leverage the same DNN learning techniques to learn DSFs.", "startOffset": 142, "endOffset": 150}, {"referenceID": 89, "context": "The reason this occurs can be explained using a document summarization example [92].", "startOffset": 79, "endOffset": 83}, {"referenceID": 89, "context": "In this section, we discuss how to addresses this problem for DSFs via a strategy that generalizes [92, 150].", "startOffset": 99, "endOffset": 108}, {"referenceID": 143, "context": "In this section, we discuss how to addresses this problem for DSFs via a strategy that generalizes [92, 150].", "startOffset": 99, "endOffset": 108}, {"referenceID": 89, "context": "This process analogous to the \u201cshells\u201d of [92].", "startOffset": 42, "endOffset": 46}, {"referenceID": 106, "context": "In [110, 64] it was shown that any set function h : 2 \u2192 R can be represented as a difference between two submodular functions.", "startOffset": 3, "endOffset": 12}, {"referenceID": 61, "context": "In [110, 64] it was shown that any set function h : 2 \u2192 R can be represented as a difference between two submodular functions.", "startOffset": 3, "endOffset": 12}, {"referenceID": 63, "context": "For example, after learning, we can utilize submodular level-set constrained submodular optimization of the kind developed in [66] for optimization.", "startOffset": 126, "endOffset": 130}, {"referenceID": 105, "context": "Other ways to generalize submodularity considers discrete generalizations of properties such as midpoint convexity over integer lattices [109].", "startOffset": 137, "endOffset": 142}, {"referenceID": 132, "context": "1 (Simple Bisubmodularity [139]).", "startOffset": 26, "endOffset": 31}, {"referenceID": 119, "context": "3 (Directed Bisubmodularity [125]).", "startOffset": 28, "endOffset": 33}, {"referenceID": 77, "context": "Directed bisubodularity functions have been generalized to what is known as k-submodular functions in [80, 62].", "startOffset": 102, "endOffset": 110}, {"referenceID": 59, "context": "Directed bisubodularity functions have been generalized to what is known as k-submodular functions in [80, 62].", "startOffset": 102, "endOffset": 110}, {"referenceID": 132, "context": "More recently, simple bisubmodularity [139] has been generalized to multivariate submodular functions [134].", "startOffset": 38, "endOffset": 43}, {"referenceID": 127, "context": "More recently, simple bisubmodularity [139] has been generalized to multivariate submodular functions [134].", "startOffset": 102, "endOffset": 107}, {"referenceID": 59, "context": "These are not the same as k-submodular functions [62] but for k = 1 we obtain standard submodular functions and for k = 2 we obtain simple bisubmodular functions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 53, "context": ", [56]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 48, "context": "This section describes a strategy for learning hash functions that utilizes DSFs, the Lov\u00e1sz extension, and the submodular Hamming metric [50].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "\u2022 For two arbitrary vectors z1, z2 \u2208 [0, 1] , we can define a relaxed form of metric as follows: d(z1, z2) = f\u0306(z1 + z2 \u2212 2z1 \u2297 z2), and for a DSF, this can be expressed as df\u0306w(z1, z2) = f\u0306w(z1 + z2 \u2212 2z1 \u2297 z2).", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "\u2022 Let us suppose that h\u0303\u03b8 : R \u2192 [0, 1] is a mapping from real vectors to vectors in the hypercube (e.", "startOffset": 32, "endOffset": 38}, {"referenceID": 34, "context": "An immediate task is to further develop practical strategies for successfully empirically learning DSFs, as was initiated in [36].", "startOffset": 125, "endOffset": 129}, {"referenceID": 155, "context": "And lastly, it remains to compare the DSF family with the family of all entropy functions [163].", "startOffset": 90, "endOffset": 95}, {"referenceID": 34, "context": "Thanks to Brian Dolhansky for helping with building an initial implementation of learning DSFs that was used in [36].", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "We start with an overview of a class of submodular functions called SCMMs (sums of concave composed with non-negative modular functions plus a final arbitrary modular). We then define a new class of submodular functions we call deep submodular functions or DSFs. We show that DSFs are a flexible parametric family of submodular functions that share many of the properties and advantages of deep neural networks (DNNs), including many-layered hierarchical topologies, representation learning, distributed representations, opportunities and strategies for training, and suitability to GPU-based matrix/vector computing. DSFs can be motivated by considering a hierarchy of descriptive concepts over ground elements and where one wishes to allow submodular interaction throughout this hierarchy. In machine learning and data science applications, where there is often either a natural or an automatically learnt hierarchy of concepts over data, DSFs therefore naturally apply. Results in this paper show that DSFs constitute a strictly larger class of submodular functions than SCMMs, thus justifying their mathematical and practical utility. Moreover, we show that, for any integer k > 0, there are k-layer DSFs that cannot be represented by a k\u2032-layer DSF for any k\u2032 < k. This implies that, like DNNs, there is a utility to depth, but unlike DNNs (which can be universally approximated by shallow networks), the family of DSFs strictly increase with depth. Despite this property, however, we show that DSFs, even with arbitrarily large k, do not comprise all submodular functions. We show this using a technique that \u201cbackpropagates\u201d certain requirements if it was the case that DSFs comprised all submodular functions. In offering the above results, we also define the notion of an antitone superdifferential of a concave function and show how this relates to submodular functions (in general), DSFs (in particular), negative second-order partial derivatives, continuous submodularity, and concave extensions. To further motivate our analysis, we provide various special case results from matroid theory, comparing DSFs with forms of matroid rank, in particular the laminar matroid. Lastly, we discuss strategies to learn DSFs, and define the classes of deep supermodular functions, deep difference of submodular functions, and deep multivariate submodular functions, and discuss where these can be useful in applications.", "creator": "LaTeX with hyperref package"}}}