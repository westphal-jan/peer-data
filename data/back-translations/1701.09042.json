{"id": "1701.09042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms", "abstract": "Frequent itemset mining is a popular data mining technique. Apriori, Eclat, and FP Growth are among the most common algorithms for frequent itemset mining. Extensive research has been conducted to compare the relative performance between these three algorithms by evaluating the scalability of each algorithm as the dataset size increases. While scalability as the data size increases is important, previous work did not examine the effects of similarly sized datasets containing different itemset characteristics. In this paper, the effects that two datasets can have on the performance of these three common itemset algorithms are examined. To perform this empirical analysis, a dataset generator has been created to measure the effects of frequent itemdensity and maximum transaction size on performance. The datasets generated contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm's maximum transaction size. The research findings of both this research report and the Growth Report clearly show that both the frequency and the growth of the P are increasing.", "histories": [["v1", "Mon, 30 Jan 2017 12:34:02 GMT  (232kb,D)", "http://arxiv.org/abs/1701.09042v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.AI", "authors": ["jeff heaton"], "accepted": false, "id": "1701.09042"}, "pdf": {"name": "1701.09042.pdf", "metadata": {"source": "CRF", "title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms", "authors": ["Jeff Heaton"], "emails": ["jeffheaton@acm.org"], "sections": [{"heading": null, "text": "This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper\u2019s research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.\nI. INTRODUCTION\nThe research covered by this paper determines how the characteristics of a dataset might affect the performance of the Apriori, Eclat, and FP-Growth frequent itemset mining algorithms. These algorithms have several popular implementations[1], [2], [3]. The goal of this research is to determine the effects of basket size and frequent itemset density on the Apriori, Eclat, and FP-Growth algorithms. The research determined that these two dataset characteristics have a significant impact on performance of the algorithms.\nMost research into frequent itemset mining focuses upon the performance differences between frequent itemset algorithms on a single dataset[4]. The effects of hyper-paramaters, such as minimum support, upon the performance of frequent itemset mining algorithms has also been explored[5]. Some papers make use of common datasets from the UCI Machine Learning Repository[6]. Many papers make use of the IBM Quest Synthetic Data Generator[7] or some variant of it. Our paper makes use of a Python-based generator that is based on IBM\u2019s work[8].\nThis research evaluates the performance of the Apriori, Eclat and FP-Growth frequent itemset mining algorithms imple-\nmented by Christian Borgelt in 2012[9]. Though, association rule mining is a similar algorithm, this research is limited to frequent itemset mining. By limiting the experimentation to a single implementation of frequent itemset mining this research is able to evaluate how the characteristics of the dataset affect the performance of these algorithms."}, {"heading": "II. FREQUENT ITEMSET MINING", "text": "Frequent itemset mining was introduced as a means to find frequent groupings of items in a database containing baskets/transactions of these items[10]. The database is composed of a series of baskets that are analogous to orders placed by customers. These orders are individual baskets that are made up of some number of items. Companies, such as Amazon, Netflix and other online retailers, make use of frequent itemsets to suggest additional items that a consumer might want to purchase, based on their past purchasing history and the history of others with similar baskets[11]. The following data show baskets that might be used for frequent itemset mining, where each line represents a single basket of items.\n[ mp3player usb\u2212c h a r g e r book\u2212d c t book\u2212t h s ] [ mp3player usb\u2212c h a r g e r ] [ usb\u2212c h a r g e r mp3player book\u2212d c t book\u2212t h s ] [ usb\u2212c h a r g e r ] [ book\u2212d c t book\u2212t h s ]\nFrom the above baskets several frequent itemsets can be defined. These are sets of items that frequently occur together, some of which are:\n[ mp3player usb\u2212c h a r g e r ] [ book\u2212d c t book\u2212t h s ] . . .\nA simple visual analysis of the data show that the items mp3-player and usb-charger frequently occur together. Likewise, book-dct and book-ths also frequently occur together. Frequent itemset algorithms make use of a variety of statistics to determine which itemsets to include.\nar X\niv :1\n70 1.\n09 04\n2v 1\n[ cs\n.D B\n] 3\n0 Ja\nn 20"}, {"heading": "III. SURVEY OF APRIORI, ECLAT AND FP-GROWTH", "text": "There are a variety of different algorithms that are used to mine frequent itemsets. First, a simple naive brute-force algorithm to build frequent itemsets will be evaluated. This paper shows how Apriori, Eclat and FP-Growth address some of the shortcomings of the naive algorithm.\nAll four algorithms must calculate statistics about itemsets that might ultimately be included in the final collection of frequent itemsets. One statistic that is common to all four of these algorithms is support. The support of a candidate frequent itemset is the total count of how many of the database baskets support that candidate. A basket is said to cover a candidate itemset if the candidate is a subset or equal to the basket. Support is sometimes expressed as a percent of the total number of baskets in the database (N ) that cover a candidate itemset (X). The following formula calculates the support percentage of a candidate itemset:\nsupp(X) = Xcount\nN (1)\nThis equation can be applied to calculate the support for {mp3-player usb-charger} from the previously presented set of baskets.\nsupp({mp3-player usb-charger}) = 3 5 = 0.6 (2)\nThe support statistic of 0.6 indicates that 60% of the five baskets contain the candidate itemset {mp3-player usbcharger}. Most frequent itemset algorithms accept a minimum support parameter to filter out less common itemsets."}, {"heading": "IV. NAIVE ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "It is not difficult to extract frequent itemsets from basket data. It is, however, difficult to do so efficiently. For the algorithms presented here, let J represent a set of items, likewise, let D represent a database of baskets that are made up of those items. Algorithm 1 is a summarization of the naive frequent itemset algorithm provided by Garcia-Molina, Ullman, and Widom[12].\nAlgorithm 1 Naive Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items. 2: OUTPUT: The sets of itemsets F1, F2, . . . , Fq , where Fi\nis the set of all itemsets of size I that appear in at least s baskets of D.\n3: METHOD: 4: R\u2190 integer array, all item combinations in D, of size 2|D| 5: for n\u2190 1 TO |D| do 6: F \u2190 all possible set combinations from Dn 7: Increase each value in R, corresponding to each in F []\nreturn all itemsets, with R[] \u2265 s\nThe naive algorithm simply generates all possible itemsets, counts their support, and then discards all itemsets below some\nthreshold level of support. The constant S or \u03c3 typically represents the support threshold. Computing all possible itemsets is only an O(N) magnitude operation in all cases, where N is the number of baskets in the database. However, the naive algorithm would also need 2i memory cells to store all of these itemsets as the counts are generated, where i is the number of individual items. These memory cells would typically be 32 or 64-bit integers. This memory requirement means that the naive algorithm is impossible for anything but a trivial number of individual items. A computer with 128GB of available RAM would theoretically only be able to calculate 34 items, when using a 64-bit integer to hold the counts. When it is considered that N might be the total count of distinct items for sale by a retailer such as Walmart or Amazon it is obvious that the naive approach is not useful in practice."}, {"heading": "V. NAIVE ALGORITHM EXAMPLE", "text": "This section demonstrates how the naive algorithm would handle the example basket set given earlier in this paper. The total number of items contained in database, |J | is equal to four. Four items can be arranged a total of |J |2, or 16 different ways. However, because one of these frequent itemsets is the empty set, only the following 15 candidate itemsets are considered:\n[ book\u2212t h s ] [ book\u2212d c t ] [ book\u2212dc t , book\u2212t h s ] [ usb\u2212c h a r g e r ] [ usb\u2212c h a r g e r , book\u2212t h s ] [ usb\u2212c h a r g e r , book\u2212d c t ] [ usb\u2212c h a r g e r , book\u2212dc t , book\u2212t h s ] [ mp3player ] [ mp3player , book\u2212t h s ] [ mp3player , book\u2212d c t ] [ mp3player , book\u2212dc t , book\u2212t h s ] [ mp3player , usb\u2212c h a r g e r ] [ mp3player , usb\u2212c h a r g e r , book\u2212t h s ] [ mp3player , usb\u2212c h a r g e r , book\u2212d c t ] [ mp3player , usb\u2212c h a r g e r , book\u2212dc t , book\u2212t h s ]\nThe above itemsets are considered candidate frequent itemsets because it has not yet been determined if all of these candidates will be included in the final list of frequent itemsets. Once the candidate itemsets have been determined, the naive algorithm will pass over all baskets and count the support for each of the candidate itemsets. Candidate itemsets that are below the required support S will be purged. The naive algorithm would calculate support for each candidate as follows:\n[ book\u2212t h s ] ; s = 3 [ book\u2212d c t ] ; s = 3 [ book\u2212d c t book\u2212t h s ] ; s = 3 [ usb\u2212c h a r g e r ] ; s = 4 [ usb\u2212c h a r g e r , book\u2212t h s ] ; s = 2 [ usb\u2212c h a r g e r , book\u2212d c t ] ; s = 2 [ usb\u2212c h a r g e r , book\u2212dc t , book\u2212t h s ] ; s = 2 [ mp3player ] ; s = 3\n[ mp3player , book\u2212t h s ] ; s = 2 [ mp3player , book\u2212d c t ] ; s = 2 [ mp3player , book\u2212dc t , book\u2212t h s ] ; s = 2 [ mp3player , usb\u2212c h a r g e r ] ; s = 3 [ mp3player , usb\u2212c h a r g e r , book\u2212t h s ] ; s = 2 [ mp3player , usb\u2212c h a r g e r , book\u2212d c t ] ; s = 2 [ mp3player , usb\u2212c h a r g e r , book\u2212dc t ,\nbook\u2212t h s ] ; s = 2\nIt is necessary to store a count for every possible itemset when using the naive algorithm. Of course, once the support counts are determined, many of the frequent itemsets will be purged. Nevertheless, the fact that these values must be kept while the database is scanned for support means the naive algorithm requires considerable memory."}, {"heading": "VI. APRIORI ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "Agrawal and Srikant initially introduced the Apriori algorithm to provide performance improvements over a naive itemset search[13]. Apriori algorithm has been around almost as long as the concept of frequent itemsets and is very popular. The naive algorithm is a theoretical concept and is not used in practice. Aprioiri has become the classic implementation of frequent itemset mining. Aprioiri, as defined by Goethals (2003) is presented as Algorithm 2[14].\nAlgorithm 2 Apriori Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items, a support\nthreshold \u03c3. 2: OUTPUT: A list of itemsets F(D, \u03c3). 3: METHOD: 4: C1 \u2190 {{i}|i \u2208 J } 5: k \u2190 1 6: while Ck 6= {} do 7: # Compute the supports of all candidate itemsets 8: for all transactions {tid, I} \u2208 D do 9: for all candidate itemsets X \u2208 Ck do\n10: if X \u2286 I then 11: X.support++ 12: # Extract all frequent itemsets 13: Fk = {X|X.support > \u03c3} 14: # Generate new candidate itemsets 15: for all X,Y \u2208 Fi,X[i] = Y [i] for 1 \u2264 i \u2264 k \u2212\n1, and X[k] < Y [k] do 16: I = X \u222a {Y [k]} 17: if \u2200J \u2282 I, |J | = k : J \u2208 Fk then 18: Ck+1 \u2190 Ck+1 \u222a I 19: k++\nApriori is based on the hierarchical monotonicity of frequent itemsets between their supersets and subsets. As implied by monotonicity, a subset of a frequent itemset must also be frequent. Likewise, a superset of an infrequent itemset must also be infrequent[13]. This allows the Apriori algorithm to be implemented as a breadth-first search. Papers by Goethals\n(2003) and others do not represent Apriori\u2019s performance in terms of big-O notation[14]. This is likely due to the fact that Apriori\u2019s outer loops are bounded by the number of common prefixes and not some easily determined constants such as the number of items or the length of the dataset. Papers describing Apriori, Eclat, and FP-Growth rely on empirical comparison of algorithms rather than big-O analysis. However, analysis covered later in this paper does allow these three algorithms to be expressed in big-O based on average basket size and frequent itemset density.\nAprioiri first builds a list of all singleton itemsets with sufficient support. Building on the monotonicity principle, the next set of candidate frequent itemsets is built of combinations of the singleton itemsets. This process continues until the maximum length specified for frequent itemsets is reached. The evaluations performed by this research did not impose this maximum length.\nThe primary issue with the Apriori algorithm is that it is necessary to perform a scan of the database at every level of the breadth-first search. Additionally, candidate generation can lead to a great number of subsets and can become a significant memory requirement. Deficiencies in the Apriori algorithm led to the development of other, more efficient, algorithms, such as Eclat and FP-Growth."}, {"heading": "VII. APRIORI ALGORITHM EXAMPLE", "text": "This section will demonstrate how the Apriori algorithm handles the basket set given earlier in this paper. The Apriori algorithm performs a breadth first search of the itemsets. Figure 1 shows a segment of this search, for the items usbcable, mp3-player, and book-dct.\nThe candidate set starts empty, and begins by adding all singleton itemsets that have sufficient individual support. For simplicity, it is assumed that only usb-cable, mp3-player, and book-dct have sufficient support. The next layer is built of combinations of the previous layer that had sufficient support. For simplicity, it is also assumed that all three combinations\nhad sufficient support. Finally, a triplet itemset with all three items is evaluated."}, {"heading": "VIII. ECLAT ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "Eclat was introduced by Zaki, Parthasarathy, Ogihara, and Li in 1997[15]. Eclat is an acronym for Equivalence Class Clustering and bottom up Lattice Traversal. The primary difference between Eclat and Apriori is that Eclat abandons Apriori\u2019s breadth-first search for a recursive depth-first search. Eclat, as defined by Goethals (2003) is presented as Algorithm 3[14].\nAlgorithm 3 Eclat Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items, a support\nthreshold \u03c3, and an item prefix I , such that I \u2286 J . 2: OUTPUT: A list of itemsets F [I](D, \u03c3) for the specified\nprefix. 3: METHOD: 4: F [i]\u2190 {} 5: for all i \u2208 J occurring in D do 6: F [I] := F [I] \u222a {I \u222a {i}} 7: # Create Di 8: Di \u2190 {} 9: for all j \u2208 J occurring in D such that j > i do\n10: C \u2190 cover({i}) \u2229 cover({j}) 11: if |C| \u2265 \u03c3 then 12: Di \u2190 Di \u222a {j, C} 13: # Depth-first recursion 14: Compute F [I \u222a i](Di, \u03c3) 15: F [I] := F [I] \u222a F [I \u222a i]\nThe input parameters to Eclat are slightly different than Apriori in that a prefix I is provided. This prefix specifies the prefix pattern that must be present in any itemsets found by the call to Eclat. This change allows a depth-wise recursive building of the itemsets. The initial call to Eclat uses an I value of {}, meaning that no specific prefix is required. This initial call would find all single-item frequent itemsets. The Apriori algorithm would then recursively call itself, each time increasing I by adding itemsets that contain the value I that the function was called with, but are one item longer. This process would continue until the value of I had grown to sufficient length that the algorithm has traversed to baskets of all lengths. Like Apriori, Eclat is not usually expressed in big-O terms; however, results obtained from this research\u2019s experimentation show how frequent itemset density and basket allow these algorithms to be expressed in terms of big-O computational cost.\nThere are several different methods for storing the support values in the recursive Eclat algorithm. The most common approach is to use a structure called a trie. This is the approach used by Borgelt (2012) to implement the versions of Apriori, Eclat and FP-Growth investigated in this research paper[9]. A trie graph always contains an empty root node. As itemsets\nare encountered, they are added to the trie by inserting a node for each item that makes up the itemset. The left-most item corresponds to a child of the root node. The second item corresponds to a child of the first item of this frequent set. No parent would ever have more than one child of the same item name; however, an item name may appear at multiple locations in the trie.\nThe trie is generated so that the algorithm can quickly find the support of an itemset by traversing the trie as the items in the set are read left-to-right. The node that contains the right-most item contains the support for that itemset. As the algorithm processes the database the trie is traversed looking for each itemset discovered. Nodes are created, if necessary, to fill out the trie to hold all itemsets. If the nodes already exist, the node for the right-most item in the itemset has its support increased. New nodes start with a support of 1. This allows Eclat to use less memory than Apriori, because the core branches of the trie allow heavily used subsets to be stored only once. Theoretically, a trie could be used with Apriori, however, the breadth-first nature of Apriori would typically require too much memory."}, {"heading": "IX. ECLAT ALGORITHM EXAMPLE", "text": "This section will demonstrate how the Eclat algorithm would handle the basket set given earlier in this paper. A portion of the trie built by Eclat is shown in Figure 2.\nThe above trie portion encodes a total of 7 different frequent itemset\u2019s support values. To find a particular frequent itemset\u2019s support simply traverse the graph, following the items from left to right. The frequent itemset mp3-player, usb-charger would have a support value of 3. Similarly, the frequent itemset mp3-player, usb-charger, book-dct would have a support value of 2. Once the algorithm completes, the trie is traversed, and all frequent itemsets are extracted from it."}, {"heading": "X. FP-GROWTH ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "Frequent pattern growth (FP-Growth) was introduced by Han, Pei, and Yin in 2000 to forego candidate generation altogether[16]. This is done by using a trie to store the actual baskets, rather than storing candidates like Apriori and Eclat\ndo. Aprori is very much a horizontal, breadth-first, algorithm. Similarly, Eclat is very much a vertical, depth-first, algorithm. The trie structure of FP-Growth provides a vertical view of the data. However, FP-Growth also adds a header table for every individual item that has support above the threshold support level. This header table contains a linked-list through the trie to connect every node of the same type. The header table gives FP-Growth a horizontal view of the data, in addition to the vertical view provided by the trie.\nThe algorithm for FP-Growth is similar to Eclat in that it was not expressed in terms of big-O analysis. FP-Growth, as defined by Goethals, is presented as Algorithm 4[14].\nAlgorithm 4 FP-Growth Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items, a support\nthreshold \u03c3, and an item prefix I , such that I \u2286 J . 2: OUTPUT: A list of itemsets F [I](D, \u03c3) for the specified\nprefix. 3: F [i]\u2190 {} 4: for all i \u2208 J occurring in D do 5: F [I]\u2190 F [I] \u222a {I \u222a {i} 6: # Create Di 7: Di \u2190 {} 8: H \u2190 {} 9: for all j \u2208 J occurring in D such that j > i do\n10: if support(I \u222a {i, j}) \u2265 \u03c3 then 11: H \u2190 H \u222a {j} 12: for all (tid,X) \u2208 D with I \u2208 X do 13: Di \u2190 Di \u222a {(tid,X \u2229H)} 14: # Depth-first recursion 15: Compute F [I \u222a {i}](Di, \u03c3) 16: F [I]\u2190 F [I] \u222a F [I \u222a {i}]"}, {"heading": "XI. FP-GROWTH ALGORITHM EXAMPLE", "text": "This section will demonstrate how the FP-Growth algorithm would handle the basket set given earlier in this paper. Figure 3 shows a portion of the FP-Growth trie and the header table generated for the earlier example data.\nThis figure demonstrates the horizontal and vertical nature of the FP-Growth algorithm. The trie, on the right, holds the\nencoded baskets, along with their supports. The header, on the left, holds the items and provides horizontal access to the data."}, {"heading": "XII. EMPIRICAL COMPARISON OF APRIORI, ECLAT AND FP-GROWTH", "text": "There are a number of papers that compare the computational performance of Apriori, Eclat and FP-Growth. These papers are typically focused primarily on comparing the differences between the algorithms on one or more datasets and at different support thresholds. Papers by Borgelt (2012) and Goethals (2003) are examples of papers that compare various implementations of Apriori, Eclat and FP-Growth[9], [14].\nThis paper attempts a different approach. The goal of this paper is to see what effect the dataset has on the algorithm. The average basket size and frequent item density will be used as independent variables to evaluate total processing time as the dependent variable. Apriori, Eclat and FP-Growth will each be evaluated independently to see which performs best at different basket sizes and frequent itemset densities. Performance will be measured as a shorter total runtime. This paper focuses on a single implementation of these algorithms using the implementations of Apriori, Eclat and FP-Growth by Borgelt in 2012[9]."}, {"heading": "XIII. DATASET GENERATION", "text": "Generated datasets are used to perform this evaluation. This generated data allows the two independent variables to be adjusted to create a total of 20 different datasets to perform the evaluations. The datasets were generated using a simple Python script created for this paper that can be found on GitHub[8]. This Python script accepts the following parameters to specify the dataset to produce: \u2022 Transaction/Basket count: 10 million default \u2022 Number of items: 50,000 default \u2022 Number of frequent sets: 100 default \u2022 Max transaction/basket size: independent variable, 5-100\nrange \u2022 Frequent set density: independent variable, 0.1 to 0.8\nrange While basket count, number of frequent sets, and number of items can easily be varied in the script, for the purposes of this paper they will remain fixed at the above values. Through informal experimentation it was determined that the basket count only had a small positive correlation to processing time. The number of items did not appear to have a meaningful impact on processing time when varied in isolation. It was observed that the strongest correlation to processing time was through variation of the maximum basket size and frequent set density.\nThe following listing shows the type of data generated for this research. Here an example file was created with 10 baskets, out of 100 items, 2 frequent itemsets, maximum basket size of 10, and a density of 0.5.\nI36 I94 I71 I13 I91 I89 I34\nF6 F5 F3 F4 I86 I39 I16 I49 I62 I31 I54 I91 I22 I31 I70 I85 I78 I63 F4 F3 F1 F6 F0 I69 I44 I82 I50 I9 I31 I57 I20 F4 F3 F1 F6 F0 I87\nAs you can see from the above file, the items are either prefixed with \u201cI\u201d or \u201cF\u201d. The \u201cF\u201d prefix indicates that this line contains one of the frequent itemsets. Items with the \u201cI\u201d prefix are not part of an intentional frequent itemset. Of course, \u201cI\u201d prefixed items might form frequent itemsets, as they are uniformly sampled from the number of items to fill out nonfrequent itemsets. Each basket will have a random size chosen, up to the maximum basket size. The frequent itsemset density specifies the probability of each line containing one of the intentional frequent itemsets. Because a density of 0.5 was used, approximately half of the lines above contain one of the two intentional frequent itemsets. A frequent itemset line may have additional random \u201cI\u201d prefixed items added to cause the line to reach the randomly chosen length for that line. If the chosen frequent itemset does cause the generated line to exceed its randomly chosen length, no truncation will occur. The intentional frequent itemsets are all chosen to be less than or equal to the maximum basket size."}, {"heading": "XIV. EFFECTS OF DATASET DENSITY", "text": "Dataset density specifies the percentage of baskets that intentionally contain frequent itemsets. As frequent itemset density increases so does the processing time of Apriori, Eclat and FP-Growth as shown by Figure 4.\nThis chart shows the results of running 10 million baskets, with an average basket size of 50, at various densities. The Eclat and FP-Growth algorithms both show very similar growth as frequent itemset density increases. The Apriori algorithm also performs very similarly to Eclat and FP-Growth, until the density surpasses 70%. As previously mentioned in this paper, Apriori has considerably larger memory needs than the other algorithms. At 70% Apriori had allocated all of the\ntest machine\u2019s 8 gigabytes of RAM. This made swapping to physical storage necessary and had a drastic impact on the algorithm\u2019s runtime. It is also interesting to note that Eclat is marginally ahead of FP-Growth at low densities. This ranking reverses at higher densities. Between 10% and 70% all three algorithms exhibit approximately an O(NlogN) complexity. Beyond 70% Apriori approached O(N2) and worse complexity, where N is the number of actual frequent items in the database."}, {"heading": "XV. EFFECTS OF BASKET SIZE", "text": "Basket size specifies the maximum number of items per basket line. Larger basket sizes mean that the frequent itemsets will also be larger. This increases the sizes of the data structures used to hold these itemsets. These larger data structures require more memory for storage and greater processing time to traverse them. Figure 5 illustrates the effect of increasing transaction sizes on the performance of the three algorithms.\nThis chart shows the results of running 10 million baskets, with a frequent itemset density of 50%, at various max basket sizes. The three algorithms show almost exactly the same O(N) performance for basket sizes up through 60. Once above 60, Apriori seems to grow much quicker than the other two. This is possibly because of the increased memory used by Apriori. Interestingly, Apriori performed the best between 60- 70 maximum transaction sizes. Further research is needed to determine why Apriori is superior in this small range."}, {"heading": "XVI. CONCLUSIONS", "text": "Apriori is an easily understandable frequent itemset mining algorithm. Because of this, Apriori is a popular starting point for frequent itemset study. However, Apriori has serious scalability issues and exhausts available memory much faster than Eclat and FP-Growth. Because of this Apriori should not be used for large datasets.\nMost frequent itemset applications should consider using either FP-Growth or Eclat. These two algorithms performed similarly for this paper\u2019s research, though FP-Growth did show slightly better performance than Eclat. Other papers also recommend FP-Growth for most cases[9]. Frequent itemset\nmining is an area of active research. New algorithms, as well as modifications of existing algorithms are often introduced. For an application where performance is critical, it is important to evaluate the dataset with newer algorithms as they are introduced, and shown to have better performance than FPGrowth or Eclat."}], "references": [{"title": "Frequent pattern mining: Current status and future directions", "author": ["J. Han", "H. Cheng", "D. Xin", "X. Yan"], "venue": "Data Mining Knowledge Discovery, vol. 15, no. 1, pp. 55\u201386, Aug. 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, vol. 11, no. 1, pp. 10\u201318, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "arules \u2013 A computational environment for mining association rules and frequent item sets", "author": ["M. Hahsler", "B. Gruen", "K. Hornik"], "venue": "Journal of Statistical Software, vol. 14, no. 15, pp. 1\u201325, October 2005. [Online]. Available: http://www.jstatsoft.org/v14/i15/", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Mafia: A maximal frequent itemset algorithm for transactional databases", "author": ["D. Burdick", "M. Calimlim", "J. Gehrke"], "venue": "Proceedings of the 17th International Conference on Data Engineering, 2001. IEEE, 2001, pp. 443\u2013452.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Real world performance of association rule algorithms", "author": ["Z. Zheng", "R. Kohavi", "L. Mason"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 401\u2013406.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/$\\sim$mlearn/{MLR} epository.html", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Market basket synthetic data generator", "author": ["A. Pitman"], "venue": "2011, http://mloss. org/software/view/294/.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Jeff Heaton\u2019s GitHub Repository - Conference/Paper Source Code", "author": ["J Heaton"], "venue": "https://github.com/jeffheaton/papers, accessed: 2016-01-31.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Frequent item set mining", "author": ["C. Borgelt"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 2, no. 6, pp. 437\u2013456, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "ACM SIGMOD Record, vol. 22, no. 2. ACM, 1993, pp. 207\u2013216.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Mining of massive datasets", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Database systems: the complete book", "author": ["H. Garcia-Molina"], "venue": "Pearson Education India,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fast algorithms for mining association rules", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proceedings of the 20th international conference of very large data bases, VLDB, vol. 1215, 1994, pp. 487\u2013499.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Survey on frequent pattern mining", "author": ["B. Goethals"], "venue": "University of Helsinki, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "New algorithms for fast discovery of association rules", "author": ["M.J. Zaki", "S. Parthasarathy", "M. Ogihara", "W. Li"], "venue": "KDD, vol. 97, 1997, pp. 283\u2013 286.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Mining frequent patterns without candidate generation", "author": ["J. Han", "J. Pei", "Y. Yin"], "venue": "ACM SIGMOD Record, vol. 29, no. 2. ACM, 2000, pp. 1\u201312.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Most research into frequent itemset mining focuses upon the performance differences between frequent itemset algorithms on a single dataset[4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "The effects of hyper-paramaters, such as minimum support, upon the performance of frequent itemset mining algorithms has also been explored[5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "Some papers make use of common datasets from the UCI Machine Learning Repository[6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Many papers make use of the IBM Quest Synthetic Data Generator[7] or some variant of it.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "makes use of a Python-based generator that is based on IBM\u2019s work[8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "and FP-Growth frequent itemset mining algorithms implemented by Christian Borgelt in 2012[9].", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "Frequent itemset mining was introduced as a means to find frequent groupings of items in a database containing baskets/transactions of these items[10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Companies, such as Amazon, Netflix and other online retailers, make use of frequent itemsets to suggest additional items that a consumer might want to purchase, based on their past purchasing history and the history of others with similar baskets[11].", "startOffset": 246, "endOffset": 250}, {"referenceID": 11, "context": "Algorithm 1 is a summarization of the naive frequent itemset algorithm provided by Garcia-Molina, Ullman, and Widom[12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "Agrawal and Srikant initially introduced the Apriori algorithm to provide performance improvements over a naive itemset search[13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Aprioiri, as defined by Goethals (2003) is presented as Algorithm 2[14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Likewise, a superset of an infrequent itemset must also be infrequent[13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Papers by Goethals (2003) and others do not represent Apriori\u2019s performance in terms of big-O notation[14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Eclat was introduced by Zaki, Parthasarathy, Ogihara, and Li in 1997[15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Eclat, as defined by Goethals (2003) is presented as Algorithm 3[14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "This is the approach used by Borgelt (2012) to implement the versions of Apriori, Eclat and FP-Growth investigated in this research paper[9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "Frequent pattern growth (FP-Growth) was introduced by Han, Pei, and Yin in 2000 to forego candidate generation altogether[16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "FP-Growth, as defined by Goethals, is presented as Algorithm 4[14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "Papers by Borgelt (2012) and Goethals (2003) are examples of papers that compare various implementations of Apriori, Eclat and FP-Growth[9], [14].", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "Papers by Borgelt (2012) and Goethals (2003) are examples of papers that compare various implementations of Apriori, Eclat and FP-Growth[9], [14].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "This paper focuses on a single implementation of these algorithms using the implementations of Apriori, Eclat and FP-Growth by Borgelt in 2012[9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "The datasets were generated using a simple Python script created for this paper that can be found on GitHub[8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Other papers also recommend FP-Growth for most cases[9].", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Frequent itemset mining is a popular data mining technique. Apriori, Eclat, and FP-Growth are among the most common algorithms for frequent itemset mining. Considerable research has been performed to compare the relative performance between these three algorithms, by evaluating the scalability of each algorithm as the dataset size increases. While scalability as data size increases is important, previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper\u2019s research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.", "creator": "LaTeX with hyperref package"}}}