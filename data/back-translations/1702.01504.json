{"id": "1702.01504", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Optimizing Cost-Sensitive SVM for Imbalanced Data :Connecting Cluster to Classification", "abstract": "Class imbalances are one of the challenging problems for machine learning in many real-world applications, such as monitoring coal and gas accidents: the early warning data is extremely smaller than normal data, but what we really focus on. Cost-sensitive adaptation is a typical algorithmic-level method that withstands the imbalance of data sets. However, for SVM's classifier, which is modified to contain different penalty parameters (C) for each of the sample groups considered, the C value is empirically determined or calculated using the evaluation metrix, which has to be calculated iteratively and time-consumingly. In this paper, a novel cost-sensitive SVM method is presented, the penalty parameter C of which has been optimized based on the cluster probability density function (PDF) and the cluster PDFs, and is estimated only on the similarity matrix and some predefined hyperparameters. Experimental results on different benchmark data sets and imbalances proposed in the world with the method are cost-sensitive.", "histories": [["v1", "Mon, 6 Feb 2017 06:14:29 GMT  (733kb)", "http://arxiv.org/abs/1702.01504v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qiuyan yan", "shixiong xia", "fanrong meng"], "accepted": false, "id": "1702.01504"}, "pdf": {"name": "1702.01504.pdf", "metadata": {"source": "CRF", "title": "Optimizing Cost-Sensitive SVM for Imbalanced Data :Connecting Cluster to Classification", "authors": ["Qiuyan Yan", "Fanrong Meng"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n01 50\n4v 1\n[ cs\n.L G\n] 6\nF eb"}, {"heading": "1 Introduction", "text": "Support vector machines (SVMs) is a popular machine learning technique, owning to its theoretical and practical advantages, which has been successfully employed in many real-world application domains. However, when shifting to imbalanced dataset, SVM will produce an undesirable model that is biased toward the majority class and has low performance on the minority class. Imbalanced learning not only presents significant new challenges to the data research community [1,2,3] but also raises many critical questions in real-world data intensive applications.In fact, in most imbalanced learning problems, the misclassification error of the minority class is far costlier than that of the majority class. Following we will give an example to clarify this problem.\nFigure 1 is the mine and gas burst electromagnetic monitoring data for one month. The red line means taking place mine and gas burst accident on the date of 05/03/2011. One day before the accident, the blue line, means the burst premonition data, which is the object we focused on. If we can classify correctly the premonition data from the other normal data, we can pre-alarm the burst accident and avoid the accident taking place. From the Fig. 1, one month electromagnetic monitoring data is 7989, and there are only 210 premonition data. If we set the premonition data is positive class, and the other normal data is negative class, then the Imbalance Ratio, equals to the ratio of negative class data number and positive class data number, that is 37.04. In fact, the\n2\nmine and gas burst accident rarely take place, so the imbalance ratio even larger. For this burst electromagnetic monitoring data, we expect positive class and negative class can be classified correctly under this extreme imbalance ratio.\nSVMs classifier resist the data imbalance from three categories: Data level, Algorithm level and Hybrid methods. Data-level methods mainly includes oversampling and undersampling, which may have the danger of hardly maintain the original minor data distribution and also may lose the important majority data information seperately. Algorithm-level methods concentrate on modifying existing learners to alleviate their bias towards majority groups. Cost-sensitive methods is a typical representation of this category. The main problem is it is difficult to set the actual values in the cost matrix and often they are not given by expert beforehand in many real-life problems. The penalty parameter(C) is determined empirically, or is calculated according to the evaluation metric, which need to be computed iteratively and time consuming. Decision threshold adjustment is another kind of algorithm-level solution for dealing with class imbalance. Unlike the other correction techniques, decision threshold adjustment strategy runs after modeling a classifier. However, the existing decision threshold adjusting approaches generally give the moving distance of classification boundary empirically, but which cannot answer the question that how far the classification hyperplane should be moved towards the majority class. Hybrid methods usually combine the algorithmlevel methods and data-level methods to extract their strong points and reduce their weakness, which can be applied to arbitrary classifier and not only restrict to SVMs.\nFrom the other side, recently, one interesting directions indicates that imbalance ratio is not the sole source of learning difficulties. Even if the disproportion is high, but both classes are well represented and come from non-overlapping distribution we may obtain good classification rates using canonical classifiers. Nevertheless, a good\n3 imbalance learning algorithm should fully understand and exploit the minority class structure.\nThe cluster probability density function(PDF) can naturedly express the class structure and distribution.In this paper, we proposed an cluster Probability optimized CSSVM method ,named as PCS-SVM ,whose C value determined by the cluster probabilistic density function.We adjust the hyperplane through optimization the value of C for the positive class data, that is adjustment the upper bound of \u03b1 . First, we introduce the L2 norm to optimization function, the dual Lagrangian form of the modified objective function includes the parameter of C+ and C\u2212. Compared to the traditional SMO algorithm results, the new \u03b1 expression function is only different from the similarity expression of support vector x and y. Then, we construct the connection between similarity and the cluster probability of two points. This is an important measure differentiate the probability density of majority class and minority class. Last, selection two opposite support vector, one is positive but false classified to negative, the other is true negative. By correcting a false negative support vector to a true positive support vector according to cluster possibility, achieve the decision boundary removing toward the majority (negative) class."}, {"heading": "2 Related works", "text": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8]. Resampling can be accomplished either by oversampling the minority class or undersampling the majority class. However, both sampling techniques have their advantages and disadvantage. Oversampling makes the classifier overfitting and increases the time of modeling, while undersampling often causes information loss.In order to bypass this problem, [4] proposed a Model-Based Oversampling method for imbalanced time series data considering the sequence structure when oversampling. Considering data structure and distribution is the trend of this area.\nThe Different Error Costs (DEC) method is a cost-sensitive learning solution proposed in [9] to overcome the same cost (i.e. C) for both positive and negative misclassification in the penalty term. As given in Equation (1)\nmin( 1 2 \u2016w\u20162 +C+ \u2211\nyi=+1 \u03bei +C\u2212 \u2211 yi=\u22121 \u03bei) (1)\ns.t.\n{\nyi \u2212 (wT xi + b)> 1\u2212 \u03bei \u03bei > 0\nIn this method, the SVM soft margin objective function is modified to assign two misclassification costs, C+ and C\u2212 is the misclassification cost for positive and negative class examples separately. As a rule of thumb, [10] have reported that reasonably good classification results could be retained from the DEC method by setting the C\u2212/C+ equal to the minority-to-majority class ratio. One-class Learning [11] trained an SVM model only with the minority class examples. [12] assigned C\u2212 = 0 and C+ = 1/N+, these methods have been observed to be more effective than general data rebalancing\n4 methods. zSVM is another algorithm modification proposed for SVMs in [13] to learn from imbalance datasets, which is an typical decision threshold adjustment method. In this method, first an SVM model is developed by using the original imbalanced training dataset. Then, the decision boundary of the resulted model is modified to remove its bias toward the majority class. In zSVM method, the magnitude of the \u03b1+i is increased by multiplying all of them by a particular value z. Then, the modified SVM decision function can be represented as follows:\nf (x) = sign(z\u2217 N1\n\u2211 i=1\n\u03b1+i yiK(xi,x)+ N2\n\u2211 i=1 \u03b1\u2212i yiK(xi,x)+b) (2)\nThe z value is optimized by gradually increasing the value of z from 0 to some positive value, M, and G-mean is adopted as the evaluation measure to determine the optimal z. The speed and efficiency of zSVM determined by the step length from 0 to M."}, {"heading": "3 Proposed method", "text": "According to DEC method, the optimized objective function has two loss terms:\nmin( 1 2 \u2016w\u20162 +C+ \u2211\nyi=+1 \u03bei +C\u2212 \u2211 yi=\u22121 \u03bei) (3)\ns.t.\n{\nyi \u2212 (wT xi + b)> 1\u2212 \u03bei \u03bei > 0\nIf introduce L2 norm regularization item of slack factor, Equation (3) can be converted into the Equation (4):\n1 2 \u2016w\u20162 +C+ \u2211\nyi=+1 \u03be 2i +C\u2212 \u2211 yi=\u22121 \u03be 2i (4)\ns.t.\n{\nyi \u2212 (wT xi + b)> 1\u2212 \u03bei \u03bei > 0\nThe dual Lagrangian form of the modified objective function is:\nLp = p\n\u2211 i=1\n\u03b1i \u2212 \u2016w\u20162\n2 +C+\np\n\u2211 {i|yi=+1} \u03be 2i +C \u2212\np\n\u2211 {i|yi=\u22121} \u03be 2i\n\u2212 p\n\u2211 i=1\n\u03b1i[yi(w \u00b7 xi + b)\u2212 1+ \u03bei]\u2212 p\n\u2211 i=1 \u00b5i\u03bei (5)\n5 Where \u03b1i > 0, and \u00b5i > 0. Partial deviation formula (5):\n\u2202L \u2202w = \u2016w\u2016\u2212 p \u2211 i=1 \u03b1iyixi = 0 \u21d2\u2016w\u2016= p \u2211 i=1 \u03b1iyixi (6)\n\u2202L \u2202b = p \u2211 i=1 \u03b1iyi = 0 (7)\n\u2202L \u2202\u03bei =C+i \u2211 yi=+1 \u03bei +C\u2212i \u2211 yi=\u22121 \u03bei \u2212 p \u2211 i=1 \u03b1i \u2212 p \u2211 i=1 \u00b5i = 0 (8)\nSubstituting (6)\u2013(8) into (5) yields the following:\nLp = p\n\u2211 i=1\n\u03b1i \u2212 \u2016w\u20162\n2 +C+\np\n\u2211 {i|yi=+1}\n\u03be 2i +C\u2212 p\n\u2211 {i|yi=\u22121} \u03be 2i\n\u2212 p\n\u2211 i=1\n\u03b1i[yi(w \u00b7 xi + b)\u2212 1+ \u03bei]\u2212 p\n\u2211 i=1 \u00b5i\u03bei\n= p\n\u2211 i=1 \u03b1i \u2212 1 2\np\n\u2211 i=1 \u03b1i\u03b1 jyiy jxTi x j +C +\np\n\u2211 {i|yi=+1} \u03be 2i +C \u2212\np\n\u2211 {i|yi=\u22121}\n\u03be 2i \u2212 p\n\u2211 i=1 (\u03b1i + \u00b5i)\u03bei\n(9)\nAccording to KKT condition, p \u2211\ni=1 \u00b5i\u03bei = 0, for \u03bei 6= 0, so \u00b5i = 0. Based on Equation (9),\nwe can get p\n\u2211 i=1\n\u03b1i = 2C+ p\n\u2211 yi=+1\n\u03bei + 2C\u2212 p\n\u2211 yi=\u22121 \u03bei (10)\nSubstituting (10) into (9) yields the following\nLp = p\n\u2211 i=1 \u03b1i \u2212 1 2\np\n\u2211 i=1\n\u03b1i\u03b1 jyiy jxTi x j \u2212C+ p\n\u2211 {i|yi=+1}\n\u03be 2i \u2212C\u2212 p\n\u2211 {i|yi=+1} \u03be 2i\n= p\n\u2211 i=1 \u03b1i \u2212 1 2\np\n\u2211 i=1 \u03b1i\u03b1 jyiy jxTi x j \u2212\np \u2211\n{i|yi=+1} \u03b12i\n4C+ \u2212\np \u2211\n{i|yi=\u22121} \u03b12i\n4C\u2212 (11)\nAccording to SMO algorithm, the original object function including L1 norm regularization item of slack factor for Equation (3) can be described as:\n\u03b1new2 = y2[y2 \u2212 y1 + y1\u03b3(K11 \u2212K12)+ v1 \u2212 v2]\nK11 +K22 \u2212 2K12\n= \u03b1old2 + y2(E1 \u2212E2)\nK , (12)\nWhere Ei = f (xi)\u2212yi,K =K11+K22\u22122K12. Ki j = p \u2211\ni=1 xTi x j, f (xi) =\np \u2211\ni=1 \u03b1iyiKi j +b,vi =\nf (xi)\u2212 2 \u2211 j=1 \u03b1 jy jKi j \u2212 b. When Lp = p \u2211 i=1 \u03b1i\u2212 12 p \u2211 i=1 \u03b1i\u03b1 jyiy jxTi x j \u2212\np \u2211\n{i|yi=+1} \u03b12i\n4C+ \u2212\np \u2211\n{i|yi=\u22121} \u03b12i\n4C\u2212 ,\n6 we select two opposite symbol support vector to modify Lp expression: One is false negative support vector, the other is true positive support vector. Under this condition, given \u03b11 is negative and \u03b12 is positive, the optimization function turns into:\nL(\u03b12) = \u03b3 \u2212 s\u03b12 +\u03b12 \u2212 1 2 (\u03b3 \u2212 s\u03b12)2K11 \u2212 1 2 \u03b122 K22 \u2212 s(\u03b3 \u2212 s\u03b12)\u03b12K12\n\u2212 y1(\u03b3 \u2212 s\u03b12)v1 \u2212 y2\u03b12v2 + constant\u2212 \u03b121\n4C\u2212 \u2212 \u03b122 4C+\n(13)\nThen in order to simplicity, we give C\u2212 = 1, and due to \u03b11 = \u03b3 \u2212 s\u03b12, s = y1 \u2217 y2 = \u22121, derivate of Equation (13), the results is\n\u2212 s+ 1+ s\u03b3K11\u2212K11\u03b12 \u2212\u03b12K22 \u2212 s\u03b3K12 + 2\u03b12K12 + sy1v1 \u2212 y2v2 \u2212 \u03b12\n2C+ \u2212 \u03b12 2 = 0\n\u21d2 \u03b1new2 = y2(E1 \u2212E2)+\u03b1old2 (K11 +K22 \u2212 2K12)\nK11 +K22 \u2212 2(K12 \u2212 1\n4C+ \u2212 1 4 )\n(14)\nCompared to Equation (11), the expression of \u03b1new2 has the change in denominator:\nFrom K11 +K22 \u2212 2K12 to K11 +K22 \u2212 2(K12 \u2212 1\n4C+ \u2212 1 4 )\nFrom the Equation (14), we can conclude that considering the cost of false positive classification can by means of adjusting the value of similarity matrix, that is, update the similarity of K12 to Knew12 = K old 12 \u2212 1 4C+ \u2212 1 4 . Through this adjustment, the new value of \u03b12 will decrease, under the condition of the other \u03b1 unchanged, \u2016w\u2016= p \u2211\ni=1 \u03b1iyixi will\nalso decrease, that is, the max margin of hyperplane will increase. As the positive support vector we selected are unchanged, increasing of max margin means the hyperplane move towards the majority data. Then, we will discuss how to determine the value of Knew12 .\nIn Fig. 2, green points are the minority (positive) class, and the yellow points are the majority (negative) class. At the first, in order to maintain the correctness of majority class, the hyperplane is bias against the minority, whereas the false negative rate is high. Based on the first classifier model, we select two nearest support vectors, one is false negative (point A, in red color), and the other is true positive (point B, in green color). For the old hyperplane (shown as in Fig. 2(a)), point A is more near to the negative point according to the similarity function than the other positive ones. If we can adjust the similarity of point A and its nearest true positive neighbor (such as point B), let the similarity larger than the similarity of point B and its nearest true positive neighbor (such as point C), then the new hyperplane (shown as in Fig. 2(b)) will classify the point A correctly.\nThe next problem is how to determine the new Knew12 . We only know that using the old learning model, A and B are classified as different class, and B and C are classified as the same class. If we suppose the probabilistic density function of minority cluster is P1(Ki j), and that of different cluster is P0(Ki j), then the probability of B and C belong to the minority cluster is pBC = P1(KBC), and the probability of A and B belong to the\ndifferent cluster is pAB = P0(KAB), the probability of A and B belong to the minority cluster is pnewAB = P1(KAB). Then we can get the Equation (15)\nKAB KnewAB \u221d pAB pnewAB , KAB KBC \u221d pAB pBC\n(15)\nUnder the same probabilistic density function P1(Ki j), if we hope pnewAB > pBC, then KnewAB > KBC, we can get:\nKAB KnewAB \u2264 KAB KBC \u221d pAB pBC \u21d2 KABnew \u2265 k\u00d7KAB \u00d7 pBC pAB\n(16)\nKnewAB = KAB \u2212 1\n4C+ \u2212 1 4 > k\u00d7KAB \u00d7 pBC pAB\n\u21d2 KAB(1\u2212 k\u00d7 pBC pAB )\u2212 1 4 > 1 4C+ \u21d2 KAB \u00b7 pAB \u2212 k\u00d7 pBC\npAB \u2212 1 4 > 1 4C+\n\u21d2C+ > pAB\n4KAB(pAB \u2212 k\u00d7 pBC)\u2212 pAB\nWithout loss of generality, we set k = 1. In Equation (16), the value of C+ depends on three parameters, KAB, pAB and pBC. KAB is known, so the problem is how to obtain the pAB and pBC. According to the reference [14], the similarity matrix W is modeled as\n8 beta distributions, which are defined on the interval of (0, 1), parameterized by two positive shape parameters \u03b1 and \u03b2 . In this case, let Wi j is a sample from a beta distribution that is parameterized by \u0398k = (\u03b1k,\u03b2k), such that it is skewed towards one. If Wi j is in off-diagonal blocks, then let \u03980 = (\u03b10,\u03b20) be the parameters for the beta distribution, then it is smaller than any other beta distributions. The probability density function of Wi j can be expressed as:\np(Wi j|{\u0398k}Kk=1,\u03980,Z) = Beta(Wi j|\u03b10,\u03b20) 1\u2212\u2211Kk zikz jk\nK\n\u220f k=1 Beta(Wi j|\u03b1k,\u03b2k)zikz jk (17)\nzn is a K-element cluster indicator zn = {znk}Kk=1 such that znk = 1 if xn belongs to the kth cluster, and otherwise znk = 0. zn follows a categorical distribution, and \u03c0 is a sample from a symmetric Dirichlet distribution\nThe prior distributions to the beta distribution parameters \u0398k and \u03980 are given:\np{\u0398k|\u03c2)\u221eBeta( \u03b1k\n\u03b1k +\u03b2k |\u03b1\u03c2 ,\u03b2\u03c2 )Lognormal(\u03b1k +\u03b2k|\u00b5\u03c2 ,\u03c32\u03c2 ) (18)\np{\u03980|\u03b7)\u221eBeta( \u03b1k\n\u03b1k +\u03b2k |\u03b1\u03b7 ,\u03b2\u03b7)Lognormal(\u03b1\u03b7 +\u03b2\u03b7 |\u00b5\u03b7 ,\u03c32\u03b7) (19)\nWe want to calculate the posterior distribution for the latent variables given the observed similarity matrix and the hyper-parameters, i.e.\np(\u03c0 ,Z,{\u0398k}Kk=1,\u03980|W,\u03c2 ,\u03b7 ,\u03bb ) (20)\nIt is computationally intractable to directly calculate this posterior distribution. Therefore, a vibrational distribution q(\u03c0 ,Z,{\u0398k}Kk=1,\u03980) is used to approximate the posterior distribution p. This distribution q can be factorized such that\nq(\u03c0 ,Z,{\u0398k}Kk=1,\u03980) = q\u03c0(\u03c0) N\n\u220f n=1\nqzn(zn) K\n\u220f k=1 q\u0398k(\u0398k)q\u03980(\u03980) (21)\nThen estimate each factorization using W,\u03c2 ,\u03b7 ,\u03bb and lastly get the probability of (19). Lastly, according to the similarity matrix and given hyper-parameters, we can get the element probability density function of Wi j, namely, the value of p0 and p2."}, {"heading": "4 Complexity analysis", "text": "PCS-SVM method includes three steps: first, developing the first SVM model through training original data set; then, computing the new C value; last, using the optimization C value trains the SVM again. The SVM complexity is O(dn2), d is the features dimension and n is the data set size. Calculation the posterior distribution for the latent variables given the observed similarity matrix and the hyper-parameters has the complexity of O(n). Whereas, the complexity of our proposed method is O(dn2).\n9"}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data sets and compared methods", "text": "We tested the proposed algorithm on 16 Keel data sets and our coal and gas burst monitoring data in first example. We focus on binary-class imbalanced problem, however, our method can be easily extended to multi-class imbalance classification. Information about standard benchmark datasets data sets is summarized in Table 1.\nFirst, we tested our proposed algorithm in comparison with five other single classification algorithms based on SVM, including:\n1. Standard SVM (SSVM): SVM classifier without any class imbalance correction technologies. 2. SVM with random undersampling (SVM-RUS): It firstly adopts RUS to preprocess the original training set, then trains SVM classifier using the balanced training data. 3. SVM with random oversampling (SVM-ROS): It first adopts ROS to preprocess the original training set, then trains SVM with classifier using the balanced training data. 4. SVM with SMOTE (SVM-SMOTE) [15]: It firstly adopts SMOTE algorithm to oversampling the minority class instances and to make the data set balance, then trains SVM classifier using the balanced training data. 5. Weighted SVM (CS-SVM) [10] : It assigns different values for the penalty factor C belong to different classes, to guarantee the fairness of classifier modeling, we make C+/C\u2212 equal to imbalance ratio (IR).\nIn the experiments, for each data set, we performed a fivefold cross validation. In views of the randomness of classification results, we repeated the cross-validation process for 20 times and provided the results in the mean values. All algorithms were\n10\nimplemented in Matlab 2015b running environment and SVM was realized by libSVM toolbox. Specifically, for SVM, polynomial kernel and RBF kernel function was adopted, the penalty factor C and the width parameter \u03c3 of RBF function were all tuned by using grid search. We evaluated these six classification algorithms by three mainly used evaluation criterions in imbalanced classification: F-measure, G mean and AUC. For each evaluation criterions and kernel function, we separate our method into two groups: one group is algorithm-level SVM method, our method PCS-SVM is compared with SVM and CS-SVM; The other group is sampling-level SVM method, our method combined with SMOTE named as PCS-SMOTE-SVM is compared with undersampling(SVM-RUS), oversampling (SVM-ROS) and SMOTE sampling SVM methods (SVM-SMOTE)."}, {"heading": "5.2 Standard benchmark datasets experiment results", "text": "In Table 2 to Table 7, seven SVM methods are evaluated on three measures on sixteen standard benchmark datasets, and each evaluation measure is separated into two groups according to two kernel function: Polynomial kernel (Tables 2, 4, and 6) and RBF kernel (Tables 3, 5, and 7). In the last row of each table, we list the number of winning method for all sixteen datasets. In terms of wins number, in general, PCSSVM and PCS-SMOTE-SVM method wins more data sets with polynomial kernel than RBF kernel. In algorithm-level SVM methods, PCS-SVM has the clear superiority on F-measure and G-means and on the AUS measure. In sampling-level SVM methods, PCS-SMOTE-SVM outperforms on most of the datasets. The SVM-SMOTE is close to PCS-SMOTE-SVM on three measures with RBF kernel.\nWe compute the average evaluation measure for each SVM method on all datasets, and the results is shown in Fig. 3. The red and black bar represent our proposed method\n11\nPCS-SVM and PCS-SMOTE-SVM respectively. We can conclude that two kernel function has no obvious influence on average on three measures for all methods. PCS-SVM is the best algorithm in algorithm-level SVM group and for the G-means measure, PCSSVM even better than SVM-RUS. Four sampling-level algorithms all have the excellent performance in AUS measure and F-measure. PCS-SMOTE-SVM has the best performance in all sixteen date sets and all the average value on three measures are above or very close to 0.9."}, {"heading": "5.3 Coal and gas burst monitoring data set experiment results", "text": "In this section, we evaluate PCS-SVM and PCS-SMOTE-SVM on our real-world data set: coal and gas burst electromagnetic monitoring data, as shown in Fig. 1. Coal and\n12\ngas burst is a kind of damaging accident in coal mining. Electromagnetic data can effective reflect the premonition of coal and gas burst accident. So if we can find the premonition of burst accident from the electromagnetic data accurately and timely, we can pre-alarming the burst accident. In our example, the data set is one month electromagnetic monitoring data with the number of 7989, and there are only 210 premonition data, with the imbalance ratio of 37.04. The data set has two features: electromagnetic intensity and electromagnetic pulse. We set the premonition data with positive class and the other normal data is negative class. We expect the premonition data can be classified from the normal data and higher accuracy of these premonition data than normal data. Because negative class is so huge, we random undersampling negative data to 2000 and SMOTE positive data to 500. As is concluded in Section 4.2, SMOTE has the better performance than ROS and RUS method, and PCS-SVM and PCS-SMOTE-SVM method\n13\nwins more data sets with polynomial kernel than RBF kernel, we only use SMOTE to oversampling data and adopt polynomial kernel. In this experiment, besides three evaluations we have used, the sensitivity and specificity also be evaluated in order to describe the accuracy rate of positive class. The results are shown in Table 8.\nFrom the Table 8, we can easily conclude that both PCS-SVM and PCS-SMOTESVM outperform compared method. Especially, sensitivity is higher than specificity which attain our expectation. PCS-SMOTE-SVM has the best performance on all the evaluation measures and the AUC values reaches 0.7770, compared to SMOTE-SVM, improve 29.54%.\n14"}, {"heading": "6 Conclusion and future work", "text": "This paper presents a novel C value optimization method on the basis of cluster probability density function, which is estimated only using similarity matrix and some predefined hyper-parameters. Experimental results on various standard benchmark datasets and real-world data with different ratios of imbalance show that the proposed method is effective in comparison with commonly used cost-sensitive techniques.\nFor future work, we plan to leverage multi-view feature representations [16,17,18,19] to improve the performance of SVMs for imbalanced data set."}], "references": [{"title": "Robust Subspace Clustering for Multi-View Data by Exploiting Correlation Consensus", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang", "Xiaodi Huang"], "venue": "IEEE Trans. Image Processing", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Shifting Hypergraphs by Probabilistic Voting", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang", "Lin Wu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "IEEE Trans Image Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "and H", "author": ["Z. Gon"], "venue": "Chen, \u201cModel-Based Oversampling for Imbalanced Sequence Classification,\u201d in Proceedings of the 25th ACM International on Conference on Information and Knowledge Management - CIKM \u201916,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "and C", "author": ["F. Cheng", "J. Zhang"], "venue": "Wen, \u201cCost-Sensitive Large margin Distribution Machine for classification of imbalanced data.,\u201d Pattern Recognit. Lett., vol. 80, pp. 107\u2013112,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "and O", "author": ["P. Cao", "D. Zhao"], "venue": "R. Za\u0131\u0308ane, \u201cA PSO-Based Cost-Sensitive Neural Network for Imbalanced Data Classification. BT - Trends and Applications in Knowledge Discovery and Data Mining - PAKDD 2013 International Workshops: DMApps, DANTH, QIMIE, BDM, CDA, CloudSD, Gold Coast, QLD, Australia, A.\u201d pp. 452\u2013463,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "and J", "author": ["G. Ristanoski", "W. Liu"], "venue": "Bailey, \u201cDiscrimination aware classification for imbalanced datasets,\u201d in Proceedings of the 22nd ACM international conference on Conference on information & knowledge management - CIKM \u201913,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "and J", "author": ["H. Cao", "V.Y.F. Tan"], "venue": "Z. F. Pang, \u201cA Parsimonious Mixture of Gaussian Trees Model for Oversampling in Imbalanced and Multimodal Time-Series Classification.,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 12, pp. 2226\u20132239,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Veropoulos, \u201cControlling the sensivity of support vector machines,", "author": ["C.N.C.K. C"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Applying Support Vector Machines to Imbalanced Datasets BT - Machine Learning: ECML 2004: 15th European Conference on Machine Learning, Pisa, Italy, September 20-24, 2004", "author": ["R. Akbani", "S. Kwek", "N. Japkowicz"], "venue": "Proceedings,\u201d J.-F. Boulicaut, F. Esposito, F. Giannotti, and D. Pedreschi, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme Re-balancing for SVMs: A Case Study,\u201d SIGKDD Explor", "author": ["B. Raskutti", "A. Kowalczyk"], "venue": "Newsl., vol. 6, no. 1, pp. 60\u201369,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "One Class SVM for Yeast Regulation Prediction,\u201d SIGKDD Explor", "author": ["A. Kowalczyk", "B. Raskutti"], "venue": "Newsl., vol. 4, no. 2, pp. 99\u2013100,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "z-SVM: An SVM for Improved Classification of Imbalanced Data,\u201d in AI 2006: Advances in Artificial Intelligence: 19th Australian Joint Conference on Artificial Intelligence, Hobart, Australia, December 4-8, 2006", "author": ["T. Imam", "K.M. Ting", "J. Kamruzzaman"], "venue": "Proceedings, A. Sattar and B. Kang, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "and J", "author": ["J. Che"], "venue": "Dy, A Generative Block-diagonal Model for Clustering, in Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "SMOTE: Synthetic Minority Over-sampling Technique, J", "author": ["N. V Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Artif. Int. Res., vol. 16, no. 1, pp. 321C357,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Effective Multi-Query Expansions: Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "ACM Multimedia", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Shifting multihypergraphs via collaborative probabilistic voting", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Qing Zhang", "Wenjie Zhang"], "venue": "Knowl. Inf. Syst", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Iterative Views Agreement: An Iterative Low-Rank Based Structured Optimization Method to Multi-View Spectral Clustering", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Meng Fang", "Shirui Pan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Imbalanced learning not only presents significant new challenges to the data research community [1,2,3] but also raises many critical questions in real-world data intensive applications.", "startOffset": 96, "endOffset": 103}, {"referenceID": 1, "context": "Imbalanced learning not only presents significant new challenges to the data research community [1,2,3] but also raises many critical questions in real-world data intensive applications.", "startOffset": 96, "endOffset": 103}, {"referenceID": 2, "context": "Imbalanced learning not only presents significant new challenges to the data research community [1,2,3] but also raises many critical questions in real-world data intensive applications.", "startOffset": 96, "endOffset": 103}, {"referenceID": 3, "context": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8].", "startOffset": 115, "endOffset": 120}, {"referenceID": 5, "context": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8].", "startOffset": 115, "endOffset": 120}, {"referenceID": 6, "context": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "In previous work, the class imbalance correction strategies for SVM mainly includes resampling [4] ,cost-sensitive [5,6], decision threshold adjustment [7], and hybrid methods [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "In order to bypass this problem, [4] proposed a Model-Based Oversampling method for imbalanced time series data considering the sequence structure when oversampling.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "The Different Error Costs (DEC) method is a cost-sensitive learning solution proposed in [9] to overcome the same cost (i.", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "As a rule of thumb, [10] have reported that reasonably good classification results could be retained from the DEC method by setting the C\u2212/C+ equal to the minority-to-majority class ratio.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "One-class Learning [11] trained an SVM model only with the minority class examples.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "[12] assigned C\u2212 = 0 and C+ = 1/N+, these methods have been observed to be more effective than general data rebalancing", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "zSVM is another algorithm modification proposed for SVMs in [13] to learn from imbalance datasets, which is an typical decision threshold adjustment method.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "According to the reference [14], the similarity matrix W is modeled as", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "SVM with SMOTE (SVM-SMOTE) [15]: It firstly adopts SMOTE algorithm to oversampling the minority class instances and to make the data set balance, then trains SVM classifier using the balanced training data.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "Weighted SVM (CS-SVM) [10] : It assigns different values for the penalty factor C belong to different classes, to guarantee the fairness of classifier modeling, we make C+/C\u2212 equal to imbalance ratio (IR).", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "For future work, we plan to leverage multi-view feature representations [16,17,18,19] to improve the performance of SVMs for imbalanced data set.", "startOffset": 72, "endOffset": 85}, {"referenceID": 16, "context": "For future work, we plan to leverage multi-view feature representations [16,17,18,19] to improve the performance of SVMs for imbalanced data set.", "startOffset": 72, "endOffset": 85}, {"referenceID": 17, "context": "For future work, we plan to leverage multi-view feature representations [16,17,18,19] to improve the performance of SVMs for imbalanced data set.", "startOffset": 72, "endOffset": 85}], "year": 2017, "abstractText": "Class imbalance is one of the challenging problems for machine learning in many real-world applications, such as coal and gas burst accident monitoring: the burst premonition data is extreme smaller than the normal data, however, which is the highlight we truly focus on. Cost-sensitive adjustment approach is a typical algorithm-level method resisting the data set imbalance. For SVMs classifier, which is modified to incorporate varying penalty parameter(C) for each of considered groups of examples. However, the C value is determined empirically, or is calculated according to the evaluation metric, which need to be computed iteratively and time consuming. This paper presents a novel cost-sensitive SVM method whose penalty parameter C optimized on the basis of cluster probability density function(PDF) and the cluster PDF is estimated only according to similarity matrix and some predefined hyper-parameters. Experimental results on various standard benchmark data sets and real-world data with different ratios of imbalance show that the proposed method is effective in comparison with commonly used cost-sensitive techniques.", "creator": "LaTeX with hyperref package"}}}