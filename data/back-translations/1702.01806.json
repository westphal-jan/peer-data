{"id": "1702.01806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Beam Search Strategies for Neural Machine Translation", "abstract": "The basic concept of Neural Machine Translation (NMT) is to train a large neural network that maximizes translation performance on a given parallel corpus. NMT then uses a simple left-right beam search decoder to generate new translations that roughly maximize the trained conditional probability. The current beam search strategy generates the target sentence word for word from left to right, while maintaining a fixed number of active candidates at each time step. First, this simple search is less adaptable as it also expands candidates whose results are much worse than the current best scores. Second, it does not expand hypotheses if they are not among the best scoring candidates, even if their scores are close to the best. The latter can be avoided by increasing the beam size until no performance improvement can be observed. While one can achieve better performance, this has the disadvantage of a slower decoding speed, even if their values are close to the best. The latter can be avoided by increasing the beam size until a performance improvement can be observed.", "histories": [["v1", "Mon, 6 Feb 2017 22:08:46 GMT  (21kb)", "https://arxiv.org/abs/1702.01806v1", null], ["v2", "Wed, 14 Jun 2017 01:00:18 GMT  (27kb)", "http://arxiv.org/abs/1702.01806v2", "First Workshop on Neural Machine Translation, 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["markus freitag", "yaser al-onaizan"], "accepted": false, "id": "1702.01806"}, "pdf": {"name": "1702.01806.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["freitagm@us.ibm.com", "onaizan@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n01 80\n6v 2\n[ cs\n.C L\n] 1\n4 Ju\nn 20\n17\nTranslation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-toright beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-toright while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German\u2192English and Chinese\u2192English without losing any translation quality."}, {"heading": "1 Introduction", "text": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT)\nmodels (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). With the recent success of NMT, attention has shifted towards making it more practical. One of the challenges is the search strategy for extracting the best translation for a given source sentence. In NMT, new sentences are translated by a simple beam search decoder that finds a translation that approximately maximizes the conditional probability of a trained NMT model. The beam search strategy generates the translation word by word from left-to-right while keeping a fixed number (beam) of active candidates at each time step. By increasing the beam size, the translation performance can increase at the expense of significantly reducing the decoder speed. Typically, there is a saturation point at which the translation quality does not improve any more by further increasing the beam. The motivation of this work is two folded. First, we prune the search graph, thus, speed up the decoding process without losing any translation quality. Secondly, we observed that the best scoring candidates often share the same history and often come from the same partial hypothesis. We limit the amount of candidates coming from the same partial hypothesis to introduce more diversity without reducing the decoding speed by just using a higher beam."}, {"heading": "2 Related Work", "text": "The original beam search for sequence to sequence models has been introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al., 2014) for neural machine translation. (Hu et al., 2015; Mi et al., 2016) improved the beam search with a constraint softmax function which only considered a limited\nword set of translation candidates to reduce the computation complexity. This has the advantage that they normalize only a small set of candidates and thus improve the decoding speed. (Wu et al., 2016) only consider tokens that have local scores that are not more than beamsize below the best token during their search. Further, the authors prune all partial hypotheses whose score are beamsize lower than the best final hypothesis (if one has already been generated). In this work, we investigate different absolute and relative pruning schemes which have successfully been applied in statistical machine translation for e.g. phrase table pruning (Zens et al., 2012)."}, {"heading": "3 Original Beam Search", "text": "The original beam-search strategy finds a translation that approximately maximizes the conditional probability given by a specific model. It builds the translation from left-to-right and keeps a fixed number (beam) of translation candidates with the highest log-probability at each time step. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam is reduced by one and the translation is stored into a final candidate list. When the beam is zero, it stops the search and picks the translation with the highest log-probability (normalized by the number of target words) out of the final candidate list."}, {"heading": "4 Search Strategies", "text": "In this section, we describe the different strategies we experimented with. In all our extensions, we first reduce the candidate list to the current beam size and apply on top of this one or several of the following pruning schemes.\nRelative Threshold Pruning. The relative\nthreshold pruning method discards those candidates that are far worse than the best active candidate. Given a pruning threshold rp and an active candidate list C , a candidate cand \u2208 C is discarded if:\nscore(cand) \u2264 rp \u2217max c\u2208C {score(c)} (1)\nAbsolute Threshold Pruning. Instead of taking\nthe relative difference of the scores into account, we just discard those candidates that are worse by a specific threshold than the best active candidate. Given a pruning threshold\nap and an active candidate list C , a candidate cand \u2208 C is discarded if:\nscore(cand) \u2264 max c\u2208C {score(c)} \u2212 ap (2)\nRelative Local Threshold Pruning. In this prun-\ning approach, we only consider the score scorew of the last generated word and not the total score which also include the scores of the previously generated words. Given a pruning threshold rpl and an active candidate list C , a candidate cand \u2208 C is discarded if:\nscorew(cand) \u2264 rpl \u2217max c\u2208C {scorew(c)}\n(3)\nMaximum Candidates per Node We observed\nthat at each time step during the decoding process, most of the partial hypotheses share the same predecessor words. To introduce more diversity, we allow only a fixed number of candidates with the same history at each time step. Given a maximum candidate threshold mc and an active candidate list C , a candidate cand \u2208 C is discarded if already mc better scoring partial hyps with the same history are in the candidate list."}, {"heading": "5 Experiments", "text": "For the German\u2192English translation task, we train an NMT system based on the WMT 2016 training data (Bojar et al., 2016) (3.9M parallel sentences). For the Chinese\u2192English experiments, we use an NMT system trained on 11 million sentences from the BOLT project.\nIn all our experiments, we use our in-house attention-based NMT implementation which is similar to (Bahdanau et al., 2014). For German\u2192English, we use sub-word units extracted by byte pair encoding (Sennrich et al., 2015) instead of words which shrinks the vocabulary to 40k sub-word symbols for both source and target. For Chinese\u2192English, we limit our vocabularies to be the top 300K most frequent words for both source and target language. Words not in these vocabularies are converted into an unknown token. During translation, we use the alignments (from the attention mechanism) to replace the unknown tokens either with potential targets (obtained from an IBM Model-1 trained on the parallel data) or with the source word itself (if no target was found) (Mi et al., 2016). We use an embedding dimension of 620 and fix the RNN GRU lay-\ners to be of 1000 cells each. For the training procedure, we use SGD (Bishop, 1995) to update model parameters with a mini-batch size of 64. The training data is shuffled after each epoch.\nWe measure the decoding speed by two numbers. First, we compare the actual speed relative to the same setup without any pruning. Secondly, we measure the average fan out per time step. For each time step, the fan out is defined as the number of candidates we expand. Fan out has an upper bound of the size of the beam, but can be decreased either due to early stopping (we reduce the beam every time we predict a end-of-sentence symbol) or by the proposed pruning schemes. For each pruning technique, we run the experiments with different pruning thresholds and chose the largest threshold that did not degrade the translation performance based on a selection set.\nIn Figure 1, you can see the German\u2192English translation performance and the average fan out per sentence for different beam sizes. Based\non this experiment, we decided to run our pruning experiments for beam size 5 and 14. The German\u2192English results can be found in Table 1. By using the combination of all pruning techniques, we can speed up the decoding process by 13% for beam size 5 and by 43% for beam size 14 without any drop in performance. The relative pruning technique is the best working one for beam size 5 whereas the absolute pruning technique works best for a beam size 14. In Figure 2 the decoding speed with different relative pruning threshold for beam size 5 are illustrated. Setting the threshold higher than 0.6 hurts the translation performance. A nice side effect is that it has become possible to decode without any fix beam size when we apply pruning. Nevertheless, the decoding speed drops while the translation performance did not change. Further, we looked at the number of search errors introduced by our pruning schemes (number of times we prune the best scoring hypothesis). 5% of the sentences change due to search errors for beam size 5 and 9% of the sentences change for beam size 14 when using all four pruning techniques together.\nThe Chinese\u2192English translation results can be found in Table 2. We can speed up the decoding process by 10% for beam size 5 and by 24% for beam size 14 without loss in translation quality. In addition, we measured the number of search errors introduced by pruning the search. Only 4% of the sentences change for beam size 5, whereas 22% of the sentences change for beam size 14."}, {"heading": "6 Conclusion", "text": "The original beam search decoder used in Neural Machine Translation is very simple. It generated translations from left-to-right while looking at a fix number (beam) of candidates from the last time step only. By setting the beam size large enough, we ensure that the best translation performance can be reached with the drawback that many candidates whose scores are far away from the best are also explored. In this paper, we introduced several pruning techniques which prune candidates whose scores are far away from the best one. By applying a combination of absolute and relative pruning schemes, we speed up the decoder by up to 43% without losing any translation quality. Putting more diversity into the decoder did not improve the translation quality."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural networks for pattern recognition", "author": ["Christopher M Bishop."], "venue": "Oxford university press.", "citeRegEx": "Bishop.,? 1995", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent."], "venue": "ISMIR. Citeseer, pages 335\u2013340.", "citeRegEx": "Boulanger.Lewandowski et al\\.,? 2013", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1211.3711 .", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Improved beam search with constrained softmax for nmt", "author": ["Haifeng Wang."], "venue": "Proceedings of MT Summit XV page 297.", "citeRegEx": "Wang.,? 2015", "shortCiteRegEx": "Wang.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL. Beijing, China, pages 1\u201310.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL. Beijing, China, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1605.03209 .", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A systematic comparison of phrase table pruning techniques", "author": ["Richard Zens", "Daisy Stanton", "Peng Xu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Zens et al\\.,? 2012", "shortCiteRegEx": "Zens et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 7, "context": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 6, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 10, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 0, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 3, "context": "The original beam search for sequence to sequence models has been introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 2, "context": "The original beam search for sequence to sequence models has been introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 10, "context": ", 2013) and by (Sutskever et al., 2014) for neural machine translation.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "(Hu et al., 2015; Mi et al., 2016) improved the beam search with a constraint softmax function which only considered a limited", "startOffset": 0, "endOffset": 34}, {"referenceID": 11, "context": "phrase table pruning (Zens et al., 2012).", "startOffset": 21, "endOffset": 40}, {"referenceID": 0, "context": "In all our experiments, we use our in-house attention-based NMT implementation which is similar to (Bahdanau et al., 2014).", "startOffset": 99, "endOffset": 122}, {"referenceID": 9, "context": "For German\u2192English, we use sub-word units extracted by byte pair encoding (Sennrich et al., 2015) instead of words which shrinks the vocabulary to 40k sub-word symbols for both source and target.", "startOffset": 74, "endOffset": 97}, {"referenceID": 8, "context": "During translation, we use the alignments (from the attention mechanism) to replace the unknown tokens either with potential targets (obtained from an IBM Model-1 trained on the parallel data) or with the source word itself (if no target was found) (Mi et al., 2016).", "startOffset": 249, "endOffset": 266}, {"referenceID": 1, "context": "For the training procedure, we use SGD (Bishop, 1995) to update model parameters with a mini-batch size of 64.", "startOffset": 39, "endOffset": 53}], "year": 2017, "abstractText": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-toright beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-toright while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German\u2192English and Chinese\u2192English without losing any translation quality.", "creator": "gnuplot 5.0 patchlevel 2"}}}