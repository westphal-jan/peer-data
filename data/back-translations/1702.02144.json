{"id": "1702.02144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Rapid parametric density estimation", "abstract": "The basis of statistics is parametric density estimation, e.g. as Gaussian distribution. Machine learning requires the cost-effective estimation of much more complex densities, and the basic approach is the relatively expensive estimation of maximum probabilities (MLE). Cheap density estimators are discussed, e.g. the literal fit of the polynomial to the sample, the coefficients of which are calculated from the mere averaging of monomies over the sample (estimators of moments). Another basic application discussed is the adjustment of the distortion to a standard distribution such as Gaussian. The estimated parameters approach the optimal values, with errors such as $1 /\\ sqrt {n} $falling, with $n $being the sample size.", "histories": [["v1", "Tue, 7 Feb 2017 16:55:37 GMT  (234kb,D)", "https://arxiv.org/abs/1702.02144v1", "6 pages, 2 figures"], ["v2", "Mon, 20 Feb 2017 14:29:27 GMT  (382kb,D)", "http://arxiv.org/abs/1702.02144v2", "8 pages, 4 figures"]], "COMMENTS": "6 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jarek duda"], "accepted": false, "id": "1702.02144"}, "pdf": {"name": "1702.02144.pdf", "metadata": {"source": "CRF", "title": "Rapid parametric density estimation", "authors": ["Jarek Duda"], "emails": ["dudajar@gmail.com"], "sections": [{"heading": null, "text": "\u221a n, where n is the sample size.\nKeywords: machine learning, statistics, density estimation, independent component analysis, clustering\nI. INTRODUCTION\nThe complexity of our world makes it useful to imagine a sample obtained from some measurements, in nearly all kind of science, as coming from some probability distribution. The natural approach to model this distribution is using a parametric function, which parameters should be estimated basing on the sample. The best known example is Gaussian (normal) distribution, which parameters are estimated from averages over the sample of all up to degree 2 monomials.\nTo model real data we usually need more complex densities. In machine learning there are popular kernel density estimators (KDE) ([1], [2]), which smoothen the sample by convolving it with a kernel: a nonnegative function integrating to 1, for instance a Gaussian distribution. However, one issue is that it requires arbitrarily choosing the width of this kernel: if it is too narrow we get a series of spikes, if too wide we loose the details. Another problem is that such estimated density is a sum of potentially large number of functions - is very costly to directly work with.\nHence, usually it would be more convenient to have a parametric model with a relatively small number of parameters, for example to locally approximate density with a polynomial. The standard approach to estimate such parameters is the maximum likelihood estimation (MLE) ([3], [4]), which finds\nthe parameters maximizing likelihood of obtaining the given sample. However, beside really simple examples, such maximization would require costly numerical procedures like gradient descent.\nAs in examples from fig. 1, we will discuss here inexpensive parametric estimation, especially with a linear combination of a chosen family of functions, for instance with a polynomial or Fourier series in a finite region. It is based on mean-square fitting of the optimal parameters to the sample smoothed with a kernel (KDE).\nar X\niv :1\n70 2.\n02 14\n4v 2\n[ cs\n.L G\n] 2\n0 Fe\nb 20\n17\n2 Surprisingly, the mean-square (L2) fitting gives the best agreement for the 0 width limit of the kernel: degenerated to Dirac delta, like in fig. 2, by the way removing the very inconvenient requirement of arbitrarily choosing the kernel width. Intuitively, it fits a function to a series of spikes, what makes it much simpler to calculate and asymptotically leads to the real parameters of the used density: the error drops like 1/ \u221a n with n being sample size. The parameters of a linear combination are obtained analogously to algebraic projection on a preferably (complete) orthogonal base of functions, making the estimated coefficient for a given function as just the\naverage of value of this function over the sample. For fitting order m polynomial in a finite region (preferably a hyperrectangle: product of ranges), we just need to calculate averages over the sample of all monomials of power up to m (estimators of moments). Orthogonality of the used family allows to ask independently about each coefficient, what can be used to directly work with as high order polynomial approximation as needed, eventually neglecting coefficients which turn out close to zero. Finally, the Stone-Weierstrass theorem theorem says that any continuous function on a closed interval can be uniformly approximated as closely as desired by polynomials, making it theoretically possible to asymptotically recreate any continuous density using the discussed approach. Analogously for Fourier series, where the coefficients are obtained as just averages of corresponding sine or cosine over the sample.\nBeside being much less expensive to calculate than MLE, minimization of mean-square error might lead to more appropriate densities for some machine learning problems. Specifically, MLE categorically requires all probabilities being positive on the sample, what seems a strong restriction for example for fitting polynomials. Mean-square optimization sometimes leads to negative values of density, what obviously may bring some issues, but can also lead to better global agreement of the fitted function.\nAnother discussed basic application, beside literally fitting polynomial to a sample in a finite region, is estimating and describing distortion from some simple expected approximated distribution, like uniform or Gaussian distribution. For example for testing long range correlations of a pseudorandom number generators (PRNG). Taking D length sequences of its values as points from [0, 1]D hypercube, ideally they should come from \u03c1 = 1 uniform density. The discussed method allows to choose some testing function (preferably orthogonal to \u03c1 = 1: integrating to 0) describing our suspicion of distortion from this uniform density, like\u220f i(xi \u2212 1/2), and estimate its coefficient basing on the sample - test if the average of its values approaches zero as expected.\nMore important example is modelling distortions form\nFigure 2. Examples of smoothing (KDE) sample of n = 25 points from density represented by the thick blue line. The three thin lines represent smoothing with Gaussian kernel for standard deviation = 0.5, 0.1 and 0.001. We can fit for example a polynomial to such smoothen sample. Large intuitively blurs the information, so we focus here on mean-square fitting to the sharpest: \u2192 0 limit spikes, sum of Dirac deltas.\nthe Gaussian distribution. The standard way is to calculate higher (central) moments, however, it is a difficult problem to translate them back into the actual probability density - so called \u201dthe problem of moments\u201d [5].\nThe most widely used methods for understanding distortion from Gaussian distribution, as noise in which we would often like to find the real signal, is probably the large family of independent component analysis (ICA) [6] methods. They usually start with normalization: shifting to the mean value, and rescaling directions of eigenvectors of the covariance matrix accordingly to the corresponding eigenvalues, getting normalized sample: with unitary covariance matrix. Then it searches for distortions from the perfect Gaussian (as noise) of this normalized sample, for example as directions maximizing kurtosis - candidates for the real signal. The discussed method allows to directly fit parameters of distortion as a linear combination of some chosen family of functions, like polynomials multiplied by e\u2212x\n2/2. Its advantages comparing to ICA is the possibility to model multiple modes for every direction (like degrees of polynomial) and allowing to reconstruct the disturbed density function. It can also work with different than Gaussian types of tail, like e\u2212|x| or 1/|x|K .\nWhile the main focus here are probabilistic densities: nonnegative and integrating to 1, this approach can be also used to model more general or abstract densities: fitting a relatively simple function like a polynomial to a smoothen set of points. For example, the original motivation for this approach was fitting a low order polynomial to a distribution of atomic masses or electron negativity along the longest axis of a chemical molecule [7]. Coefficients of this polynomial describe some prosperities of this molecule and can be used as a part of information in its fingerprint (descriptor) for\n3 virtual screening. Another basic application can be trendseeking estimation of probability of a symbol basing on its previous appearances in adaptive data compressors: exploiting trends like rising or dropping probability by fitting and extrapolating a polynomial.\nFinally, this approach can be also applied for various clustering problems. By thresholding the estimated density function we can determine regions of relatively high density, interpreted as clusters for unsupervised learning. For supervised problem: with some additional information about classification of points into clusters, we can use these labels as weights while averaging. For two classes we can use weights of opposite signs to distinguish them (under-represented class should have correspondingly higher absolute weights), then the sign of the fitted density can be used to determine the cluster. We can also directly distinguish a larger number of classes with a single function, for example by using complex (or vector) weights (like W = \u00b11 \u00b1 \u221a \u22121), then use its argument to determine the cluster (like b 2\u03c0 arg(\u03c1)c).\nThe standard approaches in machine learning, like neural networks or SVMs, are based on linear classifiers with parameters found e.g. by backpropagation. The discussed approach not only allows to generalize it to directly use for example higher order polynomials here, but also directly fits their parameters by just averaging over the sample. Generalization to higher order polynomials with clusters defined for example as thresholded polynomials (semi-algebraic sets), gives much stronger separability strength. For example sign of xy monomial already classifies the XOR pattern. Paroboloida as second order polynomial can directly separate an ellipsoid, and much more for higher order polynomials. This approach can be used for example as one of layers of neural network with initially directly fitted coefficients, then improved e.g. through backpropagation. Due to simplicity of calculating the coefficients, the formulas for fitting them can be also directly put into the backpropagation process."}, {"heading": "II. GENERAL CASE", "text": "Imagine we have a sample of n points in D dimensional space: S = {xi}i=1..n, where xi = {xi1, . . . , xiD} \u2208 RD. Assuming it comes from some probability distribution, we will try to approximate (estimate) its probability density function (PDF) as a function from some chosen parameterized family \u03c1a : RD \u2192 R for some m parameters a = {a1, . . . , am}. In other words, we would like to estimate the real parameters basing on the observed sample. For generality, assume each point has also a weight W (x), which is W = 1 for probability estimation, but generally it can represent a mass of an object, or can be negative for separation into two classes, or even be a complex number or a vector for example for simultaneous classification into multiple classes.\nPreferably, a probabilistic \u03c1a should be smooth, nonengative and integrate to one. However, as we will mostly focus on \u03c1a being a linear combination, in some cases there will appear negative values as an artifact of the method. Probabilistic normalization (integration to 1) will be enforced by construction or additional constraints.\nA natural approach is smoothing the sample (before fitting) into a continuous function, for example by convolution with a kernel (KDE):\ng (x) := 1\nn \u2211 y\u2208S W (y) k (x\u2212 y) (1)\nwhere k is nonnegative and integrates to 1, for example is the Gaussian kernel: k (x) = (2\u03c0 2)\u2212D/2 e\u2212x\u00b7x/2 2\n. Obviously, for probabilistic W = 1, g is also nonnegative and integrates to 1.\nWe can now formulate the basic problem as finding parameters minimizing some distance from g :\nmin a \u2016\u03c1a \u2212 g \u2016 (2)\nHowever, there remains a difficult question of choosing the parameter . In fact it is even worse as the kernel assumed here is spherically symmetric, while for the real data more appropriate might be for example some elliptic kernel with a larger number of parameters to choose, which additionally should be able to vary with position.\nSurprisingly, we can remove this dependency on by choosing mean-square norm (L2), which allows to perform the \u2192 0 limit of kernel: to Dirac delta as in fig. 2. The g becomes a series of spikes in this limit, no longer being a type of function expected while fitting with a family of smooth functions. However, it turns out to well behave from the mathematical point of view. Intuitively, while \u2192 \u221e limit would mean smoothing the sample into a flat function - loosing all the details, the \u2192 0 limit allows to exploit the sharpest possible picture.\nAssume we would like to minimize mean-square norm \u2016f\u2016 := \u221a \u3008f, f\u3009 for scalar product:\n\u3008f, g\u3009 := \u222b RD f(x) g(x)w(x) dx (3)\nwhere we will usually use constant weight w = 1. However, for fitting only local behavior, it might be also worth to consider some vanishing w \u2192 0 while going away from the point of interest.\nFor mean-square norm, the minimization (2) is equivalent to:\nmin a \u2016\u03c1a \u2212 g \u20162 = mina \u3008\u03c1a \u2212 g , \u03c1a \u2212 g \u3009 =\nmin a \u2016\u03c1a\u20162 \u2212 2\u3008g , \u03c1a\u3009+ \u2016g \u20162 (4)\nIn the \u2192 0 limit we would have \u2016g \u2016 \u2192 \u221e. However, as we are only interested in the optimal parameters\n4 a here, and \u2016g \u2016 does not depend on them, we can just remove this term resolving the issue with infinity. The term \u3008g , \u03c1a\u3009 becomes 1n \u2211 x\u2208S w(x)W (x) \u03c1a(x) in the\n\u2192 0 limit, finally getting:\nDefinition 1. The general considered minimization problem is\nmin a \u3008\u03c1a, \u03c1a\u3009 \u2212\n2\nn \u2211 x\u2208S w(x)W (x) \u03c1a(x). (5)\nIts necessary differential condition is:\n\u3008\u03c1a, \u2202aj\u03c1a\u3009 = 1\nn \u2211 x\u2208S w(x)W (x) (\u2202aj\u03c1a)(x) (6)\nfor all j \u2208 {1, . . . ,m}."}, {"heading": "III. DENSITY ESTIMATION WITH", "text": ""}, {"heading": "A LINEAR COMBINATION", "text": "The most convenient application of the discussed method is density estimation with a linear combination of some family of functions, for instance polynomials or sines and cosines. In this and the following section we assume:\n\u03c1a = m\u2211 i=1 aifi (7)\nfor some functions fi : RD \u2192 R (not necessarily nonnegative in the entire domain).\nWhile in practice we can rather only work on a finite family, it can be in fact an infinite complete orthogonal base - allowing to approximate any continuous function as close as needed, what is true for example for polynomials and Fourier base in a finite closed interval."}, {"heading": "A. Basic formula", "text": "In the linear combination case, \u2202aj\u03c1a = fj , and the necessary condition (6) becomes just:\u2211\ni\nai \u3008fi, fj\u3009 = 1\nn \u2211 x\u2208S w(x)W (x) fj(x) (8)\nfor j = 1, . . . ,m. Denoting (\u3008fi, fj\u3009) as the m\u00d7m matrix of scalar products, the optimal coefficients become:\naT = (\u3008fi, fj\u3009)\u22121 \u00b7 ([f1], . . . , [fm])T (9)\nwhere [f ] := 1\nn \u2211 x\u2208S w(x)W (x) f(x) (10)\nis estimator of the expected value of f . Assuming orthonormal set of functions: \u3008fi, fj\u3009 = \u03b4ij and w =W = 1 standard weights, we get a surprisingly simple formula:\nai = [fi] = 1\nn \u2211 x\u2208S fi(x) (11)\n\u03c1a(x) = \u2211 i [fi] fi(x) (12)\nWe see that using an orthonormal family of functions is very convenient as it allows to calculate estimated coefficients independently, each one being just (weighted) average over the sample of the corresponding function. It is analogous to making algebraic projections of the sample on some orthonormal base. The independence allows this base to be potentially infinite, preferably complete to allow for approximation of any function, for example a base of orthogonal polynomials. Orthonormality (independence) allows to inexpensively estimate with as high order polynomial as needed. Eventually, without orthonormality, there can be used the more general formula (9) instead.\nAs the estimated coefficients are just (weighted) average of functions over the sample, it can be naturally generalized to continuous samples, for example representing some arbitrary knowledge, where the average can be calculated by integration.\nai = [fi] = 1\nC \u222b S w(x)W (x) f(x)dx (13)\nwith some normalization if needed, for example C =\u222b S w(x)W (x)dx. For clustering applications there can be used C = 1. We can also mix some continuous arbitrary knowledge with sample of points from measurements by treating them as Dirac deltas to combine sum with integrations.\nFigures 3 and 4 shows such examples for using spirals as continuous arbitrary knowledge. They also use W = \u00b11 or W = \u00b11\u00b1 \u221a \u22121 sample weights to determine the clusters basing on the sign of the fitted function, or its argument in the complex case."}, {"heading": "B. Asymptotic behavior (n\u2192\u221e)", "text": "Assume W = w = 1 weights, orthonormal family of functions (\u3008fi, fj\u3009 = \u03b4ij) and that the real density can be indeed written as \u03c1 = \u2211 i aifi (not necessarily finite).\nFor estimation of ai, the (12) formula says to use ai \u2248 [fi]: average value of fi over the obtained sample. With the number of points going to infinity, it approaches the expected value of fi for this probability distribution:\n[fi] n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 \u222b fi \u03c1 dx = \u2329 fi, \u2211 i aifi \u232a = ai\nAs needed, [fi] approaches the exact value of ai from the assumed probability distribution. From the Central Limit Theorem, the error of i-th coefficient comes from approximately a normal distribution of width being standard deviation of fi (assuming it is finite) divided by\u221a n:\n[fi]\u2212 ai \u223c N ( 0,\n1\u221a n\n\u221a\u222b (fi \u2212 ai)2\u03c1 dx ) (14)\nHence, to obtain twice smaller errors, we need to sample 4 times more points.\n5"}, {"heading": "C. Nondegenerate kernel corrections", "text": "The zero width kernel limit (to Dirac delta) were only used to replace scalar product with sum while going from (4) to (5). For a general kernel k (nonegative, integrating to 1), orthonormal base and D = 1 case, we would analogously obtain estimation:\nai = 1\nn \u222b \u2211 x\u2208S k(y \u2212 x) fi(y)dy \u2248\n\u2248 1 n \u2211 x\u2208S ( fi(x) + 1 2 f \u2032\u2032i (x) \u222b h2k(h)dh ) where we have used second order Taylor expansion ( fi(x+ h) \u2248 fi(x) + h f \u2032i(x) + 12h 2 f \u2032\u2032i (x) )\nand assumed symmetry of kernel (k(\u2212h) = k(h)), zeroing the first order correction. Finally we got the second order correction:\nai \u2248 [fi+vf \u2032\u2032i ] for 0 \u2264 v := 1\n2\n\u222b h2k(h)dh (15)\nwith v characterizing the width (variance) of the kernel. For v 6= 0, we would approach a bit different coefficients\nthan previously, which as discussed were optimal for the \u03c1 = \u2211 i aifi density.\nAdding to a function its second derivative times a positive value corresponds to smoothing this function, analogously to evolution of diffusion equation (\u2202tf = \u2202xxf ). Hence, the v \u2192 0 limit (to Dirac delta) intuitively corresponds to the sharpest possible fit."}, {"heading": "D. Normalization of density", "text": "At least in theory, probabilistic density functions should integrate to 1. For polynomials and Fourier series it will be enforced by construction: all but the zero order fi will integrate to 0, hence, normalization of density can be obtained by just fixing the zero order coefficient. There are also situations, especially in machine learning, where the necessity of density integrating exactly to 1 is not crucial.\nLet us now briefly focus on situations where density normalization is required, but cannot be directly enforced by construction. A trivial solution is just performing additional final normalization step: divide obtained \u03c1a by \u222b \u03c1a(x) dx.\nHowever, more accurate values should be obtained by using the density normalization condition as constraint while the (2) minimization, what can be done with the\n6 Lagrange multiplier method. Denoting\nFi := \u222b fi(x)dx (16)\nthe density normalization condition becomes \u2211 i aiFi = C, where usually C = 1, but generally can be also for example C = 1n \u2211 x\u2208SW (x). For orthonormal set of functions we analogously get the following equations:\n\u2211 j ajFj = C, ai = [fi] + \u03bbFi for all i.\nSubstituting to the first equation and transforming:\u2211 j ([fj ] + \u03bbFj)Fj = C\n\u03bb = C \u2212\u2211 j [fj ]Fj  /\u2211 j (Fj) 2\nai = [fi] + C \u2212 \u2211 j [fj ]Fj\u2211\nj(Fj) 2\nFi (17)\nThe experiments from the right column of fig. 1 were made using this normalized formula for C = 1."}, {"heading": "IV. POLYNOMIALS, FOURIER AND GAUSSIAN", "text": "There will be now briefly discussed three natural examples of fitting with a linear combination, assuming weight w = 1. As discussed, choosing this family as orthogonal allows to estimate the parameters independently. The first considered example are (Legendre) polynomials, the second is Fourier series, the last one are (Hermite) polynomials multiplied by e\u2212x 2/2."}, {"heading": "A. Fitting polynomial in a finite region", "text": "A natural first application is parameterizing density function as a polynomial. The zeroth order function should be constant, but also needs to integrate to a finite value. Hence, we have to restrict to a finite region here (of volume v < \u221e), preferable a range like [\u22121, 1] in fig. 1, or a product of ranges (hyperrectangle) in a higher dimension. We will only use points inside this region to approximate (estimate) density inside this region with a polynomial - this region should be chosen as a relatively small one containing the behaviour of interest. All integrals and sums here are inside this finite region, for example by setting w = 0 outside.\nThis zero order function is f0 = 1/ \u221a v to get normalization \u2016f0\u2016 = 1. Its estimated coefficient is average of this function over the sample, which is always a0 = 1/ \u221a v. Hence, the zero order estimation is just \u03c1 \u2248 a0f0 = 1/v constant density (integrates to 1). The following functions (polynomials) should be or-\nthogonal to f0, what means integration to 0 ( \u222b fi dx = 0), hence\n\u03c1a = 1 v + \u2211 i\u22651 aifi (18)\nalways integrates to 1, probabilistic normalization is enforced for any choice of ai parameters (i \u2265 1). However, we need to remember that such density sometimes may obtain negative values in the assumed region, like some red lines going below 0 in fig. 1.\nOrthonormal polynomials for [\u22121, 1] range and w = 1 weight are known as Legendre polynomials. The first four are:\n1\u221a 2 ,\n\u221a 3\n2 x,\n\u221a 5\n8 (3x2 \u2212 1),\n\u221a 7\n8 (5x3 \u2212 3x)\nThanks to orthogonality, we can independently ask about their coefficients. Finally, density estimation in [\u22121, 1] range with second order polynomial becomes:\n\u03c1 \u2248 1 2 + 3 2 [x]x+ 5 8\n( 3[x2]\u2212 1 ) ( 3x2 \u2212 1 ) (19)\nwhere we have used linearity [\u03b1f + \u03b2g] = \u03b1[f ] + \u03b2[g], which makes it sufficient to calculate only averages of monomials over the sample - estimators of the moments. Such third order formula was used (and directly written with grouped [xi]) in experiments presented in the left column of fig. 1.\nFor a different interval, the Legendre polynomials should be properly shifted and rescaled. For D dimensional case, if the region is a product of ranges (hyperrectangle), the orthogonal polynomials can be chosen as products of D (rescaled) Legengre polynomials. Otherwise, there can be used Gram-Schmidt orthonormalization procedure to get an orthonormal base.\nFor example for [\u22121, 1]D hypercube, the polynomial for density can be naturally chosen as:\n\u03c1(x) = 1\n2D + \u2211 i1,...,iD ai1...iD fi1(x1) \u00b7 . . . \u00b7 fiD (xD)\nThe number of coefficient grow exponentially with dimension: fitting order m polynomial in D dimensions requires mD coefficients. However, some of them might be small and so neglected in the linear combination.\nBefore applying such fitting, it is crucial to properly choose the finite region of focus (like to the boundary between two classes), for example by normalizing it to [\u22121, 1]D hypercube. This approach often brings some artifacts far from the sample, especially near the boundaries of the region. To reduce this effect, better effect can be obtained by using functions vanishing at these boundaries, like sin(j\u03c0x)."}, {"heading": "B. Fourier series", "text": "Like in Fig. 3, for some type of data Fourier base might be more appropriate, still requiring working on a finite\n7 region (bounded or torus), preferably a hyperrectangle. Its zero order term is again constant 1/v and guards the normalization. The orthonormal base for [\u22121, 1] range is formed by sin(j\u03c0x) and cos(j\u03c0x) functions, which do not influence the normalization:\n\u03c1 = 1 2 + \u2211 j\u22651 aj sin(j\u03c0x) + bj cos(j\u03c0x) (20)\nwhere aj = [sin(j\u03c0x)], bj = [cos(j\u03c0x)] are just the averages of this function over the sample. For i up to m in D dimensions we need (2m)D coefficients here.\nObserve that sine terms vanish at the boundaries of interval (hypercube) - using only them we can reduce artifacts at the boundaries.\nIf we need to work on a two-dimensional sphere, the complete orthonormal base of spherical harmonics can be used. In a more general case, the orthonormal base can be obtained by Gram-Schmidt orthonormalization procedure."}, {"heading": "C. Global fitting combination of vanishing functions", "text": "Estimating density function with polynomials or Fourier series requires restriction to some finite region, what is a strong limitation and often brings some artifacts. To overcome it, we can for example use a combination of vanishing functions instead: with values dropping to zero while going away from the central point, for example in e\u2212x 2\nor e\u2212|x| or 1/|x|K way. For the convenience of applying the (12) formula, it is preferred that this set of functions is orthogonal.\nA well known example of such orthogonal family of functions are Hermite polynomial multiplied by e\u2212x\n2/2. Denoting by hi as i-th Hermite polynomial, the following set of functions is orthonormal for \u3008f, g\u3009 =\u222b R f(x)g(x)dx scalar product (weight w = 1):\nfi(x) = 1\u221a\n2i i! \u221a \u03c0 hi(x) e\n\u2212x2/2 (21)\nThis time it starts with i = 0, Hermite polynomials hi for i = 0, 1, 2, 3, 4, 5 are correspondingly:\n1, x, x2\u2212 1, x3\u2212 3x, x4\u2212 6x2 +3, x5\u2212 10x3 +15x\nAs the space has infinite volume here, we cannot use constant function in the considered base, which enforced Fi = \u222b fidx = 0 for all but the constant orthogonal\nfunctions, making \u222b \u03c1dx = 1 enforced by the zero order term, not changed by other parameters. The odd order terms are asymmetric here, hence Fi = 0 for them. However, even order terms integrate to nonzero values, hence the normalized formula (17) should lead to a bit better accuracy. It was used to obtain the right column of fig. 1.\nThe used e\u2212x 2/2 is characteristic for Gaussian distribution with standard deviation equal 1. The remaining terms from the linear combination can slightly modify this standard deviation, however, the natural first approach for applying the discussed fitting is to perform normalization first. Like in ICA: first shift to the mean value to centralize the sample, then perform PCA (principal component analysis): find eignenvectors and eigenvalues of the covariance matrix, and rescale these directions correspondingly - getting normalized sample: with mean in 0 and unitary covariance matrix.\nFor such normalized sample we can start looking for distortions from the Gaussian distribution (often corresponding to a noise). Such sample is prepared to directly start fitting the fi from formula (21). However, it might be worth to first rotate it to emphasize directions with the largest distortion from the Gaussian distribution - which are suspected to carry the real signal. In ICA these directions are usually chosen as the ones maximizing kurtosis. Here we could experiment with searching for directions maximizing some ai coefficient instead: average of fi in the given direction over the sample.\nObserve that in contrast to ICA, the discussed approach also allows to reconstruct the modelled distortion of Gaussian distribution. It can also model distortion of different probability distributions.\nGenerally, while high dimensional cases might require relatively huge base, most of the coefficients might turn out practically zero and so can be removed from the sum for density. Like in ICA, it might be useful to search for the really essential distortions (both direction and order), which may represent the real signal. For example orthogonal base can be built by searching for a function orthonormal to the previously found, which maximizes own coefficient - then adding it to the base and so on."}, {"heading": "V. SOME FURTHER POSSIBILITIES", "text": "The discussed approach is very general, here are some possibilities of extensions, optimizations for specific problems.\nOne line of possibilities is using a linear combination of different family of functions, for example obtained by Gram-Schmidt orthonormalization. For instance, while there was discussed perturbation of Gaussian distribution with Hermite polynomials, in some situations heavy tails might be worth to consider instead, vanishing for example like e\u2212|x| or 1/|x|K .\nWhile we have focused on global fitting in the considered region, there could be also considered local ones, for example as a linear combination of lattice of vanishing functions like wavelets. Or divide the space into (hyper)rectangles, fit polynomial in each of them and smoothen on the boundaries using a weighted average.\nA different approach to local fitting is through modifying the weight w: focus on some point by using\n8 a weight vanishing while going away from this point. Then for example fit polynomial describing behavior of density around this point, getting estimation of derivatives of density function in this point, which then could be combined in a lattice by using some splines.\nAnother line of possibility is that while there were only discussed linear combinations as the family of density functions, it might be worth to consider also some different families, for example enforcing being positive and vanishing, like:\nfa(x) = e \u2212\n\u2211 i ai x i or fa(x) = 1\u2211 i ai x i\nwhere in both cases the polynomial needs to have even order and positive dominant coefficient. In the latter case it additionally needs to be strictly positive. Their advantage is that they are both vanishing and nonnegative, as a density function should be.\nFinally, a wide space of possibilities is using the discussed method for different tasks than estimation of probability density integrating to 1. For example to predict probability of the current symbol basing on the previous occurrences, what is a crucial task in data compression. Or the sampled points may contain some weights, basing on which we might want to parameterize the density profile, for example of a chemical molecule [7].\nVery promising is also application for the clustering problem, especially the supervised situation: where we need to model a complex boundary between two labeled classes - using polynomials generalizes linear separators used in standard approaches, and here we can directly calculate the coefficients as just averages, eventually improving them later e.g. by backpropagation. Such classification can be done by assigning weights of opposite signs (or complex) to points from different classes, and look for the sign of the fitted density function (or argument). Its zero value manifold models the boundary. Often some classes are under-represented, what should be compensated by increasing weights of its representatives."}, {"heading": "VI. CONCLUSIONS", "text": "There was presented and discussed a very general and powerful, but still looking basic approach. It might be already known, but seems to be missed in the standard literature. Its discussed basic cases can be literally fitting a polynomial or Fourier series to the sample (in a finite region), or distortion from a standard probability distribution like Gaussian. There is a wide range of potential applications, starting with modelling of local density for data of various origin, testing PRNG, finding the real signal in a noise in analogy to ICA, reconstructing density of this distortion, or for various clustering problems. This article only introduces to this topic, discussing very basic possibilities, only mentioning some of many ways\nto expand this general approach of mean-square fitting to Dirac delta limit of smoothing the sample with a kernel."}], "references": [{"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "The Annals of Mathematical Statistics, vol. 27, no. 3, pp. 832\u2013837, 1956.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1956}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The annals of mathematical statistics, pp. 1065\u20131076, 1962.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1962}, {"title": "On the probable errors of frequency-constants (contd.)", "author": ["F.Y. Edgeworth"], "venue": "Journal of the Royal Statistical Society, vol. 71, no. 4, pp. 651\u2013678, 1908.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1908}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["S.S. Wilks"], "venue": "The Annals of Mathematical Statistics, vol. 9, no. 1, pp. 60\u201362, 1938.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1938}, {"title": "Society, The Problem of Moments, ser. Mathematical Surveys and Monographs", "author": ["J. Shohat", "J. Tamarkin"], "venue": "American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1943}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks, vol. 13, no. 4, pp. 411\u2013430, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Normalized rotation shape descriptors and lossy compression of molecular shape", "author": ["J. Duda"], "venue": "arXiv preprint arXiv:1509.09211, 2015. [Online]. Available: https://arxiv.org/pdf/1509.09211", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In machine learning there are popular kernel density estimators (KDE) ([1], [2]), which smoothen the sample by convolving it with a kernel: a nonnegative function integrating to 1, for instance a Gaussian distribution.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "In machine learning there are popular kernel density estimators (KDE) ([1], [2]), which smoothen the sample by convolving it with a kernel: a nonnegative function integrating to 1, for instance a Gaussian distribution.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "The standard approach to estimate such parameters is the maximum likelihood estimation (MLE) ([3], [4]), which finds Figure 1.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "The standard approach to estimate such parameters is the maximum likelihood estimation (MLE) ([3], [4]), which finds Figure 1.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Taking D length sequences of its values as points from [0, 1] hypercube, ideally they should come from \u03c1 = 1 uniform density.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "The standard way is to calculate higher (central) moments, however, it is a difficult problem to translate them back into the actual probability density - so called \u201dthe problem of moments\u201d [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 5, "context": "The most widely used methods for understanding distortion from Gaussian distribution, as noise in which we would often like to find the real signal, is probably the large family of independent component analysis (ICA) [6] methods.", "startOffset": 218, "endOffset": 221}, {"referenceID": 6, "context": "For example, the original motivation for this approach was fitting a low order polynomial to a distribution of atomic masses or electron negativity along the longest axis of a chemical molecule [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 6, "context": "Or the sampled points may contain some weights, basing on which we might want to parameterize the density profile, for example of a chemical molecule [7].", "startOffset": 150, "endOffset": 153}], "year": 2017, "abstractText": "Parametric density estimation, for example as Gaussian distribution, is the base of the field of statistics. Machine learning requires inexpensive estimation of much more complex densities, and the basic approach is relatively costly maximum likelihood estimation (MLE). There will be discussed inexpensive density estimation, for example literally fitting a polynomial (or Fourier series) to the sample, which coefficients are calculated by just averaging monomials (or sine/cosine) over the sample. Another discussed basic application is fitting distortion to some standard distribution like Gaussian analogously to ICA, but additionally allowing to reconstruct the disturbed density. Finally, by using weighted average, it can be also applied for estimation of non-probabilistic densities, like modelling mass distribution, or for various clustering problems by using negative (or complex) weights: fitting a function which sign (or argument) determines clusters. The estimated parameters are approaching the optimal values with error dropping like 1/ \u221a n, where n is the sample size.", "creator": "LaTeX with hyperref package"}}}