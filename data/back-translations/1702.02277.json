{"id": "1702.02277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "A Historical Review of Forty Years of Research on CMAC", "abstract": "The Cerebellar Model Articulation Controller (CMAC) is an influential brain-inspired computer model in many relevant areas. Since its introduction in the 1970s, the model has been extensively studied and many variants of the prototype have been proposed, such as Kernel CMAC, Self-Organizing Map CMAC and Linguistic CMAC. This review focuses on how the CMAC model is progressively developed and refined to meet the need for fast, adaptive and robust control. Two perspectives are presented: CMAC as a neural network and CMAC as a reference technique in the table. Three aspects of the model are discussed: architecture, learning algorithms and applications. Finally, some potential future research guidelines for this model are proposed.", "histories": [["v1", "Wed, 8 Feb 2017 04:27:11 GMT  (988kb,D)", "http://arxiv.org/abs/1702.02277v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["frank z xing"], "accepted": false, "id": "1702.02277"}, "pdf": {"name": "1702.02277.pdf", "metadata": {"source": "CRF", "title": "A Historical Review of Forty Years of Research on CMAC", "authors": ["Frank Z. Xing"], "emails": ["zxing001@e.ntu.edu.sg"], "sections": [{"heading": null, "text": "I. INTRODUCTION The Cerebellar Model Articulation Controller (CMAC) was proposed by J. S. Albus in 1975 [2]. Parallel at this time in the history, the concept of perceptron [23] had already been popular, whereas effective learning schemes to tune perceptrons [24] were not on the stage yet. In 1969, Minsky and Papert also pointed out the limitations that the exclusive disjunction logic cannot be solved by the perceptron model in their book Perceptrons: An Introduction to Computational Geometry. These facts made it less promising to consider CMAC as a neural network form. Consequently, although the name of CMAC appears bio-inspired enough, and the theory that the cerebellum is analogous to a perceptron has been proposed earlier [1], CMAC was emphasized to be understood as a table referring technique that can adaptive to real-time control system. Nevertheless, the underlying bioscience mechanism was addressed again in 1979 by Albus [3], which always gives the CMAC model two different ways of interpretation.\nThe structure of CMAC was originally described as two inter-layer mappings illustrated in Fig. 1. The control functions are represented in the weighted look-up table, rather than by solution of analytic equations or by analog [2]. If we use S to denote sensory input vectors, and let A and P stand for association cells and response output vectors respectively, both CMAC and multilayer percptrons (MLP) model can be formalized as: {\nf : S\u2192 A g : A\u2192 P\nthe final output can be calculated as:\ny = \u2211A\u2217i wi where the asterisk denotes a link or activation between certain association cell and the output. The nuance between\nMLP and CMAC model is that, for mapping f , MLP model is fully connected but CMAC restricts the association in a certain neighboring range. This property of mapping significantly accelerates the learning process of CMAC, which is considered a main advantage of it comparing to other neural network models.\nIt is notable that CMAC may not represent accurately how the human cerebellum works, even at a most simplified level. For instance, recent biological evidence from eyelid conditioning experiments suggests that the cerebellum is capable of computing exclusive disjunction [34]. However, CMAC is still an important computational model, because the restriction on mapping function effectively decreased the chance of been trapped in a local minimum during learning process.\nDespite the advantage aforementioned, CMAC has the following disadvantages as well [25]: \u2022 many more weight parameters are needed comparing to\nnormal MLP model \u2022 local generalization mechanism may cause some train-\ning data to be incorrectly interpolated, especially when data is sparse considering the size of CMAC \u2022 CMAC is a discrete model, analytical derivatives do not exist\nAs a response to these problems, modified or high order CMAC, storage space compressing, and fast convergent learning algorithms are continuously studied. These discov-\nar X\niv :1\n70 2.\n02 27\n7v 1\n[ cs\n.N E\n] 8\nF eb\n2 01\n7\neries will be elaborated in the following sections. Recent advances in Big Data and computing power seem to have watered down these problems. But in many physical scenarios, the computing power are still restricted and high speed responses are required. This serves for the reason to further study CMAC-like models, though it has been in and out of fashion for several times.\nAlthough there has been few other pioneer review works on CMAC, for instance by Mohajeri et al. in 2009 [20], this article is inventive for its chronological perspective. rather than emphasizing on detailed techniques. The remainder of the article is organized as follows: Section II provides the evolution trajectory of CMAC structure and efficient storage techniques. Section III discusses the learning algorithms. Section IV presents various circumstances that CMAC model has been applied. Section V summarizes the paper, and instigates discussions about potential improvements that can be made on CMAC model."}, {"heading": "II. ARCHITECTURE", "text": ""}, {"heading": "A. Basic Architectures", "text": "Before the CMAC was proposed in the 1970s, the anatomy and physiology of cerebellum has been studied for a long time. It is widely agreed that many different type of nerve cells are involved in cerebellar functioning. Fig. 2 shows the computational model proposed by Mauk and Donegan [18]. A more simple and implementable model proposed by Albus [3] is exhibited in Fig. 3.\nBased on the computational model of cerebellum (Fig. 3), the primary CMAC structure as shown in Fig. 1 is conceived. While it is obvious that high dimensional proximity of association rules cannot be captured in this primary form, because the nodes are arranged into one dimensional array. A simple solution to this problem is to introduce some nonlinearity to the first mapping. As a result, another layer called \u201cconceptual memory\u201d was soon added to the CMAC structure, which involves one additional mapping to the primary structure. The function of conceptual memory is illustrated in Fig. 4.\nIf we use A to represent the actual memory (Association Cells), M is the conceptual memory to encode S. Then the conceptual mapping f is more sparse and constrained within a certain range, but mapping g could be random.\nf : S\u2192M g : M\u2192 A h : A\u2192 P\nWhen it comes to implementation, the connectivism perspective to recognize CMAC as a neural network and the table referring perspective are equivalent. Fig. 5 illustrates the difference at a conceptual level [25]. To the upper part is a two input one output neural network structure, to the lower part is a two input one output table look-up structure.\nAn intuitive observation from both Fig. 4 and Fig. 5 is that the number of weights will increase exponentially with the number of input variables. This problem brings out two challenges: 1) The storage of weights become space consuming; 2) The training process becomes difficult to converge and waiting time before termination will lengthen.\nA previously used technique to solve the first challenge is called tile coding, which latter was developed to an adaptive version as well [36]. The advantage of tile coding is that we can strictly control the number of features through tile split. Another commonly employed trick is called hashing. This technique is applied to CMAC in the 1990s, and maps a large virtual space A to a smaller physical address A\u2032. Many common hashing function fh can be used, for instance from MD5 or DES, which are fundamental methods in cryptography. However, how to reduce the collision for specific problems according to the data property is still considered an art.\nLiterature [25] introduced a hardware implementation which uses selected binary bits of indexes as the hashing code. Whereas other research, e. g. [35], claims that due to the learning rate diminishing and slower convergence, hashing is not effective for enhancing the capability of approximation by CMAC. Therefore, many other attempts have been made, such as neighbor sequential, CMAC with general basis functions, and adaptive coding [7], which is a similar idea to hashing in the sense of weight space compression."}, {"heading": "B. Modified Architectures", "text": "Since simply increasing the CMAC size gives diminishing returns, two directions of modification are undertaken to push forward the research on CMAC. The first consideration is to combine multiple low dimensional CMACs. The second consideration is to introduce other properties, for example spline neurons, fuzzy coding or cooperative PID controller.\nCascade CMAC architecture was firstly proposed in 1997 for the purpose of printer calibration [10] (Fig. 6). Input variables are sequentially added to keep each of the CMAC component two dimensional.\nAnother method to combine multiple CMACs can be realized by voting technique. If we regard the Cascade CMAC model as a fusion of input information at a feature level, then the voting CMACs can be reckoned as a fusion at decision level. Each small CMAC just accept a subset of the whole input space. In this case an important antecedent is that input data is well partitioned. The reason to this requirement is that voting lift can only be achieved by heterogeneous expert networks. Taken this into account, some prior knowledge of input data or unsupervised clustering techniques can be applied in this stage.\nIf we make more efforts for dimension reduction, multiple levels of voting can be used. Then the architecture can be reckoned as a Hierarchical CMAC (H-CMAC), which is described by Tham [32] in 1996. H-CMAC has several advantages, such as less storage space and fast adaptation for learning non-linear functions. In Fig. 7, a two level HCMAC is illustrated. It is noticeable that each conventional CMAC components in different layers plays different role. The gating network works at a higher level.\nThese architectures can be employed at the same time with more fundamental modifications for the second consideration. In 1992, Lane et al. [11] descried high order CMAC as CMAC with binary neuron output replaced by spline functions. This modification brings about more parameters of splines, but makes the output derivable, which sometimes gives a better performance because the learning phase goes deeper. Sharing the idea to allow more meticulous transfer function, a similar modification can be made by introducing linguistic rules and fuzzy logic.\nLinguistic CMAC (LCMAC) was proposed by He and Lawry based on label semantics, rather than mapping functions in 2009. A cascade of LCMAC series was further developed in 2015 [8]. Borrowing the terminology of \u201cfocal element\u201d from evidence theory, the properties used to activate it is represented as membership function (usually trapezoidal) of several attributes. Therefore, for each input tuple, the excited neurons form a hypercube in the matrix of all the weights as memory. The responsive output can be distributionally depicted as:\nP(a|x) = N\n\u220f d=1 mxd (Fdi)\nwhere P is the probability of some memory unit a been activated given the input vector x. F denotes focal element. di is the index of linguistic attributes. m is the hidden weights for F called \u201cmass assignment\u201d.\nFuzzy CMAC (FCMAC) is yet another form of fuzzy coding. The intuition to use fuzzy representation is similar to using spline function. For most well defined problems, the nature of CMAC approximation is using multiple steps to emulate a smooth surface. Proper selection of fuzzy membership function would obviously relief the pressure of weight storage and training. From my understanding, FCMAC is an inverse structure of many established NeuroFuzzy Inference Systems. Usually, two extra fuzzy/defuzzy layers are added next to the association layer, the consequents can be Mamdani type, TSK type, weights, or a hybrid of them, e. g. Parametric-FCMAC (PFCMAC) [21]. More advanced FCMAC models, maybe inspired by spline methods, use interpolation to solve the discrete inference problem. In 2015, Zhou and Quek proposed FIE-FCMAC [39], which adds fuzzy interpolation and extrapolation to a rigid CMAC structure.\nRecently, CMAC applications to more specific scenarios are studied. For example, for control of time-varying nonlinear system, a combination of Radial Basis Function network and the local learning feature of CMAC is proposed (RBFCMAC). It is reported that using RBFs can prevent parameter drift and accelerate synchronization speed to the changing system [17]. For this type of combination, beside RBF, Wavelet Neural Network (WNN), fuzzy rule neuron and recurrent mechanism [15] or a mingle of them can also be employed with CMAC model simultaneously. Previous works, such as [38], have provided evidence that these features are effective for modeling complicated dynamic systems.\nIn a broader scenario, CMAC can be applied with other control systems as well (Fig. 8). Traditionally, the role of CMAC at the primary stage is to assist the output of a main controller. As the training proceed, error between CMAC and the actual model decays. CMAC takes charge of the main controller. Back-forward signal acceptor or conjugated CMACs are often used to accelerate this process [13]. More precisely, this arrangement is a change of information flow but not a change of architecture.\nSimilarly, CMAC structures that modifies the storage optimizing methods, for example quantization [16] and multiresolution [19], will only result in architectural difference from a hardware implementation sense. According to my personal understanding, they are the same thing regarding the conceptual structure, though these techniques are of sufficient interests to be discussed in Section III."}, {"heading": "III. LEARNING ALGORITHM", "text": "The original form of learning proposed Albus is based on backward propagation of errors. The fast convergence of this algorithm is proved mathematically by succeeding researches. Specifically, the convergence rate is only governed by the size of receptive fields for association cells [37]. Some study [25] suggests that it would be useful to distinguish between target training and error training, despite they share the same mathematical form. In the weights updating rule,\nwi(k+1) = wi(k)+\u03b1 Error\ncount(A\u2217) xi\nk is the epoch times, \u03b1 \u2208 [0,1] is the learning rate, xi is a state indicator of activation. The learning process is theoretically faster with a larger \u03b1 , while overshooting may occur. If the difference between output and the desired value \u2206y = y\u2212 y\u0302 can well define the error, target training and error training will be equivalent. In certain cases,\nError = 1 2c (y\u2212 y\u0302)2\nis a popular cost function as well. Based on the original learning algorithm, many developments are further derived. Most of them can be categorized into two directions of improvement. The first relies on extra supervisory signals or value assignment mechanism based on statistics. The second endeavors to optimize the use of memory. From a more practical sense, the dichotomy can be understood as learning to adjust value of weights, and learning to adjust number or size of weights."}, {"heading": "A. Adjusting value of weights", "text": "Facing the trade off between speed of convergence and learning stability, it is intuitive to consider using a relatively large \u03b1 at the beginning, and slow down the weight adjustment near the optimum point. This improvement is called adaptive learning rate [14]. It can be achieved by using two CMAC components, one as main controller, another as supervisory controller or compensated controller. Another way to achieve this adaption is by imposing a resistant term to the weight updating rule:\nwi(k+1) = wi(k)+ \u03b10 c\u03b1i Error count(A\u2217) xi\nIn the above rule, \u03b10 and c are constants, \u03b1i denotes the average activation times of the memory units that have been activated by training sample i.\nFor both fixed learning rate and adaptive learning rate, weights adjustment start from a randomized set of parameters. Experiments suggest that for sparse data, even if the training samples are within the local generalization range, perfect linear interpolation may not be achieved [25]. As a result, the approximation may appears to have many small zigzag patterns. Therefore, the weight smoothing technique is proposed. After each iteration, the weights are globally adjusted according to:\nwi(A\u2217j) = (1\u2212\u03b4 ) wi(A\u2217j)+ \u03b4\ncount(A\u2217)\ncount(A\u2217)\n\u2211 k=1 wi(A\u2217k)\nwhere \u03b4 is the proportional coefficient measuring the share of the weight with index j that needs to be replaced by the average of all activated weights A\u2217.\nWhile the weight smoothing technique tries to affect as much memory as possible in one iteration, repeated activation of the same units may not be a good thing. In 2003, research [26] pointed out that equally assign the error to each weight is not a meticulous method. During the learning process, if an associative memory is activated many times, which means many relevant samples are already learned, the weight should be more close to the desired value. In other words, the weight value is more \u201ccredible\u201d. In this situation, less error should be assigned to it so that other memory units can learn faster. This rule is called learning based on credit assignment:\nwi(k+1) = wi(k)+ \u03b1 f (i)\u22121 g \u2211\ni=1 f (i)\u22121\n(y\u2212 g\n\u2211 i=1 wi(k)) xi\nwhere g is a parameter regarding the degree of local generalization, f (i) records the times memory unit i has been activated. Further research has proved that the convergence is guaranteed with learning rate \u03b1 < 2.\nThe hardware implementation of CMAC memory enables other interpretations of the associative architecture. If we recognize it as a self-organizing feature map (SOFM), competitive learning algorithm can be realized. With input\nvariables x = {x1, x2, ..., xn}, the output can directly be the weight of the winning neuron. The weights updating rule can be formalized according to Hebbian theory:\nwx(k+1) = wx(k)+\u03b1 (y(k)\u2212wx(k))\nNote that the index of x can be time dependent. A modified version of the aforementioned rule involves not only inputs, but also errors as feedback:\nwx,e(k+1) = wx,e(k)+\u03b1 (y(k)\u2212wx,e(k))\nUsing this learning mechanism, the SOFM-like CMAC is named MCMAC. In 2000, Ang and Quek [5] proposed learning with momentum, neighborhood, and averaged fuzzy output for both CMAC and MCMAC. For CMAC and MCMAC with momentum, the weights updating rules can be written as:{ \u2206w j(k+1) = \u03b1 \u2206w j(k)+\u03b7 \u03b4 j(k)y j(k)\n\u2206wx,e(k+1) = \u03b1 \u2206wx,e(k)+\u03bb (1\u2212\u03b1)[y(k)\u2212wx,e(k)]\nwhere the first term represents a momentum, the second term is a back propagation term with learning rate \u03b7 and local gradient \u03b4 j. j is the index for activated weights.\nTherefore, given a sequential learning process, the aggregational weights adjustment can be derived from the above rules:\n\u2206w j(k+1) = \u03b7 k\n\u2211 i=1 \u03b1k\u22121\u03b4 j(i)y j(i)\n=\u2212\u03b7 k\n\u2211 i=1 \u03b1k\u22121 \u2202Error \u2202w j(k)\nWhen the sign of \u2202Error\u2202w j keeps unchanged, \u2206w j is accelerating; while if the sign is reversing, \u2206w j will slow down to stabilize the learning process.\nThe neighborhood learning rule proposed by Ang and Quek [5] serves the same purpose as weight smoothing technique. However, they used the Gaussian function to put more attention to those neurons surrounding the winning neuron instead of evenly adjusting each weight regardless of the distance. Weights updating rule for MCMAC with momentum and neighborhood in singular form is:\n\u2206wi(k+1) = \u03b1 \u2206wi(k)+hi j[\u03bb (1\u2212\u03b1)(y(k)\u2212wi(k))]\nwhere\nhi j = exp ( \u2212 |r j\u2212 ri|2 2\u03c32 )\nis the distance metric between neuron j and the winning neuron i.\nBeside using additional terms such as momentum and neighborhood, kernel method can also be applied to CMAC learning. In 2007, Kernel CMAC (KCMAC) was proposed [9], [11] to reduce the CMAC dimension which usually hazard the training speed and convergence. KCMAC treats the association memory as the feature space of a kernel machine, thus weights can be determined by solving an\noptimization problem. The supervised learning using error e as slack variables is to achieve:\nmin w,e 1 2 wT w+ \u03b2 2\nn\n\u2211 i=1 e2i\ns.t. wT \u03c6(ui)+ \u03b2 2 ei > 1, i = 1,2, ...,n\nwhere w denotes weights, coefficient \u03b2 serves as penalty parameter, \u03c6(\u00b7) is the mapping function and K(u,w) = \u03c6(u) \u00b7 \u03c6(w) is the kernel function.\nThe standard procedure to this problem is to solve the maxima-minima of Lagrangian function. Though other learning method can be employed as well, for instance, using Bayesian Ying-Yang (BYY) learning as proposed in 2013 by Tian et al [33]. The key idea behind BYY learning can be represented by harmonizing the joint probability with different product form of Bayesian components. In this specific KCMAC case, u and output z are observable, while w is a hidden variable. The joint distribution can either be written as ying form or yang form.{\npying(u,z,w) = p(w)p(u|w)p(z|u,w) pyang(u,z,w) = p(u)p(z|u)p(w|u,z)\nOur goal is to maximize H(pying, pyang), H(pying, pyang) = \u222b\u222b\u222b pying ln pyang du dz dw\nIn practice, \u2206H is frequently calculated depending on how the conditional probabilities are estimated and maximum of H is achieved by searching heuristically."}, {"heading": "B. Adjusting number of weights", "text": "Adjusting number of weights is chiefly realized by introducing multi-resolution and dynamic quantization techniques. As Section I has explained, CMAC was firstly used for real time control system. Consequently, the structural design fits the hardware implementation well. Many CMAC variants, such as LCMAC and FCMAC, inherit the memory units division with a lattice-based manner. Inputs are generally built on grids with equal space. This characteristic adds local constraints to the value of adjacent units.\nIf we consider this problem from a function approximation perspective, it is also rather intuitive that local complex shape needs a larger number of low-order elements to approach. Naturally, multi-resolution lattice techniques [19] were proposed in the 1990s.\nWith our prior knowledge, some metrics can be used to determine the resolution, for example, the variation of output in certain memory unit areas, which can be formalized as following:\nresolution \u221d v = 1 N\nN\n\u2211 j=1 |y j\u2212 y\u0304|\nwhere N is the number of output samples, v is the variance. Other attempts use a tree structure to manage the resolution hierarchically. New high resolution lattice is generated only if the threshold of variance is exceeded.\nThe concept of quantization is almost identical to resolution. The nuance may be that the terminology quantization puts more emphasis on the discretion of continuous signal. Furthermore, increasing resolution will also cause the number of weights to grow. Quantization deals with a given memory capacity problem.\nTo the best of my knowledge, the idea of adaptive quantization was initially proposed in 2006 [16]. The algorithm used to interpolate points looks on change of slopes, which is also similar to the variance metric discussed above. The input space is initialized with uniform quantization. For each point xi, the slopes are calculated by neighbor points.\n\u02c6slope j = f (x j)\u2212 f (xi)\nx j\u2212 xi , j = 1,2,3...\nHere f stands for the mapping function between input vector and association cells. The change of sign for corresponding directions indicates finer local structure, or fluctuation in other words, thus a new critical point is added to split the interval. The termination condition is that, within all intervals, the difference of output values should not exceed an experimental constant C [28]. The adaptive quantization technique is later developed to the Pseudo Self Evolving CMAC (PSECMAC) model [30], which further introduced neighborhood activation mechanism."}, {"heading": "IV. APPLICATION", "text": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7]. This can be attributed to the fast learning and stable performance of CMAC models. Moreover, engineering-oriented software and toolkit also help to promote the application of CAMC. In the following part, CMAC application to two emerging engineering fields are elaborated."}, {"heading": "A. Financial Engineering", "text": "The practice side of financial engineering has employed various instruments to model the price and risk of bond, stock, option, future and other derivatives. In most cases, historic data can be partitioned into chunks of selected time span to enable a supervised learning process. In 2008, Teddy et al [29] proposed an improved CMAC variant (PSECMAC) to model the price of currency exchange American call option. Three variables are taken into consideration: difference between strike price and current price, time of maturity, and pricing volatility. Thus the pricing function is formulated as:\nC0 = f (S0\u2212X ,T,\u03c330)\nThe article reported PSECMAC as the best-performing model among several CMAC-like systems. It is also reported that most of the CMAC models produce better result than using Black-Scholes model in sense of RMSE. Though\nfor American option, Black-Scholes model is not a good benchmark because it is sensitive to details of calculation.\nThis work further constructed an arbitrage system based on the pricing model. Positions are adjusted according to the Delta hedging ratio. Experiments suggested the model has a marginal positive ROI, omitting transaction costs."}, {"heading": "B. Adaptive Control", "text": "The application of CMAC on commercial devices may be more advantageous. The pressure to reduce cost and demand of embedded controller make CMAC-like models a good choice. Recently, studies have been carried on adaptive control of disabled wheelchair and seatless unicycle [12]. This type of problems can be formalized as controlling a set of variables in certain range (e. g. speed and balance angle) with a set of unknown and varying variables (e. g. friction to the ground and weight of the rider).\nLi et al [12] proposed a TSK type FCMAC to synthesis the equations of adaptive steering control. The back-stepping error is associated with the torque \u03c4 , which can simultaneously effect on critical moments.{\n\u03b8\u0308 = A\u0304(\u03b8 , \u03b8\u0307)+ B\u0304(\u03b8)(\u03c4\u2212\u00b5\u03c6\u0307 \u2212 c sgn(\u03c6\u0307)) \u03c6\u0308 = C\u0304(\u03b8 , \u03b8\u0307)+ D\u0304(\u03b8)(\u03c4\u2212\u00b5\u03c6\u0307 \u2212 c sgn(\u03c6\u0307))\nAs the output adapting to the moment parameters, the balance angle is controlled near zero.\nThe performance of FCMAC is benchmarked with a linear-quadratic regulator for differential equations to describe the state of the motor. Simulations suggest that LQR is not able to converge speed and angle of balance, while FCMAC provides satisfactory result."}, {"heading": "V. DISCUSSIONS", "text": "Referring to Section IV, CMAC was proved to be effective in many classic control problem and has been applied to emerging engineering problems. However, this model seems have encountered a bottleneck because of the lack of fundamental breakthrough during the past decade. Nowadays, issues discussed are mainly focused on trivial modification on memory structure and learning algorithm. The framework of error propagation or minimization of loss function is kept unchanged.\nAccording to my understanding, the limitation of current CMAC models can be ascribe to its over simplification of the cerebellum structure. Therefore, the next generation cerebellar model may adopt new discoveries from neuroscience. For example, the associative memory cells may take different roles rather than been treated identically. In fact, anatomical models usually feature several types of elementary cerebellar processing units. Meanwhile, the theory of Spike Timing Dependent Plasticity (STDP) suggests that the learning process of firing neurons may be ordered [6]. This feature can introduce far more complexity to the current learning algorithm."}], "references": [{"title": "A Theory of Cerebellar Function", "author": ["J.S. Albus"], "venue": "Mathematical Biosciences, pp. 25-61, 1971.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1971}, {"title": "A New Approach to Manipulator Control: the Cerebellar Model Articulation Controller (CMAC)", "author": ["J.S. Albus"], "venue": "Trans. ASME, Series G. Journal of Dynamic Systems, Measurement and Control, 97, pp. 220- 233, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Mechanisms of Planning and Problem Solving in the Brain", "author": ["J.S. Albus"], "venue": "Mathematical Biosciences, 45, pp. 247-293, 1979.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1979}, {"title": "Design Improvements in Associative Memories for Cerebellar Model Articulation Controllers", "author": ["P.C.E. An", "W.T. Miller", "P.C. Parks"], "venue": "Proceedings of ICANN, pp. 1207-10, 1991.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Improved MCMAC with momentum, neighborhood, and averaged trapezoidal output", "author": ["K.K. Ang", "C. Quek"], "venue": "IEEE Transactions on Systems. Man and Cybernetics: PartB, 30(3), pp. 491-590, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Spike TimingDependent Plasticity: A Hebbian Learning Rule", "author": ["N. Caporale", "Y. Dan"], "venue": "Annual Review of Neuroscience, 31(1), pp. 25-46, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "CMAC Neural Network based Neural Computation and Neuro-control", "author": ["P. Duan", "H. Shao"], "venue": "Information and Control (in Chinese), 28(3), 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A Cascade of Linguistic CMAC Neural Networks for Decision Making", "author": ["H. He", "Z. Zhu", "A. Tiwari", "A. Mills"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel CMAC with improved capability", "author": ["G. Horv\u00e1th", "T. Szab\u00f3"], "venue": "IEEE Transactions on Systems. Man and Cybernetics: PartB, 37(1), pp. 124\u201338, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Cascade-CMAC neural network applications on the color scanner to printer calibration", "author": ["K.-L. Huang", "S.-C. Hsieh", "H.-C. Fu"], "venue": "International Conference on Neural Networks, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Theory and development of higher-order CMAC neural networks", "author": ["S.H. Lane", "D.A. Handelman", "J.J. Gelfand"], "venue": "IEEE Control Systems, April, pp. 23\u201330, 1992.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive steering control using fuzzy CMAC for electric seatless unicycles", "author": ["Y.-Y. Li", "C.-C. Tsai", "F.-C. Tai", "H.-S. Yap"], "venue": "IEEE International Conference on Control & Automation, pp. 556\u2013561, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On a new CMAC control scheme, and its comparisons with the PID controllers", "author": ["C.C. Lin", "F.C. Chen"], "venue": "Proceedings of the American Control Conference, pp. 769\u2013774, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Adaptive CMAC-based supervisory control for uncertain nonlinear systems", "author": ["C.-M. Lin", "Y.-F. Peng"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 34(2), pp. 1248\u201360, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Self-organizing adaptive wavelet CMAC backstepping control system design for nonlinear chaotic systems", "author": ["C.-M. Lin", "H.-Y. Li"], "venue": "Nonlinear Analysis: Real World Applications, 14(1), pp. 206\u2013223, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "CMAC Study with Adaptive Quantization", "author": ["H.-C. Lu", "M.-F. Yeh", "J.-C. Chang"], "venue": "IEEE Intl. Conf. on Systems, Man, and Cybernetics, Taipei, pp. 2596\u20132601, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Using RBFs in a CMAC to prevent parameter drift in adaptive control", "author": ["C.J.B. Macnab"], "venue": "Neurocomputing, pp. 45\u201352, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A Model of Pavlovian Eyelid Conditioning Based on The Synaptic Organization of Cerebellum", "author": ["M.D. Mauk", "N.H. Donegan"], "venue": "Learn. Mem., 3, pp. 130-158, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "On the training of a multi-resolution CMAC neural network", "author": ["A. Menozzi", "M.-Y. Chow"], "venue": "Proceedings of the IEEE International Symposium on Industrial Electronics, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "CMAC neural networks structures", "author": ["K. Mohajeri", "G. Pishehvar", "M. Seifi"], "venue": "IEEE International Symposium on Computational Intelligence in Robotics and Automation, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy CMAC structures", "author": ["K. Mohajeri", "M. Zakizadeh", "B. Moaveni", "M. Teshnehlab"], "venue": "IEEE International Conference on Fuzzy Systems, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive CMAC model reference control system for linear piezoelectric ceramic motor", "author": ["Y.-F. Peng", "R.-J. Wai", "C.-M. Lin"], "venue": "IEEE International Symposium on Computational Intelligence in Robotics and Automation, 2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", "author": ["F. Rosenblatt"], "venue": "Spartan Books, Washington DC, 1961.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1961}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(9), 1986.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Intelligent Motion Control with an Artificial Cerebellum", "author": ["R.L. Smith"], "venue": "PhD Thesis of The University of Auckland, New Zealand, Chapter 3, 1998.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Credit assigned CMAC and its application to online learning robust controllers", "author": ["S.-F. Su", "T. Tao", "T.-H. Hung"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 33(2), 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems, pp. 1038\u20131044, MIT Press, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Hierarchically Clustered Adaptive Quantization CMAC and Its Learning Convergence", "author": ["S.D. Teddy", "E.M.-K. Lai", "C. Quek"], "venue": "IEEE TRANSACTIONS ON NEURAL NETWORKS, 18(6), 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A cerebellar associative memory approach to option pricing and arbitrage trading", "author": ["S.D. Teddy", "C. Quek", "E.M.-K. Lai"], "venue": "Neurocomputing, 19(4), 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "PSECMAC: A Novel Self- Organizing Multiresolution Associative Memory Architecture", "author": ["S.D. Teddy", "C. Quek", "E.M.-K. Lai"], "venue": "IEEE Trans. Neural Networks, 19(4), pp. 689\u2013712, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Forecasting ATM cash demands using a local learning model of cerebellar associative memory network", "author": ["S.D. Teddy", "S.K. Ng"], "venue": "International Journal of Forecasting, 27(3), pp. 760\u2013776, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical CMAC architecture for context dependent function approximation", "author": ["C.-K. Tham"], "venue": "IEEE International Conference on Neural Networks, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "KCMAC-BYY: Kernel CMAC using Bayesian YingYang learning", "author": ["K. Tian", "B. Guo", "G. Liu", "I. Mitchell", "D. Cheng", "W. Zhao"], "venue": "Neurocomputing, 101, pp. 24-31, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "The Cerebellum: An Incomplete Multilayer Perceptron", "author": ["H. Voicu"], "venue": "Neurocomputing, 72, pp. 592-599, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Hash-coding in CMAC Neural Networks", "author": ["Z.-Q. Wang", "J.L. Shiano", "M. Ginsberg"], "venue": "IEEE International Conference on Neural Networks, Washington, pp. 1698-1703, 1996.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1996}, {"title": "CMAC Learning is Governed by a Single Parameter", "author": ["Y.-F."], "venue": "IEEE International Conference on Neural Networks, San Francisco, pp. 1439-43, 1993.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1993}, {"title": "Predicting Evolving Chaotic Time Series with Fuzzy Neural Networks", "author": ["F.Z. Xing", "E. Cambria", "X. Zou"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2017.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "FIE-FCMAC: A novel fuzzy cerebellum model articulation controller (FCMAC) using fuzzy interpolation and extrapolation technique", "author": ["W.J. Zhou", "D.L. Maskell", "C. Quek"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Albus in 1975 [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "Parallel at this time in the history, the concept of perceptron [23] had already been popular, whereas effective learning schemes to tune perceptrons [24] were not on the stage yet.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Parallel at this time in the history, the concept of perceptron [23] had already been popular, whereas effective learning schemes to tune perceptrons [24] were not on the stage yet.", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "Consequently, although the name of CMAC appears bio-inspired enough, and the theory that the cerebellum is analogous to a perceptron has been proposed earlier [1], CMAC was emphasized to be understood as a table referring technique that can adaptive to real-time control system.", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Nevertheless, the underlying bioscience mechanism was addressed again in 1979 by Albus [3], which always gives the CMAC model two different ways of interpretation.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "tions are represented in the weighted look-up table, rather than by solution of analytic equations or by analog [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 33, "context": "For instance, recent biological evidence from eyelid conditioning experiments suggests that the cerebellum is capable of computing exclusive disjunction [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 24, "context": "Despite the advantage aforementioned, CMAC has the following disadvantages as well [25]:", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "in 2009 [20], this article is inventive for its chronological perspective.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "2 shows the computational model proposed by Mauk and Donegan [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "A more simple and implementable model proposed by Albus [3] is exhibited in Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "Albus\u2019 model [1] of cerebellum resembles a forward feed perceptron.", "startOffset": 13, "endOffset": 16}, {"referenceID": 24, "context": "5 illustrates the difference at a conceptual level [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "Literature [25] introduced a hardware implementation which uses selected binary bits of indexes as the hashing code.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "[35], claims that due", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Therefore, many other attempts have been made, such as neighbor sequential, CMAC with general basis functions, and adaptive coding [7], which is a similar idea to hashing in the sense of weight space", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "Cascade CMAC architecture was firstly proposed in 1997 for the purpose of printer calibration [10] (Fig.", "startOffset": 94, "endOffset": 98}, {"referenceID": 31, "context": "Then the architecture can be reckoned as a Hierarchical CMAC (H-CMAC), which is described by Tham [32] in 1996.", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "Hierarchical CMAC architecture, adapted from [32]", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "[11] descried high order CMAC as CMAC with binary neuron output replaced by spline functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A cascade of LCMAC series was further developed in 2015 [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 20, "context": "Parametric-FCMAC (PFCMAC) [21].", "startOffset": 26, "endOffset": 30}, {"referenceID": 37, "context": "In 2015, Zhou and Quek proposed FIE-FCMAC [39], which adds fuzzy interpolation and extrapolation to a rigid CMAC structure.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "It is reported that using RBFs can prevent parameter drift and accelerate synchronization speed to the changing system [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "For this type of combination, beside RBF, Wavelet Neural Network (WNN), fuzzy rule neuron and recurrent mechanism [15] or a mingle of them can also", "startOffset": 114, "endOffset": 118}, {"referenceID": 36, "context": "Previous works, such as [38], have provided evidence that these features are effective for modeling complicated dynamic systems.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Back-forward signal acceptor or conjugated CMACs are often used to accelerate this process [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Similarly, CMAC structures that modifies the storage optimizing methods, for example quantization [16] and multiresolution [19], will only result in architectural difference from a hardware implementation sense.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Similarly, CMAC structures that modifies the storage optimizing methods, for example quantization [16] and multiresolution [19], will only result in architectural difference from a hardware implementation sense.", "startOffset": 123, "endOffset": 127}, {"referenceID": 35, "context": "Specifically, the convergence rate is only governed by the size of receptive fields for association cells [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "Some study [25] suggests that it would be useful to distinguish between target training and error training, despite they share the same mathematical form.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "k is the epoch times, \u03b1 \u2208 [0,1] is the learning rate, xi is a state indicator of activation.", "startOffset": 26, "endOffset": 31}, {"referenceID": 13, "context": "This improvement is called adaptive learning rate [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "Experiments suggest that for sparse data, even if the training samples are within the local generalization range, perfect linear interpolation may not be achieved [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 25, "context": "In 2003, research [26] pointed out that equally assign the error to each weight is not a meticulous method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "In 2000, Ang and Quek [5] proposed learning with momentum, neighborhood, and averaged fuzzy output for both CMAC and MCMAC.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The neighborhood learning rule proposed by Ang and Quek [5] serves the same purpose as weight smoothing technique.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "In 2007, Kernel CMAC (KCMAC) was proposed [9], [11] to reduce the CMAC dimension which usually", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "In 2007, Kernel CMAC (KCMAC) was proposed [9], [11] to reduce the CMAC dimension which usually", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "Though other learning method can be employed as well, for instance, using Bayesian Ying-Yang (BYY) learning as proposed in 2013 by Tian et al [33].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "Naturally, multi-resolution lattice techniques [19] were proposed in the 1990s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "To the best of my knowledge, the idea of adaptive quantization was initially proposed in 2006 [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "The termination condition is that, within all intervals, the difference of output values should not exceed an experimental constant C [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "The adaptive quantization technique is later developed to the Pseudo Self Evolving CMAC (PSECMAC) model [30], which further introduced neighborhood activation mechanism.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 351, "endOffset": 355}, {"referenceID": 6, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 379, "endOffset": 382}, {"referenceID": 28, "context": "et al [29] proposed an improved CMAC variant (PSECMAC) to model the price of currency exchange American call option.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Recently, studies have been carried on adaptive control of disabled wheelchair and seatless unicycle [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Li et al [12] proposed a TSK type FCMAC to synthesis the equations of adaptive steering control.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "The Cerebellar Model Articulation Controller (CMAC) is an influential brain-inspired computing model in many relevant fields. Since its inception in the 1970s, the model has been intensively studied and many variants of the prototype, such as KCMAC, MCMAC, and LCMAC, have been proposed. This review article focus on how the CMAC model is gradually developed and refined to meet the demand of fast, adaptive, and robust control. Two perspective, CMAC as a neural network and CMAC as a table look-up technique are presented. Three aspects of the model: the architecture, learning algorithms and applications are discussed. In the end, some potential future research directions on this model are suggested.", "creator": "LaTeX with hyperref package"}}}