{"id": "1702.02287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Name Disambiguation in Anonymized Graphs using Network Embedding", "abstract": "In the real world, our DNA is unique, but many people share the same name. This phenomenon often causes erroneous aggregation of documents by multiple people who are namesakes of each other. Such errors worsen the performance of document retrieval, web search, and, more seriously, cause an inappropriate allocation of credits or blame in digital forensics. To solve this problem, the task of the name unit was designed to disambiguate, which aims to structure the documents associated with a name reference in such a way that each partition contains documents that relate to a unique person from real life. Existing solutions to this task rely largely on feature engineering, such as biographical feature extraction or the construction of auxiliary features from Wikipedia. However, in many scenarios, such features may be costly to maintain or unavailable due to the risk of privacy violation in the form proposed thereof. In this work, we propose a novel naming method of the anonymous form of the document, which represents the anonymization method proposed by the anonymous attribution method of the aspect of the fical form of the document.", "histories": [["v1", "Wed, 8 Feb 2017 04:54:09 GMT  (94kb)", "http://arxiv.org/abs/1702.02287v1", null], ["v2", "Thu, 4 May 2017 00:40:44 GMT  (0kb,I)", "http://arxiv.org/abs/1702.02287v2", "Made critical mistake in the experimental section"], ["v3", "Tue, 8 Aug 2017 14:29:03 GMT  (487kb)", "http://arxiv.org/abs/1702.02287v3", "The 26th ACM International Conference on Information and Knowledge Management (CIKM 2017) research track full paper"], ["v4", "Sat, 9 Sep 2017 23:05:04 GMT  (486kb)", "http://arxiv.org/abs/1702.02287v4", "The 26th ACM International Conference on Information and Knowledge Management (CIKM 2017) research track full paper"]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR", "authors": ["baichuan zhang", "mohammad al hasan"], "accepted": false, "id": "1702.02287"}, "pdf": {"name": "1702.02287.pdf", "metadata": {"source": "META", "title": "Name Entity Disambiguation in Anonymized Graphs using Link Analysis: A Network Embedding based Solution", "authors": ["Baichuan Zhang", "Mohammad Al Hasan"], "emails": ["zhan1910@purdue.edu", "alhasan@cs.iupui.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n02 28\n7v 1\n[ cs\n.S I]\n8 F\neb 2\n01 7\nIn real-world, our DNA is unique but many people share same names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name entity disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the aspect of methodological novelty, the proposed method uses a representation learning strategy to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name entity disambiguation methods working in a similar setting.\nCCS CONCEPTS\n\u2022Information systems \u2192Clustering; Information retrieval; Document representation; Language models;\nACM Reference format: Baichuan Zhang and Mohammad Al Hasan. 2017. Name Entity Disambiguation in Anonymized Graphs using Link Analysis: A Network Embedding based Solution. In Proceedings of ACM conference, Tokyo, Japan, Aug 2017 (SIGIR\u201917), 11 pages. DOI: 10.475/123 4\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR\u201917, Tokyo, Japan \u00a9 2017 Copyright held by the owner/author(s). 123-4567-24- 567/08/06. . . $15.00 DOI: 10.475/123 4"}, {"heading": "1 INTRODUCTION", "text": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information retrieval, counter-terrorism, and bibliographic data analysis. In information retrieval, name disambiguation is critical for sanitizing search results of ambiguous queries. For example, an online search query for \u201cMichael Jordan\u201d may retrieve pages of former US basketball player, the pages of UC Berkeley machine learning professor, and the pages of other persons having that name, and name disambiguation is necessary to split those pages into homogeneous groups. In counter-terrorism, such an exercise is essential before inserting a person\u2019s profile in a law enforcement database; failing to do so may cause severe trouble to many innocent persons who are namesakes of a known criminal. Evidently, name entity disambiguation is particularly important in the field of bibliometrics and library science. This is due to the fact that many distinct authors share the same name reference as the first name of an author is typically written in abbreviated form in the citation of many scientific articles. Thus, bibliographic servers that maintain such data may mistakenly aggregate the articles from multiple scholars (sharing the same name) into a unique profile in some digital repositories. For an example consider the name \u201cWei Wang\u201d in Google Scholar (GS) 1. Although verified as the profile page of a Computer Science Professor at UCLA, according to our labeling, more than 10 distinct person\u2019s publications are aggregated under that profile mistakenly. Such mistakes in library science over- or under-estimate a researcher\u2019s citation related impact metrics.\nDue to its importance, the name entity disambiguation task has attracted great attention from information retrieval and data mining communities. However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage. Also, contextual features such as collaborator, community affiliation, and external data source such as Wikipedia are used in some works [11, 13]. Using the biographical features is acceptable for disambiguation of authors in bibliometrics domain, but for many scenarios, especially in the national security related applications, biographical features are hard to obtain, or they may even be illegal to obtain unless a security analyst has the appropriate level of security clearance. For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.\n1https://scholar.google.com/citations?hl=en&user=UedS9LQAAAAJ\nIn this work, we solve the name entity disambiguation task without tapping into user\u2019s non-public biographical information. In fact, for a given name reference, by using only the relational information, we pre-process the input data as three graphs: coauthor graph, author-document graph, and document-document graph. Moreover, These graphs are appropriately anonymized where each of the nodes has been assigned a unique pseudo-random identifier. Then the name entity disambiguation task becomes a graph clustering task of the document-document graph, with the objective that each cluster contains documents authored by a unique reallife person. Instead of solving this clustering task by using a naive (and costly) graph clustering method, we solve it by designing a novel representation learning model, which facilitates information exchange among the three networks and jointly learns embedding of the vertices of these networks in a low dimensional latent space. Our proposed representation learning model embeds the document collection into a set of disambiguation-aware vectors in their latent space, such that a traditional hierarchical clustering [27] of the vectors generates excellent name entity disambiguation performance.\nThe key contributions of this work are summarized as below:\n(1) We solve the name entity disambiguation task by using only linked data from network topological information. The work is motivated by the growing demand for big data analysis without violating the user privacy in security sensitive domains. (2) We propose a network embedding based solution that leverages linked structures of variety of anonymized networks in order to represent each document into a low-dimensional vector space for solving the name entity disambiguation task. (3) We use two real-life bibliographic datasets for evaluating the disambiguation performance of our solution. The results demonstrate the superiority of our proposed method over the state-of-the-art methodologies for name entity disambiguation in an identical setup."}, {"heading": "2 RELATED WORK", "text": "There exist a large number of works on name entity disambiguation [3, 8, 30]. In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29]. In the supervised setting, Han et al. [8] proposed supervised name disambiguation methodologies by utilizing Naive Bayes and SVM. In these works, a distinct real-life entity can be considered as a class, and the objective is to classify each record to one of the classes. For the unsupervised name disambiguation, the records are partitioned into several clusters with the goal of obtaining a partition where each cluster contains records from a unique entity. For example, Han et al.[9] used K-way spectral clustering for name disambiguation in bibliographical data. Recently, probabilistic relational models, especially\ngraphical models have also been considered for the name entity disambiguation task. For instance, [23] proposesd to use Markov Random Fields to address name entity disambiguation challenge in a unified probabilistic framework.\nMost existing solutions to the name disambiguation task use either biographical attributes, or auxiliary features that are collected from external sources. However, the attempt of extracting biographical or external data sustains the risk of privacy violation. To address this issue, a few works [12, 16, 28] have considered name disambiguation using anonymized graphs without leveraging the node attributes. The central idea of this type of works is to exploit graph topological features to solve the name disambiguation problem without intruding user privacy through the collection of bibliographical attributes. For example, authors in [12] characterized the similarity between two nodes based on their local neighborhood structures using graph kernels and solved the name disambiguation problem using SVM. However, the major drawback of the proposed method in [12] is that it can only detect entities that should be disambiguated, but fails to further partition the documents into their corresponding homogeneous groups. Authors in [28] proposed an unsupervised solution to name disambiguation in an anonymized graph by exploiting the time-stamped network topology around a vertex. However, it also suffers from the similar issue as described above.\nOur proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning. Many of these methods are technically inspired by word embedding based language model [17]. Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks. Although, our proposed method exploits the general theme of network embedding approach, its methodologies are novel and particularly effective for solving the name entity disambiguation in a restricted setup, where network topology is the only information available."}, {"heading": "3 PROBLEM FORMULATION", "text": "We first introduce notations used in this paper. Throughout the paper, bold uppercase letter (e.g., X) denotes a matrix and Xij denotes the entry in i-th row and j-th column of X; bold lowercase letter such as xi denotes a column vector, and (\u00b7)T denotes vector transpose. \u2016X\u2016F is the Frobenius norm of matrix X. Calligraphic uppercase letter (e.g., X ) is used to denote a set and |X | is the cardinality of the set X .\nFor a given name reference a, we denoteDa = {da1 , d a 2 , ..., d a N}\nto be a set of N documents with which a is associated and Aa = {a1, a2, ..., aM} is the coauthor set of a in D\na, where a 6\u2208 Aa. If there is no ambiguity we remove the superscript a in the notations of both Da and Aa and refer the terms as D and A, respectively. For illustration, in bibliographic field,\nD can be the set of scholarly publications where a is one of the authors and A is the set of a\u2019s coauthors. In real-life, the given name reference a can be associated with multiple persons (say L) all sharing the same name. The task of name entity disambiguation is to partition D into L disjoint sets such that each partition contains documents of a unique person entity with name reference a.\nThough it may appear as a simple clustering problem, name entity disambiguation is challenging on real-life data. This is due to the fact that it requires solving a highly classimbalanced clustering task, as the number of documents associated with a distinct person follows a power-law distribution. We demonstrate it through an example from the bibliographic domain. In Figure 1, we show a histogram of paper counts of various real-life persons named \u201cS Lee\u201d. As we can observe, there are a few real-life authors (dominant entities) with the name \u201cS Lee\u201d to whom the majority of the publications belong. Only a few publications belong to each of the remaining real-life authors with name \u201cS Lee\u201d. Due to this severe class imbalance issue, majority of traditional clustering methods perform poorly on this task. Sophisticated machine learning models, like the one we propose below are needed for solving this task. This example is from bibliographic domain, but power-law distribution of possession is common in every aspect of real-life, so we expect this challenge to hold in other domains as well.\nIn this study, we investigate the name entity disambiguation problem in a restricted setup, where bibliographical features and information from external sources are not considered so that the risk of privacy violation can be alleviated. Instead, we formulate the problem using graphs in which each node has been assigned an anonymized identifier, and network topological structure is the only information available. Specifically, our solution encodes the local neighborhood structures accumulated from three different networks into a proposed network embedding model, which generates a k-dimensional vector representation for each document. The networks are author-author network, author-document network, and linked document network, which we formally define below.\nDefinition 3.1 (Author-Author Network). For a given name reference x, the author-author network, denoted as Gaa = (Ax, Eaa), captures the author co-occurrence event within the collection of documents authored by x. Ax is the coauthor set, and eij \u2208 Eaa represents the set of edges between authors ai and aj, who co-authored in at least one document.\nThe weight wij of the edge eij is defined as the number of times ai and aj have co-authored.\nThe author-author network is important because the interperson acquaintances represented by co-authorship relation can be used to discriminate the set of documents of multiple real-life persons. However, the co-authorship network does not account for the fact that the documents authored by the same real-life person are inherently similar; authordocument network and document-document network cover for this shortcoming.\nDefinition 3.2 (Author-Document Network). Author-Document Network, represented as Gad = (A \u222aD, Ead), is a bipartite network where D is the set of documents with which name reference a is associated and A is the set of co-authors of a. Ead is the set of edges between authors and documents. The edge weight wij between author ai and document dj is simply defined as the number of times ai appears in document dj. For a bibliographic dataset, the weight wij = 1.\nDefinition 3.3 (Linked Document Network). Document-Document Network, represented as Gdd = (D, Edd), where each vertex di \u2208 D is a document. If two documents di and dj are similar (more discussion is forthcoming), we build an edge between them represented as eij \u2208 Edd.\nThere are several ways document-document similarity can be captured. For instance, one can find word co-occurrence between different documents to compute this similarity. However, we refrained from using word co-occurrence due to the privacy concern as sometimes a list of a set of unique words can reveal the identity of a person [4]. Instead we define document-document similarity through a combination of authorauthor and author-document relationships. Two documents are similar if the intersection of their coauthor-sets is large (by using author-document relationship) or if the intersection of one-hop neighbors of their coauthor-sets is large (by using both author-document and author-author relationships).\nThe above definition of document similarity captures two important patterns which facilitate effective name entity disambiguation by document clustering. First, there is a high change for two documents to be authored by the same real-life person, if they have a large number of overlapping co-authors. Second, even if they do not have any overlapping co-authors, large overlap in the neighbors of their co-authors signals that the documents are most likely authored by the same person. For both cases, these two documents should be placed in close proximity in the embedded space. Mathematically, we denote A1di as the coauthor set of di. Furthermore, A 2 di is the set of coauthors by extending A1diwith all neighbors of the authors in A1di , namely A 2 di\n= A1di \u222a {NBGaa(b)}b\u2208A1di ,\nwhere NBGaa(b) is the set of neighbors of node b in authorauthor network Gaa. Then the document similarity between di and dj is simply defined as wij = |A\n2 di \u2229A2dj |. Based on our problem formulation, the name disambiguation solution consists of two phases: document representation and disambiguation. Given a name reference a, its associated document set Da (which we want to disambiguate)\nand the coauthor set Aa, the document representation phase first constructs corresponding author-author network Gaa, author-document bipartite network Gad, and linked document network Gdd. Then our proposed document representation model combines structural information from these three networks to generate a k-dimensional document embedding matrix D = [dT1 , ...,d T N ] \u2208 IR\nN\u00d7k. Then the disambiguation phase takes the document embedding matrix D as input and applies the hierarchical agglomerative clustering (HAC) with group average merging criteria to partition N documents in Da into L disjoint sets with the expectation that each set is composed of documents of a unique person entity sharing the name reference a. At this stage, L is a user-defined parameter which we match with the ground truth during the evaluation phase. In real-life though, a user needs to tune the parameter L which can easily be done with HAC, because HAC provides hierarchical organization of clusters at all levels starting from a single cluster upto the case of single-instance cluster, and a user can recover clustering for any value of L as needed without additional cost. Also, across different L values the cluster assignment is consistent (i.e., two instances that are in the same cluster for some L value will remain in the same cluster for any lower L value), which helps in choosing an appropriate L value easily."}, {"heading": "4 METHOD", "text": "In this section, we discuss our proposed network embedding model for name entity disambiguation. Our goal is to encode the local neighborhood structures captured by the three networks (see Definitions 3.1 3.2 3.3) into the k-dimensional document embedding matrix with strong name disambiguation ability."}, {"heading": "4.1 Model Formulation", "text": "The main intuition of our network embedding model is that nodes sharing similar local neighborhood structure should have similar vector representation in the embedding space. For instance, in linked document network, two neighboring vertices di and dj , i.e., eij \u2208 Gdd share similar topological context in the network, hence the embedding vector for di should be a good predictor of the embedding vector of its adjacent vertex dj . This goal can be achieved by maximizing the following objective function:\nOBJdd = max D\n\u2211\ni\u2208D\n\u2211\nj:eij\u2208Gdd\nlogP (dj|di) (1)\nwhere D \u2208 IRN\u00d7k, P (dj |di) can be viewed as the conditional probability of vertex dj generated by vertex di in Gdd and can be defined by the following softmax function:\nP (dj |di) = exp(dTj di)\n\u2211N\nt=1 exp(d T t di)\n(2)\nAs shown in Equation 2, di \u2208 IR k\u00d71 is the embedding vector of document di (which we want to obtain). Based on the above formulation, if di and dj are closer, then P (dj |di) will be large. Using identical argument, the objective functions\nfor capturing author-author and author-document relations are given as below:\nOBJaa = max A\n\u2211\ni\u2208A\n\u2211\nj:eij\u2208Gaa\nlogP (aj |ai) (3)\nOBJad = max A,D\n\u2211\ni\u2208A\n\u2211\nj\u2208D:eij\u2208Gad\nlogP (dj|ai) (4)\nwhereA \u2208 IRM\u00d7k can be thought as the author embedding matrix and M is the number of authors in co-author set A. Both P (aj |ai) and P (dj |ai) can be defined the similar way as described in Equation 2.\nThe goal of proposed network embedding framework is to unify these three types of relations together, where the author and document vertices are shared across these three networks. An intuitive manner is to collectively embed these three networks, which can be achieved by minimizing the following objective function:\nOBJ = min A,D \u2212OBJaa \u2212OBJad \u2212OBJdd +\u03bbReg(A,D) (5)\nwhere \u03bbReg(A,D) in Equation 5 is a l2-norm regularization term to prevent the model from overfitting in the learning process. Here for the computational convenience, we set Reg(A,D) as \u2016A\u20162F + \u2016D\u2016 2 F ."}, {"heading": "4.2 Model Optimization", "text": "Optimizing objective function shown in Equation 5 is computational expensive. For example, in linked document relation, the calculation of P (dj |di) in Equation 2 requires summation over all the documents in the collection, which could be very inefficient since the number of documents is usually very large. To accelerate the learning speed, we adopt the approach of negative sampling proposed in Word2Vec [17]. Specifically, the negative sampling is defined by the following objective function:\nlog\u03c3(dTj di) +\nT \u2211\nt=1\nEdt\u223cPn(d)[log\u03c3(\u2212d T t di)] (6)\nwhere the logistic function \u03c3(.) is defined as \u03c3(x) = 1 1+e\u2212x .\nThe objective function of Equation 6 has two parts, the first part maximizes the similarity between two linked document embedding vectors di and dj , whereas the second part minimizes the similarity between di and dt, where dt is the embedding vector of a randomly sampled document dt. dt is also called a negative sample because (di, dt) 6\u2208 Gdd. In fact, the second part of the objective function takes T (user defined) different negative samples and pulls the document vector di away from all the document vectors of negative samples by summing the contribution from each of them. Thus, if PGdd and NGdd are positive and negative training sets in Gdd, for every positive training instance (di, dj) \u2208 PGdd , we sample T negative instances from NGdd with a probability proportional to a noise distribution, i.e., dt \u223c Pn(d) and then optimize the expression in Equation 6. The noise distribution Pn(d) is generally a pre-defined discrete distribution\nbased on the node degree in Gdd; this is similar to the idea of unigram distribution in Word2Vec [17].\nNow we can approximate OBJdd shown in Equation 1 as below:\nmax D\n\u2211\n(di,dj)\u2208PGdd\nlog\u03c3(dTj di)+ \u2211\n(di,dt)\u2208NGdd\nlog\u03c3(\u2212dTt di) (7)\nSimilarly, for author-document relation, we approximateOBJad in Equation 4 as follows:\nmax A,D\n\u2211\n(ai,dj)\u2208PGad\nlog\u03c3(dTj ai)+ \u2211\n(ai,dt)\u2208NGad\nlog\u03c3(\u2212dTt ai) (8)\nwhere in Equation 8, (ai, dj) \u2208 Gad and we can consider it as a positive instance in positive training set of Gad, denoted as PGad . For the given author ai, we randomly sample a document dt that is not connected with ai in bipartite network Gad, and then put (ai, dt) into the negative training set of Gad, denoted as NGad .\nWith the similar idea, for author-author relation, OBJaa in Equation 3 can be written as below:\nmax A\n\u2211\n(ai,aj)\u2208PGaa\nlog\u03c3(aTj ai)+ \u2211\n(ai,at)\u2208NGaa\nlog\u03c3(\u2212aTt ai) (9)\nwhere PGaa and NGaa are positive and negative training sets in Gaa respectively.\nWith the help of negative sampling approximation, the objective function of our proposed network embedding framework shown in Equation 5 can be written as:\nf = min A,D\n\u2212 \u2211\n(ai,aj)\u2208PGaa\nlog\u03c3(aTj ai)\u2212 \u2211\n(ai,at)\u2208NGaa\nlog\u03c3(\u2212aTt ai)\n\u2212 \u2211\n(ai,dj)\u2208PGad\nlog\u03c3(dTj ai)\u2212 \u2211\n(ai,dt)\u2208NGad\nlog\u03c3(\u2212dTt ai)\n\u2212 \u2211\n(di,dj)\u2208PGdd\nlog\u03c3(dTj di)\u2212 \u2211\n(di,dt)\u2208NGdd\nlog\u03c3(\u2212dTt di)\n+\u03bbReg(A,D)\n(10)\nWe adopt the asynchronous stochastic gradient descent (ASGD) algorithm for optimizing Equation 10. Specifically, in each step we sample the training instances involved in author-author, author-document, and document-document relations accordingly. The sampling strategy is based on the edge sampling [25], which samples an edge for model update with the probability proportional to the edge weight in its corresponding network. For example, given a positive training instance (di, dj) \u2208 PGdd , using the chain rule, the gradient of the objective function f in Equation 10 w.r.t. di and dj can be computed as follows:\n\u2202f \u2202di = [\u03c3(dTj di)\u2212 1] \u00b7 dj + 2\u03bbdi \u2202f \u2202dj = [\u03c3(dTj di)\u2212 1] \u00b7 di + 2\u03bbdj\n(11)\nThen di and dj are updated as below:\ndi = di \u2212 \u03b1 \u2202f\n\u2202di\ndj = dj \u2212 \u03b1 \u2202f\n\u2202dj\n(12)\nwhere \u03b1 is the learning rate. Using a similar gradient derivation, for the training instance (di, dt) \u2208 NGdd , the vectors di and dt are updated as:\ndi = di \u2212 \u03b1 \u2202f\n\u2202di\ndt = dt \u2212 \u03b1 \u2202f\n\u2202dt (13)\nLikewise, when the training instances come from author-author network, and author-document bipartite network, we update their corresponding gradients accordingly. We omit the detailed derivations here since they are very similar to the aforementioned ones.\nAlgorithm 1 Network Embedding based Name Entity Disambiguation in Anonymized Graphs\nInput: name reference a, dimension k, \u03bb, \u03b1, L Output: document embedding matrix D and its clustering\nmembership set C 1: Given name reference a, construct its associated Da, Aa,\nGaa, Gad, Gdd 2: Given Gaa, Gad, Gdd, construct training sample sets\nPGaa , NGaa , PGad , NGad , PGdd , NGdd respectively based on edge sampling technique 3: Initialize A and D as k-dimensional matrices 4: for each training instance in training sample sets do 5: Update involved parameters using ASGD as described in Section 4.2 6: end for 7: Given D and L, perform HAC to partition N documents\nin Da into L disjoint sets for name disambiguation 8: return D, C = {c1, c2, ..., cN}"}, {"heading": "4.3 Pseudo-code and Complexity Analysis", "text": "The pseudo-code of the proposed network embedding method for name entity disambiguation under anonymized graphs is summarized in Algorithm 1. The entire process consists of two phases: network embedding for document representation\nand name entity disambiguation by clustering. Specifically, given a name reference a and its associated document set Da we aim to disambiguate, we first prepare the training instances in Line 1-2. Line 3 initializes the author and document embedding matrices A and D by randomly sampling elements from Gaussian distribution with 0 mean and 0.1 standard deviation. Then we train our proposed network embedding model and update A and D using the training samples based on the ASGD optimization in Line 4-6. Then given the obtained document embedding matrix D and L, in Line 7, we perform HAC to partition N documents in Da into L disjoint sets such that each partition contains documents of a unique person entity with name reference a. Finally in Line 8, we return document embedding matrix D and its clustering membership set C = {c1, ..., ci, ..., cN}, where 1 \u2264 ci \u2264 L.\nFor the time complexity analysis, for the document embedding, when the training sample is (di, dj) \u2208 PGdd , as observed from Equations 11 and 12, the cost of calculating gradient of f w.r.t. di and dj , and updating di and dj are both O(k). Similar analysis can be applied when training instances are from PGaa , NGaa , PGad , NGad , NGdd . Therefore, the total computational cost is (\n(T + 1)|PGaa |+ (T + 1)|PGad |+ (T +\n1)|PGdd | ) O(k). For the name disambiguation, the computational cost of hierarchical clustering is O(N2logN) [27]. So the total computational complexity of Algorithm 1 is ( (T + 1)|PGaa |+(T +1)|PGad |+(T +1)|PGdd | ) O(k)+O(N2logN)."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "We perform several experiments to validate the performance of our proposed network embedding method for solving the name entity disambiguation task in a privacy-preserving setting using only linked data. We also compare our method with various other methods to demonstrate its superiority over those methods."}, {"heading": "5.1 Datasets", "text": "A key challenge for the evaluation of name entity disambiguation task is the lack of availability of labeled datasets from diverse application domains. In recent years, the bibliographic repository sites, Arnetminer 2 and CiteSeerX 3 have published several ambiguous author name references along with respective ground truths (paper list of each real-life person), which we use for evaluation. From each of these two sources, we use 10 highly ambiguous (having a larger number of distinct authors for a given name) name references and show the performance of our method on these name references. The statistics of name references in Arnetminer and CiteSeerX datasets are shown in Table 1 and Table 2, respectively. In these tables, for each name reference, we show the number of documents, and the number of distinct authors associated with that name reference. It is important to understand that the name entity disambiguation model is built on a name reference, not on a source dataset such as, Arnetminer or\n2https://aminer.org/disambiguation 3http://clgiles.ist.psu.edu/data/\nCiteSeerX as a whole, so each name reference is a distinct dataset on which the evaluation is performed."}, {"heading": "5.2 Competing Methods", "text": "To validate the disambiguation performance of our proposed approach, we compare it against 9 different methods. For a fair comparison, all of these methods accommodate the name entity disambiguation using only relational data. Among all the competing methods, Rand, AuthorList, and AuthorListNNMF are a set of primitive baselines that we have designed. But, the remaining methods are taken from recently published works. For instance, GF, DeepWalk, LINE, Node2Vec, and PTE are existing state-of-the-art approaches for vertex embedding, which we use for name entity disambiguation by clustering the documents using HAC in the embedding space similar to our approach. Graphlet based graph kernel methods (GL3, GL4) are existing state-of-the-art approaches for name entity disambiguation in anonymized graphs. More details of each of the competing methods are given below. For each method, for a given name reference, a list of documents need to be partitioned among L (user defined) different classes. (1) Rand: This naive method randomly assigns one of existing classes to the associated documents. (2) AuthorList: Given the associated documents, we first aggregate the author-list of all documents in an author-array, then define a binary feature for each author, indicating his presence or absence in the author-list of that document. Finally we use HAC with the generated author-list as features for disambiguation task. (3) AuthorList-NNMF: We perform Non-Negative Matrix Factorization (NNMF) [15] on the generated author-list features the same way described above. Then the latent features\nfrom NNMF are used in a HAC framework for disambiguation task. (4) Graph Factorization (GF) [14]: In this method, we first represent the linked document network Gdd as an affinity matrix, and then utilize matrix factorization technique to represent each document into low-dimensional vector. For model learning, GF is optimized through ASGD for a fair comparison. (5) DeepWalk [19]: DeepWalk is an approach recently proposed for network embedding, which is only applicable for network with binary edges. For each document in Gdd, we use uniform random walk to obtain the contextual information of its neighborhood for document embedding 4. (6) LINE [25]: Given Gdd, LINE aims to learn the document embedding that preserves both the first-order and second-order proximities 5. (7) Node2Vec [7]: Similar to DeepWalk, Node2Vec designs a biased random walk procedure for document embedding. 6. (8) PTE [24]: Predictive Text Embedding (PTE) framework aims to capture the relations of word-word, word-document, and word-label. However, such keyword and label based biographical features are not available in the anonymized setup. For a fair comparison, instead we utilize local structural information of both Gaa and Gad networks to learn the document embedding. However, this approach is not able to capture the linked information among documents. (9) Graph Kernel [12]: In this work, size-3 graphlets (GL3) and size-4 graphlets (GL4) are used to build graph kernels, which measure documents\u2019 similarity in Gdd. Then the learned similarity metric is used as features in HAC for name entity disambiguation. As we can see, both kernels use only the topological information of network. 7"}, {"heading": "5.3 Experimental Setting", "text": "For each of the 20 name references, we perform name disambiguation task using our proposed method and each of the competing methods to demonstrate that our proposed method is superior than the competing methods. For evaluation metric, we use Macro-F1 measure [27], which is the average of F1 measure of each class. The range of Macro-F1 measure is between 0 and 1, and a higher value indicates better disambiguation performance. Besides comparison with competing methodologies, we also perform experiments to show that our method is robust against the variation of user defined parameters (specifically, embedding dimension and the number of clusters) over a wide range of parameter values. Experiments are also performed to show the convergence of the learning model while performing the document embedding phase. Finally, we show a comparison of 2D visualization of document clusters as obtained by using different embedding methods.\n4Code is available at http://www.perozzi.net/projects/deepwalk/ 5Implementation Code is available at https://github.com/tangjianpku/LINE 6We use the code from https://github.com/aditya-grover/node2vec 7The kernel values are obtained by source code supplied by the original authors\nThere are a few user defined parameters in our proposed embedding model. The first among these is the embedding dimension k, which we set to be 20. Then we set negative sample size T (in Equation 6) as 7. For the parameters in model learning (see Section 4.2), we set the regularization parameter \u03bb as 0.005 and fix the learning rate \u03b1 = 0.001. For the disambiguation stage, we use the actual number of classes L of each name reference as input to perform HAC. For both data processing and model implementation, we implement our own code in Python and use NumPy, SciPy, scikit-learn, and Networkx libraries for linear algebra, machine learning, and graph operations. We run all the experiments on a 2.1 GHz Machine with 4GB memory running Linux operating system."}, {"heading": "5.4 Comparison among Various Name Entity Disambiguation Methods", "text": "Table 3 and Table 4 show the performance comparison of name entity disambiguation between our proposed method and other competing methods for all 20 name references (one table for ArnetMiner names, and the other for CiteSeerX names). In both tables, the rows correspond to the name references and the columns (2 to 12) stand for various methods. The competing methods are grouped logically. The first group includes the baseline methods that we have designed such as random predictor (Rand) and methods using lowdimensional factorization of author-list for clustering. The second group includes various state-of-the-art network embedding methodologies, and the third group includes two methods using graphlet based graph kernels. The cell values are the performance of a method using Macro-F1 score for disambiguation of documents under a given name reference. The last column shows the overall improvement of our proposed method compared with the best competing method. Since ASGD (Asynchronous Stochastic Gradient Descent) based optimization technique in our proposed embedding model is a randomized method, for each name reference we execute the method 5 times and report the average Macro-F1 score. For our method, we also show the standard deviation in the parenthesis. 8 For better visual comparison, we highlight the best Macro-F1 score of each name reference with bold-face font.\nAs we observe, our proposed embedding model performs the best for 9 and 8 name references (out of 10) in Table 3, and Table 4, respectively. Besides, the overall percentage improvement that our method delivers over the second best method is relatively large. For an example, consider the name entity \u201cS Lee\u201d shown in the last row of Table 4. This is a difficult disambiguation task; from Table 2, it has 1091 documents and 74 distinct real-life authors ! A random predictor (Rand) obtains a Macro-F1 of only 0.057 due to the large number of classes. Whereas our method achieves 0.569 Macro-F1 score for this name reference; the second best method for this name (GF) achieves only 0.345, indicating a\n8Standard deviation for other competing methods are not shown due to the space limit.\nsubstantial improvement (64.9%) by our method. The relatively good performance of our proposed method across all the name references is due to the fact that the method is able to learn document embedding, which is particularly suited for the name disambiguation task by facilitating information exchange among the three networks (see Section 3).\nAmong the competing methods, AuthorList based methods perform poorly because the binary features are not intelligent enough to disambiguate documents, even after using traditional low dimensional embedding by non-negative matrix factorization. Graph kernel based methods such as GL3 and GL4 also have similar fate; the possible reason could be that the size-3 and size-4 graphlet structures are not decisive patterns to distinguish documents authored by different persons. On the other hand, embedding based methods are much better as they are able to learn effective features, which bring the documents authored by the same real-life person in close proximity in the feature space. This finding justifies our approach of choosing a document embedding method for solving name entity disambiguation. Among the competing network embedding based approaches, as we can observe from all name references, no single method emerges as a clear winner. To be more precise, PTE performs poorly as it fails to incorporate linked structural information among the documents. Both GF and LINE outperform DeepWalk\nin majority of name references. This is because DeepWalk ignores the weights of the edges, which is considered to be very important in the linked document network. However, neither of embedding based competing methods could encode the document co-occurrence by exploiting the information from multiple networks, which is exploited by our proposed model."}, {"heading": "5.5 Parameter Sensitivity of Embedding Dimension", "text": "We also perform experiment to show how the embedding dimensions k affect the disambiguation performance of our proposed method and other competing embedding based methods. For all the methods, we vary the number of embedding dimension k as 10 and 30. The disambiguation results under all 20 name references from both the sources are shown in Table 5, 6, 7, and 8. As we can observe, for both datasets, our proposed method outperforms all the competing methods in most of the name references. On the other hand, for most of name references, we note that as the dimension of embeddings increases, the disambiguation performance improves significantly. The reason is due to the fact that when the embedding dimension is too small, the representation capability of the embedding vectors is not sufficient and we may lose information. However, for a few name references such as \u201cA Kumar\u201d in CiteSeerX dataset, using our proposed\nmethod, when the embedding dimension increases from 10 to 30, the corresponding disambiguation performance in terms\nof Macro-F1 drops from 0.547 to 0.520. The possible explanation could be that when the embedding dimension is large\nfor certain name reference, the proposed embedding model is too complex and we may overfit to the data."}, {"heading": "5.6 Performance Comparison over the Number of Clusters", "text": "One of the potential problems for name disambiguation is to determine the number of real-life persons L under a given name reference, because in real-life L is generally unknown a-priori. So a method whose performance is superior over a range of L values should be preferred. For this comparison, after learning the document representation, we use various L values as input in the HAC for name entity disambiguation and record the Macro-F1 score over different L for the competing methods. In our experiment, we compare Macro-F1 value of our method with two other best performing methods over several names, but due to space limitation, we show this result only for one name (\u201cLei Wang\u201d in Arnetminer) using bar-charts in Figure 2. In this figure, we compare the performance differences between our method with two other best performing methods (GF and LINE) as we vary L as {40, 45, 50, 55, 60}. Note that the actual number of distinct authors under \u201cLei Wang\u201d is 48 as shown in Table 1. As we can see, our proposed method always outperforms the state-of-the-art with all different L values, and the overall improvement of our method over these two methods is statistically significant with a p-value of less than 0.01. Because of the robustness of our proposed embedding method for name entity disambiguation regardless of L values, this is a better method for real-life application."}, {"heading": "5.7 Convergence Analysis", "text": "We further investigate the convergence of proposed network embedding algorithm shown in Section 4. Figure 3 shows the convergence analysis of our method under two name references, where \u201cLei Wang\u201d is from Arnetminer and \u201cJ Martin\u201d is from CiteSeerX. For each iteration, we select 5\u2217 |Edd| samples to update the model parameters. We can observe that our method converges approximately within 150 iterations on both name references and achieves promising convergence\nresults. However, as shown in Equation 10, the objective function in our proposed embedding model is not convex, thus reaching global optimal solution using ASGD based optimization technique is a fairly challenging task. The possible remedy could be to decrease the learning rate \u03b1 in ASGD when number of iterations increases. Another strategy is to try multiple runs with different seeds initialization. Similar convergence patterns are observed for other name references."}, {"heading": "5.8 Document Visualization", "text": "Finally we provide an illustrative document visualization result under name reference \u201cRakesh Kumar\u201d to compare the quality of various network embedding approaches. As shown in Table 1, for name reference \u201cRakesh Kumar\u201d, it contains 82 documents associated with 5 distinct real-life authors. In Figure 4, each point represents a document and its corresponding color reflects the ground-truth cluster. In addition to that, for all embedding based approaches, we set embedding dimension to be 20.\nAs we can see, the quality of visualization of PTE is worst as documents belonging to the same real-life author are not clustered tightly. The visualization results of both DeepWalk and Node2Vec are better than PTE. However, there are still significant number of noisy points between two dominant clusters shown in blue and grass green colors. This is because both methods use a random walk based approach to enrich the neighbors of the vertices, which brings in a lot of noise due to the randomness, especially for vertices with higher degree. In contrast, our method performs quite well. For example, documents within the two dominant clusters are distributed very closely. Such high quality embedding results make it well-suited for name disambiguation using HAC. Specifically, HAC with group-average merging criteria iteratively merges the closest pair of clusters based on average of all-pair distance between clusters. Therefore, if documents within the same ground-truth cluster are concentrated with each other in their corresponding embedding spaces as shown in Figure 4(a), then HAC favors to merge these documents together, leading to the desirable performance in terms of name disambiguation. The superior performance of our method is\nalso reflected in the Macro-F1 score shown in the title of the figure for convenience."}, {"heading": "6 CONCLUSION", "text": "To conclude, in this paper we propose a network embedding based solution to solve the name entity disambiguation problem. The proposed solution uses only the relational data, so it is particularly useful for name entity disambiguation in the anonymized network, where node attributes are not available due to the privacy concern. Our experimental results on multiple datasets show that the proposed method significantly outperforms the existing state-of-the-arts for name disambiguation in a similar setup."}], "references": [{"title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "author": ["Razvan Bunescu", "Marius Pasca"], "venue": "In European Chapter of the Association for Comp. Linguistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "GraRep: Learning Graph Representations with Global Structural Information", "author": ["Shaosheng Cao", "Wei Lu", "Qiongkai Xu"], "venue": "In CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Author Disambiguation by Hierarchical Agglomerative Clustering with Adaptive Stopping Criterion", "author": ["Lei Cen", "Eduard C. Dragut", "Luo Si", "Mourad Ouzzani"], "venue": "In SIGIR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Efficient Techniques for Document Sanitization", "author": ["Venkatesan T. Chakaravarthy", "Himanshu Gupta", "Prasan Roy", "Mukesh K. Mohania"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Heterogeneous Network Embedding via Deep Architectures", "author": ["Shiyu Chang", "Wei Han", "Jiliang Tang", "Guo-Jun Qi", "Charu C. Aggarwal", "Thomas S. Huang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Multi-centrality graph spectral decompositions and their application to cyber intrusion detection", "author": ["P.Y. Chen", "S. Choudhury", "A.O. Hero"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Node2Vec: Scalable Feature Learning for Networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "In SIGKDD", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Two Supervised Learning Approaches for Name Disambiguation in Author Citations", "author": ["Hui Han", "Lee Giles", "Hongyuan Zha", "Cheng Li", "Kostas Tsioutsiouliklis"], "venue": "In Joint Conf. on Digital Libraries", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Name Disambiguation in Author Citations Using a K-way Spectral Clustering Method", "author": ["Hui Han", "Hongyuan Zha", "C. Lee Giles"], "venue": "In ACM Joint Conf. on Digital Libraries", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Collective Entity Linking in Web Text: A Graph-based Method", "author": ["Xianpei Han", "Le Sun", "Jun Zhao"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Named Entity Disambiguation by Leveraging Wikipedia Semantic Knowledge", "author": ["Xianpei Han", "Jun Zhao"], "venue": "In CIKM", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Entity Disambiguation in Anonymized Graphs Using Graph Kernels", "author": ["Linus Hermansson", "Tommi Kerola", "Fredrik Johansson", "Vinay Jethava", "Devdatt Dubhashi"], "venue": "In CIKM", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Symmetric Non-negative Matrix Factorization for Graph Clustering", "author": ["Da Kuang", "Haesun Park", "Chris H.Q. Ding"], "venue": "In SDM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Algorithms for Nonnegative Matrix Factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "In NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Unsupervised Large Graph Embedding", "author": ["Feiping Nie", "Wei Zhu", "Xuelong Li"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Deep- Walk: Online Learning of Social Representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "In SIGKDD", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": "SCIENCE", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Structure Preserving Embedding", "author": ["Blake Shaw", "Tony Jebara"], "venue": "In ICML\u201909", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Efficient Topic-based Unsupervised Name Disambiguation", "author": ["Yang Song", "Jian Huang", "Isaac G. Councill", "Jia Li", "C. Lee Giles"], "venue": "JCDL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A Unified Probabilistic Framework for Name Disambiguation in Digital Library", "author": ["Jie Tang", "Alvis C.M. Fong", "Bo Wang", "Jing Zhang"], "venue": "IEEE TKDE", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "PTE: Predictive Text Embedding Through Large-scale Heterogeneous Text Networks", "author": ["Jian Tang", "Meng Qu", "Qiaozhu Mei"], "venue": "In SIGKDD", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "LINE: Large-scale Information Network Embedding", "author": ["Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Visualizing High- Dimensional Data Using t-SNE", "author": ["van der Maaten", "G.E. Hinton"], "venue": "JMLR", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Data Mining and Analysis: Fundamental Concepts and Algorithms", "author": ["Mohammed J. Zaki", "Wagner Meira Jr."], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A Constraint-based Probabilistic Framework for Name Disambiguation", "author": ["Duo Zhang", "Jie Tang", "Juanzi Li", "Kehong Wang"], "venue": "In CIKM", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Robust and Collective Entity Disambiguation Through Semantic Embeddings", "author": ["Stefan Zwicklbauer", "Christin Seifert", "Michael Granitzer"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 7, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 25, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 0, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 2, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 9, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 12, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 26, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 10, "context": "Also, contextual features such as collaborator, community affiliation, and external data source such as Wikipedia are used in some works [11, 13].", "startOffset": 137, "endOffset": 145}, {"referenceID": 12, "context": "Also, contextual features such as collaborator, community affiliation, and external data source such as Wikipedia are used in some works [11, 13].", "startOffset": 137, "endOffset": 145}, {"referenceID": 9, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 20, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 24, "context": "learning model embeds the document collection into a set of disambiguation-aware vectors in their latent space, such that a traditional hierarchical clustering [27] of the vectors generates excellent name entity disambiguation performance.", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 7, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 26, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 0, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 69, "endOffset": 75}, {"referenceID": 7, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 69, "endOffset": 75}, {"referenceID": 2, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 90, "endOffset": 96}, {"referenceID": 8, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 90, "endOffset": 96}, {"referenceID": 19, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 20, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 25, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 7, "context": "[8] proposed supervised name disambiguation methodologies by utilizing Naive Bayes and SVM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] used K-way spectral clustering for name disambiguation in bibliographical data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "For instance, [23] proposesd to use Markov Random Fields to address name entity disambiguation challenge in a unified probabilistic framework.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "To address this issue, a few works [12, 16, 28] have considered name disambiguation using anonymized graphs without leveraging the node attributes.", "startOffset": 35, "endOffset": 47}, {"referenceID": 11, "context": "For example, authors in [12] characterized the similarity between two nodes based on their local neighborhood structures using graph kernels and solved the name dis-", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "However, the major drawback of the proposed method in [12] is that it can only detect entities that should be disambiguated, but fails to further partition the documents into their corresponding homogeneous groups.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 4, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 6, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 15, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 16, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 21, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 22, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 18, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 162, "endOffset": 165}, {"referenceID": 16, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 233, "endOffset": 237}, {"referenceID": 22, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 254, "endOffset": 258}, {"referenceID": 6, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 273, "endOffset": 276}, {"referenceID": 3, "context": "However, we refrained from using word co-occurrence due to the privacy concern as sometimes a list of a set of unique words can reveal the identity of a person [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 22, "context": "The sampling strategy is based on the edge sampling [25], which samples an edge for model update with the probability proportional to the edge weight in its corresponding network.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "For the name disambiguation, the computational cost of hierarchical clustering is O(NlogN) [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "(3) AuthorList-NNMF: We perform Non-Negative Matrix Factorization (NNMF) [15] on the generated author-list features the same way described above.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "(4) Graph Factorization (GF) [14]: In this method, we first represent the linked document network Gdd as an affinity matrix, and then utilize matrix factorization technique", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "(5) DeepWalk [19]: DeepWalk is an approach recently proposed for network embedding, which is only applicable for network with binary edges.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "(6) LINE [25]: Given Gdd, LINE aims to learn the document embedding that preserves both the first-order and second-order proximities .", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "(7) Node2Vec [7]: Similar to DeepWalk, Node2Vec designs a biased random walk procedure for document embedding.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "(8) PTE [24]: Predictive Text Embedding (PTE) framework aims to capture the relations of word-word, word-document, and word-label.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "(9) Graph Kernel [12]: In this work, size-3 graphlets (GL3) and size-4 graphlets (GL4) are used to build graph kernels, which measure documents\u2019 similarity in Gdd.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "For evaluation metric, we use Macro-F1 measure [27], which is the average of F1 measure of each class.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 85, "endOffset": 88}, {"referenceID": 21, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 85, "endOffset": 88}, {"referenceID": 21, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "Figure 4: Document visualization using our proposed embedding model on name reference \u201cRakesh Kumar\u201d, visualized with the t-SNE [26] package.", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "In real-world, our DNA is unique but many people share same names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name entity disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the aspect of methodological novelty, the proposed method uses a representation learning strategy to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name entity disambiguation methods working in a similar setting.", "creator": "LaTeX with hyperref package"}}}