{"id": "1702.02302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Autonomous Braking System via Deep Reinforcement Learning", "abstract": "In this essay, we propose a new autonomous braking system based on information on the obstacle obtained from the sensors. The problem in designing the brake control is the search for the optimal policy in the Markov Decision Model (MDP), in which the state is determined by the relative position of the obstacle and the speed of the vehicle, and the scope of action is defined whether the brake is applied or not. The policy used for brake control is learned through computer simulations using the Deep Q Network (DQN) method. In order to derive the desired braking policy, we propose the reward function, which balances the damage inflicted on the obstacle in the event of an accident and the reward achieved when the vehicle runs out of risk as quickly as possible.", "histories": [["v1", "Wed, 8 Feb 2017 06:51:33 GMT  (800kb)", "http://arxiv.org/abs/1702.02302v1", null], ["v2", "Mon, 24 Apr 2017 12:43:36 GMT  (745kb)", "http://arxiv.org/abs/1702.02302v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hyunmin chae", "chang mook kang", "byeoungdo kim", "jaekyum kim", "chung choo chung", "jun won choi"], "accepted": false, "id": "1702.02302"}, "pdf": {"name": "1702.02302.pdf", "metadata": {"source": "CRF", "title": "Autonomous Braking System via Deep Reinforcement Learning", "authors": ["Hyunmin Chae", "Chang Mook Kang", "ByeoungDo Kim", "Jaekyum Kim", "Jun Won Choi"], "emails": ["hmchae@spo.hanyang.ac.kr,", "kcm0728@hanyang.ac.kr,", "jkkim}@spo.hanyang.ac.kr)", "cchung@hanyang.ac.kr)", "junwchoi@hanyang.ac.kr)"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n02 30\n2v 1\n[ cs\n.A I]\n8 F\neb 2\n01 7\nI. INTRODUCTION\nSafety is one of top priorities that should be pursued in developing autonomous vehicles. For safe autonomous driving, autonomous vehicles should perceive the environments using the sensors such as camera, GPS, Lidar, and radar, and control the vehicle such that it can travel to the destination without any accidents. Since it is inevitable for an autonomous vehicle to encounter with dangerous and unexpected situations, it is critical to develop the reliable driving systems that can cope well with such uncertainties. Recently, several safety systems including collision avoidance, pedestrian detection, and front collision warning (FCW) have been proposed to enhance the safety of the autonomous vehicle [1]\u2013[4].\nOne critical component for enabling safe autonomous driving is the autonomous braking systems which can reduce the velocity of the vehicle automatically when a threatening obstacle is detected. The autonomous braking should offer safe and comfortable brake control without exhibiting too early or too late braking. Most conventional autonomous braking systems are rule-based, which designate the specific brake control protocol for each different situation. Unfortunately, this approach is limited in handling all scenarios that\n1H. Chae, C. M. Kang, B. Kim and J. Kim are with Dept. of Electrical Engineering, Hanyang University, Seoul 04763, Korea. (e-mail: hmchae@spo.hanyang.ac.kr, kcm0728@hanyang.ac.kr, {bdkim, jkkim}@spo.hanyang.ac.kr)\n2C. C. Chung and J. W. Choi are with Div. of Electrical and Biomedical Engineering, Hanyang University, Seoul 04763, Korea. (+82-2-2220-1724, e-mail: {cchung,junwchoi}@hanyang.ac.kr)\n\u2020:corresponding author\ncan happen in real roads. Hence, it is necessary to develop the intelligent braking system which avoid the accidents in a principled and goal-oriented manner.\nRecently, interest in machine learning has explosively grown up with the rise of parallel computing technology and a large amount of training data. In particular, the success of deep neural network (DNN) technique led the researchers to investigate the application of machine learning for autonomous driving. The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9]. Reinforcement learning (RL) technique has also been improved significantly as DNN was adopted. The technique, called deep reinforcement learning (DRL), has shown to perform reasonably well for various challenging robotics and control problems. In [10], the DRL technique called Deep Q-network (DQN) was proposed, which approximates Q-value function using DNN. The DQN achieved the performance surpassing the human experts for various Atari video games. Recently, the DRL is applied to design control systems for autonomous driving in [11], [12].\nIn this paper, we propose a new autonomous braking system based DRL, which can intelligently control the velocity of the vehicle in situations where collision is expected if no action is taken. The proposed autonomous braking system is described in Fig. 1. The agent (,i.e., vehicle) interacts with the environment where the position of the obstacle changes in time, thus affecting the risk of collision at each time step. The agent receives the information on the obstacle\u2019s position using the sensors and adapts the brake control to the state change such that the chance of accident is minimized.\nIn our work, we design the autonomous braking system for the urban road scenario where a vehicle faces the pedestrian who crosses the street at random timing. In order to find the desirable brake action for the given pedestrian\u2019s location and vehicle\u2019s speed, we need to allocate appropriate reward function for each state-action pair. In our work, we focus on finding the desirable reward function which strikes the balance between the penalty imposed to the agent when accident happens and the reward obtained when the vehicle quickly gets out of risk. Using the reward function we carefully designed, we apply DQN method to learn the policy that decides the timing of brake based on the given pedestrian\u2019s state. Note that this mechanism enables goal-oriented braking operation, thus offering more robust control behavior as compared to the conventional rule-based approaches.\nVia computer simulations, we evaluate the performance of the proposed autonomous braking system. In simulations, we consider the uncertainty of the vehicle\u2019s initial velocity, pedestrian\u2019s initial position, and whether the pedestrian will cross or not. The experimental results show that the proposed braking system exhibits desirable control behavior and achieves excellent performance in collision avoidance for various test scenarios.\nThe rest of this paper is organized as follows. In Section II, we describe the basic scenarios and the framework of the proposed system. In Section III, we provide the details of the DQN design for autonomous braking. The experimental results are provided in Section IV and the paper is concluded in Section V."}, {"heading": "II. SYSTEM DESCRIPTION", "text": "In this section, we describe the overall structure of the autonomous braking system. We first define the possible\nscenarios for autonomous braking and explain the detailed operation of the proposed system."}, {"heading": "A. Scenarios", "text": "One of the factors that hinders safe driving in autonomous driving is the threat of accidents from nearby objects, e.g. pedestrians. Many accidents could happen when the vehicle cannot detect a pedestrian or fails to stop ahead of it when a pedestrian crosses the road. Hence, in order to avoid accidents, we need smart autonomous braking system which can detect the threat that can potentially cause accidents in advance and perform appropriate brake actions to stop vehicle in front of the obstacle. However, there exist various degrees of uncertainty which make the design of autonomous braking difficult such as\n\u2022 Vehicle\u2019s initial velocity \u2022 Pedestrian\u2019s position \u2022 Pedestrian\u2019s speed \u2022 Pedestrian\u2019s crossing timing \u2022 Pedestrian\u2019s moving direction \u2022 Sensor\u2019s measurement error \u2022 Road\u2019s condition\nEven if a pedestrian is detected accurately, it is hard to know when it can become a threat to the vehicle so that the braking strategy should change for different situations. (see Fig. 2.) Hence, for the given state of the pedestrian, the autonomous braking system should intelligently decide how the brake is applied.\nIn our system, we consider the scenario where behavior of the pedestrian follows the discrete-state Markov process described in Fig. 3. The state Snobody implies that the sensors have not detected any obstacle. Once a pedestrian is detected, the state Snobody can change to the state Sstay or the state Scross, where Sstay is the state that the pedestrian stays at sidewalk and Scross is the state that the pedestrian crosses the road. The pedestrian\u2019s initial position can be either from farside and near-side of the vehicle and the pedestrian walking speed can vary between vpedmin m/s and vpedmax m/s. Note that the vehicle\u2019s initial velocity distributes between vveh\nmin m/s and vvehmax m/s. It is difficult to know the transition probabilities of the Markov process and the distribution of the pedestrian\u2019s states in practical scenarios. Therefore, reinforcement learning approach can be effective\nin learning the brake control strategy through the interaction with environment."}, {"heading": "B. Autonomous Braking System", "text": "The detailed operation of the proposed autonomous braking system is depicted in Fig. 4. The vehicle is moving at speed vveh and at the position (vehposx, vehposy). As soon as a pedestrian is detected, the autonomous braking system receives the relative position of the pedestrian, i.e., (pedposx\u2212 vehposx, pedposy\u2212 vehposy) from the sensors, where (pedposx, pedposy) is the location of the pedestrian. Using the vehicle\u2019s velocity vveh and the relative position (pedposx \u2212 vehposx, pedposy \u2212 vehposy), the vehicle decides whether it will step brake at each time step. The interval between consecutive time steps is given by \u2206T . We consider only two brake actions; no braking anothing and braking adecel. We can include more brake actions with more refined steps, e.g., strong, mid, weak brake actions, which are not considered in this work."}, {"heading": "III. DEEP REINFORCEMENT LEARNING FOR AUTONOMOUS BRAKING SYSTEM", "text": "In this section, we present the details of the proposed DRL-based autonomous braking system. We first introduce the structure of the DQN and explain the architecture and the reward function used to train the DQN."}, {"heading": "A. Structure of DRL", "text": "Our system follows the basic RL structure. The agent performs an action At given state St under policy \u03c0. The agent receives the state as feedback from the environment and gets the reward rt for the action taken. The state feedback that the agent takes from sensors consists of the velocity of the vehicle vveh and the relative position to the pedestrian, (pedposx \u2212 vehposx, pedposy \u2212 vehposy) for the past n time steps. Possible action that agent can choose is either deceleration adecel or keeping the current speed anothing . Within this relationship between the agent and the environment, the goal of our proposed autonomous braking system is to maximize the accumulated reward called \u201cvalue function\u201d that will be received in the future within a episode. Using the computer simulations, the agent learns from interaction with environment episode-by-episode. One episode starts when a pedestrian is detected. Note that the\ninitial position of the pedestrian and the initial velocity of the vehicle are random. The vehicle drives on a straight way based on the brake policy \u03c0. If the distance between the vehicle and the pedestrian is less than the safety radius l, it is considered as a collision event. (see Fig. 4.) The episode is over if at least one of the following events occurs\n\u2022 Stop : the vehicle completely stops, i.e., vveh = 0. \u2022 Bump : the distance between car and pedestrian is\nbelow the safety radius l. \u2022 Pass : the vehicle reaches the destination located 5\nmeters past the pedestrian.\nOnce one episode is over, the next episode starts with the state of environment and the value function reset."}, {"heading": "B. Deep Q-Network", "text": "Q-learning is one of the popular RL methods which searches for the optimal policy in an efficient way [13]. Basically, the Q-value function q\u03c0(s, a) is defined as\nq\u03c0(s, a) = E\u03c0[\u03a3 \u221e k=0\u03b3 krt+k+1|St = s, At = a] (1)\nfor the given state s and action a. The Q-value function is the expected sum of the future rewards which indicates how good the action a is given the state s under the policy of the agent \u03c0. The contribution to the Q-value function decays exponentially with the discounting factor \u03b3 for the rewards with far-off future. For the given Q-value function, the greedy policy is obtained as\n\u03c0(s) = argmax a q\u03c0(s, a). (2)\nOne can show that for the policy provided in (2), the following Bellman equation should hold [13];\nq\u03c0(s, a) = E [\nrt+1 + \u03b3max a\u2032 q\u03c0(St+1, a \u2032)|St = s, At = a\n]\n.\n(3) In practice, since it is hard to obtain the exact value of q\u03c0(s, a) satisfying the Bellman equation, the Q-learning method uses the following update rule for the given one step backups St, At, rt+1, St+1;\nq\u03c0(St, At)\u2190 q\u03c0(St, At)\n+ \u03b1 (\nrt+1 + \u03b3max a\nq\u03c0(St+1, a)\u2212 q\u03c0(St, At) )\n(4)\nHowever, when the state space is continuous, it is impossible to find the optimal value of the state-action pair q\u2217(s, a) for all possible states. To deal with this problem, the DQN method was proposed, which approximates the state-action value function q(s, a) using the DNN, i.e., q(s, a) \u2248 q\u03b8(s, a) where \u03b8 is the parameter of the DNN [10]. The parameter \u03b8 of the DNN is then optimized to minimize the squared value of the temporal difference error \u03b4t\n\u03b4t = rt+1 + \u03b3max a\u2032 q\u03b8(St+1, a \u2032)\u2212 q\u03b8(St, At) (5)\nFor better convergence of the DQN, instead of estimating both q(St, At) and q(St+1, a\u2032) in (5), we approximate q(St, At) and q(St+1, a\u2032) using the Q-network and the target\nnetwork parameterized by \u03b8 and \u03b8\u2212, respectively. The update of the target network parameter \u03b8\u2212 is done by cloning Qnetwork parameter \u03b8, periodically. Thus, (5) becomes\n\u03b4t = rt+1 + \u03b3max a\u2032 q\u03b8\u2212(St+1, a \u2032)\u2212 q\u03b8(St, At) (6)\nTo speed up convergence further, replay memory is adopted to save one step backups by memory size and take a part of them randomly from the memory by batch size [10]. The backups in the batch is used to calculate the loss function L which is given by\nL = \u03a3t\u2208B\u03b4t 2, (7)\nwhere B is the backups in the batch. Note that the optimization of parameter \u03b8 for minimizing L is done through the stochastic gradient decent method."}, {"heading": "C. Reward Function", "text": "Unlike video games, the reward should be appropriately defined by a system designer in autonomous braking system. As mentioned, the reward function determines the behavior of the brake control. Hence, in order to ensure the reliability of the brake control, it is crucial to use the properly defined reward function. In our model, there is conflict between two important objectives of brake control; 1) collision should be avoided no matter what happens and 2) the vehicle should get out of the risky situation quickly. If it is unbalanced, the agent becomes either too conservative or reckless. Therefore, we should use the reward function which balances two conflicting objectives. Taking this into consideration, we propose the following reward function\nrt = \u03b1vt \u2212 \u03b2vt1(St = bump) + \u03bb1(St = pass) (8)\n\u03b1, \u03b2, \u03bb > 0\nwhere vt is the velocity of the vehicle at the time step t and 1(x = y) has a value of 1 the statement inside is true and 0 otherwise. The first term \u03b1vt in the reward function guides the agent to maintain its velocity since deceleration would leads to smaller accumulated reward in the future. On the other hand, the term \u2212\u03b2vt1(St = bump) indicates the negative reward (i.e., penalty) that the agent receives when the accident occurs. Note that this penalty is proportional to the vehicle\u2019s velocity, which reflects the damage to the pedestrian in case of collision. Without such dependency on the velocity, we observe that the agent does not reduce the speed in situation when the accident is not avoidable. This is why the agent desires to maximize the first term while the second term cannot be controlled. Note that \u03b1 and \u03b2 are the parameters used to control the balance between two conflicting terms. The final term \u03bb1(St = pass) is added to prevent the agent to stop at around the destination. When the agent arrives at the destination, the episode will be over so that without this term, the agent does not receiver any reward. As a result, the agent would be reluctant to drive to the destination."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we evaluate the performance of the proposed autonomous braking system via computer simulations."}, {"heading": "A. Simulation Setup", "text": "In simulations, we used the commercial software PreScan which models vehicle dynamics in real time [16]. We generated the environment in order to train the DQN by simulating the random behavior of the pedestrian. Suppose that the location information obtained from the sensors is given to the agent. We added slight measurement noise to the relative position of the pedestrian. In each episode, the initial position of vehicle is set to (0, 0). The initial velocity of the vehicle is uniformly distributed between vvehmin = 5 m/s and vvehmax = 10 m/s. Initially, the pedestrian stands either at the far-side or at near-side of the vehicle with equal probability. The pedestrian crosses the road once the vehicle passes the pedestrian crossing point ptrig. The pedestrian crossing point is also uniformly distributed between 20 m and 30 m. The safety radius l for the pedestrian is set to 6 m. The agent chooses the brake control between adecel = \u22123 m/s2 and anothing = 0 m/s2 every \u2206T = 0.2 second. The detailed simulation setup is summarized below.\n\u2022 Initial velocity of vehicle vinit \u223c U(5, 10) m/s \u2022 Velocity of pedestrian vped \u223c U(1, 1.5) m/s \u2022 Initial pedestrian position pedposx \u223c U(60, 70) m \u2022 Trigger point(crossing scenarios)\nptrig \u223c U(20, 30) m \u2022 Radius of safety circle l = 6 m \u2022 \u2206T = 0.2 s \u2022 adecel = \u22123 m/s 2, anothing = 0 m/s 2\nIn the performance test, we try four different scenarios\n\u2022 Scenario 1 : Crossing from farside of the vehicle \u2022 Scenario 2 : Crossing from nearside of the vehicle \u2022 Scenario 3 : Standing on farside of the vehicle \u2022 Scenario 4 : Standing on nearside of the vehicle\nNote that in scenario 3 and 4, we set the pedestrian crossing point ptrig to positive infinity."}, {"heading": "B. Training of DQN", "text": "We trained the DQN using 100,000 episodes. The network used in the DQN consists of five fully-connected layers which have 40, 30, 15, 5 and 2 nodes each. We used the RMSProp [15] algorithm to minimize the loss with learning rate \u00b5 = 0.0005 and set the batch size to 32. The summary of the configuration is provided below.\n\u2022 State buffer size: n = 5 \u2022 Network architecture: five fully-connected layers \u2022 Nonlinear function: leaky ReLU [14] \u2022 Number of nodes for each layers : [15(Input layer), 40,\n30, 15, 5, 2(Output layer)] \u2022 RMSProp optimizer with learning rate 0.0005 \u2022 Replay memory size: 20000 \u2022 Batch size: 32 \u2022 Reward function: \u03b1 = 1, \u03b2 = 80, \u03b3 = 20\nFig. 5 shows the plot of the total accumulated award (i.e., value function) achieved for each episode during the training. We observe the value function converges after going through 70,000 episodes and high total reward is steadily attained in the end.\n0 1 2 3 4 5 6 7 8 9 10\nEpisode \u00d7104\n-60\n-50\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40\nA cc\num ul\nat ed\nr ew\nar ds\nFig. 5. The plot of the achieved value function versus the episode index"}, {"heading": "C. Test Results", "text": "Safety test was conducted for four different scenarios. For each scenario, we test the performance of the proposed system 10,000 times. The simulation parameters are varied according to the distribution specified in the section IV.A. We evaluate the performance by counting the number of episodes which end up with the state of Bump, Stop, and Pass, respectively. Table I provides the results for each\ntest scenario. We observe that the agent avoids collision successfully for all cases in the scenario 1 and 2 (i.e., crossing scenarios). The agent just passed the pedestrian without unnecessary stops for the scenario 3 and 4 (i.e., standing scenarios). Fig. 6 shows the plot of actual position of the stopped vehicle and that of the pedestrian for 1,000 trials in the scenario 1. We see that the vehicle stops around 10 meters in front of the pedestrian for most of cases. This seems to be reasonably safe braking operation if we account for the safety radius of l = 6 m. Note that this distance can be adjusted by changing the reward parameters \u03b1 and\n\u03b2. Overall, the experimental results show that the proposed autonomous braking system exhibits reliable and consistent brake control performance."}, {"heading": "V. CONCLUSIONS", "text": "We have presented the new autonomous braking system based on the deep reinforcement learning. The proposed system learns an intelligent way of brake control from the experiences obtained under the simulated environment. We designed the autonomous braking systems using the DQN method with carefully designed reward function. We show through computer simulations that the proposed autonomous braking system exhibits desirable and consistent brake control behavior for various scenarios where behavior of the pedestrian is uncertain. One limitation of the proposed system is incapability of training of the agent in real environment since it learns from trial and errors. However, this problem will be resolved by developing more accurate and realistic simulator. It would be interesting to find a reward function that can lead to the human-like brake action using inverse reinforcement learning techniques. This topic is left for the future research."}], "references": [{"title": "Survey of pedestrian detection for advanced driver assistance systems", "author": ["D. Geronimo", "A.M. Lopez", "A.D. Sappa", "T. Graf"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 7, pp. 1239\u20131258, July 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Research advances in intelligent collision avodiance and adaptive cruise control,", "author": ["A. Vahidi", "A. Eskandarian"], "venue": "IEEE Trans. Intelligent Transportation Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Evaluation of automotive forward collision worning and collision avoidance algorithms,", "author": ["K. Lee", "H. Peng"], "venue": "Vehicle System Dynamics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Pedestrian collision avodiance systems: a survey of computer vision based recent studies,", "author": ["T. Gandhi", "M.M. Trivedi"], "venue": "IEEE Trans. Intelligent Transportation Systems Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "An empricial evaluatio of deep learning on highway driving,", "author": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "P M. Andrilukam", "Rajpurkar", "T. Migimatsu", "R. Cheng-Yue", "F. Mujica", "A. Coates", "A.Y. Ng"], "venue": "arXiv preprint,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep convolutional neural networks for pedestrian detection,", "author": ["D. Tome", "F. Monti", "L. Baroffio", "L. Bondi", "M. Tagliasacchi", "S. Tubaro"], "venue": "Signal Processing: Image Communication,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Neural network based lane change trajectory prediction in autonomous vehicles,", "author": ["R.S. Tomar", "S. Verma"], "venue": "Transactions on computational science XIII,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "DeepDriving: learning affordance for direct perception in autonomous driving,", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision(ICCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning,", "author": ["J. Koutnik", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Cooperative adaptive cruise control: a reinforcement learning approch,", "author": ["C. Desjardins", "B. Chaib-draa"], "venue": "IEEE Trans. Intelligent Transportation Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Reinforcement learning: an introduction,", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines,", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th Internation Conference on Machine Learning(ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Lecture 6.5-rmsprop: Devide the gradient by a running average of its recent magnitude,", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural network for machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently, several safety systems including collision avoidance, pedestrian detection, and front collision warning (FCW) have been proposed to enhance the safety of the autonomous vehicle [1]\u2013[4].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "Recently, several safety systems including collision avoidance, pedestrian detection, and front collision warning (FCW) have been proposed to enhance the safety of the autonomous vehicle [1]\u2013[4].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": "Recently, the DRL is applied to design control systems for autonomous driving in [11], [12].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "Recently, the DRL is applied to design control systems for autonomous driving in [11], [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Q-learning is one of the popular RL methods which searches for the optimal policy in an efficient way [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "One can show that for the policy provided in (2), the following Bellman equation should hold [13];", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "We used the RMSProp [15] algorithm to minimize the loss with learning rate \u03bc = 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "\u2022 State buffer size: n = 5 \u2022 Network architecture: five fully-connected layers \u2022 Nonlinear function: leaky ReLU [14] \u2022 Number of nodes for each layers : [15(Input layer), 40, 30, 15, 5, 2(Output layer)] \u2022 RMSProp optimizer with learning rate 0.", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle\u2019s speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.", "creator": "LaTeX with hyperref package"}}}