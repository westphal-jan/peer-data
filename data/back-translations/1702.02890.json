{"id": "1702.02890", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Answer Set Solving with Bounded Treewidth Revisited", "abstract": "Parameterized algorithms are a way to solve difficult problems more efficiently because a specific parameter of input is small. In this paper, we apply this idea to the area of programming response records (ASP). To this end, we propose two types of graphical representations of programs to use their tree width as parameters. Tree width roughly measures the extent to which the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms that run in linear time when the tree width and weights of the given program are limited. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behavior for benchmark instances of low tree width, especially for counting response records.", "histories": [["v1", "Thu, 9 Feb 2017 16:50:23 GMT  (564kb,D)", "http://arxiv.org/abs/1702.02890v1", "This paper extends and updates a paper that has been presented on the workshop TAASP'16 (arXiv:1612.07601). We provide a higher detail level, full proofs and more examples"]], "COMMENTS": "This paper extends and updates a paper that has been presented on the workshop TAASP'16 (arXiv:1612.07601). We provide a higher detail level, full proofs and more examples", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.CC", "authors": ["johannes fichte", "markus hecher", "michael morak", "stefan woltran"], "accepted": false, "id": "1702.02890"}, "pdf": {"name": "1702.02890.pdf", "metadata": {"source": "CRF", "title": "Answer Set Solving with Bounded Treewidth Revisited\u2217", "authors": ["Johannes K. Fichte", "Michael Morak", "Markus Hecher", "Stefan Woltran"], "emails": ["lastname@dbai.tuwien.ac.at"], "sections": [{"heading": "1 Introduction", "text": "Parameterized algorithms [14, 5] have attracted considerable interest in recent years and allow to tackle hard problems by directly exploiting a small parameter of the input problem. One particular goal in this field is to find guarantees that the runtime is exponential exclusively in the parameter, and polynomial in the input size (so-called fixed-parameter tractable algorithms). A parameter that has been researched extensively is treewidth [16, 2]. Generally speaking, treewidth measures the closeness of a graph to a tree, based on the observation that problems on trees are often easier than on arbitrary graphs. A parameterized algorithm exploiting small treewidth takes a tree decomposition, which is an arrangement of a graph into a tree, and evaluates the problem in parts, via dynamic programming (DP) on the tree decomposition.\nASP [3, 13] is a logic-based declarative modelling language and problem solving framework where solutions, so called answer sets, of a given logic program directly represent the solutions of the modelled problem. Jakl et al. [11] give a DP algorithm for disjunctive rules only, whose runtime is linear in the input size of the program and double exponential in the treewidth of a particular graph representation of the program structure. However, modern ASP systems allow for an extended syntax that includes, among others, weight rules and choice rules. Pichler et al. [15] investigated the complexity of programs with weight rules. They also presented DP algorithms for programs with cardinality rules (i.e., restricted version of weight rules), but without disjunction.\nIn this paper, we propose DP algorithms for finding answer sets that are able to directly treat all kinds of ASP rules. While such rules can be transformed into disjunctive rules, we avoid the resulting polynomial overhead with our algorithms. In particular, we present two approaches based on two different types of graphs representing the program structure. Firstly, we consider the primal graph, which allows for an intuitive algorithm that also treats the extended ASP rules. While for a given disjunctive program the treewidth of the primal graph may be larger than treewidth of the graph representation used by\n\u2217This is the authors self-archived copy including detailed proofs. A preliminary version of the paper was presented on the workshop TAASP\u201916. Research was supported by the Austrian Science Fund (FWF), Grant Y698. \u2020Also affiliated with the Institute of Computer Science and Computational Science at University of Potsdam, Germany.\nar X\niv :1\n70 2.\n02 89\n0v 1\n[ cs\n.L O\n] 9\nF eb\n2 01\nJakl et al. [11], our algorithm uses simpler data structures and lays the foundations to understand how we can handle also extended rules. Our second graph representation is the incidence graph, a generalization of the representation used by Jakl et al.. Algorithms for this graph representation are more sophisticated, since weight and choice rules can no longer be completely evaluated in the same computation step. Our algorithms yield upper bounds that are linear in the program size, double-exponential in the treewidth, and single-exponential in the maximum weights. We extend two algorithms to count optimal answer sets. For this particular task, experiments show that we are able to outperform existing systems from multiple domains, given input instances of low treewidth, both randomly generated and obtained from real-world graphs of traffic networks. Our system is publicly available on github1."}, {"heading": "2 Formal Background", "text": ""}, {"heading": "2.1 Answer Set programming (ASP)", "text": "ASP is a declarative modeling and problem solving framework; for a full introduction, see, e.g., [3, 13]. Stateof-the-art ASP grounders support the full ASP-Core-2 language [4] and output smodels input format [19], which we will use for our algorithms. Let `, m, n be non-negative integers such that ` \u2264 m \u2264 n, a1, . . ., an distinct propositional atoms, w, w1, . . ., wn non-negative integers, and l \u2208 {a1,\u00aca1}. A choice rule is an expression of the form, {a1; . . . ; a`} \u2190 a`+1, . . . , am,\u00acam+1, . . . ,\u00acan, a disjunctive rule is of the form a1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 a` \u2190 a`+1, . . . , am,\u00acam+1, . . . ,\u00acan and a weight rule is of the form a` \u2190 w 6 {a`+1 = w`+1, . . . , am = wm, \u00acam+1 = wm+1, . . . ,\u00acan = wn}. Finally, an optimization rule is an expression of the form l[w]. A rule is either a disjunctive, a choice, a weight, or an optimization rule.\nFor a choice, disjunctive, or weight rule r, let Hr := {a1, . . . , a`}, B+r := {a`+1, . . . , am}, and B\u2212r := {am+1, . . . , an}. For a weight rule r, let wght(r, a) map atom a to its corresponding weight wi in rule r if a = ai for `+ 1 \u2264 i \u2264 n and to 0 otherwise, let wght(r,A) := \u2211 a\u2208A wght(r, a) for a set A of atoms, and let bnd(r) := w be its bound. For an optimization rule r, let cst(r) := w and if l = a1, let B + r := {a1} and B\u2212r := \u2205; or if l = \u00aca1, let B\u2212r := {a1} and B+r := \u2205. For a rule r, let at(r) := Hr \u222aB+r \u222aB\u2212r denote its atoms and Br := B + r \u222a {\u00acb | b \u2208 B\u2212r } its body. A program \u03a0 is a set of rules. Let at(\u03a0) := {at(r) | r \u2208 \u03a0} and let CH(\u03a0),DISJ(\u03a0),OPT(\u03a0) and WGT(\u03a0) denote the set of all choice, disjunctive, optimization and weight rules in \u03a0, respectively.\nA set M \u2286 at(\u03a0) satisfies a rule r if (i) (Hr\u222aB\u2212r )\u2229M 6= \u2205 or B+r 6\u2286M for r \u2208 DISJ(\u03a0), (ii) Hr\u2229M 6= \u2205 or \u03a3ai\u2208M\u2229B+r wi + \u03a3ai\u2208B\u2212r \\M wi < bnd(r) for r \u2208WGT(\u03a0), or (iii) r \u2208 CH(\u03a0) \u222aOPT(\u03a0). M is a model of \u03a0, denoted by M \u03a0, if M satisfies every rule r \u2208 \u03a0. Further, let Mod(C,\u03a0) := {C | C \u2208 C, C \u03a0} for C \u2286 2at(\u03a0).\nThe reduct rM (i) of a choice rule r is the set {a \u2190 B+r | a \u2208 Hr \u2229 M,B\u2212r \u2229 M = \u2205} of rules, (ii) of a disjunctive rule r is the singleton {Hr \u2190 B+r | B\u2212r \u2229 M = \u2205}, and (iii) of a weight rule r is the singleton {Hr \u2190 w\u2032 6 [a = wght(r, a) | a \u2208 B+r ]} where w\u2032 = bnd(r) \u2212 \u03a3a\u2208B\u2212r \\M wght(r, a). \u03a0M := {r\u2032 | r\u2032 \u2208 rM , r \u2208 \u03a0} is called GL reduct of \u03a0 with respect to M . A set M \u2286 at(\u03a0) is an answer set of program \u03a0 if (i) M \u03a0 and (ii) there is no M \u2032 (M such that M \u2032 \u03a0M , that is, M is subset minimal with respect to \u03a0M .\nWe call cst(\u03a0,M,A) := \u03a3r\u2208OPT(\u03a0), A\u2229[(B+r \u2229M)\u222a(B\u2212r \\M)] 6=\u2205 cst(r) the cost of model M for \u03a0 with respect\nto the set A \u2286 at(\u03a0). An answer set M of \u03a0 is optimal if its cost is minimal over all answer sets. 1See https://github.com/daajoe/dynasp.\nExample 1. Let \u03a0 := { r1\ufe37 \ufe38\ufe38 \ufe37 {a; b} \u2190 c; r2\ufe37 \ufe38\ufe38 \ufe37 c\u2190 1 6 {b = 1,\u00aca = 1}; r3\ufe37 \ufe38\ufe38 \ufe37\nd \u2228 a\u2190}. Then, the sets {a}, {c, d} and {b, c, d} are answer sets of \u03a0.\nGiven a program \u03a0, we consider the problems of computing an answer set (called AS) and outputting the number of optimal answer sets (called #AspO).\nNext, we show that under standard complexity-theoretic assumptions #Asp is strictly harder than #SAT.\nTheorem 1. #Asp for programs without optimization is #\u00b7coNP-complete.\nProof. Observe that programs containing choice and weight rules can be compiled to disjunctive ones (normalization) without these rule types (see [8]) using a polynomial number (in the original program size) of rules. Membership follows from the fact that, given such a nice program \u03a0 and an interpretation I, checking whether I is an answer of \u03a0 is coNP-complete, see e.g., [12]. Hardness is a direct consequence of #\u00b7coNP-hardness for the problem of counting subset minimal models of a CNF formula [6], since answer sets of negation-free programs and subset-minimal models of CNF formulas are essentially the same objects.\nRemark 1. The counting complexity of #Asp including optimization rules (i.e., where only optimal answer sets are counted) is slightly higher; exact results can be established employing hardness results from other sources [10]."}, {"heading": "2.2 Tree Decompositions", "text": "Let G = (V,E) be a graph, T = (N,F, n) a rooted tree, and \u03c7 : N \u2192 2V a function that maps each node t \u2208 N to a set of vertices. We call the sets \u03c7(\u00b7) bags and N the set of nodes. Then, the pair T = (T, \u03c7) is a tree decomposition (TD) of G if the following conditions hold: (i) all vertices occur in some bag, that is, for every vertex v \u2208 V there is a node t \u2208 N with v \u2208 \u03c7(t); (ii) all edges occur in some bag, that is, for every edge e \u2208 E there is a node t \u2208 N with e \u2286 \u03c7(t); and (iii) the connectedness condition: for any three nodes t1, t2, t3 \u2208 N , if t2 lies on the unique path from t1 to t3, then \u03c7(t1) \u2229 \u03c7(t3) \u2286 \u03c7(t2). We call max{|\u03c7(t)| \u2212 1 | t \u2208 N} the width of the TD. The treewidth tw(G) of a graph G is the minimum width over all possible TDs of G.\nNote that each graph has a trivial TD (T, \u03c7) consisting of the tree ({n}, \u2205, n) and the mapping \u03c7(n) = V . It is well known that the treewidth of a tree is 1, and a graph containing a clique of size k has at least treewidth k\u2212 1. For some arbitrary but fixed integer k and a graph of treewidth at most k, we can compute a TD of width 6 k in time 2O(k\n3) \u00b7 |V | [2]. Given a TD (T, \u03c7) with T = (N, \u00b7, \u00b7), for a node t \u2208 N we say that type(t) is leaf if t has no children; join if t has children t\u2032 and t\u2032\u2032 with t\u2032 6= t\u2032\u2032 and \u03c7(t) = \u03c7(t\u2032) = \u03c7(t\u2032\u2032); int (\u201cintroduce\u201d) if t has a single child t\u2032, \u03c7(t\u2032) \u2286 \u03c7(t) and |\u03c7(t)| = |\u03c7(t\u2032)|+ 1; rem (\u201cremoval\u201d) if t has a single child t\u2032, \u03c7(t) \u2286 \u03c7(t\u2032) and |\u03c7(t\u2032)| = |\u03c7(t)| + 1. If every node t \u2208 N has at most two children, type(t) \u2208 {leaf, join, int, rem}, and bags of leaf nodes and the root are empty, then the TD is called nice. For every TD, we can compute a nice TD in linear time without increasing the width [2]. In our algorithms, we will traverse a TD bottom up, therefore, let post-order(T, t) be the sequence of nodes in post-order of the induced subtree T \u2032 = (N \u2032, \u00b7, t) of T rooted at t.\nExample 2. Figure 1 (left) shows a graph G1 together with a TD of G1 that is of width 2. Note that G1 has treewidth 2, since it contains a clique on the vertices {a, b, c}. Further, the TD T in Figure 2 is a nice TD of G1.\nAlgorithm 1: Algorithm DPA(T ) for Dynamic Programming on TD T for ASP. In: Table algorithm A, nice TD T = (T, \u03c7) with T = (N, \u00b7, n) of G(\u03a0) according to A. Out: Table: maps each TD node t \u2208 T to some computed table \u03c4t.\n1 for iterate t in post-order(T,n) do 2 Child-Tabs := {Tables[t\u2032] | t\u2032 is a child of t in T} 3 Tables[t] := A(t, \u03c7(t),\u03a0t, at\u2264t,Child-Tabs)"}, {"heading": "2.3 Graph Representations of Programs", "text": "In order to use TDs for ASP solving, we need dedicated graph representations of ASP programs. The primal graph P (\u03a0) of program \u03a0 has the atoms of \u03a0 as vertices and an edge a b if there exists a rule r \u2208 \u03a0 and a, b \u2208 at(r). The incidence graph I(\u03a0) of \u03a0 is the bipartite graph that has the atoms and rules of \u03a0 as vertices and an edge a r if a \u2208 at(r) for some rule r \u2208 \u03a0. These definitions adapt similar concepts from SAT [17].\nExample 3. Recall program \u03a0 of Example 1. We observe that graph G1 (G2) in the left (right) part of Figure 1 is the primal (incidence) graph of \u03a0."}, {"heading": "2.4 Sub-Programs", "text": "Let T = (T, \u03c7) be a nice TD of graph representation H \u2208 {I(\u03a0), P (\u03a0)} of a program \u03a0. Further, let T = (N, \u00b7, n) and t \u2208 N . The bag-rules are defined as \u03a0t := {r | r \u2208 \u03a0, at(r) \u2286 \u03c7(t)} if H is the primal graph and as \u03a0t := \u03a0 \u2229 \u03c7(t) if H is the incidence graph. Further, the set at\u2264t := {a | a \u2208 at(\u03a0) \u2229 \u03c7(t\u2032), t\u2032 \u2208 post-order(T, t)} is called atoms below t, the program below t is defined as \u03a0\u2264t := {r | r \u2208 \u03a0t\u2032 , t\u2032 \u2208 post-order(T, t)}, and the program strictly below t is \u03a0<t := \u03a0\u2264t \\ \u03a0t. It holds that \u03a0\u2264n = \u03a0<n = \u03a0 and at\u2264n = at(\u03a0).\nExample 4. Intuitively, TDs of Figure 1 enable us to evaluate \u03a0 by analyzing sub-programs ({r1, r2} and {r3}) and combining results agreeing on a. Indeed, for the given TD of Figure 1 (left), \u03a0\u2264t1 = {r1, r2}, \u03a0\u2264t2 = {r3} and \u03a0 = \u03a0\u2264t3 = \u03a0<t3 = \u03a0t1 \u222a\u03a0t2 . For the TD of Figure 1 (right), we have \u03a0\u2264t1 = {r1, r2} and at\u2264t1 = {b, c}, as well as \u03a0\u2264t3 = {r3} and at\u2264t3 = {a, d}. Moreover, for TD T of Figure 2, \u03a0\u2264t1 = \u03a0\u2264t2 = \u03a0\u2264t3 = \u03a0<t4 = \u2205, at\u2264t3 = {a, b} and \u03a0\u2264t4 = {r1, r2}."}, {"heading": "3 ASP via Dynamic Programming on TDs", "text": "In the next two sections, we propose two dynamic programming (DP) algorithms, DPPRIM and DP INC, for ASP without optimization rules based on two different graph representations, namely the primal and the incidence graph. Both algorithms make use of the fact that answer sets of a given program \u03a0 are (i) models of \u03a0 and (ii) subset minimal with respect to \u03a0M . Intuitively, our algorithms compute, for each TD node t, (i) sets of atoms\u2014(local) witnesses\u2014representing parts of potential models of \u03a0, and (ii) for each local witness M subsets of M\u2014(local) counterwitnesses\u2014representing subsets of potential models of \u03a0M which (locally) contradict that M can be extended to an answer set of \u03a0. We give the the basis of our algorithms in Algorithm 1 (DPA), which sketches the general DP scheme for ASP solving on TDs. Roughly, the algorithm splits the search space based on a given nice TD and evaluates the input program \u03a0 in parts. The results are stored in so-called tables, that is, sets of all possible tuples of witnesses and counterwitnesses for a given TD node. To this end, we define the table algorithms PRIM and INC, which compute tables for a node t of the TD using the primal graph P (\u03a0) and incidence graph I(\u03a0), respectively. To be more concrete, given a table algorithm A \u2208 {PRIM, INC}, algorithm DPA visits every node t \u2208 T in post-order; then, based on \u03a0t, computes a table \u03c4t for node t from the tables of the children of t, and\nAlgorithm 2: Table algorithm PRIM(t, \u03c7t,\u03a0t, \u00b7,Child-Tabs). In: Bag \u03c7t, bag-rules \u03a0t and child tables Child-Tabs of node t. Out: Table \u03c4t.\n1 if type(t) = leaf then \u03c4t := {\u3008\u2205, \u2205\u3009} /* Abbreviations see Footnote 2. */ 2 else if type(t) = int, a \u2208 \u03c7t is introduced and \u03c4 \u2032 \u2208 Child-Tabs then 3 \u03c4t := {\u3008M+a , Mod({M} \u222a [C t {a}] \u222a C,\u03a0 M+a t )\u3009 | \u3008M, C\u3009 \u2208 \u03c4 \u2032,M+a \u03a0t} \u22c3 4 {\u3008M, Mod(C,\u03a0Mt )\u3009 | \u3008M, C\u3009 \u2208 \u03c4 \u2032,M \u03a0t} 5 else if type(t) = rem, a 6\u2208 \u03c7t is removed and \u03c4 \u2032 \u2208 Child-Tabs then 6 \u03c4t := {\u3008M\u2212a , {C\u2212a | C \u2208 C}\u3009 | \u3008M, C\u3009 \u2208 \u03c4 \u2032} 7 else if type(t) = join and \u03c4 \u2032, \u03c4 \u2032\u2032 \u2208 Child-Tabs with \u03c4 \u2032 6= \u03c4 \u2032\u2032 then 8 \u03c4t := {\u3008M, (C\u2032 \u2229 C\u2032\u2032) \u222a (C\u2032 \u2229 {M}) \u222a ({M} \u2229 C\u2032\u2032)\u3009 | \u3008M, C\u2032\u3009 \u2208 \u03c4 \u2032, \u3008M, C\u2032\u2032\u3009 \u2208 \u03c4 \u2032\u2032}\nstores \u03c4t in Tables[t]."}, {"heading": "3.1 Using Decompositions of Primal Graphs", "text": "In this section, we present our algorithm PRIM in two parts: (i) finding models of \u03a0 and (ii) finding models which are subset minimal with respect to \u03a0M . For sake of clarity, we first present only the first tuple positions (red parts) of Algorithm 2 (PRIM) to solve (i). We call the resulting table algorithm MOD.\nExample 5. Consider program \u03a0 from Example 1 and in Figure 2 (left) TD T = (\u00b7, \u03c7) of P (\u03a0) and the tables \u03c41, . . ., \u03c412, which illustrate computation results obtained during post-order traversal of T by DPMOD. Table \u03c41 = {\u3008\u2205\u3009} as type(t1) = leaf. Since type(t2) = int, we construct table \u03c42 from \u03c41 by taking M1.i and M1.i \u222a {a} for each M1.i \u2208 \u03c41 (corresponding to a guess on a). Then, t3 introduces b and t4 introduces c. \u03a0t1 = \u03a0t2 = \u03a0t3 = \u2205, but since \u03c7(t4) \u2286 at(r1) \u222a at(r2) we have \u03a0t4 = {r1, r2} for t4. In consequence, for each M4.i of table \u03c44, we have M4.i {r1, r2} since MOD enforces satisfiability of \u03a0t in node t. We derive tables \u03c47 to \u03c49 similarly. Since type(t5) = rem, we remove atom b from all elements in \u03c44 to construct \u03c45. Note that we have already seen all rules where b occurs and hence b can no longer affect witnesses during the remaining traversal. We similarly construct \u03c4t6 = \u03c410 = {\u3008\u2205\u3009, \u3008a\u3009}. Since type(t11) = join, we construct table \u03c411 by taking the intersection \u03c46 \u2229 \u03c410. Intuitively, this combines witnesses agreeing on a. Node t12 is again of type rem. By definition (primal graph and TDs) for every r \u2208 \u03a0, atoms at(r) occur together in at least one common bag. Hence, \u03a0 = \u03a0\u2264t12 and since \u03c412 = {\u3008\u2205\u3009}, we can construct a model of \u03a0 from the tables. For example, we obtain the model {a, d} = M11.2 \u222aM4.2 \u222aM9.3.\nObservation 1. Let \u03a0 be a program and T a TD of the primal graph of \u03a0. Then, for every rule r \u2208 \u03a0 there is at least one bag in T containing all atoms of r.\nProof. By Definition the primal graph contains a clique on all atoms a participating in a rule r. Since a TD must contain each edge of the original graph in some bag and has to be connected, it follows that there is at least one bag containing all (clique) atoms a of r.\nPRIM is given in Algorithm 2. Tuples in \u03c4t are of the form \u3008M, C\u3009. Witness M \u2286 \u03c7(t) represents a model of \u03a0t witnessing the existence of M\n\u2032 \u2287M with M \u2032 \u03a0\u2264t. The family C \u2286 2M contains sets of models C \u2286M of the GL reduct (\u03a0t)\nM . C witnesses the existence of a set C \u2032 with counterwitness C \u2286 C \u2032 ( M \u2032 and C \u2032 (\u03a0\u2264t)M \u2032 . There is an answer set of \u03a0 if table tn for root n contains \u3008\u2205, \u2205\u3009. Since in Example 5 we already explained the first tuple position and thus the witness part, we only briefly describe the parts for counterwitnesses. In the introduce case, we want to store only counterwitnesses for not being minimal with respect to the GL reduct of the bag-rules. Therefore, in Line 3 we construct for M+a counterwitnesses from either some witness M (M ( M+a ), or of any C \u2208 C, or of any C \u2208 C extended by a (every C \u2208 C was already a counterwitness before). Line 4 ensures that only counterwitnesses that are models of the\n2 S t {e} := {S \u222a {e} | S \u2208 S}, S+e := S \u222a {e}, and S\u2212e := S \\ {e}\nGL reduct \u03a0Mt are stored (via Mod(\u00b7, \u00b7)). Line 6 restricts counterwitnesses to its bag content, and Line 8 enforces that child tuples agree on counterwitnesses.\nExample 6. Consider Example 5, its TD T = (\u00b7, \u03c7), Figure 2 (right), and the tables \u03c41, . . ., \u03c412 obtained by DPPRIM. Since we have at(r1) \u222a at(r2) \u2286 \u03c7(t4), we require C4.i.j {r1, r2}M4.i for each counterwitness C4.i.j \u2208 C4.i in tuples of \u03c44. For M4.5 = {a, b, c} observe that the only counterwitness of {r1, r2}M4.5 = {a\u2190 c, b\u2190 c, c\u2190 1 \u2264 {b = 1}} is C4.5.1 = {a}. Note that witness M11.2 of table \u03c411 is the result of joining M4.2 with M9.1 and witness M11.3 (counterwitness C11.3.1) is the result of joining M4.3 with M9.3 (C4.3.1 with C9.3.1), and M4.5 with M9.3 (C4.5.1 with C9.3.2). C11.3.1 witnesses that neither M4.3\u222aM9.3 nor M4.5 \u222aM9.3 forms an answer set of \u03a0. Since \u03c412 contains \u3008\u2205, \u2205\u3009 there is no counterwitness for M11.2, we can construct an answer set of \u03a0 from the tables, e.g., {a} can be constructed from M4.2 \u222aM9.1.\nTheorem 2. Given a program \u03a0, the algorithm DPPRIM is correct and runs in time O(22 k+2 \u00b7 \u2016P (\u03a0)\u2016) where k is the treewidth of the primal graph P (\u03a0).\nProof. We refer to Appendix B.1."}, {"heading": "3.2 Using Decompositions of Incidence Graphs", "text": "Our next algorithm (DP INC) takes the incidence graph as graph representation of the input program. The treewidth of the incidence graph is smaller than the treewidth of the primal graph plus one, cf., [17, 7]. More importantly, the incidence graph does not enforce cliques on at(r) for some rule r. The incidence graph, compared to the primal graph, additionally contains rules as vertices and its relationship to the atoms in terms of edges. By definition, we have no guarantee that all atoms of a rule occur together in the same bag of TDs of the incidence graph. For that reason, we cannot locally check the satisfiability of a rule when traversing the TD without additional stored information (so-called rule-states that intuitively represent how much of a rule is already (dis-)satisfied). We only know that for each rule r there is a path p = tint, t1, . . . , tm, trem where tint introduces r and trem removes r and when considering trem in the table algorithm we have seen all atoms that occur in rule r. Thus, on removal of r in trem we ensure that r is satisfied while taking rule-states for choice and weight rules into account. Consequently, our tuples will contain a witness, its rule-state, and counterwitnesses and their rule-states.\nA tuple in \u03c4t for Algorithm 3 (INC) is a triple \u3008M,\u03c3, C\u3009. The set M \u2286 at(\u03a0) \u2229 \u03c7(t) represents again a witness. A rule-state \u03c3 is a mapping \u03c3 : \u03a0t \u2192 N0\u222a{\u221e}. A rule state for M represents whether rules of \u03c7(t) are either (i) satisfied by a superset of M or (ii) undecided for M . Formally, the set SR(\u03a0t, \u03c3) of satisfied bag-rules \u03a0t consists of each rule r \u2208 \u03a0t such that \u03c3(r) =\u221e. Hence, M witnesses a model M \u2032 \u2287M where M \u2032 \u03a0<t \u222a SR(\u03a0t, \u03c3). C concerns counterwitnesses.\n3\u03c3 ] \u03c1 := {(x,\u03a3(x,c1)\u2208\u03c3c1 + \u03a3(x,c2)\u2208\u03c1c2) | (x, \u00b7) \u2208 \u03c3 \u222a \u03c1}; \u03c3 + r := \u03c3 \u222a {(r, 0)}; \u03c3\u2212S := {(x, y) \u2208 \u03c3 | x 6\u2208 S}.\nAlgorithm 3: Table algorithm INC(t, \u03c7t,\u03a0t, at\u2264t,Child-Tabs).\nIn: Bag \u03c7t, bag-rules \u03a0t, atoms-below at\u2264t, child tables Child-Tabs of t. Out: Tab. \u03c4t. 1 if type(t) = leaf then \u03c4t := {\u3008\u2205, \u2205, \u2205\u3009} /* Abbreviations see Footnote 3. */ 2 else if type(t) = int, a \u2208 \u03c7t \\\u03a0t is introduced and \u03c4 \u2032 \u2208 Child-Tabs then 3 \u03c4t := {\u3008M+a , \u03c3 ] SatRules(\u03a0\u0307(t,\u03c3)t ,M+a ), {\u3008M,\u03c3 ] SatRules(\u03a0\u0307 (t,\u03c3,M+a ) t ,M)\u3009} \u222a 4 {\u3008C+a , \u03c1] SatRules(\u03a0\u0307 (t,\u03c1,M+a ) t , C + a )\u3009 | \u3008C, \u03c1\u3009 \u2208 C} \u222a 5 {\u3008C, \u03c1] SatRules(\u03a0\u0307(t,\u03c1,M + a ) t , C)\u3009 | \u3008C, \u03c1\u3009 \u2208 C}\u3009 | \u3008M,\u03c3, C\u3009 \u2208 \u03c4 \u2032} \u22c3 6 {\u3008M,\u03c3 ] SatRules(\u03a0\u0307(t,\u03c3)t ,M), 7 {\u3008C, \u03c1] SatRules(\u03a0\u0307(t,\u03c1,M)t , C)\u3009 | \u3008C, \u03c1\u3009 \u2208 C}\u3009 | \u3008M,\u03c3, C\u3009 \u2208 \u03c4 \u2032} 8 else if type(t) = int, r \u2208 \u03c7t \u2229\u03a0t is introduced and \u03c4 \u2032 \u2208 Child-Tabs then 9 \u03c4t := {\u3008M,\u03c3+r ] SatRules({r\u0307}(t,\u03c3 + r }),M),\n10 {\u3008C, \u03c1+r ] SatRules({r\u0307}(t,\u03c1 + r ,M), C)\u3009 | \u3008C, \u03c1\u3009 \u2208 C}\u3009 | \u3008M,\u03c3, C\u3009 \u2208 \u03c4 \u2032} 11 else if type(t) = rem, a 6\u2208 \u03c7t is removed atom and \u03c4 \u2032 \u2208 Child-Tabs then 12 \u03c4t := {\u3008M\u2212a , \u03c3 ] UpdtWgt(\u03a0t,M, a), 13 {\u3008C\u2212a , \u03c1] UpdtWgt&Ch(\u03a0t,M,C, a)\u3009 | \u3008C, \u03c1\u3009 \u2208 C}\u3009 | \u3008M,\u03c3, C\u3009 \u2208 \u03c4 \u2032} 14 else if type(t) = rem, r 6\u2208 \u03c7t is removed rule and \u03c4 \u2032 \u2208 Child-Tabs then 15 \u03c4t := {\u3008M,\u03c3\u2212{r}, { \u3008C, \u03c1\u2212{r}\u3009 | \u3008C, \u03c1\u3009 \u2208 C, \u03c1(r) =\u221e } \u3009 | \u3008M,\u03c3, C\u3009 \u2208 \u03c4 \u2032, \u03c3(r) =\u221e} 16 else if type(t) = join and \u03c4 \u2032, \u03c4 \u2032\u2032 \u2208 Child-Tabs with \u03c4 \u2032 6= \u03c4 \u2032\u2032 then 17 \u03c4t := {\u3008M,\u03c3\u2032 ] \u03c3\u2032\u2032, {\u3008C, \u03c1\u2032 ] \u03c1\u2032\u2032\u3009 | \u3008C, \u03c1\u2032\u3009 \u2208 C\u2032, \u3008C, \u03c1\u2032\u2032\u3009 \u2208 C\u2032\u2032} \u222a 18 {\u3008M,\u03c1] \u03c3\u2032\u2032\u3009 | \u3008M,\u03c1\u3009 \u2208 C\u2032} \u222a 19 {\u3008M,\u03c3\u2032 ] \u03c1\u3009 | \u3008M,\u03c1\u3009 \u2208 C\u2032\u2032}\u3009 | \u3008M,\u03c3\u2032, C\u2032\u3009 \u2208 \u03c4 \u2032, \u3008M,\u03c3\u2032\u2032, C\u2032\u2032\u3009 \u2208 \u03c4 \u2032\u2032}\nWe compute a new rule-state \u03c3 from a rule-state, \u201cupdated\u201d bounds for weight rules (UpdtWgt), and satisfied rules (SatRules, defined below). We define UpdtWgt(\u03a0t,M, a) := \u03c3\n\u2032 depending on an atom a with \u03c3\u2032(r) := wght(r, {a} \u2229 [(B\u2212r \\M) \u222a (B+r \u2229M)]), if r \u2208 WGT(\u03a0t). We use binary operator ]3 to combine rule-states, which ensures that rules satisfied in at least one operand remain satisfied. Next, we explain the meaning of rule-states.\nExample 7. Consider program \u03a0 from Example 1 and TD T \u2032 = (\u00b7, \u03c7) of I(\u03a0) and the tables \u03c41, . . ., \u03c418 in Figure 3 (left). We are only interested in the first two tuple positions (red and green parts) and implicitly assume that \u201ci\u201d refers to Line i in the respective table. Consider M4.1 = {c} in table \u03c44. Since Hr2 = {c}, witness M4.1 = {c} satisfies rule r2. As a result, \u03c34.1(r2) = \u221e remembering satisfied rule r2 for M4.1. Since c /\u2208 M4.2 and B+r1 = {c}, M4.2 satisfies rule r1, resulting in \u03c34.2(r1) = \u221e. Rule-state \u03c34.1(r1) represents that r1 is undecided for M4.2. For weight rule r2, rule-states remember the sum of body weights involving removed atoms. Consider M6.2 = M6.3 = \u2205 of table \u03c46. We have \u03c36.2(r2) 6= \u03c36.3(r2), because M6.2 was obtained from some M5.i of table \u03c45 with b 6\u2208M5.i and b occurs in B+r2 with weight 1, resulting in \u03c36.3(r2) = 1; whereas M6.3 extends some M5.j with b /\u2208M5.j.\nIn order to decide in node t whether a witness satisfies rule r \u2208 \u03a0t, we check satisfiability of program R\u0307(r) constructed by R\u0307, which maps rules to state-programs. Formally, for M \u2286 \u03c7(t) \\\u03a0t, SatRules(R\u0307,M) := \u03c3 where \u03c3(r) :=\u221e if (r,R) \u2208 R\u0307 and M R.\nDefinition 1. Let \u03a0 be a program, T = (\u00b7, \u03c7) be a TD of I(\u03a0), t be a node of T , P \u2286 \u03a0t, and \u03c3 : \u03a0t \u2192 N0 \u222a {\u221e} be a rule-state. The state-program P(t,\u03c3) is obtained from P \u222a {\u2190 Br | r \u2208 CH(P), Hr ( at\u2264t}4 by\n1. removing rules r with \u03c3(r) =\u221e (\u201calready satisfied rules\u201d);\n2. removing from every rule all literals a,\u00aca with a 6\u2208 \u03c7(t); and 4We require to add {\u2190 Br | r \u2208 CH(P), Hr ( at\u2264t} in order to decide satisfiability for corner cases of choice rules involving\ncounterwitnesses of Line 3 in Algorithm 3.\n3. setting new bound max{0,bnd(r)\u2212 \u03c3(r)\u2212wght(r, at(r) \\ at\u2264t)} for weight rule r.\nWe define P\u0307(t,\u03c3) : P \u2192 2P(t,\u03c3) by P\u0307(t,\u03c3)(r) := {r}(t,\u03c3) for r \u2208 P.\nExample 8. Observe \u03a0 (t1,\u2205) t1 = {{b} \u2190 c,\u2190 c, c\u2190 0 \u2264 {b = 1}} and \u03a0 (t2,\u2205) t2 = {{a} \u2190,\u2190 1 \u2264 {\u00aca = 1}} for \u03a0t1 , \u03a0t2 of Figure 1(right).\nThe following example provides an idea how we compute models of a given program using the incidence graph. The resulting algorithm IMOD is the same as INC, except that only the first two tuple positions (red and green parts) are considered.\nExample 9. Again, we consider \u03a0 of Example 1 and in Figure 3 (left) T \u2032 as well as tables \u03c41, . . ., \u03c418. Table \u03c41 = {\u3008\u2205, \u2205\u3009} as type(t1) = leaf. Since type(t2) = int and t2 introduces atom c, we construct \u03c42 from \u03c41 by taking M2.1 := M1.1\u222a{c} and M2.2 := M1.1 as well as rule-state \u2205. Because type(t3) = int and t3 introduces rule r1, we consider state program L3 := {r1}(t3,{(r1,0)}) = {\u2190 c} for SatRules(L\u03073,M2.1) = {(r1, 0)} as well as SatRules(L\u03073,M2.2) = {(r1,\u221e)} (according to Line 9 of Algorithm 3). Because type(t4) = int and t4 introduces rule r2, we consider M3.1 := M2.1 and M3.2 := M2.2 and state program L4 := {r2}(t4,{(r2,0)}) = {c \u2190 0 6 {}} = {c\u2190} for SatRules(L\u03074,M3.1) = {(r2,\u221e)} as well as SatRules(L\u03074,M3.2) = {(r2, 0)} (see Line 9). Node t5 introduces b (table not shown) and node t6 removes b. Table \u03c46 was discussed in Example 7. When we remove b in t6 we have decided the \u201cinfluence\u201d of b on the satisfiability of r1 and r2 and thus all rules where b occurs. Tables \u03c47 and \u03c48 can be derived similarly. Then, t9 removes rule r2 and we ensure that every witness M9.1 can be extended to a model of r2, i.e., witness candidates for \u03c49 are M8.i with \u03c38.i(r2) = \u221e. The remaining tables are derived similarly. For example, table \u03c417 for join node t17 is derived analogously to table \u03c417 for algorithm PRIM in Figure 2, but, in addition, also combines the rule-states as specified in Algorithm 3.\nSince we already explained how to obtain models, we only briefly describe how we handle the counterwitness part. Family C consists of tuples (C, \u03c1) where C \u2286 at(\u03a0) \u2229 \u03c7(t) is a counterwitness in t to M . Similar to the rule-state \u03c3 the rule-state \u03c1 for C under M represents whether rules of the GL reduct \u03a0Mt are either (i) satisfied by a superset of C or (ii) undecided for C. Thus, C witnesses the existence of C \u2032 (M \u2032 satisfying C \u2032 (\u03a0<t \u222a SR(\u03a0t, \u03c1))M \u2032 since M witnesses a model M \u2032 \u2287M where M \u2032 \u03a0<t \u222a SR(\u03a0t, \u03c1). In consequence, there exists an answer set of \u03a0 if the root table contains \u3008\u2205, \u2205, \u2205\u3009. In order to locally decide rule satisfiability for counterwitnesses, we require state-programs under witnesses.\nAlgorithm 4: Algorithm #OINC(t, \u03c7t,\u03a0t, at\u2264t,Child-Tabs).\nIn: Bag \u03c7t, bag-rules \u03a0t, atoms-below at\u2264t, child tables Child-Tabs of t. Out: Tab. \u03c4t. /* For \u3008M,\u03c3, C, c, n\u3009, we only state affected parts (cost c and count n); \u2018\u2018. . . \u2019\u2019 indicates\ncomputation as before. * . . . + denotes a multiset. */ 1 if type(t) = leaf then \u03c4t := {\u3008\u2205, . . . , 0, 1\u3009} 2 else if type(t) = int, a \u2208 \u03c7t \\\u03a0t is introduced and \u03c4 \u2032 \u2208 Child-Tabs then 3 \u03c4t := {\u3008M, . . . , cst(\u03a0, \u2205, {a}) + c, n\u3009 | \u3008M,\u03c3, C, c, n\u3009 \u2208 \u03c4 \u2032}\n\u22c3 4 {\u3008M+a , . . . , cst(\u03a0, {a}, {a}) + c, n\u3009 | \u3008M,\u03c3, C, c, n\u3009 \u2208 \u03c4 \u2032} 5 else if type(t) = int or rem, removed or introduced r \u2208 \u03a0t, \u03c4 \u2032 \u2208 Child-Tabs then 6 \u03c4t := {\u3008M, . . . , c, n\u3009 | \u3008M,\u03c3, C, c, n\u3009 \u2208 \u03c4 \u2032, . . .} 7 else if type(t) = rem, a /\u2208 \u03c7t is removed atom and \u03c4 \u2032 \u2208 Child-Tabs then 8 \u03c4t := cnt(kmin(*\u3008M\u2212a , . . . , c, n\u3009 | \u3008M,\u03c3, C, c, n\u3009 \u2208 \u03c4 \u2032+)) 9 else if type(t) = join and \u03c4 \u2032, \u03c4 \u2032\u2032 \u2208 Child-Tabs with \u03c4 \u2032 6= \u03c4 \u2032\u2032 then 10 \u03c4t := cnt(kmin(*\u3008M, . . . , c\u2032 + c\u2032\u2032 \u2212 cst(\u03a0,M, \u03c7t), n\u2032 \u00b7 n\u2032\u2032\u3009 11 | \u3008M,\u03c3\u2032, C\u2032, c\u2032, n\u2032\u3009 \u2208 \u03c4 \u2032, \u3008M,\u03c3\u2032\u2032, C\u2032\u2032, c\u2032\u2032, n\u2032\u2032\u3009 \u2208 \u03c4 \u2032\u2032+))\nDefinition 2. Let \u03a0 be a program, T = (\u00b7, \u03c7) be a TD of I(\u03a0), t be a node of T , P \u2286 \u03a0t, \u03c1 : \u03a0t \u2192 N0\u222a{\u221e} be a rule-state and M \u2286 at(\u03a0). We define state-program P(t,\u03c1,M) by [S(t,\u03c1)]M where S := P \u222a {\u2190 Br | r \u2208 CH(P), \u03c1(r) > 0}, and P\u0307(t,\u03c1,M) : P \u2192 2P(t,\u03c1,M) by P\u0307(t,\u03c1,M)(r) := {r}(t,\u03c1,M) for r \u2208 P.\nWe compute a new rule-state \u03c1 for a counterwitness from an earlier rule-state, satisfied rules (SatRules), and both (a) \u201cupdated\u201d bounds for weight rules or (b) \u201cupdated\u201d value representing whether the head can still be satisfied (\u03c1(r) \u2264 0) for choice rules r (UpdtWgt&Ch). Formally, UpdtWgt&Ch(\u03a0t,M,C, a) := \u03c3\u2032 depending on an atom a with (a) \u03c3\u2032(r) := wght(r, {a} \u2229 [(B\u2212r \\M) \u222a (B+r \u2229 C)]), if r \u2208 WGT(\u03a0t); and (b) |{a} \u2229Hr \u2229 (M \\ C)|, if r \u2208 CH(\u03a0t).\nTheorem 3. The algorithm DP INC is correct.\nProof. (Idea) A tuple at a node t guarantees that there exists a model for the ASP sub-program induced by the subtree rooted at t. Since this can be done for each node type, we obtain soundness. Completeness follows from the fact that while traversing the tree decomposition every answer set is indeed considered. The full proof is rather tedious as each node type needs to be investigated separately. For more details, we refer the reader to Appendix B.2.\nTheorem 4. Given a program \u03a0, algorithm DP INC runs in time O(22 k+2\u00b7`k+1 \u00b7\u2016I(\u03a0)\u2016), where k := tw(I(\u03a0)), and ` := max{3,bnd(r) | r \u2208WGT(\u03a0)}.\nProof. We refer the reader to Appendix B.3.\nThe runtime bounds stated in Theorem 4 appear to be worse than in Theorem 2. However, tw(I(\u03a0)) \u2264 tw(P (\u03a0)) + 1 and tw(P (\u03a0)) \u2265 max{|at(r)| | r \u2208 \u03a0} for a given program \u03a0. Further, there are programs where tw(I(\u03a0)) = 1, but tw(P (\u03a0)) = k, e.g., a program consisting of a single rule r with |at(r)| = k. Consequently, worst-case runtime bounds of DPPRIM are at least double-exponential in the rule size and DPPRIM will perform worse than DP INC on input programs containing large rules. However, due to the rule-states, data structures of DP INC are much more complex than of DPPRIM. In consequence, we expect DPPRIM to perform better in practice if rules are small and incidence and primal treewidth are therefore almost equal. In summary, we have a trade-off between (i) a more general parameter decreasing the theoretical worst-case runtime and (ii) less complex data structures decreasing the practical overhead to solve AS."}, {"heading": "3.3 Extensions for Optimization and Counting", "text": "In order to find an answer set of a program with optimization statements or the number of optimal answer sets (#AspO), we extend our algorithms PRIM and INC. Therefore, we augment tuples stored in tables with an integers c and n describing the cost and the number of witnessed sets. Due to space restrictions, we only present adaptions for INC. We state which parts of INC we adapt to compute the number of optimal answer sets in Algorithm 4 (#OINC). To slightly simplify the presentation of optimization rules, we assume without loss of generality that whenever an atom a is introduced in bag \u03c7(t) for some node t of the TD, the optimization rule r, where a occurs, belongs to the bag \u03c7(t). First, we explain how to handle costs making use of function cst(\u03a0,M,A) as defined in Section 2. In a leaf (Line 1) we set the (current) cost to 0. If we introduce an atom a (Line 2\u20134) the cost depends on whether a is set to true or false in M and we add the cost of the \u201cchild\u201d tuple. Removal of rules (Line 5\u20136) is trivial, as we only store the same values. If we remove an atom (Line 7\u20138), we compute the minimum costs only for tuples \u3008M\u2212a , \u03c3, C, c, n\u3009 where c is minimal among M\u2212a , \u03c3, C, that is, for a multiset S we let kmin(S) := *\u3008M\u2212a , \u03c3, C, c, n\u3009 | c = min{c\u2032 : \u3008M\u2212a , \u03c3, C, c\u2032, \u00b7\u3009 \u2208 S}, \u3008M\u2212a , \u03c3, C, c, n\u3009 \u2208 S+. We require a multiset notation for counting (see below). If we join two nodes (Line 9\u201311), we compute the minimum value in the table of one child plus the minimum value of the table of the other child minus the value of the cost for the current bag, which is exactly the value we added twice. Next, we explain how to handle the number of witnessed sets that are minimal with respect to the cost. In a leaf (Line 1), we set the counter to 1. If we introduce/remove a rule or introduce an atom (Line 2\u20136), we can simply take the number n from the child. If we remove an atom (Line 7\u20138) we first obtain a multiset from computing kmin, which can contain several tuples for M\u2212a , \u03c3, C, c as we obtained M\u2212a either from M \\ {a} if a \u2208M or M if a /\u2208M giving rise multiple solutions, that is, cnt(S) := {\u3008M,\u03c3, C, c, \u2211 \u3008M,\u03c3,C,c,n\u2032\u3009\u2208S n\n\u2032\u3009 | \u3008M,\u03c3, C, c, n\u3009 \u2208 S}. If we join nodes (Line 7\u20139), we multiply the number n\u2032 from the tuple of one child with the number n\u2032\u2032 from the tuple of the other child, restrict results with respect to minimum costs, and sum up the resulting numbers.\nCorollary 1. Given a program \u03a0, algorithm #OINC runs in time O(log(m) \u00b7 22k+2\u00b7`k+1\u2016I(\u03a0)\u20162), where k := tw(I(\u03a0)), ` := max{3,bnd(r) : r \u2208WGT(\u03a0)}, and m := \u03a3r\u2208OPT(\u03a0) wght(r)."}, {"heading": "4 Experimental Evaluation", "text": "We implemented the algorithms DPPRIM and DP INC into a prototypical solver DynASP2(\u00b7) and performed experiments to evaluate its runtime behavior. Clearly, we cannot hope to solve programs with graph representations of high treewidth. However, programs involving real-world graphs such as graph problems on transit graphs admit TDs of small width. We used both random and structured instances for our benchmarks. We refer to Appendix C for instance, machine and solver configurations and descriptions. The random instances (Sat-TGrid, 2QBF-TGrid, ASP-TGrid, 2ASP-TGrid) were designed to have a high number of variables and solutions and treewidth at most three. The structured instances model various graph problems (2Col, 3Col, Ds, St cVc, sVc) on real world mass transit graphs. For a graph, program 2Col counts all 2-colorings, 3Col counts all 3-colorings, Ds counts all minimal dominating sets, St counts\nall Steiner trees, cVc counts all cardinality-minimal vertex covers, and sVc counts all subset-minimal vertex covers. In order to draw conclusions about the efficiency of DynASP2, we mainly inspected the cpu running time and number of timeouts using the average over three runs per instance (three fixed seeds allow certain variance [1] for heuristic TD computation). We limited available memory (RAM) to 4GB (to run SharpSAT on large instances), and cpu time to 300 seconds, and then compared DynASP2 with the dedicated #SAT solvers SharpSAT [20] and Cachet [18], the QBF solver DepQBF0, and the ASP solver Clasp [9]. Figure 4 illustrates runtime results as a cactus plot. Table 1 reports on the average running times, numbers of solved instances and timeouts on the structured instance sets.\nSummary. Our empirical benchmark results confirm that DynASP2 exhibits competitive runtime behavior if the input instance has small treewidth. Compared to state-of-the-art Asp and Qbf solvers, DynASP2 has an advantage in case of many solutions, whereas Clasp and DepQBF0 perform well if the number of solutions is relatively small. However, DynASP2 is still reasonably fast on structured instances with few solutions as it yields the result mostly within less than 10 seconds. We observed that INC seems to be the better algorithm in our setting, indicating that the smaller width obtained by decomposing the incidence graph generally outweighs the benefits of simpler solving algorithms for the primal graph. However, if INC and PRIM run with graphs of similar width, PRIM benefits from its simplicity. A comparison to existing #SAT solvers suggests that, on random instances, they have a lower overhead (which is not surprising, since our algorithms are built for ASP), but, after about 150 seconds, our algorithms were still able to solve more instances than all other #SAT competitors."}, {"heading": "5 Conclusion", "text": "In this paper, we presented novel DP algorithms for ASP, extending previous work [11] in order to cover the full ASP syntax. Our algorithms are based on two graph representations of programs and run in linear time with respect to the treewidth of these graphs and weights used in the program. Experiments indicate that our approach seems to be suitable for practical use, at least for certain classes of instances with low treewidth, and hence could fit into a portfolio-based solver."}, {"heading": "A Additional Examples", "text": "In the following example, we briefly describe how we compute counterwitnesses using Algorithm 3 (INC) for selected interesting cases. The example is similar to Example 6, which, however, describes handling counterwitnesses for Algorithm PRIM.\nExample 10. We consider \u03a0 of Example 1 and T \u2032 = (\u00b7, \u03c7) of Figure 3 and explain how we compute tables \u03c41, . . ., \u03c418 in Figure 3 (right) using DP INC. Table \u03c41 = \u3008\u2205, \u2205, \u2205\u3009 as type(t1) = leaf. Node t2 introduces atom c, resulting in table {\u3008{c}, \u2205, {(\u2205, \u2205)}\u3009, \u3008\u2205, \u2205, \u2205\u3009}. Then, node t3 introduces rule r1 and node t4 introduces rule r2. As a result, table \u03c44 additionally contains computed rule-states (see SatRules) for witnesses and counterwitnesses of \u03c43. Node t5 introduces atom b, while t6 removes b. Next, we focus on table \u03c46, since rule-states for counterwitnesses require updates for choice rule r1 (see UpdtWgt&Ch). Witness M6.2 = {c} is obtained by extending some witness M5.i \u2287 {b} of \u03c45. For counterwitness C6.2.1 = {c} we require to remember \u03c36.2.1(r1) = 1 (see UpdtWgt&Ch), since t6 removes b and C6.2.1 stems from some C5.i.j1 with b 6\u2208 C5.i.j1 . The set C5.i.j1 cannot be a model of the GL reduct {r1} M5.i unless r1 is satisfied because of its body, since b \u2208 M5.i and b 6\u2208 C5.i.j1 . For choice rule r1, \u03c36.2.1(r1) 6= \u221e and \u03c36.2.1(r1) 6= 0 indicates that we can satisfy r1 only by B\n+(r1) \\M 6= \u2205 (see P \u222a {\u00b7 \u00b7 \u00b7 , \u03c1(r) > 0} in Definition 2). The remaining counterwitness C6.2.2 = \u2205 was obtained by some C5.i.j2 with b 6\u2208 C5.i.j2 , since \u03c36.2.2(r2) = 0). Further, C6.2.3 = \u2205 stems from C5.i.j3 \u2287 {b}, since \u03c36.2.3(r2) = 1."}, {"heading": "B Omitted Proofs", "text": "B.1 Proof of Theorem 2 (Correctness result of PRIM)\nProposition 1. The algorithm DPPRIM is correct. Proof (Sketch). Let \u03a0 be the given program and T = (T, \u03c7) the TD, where T = (N, \u00b7, n). We obtain correctness by slightly modifying the proof of Theorem 2 as well as relevant definitions and propositions following Appendix B.2. More precisely, we drop the mappings \u03c3 and relevant conditions for mappings \u03c3 and replace them by satisfiability of the respective rules. By definition of a primal graph of a program, we know that for every rule r \u2208 \u03a0 there is a node t \u2208 N such that \u03c7(t) \u2286 at(r). Hence, for a node t we can decide satisfiability of a rule directly, if bag \u03c7(t) contains all atoms of a rule, when computing the tables. We directly obtain completeness and soundness, which yields the proposition.\nProposition 2. Given a program \u03a0 and a TD T = (T, \u03c7) of the primal graph P (\u03a0) of width k with T = (N, \u00b7, \u00b7). For every node t \u2208 N , there are at most 2k+1 \u00b7 22k+1 tuples in table \u03c4t, which is constructed by algorithm DP INC. Proof. Let \u03a0 be a program, P (\u03a0) its primal graph, and T = (T, \u03c7) a TD of P (\u03a0) with T = (N, \u00b7, \u00b7). For every node t \u2208 T , we have by definition of a tree decomposition and its width a maximum bag size of k + 1, i.e., |\u03c7(t)| \u2212 1 \u2264 k. Therefore, we can have 2k+1 many witnesses and for each witness a subset of the set of witnesses consisting of at most 22 k+1 many counterwitnesses. Consequently, there are at most 2k+1 \u00b7 22k+1 tuples per node. Hence, the proposition is true.\nNow, we are in situation to prove Theorem 2.\nProof of Theorem 2. Let \u03a0 be a program, I(\u03a0) = (V, \u00b7) its incidence graph, and k be the treewidth of P (\u03a0). Proposition 1 establishes correctness. Then, we can compute in time 2O(k\n3) \u00b7 |V | a TD of width at most k [1]. We take such a TD and compute in linear time a nice TD [7]. Let T = (T, \u03c7) be such a nice TD with T = (N, \u00b7, n). Since the number of nodes in N is linear in the graph size and since for every node t \u2208 N the table \u03c4t is bounded by 2k+1 \u00b7 22 k+1 according to Proposition 2, we obtain a running time of O(22k+2 \u00b7 \u2016P (\u03a0)\u2016). Consequently, the theorem sustains.\nB.2 Proof of Theorem 3 (Correctness result of INC)\nIn the following, we provide insights on the correctness of Algorithm 3 (INC). The correctness proof of these algorithms need to investigate each node type separately. We have to show that a tuple at a node t guarantees existence of a model for the program \u03a0\u2264t, proving soundness. Conversely, one can show that each candidate answer set is indeed evaluated while traversing the TD, which provides completeness. We employ this idea using the notions of (i) partial solutions consisting of partial models and the notion of (ii) local partial solutions.\nDefinition 3. Let \u03a0 be a program, T = (T, \u03c7) be a TD of the incidence graph I(\u03a0) of \u03a0, where T = (N, \u00b7, \u00b7), and t \u2208 N be a node. Further, let M,C \u2286 at\u2264t be sets and \u03c3 : \u03a0\u2264t \u2192 N0 \u222a {\u221e} a mapping. The tuple (C, \u03c3) is a partial model for t under M if the following conditions hold:\n1. C (\u03a0<t)M ,\n2. for r \u2208 \u03a0\u2264t we have \u03c3(r) = 0 or \u03c3(r) =\u221e,\n3. (a) for r \u2208 DISJ(\u03a0\u2264t) we have B\u2212r \u2229M 6= \u2205 or B+r \u2229 at\u2264t 6\u2286 C or Hr \u2229 C 6= \u2205 if and only if \u03c3(r) =\u221e, (b) for r \u2208WGT(\u03a0\u2264t) we have wght(r, (at(r) \\ at\u2264t) \u222a (B\u2212r \\M) \u222a (B+r \u2229C)) < bnd(r) or Hr \u2229C 6= \u2205\nif and only if \u03c3(r) =\u221e, and (c) for r \u2208 CH(\u03a0\u2264t) we have B\u2212r \u2229M 6= \u2205 or B+r \u2229 at\u2264t 6\u2286 C or both Hr \u2286 at\u2264t and Hr \u2229 (M \\ C) = \u2205\nif and only if \u03c3(r) =\u221e.\nDefinition 4. Let \u03a0 be a program, T = (T, \u03c7) where T = (N, \u00b7, n) be a TD of I(\u03a0), and t \u2208 N be a node. A partial solution for t is a tuple (M,\u03c3, C) where (M,\u03c3) is a partial model under M and C is a set of partial models (C, \u03c1) under M with C (M .\nThe following lemma establishes correspondence between answer sets and partial solutions.\nLemma 1. Let \u03a0 be a program, T = (T, \u03c7) be a TD of the incidence graph I(\u03a0) of program \u03a0, where T = (\u00b7, \u00b7, n), and \u03c7(n) = \u2205. Then, there exists an answer set M for \u03a0 if and only if there exists a partial solution u = (M,\u03c3, \u2205) with \u03c3\u22121(\u221e) = \u03a0 for root n.\nProof. Given an answer set M of \u03a0 we construct u = (M,\u03c3, \u2205) with \u03c3(r) :=\u221e for r \u2208 \u03a0 such that u is a partial solution for n (according to Definition 4). For the other direction, Definitions 3 and 4 guarantee that M is an answer set if there exists some tuple u. In consequence, the lemma holds.\nNext, we require the notion of local partial solutions corresponding to the tuples obtained in Algorithm 3.\nDefinition 5. Let \u03a0 be a program, T = (T, \u03c7) a TD of I(\u03a0), where T = (N, \u00b7, n), t \u2208 N be a node, M,C \u2286 at(\u03a0) sets, and \u03c3 : \u03a0\u2192 N0\u222a{\u221e} be a mapping. We define the local rule-state \u03c3t,M,C := (\u03c3]\u03c3\u2032)\u2212\u03a0<t for C under M of node t where \u03c3\u2032 : \u03a0t \u2192 N0 \u222a {\u221e} by\n\u03c3\u2032(r) := { wght(r, (at\u2264t \\ \u03c7(t)) \u2229 [(B\u2212r \\M) \u222a (B+r \u2229 C)]) r \u2208WGT(\u03a0t) |(at\u2264t \\ \u03c7(t)) \u2229Hr \u2229 (M \\ C)| r \u2208 CH(\u03a0t)\nDefinition 6. Let \u03a0 be a program, T = (T, \u03c7) a TD of the incidence graph I(\u03a0), where T = (N, \u00b7, n), and t \u2208 N be a node. A tuple u = \u3008M,\u03c3, C\u3009 is a local partial solution for t if there exists a partial solution u\u0302 = (M\u0302, \u03c3\u0302, C\u0302) for t such that the following conditions hold:\n1. M = M\u0302 \u2229 \u03c7(t),\n2. \u03c3 = \u03c3\u0302t,M\u0302,M\u0302 , and\n3. C = {\u3008C\u0302 \u2229 \u03c7(t), \u03c1\u0302t,M\u0302,C\u0302\u3009 | (C\u0302, \u03c1\u0302) \u2208 C\u0302}.\nWe denote by u\u0302t the local partial solution u for t given partial solution u\u0302.\nThe following proposition provides justification that it suffices to store local partial solutions instead of partial solutions for a node t \u2208 N .\nLemma 2. Let \u03a0 be a program, T = (T, \u03c7) a TD of I(\u03a0), where T = (N, \u00b7, n), and \u03c7(n) = \u2205. Then, there exists an answer set for \u03a0 if and only if there exists a local partial solution of the form \u3008\u2205, \u2205, \u2205\u3009 for the root n \u2208 N .\nProof. Since \u03c7(n) = \u2205, every partial solution for the root n is an extension of the local partial solution u for the root n \u2208 N according to Definition 6. By Lemma 1, we obtain that the lemma is true.\nIn the following, we abbreviate atoms occurring in bag \u03c7(t) by att, i.e., att := \u03c7(t) \\\u03a0t.\nProposition 3 (Soundness). Let \u03a0 be a program, T = (T, \u03c7) a TD of incidence graph I(\u03a0), where T = (N, \u00b7, \u00b7), and t \u2208 N a node. Given a local partial solution u\u2032 of child table \u03c4 \u2032 (or local partial solution u\u2032 of table \u03c4 \u2032 and local partial solution u\u2032\u2032 of table \u03c4 \u2032\u2032), each tuple u of table \u03c4t constructed using table algorithm INC is also a local partial solution.\nProof. Let u\u2032 be a local partial solution for t\u2032 \u2208 N and u a tuple for node t \u2208 N such that u was derived from u\u2032 using table algorithm INC. Hence, node t\u2032 is the only child of t and t is either removal or introduce node.\nAssume that t is a removal node and r \u2208 \u03a0t\u2032 \\ \u03a0t for some rule r. Observe that u = \u3008M,\u03c3, C\u3009 and u\u2032 = \u3008M,\u03c3\u2032, C\u2032\u3009 are the same in witness M . According to Algorithm 3 and since u is derived from u\u2032, we have \u03c3\u2032(r) = \u221e. Similarly, for any \u3008C \u2032, \u03c1\u2032\u3009 \u2208 C\u2032, \u03c1\u2032(r) = \u221e. Since u\u2032 is a local partial solution, there exists a partial solution u\u0302\u2032 of t\u2032, satisfying the conditions of Definition 6. Then, u\u0302\u2032 is also a partial solution for node t, since it satisfies all conditions of Definitions 3 and 4. Finally, note that u = (u\u0302\u2032)t since the projection of u\u0302\u2032 to the bag \u03c7(t) is u itself. In consequence, the tuple u is a local partial solution.\nFor a \u2208 att\u2032 \\ att as well as for introduce nodes, we can analogously check the proposition. Next, assume that t is a join node. Therefore, let u\u2032 and u\u2032\u2032 be local partial solutions for t\u2032, t\u2032\u2032 \u2208 N , respectively, and u be a tuple for node t \u2208 N such that u can be derived using both u\u2032 and u\u2032\u2032 in accordance with the INC algorithm. Since u\u2032 and u\u2032\u2032 are local partial solutions, there exists partial solution u\u0302\u2032 = (M\u0302 \u2032, \u03c3\u0302\u2032, C\u0302\u2032) for node t\u2032 and partial solution u\u0302\u2032\u2032 = (M\u0302 \u2032\u2032, \u03c3\u0302\u2032\u2032, C\u0302\u2032\u2032) for node t\u2032\u2032. Using these two partial solutions, we can construct u\u0302 = (M\u0302 \u2032 \u222a M\u0302 \u2032\u2032, \u03c3\u0302\u2032 ] \u03c3\u0302\u2032\u2032, C\u0302\u2032 ./ C\u0302\u2032\u2032) where ./ (\u00b7, \u00b7) is defined in accordance with Algorithm 3 as follows:\nC\u0302\u2032 ./ C\u0302\u2032\u2032 :={(C\u0302 \u2032 \u222a C\u0302 \u2032\u2032, \u03c1\u0302\u2032 ] \u03c1\u0302\u2032\u2032) | (C\u0302 \u2032, \u03c1\u0302\u2032) \u2208 C\u0302\u2032, (C\u0302 \u2032\u2032, \u03c1\u0302\u2032\u2032) \u2208 C\u0302\u2032\u2032, C\u0302 \u2032 \u2229 att = C\u0302 \u2032\u2032 \u2229 att}\u222a {(C\u0302 \u2032 \u222a M\u0302 \u2032\u2032, \u03c1\u0302\u2032 ] \u03c3\u0302\u2032\u2032) | (C\u0302 \u2032, \u03c1\u0302\u2032) \u2208 C\u0302\u2032, C\u0302 \u2032 \u2229 att = M\u0302 \u2032\u2032 \u2229 att}\u222a {(M\u0302 \u2032 \u222a C\u0302 \u2032\u2032, \u03c3\u0302\u2032 ] \u03c1\u0302\u2032\u2032) | (C\u0302 \u2032\u2032, \u03c1\u0302\u2032\u2032) \u2208 C\u0302\u2032\u2032, M\u0302 \u2032 \u2229 att = C\u0302 \u2032\u2032 \u2229 att}.\nThen, we check all conditions of Definitions 3 and 4 in order to verify that u\u0302 is a partial solution for t. Moreover, the projection u\u0302t of u\u0302 to the bag \u03c7(t) is exactly u by construction and hence, u = u\u0302t is a local partial solution.\nSince we have provided arguments for each node type, we established soundness in terms of the statement of the proposition.\nProposition 4 (Completeness). Let \u03a0 be a program, T = (T, \u03c7) where T = (N, \u00b7, \u00b7) be a TD of I(\u03a0) and t \u2208 N be a node. Given a local partial solution u of table \u03c4t, either t is a leaf node, or there exists a local partial solution u\u2032 of child table \u03c4 \u2032 (or local partial solution u\u2032 of table \u03c4 \u2032 and local partial solution u\u2032\u2032 of table \u03c4 \u2032\u2032) such that u can be constructed by u\u2032 (or u\u2032 and u\u2032\u2032, respectively) and using table algorithm INC.\nProof. Let t \u2208 N be a removal node and r \u2208 \u03a0t\u2032 \\\u03a0t with child node t\u2032 \u2208 N . We show that there exists a tuple u\u2032 in table \u03c4t\u2032 for node t\n\u2032 such that u can be constructed using u\u2032 by INC (Algorithm 3). Since u is a local partial solution, there exists a partial solution u\u0302 = (M\u0302, \u03c3\u0302, C\u0302) for node t, satisfying the conditions of Definition 6. Since r is the removed rule, we have \u03c3\u0302(r) = \u221e. By similar arguments, we have \u03c1\u0302(r) = \u221e for any tuple (C\u0302, \u03c1\u0302) \u2208 C\u0302. Hence, u\u0302 is also a partial solution for t\u2032 and we define u\u2032 := u\u0302t\u2032 , which is the projection of u\u0302 onto the bag of t\u2032. Apparently, the tuple u\u2032 is a local partial solution for node t\u2032 according to Definition 6. Then, u can be derived using INC algorithm and u\u2032. By similar arguments, we establish the proposition for a \u2208 att\u2032 \\ att and the remaining (three) node types. Hence, the propositions sustains.\nNow, we are in situation to prove Theorem 3.\nProof of Theorem 3. We first show soundness. Let T = (T, \u03c7) be the given TD, where T = (N, \u00b7, n). By Lemma 2 we know that there is an answer set for \u03a0 if and only if there exists a local partial solution for the root n. Note that the tuple is of the form \u3008\u2205, \u2205, \u2205\u3009 by construction. Hence, we proceed by induction starting from the leaf nodes. In fact, the tuple \u3008\u2205, \u2205, \u2205\u3009 is trivially a partial solution by Definitions 3 and 4 and also a local partial solution of \u3008\u2205, \u2205, \u2205\u3009 by Definition 6. We already established the induction step in Proposition 3. Hence, when we reach the root n, when traversing the TD in post-order by Algorithm DP INC, we obtain only valid tuples inbetween and a tuple of the form \u3008\u2205, \u2205, \u2205\u3009 in the table of the root n witnesses an answer set. Next, we establish completeness by induction starting from the root n. Let therefore, M be an arbitrary answer set of \u03a0. By Lemma 2, we know that for the root n there exists a local partial solution of the form \u3008\u2205, \u2205, \u2205\u3009 for partial solution \u3008M,\u03c3, \u2205\u3009 with \u03c3(r) = \u221e for r \u2208 \u03a0. We already established the induction step in Proposition 4. Hence, we obtain some (corresponding) tuples for every node t. Finally, stopping at the leaves n. In consequence, we have shown both soundness and completeness resulting in the fact that Theorem 3 is true.\nTheorem 3 states that we can decide the problem Cons by means of Algorithm DP INC, which uses Algorithm 3.\nB.3 Proof of Theorem 4 (Worst-case Runtime Bounds of INC)\nFirst, we give a proposition on worst-case space requirements in tables for the nodes of our algorithm.\nProposition 5. Given a program \u03a0, a TD T = (T, \u03c7) with T = (N, \u00b7, \u00b7) of the incidence graph I(\u03a0), and a node t \u2208 N . Then, there are at most 2k+1 \u00b7 `k+1 \u00b7 22k+1\u00b7`k+1 tuples in \u03c4t using algorithm DP INC for width k of T and bound ` = max{3,bnd(r) : r \u2208WGT(\u03a0)}.\nProof (Sketch). Let \u03a0 be the given program, T = (T, \u03c7) a TD of the incidence graph I(\u03a0), where T = (N, \u00b7, \u00b7), and t \u2208 N a node of the TD. Then, by definition of a decomposition of the primal graph for each node t \u2208 N , we have |\u03c7(t)|\u2212 1 \u2264 k. In consequence, we can have at most 2k+1 many witnesses, and for each witness a subset of the set of witnesses consisting of at most 22 k+1\nmany counterwitnesses. Moreover, we observe that Algorithm 3 can be easily modified such that a state \u03c3 : \u03a0t \u2192 N0 \u222a {\u221e} for node t \u2208 N assigns each weight rule r \u2208WGT(\u03a0) a non-negative integer \u03c3(r) \u2264 bnd(r) + 1, each choice rule r \u2208 CH(\u03a0) a non-negative integer \u03c3(r) \u2264 2 and each disjunctive rule r \u2208 DISJ(\u03a0) a non-negative integer \u03c3(r) \u2264 1. This is the case since we need to model \u03c3(r) = 0 and \u03c3(r) =\u221e for each disjunctive rule r. Moreover, for choice rules r, it suffices to additionally model whether 1 \u2264 \u03c3(r) <\u221e, and for weight rules r, we require to remember any weight 1 \u2264 \u03c3(r) \u2264 bnd(r). In total, we need to distinguish `k+1 different rule-states for each witness of a tuple in the table \u03c4t for node t. Since for each witness in the table \u03c4t for node t \u2208 N we remember rule-states for at most k + 1 rules, we store up to `k+1 many combinations per witness. In total we end up with at most 22 k+1\u00b7`k+1 many counterwitnesses for each witness and rule-state in the worst case. Thus, there are at most 2k+1 \u00b7 `k+1 \u00b7 22k+1\u00b7`k+1 tuples in table \u03c4t for node t. In consequence, we established the proposition.\nProof of Theorem 4. Let \u03a0 be a program, I(\u03a0) = (V, \u00b7) its incidence graph, and k be the treewidth of P (\u03a0). Then, we can compute in time 2O(k\n3) \u00b7 |V | a TD of width at most k [1]. We take such a TD and compute in linear time a nice TD [7]. Let T = (T, \u03c7) be such a nice TD with T = (N, \u00b7, \u00b7). Since the number of nodes in N is linear in the graph size and since for every node t \u2208 N the table \u03c4t is bounded by 2k+1 \u00b7 `k+1 \u00b7 22k+1\u00b7`k+1 according to Proposition 5, we obtain a running time of O(22k+2\u00b7`k+1\u2016I(\u03a0)\u2016). Consequently, the theorem sustains.\nB.4 Correctness of the Algorithm DP#OINC The following propositions states that we can use Algorithm DP#OINC to actually count optimal answer sets.\nProposition 6. The algorithm DP#OINC is correct.\nProof (Sketch). We follow the proof of Theorem 3. First, we additionally need to take care of the optimization rules obtained by extending Definitions 3\u20136, the lemmas and propositions accordingly. In order to handle the counting, we have to extend Definitions 3\u20136 by counters. Further, we additionally need to ensure and prove in the induction steps, which are established by Propositions 3 and 4, that any fixed partial solution is obtained from child to parent via a corresponding local partial solution by the algorithm."}, {"heading": "C Experiments", "text": "C.1 Solvers\nThe solvers tested include our own prototypical implementation, which we refer to as DynASP, and the existing solvers\n\u2022 Cachet 1.21 [18], which is a SAT model counter,\n\u2022 DepQBF05, which is the solver DepQBF [9] where we added a naive implementation using methods described by Lonsing [8],\n\u2022 Clasp 3.1.4 [9], which is an ASP solver, and\n\u2022 SharpSAT 12.08 [20], which is a SAT model counter.\nC.2 Environment\nWe ran the experiments on an Ubuntu 12.04 Linux cluster of 3 nodes with two AMD Opteron 6176 SE CPUs of 12 physical cores each at 2.3Ghz clock speed and 128GB RAM. Input instances were given to the solvers via shared memory. All solvers have been compiled with gcc version 4.9.3. Available memory was limited to 4GB RAM, which was necessary to run SharpSAT on larger instances, and CPU time to 300 seconds. We used default options for cachet and SharpSAT, \u201c\u2013qdc\u201d for DepQBF0, \u201c\u2013stats=2 \u2013opt-mode=optN -n 0 \u2013opt-strategy=usc -q\u201d and no solution printing/recording for clasp. We also benchmarked clasp with the flag \u201cbb\u201d. However, \u201cusc\u201d outperformed \u201cbb\u201d on all our benchmarks. All solvers have been executed in single core mode.\nC.3 Instances\nWe used both random and structured instances for benchmark sets, which we briefly describe below. The benchmark sets, including instances and encodings, as well as results are available online on github6.\nThe random instances (Sat-TGrid, 2QBF-TGrid, ASP-TGrid, 2ASP-TGrid) were designed to have a high number of variables and solutions and treewidth at most three. The instances are constructed as follows: Let k and ` be some positive integers and p a rational number such that 0 < p \u2264 1. An instance F of Sat-TGrid(k, l, p) consists of the set V = {(1, 1), . . . , (1, `), (2, `), . . . , (k, `)} of variables and with probability p for each variable (i, j) such that 1 < i \u2264 k and 1 < j \u2264 ` a clause s1(i, j), s2(i\u2212 1, j), s3(i, j \u2212 1), a clause s4(i, j), s5(i\u2212 1, j), s6(i\u2212 1, j \u2212 1), and a clause s7(i, j), s8(i\u2212 1, j \u2212 1), s9(i, j \u2212 1) where si \u2208 {\u2212,+} is selected with probability one half. In that way, such an instance has an underlying dependency graph that consists of various triangles forming for probability p = 1 a graph that has a grid as subgraph. Let q be a rational number such that 0 < q \u2264 1. An instance of the set 2Qbf-TGrid(k, l, p, q) is of the form \u2203V1.\u2200V2.F where a variable belongs to V1 with probability q and to V2 otherwise. Instances of the sets ASP-TGrid or 2ASP-TGrid have been constructed in a similar way, however, as an Asp program instead of a formula. Note that the number of answer sets and the number of satisfiable assignments correspond. We fixed the parameters to p = 0.85, k = 3, and l \u2208 {40, 80, . . . , 400} to obtain instances that have with high probability a small fixed width, a high number of variables and solutions. Further, we took fixed random seeds and generated 10 instances to ensure a certain randomness.\nThe structured instances model various graph problems (2Col, 3Col, Ds, St cVc, sVc) on real world mass transit graphs of 82 cities, metropolitan areas, or countries. The graphs were extracted from publicly available mass transit data feeds [2] using gtfs2graphs [5] and split by transportation type, e.g., train, metro,\n5See https://github.com/hmarkus/depqbf/tree/depqbf0 6See https://github.com/daajoe/lpnmr17 experiments.\ntram. We excluded bus networks as size and treewidth were too large. For an input graph, the 2Col encoding counts all minimal sets S of vertices s.t. there are two sets F and S where no two neighboring vertices v and w belong to F ; 3Col counts all 3-colorings; Ds counts all minimal dominating sets; St counts all Steiner trees; cVc counts all minimal vertex covers; and sVc counts all subset-minimal vertex covers. Since we cannot expect to solve instances of high treewidth efficiently, we restricted the instances to those where we were able to find decompositions of width below 20 within 60 seconds.\nC.4 Extended Discussion on the Results\nIn order to draw conclusions about the efficiency of our approach, we mainly inspected the total cpu running time and number of timeouts on the random and structured benchmark sets. Note that we did not record I/O times. The runtime for DynASP2(\u00b7) includes decomposition times using heuristics from [3, 4]. We randomly generated three fixed seeds for the decomposition computation to allow a certain variance [1]. When evaluating the results, we took the average over the three runs per instance. Figure 4 illustrates solver runtime on the various random instance sets and a selected structured instance set as a cactus plot. Table 1 reports on the average running times, number of solved instances, and number of timeouts of the solvers on the structured instance sets.\nC.4.1 Results.\nSAT-TGrid and Asp-TGrid: Cachet solved 125 instances. Clasp always timed out. A reason could be the high number of solutions as Clasp counts the models by enumerating them (without printing them). DynASP2(\u00b7) solved each instance within at most 270 seconds (on average 67 seconds). The best configuration with respect to runtime was PRIM. However, the running times of the different configurations were close. We observed as expected a sub-polynomial growth in the runtime with an increasing number of solutions. SharpSAT timed out on 3 instances and ran into a memory out on 7 instances, but solved most of the instances quite fast. Half of the instances were solved within 1 second and more than 80% of the instances within 10 seconds, and about 9% of the instances took more than 100 seconds. The number of solutions does not have an impact on the runtime of SharpSAT. SharpSAT was the fastest solver in total. However, DynASP2(\u00b7) solved all instances. The results are illustrated in the two left graphs of Figure 4.\n2QBF-TGrid and 2ASP-TGrid: Clasp solved more than half of the instances in less than 1 second, however, timed out on 59 instances. DepQBF0 shows a similar behavior as Clasp, which is not surprising as both solvers count the number of solutions by enumerating them and hence the number of solutions has a significant impact on the runtime of the solver. However, Clasp is faster throughout than DepQBF0. DynASP2(INC) solved half of the instances within less than 1 second, about 92% of the instances within less than 10 seconds, and provided solutions also if the instance had a large number of answer sets. DynASP2(PRIM) quickly produced timeouts due to large rules in program that produced a significantly larger width of the computed decompositions.\nStructured instances: Clasp solved most of the structured instances reasonably fast. However, the number of solutions has again, similar to the random setting, a significant impact on its performance. If the number of solutions was very high, then Clasp timed out. If the instance has a small number of solutions, then Clasp yields the number almost instantly. However, DynASP2(\u00b7) also provided a solution within a second. DynASP2(\u00b7) solved for each set but the set St more than 80% of the instances in less than 1 second and the remaining instances in less than 100 seconds. For St the situation was different. Half of the instances were solved in less than 10 seconds and a little less than the other half timed out. Similar to the random setting, DynASP2(\u00b7) ran still fast on instances with a large number of solutions."}, {"heading": "Appendix References", "text": "[1] Hans L. Bodlaender. A linear-time algorithm for finding tree-decompositions of small treewidth. SIAM J. Comput., 25(6):1305\u20131317, 1996.\n[2] J. et al. Czebotar. GTFS data exchange. www.gtfs-data-exchange.com, 2016.\n[3] Holger Dell and Frances Rosamond. The 1st parameterized algorithms and computational experiments challenge \u2013 Track A: Treewidth. Technical report, 2016.\n[4] Artan Dermaku, Tobias Ganzow, Georg Gottlob, Ben McMahan, Nysret Musliu, and Marko Samer. Heuristic methods for hypertree decomposition. In MICAI\u201908, pages 1\u201311. Springer, 2008.\n[5] J. K. Fichte. daajoe/gtfs2graphs \u2013 a GTFS transit feed to graph format converter. https://github. com/daajoe/gtfs2graphs, 2016.\n[6] M. Gebser, B. Kaufmann, and T. Schaub. Conflict-driven answer set solving: From theory to practice. AIJ, 187\u2013188, 2012.\n[7] Ton Kloks. Treewidth. Computations and Approximations, volume 842 of LNCS. Springer, 1994.\n[8] F. Lonsing. Personal communication, 2016.\n[9] F. Lonsing and A. Biere. DepQBF: A dependency-aware QBF solver system description. J. Sat., Bool. Model. and Comp., 7, 2010."}], "references": [{"title": "A linear-time algorithm for finding tree-decompositions of small treewidth", "author": ["Hans L. Bodlaender"], "venue": "SIAM J. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Czebotar. GTFS data exchange", "author": [], "venue": "www.gtfs-data-exchange.com,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "The 1st parameterized algorithms and computational experiments challenge \u2013 Track A: Treewidth", "author": ["Holger Dell", "Frances Rosamond"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Heuristic methods for hypertree decomposition", "author": ["Artan Dermaku", "Tobias Ganzow", "Georg Gottlob", "Ben McMahan", "Nysret Musliu", "Marko Samer"], "venue": "In MICAI\u201908,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Fichte. daajoe/gtfs2graphs \u2013 a GTFS transit feed to graph format converter", "author": ["K. J"], "venue": "https://github. com/daajoe/gtfs2graphs,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Conflict-driven answer set solving: From theory to practice", "author": ["M. Gebser", "B. Kaufmann", "T. Schaub"], "venue": "AIJ, 187\u2013188,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Treewidth. Computations and Approximations, volume", "author": ["Ton Kloks"], "venue": "LNCS. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "DepQBF: A dependency-aware QBF solver system description", "author": ["F. Lonsing", "A. Biere"], "venue": "J. Sat., Bool. Model. and Comp.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Parameterized algorithms [14, 5] have attracted considerable interest in recent years and allow to tackle hard problems by directly exploiting a small parameter of the input problem.", "startOffset": 25, "endOffset": 32}, {"referenceID": 1, "context": "A parameter that has been researched extensively is treewidth [16, 2].", "startOffset": 62, "endOffset": 69}, {"referenceID": 2, "context": "ASP [3, 13] is a logic-based declarative modelling language and problem solving framework where solutions, so called answer sets, of a given logic program directly represent the solutions of the modelled problem.", "startOffset": 4, "endOffset": 11}, {"referenceID": 2, "context": ", [3, 13].", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "Stateof-the-art ASP grounders support the full ASP-Core-2 language [4] and output smodels input format [19], which we will use for our algorithms.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Hardness is a direct consequence of #\u00b7coNP-hardness for the problem of counting subset minimal models of a CNF formula [6], since answer sets of negation-free programs and subset-minimal models of CNF formulas are essentially the same objects.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "For some arbitrary but fixed integer k and a graph of treewidth at most k, we can compute a TD of width 6 k in time 2O(k ) \u00b7 |V | [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "For every TD, we can compute a nice TD in linear time without increasing the width [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": ", [17, 7].", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "In order to draw conclusions about the efficiency of DynASP2, we mainly inspected the cpu running time and number of timeouts using the average over three runs per instance (three fixed seeds allow certain variance [1] for heuristic TD computation).", "startOffset": 215, "endOffset": 218}, {"referenceID": 7, "context": "We limited available memory (RAM) to 4GB (to run SharpSAT on large instances), and cpu time to 300 seconds, and then compared DynASP2 with the dedicated #SAT solvers SharpSAT [20] and Cachet [18], the QBF solver DepQBF0, and the ASP solver Clasp [9].", "startOffset": 246, "endOffset": 249}], "year": 2017, "abstractText": "Parameterized algorithms are a way to solve hard problems more efficiently, given that a specific parameter of the input is small. In this paper, we apply this idea to the field of answer set programming (ASP). To this end, we propose two kinds of graph representations of programs to exploit their treewidth as a parameter. Treewidth roughly measures to which extent the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms, which run in linear time if the treewidth and weights of the given program are bounded. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth, especially for counting answer sets.", "creator": "LaTeX with hyperref package"}}}