{"id": "1702.03920", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Cognitive Mapping and Planning for Visual Navigation", "abstract": "The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified common architecture for mapping and planning, so that mapping is driven by the planner's needs, and b) spatial memory with the ability to plan the world in the face of an incomplete set of observations. CMP constructs a top-down map of the world and uses a differentiated neural network planner to produce the next action at every step. Accumulated belief in the world enables the agent to track visited regions of the environment. Our experiments show that CMP outperforms both reactive strategies and standard memory-based architectures and functions well in novel environments. In addition, we show that CMP can also achieve semantically specified goals, such as \"go to a chair.\"", "histories": [["v1", "Mon, 13 Feb 2017 18:52:04 GMT  (1879kb,D)", "http://arxiv.org/abs/1702.03920v1", "Under review for CVPR 2017. Project webpage:this https URL"], ["v2", "Sun, 23 Apr 2017 01:59:30 GMT  (1812kb,D)", "http://arxiv.org/abs/1702.03920v2", "To Appear at CVPR 2017. Project website with code, models, simulation environment and videos:this https URL"]], "COMMENTS": "Under review for CVPR 2017. Project webpage:this https URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.RO", "authors": ["saurabh gupta", "james davidson", "sergey levine", "rahul sukthankar", "jitendra malik"], "accepted": false, "id": "1702.03920"}, "pdf": {"name": "1702.03920.pdf", "metadata": {"source": "CRF", "title": "Cognitive Mapping and Planning for Visual Navigation", "authors": ["Saurabh Gupta", "James Davidson", "Sergey Levine", "Rahul Sukthankar", "Jitendra Malik"], "emails": ["malik}@eecs.berkeley.edu,", "sukthankar}@google.com"], "sections": [{"heading": null, "text": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as \u201cgo to a chair\u201d."}, {"heading": "1. Introduction", "text": "In this work we study the problem of robot navigation in novel environments. As humans when we navigate through environments, we can reason about free-space, static and dynamic obstacles, environment topology, and common sense structure of indoor environments. This allows us to make our way through novel environments without relying on an external map. Even in very complex indoor environments, human navigation is much better than uninformed exploration. Part of this is made possible by our experiences in other indoor environments. We are able to extract common sense rules and heuristics for navigation. For example, to go from one room to another, we must first exit the first room; to go to a room at the other end of the building, getting into a hallway is more likely to succeed than\nWork done when S. Gupta was an intern at Google. Project website with videos: https://sites.google.com/\nview/cognitive-mapping-and-planning/.\nentering a conference room; a kitchen is more likely to be situated in open areas of the building than in the middle of cubicles. These common sense rules allow humans to take common-sense shortcuts without having to explore all parts of a new space in detail.\nHowever, classic approaches to navigation rarely make use of such common sense patterns. Classical SLAM based approaches [14, 59] first build a 3D map using LIDAR, depth, or structure from motion, and then plan paths in this map. These maps are built purely geometrically, and nothing is known until it has been explicitly observed, even when there are obvious patterns. This becomes a problem for goal directed navigation. Humans can often guess, for example, where they will find a chair or that a hallway will probably lead to another hallway but a classical robot agent can at best only do uninformed exploration. The separation between mapping and planning also makes the overall system unnecessarily fragile. For example, the mapper might fail on texture-less regions in a corridor, leading to failure of the whole system, but precise geometry may not even be necessary if the robot just has to keep traveling straight.\nInspired by this reasoning, recently there has been an increasing interest in more end-to-end learning-based ap-\nar X\niv :1\n70 2.\n03 92\n0v 1\n[ cs\n.C V\n] 1\n3 Fe\nb 20\n17\nproaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps. These methods thus enjoy the power of being able to learn behaviors from experience. This is well demonstrated by the work from Zhu et al. [66] which trains a reactive system to output navigation macro-actions directly from visual input. They show that such an agent learns semantic patterns, e.g. that a living room area will be next to a dining area. However, such a memory-less agent cannot map, plan or explore the environment simply because it cannot remember what parts of the environment it has visited, nor their appearance. In contrast, experiments have shown that even rats build mental maps of the environments in which they live [60], and can find shortcuts through them that a reactive agent is unable to discover.\nThus, we believe for an agent to successfully navigate in a novel environment, we need a) spatial memory to capture the layout of the world, b) a planner that can plan paths given partial information and c) a unified joint architecture wherein mapping is driven by the needs of the planner."}, {"heading": "2. Overview", "text": "Figure 1 shows an overview of our proposed Cognitive Mapping and Planning architecture. Our approach is composed of two parts, a mapper and a planner. Our proposed mapper fuses information from input views as observed by the agent over time to produce a metric egocentric multiscale belief about the world in a top-down view. Our planner uses this multi-scale egocentric belief of the world to plan paths to the target (also expressed as a point in robot\u2019s top-down egocentric view) and outputs the optimal action to take. This process is repeated at each time step to obtain the final trajectory to the goal.\nAt each time step, the agent updates the belief of the world from the previous time step by a) using the egomotion to transform the belief from the previous time step into the current coordinate frame and b) incorporating information from the current view of the world to update the belief. This allows the agent to progressively improve its model of the world as it moves around. The most significant contrast with prior work is that our approach is trained end-to-end to take good actions in the world. To that end, instead of analytically computing the update to the belief (via classical structure from motion) we frame this as a learning problem and train a convolutional neural network to predict the update based on the observed first person view. We make the belief transformation and update operations differentiable thereby allowing for end-to-end training. This allows our method to adapt to the statistical patterns in real indoor scenes without the need for explicit supervision of the mapping stage.\nOur planner uses the metric belief of the world obtained through the mapping operation described above to plan\npaths to the goal. We use value iteration as our planning algorithm but crucially use a trainable, differentiable and hierarchical version of value iteration. This has three advantages, a) being trainable it naturally deals with partially observed environments by explicitly learning when and where to explore, b) being differentiable it enables us to train the mapper for navigation, and c) being hierarchical it allows us to plan paths to distant goal locations in time complexity that is logarithmic in the number of steps to the goal.\nOur approach is a reminiscent of classical work in navigation that also involves building maps and then planning paths in these maps to reach desired target locations. However, our approach differs from classical work in the following significant way: except for the architectural choice of maintaining a metric belief, everything else is learned from data. This leads to some very desirable properties: a) our model can learn statistical regularities of indoor environments, b) jointly training the mapper and the planner allows us to learn maps for the task of navigation at the same time makes our planner more robust to errors of the mapper, and c) our model can be used in an online manner in novel environments without requiring a pre-constructed map."}, {"heading": "3. Problem Setup", "text": "Before we describe the details of our proposed architecture, we introduce some modeling assumptions, notation, and details of the experimental setup.\nTo be able to focus on the high-level mapping and planning problem we remove confounding factors arising from low-level control by conducting our experiments in simulated real world indoor environments. Studying the problem in simulation makes it easier to run exhaustive evaluation experiments, while the use of scanned real world environments allows us to retains the richness and complexity of real scenes. We also only study the static version of the problem, though extensions to dynamic environments would be interesting to explore in future work.\nWe model the robot as a cylinder of a fixed radius and height, equipped with vision sensors (RGB cameras or depth cameras) mounted at a fixed height and oriented at a fixed pitch. The robot is equipped with low-level controllers which provide relatively high-level macro-actions Ax,\u03b8. These macro-actions are a) stay in place, b) rotate left by \u03b8, c) rotate right by \u03b8, and d) move forward x cm, denoted by a0, a1, a2 and a3, respectively. We further assume that the environment is a grid world and the robot uses its macro-actions to move between nodes on this graph. The robot also has access to its precise ego-motion. This amounts to assuming perfect visual odometry [51], which can itself be learned [25], but we defer the joint learning problem to future work.\nWe want to learn policies for this robot for navigating in novel environments that it has not previously encountered.\nWe study two navigation tasks, a geometric task where the robot is required to go to a target location specified in robot\u2019s coordinate frame (e.g. 250cm forward, 300cm left) and a semantic task where the robot is required to go to an object of interest (e.g. a chair). Note that these tasks are performed in novel environments, neither the exact map of the environment or its underlying topology is available to the robot.\nWe will denote positions in the world coordinate frame with capital letters and positions in the robot coordinate frame using small letters. Positions are described by their x-coordinate, y-coordinate and orientation. The location of the robot in the robot coordinate frame is always (0, 0, \u03c0/2), and Tt denotes the transformation that maps positions from the global coordinate frame to robot\u2019s coordinate frame at a given time step t.\nWe are now ready to define our navigation problem. At a given time step t, let us assume the robot is at a global position (position in the world coordinate frame) Pt. At each time step the robot receives as input the image of the environment E , It = I(E , Pt) and a target location (xgt , y g t , \u03b8 g t ) (or a semantic goal) specified in the coordinate frame of the robot. The navigation problem is to learn a policy that at every time steps uses these inputs (current image, egomotion and target specification) to output the action that will convey the robot to the target as quickly as possible."}, {"heading": "3.1. Experimental Testbed", "text": "We conduct our experiments on the Stanford large-scale 3D Indoor Spaces (S3DIS) dataset introduced by Armeni et al. [5]. The dataset consists of 3D scans collected in 6 largescale indoor areas that originate from 3 different buildings\nof educational and office use. The dataset was collected using the Matterport scanner [1]. We worked with meshes for these environments obtained from the authors. Scans from 2 buildings were used for training and the agents were tested on scans from the 3rd building.\nWe pre-processed the meshes to compute space traversable by the robot. Top views of the obtained traversable space are shown in Section A and indicate the complexity of the environments we are working with and the differences in layouts between the training and testing environments. Recall that robot\u2019s action space Ax,\u03b8 consists of macro-actions. We pick \u03b8 to be \u03c0/2 which allows us to pre-compute the set of locations (spatial location and orientation) that the robot can visit in this traversable space. We also precompute a directed graph Gx,\u03b8 consisting of this set of locations as nodes and a connectivity structure based on the actions available to the robot.\nOur setup allows us to study navigation but also enables us to independently develop and design our mapper and planner architectures. We develop our mapper by studying the problem of free space prediction from sequence of first person view as available while walking through these environments. We develop our planner by using the ground truth top view free space as 2D mazes to plan paths through. Note that this division was merely done to better understand each component, the final mapper and planner are trained jointly and there is no restriction on what information gets passed between the mapper and the planner."}, {"heading": "4. Mapping", "text": "In this section, we describe how the mapping portion of our learned network can integrate first-person camera im-\nages into a top-down 2D representation of the environment, while learning to leverage statistical structure in the world. Note that, unlike analytic mapping systems, the map in our model amounts to a latent representation. Since it is fed directly into a learned planning module, it need not encode purely free space representations, but can instead function as a general spatial memory \u2013 that is, the model can learn to store inside the map whatever information is most useful for generating successful plans. However to make discussion concrete in this section, we assume that the mapper predicts free space.\nThe mapper architecture is illustrated in Figure 2. At every time step t we maintain a cumulative estimate of the free space ft in the coordinate frame of the robot. ft is represented as a multi-channel 2D feature map that metrically represents space in the top-down view of the world. ft is estimated from the currently observed image It, cumulative estimate from the previous time step ft\u22121 and egomotion between the last step and this step et using the following update rule:\nft = U (W (ft\u22121, et) , f \u2032 t) where, f \u2032 t = \u03c6 (It) . (1)\nhere, W is a function that transforms the free space prediction from the previous time step ft\u22121 according to the egomotion in the last step et, \u03c6 is a function that takes as input the current image It and outputs an estimate of the free space based on the view of the environment from the current location (denoted by f \u2032t). U is a function which accumulates the free space prediction from the current view with the accumulated prediction from previous time steps. Next, we describe how each of the functions W , \u03c6 and U are realized.\nThe function W is realized using bi-linear sampling. Given the ego-motion, we compute a backward flow field \u03c1(et). This backward flow maps each pixel in the current\nfree space image ft to the location in the previous free space image ft\u22121 where it should come from. This backward flow \u03c1 can be analytically computed from the ego-motion (Section B). The function W uses bi-linear sampling to apply this flow field to the free space estimate from the previous frame. Bi-linear sampling allows us to back-propagate gradients from ft to ft\u22121 [34], which will make it possible to train this model end to end.\nThe function \u03c6 is realized by a convolutional neural network. Because of our choice to represent free space always in the coordinate frame of the robot, this becomes a relatively easy function to learn, given the network only has to output free space in the current coordinate, rather than in an arbitrary world coordinate frame determined by the cumulative egomotion of the robot so far.\nIntuitively, this network can use semantic cues (such as presence of scene surfaces like floor and walls, common furniture objects like chairs and tables) alongside other learned priors about size and shapes of the common objects to generate free space estimates, even for object that may only be partiality visible. Figure 3 shows a simple example for this where our proposed mapper is in fact able to make predictions for space which haven\u2019t been observed.\nThe architecture of the neural network that realizes function \u03c6 is shown in Figure 2. It is composed of a convolutional encoder which uses residual connections [27] and produces a representation of the scene in the 2D image space. This representation is transformed into one that is in the egocentric 2D top-down view via fully connected layers. This representation is up-sampled using up-convolutional layers (also with residual connections) to obtain the update to the belief about the world from the current frame.\nIn addition to producing an estimate of the free space from the current view f \u2032t the model also produces a confidence c\u2032t. This estimate is also similarly warped by the warping function W and accumulated over time into ct. This estimate allows us to simplify the update function, and can be thought of as playing the role of the update gate in a gated recurrent unit. The update function U takes in the tuples (ft\u22121, ct\u22121), and (f \u2032t , c \u2032 t) and produces (ft, ct) as fol-\nlows:\nft = ft\u22121ct\u22121 + f\n\u2032 tc \u2032 t\nct\u22121 + c\u2032t and ct = ct\u22121 + c\u2032t (2)\nThis choice for an analytic update function was made to keep the overall architecture simple and can be replaced with more sophisticated functions like those realized by LSTMs [31].\nMapper performance in isolation To demonstrate that our proposed mapper architecture works we test it in isolation on the task of free space prediction. We consider the scenario of an agent rotating about its current position, and the task is to predict free space in a 3.20 meter neighborhood of the agent. We only provide supervision for this experiment at end of the agents rotation. Figure 3 illustrates what the mapper learns. Observe that our mapper is able to make predictions where no observations are made. We also report the mean average precision for various versions of the mapper Table 1 on the test set (consisting of 2000 locations from the testing environment). We compare against an analytic mapping baseline which projects points observed in the depth image into the top view (by back projecting them into space and rotating them into the top-down view)."}, {"heading": "5. Planning", "text": "Our planner is based on value iteration networks proposed by Tamar et al. [58], who observed that a particular type of planning algorithm called value iteration [7] can be implemented as a neural network with alternating convolutions and channel-wise max pooling operations, allowing the planner to be differentiated with respect to its\ninputs. Value iteration can be thought of as a generalization of Dijkstra\u2019s algorithm, where the value of each state is iteratively recalculated at each iteration by taking a max over the values of its neighbors plus the reward of the transition to those neighboring states. This plays nicely with 2D grid world navigation problems, where these operations can be implemented with small 3 \u00d7 3 kernels followed by max-pooling over channels. Tamar et al. [58] also showed that this reformulation of value iteration can also be used to learn the planner (the parameters in the convolutional layer of the planner) by providing supervision for the optimal action for each state. Thus planning can be done in a trainable and differentiable manner by very deep convolutional network (with channel wise max-pooling). For our problem, the mapper produces the 2D top-view of the world which shares the same 2D grid world structure as described above, and we use value iteration networks as a trainable and differentiable planner.\nHierarchical Planning Value iteration networks as presented in [58](v2) are impractical to use for any longhorizon planning problem. This is because the planning step size is coupled with the action step size thus leading to a) high computational complexity at run time, and b) a hard learning problem as gradients have to flow back for as many steps. To alleviate this problem, we extend the hierarchical version presented in [58](v1).\nOur hierarchical planner plans at multiple spatial scales. We start with a k times spatially downsampled environment and conduct l value iterations in this downsampled environment. The output of this value iteration process is center cropped, upsampled, and used for doing value iterations at a finer scale. This process is repeated to finally reach the res-\nolution of the original problem. This procedure allows us to plan for goals which are as far as l2k steps away while performing (and backpropagating through) only lk planning iterations. This efficiency increase comes at the cost of more approximate planning.\nPlanning in Partially Observed Environments Value iteration networks have only been evaluated for the case where the environment is fully observed, that is the entire map is known while planning. However, for our navigation problem, the map is only partially observed. Because the planner is not hand specified but learned from data, it can learn policies which naturally take partially observed maps into account. Note that the mapper produces not just a belief about the world but also an uncertainty ct, the planner knows which parts of the map have and haven\u2019t been observed."}, {"heading": "6. Joint Architecture", "text": "Our final architecture, Cognitive Mapping and Planning (CMP) puts together the mapper and planner described above. At each time step, the mapper updates its multi-scale belief about the world based on the current observation. This updated belief is input to the planner which outputs the action to take. As described previously, all parts of the network are differentiable and allow for end-to-end training, and no additional direct supervision is needed to train the mapping module \u2013 rather than producing maps that match some ground truth free space, the mapper simply needs to produce maps that the planner can use to choose effective actions.\nTraining Procedure We train the CMP network using fully supervised training using DAGGER [54]. We generate training trajectories by sampling an arbitrary goal location on the map, and sampling a starting location which is within K actions from the goal location. We compute the optimal actions at each node along the trajectory, and use this to provide supervision for training the network. We use an online version of DAGGER, where during any episode we sample the next state based on the action from the agent\u2019s current policy, or from the expert policy. We use scheduled sampling [8], where we anneal the probability of sampling from the expert trajectories from 1 to 0 using inverse sigmoid decay.\nNote that the focus of this work is on studying different architectures for navigation. Our proposed architecture can also be trained with alternate paradigms for learning such policies, such as reinforcement learning. We chose DAGGER for training our models because we found it to be significantly more sample efficient and stable in our domain, allowing us to focus on the architecture design."}, {"heading": "7. Experiments", "text": "All our models are trained asynchronously with 16 parallel GPU workers and 16 parameter servers using TensorFlow [2]. We used ADAM [38] to optimize our loss function and trained for 60K iterations with a learning rate of 0.001 which was dropped by a factor of 10 every 20K iterations (we found this necessary for consistent training across different runs). We use weight decay of 0.0001 to regularize the network and use batch-norm [32].\nWe use ResNet-50 [28] pre-trained on ImageNet [15] to represent our RGB images. We transfer supervision from RGB images to depth images using cross modal distillation [24] between RGB-D image pairs rendered from meshes in the training set to obtain a pre-trained ResNet-50 model to represent depth images.\nWe compare our proposed CMP architecture to other alternate architectures such as a reactive agent and a LSTM based agent. Since the goal of this paper is to study various architectures for navigation we train all these architectures the same way using DAGGER [54] as described earlier.\nWe first present results for the task where the goal is specified geometrically in terms of position of the goal in robot\u2019s coordinate frame in Table 2 (top part) and Figure 5. Problems for this task are generated by first sampling a start node on the graph and then sampling an end node which is within 32 steps from the starting node and preferably in another room or in the hallway (we use room and hallway annotations from the dataset [5]). The same sampling process is used during training and testing. We sample 4000 problems for testing and these remain fixed across different algorithms that we compare. We measure performance using the distance to goal after running the learned policy for the episode length (39 time steps). We report multiple error metrics, the mean distance to goal, the 75th percentile distance to goal and the success rate (the agent succeeds if it is within a distance of three steps to the goal location at the end of the episode). Table 2 reports these metrics at end of the episode, while Figure 5 plots them across time steps. We report all numbers on the test set. The test set consists of a floor from an altogether different building not contained in the training set (see dataset website and Section A for visualizations of these environments).\nNearest Neighbor Trajectory Transfer: To quantify similarity between training and testing environments, we transfer optimal trajectories from the train set to the test set using visual nearest neighbors (in RGB ResNet-50 feature space). This transfer is done as follows. At each time step we pick the location in the training set which results in the most similar view to that seen by the agent at the current time step. We then compute the optimal action that conveys the robot to the same relative offset in the training environment from this location and execute this action at the current time step. This procedure is repeated at each time step. Such a transfer\nleads to very poor results. The mean and median distance to goal is 22 and 25 steps respectively, highlighting the differences between the train and test environments.\nNo image, goal location only with LSTM: This refers to the experimental setting where we ignore the image and simply use the relative goal location (in robot\u2019s current coordinate frame) as input to a LSTM, and predict the action that the agent should take. The relative goal location is embedded into a K dimensional space via fully connected layers with ReLU non-linearities before being input to the LSTM. As expected, this does rather poorly.\nReactive Policy, Single Frame We next compare to a reactive agent which uses the first-person view of the world. As described above we use ResNet-50 to extract features. These features are passed through a few fully connected layers, and combined with the representation for the relative goal location which is used to predict the final action. We experimented with additive and multiplicative combination strategies and both performed similarly. Note that this reactive baseline is able to perform well on the training environments obtaining a mean distance to goal of about 9 steps, but perform poorly on the test set only being able to get to within 17 steps of the goal on average. This suggests that a reactive agent is able to effectively memorize the environments it was trained on, but fails to generalize to novel environments, this is not surprising given it does not have any form of memory to allow it to map or plan. We also\nexperimented with using DropOut in all the fully connected layers for this model but found that to hurt performance on both the training and the test sets.\nReactive Policy, Multiple Frames We also consider the case where the reactive policy receives 3 previous frames in addition to the current view. Given the robot\u2019s step-size is fairly large we consider a late fusion architecture and fuse the information extracted from ResNet-50. Note that this architecture is similar to the one used in [66]. The primary differences are: goal is specified in terms of relative offset (instead of an image), training uses DAGGER (which utilizes denser supervision) instead of A3C, and testing is done in novel environments. These adaptations are necessary to make an interpretable comparison on our task. Using additional frames as input leads to a large improvement in performance for the reactive agent, specially when using depth images.\nLSTM Based Agent Finally, we also compare to an agent which uses an LSTM based memory. We introduce LSTM units on the multiplicatively combined image and relative goal location representation. Such an architecture also gives the LSTM access to the egomotion of the agent (via how the relative goal location changes between consecutive steps). Thus this model has access to all the information that our method uses. We also experimented with other LSTM based models (ones without egomotion, inputting the egomotion more explicitly, etc.), but weren\u2019t able to reliably\ntrain them in early experiments and did not pursue them further. This LSTM based model is able to consistently outperform the reactive baseline.\nWe compare these baselines with of our proposed method. We find that CMP is able to outperform all these baselines across all metrics for both RGB and depth image case. CMP achieves a lower 75th %ile distance to goal (14 and 1 as compared to 21 and 5 for the LSTM) and improves the success rate to 62.5% and 78.3% from 53.0% and 71.8%.\nWe also report variance in performance over five retrainings from different random initializations of the network for the most competitive 3 methods (Reactive with 4 frames, LSTM and CMP) for the depth image case. Figure 5 (right) shows the performance, the solid line shows the median metric value and the surrounding shaded region represents the minimum and maximum metric value over the five re-trainings. We observe that the variation in performance is reasonable small for all models and improvements due to CMP are significant.\nAblations We also present performance of ablated versions of our proposed method.\nSingle Scale Planning We replace the multi-scale plan-\nner with a single-scale planner. This results in slightly better performance but comes at the cost of increased planning cost.\nNo Planning We swap out the planner CNN with a shallower CNN. This also results in drop in performance specially for the RGB case as compared to the full system which uses the full planner.\nAnalytic Mapper We also train a model where we replace our learned mapper for an analytic mapper that projects points from the depth image into the overhead view and use it with a single scale version of the planner. We observe that this analytic mapper actually works worse than the learned one thereby validating our architectural choice of learning to map.\nAdditional experiment on an internal Matterport dataset We also conduct experiments on an internal Matterport dataset consisting of 41 scanned environments. We train on 27 of these environments, use 4 for validation and test on the remaining 10. We show results for the 10 test environments in Figure 6. We again observe that CMP consistently outperforms the 4 frame reactive baseline and LSTM.\nAdditional comparisons between LSTM and CMP We also report additional experiments on the Stanford S3DIS dataset to further compare the performance of the LSTM baseline with our model in the most competitive scenario where both methods use depth images. These are reported in Table 3. We first evaluate how well do these models perform in the setting when the target is much further away (instead of sampling problems where the goal is within 32 time steps we sample problems where the goal is 64 times steps away). We present evaluations for two cases, when this agent is run for 79 steps or 159 steps (see \u2018Far away goal\u2019 rows in Table 3). We find that both methods suffer when running for 79 steps only, because of limited time available\nfor back-tracking, and performance improves when running these agents for longer. We also see a larger gap in performance between LSTM and CMP for both these test scenarios, thereby highlighting the benefit of our mapping and planning architecture. Figure 7 and Figure 8 visualize and discuss some representative success and failure cases for CMP in this scenario. Video examples are available on the project website.\nWe also evaluate how well these models generalize when trained on a single scene (\u2018Train on 1 scene\u2019). We find that there is a smaller drop in performance for CMP as compared to LSTM. We also found CMP to transfer from internal Matterport dataset to the Stanford S3DIS Dataset slightly\nbetter (\u2018Transfer from internal dataset\u2019). We also study how performance of LSTM and CMP compares across geometric navigation tasks of different hardness in Figure 9. We define hardness as the gap between the ground truth and heuristic (Manhattan) distance between the start and goal, normalized by the ground truth distance. For each range of hardness we show the fraction of cases where LSTM gets closer to the goal (LSTM Better), both LSTM and CMP are equally far from the goal (Both Equal) and CMP gets closer to goal than LSTM (CMP Better). We observe that CMP is generally better across all values of hardness, but for RGB images it is particularly better for cases with high hardness.\nSemantic Task Here we present experiments where the target is specified semantically. In particular we study the task of \u2018going to a chair\u2019. We use the chair annotations pro-\nvided by Armeni et al. [5] and label nodes in the graph Gx,\u03b8 as being \u2018chair\u2019 nodes if they were within 80 cm of a chair. Note that this is only done to generate supervision for optimal actions during training and to evaluate the performance of the agents at test time. This supervision is not used by\nthe agent in any way, it must learn appearance models for chairs jointly with the policy to reach them. We initialize the agent such that it is within 32 time steps from a chair and train it to go towards the nearest chair.\nAs before we report the mean distance to a chair, 75th %ile distance to a chair and the success rate after executing a fixed number of steps (39 steps).\nWe compare our method to the best performing reactive baseline and LSTM based baseline from the geometric navigation task1. We report results in Table 2 (bottom part). This is a challenging task specially because the agent may start in a location from which no chair is visible, and it must learn to explore the environment to find chairs. CMP is able to achieve a higher success rate than the baselines. Figure 10 shows some sample trajectories for this task for CMP.\nPart of the reason why performance suffers at this task is the inability of the agent to recognize chairs. To understand this better we considered an easier semantic task, where chair locations are explicitly marked by an easily identifiable object (a floating cube at the location of the chair, reported as \u2018Semantic Task with Markers\u2019 in Table 3). We found this to significantly boost performance suggesting incorporating appearance models from external large-scale datasets will improve performance for such tasks.\nVisualizations To better understand the representation learned by the mapper, we train readout functions on the learned mapper representation to predict free space [4]. Figure 11 visualizes these readout functions at two time steps from an episode as the agent moves. As expected, the representation output by the mapper carries information about free space in the environment. Readouts are generally better at finer scales. Finally, Figure 12 visualizes a 1 channel projection of the value map for the single scale version of our model at five time steps from an episode."}, {"heading": "8. Related Work", "text": "Navigation is one of the most fundamental problems in mobile robotics. The standard approach is to decompose the problem into two separate stages: (1) mapping the environment, and (2) planning a path through the constructed map [17, 36]. Decomposing navigation in this manner allows each stage to be developed independently, but prevents each from exploiting the specific needs of the other. A comprehensive survey of classical approaches for mapping and planning can be found in [59], we summarize some of the important research here.\nMapping has been well studied in robotics under the problem of simultaneous localization and mapping (SLAM)\n1This LSTM is impoverished because it no longer receives the egomotion of the agent as input (because the goal can not be specified as an offset relative to the robot). We did experiment with a LSTM model which received egomotion as input but weren\u2019t able to train it in initial experiments.\n[20] in context of range sensors as well as RGB images. Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57]. These approaches take a purely geometric approach. Recent learning based approaches [26,63] study the problem in isolation thus only learning generic task-independent maps.\nPath planning in these inferred maps, has also been well studied, with pioneering works from LaValle [45], and well summarized in [44]. Numerous works have also studied the joint problem of mapping and planning [18, 19], which include works which relax the need for pre-mapping by learning and incrementally updating the map while navigating. These classical approaches treated navigation as a purely geometric problem, Konolige et al. [40] and Aydemir et al. [6] proposed approaches which leveraged semantics for more informed navigation. Kuipers et al. [42, 43] introduce a cognitive mapping model using hierarchical abstractions of maps. Semantics have also been associated with 3D environments more generally [23, 41].\nAs an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53]. A major challenge with using reinforcement learning for this task is the need to process complex sensory input, such as camera images. Recent work in deep reinforcement learning (DRL) has proposed to learn policies in an end-to-end manner [50,56] from pixels to actions. A number of works [22, 49, 56] have proposed improvements to DRL algorithms. [29,49,52,62,65] study how to incorporate memory into such neural network based models. We build on the work from Tamar et al. [58] study how explicit planning can be incorporated in such agents, but do not consider the case of first-person visual navigation, nor provide a framework for memory or mapping. [52] study the generalization behavior of these algorithms to novel environments they have not been trained on.\nIn the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66]. Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data. Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments. Most of this research (with the notable exception of [66]) focuses on the task of navigation in synthetic maze like environments which have little structure to them. Given these environments are randomly generated, the policy seeks to learn a good random exploration strategy, but has no statistical regularities in the layout that it can exploit. We instead test on layouts ob-\ntained from real buildings, and show that our cognitive mapping architecture consistently outperforms feed forward and LSTM architectures proposed in prior work.\nThe research most directly relevant to our work here is that of Zhu et al. [66]. Similar to our work, Zhu et al. also study the problem of first-person view navigation in more realistic environments instead of synthetic mazes. Zhu et al. demonstrate that a model trained in one environment can be finetuned in another environment, allowing it to succeed at visual navigation. However, in contrast to our work, Zhu et al. do not consider zero-shot generalization to previously unseen environments, and focus on smaller worlds where memorization of landmarks is feasible. In our work, we instead explicitly handle generalization to new, never before seen interiors, and show that our model generalizes successfully to floor plans not seen during training.\nRelationship to Contemporary Work Since conducting this research, numerous other works that study the problem of visual navigation have come out. Most notable among these is the work from Sadeghi and Levine [55] that shows that simulated mobility policies can transfer to the real world. Mirowski et al. [48] study sources of auxiliary supervision for better training of visual navigation policies with reinforcement learning. Bhatti et al. [9] incorporate SLAM based maps along with inferred semantics for improving the performance at playing Doom. Brahmbhatt and Hays [11] study the task of navigation in cities using Google Street View data. Zhang et al. [64] and Duan et al. [16] study the task of faster learning for related navigation tasks. [9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world. While all these works tackle the task of visual navigation, none of them leverage mapping and planning modules or propose end-to-end architectures for jointly mapping and planning which is the focus of our work."}, {"heading": "9. Discussion", "text": "In this paper, we introduced a novel end-to-end neural architecture for navigation in novel environments. Our architecture learns to map from first-person viewpoints and uses a planner with the learned map to plan actions for navigating to different goals in the environment. Our experiments demonstrate that such an approach outperforms other direct methods which do not use explicit mapping and planning modules. While our work represents exciting progress towards problems which have not been looked at from a learning perspective, a lot more needs to be done for solving the problem of goal oriented visual navigation in novel environments.\nA central limitations in our work is the assumption of perfect odometry. Robots operating in the real world do not have perfect odometry and a model that factors in uncer-\ntainty in movement is essential before such a model can be deployed in the real world.\nA related limitation is that of building and maintaining metric representations of space. This does not scale well for large environments. We overcome this by using a multi-scale representation for space. Though this allows us to study larger environments, in general it makes planning more approximate given lower resolution in the coarser scales which could lead to loss in connectivity information. Investigating representations for spaces which do not suffer from such limitations is important future work.\nIn this work we have exclusively used DAGGER for training our agents. Though this resulted in good results, it suffers from the issue that the optimal policy under an expert may be unfeasible under the information that the agent currently has. Incorporating this in learning through guided policy search or reinforcement learning may lead to better performance specially for the case when the goal is not specified geometrically."}, {"heading": "A. Environment Visualization", "text": "In this section, we show the maps for the environments that we studied in this work. Figure A1, Figure A2, Figure A3 show the environments we trained and validated on. Figure A4 shows the environment we tested on. Note that the testing environment came from a different building altogether (no floors of this building were used for training or validation). We additionally also plot some example geometric navigation problems in these environments, the task is to go from the circle node to the star node. The red bar in the corner denotes a length of 32 units (12.80 metres) in each of the environment."}, {"heading": "B. Computing backward flow field \u03c1 from egomotion", "text": "Consider a robot that rotates about its position by an angle \u03b8 and then moves t units forward. Corresponding points p in the original top-view and p\u2032 in the new top-view are related to each other as follows (R\u03b8 is a rotation matrix that rotates a point by an angle \u03b8):\np\u2032 = Rt\u03b8p\u2212 t or p = R\u03b8(p\u2032 + t) (3)\nThus given the egomotion \u03b8 and t, for each point in the new top-view we can compute the location in the original topview from which it came from.\nFigure A1: Maps for area1 and area6. Light area shows traversible space. Red bar in the corner denotes a length of 32 units (12.80 metres). We also show some example geometric navigation problems in these environments, the task is to go from the circle node to the star node.\nFigure A2: Maps for area52 and area3. Light area shows traversible space. Red bar in the corner denotes a length of 32 units (12.80 metres). We also show some example geometric navigation problems in these environments, the task is to go from the circle node to the star node.\nFigure A3: Map for area51. Light area shows traversible space. Red bar in the corner denotes a length of 32 units (12.80 metres). We also show some example geometric navigation problems in these environments, the task is to go from the circle node to the star node.\nFigure A4: Map for area4. This floor was used for testing all the models. Light area shows traversible space. Red bar in the corner denotes a length of 32 units (12.80 metres). We also show some example geometric navigation problems in these environments, the task is to go from the circle node to the star node."}], "references": [{"title": "Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["D. Abel", "A. Agarwal", "F. Diaz", "A. Krishnamurthy", "R.E. Schapire"], "venue": "arXiv preprint arXiv:1603.04119,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixels to voxels: modeling visual representation in the human brain", "author": ["P. Agrawal", "D. Stansbury", "J. Malik", "J.L. Gallant"], "venue": "arXiv preprint arXiv:1407.5104,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "3d semantic parsing of largescale indoor spaces", "author": ["I. Armeni", "O. Sener", "A.R. Zamir", "H. Jiang", "I. Brilakis", "M. Fischer", "S. Savarese"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Active visual object search in unknown environments using uncertain semantics", "author": ["A. Aydemir", "A. Pronobis", "M. G\u00f6belbecker", "P. Jensfelt"], "venue": "IEEE Transactions on Robotics, 29(4):986\u20131002,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A markovian decision process", "author": ["R. Bellman"], "venue": "Technical report, DTIC Document,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1957}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing doom with slamaugmented deep reinforcement learning", "author": ["S. Bhatti", "A. Desmaison", "O. Miksik", "N. Nardelli", "N. Siddharth", "P.H. Torr"], "venue": "arXiv preprint arXiv:1612.00380,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Model-free episodic control", "author": ["C. Blundell", "B. Uria", "A. Pritzel", "Y. Li", "A. Ruderman", "J.Z. Leibo", "J. Rae", "D. Wierstra", "D. Hassabis"], "venue": "arXiv preprint arXiv:1606.04460,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepnav: Learning to navigate large cities", "author": ["S. Brahmbhatt", "J. Hays"], "venue": "arXiv preprint arXiv:1701.09135,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2722\u20132730,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning transferable policies for monocular reactive mav control", "author": ["S. Daftry", "J.A. Bagnell", "M. Hebert"], "venue": "arXiv preprint arXiv:1608.00627,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Mobile robot localisation using active vision", "author": ["A.J. Davison", "D.W. Murray"], "venue": "European Conference on Computer Vision, pages 809\u2013825. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Rl : Fast reinforcement learning via slow reinforcement learning", "author": ["Y. Duan", "J. Schulman", "X. Chen", "P.L. Bartlett", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Sonar-based real-world mapping and navigation", "author": ["A. Elfes"], "venue": "IEEE Journal on Robotics and Automation, 3(3):249\u2013265,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1987}, {"title": "Using occupancy grids for mobile robot perception and navigation", "author": ["A. Elfes"], "venue": "Computer, 22(6):46\u201357,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "Vision-based autonomous mapping and exploration using a quadrotor mav", "author": ["F. Fraundorfer", "L. Heng", "D. Honegger", "G.H. Lee", "L. Meier", "P. Tanskanen", "M. Pollefeys"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4557\u20134564. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual simultaneous localization and mapping: a survey", "author": ["J. Fuentes-Pacheco", "J. Ruiz-Ascencio", "J.M. Rend\u00f3n- Mancha"], "venue": "Artificial Intelligence Review, 43(1),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A machine learning approach to visual perception of forest trails for mobile robots", "author": ["A. Giusti", "J. Guzzi", "D.C. Cire\u015fan", "F.-L. He", "J.P. Rodr\u0131\u0301guez", "F. Fontana", "M. Faessler", "C. Forster", "J. Schmidhuber", "G. Di Caro"], "venue": "IEEE Robotics and Automation Letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "arXiv preprint arXiv:1603.00748,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Perceptual organization and recognition of indoor scenes from RGB-D images", "author": ["S. Gupta", "P. Arbel\u00e1ez", "J. Malik"], "venue": "CVPR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross modal distillation for supervision transfer", "author": ["S. Gupta", "J. Hoffman", "J. Malik"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Backprop kf: Learning discriminative deterministic state estimators", "author": ["T. Haarnoja", "A. Ajay", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1605.07148,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning longrange vision for autonomous off-road driving", "author": ["R. Hadsell", "P. Sermanet", "J. Ben", "A. Erkan", "M. Scoffier", "K. Kavukcuoglu", "U. Muller", "Y. LeCun"], "venue": "Journal of Field Robotics, 26(2):120\u2013144,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Memorybased control with recurrent neural networks", "author": ["N. Heess", "J.J. Hunt", "T.P. Lillicrap", "D. Silver"], "venue": "arXiv preprint arXiv:1512.04455,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Rgb-d mapping: Using depth cameras for dense 3d modeling of indoor environments", "author": ["P. Henry", "M. Krainin", "E. Herbst", "X. Ren", "D. Fox"], "venue": "In the 12th International Symposium on Experimental Robotics (ISER. Citeseer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera", "author": ["S. Izadi", "D. Kim", "O. Hilliges", "D. Molyneaux", "R. Newcombe", "P. Kohli", "J. Shotton", "S. Hodges", "D. Freeman", "A. Davison", "A. Fitzgibbon"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["G. Kahn", "T. Zhang", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time obstacle avoidance for manipulators and mobile robots", "author": ["O. Khatib"], "venue": "The international journal of robotics research, 5(1):90\u201398,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1986}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["H. Kim", "M.I. Jordan", "S. Sastry", "A.Y. Ng"], "venue": "Advances in neural information processing systems, page None,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE International Conference on, volume 3, pages 2619\u20132624. IEEE,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "View-based maps", "author": ["K. Konolige", "J. Bowman", "J. Chen", "P. Mihelich", "M. Calonder", "V. Lepetit", "P. Fua"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "The spatial semantic hierarchy", "author": ["B. Kuipers"], "venue": "Artificial intelligence, 119(1):191\u2013233,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2000}, {"title": "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations", "author": ["B. Kuipers", "Y.-T. Byun"], "venue": "Robotics and autonomous systems, 8(1):47\u201363,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1991}, {"title": "Planning Algorithms", "author": ["S.M. LaValle"], "venue": "Cambridge University Press, Cambridge, U.K.,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Rapidly-exploring random trees: Progress and prospects", "author": ["S.M. Lavalle", "J.J. Kuffner Jr"], "venue": "Algorithmic and Computational Robotics: New Directions. Citeseer,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, 17(39):1\u201340,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.03673,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Visual odometry", "author": ["D. Nist\u00e9r", "O. Naroditsky", "J. Bergen"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 1, pages I\u2013652. IEEE,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "arXiv preprint arXiv:1605.09128,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural networks, 21(4):682\u2013 697,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "AISTATS, volume 1, page 6,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "CAD)RL: Real singelimage flight without a singel real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison and evaluation of multi-view stereo reconstruction algorithms", "author": ["S.M. Seitz", "B. Curless", "J. Diebel", "D. Scharstein", "R. Szeliski"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 1, pages 519\u2013528. IEEE,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "Value iteration networks", "author": ["A. Tamar", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1602.02867,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT press,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "Cognitive maps in rats and men", "author": ["E.C. Tolman"], "venue": "Psychological review, 55(4):189,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1948}, {"title": "Learning a world model and planning with a self-organizing, dynamic neural system", "author": ["M. Toussaint"], "venue": "NIPS,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "Logic Journal of IGPL, 18(5):620\u2013 634,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Generic 3d representation via pose estimation and matching", "author": ["A.R. Zamir", "T. Wekel", "P. Agrawal", "C. Wei", "J. Malik", "S. Savarese"], "venue": "European Conference on Computer Vision, pages 535\u2013553. Springer,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning with successor features for navigation across similar environments", "author": ["J. Zhang", "J.T. Springenberg", "J. Boedecker", "W. Burgard"], "venue": "arXiv preprint arXiv:1612.05533,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep neural network policies with continuous memory states", "author": ["M. Zhang", "Z. McCarthy", "C. Finn", "S. Levine", "P. Abbeel"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), pages 520\u2013527. IEEE,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei- Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Classical SLAM based approaches [14, 59] first build a 3D map using LIDAR, depth, or structure from motion, and then plan paths in this map.", "startOffset": 32, "endOffset": 40}, {"referenceID": 57, "context": "Classical SLAM based approaches [14, 59] first build a 3D map using LIDAR, depth, or structure from motion, and then plan paths in this map.", "startOffset": 32, "endOffset": 40}, {"referenceID": 44, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 48, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 64, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 64, "context": "[66] which trains a reactive system to output navigation macro-actions directly from visual input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "In contrast, experiments have shown that even rats build mental maps of the environments in which they live [60], and can find shortcuts through them that a reactive agent is unable to discover.", "startOffset": 108, "endOffset": 112}, {"referenceID": 49, "context": "This amounts to assuming perfect visual odometry [51], which can itself be learned [25], but we defer the joint learning problem to future work.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "This amounts to assuming perfect visual odometry [51], which can itself be learned [25], but we defer the joint learning problem to future work.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "using [24] 78.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "Bi-linear sampling allows us to back-propagate gradients from ft to ft\u22121 [34], which will make it possible to train this model end to end.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "It is composed of a convolutional encoder which uses residual connections [27] and produces a representation of the scene in the 2D image space.", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "This choice for an analytic update function was made to keep the overall architecture simple and can be replaced with more sophisticated functions like those realized by LSTMs [31].", "startOffset": 176, "endOffset": 180}, {"referenceID": 56, "context": "[58], who observed that a particular type of planning algorithm called value iteration [7] can be implemented as a neural network with alternating convolutions and channel-wise max pooling operations, allowing the planner to be differentiated with respect to its inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[58], who observed that a particular type of planning algorithm called value iteration [7] can be implemented as a neural network with alternating convolutions and channel-wise max pooling operations, allowing the planner to be differentiated with respect to its inputs.", "startOffset": 87, "endOffset": 90}, {"referenceID": 56, "context": "[58] also showed that this reformulation of value iteration can also be used to learn the planner (the parameters in the convolutional layer of the planner) by providing supervision for the optimal action for each state.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Hierarchical Planning Value iteration networks as presented in [58](v2) are impractical to use for any longhorizon planning problem.", "startOffset": 63, "endOffset": 67}, {"referenceID": 56, "context": "To alleviate this problem, we extend the hierarchical version presented in [58](v1).", "startOffset": 75, "endOffset": 79}, {"referenceID": 52, "context": "Training Procedure We train the CMP network using fully supervised training using DAGGER [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "We use scheduled sampling [8], where we anneal the probability of sampling from the expert trajectories from 1 to 0 using inverse sigmoid decay.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "All our models are trained asynchronously with 16 parallel GPU workers and 16 parameter servers using TensorFlow [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 36, "context": "We used ADAM [38] to optimize our loss function and trained for 60K iterations with a learning rate of 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "0001 to regularize the network and use batch-norm [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "We use ResNet-50 [28] pre-trained on ImageNet [15] to represent our RGB images.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "We use ResNet-50 [28] pre-trained on ImageNet [15] to represent our RGB images.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "We transfer supervision from RGB images to depth images using cross modal distillation [24] between RGB-D image pairs rendered from meshes in the training set to obtain a pre-trained ResNet-50 model to represent depth images.", "startOffset": 87, "endOffset": 91}, {"referenceID": 52, "context": "Since the goal of this paper is to study various architectures for navigation we train all these architectures the same way using DAGGER [54] as described earlier.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "Problems for this task are generated by first sampling a start node on the graph and then sampling an end node which is within 32 steps from the starting node and preferably in another room or in the hallway (we use room and hallway annotations from the dataset [5]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 64, "context": "Note that this architecture is similar to the one used in [66].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "[5] and label nodes in the graph Gx,\u03b8 as being \u2018chair\u2019 nodes if they were within 80 cm of a chair.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Visualizations To better understand the representation learned by the mapper, we train readout functions on the learned mapper representation to predict free space [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "The standard approach is to decompose the problem into two separate stages: (1) mapping the environment, and (2) planning a path through the constructed map [17, 36].", "startOffset": 157, "endOffset": 165}, {"referenceID": 34, "context": "The standard approach is to decompose the problem into two separate stages: (1) mapping the environment, and (2) planning a path through the constructed map [17, 36].", "startOffset": 157, "endOffset": 165}, {"referenceID": 57, "context": "A comprehensive survey of classical approaches for mapping and planning can be found in [59], we summarize some of the important research here.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "[20] in context of range sensors as well as RGB images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 31, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 45, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 55, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 24, "context": "Recent learning based approaches [26,63] study the problem in isolation thus only learning generic task-independent maps.", "startOffset": 33, "endOffset": 40}, {"referenceID": 61, "context": "Recent learning based approaches [26,63] study the problem in isolation thus only learning generic task-independent maps.", "startOffset": 33, "endOffset": 40}, {"referenceID": 43, "context": "Path planning in these inferred maps, has also been well studied, with pioneering works from LaValle [45], and well summarized in [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 42, "context": "Path planning in these inferred maps, has also been well studied, with pioneering works from LaValle [45], and well summarized in [44].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Numerous works have also studied the joint problem of mapping and planning [18, 19], which include works which relax the need for pre-mapping by learning and incrementally updating the map while navigating.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "Numerous works have also studied the joint problem of mapping and planning [18, 19], which include works which relax the need for pre-mapping by learning and incrementally updating the map while navigating.", "startOffset": 75, "endOffset": 83}, {"referenceID": 38, "context": "[40] and Aydemir et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] proposed approaches which leveraged semantics for more informed navigation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[42, 43] introduce a cognitive mapping model using hierarchical abstractions of maps.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[42, 43] introduce a cognitive mapping model using hierarchical abstractions of maps.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "Semantics have also been associated with 3D environments more generally [23, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 39, "context": "Semantics have also been associated with 3D environments more generally [23, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 35, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 37, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 51, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 48, "context": "Recent work in deep reinforcement learning (DRL) has proposed to learn policies in an end-to-end manner [50,56] from pixels to actions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 54, "context": "Recent work in deep reinforcement learning (DRL) has proposed to learn policies in an end-to-end manner [50,56] from pixels to actions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 20, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 47, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 54, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 27, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 47, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 50, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 60, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 63, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 56, "context": "[58] study how explicit planning can be incorporated in such agents, but do not consider the case of first-person visual navigation, nor provide a framework for memory or mapping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] study the generalization behavior of these algorithms to novel environments they have not been trained on.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 10, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 11, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 19, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 33, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 47, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 50, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 56, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 59, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 64, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 10, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 19, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 33, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 8, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 50, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 56, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 56, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 184, "endOffset": 188}, {"referenceID": 64, "context": "Most of this research (with the notable exception of [66]) focuses on the task of navigation in synthetic maze like environments which have little structure to them.", "startOffset": 53, "endOffset": 57}, {"referenceID": 64, "context": "[66].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Most notable among these is the work from Sadeghi and Levine [55] that shows that simulated mobility policies can transfer to the real world.", "startOffset": 61, "endOffset": 65}, {"referenceID": 46, "context": "[48] study sources of auxiliary supervision for better training of visual navigation policies with reinforcement learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] incorporate SLAM based maps along with inferred semantics for improving the performance at playing Doom.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Brahmbhatt and Hays [11] study the task of navigation in cities using Google Street View data.", "startOffset": 20, "endOffset": 24}, {"referenceID": 62, "context": "[64] and Duan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] study the task of faster learning for related navigation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 46, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}, {"referenceID": 53, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}, {"referenceID": 62, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}], "year": 2017, "abstractText": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as \u201cgo to a chair\u201d.", "creator": "LaTeX with hyperref package"}}}