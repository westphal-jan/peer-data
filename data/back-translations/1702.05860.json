{"id": "1702.05860", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Robust Sparse Estimation Tasks in High Dimensions", "abstract": "In this paper, we initiate the investigation of whether or not sparse estimation tasks can be performed efficiently in high dimensions in a robust environment where a fraction of the $\\ eps $samples are counterproductively corrupted. We investigate the natural robust version of two classic sparse estimation problems, namely sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees regarding the presence of noise, using only a number of samples similar to the number required for these problems without noise. Specifically, our sample complexities in the $d $environment dimension are sublinear. Our work also suggests evidence of new computational versus statistical gaps for these problems (similar to those for sparse non-noise PCAs) that arise only in the presence of noise.", "histories": [["v1", "Mon, 20 Feb 2017 05:22:55 GMT  (35kb)", "https://arxiv.org/abs/1702.05860v1", null], ["v2", "Tue, 28 Feb 2017 20:49:30 GMT  (35kb)", "http://arxiv.org/abs/1702.05860v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["jerry li"], "accepted": false, "id": "1702.05860"}, "pdf": {"name": "1702.05860.pdf", "metadata": {"source": "CRF", "title": "Robust Sparse Estimation Tasks in High Dimensions", "authors": ["Jerry Li"], "emails": ["jerryzli@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n05 86\n0v 2\n[ cs\n.L G\n] 2\n8 Fe\nb 20\nIn this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an \u03b5-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise."}, {"heading": "1 Introduction", "text": "In the last couple of decades, there has been a large amount of work in machine learning and statistics on how to exploit sparsity in high dimensional data analysis. Motivated by the ever-increasing quantity and dimensionality of data, the goal at a high level is to utilize the underlying sparsity of natural data to extract meaningful guarantees using a number of samples that is sublinear in the dimensionality of the data. In this paper, we will consider the unsupervised setting, where we have sample access to some distribution with some underlying sparsity, and our goal is to recover this distribution by exploiting this structure. Two natural and well-studied problems in this setting that attempt to exploit sparsity are sparse mean estimation and sparse PCA. In both problems, the shared theme is that we assume that one wishes to find a distinguished sparse direction of a Gaussian data set. However, the algorithms inspired by this line of work tend to be quite brittle\u2014it can be shown that they fail when the model is slightly perturbed.\nThis connects to a major concern in high dimensional data analysis: that of model misspecification. At a high level, the worry is that our algorithms should be able to tolerate the case when our assumed model and the true model do not perfectly coincide. In the distributional setting, this (more or less) corresponds to the regime when a small fraction of our samples are adversarially corrupted. The study of these so-called robust estimators, i.e., estimators which work in the presence of such noise, is a classical subfield of statistics. Unfortunately, the classical algorithms for these problems fail to scale as the dimensionality of the problem grows\u2014either the algorithms run in time which is exponential in the dimension, or the error guarantees for these algorithms degrade substantially as the dimension grows. In a flurry of recent work, we now know new algorithms which circumvent this \u201ccurse of dimensionality\u201d: they run efficiently, and provide dimension independent error guarantees. However, these algorithms are unable to exploit any inherent sparsity in the problem.\nThis raises the natural \u201cmeta-question\u201d:\nQuestion 1.1. Do the statistical gains (achievable by computationally efficient algorithms) for sparse estimation problems persist in the presence of noise?\n\u2217Supported by NSF grant CCF-1217921, DOE grant de-sc0008923, NSF CAREER Award CCF-145326, and a NSF Graduate Research Fel-\nlowship\nMore formally: Suppose we are asked to solve some estimation task given samples from some distributionD with some underlying sparsity constraint (e.g. sparse PCA). Suppose now an \u03b5-fraction of the samples are corrupted. Can we still solve the same sparse estimation problem? In this work, we initiate the study of such issues. Interestingly, new gaps between computational and statistical rates seem to emerge in the presence of noise. In particular, while the sparse mean estimation problem was previously quite simple to solve, the efficient algorithms which achieve the minimax rate for this problem break down in the presence of this adversarial noise. More concretely, it seems that the efficient algorithms which are robust to noise run into the same computational issues as those which plague sparse PCA. A very interesting question is whether this phenomenon is inherent to any computationally efficient algorithm."}, {"heading": "1.1 Our Contribution", "text": "We study the natural robust versions of two classical, well-studied statistical tasks involving sparsity, namely, sparse mean estimation, and sparse PCA.\nRobust sparse mean estimation Here, we get a set of d-dimensional samples from N (\u00b5, I), where \u00b5 is k-sparse, and an \u03b5-fraction of the points are corrupted adversarially. Our goal then is to recover \u00b5. Our main contribution is the following:\nTheorem 1.2 (informal, see Theorem 2.1). There is an efficient algorithm, which given a set of \u03b5-corrupted samples of size O\u0303(k 2 log d \u03b52 ) from N (\u00b5, I) where \u00b5 is k-sparse, outputs a \u00b5\u0302 so that with high probability, \u2016\u00b5\u0302\u2212 \u00b5\u20162 \u2264 \u03b5 \u221a log 1/\u03b5.\nThe recovery guarantee we achieve, namely O(\u03b5 \u221a\nlog 1/\u03b5), is off by the optimal guarantee by only a factor of\u221a log 1/\u03b5. Moreover, results of [DKS16] imply that our bound is tight for any efficient SQ algorithm. One can show that information theoretically, it suffices to take O(k log d\u03b52 ) samples to learn the mean to \u21132 error O(\u03b5), even with corrupted data. Without model misspecification, this problem is quite simple algorithmically: it turns out that the truncated empirical mean achieves the information theoretically optimal rate. However, efficient algorithms for this task break down badly given noise, and to our knowledge there is no simple way of fixing them. Very interestingly, the rate we achieve is off from this information theoretic rate by a k2 vs k factor\u2014the same computational vs. statistical gap that arises in sparse PCA. This phenomenon only seems to appear in the presence of noise, and we conjecture that this is inherent:\nConjecture 1.1. Any efficient algorithm for robust sparse mean estimation needs \u2126\u0303(k 2 log d \u03b52 ) samples.\nIn Appendix D we give some intuition for why it seems to be true. At a high level, it seems that any technique to detect outliers for the mean must look for sparse directions in which the variance is much larger than it should be; at which point the problem faces the same computational difficulties as sparse PCA. We leave closing this gap as an interesting open problem.\nRobust sparse PCA Here, we study the natural robust analogue of the spiked covariance model. Classically, two problems are studied in this setting. The detection problem is given as follows: given sample access to the distributions, we are asked to distinguish betweenN (0, I), and N (0, I + \u03c1vvT ) where v is a k-sparse unit vector. That is, we wish to understand if we can detect the presence of any sparse principal component. Our main result is the following: Theorem 1.3 (informal, see Theorem 2.2). Fix \u03c1 > 0, and let \u03b7 = O(\u03b5 \u221a log 1/\u03b5). If \u03c1 > \u03b7, there is an efficient algorithm, which given a set of \u03b5-corrupted samples of size O(k 2 log d \u03c12 ) which distinguishes between N (0, I), and N (0, I + \u03c1vvT ) with high probability.\nThe condition that \u03b5 = O\u0303(\u03c1) is necessary (up to log factors), as otherwise the problem is impossible information theoretically. Observe that this (up to log factors) matches the optimal rate for computationally efficient detection for sparse PCA without noise (under reasonable complexity theoretic assumptions, see [BR13, WBS16]), and so it seems that noise does not introduce an additional gap here. The recovery problem is similar, except now we want to recover the planted spike v, i.e. find a u minimizing L(u, v) = 1\u221a 2 \u2016uuT \u2212 vvT \u2016, which turns out to be the natural measure for this problem. For this, we show:\nTheorem 1.4 (informal, see Theorem 2.3). Fix \u03b5 > 0 and 0 < \u03c1 = O(1), and let \u03b7 = O(\u03b5 \u221a log 1/\u03b5). There is an efficient algorithm, which given a set of \u03b5-corrupted samples of size O(k 2 log d \u03b72 ) from N (0, I + \u03c1vvT ), outputs a u so\nthat L(u, v) = O (\n\u03b7 \u03c1\n) with high probability.\nThis rate is non-trivial\u2014in particular, it provides guarantees for recovery of v when the number of samples we take is at the detection threshold. Moreover, up to log factors, our rate is optimal for computationally efficient algorithms\u2013 [WBS16] gives an algorithm with rate roughlyO(\u03b5/\u03c1), and show that this is necessary.\nTechniques We first introduce a simple way to describe the optimization problems used for solving sparse mean estimation and sparse PCA. This approach is very similar to the approach taken by [CRPW12] for solving underdetermined linear systems. We observe that any set S in a Hilbert space naturally induces a dual norm \u2016x\u2016\u2217S = maxy\u2208S |\u3008x, y\u3009|, and that well-known efficient algorithms for sparse mean estimation and sparse PCA simply compute this norm, and the corresponding dual witness y \u2208 S which maximizes this norm, for appropriate choices of S. These norms give us a language to only consider deviations in directions we care about, which allows us to prove concentration bounds which are not true for more traditional norms.\nWe now describe our techniques for robust sparse mean estimation. Our starting point is the convex programming approach of [DKK+16]. We assign each sample point a weight, which morally corresponds to our belief about whether the point is corrupted, and we optimize these weights. In previous work of [DKK+16], the approach was to find weights so that the empirical covariance with these weights looked like the identity in spectral norm.\nUnfortunately, such an approach fundamentally fails for us because the spectrum of the covariance will never concentrate for us with the number of samples we take. Instead, we utilize a novel connection to sparse PCA. We show that if instead we find weights so that the empirical covariance with these weights looks like the identity in the dual norm induced by a natural SDP for sparse PCA (in the noiseless setting), then this suffices to show that the trucnated empirical mean with these weights is close to the truth. We do so by convex programming. While we cannot explicitly write down the feasible set of weights, it is a convex set. Thus, by the classical theory of convex optimization, it suffices to give a separation oracle for this convex set to optimize over this set. We show that in fact the SDP for sparse PCA gives us such a separation oracle, if one is sufficiently careful to always work with sparsity preserving objects. This in turns suffices to allow us to (approximately) find a point in the desired feasible set of points, which we show suffices to recover the true mean.\nWe now turn to robust sparse PCA. We first consider the detection problem, which is somewhat easier technically. Here, we again use the dual norm induced by the SDP for sparse PCA. We show that if we can find weights on the samples (as before) so that the empirical covariance with these samples has minimal dual norm, then the value of the dual norm gives us a distinguisher between the spiked and non-spiked case. To find such a set of weights, we observe that norms are convex, and thus our objective is convex. Thus, as before, to optimize over this set it suffices to give a separation oracle, which again the SDP for sparse PCA allows us to do.\nWe now turn our attention to the recovery problem. Here, the setup is very similar, except now we simultaneously find a set of weights and an \u201cexplainer\u201d matrix A so that the empirical covariance with these weights is \u201cmaximally explained\u201d by A, in a norm very similar to the one induced by the sparse PCA SDP. Utilizing that norms are convex, we show that this can be done via a convex program using the types of techniques described above, and that the top eigenvector of the optimal A gives us the desired solution. While the convex program would be quite difficult to write down in one shot, it is quite easily expressible using the abstraction of dual norms."}, {"heading": "1.2 Related Work", "text": "As mentioned previously, there has been a large amount of work on various ways to exploit sparsity for machine learning and statistics. In the supervised setting, perhaps the most well-known of these is compressive sensing and its variants (see [CW08, HTW15] for more details). We do not attempt to provide an exhaustive overview the field here. Other well-known problems in the same vein include general classes of linear inverse problems, see [CRPW12] and matrix completion ([CR12]).\nThe question of estimating a sparse mean is very related to a classical statistical model known as the Gaussian sequence model, and the reader is referred to [Tsy09, Joh11, Rig15] for in-depth surveys on the area. This problem\nhas also garnered a lot of attention recently in various distributed and memory-limited settings, see [GMN14, SD15, BGM+16]. The study of sparse PCA was initiated in [Joh01] and since yielded a very rich algorithmic and statistical theory ([dEGJL07, dBG08, AW08, WTH09, JNRS10, ACCD11, LZ12, Ma13, BJNP13, CMW13, OMH14, GWL14, CRZ16, BMVX16, PWBM16]). In particular, we highlight a very interesting line of work [BR13, KNV15, MW15, WGL15, WBS16], which give evidence that any computationally efficient estimator for sparse PCA must suffer a sub-optimal statistical rate rate. We conjecture that a similar phenomenon occurs when we inject noise into the sparse mean estimation problem.\nIn this paper we consider the classical notion of corruption studied in robust statistics, introduced back in the 70\u2019s in seminal works of [HR09, Tuk75, HRRS86]. Unfortunately, essentially all robust estimators require exponential time in the dimension to compute ([JP78, Ber06, HM13]). Subsequent work of [LT15, BD15] gave efficient SDPbased estimators for these problems which unfortunately had error guarantees which degraded polynomially with the dimension. However, a recent flurry of work ([DKK+16, LRV16, CSV16, DKK+17, DKS17, DKS16]) have given new, computationally efficient, robust estimators for these problems and other settings which avoid this loss, and are often almost optimal. Independent work of [DSS17] also considers the robust sparse setting. They give a similar result for robust mean estimation, and also consider robust sparse PCA, though in a somewhat different setting than we do, as well as robust sparse linear regression.\nThe questions we consider are similar to learning in the presence of malicious error studied in [Val85, KL93], which has received a lot of attention, particularly in the setting of learning halfspaces ([Ser03, KLS09, ABL14]). They also are connected to work on related models of robust PCA ([Bru09, CLMW11, LMTZ12, ZL14]). We refer the reader to [DKK+16] to a detailed discussion on the relationships between these questions and the ones we study."}, {"heading": "2 Definitions", "text": "Throughout this paper, if v is a vector, we will let \u2016v\u20162 denote its \u21132 norm. If M is a matrix, we let \u2016M\u2016 denote its spectral norm, we let \u2016M\u2016F denote its Frobenius norm, and we let \u2016M\u20161 = \u2211 ij |Mij | be its \u21131-norm if it were considered a vector. For any two distributions F,G over Rd, we let dTV(F,G) = 1 2 \u222b Rd\n|F \u2212 G|dx denote the total variation distance between the two distributions.\nWe will study the following contamination model:\nDefinition 2.1 (\u03b5-corruption). We say a a set of samples X1, X2, . . . , Xn is an \u03b5-corrupted set of samples from a distribution D if it is generated by the process following process. First, we draw n independent samples from D. Then, an adversary inspects these samples, and changes an \u03b5-fraction of them arbitrarily, then returns these new points to us, in any order. Given an \u03b5-corrupted set of samples, we let Sgood \u2286 [n] denote the indices of the uncorrupted samples, and we let Sbad \u2286 [n] denote the indices of the corrupted samples.\nAs discussed in [DKK+16], this is a strong notion of sample corruption that is able to simulate previously defined notions of error. In particular, this can simulate (up to constant factors) the scenario when our samples do not come fromD, but come from a distributionD\u2032 with total variation distance at most O(\u03b5) fromD. We may now formally define the algorithmic problems we consider.\nRobust sparse mean estimation Here, we assume we get an \u03b5-corrupted set of samples from N (\u00b5, I), where \u00b5 is k-sparse. Our goal is to recover \u00b5 in \u21132. It is not hard to show that there is an exponential time estimator which achieves rate O\u0303(k log d/\u03b52), and moreover, this rate is optimal (see Appendix A). However, this algorithm requires highly exponential time. We show: Theorem 2.1 (Efficient robust sparse mean estimation). Fix \u03b5, \u03b4 > 0, and let k be fixed. Let \u03b7 = O(\u03b5 \u221a log 1/\u03b5). Given an \u03b5-corrupted set of samplesX1, . . . , Xn \u2208 Rd from N (\u00b5, I), where \u00b5 is k-sparse, and\nn = \u2126\n( min(k2, d) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b72\n) ,\nthere is a poly-time algorithm which outputs \u00b5\u0302 so that w.p. 1\u2212 \u03b4, we have \u2016\u00b5\u2212 \u00b5\u0302\u20162 \u2264 O(\u03b7).\nIt is well-known that information theoretically, the best error one can achieve is \u0398(\u03b5), as achieved by Fact A.1. We show that it is possible to efficiently match this bound, up to a \u221a log 1/\u03b5 factor. Interestingly, our rate differs from that in Fact A.1: our sample complexity is (roughly) O\u0303(k2 log d/\u03b52) versusO(k log d/\u03b52). We conjecture this is necessary for any efficient algorithm.\nRobust sparse PCA We will consider both the detection and recovery problems for sparse PCA. We first focus detection problem for sparse PCA. Here, we are given \u03c1 > 0, and an \u03b5-corrupted set of samples from a d-dimensional distribution D, where D can is either N (0, I) or N (0, I + \u03c1vvT ) for some k-sparse unit vector v. Our goal is to distinguish between the two cases, using as few samples as possible. It is not hard to show that information theoretically,O(k log d/\u03c12) samples suffice for this problem, with an inefficient algorithm (see Appendix A). Our first result is that efficient robust sparse PCA detection is possible, at effectively the best computationally efficient rate: Theorem 2.2 (Robust sparse PCA detection). Fix \u03c1, \u03b4, \u03b5 > 0. Let \u03b7 = O(\u03b5 \u221a\nlog 1/\u03b5). Then, if \u03b7 = O(\u03c1), and we are given a we are given a \u03b5-corrupted set of samples from either N (0, I) or N (0, I + \u03c1vvT ) for some k-sparse unit vector v of size\nn = \u2126\n( min(d, k2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03c12\n)\nthen there is a polynomial time algorithm which succeeds with probability 1\u2212 \u03b4 for detection.\nIt was shown in [BR13] that even without noise, at least n = \u2126(k2 log d/\u03b52) samples are required for any polynomial time algorithm for detection, under reasonable complexity theoretic assumptions. Up to log factors, we recover this rate, even in the presence of noise.\nWe next consider the recovery problem. Here, we are given an \u03b5-corrupted set of samples from N (0, I + \u03c1vvT ), and our goal is to output a u minimizing L(u, v), where L(u, v) = 1\u221a\n2 \u2016uuT \u2212 vvT \u2016. For the recovery problem, we\nrecover the following efficient rate:\nTheorem 2.3 (Robust sparse PCA recovery). Fix \u03b5, \u03c1 > 0. Let \u03b7 be as in Theorem 2.2. There is an efficient algorithm, which given a set of \u03b5-corrupted samples of size n from N (0, I + \u03c1vvT ), where\nn = \u2126\n( min(d, k2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b72\n) ,\noutputs a u so that\nL(u, v) = O\n( (1 + \u03c1)\u03b7\n\u03c1\n) .\nIn particular, observe that when \u03b7 = O(\u03c1), so when \u03b5 = O\u0303(\u03c1), this implies that we recover v to some small constant error. Therefore, given the same number of samples as in Theorem 2.2, this algorithm begins to provide non-trivial recovery guarantees. Thus, this algorithm has the right \u201cphase transition\u201d for when it begins to work, as this number of samples is likely necessary for any computationally efficient algorithm. Moreover, our rate itself is likely optimal (up to log factors), when \u03c1 = O(1). In the non-robust setting, [WBS16] showed a rate of (roughly) O(\u03b5/\u03c1) with the same number of samples, and that any computationally efficient algorithm cannot beat this rate. We leave it as an interesting open problem to show if this rate is achievable or not in the presence of error when \u03c1 = \u03c9(1)."}, {"heading": "3 Preliminaries", "text": "In this section we provide technical preliminaries that we will require throughout the paper."}, {"heading": "3.1 Naive pruning", "text": "We will require the following (straightforward) preprocessing subroutine from [DKK+16] to remove all points which are more than \u2126\u0303(d) away from the true mean.\nFact 3.1 (c.f. Fact 4.18 in [DKK+16]). LetX1, . . . , Xn be an \u03b5-corrupted set of samples fromN (\u00b5, I), and let \u03b4 > 0. There is an algorithm NAIVEPRUNE(X1, . . . , Xn, \u03b4) which runs in O(\u03b5d\n2n2) time so that with probability 1 \u2212 \u03b4, we have that (1) NAIVEPRUNE removes no uncorrupted points, and (2) if Xi is not removed by NAIVEPRUNE, then \u2016Xi \u2212 \u00b5\u20162 \u2264 O( \u221a d log(n/\u03b4)). If these two conditions happen, we say that NAIVEPRUNE has succeeded."}, {"heading": "3.2 Concentration inequalities", "text": "In this section we give a couple of concentration inequalities that we will require in the remainder of the paper. These \u201cper-vector\u201d and \u201cper-matrix\u201d concentration guarantees are well-known and follow from (scalar) Chernoff bounds, see e.g. [DKK+16].\nFact 3.2 (Per-vector Gaussian concentration). Fix \u03b5, \u03b4 > 0. Let v \u2208 Rd be a unit vector, and let X1, . . . , Xn \u223c N (0, I), where\nn = \u2126\n( log 1/\u03b4\n\u03b52\n) .\nThen, with probability 1\u2212 \u03b4, we have \u2223\u2223\u2223\u2223\u2223 \u2329 1 n n\u2211\ni=1\nXi, v \u232a\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5 .\nFact 3.3 (Per-matrix Gaussian concentration). Fix \u03b5, \u03b4 > 0, and suppose \u03b5 \u2264 1. Let M \u2208 Rd\u00d7d be a symmetric matrix, and letX1, . . . , Xn \u223c N (0, I), where\nn = \u2126\n( log 1/\u03b4\n\u03b52\n) .\nThen, with probability 1\u2212 \u03b4, we have: \u2223\u2223\u2223\u2223\u2223 \u2329 1 n n\u2211\ni=1\nXiX T i \u2212 I,M \u232a\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5 ."}, {"heading": "3.3 The set Sn,\u03b5", "text": "For any n, \u03b5, define the set\nSn,\u03b5 =\n{ w \u2208 Rn : n\u2211\ni=1\nwi = 1, and 0 \u2264 wi \u2264 1 (1 \u2212 \u03b5)n, \u2200i } . (1)\nWe make the following observation. For any subset I \u2286 [n], if we let wI be the vector whose ith coordinate is 1/|I| if i \u2208 I and 0 otherwise, we have\nSn,\u03b5 = conv {wI : |I| = (1\u2212 \u03b5)n} .\nThe set Sn,\u03b5 will play a key role in our algorithms. We will think of elements in Sn,\u03b5 as weights we place upon our sample points, where higher weight indicates a higher confidence that the sample is uncorrupted, and a lower weight will indicate a higher confidence that the sample is corrupted."}, {"heading": "4 Concentration for sparse estimation problems via dual norms", "text": "In this section we give a clean way of proving concentration bounds for various objects which arise in sparse PCA and sparse mean estimation problems. We do so by observing they are instances of a very general \u201cmeta-algorithm\u201d we call dual norm maximization. This will prove crucial to proving the correctness of our algorithms for robust sparse recovery. While this may sound similar to the \u201cdual certificate\u201d techniques often used in the sparse estimation literature, these techniques are actually quite different.\nDefinition 4.1 (Dual norm maximization). Let H be a Hilbert space with inner product \u3008\u00b7, \u00b7\u3009. Fix any set S \u2286 H. Then the dual norm induced by S, denoted \u2016 \u00b7 \u2016\u2217S , is defined by \u2016x\u2016\u2217S = supy\u2208S |\u3008x, y\u3009|. The dual norm maximizer of x, denoted dS(x), is the vector dS(x) = argmaxv\u2208S |\u3008v, x\u3009|.\nIn particular, we will use the following two sets. Equip the space of symmetric d\u00d7 d matrices with the trace inner product, i.e., \u3008A,B\u3009 = tr(AB), so that it is a Hilbert space, and let\nUk = {u \u2208 Rd : \u2016u\u20162 = 1, \u2016u\u20160 = k} (2) Xk = {X \u2208 Rd\u00d7d : tr(X) = 1, \u2016X\u20161 \u2264 k,X 0} . (3)\nWe show in Appendix B.1 that existing well-known algorithms for sparse mean recovery and sparse PCA without\nnoise can be naturally written in this fashion.\nAnother detail we will largely ignore in this paper is the fact that efficient algorithms for these problems can only approximately solve the dual norm maximization problem. However, we explain in Appendix B.2 why this does not affect us in any meaningful way. Thus, for the rest of the paper we will assume we have access to the exact maximizer, and the exact value of the norm."}, {"heading": "4.1 Concentration for dual norm maximization", "text": "We now show how the above concentration inequalities allow us to derive very strong concentration results for the dual norm maximization problem for Uk andXk. Conceptually, we view these concentration results as being the major distinction between sparse estimation and non-sparse estimation tasks. Indeed, these results are crucial for adapting the convex programming framework for robust estimation to sparse estimation tasks. Additionally, they allow us to give an easy proof that the L1 relaxation works for sparse PCA.\nCorollary 4.1. Fix \u03b5, \u03b4 > 0. Let X1, . . . , Xn \u223c N (0, I), where\nn = \u2126\n( k + log ( d k ) + log 1/\u03b4\n\u03b52\n) .\nThen \u2016 1n \u2211n i=1 Xi\u2016\u2217Uk \u2264 \u03b5.\nProof. Fix a set of k coordinates, and let S be the set of unit vectors supported on these k coordinates. By Fact 3.2 and a net argument, one can show that for all \u03b4, given n = \u2126 (\nk+log 1/\u03b4 \u03b52\n) , we have that\n\u2223\u2223\u2223\u2223\u2223 \u2329 v, 1 n n\u2211\ni=1\nXi \u232a\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5 ,\nwith probability 1\u2212\u03b4. The result then follows by setting \u03b4\u2032 = ( d k )\u22121 \u03b4 and union bounding over all sets of k coordinates.\nThe second concentration bound, which bounds deviation in Xk norm, uses ideas which are similar at a high level, but requires a bit more technical work.\nTheorem 4.2. Fix \u03b5, \u03b4 > 0. Let X1, . . . Xn \u223c N (0, I), where\nn = \u2126\n( min(d, k2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b52\n) .\nThen\n\u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nXiX T i \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n\u2264 \u03b5 .\nLet us first introduce the following definition.\nDefinition 4.2. A symmetric sparsity pattern is a set S of indices (i, j) \u2208 [d]\u00d7 [d] so that if (i, j) \u2208 S then (j, i) \u2208 S. We say that a symmetric matrixM \u2208 Rd\u00d7d respects a symmetric sparsity pattern S if supp(M) = S.\nWith this definition, we now show:\nLemma 4.3. Let n = O\n( min(d,k2)+log (d 2\nk2)+log 1/\u03b4 \u03b52\n) . Then, with probability 1\u2212 \u03b4, the following holds:\n|tr((\u03a3\u0302\u2212 I)X)| \u2264 O(\u03b5), for all symmetric X with \u2016X\u20160 = k2 and \u2016X\u2016F \u2264 1. (4)\nProof. Fix any symmetric sparsity pattern S so that |S| \u2264 k2. By classical arguments one can show that there is a (1/3)-net over all symmetric matrices X with \u2016X\u2016F = 1 respecting S of size at most 9O(min(d,k 2)). By Fact 3.3 and a basic net argument, we know that for any \u03b4\u2032, we know that except with probability 1 \u2212 \u03b4\u2032, if we take n = O ( min(d,k2)+log 1/\u03b4\u2032\n\u03b52\n) samples, then for all symmetricX respectingS so that \u2016X\u2016F \u2264 1, we have |tr((\u03a3\u0302\u2212I)X)| \u2264 \u03b5.\nThe claim then follows by further union bounding over allO (( d2\nk2\n)) symmetric sparsity patterns S with |S| \u2264 k2.\nWe will also require the following structural lemma.\nLemma 4.4. Any PSD matrixX so that tr(X) = 1 and \u2016X\u20161 \u2264 k can be written as\nX =\nO(n2/k2)\u2211\ni=1\nYi ,\nwhere each Yi is symmetric, have \u2211O(n2/k2) i=1 \u2016Yi\u2016F \u2264 4, and each Yi is k2-sparse.\nProof. Observe that since X is PSD, then \u2016X\u2016F \u2264 tr(X) = 1. For simplicity of exposition, let us ignore that the Yi must be symmetric for this proof. We will briefly mention how to in addition ensure that the Yi are symmetric at the end of the proof. Sort the entries of X in order of decreasing |Xij |. Let Yi be the matrix whose nonzeroes are the ik2 + 1 through (i + 1)k2 largest entries of X , in the same positions as they appear in X . Then we clearly have that \u2211 Yi = Xi, and each Yi is exactly k 2-sparse.1 Thus it suffices to show that \u2211 \u2016Yi\u2016F \u2264 4. We have \u2016Y1\u2016F \u2264 \u2016X\u2016F \u2264 1. Additionally, we have \u2016Yi+1\u2016F \u2264 1 T |Yi|1 k , which follows simply because every nonzero entry of Yi+1 is at most the smallest entry of Yi, and each has exactly k 2 nonzeros (except potentially the last one, but it is not hard to see this cannot affect anything). Thus, in aggregate we have\nO(n2/k2)\u2211\ni=1\n\u2016Yi\u2016F \u2264 1 + O(n2/k2)\u2211\ni=2\n\u2016Yi\u2016F \u2264 1 + O(n2/k2)\u2211\ni=1\n1T |Yi|1 k = 1+ 1T |X |1 k \u2264 2 ,\nwhich is stronger than claimed.\n1Technically the last Yi may not be k 2 sparse but this is easily dealt with, and we will ignore this case here\nHowever, as written it is not clear that the Yi\u2019s must be symmetric, and indeed they do not have to be. The only real condition we needed was that the Yi\u2019s (1) had disjoint support, (2) summed to X , (3) are each \u0398(k\n2) sparse (except potentially the last one), and (4) the largest entry of Yi+1 is bounded by the smallest entry of Yi. It should be clear that this can be done while respecting symmetry by doubling the number of Yi, which also at most doubles the bound in the sum of the Frobenius norms. We omit the details for simplicity.\nProof of Theorem 4.2. Let us condition on the event that (4) holds. We claim then that for all X \u2208 X , we must have |tr((\u03a3\u0302\u2212 I)X)| \u2264 O(\u03b5), as claimed. Indeed, by Lemma 4.4, for allX \u2208 X , we have that\nX =\nO(d2/k2)\u2211\ni=1\nYi ,\nwhere each Yi is symmetric, have \u2211O(d2/k2) i=1 \u2016Yi\u2016F \u2264 4, and each Yi is k2-sparse. Thus,\n|tr((\u03a3\u0302\u2212 I)X)| \u2264 O(d2/k2)\u2211\ni=1\n\u2223\u2223\u2223tr((\u03a3\u0302\u2212 I)Yi) \u2223\u2223\u2223\n=\nO(d2/k2)\u2211\ni=1\n\u2016Yi\u2016F \u2223\u2223\u2223\u2223tr ( (\u03a3\u0302\u2212 I) Yi\u2016Yi\u2016F )\u2223\u2223\u2223\u2223\n(a) \u2264 O(d2/k2)\u2211\ni=1\n\u2016Yi\u2016F \u00b7O(\u03b5)\n(b) \u2264 O(\u03b5) ,\nwhere (a) follows since each Yi/\u2016Yi\u2016F satisfies the conditions in (4), and (b) follows from the bound on the sum of the Frobenius norms of the Yi."}, {"heading": "4.2 Concentration for Sn,\u03b5", "text": "We will require the following concentration inequalities for weighted sums of Gaussians, where the weights come from Sn,\u03b5, as these objects will naturally arise in our algorithms. These bounds follow by applying the above bounds, then carefully union bounding over all choices of possible subsets of ( n \u03b5n ) subsets. We need to be careful here since the number of things we are union bounding over increases as n increases. We include the proofs in Appendix C. Theorem 4.5. Fix \u03b5 \u2264 1/2 and \u03b4 \u2264 1, and fix k \u2264 d. There is a \u03b71 = O(\u03b5 \u221a log 1/\u03b5) so that for any \u03b7 > \u03b71, if\nX1, . . . , Xn \u223c N (0, I) and n = \u2126 ( min(d,k2)+log (d 2 k2)+log 1/\u03b4 \u03b72 ) , then\nPr  \u2203w \u2208 Sn,\u03b5 : \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nwiXi \u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2265 \u03b7   \u2264 \u03b4 .\nTheorem 4.6. Fix \u03b5 \u2264 1/2 and \u03b4 \u2264 1, and fix k \u2264 d. There is a \u03b7 = O(\u03b5 \u221a\nlog 1/\u03b5) so that ifX1, . . . , Xn \u223c N (0, I) and n = \u2126 ( min(d,k2)+log (d 2\nk2)+log 1/\u03b4 \u03b72\n) , then we have\nPr  \u2203w \u2208 Sn,\u03b5 : \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nwiXiX T i \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n\u2265 \u03b7   \u2264 \u03b4 ."}, {"heading": "5 A robust algorithm for robust sparse mean estimation", "text": "This section is dedicated to the description of an algorithm RECOVERROBUSTSMEAN for robustly learning Gaussian sequence models, and the proof of the following theorem: Theorem 5.1. Fix \u03b5, \u03c4 > 0. Let \u03b7 = O(\u03b5 \u221a\nlog 1/\u03b5). Given an \u03b5-corrupted set of samples of size n from N (\u00b5, I), where \u00b5 is k-sparse\nn = \u2126\n( min(k2, d) + log ( d2\nk2\n) + log 1/\u03c4\n\u03b72\n) ,\nthen RECOVERROBUSTSMEAN outputs a \u00b5\u0302 so that with probability 1\u2212 \u03c4, we have \u2016\u00b5\u0302\u2212 \u00b5\u20162 \u2264 O(\u03b7).\nOur algorithm builds upon the convex programming framework developed in [DKK+16]. Roughly speaking, the algorithm proceeds as follows. First, it does a simple naive pruning step to remove all points which are more than\nroughly \u2126( \u221a d) away from the mean. Then, for an appropriate choice of \u03b4, it will attempt to (approximately) find a point within the following convex set:\nRobustSMeanC\u03c4 =   w \u2208 Sn,\u03b5 : \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwi(Xi \u2212 \u00b5)(Xi \u2212 \u00b5)T \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n\u2264 \u03c4    . (5)\nThe main difficulty with finding a point in C\u03c4 is that \u00b5 is unknown. A key insight of [DKK +16] is that it suffices to create an (approximate) separation oracle for the feasible set, as then we may use classical convex optimization algorithms (i.e. ellipsoid or cutting plane methods) to find a feasible point. In their setting (for a differentC\u03c4 ), it turns out that a simple spectral algorithm suffices to give such a separation oracle.\nOur main contribution is the design of separation oracle for C\u03c4 , which requires more sophisticated techniques. In particular, we will ideas developed in analogy to hard thresholding and SDPs similar to those developed for sparse PCA to design such an oracle."}, {"heading": "5.1 Additional preliminaries", "text": "Throughout this section, we let X1, . . . , Xn denote an \u03b5-corrupted set of samples fromN (\u00b5, I), where \u00b5 is k-sparse. We let Sgood denote the set of uncorrupted samples, and we let Sbad denote the set of corrupted samples. For any set of weights w \u2208 Sn,\u03b5, we let wg = \u2211 i\u2208Sgood wi and w b = \u2211\ni\u2208Sbad wi. Throughout this section, we will condition on the following three deterministic events occurring:\nNAIVEPRUNE(X1, . . . , Xn, \u03b4) succeeds, (6)\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208Sgood wi(Xi \u2212 \u00b5) \u2225\u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2264 \u03b7 , \u2200w \u2208 Sn,2\u03b5 , and (7)\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i\u2208Sgood wi(Xi \u2212 \u00b5)(Xi \u2212 \u00b5)T \u2212 wgI \u2225\u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n\u2264 \u03b7 , \u2200w \u2208 Sn,2\u03b5 , (8)\nwhere \u03b7 = O(\u03b5 \u221a log 1/\u03b5). When n = \u2126\n( min(k2,d)+log (k 2\nd2)+log 1/\u03b4 \u03b72\n) these events simultaneously happen with\nprobability at least 1 \u2212 O(\u03b4) by Fact 3.1, Theorem 4.5, Theorem 4.6 and a union bound, and the observation that if w \u2208 Sn,\u03b5, then w/wg restricted to the indices in Sgood is in S(1\u2212\u03b5)n,2\u03b5."}, {"heading": "5.2 The separation oracle", "text": "Our main result in this section is the description of a polynomial time algorithm ROBUSTSMEANORACLE and the proof of the following theorem of its correctness:\nTheorem 5.2. Fix \u03b5 > 0 sufficiently small. Suppose that (7) and (8) hold. Let w\u2217 denote the set of weights which are uniform over the uncorrupted points. Then, there is a constant 1 \u2264 c \u2264 21 so that ROBUSTSMEANORACLE satisfies:\n1. (Completeness) If w = w\u2217, ROBUSTSMEANORACLE outputs \u201cYES\u201d.\n2. (Soundness) If w 6\u2208 Cc\u03b7 the algorithm outputs a hyperplane \u2113 : Rn \u2192 R so that \u2113(w) \u2265 0 but \u2113(w\u2217) < 0. Moreover, if the algorithm ever outputs a hyperplane, we have \u2113(w\u2217) < 0.\nPlugging these guarantees into an ellipsoid (or cutting-plane) method, we obtain the following:\nCorollary 5.3. Fix \u03b5 > 0 sufficiently small. Suppose that (7) and (8) hold. There is an algorithm APPROXRECOVERROBUSTSMEAN which queries ROBUSTSMEANORACLE at most poly(d, 1/\u03b5, log 1/\u03b4) times, and so runs in time poly(d, 1/\u03b5, 1/\u03b4) which outputs a w\u2032 so that \u2016w \u2212 w\u2032\u2016\u221e \u2264 \u03b5/(n \u221a d logn/\u03b4), for some w \u2208 Cc\u03c4 .\nOur separation oracle, formally described in Algorithm 1, proceeds as follows. Given w \u2208 Sn,\u03b5, it forms \u00b5\u0302 = \u2016\u00b5\u0302\u2032\u2016\u2217Uk \u00b7 dUk(\u00b5\u0302\u2032), where \u00b5\u0302 = \u2211 wiXi. It then forms the matrix \u03a3\u0302 = \u2211 wi(Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T , and computes\nA = dXk(\u03a3\u0302). The algorithm then checks if \u2223\u2223\u2223\u3008A, \u03a3\u0302\u3009 \u2223\u2223\u2223 > C for appropriately chosen threshold C. If it does not, the algorithm outputs \u201cYES\u201d. Otherwise, the algorithm outputs a separating hyperplane given by this matrix A.\nAlgorithm 1 Separation oracle for robust sparse mean estimation.\n1: function ROBUSTSMEANORACLE(X1, . . . , Xn, w) 2: Let \u00b5\u0302 = \u2211 wiXi 3: Let \u03a3\u0302 = \u2211\nwi(Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T 4: Let A = dXk(\u03a3\u0302) 5: if |\u3008A, \u03a3\u0302\u2212 I\u3009| \u2265 20\u03b7 then 6: Let \u03c3 = sgn ( \u3008A, \u03a3\u0302\u2212 I\u3009 )\n7: return the hyperplane \u2113 given by\n\u2113(w) = \u03c3\n( n\u2211\ni=1\nwi \u2329 A, (Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T \u232a \u2212 1 ) \u2212 |\u3008A, \u03a3\u0302\u2212 I\u3009| .\n8: else 9: return \u201cYES\u201d\n10: end 11: end function\nWe will require the following two lemmata:\nLemma 5.4. Let \u03c91, . . . , \u03c9m be a set of non-negative weights that sum to 1. Let a1, . . . , am be any sequence of scalars. Then\nm\u2211\ni=1\n\u03c9ia 2 i \u2265\n( m\u2211\ni=1\n\u03c9iai\n)2 .\nProof. Let Z be a random variable which is ai with probability \u03c9i. Then E[Z] = \u2211 \u03c9iai and E[Z 2] = \u2211 \u03c9ia 2 i . Then the inequality follows from the fact that E[Z2]\u2212 E[Z2] = Var[Z] \u2265 0.\nLemma 5.5. Let u \u2208 Rd. Then (\u2016u\u2016\u2217Uk)2 \u2264 \u2016uuT\u2016\u2217Xk \u2264 4(\u2016u\u2016\u2217Uk)2.\nProof. Let v = dUk(u). Then since vv T \u2208 Xk, we have that (\u2016uuT\u2016\u2217Xk) \u2265 \u3008vvT , uuT \u3009 = \u3008u, v\u30092 = (\u2016u\u2016\u2217Uk)2. This proves the first inequality.\nTo prove the other inequality, we first prove the intermediate claim that supM\u2208Y k2 uTMu \u2264 (\u2016u\u2016\u2217Uk)2, where Yk2 is the set of symmetric matrices M with at most k2-non-zeroes satisfying \u2016M\u2016F = 1. Indeed, fix any M \u2208 Yk . Let S \u2286 [n] be the set of non-zeroes of dUk(u). This is exactly the set of the k largest elements in u, sorted by absolute value. LetP be the symmetric sparsity pattern respected byM . Fix an arbitrary bijection \u03c6 : P \\(S\u00d7S) \u2192 (S\u00d7S)\\P , and letM \u2032 be the following matrix:\nM \u2032i,j =    Mij if (i, j) \u2208 P \u22c2 (S \u00d7 S) ,\nsgn (uiuj)M\u03c6\u22121(i,j) if (i, j) \u2208 (S \u00d7 S) \\ P , 0 otherwise.\nThen we claim that uTMu \u2264 uTM \u2032u. Indeed, we have uTM \u2032u\u2212 uTMu = \u2211\n(i,j)\u2208P\\(S\u00d7S) |Mij(uuT )\u03c6(i,j)| \u2212Mij(uuT )i,j\n\u2265 \u2211\n(i,j)\u2208P\\(S\u00d7S) |Mi,j |\n( |(uuT )\u03c6(i,j)| \u2212 |(uuT )i,j | ) \u2265 0 ,\nfrom the definition of S. Moreover, for anyM respecting S \u00d7 S with \u2016M\u2016F = 1, it is not hard to see that uTMu \u2264 (\u2016u\u2016\u2217Uk)2. This is because now the problem is equivalent to restricting our attention to the coordinates in S, and asking for the symmetric matrix M \u2208 RS\u00d7S with \u2016M\u2016F = 1 maximizing uTSMuS , where uS is u restricted to the coordinates in S. This is clearly maximized by M = 1\u2016uS\u201622 uSu T S , which yields the desired expression, since \u2016uS\u20162 = \u2016u\u2016Uk . We can now prove the original lemma. By Lemma 4.4 we may write A = \u2211O(n2/k2) i=1 Yi where each Yi is symmetric, k2-sparse, and have \u2211O(n2/k2)\ni=1 \u2016Yi\u2016F \u2264 4. We therefore have\nuTAu =\nO(n2/k2)\u2211\ni=1\nuTYiu\n=\nO(n2/k2)\u2211\ni=1\n\u2016Yi\u2016F (\u2016u\u2016\u2217Uk)2\n\u2264 4(\u2016u\u2016\u2217Uk)2 , as claimed, where the second line follows from the arguments above.\nThroughout the rest of this section, let Yi = Xi \u2212 \u00b5, so that so that Yi \u223c N (0, I) if i \u2208 Sgood. We first prove the following crucial proposition: Proposition 5.6. Let w \u2208 Sn,\u03b5, and let \u03c4 \u2265 \u03b7. Assuming (7) and (8) hold, if \u2016 \u2211n\ni=1 wiYi\u2016 \u2217 Uk \u2265 3\u03c4 , then\u2225\u2225\u2211n\ni=1 wiYiY T i \u2212 I \u2225\u2225\u2217 Xk \u2265 \u03c42 \u03b5 .\nProof. Observe that (7) and a triangle inequality together imply that \u2225\u2225\u2211 i\u2208Sbad wiYi \u2225\u2225\u2217 Uk\n\u2265 2\u03c4 . By definition, this implies there is a k-sparse unit vector u so that\n\u2223\u2223\u3008u,\u2211i\u2208Sbad wiYi\u3009 \u2223\u2223 \u2265 2\u03c4 . WLOG assume that \u3008u,\u2211i\u2208Sbad wiYi\u3009 \u2265 \u03b7\n(if the sign is negative a symmetric argument suffices). This is equivalent to the statement that\n\u2211\ni\u2208Sbad\nwi wb \u3008u, Yi\u3009 \u2265 2\u03c4 wb .\nObserve that the wi/w b are a set of non-negative weights summing to 1. Hence, by Lemma 5.4, we have\n\u2211\ni\u2208Sbad\nwi wb\n\u3008u, Yi\u30092 \u2265 ( 2\u03c4\nwb\n)2 .\nLet A = uuT . Observe that A \u2208 Xk. Then the above inequality is equivalent to the statement that \u2211\ni\u2208Sbad wiY\nT i AYi \u2265\n\u03c42\nwb \u2265 4\u03c4\n2\n\u03b5 .\nMoreover, by (8), we have\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208Sgood wiY T i AYi \u2212 I \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b7 ,\nand together these two inequalities imply that\nn\u2211\ni=1\nwiYiAYi \u2265 4\u03c42 \u03b5 \u2212 \u03b7 \u2265 \u03c4 2 \u03b5 ,\nas claimed. The final inequality follows from the definition of \u03b7, and since 4 > 2.\nProof of Theorem 5.2. Completeness follows from (8). We will now show soundness. Suppose w 6\u2208 C21\u03b7 . We wish to show that we will output a separating hyperplane. From the description of the algorithm, this is equivalent to showing that \u2016\u03a3\u0302\u2212 I\u2016Xk \u2265 20\u03b7. Let \u00b5\u0302 = \u2211n\ni=1 wiXi, and let \u2206 = \u00b5\u2212 \u00b5\u0302. By elementary manipulations, we may write \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1 wi(Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk = \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 wi(Yi +\u2206)(Yi +\u2206) T \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk\n(a) = \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiYiY T i +\u2206\u2206 T \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk\n(b) \u2265 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiYiY T i \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk \u2212 \u2225\u2225\u2206\u2206T \u2225\u2225 Xk\n(c) \u2265 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiYiY T i \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk \u2212 4 \u2016\u2206\u20162Uk ,\nwhere (a) follows since \u2211n\ni=1 wiYi = \u2206 by definition, (b) follows from a triangle inequality, and (c) follows from\nLemma 5.5. If \u2016\u2206\u2016Uk \u2264 \u221a \u03b7/2, then the RHS is at least 21\u03b7 since the second term is at most \u03b7, and the first term\nis at least 21\u03b7 since we assume that w 6\u2208 C21\u03b7 . Conversely, if \u2016\u2206\u2016Uk \u2265 \u221a \u03b7/2, then by Proposition 5.6, we have\n\u2016\u2211ni=1 wiYiYi \u2212 I\u2016Xk \u2265 \u2016\u2206\u20162Xk/(6\u03b5) > 48\u2016\u2206\u20162Xk as long as \u03b5 \u2264 1/288. This implies that the RHS is at least 40\u2016\u2206\u2016X 2 k \u2265 20\u03b7, as claimed.\nHence, this implies that if w 6\u2208 C4\u03b7, then we output a hyperplane \u2113. It is clear by construction that \u2113(w) \u2265 0; thus, it suffices to show that if we output a hyperplane, that \u2113(w\u2217) < 0. Letting \u00b5\u0303 = 1(1\u2212\u03b5)n \u2211 i\u2208Sgood wiYi, we have Observe that we have\nn\u2211\ni=1\nw\u2217i (Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T \u2212 I = 1 (1\u2212 \u03b5)n \u2211\ni\u2208Sgood (Yi +\u2206)(Yi +\u2206)\nT \u2212 I\n= 1\n(1\u2212 \u03b5)n\n  \u2211\ni\u2208Sgood YiY\nT i \u2212 I\n +\u2206\u00b5\u0303T + \u00b5\u0303\u2206T +\u2206\u2206T\n= 1\n(1\u2212 \u03b5)n\n  \u2211\ni\u2208Sgood YiY\nT i \u2212 I\n + (\u2206 + \u00b5\u0303)(\u2206 + \u00b5\u0303)T \u2212 \u00b5\u0303\u00b5\u0303T .\nHence by the triangle inequality and Lemma 5.5, we have\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1 w\u2217i (Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T \u2212 I \u2225\u2225\u2225\u2225\u2225 Xk \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 1 1(1\u2212 \u03b5)n \u2211 i\u2208Sgood YiY T i \u2212 I \u2225\u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n+ 4 ( \u2016\u2206+ \u00b5\u0303\u2016\u2217Uk )2 + 4 ( \u2016\u00b5\u0303\u2016\u2217Uk )2\n\u2264 \u2225\u2225\u2225\u2225\u2225\u2225 1 1(1\u2212 \u03b5)n \u2211 i\u2208Sgood YiY T i \u2212 I \u2225\u2225\u2225\u2225\u2225\u2225 Xk + 8 ( \u2016\u2206\u2016\u2217Uk )2\n+ 8 ( \u2016\u00b5\u0303\u2016\u2217Uk )2 + 4 ( \u2016\u00b5\u0303\u2016\u2217Uk )2\n\u2264 13\u03b7 + 8 ( \u2016\u2206\u2016\u2217Uk )2 , (9)\nby (7) and (8).\nObserve that to show that \u2113(w\u2217) < 0 it suffices to show that \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nw\u2217i (Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)\u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n< \u2225\u2225\u2225\u03a3\u0302\u2212 I \u2225\u2225\u2225 \u2217\nXk . (10)\nIf \u2016\u2206\u2016\u2217Uk \u2264 \u221a \u03b7/2, then this follows since the quantity on the RHS is at least 20\u03b7 by assumption, and the quantity on\nthe LHS is at most 17\u03b7 by (9). If \u2016\u2206\u2016\u2217Uk \u2265 \u221a \u03b7/2, then by Proposition 5.6, the RHS of (10) is at least ( \u2016\u2206\u2016\u2217Uk )2 /(3\u03b5), which dominates the LHS as long as \u2016\u2206\u2016\u2217Uk \u2265 \u03b7 and \u03b5 \u2264 1/288, which completes the proof."}, {"heading": "5.3 Putting it all together", "text": "We now have the ingredients to prove our main theorem. Given what we have, our full algorithm RECOVERROBUSTSMEAN is straightforward: first run NAIVEPRUNE, then run APPROXRECOVERROBUSTSMEAN on the pruned points to output some set of weightsw. We then output \u2016\u00b5\u0302\u2016UkdUk(\u00b5\u0302). The algorithm is formally defined in Algorithm 2.\nAlgorithm 2 An efficient algorithm for robust sparse mean estimation\n1: function RECOVERROBUSTSMEAN(X1, . . . , Xn, \u03b5, \u03b4) 2: Let S be the set output by NAIVEPRUNE(X1, . . . , Xn, \u03b4). WLOG assume S = [n]. 3: Let w\u2032 = APPROXRECOVERROBUSTSMEAN(X1, . . . , Xn, \u03b5, \u03b4). 4: Let \u00b5\u0302 = \u2211n i=1 w \u2032 iXi. 5: return \u2016\u00b5\u0302\u2016\u2217UkdUk(\u00b5\u0302) 6: end function\nProof of Theorem 5.1. Let us condition on the event that (6), (7), and (8) all hold simultaneously. As previously\nmentioned, when n = \u2126\n( min(k2,d)+log (k 2\nd2)+log 1/\u03b4 \u03b72\n) these events simultaneously happen with probability at least\n1 \u2212 O(\u03b4). For simplicity of exposition, let us assume that NAIVEPRUNE does not remove any points. This is okay since if it succeeds, it never removes any good points, so if it removes any points, it can only help us. Moreover, since\nit succeeds, we know that \u2016Xi\u2212\u00b5\u20162 \u2264 O( \u221a\nd log(n/\u03b4)) for all i \u2208 [n]. By Corollary 5.3, we know that there is some w \u2208 C21\u03b7 so that \u2016w \u2212 w\u2032\u2016\u221e \u2264 \u03b5/(n \u221a d logn/\u03b4). We have\n\u2016\u00b5\u0302\u2212 \u00b5\u2016Uk = \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nw\u2032iXi \u2212 \u00b5\u0302 \u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiXi \u2212 \u00b5\u0302 \u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n+\nn\u2211\ni=1\n|wi \u2212 w\u2032i| \u2016Xi \u2212 \u00b5\u20162\n\u2264 O(\u03b7) +O(\u03b5) ,\nby Proposition 5.6. We now show that this implies that if we let \u00b5\u2032 = \u2016\u00b5\u0302\u2016\u2217UkdUk(\u00b5\u0302), then \u2016\u00b5\u2032 \u2212 \u00b5\u20162 \u2264 O(\u03b7). Let S be the support of \u00b5\u2032, and let T be the support of \u00b5. Then we have\n\u2016\u00b5\u2032 \u2212 \u00b5\u201622 = \u2211\ni\u2208S\u2229T (\u00b5\u2032i \u2212 \u00b5i)2 +\n\u2211\ni\u2208S\\T (\u00b5\u2032i)\n2 + \u2211\ni\u2208T\\S \u00b52i .\nObserve that \u2211\ni\u2208S\u2229T (\u00b5 \u2032 i \u2212 \u00b5i)2 + \u2211 i\u2208S\\T (\u00b5 \u2032 i) 2 \u2264 ( \u2016\u00b5\u0302\u2212 \u00b5\u2016\u2217Uk )2 , since \u00b5 was originally nonzero on the entries in\nS \\ T . Moreover, for all i \u2208 T \\ S and j \u2208 S \\ T , we have (\u00b5\u2032i)2 \u2264 (\u00b5\u2032j)2. Thus we have\n\u2211\ni\u2208T\\S \u00b52i \u2264 2\n  \u2211\ni\u2208T\\S (\u00b5\u2212 \u00b5\u2032i)2 +\n\u2211\ni\u2208S\\T (\u00b5\u2032j) 2\n  \u2264 2 ( \u2016\u00b5\u0302\u2212 \u00b5\u2016\u2217Uk )2 .\nTherefore we have \u2016\u00b5\u2032 \u2212 \u00b5\u201622 \u2264 3 ( \u2016\u00b5\u0302\u2212 \u00b5\u2016\u2217Uk )2 , which implies that \u2016\u00b5\u2032 \u2212 \u00b5\u20162 \u2264 O(\u03b7), as claimed."}, {"heading": "6 An algorithm for robust sparse PCA detection", "text": "In this section, we give an efficient algorithm for detecting a spiked covariance matrix in the presence of adversarial noise. Our algorithm is fairly straightforward: we ask for the set of weights w \u2208 Sn,\u03b5 so that the empirical second moment with these weights has minimal deviation from the identity in the dual Xk norm. We may write this as a convex program. Then, we check the value of the optimal solution of this convex program. If this value is small, then we say it is N (0, I). if this value is large, then we say it is N (0, I + \u03c1vvT ). We refer to the former as Case 1 and the latter as Case 2. The formal description of this algorithm is given in Algorithm.\nAlgorithm 3 Learning a spiked covariance model, robustly\n1: function DETECTROBUSTSPCA(X1, . . . , Xn, \u03b5, \u03b4, \u03c1) 2: Let \u03b3 be the value of the solution\nmin w\u2208Sn,\u03b5 \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwi(XiX T i \u2212 I) \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n(11)\n3: if \u03b3 < \u03c1/2 then return Case 1 else return Case 2 4: end function"}, {"heading": "6.1 Implementing DETECTROBUSTSPCA", "text": "We first show that the algorithm presented above can be efficiently implemented. Indeed, one can show that by taking the dual of the SDP defining the \u2016 \u00b7 \u2016\u2217Xk norm, this problem can be re-written as an SDP with (up to constant factor blowups) the same number of constraints and variables, and therefore we may solve it using traditional SDP solver techniques.\nAlternatively, one may observe that to optimize Algorithm 4 via ellipsoid or cutting plane methods, it suffices to, given w \u2208 Sn,\u03b5, produce a separating hyperplane for the constraint (11). This is precisely what dual norm maximization allows us to do efficiently. It is straightforward to show that the volume of Sn,\u03b5 \u00d7Xk is at most exponential in the relevant parameters. Therefore, by the classical theory of convex optimization, (see e.g. [CITE]), for any \u03be, we may find a solution w\u2032 and \u03b3\u2032 so that \u2016w\u2032 \u2212 w\u2217\u2016\u221e \u2264 \u03be and \u03b3\u2032 so that |\u03b3 \u2212 \u03b3\u2032| < \u03be for some exact minimizer w\u2217, where \u03b3 is the true value of the solution, in time poly(d, n, 1/\u03b5, log 1/\u03be),\nAs mentioned in Section B.2, neither approach will in general give exact solutions, however, both can achieve inverse polynomial accuracy in the parameters in polynomial time. We will ignore these issues of numerical precision throughout the remainder of this section, and assume we work with exact \u03b3.\nObserve that in general it may be problematic that we don\u2019t have exact access to the minimizer w\u2217, since some of theXi may be unboundedly large (in particular, if it\u2019s corrupted) in norm. However, we only use information about \u03b3. Since \u03b3 lives within a bounded range, and our analysis is robust to small changes to \u03b3, these numerical issues do not change anything in the analysis."}, {"heading": "6.2 Proof of Theorem 2.2", "text": "We now show that Algorithm 4 provides the guarantees required for Theorem 2.2. We first show that if we are in Case 1, then \u03b3 is small:\nLemma 6.1. Let \u03c1, \u03b4 > 0. Let \u03b5, \u03b7 be as in Theorem 2.2. Let X1, . . . , Xn be an \u03b5-corrupted set of samples from N (0, I) of size n, where n is as in Theorem 2.2. Then, with probability 1\u2212 \u03b4, we have \u03b3 \u2264 \u03c1/2. Proof. Letw be the uniformweights over the uncorrupted points. Then it from Theorem4.2 that \u2016\u2211w \u2211n i=1 wi(XiX T i \u2212 I)\u2016\u2217Xk \u2264 O(\u03b7) with probability 1\u2212 \u03b4. Since w \u2208 Sn,\u03b5, this immediately implies that \u03b3 \u2264 O(\u03c1). By setting constants appropriately, we obtain the desired guarantee.\nWe now show that if we are in Case 2, then \u03b3 must be large:\nLemma 6.2. Let \u03c1, \u03b4 > 0. Let \u03b5, \u03b7, n be as in Theorem 2.2. Let X1, . . . , Xn be an \u03b5-corrupted set of samples from N (0, I) of size n. Then, with probability 1 \u2212 \u03b4, we have \u03b3 \u2265 (1 \u2212 \u03b5)\u03c1 \u2212 (2 + \u03c1)\u03b7. In particular, for \u03b5 sufficiently small, and \u03b7 = O(\u03c1), we have that \u03b3 > \u03c1/2.\nProof. Let \u03a3 = I + \u03c1vvT , and let Yi = \u03a3 \u22121/2Xi, so that if Yi is uncorrupted, then Yi \u223c N (0, I). Let w\u2217 be\nthe optimal solution to (11). By Theorem 4.6, we have that with probability 1 \u2212 \u03b4, we can write \u2211ni=1 w\u2217i YiY Ti = wg(I + N) + B, where \u2016N\u2016\u2217Xk \u2264 \u03b7, and B = \u2211 i\u2208Sbad w \u2217 i YiY T i . Therefore, we have \u2211n i=1 w \u2217XiXTi = w g(\u03a3 + \u03a31/2N\u03a31/2) + \u03a31/2B\u03a31/2 . By definition, we have\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwi(XiX T i \u2212 I) \u2225\u2225\u2225\u2225\u2225 \u2217\nXk\n\u2265 \u3008wg(\u03a3 + \u03a31/2N\u03a31/2) + \u03a31/2B\u03a31/2 \u2212 I, vvT \u3009\n\u2265 wg\u3008(\u03a3 + \u03a31/2N\u03a31/2), vvT \u3009 \u2212 1 = wg(1 + \u03c1) + wgvT\u03a31/2N\u03a31/2v \u2212 1 \u2265 (1\u2212 \u03b5)\u03c1+ (1\u2212 \u03b5)vT\u03a31/2N\u03a31/2v \u2212 \u03b5 .\nIt thus suffices to show that |vT\u03a31/2N\u03a31/2v| < (1 + \u03c1)\u03b7. Since v is an eigenvector for \u03a3 with eigenvalue 1 + \u03c1, we have that \u03a31/2v = \u221a \u03c1+ 1 \u00b7 v and thus\nvT\u03a31/2N\u03a31/2v = (1 + \u03c1)vTNv = (1 + \u03c1)\u3008N, vvT \u3009 \u2264 (1 + \u03c1)\u2016N\u2016\u2217Xk \u2264 (1 + \u03c1)\u03b7 .\nLemmas 6.1 and 6.2 together imply the correctness of DETECTROBUSTSPCA and Theorem 2.2."}, {"heading": "7 An algorithm for robust sparse PCA recovery", "text": "In this section, we prove Theorem 2.3. We give some intuition here. Perhaps the first naive try would be to simply run the same SDP in (11), and hope that the dual norm maximizer gives you enough information to recover the hidden spike. This would more or less correspond to the simplest modification SDP of the sparse PCA in the nonrobust setting that one could hope gives non-trivial information in this setting. However, this cannot work, for the following straightforward reason: the value of the SDP is always at least O(\u03c1), as we argued in Section 6. Therefore, the noise can pretend to be some other sparse vector u orthogonal to v, so that the covariance with noise looks like wg(I + \u03c1vvT ) + wg\u03c1uuT , so that the value of the SDP can be minimized with the uniform set of weights. Then it is\neasily verified that both vvT and uuT are dual norm maximizers, and so the dual norm maximizer does not uniquely determine v.\nTo circumvent this, we simply add an additional slack variable to the SDP, which is an additional matrix in Xk, which we use to try to maximally explain away the rank-one part of I + \u03c1vvT . This forces the value of the SDP to be very small, which allows us to show that the slack variable actually captures v."}, {"heading": "7.1 The algorithm", "text": "Our algorithms and analyses will make crucial use of the following convex set, which is a further relaxation of Xk:\nW(2)k = { X \u2208 Rd\u00d7d : tr(X) \u2264 2, \u2016X\u20162 \u2264 1, \u2016X\u20161 \u2264 3k,X 0 } .\nOur algorithm, given formally in Algorithm 4, will be the following. We solve a convex program which simultaneously chooses a weights in Sn,\u03b5 and a matrix A \u2208 Wk to minimize theWk distance between the sample covariance with these weights, and A. Our output is then just the top eigenvector of A.\nAlgorithm 4 Learning a spiked covariance model, robustly\n1: function RECOVERROBUSTSPCA(X1 , . . . , Xn, \u03b5, \u03b4, \u03c1) 2: Let w\u2217, A\u2217 be the solution to\nargmin w\u2208Sn,\u03b5,A\u2208Xk\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwi(XiX T i \u2212 I)\u2212 \u03c1A \u2225\u2225\u2225\u2225\u2225 \u2217\nW2k\n(12)\n3: Let u be the top eigenector of A\u2217 4: return The dUk(u)\u2016u\u2016\u2217Uk , i.e., the vector with all but the top k coordinates of v zeroed out. 5: end function\nThis algorithm can be run efficiently for the same reasons as explained for DETECTROBUSTSPCA. For the rest of the section we will assume that we have an exact solution for this problem. As before, we only use information about A, and since A comes from a bounded space, and our analysis is robust to small perturbations in A, this does not change anything."}, {"heading": "7.2 More concentration bounds", "text": "Before we can prove correctness of our algorithm, we require a couple of concentration inequalities for the set Wk.\nLemma 7.1. Fix \u03b5, \u03b4 > 0. Let X1, . . . , Xn \u223c N (0, I), where n is as in Theorem 4.2. Then with probability 1\u2212 \u03b4 \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nXiX T i \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nWk\n\u2264 O(\u03b5) .\nProof. Let \u03a3\u0302 denote the empirical covariance. Observe thatWk \u2286 \u22c3\u221e i=0 2 \u2212iX2i+1k. Moreover, for any i, by Theorem 4.2, if we take\nn = \u2126\n min(d, (2 i+1k)2) + log ( d2 (2i+1k)2 ) + log 1/\u03b4\n(2\u2212i\u03b5)2\n \n= \u2126\n( min(d, k2) + log ( d2\nk2\n) + 22i log 1/\u03b4\n\u03b52\n) ,\nthen |\u3008M, \u03a3\u0302\u3009| \u2264 \u03b5 for all M \u2208 2\u2212iX2i+1k with probability 1\u2212 \u03b4/2. In particular, if we take\nn = \u2126\n( min(d, k2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b52\n)\nsamples, then for any i, we have |\u3008M, \u03a3\u0302\u3009| \u2264 \u03b5 for allM \u2208 2\u22121X2i+1k with probability at least 1\u2212 \u03b42 2i /2. By a union bound over all these events, since \u2211\u221e\ni=0 \u03b4 22i \u2264 2\u03b4, we conclude that if we take n to be as above, then |\u3008M, \u03a3\u0302\u3009| \u2264 \u03b5 for\nallM \u2208 \u22c3\u221ei=0 2\u2212iX2i+1k with probability 1\u2212\u03b4. SinceWk is contained in this set, this implies that \u2016\u03a3\u0302\u2212\u03a3\u2016\u2217Wk \u2264 O(\u03b5) with probability at least 1\u2212 \u03b4, as claimed.\nBy the same techniques as in the proofs of Theorems 4.5 and 4.6, we can show the following bound. Because of\nthis, we omit the proof for conciseness.\nCorollary 7.2. Fix \u03b5, \u03b4 > 0. Let X1, . . . , Xn \u223c N (0, I) where n is as in Theorem 4.6. Then there is an \u03b7 = O(\u03b5 \u221a log 1/\u03b5) so that\nPr  \u2203w \u2208 Sn,\u03b5 : \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiXiX T i \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nWk\n\u2265 \u03b7   \u2264 \u03b4 ."}, {"heading": "7.3 Proof of Theorem 2.3", "text": "In the rest of this section we will condition on the following deterministic event happening:\n\u2200w \u2208 Sn,\u03b5 : \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiXiX T i \u2212 I \u2225\u2225\u2225\u2225\u2225 \u2217\nW2k\n\u2264 \u03b7 , (13)\nwhere \u03b7 = O(\u03b5 log 1/\u03b5). By Corollary 7.2, this holds if we take\nn = \u2126\n( min(d, k2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b722\n)\nsamples.\nThe rest of this section is dedicated to the proof of the following theorem, which immediately implies Theorem\n2.3.\nTheorem 7.3. Fix \u03b5, \u03b4, and let \u03b7 be as in (13). Assume that (13) holds. Let v\u0302 be the output of RECOVERYROBUSTSPCA(X1, . . . , Xn, \u03b5, \u03b4, \u03c1). Then L(v\u0302, v) \u2264 O( \u221a (1 + \u03c1)\u03b7/\u03c1).\nOur proof proceeds in a couple of steps. Let\u03a3 = I+\u03c1vvT denote the true covariance. We first need the following, technical lemma:\nLemma 7.4. Let M \u2208 Wk. Then \u03a31/2M\u03a31/2 \u2208 (1 + \u03c1)Wk. Proof. Clearly, \u03a31/2M\u03a31/2 0. Moreover, since \u03a31/2 = I + (\u221a1 + \u03c1 \u2212 1)vvT , we have that the maximum value of any element of \u03a31/2 is upper bounded by \u221a 1 + \u03c1. Thus, we have \u2016\u03a31/2M\u03a31/2\u20161 \u2264 (1 + \u03c1)\u2016M\u20161. We also have\ntr(\u03a31/2M\u03a31/2) = tr(\u03a3M)\n= tr(M) + \u03c1vTMv \u2264 1 + \u03c1 ,\nsince \u2016M\u2016 \u2264 1. Thus \u03a31/2M\u03a31/2 \u2208 (1 + \u03c1)Wk , as claimed.\nLet w\u2217, A\u2217 be the output of our algorithm. We first claim that the value of the optimal solution is quite small:\nLemma 7.5. \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nw\u2217i (XiX T i \u2212 I)\u2212 \u03c1A\u2217 \u2225\u2225\u2225\u2225\u2225 \u2217\nW2k\n\u2264 \u03b7(1 + \u03c1) .\nProof. Indeed, if we let w be the uniform set of weights over the good points, and we let A = vvT , then by (13), we have\nn\u2211\ni=1\nwiXiX T i = \u03a3 1/2(I +N)\u03a31/2 ,\nwhere \u2016N\u2016\u2217Xk \u2264 \u03b7, and \u03a3 = I + \u03c1vvT . Thus we have that \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwi(XiX T i \u2212 I)\u2212 \u03c1vvT \u2225\u2225\u2225\u2225\u2225 \u2217\nW2k\n= \u2016\u03a31/2N\u03a31/2\u2016\u2217W2k\n= max M\u2208Wk\n\u2223\u2223\u2223tr(\u03a31/2N\u03a31/2M) \u2223\u2223\u2223\n= max M\u2208Wk\n\u2223\u2223\u2223tr(N\u03a31/2M\u03a31/2) \u2223\u2223\u2223\n\u2264 (1 + \u03c1)\u2016N\u2016\u2217W2k ,\nby Lemma 7.4.\nWe now show that this implies the following:\nLemma 7.6. vTA\u2217v \u2265 1\u2212 (2 + 3\u03c1)\u03b7/\u03c1. Proof. By (13), we know that we may write \u2211n i=1 wi(XiX T i \u2212 I) = wg\u03c1vvT + B \u2212 (1 \u2212 wg)I + N , where\nB = \u2211\ni\u2208Sbad wiXiX T i , and \u2016N\u2016\u2217Wk \u2264 (1 + \u03c1)\u03b7. Thus, by Lemma 7.5 and the triangle inequality, we have that\n\u2225\u2225wg\u03c1vvT +B \u2212 \u03c1A \u2225\u2225\u2217 Wk \u2264 \u03b7 + \u2016N\u2016 \u2217 Wk + (1 \u2212 wg)\u2016I\u2016\u2217Wk + (1\u2212 wg)\u2016\u03c1A\u2016\u2217Wk\n\u2264 (1 + \u03c1)\u03b7 + \u03b5+ \u03c1\u03b5 \u2264 (1 + 2\u03c1)\u03b7 + \u03b5 .\nNow, since vvT \u2208 Wk, the above implies that\n|wg\u03c1+ vTBv \u2212 \u03c1vTA\u2217v| \u2264 (1 + 2\u03c1)\u03b7 + \u03b5 ,\nwhich by a further triangle inequality implies that\n|\u03c1(1\u2212 vTA\u2217v) + vTBv| \u2264 (1 + 2\u03c1)\u03b7 + \u03b5+ \u03b5\u03c1 \u2264 (2 + 3\u03c1)\u03b7 .\nSince 0 \u2264 vTA\u2217v \u2264 1 (since A \u2208 Xk) and B is PSD, this implies that in fact, we have\n0 \u2264 \u03c1(1\u2212 vTA\u2217v) \u2264 (2 + 3\u03c1)\u03b7 .\nHence vTA\u2217v \u2265 1\u2212 (2 + 3\u03c1)\u03b7/\u03c1, as claimed.\nLet \u03b3 = (2 + 3\u03c1)\u03b7/\u03c1. The lemma implies that the top eigenvalue of A\u2217 is at least 1 \u2212 \u03b3. Moreover, since A\u2217 \u2208 Xk, as long as \u03b3 \u2264 1/2, this implies that the top eigenvector of A\u2217 is unique up to sign. By the constraint that \u03b7 \u2264 O(min(\u03c1, 1)), for an appropriate choice of constants, we that \u03b3 \u2264 1/10, and so this condition is satisfied. Recall that u is the top eigenvector of A\u2217. Since tr(A\u2217) = 1 and A\u2217 is PSD, we may write A\u2217 = \u03bb1uuT + A1, where u is the top eigenvector of A\u2217, \u03bb1 \u2265 1\u2212 \u03b3, and \u2016A1\u2016 \u2264 \u03b3. Thus, by the triangle inequality, this implies that\n\u2016\u03c1(vvT \u2212 \u03bb1uuT ) +B\u2016\u2217X2k \u2264 O(\u03c1\u03b3)\nwhich by a further triangle inequality implies that\n\u2016\u03c1(vvT \u2212 uuT ) +B\u2016\u2217X2k \u2264 O(\u03c1\u03b3) . (14)\nWe now show this implies the following intermediate result:\nLemma 7.7. (vTu)2 \u2265 1\u2212O(\u03b3). Proof. By Lemma 7.6, we have that vTA\u2217v = \u03bb1(vTu)2 + vTA1v \u2265 1\u2212 \u03b3. In particular, this implies that (vTu)2 \u2265 (1\u2212 2\u03b3)/\u03bb1 \u2265 1\u2212 3\u03b3, since 1\u2212 \u03b3 \u2264 \u03bb \u2264 1.\nWe now wish to control the spectrum of B. For any subsets S, T \u2286 [d], and for any vector x and any matrix M , let xS denote x restricted to S andMS,T denote the matrix restricted to the rows in S and the columns in T . Let I be the support of u, and let J be the support of the largest k elements of v.\nLemma 7.8. \u2016BI,I\u2016 \u2264 O(\u03c1\u03b3). Proof. Observe that the condition (14) immediately implies that\n\u2016\u03c1(vIvTI \u2212 uIuTI ) +BI,I\u2016 \u2264 c\u03c1\u03b3 , (15)\nfor some c, since any unit vector x supported on I satisfies xxT \u2208 X2k . Suppose that \u2016BI,I\u2016 \u2265 C\u03b3 for some sufficiently large C. Then (15) immediately implies that \u2016\u03c1(vIvTI \u2212 uIuTI )\u2016 \u2265 (C \u2212 c)\u03c1\u03b3. Since (vIvTI \u2212 uIuTI ) is clearly rank 2, and satisfies tr(vIv T I \u2212uIuTI ) = 1\u2212\u2016uI\u201622 \u2265 0, this implies that the largest eigenvalue of vIvTI \u2212uIuTI is positive. Let x be the top eigenvector of vIv T I \u2212uIuTI . Then, we have xT (vIvTI \u2212uIuTI )x+xTBx = (C\u2212 c)\u03c1\u03b3+ xTBx \u2265 (C \u2212 c)\u03c1\u03b3 by the PSD-ness of B. If C > c, this contradicts (15), which proves the theorem.\nThis implies the following corollary:\nCorollary 7.9. \u2016uI\u201622 \u2265 1\u2212O(\u03b3). Proof. Lemma 7.8 and (15) together imply that \u2016vIvTI \u2212 uIuTI \u2016 \u2264 O(\u03b3). The desired bound then follows from a reverse triangle inequality.\nWe now show this implies a bound on BJ\\I,J\\I :\nLemma 7.10. \u2016BJ\\I,J\\I\u2016 \u2264 O(\u03c1\u03b3). Proof. Suppose \u2016BJ\\I,J\\I\u2016 \u2265 C\u03b3 for some sufficiently large C. Since u is zero on J \\ I , (14) implies that\n\u2016\u03c1vJ\\IvTJ\\I + BJ\\I,J\\I\u2016 \u2264 c\u03c1\u03b3 ,\nfor some universal c. By a triangle inequality, this implies that \u2016vJ\\I\u201622 = \u2016vJ\\IvTJ\\I\u2016 \u2265 (C \u2212 c)\u03b3. Since v is a unit vector, this implies that \u2016vI\u201622 \u2264 1\u2212 (C \u2212 c)\u03b3, which for a sufficiently large C, contradicts Corollary 7.9.\nWe now invoke the following general fact about PSD matrices:\nLemma 7.11. SupposeM is a PSD matrix, written in block form as\nM = ( C D DT E ) .\nSuppose furthermore that \u2016C\u2016 \u2264 \u03be and \u2016E\u2016 \u2264 \u03be. Then \u2016M\u2016 \u2264 O(\u03be). Proof. It is easy to see that \u2016M\u2016 \u2264 O(max(\u2016C\u2016, \u2016D\u2016, \u2016E\u2016)). Thus it suffices to bound the largest singular value of D. For any vectors \u03c6, \u03c8 with appropriate dimension, we have that\n(\u03c6T \u2212 \u03c8T ) M (\n\u03c6 \u2212\u03c8\n) = \u03c6TA\u03c6\u2212 2\u03c6TD\u03c8 + \u03c8TC\u03c8 \u2265 0 ,\nwhich immediately implies that the largest singular value ofD is at most (\u2016A\u2016+\u2016B\u2016)/2, which implies the claim.\nTherefore, Lemmas 7.8 and 7.10 together imply:\nCorollary 7.12. \u2016vI\u222aJvTI\u222aJ \u2212 uI\u222aJuTI\u222aJ\u2016 \u2264 O(\u03b3) .\nProof. Observe (14) immediately implies that \u2016\u03c1(vI\u222aJvTI\u222aJ \u2212uI\u222aJuTI\u222aJ)+BI\u222aJ,I\u222aJ\u2016 \u2264 O(\u03c1\u03b3), since |I \u222aJ | \u2264 2k. Moreover, Lemmas 7.8 and 7.10 with Lemma 7.11 imply that \u2016BI\u222aJ,I\u222aJ\u2016 \u2264 O(\u03c1\u03b3), which immediately implies the statement by a triangle inequality.\nFinally, we show this implies \u2016vvT \u2212 uJuTJ \u2016 \u2264 O(\u03b3), which is equivalent to the theorem.\nProof of Theorem 7.3. We will in fact show the slightly stronger statement, that \u2016uuT \u2212 vJvTJ \u2016F \u2264 O(\u03b3). Observe that since uuT \u2212 vvT is rank 2, Corollary 7.12 implies that \u2016vI\u222aJvTI\u222aJ \u2212 uI\u222aJuTI\u222aJ\u2016F \u2264 O(\u03b3), since for rank two matrices, the spectral and Frobenius norm are off by a constant factor. We have\n\u2016uuT \u2212 vvT \u20162F = \u2211\n(i,j)\u2208I\u2229J\u00d7I\u2229J (uiuj \u2212 vivj)2 +\n\u2211\n(i,j)\u2208I\u00d7I\\J\u00d7J (vivj)\n2 + \u2211\n(i,j)\u2208J\u00d7J\\I\u00d7I (uiuj)\n2 .\nWe have\n\u2211\n(i,j)\u2208I\u2229J\u00d7I\u2229J (uiuj \u2212 vivj)2 +\n\u2211\n(i,j)\u2208J\u00d7J\\I\u00d7I (uiuj)\n2 \u2264 \u2016vI\u222aJvTI\u222aJ \u2212 uI\u222aJuTI\u222aJ\u20162 \u2264 O(\u03b3) ,\nby Corollary 7.12. Moreover, we have that\n\u2211\n(i,j)\u2208I\u00d7I\\J\u00d7J (vivj)\n2 \u2264 2\n \n\u2211\n(i,j)\u2208I\u00d7I\\J\u00d7J (vivj \u2212 uiuj)2 +\n\u2211\n(i,j)\u2208I\u00d7I\\J\u00d7J (uiuj)\n2  \n\u2264 2  \u2016vI\u222aJvTI\u222aJ \u2212 uI\u222aJuTI\u222aJ\u20162 +\n\u2211\n(i,j)\u2208I\u00d7I\\J\u00d7J (uiuj)\n2  \n\u2264 2  \u2016vI\u222aJvTI\u222aJ \u2212 uI\u222aJuTI\u222aJ\u20162 +\n\u2211\n(i,j)\u2208J\u00d7J\\I\u00d7I (uiuj)\n2  \n\u2264 O(\u03b3) .\nsince J \u00d7 J contains the k2 largest entries of uuT . This completes the proof."}, {"heading": "Acknowledgements", "text": "The author would like to thank Ankur Moitra for helpful advice throughout the project, and Michael Cohen for some surprisingly2 useful conversations.\n2Is it really surprising though?"}, {"heading": "B Omitted Details from Section 4", "text": "B.1 Writing non-robust algorithms as dual norm maximization\nIn this section we will briefly review well-known non-robust algorithms for sparse mean recovery and for sparse PCA, and write them using our language.\nThresholding Recall that in the (non-robust) sparse mean estimation problem, one is given samples X1, . . . , Xn \u223c N (\u00b5, I) where \u00b5 is k-sparse. The goal is then to recover \u00b5. It turns out the simple thresholding algorithm THRESHOLDMEAN given in Algorithm 5 suffices for recovery:\nAlgorithm 5 Thresholding for sparse mean estimation\n1: function THRESHOLDMEAN(X1, . . . , Xn) 2: Let \u00b5\u0302 = 1n \u2211n i=1 Xi 3: Let S be the set of k coordinates of \u00b5\u0302 with largest magnitude 4: Let \u00b5\u0302\u2032 be defined to be \u00b5\u0302\u2032i = \u00b5\u0302i if i \u2208 S, 0 otherwise 5: return \u00b5\u0302\u2032 6: end function\nThe correctness of this algorithm follows from the following folklore result, whose proof we shall omit for conciseness:\nFact B.1 (c.f. [Rig15]). Fix \u03b5, \u03b4 > 0, and let X1, . . . , Xn be samples from N (\u00b5, I), where \u00b5 is k-sparse and\nn = \u2126\n( log ( d k ) + log 1/\u03b4\n\u03b52\n) .\nThen, with probability 1\u2212 \u03b4, if \u00b5\u0302\u2032 is the output of THRESHOLDMEAN, we have \u2016\u00b5\u0302\u2032 \u2212 \u00b5\u0302\u20162 \u2264 \u03b5.\nTo write this in our language, observe that\nTHRESHOLDSMEAN(X1, . . . , Xn) = \u2016\u00b5\u0302\u2016\u2217Uk \u00b7 dUk(\u00b5\u0302) ,\nwhere \u00b5\u0302 = 1n \u2211n i=1 Xi.\nL1 relaxation In various scenarios, including recovery of a spiked covariance, one may envision the need to take k-sparse eigenvalues a matrix A, that is, vectors which solve the following non-convex optimization problem:\nmax vTAv\ns.t. \u2016v\u20162 = 1, \u2016v\u20160 \u2264 k . (16)\nHowever, this problem is non-convex and cannot by solved efficiently. This motivates the following SDP relaxation of (16): First, one rewrites the problem as\nmax tr(AX)\ns.t. tr(X) = 1, \u2016X\u20160 \u2264 k2 , X 0 , rank(X) = 1 (17)\nwhere \u2016X\u20160 is the number of non-zeros of X . Observe that since X is rank 1 if we let X = vvT these two problems are indeed equivalent. Then to form the SDP, one removes the rank constraint, and relaxes the \u21130 constraint to a \u21131 constraint:\nmax tr(AX)\ns.t. tr(X) = 1, \u2016X\u20161 \u2264 k ,X 0 . (18)\nThe work of [dEGJL07] shows that this indeed detects the presence of a spike (but at an information theoretically suboptimal rate).\nFinally, by definition, for any PSD matrix A, ifX is the solution to (18) with input A, we haveX = dXk(A).\nB.2 Numerical precision\nIn general, we cannot find closed form solutions for dXk(A) in finite time. However, it is well-known that we can find these to very high numerical precision in polynomial time. For instance, using the ellipsoid method, we can find an M \u2032 so that \u2016M \u2032 \u2212 dXk(A)\u2016\u221e \u2264 \u03b5 in time poly(d, log 1/\u03b5). It is readily verified that if we set \u03b5\u2032 = poly(\u03b5, 1/d) then the numerical precision of the answer will not effect any of the calculations we make further on. Thus for simplicity of exposition we will assume throughout the paper that given any A, we can find dXk(A) exactly in polynomial time."}, {"heading": "C Omitted Proofs from Section 4", "text": "Proof of Theorem 4.5. Fix n as in Theorem 4.5, and let \u03b41 = ( n \u03b5n )\u22121 \u03b4. By convexity of Sn,\u03b5 and the objective function, it suffices to show that with probability 1\u2212 \u03b4, the following holds:\n\u2200wI s.t. |I| = (1\u2212 \u03b5)n : \u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\nwiXi \u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2264 \u03b7 .\nCondition on the event that \u2225\u2225\u2225\u2225\u2225 1 n n\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2264 \u03b5 . (19)\nBy Corollary 4.1, this occurs with probability 1\u2212O(\u03b4).\nFix any I \u2286 [n] so that |I| = (1 \u2212 \u03b5)n. By Corollary 4.1 applied to Ic, we have that there is some universal constant C so that as long as\n\u03b5n \u2265 C \u00b7 min(d, k 2) + log\n( d2\nk2\n) + log ( n \u03b5n ) + log 1/\u03b4 \u03b12 , (20)\nthen with probability 1\u2212 \u03b4\u2032, \u2225\u2225\u2225\u2225\u2225\u2225 1 \u03b5n \u2211 i6\u2208I Xi \u2225\u2225\u2225\u2225\u2225\u2225 \u2217\nUk\n\u2264 \u03b1 . (21)\nSince log ( n \u03b5n ) = \u0398(n\u03b5 log 1/\u03b5), (20) is equivalent to the condition that\nn ( \u03b5\u2212 C \u03b5 log 1/\u03b5\n\u03b12\n) \u2265 C \u00b7 min(d, k 2) + log ( d2 k2 ) + log 1/\u03b4\n\u03b12 .\nLet \u03b1 = O( \u221a\nlog 1/\u03b5). By our choice of \u03b7, we have that 0 \u2264 \u03b5\u2212 \u03b5 log 1/\u03b5\u03b72 \u2264 \u03b5/(2C), and by an appropriate setting of constants, since by our choice of n we have\n\u03b5n 2 \u2265 C \u00b7 min(d, k\n2) + log ( d2\nk2\n) + log 1/\u03b4\n\u03b12 ,\nwe have that (21) holds with probability 1\u2212\u03b4\u2032. Thus by a union bound over all ( n \u03b5n ) choices of I so that |I| = (1\u2212\u03b5)n, we have that except with probability 1\u2212 \u03b4, we have that (21) holds simultaneously for all I with |I| = (1\u2212 \u03b5)n. The desire result then follows from this and (19), and a union bound.\nProof of Theorem 4.6. This follows from the exact same techniques as the proof of Theorem 4.5, by replacing all Uk with Xk, and using Theorem 4.2 instead of Corollary 4.1."}, {"heading": "D Computational Barriers for sample optimal robust sparse mean estimation", "text": "We conjecture that the rate achieved by Theorem 5.1 is tight for computationally efficient algorithms (up to log factors). Intuitively, the major difficulty is that distinguishing between N (\u00b51, I) and N (\u00b52, I) given corrupted samples seems to inherently require second moment (or higher) information, for any \u00b51, \u00b52 \u2208 Rd. Certainly first moment information by itself is insufficient. In this sparse setting, this is very problematic, as this inherently asks for us to detect a large sparse eigenvector of the empirical covariance. This more or less reduces to the problem solved by (16). This in turn requires us to relax to the problem solved by SDPs for sparse PCA, for which we know \u2126(k2 log d/\u03b52) samples are necessary for non-trivial behavior to emerge. We leave resolving this gap as an interesting open problem."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an \u03b5-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise.", "creator": "LaTeX with hyperref package"}}}