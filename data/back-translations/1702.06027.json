{"id": "1702.06027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Parent Oriented Teacher Selection Causes Language Diversity", "abstract": "We studied the effects of two real-life observations: people prefer people with whom they communicate, and people interact with people who are physically close to each other. Clearly, these groups are relatively small compared to the general population. We limit the selection of teachers from such small groups, the so-called imitation groups, around the parents. Then, the child learns the language from a teacher selected within the imitation group of his parents. As a result, there are subcommunities in which his or her own language has developed. Understanding is high within subcommunities. The number of languages depends on the relative size of the imitation, which is determined by a law of power.", "histories": [["v1", "Mon, 20 Feb 2017 15:53:56 GMT  (72kb)", "https://arxiv.org/abs/1702.06027v1", null], ["v2", "Sat, 20 May 2017 17:24:23 GMT  (624kb)", "http://arxiv.org/abs/1702.06027v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ibrahim cimentepe", "haluk o bingol"], "accepted": false, "id": "1702.06027"}, "pdf": {"name": "1702.06027.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Haluk O. Bingol"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n06 02\n7v 2\n[ cs\n.C L\n] 2\n0 M\nay 2\n01 7\nParent Oriented Teacher Selection Causes Language Diversity\nIbrahim Cimentepe and Haluk O. Bingol Dept. of Computer Engineering, Bogazici University\nAn evolutionary model for emergence of diversity in language is developed. We investigated the effects of two real life observations, namely, people prefer people that they communicate with well, and people interact with people that are physically close to each other. Clearly these groups are relatively small compared to the entire population. We restrict selection of the teachers from such small groups, called imitation sets, around parents. Then the child learns language from a teacher selected within the imitation set of her parent. As a result, there are subcommunities with their own languages developed. Within subcommunity comprehension is found to be high. The number of languages is related to the relative size of imitation set by a power law.\nKeywords: language diversity; evolution of language; language-learning model"}, {"heading": "INTRODUCTION", "text": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139]. This is partly due to no agreed definition of language. A group of scientists, including Chomsky, believe that \u201ccommunication cannot be equated with language\u201d [2]. Yet another group consider language as a means to transfer meanings between individuals through signaling structures [8\u201310]. Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19]. Information theoretical approaches predict that not only symbols but word formation is necessary in order to have efficient communication, which leads to basic grammatical rules [12, 15]. There are also empirical approaches to language evolution [4, 5, 20].\nIt is believed that language evolves within generation and while it is transferred from generation to generation. One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].\nIndividuals may imitate each other or prefer to imitate experienced members in population [23]. It may be the case that one learns language by means of imitation. If it is so, who should serve as teachers in community for the\nnext generation? And which imitation strategies can be applied to the population that leads to emergence of language that is shared locally or across population?"}, {"heading": "Motivation", "text": "In this study, we use and extend the mathematical framework that is already established [12, 13, 24]. Our extension leads us to emergence of diversity in language. Language diversity is very popular indeed; it is even addressed in the well-known story of the Tower of Babel. According to the story, the people who speak the same language once scattered all around the world so that they could no longer understand each other. One expects that a child can learn language from her neighbors in the society. The neighborhood includes her parents, her kinship network, territoriality, and labor roles [16]. Ref [13] considers language as a culturally transmitted entity where cultural transmission is defined to be a type of transmission where socially obtained information is passed on, in form of teaching. Three types of neighborhoods, for child to learn a language, are investigated. (i) In the parental learning, asexually produced child learns from her parent. An agent reproduces proportionally to its mutual comprehension, which will be defined shortly, with the rest of the popula-\n2 tion. Therefore the agent who better fits to the population language-wise has better chances to transfer her language to agents of the next generation. (ii) The role model learning is based on reputation. An agent with a higher reputation is imitated more. Therefore it is not important whose offspring it is, a child imitates agents who comprehend better. So the language of an agent with better mutual comprehension is transferred more. In this learning type, T teachers are selected proportional to their mutual comprehension and child learns from them. It is observed that higher values of T produce higher mutual comprehension although it takes longer for system to settle down. (iii) In the random learning there is no structure. A child randomly selects an agent in the population as her teacher. That is, mutual comprehension has no role in teacher selection.\nIn this work, we investigate two new teacher selection strategies. A child is born to her parent. So her teacher has to be related to her parent if not the parent itself. Considering the parent, there are two possible circles of friends. (i) We assume that one is surrounded by people that understand each other well. In the context of language, parent\u2019s friends should be the ones that have high mutual comprehension. (ii) Since we all live in a physical environment, our friends should not be physically too far from us. If we assume that agents located on the nodes of 1D ring lattice, friends should be the ones within close proximity to the parent. In this paper, we modify teacher selection to investigate these two cases."}, {"heading": "BACKGROUND", "text": "We revisit the language model developed by Ref [12, 13, 24] with a slightly modified notation. Then we go over k-means clustering algorithm [25\u201327]. Finally we adapt k-means to language domain and use it to identify language subcommunities"}, {"heading": "Language Model", "text": "We model language communication in a very simple way, called proto-language, as follows: Let P be the set of N agents. An agent i thinks of a meaning \u00b5 and wants to pass it to agent j. Since she does not have means to pass a meaning in her mind directly to the mind of j, she has to use signals. She selects a signal x, which she thinks as a representation of \u00b5, and passes the signal to j. We assume that there is no noisy channel, i.e., one receives exactly what is sent. Receiving x, j tries to interpret x in his own way. Hopefully j will interpret it as \u00b5. Clearly, mappings from \u00b5 to x at i and from x back to \u00b5 at j are very important for a successful communication. We need to specify how association of meaning and signal in sending and receiving ends are done. Suppose every agent has her own statistics a\u00b5x of how frequently she uses signal x to mean meaning \u00b5. Assuming that there are M meanings and S signals, we have an M \u00d7 S association matrix A = [a\u00b5x], for each agent, from which we can derive encoding and decoding methods. Encoding matrix, E = [e\u00b5x], is an M \u00d7 S matrix where e\u00b5x is the probability of using signal x for meaning \u00b5. Decoding matrix, D = [dx\u00b5], on the other hand, is an S \u00d7 M matrix where dx\u00b5 is the probability of understanding meaning \u00b5 for given signal x. The encoding and decoding matrices can be obtained from the association matrix as follows:\ne\u00b5x = a\u00b5x\n\u2211S x\u2032=1 a\u00b5x\u2032 , dx\u00b5 =\na\u00b5x \u2211M\n\u00b5\u2032=1 a\u00b5\u2032x .\nWe will focus on A for language learning since E and D can be derived from A."}, {"heading": "Comprehension", "text": "Suppose agent i wants to pass meaning \u00b5 to agent j. Probability of doing this correctly is\nS \u2211\nx=1\ne(i)\u00b5xd (j) x\u00b5\n3 where e (i) \u00b5x and d (j) x\u00b5 are encoding of i and decoding of j, respectively. When we average that over all meanings, we obtain comprehension from i to j, that is\nF (i \u2192 j) = 1\nM\nM \u2211\n\u00b5=1\nS \u2211\nx=1\ne(i)\u00b5xd (j) x\u00b5 .\nIf we want them to communicate both ways, we consider mutual comprehension\nF (i \u2194 j) = F (i \u2192 j) + F (j \u2192 i)\n2 .\nNow, let\u2019s consider comprehension within a community C \u2286 P . Within community comprehension is defined as the average comprehension in a community C. Thus,\nW (C) = 1\n2 ( |C| 2 )\n\u2211\ni\u2208C\n\u2211\nj\u2208C j 6=i\nF (i \u2194 j).\nWithin community comprehension of the entire population, i.e., W (P), is called overall comprehension."}, {"heading": "Learning Model", "text": "The evolution of language can happen in two different ways. Language evolves both through agents interacting with each other within a generation, and as it is transferred from one generation to the next by means of learning. We follow the latter form as given in Ref [13]. At each generation, population is replaced with new set of N agents. Agents of new generation have no meaning-signal associations initially. That is, the association matrices of agents are empty. For language to be transferred from the generation of parents to the generation of children, some agents are assumed to be chosen as teachers. In Ref [13], teacher selection is a result of fitness gains. Fitness of an agent is directly related to her ability to communicate with overall population. Specifically, the fitness of agent i is\ndefined as\nFi = \u2211\nj\u2208P\nF (i \u2194 j).\nFor the next generation, offspring are produced proportional to the fitness of an agent: the chance that a particular agent arises from agent i is proportional to\nFi \u2211\nj\u2208P Fj .\nThat is, each child agent selects her teacher according to this probability distribution. Thus, agents who have better fitness are picked more. In Ref [13], it is stated that more than one teacher could be assigned for each child agent. This case is examined as a form of cultural learning, where some elite group of agents is responsible for transition of language. It is reported that since the selection mechanism remains the same, total number of teachers assigned only effects how fast the language emerges in such populations [13]. After teachers of the next generation are assigned, language is transferred from teacher to child. The learning process between the child and her teacher is similar to a naming game [28]. Child learns the language of her teacher by sampling their responses to specific meanings. For each meaning, the teacher provides Q responses and the child uses these to populate her association matrix, where Q is called sampling size.\nk-means Clustering Algorithm\nIn this section, we will explain a method to detect sub-language groups. In order to do that, we adapt k-means clustering algorithm to the context of language. Details are given in the Appendix. For a given cluster count K and a distance metric defined on set of observations, k-means clustering algorithm tries to find a partition with K clusters in such a way that average\n4 within cluster distance is optimized [25\u201327]. In this heuristic algorithm, one can find the best value of parameter K by trial and error."}, {"heading": "Finding Optimum Language Clusters", "text": "We adapt k-means to language domain. In this adaptation, k-means provides K communities in such a way that agents in the same community understand each other better. So the distance metric is mutual comprehension and the objective is maximization of within community comprehension over communities. Our approach has two steps: first we find the best partition for a given K, then we find the best K for our purpose. Let PK = {C1, C2, \u00b7 \u00b7 \u00b7 , CK} be some partition of set of agents P with K clusters. We consider clusters as language communities. The average within community comprehension is defined as\nW (PK) = 1\nK\nK \u2211\n\u03b1=1\nW (C\u03b1).\nThere are many partitions of P with K clusters. For a givenK, k-means algorithm provides partition PK , which is expected to be close to the partition with the maximum average within community comprehension. That is,\nPK = argmax K W (PK).\nUnfortunately, there is no algorithm to find the optimal community count. Therefore, we run the algorithm for K \u2208 {Kmin, . . . ,Kmax} and select the one with highest comprehension. Thus,\nK\u2217 = argmax K W (PK)\nis the optimum community count. The corresponding partition PK\u2217 is the optimum partition with the optimum within community comprehension value of\nW \u2217 = W (PK\u2217).\nNote that, given K, k-means has to return K clusters. If K is not suitable to the data set, clusters do not make sense. For example, if the data set has 5 clusters inherently but K is selected to be 2, we do not expect good results. Another example, suppose almost all data is accumulated around a point and there are two outliers close to each other but away from the accumulation point. For K = 2, k-means would cluster the condense points into one and two remote ones into another cluster. Clearly the second cluster would not be the one we want. This is the reason why we try different values of K and select the best one. We expect that K\u2217 is a good fit for the data. See Appendix for further discussion of weaknesses of k-means clustering.\nThe assignment step, in the adopted k-means in Appendix, guarantees that agent is assigned to the cluster that it comprehends best. The iterations check if every agent is in its best cluster. Therefore we expect that clusters are language communities.\nIn order to check whether clusters actually correspond to language communities, let\u2019s define comprehension from a cluster to another cluster as\nI(C\u03b1 \u2192 C\u03b2) = 1\n|C\u03b1||C\u03b2 |\n\u2211\ni\u2208C\u03b1\n\u2211\nj\u2208C\u03b2\nF (i \u2192 j)\nfor C\u03b1 6= C\u03b2 . Then inter-community comprehension is defined as\nI(C\u03b1 \u2194 C\u03b2) = I(C\u03b1 \u2192 C\u03b2) + I(C\u03b2 \u2192 C\u03b1)\n2 .\nFinally, average inter-community comprehension is given as\nI(PK) = 1\n2 ( K 2 )\n\u2211\nC\u03b1\u2208PK\n\u2211\nC\u03b2\u2208PK C\u03b2 6=C\u03b1\nI(C\u03b1 \u2194 C\u03b2).\nIf clusters are language communities, average inter-community comprehension I(PK\u2217) should be low.\n5"}, {"heading": "MODEL", "text": "We propose an evolutionary model where every generation has N agents. The system starts with the first generation, whose association matrices are randomly filled. Remaining generations fill their association matrices by learning. Every agent i makes exactly one child i\u2032. The association matrix of a child is initially empty. Each child learns her language from her teacher, denoted by ti. The teacher provides Q samples for each meaning and the child fills her association matrix based on these samples. Note that there is only one teacher for a child. For a given child, how to select a teacher, in patent\u2019s generation, is what we focus now. We use different ways to choose the teacher and investigate their effects to the language. Note that parent may not be the teacher but clearly affects the selection of it.\nSelection of teacher is done in two steps. In the first step, a set of R agents, called the imitation set, is selected. We consider three different ways to select R candidates for the imitation set.\n1. Model-A. Here, we are trying to construct a social structure that is similar to lifetime encounters. The most basic assumption is that agents make friends with whom they comprehend better. Therefore select R agents that are closest to the parent language-wise.\n2. Model-B. Another approach is that people physically close to each other interact more. We assume that agents are placed on an 1D ring lattice. Then we select R agents that are physically closest to the parent.\n3. Model-C. As a control group, we select R agents uniformly at random.\nIn the second step, the teacher is selected within the imitation set. Let agent \u2113 be in the imitation set Li of parent i. The agent \u2113 is se-\nlected proportional to\nF (i \u2194 \u2113) \u2211\nj\u2208Li F (i \u2194 j)\nas the teacher. That is, an agent who has better mutual comprehension with the parent has better chances to teach his language to the child. Note that, since the model is probabilistic, different realizations produce different results. Therefore, rather than providing results of a single realization in the figures, we report corresponding average values < W (P) >, < W \u2217 >, and < K\u2217 > obtained from averaging over 100 realizations."}, {"heading": "RESULTS AND DISCUSSION", "text": "We investigate the effects of different selection strategies to global language. We compare our findings to the role model learning of [13], which we call it Base Model. In the Base Model, parent has no effect on teacher selection. The teacher is selected proportional to agent\u2019s overall fitness from the entire population. In Fig. 1 we shared the simulation results where overall comprehension W (P) is a function of the relative size of the imitation set,\n6 that is, r = R/N . Since it is independent of r, the Base Model is represented as straight line in our figures. There are N agents using S = 15 signals to communicate M = 8 meanings with sampling size Q = 4. We have results of N = 50, 100, 150 and 200 but we report only N = 100 in Fig. 1. Each data point is an average result of 100 realizations with corresponding r values. We run each realization for 500 generations. This number is sufficient since simulations rapidly converged even in 100 generations to a state where there is no longer a change in W (P), which indicates that the simulation has reached to steady state. We note that ModelB takes more time to settle than the other two models.\nAs one can observe in Fig. 1, Model-C resulted with the best overall comprehension, compared to Models A and B. This result can be explained by the fact that in Model-C agents are essentially free to select any agent. Thus, this results in a situation where every part of the population has a chance to transmit their languages. As a result, emerged language is a product of everyone; therefore it can be globally communicated with.\nModels A and B fail to develop a global language, that provides successful communication among all members of population, unless the size of the imitation set is as large as half of the population, i.e., r > 0.5. In order to understand how bad the results of Models A and B for r < 0.5, we need a model that we can barely call a language. So lets develop one.\nLet\u2019s consider random comprehension within a population P . Random community comprehension Wr(P) is defined as the average comprehension in a population where all meaning and signal associations are equally likely, that is, e\u00b5x = 1/S and dx\u00b5 = 1/M for all possible (\u00b5, x) meaning-signal pairs. In this case, the mutual comprehension between any two agents i and j is\nF (i \u2194 j) = 1\nM .\nThus,\nWr(P) = 1\nM\nwhich yields Wr(P) = 0.125 for M = 8. We expect that any reasonable language should provide much better mutual comprehension than random comprehension. Thus,\nW (P) > Wr(P)\nmust hold in population. In Fig. 1, comprehensions of both Models A and B are below the threshold ofWr(P) = 0.125 for r < 0.05. The comprehensions increase slowly and reach to the level of Model-C as r approaches to 0.5. So clearly, the Models A and B are not good in terms of global language."}, {"heading": "Subcommunities", "text": "Bad performances of Models A and B raises the question, why selection strategies that take into account either language-wise or spatial closeness to parent fail to provide a medium for emergence of global language. One possible explanation could be that rather than single language, that is used by the entire population, many languages, that are used by subcommunities, are emerged. Testing the hypothesis above, we used kmeans algorithm to see if there are such language subcommunities. On the very same data presented in Fig. 1, we apply k-means algorithm to obtain subcommunities. We obtained within community comprehension W \u2217 of the subcommunities. In Fig. 2a, a low W \u2217 value indicates that we could not find any suitable subcommunity structure, whereas when W \u2217 is high, there is such a clustering that agents of the same subcommunity comprehend each other quite well. Except the first data point around r = 0.01, all W \u2217 values are above the minimum language level of the random community comprehension, i.e., W \u2217 > Wr(P). We observe that\n7 r 0 0.1 0.2 0.3 0.4 0.5 < W \u2217 > 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Model-A Model-B Model-C Base Model Wr(P) = 0.125\n(a) Within Community Comprehensions\nr\n0 0.1 0.2 0.3 0.4 0.5\n< I \u2217 >\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nModel-A Model-B Model-C\nWr(P) = 0.125\n(b) Inter-Community Comprehensions\nr\n0 0.1 0.2 0.3 0.4 0.5\n< K\n\u2217 >\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 Model-A Model-B Model-C Base Model\n(c) Subcommunity Counts\nFIG. 2: Language subcommunities. N = 100.\nW \u2217 curves for both Models A and B get close to that of Model-C even for as small values as r = 0.1. That indicates that multiple language communities emerge except for very low values of r. In Fig. 2b, we observe that average inter-community comprehensions are below our barely language level of Wr(P), which confirms that clusters are actually language communities.\nAnother observation is that W \u2217 value obtained in Model-A is greater than corresponding value obtained in Model-B, unless r is very small. On the other hand, as we see in Fig. 2c, the number of emerged languages K\u2217 is greater in Model-B. These two observations indicate that Model-A results languages that are less in number but more efficient in comprehension.\nAlthough Models A and B are very different from each other in structure, as we can see in Fig. 2, resulting number of communities and the quality of communication in these communities stayed approximately the same for both. That is, different strategies did not affect the resulting system. This is quite interesting since Model-A actually tries to find the best suitable candidates, whereas Model-B just picks what is physically available. As a result they both end up with similar communities in terms of comprehension and number."}, {"heading": "Number of Subcommunities", "text": "We now focus on the number of subcommunities. As we mentioned in the discussion on k-means, in order to find the optimal cluster count, one needs to try for different K values. For N = 100, maximum possible value for K is 100. We arbitrarily choose K = 10 as a maximum value. So, we tried K = 1, 2, . . . , 10 and reported optimal K\u2217 value in Fig. 2c.\nThe curve in Fig. 2c starts around 9 and decreases to 1 as r increases. Decrease to 1 is expected since when r is large enough, system converges to a single global language as one concludes from Fig. 1.\n8 r 10-2 10-1 100 \u00a1K \u2217 > 1 2 3 4 5 6 7 8 9 Model-A (N = 50) Model-B (N = 50) Model-A (N = 100) Model-B (N = 100) Model-A (N = 150) Model-B (N = 150) Model-A (N = 200) Model-B (N = 200) 0 0.1 0.2 0.3 0.4 0.5 0 2 4 6 8 10\nFIG. 3: Subcommunity counts both linear and log-log scale for various N .\nThe shape of the curve in Fig. 2c suggests a power law relation between K\u2217 and r, that is,\nK\u2217 \u221d r\u2212\u03b3 .\nWe observe the same relation in Fig. 3, in which K\u2217 is plotted as a function of r for not only N = 100 as in the case of Fig. 2c but values of N = 50, 150 and 200, too. In order to check the power law, the same values are also plotted in log-log scale in Fig. 3, where straight line patterns are observed for r > 0.03. The \u03b3 values are given in Table I.\nWe conclude that for Models A and B, the optimum community count K\u2217 dependents on r. Another way to put this observation is that the number of sub-language communities in a population can be understood and controlled via the ratio of neighborhood size to population size.\nBased on Fig.3, one can make two minor observations: (i) Model-B has slightly larger K\u2217 values compared to Model-A. We have already observed this in Fig. 2 and interpreted as ModelA produces less but more effective languages. (ii) There is a weak relation between K\u2217 and N . For each model, the value of K\u2217 is slightly less for larger values of N ."}, {"heading": "Future Work", "text": "The models we used only cover very basic form of the process and far away from analyzing many complex details of language. Various additions could be made to the model.\nFirst of all, we have assumed that each individual learns her language from one teacher in a very specific way. Different types of learning processes have been reported in Ref [13]. For example, evolution of language can be perceived as a cultural process where some group of people is responsible for the transfer [12]. That is, more than one teacher could be assigned to each child. Once many teachers case is considered, one may also consider teachers not only from the parent\u2019s generation but the generation of grandparents, too [29].\nEven though k-means is a widely used heuristics, we may need much more specialized form of community detection algorithms. (i) If the data set and the given value of K are incompatible, we do not expect the clusters to be meaningful. This problem is avoided by running the system for different K values, which is computationally costly. (ii) In order to measure the quality of clusters some new metrics, such as the comprehension in the \u201cworst\u201d community, can be developed. (iii) k-means algorithm does not guarantee communities of the similar sizes. Indeed we encountered communities that are very small in size in our simulations. Alternative approaches, that take community sizes into account, can be looked into.\nIn this work, we tried to model the fundamental forms of selection mechanisms. Specifically\n9 in Model-B we have discussed territorial differences and we use 1D ring lattice as a spatial organization. More meaningful networks other than our symmetric 1D ring lattice can be investigated. There are many other limitations that can affect selection of teachers in today\u2019s society such as division of labor, class structure, gender and racial differences. Networks, that imitate these asymmetric cases, are particularly interesting."}, {"heading": "CONCLUSIONS", "text": "In this study we investigate two real life conditions to language evolution. (i) We prefer people, whom we communicate well with, around us. (ii) We interact with people that are physically close to us. Clearly we cannot interact with all but a small percentage of the entire population. Given these, the children learn their language from teacher selected from such small group of people around their parents. Such restricted groups for transferring language result emergence of multi-language communities. Interestingly, the two selection criteria produce similar language subcommunities. Number of languages that emerged is related to the relative size of the imitation groups.\nAppendix\nk-means Algorithm\nFor a given K, k-means algorithm is an iterative algorithm, which classifies N data points {xn} N n=0 into K clusters while optimizing a cost function. Let PK(t) = {C1(t), C2(t), \u00b7 \u00b7 \u00b7 , CK(t)} be the partition of data points into K clusters at iteration t \u2208 N. The mean of cluster Ck(t) at iteration t is denoted by mk(t). (i) Initialization Phase. In the initialization phase t = 0, K data points are randomly selected as means {mk(0)} K k=0 . Each iteration t > 0 is composed of an assignment step, which\nis followed by an update step. (ii) Assignment Step. Assign each data point xn to the cluster of nearest mean. That is, assign xn to Ck\u0304(t) where\nk\u0304 = argmin k d(xn,mk(t\u2212 1)) (A1)\nwith d(xi,xj) is a distance between data points xi and xj . (iii) Update Step. Once the new clusters at t are obtained, update the means for the new configuration,\nmk(t) = 1\n|Ck(t)|\n\u2211\nxn\u2208Ck(t)\nxn. (A2)\n(iv) Termination. Steps (ii) and (iii) are repeated until there is no change in the clusters. That is, terminate at iteration tt if Ck(tt) = Ck(tt \u2212 1) for all k. There are a number of issues about k-means algorithm [26, 27]. (i) k-means always provides K clusters even if the ground truth of the data points indicates a number that is different than K. (ii) It is sensitive to initial assignment of mk(0). It may produce different clustering if it starts with different initial conditions. (iii) It is also sensitive to outliers. Traditionally the data points are points in D-dimensional Euclidean space RD. Hence xn,mk(t) \u2208 R\nD for all n, k and t. Then the distance is the Euclidean distance in RD, that is, d(xi,xj) = \u2016xi \u2212 xj\u2016.\nk-means in Language Domain\nIn language domain we want to cluster agents according to their comprehension so that agents in the same cluster comprehend better. Unfortunately, we cannot directly apply k-means to language domain. Since our agents cannot be represented as points in RD any more, sum of xn in Eq. A2 become meaningless. If we do not have mk, Eq. A1 looses its meaning, too. That problem can be bypassed by modifying k-means. We use of mutual comprehension\n10\nF (i \u2194 j) as distance between agents i and j. Of course, higher mutual comprehension means lower distance. Instead of assigning an agent to the nearest mean mk, we assign it to the cluster that it comprehends best. Again, let PK(t) = {C1(t), C2(t), \u00b7 \u00b7 \u00b7 , CK(t)} represent the partition of agents into K clusters at iteration t.\n(i) Initial Step. Initially assign every agent to one of these clusters.\n(ii) Assignment Step. Assign agent i to cluster k\u0304 where\nk\u0304 = argmax k\n1\n|Ck(t\u2212 1)|\n\u2211\nj\u2208Ck(t\u22121)\nF (i \u2194 j).\n(iii) Update Step. Update the clusters accordingly. Remove i from its previous cluster and add it to its new cluster. That is, for i \u2208 C\u2113(t\u2212 1),\nC\u2113(t) = C\u2113(t\u2212 1) \\ {i},\nCk(t) = Ck(t\u2212 1) \u222a {i}.\n(iv) Termination. Steps (ii) and (iii) are repeated until there is no change in the clusters. That is, terminate at iteration tt if Ck(tt) = Ck(tt \u2212 1) for all k.\nIn this adaptation, k-means provides K communities in such a way that agents in the same community understand each other better.\nThe adaptation has the same issues that the original k-means has. Given K, it produces K clusters even if the clusters are not suitable to the data. In order to find the best K values, we use different K values and select the best one."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially supported by Bogazici University Research Fund [BAP-200808A105], by the Turkish State Planning Organization (DPT) TAM Project [2007K120610], and by COST action TD1210.\n[1] S. Pinker and P. Bloom, \u201cNatural language and natural selection,\u201d Behavioral and Brain Sciences, vol. 13, no. 04, pp. 707\u2013727, 1990. [2] J. J. Bolhuis, I. Tattersall, N. Chomsky, and R. C. Berwick, \u201cHow could language have evolved?,\u201d PLoS Biol, vol. 12, no. 8, p. e1001934, 2014. [3] M. D. Hauser, C. Yang, R. C. Berwick, I. Tattersall, M. J. Ryan, J. Watumull, N. Chomsky, and R. C. Lewontin, \u201cThe mystery of language evolution,\u201d Frontiers in Psychology, vol. 5, p. 401, 2014. [4] M. Pagel, Q. D. Atkinson, and A. Meade, \u201cFrequency of word-use predicts rates of lexical evolution throughout indo-european history,\u201d Nature, vol. 449, no. 7163, pp. 717\u2013720, 2007. [5] E. Lieberman, J.-B. Michel, J. Jackson, T. Tang, and M. A. Nowak, \u201cQuantifying the evolutionary dynamics of language,\u201d Nature, vol. 449, no. 7163, pp. 713\u2013716, 2007. [6] W. T. Fitch, \u201cLinguistics: An invisible hand,\u201d Nature, vol. 449, no. 7163, pp. 665\u2013667, 2007. [7] D. Bickerton, \u201cLanguage evolution: A brief guide for linguists,\u201d Lingua, vol. 117, no. 3, pp. 510\u2013526, 2007. [8] M. A. Nowak, N. L. Komarova, and P. Niyogi, \u201cComputational and evolutionary aspects of language,\u201d Nature, vol. 417, no. 6889, pp. 611\u2013 617, 2002. [9] T. Gong and L. Shuai, \u201cModelling the coevolution of joint attention and language,\u201d Proceedings of the Royal Society B: Biological Sciences, vol. 279, pp. 4643\u20134651, Sep 2012. [10] S. Kirby, M. Dowman, and T. L. Griffiths, \u201cInnateness and culture in the evolution of language,\u201d Proceedings of the National Academy of Sciences, vol. 104, no. 12, pp. 5241\u20135245, 2007. [11] A. Cangelosi and D. Parisi, \u201cThe emergence of a\u2019language\u2019in an evolving population of neural networks,\u201d Connection Science, vol. 10, no. 2, pp. 83\u201397, 1998. [12] M. A. Nowak and D. C. Krakauer, \u201cThe evolution of language,\u201d Proceedings of the National Academy of Sciences, vol. 96, no. 14, pp. 8028\u2013 8033, 1999. [13] M. A. Nowak, J. B. Plotkin, and D. C. Krakauer, \u201cThe evolutionary language game,\u201d Journal of Theoretical Biology, vol. 200, no. 2, pp. 147\u2013162, 1999.\n11\n[14] M. A. Nowak, J. B. Plotkin, and V. A. Jansen, \u201cThe evolution of syntactic communication,\u201d Nature, vol. 404, no. 6777, pp. 495\u2013498, 2000. [15] J. B. Plotkin and M. A. Nowak, \u201cLanguage evolution and information theory,\u201d Journal of Theoretical Biology, vol. 205, no. 1, pp. 147\u2013 159, 2000. [16] D. C. Krakauer, \u201cSelective imitation for a private sign system,\u201d Journal of Theoretical Biology, vol. 213, no. 2, pp. 145\u2013157, 2001. [17] E. Tzafestas, \u201cOn modeling proto-imitation in a pre-associative babel,\u201d in International Conference on Simulation of Adaptive Behavior, pp. 477\u2013487, Springer, 2008. [18] M. A. Nowak and N. L. Komarova, \u201cTowards an evolutionary theory of language,\u201d Trends in Cognitive Sciences, vol. 5, no. 7, pp. 288\u2013295, 2001. [19] W. G. Mitchener and M. A. Nowak, \u201cChaos and language,\u201d Proceedings of the Royal Society of London-B, vol. 271, no. 1540, pp. 701\u2013 704, 2004. [20] S. Kirby, H. Cornish, and K. Smith, \u201cCumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language,\u201d Proceedings of the National Academy of Sciences, vol. 105, no. 31, pp. 10681\u201310686, 2008. [21] M. A. Nowak, N. L. Komarova, and P. Niyogi, \u201cEvolution of universal grammar,\u201d Science,\nvol. 291, no. 5501, pp. 114\u2013118, 2001. [22] P. Niyogi and R. C. Berwick, \u201cA language\nlearning model for finite parameter spaces,\u201d Cognition, vol. 61, no. 1, pp. 161\u2013193, 1996. [23] F. McEwen, \u201cPerspectives on imitation: From neuroscience to social science-edited by susan hurley and nick chater,\u201d Mind & Language, vol. 22, no. 2, pp. 207\u2013213, 2007. [24] J. R. Hurford, \u201cBiological evolution of the saussurean sign as a component of the language acquisition device,\u201d Lingua, vol. 77, no. 2, pp. 187\u2013222, 1989. [25] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification. John Wiley & Sons, 2012. [26] D. J. MacKay, Information theory, inference and learning algorithms. Cambridge University Press, 2003. [27] S. Theodoridis, A. Pikrakis, K. Koutroumbas, and D. Cavouras, Introduction to pattern recognition: a matlab approach. Academic Press, 2010. [28] L. Steels, \u201cA self-organizing spatial vocabulary,\u201d Artificial Life, vol. 2, no. 3, pp. 319\u2013332,\n1995. [29] T. Gong and L. Shuai, \u201cSimulating the effects\nof cross-generational cultural transmission on language change,\u201d in Towards a Theoretical Framework for Analyzing Complex Linguistic Networks, pp. 237\u2013256, Springer Berlin Heidelberg, 2016."}], "references": [{"title": "Natural language and natural selection", "author": ["S. Pinker", "P. Bloom"], "venue": "Behavioral and Brain Sciences, vol. 13, no. 04, pp. 707\u2013727, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "How could language have evolved", "author": ["J.J. Bolhuis", "I. Tattersall", "N. Chomsky", "R.C. Berwick"], "venue": "PLoS Biol, vol. 12, no. 8, p. e1001934, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The mystery of language evolution", "author": ["M.D. Hauser", "C. Yang", "R.C. Berwick", "I. Tattersall", "M.J. Ryan", "J. Watumull", "N. Chomsky", "R.C. Lewontin"], "venue": "Frontiers in Psychology, vol. 5, p. 401, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Frequency of word-use predicts rates of lexical evolution throughout indo-european history", "author": ["M. Pagel", "Q.D. Atkinson", "A. Meade"], "venue": "Nature, vol. 449, no. 7163, pp. 717\u2013720, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying the evolutionary dynamics of language", "author": ["E. Lieberman", "J.-B. Michel", "J. Jackson", "T. Tang", "M.A. Nowak"], "venue": "Nature, vol. 449, no. 7163, pp. 713\u2013716, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Linguistics: An invisible hand", "author": ["W.T. Fitch"], "venue": "Nature, vol. 449, no. 7163, pp. 665\u2013667, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Language evolution: A brief guide for linguists", "author": ["D. Bickerton"], "venue": "Lingua, vol. 117, no. 3, pp. 510\u2013526, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Computational and evolutionary aspects of language", "author": ["M.A. Nowak", "N.L. Komarova", "P. Niyogi"], "venue": "Nature, vol. 417, no. 6889, pp. 611\u2013 617, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Modelling the coevolution of joint attention and language", "author": ["T. Gong", "L. Shuai"], "venue": "Proceedings of the Royal Society B: Biological Sciences, vol. 279, pp. 4643\u20134651, Sep 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Innateness and culture in the evolution of language", "author": ["S. Kirby", "M. Dowman", "T.L. Griffiths"], "venue": "Proceedings of the National Academy of Sciences, vol. 104, no. 12, pp. 5241\u20135245, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "The emergence of a\u2019language\u2019in an evolving population of neural networks", "author": ["A. Cangelosi", "D. Parisi"], "venue": "Connection Science, vol. 10, no. 2, pp. 83\u201397, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "The evolution of language", "author": ["M.A. Nowak", "D.C. Krakauer"], "venue": "Proceedings of the National Academy of Sciences, vol. 96, no. 14, pp. 8028\u2013 8033, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "The evolutionary language game", "author": ["M.A. Nowak", "J.B. Plotkin", "D.C. Krakauer"], "venue": "Journal of Theoretical Biology, vol. 200, no. 2, pp. 147\u2013162, 1999.  11", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "The evolution of syntactic communication", "author": ["M.A. Nowak", "J.B. Plotkin", "V.A. Jansen"], "venue": "Nature, vol. 404, no. 6777, pp. 495\u2013498, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Language evolution and information theory", "author": ["J.B. Plotkin", "M.A. Nowak"], "venue": "Journal of Theoretical Biology, vol. 205, no. 1, pp. 147\u2013 159, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Selective imitation for a private sign system", "author": ["D.C. Krakauer"], "venue": "Journal of Theoretical Biology, vol. 213, no. 2, pp. 145\u2013157, 2001.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "On modeling proto-imitation in a pre-associative babel", "author": ["E. Tzafestas"], "venue": "International Conference on Simulation of Adaptive Behavior, pp. 477\u2013487, Springer, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards an evolutionary theory of language", "author": ["M.A. Nowak", "N.L. Komarova"], "venue": "Trends in Cognitive Sciences, vol. 5, no. 7, pp. 288\u2013295, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Chaos and language", "author": ["W.G. Mitchener", "M.A. Nowak"], "venue": "Proceedings of the Royal Society of London-B, vol. 271, no. 1540, pp. 701\u2013 704, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language", "author": ["S. Kirby", "H. Cornish", "K. Smith"], "venue": "Proceedings of the National Academy of Sciences, vol. 105, no. 31, pp. 10681\u201310686, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolution of universal grammar", "author": ["M.A. Nowak", "N.L. Komarova", "P. Niyogi"], "venue": "Science,  vol. 291, no. 5501, pp. 114\u2013118, 2001.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "A language learning model for finite parameter spaces", "author": ["P. Niyogi", "R.C. Berwick"], "venue": "Cognition, vol. 61, no. 1, pp. 161\u2013193, 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Perspectives on imitation: From neuroscience to social science-edited by susan hurley and nick chater", "author": ["F. McEwen"], "venue": "Mind & Language, vol. 22, no. 2, pp. 207\u2013213, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Biological evolution of the saussurean sign as a component of the language acquisition device", "author": ["J.R. Hurford"], "venue": "Lingua, vol. 77, no. 2, pp. 187\u2013222, 1989.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "Information theory, inference and learning algorithms", "author": ["D.J. MacKay"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Introduction to pattern recognition: a matlab approach", "author": ["S. Theodoridis", "A. Pikrakis", "K. Koutroumbas", "D. Cavouras"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "A self-organizing spatial vocabulary", "author": ["L. Steels"], "venue": "Artificial Life, vol. 2, no. 3, pp. 319\u2013332, 1995.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Simulating the effects of cross-generational cultural transmission on language change", "author": ["T. Gong", "L. Shuai"], "venue": "Towards a Theoretical Framework for Analyzing Complex Linguistic Networks, pp. 237\u2013256, Springer Berlin Heidelberg, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 1, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 2, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 3, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 4, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 5, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 6, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 7, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 8, "context": "Language remains mystery in many aspects including how it is emerged, how it is evolved, and how it is learned [1\u20139].", "startOffset": 111, "endOffset": 116}, {"referenceID": 1, "context": "A group of scientists, including Chomsky, believe that \u201ccommunication cannot be equated with language\u201d [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Yet another group consider language as a means to transfer meanings between individuals through signaling structures [8\u201310].", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "Yet another group consider language as a means to transfer meanings between individuals through signaling structures [8\u201310].", "startOffset": 117, "endOffset": 123}, {"referenceID": 9, "context": "Yet another group consider language as a means to transfer meanings between individuals through signaling structures [8\u201310].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 10, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 11, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 12, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 13, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 14, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 15, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 16, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 17, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 97, "endOffset": 107}, {"referenceID": 18, "context": "Assuming that language provides an evolutionary advantage, some evolutionary models are proposed [1, 11\u201318], some of which are game theoretical [19].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Information theoretical approaches predict that not only symbols but word formation is necessary in order to have efficient communication, which leads to basic grammatical rules [12, 15].", "startOffset": 178, "endOffset": 186}, {"referenceID": 14, "context": "Information theoretical approaches predict that not only symbols but word formation is necessary in order to have efficient communication, which leads to basic grammatical rules [12, 15].", "startOffset": 178, "endOffset": 186}, {"referenceID": 3, "context": "There are also empirical approaches to language evolution [4, 5, 20].", "startOffset": 58, "endOffset": 68}, {"referenceID": 4, "context": "There are also empirical approaches to language evolution [4, 5, 20].", "startOffset": 58, "endOffset": 68}, {"referenceID": 19, "context": "There are also empirical approaches to language evolution [4, 5, 20].", "startOffset": 58, "endOffset": 68}, {"referenceID": 0, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 1, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 2, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 3, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 4, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 20, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 21, "context": "One of the critical issues, which includes rich discussions on universal grammar, is how language is learned by the new generation [1\u20135, 21, 22].", "startOffset": 131, "endOffset": 144}, {"referenceID": 22, "context": "Individuals may imitate each other or prefer to imitate experienced members in population [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "In this study, we use and extend the mathematical framework that is already established [12, 13, 24].", "startOffset": 88, "endOffset": 100}, {"referenceID": 12, "context": "In this study, we use and extend the mathematical framework that is already established [12, 13, 24].", "startOffset": 88, "endOffset": 100}, {"referenceID": 23, "context": "In this study, we use and extend the mathematical framework that is already established [12, 13, 24].", "startOffset": 88, "endOffset": 100}, {"referenceID": 15, "context": "The neighborhood includes her parents, her kinship network, territoriality, and labor roles [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Ref [13] considers language as a culturally transmitted entity where cultural transmission is defined to be a type of transmission where socially obtained information is passed on, in form of teaching.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "We revisit the language model developed by Ref [12, 13, 24] with a slightly modified notation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 12, "context": "We revisit the language model developed by Ref [12, 13, 24] with a slightly modified notation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 23, "context": "We revisit the language model developed by Ref [12, 13, 24] with a slightly modified notation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 24, "context": "Then we go over k-means clustering algorithm [25\u201327].", "startOffset": 45, "endOffset": 52}, {"referenceID": 25, "context": "Then we go over k-means clustering algorithm [25\u201327].", "startOffset": 45, "endOffset": 52}, {"referenceID": 12, "context": "We follow the latter form as given in Ref [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "In Ref [13], teacher selection is a result of fitness gains.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "In Ref [13], it is stated that more than one teacher could be assigned for each child agent.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "It is reported that since the selection mechanism remains the same, total number of teachers assigned only effects how fast the language emerges in such populations [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 26, "context": "The learning process between the child and her teacher is similar to a naming game [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "within cluster distance is optimized [25\u201327].", "startOffset": 37, "endOffset": 44}, {"referenceID": 25, "context": "within cluster distance is optimized [25\u201327].", "startOffset": 37, "endOffset": 44}, {"referenceID": 12, "context": "We compare our findings to the role model learning of [13], which we call it Base Model.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "Different types of learning processes have been reported in Ref [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "For example, evolution of language can be perceived as a cultural process where some group of people is responsible for the transfer [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "Once many teachers case is considered, one may also consider teachers not only from the parent\u2019s generation but the generation of grandparents, too [29].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "There are a number of issues about k-means algorithm [26, 27].", "startOffset": 53, "endOffset": 61}, {"referenceID": 25, "context": "There are a number of issues about k-means algorithm [26, 27].", "startOffset": 53, "endOffset": 61}, {"referenceID": 0, "context": "[1] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] T.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "An evolutionary model for emergence of diversity in language is developed. We investigated the effects of two real life observations, namely, people prefer people that they communicate with well, and people interact with people that are physically close to each other. Clearly these groups are relatively small compared to the entire population. We restrict selection of the teachers from such small groups, called imitation sets, around parents. Then the child learns language from a teacher selected within the imitation set of her parent. As a result, there are subcommunities with their own languages developed. Within subcommunity comprehension is found to be high. The number of languages is related to the relative size of imitation set by a power law.", "creator": "LaTeX with hyperref package"}}}