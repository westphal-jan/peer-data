{"id": "1702.07028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "On the ability of neural nets to express distributions", "abstract": "Deep neural networks have triggered a revolution in many classification tasks, and a similar ongoing revolution - also not understood in theory - concerns their ability to serve as generative models for complicated data types such as images and text, which are trained on ideas such as varying autoencoders and generative adversarial networks.", "histories": [["v1", "Wed, 22 Feb 2017 22:21:38 GMT  (37kb)", "https://arxiv.org/abs/1702.07028v1", "Under review for COLT 2017"], ["v2", "Fri, 2 Jun 2017 15:14:17 GMT  (39kb)", "http://arxiv.org/abs/1702.07028v2", "Accepted to COLT 2017"]], "COMMENTS": "Under review for COLT 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["holden lee", "rong ge", "tengyu ma", "rej risteski", "sanjeev arora"], "accepted": false, "id": "1702.07028"}, "pdf": {"name": "1702.07028.pdf", "metadata": {"source": "CRF", "title": "On the ability of neural nets to express distributions", "authors": ["Holden Lee", "Rong Ge", "Tengyu Ma", "Andrej Risteski", "Sanjeev Arora"], "emails": ["holdenl@princeton.edu", "rongge@cs.duke.edu", "tengyu@cs.princeton.edu", "risteski@princeton.edu", "arora@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 02\n8v 2\n[ cs\n.L G\n] 2\nJ un\n2 01\nWe take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with n hidden layers. A key ingredient is Barron\u2019s Theorem [Bar93], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (\u201cBarron functions\u201d) can be approximated by a n+ 1- layer neural network.\nFor probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance\u2014a natural metric on probability distributions\u2014by a neural network applied to a fixed base distribution (e.g., multivariate gaussian).\nBuilding up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone."}, {"heading": "1 Introduction", "text": "Deep neural networks have led to state-of-the-art performance on classification tasks in many domains such as computer vision, speech recognition, and reinforcement learning ([BCV13; Sch15]). One can view a neural network as a way to learn a function mapping inputs x to outputs y. For image classification, the input is a vector representing an image and the output can be probabilities of being in various classes.\nBut another recent (and less understood) use of neural networks is as generative models for complicated probability distributions, such as distributions over images on ImageNet, handwritten characters from various alphabets, or speech. Here the network may map a stochastic input\u2014 such as a uniform normal gaussian\u2014to a realistic image. Such networks are trained using various methods such as variational autoencoders ([KW13], [RMW14]) or generative adversarial networks (GANs) ([Goo+14]). A GAN consists of a repeated zero-sum game between two networks: the\n\u2217Princeton University, Mathematics Department holdenl@princeton.edu \u2020Duke University, Computer Science Department, rongge@cs.duke.edu \u2021Princeton Univerisity, Computer Science Department, tengyu@cs.princeton.edu \u00a7Princeton Univerisity, Computer Science Department, risteski@princeton.edu \u00b6Princeton Univerisity, Computer Science Department, arora@cs.princeton.edu. Supported by NSF grants CCF-\n1302518, CCF-1527371, Simons Investigator Award, Simons Collaboration Grant, and ONR- N00014-16-1-2329\ngenerator attempts to imitate a given probability distribution; it obtains its samples by passing a base distribution (e.g. a gaussian) through its neural network. The discriminator attempts to distinguish between samples from the generator and the true distribution, and thus forces the generator to improve over many repetitions.\nThe current paper is concerned with the following natural question that appears not to have been studied before: Why are deep neural networks so well-suited to efficiently generate many distributions that occur in nature?"}, {"heading": "1.1 Our work", "text": "We give a sufficient criterion for a function to be approximable by a neural network with n hidden layers (Theorem 3.1). This criterion holds with respect to any distribution of inputs supported on a compact set. As a consequence of our main result, we obtain a criterion for a distribution to be approximately generated by a neural network with n hidden layers in the Wasserstein metric W2, a natural metric on the space of distributions (Corollary 3.3).\nOur criterion relies on Fourier properties of the function. We build on Barron\u2019s Theorem [Bar93], which says that if a certain quantity involving the Fourier transform is small, then the function can be approximated by a neural network with one hidden layer and a small number of nodes. Calling such a function a Barron function, our criterion roughly says that if a distribution is generated by a composition of n Barron functions, then the distribution can be approximately generated by a neural network with n hidden layers.\nMany nice functions, such as polynomials and ridge functions, are Barron; this property is also preserved under natural operations such as linear combinations. Thus, our result says that if nature creates a distribution by starting from a base distribution (such as a gaussian) and applying a sequence of functions in this class, then we can also generate that distribution with a neural network.\nThis \u201ccorrespondence\u201d between compositions of Barron functions and multi-layer neural networks raises questions analogous to those raised about neural nets: for example, are compositions of k Barron functions more expressive than Barron functions? Using a technique to lower-bound the Barron constant (Theorem 4.2), we show a separation theorem between Barron functions and composition of Barron functions (Theorem 4.1). This parallels \u2014and is inspired by\u2014the separation between 2-layer and 3-layer neural networks in [ES15]."}, {"heading": "1.2 Related work", "text": "Despite the practical success of neural networks, we lack a good theoretical understanding of their effectiveness. An initial attempt to understand the effectiveness of neural networks was by their function approximation properties. A series of works showed that any continuous function in a bounded domain can be approximated by a sufficiently large 2-layer neural network ([Cyb89], [Fun89], [HSW89]). However, the network size can be exponential in the dimension. Barron ([Bar93]) gave a upper bound for the size of the network required in terms of a Fourier criterion. He showed that a function f can be approximated in L2 up to error \u03b5 by a 2-layer neural network\nwith O\n\u00c5 C2\nf\n\u03b5\n\u00e3 units, where Cf depends on Fourier properties of f . One remarkable consequence is\nthat representationally speaking, neural nets can evade the curse of dimensionality: the number of parameters required to obtain a fixed error increases linearly, rather than superlinearly, in the\nnumber of dimensions. (Fixing the number of nodes in the hidden layer, the number of parameters scales linearly in the number of dimensions.)\nHowever, such approximability results only explain a small part of the success of neural networks. Firstly, they only deal with 2-layer neural networks. Empirically speaking, deep neural networks\u2014 networks with many layers\u2014appear to be much more effective than shallow neural networks. There have been several attempts to explain the effectiveness of deep neural networks. Following the paradigm in circuit complexity, one produces a function f that can be computed by a deep neural network but requires exponentially many nodes to be computed by a shallow neural network. Eldan and Shamir ([ES15]) show a certain radial function can be approximated by a 3-layer neural net but not by a 2-layer neural net with a subexponential number of nodes. [Dan17] shows such a separation but with respect to the uniform distribution on the sphere. Telgarsky ([Tel16]) shows such a separation between k2-layer and k-layer neural networks. Cohen, Sharir, and Shashua ([CSS15]) show a separation for a different model, a certain type of convolutional neural net architecture. Kane and Williams ([KW16]) show super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits, which can be thought of as a boolean analogue to neural networks.\nSecondly, these works\u2014as well as our paper\u2014do not address how to learn neural networks, or why the established method, gradient descent, has been so successful. [Bar93] and [Bar94] address the generalization theory, and show that the nodes can be chosen \u201cgreedily\u201d; however the optimization problem is nonconvex. Under the assumption that certain properties of the input distribution (related to the score function) are known and that the function is exactly representable by a 2-layer neural network, Janzamin, Sedghi, and Anandkumar ([JSA15]) give an algorithm inspired by Barron\u2019s Fourier criterion and utilizing tensor decomposition, to learn 2-layer neural networks.\nFinally, we note that the learnability for distributions has been studied for discrete distributions ([Kea+94]).\nOrganization of the paper We explain Barron\u2019s original theorem in Section 2, our criterion for representation by multi-layer neural networks in Section 3, and give our separation result in Section 4. Most proofs and background on Fourier analysis are left in Appendix."}, {"heading": "1.3 Notation and Definitions", "text": "First, we formally define the model of a feedforward neural network that we will use.\nDefinition 1.1. A neural network with n hidden layers (also referred to as a n + 1-layer neural network) is defined as follows. A neural network has an associated input space Rm0 , output space Rmn+1 , and n hidden layers of sizes m1, . . . ,mn \u2208 N.The neural network has parameters A(l) \u2208 Rml\u22121\u00d7ml and b(l) \u2208 Rml for 1 \u2264 l \u2264 n + 1. The neural network has a fixed activation function \u03c3, which is applied component-wise on a vector. On input x \u2208 Rm0 , the network computes\nx(0) : = x (1)\nx(l) : = \u03c3(A(l\u22121)x(l\u22121) + b(l)) 1 \u2264 l \u2264 n (2) x(n+1) : = A(n+1)x(n) + b(n+1). (3)\nand outputs x(n+1). This can also be written out in terms of the components:\nx (l) j := \u03c3\n( ml\u2211\nk=1\nA (l\u22121) jk x (l\u22121) k + b (l\u22121) k\n) .\nCommon choices of activation functions \u03c3 include the logistic function 11+e\u2212x , tanh(x), and the ReLU function max{0, x}.\nDefinition 1.2. For a function f : Rm \u2192 Rn, define Lip(f) = Lip2(f), the Lipschitz constant of f with respect to the L2 norm, by\ninf {C : \u2200x, y, \u2016f(x)\u2212 f(y)\u20162 \u2264 C \u2016x\u2212 y\u20162} .\nLet Bn be the unit ball in n dimensions{x \u2208 Rn : \u2016x\u2016 \u2264 1}. For sets A,B and a scalar r, let\nA+B := {x+ y : x \u2208 A, y \u2208 B} , rA := {rx : x \u2208 A} . (4)\nFor example, rBn denotes the ball of radius r in n dimensions, and A + rBn is the neighborhood of radius r around A.\nLet \u2016\u00b7\u2016 = \u2016\u00b7\u20162 denote the usual Euclidean norm on vectors in Rn. For a function f , let f\u2228(x) := f(\u2212x). (This notation is often used in Fourier analysis.) Let f (n)(x) = dndxn f(x) denote the nth derivative, and \u2206f = \u2211n i=1 \u22022\n\u2202xi2 f denote the Laplacian."}, {"heading": "2 Barron\u2019s Theorem", "text": "For f \u2208 L1(R) we define the Fourier transform of f : Rn \u2192 R with the following normalization.\nf\u0302(\u03c9) := 1\n(2\u03c0)n\n\u222b\nRn f(x)e\u2212i\u3008\u03c9,x\u3009 dx. (5)\nFor vector-valued functions f : Rn \u2192 Rm, define the Fourier transform componentwise. The inverse Fourier transform is\n(F\u22121g)(x) := \u222b\nRn g(\u03c9)ei\u3008\u03c9,x\u3009 dx = (2\u03c0)ng\u0302\u2228\nThe Fourier inversion formula, which holds for all sufficiently \u201cnice\u201d functions, is\nf(x) =\n\u222b\nRn f\u0302(x)ei\u3008\u03c9,x\u3009 dx. = (2\u03c0)n \u02c6\u0302f\u2228\nFor background on Fourier analysis with rigorous statements, see Appendix A. [Bar93] defines a norm on functions defined on a set B, and shows that a small norm implies that the function is amenable to approximation by a neural network with one hidden layer.\nDefinition 2.1. For a bounded set B \u2286 Rp let \u2016\u03c9\u2016B = supx\u2208B | \u3008\u03c9, x\u3009 |. For a function f : Rn \u2192 R, define the norm \u2016f\u2016\u2217B := \u222b Rn \u2016\u03c9\u2016B |f\u0302(\u03c9)| d\u03c9.\nWhen B = Bn is the unit ball, \u2016\u03c9\u2016B = \u2016\u03c9\u20162. In this case, using Theorem A.3,\n\u2016f\u2016\u2217B = \u222b\nRn \u2016\u03c9\u2016 |f\u0302(\u03c9)| d\u03c9 =\n\u2225\u2225\u2225 \u2225\u2225\u2225\u03c9f\u0302 \u2225\u2225\u2225 2 \u2225\u2225\u2225 1 = \u2225\u2225\u2225 \u2225\u2225\u2225\u2207\u0302f \u2225\u2225\u2225 2 \u2225\u2225\u2225 1\nwhere for a function g : Rn \u2192 Rn, \u2016g\u20162 is thought of as a function Rn \u2192 R, and \u2016\u2016g\u20162\u20161 is the L1 norm of this function.\nWe would like to define this norm for functions f : B \u2192 R. However, the Fourier transform is defined for functions f : Rn \u2192 R. Because we only care about the value of f on B, we allow arbitrary extension outside of B.\nDefinition 2.2. Let B \u2286 Rn. Let FB be the set of functions for which the Fourier inversion formula holds on B after subtracting out g(0):1\nFB = \u00df g : Rn \u2192 R : \u2200x \u2208 B, g(x) = g(0) + \u222b (ei\u3008\u03c9,x\u3009 \u2212 1)g\u0302(\u03c9) d\u03c9 \u2122 .\nDefine \u0393B = {f : B \u2192 R : \u2203g, g|B = f, g \u2208 FB}, let \u0393B(C) be the subset with norm \u2264 C \u0393B(C) = {f : B \u2192 R : \u2203g, g|B = f, \u2016g\u2016\u2217B \u2264 C, g \u2208 FB}. We say that a function f \u2208 \u0393B(C) is C-Barron on B. For a function f : B \u2192 R, let Cf,B be the minimal constant for which f \u2208 \u0393B,C :\nCf,B := inf g|B=f,g\u2208FB\n\u222b\nRn \u2016\u03c9\u2016B |g\u0302(\u03c9)| d\u03c9. (6)\nWhen the set B is clear, we just write Cf .\nThis definition is non-algorithmic. How to compute or approximate the Barron constant in general is an open problem. The difficulty stems from the fact that we have to take an infimum over all possible extensions. The Barron constant can be upper-bounded by choosing any extension f , but is more difficult to lower-bound. We will give a technique to lower-bound the Barron constant in Theorem 4.2.\nWe give some intuition on the Barron constant. First, in order for the Barron constant to be finite, f must be continuously differentiable. Indeed, the inverse Fourier transform of \u03c9f\u0302(\u03c9) is \u2212i\u2207f(x), and integrability of a function implies continuity of its (inverse) Fourier transform, so \u2207f is continuous.\nSecond, the Barron constant will be larger when f\u0302 is more \u201cspread out.\u201d One can think of \u2016g\u2016B as a kind of L1 norm. This makes sense in the context of neural networks, because if f(x) = \u2211k i=1 ci\u03c3(\u3008ai, x\u3009 + bi) then f has Fourier transform completely supported on the lines in the direction of the ai. 2 One can think of the Barron constant as a L1 relaxation of this \u201csparsity\u201d condition. Barron\u2019s Theorem gives an upper bound on how well a function can be approximated by a neural network with 1 hidden layer of k nodes, in terms of the Barron constant. For a list of functions with small Barron constant, as well as the effect of various operations on the Barron constant, see [Bar93, \u00a7IX]. Examples of Barron functions include polynomials of low degree, ridge functions, and linear combinations of Barron functions.\nDefinition 2.3. A sigmoidal function is a bounded measurable function f : R \u2192 R such that limx\u2192\u2212\u221e f(x) = 0 and limx\u2192\u221e f(x) = 1.\n1This is a strictly larger set than functions for which the Fourier inversion formula holds. 2Here f does not approach 0 as \u2016x\u2016 \u2192 \u221e, so the Fourier transform must be understood in the sense of distributions.\nTheorem 2.4 (Barron, [Bar93]). Let B \u2286 Rn be a bounded set, and \u00b5 any probability measure on B. Let f \u2208 \u0393B(C) and \u03c3 be sigmoidal. There exist ai \u2208 Rn, bi \u2208 R, ci \u2208 R with \u2211k i=1 |ci| \u2264 2C\nsuch that letting fk(x) = \u2211k i=1 ci\u03c3(\u3008ai, x\u3009+ bi), we have\n\u2016f \u2212 fk\u20162\u00b5 := \u222b\nB (f(x)\u2212 fk(x))2 \u00b5(dx) \u2264\n(2C)2\nk .\nBarron\u2019s Theorem works for the logistic function (which is sigmoidal), hyperbolic tangent (which is sigmoidal if rescaled to [0, 1]), and ReLU up to a factor of 2 in the number of nodes. Even though the ReLU function ReLU(x) = max{0, x} is not sigmoidal, the linear combination ReLU(x) = ReLU(x)\u2212 ReLU(x\u2212 1) is.\nNote that Barron\u2019s Theorem doesn\u2019t give approximability tailored to a specific measure \u00b5; it simultaneously gives approximability for all \u00b5 defined on B, and up to any degree of accuracy. This is why some degree of smoothness is necessary for f : otherwise, \u00b5 could be concentrated on the regions where B is not smooth. Note that approximability for all \u00b5 will be crucial to the proof of the main theorem (Theorem 3.1). 3"}, {"heading": "3 Multilayer Barron\u2019s Theorem", "text": ""}, {"heading": "3.1 Main theorem", "text": "Barron\u2019s Theorem says that a Barron function can be approximated by a neural net with 1 hidden layer. From this, it is reasonable to suspect that a composition of l Barron functions can be approximated by a neural network with l hidden layers. Our main theorem says that this is the case; we give a sufficient criterion for a function to be approximated by a neural network with l hidden layers, on any distribution supported in a fixed set K0.\nWe note two caveats: first, fi need to be Lipschitz to prevent the error from blowing up. Second, we will need our functions fi to be Barron on a slightly expanded set (assumption 3), because an approximation gi to fi could take points outside Ki, and we need to control the error for those points.\nGiven a sequence of functions fi and j \u2265 i, let fj:i := fj \u25e6 fj\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fi.\nTheorem 3.1 (Main theorem). Let \u03b5, s > 0 be parameters, and l \u2265 1. For 0 \u2264 i \u2264 l let mi \u2208 N. Let fi : R\nmi\u22121 \u2192 Rmi be functions, \u00b50 be any probability distribution on Rm0 , and Ki \u2282 Rmi be sets.\nSuppose the following hold.\n1. (Support of initial distribution) Supp(\u00b50) \u2282 K0.\n2. (fi is Lipschitz) Lip(fi) \u2264 1. 3Although Barron\u2019s Theorem seems to require a strong smoothness assumption, we can approximate any continuous function arbitrarily well with a smooth function and then apply Barron\u2019s Theorem. A converse to Barron\u2019s Theorem cannot hold in the form stated, because if \u2016ai\u2016 is not restricted, then \u03c3(\u3008ai, x\u3009+ bi) could have large gradient; the Barron constant of \u03c6(\u3008ai, x\u3009+ bi) would scale as \u2016ai\u2016. It is natural to ask whether we can choose the ai to have bounded norm. Barron [Bar93, Theorem 3] shows a version of the theorem that produces a representation with \u2016ai\u2016 \u2264 \u03c4 , but that incurs an additive error C\u03c4 in the approximation. Note that the following weak converse holds: the Barron constant of f = c0 + \u2211r i=1 ci\u03c3(\u3008ai, x\u3009 + bi) is bounded by\nO(diam(K) \u2211r\ni=1 |ci| \u2016ai\u2016).\n3. (fi is Barron) f1 \u2208 \u0393K0(C0) and for 1 \u2264 i \u2264 l, fi \u2208 \u0393Ki\u22121+sBmi\u22121 (Ci).\n4. (fi takes each set to the next) fi(Ki\u22121) \u2286 Ki Suppose that the diameter of Kl is D. Then there exists a neural network g with l hidden layers\nwith \u2308 4C2i mi\n\u03b52\n\u2309 nodes on the ith layer, so that\n\u00c7\u222b\nK0\n\u2016fl:1 \u2212 g\u20162 d\u00b50 \u00e5 1 2 \u2264 l\u03b5 \u00a0 (2Cl \u221a ml +D)2 l\n3s2 + 1. (7)\nWe prove this in Section 3.3. It is crucial to the proof that Barron\u2019s Theorem simultaneously gives approximability for all probability distributions on a given set.\nNote that if Kl\u22121 is a ball of radius r, by the way we defined the norm \u2016\u00b7\u2016Kl\u22121 in the Barron constant, Cl will at least scale as s + r. If we set s to be on the same order as r, then the RHS of (7) is on the order of l 3 2m 1 2\nl \u03b5."}, {"heading": "3.2 Approximating probability distributions", "text": "Theorem 3.1 can be interpreted in a very natural way when the aim is to approximate the probability distribution fl:1(x), x \u223c \u00b50. The Wasserstein distance is a natural distance defined on distributions.\nDefinition 3.2. Let \u00b5, \u03bd be two probability distributions on Rn. Let \u0393(\u00b5, \u03bd) denote the set of probability distributions on Rn \u00d7 Rn whose marginals on the first and second factors are \u00b5 and \u03bd respectively. (A distribution \u03b3 \u223c \u0393(\u00b5, \u03bd) is called a coupling of \u00b5, \u03bd.) For 1 \u2264 p < \u221e, define the pth Wasserstein distance by\nWp(\u00b5, \u03bd) =\n\u00c7 inf\n\u03b3\u2208\u0393(\u00b5,\u03bd)\n\u222b\nRn\u00d7Rn \u2016x\u2212 y\u2016p2 d\u03b3(x, y)\n\u00e5 1 p\nWhen p = 1, this is also known as the \u201cearth mover\u2019s distance.\u201d One can think of it as the minimum \u201ceffort\u201d required to change the distribution of \u00b5 to that of \u03bd by shifting probability mass (where \u201ceffort\u201d is an integral of mass times distance).\nCorollary 3.3. Keep the notation in Theorem 3.1 and suppose the diameter of the set fl:1(K0) is D. Then the Wasserstein distance between the distribution fl:1(X)(X \u223c \u00b50) and g(X), (X \u223c \u00b50) is at most l\u03b5 \u00bb 1 + (2Cl \u221a ml +D)2 l 3s2 .\nThe proof of this is simple: observe that (fl:1(X), g(X)), X \u223c \u00b50 defines a coupling between the distributions. Thus by Theorem 3.1 the W2 Wasserstein distance is at most\n\u00f1 E\nX\u223c\u00b50 \u2016fl:1(X)\u2212 g(X)\u20162\n\u00f4 1 2 \u2264 l\u03b5 \u00a0 (2Cl \u221a ml +D)2 l\n3s2 + 1.\nThe Wasserstein distance is a suitable metric in the context of GANs ([AB17], [ACB17]). One way to model a discriminator is as a function f in a certain class F that maximizes the difference between Ef on the real distribution \u00b5 and the generated distribution \u03bd,\nsup f\u2208F \u2223\u2223\u2223\u2223 Ex\u223c\u00b5 f(x)\u2212 Ey\u223c\u03bd f(y) \u2223\u2223\u2223\u2223 . (8)\nThis is called the maximal mean discrepancy ([KBG04], [DRG15]). The Wasserstein distance captures the idea that if two distributions are close, then it is hard for such a Lipschitz discriminator to tell the difference, as the following lemma shows.\nLemma 3.4 (Properties of Wasserstein metric). For any two distributions \u00b5, \u03bd over Rn, W1(\u00b5, \u03bd) \u2264 W2(\u00b5, \u03bd). Moreover, for any Lipschitz function f : R\nn \u2192 R, \u2223\u2223\u2223\u2223 Ex\u223c\u00b5 f(x)\u2212 Ey\u223c\u03bd f(y) \u2223\u2223\u2223\u2223 \u2264 Lip(f)W1(\u00b5, \u03bd). (9)\nProof is deferred to Appendix C. In the context of Corollary 3.3, Lemma 3.4 says that the distribution generated by fl:1 and by the neural network cannot be distinguished by a Lipschitz function. [ACB17] discuss why the class of Lipschitz functions is a good choice in comparison to other classes. For instance, if we maximize over the class of indicator functions (of measurable sets) instead, (8) becomes the total variation (TV) distance, which is unstable under perturbations to the function generating the distribution. In particular, the TV distance is discontinuous under perturbations of distributions supported on lower-dimensional subsets of the ambient space Rn."}, {"heading": "3.3 Proof of main theorem", "text": "To prove Theorem 3.1 we first prove the following theorem.\nTheorem 3.5. Keep conditions 1\u20134 and the notation of Theorem 3.1. Then there exists a neural network g with l hidden layers and S \u2282 Rm0 satisfying \u00b50(S) \u2265 1\u2212 \u00c4\u2211l\u22121 i=1 i 2 \u00e4 \u03b52 s2 so that\n\u00c5\u222b 1S \u2016fl:1 \u2212 g\u20162 d\u00b50 \u00e3 1 2 \u2264 l\u03b5 (10)\nProof. Let ri = \u2308 4C2i mi\n\u03b52\n\u2309 . We will show that we can take g = gl:1, where g1, . . . , gl are functions\ndefined by\ngi : R mi\u22121 \u2192 Rmi (11)\n(gi(x))j = cij0 + ri\u2211\nk=1\ncijk\u03c3(\u3008aijk, x\u3009+ bijk), (12)\nfor some parameters cijk, bijk \u2208 R, aijk \u2208 Rmi\u22121 . Note that each gi is a neural net with one hidden layer and a linear output layer. When the next layer gi+1 is applied to the output y of gi, first linear functions \u3008ai+1,j,k, y\u3009 + bi+1,j,k are applied; these linear functions can be collapsed with the linear output layer of gi. Thus only one hidden layer is added each time.\nWe prove the statement by induction on l. For l = 1, the theorem follows directly from Barron\u2019s Theorem 2.4, using assumptions 1 and 3.\nFor the induction step, assume we have functions g1, . . . , gl\u22121 satisfying the conclusion for f1, . . . , fl\u22121. Let Sl\u22121 be the set in the conclusion. Apply Barron\u2019s Theorem 2.4 to fl to get that that for each 1 \u2264 j \u2264 ml, for any \u00b5 supported on a set K \u2032l\u22121 \u2286 Rml\u22121 and any rl \u2208 N, there exists a neural net gl,j with 1 hidden layer with rl nodes such that\n\u00c5\u222b\nR ml\u22121\n[(fl)j \u2212 (gl)j]2 d\u00b5 \u00e3 1 2 \u2264 2Cfl,K \u2032l\u22121\u221a\nrl .\nNote it is vital here that Barron\u2019s Theorem applies to any distribution \u00b5 supported on K \u2032l\u22121. Let\nSl = Sl\u22121 \u2229 \u00b6 x : gl\u22121:1(x) \u2208 Kl\u22121 + sBml\u22121 \u00a9 . Apply Barron\u2019s Theorem with K \u2032l = Kl + sBml , rl =\u00b0\n4C2l ml \u03b52 \u00a7 . \u00b5 = gl\u22121:1\u2217(1Sl\u00b50).\n4 We have that \u00b5 is supported on gl\u22121:1(Sl) \u2286 Kl\u22121+sBml\u22121 = K \u2032l\u22121, as required, and fl is Cl-Barron on this set by assumption 3. (Note that \u00b5 is not a probability measure because it was restricted to the set gl\u22121:1(Sl), but it is a nonnegative measure with total L1 mass at most 1. Because Barron\u2019s Theorem holds for any probability measure, it also holds for these measures.) The conclusion of Barron\u2019s Theorem gives (gl)j such that\n\u00c5\u222b\nR ml\u22121\n[(fl)j \u2212 (gl)j]2 d(gl\u22121:1\u2217(1Sl\u00b50)) \u00e3 1\n2 \u2264 2Cl\u221a rl \u2264 \u03b5\u221a ml\n(13)\n=\u21d2 \u00c5\u222b\nR ml\u22121\n\u2016fl \u2212 gl\u20162 d(gl\u22121:1\u2217(1Sl\u00b50)) \u00e3 1 2 \u2264 \u03b5 (14)\nWe bound by the triangle inequality\n\u00c5\u222b\nRm 1Sl \u2016fl:1 \u2212 gl:1\u20162 d\u00b50\n\u00e3 1 2\n\u2264 \u00c5\u222b\nRm 1Sl \u2016fl \u25e6 fl\u22121:1 \u2212 fl \u25e6 gl\u22121:1\u20162 d\u00b50\n\u00e3 1 2\n+\n\u00c5\u222b\nRm 1Sl \u2016fl \u25e6 gl\u22121:1 \u2212 gl \u25e6 gl\u22121:1\u20162 d\u00b50\n\u00e3 1 2\n\u2264 \u00c5\u222b\nRm 1Sl \u2016fl \u25e6 fl\u22121:1 \u2212 fl \u25e6 gl\u22121:1\u20162 d\u00b50\n\u00e3 1 2\n+\n\u00c5\u222b\nR ml\u22121\n\u2016fl \u2212 gl\u20162 dgl\u22121:1\u2217(1Sl\u00b50) \u00e3 1 2\n\u2264 Lip(fl) \u00c5\u222b\nRm 1Sl \u2016(fl\u22121:1 \u2212 gl\u22121:1)\u20162 d\u00b50\n\u00e3 1 2\n+ \u03b5\n\u2264 Lip(fl) \u00c5\u222b\nRm 1Sl\u22121 \u2016(fl\u22121:1 \u2212 gl\u22121:1)\u20162 d\u00b50\n\u00e3 1 2\n+ \u03b5\n\u2264 1 \u00b7 (l \u2212 1)\u03b5 + \u03b5 = l\u03b5\nThe last inequality holds by assumption 2 and the induction hypothesis. To finish, we have to check that \u00b50(Sl) \u2265 1\u2212 \u00c4\u2211l\u22121 i=1 i 2 \u00e4 \u03b52 s2 . As above, we have that\n\u222b 1Sl\u22121 \u2016fl\u22121:1 \u2212 gl\u22121:1\u20162 d\u00b50 \u2264 (l \u2212 1)2\u03b52\nby the induction hypothesis. Also, fl\u22121:1(x) \u2208 Kl\u22121 for all x \u2208 Supp(\u00b50) by assumption 4. Thus by Markov\u2019s inequality and the induction hypothesis on Sl\u22121,\n\u00b50(Sl\u22121 \u2229 \u00b6 x : x 6\u2208 Kl\u22121 + sgl\u22121:1(Bml\u22121) \u00a9 )\n\u2264 \u00b50(Sl\u22121 \u2229 {x : \u2016fl\u22121:1(x)\u2212 gl\u22121:1(x)\u2016 \u2265 s}) \u2264 (l \u2212 1)2\u03b52\ns2\nTherefore \u00b50(Sl) \u2264 \u00b50(Sl\u22121)\u2212 (l\u22121) 2\u03b52 s2 \u2264 1\u2212 \u00c4\u2211l\u22121 i=1 i 2 \u00e4 \u03b52 s2 .\nIt is inelegant to have to exclude the sets Sl. The main theorem is a statement that doesn\u2019t involve the sets Sl. We achieve this by using the trivial bound on S c l .\n4The pushforward of a measure \u00b5 by a function f is denoted by f\u2217\u00b5 and defined by f\u2217\u00b5(S) = \u00b5(f \u22121(S)). Here,\ngl\u22121:1\u2217(1Sl\u00b50)(S) = \u00b50(g \u22121 l\u22121:1(S) \u2229 Sl).\nProof of Theorem 3.1. The functions g1, . . . , gl in Theorem 3.5 satisfy \u222b Sl \u2016fl:1 \u2212 gl:1\u20162 d\u00b50 \u2264 l2\u03b52.\nThe range of gl = ((gl)1, . . . , (gl)ml) is contained in a set of diameter 2Cl \u221a ml because the function \u03c3 has range contained in [0, 1] and Barron\u2019s Theorem gives functions (gl)j , 1 \u2264 j \u2264 ml, with\u2211r k=1 |cljk| \u2264 2Cl. Choose a constant vector k to minimize\n\u222b Sl \u2016fl:1(x)\u2212 gl:1(x)\u2212 k\u20162 d\u00b50 and replace gl with\ngl + k. Note that now, the range of gl and fl necessarily overlap; otherwise a further translation will decrease this error. We still have \u222b Sl \u2016fl:1 \u2212 gl:1\u20162 d\u00b50 \u2264 l2\u03b52. Moreover, \u2016gl(x)\u2212 fl(x)\u2016 \u2264\n2Cl \u221a ml +D for any x \u2208 K0.\nNow we have (using \u00b50(S c l ) \u2264 \u00c4\u2211l\u22121 i=1 i 2 \u00e4 \u03b52 s2 \u2264 l3\u03b52 3s2 )\n\u222b\nK0\n\u2016fl:1 \u2212 gl:1\u20162 d\u00b50 \u2264 \u222b\nSl\n\u2016fl:1 \u2212 gl:1\u20162 d\u00b50 + \u222b\nSc l\n\u2016fl:1 \u2212 gl:1\u20162 d\u00b50 (15)\n\u2264 l2\u03b52 + (2Cl \u221a ml +D) 2 l 3\u03b52\n3s2 . (16)\nTaking square roots gives the theorem."}, {"heading": "4 Separation between Barron functions and composition of Bar-", "text": "ron functions\nIn this section we produce an explicit function f : Rn \u2192 R that is a composition of two poly(n)Barron functions, but is not O(cn)-Barron for some c > 1.\nTheorem 4.1. For any n \u2261 3 (mod 4) and c > 1, there exists a function f and C2 > 0 such that\n1. (f is not Barron) Cf,C2nBn \u2265 cn.\n2. (f is the composition of 2 Barron functions) f = j \u25e6 k where for all r, s > 0, k : Rn \u2192 R is O(nr3)-Barron on rBn, and j : R \u2192 R is O(sn2)-Barron on sB1.\nThe condition n \u2261 3 (mod 4) is not necessary; we include it only to avoid case analysis. Note that this theorem gives a separation between Barron functions and compositions of Barron functions, and does not give a separation between distributions expressible by Barron functions and compositions of Barron functions. The analogous question for distributions is an open problem.\nWe will choose f to be a certain radial function f = f1(\u2016x\u2016) defined in Section 4.1.5 In order for f to have large Barron constant, it is necessary for \u222b Rn\n\u2016\u03c9\u20162 |f\u0302(\u03c9)| d\u03c9 to be large, i.e. for f\u0302 to have significant mass far away from the origin. We ensure this holds by choosing f to change sharply in the radial direction. This means f\u0302 has mass far away from the origin. Moreover, f\u0302 is radial because f is radial, so f\u0302 has significant mass in a large shell.\nHowever, lower-bounding \u222b Rn\n\u2016\u03c9\u20162 |f\u0302(\u03c9)| d\u03c9 is not sufficient because the definition of the Barron constant requires us to bound this quantity over all extensions of f .\nTo solve this problem, we give a technique to lower bound the Barron constant in Section 4.2 (Theorem 4.2). Although we cannot certify f is Barron by showing \u222b Rn \u2225\u2225\u2225\u2207\u0302f(\u03c9) \u2225\u2225\u2225 d\u03c9 = \u222b Rn\n\u2016\u03c9\u20162 |f\u0302(\u03c9)| d\u03c9 is large, it suffices to show \u222b Rn \u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 d\u03c9 is large for a judiciously chosen g. We use this to show that f is not Barron in Section E.1 (Theorem E.4).\n5For any radial function a : Rn \u2192 R, we write a1 : R \u2192 R for the function such that a(x) = a1(\u2016x\u2016).\nWe will see in Section 4.3 (Theorem 4.4) that f is a composition of two Barron functions x 7\u2192 \u2016x\u20162 and y 7\u2192 f1( \u221a y). The function x 7\u2192 \u2016x\u20162 is Barron because it is a polynomial. The function y 7\u2192 f1( \u221a y) is a function in 1 variable, and it is much easier for a 1-dimensional function h to be Barron as bounds on h, h\u2032, and h\u2032\u2032 suffice (Lemma A.6). Our result is similar to the construction in [ES15] of an explicit function that can be approximated by a 3-layer neural net but cannot be approximated (to better than constant error) by any 2-layer neural net with subexponential number of units. [ES15] use a different Fourier criterion in order to prove a certain function is not computable by a two-layer neural network.\nRoughly speaking, Eldan and Shamir implicitly show that for a specific probability measure that they chose (\u03d52, where \u03d5\u0302 = 1RnBn , where Rn is chosen so that Vol(RnBn) = 1), a necessary criterion for f to be approximated by a 2-layer neural network with k nodes is that most of its mass is concentrated in k \u201ctubes\u201d \u22c3k i=1(span{vi}+RnBn). (See [ES15, Proposition 13, Claim 15, Lemma 16].) The idea can be adapted to other measures. The main difference from Barron\u2019s Theorem is that their criterion is a necessary condition for approximability (so useful to show lower bounds), is measure-specific (rather than agnostic to the measure), and is more similar to a \u201csparsity\u201d condition than a \u201cL1 measure\u201d as in Barron\u2019s Theorem."}, {"heading": "4.1 Definition of f", "text": "Let f1 : R \u2192 R be a function such that f1 is nonnegative, Supp(f1) \u2286 [K1,K1+\u03b5], \u222b\u221e 0 f1(x) dx = 1,\nand |f (i)1 | = O \u00c4 1 \u03b5i+1 \u00e4 for all i = 0, 1, 2. This function exists by Lemma D.1(1). We will choose K1, \u03b5 depending on n. By Theorem A.5,\nf\u0302(\u03c9) = 1\n2\u03c0\n\u00c7 1\n2\u03c0 \u2016\u03c9\u2016\n\u00e5n 2 \u22121 \u222b \u221e\n0 r\nn 2 \u22121f1(r)Jn\n2 \u22121(\u2016\u03c9\u2016 r) dr. (17)\nWe will choose [K1,K1 + \u03b5] to be an interval on which Jn 2 (\u2016\u03c9\u2016 r) is large and positive for some large \u2016\u03c9\u2016. We use the notation of Lemma B.1. For x \u2265 n,\n(fn,xx) \u2032 = x\u221a x2 \u2212 \u00c4 n2\u22121 4\n\u00e4 \u2212 \u221a n2 \u2212 1 2 \u00b7 1\u221a 1\u2212 n2\u22121\n4x2\n\u00b7 \u2212 \u221a n2 \u2212 1 2x2 = \u00a0 1\u2212 n 2 \u2212 1 4x2 \u2208 [\u00a0 3 4 , 1 ] .\nLet K3 = C3 \u221a n for some C3 to be chosen. In every interval of length \u2265 4\u03c0\nK3 \u221a 3/4 there is an interval\nof length \u2265 \u03c0K3 on which\ncos \u00c7 \u2212(n+ 1)\u03c0\n4 + fd,K3rK3r\n\u00e5 \u2265 1\u221a\n2 . (18)\nLet [K1,K1 + \u03b5] be the first such interval with K1 \u2265 C1 \u221a n, where C1 is a constant to be chosen.\nNote we have K1 \u223c C1 \u221a n and \u03b5 = \u0398 \u00c4 1 K3 \u00e4 ."}, {"heading": "4.2 A technique to lower bound the Barron constant", "text": "The main difficulty in showing a function is not Barron is to lower bound the integral \u222b\nRn \u2016\u03c9\u2016 |\u201cF (\u03c9)| d\u03c9 =\n\u222b\nRn\n\u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 d\u03c9\nover all extensions F of f . In general, it is not known how to calculate the infimum over all extensions.\nTheorem 4.2 gives us a way to lower-bound the Barron constant for f over a ball rBn. The idea is the following. Instead of bounding \u222b Rn \u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 d\u03c9 for every extension F , we choose g with support in B and compute \u222b Rn \u2225\u2225\u2225\u25ca (\u2207F )g(\u03c9) \u2225\u2225\u2225 d\u03c9. This does not depend on the extension F because (\u2207F )g = (\u2207f)g. It turns out that we can bound \u222b Rn \u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 d\u03c9 in terms of \u222b Rn \u2225\u2225\u2225\u25ca (\u2207F )g(\u03c9) \u2225\u2225\u2225 d\u03c9.\nTheorem 4.2. If f is differentiable, then for any g such that Supp(g) \u2286 rBn and g, g\u0302 \u2208 L1(Rn),\nCf,rBn \u2265 r \u222b Rn |\u00f7(\u2207f)g(\u03c9)| d\u03c9\u222b Rn |g\u0302(\u03c9)| d\u03c9\nNote that g is a function that we are free to choose. To use the theorem we will choose g with Supp(g) \u2286 C2nBn and \u222b Rn\n|g\u0302(\u03c9)| d\u03c9 small. This theorem is similar to [Bar93, \u00a7IX.11], which bounds the Barron constant of a product of two functions. We defer the proof to Appendix E.\nTo use this bound for a function f , we need to judiciously choose the function g. Let b be the \u201cbump\u201d function given by Lemma D.1(3) for m = n+12 . This function has the properties that b(x) = 1 for x \u2208 [\u22121, 1], b(x) = 0 for |x| \u2265 2, and for k \u2264 m, b(k)(x) \u2264 (n + 1)k. Let g1(x) = b(K2)(x) = b \u00c4 x K2 \u00e4 and g(x) = g1(\u2016x\u2016) for K2 = C2n, where C2 is a constant to be chosen.\nIn Appendix E, we show the following lemma that bounds the Barron constant for f .\nLemma 4.3. For n \u2261 3 (mod 4) and constants C1, C2, C3 such that C1C3 \u2265 32 , C2 > C1 \u2265 1, C3 \u2265 1, the functions f, g we choose satisfy\n\u222b\nRn |g\u0302(\u03c9)| d\u03c9 = O((5eC2)\nn 2 ), (19)\n\u222b\nRn\n\u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 d\u03c9 = \u2126(C n 2 \u22123 1 C n 2 3 n \u2212 1 2 e n 2 ). (20)\nAs a result the Barron constant Cf,2K2Bn \u2265 \u2126 \u00c5 2\u2212nC n 2 \u22123 1 C n 2 3 C \u2212(n2\u22121) 2 n 1 2 \u00e3 .\nTherefore, as long as we choose C3 to be large enough this constant is exponentially large. The constraint that n \u2261 3 (mod 4) is only there to avoid case analysis. We give the proof in Section E."}, {"heading": "4.3 h is a composition of Barron functions", "text": "We can write f as the composition of a function that computes the square norm, and a one dimensional function. The Barron constant for both functions can be bounded by polynomials.\nLemma 4.4. Suppose that C1 < C3. f is the composition of the two functions\nx 7\u2192 \u2016x\u20162 Rn \u2192 R (21) y 7\u2192 f1( \u221a y) R \u2192 R. (22)\nThe function x 7\u2192 \u2016x\u20162 satisfies C\u2016x\u20162,rBn \u2264 O(nr 3) and the function y 7\u2192 f1(\n\u221a y) satisfies\nCf1( \u221a y),[\u2212s,s] = O(sC\n1 2 1 C 3 2 3 n 2) for any s.\nIntuitively, the proof uses the fact that polynomials are Barron, and all \u201cnice\u201d one dimensional functions are Barron. We leave the detailed proofs in Section E. Now it is easy to see the separation:\nof Theorem 4.1. By Lemma 4.3, we know we can choose C3 large enough so that the Barron constant for f is exponential. On the other hand, by Lemma 4.4 we know f is a composition of two Barron functions."}, {"heading": "5 Conclusion", "text": "In this paper we show if a generative model can be expressed as the composition of n Barron functions, then it can be approximated by a n+ 1-layer neural network. Along the way we proved a multi-layer version of the Barron\u2019s Theorem [Bar93], and a key observation is to use Wasserstein distance W 2 as the distance measure between distributions. This partly explains the expressive power of neural networks as generative models. However, there are still many open problems: what natural transformations can be represented by a composition of Barron functions? Is there a separation between composition of n Barron functions and composition of n+1 Barron functions? How can we learn such a representation efficiently? We hope this paper serves as a first step towards understanding the power of deep generative models."}, {"heading": "A Background from Fourier Analysis", "text": "The Fourier transform is defined in (5).\nTheorem A.1 (Fourier inversion). For continuous f such that f \u2208 L1(Rn) and f\u0302 \u2208 L1(Rn),\nf(x) = \u222b f\u0302(x)ei\u3008\u03c9,x\u3009 dx. = (2\u03c0)n \u02c6\u0302f\u2228\nTheorem A.2 (Plancherel\u2019s Theorem). For f, g : Rn \u2192 C such that f, g \u2208 L1(Rn) \u2229 L2(Rn), \u222b\nRn f(x)g(x) dx =\n\u222b\nRn (2\u03c0)nf\u0302(\u03c9)g\u0302(\u03c9) d\u03c9.\nTheorem A.3 (Fourier transform of derivative). For differentiable f : Rn \u2192 R, f \u2208 L1(Rn),\n\u2207\u0302f(x) = ixf\u0302(x).\nFor f : Rn \u2192 R such that f, \u2016x\u2016 f \u2208 L1(Rn),\n(xf)\u2227 = i\u2207f\u0302(x).\nTheorem A.4 (Fourier transform of convolution). For f, g \u2208 L1(Rn)\n\u2019f \u2217 g(x) = f\u0302(\u03c9)g\u0302(\u03c9) (23)\nFor f, g \u2208 L1(Rn) with fg, f\u0302 , g\u0302 \u2208 L1(Rn),\nf\u0302 g(x) = (f\u0302 \u2217 g\u0302)(\u03c9). (24)\nTheorem A.5 (Fourier transform of radial function). Suppose f(x) = f1(\u2016x\u2016) where f \u2208 L1(Rn), f : R\u22650 \u2192 R. Then\nf\u0302(\u03c9) = 1\n2\u03c0\n\u00c7 1\n2\u03c0 \u2016\u03c9\u2016\n\u00e5n 2 \u22121 \u222b \u221e\n0 r\nn 2 \u22121f1(r)Jn\n2 \u22121(\u2016\u03c9\u2016 r) dr.\nwhere J\u03b1 is the Bessel function of order \u03b1.\nLemma A.6 (L1 bound on Fourier transform).\n1. Let k \u2265 n+12 and k be even. Then for g : Rn \u2192 R that is k times differentiable,\n\u222b\nRn \u2016g\u0302(\u03c9)\u2016 d\u03c9 \u2264\n\u00d1 \u0393 \u00c4 1 2 \u00e4\n2n\u03c0 n 2 \u0393 \u00c4 n+1 2\n\u00e4 \u00e9 1 2 \u00c5\u222b\nRn [(I \u2212\u2206)k2 g(x)]2 dx\n\u00e3 1 2\n. (25)\n2. Let h : R \u2192 R be once or twice differentiable, respectively. Then \u222b \u221e\n\u2212\u221e |h\u0302(\u03c9)| d\u03c9 \u2264 2\u2212 12 \u00c5\u222b \u221e \u2212\u221e |h|2 + |h\u2032|2 dx \u00e3 1 2\n(26)\n\u222b \u221e\n\u2212\u221e |\u03c9h\u0302(\u03c9)| d\u03c9 \u2264 2\u2212 12 \u00c5\u222b \u221e \u2212\u221e |h\u2032|2 + |h\u2032\u2032|2 dx \u00e3 1 2 . (27)\nProof. By Cauchy-Schwarz and the fact that \u222b Rn\n1\n(1+\u2016\u03c9\u20162) n+1 2\nd\u03c9 = \u03c0\nn 2 \u0393( 12)\n\u0393(n+12 ) (this is used e.g. to\ndefine the Cauchy probability distribution)\n\u222b\nRn \u2016g\u0302(\u03c9)\u2016 d\u03c9 \u2264\n\u00d1\u222b\nRn 1 \u00c4 1 + \u2016\u03c9\u20162\n\u00e4k d\u03c9 \u00e9 1 2 \u00c5\u222b\nRn (1 + \u2016\u03c9\u20162)k|g\u0302(\u03c9)|2 d\u03c9\n\u00e3 1 2\n(28)\n\u2264\n\u00d6 \u222b\nRn 1 \u00c4 1 + \u2016\u03c9\u20162 \u00e4n+1 2 d\u03c9\n\u00e8 1 2 \u00c5\u222b\nRn (1 + \u2016\u03c9\u20162)k|g\u0302(\u03c9)|2 d\u03c9\n\u00e3 1 2\n(29)\n\u2264 \u00d1 \u03c0 n 2 \u0393 \u00c4 1 2 \u00e4\n\u0393 \u00c4 n+1 2\n\u00e4 \u00e9 1 2 \u00c5\u222b\nRn\n\u2223\u2223\u2223(1 + \u2016\u03c9\u20162)k2 g\u0302(\u03c9) \u2223\u2223\u2223 2 d\u03c9\n\u00e3 1 2\n(30)\n\u2264 \u00d1 \u03c0 n 2 \u0393 \u00c4 1 2 \u00e4\n\u0393 \u00c4 n+1 2\n\u00e4 \u00e9 1 2 (2\u03c0)\u2212 n 2 \u00c5\u222b\nRn [(I \u2212\u2206)k2 g(x)]2 dx\n\u00e3 1 2\n(31)\nwhere in the last step we used Theorem A.2 and the calculation\n\u2206\u0302g =\n( n\u2211\ni=1\n\u22022\n\u2202xi2 g\n)\u2227 = \u2212 n\u2211\ni=1\n\u03c92i g\u0302(\u03c9) = \u2212\u2016\u03c9\u20162 g\u0302(\u03c9).\nFor the second part, again by Cauchy-Schwarz and \u201ch\u2032(\u03c9) = i\u03c9h(\u03c9), \u222b \u221e\n\u2212\u221e |h\u0302(\u03c9)| d\u03c9 \u2264 \u00c7\u222b \u221e \u2212\u221e\n1 1 + |\u03c9|2 d\u03c9 \u222b \u221e \u2212\u221e |h\u0302(\u03c9)|2(1 + |\u03c9|2) d\u03c9 \u00e51 2\n(32)\n\u2264 \u221a \u03c0 \u00c5\u222b \u221e \u2212\u221e |h\u0302|2 + |\u201ch\u2032|2 d\u03c9 \u00e3 1 2\n(33)\n\u2264 \u221a \u03c0(2\u03c0)\u2212 1 2 \u00c5\u222b \u221e \u2212\u221e |h|2 + |h\u2032|2 dx \u00e3 1 2 . (34)\nThis gives the first equation. To get the second, replace h with h\u2032."}, {"heading": "B Bessel functions", "text": "We will need some facts about Bessel functions J\u03b1(x), \u03b1 \u2208 R. J\u03b1(x) has an oscillating shape like a damped sinusoid.\nLemma B.1 ([Kra14, Theorem 5], [ES15, Lemma 21]). If d \u2265 2 and x \u2265 d, then \u2223\u2223\u2223\u2223\u2223Jd/2(x)\u2212 \u221a 2\n\u03c0cd,xx cos\n\u00c7 \u2212(d+ 1)\u03c0\n4 + fd,xx \u00e5\u2223\u2223\u2223\u2223\u2223 \u2264 x \u22123/2,\nwhere\ncd,x =\n\u00a0 1\u2212 d\n2 \u2212 1 4x2 , fd,x = cd,x + \u221a d2 \u2212 1 2x arcsin \u00c7\u221a d2 \u2212 1 2x \u00e5 .\nMoreover, assuming x \u2265 d, 1 \u2265 cd,x \u2265 1\u2212 0.15 d\nx \u2265 0.85\nand\n1.3 \u2265 1 + 0.3 d x \u2265 fd,x \u2265 1\u2212 0.15 d x \u2265 0.85.\nLemma B.2 ([ES15, Lemma 20]). For any \u03b1 \u2265 1 and x \u2265 3\u03b1, J\u03b1(x) is 1-Lipschitz in x."}, {"heading": "C Properties of Wasserstein Distance", "text": "Lemma C.1 (Lemma 3.4 restated). For any two distributions \u00b5, \u03bd over Rn,\nW1(\u00b5, \u03bd) \u2264 W2(\u00b5, \u03bd). (35)\nMoreover, for any Lipschitz function f : Rn \u2192 R, \u2223\u2223\u2223\u2223 Ex\u223c\u00b5 f(x)\u2212 Ey\u223c\u03bd f(y) \u2223\u2223\u2223\u2223 \u2264 Lip(f)W1(\u00b5, \u03bd). (36)\nProof. Let \u03b3 \u2208 \u0393(\u00b5, \u03bd) be a coupling of \u00b5, \u03bd. Then by the Cauchy-Schwarz inequality,\nW1(\u00b5, \u03bd) \u2264 \u222b\nRn\u00d7Rn \u2016x\u2212 y\u20162 d\u03b3(x, y) (37)\n\u2264 \u00c5\u222b\nRn\u00d7Rn \u2016x\u2212 y\u201622 d\u03b3(x, y)\n\u00e3 1 2 \u00c5\u222b\nRn\u00d7Rn d\u03b3\n\u00e32\n\ufe38 \ufe37\ufe37 \ufe38 1\n. (38)\nThe infimum of (38) over all couplings \u03b3 \u223c \u0393(\u00b5, \u03bd) is exactly W2(\u00b5, \u03bd). This shows (35). Now for any \u03b3 \u2208 \u0393(\u00b5, \u03bd), because its marginals are \u00b5 and \u03bd,\n\u2223\u2223\u2223\u2223 Ex\u223c\u00b5 f(x)\u2212 Ey\u223c\u03bd f(y) \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 \u222b Rn\u00d7Rn f(x)\u2212 f(y) d\u03b3(x, y) \u2223\u2223\u2223\u2223 (39)\n\u2264 Lip(f) \u222b\nRn\u00d7Rn \u2016f(x)\u2212 f(y)\u20162 d\u03b3(x, y). (40)\nThe Lipschitz constant is with respect to the L2 norm because we use the L2 norm to measure the distance between f(x) and f(y). Taking the infimum of (40) gives (36).\nIn fact, (36) is sharp when \u00b5, \u03bd have bounded support. The duality theorem of Kantorovich and Rubinstein ([KR58]) says that\nW1(\u00b5, \u03bd) = sup \u00df E\nx\u223c\u00b5 f(x)\u2212 E y\u223c\u03bd f(y) : f : Rn \u2192 R,Lip(f) \u2264 1\n\u2122 ."}, {"heading": "D Test functions", "text": "For a function f , let f(K)(x) := f ( x K ) . Lemma D.1. Let m \u2265 2 be a given positive integer. 1. There exists a function g : R \u2192 R with the following properties.\n(a) g \u2265 0 everywhere. (b) Supp(g) \u2286 [0, 1]. (c) \u222b 1 0 g(x) dx = 1. (d) g is m times continuously differentiable and for all k \u2264 m, |g(k)(x)| = O((2m)k+1).\nThe function 1K g(K)(x) satisfies Supp(g(K)) \u2286 [0,K], \u222bK 0 g(K) dx = 1, and for k \u2264 m, g (k) (K)(x) = O (\u00c4 2m K \u00e4k+1) .\n2. There exists a function G : R \u2192 R with the following properties.\n(a) G is nondecreasing. (b) G(x) = 0 for x \u2264 0. (c) G(x) = 1 for x \u2265 1. (d) G is m+ 1 times continuously differentiable and for all k \u2264 m, G(k)(x) = O((2m)k).\n3. There exists a function b : R \u2192 R with the following properties:\n(a) Supp(b) \u2286 [\u22122, 2]. (b) b(x) = 1 for x \u2208 [\u22121, 1]. (c) b is is m+ 1 times continuously differentiable and for all k \u2264 m, b(k)(x) = O((2m)k).\nThe function b(K) satisfies Supp(b(K)) \u2286 [\u22122K, 2K], b(K)(x) = 1 for x \u2208 [\u2212K,K], and b (m) (K)(x) = O (\u00c4 2m K \u00e4k) .\nProof. Take\ng(x) =\n{ Cm4\nm+1xm+1(1\u2212 x)m+1, x \u2208 [0, 1] 0, else.\nwhere Cm is chosen so that \u222b 1 0 g(x) dx = 1. Note that x(1\u2212 x) \u2264 14 so g(x) \u2264 Cm and\n1 =\n\u222b 1\n0 g(x) dx \u2264 Cm (41)\n1 =\n\u222b 1\n0 g(x) dx \u2265\n\u222b 1 2 + 1 2 \u221a m\n1 2 \u2212 1 2 \u221a m\nCm4 m+1xm+1(1\u2212 x)m+1 dx (42)\n\u2265 1\u221a m Cm4 m+1\n\u00c7 1\n2 +\n1\n2 \u221a m\n\u00e5m+1 \u00c7 1\n2 \u2212 1 2 \u221a m\n\u00e5m+1 (43)\n\u2265 1\u221a m Cm\n\u00c5 1\u2212 1\nm\n\u00e3m+1 (44)\n\u2265 Cm 2e \u221a m\n(45)\nso 1 \u2264 Cm \u2264 2e \u221a m.\nNow, note that for functions u, v,\n(uv)(k) = k\u2211\nj=0\n\u00c7 k\nj\n\u00e5 u(j)v(k\u2212j). (46)\nApplying this to xm+1 and (1\u2212 x)m+1 and gives that for 0 \u2264 x \u2264 1, k \u2264 m,\n|g(k)(x)| \u2264 Cm \u221a m k\u2211\nj=0\n\u00c7 k\nj\n\u00e5 (m+ 1)j(m+ 1)k\u2212j (47)\n\u2264 O(m(2(m+ 1))k) (48) = O((2m)k+1). (49)\nFor the second part, take F (x) = \u222b x \u2212\u221e f(t) dt. The normalization \u222b 1 0 f(x) dx = 1 ensures\nF (x) = 1 for x \u2265 1, and for k \u2264 m, F (k+1)(x) = f (k)(x) = O((2m)k). For the third part, define\nb(x) =    0, |x| > 2 F (2\u2212 |x|), 1 \u2264 |x| \u2264 2 1, |x| < 1.\nFor the rescaled functions, just note that for any function f , f (k) (K)(x) = 1 Kk f (k) ( x K ) ."}, {"heading": "E Omitted Proofs in Section 4", "text": "Theorem E.1 (Theorem 4.2 restated). If f is differentiable, then for any g such that Supp(g) \u2286 rBn and g, g\u0302 \u2208 L1(Rn),\nCf,rBn \u2265 r \u222b Rn |\u00f7(\u2207f)g(\u03c9)| d\u03c9\u222b Rn\n|g\u0302(\u03c9)| d\u03c9 Proof. Let B = rBn. We have\nCf,B = inf F |B=f\n\u222b\nRn \u2016\u03c9\u2016B |\u201cF (\u03c9)| d\u03c9 (50)\n= r inf F |B=f\n\u222b\nRn \u2016\u03c9\u20162 |\u201cF (\u03c9)| d\u03c9 (51)\n= r inf F |B=f\n\u222b\nRn\n\u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 2 d\u03c9. (52)\nYoung\u2019s inequality and Theorem A.4 give \u222b\nRn\n\u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 2 d\u03c9\n\u222b\nRn |g\u0302(\u03c9)| d\u03c9 \u2265\n\u222b\nRn\n\u2225\u2225\u2225(\u2018\u2207F \u2217 g\u0302)(\u03c9) \u2225\u2225\u2225 2 d\u03c9 (53)\n=\n\u222b\nRn\n\u2225\u2225\u2225\u25ca (\u2207F )g(\u03c9) \u2225\u2225\u2225 2 d\u03c9 (54)\n=\n\u222b\nRn\n\u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 2 d\u03c9. (55)\nwhere the last step uses the fact that Supp(g) \u2286 rBn, so (\u2207F )g = (\u2207f)g. Then\n\u222b\nRn\n\u2225\u2225\u2225\u2018\u2207F (\u03c9) \u2225\u2225\u2225 2 d\u03c9 \u2265\n\u222b Rn \u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 2 d\u03c9\n\u222b Rn |g\u0302(\u03c9)| d\u03c9 . (56)\nE.1 f is not Barron\nIn this section we prove Lemma 4.3. We first prove the function g we choose gives a small denominator in the lowerbound equation.\nLemma E.2. For n \u2261 3 (mod 4), \u222b\nRn \u2016g\u0302(\u03c9)\u2016 d\u03c9 \u2264 O((5eC2)\nn 2 ).\nTo prove this we will need bound certain combinations of derivatives of a radial function.\nLemma E.3. Let f : Rn \u2192 Rn be a radial function with f(x) = f1(\u2016x\u2016). Then for k \u2208 N, 1 \u2264 k \u2264 n4 + 1,\n((I \u2212\u2206)kf)(x) = \u2211\n0 \u2264 i \u2264 2k, 0 \u2264 j \u2264 max{0, 2k \u2212 1} i+ j \u2264 2k\nci,jn jf (i) 1 (r)\nrj , r = \u2016x\u2016 (57)\nfor some ci,j with \u2211\ni,j |ci,j | \u2264 5k. Here, (I \u2212\u2206)f denotes f \u2212\u2206f .\nProof. We proceed by induction. The case k = 0 is just f(x) = f1(r). Suppose the statement is true for a given k \u2264 n4 ; we show it for k + 1. Let (I \u2212\u2206)kf be given by (57). We use the formula for the Laplacian of a radial function,\n\u2206f(x) = n\u2212 1 r f \u20321(r) + f \u2032\u2032 1 (r). (58)\nFor ease of notation, in the below the arguments of f and f1, which are x and r, are omitted. Then using (58) and the product rule,\n(I \u2212\u2206)k+1f = \u2211\n0 \u2264 i \u2264 2k, 0 \u2264 j \u2264 max{0, 2k \u2212 1} i+ j \u2264 2k\nci,jn j\n\u00c7 1\nrj f (i) 1 + n\u2212 1 r\n\u00c5 j\nrj+1 f (i) 1 \u2212\n1\nrj f (i+1) 1\n\u00e3 (59)\n+ \u00c7 \u2212j(j + 1)\nrj+2 f (i) 1 +\n2j\nrj+1 f (i+1) 1 \u2212\n1\nrj f (i+2) 1\n\u00e5\u00e5 (60)\nThe largest derivative of f1 increases by 2 and the power of r increases by 2, except when k = 0, when the power increases by 1 (from (58)). Write this as\n\u2211\n0 \u2264 i \u2264 2(k + 1), 0 \u2264 j \u2264 2k + 1 i+ j \u2264 2(k + 1)\nc\u2032i,jn jf (i) 1\nrj .\nA term is identified by the order f (i) that appears and the power 1 rj that appears. For example, the term ci,jn j n\u22121 r j rj+1 f (i) 1 = ci,jn j+2 (n\u22121)j n2 1 rj+2 f (i) 1 in (59) will contribute ci,j (n\u22121)j n2 to c \u2032 i,j+2. Noting k \u2264 n4 implies 2k \u2264 n2 , we have\n\u2211\ni,j\n|c\u2032i,j | \u2264 \u2211\n0 \u2264 i \u2264 2k, 0 \u2264 j \u2264 max{0, 2k \u2212 1} i+ j \u2264 2k\n|ci,j | \u00c7 1 +\n(n \u2212 1)j n2 + n\u2212 1 n + j(j + 1) n2 + 2j n + 1\n\u00e5 (61)\n\u2264 \u2211\ni,j\n|ci,j | \u00c5 1 + 1\n2 + 1 +\n1 4 + 1 + 1\n\u00e3 (62)\n\u2264 5 \u2211\ni,j\n|ci,j |. (63)\nThis completes the induction step and proves the theorem.\nProof of Lemma E.2. By Lemma A.6 with k = n+12 ,\n\u222b\nRn \u2016g\u0302(\u03c9)\u2016 d\u03c9 \u2264\n\u00d1 \u0393 \u00c4 1 2 \u00e4\n2n\u03c0 n 2 \u0393 \u00c4 n+1 2\n\u00e4 \u00e9 1 2 \u00c5\u222b\nRn [(I \u2212\u2206)n+14 g(x)]2 dx\n\u00e3 1 2\n. (64)\nNote [(I \u2212 \u2206)n+14 g(x)]2 is nonzero only on 2K2Bn. Then letting ci,j be as in Lemma E.3 with k = n+14 , we have\n(I \u2212\u2206)n+14 g(x) = \u2211\n0 \u2264 i \u2264 n+1 2 , 0 \u2264 j \u2264 n\u22121 2\ni+ j \u2264 n+1 2\nci,jn jg (i) 1 (r)\nrj , r = \u2016x\u2016 (65)\nWe separate out the one term g1(r), and bound the derivatives noting that g1 was defined using the bump function b(K2) in Lemma D.1. Note that g (i) 1 = 0 for r < K2, so we can take r \u2265 K2 in the sum.\n|(I \u2212\u2206)n+14 g(x)| \u2264 g1(r) + \u2211\n1 \u2264 i \u2264 n+1 2 , 0 \u2264 j \u2264 n\u22121 2\ni+ j \u2264 n+1 2\n|ci,j | nj|g(i)1 (r)|\nrj (66)\n\u2264 g1(r) + \u2211\n1 \u2264 i \u2264 n+1 2 , 0 \u2264 j \u2264 n\u22121 2\ni+ j \u2264 n+1 2\n|ci,j | njO\n( (n+1)i\n(C2n)i\n)\n(C2n)j (67)\n= O(4 n+1 4 ). (68)\n(69)\nNoting that the volume of 2K2Bn is \u03c0\nn 2\n\u0393(n2+1) (2K2)\nn,\n\u00c5\u222b\nRn [(I \u2212\u2206)n+14 g(x)]2 dx\n\u00e3 1 2\n= O\n\u00d1\u00c7 \u03c0 n 2\n\u0393 (n 2 + 1\n)(2K2)n ( 5 n+1 4\n)2\u00e5 1 2 \u00e9 (70)\n= O\n\u00d1\u00c7 \u03c0\nn 2 2nCn2 n n\n\u0393(n2 + 1)\n\u00e5 1 2\n5 n+1 4 \u00e9 . (71)\nCombining (64) and (71) and using Stirling\u2019s approximation \u0393(n+ 1) \u223c \u221a 2\u03c0n (n e )n gives\n\u222b\nRn \u2016g\u0302(\u03c9)\u2016 d\u03c9 \u2264 O\n\u00d6 C n 2\n2 n n 2 5 n+1 4\n\u0393 \u00c4 n+1 2 \u00e4 1 2 \u0393 (n 2 + 1 ) 1 2\n\u00e8\n(72)\n= O \u00c4 (5eC2) n 2 \u00e4 . (73)\nNow we are ready to bound the numerator and finish the proof.\nLemma E.4. For f defined as in Section 4.1, n \u2261 3 (mod 4), and constants C1, C2, C3 such that C1C3 \u2265 32 , C2 > C1 \u2265 1, C3 \u2265 1,\nCf,2K3Bn = \u2126\n\u00c5 2\u2212nC n 2 \u22123 1 C n 2 3 C \u2212(n2 \u22121) 2 n 1 2 \u00e3 .\nIn particular, this is exponentially large if we choose C3 large enough (i.e. if we make f vary sharply enough).\nProof. For \u2016\u03c9\u2016 = K3, by (17), (18), and Lemma B.1,\nf\u0302(\u03c9) = 1\n2\u03c0\n\u00c5 1\n2\u03c0K3\n\u00e3n 2 \u22121 \u222b K1+\u03b5\nK1\nr n 2 \u22121f1(r)Jn\n2 \u22121(K3r) dr (74)\n\u2265 1 2\u03c0\n\u00c5 1\n2\u03c0K3\n\u00e3n 2 \u22121 \u222b K1+\u03b5\nK1\nr n 2 \u22121f1(r)\n\u00c7\u00a0 2\n\u03c0K3r 1\u221a 2 \u2212 (K3r)\u2212 3 2\n\u00e5 dr (75)\n\u2265 1 2\u03c0\n\u00c5 K1\n2\u03c0K3\n\u00e3n\u22123 2 \u00a0 1\n\u03c0 (1\u2212 o(1)) (76)\nwhere in the last step we used \u222bK1+\u03b5 K1\nf1(r) = 1. Now we show that f\u0302 is also large for \u2016\u03c9\u2016 \u2248 K3. Let \u03c9, \u03c90 be such that \u2016\u03c90\u2016 = K3 and \u03c9 \u2265 \u03c90. Then using the fact that Jn\n2 \u22121 is 1-Lipschitz for\nx \u2265 3 (n2 \u2212 1 ) (Lemma B.2) and K3K1 \u2265 C3C1n \u2265 3n2 ,\n|f\u0302(\u03c9)\u2212 f\u0302(\u03c90)| \u2264 1\n2\u03c0\n\u00c5 1\n2\u03c0K3\n\u00e3n 2 \u22121 \u222b K1+\u03b5\nK1\nr n 2 \u22121f1(r)|Jn\n2 \u22121(\u2016\u03c9\u2016 r)\u2212 Jn 2 (K3r)| dr (77)\n\u2264 1 2\u03c0\n\u00c5 1\n2\u03c0K3\n\u00e3n 2 \u22121 \u222b K1+\u03b5\nK1\nr n 2 \u22121f1(r)r(\u2016\u03c9\u2016 \u2212K3) dr (78)\n\u2264 1 2\u03c0\n\u00c5 1\n2\u03c0K3\n\u00e3n 2 \u22121\n(K1 + \u03b5) n 2 (\u2016\u03c9\u2016 \u2212K3) (79)\n= O\n\u00c7\u00c5 K1\n2\u03c0K3\n\u00e3n 2 \u22121\nK 3 2 1 K 1 2 3 (\u2016\u03c9\u2016 \u2212K3) \u00e5\n(80)\nBy (76) and (80), for n \u2265 3, there exists \u03b4 such that for all \u2016\u03c9\u2016 \u2208 \u00ef K3,K3 +\n\u03b4\nK 3/2 1 K 1/2 3\n\u00f2 ,\n|f\u0302(\u03c9)| = \u2126 (\u00c5 K1\n2\u03c0K3\n\u00e3n\u22123 2 ) (81)\nThen using the fact that the surface area of a sphere in Rn is 2\u03c0 n 2\n\u0393(n2 ) ,\n\u222b\nRn \u2016\u03c9\u2016 |f\u0302(\u03c9)| d\u03c9 =\n\u222b\nK3\u2264\u2016\u03c9\u2016\u2264K3+ \u03b4 K\n3/2 1\n\u2126\n(\u00c5 K1\n2\u03c0K3\n\u00e3n\u22123 2 ) d\u03c9 (82)\n= \u2126\n( \u03c0 n 2\n\u0393 (n 2\n)Kn\u221213 \u03b4\nK 3/2 1 K 1/2 3\n\u00c5 K1\n2\u03c0K3\n\u00e3n\u22123 2 ) (83)\n= \u2126\n\u00c7 1\n\u0393 (n 2\n)K n 2 3 K n 2 \u22123 1 2 \u2212n 2 \u00e5 (84)\n= \u2126\n\u00c7\u00c5 2e\nn\u2212 2\n\u00e3n 2 \u22121\n(C3n 1 2 ) n 2 (C1n 1 2 ) n 2 \u221232\u2212 n 2 \u00e5 (85)\n= \u2126(C n 2 \u22123 1 C n 2 3 n \u2212 1 2 e n 2 ). (86)\nNote K2 = C2n > C1 \u221a n+ \u03b5 = K1 + \u03b5. Then g = 1 on the support of f , so (\u2207f)g = \u2207f and\n\u222b\nRn\n\u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 d\u03c9 =\n\u222b\nRn\n\u2225\u2225\u2225\u2207\u0302f(\u03c9) \u2225\u2225\u2225 d\u03c9 (87)\n= \u2126(C n 2 \u22123 1 C n 2 3 n \u2212 1 2 e n 2 ). (88)\nThen by Lemma E.2,\nCf,2K2Bn \u2265 2K2 \u222b Rn \u2225\u2225\u2225\u00f7(\u2207f)g(\u03c9) \u2225\u2225\u2225 d\u03c9\n\u222b Rn |g\u0302(\u03c9)| d\u03c9 (89)\n= 2K2 \u2126(C\nn 2 \u22123 1 C n 2 3 n \u2212 1 2 e n 2 )\nO((5eC2) n 2 )\n= \u2126 \u00c5 5\u2212 n 2 C n 2 \u22123 1 C n 2 3 C \u2212(n2\u22121) 2 n 1 2 \u00e3 . (90)\nE.2 h is a composition of Barron functions\nIn this section we proof Lemma 4.4. In order to do that, let us first define the following set of functions:\nDefinition E.5. Define\n\u0393(A,C) := \u00df f : Rn \u2192 R : \u222b\nRn |f\u0302(\u03c9)| d\u03c9 \u2264 A,\n\u222b\nRn \u2016\u03c9\u2016 |f\u0302(\u03c9)| d\u03c9 \u2264 C\n\u2122\nBarron functions have many nice properties:\nProposition E.6 (Properties of Barron constant). 1. (Subadditivity, [Bar93, \u00a7IV.3]) For any set B,\nC\u2211 i \u03b2ifi,B \u2264\n\u2211\ni\n|\u03b2i|Cfi,B .\n2. (Ridge functions, [Bar93, \u00a7IV.7]) Suppose f = h(\u3008a, x\u3009), where h : R \u2192 R is a 1-dimensional function and \u2016a\u20162 = 1. Then\nCf,rBn \u2264 Ch,[\u2212r,r].\n3. (Powers, [Bar93, \u00a7IV.12]) If g : R \u2192 R, g \u2208 \u0393(a, c), then g(x)k \u2208 \u0393(ak, kak\u22121c).\n4. The function f(x) = x has an extension h agreeing with x on [\u2212r, r], which satisfies h(x) \u2208 \u0393(O(r 3 2 ), O(r 1 2 )).\nProof. We show (4). Choose a bump function b as in Lemma D.1 for m = 2. Consider the extension h(x) = xb(r)(x) = xb (x r ) which is supported on [\u22122r, 2r]. Because b, b\u2032, b\u2032\u2032 are all bounded by a constant, on [\u22122r, 2r],\n|h(x)| \u2264 x (91)\n|h\u2032(x)| = |b(r)(x) + xb\u2032(r)(x)| \u2264 1 +O \u00c5 x\nr\n\u00e3 (92)\n|h\u2032\u2032(x)| = |2b\u2032(r)(x) + b\u2032\u2032(r)(x)| \u2264 O \u00c5 x\nr\n\u00e3 +O \u00c5 1\nr2\n\u00e3 . (93)\nThen by Lemma A.6(2),\n\u222b \u221e\n\u2212\u221e |h\u0302(\u03c9)| d\u03c9 \u2264 2\u2212 12 \u00c5\u222b r \u2212r |h(x)|2 + |h\u2032(x)|2 dx \u00e3 1\n2 \u2264 O(r 32 ) (94) \u222b \u221e\n\u2212\u221e |\u03c9h\u0302(\u03c9)| d\u03c9 \u2264 2\u2212 12 \u00c5\u222b r \u2212r |h\u2032(x)|2 + |h\u2032\u2032(x)|2 dx \u00e3 1 2 \u2264 O(r 12 ). (95)\nProof of Theorem 4.4. By Proposition E.6(4) and (3), the 1-dimensional function y 7\u2192 y2 has an extension k(y) with k(y) \u2208 \u0393(O(r3), O(r2)). Thus, Cy2,[\u2212r,r] \u2264 r \u222b\u221e \u2212\u221e \u2016\u03c9\u2016 |k\u0302(\u03c9)| d\u03c9 = O(r3).\nBecause x2i : R n \u2192 R is the composition of the projection x 7\u2192 \u3008ei, x\u3009 and the 1-dimensional\nfunction y 7\u2192 y2 and , by (2), Cx2i ,rBn \u2264 Cy2,[\u2212r,r] \u2264 O(r3) By (1), because \u2016x\u20162 = \u2211ni=1 x2i , C\u2016x\u20162,rBn \u2264 O(nr 3).\nNow consider the function h(y) := f1( \u221a y). We have, noting this is nonzero only for x \u2208\n[K21 , (K1 + \u03b5) 2], and f (i) 1 ( \u221a y) = O(Ki+13 ),\nh\u2032(y) = 1\n2y 1 2\nf1( \u221a y) + f \u20321( \u221a y) = O\n\u00c5\u00c5 K3\nK1\n\u00e3 +K23 \u00e3 (96)\nh\u2032\u2032(y) = 1\n4y 3 2\nf1( \u221a y) + 1\n4y f \u20321(\n\u221a y) + 1\n2y 1 2\nf \u2032\u20321 ( \u221a y) = O\n\u00c7 K3\nK31 + K23 K21 + K33 K1\n\u00e5 . (97)\nUsing C3 < C1 we have |h\u2032|2 + |h\u2032\u2032|2 = O(K43 ). Thus by Lemma A.6,\n\u222b \u221e\n0 |\u03c9h\u0302(\u03c9)| d\u03c9 =\n\u00c7\u222b (K1+\u03b5)2\nK2 1\nO \u00c4 K43 \u00e4\u00e5 12 = O (\u00c5 K1\nK3 O(K43 )\n\u00e3 1 2 ) = O \u00c5 K 1 2 1 K 3 2 3 \u00e3 .\nThus f1( \u221a x) is O(sC 1 2 1 C 3 2 3 n 2)-Barron on [\u2212s, s]."}], "references": [{"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "NIPS 2016 Workshop on Adversarial Training. In review for ICLR", "citeRegEx": "Arjovsky and Bottou.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky and Bottou.", "year": 2017}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "Arjovsky et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "Approximation and estimation bounds for artificial neural networks", "author": ["A.R. Barron"], "venue": "Machine Learning", "citeRegEx": "Barron.,? \\Q1994\\E", "shortCiteRegEx": "Barron.", "year": 1994}, {"title": "Representation learning: A review and new perspectives\u201d. In: IEEE transactions on pattern analysis and machine intelligence", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "arXiv preprint arXiv:1509.05009", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems (MCSS)", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Depth Separation for Neural Networks", "author": ["A. Daniely"], "venue": "arXiv preprint arXiv:1702.08489", "citeRegEx": "Daniely.,? \\Q2017\\E", "shortCiteRegEx": "Daniely.", "year": 2017}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani"], "venue": null, "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "The Power of Depth for Feedforward Neural Networks", "author": ["R. Eldan", "O. Shamir"], "venue": null, "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K.-I. Funahashi"], "venue": "Neural networks", "citeRegEx": "Funahashi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi.", "year": 1989}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow"], "venue": "Advances in neural information processing systems", "citeRegEx": "Goodfellow,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow", "year": 2014}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Detecting change in data streams", "author": ["D. Kifer", "S. Ben-David", "J. Gehrke"], "venue": "Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. VLDB Endowment", "citeRegEx": "Kifer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2004}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns"], "venue": "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing", "citeRegEx": "Kearns,? \\Q1994\\E", "shortCiteRegEx": "Kearns", "year": 1994}, {"title": "On a space of completely additive functions", "author": ["L.V. Kantorovich", "G.S. Rubinstein"], "venue": "Vestnik Leningrad. Univ", "citeRegEx": "Kantorovich and Rubinstein.,? \\Q1958\\E", "shortCiteRegEx": "Kantorovich and Rubinstein.", "year": 1958}, {"title": "Approximations for the Bessel and Airy functions with an explicit error term", "author": ["I. Krasikov"], "venue": "LMS Journal of Computation and Mathematics", "citeRegEx": "Krasikov.,? \\Q2014\\E", "shortCiteRegEx": "Krasikov.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits", "author": ["D.M. Kane", "R. Williams"], "venue": "Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing", "citeRegEx": "Kane and Williams.,? \\Q2016\\E", "shortCiteRegEx": "Kane and Williams.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep Learning in Neural Networks: An Overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution\u2014also theoretically not understood\u2014concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with n hidden layers. A key ingredient is Barron\u2019s Theorem [Bar93], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (\u201cBarron functions\u201d) can be approximated by a n+ 1layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance\u2014a natural metric on probability distributions\u2014by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.", "creator": "LaTeX with hyperref package"}}}