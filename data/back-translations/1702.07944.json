{"id": "1702.07944", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "abstract": "In this paper, we focus on policy evaluation with linear functional approximation over a fixed dataset. First, we transform the empirical problem of policy evaluation into a (square) convex saddle point problem and then present a method of primary and dual gradients and two stochastic methods of reducing variance to solve the problem. These algorithms scale linearly in both sample size and trait dimension. Furthermore, they achieve linear convergence even when the saddle point problem has only strong concavity in the dual variables but no strong convexity in the primordial variables. Numerical experiments on benchmark problems show the effectiveness of our methods.", "histories": [["v1", "Sat, 25 Feb 2017 20:15:55 GMT  (636kb,D)", "https://arxiv.org/abs/1702.07944v1", null], ["v2", "Fri, 9 Jun 2017 06:02:47 GMT  (641kb,D)", "http://arxiv.org/abs/1702.07944v2", "Accepted by ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY math.OC stat.ML", "authors": ["simon s du", "jianshu chen", "lihong li", "lin xiao", "dengyong zhou"], "accepted": true, "id": "1702.07944"}, "pdf": {"name": "1702.07944.pdf", "metadata": {"source": "META", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "authors": ["Simon S. Du", "Jianshu Chen", "Lihong Li", "Lin Xiao", "Dengyong Zhou"], "emails": ["<ssdu@cs.cmu.edu>,", "shuc@microsoft.com>,", "<lihongli@microsoft.com>,", "<lin.xiao@microsoft.com>,", "zho@microsoft.com>."], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) is a powerful learning paradigm for sequential decision making (see, e.g., Bertsekas & Tsitsiklis, 1995; Sutton & Barto, 1998). An RL agent interacts with the environment by repeatedly observing the current state, taking an action according to a certain policy, receiving a reward signal and transitioning to a next state. A policy specifies which action to take given the current state. Policy evaluation estimates a value function that predicts expected cumulative reward the agent would receive by following a fixed policy starting at a certain state. In addition to quantifying long-term values of states, which can be of interest on its own, value functions also provide\n1Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania 15213, USA. 2Microsoft Research, Redmond, Washington 98052, USA.. Correspondence to: Simon S. Du <ssdu@cs.cmu.edu>, Jianshu Chen <jianshuc@microsoft.com>, Lihong Li <lihongli@microsoft.com>, Lin Xiao <lin.xiao@microsoft.com>, Dengyong Zhou <denzho@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nimportant information for the agent to optimize its policy. For example, policy-iteration algorithms iterate between policy-evaluation steps and policy-improvement steps, until a (near-)optimal policy is found (Bertsekas & Tsitsiklis, 1995; Lagoudakis & Parr, 2003). Therefore, estimating the value function efficiently and accurately is essential in RL.\nThere has been substantial work on policy evaluation, with temporal-difference (TD) methods being perhaps the most popular. These methods use the Bellman equation to bootstrap the estimation process. Different cost functions are formulated to exploit this idea, leading to different policy evaluation algorithms; see Dann et al. (2014) for a comprehensive survey. In this paper, we study policy evaluation by minimizing the mean squared projected Bellman error (MSPBE) with linear approximation of the value function. We focus on the batch setting where a fixed, finite dataset is given. This fixed-data setting is not only important in itself (Lange et al., 2011), but also an important component in other RL methods such as experience replay (Lin, 1992).\nThe finite-data regime makes it possible to solve policy evaluation more efficiently with recently developed fast optimization methods based on stochastic variance reduction, such as SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014). For minimizing strongly convex functions with a finite-sum structure, such methods enjoy the same low computational cost per iteration as the classical stochastic gradient method, but also achieve fast, linear convergence rates (i.e., exponential decay of the optimality gap in the objective). However, they cannot be applied directly to minimize the MSPBE, whose objective does not have the finite-sum structure. In this paper, we overcome this obstacle by transforming the empirical MSPBE problem to an equivalent convex-concave saddle-point problem that possesses the desired finite-sum structure.\nIn the saddle-point problem, we consider the model parameters as the primal variables, which are coupled with the dual variables through a bilinear term. Moreover, without an `2-regularization on the model parameters, the objective is only strongly concave in the dual variables, but not in the primal variables. We propose a primal-dual batch gradient method, as well as two stochastic variance-reduction methods based on SVRG and SAGA, respectively. Surprisingly, we show that when the coupling matrix is full rank, these algorithms achieve linear convergence in both the primal\nar X\niv :1\n70 2.\n07 94\n4v 2\n[ cs\n.L G\n] 9\nJ un\n2 01\n7\nand dual spaces, despite the lack of strong convexity of the objective in the primal variables. Our results also extend to off-policy learning and TD with eligibility traces (Sutton & Barto, 1998; Precup et al., 2001).\nWe note that Balamurugan & Bach (2016) have extended both SVRG and SAGA to solve convex-concave saddlepoint problems with linear-convergence guarantees. The main difference between our results and theirs are\n\u2022 Linear convergence in Balamurugan & Bach (2016) relies on the assumption that the objective is strongly convex in the primal variables and strongly concave in the dual. Our results show, somewhat surprisingly, that only one of them is necessary if the primal-dual coupling is bilinear and the coupling matrix is full rank. In fact, we are not aware of similar previous results even for the primal-dual batch gradient method, which we show in this paper. \u2022 Even if a strongly convex regularization on the primal variables is introduced to the MSPBE objective, the algorithms in Balamurugan & Bach (2016) cannot be applied efficiently. Their algorithms require that the proximal mappings of the strongly convex and concave regularization functions be computed efficiently. In our saddle-point formulation, the strong concavity of the dual variables comes from a quadratic function defined by the feature covariance matrix, which cannot be inverted efficiently and makes the proximal mapping costly to compute. Instead, our algorithms only use its (stochastic) gradients and hence are much more efficient.\nWe compare various gradient based algorithms on a Random MDP and Mountain Car data sets. The experiments demonstrate the effectiveness of our proposed methods."}, {"heading": "2 Preliminaries", "text": "We consider a Markov Decision Process (MDP) (Puterman, 2005) described by (S,A,Pass\u2032 ,R, \u03b3), where S is the set of states, A the set of actions, Pass\u2032 the transition probability from state s to state s\u2032 after taking action a,R (s, a) the reward received after taking action a in state s, and \u03b3 \u2208 [0, 1) a discount factor. The goal of an agent is to find an actionselection policy \u03c0, so that the long-term reward under this policy is maximized. For ease of exposition, we assume S is finite, but none of our results relies on this assumption.\nA key step in many algorithms in RL is to estimate the value function of a given policy \u03c0, defined as V \u03c0(s) , E[ \u2211\u221e t=0 \u03b3\ntR(st, at)|s0 = s, \u03c0]. Let V \u03c0 denote a vector constructed by stacking the values of V \u03c0(1), . . . , V \u03c0(|S|) on top of each other. Then V \u03c0 is the unique fixed point of the Bellman operator T\u03c0:\nV \u03c0 = T\u03c0V \u03c0 , R\u03c0 + \u03b3P\u03c0V \u03c0 , (1)\nwhere R\u03c0 is the expected reward vector under policy \u03c0, defined elementwise as R\u03c0(s) = E\u03c0(a|s)R(s, a); and P\u03c0 is the transition matrix induced by the policy applying \u03c0, defined entrywise as P\u03c0(s, s\u2032) = E\u03c0(a|s)Pass\u2032 ."}, {"heading": "2.1 Mean squared projected Bellman error (MSPBE)", "text": "One approach to scale up when the state space size |S| is large or infinite is to use a linear approximation for V \u03c0 . Formally, we use a feature map \u03c6 : S \u2192 Rd and approximate the value function by V\u0302 \u03c0 (s) = \u03c6(s)T \u03b8, where \u03b8 \u2208 Rd is the model parameter to be estimated. Here, we want to find \u03b8 that minimizes the mean squared projected Bellman error, or MSPBE:\nMSPBE (\u03b8) , 1\n2 \u2016V\u0302 \u03c0 \u2212\u03a0T\u03c0V\u0302 \u03c0\u20162\u039e, (2)\nwhere \u039e is a diagonal matrix with diagonal elements being the stationary distribution over S induced by the policy \u03c0, and \u03a0 is the weighted projection matrix onto the linear space spanned by \u03c6(1), . . . , \u03c6(|S|), that is,\n\u03a0 = \u03a6(\u03a6T\u039e\u03a6)\u22121\u03a6T\u039e (3)\nwhere \u03a6 , [\u03c6T (1), . . . , \u03c6T (|S|)] is the matrix obtained by stacking the feature vectors row by row. Substituting (3) and (1) into (2), we obtain (see, e.g., Dann et al., 2014)\nMSPBE(\u03b8) = 1\n2 \u2016\u03a6T\u039e(V\u0302 \u03c0 \u2212 T\u03c0V\u0302 \u03c0)\u20162(\u03a6T\u039e\u03a6)\u22121 .\nWe can further rewrite the above expression for MSPBE as a standard weighted least-squares problem:\nMSPBE(\u03b8) = 1\n2 \u2016A\u03b8 \u2212 b\u20162C\u22121 ,\nwith properly defined A, b and C, described as follows. Suppose the MDP under policy \u03c0 settles at its stationary distribution and generates an infinite transition sequence {(st, at, rt, st+1)}\u221et=1, where st is the current state, at is the action, rt is the reward, and st+1 is the next state. Then with the definitions \u03c6t , \u03c6(st) and \u03c6\u2032t , \u03c6(st+1), we have\nA = E[\u03c6t(\u03c6t \u2212 \u03b3\u03c6\u2032t)T ], b = E[\u03c6trt], C = E[\u03c6t\u03c6Tt ], (4)\nwhere E[\u00b7] are with respect to the stationary distribution. Many TD solutions converge to a minimizer of MSPBE in the limit (Tsitsiklis & Van Roy, 1997; Dann et al., 2014)."}, {"heading": "2.2 Empirical MSPBE", "text": "In practice, quantities in (4) are often unknown, and we only have access to a finite dataset with n transitions D = {(st, at, rt, st+1)}nt=1. By replacing the unknown statistics with their finite-sample estimates, we obtain the Empirical MSPBE, or EM-MSPBE. Specifically, let\nA\u0302 , 1\nn n\u2211 t=1 At, b\u0302 , 1 n n\u2211 t=1 bt, C\u0302 , 1 n n\u2211 t=1 Ct, (5)\nwhere for t = 1, . . . , n,\nAt , \u03c6t(\u03c6t \u2212 \u03b3\u03c6\u2032t)T , bt , rt\u03c6t, Ct , \u03c6t\u03c6Tt . (6)\nEM-MSPBE with an optional `2-regularization is given by:\nEM-MSPBE (\u03b8) = 1\n2 \u2016A\u0302\u03b8 \u2212 b\u0302\u20162 C\u0302\u22121 + \u03c1 2 \u2016\u03b8\u20162, (7)\nwhere \u03c1 \u2265 0 is a regularization factor. Observe that (7) is a (regularized) weighted least squares problem. Assuming C\u0302 is invertible, its optimal solution is\n\u03b8? = (A\u0302>C\u0302\u22121A\u0302+ \u03c1I)\u22121A\u0302>C\u0302\u22121b\u0302. (8)\nComputing \u03b8? directly requires O(nd2) operations to form the matrices A\u0302, b\u0302 and C\u0302, and then O(d3) operations to complete the calculation. This method, known as leastsquares temporal difference or LSTD (Bradtke & Barto, 1996; Boyan, 2002), can be very expensive when n and d are large. One can also skip forming the matrices explicitly and compute \u03b8? using n recusive rank-one updates (Nedic\u0301 & Bertsekas, 2003). Since each rank-one update costs O(d2), the total cost is O(nd2).\nIn the sequel, we develop efficient algorithms to minimize EM-MSPBE by using stochastic variance reduction methods, which samples one (\u03c6t, \u03c6\u2032t) per update without precomputing A\u0302, b\u0302 and C\u0302. These algorithms not only maintain a low O(d) per-iteration computation cost, but also attain fast linear convergence rates with a log(1/ ) dependence on the desired accuracy ."}, {"heading": "3 Saddle-Point Formulation of EM-MSPBE", "text": "Our algorithms (in Section 5) are based on the stochastic variance reduction techniques developed for minimizing a finite sum of convex functions, more specifically, SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014). They deal with problems of the form\nmin x\u2208Rd\n{ f(x) , 1\nn n\u2211 i=1 fi(x) } , (9)\nwhere each fi is convex. We immediately notice that the EM-MSPBE in (7) cannot be put into such a form, even though the matrices A\u0302, b\u0302 and C\u0302 have the finite-sum structure given in (5). Thus, extending variance reduction techniques to EM-MSPBE minimization is not straightforward.\nNevertheless, we will show that the minimizing the EMMSPBE is equivalent to solving a convex-concave saddlepoint problem which actually possesses the desired finitesum structure. To proceed, we resort to the machinery of conjugate functions (e.g. Rockafellar, 1970, Section 12). For a function f : Rd \u2192 R, its conjugate function f? :\nRd \u2192 R is defined as f?(y) , supx(yTx \u2212 f(x)). Note that the conjugate function of 12\u2016x\u2016 2 C\u0302 is 12\u2016y\u2016 2 C\u0302\u22121 , i.e.,\n1 2 \u2016y\u20162 C\u0302\u22121 = max x\n( yTx\u2212 1\n2 \u2016x\u20162 C\u0302\n) .\nWith this relation, we can rewrite EM-MSPBE in (7) as\nmax w\n( wT (\u0302b\u2212 A\u0302\u03b8)\u2212 1\n2 \u2016w\u20162 C\u0302\n) + \u03c1\n2 \u2016\u03b8\u20162 ,\nso that minimizing EM-MSPBE is equivalent to solving\nmin \u03b8\u2208Rd max w\u2208Rd\n{ L(\u03b8, w) = 1\nn n\u2211 t=1 Lt(\u03b8, w) } , (10)\nwhere the Lagrangian, defined as\nL(\u03b8, w) , \u03c1 2 \u2016\u03b8\u20162 \u2212 wT A\u0302\u03b8 \u2212 (1 2 \u2016w\u20162 C\u0302 \u2212 wT b\u0302 ) , (11)\nmay be decomposed using (5), with\nLt(\u03b8, w) , \u03c1\n2 \u2016\u03b8\u20162 \u2212 wTAt\u03b8 \u2212 (1 2 \u2016w\u20162Ct \u2212 w T bt ) .\nTherefore, minimizing the EM-MSPBE is equivalent to solving the saddle-point problem (10), which is convex in the primal variable \u03b8 and concave in the dual variable w. Moreover, it has a finite-sum structure similar to (9).\nLiu et al. (2015) and Valcarcel Macua et al. (2015) independently showed that the GTD2 algorithm (Sutton et al., 2009b) is indeed a stochastic gradient method for solving the saddle-point problem (10), although they obtained the saddle-point formulation with different derivations. More recently, Dai et al. (2016) used the conjugate function approach to obtain saddle-point formulations for a more general class of problems and derived primal-dual stochastic gradient algorithms for solving them. However, these algorithms have sublinear convergence rates, which leaves much room to improve when applied to problems with finite datasets. Recently, Lian et al. (2017) developed SVRG methods for a general finite-sum composition optimization that achieve linear convergence rate. Different from our methods, their stochastic gradients are biased and they have worse dependency on the condition numbers (\u03ba3 and \u03ba4).\nThe fast linear convergence of our algorithms presented in Sections 4 and 5 requires the following assumption:\nAssumption 1. A\u0302 has full rank, C\u0302 is strictly positive definite, and the feature vector \u03c6t is uniformly bounded.\nUnder mild regularity conditions (e.g., Wasserman, 2013, Chapter 5), we have A\u0302 and C\u0302 converge in probability to A andC defined in (4), respectively. Thus, if the true statistics A is non-singular and C is positive definite, and we have enough training samples, these assumptions are usually satisfied. They have been widely used in previous works on gradient-based algorithms (e.g., Sutton et al., 2009a;b).\nA direct consequence of Assumption 1 is that \u03b8? in (8) is the unique minimizer of the EM-MSPBE in (7), even without any strongly convex regularization on \u03b8 (i.e., even if \u03c1 = 0). However, if \u03c1 = 0, then the Lagrangian L(\u03b8, w) is only strongly concave inw, but not strongly convex in \u03b8. In this case, we will show that non-singularity of the coupling matrix A\u0302 can \u201cpass\u201d an implicit strong convexity on \u03b8, which is exploited by our algorithms to obtain linear convergence in both the primal and dual spaces."}, {"heading": "4 A Primal-Dual Batch Gradient Method", "text": "Before diving into the stochastic variance reduction algorithms, we first present Algorithm 1, which is a primal-dual batch gradient (PDBG) algorithm for solving the saddlepoint problem (10). In Step 2, the vector B(\u03b8, w) is obtained by stacking the primal and negative dual gradients:\nB (\u03b8, w) , [ \u2207\u03b8L(\u03b8, w) \u2212\u2207wL(\u03b8, w) ] = [ \u03c1\u03b8 \u2212 A\u0302Tw A\u0302\u03b8 \u2212 b\u0302+ C\u0302w ] . (12)\nSome notation is needed in order to characterize the convergence rate of Algorithm 1. For any symmetric and positive definite matrix S, let \u03bbmax(S) and \u03bbmin(S) denote its maximum and minimum eigenvalues respectively, and define its condition number to be \u03ba(S) , \u03bbmax(S)/\u03bbmin(S). We also define L\u03c1 and \u00b5\u03c1 for any \u03c1 \u2265 0:\nL\u03c1 , \u03bbmax(\u03c1I + A\u0302 T C\u0302\u22121A\u0302), (13)\n\u00b5\u03c1 , \u03bbmin(\u03c1I + A\u0302 T C\u0302\u22121A\u0302). (14)\nBy Assumption 1, we have L\u03c1 \u2265 \u00b5\u03c1 > 0. The following theorem is proved in Appendix B.\nTheorem 1. Suppose Assumption 1 holds and let (\u03b8?, w?) be the (unique) solution of (10). If the step sizes are chosen as \u03c3\u03b8 = 19L\u03c1\u03ba(C\u0302) and \u03c3w = 8 9\u03bbmax(C\u0302) , then the number of iterations of Algorithm 1 to achieve \u2016\u03b8 \u2212 \u03b8?\u20162 + \u2016w \u2212 w?\u20162 \u2264 2 is upper bounded by\nO ( \u03ba ( \u03c1I + A\u0302T C\u0302\u22121A\u0302 ) \u00b7 \u03ba(C\u0302) \u00b7 log (1 )) . (15)\nWe assigned specific values to the step sizes \u03c3\u03b8 and \u03c3w for clarity. In general, we can use similar step sizes while keeping their ratio roughly constant as \u03c3w\u03c3\u03b8 \u2248 8L\u03c1 \u03bbmin(C\u0302) ; see Appendices A and B for more details. In practice, one can use a parameter search on a small subset of data to find reasonable step sizes. It is an interesting open problem how to automatically select and adjust step sizes.\nNote that the linear rate is determined by two parts: (i) the strongly convex regularization parameter \u03c1, and (ii) the positive definiteness of A\u0302T C\u0302\u22121A\u0302. The second part could be interpreted as transferring strong concavity in dual variables via the full-rank bi-linear coupling matrix A\u0302. For\nAlgorithm 1 PDBG for Policy Evaluation Inputs: initial point (\u03b8, w), step sizes \u03c3\u03b8 and \u03c3w, and\nnumber of epochs M . 1: for i = 1 to M do\n2: [ \u03b8 w ] \u2190 [ \u03b8 w ] \u2212 [ \u03c3\u03b8 0 0 \u03c3w ] B(\u03b8, w)\nwhere B(\u03b8, w) is computed according to (12). 3: end for\nthis reason, even if the saddle-point problem (10) has only strong concavity in dual variables (when \u03c1 = 0), the algorithm still enjoys a linear convergence rate.\nMoreover, even if \u03c1 > 0, it will be inefficient to solve problem (10) using primal-dual algorithms based on proximal mappings of the strongly convex and concave terms (e.g., Chambolle & Pock, 2011; Balamurugan & Bach, 2016). The reason is that, in (10), the strong concavity of the Lagrangian with respect to the dual lies in the quadratic function (1/2)\u2016w\u2016C\u0302 , whose proximal mapping cannot be computed efficiently. In contrast, the PDBG algorithm only needs its gradients.\nIf we pre-compute and store A\u0302, b\u0302 and C\u0302, which costs O(nd2) operations, then computing the gradient operator B(\u03b8, w) in (12) during each iteration of PDBG costsO(d2) operations. Alternatively, if we do not want to store these d \u00d7 d matrices (especially if d is large), then we can compute B(\u03b8, w) as finite sums on the fly. More specifically, B(\u03b8, w) = 1n \u2211n t=1Bt(\u03b8, w), where for each t = 1, . . . , n,\nBt(\u03b8, w) =\n[ \u03c1\u03b8 \u2212Atw\nAt\u03b8 \u2212 bt + Ctw\n] . (16)\nSince At, bt and Ct are all rank-one matrices, as given in (6), computing each Bt(\u03b8, w) only requires O(d) operations. Therefore, computing B(\u03b8, w) costs O(nd) operations as it averages Bt(\u03b8, w) over n samples."}, {"heading": "5 Stochastic Variance Reduction Methods", "text": "If we replace B(\u03b8, w) in Algorithm 1 (line 2) by the stochastic gradient Bt(\u03b8, w) in (16), then we recover the GTD2 algorithm of Sutton et al. (2009b), applied to a fixed dataset, possibly with multiple passes. It has a low periteration cost but a slow, sublinear convergence rate. In this section, we provide two stochastic variance reduction methods and show they achieve fast linear convergence."}, {"heading": "5.1 SVRG for policy evaluation", "text": "Algorithm 2 is adapted from the stochastic variance reduction gradient (SVRG) method (Johnson & Zhang, 2013). It uses two layers of loops and maintains two sets of parameters (\u03b8\u0303, w\u0303) and (\u03b8, w). In the outer loop, the algorithm computes a full gradient B(\u03b8\u0303, w\u0303) using (\u03b8\u0303, w\u0303), which takes\nAlgorithm 2 SVRG for Policy Evaluation Inputs: initial point (\u03b8, w), step sizes {\u03c3\u03b8, \u03c3w}, number of\nouter iterations M , and number of inner iterations N . 1: for m = 1 to M do 2: Initialize (\u03b8\u0303, w\u0303) = (\u03b8, w) and compute B(\u03b8\u0303, w\u0303). 3: for j = 1 to N do 4: Sample an index tj from {1, \u00b7 \u00b7 \u00b7 , n} and do 5: Compute Btj (\u03b8, w) and Btj (\u03b8\u0303, w\u0303).\n6:\n[ \u03b8 w ] \u2190 [ \u03b8 w ] \u2212 [ \u03c3\u03b8 0 0 \u03c3w ] Btj (\u03b8, w, \u03b8\u0303, w\u0303)\nwhere Btj (\u03b8, w, \u03b8\u0303, w\u0303) is given in (17). 7: end for 8: end for\nAlgorithm 3 SAGA for Policy Evaluation Inputs: initial point (\u03b8, w), step sizes \u03c3\u03b8 and \u03c3w, and\nnumber of iterations M . 1: Compute each gt = Bt(\u03b8, w) for t = 1, . . . , n. 2: Compute B = B(\u03b8, w) = 1n \u2211n t=1 gt. 3: for m = 1 to M do 4: Sample an index tm from {1, \u00b7 \u00b7 \u00b7 , n}. 5: Compute htm = Btm(\u03b8, w).\n6: [ \u03b8 w ] \u2190 [ \u03b8 w ] \u2212 [ \u03c3\u03b8 0 0 \u03c3w ] (B + htm \u2212 gtm). 7: B \u2190 B + 1n (htm \u2212 gtm) 8: gtm \u2190 htm . 9: end for\nO(nd) operations. Afterwards, the algorithm executes the inner loop, which randomly samples an index tj and updates (\u03b8, w) using variance-reduced stochastic gradient:\nBtj(\u03b8, w, \u03b8\u0303, w\u0303) = Btj(\u03b8, w) +B(\u03b8\u0303, w\u0303)\u2212Btj(\u03b8\u0303, w\u0303). (17)\nHere, Btj (\u03b8, w) contains the stochastic gradients at (\u03b8, w) computed using the random sample with index tj , and B(\u03b8\u0303, w\u0303) \u2212 Btj (\u03b8\u0303, w\u0303) is a term used to reduce the variance in Btj (\u03b8, w) while keeping Btj(\u03b8, w, \u03b8\u0303, w\u0303) an unbiased estimate of B(\u03b8, w).\nSince B(\u03b8\u0303, w\u0303) is computed once during each iteration of the outer loop with cost O(nd) (as explained at the end of Section 4), and each of the N iterations of the inner loop cost O(d) operations, the total computational cost of for each outer loop isO(nd+Nd). We will present the overall complexity analysis of Algorithm 2 in Section 5.3."}, {"heading": "5.2 SAGA for policy evaluation", "text": "The second stochastic variance reduction method for policy evaluation is adapted from SAGA (Defazio et al., 2014); see Algorithm 3. It uses a single loop, and maintains a single set of parameters (\u03b8, w). Algorithm 3 starts by first\ncomputing each component gradients gt = Bt(\u03b8, w) at the initial point, and also form their average B = \u2211n t gt. At each iteration, the algorithm randomly picks an index tm \u2208 {1, . . . , n} and computes the stochastic gradient htm = Btm(\u03b8, w). Then, it updates (\u03b8, w) using a variance reduced stochastic gradient: B + htm \u2212 gtm , where gtm is the previously computed stochastic gradient using the tm-th sample (associated with certain past values of \u03b8 and w). Afterwards, it updates the batch gradient estimate B as B + 1n (htm \u2212 gtm) and replaces gtm with htm . As Algorithm 3 proceeds, different vectors gt are computed using different values of \u03b8 and w (depending on when the index t was sampled). So in general we need to store all vectors gt, for t = 1, . . . , n, to facilitate individual updates, which will cost additional O(nd) storage. However, by exploiting the rank-one structure in (6), we only need to store three scalars (\u03c6t \u2212 \u03b3\u2032\u03c6)T \u03b8, (\u03c6t \u2212 \u03b3\u2032\u03c6)Tw, and \u03c6Tt w, and form gtm on the fly using O(d) computation. Overall, each iteration of SAGA costs O(d) operations."}, {"heading": "5.3 Theoretical analyses of SVRG and SAGA", "text": "In order to study the convergence properties of SVRG and SAGA for policy evaluation, we introduce a smoothness parameter LG based on the stochastic gradients Bt(\u03b8, w). Let \u03b2 = \u03c3w/\u03c3\u03b8 be the ratio between the primal and dual step-sizes, and define a pair of weighted Euclidean norms\n\u2126(\u03b8, w) , (\u2016\u03b8\u20162 + \u03b2\u22121\u2016w\u20162)1/2, \u2126\u2217(\u03b8, w) , (\u2016\u03b8\u20162 + \u03b2\u2016w\u20162)1/2.\nNote that \u2126(\u00b7, \u00b7) upper bounds the error in optimizing \u03b8: \u2126(\u03b8 \u2212 \u03b8?, w \u2212 w?) \u2265 \u2016\u03b8 \u2212 \u03b8?\u2016. Therefore, any bound on \u2126(\u03b8 \u2212 \u03b8?, w \u2212 w?) applies automatically to \u2016\u03b8 \u2212 \u03b8?\u2016. Next, we define the parameter LG through its square:\nL2G , sup \u03b81,w1,\u03b82,w2\n1 n \u2211n t=1 \u2126 \u2217(Bt(\u03b81, w1)\u2212Bt(\u03b82, w2))2 \u2126(\u03b81 \u2212 \u03b82, w1 \u2212 w2)2 .\nThis definition is similar to the smoothness constant L\u0304 used in Balamurugan & Bach (2016) except that we used the step-size ratio \u03b2 rather than the strong convexity and concavity parameters of the Lagrangian to define \u2126 and \u2126\u2217.1 Substituting the definition of Bt(\u03b8, w) in (16), we have\nL2G = \u2225\u2225\u2225\u2225 1n n\u2211 t=1 GTt Gt \u2225\u2225\u2225\u2225, where Gt , [ \u03c1I \u2212\u221a\u03b2ATt\u221a\u03b2At \u03b2Ct ] .\n(18)\nWith the above definitions, we characterize the convergence of \u2126(\u03b8m \u2212 \u03b8?, wm \u2212 w?), where (\u03b8?, w?) is the solution of (10), and (\u03b8m, wm) is the output of the algorithms\n1Since our saddle-point problem is not necessarily strongly convex in \u03b8 (when \u03c1 = 0), we could not define \u2126 and \u2126\u2217 in the same way as Balamurugan & Bach (2016).\nafter the m-th iteration. For SVRG, it is the m-th outer iteration in Algorithm 2. The following two theorems are proved in Appendices C and D, respectively. Theorem 2 (Convergence rate of SVRG). Suppose Assumption 1 holds. If we choose \u03c3\u03b8 = \u00b5\u03c1\n48\u03ba(C\u0302)L2G , \u03c3w =\n8L\u03c1\n\u03bbmin(C\u0302) \u03c3\u03b8, N = 51\u03ba2(C\u0302)L2G \u00b52\u03c1\n, where L\u03c1 and \u00b5\u03c1 are defined in (13) and (14), then E [ \u2126(\u03b8m\u2212\u03b8?, wm\u2212w?)2 ] \u2264 (4\n5\n)m \u2126(\u03b80\u2212\u03b8?, w0\u2212w?)2.\nThe overall computational cost for reaching E [ \u2126(\u03b8m \u2212\n\u03b8?, wm \u2212 w?) ] \u2264 is upper bounded by\nO (( n+\n\u03ba(C\u0302)L2G\n\u03bb2min(\u03c1I + A\u0302 T C\u0302\u22121A\u0302)\n) d log (1 )) . (19)\nTheorem 3 (Convergence rate of SAGA). Suppose Assumption 1 holds. If we choose \u03c3\u03b8 = \u00b5\u03c1\n3(8\u03ba2(C\u0302)L2G+n\u00b52\u03c1)\nand \u03c3w = 8L\u03c1\n\u03bbmin(C\u0302) \u03c3\u03b8 in Algorithm 3, then\nE [ \u2126(\u03b8m\u2212\u03b8?, wm\u2212w?)2 ] \u2264 2(1\u2212\u03c1)m\u2126(\u03b80\u2212\u03b8?, w0\u2212w?)2,\nwhere \u03c1 \u2265 \u00b5 2 \u03c1\n9(8\u03ba2(C\u0302)L2G+n\u00b52\u03c1) . The total cost to achieve E [ \u2126(\u03b8m\u2212 \u03b8?, wm\u2212w?) ] \u2264 has the same bound in (19).\nSimilar to our PDBG results in (15), both the SVRG and SAGA algorithms for policy evaluation enjoy linear convergence even if there is no strong convexity in the saddlepoint problem (10) (i.e., when \u03c1 = 0). This is mainly due to the positive definiteness of A\u0302T C\u0302\u22121A\u0302 when C\u0302 is positivedefinite and A\u0302 is full-rank. In contrast, the linear convergence of SVRG and SAGA in Balamurugan & Bach (2016) requires the Lagrangian to be both strongly convex in \u03b8 and strongly concave in w.\nMoreover, in the policy evaluation problem, the strong concavity with respect to the dual variable w comes from a weighted quadratic norm (1/2)\u2016w\u2016C\u0302 , which does not admit an efficient proximal mapping as required by the proximal versions of SVRG and SAGA in Balamurugan & Bach (2016). Our algorithms only require computing the stochastic gradients of this function, which is easy to do due to its finite sum structure.\nBalamurugan & Bach (2016) also proposed accelerated variants of SVRG and SAGA using the \u201ccatalyst\u201d framework of Lin et al. (2015). Such extensions can be done similarly for the three algorithms presented in this paper, and we omit the details due to space limit."}, {"heading": "6 Comparison of Different Algorithms", "text": "This section compares the computation complexities of several representative policy-evaluation algorithms that minimize EM-MSPBE, as summarized in Table 1.\nThe upper part of the table lists algorithms whose complexity is linear in feature dimension d, including the two new algorithms presented in the previous section. We can also apply GTD2 to a finite dataset with samples drawn uniformly at random with replacement. It costs O(d) per iteration, but has a sublinear convergence rate regarding . In practice, people may choose = \u2126(1/n) for generalization reasons (see, e.g., Lazaric et al. (2010)), leading to an O(\u03ba\u2032nd) overall complexity for GTD2, where \u03ba\u2032 is a condition number related to the algorithm. However, as verified by our experiments, the bounds in the table show that our SVRG/SAGA-based algorithms are much faster as their effective condition numbers vanish when n becomes large. TDC has a similar complexity to GTD2.\nIn the table, we list two different implementations of PDBG. PDBG-(I) computes the gradients by averaging the stochastic gradients over the entire dataset at each iteration, which costsO(nd) operations; see discussions at the end of Section 4. PDBG-(II) first pre-computes the matrices A\u0302, b\u0302 and C\u0302 using O(nd2) operations, then computes the batch gradient at each iteration with O(d2) operations. If d is very large (e.g., when d n), then PDBG-(I) would have an advantage over PDBG-(II). The lower part of the table also includes LSTD, which hasO(nd2) complexity if rankone updates are used.\nSVRG and SAGA are more efficient than the other algorithms, when either d or n is very large. In particular, they have a lower complexity than LSTD when d > (1 + \u03ba(C\u0302)\u03ba2G n ) log ( 1 ) , This condition is easy to satisfy, when n is very large. On the other hand, SVRG and SAGA algorithms are more efficient than PDBG-(I) if n is large, say n > \u03ba(C\u0302)\u03ba2G /( \u03ba(C\u0302)\u03ba \u2212 1 ) , where \u03ba and \u03baG are described in the caption of Table 1.\nThere are other algorithms whose complexity scales linearly with n and d, including iLSTD (Geramifard et al., 2007), and TDC (Sutton et al., 2009b), fLSTDSA (Prashanth et al., 2014), and the more recent algorithms of Wang et al. (2016) and Dai et al. (2016). However, their\nconvergence is slow: the number of iterations required to reach a desired accuracy grows as 1/ or worse. The CTD algorithm (Korda & Prashanth, 2015) uses a similar idea as SVRG to reduce variance in TD updates. This algorithm is shown to have a similar linear convergence rate in an online setting where the data stream is generated by a Markov process with finite states and exponential mixing. The method solves for a fixed-point solution by stochastic approximation. As a result, they can be non-convergent in off-policy learning, while our algorithms remain stable (c.f., Section 7.1)."}, {"heading": "7 Extensions", "text": "It is possible to extend our approach to accelerate optimization of other objectives such as MSBE and NEU (Dann et al., 2014). In this section, we briefly describe two extensions of the algorithms developed earlier."}, {"heading": "7.1 Off-policy learning", "text": "In some cases, we may want to estimate the value function of a policy \u03c0 from a set of data D generated by a different \u201cbehavior\u201d policy \u03c0b. This is called off-policy learning (Sutton & Barto, 1998, Chapter 8).\nIn the off-policy case, samples are generated from the distribution induced by the behavior policy \u03c0b, not the the target policy \u03c0. While such a mismatch often causes stochastic-approximation-based methods to diverge (Tsitsiklis & Van Roy, 1997), our gradient-based algorithms remain convergent with the same (fast) convergence rate.\nConsider the RL framework outlined in Section 2. For each state-action pair (st, at) such that \u03c0b(at|st) > 0, we define the importance ratio, \u03c1t , \u03c0(at|st)/\u03c0b(at|st). The EMMSPBE for off-policy learning has the same expression as in (7) except that At, bt and Ct are modified by the weight factor \u03c1t, as listed in Table 2; see also Liu et al. (2015, Eqn 6) for a related discussion.) Algorithms 1\u20133 remain the same for the off-policy case afterAt, bt andCt are modified correspondingly."}, {"heading": "7.2 Learning with eligibility traces", "text": "Eligibility traces are a useful technique to trade off bias and variance in TD learning (Singh & Sutton, 1996; Kearns & Singh, 2000). When they are used, we can pre-compute zt in Table 2 before running our new algorithms. Note that EM-MSPBE with eligibility traces has the same form of (7), with At, bt and Ct defined differently according to the last row of Table 2. At the m-th step of the learning process, the algorithm randomly samples ztm , \u03c6tm , \u03c6 \u2032 tm and rtm from the fixed dataset and computes the corresponding stochastic gradients, where the index tm is uniformly distributed over {1, . . . , n} and are independent for different values of m. Algorithms 1\u20133 immediately work for this case, enjoying a similar linear convergence rate and a com-\nputation complexity linear in n and d. We need additional O(nd) operations to pre-compute zt recursively and an additional O(nd) storage for zt. However, it does not change the order of the total complexity for SVRG/SAGA."}, {"heading": "8 Experiments", "text": "In this section, we compare the following algorithms on two benchmark problems: (i) PDBG (Algorithm 1); (ii) GTD2 with samples drawn randomly with replacement from a dataset; (iii) TD: the fLSTD-SA algorithm of Prashanth et al. (2014); (iv) SVRG (Algorithm 2); and (v) SAGA (Algorithm 3). Note that when \u03c1 > 0, the TD solution and EM-MSPBE minimizer differ, so we do not include TD. For step size tuning, \u03c3\u03b8 is chosen from{\n10\u22121, 10\u22122, . . . , 10\u22126 } 1\nL\u03c1\u03ba(C\u0302) and \u03c3w is chosen from{ 1, 10\u22121, 10\u22122 } 1\n\u03bbmax(C\u0302) . We only report the results of\neach algorithm which correspond to the best-tuned step sizes; for SVRG we choose N = 2n.\nIn the first task, we consider a randomly generated MDP with 400 states and 10 actions (Dann et al., 2014). The transition probabilities are defined as P (s\u2032|a, s) \u221d pass\u2032+10\u22125, where pass\u2032 \u223c U [0, 1]. The data-generating policy and start distribution were generated in a similar way. Each state is represented by a 201-dimensional feature vector, where 200 of the features were sampled from a uniform distribution, and the last feature was constant one. We chose \u03b3 = 0.95. Fig. 1 shows the performance of various algorithms for n = 20000. First, notice that the stochastic variance methods converge much faster than others. In fact, our proposed methods achieve linear convergence. Second, as we increase \u03c1, the performances of PDBG, SVRG and SAGA improve significantly due to better conditioning, as predicted by our theoretical results.\nNext, we test these algorithms on Mountain Car (Sutton & Barto, 1998, Chapter 8). To collect the dataset, we first ran Sarsa with d = 300 CMAC features to obtain a good policy. Then, we ran this policy to collect trajectories that comprise the dataset. Figs. 2 and 3 show our proposed stochastic variance reduction methods dominate other first-order methods. Moreover, with better conditioning (through a larger \u03c1), PDBG, SVRG and SAGA achieve faster convergence rate. Finally, as we increase sample size n, SVRG and SAGA converge faster. This simulation verifies our\ntheoretical finding in Table 1 that SVRG/SAGA need fewer epochs for large n."}, {"heading": "9 Conclusions", "text": "In this paper, we reformulated the EM-MSPBE minimization problem in policy evaluation into an empirical saddlepoint problem, and developed and analyzed a batch gradient method and two first-order stochastic variance reduction methods to solve the problem. An important result we obtained is that even when the reformulated saddle-point problem lacks strong convexity in primal variables and has only strong concavity in dual variables, the proposed algorithms are still able to achieve a linear convergence rate. We are not aware of any similar results for primal-dual\nbatch gradient methods or stochastic variance reduction methods. Furthermore, we showed that when both the feature dimension d and the number of samples n are large, the developed stochastic variance reduction methods are more efficient than any other gradient-based methods which are convergent in off-policy settings.\nThis work leads to several interesting directions for research. First, we believe it is important to extend the stochastic variance reduction methods to nonlinear approximation paradigms (Bhatnagar et al., 2009), especially with deep neural networks. Moreover, it remains an important open problem how to apply stochastic variance reduction techniques to policy optimization."}, {"heading": "A Eigen-analysis of G", "text": "In this section, we give a thorough analysis of the spectral properties of the matrix\nG =\n[ \u03c1I \u2212\u03b21/2A\u0302T\n\u03b21/2A\u0302 \u03b2C\u0302\n] , (20)\nwhich is critical in analyzing the convergence of the PDBG, SAGA and SVRG algorithms for policy evaluation. Here \u03b2 = \u03c3w/\u03c3\u03b8 is the ratio between the dual and primal step sizes in these algorithms. For convenience, we use the following notation:\nL , \u03bbmax(A\u0302 T C\u0302\u22121A\u0302),\n\u00b5 , \u03bbmin(A\u0302 T C\u0302\u22121A\u0302).\nUnder Assumption 1, they are well defined and we have L \u2265 \u00b5 > 0.\nA.1 Diagonalizability of G\nFirst, we examine the condition of \u03b2 that ensures the diagonalizability of the matrix G. We cite the following result from (Shen et al., 2008).\nLemma 1. Consider the matrix A defined as A = [ A \u2212B> B C ] , (21)\nwhereA 0, C 0, andB is full rank. Let \u03c4 = \u03bbmin(C), \u03b4 = \u03bbmax(A) and \u03c3 = \u03bbmax(B>C\u22121B). If \u03c4 > \u03b4+2 \u221a \u03c4\u03c3 holds, thenA is diagonalizable with all its eigenvalues real and positive.\nApplying this lemma to the matrix G in (20), we have\n\u03c4 = \u03bbmin(\u03b2C\u0302) = \u03b2\u03bbmin(C\u0302),\n\u03b4 = \u03bbmax(\u03c1I) = \u03c1, \u03c3 = \u03bbmax ( \u03b21/2A\u0302>(\u03b2C\u0302)\u22121\u03b21/2A\u0302 ) = \u03bbmax(A\u0302 >C\u0302\u22121A\u0302).\nThe condition \u03c4 > \u03b4 + 2 \u221a \u03c4\u03c3 translates into\n\u03b2\u03bbmin(C\u0302) > \u03c1+ 2 \u221a \u03b2\u03bbmin(C\u0302)\u03bbmax(A\u0302>C\u0302\u22121A\u0302),\nwhich can be solved as\n\u221a \u03b2 >\n\u221a \u03bbmax(A\u0302>C\u0302\u22121A\u0302)+ \u221a \u03c1+\u03bbmax(A\u0302>C\u0302\u22121A\u0302)\u221a\n\u03bbmin(C\u0302) .\nIn the rest of our discussion, we choose \u03b2 to be \u03b2 = 8 ( \u03c1+ \u03bbmax ( A\u0302>C\u0302\u22121A\u0302 )) \u03bbmin(C\u0302) = 8(\u03c1+ L) \u03bbmin(C\u0302) , (22)\nwhich satisfies the inequality above.\nA.2 Analysis of eigenvectors\nIf the matrix G is diagonalizable, then it can be written as\nG = Q\u039bQ\u22121,\nwhere \u039b is a diagonal matrix whose diagonal entries are the eigenvalues of G, and Q consists of it eigenvectors (each with unit norm) as columns. Our goal here is to bound \u03ba(Q), the condition number of the matrix Q. Our analysis is inspired by Liesen & Parlett (2008). The core is the following fundamental result from linear algebra. Theorem 4 (Theorem 5.1.1 of Gohberg et al. (2006)). Suppose G is diagonalizable. If H is a symmetric positive definite matrix and HG is symmetric, then there exist a complete set of eigenvectors of G, such that they are orthonormal with respect to the inner product induced by H:\nQ>HQ = I. (23)\nIf H satisfies the conditions in Theorem 4, then we have H = Q\u2212>Q\u22121, which implies \u03ba(H) = \u03ba2(Q). Therefore, in order to bound \u03ba(Q), we only need to find such an H and analyze its conditioning. To this end, we consider the matrix of the following form:\nH =\n[ (\u03b4 \u2212 \u03c1)I\n\u221a \u03b2A\u0302>\u221a\n\u03b2A\u0302 \u03b2C\u0302 \u2212 \u03b4I\n] . (24)\nIt is straightforward to check that HG is a symmetric matrix. The following lemma states the conditions for H being positive definite. Lemma 2. If \u03b4 \u2212 \u03c1 > 0 and \u03b2C\u0302 \u2212 \u03b4I \u2212 \u03b2\u03b4\u2212\u03c1 A\u0302A\u0302\n> 0, then H is positive definite.\nProof. The matrix H in (24) admits the following Schur decomposition:\nH = [ I 0\u221a \u03b2\n\u03b4\u2212\u03c1 A\u0302 I\n] [ (\u03b4 \u2212 \u03c1)I\nS\n] [ I \u221a \u03b2\n\u03b4\u2212\u03c1 A\u0302 >\n0 I\n] ,\nwhere S = \u03b2C\u0302\u2212 \u03b4I\u2212 \u03b2\u03b4\u2212\u03c1 A\u0302A\u0302 >. Thus H is congruence to the block diagonal matrix in the middle, which is positive definite under the specified conditions. Therefore, the matrix H is positive definite under the same conditions.\nIn addition to the choice of \u03b2 in (22), we choose \u03b4 to be\n\u03b4 = 4(\u03c1+ L). (25)\nIt is not hard to verify that this choice ensures \u03b4\u2212\u03c1 > 0 and \u03b2C\u0302\u2212 \u03b4I\u2212 \u03b2\u03b4\u2212\u03c1 A\u0302A\u0302\n> 0 so that H is positive definite. We now derive an upper bound on the condition number of H . Let \u03bb be an eigenvalue of H and [xT yT ]T be its associated eigenvector, where \u2016x\u20162 + \u2016y\u20162 > 0. Then it holds that\n(\u03b4 \u2212 \u03c1)x+ \u221a \u03b2A\u0302T y = \u03bbx, (26)\nStochastic Variance Reduction Methods for Policy Evaluation\u221a \u03b2A\u0302x+ (\u03b2C\u0302 \u2212 \u03b4I)y = \u03bby. (27)\nFrom (26), we have\nx =\n\u221a \u03b2\n\u03bb\u2212 \u03b4 + \u03c1 A\u0302T y. (28)\nNote that \u03bb\u2212 \u03b4 + \u03c1 6= 0 because if \u03bb\u2212 \u03b4 + \u03c1 = 0 we have A\u0302T y = 0 so that y = 0 since A\u0302 is full rank. With y = 0 in (27), we will have A\u0302x = 0 so that x = 0, which contradicts the assumption that \u2016x\u20162 + \u2016y\u20162 > 0. Substituting (28) into (27) and multiplying both sides with yT , we obtain the following equation after some algebra\n\u03bb2 \u2212 p\u03bb+ q = 0, (29)\nwhere\np , \u03b4 \u2212 \u03c1+ y T (\u03b2C\u0302 \u2212 \u03b4I)y \u2016y\u20162 ,\nq , (\u03b4 \u2212 \u03c1)y T (\u03b2C\u0302 \u2212 \u03b4I)y \u2016y\u20162 \u2212 \u03b2 y T A\u0302A\u0302T y \u2016y\u20162 .\nWe can verify that both p and q are positive with our choice of \u03b4 and \u03b2. The roots of the quadratic equation in (29) are given by\n\u03bb = p\u00b1 \u221a p2 \u2212 4q 2 . (30)\nTherefore, we can upper bound the largest eigenvalue as\n\u03bbmax(H) \u2264 p+ \u221a p2 \u2212 4q 2\n\u2264 p = \u03b4 \u2212 \u03c1\u2212 \u03b4 + \u03b2 y T C\u0302y\n\u2016y\u20162\n\u2264 \u2212\u03c1+ \u03b2\u03bbmax(C\u0302)\n= \u2212\u03c1+ 8(\u03c1+ L) \u03bbmin(C\u0302) \u03bbmax(C\u0302)\n\u2264 8(\u03c1+ L)\u03ba(C\u0302). (31)\nLikewise, we can lower bound the smallest eigenvalue:\n\u03bbmin(H) \u2265 p\u2212 \u221a p2 \u2212 4q 2 \u2265 p\u2212 p+ 2q/p 2 = q p\n= \u03b2 ( (\u03b4 \u2212 \u03c1)y T C\u0302y \u2016y\u20162 \u2212 yT A\u0302A\u0302T y \u2016y\u20162 ) \u2212 \u03b4(\u03b4 \u2212 \u03c1)\n\u2212\u03c1+ \u03b2 yT C\u0302y\u2016y\u20162\n(a) \u2265 \u03b2 ( (\u03b4 \u2212 \u03c1)y T C\u0302y \u2016y\u20162 \u2212 yT A\u0302A\u0302T y \u2016y\u20162 ) \u2212 \u03b4(\u03b4 \u2212 \u03c1)\n\u03b2 y T C\u0302y \u2016y\u20162\n= \u03b4 \u2212 \u03c1\u2212 y T A\u0302A\u0302T y yT C\u0302y \u2212 \u03b4(\u03b4 \u2212 \u03c1) \u03b2 \u00b7 1 yT C\u0302y \u2016y\u20162\n(b) \u2265 \u03b4 \u2212 \u03c1\u2212 L\u2212 \u03b4(\u03b4 \u2212 \u03c1) \u03b2\u03bbmin(C\u0302)\n(c) = (\u03c1+ L) ( 3\u2212 3\u03c1+ 4L\n2(\u03c1+ L) ) \u2265 \u03c1+ L, (32)\nwhere step (a) uses the fact that both the numerator and denominator are positive, step (b) uses the fact\nL , \u03bbmax ( A\u0302T C\u0302\u22121A\u0302 ) \u2265 y T A\u0302A\u0302T y\nyT C\u0302y ,\nand step (c) substitutes the expressions of \u03b4 and \u03b2. Therefore, we can upper bound the condition number of H , and thus that of Q, as follows:\n\u03ba2(Q) = \u03ba(H) \u2264 8(\u03c1+ L)\u03ba(C\u0302) \u03c1+ L = 8\u03ba(C\u0302). (33)\nA.3 Analysis of eigenvalues Suppose \u03bb is an eigenvalue of G and let ( \u03be>, \u03b7> )> be its corresponding eigenvector. By definition, we have\nG [ \u03be \u03b7 ] = \u03bb [ \u03be \u03b7 ] ,\nwhich is equivalent to the following two equations: \u03c1\u03be \u2212 \u221a \u03b2A\u0302>\u03b7 = \u03bb\u03be,\u221a\n\u03b2A\u0302\u03be + \u03b2C\u0302\u03b7 = \u03bb\u03b7.\nSolve \u03be in the first equation in terms of \u03b7, then plug into the second equation, we obtain:\n\u03bb2\u03b7 \u2212 \u03bb(\u03c1\u03b7 + \u03b2C\u0302\u03b7) + \u03b2(A\u0302A\u0302>\u03b7 + \u03c1C\u0302\u03b7) = 0.\nNow left multiply \u03b7>, then divide by the \u2016\u03b7\u201622, we have:\n\u03bb2 \u2212 p\u03bb+ q = 0.\nwhere p and q are defined as\np , \u03c1+ \u03b2 \u03b7>C\u0302\u03b7\n\u2016\u03b7\u20162 ,\nq , \u03b2\n( \u03b7T A\u0302A\u0302>\u03b7\n\u2016\u03b7\u20162 + \u03c1\n\u03b7T C\u0302\u03b7\n\u2016\u03b7\u20162\n) . (34)\nTherefore the eigenvalues of G satisfy:\n\u03bb = p\u00b1 \u221a p2 \u2212 4q 2 . (35)\nRecall that our choice of \u03b2 ensures that G is diagonalizable and has positive real eigenvalues. Indeed, we can verify that the diagonalization condition guarantees p2 \u2265 4q\nso that all eigenvalues are real and positive. Now we can obtain upper and lower bounds based on (35). For upper bound, notice that\n\u03bbmax(G) \u2264 p \u2264 \u03c1+ \u03b2\u03bbmax(C\u0302)\n= \u03c1+ 8(\u03c1+ L)\n\u03bbmin(C\u0302 \u03bbmax(C\u0302)\n= \u03c1+ 8(\u03c1+ L)\u03ba(C\u0302) \u2264 9\u03ba(C\u0302) ( \u03c1+ L ) = 9\u03ba(C\u0302)\u03bbmax ( \u03c1I + A\u0302T C\u0302\u22121A\u0302 ) . (36)\nFor lower bound, notice that\n\u03bbmin(G) \u2265 p\u2212 \u221a p2 \u2212 4q 2 \u2265 p\u2212 p+ 2q/p 2 = q/p\n= \u03b2 ( \u03b7T A\u0302A\u0302T \u03b7 \u03b7T C\u0302\u03b7 + \u03c1 )\n\u03c1 \u2016\u03b7\u2016 2\n\u03b7T C\u0302\u03b7 + \u03b2\n(a) \u2265 \u03b2(\u03c1+ \u00b5) \u03c1/\u03bbmin(C\u0302) + \u03b2 = \u03b2\u03bbmin(C\u0302)(\u03c1+ \u00b5) \u03c1+ \u03b2\u03bbmin(C\u0302)\n(b) =\n8(\u03c1+ L)(\u03c1+ \u00b5)\n\u03c1+ 8(\u03c1+ L)\n\u2265 8 9 (\u03c1+ \u00b5) = 8\n9 (\u03c1+ \u03bbmin(A\u0302\nT C\u0302\u22121A\u0302))\n= 8\n9 \u03bbmin(\u03c1I + A\u0302\nT C\u0302\u22121A\u0302), (37)\nwhere the second inequality is by the concavity property of the square root function, step (a) used the fact\n\u00b5 , \u03bbmin ( A\u0302T C\u0302\u22121A\u0302 ) \u2264 y T A\u0302A\u0302T y\nyT C\u0302y ,\nand step (b) substitutes the expressions of \u03b2.\nSince G is not a normal matrix, we cannot use their eigenvalue bounds to bound its condition number \u03ba(G)."}, {"heading": "B Linear convergence of PDBG", "text": "Recall the saddle-point problem we need to solve:\nmin \u03b8 max w L(\u03b8, w),\nwhere the Lagrangian is defined as\nL(\u03b8, w) = \u03c1 2 \u2016\u03b8\u20162 \u2212 w>A\u0302\u03b8 \u2212 1 2 w>C\u0302w + b\u0302>w. (38)\nOur assumption is that C\u0302 is positive definite and A\u0302 has full rank. The optimal solution can be expressed as\n\u03b8? = ( A\u0302>C\u0302\u22121A\u0302+ \u03c1I )\u22121 A\u0302>C\u0302\u22121b\u0302,\nw? = C\u0302 \u22121 ( b\u0302\u2212 A\u0302>\u03b8? ) .\nThe gradients of the Lagrangian with respect to \u03b8 and w, respectively, are\n\u2207\u03b8L (\u03b8, w) = \u03c1\u03b8 \u2212 A\u0302>w\n\u2207wL (\u03b8, w) = \u2212A\u0302\u03b8 \u2212 C\u0302w + b\u0302.\nThe first-order optimality condition is obtained by setting them to zero, which is satisfied by (\u03b8?, w?):[\n\u03c1I \u2212A\u0302> A\u0302 C\u0302 ] [ \u03b8? w? ] = [ 0 b\u0302 ] . (39)\nThe PDBG method in Algorithm 1 takes the following iteration: [\n\u03b8m+1 wm+1\n] = [ \u03b8m wm ] \u2212 [ \u03c3\u03b8 0 0 \u03c3w ] B(\u03b8m, wm),\nwhere\nB(\u03b8, w) = [ \u2207\u03b8L(\u03b8, w) \u2212\u2207wL(\u03b8, w) ] = [ \u03c1I \u2212A\u0302> A\u0302 C\u0302 ] [ \u03b8 w ] \u2212 [ 0 b\u0302 ] .\nLetting \u03b2 = \u03c3w/\u03c3\u03b8, we have[ \u03b8m+1 wm+1 ] = [ \u03b8m wm ] \u2212 \u03c3\u03b8 ([ \u03c1I \u2212A\u0302> \u03b2A\u0302 \u03b2C\u0302 ] [ \u03b8m wm ] \u2212 [ 0 \u03b2b\u0302 ]) .\nSubtracting both sides of the above recursion by (\u03b8?, w?) and using (39), we obtain[ \u03b8m+1 \u2212 \u03b8? wm+1\u2212w? ] = [ \u03b8m \u2212 \u03b8? wm\u2212w? ] \u2212\u03c3\u03b8 [ \u03c1I \u2212A\u0302T \u03b2A\u0302 \u03b2C\u0302 ][ \u03b8m\u2212\u03b8? wm\u2212w? ] .\nWe analyze the convergence of the algorithms by examining the differences between the current parameters to the optimal solution. More specifically, we define a scaled residue vector\n\u2206m ,\n[ \u03b8m \u2212 \u03b8?\n1\u221a \u03b2 (wm \u2212 w?)\n] , (40)\nwhich obeys the following iteration:\n\u2206m+1 = (I \u2212 \u03c3\u03b8G) \u2206m, (41)\nwhere G is exactly the matrix defined in (20). As analyzed in Section A.1, if we choose \u03b2 sufficiently large, such as in (22), then G is diagonalizable with all its eigenvalues real and positive. In this case, we let Q be the matrix of eigenvectors in the eigenvalue decomposition G = Q\u039bQ\u22121, and use the potential function\nPm , \u2225\u2225Q\u22121\u2206m\u2225\u222522\nin our convergence analysis. We can bound the usual Euclidean distance by Pm as\n\u2016\u03b8m \u2212 \u03b8?\u20162 + \u2016wm \u2212 w?\u20162 \u2264 (1 + \u03b2)\u03c32max(Q)Pm.\nIf we have linear convergence in Pm, then the extra factor (1 + \u03b2)\u03c32max(Q) will appear inside a logarithmic term.\nRemark: This potential function has an intrinsic geometric interpretation. We can view column vectors of Q\u22121 a basis for the vector space, which is not orthogonal. Our goal is to show that in this coordinate system, the distance to optimal solution shrinks at every iteration.\nWe proceed to bound the growth of Pm:\nPm+1 = \u2225\u2225Q\u22121\u2206m+1\u2225\u222522\n= \u2225\u2225Q\u22121 (I \u2212 \u03c3\u03b8G) \u2206m\u2225\u222522\n= \u2225\u2225Q\u22121 (QQ\u22121 \u2212 \u03c3\u03b8Q\u039bQ\u22121)\u2206m\u2225\u222522\n= \u2225\u2225(I \u2212 \u03c3\u03b8\u039b)Q\u22121\u2206m\u2225\u222522\n\u2264 \u2016I \u2212 \u03c3\u03b8\u039b\u201622 \u2225\u2225Q\u22121\u2206m\u2225\u222522 = \u2016I \u2212 \u03c3\u03b8\u039b\u201622 Pm (42)\nThe inequality above uses sub-multiplicity of spectral norm. We choose \u03c3\u03b8 to be\n\u03c3\u03b8 = 1\n\u03bbmax (\u039b) =\n1\n\u03bbmax(G) , (43)\nSince all eigenvalues of G are real and positive, we have\n\u2016I \u2212 \u03c3\u03b8\u039b\u20162 = (\n1\u2212 \u03bbmin(G) \u03bbmax(G) )2 \u2264 ( 1\u2212 8\n81 \u00b7 1 \u03ba(C\u0302)\u03ba(\u03c1I + A\u0302T C\u0302\u22121A\u0302)\n)2 ,\nwhere we used the bounds on the eigenvalues \u03bbmax(G) and \u03bbmin(G) in (36) and (37) respectively. Therefore, we can achieve an -close solution with\nm = O ( \u03ba(C\u0302)\u03ba(\u03c1I + A\u0302T C\u0302\u22121A\u0302) log ( P0 )) iterations of the PDBG algorithm.\nIn order to minimize \u2016I \u2212 \u03c3\u03b8\u039b\u2016, we can choose\n\u03c3\u03b8 = 2\n\u03bbmax(G) + \u03bbmin(G) ,\nwhich results in \u2016I \u2212 \u03c3\u03b8\u039b\u2016 = 1\u2212 2/(1 + \u03ba(\u039b)) instead of 1\u22121/\u03ba(\u039b). The resulting complexity stays the same order. The step sizes stated in Theorem 1 is obtained by replacing \u03bbmax in (43) with its upper bound in (36) and setting \u03c3w through the ratio \u03b2 = \u03c3w/\u03c3\u03b8 as in (22)."}, {"heading": "C Analysis of SVRG", "text": "Here we establish the linear convergence of the SVRG algorithm for policy evaluation described in Algorithm 2.\nRecall the finite sum structure in A\u0302, b\u0302 and C\u0302:\nA\u0302 = 1\nn n\u2211 t=1 At, b\u0302 = 1 n n\u2211 t=1 bt, C\u0302 = 1 n n\u2211 t=1 Ct.\nThis structure carries over to the Lagrangian L(\u03b8, w) as well as the gradient operator B(\u03b8, w), so we have\nB(\u03b8, w) = 1\nn n\u2211 t=1 Bt(\u03b8, w),\nwhere\nBt(\u03b8, w) = [ \u03c1I \u2212A>t At Ct ] [ \u03b8 w ] \u2212 [ 0 bt ] . (44)\nAlgorithm 2 has both an outer loop and an inner loop. We use the index m for the outer iteration and j for the inner iteration. Fixing the outer loop index m, we look at the inner loop of Algorithm 2. Similar to full gradient method, we first simplify the dynamics of SVRG.[ \u03b8m,j+1 wm,j+1 ] = [ \u03b8m,j wm,j ] \u2212 [ \u03c3\u03b8 \u03c3w ] \u00d7 ( B(\u03b8m\u22121, wm\u22121)\n+Btj (\u03b8m,j , wm,j)\u2212Bt(\u03b8m\u22121, wm\u22121) )\n= [ \u03b8m,j wm,j ] \u2212 [ \u03c3\u03b8 \u03c3w ] \u00d7 ([ \u03c1I \u2212A\u0302> A\u0302 C\u0302 ] [ \u03b8m\u22121 wm\u22121 ] \u2212 [ 0 b\u0302\n] + [ \u03c1I \u2212A>t At Ct ] [ \u03b8m,j wm,j ] \u2212 [ 0 bt\n] \u2212 [ \u03c1I \u2212A>t At Ct ] [ \u03b8m\u22121 wm\u22121 ] + [ 0 bt ]) .\nSubtracting (\u03b8?, w?) from both sides and using the optimality condition (39), we have[ \u03b8m,j+1 \u2212 \u03b8? wm,j+1 \u2212 w? ] = [ \u03b8m,j \u2212 \u03b8? wm,j \u2212 w? ] \u2212 [ \u03c3\u03b8 \u03c3w\n] \u00d7 ([ \u03c1I \u2212A\u0302> A\u0302 C\u0302 ][ \u03b8m\u22121 \u2212 \u03b8? wm\u22121 \u2212 w?\n] + [ \u03c1I \u2212A>t At Ct ] [ \u03b8m,j \u2212 \u03b8? wm,j \u2212 w?\n] \u2212 [ \u03c1I \u2212A>t At Ct ] [ \u03b8m\u22121 \u2212 \u03b8? wm\u22121 \u2212 w? ]) .\nMultiplying both sides of the above recursion by diag(I, 1/ \u221a \u03b2I), and using a residue vector \u2206m,j defined similarly as in (40), we obtain\n\u2206m,j+1 = \u2206m,j \u2212 \u03c3\u03b8(G\u2206m\u22121 +Gtj\u2206m,j \u2212Gtj\u2206m\u22121) = (I \u2212 \u03c3\u03b8G) \u2206m,j\n+ \u03c3\u03b8 ( G\u2212Gtj ) (\u2206m,j \u2212\u2206m\u22121) , (45)\nwhere Gtj is defined in (18).\nFor SVRG, we use the following potential functions to facilitate our analysis:\nPm , E [\u2225\u2225Q\u22121\u2206m\u2225\u22252] , (46)\nPm,j , E [\u2225\u2225Q\u22121\u2206m,j\u2225\u22252] . (47)\nUnlike the analysis for the batch gradient methods, the nonorthogonality of the eigenvectors will lead to additional dependency of the iteration complexity on the condition number of Q, for which we give a bound in (33).\nMultiplying both sides of Eqn. (45) byQ\u22121, taking squared 2-norm and taking expectation, we obtain\nPm,j+1 = E [\u2225\u2225Q\u22121[ (I \u2212 \u03c3\u03b8G) \u2206m,j + \u03c3\u03b8 ( G\u2212Gtj ) (\u2206m,j \u2212\u2206m\u22121)\n]\u2225\u22252] (a) = E\n[ \u2225\u2225(I \u2212 \u03c3\u03b8\u039b)Q\u22121\u2206m,j\u2225\u22252 ] + \u03c32\u03b8 E\n[ \u2225\u2225Q\u22121 (G\u2212Gtj) (\u2206m,j\u2212\u2206m\u22121)\u2225\u22252 ] (b)\n\u2264 \u2016I \u2212 \u03c3\u03b8\u039b\u20162 E [\u2225\u2225Q\u22121\u2206m,j\u2225\u22252]\n+ \u03c32\u03b8 E [ \u2225\u2225Q\u22121Gtj (\u2206m,j \u2212\u2206m\u22121)\u2225\u22252 ]\n(c) = \u2016I \u2212 \u03c3\u03b8\u039b\u20162 Pm,j + \u03c32\u03b8 E [ \u2225\u2225Q\u22121Gtj (\u2206m,j \u2212\u2206m\u22121)\u2225\u22252 ]. (48)\nwhere step (a) used the facts that Gtj is independent of \u2206m,j and \u2206m\u22121 and E[Gtj ] = G so the cross terms are zero, step (b) used again the same independence and that the variance of a random variable is less than its second moment, and step (c) used the definition of Pm,j in (47). To bound the last term in the above inequality, we use the simple notation \u03b4 = \u2206m,j \u2212\u2206m\u22121 and have\u2225\u2225Q\u22121Gtj\u03b4\u2225\u22252 = \u03b4TGTtjQ\u2212TQ\u22121Gtj\u03b4\n\u2264 \u03bbmax(Q\u2212TQ\u22121)\u03b4TGTtjGtj\u03b4.\nTherefore, we can bound the expectation as\nE [\u2225\u2225Q\u22121Gtj\u03b4\u2225\u22252]\n\u2264\u03bbmax(Q\u2212TQ\u22121)E [ \u03b4TGTtjGtj\u03b4 ]\n=\u03bbmax(Q \u2212TQ\u22121)E [ \u03b4TE[GTtjGtj ]\u03b4 ] \u2264\u03bbmax(Q\u2212TQ\u22121)L2GE [ \u03b4T \u03b4\n] =\u03bbmax(Q \u2212TQ\u22121)L2GE [ \u03b4TQ\u2212TQTQQ\u22121\u03b4\n] =\u03bbmax(Q \u2212TQ\u22121)\u03bbmax(Q TQ)L2GE [ \u03b4TQ\u2212TQ\u22121\u03b4\n] \u2264\u03ba(Q)2L2GE [ \u2016Q\u22121\u03b4\u20162 ] , (49)\nwhere in the second inequality we used the definition ofL2G in (18), i.e., L2G = \u2016E[GTtjGtj ]\u2016. In addition, we have\nE [ \u2016Q\u22121\u03b4\u20162 ] =E [\u2225\u2225Q\u22121(\u2206m,j \u2212\u2206m\u22121)\u2225\u22252]\n\u22642 E [\u2225\u2225Q\u22121\u2206m,j\u2225\u22252]+ 2 E[\u2225\u2225Q\u22121\u2206m\u22121\u2225\u22252]\n= 2Pm,j + 2Pm\u22121.\nThen it follows from (48) that\nPm,j+1 \u2264\u2016I \u2212 \u03c3\u03b8\u039b\u20162Pm,j + 2\u03c32\u03b8\u03ba 2(Q)L2G(Pm,j + Pm\u22121).\nNext, let \u03bbmax and \u03bbmin denote the largest and smallest diagonal elements of \u039b (eigenvalues of G), respectively. Then we have\n\u2016I \u2212 \u03c3\u03b8\u039b\u20162 = max { (1\u2212 \u03c3\u03b8\u03bbmin)2, (1\u2212 \u03c3\u03b8\u03bbmin)2 }\n\u2264 1\u2212 2\u03c3\u03b8\u03bbmin + \u03c32\u03b8\u03bb2max \u2264 1\u2212 2\u03c3\u03b8\u03bbmin + \u03c32\u03b8\u03ba2(Q)L2G,\nwhere the last inequality uses the relation\n\u03bb2max\u2264\u2016G\u20162 =\u2016EGt\u20162 \u2264 \u2016EGTt Gt\u2016= L2G \u2264 \u03ba2(Q)L2G.\nIt follows that Pm,j+1 \u2264 ( 1\u2212 2\u03c3\u03b8\u03bbmin + \u03c32\u03b8\u03ba2 (Q)L2G ) Pm,j\n+ 2\u03c32\u03b8 \u03ba 2 (Q)L2G(Pm,j + Pm\u22121) = [ 1\u2212 2\u03c3\u03b8\u03bbmin + 3\u03c32\u03b8\u03ba2(Q)L2G ] Pm,j\n+ 2\u03c32\u03b8 \u03ba 2(Q)L2GPm\u22121\nIf we choose \u03c3\u03b8 to satisfy\n0 < \u03c3\u03b8 \u2264 \u03bbmin\n3\u03ba2 (Q)L2G , (50)\nthen 3\u03c32\u03b8\u03ba 2(Q)L2G < \u03c3\u03b8\u03bbmin, which implies\nPm,j+1 \u2264 (1\u2212 \u03c3\u03b8\u03bbmin)Pm,j + 2\u03c32\u03b8 \u03ba2(Q)L2GPm\u22121.\nIterating the above inequality over j = 1, \u00b7 \u00b7 \u00b7 , N \u2212 1 and using Pm,0 = Pm\u22121 and Pm,N = Pm, we obtain\nPm = Pm,N \u2264 [( 1\u2212\u03c3\u03b8\u03bbmin )N +2\u03c32\u03b8\u03ba 2(Q)L2G N\u22121\u2211 j=0 ( 1\u2212\u03c3\u03b8\u03bbmin )j] Pm\u22121\n= [( 1\u2212\u03c3\u03b8\u03bbmin )N +2\u03c32\u03b8\u03ba 2(Q)L2G 1\u2212(1\u2212\u03c3\u03b8\u03bbmin)N\n1\u2212(1\u2212\u03c3\u03b8\u03bbmin)\n] Pm\u22121\n\u2264 [( 1\u2212 \u03c3\u03b8\u03bbmin )N + 2\u03c32\u03b8\u03ba\n2(Q)L2G \u03c3\u03b8\u03bbmin\n] Pm\u22121\n= [( 1\u2212 \u03c3\u03b8\u03bbmin )N +\n2\u03c3\u03b8\u03ba 2(Q)L2G \u03bbmin\n] Pm\u22121. (51)\nWe can choose\n\u03c3\u03b8 = \u03bbmin\n5\u03ba2(Q)L2G , N =\n1\n\u03c3\u03b8\u03bbmin = 5\u03ba2(Q)L2G \u03bb2min , (52)\nwhich satisfies the condition in (50) and results in\nPm \u2264 (e\u22121 + 2/5)Pm\u22121 \u2264 (4/5)Pm\u22121.\nThere are many other similar choices, for example,\n\u03c3\u03b8 = \u03bbmin\n3\u03ba2(Q)L2G , N =\n3\n\u03c3\u03b8\u03bbmin = 9\u03ba2(Q)L2G \u03bb2min ,\nwhich results in\nPm \u2264 (e\u22123 + 2/3)Pm\u22121 \u2264 (3/4)Pm\u22121.\nThese results imply that the number of outer iterations needed to have E[Pm] \u2264 ] is log(P0/ ). For each outer iteration, the SVRG algorithm need O(nd) operations to compute the full gradient operator B(\u03b8, w), and then N = O(\u03ba2(Q)L2G/\u03bb 2 min) inner iterations with each costingO(d) operations. Therefore the overall computational cost is\nO (( n+\n\u03ba2 (Q)L2G \u03bb2min\n) d log ( P0 )) .\nSubstituting (33) and (37) in the above bound, we get the overall cost estimate\nO (( n+\n\u03ba(C\u0302)L2G\n\u03bb2min(\u03c1I + A\u0302 T C\u0302\u22121A\u0302)\n) d log ( P0 )) .\nFinally, substituting the bounds in (33) and (37) into (52), we obtain the \u03c3\u03b8 and N stated in Theorem 2:\n\u03c3\u03b8 = \u03bbmin(\u03c1I + A\u0302\nT C\u0302\u22121A\u0302)\n48\u03ba(C\u0302)L2G ,\nN = 51\u03ba2(C\u0302)L2G\n\u03bb2min(\u03c1I + A\u0302 T C\u0302\u22121A\u0302)\n,\nwhich achieves the same complexity."}, {"heading": "D Analysis of SAGA", "text": "SAGA in Algorithm 3 maintains a table of previously computed gradients. Notation wise, we use \u03c6mt to denote that\nat m-th iteration, gt is computed using \u03b8\u03c6mt and w\u03c6mt . With this definition, \u03c6mt has the following dynamics:\n\u03c6m+1t = { \u03c6mt if tm 6= t, m if tm = t.\n(53)\nWe can write the m-th iteration\u2019s full gradient as\nB = 1\nn n\u2211 t=1 Bt ( \u03b8\u03c6mt , w\u03c6mt ) .\nFor convergence analysis, we define the following quantity:\n\u2206\u03c6mt ,\n[ \u03b8\u03c6mt \u2212 \u03b8?\n1\u221a \u03b2 (w\u03c6mt \u2212 w?)\n] . (54)\nSimilar to (53), it satisfies the following iterative relation:\n\u2206\u03c6m+1t = { \u2206\u03c6mt if tm 6= t, \u2206m if tm = t.\nWith these notations, we can express the vectors used in SAGA as\nBm = 1\nn n\u2211 t=1 [ \u03c1I \u2212ATt At Ct ] [ \u03b8\u03c6mt w\u03c6mt ] \u2212 1 n n\u2211 t=1 [ 0 bt ] ,\nhtm = [ \u03c1I \u2212ATtm Atm Ctm ] [ \u03b8m wm ] \u2212 [ 0 btm ] ,\ngtm = [ \u03c1I \u2212ATtm Atm Ctm ] [ \u03b8\u03c6mt w\u03c6mt ] \u2212 [ 0 btm ] .\nThe dynamics of SAGA can be written as[ \u03b8m+1 wm+1 ] = [ \u03b8m wm ] \u2212 [ \u03c3\u03b8 \u03c3w ] (Bm + htm \u2212 gtm)\n= [ \u03b8m wm ] \u2212 [ \u03c3\u03b8 \u03c3w ] {\n1\nn n\u2211 t=1 [ \u03c1I \u2212ATt At Ct ][ \u03b8\u03c6mt w\u03c6mt ] + 1 n n\u2211 t=1 [ 0 bt ]\n+ [ \u03c1I \u2212ATtm Atm Ctm ][ \u03b8m wm ] \u2212 [ \u03c1I \u2212ATtm Atm Ctm ][ \u03b8\u03c6mtm w\u03c6mtm ]}\nSubtracting (\u03b8?, w?) from both sides, and using the optimality condition in (39), we obtain[\n\u03b8m+1 \u2212 \u03b8? wm+1 \u2212 w?\n] = [ \u03b8m \u2212 \u03b8? wm \u2212 w? ] \u2212 [ \u03c3\u03b8 \u03c3w ] {\n1\nn n\u2211 t=1 [ \u03c1I \u2212ATt At Ct ][ \u03b8\u03c6mt \u2212 \u03b8? w\u03c6mt \u2212 w? ] +\n[ \u03c1I \u2212ATtm Atm Ctm ][ \u03b8m \u2212 \u03b8? wm \u2212 w? ]\n\u2212 [ \u03c1I \u2212ATtm Atm Ctm ][ \u03b8\u03c6mtm \u2212 \u03b8? w\u03c6mtm \u2212 w? ]} .\nMultiplying both sides by diag(I, 1/ \u221a \u03b2I), we get\n\u2206m+1 = \u2206m \u2212 ( \u03c3\u03b8 n n\u2211 t=1 Gt\u2206\u03c6mt ) \u2212 \u03c3\u03b8Gtm ( \u2206m \u2212\u2206\u03c6mtm ) . (55)\nwhere Gtm is defined in (18).\nFor SAGA, we use the following two potential functions: Pm = E \u2225\u2225Q\u22121\u2206m\u2225\u222522 ,\nQm = E [ 1\nn n\u2211 t=1 \u2225\u2225Q\u22121Gt\u2206\u03c6mt \u2225\u222522] = E[\u2225\u2225\u2225Q\u22121Gtm\u2206\u03c6mtm\u2225\u2225\u222522 ] .\nThe last equality holds because we use uniform sampling. We first look at how Pm evolves. To simplify notation, let\nvm = ( \u03c3\u03b8 n n\u2211 t=1 Gt\u2206\u03c6mt ) + \u03c3\u03b8Gtm ( \u2206m \u2212\u2206\u03c6mtm ) ,\nso that (55) becomes \u2206m+1 = \u2206m \u2212 vm. We have\nPm+1 = E [\u2225\u2225Q\u22121\u2206m+1\u2225\u222522]\n= E [\u2225\u2225Q\u22121 (\u2206m \u2212 vm)\u2225\u22252]\n= E [\u2225\u2225Q\u22121\u2206m\u2225\u222522\u22122\u2206>mQ\u2212>Q\u22121vm+\u2225\u2225Q\u22121vm\u2225\u222522 ]\n= Pm \u2212 E [ 2\u2206>mQ \u2212>Q\u22121vm ] + E [ \u2225\u2225Q\u22121vm\u2225\u222522 ].\nSince \u2206m is independent of tm, we have E [ 2\u2206>mQ \u2212>Q\u22121vm ] = E [ 2\u2206>mQ \u2212>Q\u22121Etm [vm] ] ,\nwhere the inner expectation is with respect to tm conditioned on all previous random variables. Notice that\nEtm [ Gtm\u2206\u03c6mtm ] = 1\nn n\u2211 t=1 Gt\u2206\u03c6mt ,\nwhich implies Etm [vm] = \u03c3\u03b8Etm [Gtm ]\u2206m = \u03c3\u03b8G\u2206m. Therefore, we have\nPm+1 = Pm \u2212 E [ 2\u03c3\u03b8\u2206 T mQ \u2212TQ\u22121G\u2206m ] + E [ \u2225\u2225Q\u22121vm\u2225\u222522 ] = Pm \u2212 E2\u03c3\u03b8 [ \u2206TmQ \u2212T\u039bQ\u22121\u2206m ] + E\n[ \u2225\u2225Q\u22121vm\u2225\u222522 ] \u2264 Pm \u2212 2\u03c3\u03b8\u03bbminE\n[\u2225\u2225Q\u22121\u2206m\u2225\u22252]+ E[ \u2225\u2225Q\u22121vm\u2225\u222522 ] = (1\u2212 2\u03c3\u03b8\u03bbmin)Pm + E [\u2225\u2225Q\u22121vm\u2225\u222522] , (56)\nwhere the inequality used \u03bbmin,\u03bbmin(\u039b)=\u03bbmin(G) > 0, which is true under our choice of \u03b2 = \u03c3w/\u03c3\u03b8 in Section A.1. Next, we bound the last term of Eqn. (56):\nE [ \u2225\u2225Q\u22121vm\u2225\u222522 ]\n= E [\u2225\u2225\u2225Q\u22121(\u03c3\u03b8\nn n\u2211 t=1 Gt\u2206\u03c6mt +\u03c3\u03b8Gtm ( \u2206m\u2212\u2206\u03c6mtm ))\u2225\u2225\u22252] \u2264 2\u03c32\u03b8E\n[\u2225\u2225Q\u22121Gtm\u2206m\u2225\u222522] + 2\u03c32\u03b8E [\u2225\u2225\u2225Q\u22121( 1 n n\u2211 t=1 Gt\u2206\u03c6mt \u2212Gtm\u2206\u03c6mtm )\u2225\u2225\u22252]\n\u2264 2\u03c32\u03b8E [\u2225\u2225Q\u22121Gtm\u2206m\u2225\u222522 ]+ 2\u03c32\u03b8E[\u2016Q\u22121Gtm\u2206\u03c6mtm \u20162]\n= 2\u03c32\u03b8E [\u2225\u2225Q\u22121Gtm\u2206m\u2225\u222522 ]+ 2\u03c32\u03b8Qm,\nwhere the first inequality uses \u2016a+ b\u201622 \u2264 2 \u2016a\u2016 2 2 +2 \u2016b\u2016 2 2, and the second inequality holds because for any random variable \u03be, E \u2016\u03be \u2212 E [\u03be]\u201622 = E \u2016\u03be\u2016 2 2 \u2212 \u2016E\u03be\u2016 2 2 \u2264 E \u2016\u03be\u2016 2 2. Using similar arguments as in (49), we have\nE [ \u2225\u2225Q\u22121Gtm\u2206m\u2225\u222522 ] \u2264 \u03ba2(Q)L2GPm, (57)\nTherefore, we have Pm+1 \u2264 ( 1\u2212 2\u03c3\u03b8\u03bbmin + 2\u03c32\u03b8\u03ba2 (Q)L2G ) Pm\n+ 2\u03c32\u03b8Qm. (58)\nThe inequality (58) shows that the dynamics ofPm depends on both Pm itself and Qm. So we need to find another iterative relation for Pm and Qm. To this end, we have\nQm+1 = E\n[ 1\nn n\u2211 t=1 \u2225\u2225\u2225Q\u22121Gt\u2206\u03c6m+1t \u2225\u2225\u222522 ]\n= E [ 1\nn \u2016Q\u22121Gtm\u2206\u03c6m+1tm \u2016 2\n+ 1\nn \u2211 t 6=tm \u2016Q\u22121Gt\u2206\u03c6m+1t \u2016 2 ] (a) = E [ 1\nn \u2016Q\u22121Gtm\u2206m\u20162\n+ 1\nn \u2211 t 6=tm \u2016Q\u22121Gt\u2206\u03c6mt \u2016 2\n]\n= E [ 1\nn \u2016Q\u22121Gtm\u2206m\u20162 \u2212\n1 n \u2016Q\u22121Gtm\u2206\u03c6mtm\u2016 2\n+ 1\nn n\u2211 t=1 \u2016Q\u22121Gt\u2206\u03c6mt \u2016 2 ] = 1\nn E[\u2016Q\u22121Gtm\u2206m\u20162]\u2212\n1 n E[\u2016Q\u22121Gtm\u2206\u03c6mtm\u2016 2]\n+ E [ 1\nn n\u2211 t=1 \u2016Q\u22121Gt\u2206\u03c6mt \u2016 2\n]\n= 1\nn E[\u2016Q\u22121Gtm\u2206m\u20162]\u2212\n1 n E[\u2016Q\u22121Gtm\u2206\u03c6mtm\u2016 2]\n+ E [ \u2016Q\u22121Gtm\u2206\u03c6mtm\u2016 2 ] = 1\nn E[\u2016Q\u22121Gtm\u2206m\u20162] + n\u2212 1 n Qm\n(b) \u2264 \u03ba 2(Q)L2G n Pm + n\u2212 1 n Qm. (59)\nwhere step (a) uses (53) and step (b) uses (57).\nTo facilitate our convergence analysis on Pm, we construct a new Lyapunov function which is a linear combination of Eqn. (58) and Eqn. (59). Specifically, consider\nTm = Pm + n\u03c3\u03b8\u03bbmin (1\u2212 \u03c3\u03b8\u03bbmin)\n\u03ba2(Q)L2G Qm.\nNow consider the dynamics of Tm. We have\nTm+1 = Pm+1 + n\u03c3\u03b8\u03bbmin (1\u2212 \u03c3\u03b8\u03bbmin)\n\u03ba2(Q)L2G Qm+1 \u2264 ( 1\u2212 2\u03c3\u03b8\u03bbmin + 2\u03c32\u03b8\u03ba2 (Q)L2G ) Pm + 2\u03c3 2 \u03b8Qm\n+ n\u03c3\u03b8\u03bbmin(1\u2212\u03c3\u03b8\u03bbmin)\n\u03ba2(Q)L2G\n( \u03ba2(Q)L2G\nn Pm+ n\u22121 n Qm ) = ( 1\u2212 \u03c3\u03b8\u03bbmin + 2\u03c32\u03b8\u03ba2(Q)L2G \u2212 \u03c32\u03b8\u03bb2min ) Pm\n+ 2\u03c32\u03b8\u03ba 2(Q)L2G+(n\u22121)\u03c3\u03b8\u03bbmin(1\u2212\u03c3\u03b8\u03bbmin) \u03ba2(Q)L2G Qm.\nLet\u2019s define\n\u03c1 = \u03c3\u03b8\u03bbmin \u2212 2\u03c32\u03b8\u03ba2(Q)L2G.\nThe coefficient for Pm in the previous inequality can be upper bounded by 1\u2212 \u03c1 because 1\u2212 \u03c1\u2212 \u03c32\u03b8\u03bb2min \u2264 1\u2212 \u03c1. Then we have\nTm+1\n\u2264 (1\u2212 \u03c1)Pm+ 2\u03c32\u03b8\u03ba\n2 (Q)L2G+(n\u22121)\u03c3\u03b8\u03bbmin (1\u2212\u03c3\u03b8\u03bbmin) \u03ba2(Q)L2G Qm\n= (1\u2212 \u03c1) ( Pm +\nn\u03c3\u03b8\u03bbmin (1\u2212 \u03c3\u03b8\u03bbmin) \u03ba2(Q)L2G Qm ) + \u03c3\u03b8 2\u03c3\u03b8\u03ba 2(Q)L2G + (n\u03c1\u2212 1)\u03bbmin(1\u2212 \u03c3\u03b8\u03bbmin)\n\u03ba2(Q)L2G Qm\n= (1\u2212 \u03c1)Tm\n+ \u03c3\u03b8 2\u03c3\u03b8\u03ba 2(Q)L2G + (n\u03c1\u2212 1)\u03bbmin(1\u2212 \u03c3\u03b8\u03bbmin) \u03ba2(Q)L2G Qm.\n(60)\nNext we show that with the step size\n\u03c3\u03b8 = \u03bbmin\n3 (\u03ba2 (Q)L2G + n\u03bb 2 min)\n(61)\n(or smaller), the second term on the right-hand side of (60) is non-positive. To see this, we first notice that with this choice of \u03c3\u03b8, we have\n\u03bb2min 9 (\u03ba2 (Q)L2G+n\u03bb 2 min) \u2264 \u03c1 \u2264 \u03bb 2 min 3 (\u03ba2 (Q)L2G+n\u03bb 2 min) ,\nwhich implies\nn\u03c1\u2212 1 \u2264 n\u03bb 2 min\n3 (\u03ba2 (Q)L2G+n\u03bb 2 min)\n\u2212 1 \u2264 1 3 \u2212 1 = \u22122 3 .\nThen, it holds that\n2\u03c3\u03b8\u03ba 2(Q)L2G + (n\u03c1\u2212 1)\u03bbmin(1\u2212 \u03c3\u03b8\u03bbmin)\n\u22642\u03c3\u03b8\u03ba2(Q)L2G \u2212 2\n3 \u03bbmin(1\u2212 \u03c3\u03b8\u03bbmin)\n=\u2212 (6n\u2212 2)\u03bb 3 min\n9 (\u03ba2(Q)L2G + n\u03bb 2 min)\n< 0.\nTherefore (60) implies\nTm+1 \u2264 (1\u2212 \u03c1)Tm.\nNotice that Pm \u2264 Tm and Q0 = P0. Therefore we have T0 \u2264 2P0 and\nPm \u2264 2(1\u2212 \u03c1)mP0.\nUsing (61), we have\n\u03c1 = \u03c3\u03b8\u03bbmin(G)\u2212 2\u03c32\u03b8\u03ba2(Q)L2G \u2265 \u03bb2min 9 ( \u03ba2(Q)L2G + n\u03bb 2 min ) . To achieve Pm \u2264 , we need at most\nm = O (( n+\n\u03ba2 (Q)L2G \u03bb2min\n) log ( P0 )) iterations. Substituting (37) and (33) in the above bound, we get the desired iteration complexity\nO (( n+\n\u03ba(C\u0302)L2G\n\u03bb2min(\u03c1I + A\u0302 T C\u0302\u22121A\u0302)\n) log ( P0 )) .\nFinally, using the bounds in (33) and (37), we can replace the step size in (61) by\n\u03c3\u03b8 = \u00b5\u03c1 3 (\n8\u03ba2(C\u0302)L2G + n\u00b5 2 \u03c1 ) , where \u00b5\u03c1 = \u03bb2min(\u03c1I + A\u0302 T C\u0302\u22121A\u0302) as defined in (14)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.", "creator": "LaTeX with hyperref package"}}}