{"id": "1702.08623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Progress Estimation and Phase Detection for Sequential Processes", "abstract": "We implemented a real-time sensor-based system for modeling, detecting and estimating the completeness of a process. We implemented a multimodal CNN-LSTM structure to extract spatial and temporal characteristics from different sensory data types. We used a novel deep regression structure for the overall completeness estimation. By combining process completeness estimation with a Gaussian blend model, our system can predict the process phase based on the estimated completeness. We also introduced the reflected hyperbolic tangent function (rtanh) and conditional loss to support the training process. Based on the completeness estimation result and performance velocity calculations, we also implemented an online estimator of the remaining time. We tested this system based on data obtained from a medical process trauma (resuscitation) and a conditional loss to support the training process.", "histories": [["v1", "Tue, 28 Feb 2017 03:11:33 GMT  (2027kb,D)", "http://arxiv.org/abs/1702.08623v1", "16 pages, 11 figures"], ["v2", "Thu, 29 Jun 2017 05:04:49 GMT  (2091kb,D)", "http://arxiv.org/abs/1702.08623v2", "Accepted by IMWUT/Ubicomp 2017"], ["v3", "Fri, 14 Jul 2017 20:01:31 GMT  (2024kb,D)", "http://arxiv.org/abs/1702.08623v3", "Accepted by IMWUT/Ubicomp 2017"]], "COMMENTS": "16 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["xinyu li", "yanyi zhang", "jianyu zhang", "yueyang chen", "shuhong chen", "yue gu", "moliang zhou", "richard a farneth", "ivan marsic", "randall s burd"], "accepted": false, "id": "1702.08623"}, "pdf": {"name": "1702.08623.pdf", "metadata": {"source": "CRF", "title": "Process Progress Estimation and Phase Detection", "authors": ["Xinyu Li", "Yanyi Zhang", "Jianyu Zhang", "Yueyang Chen", "Shuhong Chen", "Yue Gu", "Ivan Marsic", "Richard A. Farneth", "Randall S. Burd"], "emails": ["marsic}@rutgers.edu", "rburd}@childrensnational.org"], "sections": [{"heading": "1 Introduction", "text": "Sensor data at a specific time instance defines a fixed scenario, such as a photo of a person holding a carrot while cooking. A sequence of data collected at each time instance defines activities. For example, a frame of a video clip of a person chopping a carrot while cooking. A sequence of activities defines a process. For instance, chopping vegetables, applying sauce, and stirring defines the process of making salad. Several successful systems have been introduced for image classification [1] and activity recognition [13]. We take the next step in this type of analysis by proposing a system for real-time process progress estimation. Online progress estimation has many real-world applications in automated human computer interaction systems. For example, online detection of sports phase can be used to guide the camera on overhead drones for video broadcasting. Online medical process progress information, such as the completeness of resuscitation process and the time remaining to the end (time-remaining), can help medical providers organize medical resources and schedule treatment timing. We designed our system to model and estimate the process progress using sensor data from three aspects:\n1. The process phase that indicates the current stage of the process. We analyzed a trauma resuscitation dataset and an Olympic swimming dataset each with six phases (Table 1).\n2. The process completeness that indicates the percent completion of the whole process.\n3. The time-remaining that represents the estimated time left for the process to accomplish.\nar X\niv :1\n70 2.\n08 62\n3v 1\n[ cs\n.L G\n] 2\n8 Fe\nPrevious research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems. Previous approaches using domain knowledge based methods and certain instrument only worked for specific applications, such as a system specialized for the laparoscopic appendectomy [14]. Sensor data may be hard to obtain in medical settings and surgical instruments may be expensive and hard to deploy. Wearable sensors may interfere with work in many applications. Features obtained via feature extraction do not generalize well and each sensory type may require arbitrary \u201cbest features\u201d [13]. Most importantly, these existing systems have treated the problem as multiclass classification by partitioning the process into several independent phases rather than considering the process as a continuous procedure. These approaches do not address the associations between process phase and percentage completion, allowing the system to make phase predictions that do not always follow the logical order [12].\nTo address these issues and challenges, we introduced a multimodal deep regression structure that first extracts the spatio-temporal features from different sensor types, and then performs regression for completeness estimation and phase prediction. Our proposed system processes data in three steps. First, it extracts and learns the spatio-temporal associations from input data using a multimodal deep learning structure. Second, regression is used for percent-completeness estimation. Unlike regression models that use the polynomial function or a combination of linear functions to generate regression value, we used a deep neural network with a single neuron in the output layer to generate the regression result. We introduced the rectified hyperbolic tangent (rtanh) activation function to ensure that the regression output stays within the bounds of process completeness values. The phase prediction was made based on the regression results using a Gaussian Mixture Model (GMM) based decision making approach. We also introduced a conditional loss to train the system based on the errors made by the regression and phase detection. The third and final step is the estimation of the time-remaining using the estimated overall completeness as indicator.\nWe introduce and tested our model with two datasets: a medical dataset recorded in 35 actual trauma resuscitations at Children\u2019s National Medical Center (CNMC) using installed sensors (include depth camera and microphone array) [12], and an Olympic swimming dataset including 60 YouTube videos of Olympic swimming competitions in different swimming styles. Our process progress estimation system achieved average 87% online process phase detection accuracy for the resuscitation dataset, outperforming existing systems applied in same settings, and 80% accuracy for the Olympic swimming dataset. The system had less than 9% completeness estimation error with average 6.5 minutes (14% total duration) time-remaining estimation error for trauma resuscitation dataset, and less than 5% completeness estimation error with average 2.2 minutes (18% total duration) time-remaining estimation error for Olympic swimming dataset. Our system outperformed the previously proposed systems using the trauma resusciation dataset [21, 22]. Our resuscitation process phase detection system has been deployed in a trauma room and is generating data that can be used in future applications. The contributions of this paper are:\n1. A novel deep regression-based approach for process progress estimation and phase detection using\ncommercially available sensors, as well as a new rectified hyperbolic tangent (rtanh) activation.\n2. A GMM-based phase detection approach that uses the completeness regression results, as well as a conditional loss function for model tuning using regression and classification error.\n3. The trauma resuscitation dataset and Olympic swimming dataset that can be used in future research, as well as the details of the deployment of our system in an actual medical setting (which can be used as reference for other applications).\nThe rest of the paper is organized as follows: Section 2 introduces the problem; Section 3 presents our proposed method; Section 4 describes experimental results and performance comparison to other systems; Section 5 discusses the results; and Section 6 concludes the paper."}, {"heading": "2 Problem Description and Related Work", "text": "Modeling the progress of a process can be considered from three aspects: the completeness, the process phase, and remaining time. Few previous studies have addressed process progress estimation and several challenges remain for the process modeling and estimation using sensor data [2,12]. Classification models,which can only generate discrete predictions, are not as suitable for process completeness estimation, which is a continuous variable, as they are suitable for activity recognition [7]. Without an overall completeness estimation, the system might generate irrelevant phase predictions (such as the patient leaving the trauma room before arriving). The duration of process performance depends on many factors and can vary significantly. For example, trauma resuscitation duration may depend on injury type, severity, preexisting conditions, and the availability of different team roles. The process\u2019s pace performance may also change during the same event, so the remaining time estimates may need to be dynamically updated. Finally, the real-world applications requires online process progress estimation using sensor data. Sensor data collected in real-world settings, however is noisy and cannot be simply filtered or modeled because of the diversity of application scenarios.\nPrevious published research has focused only on the process phase detection. The research initially tried to approach the process modeling using key activities estimated based on the output from surgical instruments [19]. Directly using these signals provided \u201cclean\u201d data compared to mobile sensor data, but also required expensive and hard-to-deploy equipment. Endoscope video recordings were used for workflow analysis with HMM [3]. However, this approach does not generalize easily to other surgical processes.\nFor economical reasons and ease of deployment, mobile sensors such as passive RFID and depth sensor have been used for trauma resuscitation process phase detection [2, 12]. These sensors lead to data that is noisier compared with the data collected by medical equipment. Deep Convolutional Neural Networks (CNNs) have been implemented for surgical phase detection with endoscope image [15, 22]. Our previous research used multimodal deep learning with low resolution depth images and microphone array audio for trauma resuscitation phase estimation [12], which is both privacy-preserving and easy to deploy. This system, however used a purely CNN based structure which is only able to learn the spatial feature in the input depth frame and ignore the temporal association between depth frames in time. More recent work has used the temporal association for better process phase and activity modeling [10,11]. Using both spatial and temporal features, we propose a deep online regression model capable of simultaneously predicting process phase, completeness, and the time-remaining for the process."}, {"heading": "3 System Structure", "text": ""}, {"heading": "3.1 Overall Structure", "text": "Our model consists of three steps (Figure 1): Feature extraction: Because we used different sensors for data collection in different applications, the first step is data representation and feature extraction. Instead of several types of manually crafted features, we applied CNN and LSTM to learn the spatio-temporal features from the data [16]. We used a multimodal structure to fuse the features extracted from different sensory data types.\nCompleteness and Phase estimation: We implemented a regression model based on deep learning that directly produces a single output value as the regression output. We introduced a rtanh activation function for the output neuron and a conditional loss function to train the regression model. We further used the GMM as probabilistic inference to make phase detection and generated conditional loss for model training.\nEstimation of Time-remaining: Our system dynamically updates the time-remaining estimation based on the process execution speed, which we defined as the rate at which one percent of the process is being accomplished."}, {"heading": "3.2 Feature Extraction", "text": "Similar to activity recognition [7,13], the estimation of process progress relies on both temporal and spatial features. The learnable filters in CNN are commonly used to extract the spatial features [23], and longshort-term memory networks (LSTM) are often used to model temporal dependencies of sequential data [6]. Given different sensor types, different data representations and CNN-LSTM structures are required. Our dataset from trauma resuscitations contained low-resolution depth images and audio from a Kinect [12]. Other types of sensors can be easily added and combined into our system by adding additional multimodal branches. We chose the Kinect depth sensor for our medical application because it is privacy preserving (does not capture any facial details) and does not require active user participation to support the sensing. For relatively texture-less depth images, we used an AlexNet [9] (Figure 2, top). For audio data, we extracted MFSC feature maps [1] for every second, and fed it into an AlexNet [9].\nWe implemented the CNN-LSTM structure for spatio-temporal feature extraction and used a multimodal structure for feature fusion from different sensor datatypes. The features extracted from different sensors were later combined in a fusion layer, based on a previous implementation [12] (Figure 2 top). We implemented a smaller neural network (using smaller input images) works on a mini PC (Intel NUC) mounted on a side wall in the trauma room using the same configuration as previously used [13]. This implementation will allow us to eventually provide contemporaneous information about work progress and process phase.\nThe Olympic swimming dataset was collected from YouTube videos recorded using different types cameras (including cellphone cameras and professional cameras). This dataset contained videos of different resolutions and audio recorded by cameras at different distances. We first down-sampled the RGB frames to one frame per second, and resized them to 256\u00d7256px. We used a VGG Net structure with 11 convolutional layers for image feature extraction, because RGB images contain more textural details than depth images. The weights from previous trained VGG Net were directly used for initialization [17]. As for the trauma resuscitation dataset, an AlexNet was used to extract features from MFSC feature maps (Figure 2, bottom)."}, {"heading": "3.3 Regression", "text": ""}, {"heading": "3.3.1 Regression and Completeness Estimation", "text": "Unlike image classification or activity recognition, which use the data captured at a given moment, process completeness is continuous, and cannot be solved as a discrete classification problem. For this reason, we propose a regression-fitting model that uses the extracted spatio-temporal features for process completeness estimation (Figure 3, completeness estimation part).\nGiven an appropriate neural output activation function, a deep neural network extracts features from the input data and performs regression by assigning it an overall completeness value (between zero and one). The regression can be expressed as:\ny\u0302 = f( N\u2211 i=1 (wixi + bi)) (1)\nwhere, wi is the weight connected the i th neuron in the last fully connected layer (with N neurons in total) to the output neuron and xi is the output value of the i th neuron in the last fully connected layer (Figure 3, fully connected layer 2). The bi is the bias term and the f(\u00b7) is the activation function of the output neuron. To model process progress, our activation function must have the following properties: (1) the activation output should be zero if the process has not started (e.g. pre-arrival phase of the trauma resuscitation). (2) the regression value changes from zero to one as the process proceeds. (3) the activation should be equal one if the process has been completed (e.g., after the patient leaves the trauma room). We modified the hyperbolic tangent function for this purpose, and we named it as rectified hyperbolic tangent (rtanh):\nrtanh(x) = max(0, tanh(x)) = max(0, e2x \u2212 1 e2x + 1 ) (2)\nThis activation function generates zero for negative inputs and incremental positive values up to one for positive inputs. This value range makes it suitable for returning an progress completeness percentage.\nThe sigmoid function has the same value range, but its range of positive values is distributed across all real numbers, making it hard to train. Similar to a classifier, a regression model can be trained using backpropagation. Completeness should be labeled differently for different applications. For example, the completeness of a trauma resuscitation is zero before the patient enters the room and remains one after the patient leaves. Given the time at which it was collected relative to the process\u2019s beginning and end, we can label the data at certain time instance with completeness range the time instance falls in. We divided the data into 20\u00d7(5%-segments), and labeled each segment with an associated completeness between 0% and 100% witch forms stepwise function with 5% increments. The output of the rtanh neuron can be directly used as the regression result (Figure 3, decision making part). To make the regression smooth, we applied a Gaussian smooth filter after the rtanh neuron. The loss from completeness estimation error Lc, which can be expressed as:\nLc(\u03b8 = {w, b}, D) = \u2211|D|\ni=0 abs(R(\u03b8 = {w, b}, Di)\u2212 pi) |D|\n(3)\nwhere the \u03b8 = {w, b} denotes the model with parameter set \u03b8 and input dataset D. R(\u03b8 = {w, b}, Di) denotes the regression output from the model with parameter set \u03b8 on dataset D at time instance i as input. pi denotes the label for regression. We used the mean absolute error for loss, which is the abs(\u00b7) term in the loss function. Although the error measurements such as mean squared error could be used, the square error would inhibit training by making the loss even smaller which makes the training slower considering that the regression error can be a small number."}, {"heading": "3.3.2 Phase Detection and Conditional Loss", "text": "The regression model can intuitively estimate completeness but cannot predict phase because the regression only produces continuous results. A separate classification model using the same extracted features can be used for phase detection [12], but detecting phase independently from completeness ignores the associations between phase and the completeness (e.g., the phase distribution along completeness), making the deep neural network less computational efficiency and less accurate.\nTraining the Gaussian mixture model only requires obtaining the phase distribution with overall completeness, which is obtainable from the ground truth data. We can pre-train the GMM using ground truth and establish the probabilistic association between the overall completeness and process phase. We set the number of centroids equal to the number of phases, because each Gaussian distribution is used to model the distribution of one phase. The phase recognition results can be calculated from:\np\u0302 = argmax i {log(w)\u2212 2 log(det(\u03a3)) + 1 2 (x\u2212 \u00b5)T\u03a3\u22121(x\u2212 \u00b5)} (4)\nwhere p\u0302 is the predicated phase. w (N \u00d71) is the weight for each Gaussian kernel, \u00b5 (N \u00d71) is the vector of means of all Gaussian kernels. The \u03a3 (N \u00d7N) denotes the covariance matrix. N is the number of phases. argmax\ni {\u00b7} denotes the function finding the index i with the largest likelihood among all N indices.\nWith equation 4 and the regression result, we can directly use the GMM for phase prediction (Figure 4). We only applied the GMM for four out of the six phases in the trauma resuscitation dataset, removing pre-arrival and patient-leave. This is because the duration of pre-arrival phase depends on the transport time from the injury scene to the hospital, which our data set doesn\u2019t contain, and the patient-leave phase only takes one second in the coding. The GMM-based phase prediction approach relies only on the regression result, and the phase prediction error would not have impacted the backpropagation training process used to tune the regression. The regression model can be trained based on both the completeness estimation error and the phase prediction error. Moreover, we introduced an additional training loss from phase prediction error. We assume that the regression value is correct, and allows the GMM to make the correct phase prediction. Based on this assumption, we designed the system to generate no additional loss if the GMMbased phase detection result was correct. Otherwise, we take the distance from regression result to the mean of the actual current phase distribution as the additional loss from phase prediction error. In conclusion, the loss from phase Lossp is conditional based on the phase detection result and can be expressed as:\nLossp =\n{ 0 p\u0302 = p\n|rtanh(Dp)\u2212 \u00b5p| p\u0302 6= p (5)\nwhere the p\u0302 denotes the GMM predicted phase and p is the actual current phase. Dp is the input data for phase p and rtanh(Dp) denotes the regression model output. The \u00b5p is the mean of the distribution for the actual current phase p. By combining the loss from regression error and the classification error, the regression model can be tuned to make completeness estimations and phase predictions that follow a logical order. The loss used for backpropagation is defined as:\nLoss = Lossc + Lossp (6)\nTraining the system with proposed loss can be intuitively understood as tuning a regression model to fit both the overall completeness ground truth and phase ground truth."}, {"heading": "3.4 Time Left Estimation", "text": "The speed of a process\u2019s performance can be different, so our time-remaining estimator requires a dynamicallyupdating strategy. We estimated the time remaining by calculating how long that system takes to change 1%. If we use p for the current completeness (in percentage) and t for time elapsed since starting, the time-remaining can be estimated as:\ntime remaining = (t/p)\u00d7 (1\u2212 p) (7)\nwhere (t/p) denotes the time that system takes to complete and (1 \u2212 p) denotes the remaining percentage for the process to complete."}, {"heading": "3.5 System Implementation", "text": "We implemented our model with the Keras framework and TensorFlow backend. Because our proposed rtanh activation and loss function do not exist in Keras, we developed these functions for this application. As proposed in previous research [9], we used the rectified linear unit (ReLU) as our CNN activation function. For the trauma resuscitation dataset, we initialized and trained the model using one GTX 1080 GPU. For the YouTube Olympic swimming dataset that has a larger input data format and larger network structure, we used dual GTX 1080 GPUs for training. We directly used the trained weights in VGG Net [17] to initialize the network for RGB frame processing. An Adam optimizer [8] was implemented with initial learning rate 0.01 and 10\u22128 decay. We configured the system to stop automatically if the performance did not change for three epochs using the Keras callback function.\nDue to large model size, overfitting was an issue. We adopted dropout [18] for the network to address this issue. We also partitioned the training and testing set by whole case instead of segmenting each case for evaluation (to minimize similarities between training and testing sets) as suggested in [12].\nUnlike image classification models with a target in each training image, process phase classification may be significantly different from case to case (e.g., treating patients with different injuries or performing different swimming styles). Entering data into the classifier simultaneously causes slow convergence. For this reason,\nwe initially feed only two cases into the classifier for training. When the system achieved a specified loss value (manually defined), we added one more case of data into the classifier. Using this approach, the model learned specific scenarios rapidly and was later able to learn to discriminate similar classes in other cases.\nDuring the training, we added weights \u03b1 in from of Lossc and \u03b2 in front of Lossp in equation 6:\nLoss = \u03b1Lossc + \u03b2Lossp (8)\nTraining the system with larger \u03b1 would cause the system to prioritize overall completeness regression instead of minimizing phase prediction errors. Larger \u03b2 would do the opposite."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Data Collection", "text": "Trauma Resuscitation Dataset: The trauma resuscitation dataset was collected in a trauma room at the Children\u2019s National Medical Center in Washington D.C. Use of this data for research purpose has been approved by the IRB. One hundred and fifty trauma resuscitations with seriously injured patients were manually coded for ground truth. The data was collected through a Kinect depth sensor mounted on the side wall of the trauma room [12,13] (Figure 5, left). Of the 150 trauma resuscitations, 50 had synchronized depth data and 35 out of 50 had synchronized audio data. We used these 150 coded cases to generate the GMM phase distributions, and 35 cases with both depth and audio data for model training and testing. Given the different patient conditions and times of the day, the durations of resuscitation phases varied (Figure 5, right). Olympic Swimming Dataset: The Olympic swimming dataset includes 60 videos from 2004 to 2016 manually downloaded from YouTube. The videos were recorded by different devices and at different angles. We coded the six phases for swimming competitions manually. We manually divided some of the downloaded videos to ensure all the video clips contained only swimming-related content. Because some videos did not record pre-competition phase, some cases did not have all six phases."}, {"heading": "4.2 Evaluation of Process Progress Estimation", "text": "We evaluated the proposed system with the trauma resuscitation dataset and the Olympic swimming dataset for estimating overall completeness, phase, and time-remaining. We first calculated the overall completeness estimation error of the system by inputting (one-by-one) 20% of cases into the trained network for progress estimation using mean absolute error. The system achieved average 9.3% overall completeness error for trauma resuscitations and 5.2% error for Olympic swimming dataset. We then visualized Mean Absolute Error (MAE) of the completeness estimation with normalized process duration of testing cases (Figure\n6). Large MAE indicates that the system has difficulty to distinguish similar scenarios and large variance indicates that the system does not generalize well for all testing cases. We noticed that larger estimation error always comes with a larger variance, indicating that system does not generalize well for some similar phases in our testing cases. Training system with more data can miltigate the generalization issue.\nWe further evaluate the phase prediction error (Figure. 7) from the completeness estimation error (Figure. 6) and the GMM based probablistic distribution for each phase, which gives more informative understanding of our model. Our evaluation showed that the system performed well in pre-arrival and patient-leave phases of the trauma resuscitation dataset, as well as the pre-game and result phase of the swimming dataset. The good performance during these starting and ending phases is probably due to the proposed rtanh function\u2019s ability to hold regression output at zero or one at the beginning or end. The high error rate in the postsecondary phase may be related to the high similarity between the secondary and post-secondary phases. The competition and replay phases of the Olympic swimming dataset may have a higher error rate for the same reason that the replay phase is basically a slow-motion replaying of the competition. However, given a 1fps sampling rate, the two phases can barely be distinguished. For this reason, the LSTM may confuse the replay phase with the actual competition phase. These results suggest that system performance can be further improved by: 1. adding other sensors to provide more input features and better distinguish the phases with similar sensor data and 2. using a higher frame rate for data pre-processing to provide more sufficeint temporal association details. Because almost no related research for progress estimation exists, few examples are available for comparing the performance of our system. In a building construction application [4], the processes are often labeled based on the availability of different construction components, and are generally invariantly sequential. The process completeness for activity sequences is more challenging, because the activities may be performed in different orders and durations.\nFor the process phase prediction, we plotted the confusion matrices of the phase prediction results (Figure 8). The system achieves average 87.03% accuracy for trauma resuscitation phase prediction and average\n80.06% accuracy for the Olympic swimming data.\nConfusion Matrix of Trauma Resuscitation Dataset Confusion Matrix of Olympic Swimming Dataset\nOur analysis of the confusion matrix showed that the proposed system is able to make reasonable phase predictions (Figure 8). The zero values in most of the upper and lower triangles of the matrices indicate that the system only makes adjacent phase prediction errors. This finding shows that the system makes phase predictions with a certain order and will not jump between non-adjacent orders. By further comparing the confusion matrices of both datasets, we found that:\n\u2022 The system accurately predicted the starting and ending phase, due to rtanh\u2019s ability to maintain the zeros and ones for the associated starting and ending phases.\n\u2022 The phase detection performance is associated with regression performance. For example, the regression and phase systems poorly predicted the post-secondary compared to other phases (7). This issue is related to the use of the regression output as input for GMM phase detection.\n\u2022 The system achieved similar performance for both datasets, despite the differences in camera mobility. The trauma resuscitation dataset was recorded with a fixed camera, while the Olympic swimming dataset was captured by moving cameras. The fixed camera always captures the activity\u2019s scene and provides continuous activity sequence information, while the moving camera might capture some discontinuous unrelated data. Our approach achieved similar performance under both scenarios, showing its potential for analyzing the temporal activity processes regardless of the video input\u2019s changing viewpoints. The system is able to achieve similar performance with both fixed and moving cameras because of the CNN-LSTM structure\u2019s ability to learn and extract very different temporal features for decision making.\nGround truth phase coding was performed manually. To compare our system\u2019s phase prediction performance with that of domain experts, 10 additional cases were coded. To better simulate the approach used by our system, we started the video clips from the beginning of the trauma resuscitation and stopped at a random time instance before the moment the patient left trauma room. Using this approach, the domain experts could not calculate the overall completeness by looking at the video\u2019s progress bar. Our results (Table 2) showed that humans are slightly more accurate, but significantly slower (30 times slower than machine) at coding phases than our system. Coders used specific indicators for process phase detection, such as certain medical activities or indicative object usage. Recognizing certain activities requires both experience and domain knowledge. This knowledge may be hard for a person to obtain from only depth videos and audio. It was difficult for the domain experts to estimate overall completeness and almost impossible for them to estimate the remaining time.\nWe also calculated the time-remaining estimation error made by the system, which was 6.5 minutes(14% total duration) for the trauma resuscitation dataset and 2.2 minutes(18% total duration) for the Olympic swimming dataset. Similar to calculation of completeness error, we evaluate the time-remaining errors for different phases (Figure 9). We found that the time-remaining estimation error was not associated with completeness estimation error, but associated with completeness. A small time-remaining estimation error in an earlier stage may lead to a large error during future phases, as the estimated process speed will be multiplied by the percentage left in the process. Comparing the time-remaining estimation error of the trauma resuscitation dataset with the Olympic swimming, we found that the error was also associated with process length (Figure 5, right); the longer processes had greater time-remaining estimation errors."}, {"heading": "4.3 Comparison of Phase Prediction to Previous Work", "text": "We further compared the proposed system to our previous work. Since we are the first to propose completeness estimation and time-remaining estimation, we only compare the system performance of process phase detection with previous research. We used the same trauma resuscitation dataset that was previously used in this previous study [12] and the EndoTube dataset [22], which has seven phases previously used to evaluate EndoNet (with the same training and testing split). Finally, we used the TUM LapChole dataset [20] which has been used in previous surgical process phase analysis [22].\nThe proposed model achieved significantly higher performance compared with our previous model [12] (Figure 10). A major drawback of the previous work was its reliance on spatial information and ignorance of temporal associations [12]. The previous model [12] could make incorrect predictions that do not follow common sense, e.g., predicting primary phase before patient arrival. To address this issue, we previously proposed a constraint softmax layer. Because the constraint softmax requires information across a window centered at the current time instance, the decision making must wait for future data. With such limitation, the previous system can only be used for post-event analysis. The system in this paper instead relies on the incremental nature of the regression curve to make logically ordered predictions (i.e., patient arrival will happen before the primary survey). For this reason, the proposed system can work online. We further compared the phase prediction performance (Figure 10). The results show that the proposed system outperformed the existing systems (Table 3). The proposed system maintained the same F-score for pre-arrival and patient-leave, but significantly improved the scores for other phases.\nWe next analyzed the EndoTube dataset, which was a part of the endoscope video challenge in 2015 and was recently used in EndoNet [22], and the TUM LapChole dataset used in the M2CAI workflow recognition challenge at the MICCAI conference 2016 [20]. Since the EndoTube dataset and TUM LapChole dataset contains repeated workflow (with repeatedly performed surgical phases), we have to manually remove the repeated instances of the workflow during the training phase. Our results confirm that our CNN-LSTM and regression structure better models the temporal associations of the data, resulting in better performance. The experiments using both datasets again proved the importance of temporal associations in process phase detection, as our system again outperformed the previous purely CNN-based system [22].\nWe also provide a detailed comparison of our system with other previous phase detection implementations (Table 3). Because not all of the previous work has been evaluated with the same dataset, the evaluation scores may not reflect the system performance in each setting. Our system achieved adequate performance\nbased on the F-measure. The system with best performance [5] can only predict high-level phase based on manually generated low-level activity logs. This is a significant limitation for real-world applications. The automatically generated low-level activity log using sensor data might contain errors, and may significantly influence the system performance. Moreover, our system is well generalized to work with all types of trauma resuscitations. Previous resuscitation based research were trained on the certain datasets with specific patient injuries, but our system is trained on cases with a variety of patient injuries, ages and conditions. In terms of data collection sensors, our system uses only a cheap, easily-deployed, commercially available depth camera\nalong with microphone array. Other early systems required human input, specific medical equipment or obtrusive wearable sensors."}, {"heading": "5 Discussion", "text": ""}, {"heading": "5.1 Application \u2014 Online Trauma Resuscitation Progress Detection", "text": "Instead of testing the proposed system on recorded data only, we deployed the proposed system in one of the trauma rooms at CNMC. As shown (Figure 5, left), we mounted the entire system (a Kinect depth sensor and an Intel NUC mini PC) on the side wall of the trauma room. Since the mini PC uses a low power CPU with no dedicated GPU, we had to shrink the input data size to ensure its capability of running an online feedforward network. Considering that real time MFSC feature extraction is computationally expensive, we used only depth video for resuscitation progress estimation. Since the mini PC has very limited runtime computational resources, we further resized all the online captured depth images down to 64\u00d764px before feeding them into the network. Using depth-only and down-sampling do impact system performance (Figure 11). Fortunately, the problem can be addressed by using more powerful computers than a mini PC in the future, and based on our experiments the system performance drop is not very significant.\nAs we mentioned earlier, we trained the model with Keras using the TensorFlow backend. Since a C# environment is required for Kinect-based depth image capturing, we established a local TCP communication to connect the C# data capturing system to the feedforward network running on a python terminal. We trained the model with depth images captured during 50 cases and tested the system with 10 actual trauma resuscitations. The system achieved 8% overall progress estimation error and 86% phase detection accuracy (Figure 11). The accuracy is slightly lower than systems using larger depth images with MFSC data as input, but the system performance is still applicable for many applications."}, {"heading": "5.2 Limitations", "text": "The proposed system works well with several different datasets recorded in real-world scenarios. However, the completeness labels we used were only based on relative time. As previously argued, the time of some complex process can be affected by many factors (e.g., the availability of medical resources affects the waiting time of post-secondary phase), and long wait times do not always indicate the change of progress. Instead of labeling the data using time-based completeness, it might be reasonable to use activity combination indicators. For example, if the left-eye and right-eye are both checked, then 5% of the trauma resuscitation has been finished. Labeling the detailed activity with the associated completeness will require more manual coding work. Besides, for complex datasets like medical activity sequences, it is hard to determine an activity combination\u2019s contribution to overall completeness given different patient conditions. More work and collaboration with the medical team is necessary to design a phase recognition model that determines process progress based on both time and activities performed.\nBesides, the current system assumes the workflow follows a certain order, therefore the training process prefers that input data follows a certain order. The system may not work well if the process is not fully linear workflow like the EndoTube dataset and the TUM LapChole dataset. This is because the GMM based approach assumes a single normal distribution for each phase which cannot be used to describe the repeated\nphase. Training a model that is able to model the process with different process order will be our future work."}, {"heading": "5.3 Extension \u2014 Smart Human Computer Interaction", "text": "Online progress estimation is not the goal, since the online prediction itself is not very valuable. However, progress detection information is precious in creating smarter high-level human computer interaction systems to improve work efficiency.\nThe proposed system has great potential in many real-world applications. For example, a possible extension of the system is an online trauma resuscitation check list system. Currently the check list is used in the trauma room to assist medical teams with resuscitation managemnet and to ensure that no required tasks are omitted. The challenge is that trauma resuscitations are fast-paced and the doctors might forget to check certain items during the resuscitation. The online phase detection system can be used to remind the medical team to check certain items at the end of each process phase. The future online activity recognition system will provide more contextual information to make the check list digital and smarter."}, {"heading": "6 Conclusion", "text": "We introduce a system capable of estimating the progress of complex processes. We provide two realworld datasets collected from different commercially available sensors and used several published datasets to evaluate the system. The system outperformed previous research and can be extended to many real-world applications providing fundamental information for advanced human computer interaction. We introduced our deployment of the system in an actual trauma room for online resuscitation progress estimation and a potential extension of the system. We hope the paper can deliver the following benefits to the community:\n1. A regression based system structure for process completeness estimation.\n2. The GMM based approach and conditional loss that can be used with the regression model for classification tasks.\n3. The detailed system deployment introductions that can be used as a guideline to extend the system to other fields.\n4. The trauma resuscitation dataset and Olympic swimming dataset will be published with ground truth labels, which can be used for future study."}, {"heading": "7 Acknowledgement", "text": "This research was supported by the National Institutes of Health under Award Number R01LM011834. The authors would like to thank Colin Lea for providing the EndoTube dataset."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "Mohamed", "A.-r", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on audio, speech, and language processing 22,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Phase recognition during surgical procedures using embedded and body-worn sensors", "author": ["J.E. Bardram", "A. Doryab", "R.M. Jensen", "P.M. Lange", "K.L. Nielsen", "S.T. Petersen"], "venue": "In Pervasive Computing and Communications (PerCom),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Modeling and online recognition of surgical phases using hidden markov models", "author": ["T. Blum", "N. Padoy", "H. Feu\u00dfner", "N. Navab"], "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Vision-based material recognition for automated monitoring of construction progress and generating building information modeling from unordered site image collections", "author": ["A. Dimitrov", "M. Golparvar-Fard"], "venue": "Advanced Engineering Informatics 28,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Automatic phase prediction from low-level surgical activities. International journal of computer assisted radiology and surgery", "author": ["G. Forestier", "L. Riffaud", "P. Jannin"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "An end-to-end generative framework for video segmentation and recognition", "author": ["H. Kuehne", "J. Gall", "T. Serre"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning convolutional action primitives for fine-grained action recognition", "author": ["C. Lea", "R. Vidal", "G.D. Hager"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Online process phase detection using multimodal deep learning", "author": ["X. Li", "Y. Zhang", "M. Li", "S. Chen", "F.R. Austin", "I. Marsic", "R.S. Burd"], "venue": "In Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON), IEEE Annual (2016),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Deep learning for rfid-based activity recognition", "author": ["X. Li", "Y. Zhang", "I. Marsic", "A. Sarcevic", "R.S. Burd"], "venue": "In Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM (2016),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Statistical modeling and recognition of surgical workflow", "author": ["N. Padoy", "T. Blum", "Ahmadi", "S.-A", "H. Feussner", "Berger", "M.-O", "N. Navab"], "venue": "Medical image analysis 16,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Temporal segmentation of laparoscopic videos into surgical phases", "author": ["M.J. Primus", "K. Schoeffmann", "L. B\u00f6sz\u00f6rmenyi"], "venue": "In Content-Based Multimedia Indexing (CBMI),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Random forests for phase detection in surgical workflow analysis", "author": ["R. Stauder", "A. Okur", "L. Peter", "A. Schneider", "M. Kranzfelder", "H. Feussner", "N. Navab"], "venue": "In International Conference on Information Processing in Computer-Assisted Interventions", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The tum lapchole dataset for the m2cai 2016 workflow challenge", "author": ["R. Stauder", "D. Ostler", "M. Kranzfelder", "S. Koller", "H. Feu\u00dfner", "N. Navab"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Combining embedded accelerometers with computer vision for recognizing food preparation activities", "author": ["S. Stein", "S.J. McKenna"], "venue": "In Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing (2013),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Endonet: A deep architecture for recognition tasks on laparoscopic videos", "author": ["A.P. Twinanda", "S. Shehata", "D. Mutter", "J. Marescaux", "M. de Mathelin", "N. Padoy"], "venue": "IEEE Transactions on Medical Imaging 36,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Visualizing and understanding convolutional networks. In European conference on computer vision", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Several successful systems have been introduced for image classification [1] and activity recognition [13].", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Several successful systems have been introduced for image classification [1] and activity recognition [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 4, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 14, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "Previous approaches using domain knowledge based methods and certain instrument only worked for specific applications, such as a system specialized for the laparoscopic appendectomy [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "Features obtained via feature extraction do not generalize well and each sensory type may require arbitrary \u201cbest features\u201d [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "These approaches do not address the associations between process phase and percentage completion, allowing the system to make phase predictions that do not always follow the logical order [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "We introduce and tested our model with two datasets: a medical dataset recorded in 35 actual trauma resuscitations at Children\u2019s National Medical Center (CNMC) using installed sensors (include depth camera and microphone array) [12], and an Olympic swimming dataset including 60 YouTube videos of Olympic swimming competitions in different swimming styles.", "startOffset": 228, "endOffset": 232}, {"referenceID": 20, "context": "Our system outperformed the previously proposed systems using the trauma resusciation dataset [21, 22].", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "Our system outperformed the previously proposed systems using the trauma resusciation dataset [21, 22].", "startOffset": 94, "endOffset": 102}, {"referenceID": 1, "context": "Few previous studies have addressed process progress estimation and several challenges remain for the process modeling and estimation using sensor data [2,12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 11, "context": "Few previous studies have addressed process progress estimation and several challenges remain for the process modeling and estimation using sensor data [2,12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "Classification models,which can only generate discrete predictions, are not as suitable for process completeness estimation, which is a continuous variable, as they are suitable for activity recognition [7].", "startOffset": 203, "endOffset": 206}, {"referenceID": 18, "context": "The research initially tried to approach the process modeling using key activities estimated based on the output from surgical instruments [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "Endoscope video recordings were used for workflow analysis with HMM [3].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "For economical reasons and ease of deployment, mobile sensors such as passive RFID and depth sensor have been used for trauma resuscitation process phase detection [2, 12].", "startOffset": 164, "endOffset": 171}, {"referenceID": 11, "context": "For economical reasons and ease of deployment, mobile sensors such as passive RFID and depth sensor have been used for trauma resuscitation process phase detection [2, 12].", "startOffset": 164, "endOffset": 171}, {"referenceID": 14, "context": "Deep Convolutional Neural Networks (CNNs) have been implemented for surgical phase detection with endoscope image [15, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 21, "context": "Deep Convolutional Neural Networks (CNNs) have been implemented for surgical phase detection with endoscope image [15, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 11, "context": "Our previous research used multimodal deep learning with low resolution depth images and microphone array audio for trauma resuscitation phase estimation [12], which is both privacy-preserving and easy to deploy.", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "More recent work has used the temporal association for better process phase and activity modeling [10,11].", "startOffset": 98, "endOffset": 105}, {"referenceID": 10, "context": "More recent work has used the temporal association for better process phase and activity modeling [10,11].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "Instead of several types of manually crafted features, we applied CNN and LSTM to learn the spatio-temporal features from the data [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "2 Feature Extraction Similar to activity recognition [7,13], the estimation of process progress relies on both temporal and spatial features.", "startOffset": 53, "endOffset": 59}, {"referenceID": 12, "context": "2 Feature Extraction Similar to activity recognition [7,13], the estimation of process progress relies on both temporal and spatial features.", "startOffset": 53, "endOffset": 59}, {"referenceID": 22, "context": "The learnable filters in CNN are commonly used to extract the spatial features [23], and longshort-term memory networks (LSTM) are often used to model temporal dependencies of sequential data [6].", "startOffset": 79, "endOffset": 83}, {"referenceID": 5, "context": "The learnable filters in CNN are commonly used to extract the spatial features [23], and longshort-term memory networks (LSTM) are often used to model temporal dependencies of sequential data [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "Our dataset from trauma resuscitations contained low-resolution depth images and audio from a Kinect [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "For relatively texture-less depth images, we used an AlexNet [9] (Figure 2, top).", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "For audio data, we extracted MFSC feature maps [1] for every second, and fed it into an AlexNet [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "For audio data, we extracted MFSC feature maps [1] for every second, and fed it into an AlexNet [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "The features extracted from different sensors were later combined in a fusion layer, based on a previous implementation [12] (Figure 2 top).", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "We implemented a smaller neural network (using smaller input images) works on a mini PC (Intel NUC) mounted on a side wall in the trauma room using the same configuration as previously used [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The weights from previous trained VGG Net were directly used for initialization [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "A separate classification model using the same extracted features can be used for phase detection [12], but detecting phase independently from completeness ignores the associations between phase and the completeness (e.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "As proposed in previous research [9], we used the rectified linear unit (ReLU) as our CNN activation function.", "startOffset": 33, "endOffset": 36}, {"referenceID": 16, "context": "We directly used the trained weights in VGG Net [17] to initialize the network for RGB frame processing.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "An Adam optimizer [8] was implemented with initial learning rate 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": "We adopted dropout [18] for the network to address this issue.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "We also partitioned the training and testing set by whole case instead of segmenting each case for evaluation (to minimize similarities between training and testing sets) as suggested in [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The data was collected through a Kinect depth sensor mounted on the side wall of the trauma room [12,13] (Figure 5, left).", "startOffset": 97, "endOffset": 104}, {"referenceID": 12, "context": "The data was collected through a Kinect depth sensor mounted on the side wall of the trauma room [12,13] (Figure 5, left).", "startOffset": 97, "endOffset": 104}, {"referenceID": 3, "context": "In a building construction application [4], the processes are often labeled based on the availability of different construction components, and are generally invariantly sequential.", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "We used the same trauma resuscitation dataset that was previously used in this previous study [12] and the EndoTube dataset [22], which has seven phases previously used to evaluate EndoNet (with the same training and testing split).", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "We used the same trauma resuscitation dataset that was previously used in this previous study [12] and the EndoTube dataset [22], which has seven phases previously used to evaluate EndoNet (with the same training and testing split).", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Finally, we used the TUM LapChole dataset [20] which has been used in previous surgical process phase analysis [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "Finally, we used the TUM LapChole dataset [20] which has been used in previous surgical process phase analysis [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "The proposed model achieved significantly higher performance compared with our previous model [12] (Figure 10).", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "A major drawback of the previous work was its reliance on spatial information and ignorance of temporal associations [12].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "The previous model [12] could make incorrect predictions that do not follow common sense, e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "We next analyzed the EndoTube dataset, which was a part of the endoscope video challenge in 2015 and was recently used in EndoNet [22], and the TUM LapChole dataset used in the M2CAI workflow recognition challenge at the MICCAI conference 2016 [20].", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "We next analyzed the EndoTube dataset, which was a part of the endoscope video challenge in 2015 and was recently used in EndoNet [22], and the TUM LapChole dataset used in the M2CAI workflow recognition challenge at the MICCAI conference 2016 [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "The experiments using both datasets again proved the importance of temporal associations in process phase detection, as our system again outperformed the previous purely CNN-based system [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "PA A P S PS PL [12] with softmax", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[12] with constraint softmax", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "F-S Trauma Resuscitation Dataset Multimodal CNN [12] Yes Depth and audio Multimodal CNN 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "50 Multimodal CNN [12] Yes Depth and audio Multimodal CNN + constraint softmax 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Endovis Dataset Endonet [22] Yes Endoscope video CNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "TUM LapChole Dataset CNN based approach [20] Yes Laparoscopic video AlexNet + time window n/a 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Other Previous Research Phase detection from lowlevel activities [5] No Activity log Decision Tree n/a 0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Surgical phases detection [3] No Instrument signal HMM 0.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Phase recognition using mobile sensors [2] No Wearable sensor data Decision Tree 0.", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "Surgical workflow modeling [13] No Instrument signal HMM 0.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Food Preparation Activities [21] No RGB video, depth and accelerometer data Random Forest n/a 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "The system with best performance [5] can only predict high-level phase based on manually generated low-level activity logs.", "startOffset": 33, "endOffset": 36}], "year": 2017, "abstractText": "Process modeling and understanding is fundamental for advanced human-computer interfaces and automation systems. Recent research focused on activity recognition, but little work has focused on process progress detection from sensor data. We introduce a real-time, sensor-based system for modeling, recognizing and estimating the completeness of a process. We implemented a multimodal CNN-LSTM structure to extract the spatio-temporal features from different sensory datatypes. We used a novel deep regression structure for overall completeness estimation. By combining process completeness estimation with a Gaussian mixture model, our system can predict the process phase using the estimated completeness. We also introduce the rectified hyperbolic tangent (rtanh) activation function and conditional loss to help the training process. Using the completeness estimation result and performance speed calculations, we also implemented an online estimator of remaining time. We tested this system using data obtained from a medical process (trauma resuscitation) and sport events (swim competition). Our system outperformed existing implementations for phase prediction during trauma resuscitation and achieved over 80% of process phase detection accuracy with less than 9% completeness estimation error and time remaining estimation error less than 18% of duration in both dataset.", "creator": "LaTeX with hyperref package"}}}