{"id": "1702.08745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Optimal Categorical Attribute Transformation for Granularity Change in Relational Databases for Binary Decision Problems in Educational Data Mining", "abstract": "This paper presents an approach to transforming data granularity in hierarchical databases for binary decision problems by applying regression to categorical attributes at the lower corneal levels. Attributes of a lower hierarchical unit in the relational database have optimized their information content by regression to the histogram categories, which are trained using a small, exclusively labeled sample instead of the usual modal category of distribution. It validates the approach of a binary decision task to evaluate the quality of secondary schools, focusing on how logistic regression transforms the attributes of students and teachers into school attributes. Experiments were performed on Brazilian public datasets using a 10-fold cross-validation comparison of the ranking order also generated by logistic regression. The proposed approach achieved a higher performance than the usual transformation in distribution mode and corresponds to the expert weighting approach, which is measured by the maximum Kolmogorov RO01 curve at the Smirnov range and at the signov level.", "histories": [["v1", "Tue, 28 Feb 2017 11:13:17 GMT  (232kb)", "http://arxiv.org/abs/1702.08745v1", "5 pages, 2 figures, 2 tables"]], "COMMENTS": "5 pages, 2 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["paulo j l adeodato", "f\\'abio c pereira", "rosalvo f oliveira neto"], "accepted": false, "id": "1702.08745"}, "pdf": {"name": "1702.08745.pdf", "metadata": {"source": "CRF", "title": "Optimal Categorical Attribute Transformation for Granularity Change in Relational Databases for Binary Decision Problems in Educational Data Mining", "authors": ["Paulo J.L. Adeodato", "F\u00e1bio C. Pereira", "Rosalvo F. Oliveira Neto"], "emails": ["pjla@cin.ufpe.br", "rosalvo.oliveira@univasf.edu.br"], "sections": [{"heading": null, "text": "Keywords\u2014Granularity transformation; Categorical attributes; Educational data mining; Relational databases; Distribution mode;"}, {"heading": "Regression", "text": ""}, {"heading": "ACM-Classification", "text": ""}, {"heading": "I.2 ARTIFICIAL INTELLIGENCE", "text": "H.2.8 Database Applications, J.1 ADMINISTRATIVE DATA PROCESSING-Education;\nI. INTRODUCTION One of the most important aspects in decision support systems (DSS) is to explore all data and information for improving their performance while preserving the domain semantics on the attributes. A difficulty arises when the databases contain information in several granularities different from that of the decision level. This occurs in relational databases where there are 1:n relationships among the entities defined by their tables in various hierarchical levels.\nRelational Data Mining (RDM) [1] is a research area in the borderline between databases and artificial intelligence that handles information in different granularities. Its \"proposicionalization\" approach produces a denormalized table\nin the decision grain [1], as required by most knowledge extraction algorithms. That forces the summarization of lower level attributes\u00b4 distributions in one or few indicators/features on the decision level.\nFig. 1 presents the scenario for a domain such as education, with schools being assessed by their own and their students\u00b4 and their teachers\u00b4 features in a hierarchical structure. \"How to assign the students\u00b4 father education level attribute as a school attribute based on the distribution of its students?\" is an example of question that illustrates the point.\nA further constraint to the granularity concept adopted here is that all lower level grains do not contain objects that form a series or have any type of order relation. That is different from the sequences of events along the temporal dimension considered in RFM analysis (Recency, Frequency and Monetary value analysis) applied in behavior scoring for credit risk analysis [2].\nIn such context, this paper proposes a granularity transformation that uses regression techniques to optimize the information content of single categorical attributes. The transformation maximizes the information gain towards the target variable in binary decision problems, preserving the conceptual value of the attribute in the original lower grain. It represents an improvement of a previous approach, which does a similar transformation based on human expert\u00b4s knowledge if that is available [3]. The proposal is tested on the school quality assessment problem based on their teachers and students\u2019 data.\nThis paper is organized in 5 more sections. Section II presents an overview the topic on related fields and discusses how they deal with the granularity change problem. Section III presents the proposed optimization approach. Section IV describes the experimental project to validate the proposed approach detailing each step from database selection to the performance assessment metrics. Section V presents the results and discusses its impacts. Section VI concludes the paper summarizing the main contributions of the proposed approach, analyzing its limitations and suggesting potential ways for expanding it to other types of attributes and supervised problems.\nII. RELATED FIELDS OF RESEARCH Granularity change has been an important topic in several research areas such as Relational Data Mining (RDM) [1], Granular Computing (GrC) [4] and Function Concept Analysis (FCA) [5] with different approaches, but partial overlap on their objectives towards how to preserve the relevant concepts in the transformation. These areas all deal with high complexity data mining problems in the borderline between databases and artificial intelligence.\nA key aspect in many high complexity data mining problems is the proper understanding and transformation of the relationships among the entities defined by the tables of relational databases. Several areas have proposed different approaches for dealing with the matter from different perspectives and levels of formal background.\nThe database structure associated with the decision-making process workflow defined by the project goals determine the so called grains of information of the problem to be solved. In statistics, the lowest hierarchical level of the database structure (finest granularity) stores the so-called microdata statistics [7]."}, {"heading": "A. Relational Data Mining (RDM)", "text": "Relational Data Mining (RDM) [1] refers to hierarchies, entities, granularities in relational databases for knowledge extraction. For the present context, in most cases, the approaches are focused on aggregation for clustering or discovery of association rules applications [1] which are based on the nonsupervised learning paradigm. Guo and Viktor [6] have proposed an improvement for constructing artificial neural networks classifiers starting with an aggregation stage as well. In general, the RDM approach aims at forming higher level hierarchical concepts by collecting lower level attributes related to them [1,6]. These approaches propose automatic transformations directly from a database without taking into account application other domain characteristics, different from\nthe transformations to capture temporal relations in credit risk analysis [2].\nAggregation can be seen as an operation that summarizes a set of values related to variables; a distribution. In the case of numerical variables, descriptive statistics can be used, such as the minimum value, the mean, the median, the maximum value are the first recommendation. For categorical variables, the typical option is the distribution mode (the most frequent value) [7].\nThe typical way to represent a distribution by a single or few concepts in the decision grain is done by the taking the lowest order momenta of the probability distributions. That idea is present in the moment invariant approaches used for feature extraction in image processing [8], for example. With the exception of anomaly detection such as fraud detection [9], cancer diagnosis [10] or pulmonary embolism diagnosis [11] which uses the distributions\u00b4 extreme values, most applications use the central tendency of the distribution as decision level feature."}, {"heading": "B. Granular Computing (GrC)", "text": "Granular Computing (GrC) [4] is another area of research which also deals with the change of granularity in relational databases. It forms higher level hierarchical concepts by collecting lower level attributes focusing either on clustering or discovery of association rules applications [4] thus based on non-supervised learning. However granular computing applies mainly rough set theory and fuzzy logic as tools for achieving its goals. It must be emphasized that it differs from the approach proposed here in two main aspects: no supervised optimization and no embedding of concept from a single lower level attribute. In general terms, granular computing can be considered as a field of multidisciplinary study, dealing with theories, methodologies, techniques and tools that make use of granules in the process of problem solving [12]."}, {"heading": "C. Formal Concept Analysis (FCA)", "text": "Formal Concept Analysis (FCA) [5] is an approach of very high level of abstraction that, if properly instantiated to very specific conditions, could lead to structures similar to RDM and GrC. It is a branch of applied mathematics and provides a framework called lattice of concepts (based on partial ordering) that presents the relations between objects and values of attributes in diagrammatic format. These structures can be used in data mining tasks [13,14] and make explicit the search space in classification methods. However, the benefits of using conceptual cross-linking are accompanied by costly construction and manipulation [15] in a grid that grows exponentially in relation to the formal context that is composed of objects, attributes, attribute values, and incidence relationships. Furthermore, it is a very broad approach and does not give clear paths on how to optimize the concepts in a univariate supervised learning process."}, {"heading": "D. Weighted Granularity Transform (WGT)", "text": "Recently, a new approach has been proposed for granularity change inspired by and tested on education problems. It is an approach based on human expert\u00b4s knowledge to make the granularity transformation [3] that this paper will refer to as the Weighted Granularity Transform (WGT). The human expert\nsets weights for each category of the attribute to express its propensity towards the binary target in a way to produce a monotonic propensity mapping.\nFor example, for the faculty education distribution of the school, the weights were arbitrarily set as w(PhD)=4, w(MSc)=3, w(MBA)=2 and w(BSc)=1, just preserving an order relation according to the expert\u2019s domain knowledge. The attribute transformation was the weighted sum of the relative frequencies of the lower level categories in a single continuous scalar indicator.\nThat produced a statistically significant improvement in performance compared to the typical distribution mode transformation.\nDespite that improvement, that approach presents some drawbacks this paper attempts to solve:\n1. It can only be applied when there is semantics on the attribute\u00b4s categories,\n2. It depends on human expert\u00b4s capacity to capture that semantics as knowledge\n3. This knowledge has to be expressed at least as an order relation among the attribute\u00b4s categories, usually by setting arbitrary linear weights, and\n4. This human knowledge is not always confirmed on the data.\nIII. PROPOSED ALGORITHM \u2013 REGRESSION GRANULARITY"}, {"heading": "TRANSFORM (RGT)", "text": "As stated in the Introduction, this paper focuses on transforming relational databases, that represent systems composed of subsystems in various hierarchical levels and the example of school quality assessment will be used to illustrate the ideas put forward here. The concept of \"data granularity\" (or \"information granularity\") is embodied in so-called \"grain\" that represent the relationship-entity levels for the desired data mining solution.\nIn this work, the main aspects of understanding the granularity relations in a data mining project are:\n1. Allow the modeler to identify subsystems and their individual and collective features to enable the generation of subsets of data with statistical independence,\n2. Allow the modeler together with the domain human expert identify transformations of high added value for information granularity change and\n3. Allow the modeler to embed statistical knowledge from the data on the transformations for granularity change.\nAll these aspects are taken into account in a Domain-Driven Data Mining (D3M) approach [16] for problem solving.\nDifferent from the approaches presented in Related Fields of Research, the proposed approach is focused on binary decision problems and attempts to optimize the attribute information content towards the target class.\nThe optimization is achieved by regression on the categorical attribute distribution having its histogram with the categories relative frequencies as input and the decision level target as output, as depicted in Fig. 2.\nThe histogram is built on an independent data sample extracted from the modelling dataset used for this sole purpose and discarded afterwards to prevent optimistic bias because of the use of a posteriori information.\nAny type of regression can do the job but logistic regression [17] is recommended for its non-linear mapping capacity, ease of interpretation and for its low data consumption compared to other good techniques such as neural networks.\nThe algorithm can be summarized as follows:\n1. Select the categorical attributes with more than 2 categories;\n2. Extract a data sample stratified by the target class on the decision grain from the modelling data set;\n3. Build the attribute histogram for each example on the decision grain;\n4. Run the regression algorithm for parameter estimation (learning);\n5. Discard this data sample;\n6. Apply the transformation learned through regression to the remaining modelling data sample.\nThe granularity transformation produces a continuous indicator of the categorical attribute optimized to the target variable on the decision grain still preserving the concept of the original lower grain attribute.\nIV. EXPERIMENTAL PROJECT The experiments were carried out on the binary problem of assessing the quality of Brazilian private secondary schools [3,18], using logistic regression as classifier and using the Area Under the Receiver Operating Characteristic (ROC) curve and the Maximum Kolmogorov-Smirnov (KS) distance as performance metrics. The goal was to compare if the proposed algorithm would improve the overall classification performance against the previous weighted score algorithm and the usual\nmode for granularity transformation of the lower level categorical attributes. To verify significant differences, two tailed paired t-test was applied in a stratified 10-fold cross validation procedure at 0.01 significance level [19].\nThe database consisted of 4,400 schools with microdata from the National Secondary School Exam 2012 (ENEM 2012) and the School Census 2012 [18] with the target class defined as the schools having their students\u00b4 average score on the top quartile, as described in Adeodato\u00b4s paper [3].\nOnly the categorical attributes in the lower level grain were considered in the school model. The modelling process was further constrained to the attributes where human expertise could clearly contribute to define the weights. So the only attributes used in the experiments were the Father Education Level and Mother Education Level for the students\u2019 database and Teacher Education Level for the teachers\u2019 database.\nLogistic regressions [17] was the regression technique used both in the categorical attribute transformation and in the global binary classifier for its properties previously mentioned such as non-linear mapping capacity, ease of interpretation, ease of use, low data consumption and continuous output and also for its implementations freely available.\nConsidering the comparison assesses the difference in the discriminant power of the ranking score from all the algorithms, their performances are measured on each data fold by an area metric and a single point metric. The Area Under the Curve of the Receiver Operating Characteristics (AUC_ROC) [20] was calculated by just adding 0.5 to the Area Under the Curve of the Kolmogorov-Smirnov distribution [21]. The point metric was the Maximum Kolmogorov-Smirnov distance between the cumulative distribution functions of the target and complementary classes (Max_KS2) [22]. Both metrics used are invariant to score range and scale.\nV. EXPERIMENTAL RESULTS AND INTERPRETATION The results on Table I show that for both performance metrics the proposed approach (RGT) and the human expert weighted approach (WGT) are systematically better than the mode approach in all folds. However, RGT and WGT have similar performance in all folds.\nThe hypothesis tests (Table II, in boldface figures) show that both approaches that combine the influence of all categories in the attribute (histogram) are significantly different from the mode for granularity change in the binary classification problem presented.\nVI. CONCLUSIONS This paper has presented a systematic approach for producing granularity transformations of categorical attributes in binary decision problems. The concepts in the lower levels are summarized in a single continuous indicator learned from the attributes\u00b4 distribution histograms via regression on an independent sample, as a feature. It preserves the concept of the attribute in the original lower grain adding collective meaning to it, independent of any human knowledge about the application domain.\nThorough experimental procedure proved that the proposed granularity transformation produced statistically significant improvement in performance in the quality assessment of Brazilian secondary schools compared to the typical and simple distribution mode approach. It also showed that in the few cases where human knowledge is available, the proposed approach is equivalent to the existing WGT approach with human defined weights. Therefore, in the general case of having no assumption of human knowledge availability, the proposed approach presents a superior performance in binary decision problems.\nFurther experiments will be carried out on an even more realistic fashion by learning the distributions weights and developing the model from data from the previous education year and assessing the performance on the following year. There are several binary problems yet to be addressed in education at several stages of the process. There are also other domains with characteristics that suit the application of the proposed approach such as breast cancer diagnosis."}, {"heading": "Metrics Approaches Mean Std.Dev. LimInf LimSup p-Val.", "text": "[4] Lin T.Y.: Granular computing, Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing. In Proc. of the 9th Int. Conf. on RSFDGrC, LNCS 2639, pp. 16\u201324, 2003.\n[5] Ganter B., Wille R.: Formal concept analysis: Mathematical foundations. Springer Verlag, 1996.\n[6] Guo H., Viktor H. L.: Multi-view ANNs for multi-relational classification. In Proc. IJCNN 2006, Vancouver-CA, pp. 5259\u20135266, 2006. DOI= http://dx.doi.org/10.1109/IJCNN.2006.247280.\n[7] Johnson R. A., Wichern D. W.: Applied Multivariate Statistical Analysis (6th ed.), 2007.\n[8] Hu M. K.: Visual pattern recognition by moment invariants. IRE Trans. Inform. Theory, 8(2), pp.179\u2013187, 1962.\n[9] Bolton R. J., Hand D. J.: Statistical Fraud Detection: A Review. Statist. Sci., 17(3), pp.235\u2013255, 2002.\n[10] Eltoukhy M. M., Faye I., Samir B.: A statistical based feature extraction method for breast cancer diagnosis in digital mammogram using multiresolution representation. Comput. Biol. Med., 42, pp. 123\u2013128, 2012. DOI = http://dx.doi.org/10.1016/j.compbiomed.2011.10.016.\n[11] Lane T., Rao B., Bi J., Liang J., Salganicoff M.: On the Medical Frontier: The 2006 KDD Cup Competition and Results, 2006.\n[12] Yao Y.Y.: Granular computing: basic issues and possible solutions. In Proc. of the 5th Joint Conference on Information Sciences, 186\u2013189, 2000.\n[13] Priss U.: Formal concept analysis in information science. Annual Review of Information Science and Technology, 2005.\n[14] Stumme G.: Conceptual knowledge discovery and processing. International Journal of Human and Computer Studies, 2003.\n[15] Fu H., Nguifo E.: How well go lattice algorithms on currently used machine learning testbeds? In Proc. 1st International Conference on Formal Concept Analysis, 2003.\n[16] Cao L., Yu, P. S., Zhang, C., Zhang, H.: Introduction to Domain Driven Data Mining, in Data Mining for Business Applications, Springer, 2008.\n[17] Hair Jr. J. F., Black W. C., Babin B. J., Anderson R. E., Tatham R. L.: Multivariate Data Analysis. Chap. 6, (7th ed.) Upper Saddle River, NJ: Pearson Prentice Hall, 2013.\n[18] INEP-Databases. http://portal.inep.gov.br/basica-levantamentos-acessar. Last accessed: Nov. 2016.\n[19] Jain R.: Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling, Chap. 13. Wiley Comp. Pub., New York, USA, 1991. ISBN: 0471503363.\n[20] Provost F., Fawcett T.: Robust Classification for Imprecise Environments. Machine Learning Jour., 42(3) pp. 203\u2013231, Mar. 2001.\n[21] Adeodato P. J. L., Melo S. B.: On the equivalence between KolmogorovSmirnov and ROC curve metrics for binary classification. Cornell University Library ARXIV, 2016arXiv160600496A, 2016. DOI= https://arxiv.org/abs/1606.00496. [22] Conover W.J.: Practical Nonparametric Statistics. Chap. 6, 3rd ed. New York: Wiley, 1999."}], "references": [{"title": "Facets of aggregation approaches to propositionalization", "author": ["A. Krogel M", "S. Wrobel"], "venue": "In Proc. of the Work-in-progress track of the 13 Int. Conf. on Inductive Logic Programming (ILP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A Framework for Data Transformation in Credit Behavioral Scoring Applications Based on Model Driven Development", "author": ["Neto R.F. Oliveira", "L. Adeodato P. J", "C. Salgado A"], "venue": "Expert Systems With Applications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Variable Transformation for Granularity Change in Hierarchical Databases in Actual Data Mining Solutions", "author": ["L. Adeodato P. J"], "venue": "In Proc. IDEAL-", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Granular computing, Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing", "author": ["T.Y. Lin"], "venue": "In Proc. of the 9 Int. Conf. on RSFDGrC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Formal concept analysis: Mathematical foundations", "author": ["B. Ganter", "R. Wille"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Multi-view ANNs for multi-relational classification", "author": ["H. Guo", "L. Viktor H"], "venue": "In Proc. IJCNN 2006, Vancouver-CA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Applied Multivariate Statistical Analysis", "author": ["A. Johnson R", "W. Wichern D"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Visual pattern recognition by moment invariants", "author": ["K. Hu M"], "venue": "IRE Trans. Inform. Theory, 8(2),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1962}, {"title": "Statistical Fraud Detection: A Review", "author": ["J. Bolton R", "J. Hand D"], "venue": "Statist. Sci., 17(3),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A statistical based feature extraction method for breast cancer diagnosis in digital mammogram using multiresolution representation", "author": ["M. Eltoukhy M", "I. Faye", "B. Samir"], "venue": "Comput. Biol. Med.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "On the Medical Frontier", "author": ["T. Lane", "B. Rao", "J. Bi", "J. Liang", "M. Salganicoff"], "venue": "The 2006 KDD Cup Competition and Results,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Granular computing: basic issues and possible solutions", "author": ["Y.Y. Yao"], "venue": "In Proc. of the 5 Joint Conference on Information Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Formal concept analysis in information science", "author": ["U. Priss"], "venue": "Annual Review of Information Science and Technology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Conceptual knowledge discovery and processing", "author": ["G. Stumme"], "venue": "International Journal of Human and Computer Studies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "How well go lattice algorithms on currently used machine learning testbeds", "author": ["H. Fu", "E. Nguifo"], "venue": "In Proc. 1 International Conference on Formal Concept Analysis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Introduction to Domain Driven Data Mining, in Data Mining for Business", "author": ["Cao L", "P.S. Yu", "C. Zhang", "H. Zhang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling, Chap", "author": ["R. Jain"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "Robust Classification for Imprecise Environments", "author": ["F. Provost", "T. Fawcett"], "venue": "Machine Learning Jour.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "On the equivalence between Kolmogorov- Smirnov and ROC curve metrics for binary classification", "author": ["L. Adeodato P. J", "B. Melo S"], "venue": "Cornell University Library ARXIV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Practical Nonparametric Statistics. Chap. 6, 3 ed", "author": ["W.J. Conover"], "venue": "New York: Wiley,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Relational Data Mining (RDM) [1] is a research area in the borderline between databases and artificial intelligence that handles information in different granularities.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Its \"proposicionalization\" approach produces a denormalized table in the decision grain [1], as required by most knowledge extraction algorithms.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "That is different from the sequences of events along the temporal dimension considered in RFM analysis (Recency, Frequency and Monetary value analysis) applied in behavior scoring for credit risk analysis [2].", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "It represents an improvement of a previous approach, which does a similar transformation based on human expert \u0301s knowledge if that is available [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "Granularity change has been an important topic in several research areas such as Relational Data Mining (RDM) [1], Granular Computing (GrC) [4] and Function Concept Analysis (FCA) [5] with different approaches, but partial overlap on their objectives towards how to preserve the relevant concepts in the transformation.", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Granularity change has been an important topic in several research areas such as Relational Data Mining (RDM) [1], Granular Computing (GrC) [4] and Function Concept Analysis (FCA) [5] with different approaches, but partial overlap on their objectives towards how to preserve the relevant concepts in the transformation.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "Granularity change has been an important topic in several research areas such as Relational Data Mining (RDM) [1], Granular Computing (GrC) [4] and Function Concept Analysis (FCA) [5] with different approaches, but partial overlap on their objectives towards how to preserve the relevant concepts in the transformation.", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "In statistics, the lowest hierarchical level of the database structure (finest granularity) stores the so-called microdata statistics [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Relational Data Mining (RDM) [1] refers to hierarchies, entities, granularities in relational databases for knowledge extraction.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "For the present context, in most cases, the approaches are focused on aggregation for clustering or discovery of association rules applications [1] which are based on the nonsupervised learning paradigm.", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Guo and Viktor [6] have proposed an improvement for constructing artificial neural networks classifiers starting with an aggregation stage as well.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "In general, the RDM approach aims at forming higher level hierarchical concepts by collecting lower level attributes related to them [1,6].", "startOffset": 133, "endOffset": 138}, {"referenceID": 5, "context": "In general, the RDM approach aims at forming higher level hierarchical concepts by collecting lower level attributes related to them [1,6].", "startOffset": 133, "endOffset": 138}, {"referenceID": 1, "context": "These approaches propose automatic transformations directly from a database without taking into account application other domain characteristics, different from the transformations to capture temporal relations in credit risk analysis [2].", "startOffset": 235, "endOffset": 238}, {"referenceID": 6, "context": "For categorical variables, the typical option is the distribution mode (the most frequent value) [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "That idea is present in the moment invariant approaches used for feature extraction in image processing [8], for example.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "With the exception of anomaly detection such as fraud detection [9], cancer diagnosis [10] or pulmonary embolism diagnosis [11] which uses the distributions \u0301 extreme values, most applications use the central tendency of the distribution as decision level feature.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "With the exception of anomaly detection such as fraud detection [9], cancer diagnosis [10] or pulmonary embolism diagnosis [11] which uses the distributions \u0301 extreme values, most applications use the central tendency of the distribution as decision level feature.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "With the exception of anomaly detection such as fraud detection [9], cancer diagnosis [10] or pulmonary embolism diagnosis [11] which uses the distributions \u0301 extreme values, most applications use the central tendency of the distribution as decision level feature.", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Granular Computing (GrC) [4] is another area of research which also deals with the change of granularity in relational databases.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "It forms higher level hierarchical concepts by collecting lower level attributes focusing either on clustering or discovery of association rules applications [4] thus based on non-supervised learning.", "startOffset": 158, "endOffset": 161}, {"referenceID": 11, "context": "In general terms, granular computing can be considered as a field of multidisciplinary study, dealing with theories, methodologies, techniques and tools that make use of granules in the process of problem solving [12].", "startOffset": 213, "endOffset": 217}, {"referenceID": 4, "context": "Formal Concept Analysis (FCA) [5] is an approach of very high level of abstraction that, if properly instantiated to very specific conditions, could lead to structures similar to RDM and GrC.", "startOffset": 30, "endOffset": 33}, {"referenceID": 12, "context": "These structures can be used in data mining tasks [13,14] and make explicit the search space in classification methods.", "startOffset": 50, "endOffset": 57}, {"referenceID": 13, "context": "These structures can be used in data mining tasks [13,14] and make explicit the search space in classification methods.", "startOffset": 50, "endOffset": 57}, {"referenceID": 14, "context": "However, the benefits of using conceptual cross-linking are accompanied by costly construction and manipulation [15] in a grid that grows exponentially in relation to the formal context that is composed of objects, attributes, attribute values, and incidence relationships.", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "It is an approach based on human expert \u0301s knowledge to make the granularity transformation [3] that this paper will refer to as the Weighted Granularity Transform (WGT).", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "All these aspects are taken into account in a Domain-Driven Data Mining (D3M) approach [16] for problem solving.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "The experiments were carried out on the binary problem of assessing the quality of Brazilian private secondary schools [3,18], using logistic regression as classifier and using the Area Under the Receiver Operating Characteristic (ROC) curve and the Maximum Kolmogorov-Smirnov (KS) distance as performance metrics.", "startOffset": 119, "endOffset": 125}, {"referenceID": 16, "context": "01 significance level [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "The database consisted of 4,400 schools with microdata from the National Secondary School Exam 2012 (ENEM 2012) and the School Census 2012 [18] with the target class defined as the schools having their students \u0301 average score on the top quartile, as described in Adeodato \u0301s paper [3].", "startOffset": 282, "endOffset": 285}, {"referenceID": 17, "context": "The Area Under the Curve of the Receiver Operating Characteristics (AUC_ROC) [20] was calculated by just adding 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "5 to the Area Under the Curve of the Kolmogorov-Smirnov distribution [21].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "The point metric was the Maximum Kolmogorov-Smirnov distance between the cumulative distribution functions of the target and complementary classes (Max_KS2) [22].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "This paper presents an approach for transforming data granularity in hierarchical databases for binary decision problems by applying regression to categorical attributes at the lower grain levels. Attributes from a lower hierarchy entity in the relational database have their information content optimized through regression on the categories \u0301 histogram trained on a small exclusive labelled sample, instead of the usual mode category of the distribution. The paper validates the approach on a binary decision task for assessing the quality of secondary schools focusing on how logistic regression transforms the students \u0301 and teachers \u0301 attributes into school attributes. Experiments were carried out on Brazilian schools \u0301 public datasets via 10-fold crossvalidation comparison of the ranking score produced also by logistic regression. The proposed approach achieved higher performance than the usual distribution mode transformation and equal to the expert weighing approach measured by the maximum Kolmogorov-Smirnov distance and the area under the ROC curve at 0.01 significance level. Keywords\u2014Granularity transformation; Categorical attributes; Educational data mining; Relational databases; Distribution mode; Regression ACM-Classification I.2 ARTIFICIAL INTELLIGENCE H.2.8 Database Applications, J.1 ADMINISTRATIVE DATA PROCESSING-Education;", "creator": "Word"}}}