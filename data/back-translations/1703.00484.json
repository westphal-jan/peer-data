{"id": "1703.00484", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Truth and Regret in Online Scheduling", "abstract": "We look at a planning problem where a cloud service provider has multiple units of a resource available over time. Selfish customers submit jobs, each with an arrival time, an appointment, a length, and a value. The goal of the service provider is to implement a truthful online workplace planning mechanism to maximize the social well-being of the schedule. Recent work shows that under a stochastic assumption of work arrivals, there is a family of mechanisms that achieve near-optimal social well-being. We show that in the face of such a family of near-optimal online mechanisms, there is an online mechanism that works nearly as well as the best of the mechanisms at worst. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt and achieve optimal regret (within constant factors).", "histories": [["v1", "Wed, 1 Mar 2017 20:09:43 GMT  (135kb,D)", "http://arxiv.org/abs/1703.00484v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.DS cs.LG", "authors": ["shuchi chawla", "nikhil devanur", "janardhan kulkarni", "rad niazadeh"], "accepted": false, "id": "1703.00484"}, "pdf": {"name": "1703.00484.pdf", "metadata": {"source": "CRF", "title": "Truth and Regret in Online Scheduling", "authors": ["Shuchi Chawla", "Nikhil Devanur", "Janardhan Kulkarni"], "emails": ["(shuchi@cs.wisc.edu)", "(nikdev@microsoft.com)", "(jakul@microsoft.com)", "(rad@cs.cornell.edu)"], "sections": [{"heading": null, "text": "We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms.\nWe further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within polylogarithmic factors).\nar X\niv :1\n70 3.\n00 48\n4v 1\n[ cs\n.G T\n] 1\nM ar"}, {"heading": "1 Introduction", "text": "We consider an online mechanism design problem inspired by the allocation and scheduling of cloud services. A scheduler allocates scarce resources to jobs arriving over time with the goal of maximizing economic efficiency or social welfare. The jobs are submitted by selfish users who can lie about the job\u2019s value, length, arrival, or deadline, so as to obtain a better allocation or pay a cheaper price. Our goal is to design an online mechanism that is truthful and obtains good welfare guarantees in the worst case.\nThere is a vast and rich literature on online mechanism design in settings where selfish agents participate in the mechanism over time. What makes the scheduling problem described above interesting is that even ignoring the users\u2019 incentive constraints, there are strong lower bounds in the worst case for the purely algorithmic problem of scheduling jobs with deadlines to maximize welfare. Canetti and Irani (1998) showed, in particular, that no online algorithm can achieve a less than polylogarithmic competitive ratio for this problem in comparison to the hindsight optimal schedule. On the other hand, Lavi and Nisan (2015) showed that no deterministic mechanism that is truthful with respect to all of the parameters can approximate social welfare better than a factor T in the worst case, where T is the time horizon, even for unit length jobs on a single machine. In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al. (2005), satisfying incentive compatibility with respect to only a few of the jobs\u2019 parameters Cole et al. (2008); Azar and Khaitsin (2011), etc.\nA Bayesian benchmark. In this paper we follow an alternate approach of competing against a benchmark inspired by a stochastic model for job arrivals. For many online problems, the hindsight optimum is too pessimistic and strong a benchmark to compete against. A classic example from algorithmic mechanism design is the digital goods auction for which no mechanism can compete against the hindsight optimum that obtains the entire social welfare. But picking the right benchmark to compete against, namely the optimal posted price, has led to the design of many beautiful mechanisms with robust and strong revenue guarantees. Hartline and Roughgarden (2008) advocate a general framework for generating an appropriate benchmark for such online problems\u2014determine the class of all mechanisms that are optimal for the problem in an appropriate stochastic setting; compete against the best of these Bayesian optimal mechanisms for a worst case instance.\nIn this paper we apply the Hartline and Roughgarden approach to online scheduling. We show that for any given finite class of truthful scheduling mechanisms, we can design an online mechanism that is competitive against the best of the given mechanisms. Our mechanism is truthful with respect to all of the parameters of the jobs\u2019 types, is computationally efficient, and requires no assumptions on the input instance. We achieve asymptotically optimal regret guarantees with respect to all of the involved problem parameters.\nWhile the Hartline and Roughgarden agenda has been successfully applied in mechanism design settings, it has not yet seen much use in algorithmic settings where the worst case optimization problem is hard but positive results are known under stochastic assumptions.1 Our hope is that this work will spur future work in this direction.\nOur work is inspired by the recent work of Chawla et al. (2017) that shows that when jobs are drawn from an i.i.d. distribution in every time period, a family of simple mechanisms achieves\n1The idea of combining several online algorithms into one that is nearly as good as the best of them has been explored previously, but in the limited context of metrical task systems and adaptive data structures. See, e.g., Blum and Burch (2000); Blum et al. (2002).\nnear-optimal social welfare. Chawla et al.\u2019s mechanism is a simple greedy best-effort mechanism based on posted prices. The mechanism announces a price per-unit of resource for each time period into the future. When a job arrives it gets scheduled in a best-effort FIFO manner in the cheapest slots that satisfy its requirements. Chawla et al. show that if the number of resources per time period is large enough, then for any underlying distribution over job types, there exists a set of prices such that this posted-pricing-FIFO mechanism achieves a 1 \u2212 approximation to expected social welfare. Unfortunately, finding the best price to offer requires knowing the fine details of the distribution over job types and solving a large linear program.\nChawla et al.\u2019s mechanisms are parameterized by a single price. Machine learning techniques have been succesfully used to tune parameters of heuristics for a wide variety of problems Xu et al. (2008); Hutter et al. (2009). This is typically done in a batch setting, where past data is used to find a good setting of parameters for the heuristic. For an inherently online problem such as scheduling, we seek to do the parameter tuning itself in an online manner. Can we search for the right parameters as we are running the heuristic?\nWe model the online mechanism design problem as a sort of \u201clearning from expert advice\u201d problem where the experts are algorithms in the set of all Bayesian optimal algorithms. We present a general black-box reduction from the online scheduling problem to online learning algorithms for the experts setting. Previous work along these lines has looked at online settings where the mechanism gets a new instance of the problem in every time step, and the learning algorithm can adapt the parameters of the mechanism as more and more instances are seen. In the scheduling setting the algorithms run on a single instance of the problem, and we want to achieve low regret over all Bayesian optimal mechanisms for this specific instance.\nChallenges. There are several novel challenges that the scheduling context imposes. First, in our setting, jobs can grab resources for multiple consecutive units of time. So the decision of scheduling a job in the current time step can affect the availability of resources in many future steps. Furthermore it is unclear at what time an algorithm should receive credit for scheduling a job \u2013 when the job starts, or when it ends, or throughout its execution? What if the algorithm uses preemption?\nSecond, while an online algorithm for the experts problem can cleanly switch from one expert to another during execution, in the scheduling setting switching can be tricky. On the one hand, we cannot abandon jobs that have already been started but not finished by the previous algorithm. On the other hand, we must try to match the state of the new algorithm that we are following, so as to obtain the same reward. Most importantly, switching impacts the incentive properties of the underlying mechanisms. Indeed there are several ways in which jobs may lie to affect their outcomes in the combined mechanism, even if they cannot in the underlying algorithms. For example, jobs may be able to influence the time at which a switch happens, or the next algorithm that is selected. Moreover, a job may be able to benefit indirectly by influencing the schedule of the algorithm that is used just prior to the algorithm that determines the job\u2019s allocation.\nOur techniques. We design a switching procedure that overcomes all of these challenges. Each switch takes a few time steps to finish previously scheduled jobs, synchronize with the state of the new mechanism, as well as handle the scheduling of intermediate jobs in a manner so as to preserve their incentive structure. As a consequence of this \u201cswitching delay\u201d our algorithm incurs a bounded cost for every switch. Using this switching procedure in tandem with a reduction to experts with switching costs allow us to obtain a sublinear regret guarantee in addition to truthfulness.\nOur truthful switching technique is oblivious to the details of the underlying truthful mechanisms and works as long as the underlying mechanisms are prompt, that is, they announce the allocation\nand payment of a job right at the time of the job\u2019s arrival. To our knowledge this is the first result that combines truthful mechanisms online into an overall truthful mechanism.\nNon-clairvoyance. Finally, one challenge faced by schedulers in a real-world setting is that jobs may not know how long they would take to run. Fortunately, the benchmark suggested by Chawla et al. (2017), namely the optimal posted-pricing-FIFO mechanism, continues to obtain near-optimal welfare\u2014the mechanism does not need to know jobs\u2019 lengths in order to make scheduling decisions, although the existence of a good posted price assumes that lengths along with other job parameters are drawn from some i.i.d. distribution.\nNon-clairvoyance poses extra challenges in the online scheduling setting. We can no longer keep track of different algorithms\u2019 rewards or their states at the time of a switch. Synchronization with an algorithm\u2019s state at the time of a switch is crucial because for some algorithms, such as posted-price-FIFO in particular, out of sync execution can cost almost the entire social welfare of the algorithm. (See, e.g., Example 4.1.)2 We therefore consider a slightly modified benchmark where the performance of an algorithm is measured according to the welfare it accumulates if it is periodically (randomly) restarted with an empty state. These random restarts do not significantly affect the performance of algorithms such as posted-price-FIFO in the stochastic setting.\nWe give a reduction from the non-clairvoyant setting to a multi-armed bandit (MAB) problem. The main challenge in this reduction is to couple the random restarts of the expert algorithms with the times at which the MAB algorithm decides to switch experts, in a manner that ensures that the restarts are independent of the internal coin flips of the MAB algorithm. To do so, we partition the scheduling process into mini-batches synchronized with random restarts, and run the MAB algorithm over this batched instance. Because switching between algorithms happens exactly at the time of a random restart, it becomes possible for us to sync with the states of the expert algorithms.\nRandom restarts can once again break the incentive properties of the overall mechanism. We need to take care to ensure that jobs that are caught in the middle of a restart cannot benefit by misreporting their arrival or deadline. This necessitates a careful redesign of the switching protocol for non-clairvoyant settings. As in the clairvoyant setting, we obtain the optimal dependence within polylogarithmic factors of the regret on the time horizon.\nOutline. In Section 3 we present a truthful online algorithm for the clairvoyant setting along with an upper bound on its regret. We extend both the truthfulness and regret guarantees to the non-clairvoyant setting in Section 4. Section 5 presents matching lower bounds on regret."}, {"heading": "2 Model and definitions", "text": ""}, {"heading": "2.1 The online job scheduling problem", "text": "An instance of the online job scheduling problem consists of a finite set of jobs J , a time horizon T , and the number m of resources (machines) available per unit of time. Each job j \u2208 J arrives at time aj \u2208 [T ], has a deadline dj \u2208 [aj , aj + dmax] and a processing length lj \u2208 [0, `max]. Assume all the deadlines are in the time horizon [T ]. Completing each time unit of job j \u2208 J generates a value-per-length vj \u2208 [0, vmax].\n2 We also show that in a continuous time setting no algorithm can get a sub-linear regret, in Appendix C.\nOnline scheduling algorithms. An online scheduler is an algorithm that determines which jobs to schedule and when, and how much to charge each scheduled job. Each job j, at the time of its arrival, reports its arrival time, deadline, length, and value; this four-tuple is called the job\u2019s type. The scheduling algorithm determines whether or not to schedule the job (admission control step) and, if the job is scheduled, maps it to a set \u03c4j of time units (scheduling step) and charges it a payment pj . If |\u03c4j \u2229 [aj , dj ]| \u2265 lj , that is, the job is allocated at least lj time units before its deadline, then the job obtains a utility of (vj \u2212 pj) \u00b7 lj . The schedule produced by the algorithm is feasible if no more than m jobs are assigned to each time unit.\nWe now discuss various features of online scheduling algorithms:\nPreemption: We say that the algorithm is non-preemptive if the set \u03c4j consists of contiguous time units for every job j. In other words, when a job is started, the algorithm processes it without pausing until it is finished.\nTruthfulness: A scheduling algorithm is truthful if for every job j, fixing the reported types of jobs in J\u2212j , job j\u2019s utility is maximized by reporting its true type. Jobs can misreport any of the four components of their type, however, following convention we assume that jobs cannot report an earlier arrival time.\nPromptness: A scheduling algorithm is prompt if for every job j, the job\u2019s allocation and payment, (\u03c4j , pj), are determined at the time of the job\u2019s arrival. At times we will refer to a weaker property: an algorithm is order respecting if for every job j, the job\u2019s allocation and payment, (\u03c4j , pj), are functions of jobs in J that arrive prior to j and not of those jobs that arrive after j.\nClairvoyance: The clairvoyant scheduling problem is the setting where every job j reports its length lj to the scheduling mechanism, together with other parts of its type, upon its arrival, whereas in the non-clairvoyant scheduling problem jobs do not report their lengths upon arrival. In fact, the scheduling mechanism observes the length of a job only after it completes the job. Since the length of job j is unknown prior to its completion in the non-clairvoyant scheduling, we have to slightly modify other aspects of the setting:\n\u2013 we change the definition of deadline dj to denote the latest time that j can be started.3\n\u2013 We do not allow preemption in the non-clairvoyant setting.\nWith these two modifications, it is indeed guaranteed that if a job j is allocated at a time no later than its deadline, then it will be scheduled properly, i.e. it will be given enough time to be completed.\nLet ONL denote an online scheduling algorithm, and let J(ONL) = J \u2229 {j : |\u03c4j \u2229 [aj , dj ]| \u2265 lj} denote the set of jobs that receive service in ONL. We use Wt(ONL,J ) to denote the value generated by the algorithm at time unit t:\nWt(ONL,J ) = \u2211\nj\u2208J(ONL): t\u2208\u03c4j\nvj\nThe total value generated by the algorithm, a.k.a. its social welfare, is given by:\nW(ONL,J ) = T\u2211 t=1 Wt(ONL,J ) = \u2211 j\u2208J(ONL) vjlj\nWe drop the argument J when it is clear from the context. 3Note the difference with deadlines in the clairvoyant setting, where dj was defined to be the latest time that j could be completed.\nRegret minimization in online scheduling. We consider an online learning problem, where we are given a finite set of scheduling algorithms and our goal is to compete with the best one in hindsight with respect to the social welfare objective. Let {ALG1, . . . , ALGn} be the set of n online schedulers. Given an instance J , let OPT(J ) = maxi\u2208[n]W(ALGi,J ) denote the social welfare obtained by the hindsight optimal algorithm on this instance. Let ONL denote our online scheduling algorithm. The regret of ONL is defined as:\nReg(ONL) , max J (OPT(J )\u2212W(ONL,J ))"}, {"heading": "2.2 Learning from expert advice", "text": "We will reduce the regret minimization problem for online scheduling to the problem of learning from expert advice. In the latter, we are given n experts indexed by i. In each time step t \u2208 [T ], the online algorithm must choose a (potentially random) expert, it \u2208 [n], to follow. An adversary then reveals a reward vector {r(i)t }. We assume that the adversary is oblivious, that is, it cannot observe the internal coin flips of the algorithm. The total payoff of expert i is given by \u2211 t\u2208[T ] r (i) t .\nThe payoff of the algorithm is given by E [\u2211\nt\u2208[T ] r (ii) t\n] , where the expectation is taken over the\nalgorithm\u2019s internal coin flips. The regret of the algorithm is:max i\u2208[n] \u2211 t\u2208[T ] r (i) t \u2212E \u2211 t\u2208[T ] r (ii) t  Let R denote an upper bound on r(i)t for any i \u2208 [n] and t \u2208 [T ]. Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).\nExperts with switching costs. This is a variant of the problem of learning from expert advice in which the algorithm faces a switching cost of C units every time it switches from one expert to another in consecutive time steps. In particular, the payoff of the algorithm is given by E [\u2211\nt\u2208[T ] r (ii) t ] \u2212C |{t \u2208 [T ] : it 6= it\u22121}|. The first term corresponds to the rewards and the second\ncorresponds to the switching cost. Accordingly, the regret of the algorithm is:max i\u2208[n] \u2211 t\u2208[T ] r (i) t \u2212E \u2211 t\u2208[T ] r (ii) t + C E[|{t \u2208 [T ] : it 6= it\u22121}|] Theorem 2.1 (Kalai and Vempala (2005)). There is an algorithm Expert-ALG(C) for the experts problem with a switching cost of C such that\nReg(Expert-ALG(C)) \u2264 O (\u221a R(R+ C)T log n ) .\nMulti-armed bandit setting. In the multi-armed bandit (MAB) setting, the online algorithm may only observe the reward r(it)t of the expert that it selects at time t, and cannot observe the remaining rewards. Algorithms for MAB typically mix some exploration alongside following the recommendation of an online learning algorithm for the full-information setting.\nTheorem 2.2 (Auer et al. (1995)). There is an algorithm Bandit-ALG for the MAB problem with Reg(Bandit-ALG) \u2264 O ( R \u221a Tn log n ) .\nWe can further consider an extension of the MAB setting to the setting with a switching cost. For this problem, we mainly use that there is no algorithm with a regret of o(T 2/3) (Dekel et al., 2014), to get a similar lower bound for our problem. The precise statement of their result is in Section 5."}, {"heading": "3 The clairvoyant setting", "text": "In this section we consider the online scheduling problem in the clairvoyant setting, namely where every job reports its length (in addition to the rest of its type) at the time of its arrival. We are given n online scheduling algorithms, ALG1, . . . , ALGn, and our goal is to design an online algorithm that minimizes regret relative to the best of the n algorithms in hindsight. We begin by showing how to switch between algorithms in a way that preserves truthfulness in Section 3.1. In Section 3.2 we present a reduction from this problem to the problem of learning from expert advice with switching costs. In Section 5 we prove that the regret guarantee we obtain from the reduction is optimal."}, {"heading": "3.1 Truthful switching", "text": "In this section, we show how to switch between truthful mechanisms while preserving truthfulness and making sure the loss in welfare is bounded. We consider the following setting. Let A and B be two order respecting truthful scheduling mechanisms. Our goal is to switch from mechanism A to mechanism B at time 0. (This is just a normalization of the time index for ease of notation.) We consider show how to perform this switch in the clairvoyant setting, and extend our algorithm to the non-clairvoyant setting in Section 4.2. The loss in welfare from our switching algorithm is captured in the following lemma.\nLemma 3.1. Given order respecting truthful mechanisms A and B, there exists an order respecting truthful mechanism C that obtains welfare at least\u2211\nt\u22640 Wt(A) + \u2211 t\u22651 Wt(B)\u2212 2vmaxdmaxm.\nIn particular, all jobs that arrive by time 0 and are completed by mechanism A are also completed by C. We can compose any number of \u201cswitching\u201d steps, losing an additive 2vmaxdmaxm amount in welfare each time.\nTheorem 3.2. Suppose we wish to switch among many order respecting truthful mechanisms as follows: start with A0 at time 1, switch to A1 at time t1, then to A2 at time t2 and so on till you switch to AL at time tL for some L \u2208 Z+. Let t0 = 0 and tL+1 = T for notational convenience. Then there is an order respecting truthful mechanism whose welfare is at least\nL\u2211 i=0 \u2211 t\u2208(ti,ti+1] Wt(Ai)\u2212 2Lvmaxmdmax.\nProof Let B1 be the mechanism obtained by applying Lemma 3.1 to switch from A0 to A1 at time t1. Apply the lemma again to switch from B1 to A2 at time t2; let the resulting mechanism be B2. Continuing this way, we apply the lemma to switch from Bi to Ai+1 at time ti+1, to get mechanism Bi+1, for all i up to L\u2212 1. The resulting mechanism at the end, BL, is the one we want.\nIn the rest of this section we prove Lemma 3.1."}, {"heading": "3.1.1 The online switching algorithm", "text": "Definition 3.1. The mechanism C (Lemma 3.1) is as follows. 1. For jobs that arrive by time 0, mimic mechanism A and return the same allocation, schedule\nand prices. Observe that jobs that are scheduled in this step are terminated by time dmax.\n2. Mark the remaining time slots in [1, dmax] as unavailable. This means that for all jobs j that were not considered in the previous step (because aj > 0) and have deadline dj \u2264 dmax, we decline service and charge a price of 0.\n3. For all remaining jobs, i.e., jobs j with aj > 0 and dj > dmax, consider the jobs in the order of arrival, and do the following:\n(a) If B rejects j, then reject j.\n(b) If there are not enough slots available to cover j\u2019s length prior to its deadline, reject j.\n(c) Otherwise, accept and schedule j in a \u201cbest effort\u201d manner. Specifically, assign to the job all of the slots that it gets in B and that are still available in C\u2019s schedule. If any of these slots is unavailable, replace it with the earliest available slot in C\u2019s schedule. We call these newly assigned slots the \u201creplacement\u201d slots for job j. Charge j the same payment as in mechanism B.\nDesign choices. We explain the design choices made in the above mechanism. In step (1) we continue to process jobs that arrive by time 0 according to mechanim A. If we abruptly stop mechanism A, then there may be an incentive for some jobs to lie so that they get scheduled by time 0. In step (2) we make the remaining slots unavailable. Why not directly go to step (3) and schedule jobs that B has accepted in a best effort manner? One of the properties we need for truthfulness to hold is that jobs that arrive after time 0 finish at a time in mechanism C that is no earlier than their finish time in mechanism B. Otherwise there may be an incentive for a job to lie so that it gets accepted in B but is scheduled to finish after its true deadline, whereas mechanism C ends up scheduling it within its true deadline. Lemma 3.3 below shows that the algorithm C satisfies this property In step (3) (a) if we start considering jobs that B rejected because we have some more available slots than B, we might break the truthfulness of B. Finally, in step (3) (c) we first assign the same slots to the job as in B in order to ensure the no early completion property. Assigning the remaining available slots in the chronological order is crucial for the welfare analysis."}, {"heading": "3.1.2 Truthfulness", "text": "We begin by proving the no early completion property.\nLemma 3.3. Any replacement slot assigned in step (3) (c) is always later in time relative to the unavailable slot it replaces.\nProof Suppose one of the slots assigned to a job j in mechanism B, say at time t, is unavailable in mechanism C. If t \u2264 dmax then by construction the replacement slot is later. Otherwise, t itself is a replacement slot for some other job j\u2032. The arrival time of j\u2032, aj\u2032 is no larger than aj because jobs are processed in FIFO order. Since replacement slots are assigned in chronological order, all slots in [aj\u2032 , t\u2212 1] must have been unavailable when t was assigned to j\u2032. Now all slots in [aj , t] are unavailable when we consider job j, so its replacement for slot t can only be later.\nLemma 3.4. Mechanism C is truthful.\nProof We consider three cases depending on which step of the mechanism handles the job. Recall that we assume that jobs cannot report an earlier arrival time. 1. Suppose that aj \u2264 0, which means that the job gets processed in step (1) and gets an allocation\nand payment as per mechansim A. If the job reports an arrival time > 0, then it is not processed in step (1) and gets none of the slots in [aj , dj ], because dj \u2264 dmax, and all those slots are marked unavailable at the beginning of step (2). Any other misreport means that the job still gets processed in step (1). Now we can appeal to the fact that algorithm A is truthful to assert that the job does not benefit from misreporting its type.\n2. Suppose that aj > 0 and dj \u2264 dmax, which means it is processed in step (2). In this case, regardless of its actual report the job gets no slots in its time window.\n3. Suppose that aj > 0 and dj > dmax, which means it is processed in step (3). Since the job cannot report an earlier arrival time, it cannot be processed in step (1), and reporting a deadline \u2264 dmax means it gets no slots. Hence the only misreports we need to consider are such that the job is still processed in step (3). The truthfulness of B should now imply that no misreport can be beneficial in C as well. This is almost true since, for instance, the price paid is the same in both (on acceptance). However, there is a possibility that misreporting a later deadline in B (possibly combined with a misreport of other parameters) results in a lower price, but that in B\u2019s schedule the job finishes after its true deadline dj . This would be a non-beneficial misreport in B but could be beneficial in C if it actually finishes earlier than dj in C, while enjoying the lower price. Lemma 3.3 ensures this does not happen."}, {"heading": "3.1.3 Welfare", "text": "Define a time slot t > dmax to be \u201cfree\u201d if mechanism C schedules fewer jobs in time t than mechanism B. The number of free slots at time t is the difference, given that it is non-negative, and zero otherwise. We first argue that there are few free slots in C\u2019s schedule.\nLemma 3.5. All replacement slots occur before the first free slot.\nProof Let t be the first free slot. Consider a job that arrives before t. This job is not assigned any replacement slots after t since t is free and hence available, and replacement slots are assigned in chronological order. We will argue that jobs arriving after t have no replacement slots, i.e., they get the same slots as in B. This is by induction on the arrival order of these jobs. Consider the very first such job. All earlier jobs arrive before t by definition, and have no replacement slots after t as already argued, therefore all of the slots assigned to this job in B\u2019s schedule are available. This is the base case. The argument for the inductive case is almost exactly the same.\nLemma 3.6. The total number of free slots is at most mdmax. In particular, if t is the earliest time of a free slot, then all the free slots are in the interval [t, t+ dmax]. In other words, mechanisms B and C get synchronized after time t+ dmax.\nProof From Lemma 3.5, there are no replacement slots after t. Any job that arrives at t or later and is scheduled in B gets the same slots in C as in B, and hence there are no free slots corresponding to such a job. All the free slots must correspond to jobs that arrive before t, and are therefore in the interval [t, t+ dmax].\nWe are now ready to prove the main lemma of this section.\nProof of Lemma 3.1 It is easy to see that mechanism C is also order respecting. Since we already showed that the mechanism is truthful in Lemma 3.4, we only need to argue about the welfare.\nAny job that arrives before time 0 and is accepted by A is also accepted by C and completed, therefore it gets the same welfare as A upto time 0. Now we argue about the total loss in welfare during the time t \u2265 1. Let `B(t) (resp. `C(t)) be the number of jobs scehduled at time t \u2265 1 by mechanism B (resp. mechanism C), and let `F (t) be the number of free slots at time t. By the definition of a free slot, we have that\u2211\nt\u22651 `B(t) \u2264 \u2211 t\u2265dmax+1 (`C(t) + `F (t)) +mdmax.\nThe set of jobs accepted by C is a subset of the set of jobs accepted by B, due to steps (2) and (3a). The total length of all jobs that are accepted by B but not by C is equal to \u2211 t\u22651 (`B(t)\u2212 `C(t)) \u2264\u2211\nt\u2265dmax+1 `F (t) +mdmax \u2264 2mdmax, where the last inequality is from Lemma 3.6. Thus the total loss is at most 2mvmaxdmax."}, {"heading": "3.2 Reduction to experts with switching costs", "text": "Let Expert-ALG(C) denote an online algorithm for the problem of learning from expert advice with switching cost C that achieves the regret guarantee of Theorem 2.1. Expert-ALG is given an instance with n experts, indexed by i \u2208 [n]. It specifies for every time step t \u2208 [T ] a random expert it, and then receives a reward vector {r(i)t }. Our online scheduling algorithm, that we call Follow-The-Switcher or FTS, simulates Expert-ALG in a black-box fashion and follows its advice on which expert, a.k.a. algorithm, to run at every time step.\nDefinition 3.2. Given the n online scheduling algorithms, ALG1, . . . , ALGn, the Follow-The-Switcher, a.k.a. FTS, algorithm simulates the online algorithm Expert-ALG(C) with C set to 2vmaxdmaxm. It then proceeds as follows.\nAt each time t \u2208 [T ]: 1. Simulate algorithms ALG1, . . . , ALGn on the freshly arrived set of jobs.\n2. Query Expert-ALG to obtain the index it \u2208 [n]. 3. If it 6= it\u22121, then switch from ALGit\u22121 to ALGit as described in Section 3.1. Otherwise continue\nrunning the same algorithm ALGit\u22121 = ALGit .\n4. Set r(i)t \u2190Wt(ALGit) for all i \u2208 [n]. Send the reward vector {r (i) t } to Expert-ALG.\nThe following theorem now immediately follows from Theorem 3.2.\nTheorem 3.7. Let C = 2vmaxdmaxm. Then the Follow-The-Switcher (FTS) algorithm, described in Definition 3.2, admits the following regret-bound:\nReg(FTS) \u2264 Reg(Expert-ALG(C)) \u2264 O ( mvmax \u221a Tdmax log n ) ."}, {"heading": "4 Non-clairvoyant setting", "text": "In this section, we look at regret minimization in the non-clairvoyant setting; we recall the main differences here. Every job reports all parts of its type except its length, and the algorithm only observes the length of a job when (and if) it is completed. Hence a non-clairvoyant algorithm cannot\nplan for a complete schedule ahead of time. The algorithm maintains a queue of unfinished jobs and at every time t decides which job to schedule from this queue at that time. The deadline for a job is now the number of time slots that the job is willing to wait out. If a job passes its deadline, which means that the number of time slots that a job j waits since its arrival exceeds a threshold dj , then the job is deleted from the queue. Due to this reason, the notion of promptness is not quite applicable to the non-clairvoyant setting. In its place, we use the order respecting property, which states that only jobs arriving earlier can influence the allocation and payments for a given job. We assume that there is a total order on the arrival time of the jobs, by breaking ties arbitrarily in case multiple jobs arrive at the same time.\nSimilar to the clairvoyant setting, we have a set of n online scheduling algorithms, ALG1, . . . , ALGn, and we aim to design an online algorithm that minimizes the regret relative to the best of these algorithms in hindsight. We begin our discussion in Section 4.1, where we show that this benchmark by itself is impossible to compete with, which motivates a reasonable modification. In Section 4.2 we show how to switch between two mechanisms truthfully. Finally in Section 4.3, we show how to use multi-armed bandit algorithms to get tight regret bounds."}, {"heading": "4.1 Scheduling algorithms with random restarts", "text": "Robustness of the benchmark. In the non-clairvoyant setting, it is easy to come up with examples showing that the welfare obtained by an online scheduling mechanism is very sensitive to timing in the adversarial model of jobs, i.e., by slightly changing the starting time of the mechanism the obtained welfare can be drastically different. This has been demonstrated in Example 4.1.\nExample Consider running FIFO scheduling with pricing admission control at p = 1. Suppose we have three jobs J1, J2, J3, with v1 = v2 = v3 = 1. Suppose (a1, l1) = (1, 3), (a2, l2) = (3, 3) and (a3, l3) = (4, T \u22124). (All of them have immediate deadlines, which means they need to be scheduled when they arrive.) Normally, we schedule jobs J1 and J3 and generate a welfare equal to T . Now consider starting at time t = 2. Then we only schedule job J2 (and in the non-clairvoyant setting we will not even notice how valuable job J3 was!) and get only welfare equal to 3.\nAs it is clear from this example, the welfare obtained form such a mechanism cannot be a reasonable benchmark for our regret minimization, as one has to think very carefully about when to start running such a mechanism to calculate the benchmark. Otherwise, the benchmark mechanism could easily get tricked into a false start. In other words, we need to define the benchmark in a way that is robust to this sort of timing issues, independent of the choice of scheduling mechanism defining the benchmark.\nSyncing issues. In the non-clairvoyant setting, our scheduling mechanism is not able to simulate an arbitrary candidate scheduling mechanism ALGi starting from an arbitrary time, since there is no way of knowing its state. Accordingly, following the decisions of a bandit algorithm, similar to what we did in Section 3.2, is generally not possible in the non-clairvoyant setting.\nHowever, if both our mechanism and the new switched scheduling mechanism restart from a fresh state at exactly the same time after switching (e.g. slightly after the switching time when our mechanism is done with its current jobs) then our mechanism can sync with the new scheduling mechanism.\nTo address above concerns and to be able to design a truthful online scheduling mechanism that achieves a meaningful regret bound, we introduce a couple of new ingredients in our model and redefine our benchmark. We start by defining the notion of a random restart formally as following.\nAn important property of the way we restart is that it preserves truthfulness: a mechanism that was truthful to begin with is still truthful with a restart.\nDefinition 4.1. Given an online scheduling mechanism A, we define the restart at time t as follows.\n\u2022 During [t : t+ (`max + dmax)], mechanismM continues working on the jobs that have arrived before t.\n\u2022 If a job j arrives during [t : t+ (`max + dmax)], modify it as follows.\n\u2013 Shift its arrival time to the end of this interval, i.e. aj \u2190 t+ (`max + dmax) + 1. \u2013 Adjust the deadline of the job so that it reflects the time lost during the interval [t, t + dmax+`max]. This might mean some jobs are past their deadline. These jobs are rejected.\n\u2013 Preserve the arrival order. Use the tie breaking rule to make sure the arrival order of jobs whose starting time was set to t+dmax +`max is the same as in the original instance.\nHaving the formal definition of a restart, we ask the following question: how can one define a robust benchmark in the non-clairvoyant scheduling problem, given a set of candidate scheduling mechanisms? Here is an adaptation of our previous benchmark, i.e. welfare of the best-in-hindsight scheduling mechanism, for the non-clairvoyant setting.\nDefinition 4.2. Given candidate scheduling mechanisms ALG1, . . . , ALGn, and a parameter \u03b3 \u2208 [0, 1], the random-restarting mechanisms ALG1, . . . , ALGn are defined to be the original candidate mechanisms accompanied by independent random restarts of probability \u03b3 at every time t \u2208 [T ]. We define the random-restarting benchmark for an instance J to be OPT(J ) = maxi\u2208[n]E [ W(ALGi,J ) ] .\nThe benefits of using benchmark OPT(J ) are twofold. First, this benchmark is robust to timing issues, because the scheduling mechanism generating the benchmark restarts independently at random at every time t with probability \u03b3. Therefore, the benchmark loses no more than the generated welfare between two consecutive random-restarts due to timing issues. Second, while this property does not hold in general, for a large family of scheduling mechanisms (such as postedpricing-FIFO) and under the stochastic model of jobs (e.g. see Chawla et al. (2017)), the welfare loss due to independent (but infrequent) random restarts will easily be bounded. This property of the pair (stochastic model, online scheduling mechanisms), which we call robustness-to-welfare-loss, is formalized as following.\nDefinition 4.3. Given a distribution over jobs D, the random-restarting benchmark OPT(J ) with parameter \u03b3 is robust-to-welfare-loss in expectation over stochastic jobs D if\nEJ\u223cD [ OPT(J ) ] \u2265 EJ\u223cD[OPT(J )]\u2212 \u03b3 \u00b7 T \u00b7 vmax(`max + dmax)\nMoreover, a random-restarting online scheduling mechanism ALGi with parameter \u03b3 is robust-towelfare-loss in expectation under stochastic jobs D if\nEJ\u223cD \u2211 t\u2208[T ] Wt(ALGi,J )  \u2265 EJ\u223cD \u2211 t\u2208[T ] Wt(ALGi,J ) \u2212 \u03b3 \u00b7 T \u00b7 vmax(`max + dmax) Clearly, if all of the random-restarting mechanisms ALG1, . . . , ALGn are robust-to-welfare-loss\nunder stochastic job model D, then OPT(J ) will also be robust-to-welfare-loss under D."}, {"heading": "4.2 Truthful switching in the non-clairvoyant setting", "text": "We now show how to switch between truthful non-clairvoyant mechanisms. We reiterate a subtle aspect of truthfulness in the non-clairvoyant setting: a job that tries to influence the mechanism by switching its position in the arrival order can be treated the same way as a job reporting a later arrival time: it cannot be beneficial to do this given that the mechanism is truthful. Posted-pricingFIFO is an example of a mechanism that is both truthful and order respecting: all jobs whose values are less than a threshold price are rejected, and the rest of the jobs are scheduled in arrival order.\nWe claim that the random restart algorithm of Definition 4.1 preserves the truthfulness of the underlying scheduling mechanism. The following lemma is proved in Appendix A.\nLemma 4.1. If A is an order respecting truthful mechanism, then A with (arbitrary) restarts is also order respecting and truthful.\nCombining ideas from the truthful switching algorithm in the clairvoyant setting and the truthful random restart algorithm, we develop a truthful switching algorithm for non-clairvoyant settings that switches from a mechanism A to a mechanism B at time 0.\nDefinition 4.4. The mechanism C is as follows.\n1. For jobs that arrive by time 0, mimic mechanism A and return the same allocation, schedule and prices as A. All these jobs are completed by time dmax + `max.\n2. For all remaining jobs, i.e., jobs j with aj \u2265 1 run mechanism B on these with the following modifications.\n(a) If the arrival time of a job is < dmax + `max, set its arrival time to dmax + `max + 1.\n(b) Adjust the deadline of the job so that it reflects the time lost during the interval [1, dmax+ `max]. This might mean some jobs are past their deadline. These jobs are rejected.\n(c) Preserve the arrival order. Use the tie breaking rule to make sure the arrival order of jobs whose starting time was set to dmax + `max is the same as in the original instance.\nLemma 4.2. The state of the algorithm C at time dmax + `max is the same as the state of the algorithm B at time dmax + `max, given that B is restarted during the interval [1, dmax + `max].\nProof This follows by observing that Step (2) of mechanism C is identical to the modifications made during a restart. Any job that arrives by time 0 does not influence the state of mechanism B at time dmax + `max in either case.\nWe obtain the following theorem (see Appendix A for a proof).\nTheorem 4.3. Given order respecting truthful mechanisms A and B in the non-clairvoyant setting, switching mechanism C in Definition 4.4 is order respecting, truthful, and obtains welfare at least\u2211\nt\u2264dmax+`max Wt(A) + \u2211 t\u22651+dmax+`max Wt(B),\ngiven that A and B are restarted at time 1."}, {"heading": "4.3 Reduction to multi-armed bandits", "text": "In this section, we show how to design a truthful online learning algorithm that minimizes the regret relative to the random-restarting benchmark, i.e. best-in-hindsight of random-restarting truthful mechanisms ALG1, . . . , ALGn. Similar to Section 3.2, we consider a relevant adversarial Multi-Armed Bandits problem or MAB, where we have an arm for each of the n online scheduling mechanisms and we have bandit feedback, meaning that any algorithm only observes the reward of the arm it plays and not the other arms.\nIn such a setting, we assume having query access to a MAB algorithm Bandit-ALG that admits the optimal regret bound O(R \u221a Tn log n) in Theorem 2.2, where n is the number of arms, T is the time horizon and R is an upper-bound on the reward of an arm. Moreover, for the sake of simplicity, we assume Bandit-ALG does not need to know the time horizon T or rewards range R in advance, and it only needs to know these quantities are bounded. This assumption can be removed by using a doubling trick : given black-box access to a bandit algorithm Bandit-ALG1(R, T ) that requires knowing R and T , one can come up with another black-box algorithm Bandit-ALG2 with the same asymptotic regret bound that does not need this information. This reduction is explained in Appendix B.\nOur proposed algorithm, which we call Follow-The-Bandit-Switcher or FTBS, uses Bandit-ALG in a black-box fashion when looped in with the right rewards. It then follows Bandit-ALG\u2019s advice by truthful switching between arms, as suggested by Theorem 4.3. This helps the FTBS to aggregate truth and welfare guarantees of mechanisms ALG1, . . . , ALGn even in the non-clairvoyant setting. We formally define the FTBS as following.\nDefinition 4.5. Given the n online scheduling mechanisms, ALG1, . . . , ALGn, query access to online bandit algorithm Bandit-ALG, and parameter \u03b3 \u2208 [0, 1], the Follow-The-Bandit-Switcher mechanism proceeds as follows.\nInitialize t\u0302 = 1. (This index is counter for number of Heads)\nQuery Bandit-ALG for the initial arm i1, and run algorithm ALGi1 .\nAt each time t \u2208 [T ]: 1. Flip an independent coin \u03ba(t) with Pr(\u03ba(t) = Heads) = \u03b3.\n2. If coin \u03ba(t) shows a Heads,\n(a) Let t\u2032 = max{t\u2032\u2032 < t : \u03ba(t\u2032\u2032) = Heads}. If no such an integer exists, let t\u2032 = 1. (b) Update the sequence of bandit rewards between two consecutive Heads:\nr (it\u0302) x = { 0, if x \u2208 [t\u2032,min(t\u2032 + `max + dmax, t\u2212 1)] Wx(FTBS), if x \u2208 [min(t\u2032 + `max + dmax, t\u2212 1) + 1, t\u2212 1]\n(c) Set R(it\u2032 ) t\u0302 \u2190 \u2211 x\u2208[t\u2032,t\u22121] r (it\u2032 ) x and send this bandit feedback to Bandit-ALG.\n(d) Set t\u0302\u2190 t\u0302+ 1. (e) Let the new arm chosen by Bandit-ALG be it\u0302 \u2208 [n]. (f) If it\u0302 6= it\u0302\u22121, switch from ALGit\u0302\u22121to ALGit\u0302 using the mechanism in Section 4.2. (g) Otherwise, restart ALGit at time t.\n3. If coin \u03ba(t) shows a Tails, continue running ALGit\u0302 .\nWe now state and prove a tight regret bound (up to logarithmic factor) for FTBS.\nTheorem 4.4. The Follow-The-Bandit-Switcher (FTBS) scheduling mechanism, described in Definition 4.5, admits the following regret-bound if \u03b3 = (`max + dmax)\u22122/3T\u22121/3(n log(n))1/3:\nReg(FTBS) \u2264 O(m \u00b7 vmax(`max + dmax)1/3(n log(n))1/3T 2/3 log T ) = O\u0303(T 2/3)\nwhere Reg(FTBS) is the regret relative to OPT, i.e. the random-restarting benchmark as in Definition 4.2.\nWe finally consider stochastic jobs, and we focus on benchmarks that are robust-to-welfare-loss in expectation under this stochastic model, as described in Definition 4.3. The following corollary is immediate.\nCorollary 4.5. Given a distribution D over jobs and a robust-to-welfare-loss benchmark OPT(J ) (Definition 4.3), there exists a scheduling mechanism whose expected regret relative to EJ\u223cD[OPT(J )] is bounded by O\u0303(T 2/3).\nProof of Theorem 4.4 Let C1 , vmax(`max + dmax)m and C2 , vmaxm be constants. Fix a sequence of coins K[T ] , [\u03ba(1), . . . , \u03ba(T )] and let t1, t2, . . . , tT\u0302 be the times t \u2208 [T ] that coin \u03ba(t) shows a Heads. As a convention, let t0 , 1 and tT\u0302+1 , T\u0303 \u2265 T be the next time that coin \u03ba(t) flips a Heads if we continue flipping after T . By abuse of notation, we will use ALGi to denote mechanism ALGi restarted at exactly these times. For each i \u2208 [n] and x \u2208 [1, T\u0302 ], let R(i)x be:\nR(i)x , tx\u22121\u2211\nt=min(tx\u22121+`max+dmax,tx\u22121)+1 Wt(ALGi) \u2265 tx\u22121\u2211 t=tx\u22121 Wt(ALGi)\u2212 C1\nAlso, for the last interval [tT\u0302 , T ] and for each i \u2208 [n], let R (i) T\u0302+1 be\nR (i)\nT\u0302+1 , T\u2211 t=min(tT\u0302+`max+dmax,tx\u22121)+1 Wt(ALGi) \u2265 T\u2211 t=tT\u0302 Wt(ALGi)\u2212 C1\nTherefore, by summing over all intervals {[tx\u22121, tx]}x\u2208[T\u0302 ] \u222a [tT\u0302 , T ], we have:\n\u2200i \u2208 [n] : \u2211\nx\u2208[T\u0302+1]\nR(i)x \u2265W(ALGi)\u2212 C1(T\u0302 + 1) (1)\nNote that after the coin shows a Heads at time tx, our mechanism either stays with the same algorithm and does a restart or the truthful switching from ALGix\u22121 to ALGix as in Section 4.2. Also, Theorem 4.3 and the definition of truthful restart in Definition 4.1 guarantee that our scheduling mechanism will be synced with the restarting algorithms ALGix after `max + dmax units of time, and therefore the mechanism generates a welfare that is at least the welfare generated by the new arm in the interval [min(tx\u22121 + `max + dmax, tx \u2212 1) + 1, tx \u2212 1]. Formally speaking,\n\u2200x \u2208 [T\u0302 ] : tx\u2211\nt=tx\u22121\nWt(FTBS) \u2265 R(ix)x , T\u2211\nt=tT\u0302\nWt(FTBS) \u2265 R (iT\u0302 )\nT\u0302\nand therefore, by summing over all intervals {[tx\u22121, tx]}x\u2208[T\u0302 ] \u222a [tT\u0302 , T ], we have:\nW(FTBS) \u2265 \u2211\nx\u2208[T\u0302+1]\nR(ix)x (2)\nConditioned on the sequence of coin flips K[T ] (and therefore times t1, t2, . . . , tT\u0302 ), the rewards {R(i)x }x\u2208[T\u0302+1],i\u2208[n] define an (oblivious) adversarial instance of a MAB problem. For this adversarial instance, time horizon is indeed the random variable (T\u0302 + 1), i.e. number of Heads in the sequence K[T ] plus one. Moreover, if we let U\u0302 = maxj\u2208[T\u0302+1] Uj where\nj \u2208 [1, T\u0302 + 1] : Uj , tj \u2212 tj\u22121\nthen the rewards of this instance would also be upper-bounded by the random variable C2U\u0302 . Then, due to the optimal regret bound of Bandit-ALG (Theorem 2.2), we have:\n\u2200i \u2208 [n] : E  \u2211 x\u2208[T\u0302+1] R(i)x | K[T ] \u2212E  \u2211 x\u2208[T\u0302+1] R(ix)x | K[T ]  \u2264 O(U\u0302 T\u0302 1/2C2(n log n)1/2) (3) Combining Inequalities (1), (2) and (3) and taking expectations:\nReg(FTBS) \u2264 O(E [ U\u0302 T\u0302 1/2C2(n log n) 1/2 ] ) + E [ C1(T\u0302 + 1) ] (4)\nNow, E [ C1(T\u0302 + 1) ] = O(\u03b3 \u00b7 C1T ). To bound the other term, we use the following fact, proved\nin Eisenberg (2008), about independent and identically distributed geometric random variables.\nLemma 4.6 (Eisenberg (2008)). If g1, . . . , gk are i.i.d. and gi \u223c Geometric(\u03b3), then E [ max j\u2208[k] gj ] \u2264 Hk \u00b7 \u03b3\u22121 = O(\u03b3\u22121 log k)\n.\nNote that conditioned on T\u0302 , random variables {Uj}j\u2208T\u0302+1 are (T\u0302+1) i.i.d. Geometric(\u03b3) random variables. Using Lemma 4.6 we have:\nE [ U\u0302 T\u0302 1/2|T\u0302 ] = T\u0302 1/2E [ max j\u2208[T\u0302 ] Uj |T\u0302 ] \u2264 T\u0302 1/2 \u00b7HT\u0302 \u00b7 \u03b3 \u22121 = \u03b3\u22121 \u00b7O(T\u0302 1/2 log(T\u0302 ))\nNow, function f(x) = x1/2 log(x) is concave. By taking expectation and using Jensen\u2019s inequality, we further upper-bound this term. Hence:\nE [ U\u0302 T\u0302 1/2 ] \u2264 \u03b3\u22121.O(f(E [ T\u0302 ] )) = \u03b3\u22121/2 \u00b7 T 1/2 log(\u03b3T )\nSo, we can upper-bound the RHS of (4) by\nO(\u03b3\u22121/2T 1/2 log(\u03b3T )C2(n log n) 1/2) +O(\u03b3.C1T )\nBy setting \u03b3 = (C2C1 ) 2/3T\u22121/3(n log n)1/3, we get the desired regret bound."}, {"heading": "5 Lower Bounds", "text": "We first state a lower bound of \u2126( \u221a T ) on the regret for the clairvoyant scheduling problem. The proof of this theorem can be found in Appendix C. We note that this lower bound can be extended to \u2126\u0303(m \u221a T ) when there are m machines by simply having m copies of the same set of jobs every time. Similarly the lower bound can be made to scale linearly with vmax and \u221a dmax, matching the upper bound we give in Theorem 3.7.\nTheorem 5.1. There exists an instance of the clairvoyant scheduling problem where the regret of any online algorithm relative to the hindsight optimal posted-pricing-FIFO algorithm is \u2126( \u221a T ).\nNext we show a lower bound of \u2126\u0303(T 2/3) on the regret for the non-clairvoyant setting that matches our upper bound within polylogarithmic factors. Our lower bound follows by a reduction from the lower bound given in Dekel et al. (2014) for the multi-armed bandit problem with switching costs.\nFor the bandit problem with switching costs with n actions, Dekel et al. (2014) show that there exists a sequence of loss functions `1, `2, . . . `T , where `i : [n]\u2192 [0, 1], such that any online algorithm incurs a regret of at least \u2126\u0303(n1/3T 2/3). We use this loss sequence to define an instance of the nonclairvoyant scheduling problem as follows. First we give a reduction to regret against the welfare benchmark without random restarts. Later we show how to extend the lower bound to apply against the random restart benchmark.\nIn our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1]. Our instance has 4 sets of jobs J1, J2, J3, and J4, as shown in Figure 1. In each round, one job from each set arrives. Jobs in the set J1 arrive at the beginning of each round; that is, at time steps 8i for i = 0, 1, 2, . . . ..T . The processing length of a job j \u2208 J1 that arrives in the round i is 6 with probability pi(1) and 8 with probability (1\u2212 pt(1)), where pi(1) = 1/2 + `i(1)/2. Observe that processing lengths of the jobs in J1 depend on losses defined by Dekel et al. (2014). Further, the jobs in J1 have a value of 1 per unit length. In round i, a job from J2 arrives at time 8(i+ 1)\u2212 2 for i = 0, 1, 2, 3, . . . ..T , and has a processing length of 2. The value per unit length of jobs in J2 is 3.\nThe set J3 consists of jobs that arrive at time instants 8i\u2212 3 for i = 1, 2, . . . T , and have value per unit length of 2. The processing length of job j \u2208 J3 released in the round i is 4 with probability pi(2) and 2 with probability 1\u2212 pi(2), where pi(2) = `i(2). Similar to the jobs in J1, the processing lengths of jobs in J3 depend on the losses defined by the result of Dekel et al. (2014). Finally, the jobs in set J4 are released at time steps 8i\u2212 1 for i = 1, 2, . . . T , and have a processing length of 2 and value per unit length of 3.\nThe deadlines of all jobs are equal to their arrival times. (This condition is not necessary but\nsimplifies the construction.) Hence, if a job is not scheduled upon its arrival, the algorithm loses the job.\nLet ALG1 and ALG2 denote the two posted price scheduling mechanisms with prices 1 and 2 and FIFO scheduling policy. (Note that other posted price mechanisms, for example one that posts a price of 3, lose a constant factor in each round, hence we do not consider them.) The algorithms ALG1 and ALG2 correspond to the action set in the bandit problem with switching costs. The following two lemmas follow from the construction of lower bound instance. See Appendix C for a proof.\nLemma 5.2. For all rounds i = 0, 1, . . . T , the expected value of ALG1 in round i is 10\u2212 2`i(1) and expected value of ALG2 in round i is 10\u2212 2`i(2).\nLemma 5.3. If an online algorithm switches from ALG2 to ALG1 in any round i, it incurs a loss of at least 6.\nFrom Lemmas 5.2 and 5.3, we get the following theorem.\nTheorem 5.4. Minimax regret of non-clairvoyant scheduling problem is at least \u2126\u0303(T 2/3).\nProof Consider an online scheduling algorithm for the non-clairvoyant scheduling problem. At the beginning of each round i, it can either follow ALG1 and obtain a value of 10\u2212 2`i(1) or follow ALG2 and get a value 10 \u2212 2`i(2). Furthermore, since we are in the non-clairvoyant setting, the online algorithm won\u2019t know the value it can obtain from the algorithm it is not following. Since, the value obtained by ALG1 and ALG2 are exactly the same in each round except for the terms \u2212`i(1) and \u2212`i(2), the regret of the non-clairvoyant scheduling algorithm is equal to the regret it suffers on the losses at each round i. Moreover, switching from ALG2 to ALG1 in any round incurs a cost of 6. Hence, our scheduling instance corresponds to bandit with switching cost problem, where switching from action 2 to action 1 incurs a cost. The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u2126\u0303(T 2/3), when there is a switching cost between any pair of actions. However, it is easy to modify the proof in Dekel et al. (2014), where there is a switching cost only between action 2 to action 1, losing a factor of 2 in the regret bound Dekel et al. (2014). This completes our reduction.\nNow we argue that Theorem (5.4) can be extended to random restarting benchmarks. Before that we make the following simple observation regarding the lower bound instance in Theorem (5.4). Consider the posted price mechanisms ALG1 and ALG2 as defined in the proof of Theorem (5.4). ALG1(\u03b3), ALG2(\u03b3) denote the random restarting versions of them with restart parameter \u03b3.\nLemma 5.5. For some constant c \u2265 11, we have:\nj \u2208 {1, 2} : E[W (ALGj(\u03b3), J)] \u2265W (ALGj , J)\u2212 c \u00b7 \u03b3 \u00b7 T\nProof Proof of the lemma follows immediately from the observation that if ALGj for j = 1, 2 restarts at time t, and t is in round i, then it loses values of jobs in that round. The expected number of restarts by the algorithms is at most \u03b3 \u00b7 T . As the total value of jobs in each round is at most 11, the statement of the lemma holds for c \u2265 11.\nTo extend the lower bound to random restarting benchmarks, we need the following theorem from Dekel et al. (2014) for the bandit with switching costs problem.\nTheorem 5.6 (Dekel et al. (2014)). Let A be a multi-armed bandit algorithm that guarantees an expected regret (without switching costs) of O(T\u03b1) then there exists a sequence of loss functions that forces A to make \u2126\u0303(T 2(1\u2212\u03b1)) switches.\nCombining Lemma 5.5 and above theorem, we prove the following theorem.\nTheorem 5.7. No online algorithm can achieve a regret O(T 2/3\u2212 ) for any > 0 against a random restarting benchmark with restart parameter \u03b3 \u2208 (T\u22121, T\u22121/3].\nProof Proof is by contradiction. Suppose there is an online non-clairvoyant algorithm A that achieves a regret of O(T 2/3\u2212 ) for some > 0. From Lemma 5.5, this implies that it achieves a regret of at most O(T 2/3\u2212 ) against the non-restarting benchmark of Theorem 5.4. Our proof of Theorem 5.4 gives a reduction from the bandit with switching cost problem to the non-clairvoyant scheduling problem. Therefore, we can invoke Theorem 5.6, which implies that A does at least \u2126\u0303(T 2(1\u2212(2/3\u2212 ))) switches. Since there are only two actions in our lower bound instance, A still pays a switching cost of at least \u2126\u0303(T 2(1\u2212(2/3\u2212 ))) = \u2126\u0303(T 2/3+2 ) in switching from ALG2 to ALG1. This is gives a contradiction to the regret of A being O(T 2/3\u2212 ) against the non-restarting benchmark, and completes the proof."}, {"heading": "A Deferred proofs for upper bound constructions", "text": "Lemma 4.1. If A is an order respecting truthful mechanism, then A with (arbitrary) restarts is also order respecting and truthful.\nProof We assume that A is restarted in the interval [1, dmax+`max], and that the decision to restart is not dependent on what the jobs report. For a job that arrives by time 0, reporting an arrival time after 0 is not beneficial since that would mean that this job can only be processed beginning time dmax + `max + 1, and the job would be past its deadline by then. If the job reports an arrival time before time 1, then truthfulness of A guarantees that no misreport is beneficial.\nNow consider a job j that arrives after time 0. If the deadline of this job is such that it has to start by time dmax + `max, then no matter what it reports it does not get scheduled. For all other jobs, consider the instance where the jobs that arrive during the interval [1, dmax + `max] actually arrive at time dmax + `max + 1, with the same arrival order. Truthfulness of A for this instance guarantees truthfulness for such jobs.\nTheorem 4.3. Given order respecting truthful mechanisms A and B in the non-clairvoyant setting, switching mechanism C in Definition 4.4 is order respecting, truthful, and obtains welfare at least\u2211\nt\u2264dmax+`max Wt(A) + \u2211 t\u22651+dmax+`max Wt(B),\ngiven that A and B are restarted at time 1.\nProof The mechanism C is order respecting by definition. Truthfulness of mechanism C follows essentially from the truthfulness of restarts (Lemma 4.1). For any job that arrives before time 1, reporting an arrival time \u2265 1 is not beneficial since it would only begin processing after dmax + `max by which time its deadline would have passed. If it reports a time before 1, then the truthfulness of A guarantees that no misreport can be beneficial. For any job that arrives after time 0, the situation is exactly the same as a restart. What mechanism was run before time 1 has no bearing on the allocation and payments of this job. Lemma 4.1 guarantees truthfulness for these jobs.\nThe welfare guarantee follows from observing that mechanism C completes all jobs that mechanism A started before time 1, and that after time dmax + `max, the welfare of C matches that of B due to Lemma 4.2. The only loss of welfare is in the interval [1, dmax + `max] which is bounded by vmax(dmax + `max)."}, {"heading": "B Doubling Trick In the Multi-Armed Bandit Problem", "text": "In this section, we briefly explain the doubling trick. This helps with the case when algorithm Bandit-ALG1(R, T ) (that admits a regret guarantee of O(R \u221a T log(T ))) needs to know R and T in advance, and now we want to design an algorithm Bandit-ALG2 that does not need knowing these parameters and still wants to achieve O(R \u221a T log(T )) regret bound (assuming R and T are finite). Doubling trick for time horizon T is fairly standard, e.g. see Auer et al. (2007). We show how doubling trick works for range R, and then how to merge the two doubling tricks.\nDoubling trick for range R. For simplicity, suppose R = 2K for some integer K. Bandit-ALG2 does the following. It starts with a guess (initialized to 1) for R and simulates Bandit-ALG1 with this guess. Every time it sees a reward that is not in the guessed range, it doubles the guess (it may\ndouble it many times at the same time instance) and starts from scratch. Suppose T1, T2, . . . , Tk are the length of time intervals between two doubling. Therefore, for some constant c > 0,\nReg(Bandit-ALG2) \u2264 K\u2211 j=1 c.2j \u221a Tj log(Tj) \u2264 \u221a T log(T ) K\u2211 j=1 c.2j = O(R \u221a T log(T ))\nDoubling trick for both range R and time horizon T . In order to do so, use the doubling trick for R as a black-box, and during the steps of doubling trick for T , use this black-box."}, {"heading": "C Deferred proofs for lower bound constructions", "text": "C.1 Lower bound of \u2126( \u221a T ) for the clairvoyant setting\nTheorem 5.1. There exists an instance of the clairvoyant scheduling problem where the regret of any online algorithm relative to the hindsight optimal posted-pricing-FIFO algorithm is \u2126( \u221a T ).\nProof Our proof is an adaptation of the lower bound instance for the problem of prediction with expert advice. Recall the lower bound of \u2126( \u221a T ) for the problem of prediction with expert advice. In this instance, we have two experts. In each round, the adversary chooses one of the two experts uniformly at random (and independently of the previous rounds), and assigns a reward of 1. The adversary sets the reward of the other expert to zero. The expected reward of any online algorithm is T/2 but suffers a regret of \u2126( \u221a T ), as the expected reward of the best expert is E(max{#Heads,#Tails}) = T/2 + \u2126( \u221a T ). We adapt this lower bound to our scheduling problem as follows. We map each round of the game to two time steps. We now describe our scheduling instance; we refer the reader to Figure 2. Let round i of the game correspond to time steps [t, t + 1]. In our instance, all jobs have unit processing lengths. At time t, we release two jobs: j1 and j2. The deadline of job j1 is t+ 1 whereas the deadline of job j2 is t+ 2. Job j1 has a value of 1 and job j2 has a value of 2. At time t+ 1, we toss an unbiased coin. If the coin lands heads, we release job j3 that has a deadline of t+ 2 and value 2. We repeat this instance in each round of the game.\nNow consider two posted price algorithms ALG1 and ALG2 with prices 1 and 2 and FIFO scheduling policy. It is easy to check that ALG1 schedules jobs j1 and j2 in all rounds and obtains a total value\nof 3T . On the other hand, ALG2 processes jobs j2 in all the rounds, and processes jobs j3 if it arrives, depending on the outcome of the coin toss. Therefore, the expected value obtained by ALG2 is also 3T .\nConsider the decision of an online algorithm at the beginning of each round. If it decides to schedule job j1, then the maximum value it can get in a round is equal to that of ALG1. On the other hand, if it decides schedule j2, then the maximum value it can get is equal to that of ALG2. Hence, the expected value obtained by any online algorithm is 3T.\nConsider the expected value of the better of ALG1 and ALG2. Let ALG1(i) and ALG1(i) denote the value obtained by the algorithms in round i. Let Xi denote a random variable that takes a value of 1 with probability 1/2 and -1 with probability 1/2.\nE(max{ \u2211 i ALG1(i), \u2211 i ALG1(2)}) = E(max{3T, 3T + \u2211 i Xi})\n= 3T + E(max{0, \u2211 i Xi}) = 3T + \u2126( \u221a T )\nTherefore, the regret of the online algorithm is at least \u2126( \u221a T ).\nC.2 Lower bound for the non-clairvoyant setting with random restarts\nLemma 5.2. For all rounds i = 0, 1, . . . T , the expected value of ALG1 in round i is 10\u2212 2`i(1) and expected value of ALG2 in round i is 10\u2212 2`i(2).\nProof Consider ALG1. We note that ALG1 only schedules jobs from the sets J1 and J2. This is because, whenever ALG1 finishes processing a job, there is a job belonging to J1 or J2 that is released exactly at that time and no jobs belonging to J3 and J4 are available for processing. Moreover, ALG1 processes jobs from the set J1 in all rounds, and it processes the job from J3 if the processing length of job from J1 is 6. This follows from our construction where the completion time of the job from J1 with processing length 6 coincides exactly with the release time of the job from J2. Therefore, the expected reward obtained by ALG1 in round i is 6 + 2 \u00b7 pi(1) + (1\u2212 pi(2))2 \u00b7 3 = 10\u2212 2`i(1).\nSimilarly, it is easy to check that ALG2 only schedules jobs from the sets J3 and J4. Therefore, the expected reward obtained by ALG2 in round i is 4 + 4 \u00b7 pi(2) + (1\u2212 pi(2))2 \u00b7 3 = 10\u2212 2`i(2).\nLemma 5.3. If an online algorithm switches from ALG2 to ALG1 in any round i, it incurs a loss of at least 6.\nProof We can assume that the switching times of the online algorithm correspond to the completion time of some job or idle periods. If the algorithm switches in the middle of processing a job, the lemma follows trivially since the value of every job in our instance is at least 6. Fix a round i, and consider a time instant t when the algorithm switches from ALG2 to ALG1. We consider two cases.\nCase 1: Switching time t corresponds to the completion time of a job from the set J4. In this case, ALG1 is already processing a job from the set J1, and since deadlines of the jobs are same as arrival times, the online algorithm will not be able to schedule the job from the set J1. Therefore, the online algorithm loses a job of value of at least 6 in this round, which we charge to the switching cost.\nCase 2: Switching time t corresponds to the completion time of a job from the set J3. This case has two sub-cases depending on whether the job from the set J3 had a processing length of 2 or 4. Suppose processing length of the job was 2. In this scenario, the online algorithm will not be able\nto process the job from the set J4, and hence loses a value of 6 in round i. On the other hand, if the processing length of the job from the set J3 was 4, then, the algorithm loses the job j from set J1 in the round i+ 1, since the time of switch t is greater than the deadline dj of the job.\nIn the both cases, the online algorithm loses a value of 6 in round i, which we charge to the switching cost.\nC.3 Linear lower bound for non-clairvoyant setting without random restarts, continuous time\nHere we show that without our assumption regarding random restarting benchmark, no algorithm can get a sub-linear regret for the non-clairvoyant case, in a continuous time setting. By this we mean that the arrival time, deadline and processing lengths could be real numbers rather than integers.\nTheorem C.1. The minimax regret for the non-clairvoyant scheduling problem in a continuous time setting, compared against a benchmark without the random restart, is at least \u2126(T ).\nProof By Yao\u2019s minimax principle Motwani and Raghavan (1995), we assume that our algorithm A is deterministic. We now give a distribution over jobs which proves the theorem.\nWe map each round of the game into two time steps. Similar to previous lower bound constructions, let ALG1 and ALG2 denote the two posted price scheduling mechanisms with prices 1 and 2 and FIFO scheduling policy. All jobs in our lower bound instance have tight deadlines. See Figure 3 for an illustration of the lower bound instance.\nIn the first round, the adversary releases two special jobs j\u22171 and j\u22172 . Job j\u22171 has a value 1 and job j\u22172 has a value 2; j\u22171 arrives slightly earlier than j\u22172 . From the definition, ALG1 schedules j\u22171 and ALG2 schedules j\u22172 . At the beginning of the game, the adversary tosses an unbiased coin. If the coin comes up heads, then adversary releases a set of jobs S1; otherwise, a set of jobs S2 is released.\nThe set S1 consists of T jobs, where T is the number of rounds of the game. The value per unit length of each job is 2. The release time of a job j \u2208 S1 is t+Xj , where t is the beginning of round i, and Xj is a uniform (0, 1) random variable. The adversary chooses one of the T jobs j\u2032 \u2208 S1 uniformly at random, and makes it a long job by setting the processing length of the job `j\u2032 = 2. The remaining jobs in S1 have length 1. Furthermore, the adversary correlates the processing length of the special job j\u22171 with the release time of the job j\u2032, and sets it equal to 2 +Xj\u2032 . In other words, the completion time of the special job j\u22171 exactly coincides with the release time of j\u2032. The adversary releases S1 at the beginning of each round. Note that the random variables Xj and the long job j\u2032 are sampled only in the beginning of game, and do not change over the course of the game. From the construction it is easy to see that ALG1 only schedules long jobs from the set S1, since the completion of the special job j\u22171 coincides with the release of the long job, and this repeats in each round till the game ends. The total value obtained by ALG1 for this case is 2T since it schedules a long job in each round.\nThe set S2 is constructed exactly same as the set S1, except for the following differences: 1) The value per unit length of each job is 2, and 2) the adversary correlates processing length of the special job j\u22172 with release time of the job j\u2032 \u2208 S2, and sets it equal to 2 + Yj\u2032 , where j\u2032 denotes the long job in set S2. Also notice that random variables Yj for the set S2 are different from the set S1, but are (0, 1) uniform random variables. Again from the construction it is easy to check that ALG2 only schedules long jobs, since completion of the job j\u22172 coincides with the release of the long job, and this repeats in every round till the game ends. If the coin comes up tails, then the total value obtained by ALG2 is 4T since it schedules a long job in each round.\nTherefore, the expected value of the benchmark is at least 1/2 \u00b7 4T + 1/2 \u00b7 4T = 4T . Now let us analyze the value obtained by an online algorithm A. For any realization of the job arrivals, only one of the two algorithms ALG1 or ALG2 generate non-trivial value. Since, this is chosen uniformly at random, we conclude that with probability half A follows the wrong algorithm in the first round. Therefore, if it needs to achieve a sub-linear regret it has to switch from the algorithm it followed in the first round to the other algorithm. Let us focus on the case when A switches from ALG2 to ALG1 at time t.\nNow, consider the situation faced by A at the beginning of the round i that follows time t. It sees T jobs, each with value per unit length of 1, and arriving in the interval [i, i + 1] uniformly at random. One of these jobs is a long job, but A cannot distinguish this since we are in nonclairvoyant setting and the long job is chosen uniformly at random. Therefore, in expectation it takes T/2 rounds to identify the long job A. Hence, A schedules small jobs in T/2 rounds. This implies that in expectation A can only get a value of T/2 \u00b71 +T/2 \u00b74 = 2.5T . Similarly, it is easy to argue that if A switches from ALG1 to ALG2 it can get at most T/2 \u00b7 1 + T/2 \u00b7 4 = 2.5T . Therefore, the expected value obtained A on this distribution of jobs is at most 2.5T .\nTherefore, it suffers a regret of 1.5T , and this completes our proof."}], "references": [{"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Improved rates for the stochastic continuumarmed bandit problem", "author": ["Peter Auer", "Ronald Ortner", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Prompt mechanism for ad placement over time", "author": ["Yossi Azar", "Ety Khaitsin"], "venue": "In International Symposium on Algorithmic Game Theory,", "citeRegEx": "Azar and Khaitsin.,? \\Q2011\\E", "shortCiteRegEx": "Azar and Khaitsin.", "year": 2011}, {"title": "Truthful online scheduling with commitments", "author": ["Yossi Azar", "Inna Kalp-Shaltiel", "Brendan Lucier", "Ishai Menache", "Joseph Seffi Naor", "Jonathan Yaniv"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "Azar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2015}, {"title": "On-line learning and the metrical task system problem", "author": ["Avrim Blum", "Carl Burch"], "venue": "Machine Learning,", "citeRegEx": "Blum and Burch.,? \\Q2000\\E", "shortCiteRegEx": "Blum and Burch.", "year": 2000}, {"title": "Static optimality and dynamic search-optimality in lists and trees", "author": ["Avrim Blum", "Shuchi Chawla", "Adam Kalai"], "venue": "In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Blum et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2002}, {"title": "Bounding the power of preemption in randomized scheduling", "author": ["Ran Canetti", "Sandy Irani"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Canetti and Irani.,? \\Q1998\\E", "shortCiteRegEx": "Canetti and Irani.", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Stability of service under time-of-use pricing", "author": ["Shuchi Chawla", "Nikhil Devanur", "Alexander Holroyd", "Anna Karlin", "James Martin", "Balasubramanian Sivan"], "venue": "In STOC,", "citeRegEx": "Chawla et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2017}, {"title": "Prompt mechanisms for online auctions", "author": ["Richard Cole", "Shahar Dobzinski", "Lisa Fleischer"], "venue": "In International Symposium on Algorithmic Game Theory,", "citeRegEx": "Cole et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cole et al\\.", "year": 2008}, {"title": "Bandits with switching costs: T2/3 regret", "author": ["Ofer Dekel", "Jian Ding", "Tomer Koren", "Yuval Peres"], "venue": "In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Dekel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2014}, {"title": "On the expectation of the maximum of iid geometric random variables", "author": ["Bennett Eisenberg"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Eisenberg.,? \\Q2008\\E", "shortCiteRegEx": "Eisenberg.", "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In European conference on computational learning theory,", "citeRegEx": "Freund and Schapire.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1995}, {"title": "Online auctions with re-usable goods", "author": ["Mohammad T. Hajiaghayi", "Robert D. Kleinberg", "MohammadMahdian", "David C. Parkes"], "venue": "In Proceedings of the 6th ACM conference on Electronic commerce,", "citeRegEx": "Hajiaghayi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hajiaghayi et al\\.", "year": 2005}, {"title": "Optimal mechanism design and money burning", "author": ["J. Hartline", "T. Roughgarden"], "venue": "In Proc. 39th ACM Symp. on Theory of Computing,", "citeRegEx": "Hartline and Roughgarden.,? \\Q2008\\E", "shortCiteRegEx": "Hartline and Roughgarden.", "year": 2008}, {"title": "Paramils: an automatic algorithm configuration framework", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown", "Thomas St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Online ascending auctions for gradually expiring items", "author": ["Ron Lavi", "Noam Nisan"], "venue": "Journal of Economic Theory,", "citeRegEx": "Lavi and Nisan.,? \\Q2015\\E", "shortCiteRegEx": "Lavi and Nisan.", "year": 2015}, {"title": "Randomized Algorithms", "author": ["Rajeev Motwani", "Prabhakar Raghavan"], "venue": null, "citeRegEx": "Motwani and Raghavan.,? \\Q1995\\E", "shortCiteRegEx": "Motwani and Raghavan.", "year": 1995}, {"title": "Satzilla: portfolio-based algorithm selection for sat", "author": ["Lin Xu", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "log(T )) regret bound (assuming R and T are finite). Doubling trick for time horizon T is fairly standard", "author": [], "venue": "e.g. see Auer et al", "citeRegEx": "\u221a,? \\Q2007\\E", "shortCiteRegEx": "\u221a", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Canetti and Irani (1998) showed, in particular, that no online algorithm can achieve a less than polylogarithmic competitive ratio for this problem in comparison to the hindsight optimal schedule.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Canetti and Irani (1998) showed, in particular, that no online algorithm can achieve a less than polylogarithmic competitive ratio for this problem in comparison to the hindsight optimal schedule. On the other hand, Lavi and Nisan (2015) showed that no deterministic mechanism that is truthful with respect to all of the parameters can approximate social welfare better than a factor T in the worst case, where T is the time horizon, even for unit length jobs on a single machine.", "startOffset": 0, "endOffset": 238}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al.", "startOffset": 219, "endOffset": 238}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al. (2005), satisfying incentive compatibility with respect to only a few of the jobs\u2019 parameters Cole et al.", "startOffset": 219, "endOffset": 311}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al. (2005), satisfying incentive compatibility with respect to only a few of the jobs\u2019 parameters Cole et al. (2008); Azar and Khaitsin (2011), etc.", "startOffset": 219, "endOffset": 417}, {"referenceID": 2, "context": "(2008); Azar and Khaitsin (2011), etc.", "startOffset": 8, "endOffset": 33}, {"referenceID": 11, "context": "Hartline and Roughgarden (2008) advocate a general framework for generating an appropriate benchmark for such online problems\u2014determine the class of all mechanisms that are optimal for the problem in an appropriate stochastic setting; compete against the best of these Bayesian optimal mechanisms for a worst case instance.", "startOffset": 0, "endOffset": 32}, {"referenceID": 6, "context": "Our work is inspired by the recent work of Chawla et al. (2017) that shows that when jobs are drawn from an i.", "startOffset": 43, "endOffset": 64}, {"referenceID": 4, "context": ", Blum and Burch (2000); Blum et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 4, "context": ", Blum and Burch (2000); Blum et al. (2002).", "startOffset": 2, "endOffset": 44}, {"referenceID": 8, "context": "Chawla et al.\u2019s mechanism is a simple greedy best-effort mechanism based on posted prices. The mechanism announces a price per-unit of resource for each time period into the future. When a job arrives it gets scheduled in a best-effort FIFO manner in the cheapest slots that satisfy its requirements. Chawla et al. show that if the number of resources per time period is large enough, then for any underlying distribution over job types, there exists a set of prices such that this posted-pricing-FIFO mechanism achieves a 1 \u2212 approximation to expected social welfare. Unfortunately, finding the best price to offer requires knowing the fine details of the distribution over job types and solving a large linear program. Chawla et al.\u2019s mechanisms are parameterized by a single price. Machine learning techniques have been succesfully used to tune parameters of heuristics for a wide variety of problems Xu et al. (2008); Hutter et al.", "startOffset": 0, "endOffset": 921}, {"referenceID": 8, "context": "Chawla et al.\u2019s mechanism is a simple greedy best-effort mechanism based on posted prices. The mechanism announces a price per-unit of resource for each time period into the future. When a job arrives it gets scheduled in a best-effort FIFO manner in the cheapest slots that satisfy its requirements. Chawla et al. show that if the number of resources per time period is large enough, then for any underlying distribution over job types, there exists a set of prices such that this posted-pricing-FIFO mechanism achieves a 1 \u2212 approximation to expected social welfare. Unfortunately, finding the best price to offer requires knowing the fine details of the distribution over job types and solving a large linear program. Chawla et al.\u2019s mechanisms are parameterized by a single price. Machine learning techniques have been succesfully used to tune parameters of heuristics for a wide variety of problems Xu et al. (2008); Hutter et al. (2009). This is typically done in a batch setting, where past data is used to find a good setting of parameters for the heuristic.", "startOffset": 0, "endOffset": 943}, {"referenceID": 8, "context": "Fortunately, the benchmark suggested by Chawla et al. (2017), namely the optimal posted-pricing-FIFO mechanism, continues to obtain near-optimal welfare\u2014the mechanism does not need to know jobs\u2019 lengths in order to make scheduling decisions, although the existence of a good posted price assumes that lengths along with other job parameters are drawn from some i.", "startOffset": 40, "endOffset": 61}, {"referenceID": 11, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 115, "endOffset": 142}, {"referenceID": 11, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 115, "endOffset": 168}, {"referenceID": 7, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 169, "endOffset": 200}, {"referenceID": 16, "context": "1 (Kalai and Vempala (2005)).", "startOffset": 3, "endOffset": 28}, {"referenceID": 0, "context": "2 (Auer et al. (1995)).", "startOffset": 3, "endOffset": 22}, {"referenceID": 10, "context": "For this problem, we mainly use that there is no algorithm with a regret of o(T 2/3) (Dekel et al., 2014), to get a similar lower bound for our problem.", "startOffset": 85, "endOffset": 105}, {"referenceID": 8, "context": "see Chawla et al. (2017)), the welfare loss due to independent (but infrequent) random restarts will easily be bounded.", "startOffset": 4, "endOffset": 25}, {"referenceID": 11, "context": "To bound the other term, we use the following fact, proved in Eisenberg (2008), about independent and identically distributed geometric random variables.", "startOffset": 62, "endOffset": 79}, {"referenceID": 11, "context": "To bound the other term, we use the following fact, proved in Eisenberg (2008), about independent and identically distributed geometric random variables. Lemma 4.6 (Eisenberg (2008)).", "startOffset": 62, "endOffset": 182}, {"referenceID": 10, "context": "Our lower bound follows by a reduction from the lower bound given in Dekel et al. (2014) for the multi-armed bandit problem with switching costs.", "startOffset": 69, "endOffset": 89}, {"referenceID": 10, "context": "Our lower bound follows by a reduction from the lower bound given in Dekel et al. (2014) for the multi-armed bandit problem with switching costs. For the bandit problem with switching costs with n actions, Dekel et al. (2014) show that there exists a sequence of loss functions `1, `2, .", "startOffset": 69, "endOffset": 226}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1].", "startOffset": 129, "endOffset": 149}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1]. Our instance has 4 sets of jobs J1, J2, J3, and J4, as shown in Figure 1. In each round, one job from each set arrives. Jobs in the set J1 arrive at the beginning of each round; that is, at time steps 8i for i = 0, 1, 2, . . . ..T . The processing length of a job j \u2208 J1 that arrives in the round i is 6 with probability pi(1) and 8 with probability (1\u2212 pt(1)), where pi(1) = 1/2 + `i(1)/2. Observe that processing lengths of the jobs in J1 depend on losses defined by Dekel et al. (2014). Further, the jobs in J1 have a value of 1 per unit length.", "startOffset": 129, "endOffset": 754}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1]. Our instance has 4 sets of jobs J1, J2, J3, and J4, as shown in Figure 1. In each round, one job from each set arrives. Jobs in the set J1 arrive at the beginning of each round; that is, at time steps 8i for i = 0, 1, 2, . . . ..T . The processing length of a job j \u2208 J1 that arrives in the round i is 6 with probability pi(1) and 8 with probability (1\u2212 pt(1)), where pi(1) = 1/2 + `i(1)/2. Observe that processing lengths of the jobs in J1 depend on losses defined by Dekel et al. (2014). Further, the jobs in J1 have a value of 1 per unit length. In round i, a job from J2 arrives at time 8(i+ 1)\u2212 2 for i = 0, 1, 2, 3, . . . ..T , and has a processing length of 2. The value per unit length of jobs in J2 is 3. The set J3 consists of jobs that arrive at time instants 8i\u2212 3 for i = 1, 2, . . . T , and have value per unit length of 2. The processing length of job j \u2208 J3 released in the round i is 4 with probability pi(2) and 2 with probability 1\u2212 pi(2), where pi(2) = `i(2). Similar to the jobs in J1, the processing lengths of jobs in J3 depend on the losses defined by the result of Dekel et al. (2014). Finally, the jobs in set J4 are released at time steps 8i\u2212 1 for i = 1, 2, .", "startOffset": 129, "endOffset": 1375}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions. However, it is easy to modify the proof in Dekel et al. (2014), where there is a switching cost only between action 2 to action 1, losing a factor of 2 in the regret bound Dekel et al.", "startOffset": 14, "endOffset": 221}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions. However, it is easy to modify the proof in Dekel et al. (2014), where there is a switching cost only between action 2 to action 1, losing a factor of 2 in the regret bound Dekel et al. (2014). This completes our reduction.", "startOffset": 14, "endOffset": 350}, {"referenceID": 10, "context": "To extend the lower bound to random restarting benchmarks, we need the following theorem from Dekel et al. (2014) for the bandit with switching costs problem.", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": "6 (Dekel et al. (2014)).", "startOffset": 3, "endOffset": 23}], "year": 2017, "abstractText": "We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider\u2019s goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret. We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within polylogarithmic factors). 1 ar X iv :1 70 3. 00 48 4v 1 [ cs .G T ] 1 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}