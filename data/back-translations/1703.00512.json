{"id": "1703.00512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison", "abstract": "Selecting, developing or comparing machine learning methods in data mining can be a difficult task based on the goal problem and objectives of a particular study. Numerous publicly accessible and simulated benchmark data sets have emerged from different sources, but their organization and adoption as standards have been inconsistent. Therefore, selecting and curating certain benchmarks remains an unnecessary burden for machine learning practitioners and data scientists. This study provides an accessible, curated and developed public benchmark resource to facilitate the identification of the strengths and weaknesses of various machine learning methods. We compare meta characteristics between the current benchmark data sets in this resource to characterize the diversity of available data. Finally, we apply a set of established machine learning methods to the entire benchmark collection and analyze how data sets and algorithms interact with performance clusters. This work is an important first step toward understanding the limitations of more popular resource searches and benchmarking of existing and multifaceted benchmarks in the future.", "histories": [["v1", "Wed, 1 Mar 2017 21:20:11 GMT  (697kb,D)", "http://arxiv.org/abs/1703.00512v1", "14 pages, 5 figures, submitted for review to JMLR"]], "COMMENTS": "14 pages, 5 figures, submitted for review to JMLR", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["randal s olson", "william la cava", "patryk orzechowski", "ryan j urbanowicz", "jason h moore"], "accepted": false, "id": "1703.00512"}, "pdf": {"name": "1703.00512.pdf", "metadata": {"source": "CRF", "title": "PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison", "authors": ["Randal S. Olson", "William La Cava", "Patryk Orzechowski", "Ryan J. Urbanowicz", "Jason H. Moore"], "emails": ["olsonran@upenn.edu", "lacava@upenn.edu", "patryk@upenn.edu", "ryanurb@upenn.edu", "jhmoore@upenn.edu"], "sections": [{"heading": null, "text": "Keywords: machine learning, benchmarking, data repository, classification"}, {"heading": "1. Introduction", "text": "The term benchmarking is used in machine learning (ML) to refer to the evaluation and comparison of ML methods regarding their ability to learn patterns in \u2018benchmark\u2019 datasets that have been applied as \u2018standards\u2019. Benchmarking could be thought of simply as a sanity check to confirm that a new method successfully runs as expected and can reliably find simple patterns that existing methods are known to identify Hastie et al. (2009). A more rigorous way to view benchmarking is as an approach to identify the respective strengths\nc\u00a92017 Olson, La Cava, Orzechowski, Urbanowicz, and Moore.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/vx/x.html.\nar X\niv :1\n70 3.\n00 51\n2v 1\nand weaknesses of a given methodology in contrast with others Caruana and NiculescuMizil (2006). Comparisons could be made over a range of evaluation metrics, e.g., power to detect signal, prediction accuracy, computational complexity, and model interpretability. This approach to benchmarking would be important for demonstrating new methodological abilities or simply to guide the selection of an appropriate ML method for a given problem.\nBenchmark datasets typically take one of three forms. The first is accessible, well-studied real-world data, taken from different real-world problem domains of interest. The second is simulated data, or data that has been artificially generated, often to \u2018look\u2019 like real-world data, but with known, underlying patterns. For example, the GAMETES genetic-data simulation software generates epistatic patterns of association in \u2018mock\u2019 single nucleotide polymorphism (SNP) data Urbanowicz et al. (2012b,a). The third form is toy data, which we will define here as data that is also artificially generated with a known embedded pattern but without an emphasis on representing real-world data, e.g., the parity or multiplexer problems Blum et al. (2003); Koza (1992). It is worth noting that the term \u2018toy dataset\u2019 has often been used to describe a small and simple dataset such as the examples included with algorithm software.\nWhile some benchmark repositories and datasets have emerged as more popular than others, ML still lacks a central, comprehensive, and concise set of benchmark datasets that accentuate the strengths and weaknesses of established ML methods. Individual studies often restrict their benchmarking efforts for various reasons, for example based on comparing variants of the ML algorithm of interest. The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al. (2013). Benchmarking efforts may focus on a specific application of interest, e.g. traffic sign detection Stallkamp et al. (2012), or a more narrowly defined ML problem type, e.g. classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements.\nThere are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them. As a result, many benchmark datasets go unused simply because they are too difficult to preprocess. Further, while real-world benchmarks can be derived from many different problem domains, from a strict data science perspective, many of the benchmarks in repositories can have very similar meta-features (e.g. the number of instances, number of features, number of classes, presence of missing data, and similar signal to noise ratios, etc.), such that while they are representative of different real-world problems, they may not represent a diverse assembly of data science problems. This issue has been raised previously; when applying UCI datasets as benchmarks, it was noted that the scope of included datasets limited method evaluation, and suggested that repositories such as UCI should be expanded Segal (2004).\nAnother challenge in benchmarking is that researchers often use only a handful of datasets when evaluating their methods, which can make it difficult to properly compare one ML method to the state-of-the-art ML methods. For example, these datasets may be handpicked to highlight the strengths of the proposed method, while failing to demonstrate the proposed method\u2019s potential weaknesses. As a result, although a ML method may perform well on a handful of datasets, it may fail to generalize to a broader range of problems. We submit that it is just as important to clearly identify the limitations of an algorithm in benchmarking practices, something that is often overlooked. While there will always be a need to identify and generate custom benchmarks for new or specialized problem domains, e.g. physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcala\u0301 et al. (2010) and the UCI ML repository Lichman (2013). In addition to collecting data from these resources, PMLB standardizes the format of these data and provides useful interfaces for fetching datasets directly from the web.\nThis initial PMLB repository is not meant to be comprehensive; it includes mainly real-world datasets and excludes regression datasets (i.e. those with a continuous-valued dependent variable), as well as any datasets with missing values. We have chosen to focus our initial assessment on available datasets in classification. This paper includes a highlevel analysis of the properties (i.e. meta-features) of the founding PMLB datasets, such as feature counts, class imbalance, etc. Further, we evaluate the performance of 13 standard statistical ML methods from scikit-learn Pedregosa et al. (2011) over the full set of PMLB datasets. We then assess the diversity of these benchmark datasets from the perspective of their meta-features as well as based on the predictive performance over the set of ML methods applied. Beyond introducing a new simplified resource for ML benchmarks, this study was designed to provide insight into the limitations of currently utilized benchmarks, and direct the expansion and curation of a future improved PMLB dataset suite that more efficiently and comprehensivly allows for the comparison of ML methods. This work provides an important first step in the assembly of a effective and diverse set of benchmarking standards integrating real-world, simulated, and toy datasets for generalized ML evaluation and comparison."}, {"heading": "2. Penn Machine Learning Benchmark (PMLB)", "text": "We compiled the Penn Machine Learning Benchmark (PMLB) datasets from a wide range of existing ML benchmark suites including the UCI ML repository Lichman (2013), Kaggle Goldbloom (2010), KEEL Alcala\u0301 et al. (2010), and meta-learning benchmark Reif (2012). As such, the PMLB includes most of the real-world benchmark datasets commonly used in ML benchmarking studies.\nTo make the PMLB easier to use, we preprocessed all of the datasets to follow a standard row-column format, where the features correspond to columns in the dataset and every\ninstance in the data set is a row. All categorical features and labels with non-numerical encodings were replaced with numerical equivalents (e.g., \u201cLow\u201d, \u201cMedium\u201d, and \u201cHigh\u201d were replaced with 0, 1, and 2). Additionally, in every dataset, the dependent variable column was labeled as \u201cclass\u201d. Finally, all benchmark datasets with missing data were excluded from PMLB, as many ML algorithms cannot handle missing data in their standard implementations and we wished to avoid imposing a particular data imputation method in this initial study.\nCurrently, the PMLB consists of datasets for supervised classification (binary and multiclass). In supervised classification, we wish to find a mapping y\u0302(x) : Rp \u2192 Y that associates the vector of features x \u2208 Rp with class labels from the set Y = {1 . . . K} using N paired examples from the training set T = {(xi, yi), i = 1 . . . N}. In the future we plan to expand PMLB to include datasets for regression."}, {"heading": "2.1 PMLB Meta-Features", "text": "In the current release, the PMLB includes 165 datasets. The meta-features of these datasets are sumarized in Figure 1. These meta-features are defined as follows:\n\u2022 # Instances: The number of instances in each dataset.\n\u2022 # Features: The number of features in each dataset.\n\u2022 # Binary Features: The number of categorical features in each dataset with only two levels.\n\u2022 # Categorical and Ordinal Features: The number of discrete features in each dataset with >2 levels.\n\u2022 # Continuous Features: The number of continuous-valued features in each dataset. Discriminating categorical and ordinal features from continuous features was determined automatically based on whether a variable was considered to be a \u2018float\u2019 in a Pandas dataframe pan.\n\u2022 Endpoint Type: Whether each dataset is a binary or multiclass supervised classification problem. Again, continuous endpoints for regression have been excluded in this study.\n\u2022 # Classes: The number of classes to predict in each dataset\u2019s endpoint.\n\u2022 Class Imbalance: The level of class imbalance in each dataset \u2208 [0 1), where 0.0 corresponds to perfectly balanced classes and a value approaching 1.0 corresponds to extreme class imbalance, i.e. where nearly all instances have the same class value. Imbalance is calculated by measuring the squared distance of each class\u2019s instance proportion from perfect balance in the dataset, as:\nI = K K\u2211 i=1 ( ni N \u2212 1 K )2\nwhere ni is the number of instances of class i \u2208 Y.\nMost of the datasets have under 5,000 instances and 500 features, and a fairly balanced class distribution. Roughly half of the datasets are binary classification problems, whereas the remaining half are multiclass classification problems ranging from 3-26 classes. Of the 165 datasets, 49 datasets have a mix of discrete (i.e. binary, categorical or ordinal) and continuous features, while 12 include only binary features, and 53 contain only continuous features. It is worth noting that the PMLB datasets cover a broad range of application areas, including biomedical studies, signal processing, and image classification, among others."}, {"heading": "2.2 PMLB Python Interface", "text": "To make the PMLB datasets easier to access, we published an open source Python interface for PMLB on PyPi1. This interface provides a simple fetch data function that returns any dataset in the PMLB as a pandas pan DataFrame. For example, to fetch the clean2 dataset:\nimport pmlb c l ean2 data = pmlb . f e t c h d a t a ( \u2018 c lean2 \u2019 )\nThe clean2 data variable will then contain a data frame of the clean2 dataset, where the class column corresponds to the class labels and the remaining columns are the features. The fetch data function has several caching and preprocessing options, all of which are documented in the PMLB repository2.\nTo acquire a full list of all datasets available in PMLB, users can access the dataset names variable:\n1. https://pypi.python.org/pypi/pmlb/ 2. https://github.com/EpistasisLab/penn-ml-benchmarks\nimport pmlb pr in t ( pmlb . dataset names )\nwhich is simply a Python list that contains the names of all PMLB datasets. For the remainder of the experiments described below, we used this Python interface to load the datasets prior to analysis."}, {"heading": "3. Evaluating Machine Learning Methods", "text": "To provide a basis for comparison, we evaluated 13 supervised ML classification methods from scikit-learn Pedregosa et al. (2011) on the 165 datasets in PMLB. These methods include:\n\u2022 Gaussian Na\u0308\u0131ve Bayes (NB)\n\u2022 Bernoulli Na\u0308\u0131ve Bayes\n\u2022 Multinomial Na\u0308\u0131ve Bayes\n\u2022 Logistic Regression\n\u2022 Linear classifier trained via stochastic gradient descent (SGD)\n\u2022 Support Vector Classifier (SVC) with a linear, polynomial, sigmoid, or RBF kernel\n\u2022 Passive Aggressive classifier\n\u2022 K-Nearest Neighbor (KNN)\n\u2022 Decision Tree\n\u2022 Random Forest\n\u2022 Extra Random Forest (a.k.a. Extra Trees Classifier)\n\u2022 AdaBoost\n\u2022 Gradient Tree Boosting\nML methods were evaluated using balanced accuracy Velez et al. (2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies. For more information on these ML methods, see Hastie et al. (2009) and the scikit-learn documentation Pedregosa et al. (2011). When we evaluated each ML method, we first scaled the features of the datasets by subtracting the mean and scaling the features to unit variance. This scaling step was necessary for some ML methods, such as the K-Nearest Neighbor classifier, which assumes that the datasets will be scaled appropriately beforehand. (Note that the datasets provided in PMLB are not scaled or normalized in order to keep them as close as possible to their original form.)\nOnce the datasets were scaled, we performed a comprehensive grid search of each of the ML method\u2019s parameters using 10-fold cross-validation to find the best parameters\n(according to mean cross-validation balanced accuracy) for each ML method on each data set. This process resulted in a total of over 5.5 million evaluations of the 13 ML methods over the 165 data sets. For a comprehensive parameter search, we used expert knowledge about the ML methods to decide what parameters and parameter values to evaluate. The complete code for running the experiment is available online3. It should be noted that due to the different number of parameters for each algorithm, not every algorithm had the same number of evaluations."}, {"heading": "4. Results", "text": "In order to characterize the datasets in PMLB, they are clustered based on their metafeatures in Section 4.1. We then analyze the datasets based on ML performance in Section 4.2, which identifies which datasets can be solved with high or low accuracy, as well as which datasets are appear universally easy or hard for the set of different ML algorithms to model accurately versus which ones appear to be particularly useful for highlighting differential ML algorithm performance."}, {"heading": "4.1 Dataset Meta-Features", "text": "We used k-means to cluster the normalized meta-features of the datasets into 5 clusters, visualized along the first two principal component axes in Figure 2 (note that the first two components of the PCA explain 49% of the variance, so we expect there to be some overlap of clusters in visualization). The number of clusters was chosen to compromise between the interpretability of the clusters and the adequate separation of the clustered datasets, as defined by the silhouette score. Figure 2 includes two clusters centered on outlier datasets (clusters 2 and 4). All clusters are compared in more detail according to the mean values of the dataset meta-features in each cluster in Figure 3. Clusters 0 and 1 contain most of the datasets, and are separated by their endpoint type, i.e. cluster 0 is comprised of binary classification problems, whereas cluster 1 is comprised of multiclass classification problems. Cluster 2 is made up of 3 datasets with relatively high numbers features (a GAMETES dataset with 1000 features and the MNIST dataset with 784). Cluster 3 contains datasets with high imbalance between classes in the data set. Finally, cluster 4 is reserved for the KDD Cup dataset, which has exceptionally high number of instances (nearly 500,000). The clustering analysis thus reflects fairly intuitive ways in which the challenges presented by a particular dataset can be categorized, namely: large numbers of instances, large numbers of features, high class imbalance, and binary versus multiclass classification."}, {"heading": "4.2 Model-Dataset Biclustering", "text": "Figure 4 summarizes the results of biclustering the balanced accuracy of the tuned models according to the ML method and dataset using a spectral biclustering algorithm Kluger et al. (2003). The methods and datasets are grouped into 40 contiguous biclusters (4 MLwise clusters by 10 data-wise clusters) in order to expose relationships between models and datasets. Figure 4A presents the balanced accuracy. Figure 4B preserves the clustering from \u2018A\u2019, but presents the deviation from the mean balanced accuracy among all 13 ML\n3. https://github.com/rhiever/sklearn-benchmarks/tree/master/model code/\nmethods, in order to clearly identify datasets upon which all ML methods perform similarly, and those where some methods performed better than others. Figure 4C simply delineates the 40 identified biclusters defined by balanced accuracy biclustering in Figure 4A and preserved in 4B.\nIt is interesting to note that the ML methods tend to group according to their underlying approach; for example, Gaussian and Multinomial Na\u0308\u0131ve Bayes methods cluster together, Logistic Regression, Passive Aggressive and SGD cluster together (all hyperplane estima-\ntors), and the tree-based methods Decision Tree, Extra Trees and Random Forest also form a separate cluster. Datasets that are derived from the same origin are observed to cluster in certain instances. For example, dataset cluster 1 (i.e. the left-most dataset cluster identified in the Figure 4C, including 4 separate biclusters) contains most of the GAMETES data sets; cluster 2 contains most of the mfeats datasets and the Breast Cancer datasets; and cluster 10 includes both of the Wine Quality datasets and several thyroid-related datasets (new-thyroid, allhyper, allbp, allrep).\nFigure 4A allows us to interpret the utility of certain datasets in terms of difficulty across all methods and across classes of methods. For example, the light-blue stripes of low balanced accuracy indicate that none of the models achieve good performance on datasets 22, 118, and 164, which correspond to the GAMETES Epistasis datasets that are known to be difficult due to the lack of univariate correlations between features and classes and the high amount of noise. In contrast, nearly every method solves dataset 140 (clean2) with a high degree of accuracy because there are simple linear correlations between the features and classes and no noise.\nOther clusters of datasets and ML methods reveal contrasts in performance. Dataset cluster 3 is the only cluster to contain a single dataset, the parity5 problem, corresponding to dataset 66 in Figure 4. This is a unique problem in which a ML method must be able to quantify whether the number of features with a given binary value is even or odd in order to correctly classify each instance. As a result, methods that consider the main effect of features independently are not able to solve it (e.g. the Na\u0308\u0131ve Bayes methods). In contrast, methods with high capacity for interactions between features do well (e.g. Gradient Boosting, K-Nearest Neighbor, SVC). This contrast is also seen in cluster 4 (datasets 67 - 75), which contains several datasets with strong interactions between features (e.g. tic-tac-toe, parity5+5, and multiplexer-6). Again we observe a contrast between ML methods that make assumptions of linear independence and those that do not across this cluster of datasets. Contrasting Figure 4A with 4B helps to differentiate differences in overall performance on given datasets from differences in performance based on selected ML methodology. One important observation is that a reasonably large proportion of benchmarks included in this study yielded similar performance over the spectrum of ML methods applied. This is likely because the signals identified in these datasets were either universally easy or difficult to detect. Furthermore, for those datasets where variable performance was observed, often a group of datasets clustered together with a similar signature of better than average/worse than average performance (see Figure 4B).\nOverall, the current suite of datasets span a reasonable range of difficulty for the tested ML approaches. Figure 5 shows the distribution of scores for each tuned ML method for each dataset in the suite, sorted by best balanced accuracy score achieved by any method. The left-most dataset corresponds to clean2, mentioned above, and the right-most is analcatdata dmft, with a maximum accuracy score of 0.544 for the methods tested. Approximately half (87) of the current suite can be classified with a balanced accuracy of 0.9 or higher, and nearly all (98.8%) of the datasets can be classified with a balanced accuracy of 0.6 or higher. Thus, although a range of model fidelity is observed, the datasets are biased towards problems that can be solved with a higher balanced accuracy."}, {"heading": "5. Discussion and Conclusion", "text": "The primary goal of this paper is to introduce an ongoing research project for benchmarking ML methods. Specifically, we have collected and curated 165 datasets from the most popular data repositories and introduced PMLB, a new evolving set of benchmark standards for comparing and evaluating different ML methods. Apart from the repository itself, we have conducted a comprehensive analysis of the performance of numerous standard ML methods, which may be used as a baseline for evaluating and comparing newly developed ML methods, and assessed the diversity of these existing benchmark datasets to identify shortcomings to be addressed by the subsequent addition of further benchmarks in a future release.\nSimplicity and diversity are the ultimate priorities of the PMLB suite. This motivated us to clean and standardize the presentation of datasets in the repository, develop a simple interface for fetching data, and include datasets from multiple sources. Interestingly, when we analyzed the meta-features of the datasets in PMLB, we found that most of the datasets fall into a handful of categories based on feature types, class imbalance, dimensionality and numbers of classes. We also found that by biclustering the performance of a set of different ML algorithms on the datasets, we could observe classes of problems and algorithms that work well or poorly in conjunction.\nOf course, PMLB is not yet a fully comprehensive benchmark suite for supervised classification methods. For instance, it currently excludes datasets with missing values or regression tasks and PMLB only has a handful of highly imbalanced datasets. One approach to adding diversity, pursued by the KEEL repository, is to augment existing datasets to simulated missingness and noise. However, we propose to avoid adding multiple variants of the same dataset, and instead identify and simulate new datasets with varying properties and meta-features to expand the PMLB suite and \u201cfill in the gaps\u201d of underrepresented problem\ntypes from a data science perspective. As in the present study, we plan to use performance comparisons over a diversity of ML methods in order to identify a limited set of benchmark standards able to diversely identify methodological advantages and disadvantages.\nWe expect this future work to lead to a more comprehensive benchmark tool that will better aid researchers in discovering the strengths and weaknesses of ML methods, and ultimately lead to more thorough\u2014and honest\u2014comparisons between ML methods."}, {"heading": "Acknowledgments", "text": "We thank Dr. Andreas C. Mu\u0308ller for his valuable input during the development of this project. We also thank the Penn Medicine Academic Computing Services for the use of their computing resources. This work was supported by National Institutes of Health grants AI116794, DK112217, ES013508, EY022300, HL134015, LM009012, LM010098, LM011360, TR001263, and the Warren Center for Network and Data Science."}], "references": [{"title": "Spectral biclustering", "author": ["Yuval Kluger", "Ronen Basri", "Joseph T Chang", "Mark Gerstein"], "venue": null, "citeRegEx": "Kluger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kluger et al\\.", "year": 2009}, {"title": "Natural Selection", "author": ["William La Cava", "Kourosh Danai", "Lee Spector"], "venue": null, "citeRegEx": "Cava et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Cava et al\\.", "year": 1992}, {"title": "A flexible computational framework for detecting, characterizing, and interpreting statistical patterns of epistasis in genetic studies of human disease susceptibility", "author": ["Jason H Moore", "Joshua C Gilbert", "Chia-Ti Tsai", "Fu-Tien Chiang", "Todd Holden", "Nate Barney", "Bill C White"], "venue": "Journal of Theoretical Biology,", "citeRegEx": "Moore et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2006}, {"title": "Open issues in genetic programming", "author": ["Michael O\u2019Neill", "Leonardo Vanneschi", "Steven Gustafson", "Wolfgang Banzhaf"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "O.Neill et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Neill et al\\.", "year": 2010}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "A comprehensive dataset for evaluating approaches of various meta-learning tasks", "author": ["Matthias Reif"], "venue": "In First International Conference on Pattern Recognition and Methods (ICPRAM),", "citeRegEx": "Reif.,? \\Q2012\\E", "shortCiteRegEx": "Reif.", "year": 2012}, {"title": "Creating and benchmarking a new dataset for physical activity monitoring", "author": ["Attila Reiss", "Didier Stricker"], "venue": "In Proceedings of the 5th International Conference on Pervasive Technologies Related to Assistive Environments,", "citeRegEx": "Reiss and Stricker.,? \\Q2012\\E", "shortCiteRegEx": "Reiss and Stricker.", "year": 2012}, {"title": "Machine learning benchmarks and random forest regression", "author": ["Mark R Segal"], "venue": "Center for Bioinformatics & Molecular Biostatistics,", "citeRegEx": "Segal.,? \\Q2004\\E", "shortCiteRegEx": "Segal.", "year": 2004}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["Johannes Stallkamp", "Marc Schlipsing", "Jan Salmen", "Christian Igel"], "venue": "Neural Networks,", "citeRegEx": "Stallkamp et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stallkamp et al\\.", "year": 2012}, {"title": "Exstracs 2.0: description and evaluation of a scalable learning classifier system", "author": ["Ryan J Urbanowicz", "Jason H Moore"], "venue": "Evolutionary Intelligence,", "citeRegEx": "Urbanowicz and Moore.,? \\Q2015\\E", "shortCiteRegEx": "Urbanowicz and Moore.", "year": 2015}, {"title": "Predicting the difficulty of pure, strict, epistatic models: metrics for simulated model selection", "author": ["Ryan J Urbanowicz", "Jeff Kiralis", "Jonathan M Fisher", "Jason H Moore"], "venue": "BioData Mining,", "citeRegEx": "Urbanowicz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Urbanowicz et al\\.", "year": 2012}, {"title": "Gametes: a fast, direct algorithm for generating pure, strict, epistatic models with random architectures", "author": ["Ryan J Urbanowicz", "Jeff Kiralis", "Nicholas A Sinnott-Armstrong", "Tamra Heberling", "Jonathan M Fisher", "Jason H Moore"], "venue": "BioData Mining,", "citeRegEx": "Urbanowicz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Urbanowicz et al\\.", "year": 2012}, {"title": "A balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction", "author": ["Digna R Velez"], "venue": "Genetic Epidemiology,", "citeRegEx": "Velez,? \\Q2007\\E", "shortCiteRegEx": "Velez", "year": 2007}, {"title": "Better gp benchmarks: community survey results and proposals", "author": ["David R. White", "James McDermott", "Mauro Castelli", "Luca Manzoni", "Brian W. Goldman", "Gabriel Kronberger", "Wojciech Ja\u015bkowski", "Una-May O\u2019Reilly", "Sean Luke"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "White et al\\.,? \\Q2013\\E", "shortCiteRegEx": "White et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "For example, the GAMETES genetic-data simulation software generates epistatic patterns of association in \u2018mock\u2019 single nucleotide polymorphism (SNP) data Urbanowicz et al. (2012b,a). The third form is toy data, which we will define here as data that is also artificially generated with a known embedded pattern but without an emphasis on representing real-world data, e.g., the parity or multiplexer problems Blum et al. (2003); Koza (1992).", "startOffset": 154, "endOffset": 428}, {"referenceID": 6, "context": "For example, the GAMETES genetic-data simulation software generates epistatic patterns of association in \u2018mock\u2019 single nucleotide polymorphism (SNP) data Urbanowicz et al. (2012b,a). The third form is toy data, which we will define here as data that is also artificially generated with a known embedded pattern but without an emphasis on representing real-world data, e.g., the parity or multiplexer problems Blum et al. (2003); Koza (1992). It is worth noting that the term \u2018toy dataset\u2019 has often been used to describe a small and simple dataset such as the examples included with algorithm software.", "startOffset": 154, "endOffset": 441}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al.", "startOffset": 126, "endOffset": 148}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al.", "startOffset": 126, "endOffset": 173}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al. (2013). Benchmarking efforts may focus on a specific application of interest, e.", "startOffset": 126, "endOffset": 194}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al. (2013). Benchmarking efforts may focus on a specific application of interest, e.g. traffic sign detection Stallkamp et al. (2012), or a more narrowly defined ML problem type, e.", "startOffset": 126, "endOffset": 317}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements.", "startOffset": 47, "endOffset": 85}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them.", "startOffset": 47, "endOffset": 700}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them.", "startOffset": 47, "endOffset": 728}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them. As a result, many benchmark datasets go unused simply because they are too difficult to preprocess. Further, while real-world benchmarks can be derived from many different problem domains, from a strict data science perspective, many of the benchmarks in repositories can have very similar meta-features (e.g. the number of instances, number of features, number of classes, presence of missing data, and similar signal to noise ratios, etc.), such that while they are representative of different real-world problems, they may not represent a diverse assembly of data science problems. This issue has been raised previously; when applying UCI datasets as benchmarks, it was noted that the scope of included datasets limited method evaluation, and suggested that repositories such as UCI should be expanded Segal (2004).", "startOffset": 47, "endOffset": 1725}, {"referenceID": 4, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods.", "startOffset": 95, "endOffset": 114}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013).", "startOffset": 95, "endOffset": 637}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013). In addition to collecting data from these resources, PMLB standardizes the format of these data and provides useful interfaces for fetching datasets directly from the web.", "startOffset": 95, "endOffset": 678}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013). In addition to collecting data from these resources, PMLB standardizes the format of these data and provides useful interfaces for fetching datasets directly from the web. This initial PMLB repository is not meant to be comprehensive; it includes mainly real-world datasets and excludes regression datasets (i.e. those with a continuous-valued dependent variable), as well as any datasets with missing values. We have chosen to focus our initial assessment on available datasets in classification. This paper includes a highlevel analysis of the properties (i.e. meta-features) of the founding PMLB datasets, such as feature counts, class imbalance, etc. Further, we evaluate the performance of 13 standard statistical ML methods from scikit-learn Pedregosa et al. (2011) over the full set of PMLB datasets.", "startOffset": 95, "endOffset": 1451}, {"referenceID": 5, "context": "(2010), and meta-learning benchmark Reif (2012). As such, the PMLB includes most of the real-world benchmark datasets commonly used in ML benchmarking studies.", "startOffset": 36, "endOffset": 48}, {"referenceID": 4, "context": "Evaluating Machine Learning Methods To provide a basis for comparison, we evaluated 13 supervised ML classification methods from scikit-learn Pedregosa et al. (2011) on the 165 datasets in PMLB.", "startOffset": 142, "endOffset": 166}, {"referenceID": 10, "context": "ML methods were evaluated using balanced accuracy Velez et al. (2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies.", "startOffset": 50, "endOffset": 70}, {"referenceID": 8, "context": "(2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies.", "startOffset": 8, "endOffset": 36}, {"referenceID": 8, "context": "(2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies. For more information on these ML methods, see Hastie et al. (2009) and the scikit-learn documentation Pedregosa et al.", "startOffset": 8, "endOffset": 288}, {"referenceID": 4, "context": "(2009) and the scikit-learn documentation Pedregosa et al. (2011). When we evaluated each ML method, we first scaled the features of the datasets by subtracting the mean and scaling the features to unit variance.", "startOffset": 42, "endOffset": 66}, {"referenceID": 0, "context": "2 Model-Dataset Biclustering Figure 4 summarizes the results of biclustering the balanced accuracy of the tuned models according to the ML method and dataset using a spectral biclustering algorithm Kluger et al. (2003). The methods and datasets are grouped into 40 contiguous biclusters (4 MLwise clusters by 10 data-wise clusters) in order to expose relationships between models and datasets.", "startOffset": 198, "endOffset": 219}], "year": 2017, "abstractText": "The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.", "creator": "LaTeX with hyperref package"}}}