{"id": "1703.01717", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Measuring Sample Quality with Kernels", "abstract": "Since conventional MCMC diagnostics do not detect these distortions, researchers have developed calculable stone discrepancy measures that demonstrably determine the convergence of a sample with its target distribution. Recently, this approach has been combined with the theory of nucleus reproduction to define a closed-form kernel-stone discrepancy (KSD) that can be calculated by summing kernel assessments via pairs of sample points. We are developing a theory of weak convergence for KSDs based on the Stein Method, which shows that commonly used KSDs do not detect inconvergence even for Gaussian targets, and show that nuclei with slow-falling tails have been proven to determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing distorted, exact, and deterministic sample sequences, and are easier to calculate and parallelise than stone discrepancies.", "histories": [["v1", "Mon, 6 Mar 2017 03:22:39 GMT  (1796kb,D)", "https://arxiv.org/abs/1703.01717v1", null], ["v2", "Mon, 12 Jun 2017 06:04:43 GMT  (1834kb,D)", "http://arxiv.org/abs/1703.01717v2", null], ["v3", "Fri, 7 Jul 2017 20:41:24 GMT  (1834kb,D)", "http://arxiv.org/abs/1703.01717v3", null], ["v4", "Tue, 11 Jul 2017 23:30:56 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v4", null], ["v5", "Fri, 21 Jul 2017 04:38:46 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v5", null], ["v6", "Thu, 3 Aug 2017 21:23:32 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v6", null], ["v7", "Sat, 19 Aug 2017 01:35:40 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v7", null], ["v8", "Wed, 13 Sep 2017 20:51:38 GMT  (1835kb,D)", "http://arxiv.org/abs/1703.01717v8", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jackson gorham", "lester w mackey"], "accepted": true, "id": "1703.01717"}, "pdf": {"name": "1703.01717.pdf", "metadata": {"source": "META", "title": "Measuring Sample Quality with Kernels", "authors": ["Jackson Gorham", "Lester Mackey"], "emails": ["<jgorham@stanford.edu>,", "<lmackey@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample averages EQn [h(X)] = 1n \u2211n i=1 h(xi). However, many exact\n1Stanford University, Palo Alto, CA USA 2Microsoft Research New England, Cambridge, MA USA. Correspondence to: Jackson Gorham <jgorham@stanford.edu>, Lester Mackey <lmackey@microsoft.com>.\nMCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.\nSince standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points.\nThis closed form represents a significant practical advantage, as no linear program solvers are necessary, and the computation of the discrepancy can be easily parallelized. However, as we will see in Section 3.2, not all kernel Stein discrepancies are suitable for our setting. In particular, in dimension d \u2265 3, the kernel Stein discrepancies previously recommended in the literature fail to detect when a sample is not converging to the target. To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions.\nAfter formally describing our goals for measuring sample quality in Section 2, we outline our strategy, based on Stein\u2019s method, for constructing and analyzing practical quality measures at the start of Section 3. In Section 3.1, we define our family of closed-form quality measures \u2013 the kernel Stein discrepancies (KSDs) \u2013 and establish several appealing practical properties of these measures. We an-\nar X\niv :1\n70 3.\n01 71\n7v 8\n[ st\nat .M\nL ]\n1 3\nSe p\n20 17\nalyze the convergence properties of KSDs in Sections 3.2 and 3.3, showing that previously proposed KSDs fail to detect non-convergence and proposing practical convergencedetermining alternatives. Section 4 illustrates the value of convergence-determining kernel Stein discrepancies in a variety of applications, including hyperparameter selection, sampler selection, one-sample hypothesis testing, and sample quality improvement. Finally, in Section 5, we conclude with a discussion of related and future work.\nNotation We will use \u00b5 to denote a generic probability measure and \u21d2 to denote the weak convergence of a sequence of probability measures. We will use \u2016\u00b7\u2016r for r \u2208 [1,\u221e] to represent the `r norm on Rd and occasionally refer to a generic norm \u2016\u00b7\u2016 with associated dual norm \u2016a\u2016\u2217 , supb\u2208Rd,\u2016b\u2016=1 \u3008a, b\u3009 for vectors a \u2208 Rd. We let ej be the j-th standard basis vector. For any function g : Rd \u2192 Rd\u2032 , we define M0(g) , supx\u2208Rd\u2016g(x)\u20162, M1(g) , supx 6=y\u2016g(x)\u2212 g(y)\u20162/\u2016x\u2212 y\u20162, and \u2207g as the gradient with components (\u2207g(x))jk , \u2207xkgj(x). We further let g \u2208 Cm indicate that g is m times continuously differentiable and g \u2208 Cm0 indicate that g \u2208 Cm and \u2207lg is vanishing at infinity for all l \u2208 {0, . . . ,m}. We define C(m,m) (respectively, C(m,m)b and C (m,m) 0 ) to be the set of functions k : Rd \u00d7 Rd \u2192 R with (x, y) 7\u2192 \u2207lx\u2207lyk(x, y) continuous (respectively, continuous and uniformly bounded, continuous and vanishing at infinity) for all l \u2208 {0, . . . ,m}."}, {"heading": "2. Quality measures for samples", "text": "Consider a target distribution P with continuously differentiable (Lebesgue) density p supported on all of Rd. We assume that the score function b , \u2207 log p can be evaluated1 but that, for most functions of interest, direct integration under P is infeasible. We will therefore approximate integration under P using a weighted sample Qn =\u2211n i=1 qn(xi)\u03b4xi with sample points x1, . . . , xn \u2208 Rd and qn a probability mass function. We will make no assumptions about the origins of the sample points; they may be the output of a Markov chain or even deterministically generated. Each Qn offers an approximation EQn [h(X)] =\u2211n i=1 qn(xi)h(xi) for each intractable expectation EP [h(Z)], and our aim is to effectively compare the quality of the approximation offered by any two samples targeting P . In particular, we wish to produce a quality measure that (i) identifies when a sequence of samples is converging to the target, (ii) determines when a sequence of samples is not converging to the target, and (iii) is efficiently computable. Since our interest is in approximating expectations, we will consider discrepancies\n1No knowledge of the normalizing constant is needed.\nquantifying the maximum expectation error over a class of test functionsH:\ndH(Qn, P ) , sup h\u2208H |EP [h(Z)]\u2212 EQn [h(X)]|. (1)\nWhen H is large enough, for any sequence of probability measures (\u00b5m)m\u22651, dH(\u00b5m, P )\u2192 0 only if \u00b5m \u21d2 P . In this case, we call (1) an integral probability metric (IPM) (Mu\u0308ller, 1997). For example, when H = BL\u2016\u00b7\u20162 , {h : Rd \u2192 R |M0(h) + M1(h) \u2264 1}, the IPM dBL\u2016\u00b7\u20162 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution. Alternatively, when H = W\u2016\u00b7\u20162 , {h : R\nd \u2192 R |M1(h) \u2264 1} is the set of 1-Lipschitz functions, the IPM dW\u2016\u00b7\u2016 in (1) is known as the Wasserstein metric.\nAn apparent practical problem with using the IPM dH as a sample quality measure is that EP [h(Z)] may not be computable for h \u2208 H. However, if H were chosen such that EP [h(Z)] = 0 for all h \u2208 H, then no explicit integration under P would be necessary. To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein\u2019s method for characterizing distributional convergence."}, {"heading": "3. Stein\u2019s method with kernels", "text": "Stein\u2019s method (Stein, 1972) provides a three-step recipe for assessing convergence in distribution:\n1. Identify a Stein operator T that maps functions g : Rd \u2192 Rd from a domain G to real-valued functions T g such that\nEP [(T g)(Z)] = 0 for all g \u2208 G.\nFor any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as\nS(\u00b5, T ,G) , sup g\u2208G |E\u00b5[(T g)(X)]| = dT G(\u00b5, P ) (2)\nwhich, crucially, avoids explicit integration under P .\n2. Lower bound the Stein discrepancy by an IPM dH known to dominate weak convergence. This can be done once for a broad class of target distributions to ensure that \u00b5m \u21d2 P whenever S(\u00b5m, T ,G) \u2192 0 for a sequence of probability measures (\u00b5m)m\u22651 (Desideratum (ii)).\n3. Provide an upper bound on the Stein discrepancy ensuring that S(\u00b5m, T ,G) \u2192 0 under suitable convergence of \u00b5m to P (Desideratum (i)).\nWhile Stein\u2019s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality. The subsections to follow develop a specific, practical instantiation of the abstract Stein\u2019s method recipe based on reproducing kernel Hilbert spaces. An empirical analysis of the Stein discrepancies recommended by our theory follows in Section 4."}, {"heading": "3.1. Selecting a Stein operator and a Stein set", "text": "A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017),\n(T g)(x) , 1p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).\nInspired by the generator method of Barbour (1988; 1990) and Go\u0308tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator\n(TP g)(x) , 1p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009\nfor functions g : Rd \u2192 Rd was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity.\nHereafter, we will let k : Rd\u00d7Rd \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from Rd \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 Rd, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk.\nWith this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2\nGk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}.\nThe following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C(1,1)b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016.\n2Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here.\nThe Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u00b5, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016). Our next result shows that, for any \u2016\u00b7\u2016, the KSD admits a closed-form solution. Proposition 2 (KSD closed form). Suppose k \u2208 C(1,1), and, for each j \u2208 {1, . . . d}, define the Stein kernel\nkj0(x, y) , 1 p(x)p(y)\u2207xj\u2207yj (p(x)k(x, y)p(y)) (3)\n= bj(x)bj(y)k(x, y) + bj(x)\u2207yjk(x, y) + bj(y)\u2207xjk(x, y) +\u2207xj\u2207yjk(x, y).\nIf \u2211d j=1 E\u00b5 [ kj0(X,X) 1/2 ] < \u221e, then S(\u00b5, TP ,Gk,\u2016\u00b7\u2016) =\n\u2016w\u2016 where wj , \u221a E\u00b5\u00d7\u00b5[kj0(X, X\u0303)] with X, X\u0303 iid\u223c \u00b5.\nThe proof is found in Section C. Notably, when \u00b5 is the discrete measure Qn = \u2211n i=1 qn(xi)\u03b4xi , the KSD reduces to evaluating each kj0 at pairs of support points as wj =\u221a\u2211n i,i\u2032=1 qn(xi)k j 0(xi, xi\u2032)qn(xi\u2032), a computation which is easily parallelized over sample pairs and coordinates j.\nOur Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k0 =\u2211d j=1 k j 0 to develop nonparametric control variates. Each term wj in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between \u00b5 and P measured with respect to the Stein kernel kj0. In standard uses of MMD, an arbitrary kernel function is selected, and one must be able to compute expectations of the kernel function under P . Here, this requirement is satisfied automatically, since our induced kernels are chosen to have mean zero under P .\nFor clarity we will focus on the specific kernel Stein set choice Gk , Gk,\u2016\u00b7\u20162 for the remainder of the paper, but our results extend directly to KSDs based on any \u2016\u00b7\u2016, since all KSDs are equivalent in a strong sense: Proposition 3 (Kernel Stein set equivalence). Under the assumptions of Proposition 2, there are constants cd, c\u2032d > 0 depending only on d and \u2016\u00b7\u2016 such that cdS(\u00b5, TP ,Gk,\u2016\u00b7\u2016) \u2264 S(\u00b5, TP ,Gk,\u2016\u00b7\u20162) \u2264 c\u2032dS(\u00b5, TP ,Gk,\u2016\u00b7\u2016).\nThe short proof is found in Section D."}, {"heading": "3.2. Lower bounding the kernel Stein discrepancy", "text": "We next aim to establish conditions under which the KSD S(\u00b5m, TP ,Gk) \u2192 0 only if \u00b5m \u21d2 P (Desideratum (ii)). Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:\nDefinition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)). A distribution P is distantly dissipative if \u03ba0 , lim infr\u2192\u221e \u03ba(r) > 0 for\n\u03ba(r) = inf{\u22122 \u3008b(x)\u2212b(y),x\u2212y\u3009\u2016x\u2212y\u201622 : \u2016x\u2212 y\u20162 = r}. (4)\nExamples of distributions in P include finite Gaussian mixtures with common covariance and all distributions strongly log-concave outside of a compact set, including Bayesian linear, logistic, and Huber regression posteriors with Gaussian priors (see Gorham et al., 2016, Section 4). Moreover, when d = 1, membership in P is sufficient to provide a lower bound on the KSD for most common kernels including the Gaussian, Mate\u0301rn, and inverse multiquadric kernels. Theorem 5 (Univariate KSD detects non-convergence). Suppose that P \u2208 P and k(x, y) = \u03a6(x \u2212 y) for \u03a6 \u2208 C2 with a non-vanishing generalized Fourier transform. If d = 1, then S(\u00b5m, TP ,Gk)\u2192 0 only if \u00b5m \u21d2 P .\nThe proof in Section E provides a lower bound on the KSD in terms of an IPM known to dominate weak convergence. However, our next theorem shows that in higher dimensions S(Qn, TP ,Gk) can converge to 0 without the sequence (Qn)n\u22651 converging to any probability measure. This deficiency occurs even when the target is Gaussian. Theorem 6 (KSD fails with light kernel tails). Suppose k \u2208 C(1,1)b and define the kernel decay rate\n\u03b3(r) , sup{max(|k(x, y)|, \u2016\u2207xk(x, y)\u20162, |\u3008\u2207x,\u2207yk(x, y)\u3009|) : \u2016x\u2212 y\u20162 \u2265 r}.\nIf d \u2265 3, P = N (0, Id), and \u03b3(r) = o(r\u2212\u03b1) for \u03b1 , ( 12 \u2212 1 d ) \u22121, then S(Qn, TP ,Gk)\u2192 0 does not imply Qn \u21d2 P .\nTheorem 6 implies that KSDs based on the commonly used Gaussian kernel, Mate\u0301rn kernel, and compactly supported kernels of Wendland (2004, Theorem 9.13) all fail to detect non-convergence when d \u2265 3. In addition, KSDs based on the inverse multiquadric kernel (k(x, y) = (c2 + \u2016x\u2212 y\u201622)\u03b2) for \u03b2 < \u22121 fail to detect non-convergence for any d > 2\u03b2/(\u03b2 + 1). The proof in Section F shows that the violating sample sequences (Qn)n\u22651 are simple to construct, and we provide an empirical demonstration of this failure to detect non-convergence in Section 4.\nThe failure of the KSDs in Theorem 6 can be traced to their inability to enforce uniform tightness. A sequence of probability measures (\u00b5m)m\u22651 is uniformly tight if for every > 0, there is a finite number R( ) such that lim supm \u00b5m(\u2016X\u20162 > R( )) \u2264 . Uniform tightness implies that no mass in the sequence of probability measures escapes to infinity. When the kernel k decays more rapidly than the score function grows, the KSD ignores excess mass in the tails and hence can be driven to zero by a\nnon-tight sequence of increasingly diffuse probability measures. The following theorem demonstrates uniform tightness is the missing piece to ensure weak convergence. Theorem 7 (KSD detects tight non-convergence). Suppose that P \u2208 P and k(x, y) = \u03a6(x\u2212y) for \u03a6 \u2208 C2 with a nonvanishing generalized Fourier transform. If (\u00b5m)m\u22651 is uniformly tight, then S(\u00b5m, TP ,Gk)\u2192 0 only if \u00b5m \u21d2 P .\nOur proof in Section G explicitly lower bounds the KSD S(\u00b5, TP ,Gk) in terms of the bounded Lipschitz metric dBL\u2016\u00b7\u2016(\u00b5, P ), which exactly metrizes weak convergence.\nIdeally, when a sequence of probability measures is not uniformly tight, the KSD would reflect this divergence in its reported value. To achieve this, we consider the inverse multiquadric (IMQ) kernel k(x, y) = (c2 + \u2016x\u2212 y\u201622)\u03b2 for some \u03b2 < 0 and c > 0. While KSDs based on IMQ kernels fail to determine convergence when \u03b2 < \u22121 (by Theorem 6), our next theorem shows that they automatically enforce tightness and detect non-convergence whenever \u03b2 \u2208 (\u22121, 0). Theorem 8 (IMQ KSD detects non-convergence). Suppose P \u2208 P and k(x, y) = (c2 + \u2016x\u2212 y\u201622)\u03b2 for c > 0 and \u03b2 \u2208 (\u22121, 0). If S(\u00b5m, TP ,Gk)\u2192 0, then \u00b5m \u21d2 P .\nThe proof in Section H provides a lower bound on the KSD in terms of the bounded Lipschitz metric dBL\u2016\u00b7\u2016(\u00b5, P ). The success of the IMQ kernel over other common characteristic kernels can be attributed to its slow decay rate. When P \u2208 P and the IMQ exponent \u03b2 > \u22121, the function class TPGk contains unbounded (coercive) functions. These functions ensure that the IMQ KSD S(\u00b5m, TP ,Gk) goes to 0 only if (\u00b5m)m\u22651 is uniformly tight."}, {"heading": "3.3. Upper bounding the kernel Stein discrepancy", "text": "The usual goal in upper bounding the Stein discrepancy is to provide a rate of convergence to P for particular approximating sequences (\u00b5m)\u221em=1. Because we aim to directly compute the KSD for arbitrary samples Qn, our chief purpose in this section is to ensure that the KSD S(\u00b5m, TP ,Gk) will converge to zero when \u00b5m is converging to P (Desideratum (i)).\nProposition 9 (KSD detects convergence). If k \u2208 C(2,2)b and \u2207 log p is Lipschitz with EP [\u2016\u2207 log p(Z)\u201622] < \u221e, then S(\u00b5m, TP ,Gk) \u2192 0 whenever the Wasserstein distance dW\u2016\u00b7\u20162 (\u00b5m, P )\u2192 0.\nProposition 9 applies to common kernels like the Gaussian, Mate\u0301rn, and IMQ kernels, and its proof in Section I provides an explicit upper bound on the KSD in terms of the Wasserstein distance dW\u2016\u00b7\u20162 . When Qn = 1 n \u2211n i=1 \u03b4xi for xi iid\u223c \u00b5, (Liu et al., 2016, Thm. 4.1) further implies that S(Qn, TP ,Gk)\u21d2 S(\u00b5, TP ,Gk) at an O(n\u22121/2) rate under continuity and integrability assumptions on \u00b5."}, {"heading": "4. Experiments", "text": "We next conduct an empirical evaluation of the KSD quality measures recommended by our theory, recording all timings on an Intel Xeon CPU E5-2650 v2 @ 2.60GHz. Throughout, we will refer to the KSD with IMQ base kernel k(x, y) = (c2 + \u2016x\u2212 y\u201622)\u03b2 , exponent \u03b2 = \u2212 1 2 , and c = 1 as the IMQ KSD. Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.github.io/ SteinDiscrepancy.jl/."}, {"heading": "4.1. Comparing discrepancies", "text": "Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.5 as our target P and generate a first sample point sequence i.i.d. from the target and a second sequence i.i.d. from one component of the mixture, N (\u2212\u2206e1, Id). As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n\u22120.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample. This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy.\nThe middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P . Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.0.4 linear program solver for the graph Stein discrepancy. We find that the two methods have nearly identical runtimes when d = 1 but that the KSD is 10 to 1000 times faster when d = 4. In addition, the KSD is straightforwardly parallelized and does not require access to a linear program solver, making it an appealing practical choice for a quality measure.\nFinally, the right panel displays the optimal Stein functions, gj(y) = EQn [bj(X)k(X,y)+\u2207xj k(X,y)]\nS(Qn,TP ,Gk) , recovered by the IMQ KSD when d = 1 and n = 103. The associated\ntest functions h(y) = (TP g)(y) = \u2211d j=1 EQn [k j 0(X,y)]\nS(Qn,TP ,Gk) are the mean-zero functions under P that best discriminate the target P and the sample Qn. The optimal test function for the single component sample features large positive values in the oversampled region that fail to be offset by negative values in the undersampled region near the missing mode."}, {"heading": "4.2. The importance of kernel choice", "text": "Theorem 6 established that kernels with rapidly decaying tails yield KSDs that can be driven to zero by offtarget sample sequences. Our next experiment provides an empirical demonstration of this issue for a multivariate Gaussian target P = N (0, Id) and KSDs based on the popular Gaussian (k(x, y) = e\u2212\u2016x\u2212y\u2016 2 2/2) and Mate\u0301rn (k(x, y) = (1 + \u221a 3\u2016x\u2212 y\u20162)e\u2212 \u221a 3\u2016x\u2212y\u20162 ) radial kernels.\nFollowing the proof Theorem 6 in Section F, we construct an off-target sequence (Qn)n\u22651 that sends S(Qn, TP ,Gk) to 0 for these kernel choices whenever d \u2265 3. Specifically, for each n, we let Qn = 1n \u2211n i=1 \u03b4xi where, for all i and j, \u2016xi\u20162 \u2264 2n1/d log n and \u2016xi \u2212 xj\u20162 \u2265 2 log n. To select these sample points, we independently sample candidate points uniformly from the ball {x : \u2016x\u20162 \u2264 2n1/d log n}, accept any points not within 2 log n Euclidean distance of any previously accepted point, and terminate when n points have been accepted.\nFor various dimensions, Figure 2 displays the result of applying each KSD to the off-target sequence (Qn)n\u22651 and an \u201con-target\u201d sequence of points sampled i.i.d. from P . For comparison, we also display the behavior of the IMQ KSD which provably controls tightness and dominates weak convergence for this target by Theorem 8. As predicted, the Gaussian and Mate\u0301rn KSDs decay to 0 under the off-target sequence and decay more rapidly as the dimension d increases; the IMQ KSD remains bounded away from 0."}, {"heading": "4.3. Selecting sampler hyperparameters", "text": "The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) \u221d \u03c0(x) \u220fL l=1 \u03c0(yl|x) for \u03c0(\u00b7) a prior distribution on Rd and \u03c0(yl|x) the likelihood of a datapoint yl. A standard slice sampler must evaluate the likelihood of all L datapoints to draw each new sample point xi. To reduce this cost, the approximate slice sampler introduces a tuning parameter which determines the number of datapoints that contribute to an approximation of the slice sampling step; an appropriate setting of this parameter is imperative for accurate inference. When is too small, relatively few sample points will be generated in a given amount of sampling time, yielding sample expectations with high Monte Carlo variance. When is too large, the large approximation error will produce biased samples that no longer resemble the target.\nTo assess the suitability of the KSD for tolerance parameter selection, we take as our target P the bimodal Gaussian mixture model posterior of (Welling & Teh, 2011). For an array of values, we generated 50 independent approximate slice sampling chains with batch size 5, each with a\nbudget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3. ESS, which does not detect Markov chain bias, is maximized at the largest hyperparameter evaluated ( = 10\u22121), while the KSD is minimized at an intermediate value ( = 10\u22122). The right panel of Figure 3 shows representative samples produced by several settings of . The sample produced by the ESS-selected chain is significantly overdispersed, while the sample from = 0 has minimal coverage of the second mode due to\nits small sample size. The sample produced by the KSDselected chain best resembles the posterior target. Using 4 cores, the longest KSD computation with n = 103 sample points took 0.16s."}, {"heading": "4.4. Selecting samplers", "text": "Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (termed SGFS-f), a d\u00d7 d matrix must be inverted to draw each new sample point. Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point. Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution. Hence we will use the KSD \u2013 a quality measure that accounts for asymptotic bias \u2013 to evaluate and choose between these samplers.\nSpecifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1). The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 104 MNIST handwritten digit images. From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9. After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 \u00d7 104 points and d = 51 dimensions (including the intercept term). Figure 4 displays the IMQ KSD applied to the first n points in each sample. As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a\nHamiltonian Monte Carlo chain with 105 iterates. Both the KSD and the surrogate ground truth suggest that the moderate speed-up provided by SGFS-d (0.0017s per sample vs. 0.0019s for SGFS-f) is outweighed by the significant loss in inferential accuracy. However, the KSD assessment does not require access to an external trustworthy ground truth sample. The longest KSD computation took 400s using 16 cores."}, {"heading": "4.5. Beyond sample quality comparison", "text": "While our investigation of the KSD was motivated by the desire to develop practical, trustworthy tools for sample quality comparison, the kernels recommended by our theory can serve as drop-in replacements in other inferential tasks that make use of kernel Stein discrepancies."}, {"heading": "4.5.1. ONE-SAMPLE HYPOTHESIS TESTING", "text": "Chwialkowski et al. (2016) recently used the KSD S(Qn, TP ,Gk) to develop a hypothesis test of whether a given sample from a Markov chain was drawn from a target distribution P (see also Liu et al., 2016). However, the authors noted that the KSD test with their default Gaussian base kernel k experienced a considerable loss of power as the dimension d increased. We recreate their experiment and show that this loss of power can be avoided by using our default IMQ kernel with \u03b2 = \u2212 12 and c = 1. Following (Chwialkowski et al., 2016, Section 4) we draw zi iid\u223c N (0, Id) and ui iid\u223c Unif[0, 1] to generate a sample (xi) n i=1 with xi = zi + ui e1 for n = 500 and various dimensions d. Using the authors\u2019 code (modified to include an IMQ kernel), we compare the power of the Gaussian KSD test, the IMQ KSD test, and the standard normality test of Baringhaus & Henze (1988) (B&H) to discern whether the sample (xi)500i=1 came from the null distribution P = N (0, Id). The results, averaged over 400 simula-\ntions, are shown in Table 1. Notably, the IMQ KSD experiences no power degradation over this range of dimensions, thus improving on both the Gaussian KSD and the standard B&H normality tests."}, {"heading": "4.5.2. IMPROVING SAMPLE QUALITY", "text": "Liu & Lee (2016) recently used the KSD S(Qn, TP ,Gk) as a means of improving the quality of a sample. Specifically, given an initial sample Qn supported on x1, . . . , xn, they minimize S(Q\u0303n, TP ,Gk) over all measures Q\u0303n supported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = TPGk. In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e\u2212 1 h\u2016x\u2212y\u2016 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points. Using the authors\u2019 code, we recreate the experiment from (Liu & Lee, 2016, Fig. 2b) and introduce a KSD objective with an IMQ kernel k(x, y) = (1 + 1h\u2016x\u2212 y\u2016 2 2) \u22121/2 with bandwidth selected in the same fashion. The starting sample is given by Qn = 1 n \u2211n i=1 \u03b4xi for n = 100, various dimensions d, and each sample point drawn i.i.d. from P = N (0, Id). For the initial sample and the optimized samples produced by each KSD, Figure 5 displays the mean squared error (MSE) 1 d\u2016EP [Z]\u2212 EQ\u0303n [X]\u2016 2 2 averaged across 500 independently generated initial samples. Out of the box, the IMQ kernel produces better mean estimates than the standard Gaussian."}, {"heading": "5. Related and future work", "text": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested. For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.\nFor an approximation \u00b5 with continuously differentiable density r, Chwialkowski et al. (2016, Thm. 2.1) and Liu et al. (2016, Prop. 3.3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u00b5[k0(X,X) + \u2016\u2207 log p(X)r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u00b5, TP ,Gk) = 0 only if \u00b5 = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mate\u0301rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains,\nwhere tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Scho\u0308lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence.\nWhile assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016).\nIn the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. A reader may also wonder for which distributions outside ofP the KSD dominates weak convergence. The following theorem, proved in Section J, shows that no KSD with aC0 kernel dominates weak convergence when the target has a bounded score function. Theorem 10 (KSD fails for bounded scores). If \u2207 log p is bounded and k \u2208 C(1,1)0 , then S(Qn, TP ,Gk) \u2192 0 does not imply Qn \u21d2 P .\nHowever, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator TP with diffusion Stein operators of the form (T g)(x) =\n1 p(x) \u3008\u2207, p(x)(a(x) + c(x))g(x)\u3009. An analogous construction should yield convergence-determining diffusion KSDs for P outside of P . Our results also extend to targets P supported on a convex subset X of Rd by choosing k to satisfy p(x)k(x, \u00b7) \u2261 0 for all x on the boundary of X ."}, {"heading": "Acknowledgments", "text": "We thank Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton for sharing their hypothesis testing code, Qiang Liu for sharing his black-box importance sampling code, and Sebastian Vollmer and Andrew Duncan for many helpful conversations regarding this work. This material is based upon work supported by the National Science Foundation DMS RTG Grant No. 1501767, the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747, and the Frederick E. Terman Fellowship."}, {"heading": "A. Additional appendix notation", "text": "We use f \u2217 h to denote the convolution between f and h, and, for absolutely integrable f : Rd \u2192 R, we say f\u0302(\u03c9) , (2\u03c0)\u2212d/2 \u222b f(x)e\u2212i\u3008x,\u03c9\u3009dx is the Fourier transform of f . For g \u2208 Kdk we define \u2016g\u2016Kdk , \u221a\u2211d j=1\u2016gj\u2016 2 Kk . Let L 2\ndenote the Banach space of real-valued functions f with \u2016f\u2016L2 , \u222b f(x)2 dx < \u221e. For Rd-valued g, we will overload\ng \u2208 L2 to mean \u2016g\u2016L2 , \u221a\u2211d j=1\u2016gj\u2016 2 L2 < \u221e. We define the operator norm of a vector a \u2208 R\nd as \u2016a\u2016op , \u2016a\u20162 and of a matrix A \u2208 Rd\u00d7d as \u2016A\u2016op , supx\u2208Rd,\u2016x\u20162=1\u2016Ax\u20162. We further define the Lipschitz constant M2(g) , supx 6=y\u2016\u2207g(x)\u2212\u2207g(y)\u2016op/\u2016x\u2212 y\u20162 and the ball B(x, r) , {y \u2208 Rd | \u2016x\u2212 y\u20162 \u2264 r} for any x \u2208 Rd and r \u2265 0."}, {"heading": "B. Proof of Proposition 1: Zero mean test functions", "text": "Fix any g \u2208 G. Since k \u2208 C(1,1), supx\u2208Rd k(x, x) <\u221e, and supx\u2208Rd\u2016\u2207x\u2207yk(x, x)\u2016op <\u221e, Cor. 4.36 of (Steinwart & Christmann, 2008) implies that M0(gj) <\u221e and M1(gj) <\u221e for each j \u2208 {1, . . . , d}. As EP [\u2016b(Z)\u20162] <\u221e, the proof of (Gorham & Mackey, 2015, Prop. 1) now implies EP [(TP g)(Z)] = 0."}, {"heading": "C. Proof of Proposition 2: KSD closed form", "text": "Our proof generalizes that of (Chwialkowski et al., 2016, Thm. 2.1). For each dimension j \u2208 {1, . . . , d}, we define the operator T jP via (T j P g0)(x) , 1 p(x)\u2207xj (p(x)g0(x)) = \u2207xjg0(x) + bj(x)g0(x) for g0 : R\nd \u2192 R. We further let \u03a8k : Rd \u2192 Kk denote the canonical feature map of Kk, given by \u03a8k(x) , k(x, \u00b7). Since k \u2208 C(1,1), the argument of (Steinwart & Christmann, 2008, Cor. 4.36) implies that\nTP g(x) = \u2211d j=1(T j P gj)(x) = \u2211d j=1 T j P \u3008gj ,\u03a8k(x)\u3009Kk = \u2211d j=1\u3008gj , T j P\u03a8k(x)\u3009Kk (5)\nfor all g = (g1, . . . , gd) \u2208 Gk,\u2016\u00b7\u2016 and x \u2208 Rd. Moreover, (Steinwart & Christmann, 2008, Lem. 4.34) gives\n\u3008T jP\u03a8k(x), T j P\u03a8k(y)\u3009 = \u3008bj(x)\u03a8k(x) +\u2207xj\u03a8k(x), bj(y)\u03a8k(y) +\u2207yj\u03a8k(y)\u3009Kk\n= bj(x)bj(y)k(x, y) + bj(x)\u2207yjk(x, y) + bj(y)\u2207xjk(x, y) +\u2207xj\u2207yjk(x, y) = k j 0(x, y) (6)\nfor all x, y \u2208 Rd and j \u2208 {1, . . . , d}. The representation (6) and our \u00b5-integrability assumption together imply that, for each j, T jP\u03a8k is Bochner \u00b5-integrable (Steinwart & Christmann, 2008, Definition A.5.20), since\nE\u00b5 [\u2225\u2225\u2225T jP\u03a8k(X)\u2225\u2225\u2225Kk ] = E\u00b5 [\u221a kj0(X,X) ] <\u221e.\nHence, we may apply the representation (6) and exchange expectation and RKHS inner product to discover\nw2j = E [ kj0(X, X\u0303) ] = E [ \u3008T jP\u03a8k(X), T j P\u03a8k(X\u0303)\u3009Kk ] = \u2225\u2225\u2225E\u00b5[T jP\u03a8k(X)]\u2225\u2225\u22252Kk . (7)\nfor X, X\u0303 iid\u223c \u00b5. To conclude, we invoke the representation (5), Bochner \u00b5-integrability, the representation (7), and the Fenchel-Young inequality for dual norms twice:\nS(\u00b5, TP ,Gk,\u2016\u00b7\u2016) = sup g\u2208Gk,\u2016\u00b7\u2016 E\u00b5[(TP g)(X)] = sup \u2016gj\u2016Kk=vj ,\u2016v\u2016 \u2217\u22641\n\u2211d j=1\u3008gj ,E\u00b5[T j P\u03a8k(X)]\u3009Kk\n= sup \u2016v\u2016\u2217\u22641\n\u2211d j=1vj \u2225\u2225\u2225E\u00b5[T jP\u03a8k(X)]\u2225\u2225\u2225Kk = sup\u2016v\u2016\u2217\u22641\u2211dj=1vjwj = \u2016w\u2016."}, {"heading": "D. Proof of Proposition 3: Stein set equivalence", "text": "By Proposition 2, S(\u00b5, TP ,Gk,\u2016\u00b7\u2016) = \u2016w\u2016 and S(\u00b5, TP ,Gk,\u2016\u00b7\u20162) = \u2016w\u20162 for some vector w, and by (Bachman & Narici, 1966, Thm. 8.7), there exist constants cd, c\u2032d > 0 depending only on d and \u2016\u00b7\u2016 such that cd\u2016w\u2016 \u2264 \u2016w\u20162 \u2264 c\u2032d\u2016w\u2016."}, {"heading": "E. Proof of Theorem 5: Univariate KSD detects non-convergence", "text": "While the statement of Theorem 5 applies only to the univariate case d = 1, we will prove all steps for general d when possible. Our strategy is to define a reference IPM dH for which \u00b5m \u21d2 P whenever dH(\u00b5m, P ) \u2192 0 and then upper bound dH by a function of the KSD S(\u00b5m, TP ,Gk). To construct the reference class of test functions H, we choose some integrally strictly positive definite (ISPD) kernel kb : Rd \u00d7 Rd \u2192 R, that is, we select a kernel function kb such that\u222b\nRd\u00d7Rd kb(x, y)d\u00b5(x)d\u00b5(y) > 0\nfor all finite non-zero signed Borel measures \u00b5 on Rd (Sriperumbudur et al., 2010, Section 1.2). For this proof, we will choose the Gaussian kernel kb(x, y) = exp ( \u2212\u2016x\u2212 y\u201622/2 ) , which is ISPD by (Sriperumbudur et al., 2010, Section 3.1).\nSince r(x) , exp ( \u2212\u2016x\u201622/2 ) is bounded and continuous and never vanishes, the kernel k\u0303b(x, y) = kb(x, y)r(x)r(y) is\nalso ISPD. Let H , {h \u2208 Kk\u0303b | \u2016h\u2016k\u0303b \u2264 1}. By (Sriperumbudur, 2016, Thm. 3.2), since k\u0303b is ISPD with k\u0303b(x, \u00b7) \u2208 C0(Rd) for all x, we know that dH(\u00b5m, P ) \u2192 0 only if \u00b5m \u21d2 P . With H in hand, Theorem 5 will follow from our next theorem which upper bounds the IPM dH(\u00b5, P ) in terms of the KSD S(\u00b5, TP ,Gk). Theorem 11 (Univariate KSD lower bound). Let d = 1, and consider the set of univariate functions H = {h \u2208 Kk\u0303b | \u2016h\u2016k\u0303b \u2264 1}. Suppose P \u2208 P and k(x, y) = \u03a6(x \u2212 y) for \u03a6 \u2208 C\n2 with generalized Fourier transform \u03a6\u0302 and F (t) , sup\u2016\u03c9\u2016\u221e\u2264t \u03a6\u0302(\u03c9)\n\u22121 finite for all t > 0. Then there exists a constant MP > 0 such that, for all probability measures \u00b5 and > 0,\ndH(\u00b5, P ) \u2264 + ( \u03c0 2 )1/4MPF( 12 log 2\u03c0 (1 +M1(b)MP ) \u22121)1/2S(\u00b5, TP ,Gk). Remarks An explicit value for the Stein factor MP can be derived from the proof in Section E.1 and the results of Gorham et al. (2016). After optimizing the bound dH(\u00b5, P ) over > 0, the Gaussian, inverse multiquadric, and Mate\u0301rn (v > 1) kernels achieve rates of O(1/ \u221a log( 1S(\u00b5,TP ,Gk) )), O(1/ log( 1 S(\u00b5,TP ,Gk) )), and O(S(\u00b5, TP ,Gk) 1/(v+1/2)) respectively as S(\u00b5, TP ,Gk)\u2192 0.\nIn particular, since \u03a6\u0302 is non-vanishing, F (t) is finite for all t. If S(\u00b5m, TP ,Gk) \u2192 0, then, for any fixed > 0, we have lim supm\u2192\u221e dH(\u00b5m, P ) \u2264 . Taking \u2192 0 shows that limm\u2192\u221e dH(\u00b5m, P )\u2192 0, which implies that \u00b5m \u21d2 P .\nE.1. Proof of Theorem 11: Univariate KSD lower bound\nFix any probability measure \u00b5 and h \u2208 H, and define the tilting function \u039e(x) , (1 + \u2016x\u201622)1/2. The proof will proceed in three steps.\nStep 1: Uniform bounds on M0(h), M1(h) and supx\u2208Rd\u2016\u039e(x)\u2207h(x)\u20162 We first bound M0(h), M1(h) and supx\u2208Rd\u2016\u039e(x)\u2207h(x)\u20162 uniformly overH. To this end, we define the finite value c0 , supx\u2208Rd(1+\u2016x\u2016 2 2)r(x) = 2e\n\u22121/2. For all x \u2208 Rd, we have\n|h(x)| = |\u3008h, k\u0303b(x, \u00b7)\u3009Kk\u0303b | \u2264 \u2016h\u2016Kk\u0303b k\u0303b(x, x) 1/2 \u2264 1.\nMoreover, we have \u2207xkb(x, y) = (y \u2212 x)kb(x, y) and \u2207r(x) = \u2212xr(x). Thus for any x, by (Steinwart & Christmann, 2008, Corollary 4.36) we have\n\u2016\u2207h(x)\u20162 \u2264 \u2016h\u2016Kk\u0303b \u3008\u2207x,\u2207yk\u0303b(x, x)\u30091/2 \u2264 [d r(x)2 + \u2016x\u201622 r(x) 2]1/2kb(x, x) 1/2 \u2264 [(d\u2212 1)1/2 + (1 + \u2016x\u201622) 1/2)]r(x),\nwhere in the last inequality we used the triangle inequality. Hence \u2016\u2207h(x)\u20162 \u2264 (d \u2212 1)1/2 + 1 and \u2016\u039e(x)\u2207h(x)\u20162 \u2264 (d\u2212 1)1/2 + c0 for all x, completing our bounding of M0(h), M1(h) and supx\u2208Rd\u2016\u039e(x)\u2207h(x)\u20162 uniformly overH.\nStep 2: Uniform bound on \u2016gh\u2016L2 for Stein solution gh We next show that there is a solution to the P Stein equation\n(TP gh)(x) = h(x)\u2212 EP [h(Z)] (8)\nwith gh(x) \u2264 MP /(1 + \u2016x\u201622)1/2 for every h \u2208 H. When d = 1, this will imply that \u2016gh\u2016L2 is bounded uniformly over H. To proceed, we will define a tilted distribution P\u0303 \u2208 P and a tilted function f , show that a solution g\u0303f to the P\u0303 Stein equation is bounded, and construct a solution gh to the Stein equation of P based on g\u0303f .\nDefine P\u0303 via the tilted probability density p\u0303(x) \u221d p(x)/\u039e(x) with score function b\u0303(x) , \u2207 log p\u0303(x) = b(x) \u2212 \u03be(x) for \u03be(x) , \u2207 log \u039e(x) = x/(1 + \u2016x\u201622). Since b is Lipschitz and \u2207\u03be(x) = (1 + \u2016x\u2016 2 2) \u22121[I \u2212 2 xx >\n1+\u2016x\u201622 ] has its\noperator norm uniformly bounded by 3, b\u0303 is also Lipschitz. To see that P\u0303 is also distantly dissipative, note first that |\u3008\u03be(x)\u2212 \u03be(y), x\u2212 y\u3009| \u2264 \u2016\u03be(x)\u2212 \u03be(y)\u20162 \u00b7 \u2016x\u2212 y\u20162 \u2264 \u2016x\u2212 y\u20162 since supx\u2016\u03be(x)\u20162 \u2264 1/2. Because P is distantly dissipative, we know \u3008b(x)\u2212 b(y), x\u2212 y\u3009 \u2264 \u2212 12\u03ba0\u2016x\u2212 y\u2016 2 2 for some \u03ba0 > 0 and all \u2016x\u2212 y\u20162 \u2265 R for some R > 0. Thus for all \u2016x\u2212 y\u20162 \u2265 max(R, 4/\u03ba0), we have\n\u3008b\u0303(x)\u2212 b\u0303(y), x\u2212 y\u3009 = \u3008b(x)\u2212 b(y), x\u2212 y\u3009+ \u3008\u03be(x)\u2212 \u03be(y), x\u2212 y\u3009 \u2264 \u22121 2 \u03ba0\u2016x\u2212 y\u201622 + \u2016x\u2212 y\u20162 \u2264 \u2212 1 2 \u03ba0 2 \u2016x\u2212 y\u201622,\nso P\u0303 is also distantly dissipative and hence in P .\nLet f(x) , \u039e(x)(h(x) \u2212 EP [h(Z)]). Since EP\u0303 [f(Z)] = EP [h(Z)\u2212 EP [h(Z)]] = 0, Thm. 5 and Sec. 4.2 of (Gorham et al., 2016), imply that the P\u0303 Stein equation (TP\u0303 g\u0303f )(x) = f(x) has a solution g\u0303f with M0(gf ) \u2264 M\u2032PM1(f) forM\u2032P a constant independent of f and h. Since\u2207f(x) = \u2207\u039e(x)(h(x)\u2212EP [h(Z)]) + \u039e(x)\u2207h(x) and \u2016\u2207\u039e(x)\u20162 =\n\u2016x\u20162 (1+\u2016x\u201622)1/2\nis bounded by 1, M0(gf ) \u2264M\u2032P (2 + (d\u2212 1)1/2 + c0) ,MP , a constant independent of h.\nFinally, we note that gh(x) , g\u0303f (x)/\u039e(x) is a solution to the P Stein equation (8) satisfying gh(x) \u2264 MP /\u039e(x) = MP /(1 + \u2016x\u201622)1/2. Hence, in the case d = 1, we have \u2016gh\u2016L2 \u2264MP \u221a \u03c0.\nStep 3: Approximate TP gh using TPGk In our final step, we will use the following lemma, proved in Section E.2, to show that we can approximate TP gh arbitrarily well by a function in a scaled copy of TPGk. Lemma 12 (Stein approximations with finite RKHS norm). Suppose that g : Rd \u2192 Rd is bounded and belongs to L2\u2229C1 and that h = TP g is Lipschitz. Moreover, suppose k(x, y) = \u03a6(x\u2212 y) for \u03a6 \u2208 C2 with generalized Fourier transform \u03a6\u0302. Then for every > 0, there is a function g : Rd \u2192 Rd such that supx\u2208Rd |(TP g )(x)\u2212 (TP g)(x)| \u2264 and\n\u2016g \u2016Kdk \u2264 (2\u03c0) \u2212d/4F\n( 12d log 2 \u03c0 (M1(h) +M1(b)M0(g)) \u22121 )1/2 \u2016g\u2016L2 ,\nwhere F (t) , sup\u2016\u03c9\u2016\u221e\u2264t \u03a6\u0302(\u03c9) \u22121.\nWhen d = 1, Lemma 12 implies that for every > 0 there is a function g : R \u2192 R such that M0(TP g \u2212 h) \u2264 and \u2016g \u2016Kk \u2264 ( \u03c0 2 ) 1/4MPF ( 12 log 2\u03c0 (M1(h) +M1(b)MP ) \u22121)1/2. Hence we have\n|EP [h(Z)]\u2212 E\u00b5[h(X)]| \u2264 |E\u00b5[h(X)\u2212 (TP g )(X)]|+ |E\u00b5[(TP g )(X)]| \u2264 + \u2016g \u2016KkS(\u00b5, TP ,Gk)\n\u2264 + (2\u03c0)\u22121/4MP \u221a \u03c0F ( 12 log 2 \u03c0 (M1(h) +M1(b)MP ) \u22121 )1/2 S(\u00b5, TP ,Gk).\nTaking a supremum over h \u2208 H yields the advertised result.\nE.2. Proof of Lemma 12: Stein approximations with finite RKHS norm Let us define the function S : Rd \u2192 R via the mapping S(x) , \u220fd j=1 sin xj xj . Then S \u2208 L2 and \u222b Rd\u2016x\u20162S(x)\n4 < \u221e. We will then define the density function \u03c1(x) , Z\u22121S(x)4, where Z , \u222b Rd S(x)\n4 dx = (2\u03c0/3)d is the normalization constant. One can check that \u03c1\u0302(\u03c9)2 \u2264 (2\u03c0)\u2212dI[\u2016\u03c9\u2016\u221e \u2264 4].\nLet Y be a random variable with density \u03c1. For each \u03b4 > 0, let us define \u03c1\u03b4(x) = \u03b4\u2212d\u03c1(x/\u03b4) and for any function f let us denote f\u03b4(x) , E[f(x+ \u03b4Y )]. Since h = TP g is assumed Lipschitz, this implies |h\u03b4(x) \u2212 h(x)| = |E\u03c1[h(x+ \u03b4Y )\u2212 h(x)]| \u2264 \u03b4M1(h)E\u03c1[\u2016Y \u20162] for all x \u2208 Rd.\nNext, notice that for any \u03b4 > 0 and x \u2208 Rd,\n(TP g\u03b4)(x) = E\u03c1[\u3008b(x), g(x+ \u03b4Y )\u3009] + E[\u3008\u2207, g(x+ \u03b4Y )\u3009], and h\u03b4(x) = E\u03c1[\u3008b(x+ \u03b4Y ), g(x+ \u03b4Y )\u3009] + E[\u3008\u2207, g(x+ \u03b4Y )\u3009].\nThus because we assume b is Lipschitz, we can deduce from above for any x \u2208 Rd,\n|(TP g\u03b4)(x)\u2212 h\u03b4(x)| = |E\u03c1[\u3008b(x)\u2212 b(x+ \u03b4Y ), g(x+ \u03b4Y )\u3009]| \u2264 E\u03c1[\u2016b(x)\u2212 b(x+ \u03b4Y )\u20162\u2016g(x+ \u03b4Y )\u20162] \u2264M0(g)M1(b) \u03b4 E\u03c1[\u2016Y \u20162].\nThus for any > 0, letting \u0303 = /((M1(h) +M1(b)M0(g))E\u03c1[\u2016Y \u20162]), we have by the triangle inequality\n|(TP g\u0303)(x)\u2212 (TP g)(x)| \u2264 |(TP g\u0303)(x)\u2212 h\u0303(x)|+ |h\u0303(x)\u2212 h(x)| \u2264 .\nThus it remains to bound the RKHS norm of g\u03b4 . By the Convolution Theorem (Wendland, 2004, Thm. 5.16), we have g\u0302\u03b4(\u03c9) = (2\u03c0) d/2g\u0302(\u03c9)\u03c1\u0302\u03b4(\u03c9), and so the squared norm of g\u03b4 in Kdk is equal to (Wendland, 2004, Thm. 10.21)\n(2\u03c0)\u2212d/2 \u222b Rd |g\u0302\u03b4(\u03c9)|2 \u03a6\u0302(\u03c9) d\u03c9 = (2\u03c0)d/2 \u222b Rd |g\u0302(\u03c9)|2\u03c1\u0302\u03b4(\u03c9)2 \u03a6\u0302(\u03c9) d\u03c9 \u2264 (2\u03c0)\u2212d/2 { sup \u2016\u03c9\u2016\u221e\u22644\u03b4\u22121 \u03a6\u0302(\u03c9)\u22121 }\u222b Rd |g\u0302(\u03c9)|2 d\u03c9,\nwhere in the inequality we used the fact that \u03c1\u0302\u03b4(\u03c9) = \u03c1\u0302(\u03b4\u03c9). By Plancherel\u2019s theorem (Herb & Sally Jr., 2011, Thm. 1.1), we know that f \u2208 L2 implies that \u2016f\u2016L2 = \u2016f\u0302\u2016L2 . Thus we have \u2016g\u03b4\u2016Kdk \u2264 (2\u03c0)\n\u2212d/4F (4\u03b4\u22121)1/2\u2016g\u2016L2 . The final result follows from noticing that \u222b R sin\n4(x)/x4 dx = 2\u03c03 and also\u222b Rd \u2016x\u20162 d\u220f j=1 sin4 xj x4j dx \u2264 \u222b Rd \u2016x\u20161 d\u220f j=1 sin4 xj x4j dx = d\u2211 j=1 \u222b Rd (sinxj) 4 |xj |3 \u220f k 6=j sin4 xk x4k dx = 2d(log 2) ( 2\u03c0 3 )d\u22121 ,\nwhich implies E\u03c1[\u2016Y \u20162] \u2264 3d log 2 \u03c0 ."}, {"heading": "F. Proof of Theorem 6: KSD fails with light kernel tails", "text": "First, define the generalized inverse function \u03b3\u22121(s) , inf{r \u2265 0 | \u03b3(r) \u2264 s}. Next, fix an n \u2265 1, let \u2206n , max(1, \u03b3\u22121(1/n)), and define rn , \u2206nn1/d. Select n distinct points x1, . . . , xn \u2208 Rd so that zi,i\u2032 , xi \u2212 xi\u2032 satisfies \u2016zi,i\u2032\u20162 > \u2206n for all i 6= i\n\u2032 and \u2016xi\u20162 \u2264 rn for all i. By (Wainwright, 2017, Lems. 5.1 and 5.2), such a point set always exists. Now define Qn = 1n \u2211n i=1 \u03b4xi . We will show that if \u2206n grows at an appropriate rate then S(Qn, TP ,Gk) \u2192 0 as n\u2192\u221e.\nSince the target distribution P is N (0, Id), the associated gradient of the log density is b(x) = \u2212x. Thus\nk0(x, y) , d\u2211 j=1 kj0(x, y) = \u3008x, y\u3009k(x, y)\u2212 \u3008y,\u2207xk(x, y)\u3009 \u2212 \u3008x,\u2207yk(x, y)\u3009+ \u3008\u2207x,\u2207yk(x, y)\u3009.\nFrom Proposition 2, we have\nS(Qn, TP ,Gk)2 = 1\nn2 n\u2211 i,i\u2032=1 k0(xi, xi\u2032) = 1 n2 n\u2211 i=1 k0(xi, xi) + 1 n2 \u2211 i 6=i\u2032 k0(xi, xi\u2032). (9)\nSince k \u2208 C(2,2)b , \u03b3(0) <\u221e. Thus by Cauchy-Schwarz, the first term of (9) is upper bounded by\n1\nn2 n\u2211 i=1 k0(xi, xi) \u2264 1 n2 n\u2211 i=1 \u2016xi\u201622k(xi, xi) + \u2016xi\u20162(\u2016\u2207xk(xi, xi)\u20162 + \u2016\u2207yk(xi, xi)\u20162) + |\u3008\u2207x,\u2207yk(xi, xi)\u3009|\n\u2264 \u03b3(0) n [r2n + 2rn + 1] \u2264 \u03b3(0) n (n1/d\u2206n + 1) 2.\nTo handle the second term of (9), we will use the assumed bound on k and its derivatives from \u03b3. For any fixed i 6= i\u2032, by the triangle inequality, Cauchy-Schwarz, and fact \u03b3 is monotonically decreasing we have\n|k0(xi, xi\u2032)| \u2264 \u2016xi\u20162\u2016xi\u2032\u20162|k(xi, xi\u2032)|+ \u2016xi\u20162\u2016\u2207yk(xi, xi\u2032)\u20162 + \u2016xi\u2032\u20162\u2016\u2207xk(xi, xi\u2032)\u20162 + |\u3008\u2207x,\u2207yk(xi, xi\u2032)\u3009| \u2264 r2n\u03b3(\u2016zi,i\u2032\u20162) + rn\u03b3(\u2016zi,i\u2032\u20162) + rn\u03b3(\u2016zi,i\u2032\u20162) + \u03b3(\u2016zi,i\u2032\u20162) \u2264 (n1/d\u2206n + 1)2\u03b3(\u2206n).\nOur upper bounds on the Stein discrepancy (9) and our choice of \u2206n now imply that\nS(Qn, TP ,Gk) = O(n1/d\u22121/2\u03b3\u22121(1/n) + n\u22121/2).\nMoreover, since \u03b3(r) = o(r\u2212\u03b1), we have \u03b3\u22121(1/n) = o(n1/\u03b1) = o(n1/2\u22121/d), and hence S(Qn, TP ,Gk)\u2192 0 as n\u2192\u221e.\nHowever, the sequence (Qn)n\u22651 is not uniformly tight and hence converges to no probability measure. This follows as, for each r > 0,\nQm(\u2016X\u20162 \u2264 r) \u2264 (r + 4r/\u2206m)\nd\nm \u2264 5\ndrd\nm \u2264 1 5\nfor m = d5d+1rde, since at most (r + 4r/\u2206m)d points with minimum pairwise Euclidean distance greater than \u2206m can fit into a ball of radius r (Wainwright, 2017, Lems. 5.1 and 5.2)."}, {"heading": "G. Proof of Theorem 7: KSD detects tight non-convergence", "text": "For any probability measure \u00b5 on Rd and > 0, we define its tightness rate as\nR(\u00b5, ) , inf{r \u2265 0 |\u00b5(\u2016X\u20162 > r) \u2264 }. (10)\nTheorem 7 will follow from the following result which upper bounds the bounded Lipschitz metric dBL\u2016\u00b7\u20162 (\u00b5, P ) in terms of the tightness rate R(\u00b5, ), the rate of decay of the generalized Fourier transform \u03a6\u0302, and the KSD S(\u00b5, TP ,Gk). Theorem 13 (KSD tightness lower bound). Suppose P \u2208 P and let \u00b5 be a probability measure with tightness rate R(\u00b5, ) defined in (10). Moreover, suppose the kernel k(x, y) = \u03a6(x\u2212 y) with \u03a6 \u2208 C2 and F (t) , sup\u2016\u03c9\u2016\u221e\u2264t \u03a6\u0302(\u03c9)\n\u22121 finite for all t > 0. Then there exists a constantMP such that, for all , \u03b4 > 0,\ndBL\u2016\u00b7\u20162 (\u00b5, P ) \u2264 + min( , 1)(1 + + \u03b4\u22121d\u03b8d\u22121 \u03b8d MP )\n+ (2\u03c0)\u2212d/4V 1/2 d MP (R(\u00b5, ) + 2\u03b4)\nd/2F (\n12d log 2 \u03c0 (1 +M1(b)MP ) \u22121 )1/2 S(\u00b5, TP ,Gk),\nwhere \u03b8d , d \u222b 1 0 exp ( \u22121/(1\u2212 r2) ) rd\u22121 dr for d > 0 (and \u03b80 , e\u22121), and Vd is the volume of the unit Euclidean ball in dimension d.\nRemarks An explicit value for the Stein factor MP can be derived from the proof in Section G.1 and the results of Gorham et al. (2016). When bounds on R and F are known, the final expression can be optimized over and \u03b4 to produce rates of convergence in dBL\u2016\u00b7\u20162 .\nConsider now a sequence of probability measures (\u00b5m)m\u22651 that is uniformly tight. This implies that lim supmR(\u00b5m, ) < \u221e for all > 0. Moreover, since \u03a6\u0302 is non-vanishing, F (t) is finite for all t. Thus if S(\u00b5m, TP ,Gk)\u2192 0, then for any fixed < 1, lim supm dBL\u2016\u00b7\u20162 (\u00b5m, P ) \u2264 (2 + + \u03b4\u22121d\u03b8d\u22121 \u03b8d MP ). Taking \u2192 0 yields dBL\u2016\u00b7\u20162 (\u00b5m, P )\u2192 0.\nG.1. Proof of Theorem 13: KSD tightness lower bound\nFix any h \u2208 BL\u2016\u00b7\u20162 . By Theorem 5 and Section 4.2 of (Gorham et al., 2016), there exists a g \u2208 C 1 which solves the Stein equation TP g = h \u2212 E[h(Z)] and satisfies M0(g) \u2264 MP forMP a constant independent of h and g. To show that we can approximate TP g arbitrarily well by a function in a scaled copy of TPGk, we will form a truncated version of g using a smoothed indicator function described in the next lemma.\nLemma 14 (Smoothed indicator function). For any compact set K \u2282 Rd and \u03b4 > 0, define the set inflation K2\u03b4 , {x \u2208 Rd | \u2016x\u2212 y\u20162 \u2264 2\u03b4, \u2200y \u2208 K}. There is a function vK,\u03b4 : Rd \u2192 [0, 1] such that\nvK,\u03b4(x) = 1 for all x \u2208 K and vK,\u03b4(x) = 0 for all x /\u2208 K2\u03b4, (11) \u2016\u2207vK,\u03b4(x)\u20162 \u2264 \u03b4\u22121d\u03b8d\u22121 \u03b8d I [ x \u2208 K2\u03b4 \\K ] , (12)\nwhere \u03b8d , d \u222b 1 0 exp ( \u22121/(1\u2212 r2) ) rd\u22121 dr for d > 0 and \u03b80 , e\u22121.\nThis lemma is proved in Section G.2.\nFix any , \u03b4 > 0, and let K = B(0, R(\u00b5, )) with R(\u00b5, ) defined in (10). This set is compact since our sequence is uniformly tight. Hence, we may define gK,\u03b4(x) , g(x) vK,\u03b4(x) as a smooth, truncated version of g based on Lemma 14. Since\n(TP g)(x)\u2212 (TP gK,\u03b4)(x) = (1\u2212 vK,\u03b4(x))[\u3008b(x), g(x)\u3009+ \u3008\u2207, g\u3009(x)] + \u3008\u2207vK,\u03b4(x), g(x)\u3009 = (1\u2212 vK,\u03b4(x))(TP g)(x) + \u3008\u2207vK,\u03b4(x), g(x)\u3009,\nproperties (11) and (12) imply that (TP g)(x) = (TP gK,\u03b4)(x) for all x \u2208 K, (TP gK,\u03b4)(x) = 0 when x /\u2208 K2\u03b4 , and\n|(TP g)(x)\u2212 (TP gK,\u03b4)(x)| \u2264 |(TP g)(x)|+ \u2016\u2207vK,\u03b4(x)\u20162 \u2016g(x)\u20162 \u2264 |(TP g)(x)|+ \u03b4 \u22121d\u03b8d\u22121 \u03b8d \u2016g(x)\u20162 \u2264 1 + \u03b4\u22121d\u03b8d\u22121 \u03b8d MP\nfor x \u2208 K2\u03b4 \\K by Cauchy-Schwarz.\nMoreover, since vK,\u03b4 has compact support and is in C1 by (11), gK,\u03b4 \u2208 C1 with \u2016gK,\u03b4\u2016L2 \u2264 Vol(K 2\u03b4)1/2M0(g) \u2264 Vol(K2\u03b4)1/2MP . Therefore, Lemma 12 implies that there is a function g \u2208 Kdk such that |(TP g )(x)\u2212 (TP gK,\u03b4)(x)| \u2264 for all x with norm\n\u2016g \u2016Kdk \u2264 (2\u03c0) \u2212d/4F ( 12d log 2\u03c0 (1 +M1(b)MP \u22121))1/2Vol(K2\u03b4)1/2MP . (13)\nUsing the fact that TP gK,\u03b4 and TP g are identical on K, we have |(TP g )(x) \u2212 (TP g)(x)| \u2264 for all x \u2208 K. Moreover, when x /\u2208 K, the triangle inequality gives\n|(TP g )(x)\u2212 (TP g)(x)| \u2264 |(TP g )(x)\u2212 (TP gK,\u03b4)(x)|+ |(TP gK,\u03b4)(x)\u2212 (TP g)(x)| \u2264 1 + + \u03b4 \u22121d\u03b8d\u22121 \u03b8d MP .\nBy the triangle inequality and the fact that our choice of K ensures \u00b5(I[X /\u2208 K]) \u2264 min( , 1), we have\n|E\u00b5[h(X)]\u2212 EP [h(Z)]| = |E\u00b5[(TP g)(X)]| \u2264 |E[(TP g)(X)\u2212 (TP g )(X)]|+ |E\u00b5[(TP g )(X)]| \u2264 |E\u00b5[((TP g)(X)\u2212 (TP g )(X))I[X \u2208 K]]|+ |E\u00b5[((TP g)(X)\u2212 (TP g )(X))I[X /\u2208 K]]|+ |E\u00b5[(TP g )(X)]|\n\u2264 + min( , 1)(1 + + \u03b4 \u22121d\u03b8d\u22121 \u03b8d MP ) + \u2016g \u2016Kdk S(\u00b5m, TP ,Gk) \u2264 + min( , 1)(1 + + \u03b4 \u22121d\u03b8d\u22121 \u03b8d MP )\n+ (2\u03c0)\u2212d/4Vol(B(0, R(\u00b5, ) + 2\u03b4))1/2F (\n12d log 2 \u03c0 (1 +M1(b)MP ) \u22121 )1/2 MPS(\u00b5, TP ,Gk).\nThe advertised result follows by substituting Vol(B(0, r)) = Vdrd and taking the supremum over all h \u2208 BL\u2016\u00b7\u2016.\nG.2. Proof of Lemma 14: Smoothed indicator function\nFor all x \u2208 Rd, define the standard normalized bump function \u03c8 \u2208 C\u221e as \u03c8(x) , I\u22121d exp ( \u22121/(1\u2212 \u2016x\u201622) ) I[\u2016x\u20162 < 1],\nwhere the normalizing constant is given by\nId = \u222b B(0,1) exp ( \u22121/(1\u2212 \u2016x\u201622) ) dx = \u03b8d Vd\nfor Vd being the volume of the unit Euclidean ball in d dimensions (Baker, 1999). LettingW be a random variable with density \u03c8, define vK,\u03b4(x) , E [ I [ x+ \u03b4W \u2208 K\u03b4 ]] as the smoothed approximation of x 7\u2192 I[x \u2208 K], where \u03b4 > 0 controls the amount of smoothing. Since supp(W ) = B(0, 1), we can immediately conclude (11) and also supp(\u2207vK,\u03b4) \u2286 K2\u03b4 \\K.\nThus to prove (12), it remains to consider x \u2208 K2\u03b4 \\K. We see \u2207vK,\u03b4(x) = \u03b4\u2212d\u22121 \u222b B(x,\u03b4)\u2207\u03c8( x\u2212y \u03b4 )I [ y \u2208 K\u03b4 ] dy by Leibniz rule. Letting K\u03b4x , \u03b4 \u22121(K\u03b4 \u2212 x), then by Jensen\u2019s inequality we have\n\u2016\u2207vK,\u03b4(x)\u20162 \u2264 \u03b4 \u2212d\u22121 \u222b B(x,\u03b4)\u2229K\u03b4 \u2225\u2225\u2225\u2225\u2207\u03c8(x\u2212 y\u03b4 )\u2225\u2225\u2225\u2225\n2 dy = \u03b4\u22121 \u222b B(0,1)\u2229K\u03b4x \u2016\u2207\u03c8(z)\u20162 dz \u2264 \u03b4 \u22121 \u222b B(0,1) \u2016\u2207\u03c8(z)\u20162 dz\nwhere we used the substitution z , (x\u2212 y)/\u03b4. By differentiating \u03c8, using (Baker, 1999) with the substitution r = \u2016z\u20162, and employing integration by parts we have\u222b\nB(0,1) \u2016\u2207\u03c8(z)\u20162 dz = I \u22121 d \u222b 1 0\n2r\n(1\u2212 r2)2 exp\n( \u22121\n1\u2212 r2\n) (dVdr d\u22121) dr\n= d\n\u03b8d\n[ \u2212rd\u22121 exp ( \u22121\n1\u2212 r2 )\u2223\u2223\u2223\u2223r=1 r=0 + \u222b 1 0 (d\u2212 1)rd\u22122 exp ( \u22121 1\u2212 r2 ) dr ]\n= d\n\u03b8d [e\u22121I[d = 1] + I[d 6= 1]\u03b8d\u22121] = d\u03b8d\u22121 \u03b8d\nyielding (12)."}, {"heading": "H. Proof of Theorem 8: IMQ KSD detects non-convergence", "text": "We first use the following theorem to upper bound the bounded Lipschitz metric dBL\u2016\u00b7\u2016(\u00b5, P ) in terms of the KSD S(\u00b5, TP ,Gk).\nTheorem 15 (IMQ KSD lower bound). Suppose P \u2208 P and k(x, y) = (c2 + \u2016x\u201622)\u03b2 for c > 0, and \u03b2 \u2208 (\u22121, 0). Choose any \u03b1 \u2208 (0, 12 (\u03b2 + 1)) and a > 1 2c. Then there exist an 0 > 0 and a constantMP such that, for all \u00b5,\ndBL\u2016\u00b7\u2016(\u00b5, P ) \u2264 inf \u2208[0, 0),\u03b4>0\n( 2 + + \u03b4\n\u22121d\u03b8d\u22121 \u03b8d MP ) + (2\u03c0)\u2212d/4MPV 1/2d \u00d7[(\nD(a,c,\u03b1,\u03b2)1/2(S(\u00b5,TP ,Gk)\u2212\u03b6(a,c,\u03b1,\u03b2)) \u03b1\u03ba0\n)1/\u03b1 + 2\u03b4 ]d/2\u221a FIMQ( 12d log 2 \u03c0 (1 +M1(b)MP ) \u22121)S(\u00b5, TP ,Gk) (14)\n= O ( 1/ log (\n1 S(\u00b5,TP ,Gk)\n)) as S(\u00b5, TP ,Gk)\u2192 0, (15)\nwhere \u03b8d , d \u222b 1 0 exp ( \u22121/(1\u2212 r2) ) rd\u22121 dr for d > 0 and \u03b80 , e\u22121, Vd is the volume of the Euclidean unit ball in d-dimensions, the function D is defined in (20), the function \u03b6 is defined in (17), and finally\nFIMQ(t) , \u0393(\u2212\u03b2) 21+\u03b2 (\u221a d c )\u03b2+d/2 t\u03b2+d/2\nK\u03b2+d/2(c \u221a dt)\n(16)\nwhere Kv is the modified Bessel function of the third kind. Moreover, if lim supm S(\u00b5m, TP ,Gk) < \u221e then (\u00b5m)m\u22651 is uniformly tight.\nRemark The Stein factorMP can be determined explicitly based on the proof of Theorem 15 in Section H.1 and the results of Gorham et al. (2016).\nNote that FIMQ(t) is finite for all t > 0, so fix any \u2208 [0, 0) and \u03b4 > 0. If S(\u00b5m, TP ,Gk) \u2192 0, then lim supm dBL\u2016\u00b7\u2016(\u00b5m, P ) \u2264 (2 + + \u03b4\u22121d\u03b8d\u22121 \u03b8d\nMP ) . Thus taking \u2192 0 yields dBL\u2016\u00b7\u2016(\u00b5m, P ) \u2192 0. Since dBL\u2016\u00b7\u2016(\u00b5m, P )\u2192 0 only if \u00b5m \u21d2 P , the statement of Theorem 8 follows.\nH.1. Proof of Theorem 15: IMQ KSD lower bound\nFix any \u03b1 \u2208 (0, 12 (\u03b2 + 1)) and a > 1 2c. Then there is some g\u030a \u2208 Gk such that TP g\u030a is bounded below by a constant \u03b6(a, c, \u03b1, \u03b2) and has a growth rate of \u2016x\u20162\u03b12 as \u2016x\u20162 \u2192 \u221e. Such a function exists by the following lemma, proved in Section H.2.\nLemma 16 (Generalized multiquadric Stein sets yield coercive functions). Suppose P \u2208 P and k(x, y) = \u03a6c,\u03b2(x \u2212 y) for \u03a6c,\u03b2(x) , (c2 + \u2016x\u201622)\u03b2 , c > 0, and \u03b2 \u2208 R \\N0. Then, for any \u03b1 \u2208 (0, 1 2 (\u03b2 + 1)) and a > 1 2c, there exists a function g\u030a \u2208 Gk such that TP g\u030a is bounded below by\n\u03b6(a, c, \u03b1, \u03b2) , \u2212D(a, c, \u03b1, \u03b2) 1/2\n2\u03b1\n[ M1(b)R\n2 0 + \u2016b(0)\u20162R0 + d a2(1\u2212\u03b1)\n] , (17)\nwhere the function D is defined in (20) and R0 , inf{r > 0 |\u03ba(r\u2032) \u2265 0,\u2200r\u2032 \u2265 r}. Moreover, lim inf\u2016x\u2016\u22122\u03b12 (TP g\u030a)(x) \u2265 \u03b1 D(a,c,\u03b1,\u03b2)1/2\u03ba0 as \u2016x\u20162 \u2192\u221e.\nOur next lemma connects the growth rate of TP g\u030a to the tightness rate of a probability measure evaluated with the Stein discrepancy. Its proof is found in Section H.3.\nLemma 17 (Coercive functions yield tightness). Suppose there is a g \u2208 G such that TP g is bounded below by \u03b6 \u2208 R and lim inf\u2016x\u20162\u2192\u221e\u2016x\u2016 \u2212u 2 (TP g)(x) > \u03b7 for some \u03b7, u > 0. Then for all sufficiently small and any probability measure \u00b5 the tightness rate (10) satisfies\nR(\u00b5, ) \u2264 [ 1\n\u03b7 (S(\u00b5, TP ,G)\u2212 \u03b6)\n]1/u .\nIn particular, if lim supm S(\u00b5m, TP ,Gk) is finite, (\u00b5m)m\u22651 is uniformly tight.\nWe can thus plug the tightness rate estimate of Lemma 17 applied to the function g\u030a into Theorem 13. Since \u2016w\u2016\u221e \u2264 t implies \u2016w\u20162 \u2264 \u221a dt, we can use the formula for the generalized Fourier transform of the IMQ kernel in (18) to see \u03a6\u0302(\u03c9) is monotonically decreasing in \u2016w\u20162 to establish (16). By taking \u03b7 \u2192 \u03b1 D(a,c,\u03b1,\u03b2)1/2\u03ba0 we obtain (14). To prove (15), notice that FIMQ(t) = O(e(c \u221a d+\u03bb)t) as t \u2192 \u221e for any \u03bb > 0 by (19). Hence, by choosing = c \u221a d/ log( 1S(\u00b5,TP ,Gk) ) and \u03b4 = 1/\n1/\u03b1 we obtain the advertised decay rate as S(\u00b5, TP ,Gk) \u2192 0. The uniform tightness conclusion follows from Lemma 17.\nH.2. Proof of Lemma 16: Generalized multiquadric Stein sets yield coercive functions\nBy (Wendland, 2004, Thm. 8.15), \u03a6c,\u03b2 has a generalized Fourier transform of order max(0, d\u03b2e) given by\n\u03a6\u0302c,\u03b2(\u03c9) = 21+\u03b2\n\u0393(\u2212\u03b2) ( \u2016\u03c9\u20162 c )\u2212\u03b2\u2212d/2 K\u03b2+d/2(c\u2016\u03c9\u20162), (18)\nwhere Kv(z) is the modified Bessel function of the third kind. Furthermore, by (Wendland, 2004, Cor. 5.12, Lem. 5.13, Lem. 5.14), we have the following bounds on Kv(z) for v \u2208 R, z \u2208 (0,\u221e):\nKv(z) \u2265 \u03c4v e\u2212z\u221a z\nfor z \u2265 1 where \u03c4v = \u221a \u03c0\n2 for |v| \u2265 1 2 and \u03c4v =\n\u221a \u03c03|v|\u22121/2\n2|v|+1\u0393(|v|+ 1/2) for |v| < 1 2 , (19)\nKv(z) \u2265 e\u22121\u03c4vz\u2212|v| for z \u2264 1, (since x 7\u2192 xvK\u2212v(x) is non-increasing and Kv = K\u2212v) Kv(z) \u2264 \u221a 2\u03c0\nz e\u2212z+v 2/(2z) for z > 0,\nKv(z) \u2264 2|v|\u22121\u0393(|v|)z\u2212|v| for v 6= 0, z > 0.\nNow fix any a > c/2 and \u03b1 \u2208 (0, 12 (\u03b2 + 1)), and consider the functions gj(x) = \u2207xj\u03a6a,\u03b1(x) = 2\u03b1xj(a 2 + \u2016x\u201622)\u03b1\u22121. We will show that g = (g1, . . . , gd) \u2208 Kdk. Note that g\u0302j(\u03c9) = (i\u03c9j)\u03a6\u0302a,\u03b1(\u03c9). Using (Wendland, 2004, Thm. 10.21), we\nknow \u2016gj\u2016Kk = \u2225\u2225\u2225\u2225g\u0302j/\u221a\u03a6\u0302c,\u03b2\u2225\u2225\u2225\u2225\nL2 , and thus \u2016g\u2016Kdk = \u2225\u2225\u2225\u2225g\u0302/\u221a\u03a6\u0302c,\u03b2\u2225\u2225\u2225\u2225 L2 . Hence\n\u2016g\u20162Kdk = d\u2211 j=1 \u222b Rd g\u0302j(\u03c9)g\u0302j(\u03c9)/\u03a6\u0302c,\u03b2(\u03c9) d\u03c9\n= d\u2211 j=1 \u222b Rd 22(1+\u03b1)/\u0393(\u2212\u03b1)2 21+\u03b2/\u0393(\u2212\u03b2) a2\u03b1+d c\u03b2+d/2 \u03c92j \u2016\u03c9\u2016 \u03b2\u22122\u03b1\u2212d/2 2 K\u03b1+d/2(a\u2016\u03c9\u20162)2 K\u03b2+d/2(c\u2016\u03c9\u20162) d\u03c9\n= c0 \u222b Rd \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+22 K\u03b1+d/2(a\u2016\u03c9\u20162)2 K\u03b2+d/2(c\u2016\u03c9\u20162) d\u03c9,\nwhere c0 = 22(1+\u03b1)/\u0393(\u2212\u03b1)2 21+\u03b2/\u0393(\u2212\u03b2) a2\u03b1+d c\u03b2+d/2 . We can split the integral above into two, with the first integrating over B(0, 1) and the second integrating over B(0, 1)c = Rd \\ B(0, 1). Thus using the inequalities from (19) with v0 , \u03b2 + d/2, we have\u222b B(0,1) \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+22 K\u03b1+d/2(a\u2016\u03c9\u20162)2 K\u03b2+d/2(c\u2016\u03c9\u20162) d\u03c9 \u2264 \u222b B(0,1) \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+22 22\u03b1+d\u22122\u0393(\u03b1+ d/2)2(a\u2016w\u20162)\u22122\u03b1\u2212d e\u22121\u03c4v0 \u00b7 \u2016c\u03c9\u2016 \u2212\u03b2\u2212d/2 2 d\u03c9\n= 22\u03b1+d\u22122\u0393(\u03b1+ d/2)2 e\n\u03c4v0\nc\u03b2+d/2\na2\u03b1+d \u222b B(0,1) \u2016\u03c9\u20162\u03b2\u22124\u03b1\u2212d+22 e c\u2016\u03c9\u20162 d\u03c9\n= d Vd 2 2\u03b1+d\u22122\u0393(\u03b1+ d/2)2\ne\n\u03c4v0\nc\u03b2+d/2\na2\u03b1+d \u222b 1 0 r2\u03b2\u22124\u03b1+1ecr dr,\nwhere Vd is the volume of the unit ball in d-dimensions and in the last step we used the substitution r = \u2016\u03c9\u20162 (Baker, 1999). Since \u03b1 < 12 (\u03b2 + 1) and the function r 7\u2192 r\nt is integrable around the origin when t > \u22121, we can bound the integral above by \u222b 1\n0\nr2\u03b2\u22124\u03b1+1ecr dr \u2264 ec \u222b 1\n0\nr2\u03b2\u22124\u03b1+1 dr = ec\n2\u03b2 \u2212 4\u03b1+ 2 .\nWe can apply the technique to the other integral, yielding\u222b B(0,1)c \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+22 K\u03b1+d/2(a\u2016\u03c9\u20162)2 K\u03b2+d/2(c\u2016\u03c9\u20162) d\u03c9 \u2264 \u222b B(0,1)c \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+22 2\u03c0/(a\u2016\u03c9\u20162) \u00b7 e\u22122a\u2016\u03c9\u20162+(\u03b1+d/2) 2/(a\u2016\u03c9\u20162) \u03c4v0e \u2212c\u2016\u03c9\u20162/ \u221a c\u2016\u03c9\u20162 d\u03c9\n\u2264 2\u03c0 \u221a c\na\u03c4v0 \u222b B(0,1)c \u2016\u03c9\u2016\u03b2\u22122\u03b1\u2212d/2+3/22 e (c\u22122a)\u2016\u03c9\u20162+(\u03b1+d/2) 2/(a\u2016\u03c9\u20162) d\u03c9\n= d Vd 2\u03c0 \u221a c\na\u03c4v0 \u222b \u221e 1 r\u03b2\u22122\u03b1+d/2+1/2e(c\u22122a)r+(\u03b1+d/2) 2/(ar) dr\nSince c\u2212 2a < 0, we can upper bound the last integral above by the quantity\u222b \u221e 1 r\u03b2\u22122\u03b1+d/2+1/2e(c\u22122a)r+(\u03b1+d/2) 2/(ar) dr\n\u2264 e(c\u22122a)+(\u03b1+d/2) 2/a \u222b \u221e 1 r\u03b2\u22122\u03b1+d/2+1/2e(c\u22122a)r dr = e(c\u22122a)+(\u03b1+d/2) 2/a(2a\u2212 c)\u2212\u03b2+2\u03b1\u2212d/2\u22123/2\u0393(\u03b2 \u2212 2\u03b1+ d/2 + 3/2, 2a\u2212 c),\nwhere \u0393(s, x) , \u222b\u221e x ts\u22121e\u2212t dt is the upper incomplete gamma function. Hence, the function g belongs to Kdk with norm upper bounded by D(a, b, \u03b1, \u03b2)1/2 where\nD(a, c, \u03b1, \u03b2) , d Vd 21+2\u03b1\u2212\u03b2 a2\u03b1+d\u0393(\u2212\u03b2) c\u03b2+d/2\u0393(\u2212\u03b1)2\n( 22\u03b1+d\u22122\u0393(\u03b1+ d/2)2ec+1c\u03b2+d/2\n\u03c4v0(2\u03b2 \u2212 4\u03b1+ 2)a2\u03b1+d +\n2\u03c0 \u221a c\na\u03c4v0 e(c\u22122a)+(\u03b1+d/2) 2/a(2a\u2212 c)\u2212\u03b2+2\u03b1\u2212d/2\u22123/2\u0393(\u03b2 \u2212 2\u03b1+ d/2 + 3/2, 2a\u2212 c)\n) . (20)\nNow define g\u030a = \u2212D(a, c, \u03b1, \u03b2)\u22121/2g so that g\u030a \u2208 Gk. We will lower bound the growth rate of TP g\u030a and also construct a uniform lower bound. Note\nD(a, c, \u03b1, \u03b2)1/2\n2\u03b1 (TP g\u030a)(x) = \u2212 \u3008b(x), x\u3009 (a2 + \u2016x\u201622)1\u2212\u03b1 \u2212 d (a2 + \u2016x\u201622)1\u2212\u03b1 + 2(1\u2212 \u03b1)\u2016x\u201622 (a2 + \u2016x\u201622)2\u2212\u03b1 . (21)\nThe latter two terms are both uniformly bounded in x. By the distant dissipativity assumption, there is some \u03ba > 0 such that lim sup\u2016x\u20162\u2192\u221e 1 \u2016x\u201622 \u3008b(x), x\u3009 \u2264 \u2212 12\u03ba. Thus the first term of (21) grows at least at the rate 1 2\u03ba\u2016x\u2016 2\u03b1 2 . This assures lim inf\u2016x\u2016\u22122\u03b12 (TP g\u030a)(x) \u2265 \u03b1 D(a,c,\u03b1,\u03b2)1/2\u03ba as \u2016x\u20162 \u2192\u221e.\nMoreover, because b is Lipschitz, we have\n|\u3008b(x), x\u3009| \u2264 |\u3008b(x)\u2212 b(0), x\u2212 0\u3009|+ |\u3008b(0), x\u3009| \u2264M1(b)\u2016x\u201622 + \u2016b(0)\u2016\u2016x\u20162,\nHence for any x \u2208 B(0, R0), we must have\u2212\u3008b(x), x\u3009 \u2265 \u2212M1(b)R20\u2212\u2016b(0)\u20162R0. By choice ofR0, for all x /\u2208 B(0, R0), the distant dissipativity assumption implies\u2212\u3008b(x), x\u3009 \u2265 0. Hence applying this to (21) shows that TP g\u030a is uniformly lower bounded by \u03b6(a, c, \u03b1, \u03b2).\nH.3. Proof of Lemma 17: Coercive functions yield tightness\nPick g \u2208 Gk such that lim inf\u2016x\u20162\u2192\u221e\u2016x\u2016 \u2212u 2 (TP g)(x) > \u03b7 and infx\u2208Rd(TP g)(x) \u2265 \u03b6. Let us define \u03b3(r) , inf{(TP g)(x) \u2212 \u03b6 | \u2016x\u20162 \u2265 r} \u2265 0 for all r > 0. Thus for sufficiently large r, we have \u03b3(r) \u2265 \u03b7ru. Then, for any measure \u00b5 by Markov\u2019s inequality,\n\u00b5(\u2016X\u20162 \u2265 r) \u2264 E\u00b5[\u03b3(\u2016X\u20162)] \u03b3(r) \u2264 E\u00b5[(TP g)(X)\u2212 \u03b6] \u03b3(r) .\nThus we see that \u00b5(\u2016X\u20162 \u2265 r ) \u2264 whenever \u2265 (S(\u00b5, TP ,Gk)\u2212 \u03b6)/\u03b3(r ). This implies that for sufficiently small , if r \u2265 [ 1\n\u03b7 (S(\u00b5, TP ,Gk)\u2212 \u03b6)\n]1/u ,\nwe must have \u00b5(\u2016X\u20162 \u2265 r ) \u2264 . Hence whenever lim supm S(\u00b5m, TP ,Gk) is bounded, we must have (\u00b5m)m\u22651 is uniformly tight as lim supmR(\u00b5m, ) is finite."}, {"heading": "I. Proof of Proposition 9: KSD detects convergence", "text": "We will first state and prove a useful lemma. Lemma 18 (Stein output upper bound). Let Z \u223c P and X \u223c \u00b5. If the score function b = \u2207 log p is Lipschitz with EP [\u2016b(Z)\u201622] <\u221e, then, for any g : Rd \u2192 Rd with max(M0(g),M1(g),M2(g)) <\u221e,\n|E\u00b5[(TP g)(X)]| \u2264 (M0(g)M1(b) +M2(g)d)dW\u2016\u00b7\u20162 (\u00b5, P ) + \u221a 2M0(g)M1(g)EP [\u2016b(Z)\u201622] dW\u2016\u00b7\u20162 (\u00b5, P ),\nwhere the Wasserstein distance dW\u2016\u00b7\u20162 (\u00b5, P ) = infX\u223c\u00b5,Z\u223cP E[\u2016X \u2212 Z\u20162].\nProof By Jensen\u2019s inequality, we have EP [\u2016b(Z)\u20162] \u2264 \u221a EP [\u2016b(Z)\u201622] < \u221e, which implies that EP [(TP g)(Z)] = 0 (Gorham & Mackey, 2015, Prop. 1). Thus, using the triangle inequality, Jensen\u2019s inequality, and the Fenchel-Young inequality for dual norms,\n|E\u00b5[(TP g)(X)]| = |E[(TP g)(Z)\u2212 (TP g)(X)]| = |E[\u3008b(Z), g(Z)\u2212 g(X)\u3009+ \u3008b(Z)\u2212 b(X), g(X)\u3009+ \u3008I,\u2207g(Z)\u2212\u2207g(X)\u3009]| \u2264 E[|\u3008b(Z), g(Z)\u2212 g(X)\u3009|] + (M0(g)M1(b) +M2(g)d)E[\u2016X \u2212 Z\u20162],\nTo handle the other term above, notice that by Cauchy-Schwarz and the fact that min(a, b) \u2264 \u221a ab for a, b \u2265 0,\nE[|\u3008b(Z), g(Z)\u2212 g(X)\u3009|] \u2264 E[min(2M0(g),M1(g)\u2016X \u2212 Z\u20162)\u2016b(Z)\u20162] \u2264 (2M0(g)M1(g))1/2E [ \u2016X \u2212 Z\u20161/22 \u2016b(Z)\u20162 ] \u2264 \u221a\n2M0(g)M1(g)E[\u2016X \u2212 Z\u20162]EP [\u2016b(Z)\u2016 2 2].\nThe stated inequality now follows by taking the infimum of these bounds over all joint distributions (X,Z) with X \u223c \u00b5 and Z \u223c P .\nNow we are ready to prove Proposition 9. In the statement below, let us use \u03b1 \u2208 Nd as a multi-index for the differentiation operator D\u03b1, that is, for a differentiable function f : Rd \u2192 R we have for all x \u2208 Rd,\nD\u03b1f(x) , d|\u03b1|\n(dx1)\u03b11 . . . (dxd)\u03b1d f(x)\nwhere |\u03b1| = \u2211d j=1 \u03b1j . Pick any g \u2208 Gk, and choose any multi-index \u03b1 \u2208 Nd such that |\u03b1| \u2264 2. Then by Cauchy-Schwarz and (Steinwart & Christmann, 2008, Lem. 4.34), we have\nsup x\u2208Rd |D\u03b1gj(x)| = sup x\u2208Rd |D\u03b1\u3008gj , k(x, \u00b7)\u3009Kk | \u2264 sup x\u2208Rd \u2016gj\u2016Kk \u2016D \u03b1k(x, \u00b7)\u2016Kk = \u2016gj\u2016Kk sup x\u2208Rd (D\u03b1xD \u03b1 y k(x, x)) 1/2.\nSince \u2211d j=1\u2016gj\u2016 2 Kk \u2264 1 for all g \u2208 Gk and D \u03b1 xD \u03b1 y k(x, x) is uniformly bounded in x for all |\u03b1| \u2264 2, the elements of the vector g(x), matrix \u2207g(x), and tensor \u22072g(x) are uniformly bounded in x \u2208 Rd and g \u2208 Gk. Hence, for some \u03bbk, supg\u2208Gk max(M0(g),M1(g),M2(g)) \u2264 \u03bbk <\u221e, so the advertised result follows from Lemma 18 as\nS(\u00b5, TP ,Gk) \u2264 \u03bbk ( (M1(b) + d)dW\u2016\u00b7\u20162 (\u00b5, P ) + \u221a 2EP [\u2016b(Z)\u201622] dW\u2016\u00b7\u20162 (\u00b5, P ) ) ."}, {"heading": "J. Proof of Theorem 10: KSD fails for bounded scores", "text": "Fix some n \u2265 1, and let Qn = 1n \u2211n i=1 \u03b4xi where xi , ine1 \u2208 Rd for i \u2208 {1, . . . , n}. This implies \u2016xi \u2212 xi\u2032\u20162 \u2265 n for all i 6= i\u2032. We will show that when M0(b) is finite, S(Qn, TP ,Gk)\u2192 0 as n\u2192\u221e.\nWe can express k0(x, y) , \u2211d j=1 k j 0(x, y) as\nk0(x, y) = \u3008b(x), b(y)\u3009k(x, y) + \u3008b(x),\u2207yk(x, y)\u3009+ \u3008b(y),\u2207xk(x, y)\u3009+ \u3008\u2207x,\u2207yk(x, y)\u3009.\nFrom Proposition 2, we have\nS(Qn, TP ,Gk)2 = 1\nn2 n\u2211 i,i\u2032=1 k0(xi, xi\u2032) = 1 n2 n\u2211 i=1 k0(xi, xi) + 1 n2 \u2211 i 6=i\u2032 k0(xi, xi\u2032). (22)\nLet \u03b3 be the kernel decay rate defined in the statement of Theorem 6. Then as k \u2208 C(1,1)0 , we must have \u03b3(0) < \u221e and limr\u2192\u221e \u03b3(r) = 0. By the triangle inequality\nlim n\u2192\u221e \u2223\u2223\u2223\u2223\u2223 1n2 n\u2211 i=1 k0(xi, xi) \u2223\u2223\u2223\u2223\u2223 \u2264 limn\u2192\u221e 1n2 n\u2211 i=1 |k0(xi, xi)| \u2264 lim n\u2192\u221e \u03b3(0) n (M0(b) + 1) 2 = 0.\nWe now handle the second term of (22). By repeated use of Cauchy-Schwarz we have\n|k0(xi, xi\u2032)| \u2264 |\u3008b(xi), b(xi\u2032)\u3009k(xi, xi\u2032)|+ |\u3008b(xi),\u2207yk(xi, xi\u2032)\u3009|+ |\u3008b(xi\u2032),\u2207xk(xi, xi\u2032)\u3009|+ |\u3008\u2207x,\u2207yk(xi, xi\u2032)\u3009| \u2264 \u2016b(xi)\u20162\u2016b(xi\u2032)\u20162|k(xi, xi\u2032)|+ \u2016b(xi)\u20162\u2016\u2207yk(xi, xi\u2032)\u20162 + \u2016b(xi\u2032)\u20162\u2016\u2207xk(xi, xi\u2032)\u20162\n+ |\u3008\u2207x,\u2207yk(xi, xi\u2032)\u3009| \u2264 \u03b3(n)(M0(b) + 1)2.\nBy assumption, \u03b3(r) \u2192 0 as r \u2192 \u221e. Furthermore, since the second term of (22) is upper bounded by the average of the terms k0(xi, x\u2032i) for i 6= i\u2032, we have S(Qn, TP ,Gk) \u2192 0 as n \u2192 \u221e. However, (Qn)n\u22651 is not uniformly tight and hence does not converge to the probability measure P ."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In Proc. 29th ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Functional Analysis. Academic Press textbooks in mathematics", "author": ["G. Bachman", "L. Narici"], "venue": "Dover Publications,", "citeRegEx": "Bachman and Narici,? \\Q1966\\E", "shortCiteRegEx": "Bachman and Narici", "year": 1966}, {"title": "Integration of radial functions", "author": ["J. Baker"], "venue": "Mathematics Magazine,", "citeRegEx": "Baker,? \\Q1999\\E", "shortCiteRegEx": "Baker", "year": 1999}, {"title": "Stein\u2019s method and Poisson process convergence", "author": ["A.D. Barbour"], "venue": "J. Appl. Probab., (Special Vol. 25A):175\u2013184,", "citeRegEx": "Barbour,? \\Q1988\\E", "shortCiteRegEx": "Barbour", "year": 1988}, {"title": "Stein\u2019s method for diffusion approximations", "author": ["A.D. Barbour"], "venue": "Probab. Theory Related Fields,", "citeRegEx": "Barbour,? \\Q1990\\E", "shortCiteRegEx": "Barbour", "year": 1990}, {"title": "A consistent test for multivariate normality based on the empirical characteristic function", "author": ["L. Baringhaus", "N. Henze"], "venue": "Metrika,", "citeRegEx": "Baringhaus and Henze,? \\Q1988\\E", "shortCiteRegEx": "Baringhaus and Henze", "year": 1988}, {"title": "Julia: A fresh approach to numerical computing", "author": ["J. Bezanson", "A. Edelman", "S. Karpinski", "V.B. Shah"], "venue": "arXiv preprint arXiv:1411.1607,", "citeRegEx": "Bezanson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bezanson et al\\.", "year": 2014}, {"title": "Handbook of Markov chain Monte Carlo", "author": ["S. Brooks", "A. Gelman", "G. Jones", "Meng", "X.-L"], "venue": "CRC press,", "citeRegEx": "Brooks et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brooks et al\\.", "year": 2011}, {"title": "Vector valued reproducing kernel hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanit\u00e1"], "venue": "Analysis and Applications,", "citeRegEx": "Carmeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carmeli et al\\.", "year": 2010}, {"title": "Nonnormal approximation by Stein\u2019s method of exchangeable pairs with application to the Curie-Weiss model", "author": ["S. Chatterjee", "Q. Shao"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "Chatterjee and Shao,? \\Q2011\\E", "shortCiteRegEx": "Chatterjee and Shao", "year": 2011}, {"title": "Normal approximation by Stein\u2019s method. Probability and its Applications", "author": ["L. Chen", "L. Goldstein", "Q. Shao"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "A kernel test of goodness of fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "In Proc. 33rd ICML,", "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2016}, {"title": "Approximate slice sampling for Bayesian posterior inference", "author": ["C. DuBois", "A. Korattikara", "M. Welling", "P. Smyth"], "venue": "In Proc. 17th AISTATS,", "citeRegEx": "DuBois et al\\.,? \\Q2014\\E", "shortCiteRegEx": "DuBois et al\\.", "year": 2014}, {"title": "Reflection couplings and contraction rates for diffusions", "author": ["A. Eberle"], "venue": "Probab. Theory Related Fields, pp", "citeRegEx": "Eberle,? \\Q2015\\E", "shortCiteRegEx": "Eberle", "year": 2015}, {"title": "Output assessment for Monte Carlo simulations via the score statistic", "author": ["Y. Fan", "S.P. Brooks", "A. Gelman"], "venue": "J. Comp. Graph. Stat.,", "citeRegEx": "Fan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2006}, {"title": "Kernel measures of conditional dependence", "author": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C.J. Geyer"], "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface,", "citeRegEx": "Geyer,? \\Q1991\\E", "shortCiteRegEx": "Geyer", "year": 1991}, {"title": "Measuring sample quality with Stein\u2019s method", "author": ["J. Gorham", "L. Mackey"], "venue": "Adv. NIPS", "citeRegEx": "Gorham and Mackey,? \\Q2015\\E", "shortCiteRegEx": "Gorham and Mackey", "year": 2015}, {"title": "Measuring sample quality with diffusions", "author": ["J. Gorham", "A. Duncan", "S. Vollmer", "L. Mackey"], "venue": null, "citeRegEx": "Gorham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorham et al\\.", "year": 2016}, {"title": "On the rate of convergence in the multivariate CLT", "author": ["F. G\u00f6tze"], "venue": "Ann. Probab.,", "citeRegEx": "G\u00f6tze,? \\Q1991\\E", "shortCiteRegEx": "G\u00f6tze", "year": 1991}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "The Plancherel formula, the Plancherel theorem, and the Fourier transform of orbital integrals", "author": ["R. Herb", "P.J. Sally Jr."], "venue": "In Representation Theory and Mathematical Physics: Conference in Honor of Gregg Zuckerman\u2019s 60th Birthday,", "citeRegEx": "Herb and Jr.,? \\Q2009\\E", "shortCiteRegEx": "Herb and Jr.", "year": 2009}, {"title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget", "author": ["A. Korattikara", "Y. Chen", "M. Welling"], "venue": "In Proc. of 31st ICML,", "citeRegEx": "Korattikara et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Korattikara et al\\.", "year": 2014}, {"title": "Stein\u2019s method for comparison of univariate distributions", "author": ["C. Ley", "G. Reinert", "Y. Swan"], "venue": "Probab. Surveys,", "citeRegEx": "Ley et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ley et al\\.", "year": 2017}, {"title": "Two methods for wild variational inference", "author": ["Q. Liu", "Y. Feng"], "venue": "arXiv preprint arXiv:1612.00081,", "citeRegEx": "Liu and Feng,? \\Q2016\\E", "shortCiteRegEx": "Liu and Feng", "year": 2016}, {"title": "Variational Gradient Descent: A General Purpose", "author": ["Q. Liu", "Wang", "D. Stein"], "venue": "Bayesian Inference Algorithm", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A kernelized Stein discrepancy for goodness-of-fit tests", "author": ["Q. Liu", "J. Lee", "M. Jordan"], "venue": "In Proc. of 33rd ICML, volume 48 of ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multivariate Stein factors for a class of strongly log-concave distributions", "author": ["L. Mackey", "J. Gorham"], "venue": "Electron. Commun. Probab.,", "citeRegEx": "Mackey and Gorham,? \\Q2016\\E", "shortCiteRegEx": "Mackey and Gorham", "year": 2016}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["A. M\u00fcller"], "venue": "Ann. Appl. Probab.,", "citeRegEx": "M\u00fcller,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller", "year": 1997}, {"title": "Control functionals for QuasiMonte Carlo integration", "author": ["C. Oates", "M. Girolami"], "venue": null, "citeRegEx": "Oates and Girolami,? \\Q2015\\E", "shortCiteRegEx": "Oates and Girolami", "year": 2015}, {"title": "Convergence rates for a class of estimators based on steins method", "author": ["C. Oates", "J. Cockayne", "F. Briol", "M. Girolami"], "venue": "arXiv preprint arXiv:1603.03220,", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Control functionals for Monte Carlo integration", "author": ["C.J. Oates", "M. Girolami", "N. Chopin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. n/a\u2013n/a,", "citeRegEx": "Oates et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oates et al\\.", "year": 2016}, {"title": "Operator variational inference", "author": ["R. Ranganath", "D. Tran", "J. Altosaar", "D. Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions", "author": ["C. Simon-Gabriel", "B. Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1604.05251,", "citeRegEx": "Simon.Gabriel and Sch\u00f6lkopf,? \\Q2016\\E", "shortCiteRegEx": "Simon.Gabriel and Sch\u00f6lkopf", "year": 2016}, {"title": "On the optimal estimation of probability measures in weak and strong", "author": ["B. Sriperumbudur"], "venue": "topologies. Bernoulli,", "citeRegEx": "Sriperumbudur,? \\Q2016\\E", "shortCiteRegEx": "Sriperumbudur", "year": 2016}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G. Lanckriet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "A bound for the error in the normal approximation to the distribution of a sum of dependent random variables", "author": ["C. Stein"], "venue": "In Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley,", "citeRegEx": "Stein,? \\Q1971\\E", "shortCiteRegEx": "Stein", "year": 1971}, {"title": "Use of exchangeable pairs in the analysis of simulations. In Stein\u2019s method: expository lectures and applications, volume 46 of IMS Lecture Notes Monogr", "author": ["C. Stein", "P. Diaconis", "S. Holmes", "G. Reinert"], "venue": "Ser., pp. 1\u201326. Inst. Math. Statist., Beachwood,", "citeRegEx": "Stein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Stein et al\\.", "year": 2004}, {"title": "Positive definite functions and generalizations, an historical survey", "author": ["J. Stewart"], "venue": "Rocky Mountain J. Math.,", "citeRegEx": "Stewart,? \\Q1976\\E", "shortCiteRegEx": "Stewart", "year": 1976}, {"title": "Calculation of the Wasserstein distance between probability distributions on the line", "author": ["S. Vallender"], "venue": "Theory Probab. Appl.,", "citeRegEx": "Vallender,? \\Q1974\\E", "shortCiteRegEx": "Vallender", "year": 1974}, {"title": "High-dimensional statistics: A non-asymptotic viewpoint. 2017. URL http: //www.stat.berkeley.edu/ \u0303wainwrig/ nachdiplom/Chap5_Sep10_2015.pdf", "author": ["M. Wainwright"], "venue": null, "citeRegEx": "Wainwright,? \\Q2015\\E", "shortCiteRegEx": "Wainwright", "year": 2015}, {"title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning", "author": ["D. Wang", "Q. Liu"], "venue": null, "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y. Teh"], "venue": "In ICML,", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Scattered data approximation, volume 17", "author": ["H. Wendland"], "venue": "Cambridge university press,", "citeRegEx": "Wendland,? \\Q2004\\E", "shortCiteRegEx": "Wendland", "year": 2004}, {"title": "Proof of Proposition 2: KSD closed form Our proof generalizes that of (Chwialkowski et al., 2016", "author": ["C. EP [(TP g)(Z"], "venue": "Thm. 2.1). For each dimension j \u2208 {1,", "citeRegEx": "0.,? \\Q2016\\E", "shortCiteRegEx": "0.", "year": 2016}, {"title": "x)kb(x, y) and \u2207r(x) = \u2212xr(x). Thus for any x, by (Steinwart & Christmann, 2008, Corollary 4.36) we have \u2016\u2207h(x)\u20162 \u2264 Kk\u0303b \u3008\u2207x,\u2207yk\u0303b(x", "author": ["\u2207xkb(x"], "venue": null, "citeRegEx": "\u2207xkb.x and y,? \\Q2008\\E", "shortCiteRegEx": "\u2207xkb.x and y", "year": 2008}, {"title": "When bounds on R and F are known, the final expression can be optimized over and \u03b4 to produce rates of convergence in dBL\u2016\u00b7\u20162 . Consider now a sequence of probability measures (\u03bcm)m\u22651 that is uniformly tight. This implies that lim supmR(\u03bcm, ) < \u221e for all", "author": ["Gorham"], "venue": null, "citeRegEx": "Gorham,? \\Q2016\\E", "shortCiteRegEx": "Gorham", "year": 2016}, {"title": "Taking \u2192 0 yields dBL\u2016\u00b7\u20162 (\u03bcm, P )\u2192", "author": ["\u03b8d MP"], "venue": null, "citeRegEx": "..,? \\Q2016\\E", "shortCiteRegEx": "..", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al.", "startOffset": 58, "endOffset": 71}, {"referenceID": 7, "context": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample averages EQn [h(X)] = 1 n \u2211n i=1 h(xi).", "startOffset": 219, "endOffset": 240}, {"referenceID": 0, "context": "MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.", "startOffset": 114, "endOffset": 190}, {"referenceID": 22, "context": "MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.", "startOffset": 114, "endOffset": 190}, {"referenceID": 7, "context": "Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P .", "startOffset": 150, "endOffset": 171}, {"referenceID": 18, "context": "To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions.", "startOffset": 125, "endOffset": 192}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P .", "startOffset": 23, "endOffset": 366}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al.", "startOffset": 23, "endOffset": 553}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al.", "startOffset": 23, "endOffset": 577}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P .", "startOffset": 23, "endOffset": 599}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al.", "startOffset": 23, "endOffset": 886}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al.", "startOffset": 23, "endOffset": 914}, {"referenceID": 0, "context": ", Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points.", "startOffset": 23, "endOffset": 936}, {"referenceID": 28, "context": "In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997).", "startOffset": 63, "endOffset": 77}, {"referenceID": 28, "context": "In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997). For example, when H = BL\u2016\u00b7\u20162 , {h : R \u2192 R |M0(h) + M1(h) \u2264 1}, the IPM dBL\u2016\u00b7\u20162 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution. Alternatively, when H = W\u2016\u00b7\u20162 , {h : R d \u2192 R |M1(h) \u2264 1} is the set of 1-Lipschitz functions, the IPM dW\u2016\u00b7\u2016 in (1) is known as the Wasserstein metric. An apparent practical problem with using the IPM dH as a sample quality measure is that EP [h(Z)] may not be computable for h \u2208 H. However, if H were chosen such that EP [h(Z)] = 0 for all h \u2208 H, then no explicit integration under P would be necessary. To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein\u2019s method for characterizing distributional convergence.", "startOffset": 64, "endOffset": 816}, {"referenceID": 36, "context": "For any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as", "startOffset": 13, "endOffset": 68}, {"referenceID": 18, "context": "While Stein\u2019s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality.", "startOffset": 128, "endOffset": 172}, {"referenceID": 10, "context": "Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).", "startOffset": 130, "endOffset": 216}, {"referenceID": 23, "context": "Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).", "startOffset": 130, "endOffset": 216}, {"referenceID": 18, "context": "While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity.", "startOffset": 123, "endOffset": 144}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions.", "startOffset": 36, "endOffset": 74}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions.", "startOffset": 36, "endOffset": 98}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals.", "startOffset": 36, "endOffset": 360}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al.", "startOffset": 36, "endOffset": 1996}, {"referenceID": 3, "context": "Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016). Our next result shows that, for any \u2016\u00b7\u2016, the KSD admits a closed-form solution.", "startOffset": 36, "endOffset": 2018}, {"referenceID": 20, "context": "Each term wj in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between \u03bc and P measured with respect to the Stein kernel k 0.", "startOffset": 102, "endOffset": 124}, {"referenceID": 29, "context": "Our Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k0 = \u2211d j=1 k j 0 to develop nonparametric control variates.", "startOffset": 50, "endOffset": 71}, {"referenceID": 18, "context": "Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:", "startOffset": 10, "endOffset": 31}, {"referenceID": 13, "context": "Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).", "startOffset": 36, "endOffset": 71}, {"referenceID": 18, "context": "Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).", "startOffset": 36, "endOffset": 71}, {"referenceID": 6, "context": "Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.", "startOffset": 59, "endOffset": 82}, {"referenceID": 39, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015).", "startOffset": 278, "endOffset": 295}, {"referenceID": 36, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.", "startOffset": 319, "endOffset": 363}, {"referenceID": 36, "context": "Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.5 as our target P and generate a first sample point sequence i.i.d. from the target and a second sequence i.i.d. from one component of the mixture, N (\u2212\u2206e1, Id). As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n\u22120.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample. This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy. The middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P . Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.", "startOffset": 319, "endOffset": 1226}, {"referenceID": 12, "context": "Selecting sampler hyperparameters The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) \u221d \u03c0(x) \u220fL l=1 \u03c0(yl|x) for \u03c0(\u00b7) a prior distribution on R and \u03c0(yl|x) the likelihood of a datapoint yl.", "startOffset": 67, "endOffset": 88}, {"referenceID": 7, "context": "budget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3.", "startOffset": 167, "endOffset": 188}, {"referenceID": 0, "context": "Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS).", "startOffset": 19, "endOffset": 37}, {"referenceID": 0, "context": "Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (termed SGFS-f), a d\u00d7 d matrix must be inverted to draw each new sample point. Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point. Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution. Hence we will use the KSD \u2013 a quality measure that accounts for asymptotic bias \u2013 to evaluate and choose between these samplers. Specifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1). The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 10 MNIST handwritten digit images. From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9. After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 \u00d7 10 points and d = 51 dimensions (including the intercept term). Figure 4 displays the IMQ KSD applied to the first n points in each sample. As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a", "startOffset": 19, "endOffset": 1553}, {"referenceID": 44, "context": "IMPROVING SAMPLE QUALITY Liu & Lee (2016) recently used the KSD S(Qn, TP ,Gk) as a means of improving the quality of a sample. Specifically, given an initial sample Qn supported on x1, . . . , xn, they minimize S(Q\u0303n, TP ,Gk) over all measures Q\u0303n supported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = TPGk. In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e\u2212 1 h\u2016x\u2212y\u2016 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points.", "startOffset": 37, "endOffset": 414}, {"referenceID": 14, "context": "For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.", "startOffset": 23, "endOffset": 41}, {"referenceID": 12, "context": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.", "startOffset": 23, "endOffset": 108}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1088}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1113}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1172}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al.", "startOffset": 41, "endOffset": 1191}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016).", "startOffset": 41, "endOffset": 1216}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence.", "startOffset": 41, "endOffset": 1292}, {"referenceID": 8, "context": "3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. A reader may also wonder for which distributions outside ofP the KSD dominates weak convergence. The following theorem, proved in Section J, shows that no KSD with aC0 kernel dominates weak convergence when the target has a bounded score function. Theorem 10 (KSD fails for bounded scores). If \u2207 log p is bounded and k \u2208 C 0 , then S(Qn, TP ,Gk) \u2192 0 does not imply Qn \u21d2 P . However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator TP with diffusion Stein operators of the form (T g)(x) = 1 p(x) \u3008\u2207, p(x)(a(x) + c(x))g(x)\u3009.", "startOffset": 41, "endOffset": 1951}], "year": 2017, "abstractText": "Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein\u2019s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.", "creator": "LaTeX with hyperref package"}}}