{"id": "1703.01946", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Metric Learning for Generalizing Spatial Relations to New Objects", "abstract": "Human-centered environments are rich in a variety of spatial relationships between everyday objects. In order for autonomous robots to work effectively in such environments, they should be able to think about these relationships and generalize them into objects of different shapes and sizes. For example, after learning to put a toy in a basket, a robot should be able to generalize this concept with a spoon and a cup. This requires a robot to have the flexibility to learn arbitrary relationships throughout life, which makes it difficult for an expert to program it beforehand with enough knowledge to do so. In this essay, we address the problem of learning spatial relationships by introducing a novel method from the perspective of remote metric learning. Our approach allows a robot to think about the similarity between paired spatial relationships, enabling it to use its previous knowledge with a new relationship to imitate. We show how this makes it possible to play random user relationships with a small number of objects in an effective way.", "histories": [["v1", "Mon, 6 Mar 2017 16:13:17 GMT  (5474kb,D)", "https://arxiv.org/abs/1703.01946v1", "Submission to the IEEE/RSJ International Conference on Intelligent Robots and Systems"], ["v2", "Tue, 7 Mar 2017 12:47:56 GMT  (5474kb,D)", "http://arxiv.org/abs/1703.01946v2", "Under review at the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems. The corresponding demo video can be found atthis http URL"], ["v3", "Mon, 24 Jul 2017 12:24:31 GMT  (5739kb,D)", "http://arxiv.org/abs/1703.01946v3", "Accepted at the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems. The new Freiburg Spatial Relations Dataset and a demo video of our approach running on the PR-2 robot are available at our project website:this http URL"]], "COMMENTS": "Submission to the IEEE/RSJ International Conference on Intelligent Robots and Systems", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["oier mees", "nichola abdo", "mladen mazuran", "wolfram burgard"], "accepted": false, "id": "1703.01946"}, "pdf": {"name": "1703.01946.pdf", "metadata": {"source": "CRF", "title": "Metric Learning for Generalizing Spatial Relations to New Objects", "authors": ["Oier Mees", "Nichola Abdo", "Mladen Mazuran", "Wolfram Burgard"], "emails": ["burgard}@informatik.uni-"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nUnderstanding spatial relations is a crucial faculty of autonomous robots operating in human-centered environments. We expect future service robots to undertake a variety of everyday tasks such as setting the table, tidying up, or assembling furniture. In this context, a robot should be able to reason about the best way to reproduce a spatial relation between two objects, e.g., by placing an item inside a drawer, or aligning two boxes side by side.\nHowever, our everyday environments typically include a rich spectrum of potential spatial relations. For example, each user may have different preferences with respect to object arrangements, which requires robots to be flexible enough to handle arbitrary relations they have not encountered before. Similarly, robots should be able to generalize relations they have learned and achieve them using new objects of different shapes or sizes. For these reasons, it is highly impractical to expect an expert to pre-program a robot with the knowledge it needs to handle all potential situations in the real world, e.g., in the form of symbols. Instead, we aim for a lifelong learning approach that enables non-expert users to teach new spatial relations to robots in an intuitive manner.\nOne way to do this is to provide a robot with several examples using different objects in order to learn a model for a new relation, e.g., \u201cinside.\u201d On the one hand, this may require generating large amounts of data to learn the new\nAll authors are with the Department of Computer Science, University of Freiburg, Germany. {meeso, abdon, mazuran, burgard}@informatik.unifreiburg.de\nrelation, which is impractical in setups in which a robot learns from a non-expert teacher. On the other hand, this requires learning a new model for each relation individually, making it hard for the robot to reuse its knowledge from previous relations.\nIn this paper, we address this problem from the perspective of distance metric learning and focus on learning relations between pairs of objects. We present a novel method that allows the robot to reason about how similar two relations are to each other. By doing so, we formulate the problem of reproducing a relation using two new objects as one of minimizing the distance between the reproduced relation and the teacher demonstrations. More importantly, our approach enables the robot to use a few teacher demonstrations as queries for retrieving similar relations it has seen before, thereby leveraging prior knowledge to bootstrap imitating the new relation. Therefore, rather than learning a finite set of individual relation models, our method enables reasoning on a continuous spectrum of relations.\nConcretely, we make the following contributions: i) we present a novel approach from the perspective of distance metric learning to address the problem of learning pairwise spatial relations and generalizing them to new objects1, ii) we introduce a novel descriptor that encodes pairwise spatial relations based only on the object geometries and their relative pose, iii) we demonstrate how our method enables bootstrapping the learning of a new relation by relating it to similar, previously-learned relations, iv) we present an interactive learning method that enables non-expert users\n1The Freiburg Spatial Relations Dataset and a demo video of our approach running on the PR-2 robot are available at http:// spatialrelations.cs.uni-freiburg.de\nar X\niv :1\n70 3.\n01 94\n6v 3\n[ cs\n.R O\n] 2\n4 Ju\nl 2 01\n7\nto teach arbitrary spatial relations from a small number of examples, and v) we present an extensive evaluation of our method based on real-world data we gathered from different user demonstrations."}, {"heading": "II. RELATED WORK", "text": "In the context of robotics, previous work has focused on leveraging predefined relations in the form of symbolic predicates for solving tasks, as in the case of combined task and motion planning or in the context of relational reinforcement learning [4, 8, 12, 22]. Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20]. As opposed to these works, we reason about the similarity between relations by learning the distance between scenes, allowing us to compute scenes that generalize a relation to new objects.\nRelated to this is the work by Rosman and Ramamoorthy, which proposes constructing a contact point graph to classify spatial relations [23]. Similarly, Fichtl et al. train random forest classifiers for relations based on histograms that encode the relative position of surface patches [5]. Guadarrama et al. learn models of pre-defined prepositions by training a multi-class logistic regression model using data gathered from crowdsourcing [6]. As opposed to those works, we propose learning a distance metric that captures the similarities between different relations without specifying explicit classes.\nMoreover, related to our work is the interactive approach by Kulick et al. for learning relational symbols from a teacher [16]. They use Gaussian Process classifiers to model symbols and therefore enable a robot to query the teacher with examples to increase its confidence in the learned models. Similarly, our method enables a robot to generalize a relation by interacting with a teacher. However, we do this from the perspective of metric learning, allowing the robot to re-use previous demonstrations of other relations.\nSimilar to our work, Zampogiannis et al. model spatial relations based on the geometries of objects given their point cloud models [27]. However, they define a variety of common relations and focus on addressing the problem of extracting the semantics of manipulation actions through temporal analysis of spatial relations between objects. Other methods have also relied on the geometries of objects and scenes to reason about preferred object placements [11] or likely places to find an object [3]. Moreover, Kroemer and Peters used 3D object models to extract contact point distributions for predicting interactions between objects [15].\nFinally, our approach leverages distance metric learning for reasoning about the similarity between relations. Metric learning is a popular paradigm in the machine learning and computer vision communities. Learned metrics have been applied to address face recognition [7], image classification [9] and image segmentation [26]. In the context of robotics, metric learning has been used to address problems related to object instance or place recognition [17, 24]."}, {"heading": "III. NOTATION AND PROBLEM FORMULATION", "text": "In this section, we formalize the problem we address in this paper."}, {"heading": "A. Object Representation", "text": "We consider the problem of learning spatial relations between pairs of objects. We denote an object by o. In this work, we assume to have no semantic knowledge about objects such as their type, e.g., box. Instead, we aim to learn relations based on object geometries and assume to have a 3D model of each object ok in the form of a point cloud Pk. We consider only points on the surface of the objects. We model the state using the 3D poses of objects in SE (3) and express the pose Tk of ok relative to the world frame as a homogeneous transform consisting of a translation vector tk \u2208 R3 and a rotation matrix Rk \u2208 SO(3). We denote the pose of ol relative to ok by kTl. Additionally, we assume the world frame to be specified such that the \u2212z-axis aligns with the gravity vector g."}, {"heading": "B. Pairwise Relations", "text": "We consider learning pairwise spatial relations between objects, i.e., we consider scenes with two objects only. In this work, we do not address the perception problem and rely on existing techniques to segment the scene and compute the object poses based on their point clouds. Accordingly, we express a scene with ok and ol as a tuple s := \u3008Pk,Pl, kTl\u3009. In this work, we assume that one of the objects (ok) is labeled as the reference object, and therefore express the scene using the pose kTl of ol relative to ok. Given a scene s, we aim to express the spatial relation between the two objects in it. For this, we rely on a feature function (descriptor) f to express the relation as a K-dimensional feature vector r, i.e., f(s) = r \u2208 RK . Moreover, our goal is to enable the robot to reason about the how similar two scenes are with respect to the pairwise relations in them. We capture the similarity between s1 and s2 using a distance function dist that computes the distance dist(r1, r2) \u2265 0 between the two scenes with respect to their feature vectors r1 and r2. In this work we do not consider relations involving more than two objects, as they can be defined by combining pairwise relations. Finally, we do not explicitly treat object symmetries."}, {"heading": "C. The Problem", "text": "The problem we address is threefold. 1) Representing relations: First, we seek a descriptor f that enables us to capture the underlying spatial relation in a scene based only on the geometries (point clouds) of the objects, their relative poses, and the direction of gravity g.\n2) Learning the distance between relations: Given f , we aim to learn a distance metric dist for computing the distance between two scenes. For this, we rely on training data D = {s(1), . . . , s(N)} consisting of N demonstrated scenes. Additionally, we assume to have a symmetric similarity matrix Y of size N \u00d7 N with unit diagonal values. The value yi,j in the i-th row and j-th column of Y captures the\ndegree of similarity between scenes s(i) and s(j) in D. In this work, we consider binary similarities y \u2208 {0, 1}, such that 0 represents dissimilar relations and 1 means that the relations in both scenes are identical. Note that we do not assume Y to be completely specified, i.e., some entries may be missing. Therefore, we aim for a method that can learn with partiallylabeled data with respect to scene similarities. Given D and Y, our goal is to learn a distance metric dist that captures the distance between scenes s1 and s2 based on their features r1 = f(s1) and r2 = f(s2). We learn this metric such that dist(r1, r2) is \u201csmall\u201d if s1 and s2 represent similar relations, and \u201clarge\u201d if they represent dissimilar relations.\n3) Generalizing a relation to new objects: Given f and a distance metric dist, our goal is to learn a new, arbitrary relation from a teacher. We assume the teacher provides a small set of demonstrations D\u2032 = {s(1), . . . , s(N \u2032)} of the new relation, where 1 \u2264 N \u2032 N . Given two new objects ok and ol, the robot has to \u201cimitate\u201d the demonstrated relation in D\u2032 by computing the pose kTl of ok relative to ol such that the resulting scene s\u2217 = \u3008Pk,Pl, kTl\u3009 is close to the demonstrations with respect to the corresponding features.\nConcretely, we seek the best kT\u2217l to a problem of the form:\nminimize L(R\u2032, r\u2217) subject to kTl \u2208 SE (3), s\u2217 \u2208 Sfeas,\n(1)\nwhere R\u2032 = {r(1), . . . , r(N \u2032)} is the set of features for the demonstrated scenes D\u2032, r\u2217 = f(s\u2217) is the feature vector of the test scene s\u2217, and L is a loss function describing the distance between the demonstrations and the test scene based on dist. Additionally, Sfeas denotes the set of physically feasible scenes. In this work, we focus on computing the desired pose between the two objects and do not consider the problem of manipulating the objects to achieve this pose. Therefore, we consider Sfeas as the set of scenes in which ok and ol are not colliding."}, {"heading": "IV. PROPOSED FEATURE REPRESENTATION", "text": "In this section, we present our model for f and propose a descriptor for modeling pairwise spatial relations, thereby addressing Sec. III-C.1. We model relations based only on the spatial interaction between their point clouds Pk and Pl given the direction of the gravity vector g. Note that in this work, we do not address the correspondence problem between scenes and assume the teacher specifies the reference object. We rely on the directions of the vectors between the surface points of the objects as a signature of the underlying relation between them. Defining these directions purely based on a fixed (world) reference frame is sub-optimal as this results in a descriptor that is affected by translations and rotations of the scene. At the same time, computing a local reference frame using one of the objects (e.g., using PCA) introduces the challenge of ensuring consistency and reproducibility of the axes across different scenes. We address this problem by computing angles between direction vectors between points on both objects and the centroid of the reference object ok, see Fig. 2. This is analogous to\nmethods for computing rotationally-invariant descriptors for 2D images such as RIFT [18]. Accordingly, we propose a descriptor that is based on three histograms as follows:\nf(s) := [h\u03b8 h\u03d5 hd] >. (2)\nWith the first histogram h\u03b8, we capture the angular relation between the two objects regardless of how the scene is oriented in the global reference frame. We construct h\u03b8 as a distribution over the angle \u03b8 between vectors (pk \u2212 ck) and (pl \u2212 pk) based on all points pk \u2208 Pk and pl \u2208 Pl, i.e.,\n\u03b8 = arccos ( (pk \u2212 ck)>(pl \u2212 pk) \u2016pk \u2212 ck\u20162 \u2016pl \u2212 pk\u20162 ) , (3)\nwhere ck is the centroid of the reference object ok, see Fig. 2. For the same relative pose kTl, h\u03b8 provides a unique signature of the scene that is invariant to its translation or rotation in the global reference frame. However, in the context of everyday manipulation tasks, it is typically useful to also reason about spatial constraints with respect to the world frame, e.g., a supporting surface such as a table. For example, this enables the robot to disambiguate scenes in which the two objects are on top of each other from those in which they are next to each other for the same kTl.\nWe achieve this disambiguation using the second histogram h\u03d5, which is a distribution over the angle \u03d5 around the vector (pk \u2212 ck), see Fig. 2. We take this as the angle between two planes. The first plane is defined by the two vectors (pk \u2212 ck) and g, whereas the second is defined by the two vectors (pk \u2212 ck) and (pl \u2212pk). We compute \u03d5 as the angle between the respective normal vectors n1 and n2 of those planes, i.e.,\nn1 = (pk \u2212 ck)\u00d7 g \u2016(pk \u2212 ck)\u00d7 g\u20162 ,n2 = (pk \u2212 ck)\u00d7 (pl \u2212 pk) \u2016(pk \u2212 ck)\u00d7 (pl \u2212 pk)\u20162 ,\n\u03d5 = arccos(n>1 n2). (4)\nWe populate h\u03d5 by computing \u03d5 using all surface points pk \u2208 Pk and pl \u2208 Pl. As the direction of g is fixed, rotating the scene while maintaining kTl results in changes in \u03d5, i.e., the discriminative behavior we seek. On the other hand, h\u03b8 and h\u03d5 are invariant to translations or rotations around g.\nWhereas h\u03b8 and h\u03d5 encode the relation with respect to the relative rotation between the two objects, we encode the desired distance between them using the third histogram hd. We compute hd as a distribution over the Euclidean distance \u2016pk \u2212 pl\u20162 between points pk \u2208 Pk and pl \u2208 Pl. Rather than doing so using all |Pk| |Pl| pairs of points {pk,pl}, we consider the subset of pairs with the smallest 10% distances over all pairs, as this is indicative of how close the two objects are and is less sensitive to differences in object sizes.\nWe discretize both h\u03b8 and h\u03d5 with a bin resolution of 20 deg, and discretize hd with a resolution of 6 cm. We normalize all histograms using the number of points used to compute them such that f(s) is independent of the object point cloud densities and object sizes. The resulting descriptor f(s) = r has 39 dimensions. To speed up computation, besides considering only surface points, we also downsample the point clouds."}, {"heading": "V. DISTANCE METRIC LEARNING", "text": "In this section, we discuss how we learn a metric dist that models the similarities between relations given the feature representation r = f(s) above (see Sec. III-C.2). For this, we leverage a popular metric learning technique originally introduced to improve the performance of k-NN classification: large margin nearest neighbor (LMNN) [25].\nWe follow the terminology of Weinberger and Saul and define the set of target neighbors R+i for an example ri as the k nearest neighbors of ri that are labeled as similar, i.e. yi,j = 1 for rj \u2208 R+j . These target neighbors define a region around ri. We refer to all examples rk within this region that are not similar to ri (i.e., yi,k = 0) as imposters R\u2212i . The original LMNN formulation identifies R+i and R \u2212 i by assuming training data that is labeled with pre-specified classes. In our context, we achieve this based on the similarity labels y without requiring class labels to be specified by the teacher.\nIn the general form, LMNN learns a metric dist\u03c6 parametrized by \u03c6 by minimizing a loss function with two objectives: i) for each training relation ri, pull target neighbors rj \u2208 R+i close, and ii) push imposters rk \u2208 R \u2212 i away such that they are further than target neighbors rj by at least a large margin \u03b6 (see [25]), i.e.,\nminimize \u03c6 \u2211 ri\u2208D, rj\u2208R+i pull a similar neighbor rj close\ufe37 \ufe38\ufe38 \ufe37 dist\u03c6(ri, rj) 2 +\n\u03bb \u2211\nrk\u2208R\u2212i\n[ \u03b6 + dist\u03c6(ri, rj) 2 \u2212 dist\u03c6(ri, rk)2 ] +\ufe38 \ufe37\ufe37 \ufe38\npush a dissimilar neighbor rk further than rj by at least \u03b6\n,\n(5)\nwhere [d]+ = max(0, d) is the hinge loss and \u03bb is a constant that controls the trade-off between the two objectives.\nIn this work, we consider three LMNN-based methods for learning a dist\u03c6 parametrized by \u03c6. The linear LMNN case learns a generalized Euclidean (Mahalanobis) distance by parametrizing dist\u03c6 using a linear mapping L \u2208 RK\u00d7K , i.e., \u03c6(r) = Lr, see [25]. \u03c72-LMNN also learns a linear\nmapping but uses the \u03c72 distance instead of the Euclidean distance, see [13]. Finally, gradient-boosted LMNN (GBLMNN) models arbitrary non-linear mappings \u03c6(r) of the input space using gradient-boosted regression trees, see [13]."}, {"heading": "VI. REPRODUCING A NEW RELATION FROM A FEW DEMONSTRATIONS", "text": "In this section, we present our approach for imitating a new relation from a small number of teacher demonstrations (Sec. III-C.3). We assume that the robot is already equipped with a set of relation scenes D = {s(1), . . . , s(N)} and a (partially-filled) N \u00d7 N matrix Y consisting of their similarity labels as in Sec. III-C.2. These are either provided by an expert beforehand, or are accumulated by the robot when learning previous relations over time. Using D and Y, we assume the robot has already learned a prior distance metric dist\u03c60 parametrized by \u03c60 as described in Sec. V. This is done offline and without knowledge of the new relation.\nWe now consider a teacher providing the robot with a small set of demonstrations D\u2032 of size N \u2032 for a new, arbitrary relation. The teacher can use different pairs of objects, such that all scenes in D\u2032 are equally valid ways of achieving this relation, i.e., yi,j = 1 for all s(i), s(j) \u2208 D\u2032. Given two objects ok and ol and their respective models Pk and Pl, our goal is to compute a pose kT\u2217l such that the resulting scene s\u2217 = \u3008Pk,Pl, kT\u2217l \u3009 corresponds to the intention of the teacher for the new relation, i.e., minimizing L in Eq. (1).\nGiven a metric dist\u03c6* , there are different ways to model L to express the distance between the features r\u2217 = f(s\u2217) of the test scene and the features R\u2032 of the demonstrations D\u2032. In general, as |R\u2032| \u2265 1, Eq. (1) is a multi-objective optimization problem seeking to minimize the distance between r\u2217 and all r\u2032 \u2208 R\u2032. In such settings, it is typically challenging to satisfy all objectives. Minimizing the (mean) distance to D\u2032 can thus lead to sub-optimal solutions that \u201caverage\u201d the demonstrated scenes. Instead, we consider each demonstration to represent a mode of the target relation and seek the best solution with respect to any of them as follows:\nL(R\u2032, r\u2217) := min r\u2032\u2208R\u2032 dist\u03c6*(r \u2032, r\u2217). (6)\nA. Interactive Local Metric Learning\nOne way to obtain dist\u03c6* in Eq. (6) is to use the prior metric dist\u03c60 . However, we learned this metric using a set of previous relations D and their similarities. Therefore, it is not directly clear if dist\u03c60 is able to generalize to novel relations shown by the teacher.\nWe answer this question using an interactive approach. For each demonstration in D\u2032, we use dist\u03c60 to retrieve the Q nearest neighbor examples corresponding to scenes from the database D. We query the teacher with these examples and ask her to indicate whether they align with her intention (y = 1) for the new relation or not (y = 0). In our experiments, we achieved this by means of a graphical user interface visualizing Q = 8 nearest neighbors per query. Let DNN \u2282 D be the set of all nearest neighbor scenes for D\u2032. We measure the confidence in the ability of dist\u03c60 to\ngeneralize to the new relation as the ratio NN of scenes in DNN for which the teacher indicated a similarity to the new relation (i.e., y = 1). NN values larger than a threshold \u2217 indicate that we are able to relate the new relation to ones the robot has seen in the past. Therefore, we use dist\u03c60 to compute Eq. (6), i.e., \u03c6\u2217 = \u03c60. We empirically set \u2217 to 77% in our experiments. On the other hand, NN < \u2217 indicates that the new relation is far in feature space from (target neighbor) relations in D. We address this by learning a new local metric dist\u03c6* using the set of scenes D\u2217 = D\u2032 \u222a DNN and labels Y\u2217 of size N\u2217 \u00d7 N\u2217, where N\u2217 = |D\u2217|. This is a smaller problem (compared to learning the prior metric dist\u03c60 ) in which Y \u2217 is completely specified. We set the similarity yi,j to 1 for all s(i), s(j) \u2208 D\u2032. For rows i and columns j corresponding to scenes s(i) \u2208 D\u2032 and s(j) \u2208 DNN (or vice versa), we set yi,j to the similarity labels obtained from querying the teacher. We use the transitivity property to set the similarity between s(j), s(k) \u2208 DNN . For example, if the teacher labeled yi,j = 0 and yi,k = 0, we set yj,k = 1.\nFinally, we highlight two main advantages of leveraging the previous relations D and prior metric dist\u03c60 . Firstly, we enable the robot to consider whether its previous knowledge is sufficient to reproduce the new relation or not. Secondly, even for new relations that are significantly different from previously-known ones, we are able to augment the teacher\u2019s demonstrations D\u2032 with additional training data DNN consisting of target neighbors and imposters retrieved from D without requiring the teacher to demonstrate them."}, {"heading": "B. Sample-Based Pose Optimization", "text": "Given the metric dist\u03c6* to model Eq. (6), we present our approach for solving Eq. (1) to compute kT\u2217l for reproducing the new relation using ok and ol. In this work, we simplify this problem by assuming that the reference object ok is stationary and therefore only reason about desirable poses of ol relative to it. Due to the discretization in computing\nour descriptor f , we cannot rely on gradient-based methods as our loss function is piecewise constant.\nWe address this using a sample-based approach. We discretize the space of poses by searching over a grid of translations ktl of ol relative to ok. For each translation, we sample rotations kRl uniformly. We use the resulting kTl to transform Pl and compute L based on the corresponding feature value r\u2217 of the scene. Whenever we find a new local minima during optimization, we check for collisions between the two objects and reject infeasible solutions. Finally, we take kT\u2217l as the feasible pose minimizing L over all sampled poses. We implemented this process efficiently by parallelizing the grid search over translations. Fig. 3 shows an overview of our method for reproducing a new relation."}, {"heading": "VII. EXPERIMENTAL EVALUATION", "text": "In this section, we present the experimental evaluation of our approach. Through our experiments, we demonstrate the following: i) our proposed descriptor is able to capture different spatial relations and to generalize to the shapes and sizes of the objects, ii) using distance metric learning, we are able to capture the similarities between scenes even for relations not encountered before, iii) our interactive learning method enables non-expert users to teach new relations based on small number of examples, and iv) we outperform several baselines that do not learn a metric based on the similarities between scenes."}, {"heading": "A. Baselines", "text": "In our experiments, we used three variants of LMNNbased metrics: vanilla (linear) LMNN, \u03c72-LMNN, and GBLMNN, which we learned as in Sec. V. We compared those learned metrics to a variety of standard distance metrics. This includes the Euclidean, \u03c72, Bhattacharyya, and the correlation distances, as well as the Kullback-Leibler divergence (KL) and the Jensen-Shannon divergence (JS)."}, {"heading": "B. Dataset", "text": "We recorded 3D models of 26 household objects and used SimTrack to detect them and compute their poses in a scene using a Kinect camera [21]. Using this setup, we recorded a set of demonstrations D consisting of 546 scenes, see Fig. 4 for examples. For the purpose of evaluation, we manually labeled the similarities Y between all scenes."}, {"heading": "C. Nearest Neighbor Classification", "text": "In this experiment, we evaluated the ability of distance metric learning to relate scenes based on the similarities of their relations. We formulated this as a k-NN classification problem, with k = 5, and evaluated using 15 random splits. For each split we used 75% of the data for the training set and 25% for the test set. We considered a success if at least 3 out of 5 of the retrieved nearest neighbors were similar to the test example.\nThe results are shown in Tab. I. LMNN-based metrics outperform the baselines, i.e., the learned metrics can better capture the distances between scenes. We achieved the highest success rate of 87.6% using GB-LMNN. Note that by directly computing the Euclidean distance in the original feature space, we are able to achieve a success rate of 82.32%. This demonstrates that our proposed feature descriptor is suitable for encoding arbitrary spatial relations. Fig. 5 shows a qualitative example of the nearest neighbors of a test scene using both the Euclidean distance and LMNN."}, {"heading": "D. Distance to New Relations", "text": "In this qualitative experiment, we investigated the ability of a learned metric to capture the similarities between relations that were not used for training. We trained LMNN with data from three relations (upper row of Fig. 4), which can be semantically described as \u201con top\u201d, \u201cinside\u201d, and \u201cnext to\u201d. We used the learned metric to map all six relations in Fig. 4 to the new space and visualized the data using t-SNE, a popular non-linear embedding technique for visualizing high dimensional data [19]. We show this in Fig. 6. This qualitatively illustrates the separation between the three relations used for training the metric. Moreover, the metric is able to capture the semantic similarity between the relations used for training and the new ones, which we denote by \u201cinclined\u201d, \u201con top corner\u201d and \u201cinclined inside\u201d (bottom row of Fig. 4)."}, {"heading": "E. Generalizing a Relation to New Objects", "text": "In this experiment, we evaluated our approach for reproducing a demonstrated relation using two new objects, see Sec. III-C.3. We recorded 30 demonstrations for each of five new relations. We then selected two new objects that were not used in the demonstrations and evaluated our\nmethod\u2019s ability to generalize each of the five relations to the new objects. In each case, we provided our method with |D\u2032| = 5 examples. Using those examples, we retrieved nearest neighbor queries DNN from D as described in Sec. VI. As we aimed for a quantitative evaluation in this experiment, we implemented an \u201coracle\u201d that provides the binary labels for the queries automatically, and used this to learn a local metric as in Sec. VI-A.\nFor evaluation, we provided our method with a set of 75 poses kTl between the new objects. Only 15 of them are correct ways of imitating the relation in question, whereas the rest represent other relations. For each pose, we computed r\u2217 of the test scene and the corresponding L (Eq. (6)). We sorted the poses according to L. Ideally, the correct poses should be in the top 15 positions. We evaluated this using the mean average precision.\nAfter each such test, we added the five demonstrations D\u2032 to the database D and extended Y with the new labels from the nearest neighbor queries. We used this to re-learn the prior metric dist\u03c60 . We did this six times, each with five new demonstrations, until the 30 demonstrations have been used. We repeated the whole experiment 50 times using different random orders of providing five demonstrations. We report the results in Fig. 7 averaged over all runs and five relations.\nThe metrics we learned with GB-LMNN and LMNN outperform the other metrics in their ability to identify the correct ways of reproducing a relation with two new objects. Although in each case we compute Eq. (6) based on five new demonstrations only, our approach enables those metrics to re-use demonstrations added to D from the previous tests to learn the local metric. Accordingly, GB-LMNN achieves a mean average precision of 0.82 after having seen at least five demonstrations in the past. This demonstrates the ability of our approach to use previous demonstrations when generalizing a new relation in a lifelong learning manner.\nF. Interactive Learning of a New Relation\nWe conducted a small survey to evaluate our approach for learning a new relation interactively. We recorded 250 scenes consisting of 50 relations demonstrated by nine different teachers. Each teacher provided five demonstrations per relation using different objects they chose. We used only three of those demonstrations to learn each relation. In each case, we queried the teacher with nearest neighbor examples from D and used the result to generalize the three demonstrations D\u2032 and reproduce one of the scenes we left out from the training as in Sec. VI. We computed the best pose for reproducing the relation using the sample-based approach in Sec. VI-B.\nWe showed the reproduced relations to their corresponding teachers in a 3D visualization environment and asked them to score the quality of the result with 0, 0.5, or 1, where 0 represents unsuccessful and 1 represents successful. The teachers scored results from different baselines shown in random order.\nWe show the mean scores in Tab. II. The reproduced scenes using both LMNN and GB-LMNN metrics were judged to be the best by the teachers, achieving an average score of 0.72 and 0.71 respectively. Fig. 8 illustrates one such generalization from our experiments. The results confirm that our approach enables non-expert users to teach arbitrary spatial relations to a robot from a small number of examples.\nAs a final qualitative evaluation, we carried out a small survey in which we asked six participants to judge the quality of scenes generated by our sample-based approach in Sec. VI-B. For this, we selected 30 scenes computed by LMNN, and which were scored with either 0.5 or 1 in the experiment above. We asked the six participants to judge whether the relations in those scenes were demonstrated by a human or generated by a computer (whereas in fact they were all computed using our approach). Despite the fact that we do not consider physics checks (e.g., scene stability) or optimize\ncomputed scenes to make them more realistic, 63.05% of the scenes were thought to have been produced by a human."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we presented a novel approach to the problem of learning pairwise spatial relations and generalizing them to different objects. Our method is based on distance metric learning and enables a robot to reason about the similarity between scenes with respect to the relations they represent. To encode a relation, we introduced a novel descriptor based on the geometries of the objects. By learning a distance metric using this representation, our method is able to reproduce a new relation from a small number of teacher demonstrations by reasoning about its similarity to previously-encountered ones. In this way, our approach allows for a lifelong learning scenario by continuously leveraging its prior knowledge about relations to bootstrap imitating new ones. We evaluated our approach extensively using real-world data we gathered from non-expert teachers. Our results demonstrate the effectiveness of our approach in reasoning about the similarity between relations and its ability to reproduce arbitrary relations to new objects by learning interactively from a teacher. In the future, we plan to extend the approach to partial observations of point cloud data.\nACKNOWLEGMENTS\nThis work has partly been supported by the German Research Foundation under research unit FOR 1513 (HYBRIS) and grant number EXC 1086. We thank Gian Diego Tipaldi, Marc Toussaint, and Oliver Kroemer for the valuable discussions during the early stages of this work."}], "references": [{"title": "Learning manipulation actions from a few demonstrations", "author": ["N. Abdo", "H. Kretzschmar", "L. Spinello", "C. Stachniss"], "venue": "In Int. Conf. on Robotics & Automation (ICRA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Learning symbolic representations of actions from human demonstrations", "author": ["S.R. Ahmadzadeh", "A. Paikan", "F. Mastrogiovanni", "L. Natale", "P. Kormushev", "D.G. Caldwell"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Exploiting and modeling local 3d structure for predicting object locations", "author": ["A. Aydemir", "P. Jensfelt"], "venue": "In Int. Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "CRAM-A cognitive robot abstract machine for everyday manipulation in human environments", "author": ["M. Beetz", "L. M\u00f6senlechner", "M. Tenorth"], "venue": "In Int. Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning spatial relationships from 3d vision using histograms", "author": ["S. Fichtl", "A. McManus", "W. Mustafa", "D. Kraft", "N. Kr\u00fcger", "F. Guerin"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Grounding spatial relations for humanrobot interaction", "author": ["S. Guadarrama", "L. Riano", "D. Golland", "D. Go", "Y. Jia", "D. Klein", "P. Abbeel", "T. Darrell"], "venue": "In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "In 2009 IEEE 12th International Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Coupled learning of action parameters and forward models for manipulation", "author": ["S. H\u00f6fer", "O. Brock"], "venue": "In Int. Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S.C. Hoi", "W. Liu", "S.-F. Chang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Learning grounded relational symbols from continuous data for abstract reasoning", "author": ["N. Jetchev", "T. Lang", "M. Toussaint"], "venue": "ICRA Workshop on Autonomous Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Learning to place new objects in a scene", "author": ["Y. Jiang", "M. Lim", "C. Zheng", "A. Saxena"], "venue": "Int. J. of Robotics Research (IJRR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Integrated task and motion planning in belief space", "author": ["L.P. Kaelbling", "T. Lozano-P\u00e9rez"], "venue": "Int. J. of Robotics Research (IJRR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Non-linear metric learning", "author": ["D. Kedem", "S. Tyree", "F. Sha", "G.R. Lanckriet", "K.Q. Weinberger"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Constructing symbolic representations for high-level planning", "author": ["G. Konidaris", "L.P. Kaelbling", "T. Lozano-Perez"], "venue": "In National Conf. on Artificial Intelligence (AAAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Predicting object interactions from contact distributions", "author": ["O. Kroemer", "J. Peters"], "venue": "In 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Active learning for teaching a robot grounded relational symbols", "author": ["J. Kulick", "M. Toussaint", "T. Lang", "M. Lopes"], "venue": "In IJCAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Sparse distance learning for object recognition combining rgb and depth information", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A sparse texture representation using local affine regions", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Learning symbolic models of stochastic domains", "author": ["H.M. Pasula", "L.S. Zettlemoyer", "L.P. Kaelbling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Simtrack: A simulation-based framework for scalable real-time object pose detection and tracking", "author": ["K. Pauwels", "D. Kragic"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Do what i want, not what i did: Imitation of skills by planning sequences of actions", "author": ["C. Paxton", "F. Jonathan", "M. Kobilarov", "G.D. Hager"], "venue": "In Int. Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Learning spatial relationships between objects", "author": ["B. Rosman", "S. Ramamoorthy"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Dtlc: Deeply trained loop closure detections for lifelong visual slam", "author": ["M. Shahid", "T. Naseer", "W. Burgard"], "venue": "Workshop at the Robotics Science and Systems (RSS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Learning a mahalanobis distance metric for data clustering and classification", "author": ["S. Xiang", "F. Nie", "C. Zhang"], "venue": "Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Learning the spatial semantics of manipulation actions through preposition grounding", "author": ["K. Zampogiannis", "Y. Yang", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "In the context of robotics, previous work has focused on leveraging predefined relations in the form of symbolic predicates for solving tasks, as in the case of combined task and motion planning or in the context of relational reinforcement learning [4, 8, 12, 22].", "startOffset": 250, "endOffset": 264}, {"referenceID": 7, "context": "In the context of robotics, previous work has focused on leveraging predefined relations in the form of symbolic predicates for solving tasks, as in the case of combined task and motion planning or in the context of relational reinforcement learning [4, 8, 12, 22].", "startOffset": 250, "endOffset": 264}, {"referenceID": 11, "context": "In the context of robotics, previous work has focused on leveraging predefined relations in the form of symbolic predicates for solving tasks, as in the case of combined task and motion planning or in the context of relational reinforcement learning [4, 8, 12, 22].", "startOffset": 250, "endOffset": 264}, {"referenceID": 21, "context": "In the context of robotics, previous work has focused on leveraging predefined relations in the form of symbolic predicates for solving tasks, as in the case of combined task and motion planning or in the context of relational reinforcement learning [4, 8, 12, 22].", "startOffset": 250, "endOffset": 264}, {"referenceID": 0, "context": "Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20].", "startOffset": 187, "endOffset": 205}, {"referenceID": 1, "context": "Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20].", "startOffset": 187, "endOffset": 205}, {"referenceID": 9, "context": "Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20].", "startOffset": 187, "endOffset": 205}, {"referenceID": 13, "context": "Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20].", "startOffset": 187, "endOffset": 205}, {"referenceID": 19, "context": "Rather than relying on grounding existing symbols, other works have addressed learning symbols and effects of actions to abstract continuous states for the purpose of high-level planning [1, 2, 10, 14, 20].", "startOffset": 187, "endOffset": 205}, {"referenceID": 22, "context": "Related to this is the work by Rosman and Ramamoorthy, which proposes constructing a contact point graph to classify spatial relations [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "train random forest classifiers for relations based on histograms that encode the relative position of surface patches [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "learn models of pre-defined prepositions by training a multi-class logistic regression model using data gathered from crowdsourcing [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 15, "context": "for learning relational symbols from a teacher [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "model spatial relations based on the geometries of objects given their point cloud models [27].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Other methods have also relied on the geometries of objects and scenes to reason about preferred object placements [11] or likely places to find an object [3].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Other methods have also relied on the geometries of objects and scenes to reason about preferred object placements [11] or likely places to find an object [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 14, "context": "distributions for predicting interactions between objects [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "applied to address face recognition [7], image classification [9] and image segmentation [26].", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "applied to address face recognition [7], image classification [9] and image segmentation [26].", "startOffset": 62, "endOffset": 65}, {"referenceID": 25, "context": "applied to address face recognition [7], image classification [9] and image segmentation [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "In the context of robotics, metric learning has been used to address problems related to object instance or place recognition [17, 24].", "startOffset": 126, "endOffset": 134}, {"referenceID": 23, "context": "In the context of robotics, metric learning has been used to address problems related to object instance or place recognition [17, 24].", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "methods for computing rotationally-invariant descriptors for 2D images such as RIFT [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "For this, we leverage a popular metric learning technique originally introduced to improve the performance of k-NN classification: large margin nearest neighbor (LMNN) [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "In the general form, LMNN learns a metric dist\u03c6 parametrized by \u03c6 by minimizing a loss function with two objectives: i) for each training relation ri, pull target neighbors rj \u2208 R+i close, and ii) push imposters rk \u2208 R \u2212 i away such that they are further than target neighbors rj by at least a large margin \u03b6 (see [25]), i.", "startOffset": 314, "endOffset": 318}, {"referenceID": 24, "context": ", \u03c6(r) = Lr, see [25].", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "distance, see [13].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "Finally, gradient-boosted LMNN (GBLMNN) models arbitrary non-linear mappings \u03c6(r) of the input space using gradient-boosted regression trees, see [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "We recorded 3D models of 26 household objects and used SimTrack to detect them and compute their poses in a scene using a Kinect camera [21].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "4 to the new space and visualized the data using t-SNE, a popular non-linear embedding technique for visualizing high dimensional data [19].", "startOffset": 135, "endOffset": 139}], "year": 2017, "abstractText": "Human-centered environments are rich with a wide variety of spatial relations between everyday objects. For autonomous robots to operate effectively in such environments, they should be able to reason about these relations and generalize them to objects with different shapes and sizes. For example, having learned to place a toy inside a basket, a robot should be able to generalize this concept using a spoon and a cup. This requires a robot to have the flexibility to learn arbitrary relations in a lifelong manner, making it challenging for an expert to pre-program it with sufficient knowledge to do so beforehand. In this paper, we address the problem of learning spatial relations by introducing a novel method from the perspective of distance metric learning. Our approach enables a robot to reason about the similarity between pairwise spatial relations, thereby enabling it to use its previous knowledge when presented with a new relation to imitate. We show how this makes it possible to learn arbitrary spatial relations from non-expert users using a small number of examples and in an interactive manner. Our extensive evaluation with realworld data demonstrates the effectiveness of our method in reasoning about a continuous spectrum of spatial relations and generalizing them to new objects.", "creator": "LaTeX with hyperref package"}}}