{"id": "1703.02239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Functions that Emerge through End-to-End Reinforcement Learning - The Direction for Artificial General Intelligence -", "abstract": "Lately, triggered by the impressive results in television games or Go by Google DeepMind, end-to-end reinforcement learning (RL) has been gaining attention. Although little is known, the group of authors has been proposing this framework for about 20 years and has already demonstrated a variety of functions that arise in a neural network (NN) through RL. In this paper, they will be presented again at this time.", "histories": [["v1", "Tue, 7 Mar 2017 06:51:19 GMT  (1398kb)", "http://arxiv.org/abs/1703.02239v1", "5 pages, 4 figures"], ["v2", "Tue, 16 May 2017 07:22:07 GMT  (1385kb)", "http://arxiv.org/abs/1703.02239v2", "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 4 figures"]], "COMMENTS": "5 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["katsunari shibata"], "accepted": false, "id": "1703.02239"}, "pdf": {"name": "1703.02239.pdf", "metadata": {"source": "CRF", "title": "Functions that Emerge through End-to-endReinforcement Learning", "authors": ["Katsunari Shibata"], "emails": ["shibata@oita-u.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n02 23\n9v 1\n[ cs\n.A I]\n7 M\nar 2\n01 7\n\u201cFunction Modularization\u201d approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. \u201cState space\u201d or \u201caction space\u201d generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them.\nThe functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.\nKeywords: function emergence, end-to-end reinforcement learning (RL), recurrent neural network (RNN), higher functions, artificial general intelligence (AGI)"}, {"heading": "Acknowledgements", "text": "This research has been supported by JSPS KAKENHI Grant Numbers JP07780305, JP08233204, JP13780295, JP15300064, JP19300070 and many our group members\n\u2217http://shws.cc.oita-u.ac.jp/\u02dcshibata/home.html"}, {"heading": "1 Introduction", "text": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions. One remarkable point especially in the results in TV-games is the gap such that even though the inputs of a deep NN are raw image pixels without any pre-processing and the NN is just learned through RL, the ability acquired through learning extends to the excellent strategies for several games. The learning do not need special knowledge about learned tasks and necessary functions emerge through learning, and so it is expected to open the way to the AGI (Artificial General Intelligence) or Strong AI.\nIt has been general that a NN is considered as just a non-linear function approximator for RL, and a recurrent neural network (RNN) is used to avoid POMDP (Partially Observable Markov Decision Problem). Under such circumstances, the origin of the end-to-end RL can be found in the Tesauro\u2019s work called TD-gammon[4]. The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.\nIn this paper, the author\u2019s unwavering direction that end-to-end RL becomes an important key for explaining human intelligence or developing human-like intelligence especially for the higher functions is introduced at first. It is also shown that a variety of functions emerge through end-to-end (oriented) RL; from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in an RNN. All of the works here have been published already, but the author believes that it is worthwhile to know what functions emerge and what functions hardly emerge at this timing when the end-to-end RL begins to be focused on."}, {"heading": "2 The Direction for Human-like Intelligence", "text": "There is probably little doubt that human intelligence is realized thanks to the massively parallel and cohesively flexible processing on a huge degree of freedom in our brain. On the other hand, unlike in the case of unconsciousness, our consciousness looks linguistic and so it is not parallel but sequential. Therefore, it is impossible to completely understand the brain functions through our consciousness. Nevertheless, the researchers have tried to understand the brain or develop human-like intelligence by hands. We are likely to divide a difficult problem into sub-problems expecting each divided one is easier to solve. Then \u201cFunction Modularization\u201d approach has been deeply penetrated subconsciously. However, for each module, we have to decide what are the inputs and outputs at first. It is easily known that to decide the information that comes and goes between divided modules, it is necessary to understand the entire function. Then to decide the information, some simple frame is set in advance and that causes the \u201cFrame Problem\u201d. It can be also thought that the division into \u201csymbolic (logical) processing\u201d and \u201cpattern processing\u201d causes the \u201cSymbol Grounding Problem\u201d. In our brain, they may be processed in different areas, but they are both processed in the same brain as one NN.\n\u201cState space\u201d or \u201caction space\u201d, which is generally used in RL, can be another aspect of the function modularization. Researchers have limited the learning only to the actions that make the mapping from state space to action space, and have tried to develop the way of construction of state space from sensor signals and given the way of generating motor commands from each action separately from RL. It has also been taken for granted that classification categories are given in recognition function and reference trajectories are given in control function. However, we can recognize complicated situations from a picture, but it is clear that all of them cannot be given as classification targets in advance. There is no evidence that reference trajectory is explicitly generated in our brain. From the viewpoint of understanding and developing human-like intelligence, they are by-products of the function modularization. They give unnecessary constraints on a huge degree of freedom, and disturb the flexible and comprehensive learning despite the intension of the researchers.\nBased on the above, the author has thought that the interruption of human designers should be cut off and the development of functions should be committed to learning of high-dimensional parallel systems as much as possible. The inputs for a learning system can be raw sensor signals, the outputs can be raw actuator signals, and the entire process from sensors to actuators (motors) should be learned without dividing into functional modules. A NN has an ability to optimize the parallel process according to some value or cost function through learning. If training signals are given by humans, they can be constraints for learning. However, RL has an ability to learn autonomously without receiving training signals directly based on trials and errors using a motion-perception feedback loop with the environment.\nFor the higher functions, which are different from the recognition or control, it is difficult to decide either inputs or outputs, and that has disturbed the progress of the research on them. The author is expecting that higher functions also emerge through comprehensive end-to-end RL from sensors to motors using a recurrent NN. In image recognition, the use of deep learning makes the performance better than the conventional approaches that need to design a feature space. In speech recognition, to leave the learning to an RNN makes the performance better than the combination with conventional methods like HMM. They seem to support the author\u2019s direction.\nThe author has thought that what emerges should be called \u201cfunctions\u201d rather than \u201crepresentation\u201d because it is not just a choice of representation, but the result of the processing to get there. Furthermore to be more accurate, the function that emerges cannot be localized clearly in the NN, but it is just a label for us to understand through our consciousness."}, {"heading": "3 Functions that Emerge through Reinforcement Learning (RL)", "text": "Here, functions that have been observed to emerge in a NN through RL in our works are introduced. The NN is trained using the training signals produced automatically from RL; Q-learning, actor-critic or Actor-Q (for learning of both continuous motion and discrete action) based on TD learning. By using actor outputs in actor-critic or actor-Q directly as motion commands, continuous motions have been learned. The convolutional structure is not used except for [20]. The details can be seen in each reference.\n3.1 Static Functions that Emerge in a Layered Neural Network (NN)\nA layered neural network is used and trained according to error backpropagation (BP), and static functions emerge as follows.\n\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6]. That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig. 1 and Fig. 2.\n\u00a7 Color Constancy (Optical Illusion) Motion control of a colored object to the goal location decided by the object color was learned. The top view image covered by a randomlyappearing colored filter was the input. From the internal representation, we tried to explain the optical illusion of color constancy[11]. \u00a7 Sensor Motion (Active Recognition) A pattern image was the input, and from the reward that indicates whether the recognition result is correct or not, both recognition and camera motion for better recognition were acquired through RL[12]. \u00a7 Hand-Eye Coordination and Hand ReachingMovement\nA NN whose inputs were joint angles of an arm and also image on which its hand can be seen, and whose outputs were the joint torques learned to reach the hand to the randomly-located target that could be also seen on the image. No explicit reference trajectory was given. Furthermore, adaptation of force field and its after effect were observed[13]. \u00a7 Explanation of the Brain Activations during Tool Use From the internal representation after learning of a reaching task with variable link length, we tried to explain the emergence of the activities observed in the monkey brain when the monkey used a tool to get a food[14]. \u00a7 Knowledge Transfer between Different Sensors An agent has two kinds of sensors and two kinds of motors. There are four sensor-and-motor combinations. There is a task that could be achieved using either of the combinations. After the agent learned the task using 3 combinations, learning for the remainder sensor-and-motor combination was drastically accelerated[15]. \u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])"}, {"heading": "3.2 Dynamic Functions that Emerge in a Recurrent Neural Network (RNN)", "text": "Here, the emergence of dynamic functions is introduced in which expansion along the time axis is required. In this case, a RNN is used to deal with dynamics, and is trained by BPTT(Back Propagation Through Time) with the training signals generated automatically based on RL. For both acquisition of memory and error propagation, the feedback connection weights are set so that the transition matrix is the identity matrix or close to it when being linearly approximated. \u00a7 Memory There are someworks in which necessary informationwas extracted,memorized and reflected to behaviors after learning almost only from the reward or punishment at each goal. In [16], a very interesting behavior in which if unexpected results occurred, an agent went back to check the state in the previous stage without any direction could be observed. In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned. Purposive associative memory could be also observed[17]. \u00a7 Selective Attention It was learned that based on the previously-presented image, the attended area of the next presented pattern is changed without any special structure for attention and the area of the image was correctly classified[19]. Here, TD-based RL was not used, but learning was done by the reinforcement signal representing only whether the final answer was correct or not. Purposive associative memory could be also observed. \u00a7 Prediction (explained in the next subsection[20]) \u00a7 Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21]. \u00a7 Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23]. \u00a7 Communication (introduced in another paper[24])\nAlthough each function has been examined in a very simple task, it is known that a variety of functions emerge based on extraction and memory of necessary information using an RNN. However, the emergence of \u201cthinking\u201d or \u201csymbol processing\u201d that needs multi-step state transition was very difficult."}, {"heading": "3.3 Frame-free Function Emergence for AGI (Artificial General Intelligence)", "text": "As mentioned before, through end-to-end RL from sensors to motors, entire process is learned and comprehensive function is acquired. For example, in the learning of [20], an agent who has a visual sensor and an RNN as shown in Fig. 3 learned both motion and capture timing of a moving object that sometimes becomes invisible randomly.\nIn a general approach, the object motion is estimated from some frames of image using some givenmodel, the future object location is predicted, the capture point and time are decided by some optimizationmethod, a reference trajectory is derived from the capture point, and the motions are controlled to follow the trajectory.\nFig. 4 shows four examples after RL based on the reward given for object capture. The agent did not know in advance the way to predict the motion or even the fact that prediction is necessary to catch it. Nevertheless, it moved to the very front of its range of motion, waited the object, and when the object came to close, the agent moved backward with it and caught it. Though the object became invisible or visible again suddenly, the agent could behave appropriately. Since the moving direction of the object changed sometimes when it was invisible during learning, the agent learned to wait close to the center (y = 1.5) where it can react the unexpected object motion. As shown in case 4, when the object changed its direction unexpectedly, the agent could catch it though the timing is a bit later than the case of expected motion (case 3)."}], "references": [{"title": "Playing Atari with Deep Reinforcement Learning,NIPS", "author": ["V. Minh", "K Kavukcuoglu"], "venue": "Deep Learning Workshop", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Minh", "K. Kavukcuoglu", "D Silver"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree search,Nature", "author": ["D. Silver", "A Huang"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Practical Issues in Temporal Difference", "author": ["G. Tesauro"], "venue": "Learning,Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Reinforcement Learning When Visual Sensory Signals are Directly Given as Inputs", "author": ["K. Shibata", "Y. Okabe"], "venue": "Proc. of ICNN(Int\u2019l Conf. on Neural Networks)97,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Direct-Vision-Based Reinforcement Learning in \u201dGoing to an Target\u201d Task with an Obstacle and with a Variety of Target Sizes", "author": ["K. Shibata", "Y. Okabe", "K. Ito"], "venue": "Proc. of NEURAP(Neural Networks and their Applications)\u201998,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Emergence of Intelligence through Reinforcement Learning with a Neural Network, Advances in Reinforcement Learning, Intech, 99\u2013120", "author": ["K. Shibata"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Acquisition of Box Pushing by Direct-Vision-Based Reinforcement Learning", "author": ["K. Shibata", "M. Iida"], "venue": "Proc. of SICE Annual Conf", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network", "author": ["K. Shibata", "T. Kawano"], "venue": "Adv. in Neuro-Information Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Acquisition of Flexible Image Recognition by Coupling of Reinforcement Learning and a Neural Network", "author": ["K. Shibata", "T. Kawano"], "venue": "SICE J. of Control, Measurement, and System Integration,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Emergence of Color Constancy Illusion through Reinforcement Learning with a Neural Network", "author": ["K. Shibata", "S. Kurizaki"], "venue": "Proc. of ICDL-EpiRob(Int\u2019l Conf. on Developmental Learning & Epigenetic Robotics)2012,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Effect of Force Load in Hand Reaching Movement Acquired by Reinforcement Learning", "author": ["K. Shibata", "K. Ito"], "venue": "Proc. of ICONIP(Int\u2019l Conf. on Neural Information", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Hidden Representation after Reinforcement Learning of Hand Reaching Movement with Variable Link Length", "author": ["K. Shibata", "K. Ito"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Spatial Abstraction and Knowledge Transfer in Reinforcement Learning Using a Multi-Layer Neural Network", "author": ["K. Shibata"], "venue": "Proc. of ICDL (5th Int\u2019l Conf. on Development and Learning)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Contextual Behavior and Internal Representations Acquired by Reinforcement Learning with a Recurrent Neural Network", "author": ["H. Utsunomiya", "K. Shibata"], "venue": "Adv. in Neuro-Information Processing, Lecture Notes in Comp. Sci., Proc. of ICONIP", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Discovery of Pattern Meaning from Delayed Rewards by Reinforcement Learning with a Recurrent Neural Network", "author": ["K. Shibata", "H. Utsunomiya"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Acquisition of Context-Based Active Word Recognition by Q-Learning Using a Recurrent Neural Network, Robot", "author": ["A.A.M. Faudzi", "K. Shibata"], "venue": "Intelligent Technology and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "2004)Dynamics of a RecurrentNeural NetworkAcquired throughLearning of a Context-basedAttention Task", "author": ["K. Shibata", "M. Sugisaka"], "venue": "Artificial Life and Robotics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning", "author": ["K. Shibata", "K. Goto"], "venue": "Proc. of ICDL-Epirob (Int\u2019l Conf. on Developmental Learning & Epigenetic Robotics)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "A model to explain the emergence of reward expectancy neurons using reinforcement learning and neural network,Neurocomputing", "author": ["Ishii S", "M. Shidara", "K. Shibata"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Learning of Deterministic Exploration and Temporal Abstraction in Reinforcement Learning", "author": ["K. Shibata"], "venue": "Proc. of SICE-ICCAS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Acquisition of Deterministic Exploration and Purposive Memory through Reinforcement Learning with a Recurrent Neural Network", "author": ["K. Goto", "K. Shibata"], "venue": "Proc. of SICE Annual Conf. 2010,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Communications that Emerge through Reinforcement Learning Using a Neural Network, RLDM2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 57, "endOffset": 63}, {"referenceID": 1, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "Under such circumstances, the origin of the end-to-end RL can be found in the Tesauro\u2019s work called TD-gammon[4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 160, "endOffset": 166}, {"referenceID": 5, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 160, "endOffset": 166}, {"referenceID": 6, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 249, "endOffset": 252}, {"referenceID": 18, "context": "The convolutional structure is not used except for [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "(2003)[8] A layered neural network is used and trained according to error backpropagation (BP), and static functions emerge as follows.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 28, "endOffset": 34}, {"referenceID": 4, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 158, "endOffset": 164}, {"referenceID": 5, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 158, "endOffset": 164}, {"referenceID": 7, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 126, "endOffset": 133}, {"referenceID": 9, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 126, "endOffset": 133}, {"referenceID": 10, "context": "From the internal representation, we tried to explain the optical illusion of color constancy[11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Furthermore, adaptation of force field and its after effect were observed[13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "\u00a7 Explanation of the Brain Activations during Tool Use From the internal representation after learning of a reaching task with variable link length, we tried to explain the emergence of the activities observed in the monkey brain when the monkey used a tool to get a food[14].", "startOffset": 271, "endOffset": 275}, {"referenceID": 13, "context": "After the agent learned the task using 3 combinations, learning for the remainder sensor-and-motor combination was drastically accelerated[15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 0, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 1, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 2, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 8, "context": "(2008)[9]", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "In [16], a very interesting behavior in which if unexpected results occurred, an agent went back to check the state in the previous stage without any direction could be observed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Purposive associative memory could be also observed[17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "\u00a7 Selective Attention It was learned that based on the previously-presented image, the attended area of the next presented pattern is changed without any special structure for attention and the area of the image was correctly classified[19].", "startOffset": 236, "endOffset": 240}, {"referenceID": 18, "context": "\u00a7 Prediction (explained in the next subsection[20]) \u00a7 Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "\u00a7 Prediction (explained in the next subsection[20]) \u00a7 Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].", "startOffset": 317, "endOffset": 321}, {"referenceID": 20, "context": "\u00a7 Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].", "startOffset": 172, "endOffset": 180}, {"referenceID": 21, "context": "\u00a7 Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].", "startOffset": 172, "endOffset": 180}, {"referenceID": 22, "context": "\u00a7 Communication (introduced in another paper[24]) Although each function has been examined in a very simple task, it is known that a variety of functions emerge based on extraction and memory of necessary information using an RNN.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "For example, in the learning of [20], an agent who has a visual sensor and an RNN as shown in Fig.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "(2013)[20]", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "(2013)[20] In a general approach, the object motion is estimated from some frames of image using some givenmodel, the future object location is predicted, the capture point and time are decided by some optimizationmethod, a reference trajectory is derived from the capture point, and the motions are controlled to follow the trajectory.", "startOffset": 6, "endOffset": 10}], "year": 2017, "abstractText": "Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author\u2019s group has propounded this framework for around 20 years and already has shown a variety of functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing. \u201cFunction Modularization\u201d approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. \u201cState space\u201d or \u201caction space\u201d generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them. The functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.", "creator": "LaTeX with hyperref package"}}}