{"id": "1703.02721", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "On Approximation Guarantees for Greedy Low Rank Optimization", "abstract": "We offer new approximation guarantees for greedy low-grade matrix estimates under the usual assumptions of limited strong convexity and smoothness. Our novel analysis also reveals previously unknown relationships between low-grade estimates and combinatorial optimization, so that our limits are reminiscent of corresponding approximation limits at submodular maximization. In addition, we also offer statistical recovery guarantees. Finally, we present an empirical comparison of greedy estimates with established baselines on two important real-world problems.", "histories": [["v1", "Wed, 8 Mar 2017 06:20:10 GMT  (1180kb,D)", "http://arxiv.org/abs/1703.02721v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["rajiv khanna", "ethan r elenberg", "alexandros g dimakis", "joydeep ghosh", "sahand negahban"], "accepted": true, "id": "1703.02721"}, "pdf": {"name": "1703.02721.pdf", "metadata": {"source": "CRF", "title": "On Approximation Guarantees for Greedy Low Rank Optimization", "authors": ["Rajiv Khanna", "Ethan R. Elenberg", "Alexandros G. Dimakis", "Sahand Negahban"], "emails": ["rajivak@utexas.edu,", "elenberg@utexas.edu,", "dimakis@austin.utexas.edu", "sahand.negahban@yale.edu"], "sections": [{"heading": "1 Introduction", "text": "Low rank matrix estimation stands as a major tool in modern dimensionality reduction and unsupervised learning. The singular value decomposition can be used when the optimization objective is rotationally invariant to the parameters. However, if we wish to optimize over more complex objectives we must choose to either optimize over the non-convex space (which have seen recent theoretical success) [1, 2, 3, 4, 5] or rely on convex relaxations to the non-convex optimization [6, 7, 8].\nMore concretely, in the low-rank matrix optimization problem we wish to solve\narg max \u0398\n`(\u0398) s.t. rank(\u0398) \u2264 r, (1)\nand rather than perform the computationally intractable optimization above researchers have studied convex relaxations of the form\narg max \u0398\n`(\u0398)\u2212 \u03bb|||\u0398|||nuc.\nUnfortunately, the above optimization can be computationally taxing. General purpose solvers for the above optimization problem that rely on semidefinite programming require \u0398(n3d3) computation, which is prohibitive. Gradient descent techniques require \u0398( \u22121/2(n3 +d3)) computational cost for an epsilon accurate solution. This improvement is sizeable in comparison to SDP solvers. Unfortunately, it is still prohibitive for large scale matrix estimation.\nTo alleviate some of the computational issues an alternate vein of research has focused on directly optimizing the non-convex problem in (1). To that end, authors have studied the convergence properties of\narg max U\u2208Rn\u00d7r,V\u2208Rd\u00d7r\n`(UVT ).\nar X\niv :1\n70 3.\n02 72\n1v 1\n[ st\nat .M\nL ]\nSolving the problem above automatically forces the solution to be low rank and recent results have shown promising behavior. An alternative approach is to optimize via rank one updates to the current estimate [9, 10]. This approach has also been studied in more general contexts such as boosting [11], coordinate descent [12, 13], and incremental atomic norm optimization [14, 15, 16, 17]."}, {"heading": "1.1 Set Function Optimization and Coordinate Descent", "text": "The perspective that we take is treating low rank matrix estimation as a set optimization over an infinite set of atoms. Specifically, we wish to optimize\narg max {X1,...Xk}\u2208A ` ( k\u2211 i=1 \u03b1iXi ) ,\nwhere the set of atoms A is the set of all rank one matrices with unit operator norm. This settings is analogous to that taken in the results studying atomic norm optimization, coordinate descent via the norm in total variation, and Frank-Wolfe style algorithms for atomic optimization. This formulation allows us to connect the problem of low rank matrix estimation to that of submodular set function optimization, which we discuss in the sequel. Before proceeding we discuss related work and an informal statement of our result."}, {"heading": "1.2 Informal Result and Related Work", "text": "Our result demonstrates an exponential decrease in the amount of error suffered by greedily adding rank one matrices to the low rank matrix approximation.\nTheorem 1 (Approximation Guarantee, Informal). If we let \u0398k be our estimate of the rank r matrix \u0398\u2217 at iteration k, then for some universal constant c related to the restricted condition number of the problem we have\n`(\u0398k)\u2212 `(0) \u2265 (1\u2212 exp(\u2212ck/r))(`(\u0398\u2217)\u2212 `(0)).\nNote that after k iterations the matrix \u0398k will be at most rank k. Now, we can contrast this result to related work.\nRelated work: There has been a wide array of studies looking at the computational and statistical benefits of rank one updates to estimating a low rank matrix. At its most basic, the singular value decomposition will keep adding rank one approximations through deflation steps. Below we discuss a few of the results.\nThe work can be generally segmented into to sets of results. Those results that present sublinear rates of convergence and those that obtain linear rates. Interestingly, parallel lines of work have also demonstrated similar convergence bounds for more general atomic or dictionary element approximations [11, 14, 15, 16]. For space constraints, we will summarize these results into two categories rather than explicitly state the results for each individual paper.\nIf we define the atomic norm of a matrix M \u2208 Rn\u00d7d to be |||M|||nuc to be the sum of the singular values of that matrix, then the bounds establish in the sublinear convergence cases behave as\n`(\u0398\u2217)\u2212 `(\u0398k) \u2264 |||\u0398\u2217|||2nuc\nk ,\nwhere we take \u0398\u2217 to be the best rank k solution. What we then see is convergence towards the optimal bound. However, we expect our statistical error to behave as r(n+ d)/n where n is the number of samples that we have received from our statistical model and \u0398\u2217 is rank r [7, 8]. We can take |||\u0398\u2217|||nuc \u2248 r, which would then imply that we would need k to behave as n/(n+ d). However, that would then imply that the rank of our matrix should grow linearly in the number of observations in order to achieve the same statistical error bounds. The above error bounds are \u201cfast.\u201d If we consider a model that yields slow error bounds, then we expect the error to behave like |||\u0398\u2217|||nuc \u221a n+d n . In that case, we can take\nk \u2265 |||\u0398\u2217|||nuc \u221a n n+d , which looks better, but still requires significant growth in k as a function of n.\nTo overcome the above points, some authors have aimed to study similar greedy algorithms that then enjoy exponential rates of convergence as we show in our paper. These results share the most similarities with our own and behave as\n`(\u0398k) \u2265 (1\u2212 \u03b3k)`(\u0398\u2217)\nwhere \u0398\u2217 is the best over all set of parameters. This result decays exponentially. However, when one looks at the behavior of \u03b3 it will typically act as exp (\u22121/min(n,d)), for an n\u00d7 d matrix. As a result, we would need to take k on the order of the number of the dimensionality of the problem in order to begin to see gains. In contrast, for our result listed above, if we seek to only compare to the best rank r solution, then the gamma we find is \u03b3 = exp (\u22121/r). Of course, if we wish to find a solution with full-rank, then the bounds we stated above match the existing bounds.\nIn order to establish our results we rely on a notion introduced in the statistical community called restricted strong convexity. This assumption has connections to ideas such as the Restricted Isometry Property, Restricted Eigenvalue Condition, and Incoherence. In the work by Shalev-Shwartz, Gonen, and Shamir [9] they present results under a form of strong convexity condition imposed over matrices. Under that setting, the authors demonstrate that\n`(\u0398k) \u2265 `(\u0398\u2217)\u2212 `(0)r\nk\nwhere r is the rank of \u0398\u2217. In contrast, our bound behaves as\n`(\u0398k) \u2265 `(\u0398\u2217) + (`(\u0398\u2217)\u2212 `(0)) exp (\u2212k/r)\nOur contributions: We improve upon the linear rates of convergence for low-rank approximation using rank one updates by connecting the coordinate descent problem to that of submodular optimization. We present this result in the sequel along with the algorithmic consequences. We demonstrate the good performance of these rank one updates in the experimental section."}, {"heading": "2 Background", "text": "We begin by fixing some notation. We represent sets using sans script fonts e.g. A,B. Vectors are represented using lower case bold letters e.g. x,y, and matrices are represented using upper case bold letters e.g. X,Y. Non-bold face letters are used for scalars e.g. j,M, r and function names e.g. f(\u00b7). The transpose of a vector or a matrix is represented by > e.g. X>. Define [p] := {1, 2, . . . , p}. For singleton sets, we write f(j) := f({j}). Size of a set S is denoted by |S|. \u3008\u00b7, \u00b7\u3009 is used for matrix inner product.\nOur goal is to analyze greedy algorithms for low rank estimation. Consider the classic greedy algorithm that picks up the next element myopically i.e. given the solution set built so far, the algorithm picks the next element as the one which maximizes the gain obtained by adding the said element into the solution set. Approximation guarantees for the greedy algorithm readily imply for the subclass of functions called submodular functions which we define next.\nDefinition 1. A set function f(\u00b7) : [p]\u2192 R is submodular if for all A,B \u2286 [p],\nf(A) + f(B) \u2265 f(A \u222a B) + f(A \u2229 B).\nSubmodular set functions are well studied and have many desirable properties that allow for efficient minimization, and maximization with approximation guarantees. Our low rank estimation problem also falls under the purview of another class of functions called monotone functions. A function is called monotone if and only if f(A) \u2264 f(B) for all A \u2286 B. For the specific case of maximizing monotone submodular set functions, it is known that the greedy algorithm run for (say) k iterations is guaranteed to return a solution that is within (1\u2212 1/e) of the optimum set of size k [18]. Moreover, without further assumptions or knowledge of the function, no other polynomial time algorithm can provide a better approximation guarantee unless P=NP [19].\nMore recently, a line of works have shown that the greedy approximation guarantee that is typically applicable to monotone submodular functions can be extended to a larger class of functions called weakly submodular functions [20, 21]. Central to the notion of weak submodularity is the quantity submodularity ratio which we define next.\nDefinition 2 (Submodularity Ratio [22]). Let S, L \u2282 [p] be two disjoint sets, and f(\u00b7) : [p]\u2192 R. The submodularity ratio of L with respect to S is given by\n\u03b3L,S := \u2211 j\u2208S [f(L \u222a {j})\u2212 f(L)] f(L \u222a S)\u2212 f(L) . (2)\nThe submodularity ratio of a set U with respect to an integer k is given by\n\u03b3U,k := min L,S:L\u2229S=\u2205, L\u2286U,|S|\u2264k \u03b3L,S. (3)\nIt is easy to show that f(\u00b7) is submodular if and only if \u03b3L,S \u2265 1 for all sets L and S. However, as noted by Das and Kempe [22], Elenberg et al. [20], an approximation guarantee is guaranteed when 0 < \u03b3L,S \u2200L,S, thereby extending the applicability of the greedy algorithm to a much larger class of functions. The subset of monotone functions which have \u03b3L,S > 0 \u2200L,S are called weakly submodular functions in the sense that even though the function is not submodular, it still provides provable bound for greedy selections.\nAlso vital to our analysis is the notion of restricted strong concavity and smoothness [23, 24].\nDefinition 3 (Low Rank Restricted Strong Concavity, Restricted Smoothness). A function ` : Rn\u00d7d \u2192 R is said to be restricted strong concave with parameter m\u2126 and restricted smooth with parameter M\u2126 if for all X,Y \u2208 \u2126 \u2282 Rn\u00d7d,\n\u2212m\u2126 2 \u2016Y \u2212X\u20162F \u2265 `(Y)\u2212 `(X)\u2212 \u3008\u2207`(X),Y \u2212X\u3009\n\u2265 \u2212M\u2126 2 \u2016Y \u2212X\u20162F .\nRemark 1. If a function `(\u00b7) has restricted strong concavity parameter m, then its negative \u2212`(\u00b7) has restricted strong convexity parameter m. We choose to use the nomenclature of concavity for ease of exposition in terms of relationship to submodular maximization. Further, note that we define RSC/RSM conditions on the space of matrices rather than vectors.\nIt is straightforward to see that if \u2126\u2032 \u2286 \u2126, then M\u2126\u2032 \u2264M\u2126 and m\u2126\u2032 \u2265 m\u2126."}, {"heading": "3 Setup", "text": "In this section, we delineate our setup of low rank estimation. For the sake of convenience of relating to the framework of weak submodular maximization, we operate in the setting of maximization of a concave matrix variate function under a low rank constraint. This is equivalent to minimizing a convex matrix variate function under the low rank constraint as considered by Shalev-Shwartz et al. [9] or under nuclear norm constraint or regularization as considered by Jaggi and Sulovsky\u0301 [13]. The goal is to maximize a function l : Rn\u00d7d \u2192 R under a low rank constraint:\nmax rank(X)\u2264r `(X). (4)\nInstead of using a convex relaxation of the constrained problem (4), our approach is to enforce the rank constraint directly by adding rank 1 matrices greedily until X is of rank k. The rank 1 matrices to be added are obtained as outer product of vectors from the given vector sets U and V . e.g. U := {x \u2208 Rn s.t. \u2016x\u20162 = 1} and V := {x \u2208 Rd s.t. \u2016x\u20162 = 1}.\nThe problem (4) can be interpreted in a context of sparsity as long as U and V are enumerable. For example, by using the SVD theorem, it is known that we can rewrite X as \u2211k i=1 \u03b1iuiv > i , where \u2200i, ui \u2208 U and vi \u2208 V . By using enumeration of the sets U and V under a finite precision representation of real values, one can rethink of the optimization (4) as finding a sparse solution for the infinite dimensional vector \u03b1 [9]. As a first step, we can define an optimization over specified support sets, similar to choosing support for classical sparsity in vectors. For a support set L,\nlet UL and VL be the matrices formed by stacking the chosen elements of U and V respectively. For the support L, we can define a set function that maximizes `(\u00b7) over L:\nf(L) = max H\u2208R|L|\u00d7|L|\n`(U>L HVL)\u2212 `(0). (5)\nWe will denote the optimizing matrix for a support set L as B(L). In other words, let H\u0302L be the argmax obtained in (5), then B(L) := U>L H\u0302LVL.\nThus, the low rank matrix estimation problem (4) can be reinterpreted as the following equivalent combinatorial optimization problem:\nmax |S|\u2264k f(S). (6)"}, {"heading": "3.1 Algorithms", "text": "We briefly state the algorithms. Our greedy algorithm is illustrated in Algorithm 1. The greedy algorithm builds the support set incrementally \u2013 adding a rank 1 matrix one at a time, so that at iteration i for 1 \u2264 i \u2264 k the size of the chosen support set and hence rank of the current iterate is i. We assume access to a subroutine GreedySel for the greedy selection (Step 4). This subroutine solves an inner optimization problem by calling a subroutine GreedySel which returns an atom s from the candidate support set that ensures\nf(SGi\u22121 \u222a {s})\u2212 f(SGi\u22121) \u2265 \u03c4 ( f(SGi\u22121 \u222a {s?})\u2212 f(SGi\u22121) ) ,\nwhere\ns? \u2190 arg max a\u2208(U\u00d7V)\u22a5SGi\u22121 f(SGi\u22121 \u222a {a})\u2212 f(SGi\u22121).\nIn words, the subroutine GreedySel ensures that the gain in f(\u00b7) obtained by using the selected atom is within \u03c4 \u2208 (0, 1] multiplicative approximation to the atom with the best possible gain in f(\u00b7). The hyperparameter \u03c4 governs a tradeoff allowing a compromise in myopic gain for a possibly quicker selection.\nThe greedy selection requires to fit and score every candidate support, which is prohibitively expensive. An alternative is to choose the next atom by using the linear maximization oracle used by Frank-Wolfe [12] or Matching Pursuit algorithms [14]. This step replaces Step 4 of Algorithm 1 as illustrated in Algorithm 2. Let L = SOi\u22121 be the set constructed by the algorithm at iteration (i\u2212 1). The linear oracle OMPSel returns an atom s for iteration i ensuring\n\u3008\u2207`(B(L)),usv>s \u3009 \u2265 \u03c4 max (u,v)\u2208(U\u00d7V)\u22a5SOi\u22121 \u3008\u2207`(B(L)),uv>\u3009.\nThe linear problem OMPSel can be considerably faster that GreedySel. OMPSel reduces to finding the left and right singular vectors of\u2207`(B(L)) corresponding to its largest singular value, which is O( t1\u2212\u03c4 (log n+ log d)), where t is the number of non-zero entries of\u2207`(B(L)).\nAlgorithm 2 is the same as considered by Shalev-Shwartz et al. [9] as GECO (Greedy Efficient Component Optimization). However, as we shall see, our analysis provides stronger bounds than their Theorem 2.\nAlgorithm 1 GREEDY(U , V , k, \u03c4 ) 1: Input: sparsity parameter k, vector sets U , V 2: SG0 \u2190 \u2205 3: for i = 1 . . . k do 4: s\u2190 GreedySel(\u03c4) 5: SGi \u2190 SGi\u22121 \u222a {s} 6: end for 7: return SGk , B(S G k ), f(SGk ).\nAlgorithm 2 GECO(U , V , k, \u03c4 ) same as Algorithm 1 except\n4: s\u2190 OMPSel(\u03c4)\nRemark 2. We note that Step 5 of Algorithms 1,2 requires solving the RHS of (5) which is a matrix variate problem of size i2 at iteration i. This refitting is equivalent to the \u201cfully-corrective\u201d versions of Frank-Wolfe/Matching Pursuit algorithms which, intuitively speaking, extract out all the information w.r.t `(\u00b7) from the chosen set of atoms, thereby ensuring that the next rank 1 atom chosen has row and column space orthogonal to the previously chosen atoms. Thus the constrained maximization on the orthogonal complement of SGi in subroutines OMPSel and GreedySel need not be explicitly enforced, but is still shown for clarity."}, {"heading": "4 Analysis", "text": "In this section, we prove that low rank matrix optimization over the rank one atoms satisfies weak submodularity. This helps us bound the function value obtained till k greedy iterations vis-a-vis the function value at the optimal k sized selection.\nWe explicitly delineate some notation and assumptions. With slight abuse of notation, we assume `(\u00b7) is mi-strongly concave and Mi-smooth over matrices of rank i. For i \u2264 j, note that mi \u2265 mj and Mi \u2264 Mj . Additionally, let \u2126\u0303 := {(X,Y) : rank(X\u2212Y) \u2264 1}, and assume `(\u00b7) is M\u03031-smooth over \u2126\u0303. It is easy to see M\u03031 \u2264M1.\nSince we obtain approximation bounds for the greedy algorithm similar to ones obtained using classical methods, we must also mention the corresponding assumptions in the submodular literature that further draws parallels to our analysis. Submodularity guarantees that greedy maximization of monotone normalized functions yields a (1 \u2212 1/e) approximation. Since we are doing support selection, increasing the support size does not decrease the function value. Hence the set function we consider is monotone. Further, we also subtract `(0) to make sure f(\u2205) = 0 (see (5)) so that our set function is also normalized. We shall see that our bounds are of similar flavor as the classical submodularity bound.\nAs the first step, we prove that if the low rank RSC holds, then the submodularity ratio (Definition 2) is lowerbounded by the inverse condition number.\nTheorem 2. Let L be a set of k rank 1 atoms and S be a set of r rank 1 atoms where we sequentially orthogonalize the atoms against L. If `(\u00b7) is mi-strongly concave over matrices of rank i, and M\u03031-smooth over the set \u2126\u0303 := {(X,Y) : rank(X\u2212Y) = 1}, then\n\u03b3L,r := \u2211 a\u2208S [f(L \u222a {a})\u2212 f(L)] f(L \u222a S)\u2212 f(L) \u2265 mr+k M\u03031 .\nThe proof of Theorem 2 is structured around individually obtaining a lower bound for the numerator and an upper bound for the denominator of the submodularity ratio by exploiting the concavity and convexity conditions. Bounding the submodularity ratio is crucial to obtaining the approximation bounds for the greedy algorithm as we shall see in the sequel."}, {"heading": "4.1 Greedy Improvement", "text": "In this section, we obtain approximation guarantees for Algorithm 1.\nTheorem 3. Let S := SGk be the greedy solution set obtained by running Algorithm 1 for k iterations, and let S? be an optimal support set of size r. Let `(\u00b7) be mi strongly concave on the set of matrices with rank less than or equal to i, and M\u03031 smooth on the set of matrices in the set \u2126\u0303. Then,\nf(S) \u2265 (1\u2212 1 ec1 )f(S?)\n\u2265 (1\u2212 1 ec2 )f(S?),\nwhere c1 = \u03c4\u03b3S,r kr and c2 = \u03c4 mr+k M\u03031 k r .\nThe proof technique for the first inequality of Theorem 3 relies on lower bounding the progress made in each iteration of Algorithm 1. Intuitively, it exploits the weak submodularity to make sure that each iteration makes enough progress, and then applying an induction argument for r iterations. We also emphasize that the bounds in Theorem 3 are for normalized set function f(\u00b7) (which means f(\u2205) = 0). A more detailed proof is presented in the appendix.\nRemark 3. Theorem 3 provides the approximation guarantees for running the greedy selection algorithm up to k iterations to obtain a rank k matrix iterate vis-a-vis the best rank r approximation. For r = k, and \u03c4 = 1, we get an approximation bound (1\u2212 e\u2212m/M) which is reminiscent of the greedy bound of (1\u2212 1/e) under the framework of submodularity. Note that our analysis can not be used to establish classical submodularity. However, establishing weak submodularity that lower bounds \u03b3 is sufficient to provide slightly weaker than classical submodularity guarantees.\nRemark 4. Theorem 3 implies that to obtain (1\u2212 ) approximation guarantee in the worst case, running Algorithm 1 for k = rMm\u03c4 log 1 ) = O(r log\n1/ ) iterations suffices. This is useful when the application allows a tradeoff: compromising on the low rank constraint a little to achieve tighter approximation guarantees.\nRemark 5. Das and Kempe [22] considered the special case of greedily maximizing R2 statistic for linear regression, which corresponds to classical sparsity in vectors. They also obtain a bound of (1\u2212 1/e\u03b3), where \u03b3 is the submodularity ratio for their respective setup. This was generalized by Elenberg et al. [20] to general concave functions under sparsity constraints. Our analysis is for the low rank constraint, as opposed to sparsity in vectors that was considered by them."}, {"heading": "4.2 GECO Improvement", "text": "In this section, we obtain the approximation guarantees for Algorithm 2. The greedy search over the infinitely many candidate atoms is infeasible, especially when \u03c4 = 1. Thus while Algorithm 1 establishes interesting theoretical connections with submodularity, it is, in general, not practical. To obtain a tractable and practically useful algorithm, the greedy search is replaced by a Frank Wolfe or Matching Pursuit style linear optimization which can be easily implemented as finding the top singular vectors of the gradient at iteration i. In this section, we show that despite the speedup, we lose very little in terms of approximation guarantees. In fact, if the approximation factor \u03c4 in OMPSel() is 1, we get the same bounds as those obtained for the greedy algorithm.\nWe now present our main result for Algorithm 2.\nTheorem 4. Let S := SOk be the greedy solution set obtained using Algorithm 2 for k iterations, and let S? be the optimum size r support set. Let `(\u00b7) be mr+k strongly concave on the set of matrices with rank less than or equal to (r + k), and M\u03031 smooth on the set of matrices with rank in the set \u2126\u0303. Then,\nf(S) \u2265 (1\u2212 1 ec3 )f(S?),\nwhere c3 = \u03c42 mr+k M\u03031 k r .\nThe proof of Theorem 4 follows along the lines of Theorem 3. The central idea is similar - to exploit the RSC conditions to make sure that each iteration makes sufficient progress, and then provide an induction argument for r iterations. Unlike the greedy algorithm, however, using the weak submodularity is no longer required. Note that the bound obtained in Theorem 4 is similar to Theorem 3, except the exponent on the approximation factor \u03c4 .\nRemark 6. Our proof technique for Theorem 4 can be applied for classical sparsity to improve the bounds obtained by Elenberg et al. [20] for OMP for support selection under RSC, and by Das and Kempe [22] for R2 statistic. If \u03c4 = 1, r = k, their bounds involve terms of the form O(m2/M2) in the exponent, as opposed to our bounds which only has m/M in the exponent."}, {"heading": "5 Recovery Guarantees", "text": "While understanding approximation guarantees are useful, providing parameter recovery bounds can further help us understand the practical utility of the greedy algorithm. In this section, we present a general theorem that provides us with recovery bounds of the true underlying low rank structure.\nTheorem 5. Suppose that an algorithm achieves the approximation guarantee:\nf(Sk) \u2265 Cr,kf(S?r),\nwhere Sk is the set of size k at iteration k of the algorithm, S?r be the optimal solution for r-cardinality constrained maximization of f(\u00b7), and Cr,k be the corresponding approximation ratio guaranteed by the algorithm. Recall that we represent by US,VS the matrices formed by stacking the vectors represented by the support set S chosen from U ,V respectively, s.t. |S| = r. Then under mk+r RSC, with Br = U>S HVS for any H \u2208 Rr\u00d7r, we have\n\u2016B(Sk) \u2212Br\u20162F \u2264 4(k + r) \u2016\u2207`(Br)\u201622 m2k+r\n+ 4(1\u2212 Cr,k) mk+r [`(Br)\u2212 `(0)]\nTheorem 5 can be applied for Br = B(S ? r), which is the argmax for maximizing `(\u00b7) under the low rank constraint. It is general - in the sense that it can be applied for getting recovery bounds from approximation guarantees for any algorithm, and hence is applicable for both Algorithms 1 and 2.\nFor specific function `(\u00b7) and statistical model, statistical recovery guarantees guarantees can be obtained from Theorem 5 for specific `(\u00b7) and statistical model, Consider the case of low rank matrix estimation from linear measurements. Say Xi \u2208 Rm1\u00d7m2 for i \u2208 [n] are generated so that each entry of Xi is N (0, 1). We observe yi = \u3008Xi,\u0398?\u3009 + \u03b5, where \u0398? is low rank, and say \u03b5 \u223c N (0, \u03c32). Let N = m1m2, and let \u03d5(\u0398) : Rm1\u00d7m2 \u2192 Rn be the linear operator so that [\u03d5(\u0398)]i = \u3008Xi,\u0398\u3009. Our corresponding function is now `(\u0398) = \u2212 1n\u2016y \u2212 \u03d5(\u0398)\u2016 2 2. For this function, using arguments by Negahban et al. [23], we know \u2016\u2207`(BS?r )\u201622 \u2264 logN n and `(B\nS?r )\u2212 `(0) \u2264 (s+ 1) with high probability. It is also straightforward to apply their results to bound mk+r \u2265 ( 1 32 \u2212 162(k+r) logN n ) , and M1 \u2264 1, which gives explicit bounds as per Theorem 5 for Algorithms 1, 2 for the considered function and the design matrix."}, {"heading": "6 Experiments", "text": "In this section, we empirically evaluate the proposed algorithms."}, {"heading": "6.1 Clustering under Stochastic Block Model", "text": "In this section, we test empirically the performance of GECO (Algorithm 2) for a clustering task. We are provided with a graph with nodes and the respective edges between the nodes. The observed graph is assumed to have been noisily generated from a true underlying clustering. The goal is to recover the underlying clustering structure from the noisy graph provided to us. The adjacency matrix of the true underlying graph is low rank. As such, our greedy framework is applicable . We compare performance of Algorithm 2 on simulated data against standard baselines of spectral clustering which are commonly used for this task. We begin by describing a generative model for creating edges between nodes given the ground truth.\nThe Stochastic Block Model is a model to generate random graphs. It takes its input the set of n nodes, and a partition of [n] which form a set of disjoint clusters, and returns the graph with nodes and the generated edges. The model is also provided with generative probabilities (p, q) \u2013 so that a pair of nodes within the same cluster have an edge between them with probability p, while a pair of nodes belonging to different clusters have an edge between them with probability q. For simplicity we assume q = (1\u2212 p). The model then iterates over each pair of nodes. For each such\npair that belongs to same cluster, it samples an edge as Bernoulli(p), otherwise as Bernoulli(1\u2212 p). This provides us with a {0, 1} adjacency matrix.\nFor baselines, we compare against two versions of spectral clustering, which is a standard technique applied to find communities in a graph. The method takes as input the n\u00d7 n adjacency matrix A, which is a {0, 1} matrix with an entry Aij = 1 if there is an edge between node i and j, and is 0 otherwise. From the adjacency matrix, the graph Laplacian L is constructed. The Laplacian may be unnormalized, in which case it is simply L = D \u2212A, where D is the diagonal matrix of degrees of nodes. A normalized Laplacian is computed as Lnorm = D\u2212\n1/2LD\u22121/2. After calculating the Laplacian, the algorithm solves for bottom k eigenvectors of the Laplacian, and then apply k-means clustering on the rows of the thus obtained eigenvector matrix. We refer to the works of Shi and Malik [25], Ng et al. [26] for the specific details of clustering algorithms using unnormalized and normalized graph Laplacian respectively.\nWe compare the spectral clustering algorithms with logistic PCA, which is a special case of the exponential family PCA [27]. The exponential family extension of classical PCA is analogous to the extension of the linear regression to generalized linear models (GLMs). For a given matrix X, the GLM generative model assumes that each cell Xij is independently drawn with likelihood proportional to exp \u3008\u0398ij ,Xij\u3009 \u2212G(\u0398ij), where \u0398 is the true underlying parameter, and G(\u00b7) is the partition function. It is easy to see we can apply our framework of greedy selection by defining `(\u00b7) as the log-likelihood:\n`(\u0398) = \u3008\u0398,X\u3009 \u2212 \u2211 i,j G(\u0398ij),\nwhere \u0398 is the true parameter matrix of p and q that generates a realization of A. Since the true \u0398 is low rank, we get the low rank constrained optimization problem:\nmax rank(\u0398)\u2264k `(\u0398),\nwhere k is the hyperparameter that is suggestive of true number of clusters. Note that lack of knowledge of true value of k is not more restrictive than spectral clustering algorithms which typically also require the true value of k, albeit some subsequent works have tried to address tuning for k.\nHaving cast the clustering problem in the same form as (4), we can apply our greedy selection algorithm as opposed to the more costly alternating minimizing algorithms suggested by Collins et al. [27]. Since the given matrix is {0, 1} with each entry sampled from a Bernoulli, we use G(x) = log(1 + ex) which gives us logistic PCA.\nWe generate the data as follows. For n = 100 nodes, and fixed number of cluster k = 5, we vary the within cluster edge generation probability p from 0.55 to 0.95 in increments of 0.05, and use the Stochastic Block model to generate a noisy graph with each p. Note that smaller p means that the sampled graph will be more noisy and likely to be more different than the underlying clustering.\nWe compare against the spectral clustering algorithm using unnormalized Laplacian of Shi and Malik [25] which we label \u201cSpectral unnorm{k}\u201d for k = {3, 5, 10}, and the spectral clustering algorithm using normalized Laplacian of Ng et al. [26] which we label \u201cSpectral norm{k}\u201d for k = {3, 5, 10}. We use Algorithm 2 which we label \u201cGreedy{k}\u201d for k = {3, 5, 10}. For each of these models, the referred k is the supplied hyperparameter. We report the least squares error of the output from each model to the true underlying \u0398 (which we call the generalization error), and to the instantiation used for training X (which we call the reconstruction error). The results are presented in Figure 1.\nFigure 1 shows that the greedy logistic PCA performs well in not only re-creating the given noisy matrix (reconstruction) but also captures the true low rank structure better (generalization). Further, note that providing the true hyper parameter k is vital for spectral clustering algorithms, while on the other hand greedy is less sensitive to k which is very useful in practice as k is typically not known. So the spectral clustering algorithms typically would involve taking an SVD and re-running the k \u2212means for different values of k to choose the best performing hyperparameter. The greedy factorization on the other hand is more robust, and moreover is incremental - it does not require to be re-run from scratch for different values of k."}, {"heading": "6.2 Word Embeddings", "text": "The task of embedding text into a vector space yields a representation that can have many advantages, such as using them as features for subsequent tasks as sentiment analysis. Mikolov et al. [28] proposed a context-based embedding\ncalled skip-gram or word2vec). The context of a word can be defined as a set of words before, around, or after the respective word. Their model strives to find an embedding of each word so that the representation predicts the embedding of each context word around it. In a recent paper, Levy and Goldberg [29] showed that the word embedding model proposed by Mikolov et al. [28] can be re-interpreted as matrix factorization of the PMI matrix constructed as follows. A word c is in context of w if it lies within the respective window of w. The PMI matrix is then calculated as\nPMIw,c = log ( p(w, c)\np(w)p(c)\n) .\nIn practice the probabilities p(w, c), p(w), p(c) are replaced by their empirical counterparts. Further, note that p(w, c) is 0 if words c and w do not co-exist in the same, context which yields \u2212\u221e for PMI. Levy and Goldberg [29] suggest using an alternative: PPMIw,c = max{PMIw,c, 0}. They also suggest variations of PMI hyper parameterized by k which corresponds to the number of negative samples in the training of skip gram model of Mikolov et al. [28].\nWe employ the binomial model on the normalized count matrix (instead of the PMI), in a manner similar to the clustering approach in Section 6.1. The normalized counts matrix is calculated simply as p(w,c)p(w) , without taking explicit logs unlike the PMI matrix. This gives us a probability matrix which has each entry between 0 and 1, which can be factorized under the binomial model greedily as per Algorithm 2, similar to the way we do it in Section 6.1.\nWe note that embeddings using the SVD is more scalable than our greedy approach because of advancements in linear algebraic techniques for SVD on sparse matrices that PPMI yields. Our experiments show that binomial PCA can be competitive to other existing embedding methods. Since our current implementation is not as scalable, we are further investigating this as on-going work.\nWe empirically study the embeddings obtained by binomial factorization on two tasks - word similarity and analogies. For word similarity, we use the W353 dataset [30] which has 353 queries and the MEN data [31] which has 3000 queries. Both these datasets contain words with human assigned similarity scores. We evaluate the embeddings by their cosine similarity, and measuring the correlation with the available human ratings. For the analogy task, we use the Microsoft Research (MSR) syntactic analogies [32] which has 8000 queries, and the Google mixed analogies dataset [33] with 19544 queries. To compute accuracy, we use the multiplication similarity metric as used by Levy and Goldberg [29]. To train the word embeddings, we use the 2013 news crawl datasethttp://www.statmt.org/wmt14/trainingmonolingual-news-crawl. We filter out stop words and non-ascii characters, and keep only the words which occurs atleast 2000 times which yields vocabulary of 6713. Note that since we filter only the most common words, several queries from the datasets are invalid because we do not have embeddings for words appearing in them. However, we do\ninclude them and report the overall average over the entire dataset, with metric being 0 by default for each query we are not able to process.\nTable 1 shows the empirical evaluation. SVD and PPMI are the models proposed by Levy and Goldberg [29], while SGNS is skipgram with negative sampling model of Mikolov et al. [28]. We run each of these for k = {5, 10, 15, 20} and report the best numbers. The results show that alternative factorizations such as our application of binomial PCA to those of taking SVD of the PPMI matrix can be more consistent and competitive with other embedding methods."}, {"heading": "7 Conclusion", "text": "We have connected the problem of greedy low-rank matrix estimation to that of submodular optimization. Through that connection we have provided improved exponential rates of convergence for the algorithm. An interesting area of future study will be to connect these ideas to general atoms or dictionary elements."}, {"heading": "A Supplement", "text": "In this section, we provide the missing proofs.\nA.1 Proof of Theorem 2 Proof. An important aspect of the assumptions is that the space of atoms spanned by S is orthogonal to the span of L. Furthermore, span(L \u222a S) \u2283 span(S). Let k\u0304 = k + r. We will first upper bound the denominator in the submodularity ratio. From strong concavity,\nmk\u0304 2 \u2016B(L\u222aS) \u2212B(L)\u20162F \u2264 `(B(L))\u2212 `(B(L\u222aS)) + \u3008\u2207`(B(L)),B(L\u222aS) \u2212B(L)\u3009\nRearranging\n0 \u2264 `(B(L\u222aS))\u2212 `(B(L)) \u2264 \u3008\u2207`(B(L)),B(L\u222aS) \u2212B(L)\u3009 \u2212 mk\u0304 2 \u2016B(L\u222aS) \u2212B(L)\u20162F\n\u2264 arg max X:\nX=UL\u222aSHVL\u222aS H\u2208R|L\u222aS|\u00d7|L\u222aS|\n\u3008\u2207`(B(L)),X\u2212B(L)\u3009 \u2212 mk\u0304 2 \u2016X\u2212B(L)\u20162F\n= arg max X:\nX=UL\u222aSHVL\u222aS H\u2208R|L\u222aS|\u00d7|L\u222aS|\n\u3008PUS(\u2207`(B(L)))PVS ,X\u2212B(L)\u3009 \u2212 mk\u0304 2 \u2016X\u2212B(L)\u20162F ,\nwhere the last equality holds because \u3008(\u2207`(B(L))), PULXPVL \u2212B(L)\u3009 = 0. Solving the argmax problem, we get X = B(L) + 1mk\u0304 PUS(\u2207`(B(L)))PVS . Plugging in, we get,\n`(B(L\u222aS))\u2212 `(B(L)) \u2264 1 2mk\u0304 \u2016PUS(\u2207`(B(L)))PVS\u20162F\nWe next bound the numerator. Recall that the atoms in S are orthogonal to each other i.e. US and VS are both orthonormal.\nFor clarity, we define the shorthand, B(L\u222aS)ij = \u3008uiv>j ,B(L\u222aS)\u3009uiv>j , for i, j \u2208 [|L \u222a S|]. With an arbitrary i \u2208 S, and arbitrary scalars \u03b1ii, \u03b1ij , \u03b1ji for j \u2208 L,\n`(B(L\u222a{i}))\u2212 `(B(L)) \u2265 `(B(L) + \u03b1iiB(L\u222aS)ii + \u2211 j\u2208L \u03b1ijB (L\u222aS) ij + \u2211 j\u2208L \u03b1jiB (L\u222aS) ji )\u2212 `(B (L))\n\u2265 \u3008\u2207`(B(L)), \u03b1iiB(L\u222aS)ii + \u2211 j\u2208L \u03b1ijB (L\u222aS) ij + \u2211 j\u2208L \u03b1jiB (L\u222aS) ji \u3009\n\u2212 M\u03031 2 \u03b12ii\u2016B(L\u222aS)ii \u20162F +\u2211 j\u2208L \u03b12ij\u2016B (L\u222aS) ij \u2016 2 F + \u2211 j\u2208L \u03b12ji\u2016B (L\u222aS) ji \u2016 2 F  . \u2265 \u3008\u2207`(B (L)),B (L\u222aS) ii \u30092\n2M\u03031\u2016B(L\u222aS)ii \u20162F + \u2211 j\u2208L\n( \u3008\u2207`(B(L)),B(L\u222aS)ij \u30092\n2M\u03031\u2016B(L\u222aS)ij \u20162F + \u3008\u2207`(B(L)),B(L\u222aS)ji \u30092 2M\u03031\u2016B(L\u222aS)ji \u20162F\n) ,\nwhere the last inequality follows by setting \u03b1ij = \u3008\u2207`(B(L)),B(L\u222aS)ij \u3009 M\u03031\u2016B(L\u222aS)ij \u20162F\nfor j \u2208 L, and for j = i. Summing up for all i \u2208 S, we get\n\u2211 i\u2208S `(B(L\u222a{i}))\u2212 `(B(L)) \u2265 \u2211 i\u2208S  \u3008\u2207`(B(L)),B(L\u222aS)ii \u30092 2M\u03031\u2016B(L\u222aS)ii \u20162F + \u2211 j\u2208L ( \u3008\u2207`(B(L)),B(L\u222aS)ij \u30092 2M\u03031\u2016B(L\u222aS)ij \u20162F + \u3008\u2207`(B(L)),B(L\u222aS)ji \u30092 2M\u03031\u2016B(L\u222aS)ji \u20162F ) = 1\n2M\u03031 \u2016PUS\u2207`(B(L))PVS\u20162F\nA.2 Proofs for greedy improvement Let SGi be the support set formed by Algorithm 1 at iteration i. Define A(i) := f(S G i )\u2212 f(SGi\u22121) with A(0) = 0 as the greedy improvement. We also define B(i) := f(S\u2217)\u2212 f(SGi ) to be the remaining amount to improve, where S? is the optimum k-sized solution. We provide an auxiliary Lemma that uses the submodularity ratio to lower bound the greedy improvement in terms of best possible improvement from step i.\nLemma 1. At iteration i, the incremental gain of the greedy method (Algorithm 1) is\nA(i+ 1) \u2265 \u03c4\u03b3SGi ,r\nr B(i).\nProof. Let S = SGi . Let S R be the sequential orthogonalization of the atoms in S\u2217 relative to S. Thus,\nrA(i+ 1) \u2265 |SR|A(i+ 1) \u2265 \u03c4 |SR|max a\u2208SR f(S \u222a {a})\u2212 f(S) \u2265 \u03c4 \u2211 a\u2208SR [f(S \u222a {a})\u2212 f(S)]\n\u2265 \u03c4\u03b3S,|SR|[f(S \u222a SR)\u2212 f(S)] \u2265 \u03c4\u03b3S,|SR|B(i)\nNote that the last inequality follows because f(S \u222a SR) \u2265 f(S\u2217). The penultimate inequality follows by the definition of weak submodularity, which applies in this case because the atoms in SR are orthogonal to eachother and are also orthogonal to S.\nUsing Lemma 1, one can prove an approximation guarantee for Algorithm 1.\nA.2.1 Proof of Theorem 3\nProof. From the notation used for Lemma 1, A(i+ 1) = B(i)\u2212B(i+ 1). Let C = \u03c4\u03b3 SG i ,r\nr . From Lemma 1, we have,\nB(i+ 1) \u2264 (1\u2212 C)B(i) \u2264 (1\u2212 C)i+1B(0).\nFrom its definition, B(0) = f(S?)\u2212 f(\u2205). So we get,\n[f(S?)\u2212 f(\u2205)]\u2212 [ f(SGi )\u2212 f(\u2205) ] \u2264 (1\u2212 C)i [f(S?)\u2212 f(\u2205)]\n=\u21d2 [ f(SGi )\u2212 f(\u2205) ] \u2265 (1\u2212 (1\u2212 C)i) [f(S?)\u2212 f(\u2205)] \u2265 ( 1\u2212 1\ne \u03c4\u03b3 SG i ,r k r\n) [f(S?)\u2212 f(\u2205)]\nfrom which the result follows.\nA.3 Proof for GECO bounds Let SOi be the support set selected by the GECO procedure (Algorithm 2) at iteration i. Similar to the section on greedy improvement, we define some notation. Let D(i) := f(SOi ) \u2212 f(SOi\u22121) be the improvement made at step i, and as before we have B(i) = f(S?)\u2212 f(SOi ) be the remaining amount to improve.\nWe prove the following auxiliary lemma which lower bounds the gain after adding the atom selected by the subroutine OMPSelin terms of operator norm of the gradient of the current iterate and smoothness of the function.\nLemma 2. Assume that `(\u00b7) is mi-strongly concave and Mi-smooth over matrices of in the set \u2126\u0303 := {(X,Y) : rank(X\u2212Y) \u2264 1}. Then,\nD(i+ 1) \u2265 \u03c4mr+k rM\u03031 B(i).\nProof. For simplicity, say L = SOi . Recall that for a given support set L, f(L) = `(B (L)) i.e. we denote by B(L) the argmax for `(\u00b7) for a given support set L. Hence, by the optimality of B(L\u222a{i}),\nD(i+ 1) = `(B(L\u222a{i}))\u2212 `(B(L)) \u2265 `(B(L) + \u03b1uv>)\u2212 `(B(L))\nfor an arbitrary \u03b1 \u2208 R, and the vectors u,v selected by OMPSel. Using the smoothness of the `(\u00b7), we get,\nD(i+ 1) \u2265 \u03b1\u3008\u2207`(B(L)),uv>\u3009 \u2212 \u03b12 M\u03031 2\nPutting in \u03b1 = \u03c4 M\u03031 \u2016\u2207`(B(L))\u20162, and by \u03c4 -optimality of OMPSel, we get,\nD(i+ 1) \u2265 \u03c4 2\n2M\u03031 \u2016\u2207`(B(L))\u201622\nLet SR be obtained from after sequentially orthogonalizing S? w.r.t. Si. By definition of the operator norm, we further get,\nD(i+ 1) \u2265 \u03c4 2\n2M\u03031 \u2016\u2207`(B(L))\u201622\n\u2265 \u03c4 2\n2rM\u03031 \u2211 i\u2208SR \u3008uiv>i ,\u2207`(B(L))\u30092\n= \u2016PUSR\u2207`(B (L))PVSR \u2016 2 F \u2265 \u03c4 2mr+k\nrM\u03031\n( `(BL\u222aS R )\u2212 `(B(L)) )\n\u2265 \u03c4 2mr+k\nrM\u03031\n( `(BS ? )\u2212 `(B(L)) )\n= \u03c42mr+k\nrM\u03031 B(i)\nThe proof for Theorem 4 from Lemma 2 now follows using the same steps as for Theorem 3 from Lemma 2.\nA.4 Proof for recovery bounds A.4.1 Proof of Theorem 5\nFor clarity of representation, let C = Cr,k, and for an arbitrary H \u2208 Rr\u00d7r, let Br = U>S HVS, and \u2206 := B(Sr) \u2212Bs. Note that \u2206 has rank atmost (k + r). Recall that by the mk+r RSC (Definition 3),\n`(B(Sk))\u2212 `(Br)\u2212 \u3008\u2207`(Br),\u2206\u3009 \u2264 \u2212mk+r\n2 \u2016\u2206\u20162F .\nFrom the approximation guarantee, we have,\n`(B(Sk))\u2212 `(Br) \u2265 (1\u2212 C)[`(0)\u2212 `(Br)] =\u21d2 `(B(Sk))\u2212 `(Br)\u2212 \u3008\u2207`(Br),\u2206\u3009 \u2265 (1\u2212 C)[`(0)\u2212 `(Br)]\u2212 \u3008\u2207`(Br),\u2206\u3009 =\u21d2 \u2212mk+r 2 \u2016\u2206\u20162F \u2265 (1\u2212 C)[`(0)\u2212 `(Br)]\u2212 \u3008\u2207`(Br),\u2206\u3009\n\u2265 (1\u2212 C)[`(0)\u2212 `(Br)]\u2212 (k + r) 1/2\u2016\u2207`(Br)\u20162\u2016\u2206\u2016F ,\nwhere the last inequality is due to generalized Holder\u2019s inequality. Using 2ab \u2264 ca2 + b 2\nc for any positive numbers a, b, c, we get\nmk+r 2 \u2016\u2206\u20162F \u2264 (k + r) \u2016\u2207`(Br)\u201622 mk+r + mk+r\u2016\u2206\u20162F 4 + (1\u2212 C)[`(Br)\u2212 `(0)],\nwhich completes the proof."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions<lb>of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections<lb>between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of<lb>corresponding approximation bounds in submodular maximization. Additionally, we also provide statistical recovery<lb>guarantees. Finally, we present empirical comparison of greedy estimation with established baselines on two important<lb>real-world problems.", "creator": "LaTeX with hyperref package"}}}