{"id": "1703.02728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Inference in Sparse Graphs with Pairwise Measurements and Side Information", "abstract": "binary markup for the vertices of a graph G to low hamming errors from measurements of noise edges and vertex points. We present new algorithms and a sharp analysis of finite samples for this problem on trees and sparse graphs with poor expansion properties such as hypergrids and ring grids. Our method generalizes and improves on Globerson et al. (2015), which introduced the problem for two-dimensional grids.", "histories": [["v1", "Wed, 8 Mar 2017 06:51:41 GMT  (1875kb,D)", "http://arxiv.org/abs/1703.02728v1", "33 pages"]], "COMMENTS": "33 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dylan j foster", "daniel reichman", "karthik sridharan"], "accepted": false, "id": "1703.02728"}, "pdf": {"name": "1703.02728.pdf", "metadata": {"source": "CRF", "title": "Inference in Sparse Graphs with Pairwise Measurements and Side Information", "authors": ["Dylan J. Foster", "Daniel Reichman", "Karthik Sridharan"], "emails": [], "sections": [{"heading": null, "text": "For trees we provide a simple, efficient, algorithm that infers the ground truth with optimal Hamming error and implies recovery results for all connected graphs. Here, the presence of side information is critical to obtain a non-trivial recovery rate. We then show how to adapt this algorithm to tree decompositions of edge-subgraphs of certain graph families such as lattices, resulting in optimal recovery error rates that can be obtained in a highly efficient manner.\nThe thrust of our analysis is to 1) use the tree decomposition along with edge measurements to produce a small class of viable vertex labelings and 2) apply fast rate results from statistical learning theory to show that we can infer the ground truth from this class using vertex measurements. We show the power of our method by providing several examples including hypergrids, ring lattices, and the Newman-Watts model for small world graphs. For two-dimensional grids, our results improve over Globerson et al. (2015) by obtaining optimal recovery in the constant-height regime."}, {"heading": "1 Introduction", "text": "Statistical inference over graphs and networks is a fundamental problem that has received extensive attention in recent years (Fortunato, 2010; Krzakala et al., 2013; Abbe et al., 2014; Hajek et al., 2014). Typically, these inference problems involve noisy observations of discrete labels assigned to edges of a given network, and our goal is to infer a \u201cground truth\u201d labeling of the vertices (perhaps up to the right sign) that best explains these observations. Such problems occur in a wide range of disciplines including statistical physics, sociology, community detection, average case analysis, and graph partitioning. This inference problem is also related to machine learning tasks involving structured prediction that arise in computer vision, speech recognition and other applications such as natural language processing. Despite the intractability of maximum likelihood estimation, maximum a-posteriori estimation, and marginal inference in network models in the worst case, it has been observed that approximate inference algorithms work surprisingly well in practice (Sontag et al., 2012), and recent work has focused on improving our theoretical understanding of this phenomenon (Globerson et al., 2015).\n\u2217Department of Computer Science, Cornell University. Supported in part by the NDSEG fellowship. \u2020Electrical Engineering and Computer Science, University of California, Berkeley \u2021Department of Computer Science, Cornell University\nar X\niv :1\n70 3.\n02 72\n8v 1\n[ cs\n.L G\n] 8\nM ar\n2 01\nA key feature of the inference model introduced in (Globerson et al., 2015) is that in addition to observing noisy edge labels, one also observes noisy vertex labels. Our main motivation in this paper is to consider the extent to which the addition of noisy vertex observations improves our prospects for approximate recovery. Specifically, we consider the following model:\nModel 1. We receive an undirected graph G = (V,E) whose vertices are labeled according to an unknown ground truth Y \u2208 {\u00b11}V . We receive noisy edge measurements X \u2208 {\u00b11}E , where Xuv = YuYv with probability 1 \u2212 p and Xuv = \u2212YuYv otherwise. We receive vertex measurements Z \u2208 {\u00b11}V , where Zu = Yu with probability 1\u2212 q and Zu = \u2212Yu otherwise. We assume q > p is constant. Our goal is to produce a labeling Y\u0302 \u2208 {\u00b11}V such that with high probability the Hamming error \u2211 v\u2208V 1{Y\u0302v 6= Yv} is bounded by O(f(p)n) where limp\u21920 f(p) = 0.\nAs a concrete example, consider the problem of trying to recover opinions of individuals in social networks. Suppose that every individual in a social network can hold one of two opinions labeled by \u22121 or +1. We receive a measurement of whether neighbors in the network have the same opinion, but the value of each measurement is flipped with probability p. Likewise we receive estimates of the opinion of each individual, perhaps using a classification model on their profile, but these estimates are corrupted with probability q. The reader should thing of pairwise measurements as being fairly accurate while the vertex estimates as being fairly noisy (flip probability q close to 1/2). Model 1 then translates to producing an estimate of the opinions of users in the social network which predicts the opinion of few users incorrectly.\nA first step in studying recovery problems on graphs with noisy vertex observations was taken by Globerson et al. (2014, 2015) who studied Model 1 on square grids. They prove that the statistical complexity of the problem is essentially determined by the number of cuts with cutset of size k. This observation together with a clever use of planar duality enables them to determine the error rate for the square grid. Furthermore, they use the fact that certain quadratic programs are polytime solvable over planar graphs to devise a polynomial algorithm that attains an optimal rate. We comment that as in Globerson et al. (2014, 2015) we focus on finding a labeling of low Hamming error (as opposed to exact recovery, where one seeks to find the error probability that with which we recover all labels correctly). Chen et al. (2016) have recently considered exact recovery for edges in this setting for sparse graphs such as grid and rings. They consider the case where there are multiple i.i.d observations of edge labels. In contrast, we focus on the case where there is a single (noisy) observation for each edge, on side information, and on partial recovery. 1\nRelated community detection models such as the well known Stochastic Block Model (SBM) and Censored Block Model (CBM) consider the case where one wishes to detect two communities based on noisy edge observations. Namely, in these models only noisy edges observations are provided and one wishes to recover the correct labeling of vertices up to sign. Block model literature has focused on graphs which have \u201cgood\u201d expansion properties such as complete graphs, random graphs, and spectral expanders. By including side information, our model allows for nontrivial recovery rates and efficient algorithms for graphs with \u201csmall\u201d separators such as trees, thin grids, and ring lattices. Studying recovery problems in such \u201cnon-expanding\u201d graphs is of interest as many graphs arising in applications such as social networks (Flaxman, 2007) have poor expansion.\nThe availability of vertex observations changes the statistical nature of the problem in some important ways. For example, for the n-vertex path, it is not difficult (Globerson et al., 2014) to show that when there are only noisy edge observations any algorithm will fail to find the correct\n1We refer the reader to Appendix A for further discussion of related models.\nlabeling (up to sign) of \u2126(n) edges. In contrast, when noisy vertex observations are available, we can obtain a labeling whose expected Hamming error is at most O(pn). That phenomenon that side information with constant noise level enables partial recovery in virtually all sparsity regimes was also observed in Mossel and Xu (2016).\nChallenges and Results The key challenge in designing algorithms for Model 1 is understanding statistical performance: Even for graphs such as trees in which the optimal estimator (the marginalized estimator) can be computed efficiently, it is typically unclear what Hamming error rate this estimator obtains. Our approach is to tackle this statistical challenge directly; we obtain efficient algorithms as a corollary.\nOur first observation is that the optimal Hamming error for trees is \u0398\u0303(pn) provided q is bounded away from 1/22. This is obtained by an efficient message passing algorithm. We then (efficiently) extend our algorithm for trees to more general graphs using a tree decompositions of (edge)-subgraphs. Our main observation is that a bound on the error rate for each component in the tree decomposition can be lifted to a bound on the error rate for the entire graph by leveraging side information. Error bounds for components are easily obtained using cut structure.\nThis approach has the advantage that it applies to non-planar graphs such as high dimensional grids; it is not clear how to apply the machinery of Globerson et al. (2015) to such graphs both because planar duality no longer applies, and the quadratic program they solve no longer admits an efficient solution. Our decomposition-based approach also enables us to obtain optimal error bounds for thin grids which do not have the so-called weak expansion property that is necessary for the analysis in Globerson et al. (2015).\nSee Section 4 for an extensive discussion of concrete graph families we consider and the error rates we achieve."}, {"heading": "1.1 Preliminaries", "text": "We work with an undirected graph G = (V,E), with |V | = n and |E| = m. For W \u2286 V , we let G(W ) be the induced subgraph and E(W ) be the edge set of the induced subgraph. Let N(v) be the neighborhood of a vertex v. When it is not clear from context we will use NG(v) to denote neighborhood with respect to a specific graph G. Likewise, for S \u2286 V we use \u03b4G(S) to denote its cut-set (edges with one endpoint in S) with respect to G. For a directed graph, we let \u03b4+(v) denote the outgoing neighbors and \u03b4\u2212(v) denote the incoming neighbors of v. For a subset W \u2286 V we let NG(W ) = \u22c3 v\u2208W NG(v). We let deg(G) denote the maximum degree and \u2206avg the average degree.\nParameter range We treat q = 1/2\u2212 as constant unless otherwise specified. Furthermore, we shall assume throughout that p \u2265 w(1/n), so the expected number of edge errors is super-constant. We use O\u0303 to hide log(n), log(1/p), and 1/ factors. We use the phrase \u201cwith high probability\u201d to refer to events that occur with probability at most 1\u2212 on(1).\nWe assume deg(G) is constant; in the appendix (Theorem 6) we show that if the minimum degree of the graph is \u2126(log n) there is a trivial strategy that achieves arbitrarily small Hamming error.\n2The assumption on q is necessary as when q approaches 1/2 it is proven in Globerson et al. (2015) that an error of \u2126(n) is unavoidable for certain trees."}, {"heading": "2 Inference for Trees", "text": "In this section we show how to efficiently and optimally perform inference in Model 1 when the graph G is a tree. In this case note that the expected number of edges (u, v) of the tree with Xuv flipped is p(n \u2212 1). In fact, using a simple Chernoff bound we can show that with high probability, at most 2pn + O\u0303(1) edges are flipped. This implies that for the ground truth Y ,\u2211\n(u,v)\u2208E 1{Yu 6= Xu,vYv} \u2264 2pn + O\u0303(1) with high probability over sampling of the edge labels. Hence to estimate ground truth, it is sufficient to search over labelings Y\u0302 that satisfy the inequality\u2211\n(u,v)\u2208E\n1{Y\u0302u 6= Xu,vY\u0302v} \u2264 2pn+ O\u0303(1). (1)\nWe choose the estimator that is most correlated with the vertex observations Z subject to the aformentioned inequality. That is, we find\nY\u0302 = arg min Y\u0302 \u2208{\u00b11}V \u2211 v\u2208V 1{Y\u0302v 6= Zv} s.t. \u2211 (u,v)\u2208E 1{Y\u0302u 6= Xu,vY\u0302v} \u2264 2pn+ O\u0303(1) (2)\nThis optimization problem can be solved efficiently \u2014 O(dpnen2deg(G)) time for general trees and O(dpnen) time for stars and line graphs \u2014 with message passing. The full algorithm is stated in Appendix D.\nOn the statistical side we use results from statistical learning theory to show that the Hamming error of Y\u0302 obtained above is with high probability bounded by O\u0303(pn). To move to the statistical learning setting (see Appendix C for an overview) we first define our \u201chypothesis class\u201d F , {Y \u2032 \u2208 {\u00b11}V | \u2211 (u,v)\u2208E 1{Y \u2032u 6= Xu,vY \u2032v} \u2264 2pn+ O\u0303(1)}; note that this is precisely the set of Y \u2032 satisfying (1). The critical observation here is that for any Y\u0302 the Hamming error (with respect to the ground truth) is proportional to the excess risk in the statistical learning setting over Z with class F :\n\u2211 v\u2208V 1{Y\u0302v 6= Yv} = 1 1\u2212 2q [\u2211 v\u2208V PZ{Y\u0302v 6= Zv} \u2212 min Y \u2032\u2208F \u2211 v\u2208V PZ{Y \u2032v 6= Zv} ] . (3)\nCombing (3) with a so-called fast rate from statistical learning theory (Corollary 2) implies that if we take Y\u0302 to be the empirical risk minimizer over F given Z, which is in fact the solution to (2), then we have \u2211 v\u2208V 1{Y\u0302v 6= Yv} \u2264 O(log(|F|/\u03b4)/ 2) with probability at least 1\u2212 \u03b4. Connectivity of G implies |F| \u2248 ( ep) 2pn+O\u0303(1), giving the final O\u0303(pn) rate. Theorem 1 makes this result precise:\nTheorem 1 (Decoding in Trees). Let Y\u0302 be the solution to (2). Then with high probability,\u2211 v\u2208V 1 { Y\u0302v 6= Yv } \u2264 O\u0303(pn). (4)\nMore precisely, with probability at least 1\u2212 \u03b4,\u2211 v\u2208V 1 { Y\u0302v 6= Yv } \u2264 1 2 (2pn+ 2 log(2/\u03b4) + 1) log(2e/p\u03b4). (5)\nWe remark that side information is critical in this result. For trees \u2014 in particular the line graph \u2014 no estimator can achieve below \u2126(n) hamming error until p = O(1/n)."}, {"heading": "3 Inference for General Graphs", "text": ""}, {"heading": "3.1 Upper Bound: Inference with Tree Decompositions", "text": "Our main algorithm, TreeDecompositionDecoder (Algorithm 1) produces estimators for Model 1 for graphs G that admit a tree decomposition in the sense of Robertson and Seymour (Robertson and Seymour (1986)). Recall that a tree decomposition for a graph G = (V,E) is new graph T = (W, F ) in which each node in W corresponds to a subset of nodes in the original graph G. The edge set F forms a tree over W and must satisfy a property known as coherence, which guarantees that the connectivity structure of T captures that of G. The approach of TreeDecompositionDecoder is to use the edge observations X to produce a local estimator for each component of the tree decomposition T , then use the vertex observations Z to combine the many local estimators into a single global estimator.\nTree decompositions have found extensive use in algorithm design and machine learning primarily for computational reasons: These objects allow one to lift algorithmic techniques that are only feasible computationally on constant-sized graphs, such as brute force enumeration, into algorithms that run efficiently on graphs of all sizes. It is interesting to note that our algorithm obeys this principle, but for statistical performance in addition to computational performance: We are able to lift an analysis technique that is only tight for constant-sized graphs, the union bound, into an analysis that is tight for arbitrarily large graphs from families such as grids. However, as our analysis for trees shows, this approach is only made possible by the side information Z.\nThe width wid(T ) of a tree decomposition T is the size of the largest component in T , minus one (by convention). To place a guarantee on the performance of TreeDecompositionDecoder, both statistically and computationally, it is critical that the width be at most logarithmic in n. At first glance this condition may seem restrictive there are graphs of interests such as grids for which the treewidth tw(G) \u2014 the smallest treewidth of any tree decomposition \u2014 is order \u221a n. For such graphs, our approach is to choose a subset E\u2032 \u2286 E of edges to probe so that the graph G\u2032 = (V,E\u2032) has small treewidth. For all of the graphs we consider this approach obtains optimal sample complexity in spite of discarding information. This is remarkable because for grids this entails discarding a constant fraction of the pairwise measurements.\nHaving found a decomposition of small treewidth for G\u2032 we apply the following algorithm. For each component of this decomposition, we compute the maximum likelihood estimator for the labels in this component given the edge measurements X. This is done by brute-force enumeration over vertex labels, which can be done efficiently because we require small treewidth. With these estimates we have a tree decomposition in which for each component, up to sign, we have the right labels for the vertices in the component (to within a tolerable failure probability). Furthermore by concentration of measure we will have that the total number of failures across all components will not be much larger than the expected number of failures.\nFor a given component, there will be two estimators that match the edges in that component equally well due to sign ambiguity. The remaining problem is to select a set of signs \u2014 one for each component \u2014 so that local estimators agree globally. For this task we leverage the side information Z. Our approach will mirror that of Section 2: To produce a global prediction Y\u0302 we solve a global optimization problem over the tree decomposition using dynamic programming, then analyze the statistical performance of Y\u0302 using statistical learning theory.\nInformally, if there is some \u2206 such that we can show a p\u2206 failure probability for estimating up to sign the vertex labels within each component of the tree decomposition, the prediction produces by Algorithm 1 will attain a high probability p\u2206n Hamming error bound for the entire graph. For example, in Section 4 we show a p2 failure probability for estimating vertex labels in a grid of\nsize 3\u00d7 2, which through Algorithm 1 translates to a O(p2n) rate with high probability on both\u221a n\u00d7 \u221a n and 3\u00d7 n/3 grids.\nDefinition 1 (E.g., Cowell et al. (2006)). A tree T = (W, F ) is a tree decomposition for G = (V,E) if it satisfies\n1. Vertex Inclusion: Each node in v \u2208 V belongs to at least one component W \u2208 W.\n2. Edge Inclusion: For each edge (u, v) \u2208 E, there is some W \u2208 W containing both u and v.\n3. Coherence: Let W1,W2,W3 \u2208 W with W2 on the path between W1 and W3 in T . Then if v \u2208 V belongs to W1 and W3, it also belongs to W2.\nWe assume the following additional properties without loss of generality:\n\u2022 T is not redundant, i.e. there is no (W,W \u2032) \u2208 F with W \u2032 \u2286W .\nThe next definition concerns the subsets of the graph G used in the local inference procedure within Algorithm 1. We allow the local maximum likelihood estimator for a component W to consider a superset of nodes, Extend(W ), whose definition will be specialized to different classes of graphs.\nDefinition 2 (Component Extension Function). For a given W \u2208 W, the extended component W ? \u2287W denotes the result of Extend(W ).\nUseful choices for the extension function include Extend(W ) = W and the neighborhood of W with respect to the probed graph:\nExtend(W ) = W \u222a \u22c3 v\u2208W NG\u2032(v). (6)\nConcrete instantiations of Extend are given in Section 4. We now define quantitative properties of the tree decomposition in Table 1. For a given property, the corresponding (?) version will denote the analogue the arises in analyzing performance when using extended components. For simplicity, the reader may wish to imagine each (?) property as the corresponding non-(?) property on their first read-through.\nDefinition 3 (Admissible Tree Decomposition). We will call a tree decomposition T = (W, F ) admissible if it satisfies the following properties:\n\u2022 deg(T ), deg?E(T ), maxW\u2208W |E(W ?)|, and wid?(T ) are constant.\n\u2022 G\u2032(W ?) is connected for all W \u2208 W3. 3Together with our other assumptions, this implies the connected treewidth of G\u2032 (Diestel and Mu\u0308ller, 2016) is\nconstant.\nIn the rest of this section, the O\u0303 notation will hide all of the constant quantities from Definition 3.\nTheorem 2 (Main Theorem). Let Y\u0302 be the labeling produced using Algorithm 1 with an admissible tree decomposition. Then, with high probability over the draw of X and Z,\n\u2211 v\u2208V 1 { Y\u0302v 6= Yv } \u2264 O\u0303 ( \u2211 W\u2208W pdmincut ?(W )/2e ) . (7)\nIn particular, let \u2206 be such that \u2206 \u2264 mincut?(W ) for all W \u2208 W. Then, with high probability,\u2211 v\u2208V 1 { Y\u0302v 6= Yv } \u2264 O\u0303 ( pd\u2206/2en ) . (8)\nAlgorithm 1 runs in time O\u0303(dp\u2206/2nen2) for general tree decompositions and time O\u0303(dp\u2206/2nen) when T is a line graph.\nAlgorithm 1 TreeDecompositionDecoder Parameters: Graph G = (V,E). Probed edges E\u2032 \u2286 E. Extension function Extend. Tree decomposition T = (W, F ) for (V,E\u2032). Failure probability \u03b4 > 0. Input: Edge measurements X \u2208 {\u00b11}E . Vertex measurements Z \u2208 {\u00b11}V .\n1: procedure TreeDecompositionDecoder\n2: Stage one: 3: for W \u2208 W do . Solve edge MLE for each component. 4: W ? \u2190 Extend(W ). . See Definition 2 5: Y\u0303W ? \u2190 arg minY\u0303 \u2208{\u00b11}W? \u2211\nuv\u2208E\u2032(W?) 1{Y\u0303uY\u0303v 6= Xuv}. 6: Let Y\u0302W ? be the restriction of Y\u0303W ? to W .\n7: end for\n8: Stage two:\n9: for W \u2208 W do . Compute array of meta-vertex costs for tree decomposition. 10: CostW [+1]\u2190 \u2211 v\u2208W 1{Y\u0302W ? v 6= Zv} & CostW [\u22121]\u2190 \u2211 v\u2208W 1{\u2212Y\u0302W ?\nv 6= Zv}. 11: end for 12: for (W1,W2) \u2208 F do . Compute meta-edge costs for tree decomposition. 13: Let v \u2208W1 \u2229W2. 14: S(W1,W2)\u2190 Y\u0302 W?1 v \u00b7 Y\u0302W ? 2 v . 15: end for\n16: Kn \u2190 ddeg(T ) ( 2wid ?(T )+2 \u2211 W\u2208W p dmincut?(W )/2e + 6deg?E(T ) maxW\u2208W |E(W ?)| log(2/\u03b4) ) e. 17: s\u0302\u2190 TreeDecoder(T,Cost, S,Kn). . Compute optimal signing of components. See Appendix D. 18: for v \u2208 V do . Collapse tree decomposition into final estimator. 19: Choose an arbitrary W such that v \u2208W and set Y\u0302v \u2190 s\u0302W Y\u0302W ? v . 20: end for\n21: return Y\u0302 .\n22: end procedure"}, {"heading": "3.2 Main theorem: Proof sketch", "text": "Let us sketch the analysis of Theorem 2 in the simplest case, where Extend(W ) = W for all W \u2208 W and consequently all (?) properties are replaced with their non-(?) counterparts. We give a bound begin by bounding that probability that a single component-wise estimator Y\u0302 W computed on Line 6 of Algorithm 1 fails to exactly recover the ground truth within its component.\nDefinition 4 (Component Estimator). The (edge) maximum likelihood estimator for W is given by\nY\u0302 W , arg min Y\u0302 \u2208{\u00b11}W \u2211 uv\u2208E\u2032(W ) 1{Y\u0302uY\u0302v 6= Xuv}. (9)\nY\u0302 W can be computed by enumeration over all labelings in time 2|W |. There are always two solutions to (9) due to sign ambiguity; we take one arbitrarily.\nProposition 1 (Error Probability for Component Estimator). P (\nmin s{\u00b11}\n1{sY\u0302 W 6= Y W } > 0 ) \u2264 O\u0303(pdmincut(W )/2e)\nProof. Assume that both Y\u0302 W and \u2212Y\u0302 W disagree with the ground truth or else we are done. Let S be a maximal connected component of the set of vertices v for which Y\u0302 Wv 6= Yv. It must be the case that at least d|\u03b4(S)|/2e edges (u, v) in \u03b4(S) have Xuv flipped from the ground truth, or else we could flip all the vertices in S to get a new estimator that agrees with X better than Y\u0302 ; this would be a contradiction since Y\u0302 minimizes \u2211 uv\u2208E\u2032(W ) 1{Y\u0302uY\u0302v 6= Xuv}. We now apply union bound to get the result:\nP (\nmin s{\u00b11}\n1{sY\u0302 W 6= Y W } > 0 ) \u2264 \u2211 S\u2286W :S 6=\u2205,S 6=W pd|\u03b4(S)/2|e \u2264 2|W |pdmincut(W )/2e. (10)\nProposition 1 bounds the probability of failure for individual components, but does not immediately imply a bound on the total number of components that may fail for a given realization of X. If the components W did not overlap one could apply a Chernoff bound to establish such a result, as their predictions would be independent. Since components can in fact overlap their predictions are dependent, but using a more sophisticated concentration inequality (the entropy method (Boucheron et al., 2003)) we can show that \u2013 so long as no edge appears in too many components \u2013 an analogous concentration result holds and total number of components failures is close to the expected number with high probability.\nLemma 1 (Informal). With high probability over the draw of X,\nmin s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302W 6= YW } \u2264 O\u0303 ( \u2211 W\u2208W pdmincut(W )/2e ) (11)\nIn light of (11), consider the signing of the component-wise predictions (Y\u0302 W ) that best matches the ground truth.\ns? = arg min s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302 W 6= Y W }.\nIf we knew the value of s? we could use it to produce a vertex prediction with a Hamming error bound matching (7). Computing the s? is information-theoretically impossible because we do not have access to Y . We get the stated result by proceeding in a manner similar to the algorithm (2) for the tree. We first define a class F \u2286 {\u00b11}W which has the property that 1) s? \u2208 F with high probability and 2) |F| . 2O\u0303( \u2211 W\u2208W 2\n|W |pdmincut(W )/2e). Then we take the component labeling s\u0302 computed in Line 17 of Algorithm 1 is simply the element of F that is most correlated with the vertex observations Z: s\u0302 = arg mins\u2208F \u2211 W\u2208W \u2211 v\u2208W 1 { sW Y\u0302 W v 6= Zv } . Finally, to produce the final prediction Y\u0302v for a given vertex v, we find W \u2208 W with v \u2208 W and take Y\u0302v = s\u0302W \u00b7 Y\u0302 Wv . A generalization bound from statistical learning theory then implies that this predictor enjoys error at most O\u0303(log|F|) = O\u0303 (\u2211 W\u2208W 2 |W |pdmincut(W )/2e ) , which establishes the main theorem.\nEfficient implementation Both the tree algorithm and Algorithm 1 rely on solving a constrained optimization problem of the form (2). In Appendix D we show how to perform this procedure efficiently using a message passing scheme."}, {"heading": "3.3 Lower Bounds: General Tools", "text": "In this section we state simple lower bound techniques for Model 1. Recall that we consider q as a constant, and thus we are satisfied with lower bounds that coincide with our upper bounds up to polynomial dependence on q. Theorem 3. Assume p < q. Then any algorithm for Model 1 incurs expected hamming error \u2126( \u2211\nv\u2208V p ddeg(v)/2e).\nCorollary 1. Any algorithm for Model 1 incurs expected hamming error \u2126(p\u2206avg/2+1n).\nTheorem 4. Let W be a collection of disjoint constant-sized subsets of V . Then for all p below some constant, any algorithm for Model 1 incurs expected Hamming error \u2126( \u2211 W\u2208W p d|\u03b4G(W )|/2e)"}, {"heading": "4 Concrete Results for Specific Graphs", "text": "We now provide tight upper and lower bounds on recovery for concrete classes of graphs."}, {"heading": "4.1 Connected Graphs", "text": "Example 1 (Arbitrary graphs). For any connected graph G, the following procedure attains an error rate of O\u0303(pn) with high probability:\n1. Find a spanning tree T for G. 2. Run the algorithm from Section 2 on T .\nThis rate is sharp, in the sense that there are connected graphs \u2014 in particular, all trees \u2014 for which \u2126(pn) Hamming error is optimal. Furthermore, for all graphs one can attain an estimator whose Hamming error is bounded as O\u0303(pn+ #connected components) by taking a spanning tree for each component. This bound is also sharp.\nThe next example shows that there are connected graphs beyond trees for which \u2126(pn) Hamming error is unavoidable.\nExample 2 (Supercritical Erdo\u030bs-Re\u0301nyi). Consider the supercritical Erdo\u030bs-Re\u0301nyi model G(n, p) where p = c/n and c > 1 is a fixed constant. It is well known that with high probability G(n, p) contains a giant component with \u2126(n) vertices, which we call H(n, p). It is known that with high probability H(n, p) contains \u2126(n) vertices of degree at most 2 (Ding et al., 2014; Pittel and Wormald, 2005). It follows that the optimal hamming error for H(n, p) is \u2126(pn). Such error can be attained efficiently by applying the tree algorithm for a spanning tree of H(n, p).\nLooking at Theorem 3, one might be tempted to guess that the correct rate for inference is determined entirely by the degree profile of a graph. This would imply, for instance, that for any d-regular graph the correct rate is \u0398(pdd/2en). The next example \u2014 via Theorem 4 \u2014 shows that this is not the case.\nExample 3. For any constant d, there exists a family of d-regular graphs on n vertices for which no algorithm in Model 1 attains lower than \u2126(pn) Hamming error.\nThis construction for d = 3 is illustrated in Figure 1. We note that this lower bound is exponentially weak in d, but for constant q and d it is \u2126(pn) nonetheless."}, {"heading": "4.2 Grid Lattices", "text": "In this section we illustrate how to use the tree-decomposition based algorithm, Algorithm 1, to obtain optimal rates for grid lattices. Example 4 (2-dimensional grid). Let G be a 2-dimensional grid lattice of size c\u00d7n/c where c \u2264 \u221a n. For grid of height c = 3 (or above) using Algorithm 1, we obtain an estimator Y\u0302 such that with high probability, the Hamming error is bounded as O(p2n). This estimator runs in time O(dp2nen), By the degree profile argument (also given in Globerson et al. (2015)), there is a matching lower bound of \u2126(p2n). For a grid of height c = 1 there is an obvious lower bound of \u2126(pn) since this graph is a tree.\nThe estimator of Globerson et al. (2015) can be shown to have expected Hamming error of O(p2n) for the 2-dimensional grid with c = \u2126(log n). However, this error rates is only achieved when c order log n or above. Our method works for constant height grids and with high probability.\nAlgorithm 1 of course requires a tree decomposition as input. The tree decomposition used to obtain Example 4 for constant-height grids is illustrated in Figure 2 for c = 3: The grid is covered in overlapping 3\u00d7 2 components, and these are connected as a path graph to form the tree decomposition.\nThe savvy reader will observe that this tree decomposition has mincut(W ) = 2, and so only implies a O(pn) Hamming error bound through Theorem 2. This rate falls short of the O(p2n) rate promised in the example; it is no better than the rate if G were a tree. The problem is that within each 3 \u00d7 2 block, there are four \u201ccorner\u201d nodes each with degree 2. Indeed if either edge connected to a corner is flipped from the ground truth, which happens with probability p, this corner is effectively disconnected from the rest of W in terms of information. To sidestep this\nissue, we define Extend(W ) = \u22c3 v\u2208W N(v). With this extension, we have mincut\n?(W ) = 3 for all components except the endpoints, which implies the O(p2n) rate.\nProbing Edges We now illustrate how to extend the tree decomposition construction for constantheight grids to a construction for grids of arbitrary height. Recall that Algorithm 1 takes as input a subset E\u2032 \u2286 E and a tree decomposition T for G\u2032 = (V,E\u2032). To see where using only a subset of edges can be helpful consider Figure 2 and Figure 3. The 3 \u00d7 n/3 grid is ideal for our decoding approach because it can be covered in 3\u00d7 2 blocks as in Figure 2 and thus has treewidth at most 5. The \u221a n\u00d7 \u221a n grid is more troublesome because it has treewidth \u221a n, but we can arrive at G\u2032 with constant treewidth by removing \u0398(n) edges through the \u201czig-zagging\u201d cut shown in Figure 3. Observe that once the marked edges in Figure 3 are removed we can \u201cunroll\u201d the graph and apply a decomposition similar to Figure 2.\nThe tree decomposition construction we have outlined for two-dimensional grids readily lifts to higher dimension. This gives rise to the next example.\nExample 5 (Hypergrids and Hypertubes). Consider a three-dimensional grid lattice of of length n/c2, height c, and width c. If c = n1/3 \u2014 that is, we have a cube \u2014 then Algorithm 1 obtains Hamming error O\u0303(p3n) with high probability, which is optimal by Theorem 3.\nWhen c is constant, however, the optimal rate is \u2126(p2n); this is also obtained by Algorithm 1. This contrasts the two-dimensional grid, where the optimal rate is the same for all 3 \u2264 c \u2264 \u221a n.\nAlgorithm 1 can be applied to any d-dimensional hypergrid of shape c\u00d7 c\u00d7 . . . n/cd\u22121 to achieve O(pdn) Hamming error when c \u2248 n1/d. For c constant, the optimal rate is \u0398(pd d+1 2 en). More generally, the optimal rate interpolates between these extremes.\nThe next two examples briefly sketch how to apply tree decompositions to more lattices. Recall that the triangular lattice and hexagonal lattice are graphs whose drawings can be embedded in R2 to form regular triangular and hexagonal tilings, respectively.\nExample 6 (Triangular Lattice). Consider a triangular lattice of height and width \u221a n (i.e., a parallelogram with side length \u221a n). Let each component to be a vertex and its 6 neighbors (except for the edges of the mesh), and choose these components such that the graph is covered completely. For a given component, let W ? to be the neighborhood W . One can see that for this decomposition mincut?(W ) is 6. Hence, Algorithm 1 achieves Hamming error O\u0303(p3n). This rate is optimal because all vertices in the graph have degree 6 besides those at the boundary, but the number of vertices on the boundary is sub-constant. Example 7 (Hexagonal Lattice). Consider a \u221a n\u00d7 \u221a n hexagonal lattice. Take each component W to be a node v and its neighbors, and choose the nodes v so that the graph is covered. Choose W ? to be the neighborhood of the component W . Thus mincut?(W ) for each component is 3, leading to a Hamming error rate of O\u0303(p2n). This rate is optimal because all vertices on the lattice except those at the boundary have degree 3."}, {"heading": "4.3 Newman-Watts Model", "text": "To define the Newman-Watts small world model (Newman and Watts, 1999), we first define the regular ring lattice, which serves as the base graph for this model. The regular ring lattice Rn,k is a 2k-regular graph on n vertices defined as follows: 1) V = {1, . . . , n}. 2) E = {(i, j) | j \u2208 {i+ 1, . . . , i+ k (mod n)}}. Theorem 3 immediately implies that the best rate possible in this model is \u2126(pkn). Using Algorithm 1 with an appropriate decomposition it is indeed possible to achieve this rate.\nExample 8. The optimal Hamming rate for Rn,k in Model 1 is \u0398\u0303(p kn). Moreover, this rate is achieved by an efficiently by Algorithm 1 in time O(dpknen).\nWe can now move on to the Newman-Watts model itself:\nDefinition 5 (Newman-Watts Model). To produce a sample from the Newman-Watts model Hn,k,\u03b1, begin with Rn,k, then independently replace every non-edge with an edge with probability \u03b1/n.\nFor any constant \u03b1 < 1, a constant fraction of the vertices in Rn,k will be untouched in Hn,k,\u03b1. Thus, the inference lower bound for Example 8 still applies, meaning that the optimal rate is O(pkn). Algorithmically, this result can be obtained by discarding the new edges and using the same decomposition as in Example 8.\nExample 9. For any \u03b1 < 1, the optimal Hamming rate for Hn,k,\u03b1 in Model 1 is \u0398\u0303(p kn). Moreover, this rate is achieved by an efficiently by Algorithm 1 in time O(dpknen)."}, {"heading": "5 Extensions", "text": "Semi-random model Our tree decomposition based approach readily extends to the following semi-random model: Each edge is marked independently with probability p. For unmarked edges (u, v) \u2208 E we are provided withthe ground truth labeling Yu \u00b7 Yv. For marked edges, an adversary is allowed to arbitrarily set labels. For the noisy vertex labels we assume the same random model as in Model 1.\nAll our results (e.g. Theorem 2) immediately extend to this setting with the same rates. To see this, observe that if Lemma 2 and Lemma 4 hold in the semi-random setting, then the rest of the analysis can be used unchanged. Now Lemma 2 holds for the semi-random setting for any arbitrary labeling of marked nodes within each component and its extension. This is because the lemma only\ndepends on bounding probability that more than half the edges in a cut are flipped, and does not depend on how marked edges are relabeled. To prove the Lemma 4 in the semi-random setting, we move to an upper bound by allowing edges that appear in multiple components to be labeled differently across components (we can have inconsistent labelings). That is, for each component we consider adversarial labelings local to that component and then use the same concentration result used to prove the bound in Lemma 4.\nThis observation shows that side information enables tight recovery in the semi-random setting. It should be noted that more common inference methods for the pure edge recovery setting such as spectral algorithms often are not robust to semi-random measurements.\nGeneral Sub-Algorithms While Algorithm 1 considers components of constant size and we use a brute-force algorithm to make inference in each component using edge labels, a careful look at our proof technique reveals that one can replace inference within each component by any arbitrary sub-algorithm (e.g. spectral) that obtains exact recovery within each component up to sign (with bounded failure probability). Thus, our decomposition methodology can be used at a high level to combine different inference techniques (based on edge labels) within each components. This approach can apply even when components are allowed to have super-constant size, and suggests an interesting avenue for exploring computational-statistical tradeoffs. We remark that even for the brute force enumeration scheme we use in each component, though we currently assume constant size components, we can easily still produce polynomial time algorithms if the component sizes are allowed to be of order log n."}, {"heading": "6 Discussion", "text": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels. We make several improvements over Globerson et al. (2015). First, their work provide the right rates for grid graphs and provide upper bounds on recovery rates for planar graphs and expanders. Further, the proof techniques in Globerson et al. (2015) critically relied on a weak expansion property. In contrast, we provide a general method to deal with arbitrary graphs that admit tree decompositions of (edge)-subgraphs.We recover the results in Globerson et al. (2015) for grids, but are able to provide rates for graphs that do not satisfy the weak expansion property. While Globerson et al. (2015) use vertex labels only to break ties (this is unnecessary if wish to recover only up to a global sign), we show that for more general graphs the vertex labels, though very noisy, play a critical role.\nFinding Decompositions Throughout this paper we assumed that one could find tree decomposition for the given graph (after deleting some edges) that are admissible in the sense of having low tree width. For all the examples we consider, we directly constructed such decompositions. However, one might ask is whether such a decomposition can be automatically found. That is, in general is there an efficient procedure to find the probed edge subset E\u2032 and tree decomposition T? While Bodlaender\u2019s celebrated algorithm (Bodlaender, 1996) can efficiently find tree decompositions of constant width, it is unclear how to find the edge subset E\u2032 when E does not directly admit a decomposition of constant width. Approximation algorithms for this interdiction task exist (Bansal et al., 2017), but we do not know how to translate their approximation guarantees into statistical performance guarantees. Even if a set E\u2032 which admits a constant width decomposition is known to exist a-priori, we cannot guarantee that an approximation algorithm will not delete extra edges and consequently degrade the statistical recovery rate."}, {"heading": "A Further discussion of related work", "text": "Censored Block Model A recent line of research has studied recovery under the so-called censored block model (CBM). In CBM, vertices are labeled by \u00b11 and for every edge uv, the number YuYv is observed independently with probability 1\u2212 q (where Yu, Yv are the labels of the vertices). The goal is to find the true label YuYv of each edge uv correctly with high probability (based on the noisy observations). For partial recovery in the censored block model we ask for a prediction whose correlation with the ground truth (up to sign) is constant strictly greater than 1/2 as n\u2192\u221e. For the Erdo\u0308s-Re\u0301nyi random graph model, G(n, \u03b1/n) both the threshold (how large \u03b1 needs to be in terms of p) for partial Saade et al. (2015) and exact Abbe et al. (2014) recovery have been determined Exact recovery is obtained through maximum likelihood estimation which is generally intractable. The authors provide a polynomial time algorithm based on semidefinite programming that matches this threshold up to constant factors.\nWe observe that in our setting, due to the presence of side information, there is a simple and efficient algorithm that achieves exact recovery with high probability when the minimal degree is \u2126(log n): Theorem 6. Such exact recovery algorithms are known for CBM model only under additional spectral expansion conditions Abbe et al. (2014).\nRecovery from Pairwise Measurements Chen and Goldsmith (2014) also provides conditions on exact recovery in a censored block model-like setting which, like our own, considers structured classes of graphs. Motivated by applications in computational biology and social networks analysis, Chen et al. (2016) have recently considered exact recovery for edges in this setting for sparse graphs such as grid and rings. They consider the case where there are multiple i.i.d observations of edge labels. In contrast, we focus on the case where there is a single (noisy) observation for each edge.\nCorrelation Clustering Correlation clustering focuses on a combinatorial optimization problem closely related to the maximum likelihood estimation problem for our setting when we are only given edge labels. The main difference from our work is that the number of clusters is not predetermined. Most work on this setting has focused on obtaining approximation algorithms and has not considered any particular generative model for the weights (as in our case). An exception is Joachims and Hopcroft (2005), which gives partial recovery results in a model similar to the one we consider, in which a ground truth partition is fixed and the observed edge labels correspond to some noisy notion of similarity. However, these authors focus on the case where G is the complete graph.\nMakarychev et al. (2015) consider correlation clustering where the model is a semi-random variant of the one we consider for the edge inference problem: Fix a graph G = (V,E) and a vertex label Y . For each uv \u2208 E, we observe Xuv where Xuv = YuYv with probability 1 \u2212 p and has its value in selected by an adversary otherwise. They do not consider side information, nor are they interested in concrete structured classes of graphs like grids.\nAnomaly Detection Sharpnack et al. (2013) considered anomaly detection on graphs using only vertex measurements. Here it was also observed that constant correlation in vertex measurements leads to near-optimal recovery rates on structured classes of graphs."}, {"heading": "B Omitted Proofs", "text": "B.1 Proofs from Section 2\nProof of Theorem 1. By the Bernstein inequality we have that with probability at least 1\u2212 \u03b4/2,\u2211 (u,v)\u2208E 1{Yu 6= Xu,vYv} \u2264 2pn+ 2 log(2/\u03b4).\nThus, if we take F = { Y\u0302 : \u2211 (u,v)\u2208E 1{Y\u0302u 6= Xu,vY\u0302v} \u2264 2pn+ 2 log(2/\u03b4) } we will have Y \u2208 F with probability at least 1\u2212 \u03b4/2. Fix Y\u0302 \u2208 {\u00b11}V . We can verify by substitution that for each v \u2208 V , 1{Y\u0302v 6= Yv} =\n1 1\u22122q [ PZ(Y\u0302v 6= Zv)\u2212 PZ(Yv 6= Zv) ] , so when Y \u2208 F we have the following relation for Hamming error: \u2211 v\u2208V 1 { Y\u0302v 6= Yv } = 1 1\u2212 2q [\u2211 v\u2208V P(Y\u0302v 6= Zv)\u2212 min Y \u2032\u2208F \u2211 v\u2208V P ( Y \u2032v 6= Zv )] .\nCorollary 2 now implies that if we take Y\u0302 = arg minY \u2032\u2208F \u2211\nv\u2208V 1{Y \u2032v 6= Zv}, which is precisely the solution to (2), we will have that with probability at least 1\u2212 \u03b4/2,\u2211\nv\u2208V P ( Y\u0302v 6= Zv ) \u2212 min Y \u2032\u2208F \u2211 v\u2208V P ( Y \u2032v 6= Zv ) \u2264 ( 4 3 + 1 ) log ( 2|F| \u03b4 ) .\nUsing that |F| \u2264 \u22112pn+2 log(2/\u03b4)\nk=0\n( n k ) \u2264 (e/p)2pn+2 log(2/\u03b4) and \u2264 1/2 we further have that the\nRHS is bounded as 2 log(2e/p\u03b4)(2pn+ 2 log(2/\u03b4) + 1). Putting everything together (and recalling 1\u2212 2q = 2 ), we see that with probability at least 1\u2212 \u03b4\u2211\nv\u2208V 1\n{ Y\u0302v 6= Yv } \u2264 1 2 (2pn+ 2 log(2/\u03b4) + 1) log(2e/p\u03b4).\nB.2 Proofs from Section 3\nProof of Theorem 3. The minimax value of the estimation problem is given by\nmin Y\u0302 max Y E X,Z|Y \u2211 v\u2208V 1 { Y\u0302v(X,Z) 6= Yv } .\nWe can move to a lower bound by considering a game where each vertex predictor Y\u0302v is given access to the true labels Y of all other vertices in G:\n\u2265 min {Y\u0302v}v\u2208V max Y E X,Z|Y \u2211 v\u2208V 1 { Y\u0302v(X,Z, YV \\{v}) 6= Yv } .\nUnder the new model, the minimax optimal predictor for a given node v is given by the MAP predictor:\nY\u0302v = arg min Y\u0302 \u2208{\u00b11} log\n( 1\u2212 q q ) 1 { Y\u0302 6= Zv } + log ( 1\u2212 p p ) \u2211 u\u2208Nv 1 { Y\u0302 6= YuXuv } .\nWhen p < q, the minimax optimal estimator for v takes the majority of the predictions suggested by its edges (that is, Yu \u00b7Xuv for each neighbor u) and uses the vertex observation Zv to break ties.\nWhen deg(v) is odd, the majority will be wrong if at least ddeg(v)e of the edges in the neighbor of v are flipped, and will be correct otherwise. When deg(v) is even there are two cases: 1) Strictly more than ddeg(v)e of the edges in N(v) have been flipped, in which case the majority will be wrong. 2) Exactly half the edges are wrong, in which the optimal estimator will take the label Zv as its prediction, which will be wrong with probability q. We thus have\nP(Y\u0302v 6= Yv) = deg(v)\u2211\nk=ddeg(v)/2e\n( deg(v)\nk\n) pk(1\u2212 p)deg(v)\u2212k\n\u2265 ( deg(v)\nddeg(v)/2e\n) pddeg(v)/2e(1\u2212 p)deg(v)\u2212k\n\u2265 ( deg(v)\nddeg(v)/2e\n)ddeg(v)/2e pddeg(v)/2e(1/2)ddeg(v)/2e\n\u2265 \u2126(pddeg(v)/2e).\nIn the last line we have used that we treat deg(v) as constant to suppress a weak dependence on it that arises when deg(v) is odd. Putting everything together, we see that in expectation we have the bound\nE [\u2211 v\u2208V 1 { Y\u0302v 6= Yv }] \u2265 \u2126 ( q \u2211 v\u2208V pddeg(v)/2e ) .\nProof of Theorem 4. Recall that the minimax value of the estimation problem is given by\nmin Y\u0302 max Y E X,Z|Y \u2211 v\u2208V 1 { Y\u0302v(X,Z) 6= Yv } .\nAs in the proof of Theorem 3, we will move to a lower bound where predictors are given access to extra data. In this case, we consider a set of disjoint predictors { Y\u0302 W } , one for each component\nW \u2208 W. We assume that Y\u0302 W see the ground truth Yv for each vertex v /\u2208W , and further sees the product Yuv , YuYv for each edge e \u2208 E(W ). Assuming G(W ) is connected (this clearly can only make the problem easier), the learner now only needs to infer one bit of information per component. The minimax value of the new game can be written as:\n\u2265 min {Y\u0302W}\nW\u2208W\nmax Y E X,Z|Y \u2211 W\u2208W \u2211 v\u2208W 1 { Y\u0302 Wv (X,Z, YV \\W , {Yuv | uv \u2208 E(W )}) 6= Yv } .\nBecause the learner only needs to infer a single bit per component, we have reduced to the setting of Theorem 3, components in our setting as vertices in that setting (so deg(v) is replaced by \u03b4G(W )). The only substantive difference is the following: In that lower bound, we required that p < q. For the new setting, we have that \u201cq\u201d is actually (pessimistically) q|W |, and so we require that p < qmaxW\u2208W |W | for the bound to apply across all components. Using the final bound from Theorem 3, we have\nE [\u2211 v\u2208V 1 { Y\u0302v 6= Yv }] \u2265 \u2126 ( qmaxW\u2208W |W | \u2211 W\u2208W pd\u03b4G(W )/2e ) .\nB.3 Proofs from Section 4\nProof of Example 1. We will show that \u2126(pn) Hamming error is optimal for all trees by establishing that all trees have constant fraction of vertices whose degree is at most two, then appealing to Theorem 3.\nLet T be the tree under consideration. T is bipartite. Let (A,B) be the bipartition of T into two disjoint independent sets. Suppose without loss of generality that |A| \u2265 n/2. If a is the number of vertices in A of degree at least 3 and a\u2032 = |A| \u2212 a, we have that 3a \u2264 n\u2212 1, hence a \u2264 (n\u2212 1)/3. Therefore a\u2032 \u2265 n/2\u2212 a \u2265 (n\u2212 1)/6. Letting A\u2032 be the set of vertices in A with at most 2 neighbors, we see that A\u2032 is an independent set of size at least (n\u2212 1)/6, and so we appeal to Theorem 3 for the result.\nProof of Example 3. Fix d \u2265 3. We will construct a graph G of size (d+ 1)n. By building up from components as follows:\n\u2022 For each k \u2208 [n] let Gk be the complete graph on d + 1 vertices. Remove an edge from an arbitrary pair of vertices (uk, vk).\n\u2022 Form G by taking the collection of all Gk, then adding an edge connecting vk to uk+1 for each k, with the convention un+1 = u1.\nThis construction for d = 3 is illustrated in Figure 1. Observe that G is d-regular. We obtain the desired result by applying Theorem 4 with the collection {Gk} as the set system and observing that the each component Gk has only two edges leaving.\nProof of Example 4. We first examine the case where c = 3. Here we take the tree decomposition illustrated in Figure 2, where we cover the graph with overlapping 3 \u00d7 2 components, and take W ? = \u22c3 v\u2208W Nv. This yields mincut\n?(W ) = 3 for all components except those at the graph\u2019s endpoints. We now connect the components as a path graph and appeal to Theorem 2, which implies a rate of O\u0303(p2n).\nWhen c = \u03c9(1) we can build a decomposition as follows (informally): Produce E\u2032 as in Figure 3 by performing the zig-zag cut with every third row of edges, leaving only 3 edges on the left or right side (alternating). We can now produce T (a line graph) by tiling G\u2032 with overlapping 3\u00d7 3 components. Again, take W ? = \u22c3 v\u2208W Nv.\nWe can verify that if we perform extended inference we have mincut?(W ) = 3 for the O(n) components in the interior of the graph and mincut?(W ) = 2 for the O( \u221a n) components at the boundary. The tree decomposition is illustrated in Figure 4. We have wid?(T ) = O(1) and degE(T ) = O(1).\nApplying Theorem 2 thus gives an upper bound of O\u0303(p2n+ p \u221a n) with probability at least 1\u2212 \u03b4.\nSince T is a line graph, we pay O(ndp2ne) in computation as per Appendix D.\nProof of Example 5. We will prove this result for the three-dimensional case. We first show the lower bound.\nSuppose c \u2265 3 is constant, so that we are in the \u201chypertube\u201d regime. Note that vertices on the outermost \u201cedges\u201d of the hypertube, examples of which are circled in Figure 5, have degree at most 4. There are \u2126(n) such vertices, so appealing to Theorem 3 yields a lower bound on Hamming error of \u2126(p2n). In fact for the n/c2 \u00d7 c\u00d7 c hyper-tube one can achieve the O(p2n) rate using our method. Simply take each components of size 2\u00d7 c\u00d7 c connected in a path as in the example for the 2D grid. Since the minimum cut for each component is already at least 3, we don\u2019t need to consider extended components and simply use brute-force on the components themselves.\nWe now sketch the upper bound for the n1/3 \u00d7 n1/3 \u00d7 n1/3 hypergrid. We use a technique similar to that used for the 2D grid in Example 4: We take T to be a line graph obtained by covering the hypergrid in overlapping 3\u00d7 3\u00d7 3 components in a zig-zagging pattern. Note that each 3\u00d7 3\u00d7 3 component will contain nodes similar to those highlighted in Figure 5 with degree at most 4. This means mincut?(W ) = 4, so to obtain the O(p3n) Hamming error we must consider extended components. Take W ? = \u22c3 v\u2208W Nv. Then mincut\n?(W ) = 6 for all components except those at the boundary of the hypergrid, which have mincut?(W ) \u2208 {4, 5}. There are only o(n) such components, so we achieve the O(p3n) upper bound by appealing to Theorem 2.\nFor higher-dimensional hypergrids, the strategy of taking components to be constant-sized hypergrids and T to be a zig-zagging line graph readily extends. The lower bound stated follows from a simple counting argument.\nIn general, we can associated vertices of a c1\u00d7 c2\u00d7 . . .\u00d7 cd hypergrid with the elements of Zc1 \u00d7 Zc2\u00d7 . . .\u00d7Zcd . For a vertex v = (v1, . . . , vd), the degree is given by deg(v) = |{k \u2208 [d] | vk \u2208 {0, ck}}|.\nConsider the case where c1, . . . cd\u22121 = c, cd = n/c d\u22121. In this case, the degree argument above\nimplies\n|{v | deg(v) = d+ 1}| \u2265 \u2211\nvk\u2208{0,c}:k 6=d\n(n\u2212 2) = \u2126(n).\nThus, a constant fraction of vertices have degree d+ 1, and so Theorem 3 implies a lower bound of \u2126(pd d+1 2 en).\nProof of Example 8. Upper bound: Tree decomposition We first formally define the tree decomposition T = (W, F ) that we will use with Algorithm 1. Assume for simplicity what n = n\u2032 \u00b7 (2k + 1). We will define a vertex set {v1, . . . , vn\u2032} as follows: v1 = 1, vi+1 = vi + k + 1. We will now define a component for each of these vertices:\nW (vi) = NG(vi).\nLet W will be the union of these components. Since we assumed n to be divisible by (2k + 1), the components a partition of V . We now define the Extend function for this decomposition:\nExtend(W ) = \u22c3 v\u2208W NG(v).\nThat is, the extended component W ?(vi) is the set of all vertices removed from vi by paths of length 2.\nFinally, we construct the edge set F by adding edges of the form (W (vi),W (vi+1)) for i \u2208 {1, . . . , n\u2032 \u2212 1}. This means that the decomposition is a line graph. The decomposition is clearly admissible in the sense of Definition 3.\nWe can observe that mincut?(W ) = 2k just as the minimum cut of Rn,k is itself 2k. Theorem 2 thus implies a recovery rate of O\u0303(pkn). Since T is a line graph, the algorithm runs in time O(dpknen).\nLower bound That O(pkn) is optimal can be seen by appealing to Theorem 3 with the fact that Rn,k is 2k-regular.\nProof of Example 9. The average number of vertices added is \u03b1n. By the Chernoff bound, with high probability the number of vertices added is bounded as \u03b1n + c \u221a \u03b1n log n for some constant c. This means that for any > 0, there is some minimum n for which an (1 \u2212 \u03b1 + ) fraction of vertices have no edges added. This means that there are at least (1\u2212 \u03b1+ )n edges with degree 2k, so Theorem 3 yields the result.\nB.4 Analysis of TreeDecompositionDecoder\nProperties of Tree Decompositions We begin by recalling a few properties of tree decompositions that are critical for proving the performance bounds for Algorithm 1.\nProposition 2. For any tree decomposition T = (W, F ), the following properties hold:\n1. For each v \u2208 V there exists W with v \u2208W . Guarantees that we produce a prediction for each vertex.\n2. If (W1,W2) \u2208 F , there is some v \u2208 V with v \u2208W1,W2. Guarantees that the class F (see (20)) is well-defined.\n3. T is connected So that |F| . 2K .\n4. |W| \u2264 n. So that a mistake bound for components of the tree decomposition translates to a mistake bound for vertices of G.\nProof of Proposition 2. 1. Definition 1.\n2. Suppose there is some edge (W1,W2) \u2208 F with no common vertices. Consider the subtrees T1 and T2 created by removing (W1,W2) \u2208 F . By the coherence property (Definition 1), the subgraphs of G\u2032 associated with these decompositions (call them G\u2032T1 and G \u2032 T2\n) must have no common nodes. Yet, G\u2032 is connected, so there must be (u, v) \u2208 E\u2032 with u \u2208 G\u2032T1 , v \u2208 G \u2032 T2\n. Our hypothesis now implies that there is no W \u2208 W containing u and v, so T violates the edge inclusion property of the tree decomposition.\n3. Definition 1\n4. This follows directly from the non-redundancy assumption of Definition 1. See, e.g., (Kleinberg and Tardos, 2006, 10.16).\nEstimation in Tree Decomposition Components We now formally define and analyze the component-wise estimators computed in Line 6 of Algorithm 1.\nDefinition 6 (Extended Component Estimator). Consider the (edge) maximum likelihood estimator over W ?:\nY\u0303 W ? , arg min\nY\u0303 \u2208{\u00b11}W? \u2211 uv\u2208E\u2032(W ?) 1{Y\u0303uY\u0303v 6= Xuv}. (12)\nWe define the extended component estimator Y\u0302 W ? \u2208 {\u00b11}W as restriction of Y\u0303 W ? to W .\nFor Y\u0302 W ? estimation performance is governed by mincut?(W ) rather than mincut(W ), as the next lemma shows:\nLemma 2 (Error Probability for Extended Component Estimator). P (\nmin s{\u00b11}\n1{sY\u0302 W ? 6= Y W } > 0 ) \u2264 2|W ?|pdmincut ?(W )/2e.\nProof of Lemma 2. Suppose Y\u0302 W ? 6= Y W and consider D = {v \u2208 W ? : Y\u0303 W ?v 6= Yv}. Then there is some maximal connected component S of D containing at least one vertex of W . It must then be the case that at least half the edge samples in \u03b4(S) are flipped with respect to the ground truth. Subsequently we have the bound\nP (\nmin s{\u00b11}\n1{sY\u0302 W ? 6= Y W } > 0 ) \u2264 \u2211 S\u2286W ?:S\u2229W 6=\u2205,S\u0304\u2229W 6=\u2205 pd|\u03b4(S)|/2e\n\u2264 \u2211 S\u2286W ? pdmincut ?(W )/2e \u2264 2|W ?|pdmincut ?(W )/2e.\nLemma 2 shows that considering mincut? offers improved failure probability over mincut because it allows us to take advantage of all of the information in W ?, yet only pay (in terms of errors) for cuts that involve nodes in the core component W . In Figure 2, all components of the tree decomposition except the endpoints have mincut?(W ) = 3, and so their extended component estimators achieve O(p2) failure probability.\nConcentration We begin by stating a concentration result for functions of independent random variables, which we will use to establish a bound on the total number of components that fail in the first stage of our algorithm. Let X1, . . . , Xn be independent random variables each taking values in a probability space X , and let F : X n \u2192 R. We will be interested in the concentration of the random variable S = F (X1, . . . , Xn). Letting X \u2032 1, . . . , X \u2032 n be independent copies of X1, . . . , Xn, we define S(i) = F (X1, . . . , Xi\u22121, X \u2032 i, Xi+1, . . . , Xn). Finally, we define a new random variable\nV+ = n\u2211 i=1 E [ (S \u2212 S(i))2+ | X1, . . . , Xn ] .\nTheorem 5 (Entropy Method with Efron-Stein Variance (Boucheron et al., 2003)). If there exists a constant a > 0 such that V+ \u2264 aS then\nP{S \u2265 E[S] + t} \u2264 exp (\n\u2212t2\n4aE[S] + 2at\n) .\nSubsequently, with probability at least 1\u2212 \u03b4,\nS \u2264 E[S] + max { 4a log(1/\u03b4), 2 \u221a 2aE[S] log(1/\u03b4) } \u2264 2E[S] + 6a log(1/\u03b4).\nWith Theorem 5 in mind, we may proceed to a bound on the number of components with mistakes when the basic component estimator (9) is used.\nLemma 3 (Formal Version of Lemma 1). For all \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the draw of X,\nmin s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302W 6= YW } \u2264 2 \u2211 W\u2208W 2|W |pdmincut(W )/2e + 6 max e\u2208E |W(e)| max W\u2208W |E\u2032(W )| log(1/\u03b4). (13)\n(14)\nProof of Lemma 3. Define a random variable S(X) = \u2211 W\u2208W min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } ,\nwhere Y\u0302 W are the component-wise estimators produced by Algorithm 1 and X are the edge observations. To prove the lemma we will apply Theorem 5 by showing that there is a constant a such that the necessary variance bound V+ \u2264 aS holds.\nTo this end, consider\nS(X)\u2212 S(X(e)) = \u2211 W\u2208W ( min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } \u2212 min s\u2208{\u00b11} 1 { sY\u0302 W (X(e)) 6= Y W }) ,\nwhere X(e) is defined as in Theorem 5. To be more precise, we draw (X \u2032e)e\u2208E from the same distribution as X, then let X(e) be the result of replacing Xe with X \u2032 e.\nWe have\nS(X)\u2212 S(X(e)) = \u2211\nW\u2208W(e)\n( min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } \u2212 min s\u2208{\u00b11} 1 { sY\u0302 W (X(e)) 6= Y W }) ,\nsince changing Xe can only change Y\u0302 W if e \u2208W . Now, since S(X(e)) is nonnegative we have\n(S(X)\u2212 S(X(e))2+ =  \u2211 W\u2208W(e) ( min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } \u2212 min s\u2208{\u00b11} 1 { sY\u0302 W (X(e)) 6= Y W })2 +\n\u2264  \u2211 W\u2208W(e) min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W }2\n\u2264 |W(e)| \u2211\nW\u2208W(e)\nmin s\u2208{\u00b11} 1\n{ sY\u0302 W (X) 6= Y W } .\nWe now sum over all edges to arrive at an upper bound on V+: V+ = \u2211 e\u2208E E [ (S(X)\u2212 S(X(e))2+ | X ] \u2264 max\ne\u2208E |W(e)| \u2211 e\u2208E \u2211 W\u2208W(e) min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } = max\ne\u2208E |W(e)| \u2211 W\u2208W \u2211 e\u2208E(W ) min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } \u2264 max\ne\u2208E |W(e)| max W\u2208W |E(W )| \u2211 W\u2208W min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } \u2264 max\ne\u2208E |W(e)| max W\u2208W |E(W )| \u2211 W\u2208W min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W } = max\ne\u2208E |W(e)| max W\u2208W |E(W )|S(X).\nWe now appeal to Theorem 5 with a = maxe\u2208E |W(e)|maxW\u2208W |E(W )|, which yields that with probability at least 1\u2212 \u03b4\nS \u2264 2E[S] + 6 max e\u2208E |W(e)| max W\u2208W |E(W )| log(1/\u03b4).\nFinally, the bound on E[S] follows from Proposition 1:\nE[S] = \u2211 W\u2208W P ( min s\u2208{\u00b11} 1 { sY\u0302 W (X) 6= Y W }) \u2264 \u2211 W\u2208W 2|W |pdmincut(W )/2e.\nAn analogous concentration result to Lemma 3 holds to bounds the number of components that fail over the whole graph when the extended component estimator is used:\nLemma 4. For all \u03b4 > 0, with probability at least 1\u2212 \u03b4 over the draw of X,\nmin s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302W ? 6= YW ? } \u2264 2 \u2211 W\u2208W 2|W ?|pdmincut ?(W )/2e + 6 max e\u2208E |W?(e)| max W\u2208W |E\u2032(W ?)| log(1/\u03b4).\n(15)\nProof of Lemma 4. This proof proceeds exactly as in the proof of Lemma 3 using S(X) = \u2211 W\u2208W min s\u2208{\u00b11} 1 { sY\u0302 W ? (X) 6= Y W } .\nThe only difference is that edges are more influential than in that lemma because each extended component estimator Y\u0302 W ? may depend on more edges than the simpler component estimator Y\u0302 W . To this end, define W?(e) = {W | e \u2208 E\u2032(W ?)}. One can verify that if we replace every instance of W(e) in the proof of Lemma 3 withW?(e) it holds that V+ \u2264 aS with a = maxe\u2208E |W?(e)|maxW\u2208W |E(W ?)|. Theorem 5 then implies that with probability at least 1\u2212 \u03b4,\nS \u2264 2E[S] + 6 max e\u2208E |W?(e)| max W\u2208W |E(W ?)| log(1/\u03b4)\n= 2E[S] + 6deg?E(T ) max W\u2208W |E(W ?)| log(1/\u03b4).\nProof of Theorem 2. Full theorem statement We will prove the following error bound: If T = (W, F ) is admissible, with probability at least 1\u2212 \u03b4 over the draw of X and Z, Y\u0302 satisfies:\u2211\nv\u2208V 1\n{ Y\u0302v 6= Yv } (16)\n\u2264 O\n( 1\n2\n( 2wid ?(T ) \u2211\nW\u2208W pdmincut ?(W )/2e + deg?E(T ) max W\u2208W |E(W ?)| log(1/\u03b4)\n) \u00b7 (wid(T ) + deg(T ) log n) ) (17)\nThis statement specializes to (7) when all of the tree decomposition quantities are constant and \u03b4 = 1/n.\nError bound for individual components Lemma 2 implies that for a fixed component W \u2208 W , the probability that the estimator produced by the brute-force enumeration routine fails to exactly recover the labels in W (up to sign) is bounded as\nP (\nmin s{\u00b11}\n1{sY\u0302 W ? 6= Y W } > 0 ) \u2264 2|W ?|pdmincut ?(W )/2e.\nError bound across all components Consider the following random variable, which is the total number components\nS(X) = \u2211 W\u2208W min s\u2208{\u00b11} 1 { sY\u0302 W ? (X) 6= Y W } .\nThe bound on component failure probability immediately implies in in-expectation bound on S: E[S] \u2264 \u2211 W\u2208W 2|W ?|pdmincut ?(W )/2e.\nLemma 4 shows that S concentrates tightly around its expectation. More precisely, let A = 6deg?E(T ) maxW\u2208W |E(W ?)| and\nKn = 2 wid?(T )+2 \u2211 W\u2208W pdmincut ?(W )/2e +A log(2/\u03b4). (18)\nThen Lemma 4 implies that with probability at least 1\u2212 \u03b4/2,\nmin s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302 W ? 6= Y W } \u2264 2 \u2211 W\u2208W 2|W ?|pdmincut ?(W )/2e +A log(2/\u03b4)\n\u2264 Kn (19)\nInference with side information: Hypothesis class Consider the following binary signing of the components in T :\ns? = arg min s\u2208{\u00b11}W \u2211 W\u2208W 1{sW Y\u0302 W ? 6= Y W }.\ns? is signing of the component-wise predictions (Y\u0302 W ? ) that best matches the ground truth. If we knew the value of s? we could use it to produce a vertex prediction with at most Kn mistakes.\nComputing the s? is information-theoretically impossible because we do not have access to Y , but we will show that the signing we produce using the side information Z is close.\nLet Ln = deg(T ) \u00b7Kn. We will argue that (19) implies that s? lies in the class\nF(X) , s \u2208 {\u00b11}W | \u2211 (W1,W2)\u2208F 1{sW1 6= sW2 \u00b7 S(W1,W2)} \u2264 Ln . (20) First, consider the For loop on Algorithm 1, Line 12. Proposition 2 implies that S(W1,W2) as defined in this loop is well-defined, because there always exists some v \u2208W1 \u2229W2.\nSecond, consider the value of\u2211 (W1,W2)\u2208F 1{s?W1 6= s ? W2 \u00b7 S(W1,W2)} = \u2211 (W1,W2)\u2208F 1{s?W1 6= s ? W2 \u00b7 Y\u0302 W ?1 v \u00b7 Y\u0302 W ?2 v }.\nWe can bound this quantity in terms of the number of components W for which\nmin s\u2208{\u00b11} 1\n{ sY\u0302 W ? 6= Y W } = 1.\nObserve that if mins\u2208{\u00b11} 1 { sY\u0302 W ? 6= Y W } = 0 then there is some s\u0304W \u2208 {\u00b11} such that Y\u0302 W ? = s\u0304WY W . If we take s?W = s\u0304W in all the components with no errors, and choose the sign arbitrarily for others, we will have 1{s?W1 6= s ? W2 \u00b7 Y\u0302 W ? 1 v \u00b7 Y\u0302 W ?2 v } = 0 whenever both W1 and W2 have no errors. Pessimistically, there are at most Ln = deg(T ) \u00b7Kn edges (W1,W2) where at least one of W1 or W2 has an error, and therefore (19) implies that with probability at least 1\u2212 \u03b4/2, s? \u2208 F .\nWe conclude this discussion by showing that |F(X)| small. Since by Proposition 2 T is connected, labelings of the edges of T are in one to one correspondence with labelings of the components. Consequently,\n|F(X)| \u2264 Kn\u2211 k=0 ( |W| k ) \u2264 ( e|W| Kn )Kn \u2264 ( en Kn )Kn . (21)\nThe last inequality uses that, from Proposition 2, |W| \u2264 n.\nFinal error bound for inference with side information We now use the properties of F(X) to derive an error bound for the prediction Y\u0302 . Recall from Algorithm 1 that Y\u0302 is defined in terms of\ns\u0302 = min s\u2208F(X) \u2211 W\u2208W \u2211 v\u2208W 1{sW Y\u0302 W ? v 6= Zv}. (22)\nWe reduce the analysis of the error rate of s\u0302 to analysis of excess risk in a manner that parallels the proof of Theorem 1, but is slightly more involved because the best predictor in F does not perfectly\nmatch the ground truth. Fix s\u0302 \u2208 {\u00b11}W . For each component W \u2208 W we have\u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= Yv} \u2264 \u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= s\u2217W Y\u0302 W ? v }+ \u2211 v\u2208W 1{s\u2217W Y\u0302 W ? v 6= Yv}\n\u2264 \u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= s\u2217W Y\u0302 W ? v }+ |W | 1{s\u2217W Y\u0302 W ? 6= Y W } = 1 1\u2212 2q \u2211\nv\u2208W :s\u2217W Y\u0302W ? v =Yv\n( PZ ( s\u0302W Y\u0302 W ? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W ? v \u00b7 Zv < 0 ))\n\u2212 1 1\u2212 2q \u2211 v\u2208W :s\u2217W Y\u0302W ? v 6=Yv ( PZ ( s\u0302W Y\u0302 W ? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W ? v \u00b7 Zv < 0 )) + |W | 1{s\u2217W Y\u0302W 6= YW }.\nNow note that given that Zv is drawn as a noisy version of Yv,\u2223\u2223\u2223PZ (s\u0302W Y\u0302 W ?v \u00b7 Zv < 0)\u2212 PZ (s\u2217W Y\u0302 W ?v \u00b7 Zv < 0)\u2223\u2223\u2223 = 1\u2212 2q and so \u2212 1 1\u2212 2q \u2211\nv\u2208W :s\u2217W Y\u0302 W ? v 6=Yv\n( PZ ( s\u0302W Y\u0302 W? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W? v \u00b7 Zv < 0 ))\n\u2264 2 \u2211 v\u2208W 1{s\u2217W Y\u0302W ? v 6= Yv}+ 1 1\u2212 2q \u2211\nv\u2208W :s\u2217W Y\u0302 W ? v 6=Yv\n( PZ ( s\u0302W Y\u0302 W? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W? v \u00b7 Zv < 0 ))\n\u2264 2|W |1{s\u2217W Y\u0302W ? 6= YW }+ 1 1\u2212 2q \u2211 v\u2208W :s\u2217W Y\u0302 W ? v 6=Yv ( PZ ( s\u0302W Y\u0302 W? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W? v \u00b7 Zv < 0 )) .\nWe thus conclude that\u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= Yv}\n\u2264 3|W |1{s\u2217W Y\u0302 W ? 6= Y W }+ 1 1\u2212 2q \u2211 v\u2208W ( PZ ( s\u0302W Y\u0302 W ? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W ? v \u00b7 Zv < 0 )) .\nSumming over all the components W \u2208 W we arrive at the bound\u2211 W\u2208W \u2211 v\u2208W 1{s\u0302W Y\u0302W ? v 6= Yv}\n\u2264 3 (\nmax W\u2208W\n|W | ) \u2211\nw\u2208W 1{s\u2217W Y\u0302W\n? 6= YW }+ 1 1\u2212 2q \u2211 W\u2208W \u2211 v\u2208W ( PZ ( s\u0302W Y\u0302 W? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W? v \u00b7 Zv < 0 ))\n\u2264 3 (\nmax W\u2208W\n|W | ) Kn + 1 1\u2212 2q \u2211\nW\u2208W \u2211 v\u2208W ( PZ ( s\u0302W Y\u0302 W? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W? v \u00b7 Zv < 0 ))\nWe can now appeal to our statistical learning tools to bound the RHS of this expression. Lemma 5 implies that if we take s\u0302 = arg mins\u2208F \u2211 W\u2208W \u2211 v\u2208W 1 { s\u0302W Y\u0302 W ? v \u00b7 Zv < 0 } , which is precisely the solution to (22), we obtain the excess risk bound,\u2211 W\u2208W \u2211 v\u2208W ( PZ ( s\u0302W Y\u0302 W ? v \u00b7 Zv < 0 ) \u2212 PZ ( s\u2217W Y\u0302 W ? v \u00b7 Zv < 0 ))\n\u2264 ( 2\n3 + c 2\n) log(2|F|/\u03b4) + 1\nc \u2211 w\u2208W \u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= Yv},\nwith probability at least 1\u2212 \u03b4/2 over Z for all c > 0. If we choose c = 1/ , rearrange, and apply the union bound, this implies that with probability at least 1\u2212 \u03b4 over the draw of X and Z we have\u2211\nW\u2208W \u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= Yv} \u2264 6 ( max W\u2208W |W | ) Kn + 2 2 log(2|F|/\u03b4).\nRecall that |F| \u2264 (e|W|/Ln)Ln , which implies a bound of\n\u2211 W\u2208W \u2211 v\u2208W 1{s\u0302W Y\u0302 W ? v 6= Yv}\n\u2264 O ( 1\n2 [wid(T ) \u00b7Kn + Ln \u00b7 log(en/Ln) + log(1/\u03b4)] ) \u2264 O ( 1\n2 [Kn \u00b7 (wid(T ) + deg(T ) \u00b7 log(en/Kn)) + log(1/\u03b4)] ) \u2264 O ( 1\n2\n( 2wid\n?(T ) \u2211 W\u2208W pdmincut ?(W )/2e + deg?E(T ) max W\u2208W |E(W ?)| log(1/\u03b4)\n) \u00b7 (wid(T ) + deg(T ) log n) )\nOur choice of Y\u0302 in Algorithm 1 ensures that the Hamming error \u2211 v\u2208V 1 { Y\u0302v 6= Yv } inherits this bound. Proposition 2 implies that every v \u2208 V is in some component, so this choice is indeed well-defined."}, {"heading": "C Statistical Learning Theory", "text": "Here we consider a fixed design version of the statistical learning setting. Fix an input space X and output space Z. We are given a fixed set X1, . . . , Xn \u2208 X and samples Z1, . . . , Zn \u2208 Z with Zi drawn from P (Zi | Xi) for some distribution P . We fix a hypothesis class F which is some subset of mappings from X to Z, and we would like to use Z to find Y\u0302 \u2208 F that will predict future observations of Z on X. To evaluate prediction we define a loss function ` : Z \u00d7 Z \u2192 R+, and define Li(Y ) = EZ|Xi [`(Y,Z)]. Our goal is to use Z to select Y\u0302 \u2208 F to minimize the excess risk :\u2211\ni\u2208[n]\nLi(Y\u0302 (Xi))\u2212 min Y \u2208F \u2211 i\u2208[n] Li(Y (Xi)). (23)\nTypically this is accomplished using the empirical risk minimizer (ERM):\nY\u0302 = arg min Y \u2208F \u2211 i\u2208[n] `(Y (Xi), Zi) 4.\nIn this paper we consider a specific instantiation of the above framework in which\n\u2022 X = V , the vertex set for some graph (possibly a tree decomposition), and X1, . . . , Xn are an arbitrary ordering of V (so n = |V |). In light of this we index all variables using V going forward.\n\u2022 Z = {\u00b11}. We fix Y \u2208 {\u00b11}V and let Zv = Yv with probability 1\u2212 q and Zv = \u2212Yv otherwise (as in Model 1).\n4There are a many standard bounds quantifying the performance of ERM in settings beyond the one we consider. See Bousquet et al. (2004) for a survey.\n\u2022 `(Y, Z) = 1{Y 6= V }, so Li(Y ) = PZ(Y 6= Zv).\n\u2022 F \u2286 {\u00b11}V can be arbitrary.\nFor this setting the excess risk for a predictor Y\u0302 \u2208 {\u00b11}V can be written as\u2211 v\u2208V P(Y\u0302v 6= Zv)\u2212 min Y \u2032\u2208F \u2211 v\u2208V P ( Y \u2032v 6= Zv ) , (24)\nand the empirical risk minimizer is given by Y\u0302 = arg minY \u2032\u2208F \u2211\nv\u2208V 1{Y \u2032v 6= Zv}. We assume this setting exclusively for the remainder of the section.\nLemma 5 (Excess Risk Bound for ERM). Let Y\u0302 be the ERM and let Y ? = arg minY \u2032\u2208F \u2211\nv\u2208V P(Y \u2032 6= Z). Then with probability at least 1\u2212 \u03b4 over the draw of Z,\u2211\nv\u2208V P ( Y\u0302v 6= Zv ) \u2212 min Y \u2032\u2208F \u2211 v\u2208V P ( Y \u2032v 6= Zv ) \u2264 ( 2 3 + c 2 ) log ( |F| \u03b4 ) + 1 c \u2211 v\u2208V 1 { Y\u0302v 6= Y ?v } (25)\nfor all c > 0.\nCorollary 2 (ERM Excess Risk: Well-Specified Case). When Y \u2208 F we have that with probability at least 1\u2212 \u03b4, \u2211\nv\u2208V P ( Y\u0302v 6= Zv ) \u2212 min Y \u2208F \u2211 v\u2208V P (Yv 6= Zv) \u2264 ( 4 3 + 1 ) log ( |F| \u03b4 ) , (26)\nrecalling q = 1/2\u2212 .\nProof of Corollary 2. When Y \u2208 F , Y ? = Y , and we have\u2211 v\u2208V 1 { Y\u0302v 6= Yv } = 1 1\u2212 2q \u2211 v\u2208V ( P ( Y\u0302v 6= Zv ) \u2212 P(Yv 6= Zv) ) .\nApplying this inequality to the right hand side of (25) and rearranging yields( 1\u2212 1\nc(1\u2212 2q) )\u2211 v\u2208V ( P ( Y\u0302v 6= Zv ) \u2212 P(Yv 6= Zv) ) \u2264 ( 2 3 + c 2 ) log(|F|/\u03b4).\nTo complete the proof we take c = 21\u22122q , which gives\n1\n2 \u2211 v\u2208V ( P ( Y\u0302v 6= Zv ) \u2212 P(Yv 6= Zv) ) \u2264 ( 2 3 +\n1\n1\u2212 2q\n) log(|F|/\u03b4).\nProof of Lemma 5. We will use Lemma 6 with F as the index set so that every i \u2208 [N ] corresponds to one Y \u2032 \u2208 F . We define our collection of random variables as\nT Y \u2032 v = 1{Y \u2032v 6= Zv} \u2212 1{Y ?v 6= Zv}\nwhere Y is the ground truth and Y \u2032 is any element of F . Now using Lemma 6 and recalling \u03c32Y \u2032 = \u2211 v\u2208V Var(T\nY \u2032 v ), we have that with probability at least 1\u2212 \u03b4, simultaneously for all Y \u2032,\u2211 v\u2208V (E[T Y \u2032 v ]\u2212 T Y \u2032 v ) \u2264 2 3 log(|F|/\u03b4) + \u221a 2\u03c32Y \u2032 log(|F|/\u03b4)\n\u2264 inf c>0\n[( 2\n3 + c 2\n) log(|F|/\u03b4) + \u03c32Y \u2032/c ] \u2264 inf\nc>0\n[( 2\n3 + c 2\n) log(|F|/\u03b4) + 1\nc \u2211 v\u2208V E[(T Y \u2032 v ) 2]\n] .\nIn particular this implies that for Y\u0302 = arg minY \u2032\u2208F \u2211\nv\u2208V 1{Y \u2032v 6= Zv} we have that for all c > 0,\u2211 v\u2208V ( P ( Y\u0302v 6= Zv ) \u2212 P (Y ?v 6= Zv) ) \u2264 \u2211 v\u2208V ( 1 { Y\u0302v 6= Zv } \u2212 1 {Y ?v 6= Zv} ) + ( 2 3 + c 2 ) log(|F|/\u03b4)\n+ 1\nc \u2211 v\u2208V E [( 1{Y\u0302v 6= Zv} \u2212 1{Y ?v 6= Zv} )2] .\nNow since Y ? \u2208 F and Y\u0302 is the ERM, we get that \u2211\nv\u2208V\n( 1 { Y\u0302v 6= Zv } \u2212 1 {Y ?v 6= Zv} ) \u2264 0\nand so,\u2211 v\u2208V ( P ( Y\u0302v 6= Zv ) \u2212 P (Y ?v 6= Zv) ) \u2264 ( 2 3 + c 2 ) log(|F|/\u03b4) + 1 c \u2211 v\u2208V E [( 1{Y\u0302v 6= Zv} \u2212 1{Y ?v 6= Zv} )2] = ( 2\n3 + c 2\n) log(|F|/\u03b4) + 1\nc \u2211 v\u2208V 1 { Y\u0302v 6= Y ?v } .\nLemma 6 (Maximal Inequality). For each i \u2208 [N ], let {T iv}v\u2208V be a random process with each variable T iv bounded in absolute value by 1. Define \u03c3 2 i = \u2211 v\u2208V Var(T i v). With probability at least 1\u2212 \u03b4, \u2211 v\u2208V (E[T iv]\u2212 T iv) \u2264 2 3 log(N/\u03b4) + \u221a 2\u03c32i log(N/\u03b4) \u2200i \u2208 [N ]. (27) Proof of Lemma 6. Let us start by writing out the Bernstein bound for the random variable\u2211n t=1 Z i t :\nP (\u2211 v\u2208V (E[T iv]\u2212 T iv) > \u03b8 ) \u2264 exp ( \u2212 \u03b8 2 i 2\u03c32i + 2 3\u03b8i ) .\nWe now consider the family of processes {T iv}v\u2208V and see that by union bound we have\nP ( max i\u2208[N ] \u2211 v\u2208V (E[T iv]\u2212 T iv)\u2212 \u03b8i > 0 ) \u2264 \u2211 i\u2208[N ] exp ( \u2212 \u03b8 2 i 2\u03c32i + 2 3\u03b8i ) .\nSolving the quadratic formula we see that if we take\n\u03b8i \u2265 1\n3 log(N/\u03b4) +\n\u221a log2(N/\u03b4)/9 + 2\u03c32i log(N/\u03b4),\nthen we have \u2211 i\u2208[N ] exp\n( \u2212 \u03b8 2 i\n2\u03c32i + 4 3\n) \u2264 \u03b4.\nWe can conclude that\nP ( \u2200i \u2208 [N ],\n\u2211 v\u2208V (E[T iv]\u2212 T iv) > 1 3 log(N/\u03b4) + \u221a log2(N/\u03b4)/9 + 2\u03c32i log(N/\u03b4) ) \u2264 \u03b4."}, {"heading": "D Algorithms", "text": "The tree inference algorithm from Section 2 and the full tree decomposition inference algorithm, Algorithm 1, rely on the solution of a constrained minimization problem over the edges and vertices of a tree. This minimization problem is stated in its most general form as Algorithm 2. This problem can be solved efficiently using the following tree-structured graphical model:\n\u2022 Fix an arbitrary order on T , and let p(v) denote the parent of a vertex v under this order.\n\u2022 Define variables s \u2208 {\u00b11}V and C \u2208 {1, . . . ,Kn}V .\n\u2022 For each variable v \u2208 V define factor:\n\u03c8v(sv, sp(v),Cv ,C\u03b4+(v) ) = e\u22121{Costv [sv ]} \u00b7 1  \u2211 u\u2208\u03b4+(v) Cu \u2264 Cv \u2212 1 { sv 6= sp(v) \u00b7 S(v, p(v)) }. With this formulation it is clear that given (s, C) maximizing the potential\n\u03c8(s, C) = \u220f v\u2208V \u03c8v(sv, sp(v),Cv ,C\u03b4+(v) )\nthe node labels s are a valid solution for Algorithm 2. Since \u03c8 is a tree-structured MRF the maximizer can be calculated exactly using max-sum message passing (see e.g. Cowell et al. (2006)). The only catch is that naively this procedure\u2019s running time will scale as ndeg(T ), because each of the variables Cv has a range that scales with n. For example, the range of Cv is O\u0303(pn) for the setup in Section 2. We now show that the structure of the factors can be exploited to perform message passing in polynomial time in deg(T ) and n. In particular, message passing can be performed in time time O\u0303(deg(T )Knn 3) for general trees and time O\u0303(Knn 2) when T is a line graph.\nAlgorithm 2 TreeDecoder Input: Tree T = (V,E), {Costv}v\u2208V , {S(u, v)}(u,v)\u2208E , Kn \u2208 N.\ns\u0302 = arg min s\u2208{\u00b11}V \u2211 v\u2208V Costv[sv]\ns.t. \u2211\n(u,v)\u2208E\n1{su 6= sv \u00b7 S(u, v)} \u2264 Kn\nReturn: s\u0302 \u2208 {\u00b11}V .\nTo solve TreeDecoder efficiently, we first turn T into a DAG by running a BFS from a given vertex r and directing edges according to the time of discovery. We denote this DAG by \u2212\u2192 T . We root this directed tree at r, and denote the parent of a vertex u 6= r by p(u). For u \u2208 V , let \u2212\u2192 T u denote the (directed) subtree rooted at u. Given a labeling Y to the vertices of T , an edge uv for which su 6= sv \u00b7 S(u, v) is called a violated edge.\nWe now define a table OPT that will be used to store values for sub-problems of Algorithm 2. For u 6= r, and budget K, we define OPT (u,K|1) to be the optimal value of the optimization problem in Algorithm 2 over the subtree \u2212\u2192 T u for budget K, where the label of p(u) is constrained to have value 1. Importantly, the edge (u, p(u)) is also considered in the count of violated edges (in addition to the edges in \u2212\u2192 T u). OPT (u,K| \u2212 1) is defined likewise, but for p(u) constrained to label value \u22121.\nOPT (u,K|1) = min s\u2208{\u22121,1} min\u2211 v\u2208Nu Kv=K\u22121{s 6=Sp(u)\u00b7S(u,p(u))}  \u2211 v\u2208N(u) OPT (v,Kv|s) + Costv[s]  . Here s is simply the value assigned to u. We constrain the budgets Kv to satisfy 0 \u2264 Kv \u2264 | \u2212\u2192 T v|\n(clearly no subtree \u2212\u2192 T v can violate more than | \u2212\u2192 T v| edges). For the sake of readability, we do not include this constraint in the recursive formula above. A similar recursion can be obtained for OPT (u,K| \u2212 1).\nOne can verify that if we can compute OPT (u,K|s) for all nonroot nodes and all values of K \u2264 Kn, s \u2208 {\u22121, 1} then we can find the optimum of the problem of our whole tree. To achieve this, simply attach a degree one node r\u2032 to the root of the tree, add a directed edge (r\u2032, r) and set the label of the root to equal 1. Then we simply solve for OPT (r,K|1), where S(r, r\u2032) = 1 as well as OPT (r,K, 1), where S(r\u2032, r) is \u22121 and return the minimum of the the values.\nFor a leaf node w, the value of OPT (w,K \u2032|s) can be calculated as follows: it is min(cost[sw = \u22121], cost[sw = 1]), for K \u2032 \u2265 1. If K = 0, it is cost[s\u2032] where s\u2032 is the unique label not violating the constraint s 6= s\u2032 \u00b7 S(w, p(w))\nWe now show how to calculate OPT (u,Ku|s) for any vertex in the tree, assuming OPT has already been calculated for its children. To do this, we try both values of su, and then condition on its value to optimize\nmin\u2211 j\u2208[1,k]Kj=K\u22121{s 6=sp(u)\u00b7S(u,p(u))} \u2211 u\u2208[1,k] OPT (j,Kj |s).\nThe function \u2211\nv\u2208Nu OPT (v,Kv|s) can be minimized using another layer of dynamic programming as follows: For r \u2264 s, let [r, s] be the set of integers between r and s. Assuming we enumerate the vertices in N(u) by 1, ..., k := |N(u)| and setting Kj to be the budget for the jth node, we have the equality\nmin\u2211 j\u2208[1,k]Kj=K\u22121{s 6=sp(u)\u00b7S(u,p(u))} \u2211 u\u2208[1,k] OPT (j,Kj |s)\n= min K1\u2208[0,K\u22121{s 6=sp(u)\u00b7S(u,p(u))}] OPT (1,K1|s)+ min\u2211 j\u2208[2,k]Kj=K\u2212K1\u22121{s6=sp(u)\u00b7S(u,p(u))} \u2211 j\u2208[2,k] OPT (j,Kj |s).\nThe minimization problem can be solved in time O(|N(u)|K2n) time. We first calculate the minimum cost for the first two vertices where the number of constraints violated can range between 1 to K. This can be done in time O(K2). We then examine the minimum cost for the first\nthree vertices (assuming of course u has at least three descendants) where the number of violated constraints ranges between 0 and K. Since we have the information for the first two vertices, these values can be calculated again in time O(K2). We repeat this iteration until all descendants of u are considered. It follows that the overall running time of this algorithm is \u2211 u\u2208V |N(u)|K2n = O(nK2n), since T is a tree. When T is a line graph each node has a single child, the recursion collapses to time O(nKn)."}, {"heading": "E Further Techniques for General Graphs", "text": "Here we give a simple proof that if the minimal degree of G is \u2126(log n), then there is an algorithm that achieves arbitrarily small error for each vertex as n\u2192\u221e as soon as q = 1/2\u2212 is constant.\nTheorem 6. There is an efficient algorithm that guarantees\nE [\u2211 v\u2208v 1 { Y\u0302v 6= Yv }] \u2264 \u2211 v\u2208V exp(\u2212Cdeg(v) 2(1\u2212 2p)2).\nfor some C > 0.\nObserve that this rate quickly approaches 0 with n as soon as deg(G) = \u2126(log n) (i.e., it has o(n) Hamming error) . On the other hand, if degree is constant (say d), then even when p = 0 the rate of this algorithm is only e\u2212dO(\n2)n, so the algorithm does not have the desired property of having error approach 0 as p\u2192 0.\nProof. Fix a vertex v and, for each vertex u in its neighborhood, define an estimate Su = Zu \u00b7Xuv. We can observe that P(Su = Yv) = (1\u2212 p)(1\u2212 q) + pq = 12 + (1\u2212 2p). Our algorithm will be to use the estimator Y\u0302v = Majority({Su}u\u2208N(v)). Since each Su is independent, the Hoeffding bound gives that\nP(Y\u0302v 6= Yv) \u2264 exp(\u2212Cdeg(v) 2(1\u2212 2p)2).\nTaking this prediction for each vertex gives an expected hamming error bound of\nE [\u2211 v\u2208v 1 { Y\u0302v 6= Yv }] \u2264 \u2211 v\u2208V exp(\u2212Cdeg(v) 2(1\u2212 2p)2)."}], "references": [{"title": "Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery", "author": ["Emmanuel Abbe", "Afonso S Bandeira", "Annina Bracher", "Amit Singer"], "venue": "Network Science and Engineering, IEEE Transactions on,", "citeRegEx": "Abbe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abbe et al\\.", "year": 2014}, {"title": "Lp-based robust algorithms for noisy minor-free and bounded treewidth graphs", "author": ["Nikhil Bansal", "Daniel Reichman", "Seeun William Umboh"], "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Bansal et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2017}, {"title": "A linear-time algorithm for finding tree-decompositions of small treewidth", "author": ["Hans L Bodlaender"], "venue": "SIAM Journal on computing,", "citeRegEx": "Bodlaender.,? \\Q1996\\E", "shortCiteRegEx": "Bodlaender.", "year": 1996}, {"title": "Concentration inequalities using the entropy method", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "Annals of Probability,", "citeRegEx": "Boucheron et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2003}, {"title": "Introduction to statistical learning theory", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Information recovery from pairwise measurements", "author": ["Yuxin Chen", "Andrea J Goldsmith"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Chen and Goldsmith.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Goldsmith.", "year": 2014}, {"title": "Community recovery in graphs with locality", "author": ["Yuxin Chen", "Govinda Kamath", "Changho Suh", "David Tse"], "venue": "In International Conference on Machine Learning. International Machine Learning Society,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Probabilistic networks and expert systems: Exact computational methods for Bayesian networks", "author": ["Robert G Cowell", "Philip Dawid", "Steffen L Lauritzen", "David J Spiegelhalter"], "venue": "Springer Science & Business Media,", "citeRegEx": "Cowell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cowell et al\\.", "year": 2006}, {"title": "Anatomy of the giant component: The strictly supercritical regime", "author": ["Jian Ding", "Eyal Lubetzky", "Yuval Peres"], "venue": "European Journal of Combinatorics,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Expansion and lack thereof in randomly perturbed graphs", "author": ["Abraham D Flaxman"], "venue": "Internet Mathematics,", "citeRegEx": "Flaxman.,? \\Q2007\\E", "shortCiteRegEx": "Flaxman.", "year": 2007}, {"title": "Community detection in graphs", "author": ["Santo Fortunato"], "venue": "Physics reports,", "citeRegEx": "Fortunato.,? \\Q2010\\E", "shortCiteRegEx": "Fortunato.", "year": 2010}, {"title": "Tight error bounds for structured prediction", "author": ["Amir Globerson", "Tim Roughgarden", "David Sontag", "Cafer Yildirim"], "venue": "arXiv preprint arXiv:1409.5834,", "citeRegEx": "Globerson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2014}, {"title": "How hard is inference for structured prediction", "author": ["Amir Globerson", "Tim Roughgarden", "David Sontag", "Cafer Yildirim"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Globerson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2015}, {"title": "Computational lower bounds for community detection on random graphs", "author": ["Bruce Hajek", "Yihong Wu", "Jiaming Xu"], "venue": "arXiv preprint arXiv:1406.6625,", "citeRegEx": "Hajek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hajek et al\\.", "year": 2014}, {"title": "Error bounds for correlation clustering", "author": ["Thorsten Joachims", "John E Hopcroft"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning", "citeRegEx": "Joachims and Hopcroft.,? \\Q2005\\E", "shortCiteRegEx": "Joachims and Hopcroft.", "year": 2005}, {"title": "Spectral redemption in clustering sparse networks", "author": ["Florent Krzakala", "Cristopher Moore", "Elchanan Mossel", "Joe Neeman", "Allan Sly", "Lenka Zdeborov\u00e1", "Pan Zhang"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Krzakala et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krzakala et al\\.", "year": 2013}, {"title": "Correlation clustering with noisy partial information", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "Makarychev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Makarychev et al\\.", "year": 2015}, {"title": "Local algorithms for block models with side information", "author": ["Elchanan Mossel", "Jiaming Xu"], "venue": "In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Mossel and Xu.,? \\Q2016\\E", "shortCiteRegEx": "Mossel and Xu.", "year": 2016}, {"title": "Renormalization group analysis of the small-world network model", "author": ["Mark EJ Newman", "Duncan J Watts"], "venue": "Physics Letters A,", "citeRegEx": "Newman and Watts.,? \\Q1999\\E", "shortCiteRegEx": "Newman and Watts.", "year": 1999}, {"title": "Counting connected graphs inside-out", "author": ["Boris Pittel", "Nicholas C Wormald"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "Pittel and Wormald.,? \\Q2005\\E", "shortCiteRegEx": "Pittel and Wormald.", "year": 2005}, {"title": "Graph minors. ii. algorithmic aspects of tree-width", "author": ["Neil Robertson", "Paul D. Seymour"], "venue": "Journal of algorithms,", "citeRegEx": "Robertson and Seymour.,? \\Q1986\\E", "shortCiteRegEx": "Robertson and Seymour.", "year": 1986}, {"title": "Spectral detection in the censored block model", "author": ["Alaa Saade", "Florent Krzakala", "Marc Lelarge", "Lenka Zdeborova"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Saade et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Saade et al\\.", "year": 2015}, {"title": "Near-optimal anomaly detection in graphs using lovasz extended scan statistic", "author": ["James L Sharpnack", "Akshay Krishnamurthy", "Aarti Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sharpnack et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sharpnack et al\\.", "year": 2013}, {"title": "Tightening lp relaxations for map using message passing", "author": ["David Sontag", "Talya Meltzer", "Amir Globerson", "Tommi S Jaakkola", "Yair Weiss"], "venue": "arXiv preprint arXiv:1206.3288,", "citeRegEx": "Sontag et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2012}, {"title": "2015) consider correlation clustering where the model is a semi-random variant of the one we consider for the edge inference problem: Fix a graph G = (V,E) and a vertex label Y . For each uv \u2208 E, we observe Xuv where Xuv = YuYv with probability 1 \u2212 p and has its value in selected by an adversary otherwise", "author": ["Makarychev"], "venue": null, "citeRegEx": "Makarychev,? \\Q2015\\E", "shortCiteRegEx": "Makarychev", "year": 2015}, {"title": "The only catch is that naively this procedure\u2019s running time will scale as ndeg(T ), because each of the variables Cv has a range that scales with n. For example, the range of Cv is \u00d5(pn) for the setup in Section 2. We now show that the structure of the factors can be exploited to perform message", "author": ["Cowell"], "venue": null, "citeRegEx": "Cowell,? \\Q2006\\E", "shortCiteRegEx": "Cowell", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "Our method generalizes and improves over (Globerson et al., 2015), who introduced the problem for twodimensional grid lattices.", "startOffset": 41, "endOffset": 65}, {"referenceID": 11, "context": "Our method generalizes and improves over (Globerson et al., 2015), who introduced the problem for twodimensional grid lattices. For trees we provide a simple, efficient, algorithm that infers the ground truth with optimal Hamming error and implies recovery results for all connected graphs. Here, the presence of side information is critical to obtain a non-trivial recovery rate. We then show how to adapt this algorithm to tree decompositions of edge-subgraphs of certain graph families such as lattices, resulting in optimal recovery error rates that can be obtained in a highly efficient manner. The thrust of our analysis is to 1) use the tree decomposition along with edge measurements to produce a small class of viable vertex labelings and 2) apply fast rate results from statistical learning theory to show that we can infer the ground truth from this class using vertex measurements. We show the power of our method by providing several examples including hypergrids, ring lattices, and the Newman-Watts model for small world graphs. For two-dimensional grids, our results improve over Globerson et al. (2015) by obtaining optimal recovery in the constant-height regime.", "startOffset": 42, "endOffset": 1120}, {"referenceID": 10, "context": "Statistical inference over graphs and networks is a fundamental problem that has received extensive attention in recent years (Fortunato, 2010; Krzakala et al., 2013; Abbe et al., 2014; Hajek et al., 2014).", "startOffset": 126, "endOffset": 205}, {"referenceID": 15, "context": "Statistical inference over graphs and networks is a fundamental problem that has received extensive attention in recent years (Fortunato, 2010; Krzakala et al., 2013; Abbe et al., 2014; Hajek et al., 2014).", "startOffset": 126, "endOffset": 205}, {"referenceID": 0, "context": "Statistical inference over graphs and networks is a fundamental problem that has received extensive attention in recent years (Fortunato, 2010; Krzakala et al., 2013; Abbe et al., 2014; Hajek et al., 2014).", "startOffset": 126, "endOffset": 205}, {"referenceID": 13, "context": "Statistical inference over graphs and networks is a fundamental problem that has received extensive attention in recent years (Fortunato, 2010; Krzakala et al., 2013; Abbe et al., 2014; Hajek et al., 2014).", "startOffset": 126, "endOffset": 205}, {"referenceID": 23, "context": "Despite the intractability of maximum likelihood estimation, maximum a-posteriori estimation, and marginal inference in network models in the worst case, it has been observed that approximate inference algorithms work surprisingly well in practice (Sontag et al., 2012), and recent work has focused on improving our theoretical understanding of this phenomenon (Globerson et al.", "startOffset": 248, "endOffset": 269}, {"referenceID": 12, "context": ", 2012), and recent work has focused on improving our theoretical understanding of this phenomenon (Globerson et al., 2015).", "startOffset": 99, "endOffset": 123}, {"referenceID": 12, "context": "A key feature of the inference model introduced in (Globerson et al., 2015) is that in addition to observing noisy edge labels, one also observes noisy vertex labels.", "startOffset": 51, "endOffset": 75}, {"referenceID": 9, "context": "Studying recovery problems in such \u201cnon-expanding\u201d graphs is of interest as many graphs arising in applications such as social networks (Flaxman, 2007) have poor expansion.", "startOffset": 136, "endOffset": 151}, {"referenceID": 11, "context": "For example, for the n-vertex path, it is not difficult (Globerson et al., 2014) to show that when there are only noisy edge observations any algorithm will fail to find the correct We refer the reader to Appendix A for further discussion of related models.", "startOffset": 56, "endOffset": 80}, {"referenceID": 6, "context": "Chen et al. (2016) have recently considered exact recovery for edges in this setting for sparse graphs such as grid and rings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "That phenomenon that side information with constant noise level enables partial recovery in virtually all sparsity regimes was also observed in Mossel and Xu (2016).", "startOffset": 144, "endOffset": 165}, {"referenceID": 11, "context": "This approach has the advantage that it applies to non-planar graphs such as high dimensional grids; it is not clear how to apply the machinery of Globerson et al. (2015) to such graphs both because planar duality no longer applies, and the quadratic program they solve no longer admits an efficient solution.", "startOffset": 147, "endOffset": 171}, {"referenceID": 11, "context": "This approach has the advantage that it applies to non-planar graphs such as high dimensional grids; it is not clear how to apply the machinery of Globerson et al. (2015) to such graphs both because planar duality no longer applies, and the quadratic program they solve no longer admits an efficient solution. Our decomposition-based approach also enables us to obtain optimal error bounds for thin grids which do not have the so-called weak expansion property that is necessary for the analysis in Globerson et al. (2015). See Section 4 for an extensive discussion of concrete graph families we consider and the error rates we achieve.", "startOffset": 147, "endOffset": 523}, {"referenceID": 11, "context": "The assumption on q is necessary as when q approaches 1/2 it is proven in Globerson et al. (2015) that an error of \u03a9(n) is unavoidable for certain trees.", "startOffset": 74, "endOffset": 98}, {"referenceID": 20, "context": "1 Upper Bound: Inference with Tree Decompositions Our main algorithm, TreeDecompositionDecoder (Algorithm 1) produces estimators for Model 1 for graphs G that admit a tree decomposition in the sense of Robertson and Seymour (Robertson and Seymour (1986)).", "startOffset": 202, "endOffset": 254}, {"referenceID": 7, "context": ", Cowell et al. (2006)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "Since components can in fact overlap their predictions are dependent, but using a more sophisticated concentration inequality (the entropy method (Boucheron et al., 2003)) we can show that \u2013 so long as no edge appears in too many components \u2013 an analogous concentration result holds and total number of components failures is close to the expected number with high probability.", "startOffset": 146, "endOffset": 170}, {"referenceID": 8, "context": "It is known that with high probability H(n, p) contains \u03a9(n) vertices of degree at most 2 (Ding et al., 2014; Pittel and Wormald, 2005).", "startOffset": 90, "endOffset": 135}, {"referenceID": 19, "context": "It is known that with high probability H(n, p) contains \u03a9(n) vertices of degree at most 2 (Ding et al., 2014; Pittel and Wormald, 2005).", "startOffset": 90, "endOffset": 135}, {"referenceID": 11, "context": "This estimator runs in time O(dp2nen), By the degree profile argument (also given in Globerson et al. (2015)), there is a matching lower bound of \u03a9(p2n).", "startOffset": 85, "endOffset": 109}, {"referenceID": 11, "context": "This estimator runs in time O(dp2nen), By the degree profile argument (also given in Globerson et al. (2015)), there is a matching lower bound of \u03a9(p2n). For a grid of height c = 1 there is an obvious lower bound of \u03a9(pn) since this graph is a tree. The estimator of Globerson et al. (2015) can be shown to have expected Hamming error of O(p2n) for the 2-dimensional grid with c = \u03a9(log n).", "startOffset": 85, "endOffset": 291}, {"referenceID": 18, "context": "3 Newman-Watts Model To define the Newman-Watts small world model (Newman and Watts, 1999), we first define the regular ring lattice, which serves as the base graph for this model.", "startOffset": 66, "endOffset": 90}, {"referenceID": 11, "context": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels.", "startOffset": 37, "endOffset": 61}, {"referenceID": 11, "context": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels. We make several improvements over Globerson et al. (2015). First, their work provide the right rates for grid graphs and provide upper bounds on recovery rates for planar graphs and expanders.", "startOffset": 37, "endOffset": 231}, {"referenceID": 11, "context": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels. We make several improvements over Globerson et al. (2015). First, their work provide the right rates for grid graphs and provide upper bounds on recovery rates for planar graphs and expanders. Further, the proof techniques in Globerson et al. (2015) critically relied on a weak expansion property.", "startOffset": 37, "endOffset": 423}, {"referenceID": 11, "context": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels. We make several improvements over Globerson et al. (2015). First, their work provide the right rates for grid graphs and provide upper bounds on recovery rates for planar graphs and expanders. Further, the proof techniques in Globerson et al. (2015) critically relied on a weak expansion property. In contrast, we provide a general method to deal with arbitrary graphs that admit tree decompositions of (edge)-subgraphs.We recover the results in Globerson et al. (2015) for grids, but are able to provide rates for graphs that do not satisfy the weak expansion property.", "startOffset": 37, "endOffset": 643}, {"referenceID": 11, "context": "We considered Model 1, introduced in Globerson et al. (2015), of approximately inferring the ground truth labels for nodes of a graph based on noisy edge and vertex labels. We make several improvements over Globerson et al. (2015). First, their work provide the right rates for grid graphs and provide upper bounds on recovery rates for planar graphs and expanders. Further, the proof techniques in Globerson et al. (2015) critically relied on a weak expansion property. In contrast, we provide a general method to deal with arbitrary graphs that admit tree decompositions of (edge)-subgraphs.We recover the results in Globerson et al. (2015) for grids, but are able to provide rates for graphs that do not satisfy the weak expansion property. While Globerson et al. (2015) use vertex labels only to break ties (this is unnecessary if wish to recover only up to a global sign), we show that for more general graphs the vertex labels, though very noisy, play a critical role.", "startOffset": 37, "endOffset": 774}, {"referenceID": 2, "context": "That is, in general is there an efficient procedure to find the probed edge subset E\u2032 and tree decomposition T? While Bodlaender\u2019s celebrated algorithm (Bodlaender, 1996) can efficiently find tree decompositions of constant width, it is unclear how to find the edge subset E\u2032 when E does not directly admit a decomposition of constant width.", "startOffset": 152, "endOffset": 170}, {"referenceID": 1, "context": "Approximation algorithms for this interdiction task exist (Bansal et al., 2017), but we do not know how to translate their approximation guarantees into statistical performance guarantees.", "startOffset": 58, "endOffset": 79}, {"referenceID": 20, "context": "For the Erd\u00f6s-R\u00e9nyi random graph model, G(n, \u03b1/n) both the threshold (how large \u03b1 needs to be in terms of p) for partial Saade et al. (2015) and exact Abbe et al.", "startOffset": 121, "endOffset": 141}, {"referenceID": 0, "context": "(2015) and exact Abbe et al. (2014) recovery have been determined Exact recovery is obtained through maximum likelihood estimation which is generally intractable.", "startOffset": 17, "endOffset": 36}, {"referenceID": 0, "context": "(2015) and exact Abbe et al. (2014) recovery have been determined Exact recovery is obtained through maximum likelihood estimation which is generally intractable. The authors provide a polynomial time algorithm based on semidefinite programming that matches this threshold up to constant factors. We observe that in our setting, due to the presence of side information, there is a simple and efficient algorithm that achieves exact recovery with high probability when the minimal degree is \u03a9(log n): Theorem 6. Such exact recovery algorithms are known for CBM model only under additional spectral expansion conditions Abbe et al. (2014).", "startOffset": 17, "endOffset": 637}, {"referenceID": 5, "context": "Recovery from Pairwise Measurements Chen and Goldsmith (2014) also provides conditions on exact recovery in a censored block model-like setting which, like our own, considers structured classes of graphs.", "startOffset": 36, "endOffset": 62}, {"referenceID": 5, "context": "Recovery from Pairwise Measurements Chen and Goldsmith (2014) also provides conditions on exact recovery in a censored block model-like setting which, like our own, considers structured classes of graphs. Motivated by applications in computational biology and social networks analysis, Chen et al. (2016) have recently considered exact recovery for edges in this setting for sparse graphs such as grid and rings.", "startOffset": 36, "endOffset": 305}, {"referenceID": 14, "context": "An exception is Joachims and Hopcroft (2005), which gives partial recovery results in a model similar to the one we consider, in which a ground truth partition is fixed and the observed edge labels correspond to some noisy notion of similarity.", "startOffset": 16, "endOffset": 45}, {"referenceID": 14, "context": "An exception is Joachims and Hopcroft (2005), which gives partial recovery results in a model similar to the one we consider, in which a ground truth partition is fixed and the observed edge labels correspond to some noisy notion of similarity. However, these authors focus on the case where G is the complete graph. Makarychev et al. (2015) consider correlation clustering where the model is a semi-random variant of the one we consider for the edge inference problem: Fix a graph G = (V,E) and a vertex label Y .", "startOffset": 16, "endOffset": 342}, {"referenceID": 22, "context": "Anomaly Detection Sharpnack et al. (2013) considered anomaly detection on graphs using only vertex measurements.", "startOffset": 18, "endOffset": 42}, {"referenceID": 3, "context": "Theorem 5 (Entropy Method with Efron-Stein Variance (Boucheron et al., 2003)).", "startOffset": 52, "endOffset": 76}, {"referenceID": 4, "context": "See Bousquet et al. (2004) for a survey.", "startOffset": 4, "endOffset": 27}, {"referenceID": 7, "context": "Cowell et al. (2006)).", "startOffset": 0, "endOffset": 21}], "year": 2017, "abstractText": "We consider the statistical problem of recovering a hidden \u201cground truth\u201d binary labeling for the vertices of a graph G up to low Hamming error from noisy edge and vertex measurements. We present new algorithms and a sharp finite-sample analysis for this problem on trees and sparse graphs with poor expansion properties such as hypergrids and ring lattices. Our method generalizes and improves over (Globerson et al., 2015), who introduced the problem for twodimensional grid lattices. For trees we provide a simple, efficient, algorithm that infers the ground truth with optimal Hamming error and implies recovery results for all connected graphs. Here, the presence of side information is critical to obtain a non-trivial recovery rate. We then show how to adapt this algorithm to tree decompositions of edge-subgraphs of certain graph families such as lattices, resulting in optimal recovery error rates that can be obtained in a highly efficient manner. The thrust of our analysis is to 1) use the tree decomposition along with edge measurements to produce a small class of viable vertex labelings and 2) apply fast rate results from statistical learning theory to show that we can infer the ground truth from this class using vertex measurements. We show the power of our method by providing several examples including hypergrids, ring lattices, and the Newman-Watts model for small world graphs. For two-dimensional grids, our results improve over Globerson et al. (2015) by obtaining optimal recovery in the constant-height regime.", "creator": "LaTeX with hyperref package"}}}