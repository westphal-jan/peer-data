{"id": "1703.03073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations", "abstract": "To mitigate these problems to some extent, previous research has used low-precision fixed-point numbers to represent CNN weights and activations, but the minimum required data accuracy of fixed-point numbers varies between different networks and also between different levels of the same network. In this paper, we propose to use floating-point numbers to represent weights and fixed-point numbers to represent activations. We also show that using floating-point representations for weights is more efficient than displaying fixed-point numbers for the same bit width and demonstrate them on popular large CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such a display scheme multiplies and accumulates compact hardware (MAC). Experimental results show that the proposed scheme reduces weight memory by up to 36% and power consumption by up to 50%.", "histories": [["v1", "Wed, 8 Mar 2017 23:49:20 GMT  (1608kb,D)", "http://arxiv.org/abs/1703.03073v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["liangzhen lai", "naveen suda", "vikas chandra"], "accepted": false, "id": "1703.03073"}, "pdf": {"name": "1703.03073.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations", "authors": ["Liangzhen Lai", "Naveen Suda", "Vikas Chandra"], "emails": ["LIANGZHEN.LAI@ARM.COM", "NAVEEN.SUDA@ARM.COM", "VIKAS.CHANDRA@ARM.COM"], "sections": [{"heading": "1. Introduction", "text": "Deep learning spearheaded by convolutional neural networks (CNN) and recurrent neural networks (RNN) has been pushing the frontiers in many computer vision ap-\nPreliminary work. Submitted for review at the International Conference on Machine Learning (ICML) 2017.\nplications (LeCun et al., 2015). Deep learning algorithms have achieved or surpassed human-levels of perception in some applications (He et al., 2016; Xiong et al., 2016), enabling them to be deployed in the real-world applications. The key challenges of deep learning algorithms are the computational complexity and model size, which impede their deployment on end-user client devices, thus limiting them to cloud-based high performance servers. For example, AlexNet (Krizhevsky et al., 2012), a popular CNN model, requires 1.45 billion operations per input image with 240MB weights (Suda et al., 2016).\nMany researchers have explored hardware accelerators for CNNs to enable deployment of CNNs in embedded devices and demonstrated good performance at low power consumption (Qiu et al., 2016; Shin et al., 2017). Reducing the data precision is a commonly used technique for improving the energy efficiency of CNNs. Typically, CNNs are trained on high performance CPU/GPU with 32-bit floating-point data. Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.\nThis work focuses on the number representation schemes for implementing CNN inference. The representation scheme has the following requirements:\n\u2022 Accuracy: the representation should achieve the desired network accuracy with limited bit-width.\n\u2022 Efficiency: the representation can be implemented in hardware efficiently.\n\u2022 Consistency: the representation should be consistent across different CNNs.\nar X\niv :1\n70 3.\n03 07\n3v 1\n[ cs\n.L G\n] 8\nM ar\nBased on these requirements, we propose using floatingpoint numbers for CNN weights and fixed-point numbers for activations. We justify this choice from both algorithmic and hardware implementation perspectives. From the algorithmic perspective, using popular largescale CNNs such as AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al., 2015) and VGG-16 (Simonyan & Zisserman, 2014), we show that the representation range of the weights is the main factor that determines the inference accuracy, which can be better represented in floating-point format. From the hardware perspective, we show that multiplication can be implemented more efficiently with one floating-point operand and one fixed-point operand generating a fixedpoint product.\nThe rest of the paper is organized as follows, Section 2 discusses related work. Section 3 gives some background about different number representation formats and typical hardware implementation of CNNs. Section 4 describes the proposed number representation scheme. Section 5 and Section 6 discuss the scheme from the algorithmic and implementation perspective. Section 7 presents the experimental results, and Section 8 concludes the paper."}, {"heading": "2. Related Work", "text": "Precision of the neural network weights and activations plays a major role in determining the efficiency of the CNN hardware or software implementations. A lot of research focuses on replacing the standard 32-bit floating-point data with reduced precision data for CNN inference. For example, Gysel et al. (Gysel, 2016) propose representing both CNN weights and activations using minifloat, i.e., floatingpoint number with shorter bit-width. Since fixed-point arithmetic is more hardware efficient than floating-point arithmetic, most research focuses on fixed-point quantization. Gupta et al. (Gupta et al., 2015) present the impacts of different fixed-point rounding schemes on the accuracy. Judd et al. (Judd et al., 2015) demonstrate that the minimum required data precision not only varies across different networks, but also across different layers of the same network. Lin et al. (Lin et al., 2015) present a fixed-point quantization methodology to identify the optimal data precision for all layers of a network. Gysel et al. (Gysel et al., 2016) present a framework Ristretto for fixed-point quantization and re-training of CNNs based on Caffe (Jia et al., 2014).\nResearchers have also explored training neural networks directly with fixed-point weights. In (Hammerstrom, 1990), the author presents a hardware architecture for onchip learning with fixed-point operations. More recently, in (Courbariaux et al., 2014), the authors train neural networks with floating-point, fixed-point and dynamic fixedpoint formats and demonstrate that fixed-point weights\nare sufficient for training. Gupta et al. (Gupta et al., 2015) demonstrate network training with 16-bit fixed-point weights using stochastic rounding scheme.\nMany other approaches for memory reduction of neural networks have been explored. Han et al. (Han et al., 2015) propose a combination of network pruning, weight quantization during training and compression based on Huffman coding to reduce the VGG-16 network size by 49X. In (Deng et al., 2015), the authors propose to store both 8-bit quantized floating-point weights and 32-bit full precision weights. At runtime, quantized weights or fullprecision weights are randomly fetched in order to reduce memory bandwidth. The continuous research effort to reduce the data precision has led to many interesting demonstrations with 2-bit weights (Venkatesh et al., 2016) and even binary weights/activations (Courbariaux & Bengio, 2016; Rastegari et al., 2016). Zhou et al. (Zhou et al., 2016) demonstrate AlexNet training with 1-bit weights, 2-bit activations and 6-bit gradients. These techniques require additional re-training and can result in sub-optimal accuracies.\nIn contrast to prior works, this work proposes quantization of a pre-trained neural network weights into floatingpoint numbers and implementation of activations in fixedpoint format both for memory reduction and hardware efficiency. It further shows that floating-point representation of weights achieves better range/accuracy trade-off compared for the fixed-point representation of same number of bits and we empirically demonstrate it on state of the art CNNs such as AlexNet (Krizhevsky et al., 2012), VGG-16 (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015) and SqueezeNet (Iandola et al., 2016). Although this work is based on quantization only without the need for retraining the network, retraining may also be applied to reclaim part of the accuracy loss due to quantization."}, {"heading": "3. Background", "text": ""}, {"heading": "3.1. Fixed-Point Number Representation", "text": "Fixed-point representation is very similar to integer representation. The difference is that integer has a scaling factor of 1 and fixed-point can have a pre-defined scaling factor as power of 2. Some examples of fixed-point numbers are shown in Fig. 1.\nUsually, all fixed-point representation is assumed to share the same scaling factor during the entire computation. In some scenarios, the computation can be classified into different sets, e.g., for different CNN layers, with fixed-point numbers of different scaling factors. This is also referred as dynamic fixed-point representation (Courbariaux et al., 2014)."}, {"heading": "3.2. Floating-point Number Representation", "text": "One example of floating-point number representation is shown in Fig. 2. For a floating-point representation, there are typically three parts: sign, mantissa and exponent. The sign bit determines whether the number is a positive or negative number. The mantissa determines the significand part and the exponent determine the scale of the value. Usually, there are some special encodings used for representing some special numbers (e.g., 0, NaN and +/- infinity),\nFor binary floating-point numbers, the mantissa can assume an implicit bit, which is also adopted by IEEE floatingpoint standard. This ensures that the value of mantissa is always between 1 and 2, so the leading bit 1 can be omitted to save storage space. However, such an implicit bit places a limit on the smallest representable number for the significand part.\u2018\nThe exponent is typically represented as an unsigned integer number with a bias. For example, for an 8-bit exponent with a bias of 127, it can represent numbers from -127 to 128, i.e, 0-127 to 255-127."}, {"heading": "3.3. Hardware Implementation of CNNs", "text": "CNNs typically consist of multiple convolution layers interspersed by pooling, ReLU and normalization layers followed by fully-connected layers. Convolution and fullyconnected layers are the most compute and data intensive layers respectively (Qiu et al., 2016). The computation in these layers consist of multiply-and-accumulate (MAC) operations. The data path is illustrated in Fig. 3, where the input features are multiplied with the weights to get the intermediate data (i.e., partial sums). These partial sums are accumulated to generate the output features. Since fixedpoint arithmetic is typically more efficient for hardware implementation, most hardware accelerators implement the MAC operations using fixed-point representation.\nThe power/area breakdown of the CNN hardware accelerator mostly depends on the data flow architecture. For example, in Eyeriss (Chen et al., 2016), in each processing element (PE), MAC and memory account for about 9% and 52% area respectively. For Origami (Cavigelli et al., 2015), MAC and memory account for about 32% and 34% area respectively."}, {"heading": "4. Proposed Number Representation Scheme", "text": "The overview of our proposed number representation scheme is shown in Fig. 3. Different from most existing CNN implementations, we propose using a combination of floating-point and fixed-point representations. The network weights are represented as floating-point numbers while the input/output features are represented as fixedpoint numbers. The multiplier is implemented to take one floating-point number and one fixed-point number and produces output, i.e., intermediate data, in fixed-point format. The intermediate data is in fixed-point format and can have wider bit-width than the input/output features. The accumulation is the same as fixed-point adder, which can have higher bit-width.\nFrom hardware perspective, it is more efficient to imple-\nment multiplication using floating-point number and addition using fixed-point number. The multiplication operations have one fixed-point number input, one floating-point number input and fixed-point number output. This can be implemented with a multiplier and a shifter. The multiplier will multiply the fixed-point number with the mantissa part of the floating-point number, and the shifter will shift the results according to the exponent value. Therefore, we propose the hardware architecture illustrated in Fig. 3.\nThe accumulation/addition works with fixed-point numbers, which can be wider (i.e., with larger bit-width) than either of the inputs. This part is similar to most fixed-point number based implementation of CNN accelerators."}, {"heading": "5. Algorithmic Perspective", "text": "In this section, we investigate and explain why the proposed number representation scheme is better from the algorithmic perspective. Section 5.1 demonstrates that different CNNs can have inconsistent fixed-point bit-width requirements for representing the weights. Section 5.2 investigates this inconsistency by analyzing CNN weight distribution and properties. Section 5.3 shows that the representation range is the main factor that determines the inference accuracy. Section 5.4 shows that floating-point representation is more efficient and consistent representation for CNN weights."}, {"heading": "5.1. CNN Accuracy with Fixed-Point Weights", "text": "To evaluate different number representation schemes, we implement weight quantization based on Caffe (Jia et al., 2014) framework. To make a fair comparison, we assume that there is always a sign bit for representing negative values for both fixed-point and floating-point numbers. We will represent the bit-width of floating point representation as m + e, where m is the number of mantissa bits and e is the number of exponent bits.\nWe apply the weight quantization on four popular CNN networks: AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al., 2015) and VGG-16 (Simonyan & Zisserman, 2014). We evaluate the network accuracy by doing quantization for all convolutional and fully-connected layer weights. The activation is quantized to 16-bit fixed-point. For each layer, we normalize the weights so that the maximum absolute value equals 1. This is similar to the dynamic fixedpoint quantization (Moons & Verhelst, 2016; Gysel et al., 2016; Courbariaux et al., 2014). All accuracy results are top-1 accuracy and normalized with the top-1 accuracy using 32-bit floating-point representation. The results of top5 accuracy correlate with that of top-1 accuracy.\nThe network accuracy results using fixed-point representa-\ntion are shown in Fig. 4. Two observations can be made here:\n\u2022 For all networks, the accuracy starts increasing sharply after a certain threshold. For all networks, the normalized accuracy increases from close to 0 to close to 1 within 2 or 3 bits difference. This suggests that there is something dramatically different with these additional bits.\n\u2022 Among different networks, the required number of bits is very inconsistent. With 7-bit fixed-point number, AlexNet and SqueezeNet can achieve close to full accuracy, while GoogLeNet and VGG-16 have very low accuracy. GoogLeNet and VGG-16 need 10 to 11 bits to achieve full accuracy.\nThis inconsistency in bit-width requirements across different CNNs poses challenges for hardware implementation. For the design to be general-purpose and future-proof, the designer has to use margined bit-width or use runtime adaptation (Moons & Verhelst, 2016), both of which incur significant overhead."}, {"heading": "5.2. Weight Distribution", "text": "There can be several reasons that cause the inconsistency in Fig. 4. The network depth is one of the possible reasons. Similar to the idea in (Lin et al., 2015), the fixed-point quantization can be modeled as quantization noise for each layer. The network accuracy may drop further, i.e., accumulate more noise, as the network depth increases. The other possible reason is the number of MAC operations in each layer. Small quantization error can accumulate over a large amount of MAC operations. For example, the total number of MAC operations to calculate one output for convolutional and fully-connected layers in AlexNet are (363, 1200, 2304, 1728, 1728, 9216, 4096, 4096).\nHowever, none of the above reasons can explain the first observation earlier about the sharp, instead of gradual, change in accuracy. To further investigate this, we plot the weight distribution of four different layers in AlexNet in Fig. 5. Most weights have small values even after the perlayer normalization. The distribution is concentrated at the center, which is also the motivation for Huffman encoding of the weights proposed in (Han et al., 2015).\nTo better visualize the difference, we plot the same weight distribution in log-scale in Fig. 6. This plot is easier to spot the weight distribution difference and explains the accuracy behavior under fixed-point quantization observed in Fig. 4. Under fixed-point quantization, the layer with the most small-valued weights, conv3, will be the most susceptible to quantization errors. With 4-bit fixed-point representation, more than 90% weights in conv3 are unrepresentable, i.e., quantized to 0. This also explains why stochastic rounding works better than round-to-nearest reported in (Gupta et al., 2015).\nSince the layers are cascaded, the accuracy of the entire network is limited by the weakest layer. This is why the network produces close to 0 accuracy with small bit-width as shown in Fig. 4. With higher fixed-point bit-width, a larger number of weights in conv3 become representable, which results in the quick increase of the network accuracy.\nThe inconsistency in bit-width requirements observed in Fig. 4 can also be explained with the weight distribution. We pick the two layers with largest and smallest weights from AlexNet and VGG-16 and plot the weight distribution in Fig. 7. The layer conv3 1 in VGG-16 has more weights with smaller exponent values. This explains why VGG-16 requires more bit-width when using fixed-point representation."}, {"heading": "5.3. Range vs. Precision", "text": "The results in Fig. 4 and the weight distributions in Fig. 6 show that the network can achieve almost full accuracy, e.g., with 7-bit fixed-point for AlexNet, even when most weights are barely representable, i.e., only with 1 or 2 significant bits. This means that representation range, i.e, the ability to represent larger/smaller values, is more important than representation precision, i.e., differentiation between nearby values.\nSince the representation range and representation precision is hard to decompose in fixed-point representation, we investigate this using floating-point representation. For floating-point representation, the mantissa bit-width controls the precision and the exponent bit-width controls the range.\nFig. 8 highlights some of the results of network accuracy using floating-point representation with varying exponent\nbit-width (i.e., representation range). The floating-point has 3-bit mantissa with the implicit bit. The implicit bit limits the value of the significant part, so that the representation range is controlled by the exponent part. With floating-point representation, the networks show consistent trend, with almost 0 accuracy with 2-bit exponent and quickly increase to almost full accuracy with 4-bit exponent. Unlike the behavior seen in Fig. 4, this is expected as 2 additional bits in exponent offers 4X increase in the representation range.\nTo get more insight into the impact of representation range, we also run experiments with floating-point like representation where we limit the exponent range rather than the exponent bits. For example, exponent range of 4 is equivalent to 2-bit exponent and exponent range of 8 is equivalent to 3-bit exponent. The results of exponent range experi-\nments are shown in Fig. 9. The floating-point number has 2-bit mantissa with the implicit bit. The behavior of the accuracy is similar to the results with fixed-point representation, i.e., accuracy increases rapidly from almost 0 to almost 1. The relative ordering exponent range requirements also matches the bit-width requirements for fixed-point representation. This suggests that representation range is the main impacting factor for network accuracy.\nWith 2-bit mantissa, some networks saturate with normalized accuracy less than 1, as shown in Fig. 9. To compare with the effect of precision, the experiments are repeated with 6-bit mantissa as shown in Fig. 10. Comparing Fig. 10 with Fig. 9, the additional 4 bits in mantissa does not have significant impact on the network accuracy. This also validates the initial hypothesis that representation range is more important than representation accuracy."}, {"heading": "5.4. CNN Accuracy with Floating-Point Weights", "text": "Comparing to fixed-point representation, floating-point is better for representation range, which increases exponentially with the exponent bit-width. The results shown in Fig. 8 show that 4-bit exponent is adequate and consistent across different networks.\nThe next question for floating-point representation is how much precision, i.e., how many mantissa bits are needed. Fig. 11 highlights some of the results of network accuracy using floating-point representation with varying mantissa bit-width. The floating-point number has 4-bit exponent, and is with the implicit bit. Most networks have very high accuracy even with 1-bit mantissa and achieve full accuracy with 3-bit mantissa. This is also consistent across different networks, which further proves that the inconsistency with\nfixed-point representation seen in Fig. 4 is mainly from the inconsistent requirements for representation range rather than from the representation precision.\nWe also repeat the experiments with floating-point representation without the implicit bit in mantissa. The results are highlighted in Fig. 12 and Fig. 13. The implicit bit in mantissa limits the range of the significand part to [0.5, 2). Removing it helps further extend the range of the representation, especially for representing small numbers. That is why the network accuracy saturates with 3-bit exponent instead of 4-bit. The implicit bit also improves the precision of the significand part. Hence, we need 4-bit mantissa instead of 3-bit to achieve full accuracy."}, {"heading": "6. Implementation Perspective", "text": "This section motivates the proposed number representation scheme from the hardware implementation perspective. The implementation considerations are discussed in\nSection 6.1. Hardware trade-off results are presented in Section 6.2."}, {"heading": "6.1. Hardware Implementation Considerations", "text": "As discussed in Section 3.3, computations in CNNs are typically implemented as MAC operations. For the same 32-bit wide operations, hardware implementation of fixedpoint arithmetic can be more efficient than floating-point arithmetic. This is one of the reasons why most of previous work focuses on the optimization for fixed-point representation.\nThe comparison becomes less obvious when the bit-width is smaller, especially when the number of exponent bits in floating-point representation is small. For example, as shown in Fig. 14, multiplier with a floating-point number\ncan be implemented with a multiplier (of the bit-width of mantissa) and a barrel shifter (of the bit-width of exponent). This can be more efficient than multiplying two fixed-point numbers, as the multiplier becomes smaller and the shifter is simpler."}, {"heading": "6.2. Hardware Trade-off Results", "text": "To validate the claim in Section 6.1, we implement the multiplier with different operand configurations using a commercial 16nm process technology and libraries. The results are highlighted in Fig. 15. The floating-point configuration is represented as m+e, i.e., mantissa bit-width + exponent bit-width. Here we assume the baseline is a multiplier with two 8-bit fixed-point operands, i.e., 8\u00d78. The area and power numbers are all normalized with respect to the baseline. With the same bit-width, the proposed scheme of combining fixed-point and floating-operands can reduce both area and power. The reduction increases with less mantissa and more exponent bits, as a shifter is more efficient than a multiplier. As an example, the floating-point operand with 4-bit mantissa and 4-bit exponent, i.e. 8x4+4, can reduce the power and area by more than 20%, compared to 8x8 fixed-point multiplication."}, {"heading": "7. Experimental Results", "text": "As discussed in Section 5.3, we perform the weight quantization based on Caffe (Jia et al., 2014). We denote the representation as (m, e), where m is the mantissa bit-width and e is the exponent bit-width. e=0 means fixed-point representation.\nWe evaluate the network accuracy with different bit-width setting. Table 1 highlights some results for AlexNet. The\nnetwork is non-functioning (i.e., with close to 0 accuracy) when e is small, i.e., with limited representation range. To achieve full accuracy, AlexNet requires 8 bits for fixedpoint representation, i.e., (7, 0) with 1 sign bit, or 7 bits for floating-point representation with (3, 3) configuration. If the implementation only targets AlexNet, the proposed number representation can achieve 12.5% weight storage reduction and 8% power reduction in multiplication. The benefit will increase for CNNs that require more fixedpoint bit-width.\nAs discussed earlier, one of the requirement for number representation scheme is the consistency across different networks. This is especially important for the hardware implementation to be future-proof and viable for different CNN models. Some results of the normalized accuracy of different network are highlighted in Table 2. The 7-bit fixed-point configuration used for AlexNet also works for SqueezeNet, but is not adequate for GoogLeNet and VGG16. 10-bit fixed-point representation is required to get consistent accuracy across all networks used in this study.\nBy using proposed number representation scheme, we only need 7-bit floating-point, i.e, (3, 4) configuration. Therefore, we can replace 11-bit weights (10-bit fixed-point number plus sign bit) with 8-bit weights (3-bit mantissa, 4-bit exponent and 1 sign bit). This results in 36% storage reduction for weights and 50% power reduction in multiplication."}, {"heading": "8. Conclusion", "text": "In this work, we propose CNN inference implementation with floating-point weights and fixed-point activations. We\ngive the motivation for the proposed number representation scheme from both algorithmic and hardware implementation perspectives. The proposed scheme can reduce the weight storage by up to 36% and the multiplier power by up to 50%. Future work will investigate the impacts of network topology and training on the number representation requirements."}], "references": [{"title": "Origami: A convolutional network accelerator", "author": ["Cavigelli", "Lukas", "Gschwend", "David", "Mayer", "Christoph", "Willi", "Samuel", "Muheim", "Beat", "Benini", "Luca"], "venue": "In Proceedings of the 25th edition on Great Lakes Symposium on VLSI,", "citeRegEx": "Cavigelli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cavigelli et al\\.", "year": 2015}, {"title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel S", "Sze", "Vivienne"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Courbariaux", "Matthieu", "David", "Jean-Pierre", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Reduced-precision memory value approximation for deep learning", "author": ["Deng", "Zhaoxia", "Xu", "Cong", "Cai", "Qiong", "Faraboschi", "Paolo"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Ristretto: Hardware-oriented approximation of convolutional neural networks", "author": ["Gysel", "Philipp"], "venue": "arXiv preprint arXiv:1605.06402,", "citeRegEx": "Gysel and Philipp.,? \\Q2016\\E", "shortCiteRegEx": "Gysel and Philipp.", "year": 2016}, {"title": "Hardware-oriented approximation of convolutional neural networks", "author": ["Gysel", "Philipp", "Motamedi", "Mohammad", "Ghiasi", "Soheil"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "A vlsi architecture for highperformance, low-cost, on-chip learning", "author": ["Hammerstrom", "Dan"], "venue": "In Neural Networks,", "citeRegEx": "Hammerstrom and Dan.,? \\Q1990\\E", "shortCiteRegEx": "Hammerstrom and Dan.", "year": 1990}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size", "author": ["Iandola", "Forrest N", "Han", "Song", "Moskewicz", "Matthew W", "Ashraf", "Khalid", "Dally", "William J", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1602.07360,", "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Reduced-precision strategies for bounded memory in deep neural nets", "author": ["Judd", "Patrick", "Albericio", "Jorge", "Hetherington", "Tayler", "Aamodt", "Tor", "Jerger", "Natalie Enright", "Urtasun", "Raquel", "Moshovos", "Andreas"], "venue": "arXiv preprint arXiv:1511.05236,", "citeRegEx": "Judd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Lin", "Darryl D", "Talathi", "Sachin S", "Annapureddy", "V Sreekanth"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "An energy-efficient precision-scalable convnet processor in a 40-nm cmos", "author": ["Moons", "Bert", "Verhelst", "Marian"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "Moons et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moons et al\\.", "year": 2016}, {"title": "Going deeper with embedded fpga platform for convolutional neural network", "author": ["Qiu", "Jiantao", "Wang", "Jie", "Yao", "Song", "Guo", "Kaiyuan", "Li", "Boxun", "Zhou", "Erjin", "Yu", "Jincheng", "Tang", "Tianqi", "Xu", "Ningyi", "Sen"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Sympo-", "citeRegEx": "Qiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": null, "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Dnpu: An 8.1tops/w reconfigurable cnn-rnn processor for general-purpose deep neural networks", "author": ["Shin", "Dongjoo", "Lee", "Jinmook", "Jinsu", "Yoo", "HoiJun"], "venue": "In SolidState Circuits Conference (ISSCC),", "citeRegEx": "Shin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks", "author": ["Suda", "Naveen", "Chandra", "Vikas", "Dasika", "Ganesh", "Mohanty", "Abinash", "Ma", "Yufei", "Vrudhula", "Sarma", "Seo", "Jae-sun", "Cao", "Yu"], "venue": "In Proceedings of the 2016 ACM/SIGDA International", "citeRegEx": "Suda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suda et al\\.", "year": 2016}, {"title": "Accelerating deep convolutional networks using lowprecision and sparsity", "author": ["Venkatesh", "Ganesh", "Nurvitadhi", "Eriko", "Marr", "Debbie"], "venue": null, "citeRegEx": "Venkatesh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venkatesh et al\\.", "year": 2016}, {"title": "The microsoft 2016 conversational speech recognition system", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1609.03528,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Zhou", "Shuchang", "Ni", "Zekun", "Xinyu", "Wen", "He", "Wu", "Yuxin", "Zou", "Yuheng"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep learning algorithms have achieved or surpassed human-levels of perception in some applications (He et al., 2016; Xiong et al., 2016), enabling them to be deployed in the real-world applications.", "startOffset": 100, "endOffset": 137}, {"referenceID": 13, "context": "For example, AlexNet (Krizhevsky et al., 2012), a popular CNN model, requires 1.", "startOffset": 21, "endOffset": 46}, {"referenceID": 20, "context": "45 billion operations per input image with 240MB weights (Suda et al., 2016).", "startOffset": 57, "endOffset": 76}, {"referenceID": 16, "context": "Many researchers have explored hardware accelerators for CNNs to enable deployment of CNNs in embedded devices and demonstrated good performance at low power consumption (Qiu et al., 2016; Shin et al., 2017).", "startOffset": 170, "endOffset": 207}, {"referenceID": 18, "context": "Many researchers have explored hardware accelerators for CNNs to enable deployment of CNNs in embedded devices and demonstrated good performance at low power consumption (Qiu et al., 2016; Shin et al., 2017).", "startOffset": 170, "endOffset": 207}, {"referenceID": 12, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 5, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 7, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 14, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 13, "context": "From the algorithmic perspective, using popular largescale CNNs such as AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al.", "startOffset": 80, "endOffset": 105}, {"referenceID": 10, "context": ", 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 5, "context": "(Gupta et al., 2015) present the impacts of different fixed-point rounding schemes on the accuracy.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "(Judd et al., 2015) demonstrate that the minimum required data precision not only varies across different networks, but also across different layers of the same network.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "(Lin et al., 2015) present a fixed-point quantization methodology to identify the optimal data precision for all layers of a network.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "(Gysel et al., 2016) present a framework Ristretto for fixed-point quantization and re-training of CNNs based on Caffe (Jia et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": ", 2016) present a framework Ristretto for fixed-point quantization and re-training of CNNs based on Caffe (Jia et al., 2014).", "startOffset": 106, "endOffset": 124}, {"referenceID": 3, "context": "More recently, in (Courbariaux et al., 2014), the authors train neural networks with floating-point, fixed-point and dynamic fixedpoint formats and demonstrate that fixed-point weights are sufficient for training.", "startOffset": 18, "endOffset": 44}, {"referenceID": 5, "context": "(Gupta et al., 2015) demonstrate network training with 16-bit fixed-point weights using stochastic rounding scheme.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "(Han et al., 2015) propose a combination of network pruning, weight quantization during training and compression based on Huffman coding to reduce the VGG-16 network size by 49X.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "In (Deng et al., 2015), the authors propose to store both 8-bit quantized floating-point weights and 32-bit full precision weights.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": "The continuous research effort to reduce the data precision has led to many interesting demonstrations with 2-bit weights (Venkatesh et al., 2016) and even binary weights/activations (Courbariaux & Bengio, 2016; Rastegari et al.", "startOffset": 122, "endOffset": 146}, {"referenceID": 17, "context": ", 2016) and even binary weights/activations (Courbariaux & Bengio, 2016; Rastegari et al., 2016).", "startOffset": 44, "endOffset": 96}, {"referenceID": 23, "context": "(Zhou et al., 2016) demonstrate AlexNet training with 1-bit weights, 2-bit activations and 6-bit gradients.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "It further shows that floating-point representation of weights achieves better range/accuracy trade-off compared for the fixed-point representation of same number of bits and we empirically demonstrate it on state of the art CNNs such as AlexNet (Krizhevsky et al., 2012), VGG-16 (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al.", "startOffset": 246, "endOffset": 271}, {"referenceID": 10, "context": ", 2015) and SqueezeNet (Iandola et al., 2016).", "startOffset": 23, "endOffset": 45}, {"referenceID": 3, "context": "This is also referred as dynamic fixed-point representation (Courbariaux et al., 2014).", "startOffset": 60, "endOffset": 86}, {"referenceID": 16, "context": "Convolution and fullyconnected layers are the most compute and data intensive layers respectively (Qiu et al., 2016).", "startOffset": 98, "endOffset": 116}, {"referenceID": 1, "context": "For example, in Eyeriss (Chen et al., 2016), in each processing element (PE), MAC and memory account for about 9% and 52% area respectively.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "For Origami (Cavigelli et al., 2015), MAC and memory account for about 32% and 34% area respectively.", "startOffset": 12, "endOffset": 36}, {"referenceID": 11, "context": "To evaluate different number representation schemes, we implement weight quantization based on Caffe (Jia et al., 2014) framework.", "startOffset": 101, "endOffset": 119}, {"referenceID": 13, "context": "We apply the weight quantization on four popular CNN networks: AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al.", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": ", 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 7, "context": "This is similar to the dynamic fixedpoint quantization (Moons & Verhelst, 2016; Gysel et al., 2016; Courbariaux et al., 2014).", "startOffset": 55, "endOffset": 125}, {"referenceID": 3, "context": "This is similar to the dynamic fixedpoint quantization (Moons & Verhelst, 2016; Gysel et al., 2016; Courbariaux et al., 2014).", "startOffset": 55, "endOffset": 125}, {"referenceID": 14, "context": "Similar to the idea in (Lin et al., 2015), the fixed-point quantization can be modeled as quantization noise for each layer.", "startOffset": 23, "endOffset": 41}, {"referenceID": 9, "context": "The distribution is concentrated at the center, which is also the motivation for Huffman encoding of the weights proposed in (Han et al., 2015).", "startOffset": 125, "endOffset": 143}, {"referenceID": 5, "context": "This also explains why stochastic rounding works better than round-to-nearest reported in (Gupta et al., 2015).", "startOffset": 90, "endOffset": 110}, {"referenceID": 11, "context": "3, we perform the weight quantization based on Caffe (Jia et al., 2014).", "startOffset": 53, "endOffset": 71}], "year": 2017, "abstractText": "Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices. To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations. However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network. In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG16. We also show that such a representation scheme enables compact hardware multiply-andaccumulate (MAC) unit design. Experimental results show that the proposed scheme reduces the weight storage by up to 36% and power consumption of the hardware multiplier by up to 50%.", "creator": "LaTeX with hyperref package"}}}