{"id": "1703.04730", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2017", "title": "Understanding Black-box Predictions via Influence Functions", "abstract": "How can we explain the predictions of a black box model? In this paper, we use influence functions - a classic technique of robust statistics - to trace the prediction of a model through the learning algorithm back to its training data and identify the points that are most responsible for a particular prediction. By applying second-order optimization ideas, we extend influence functions to modern machine learning environments and show that they can be applied to high-dimensional black box models, even in non-convex and non-differentiable environments. We offer a simple, efficient implementation that requires only oracle access to gradients and Hessian vector products. Using linear models and Convolutionary Neural Networks, we show that influence functions are useful for many different purposes: to understand model behavior, to debug models, and to detect data failure and even to identify intruders and training units.", "histories": [["v1", "Tue, 14 Mar 2017 21:07:01 GMT  (4753kb,D)", "http://arxiv.org/abs/1703.04730v1", null], ["v2", "Mon, 10 Jul 2017 02:31:54 GMT  (4538kb,D)", "http://arxiv.org/abs/1703.04730v2", "International Conference on Machine Learning, 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["pang wei koh", "percy liang"], "accepted": true, "id": "1703.04730"}, "pdf": {"name": "1703.04730.pdf", "metadata": {"source": "META", "title": "Understanding Black-box Predictions via Influence Functions", "authors": ["Pang Wei Koh", "Percy Liang"], "emails": ["<pangwei@cs.stanford.edu>,", "ang@cs.stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "A key question often asked of machine learning systems is \u201cWhy did the system make this prediction?\u201d We want models that are not just high-performing but also explainable. By understanding why a model does what it does, we can hope to improve the model (Amershi et al., 2015), discover new science (Shrikumar et al., 2016), and provide end-users with explanations of actions that impact them (Goodman & Flaxman, 2016).\nHowever, the best-performing models in many domains \u2014 e.g., deep neural networks for image and speech recognition (Krizhevsky et al., 2012) \u2014 are complicated, blackbox models whose predictions seem hard to explain. Work on interpreting these black-box models has focused on understanding how a given model leads to particular predictions, e.g., by locally fitting a simpler model around the test point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al., 2013; Li\n1Stanford University, Stanford, CA. Correspondence to: Pang Wei Koh <pangwei@cs.stanford.edu>, Percy Liang <pliang@cs.stanford.edu>. Copyright 2017 by the author(s).\net al., 2016b; Datta et al., 2016; Adler et al., 2016). These works explain the predictions in terms of the model, but how can we explain where the model came from?\nIn this paper, we tackle this question by tracing a model\u2019s predictions through its learning algorithm and back to its training data, where the model parameters ultimately derive from. To formalize the impact on a training point on a prediction, we ask the counterfactual: what would happen if we did not have this training point, or if the values of this training point were changed slightly?\nAnswering this question by perturbing the data and retraining the model can be prohibitively expensive. To overcome this problem, we use influence functions, a classic technique from robust statistics (Cook & Weisberg, 1980) that tells us how the model parameters change as we upweight a training point by an infinitesimal amount. With these, we can \u201cdifferentiate through the training\u201d to estimate in closed-form the effect of a variety of training perturbations.\nDespite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is a recent paper from Wojnowicz et al. (2016), which introduced a method for approximating Cook\u2019s distance (a quantity related to influence) in generalized linear models. One obstacle to adoption is that influence functions require expensive second derivative calculations and assume differentiability and convexity of the model, limiting their applicability in modern contexts where models are often nondifferentiable, non-convex, and high-dimensional.\nWe address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016). We also demonstrate that influence functions can remain accurate even as their underlying assumptions of differentiability and convexity degrade.\nInfluence functions capture the core idea of studying models through the lens of their training data. We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior; debugging models; detecting dataset errors; and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015).\nar X\niv :1\n70 3.\n04 73\n0v 1\n[ st\nat .M\nL ]\n1 4\nM ar\n2 01\n7"}, {"heading": "2. Approach", "text": ""}, {"heading": "2.1. Changing the weight of a training point", "text": "Our goal is to understand the effect of training points on a model\u2019s predictions. We formalize this goal by asking the counterfactual: how would the model\u2019s predictions change if we did not have this training point?\nConsider an unconstrained empirical risk minimization setting, i.e., the parameters \u03b8\u0302 , arg min\u03b8\u2208\u0398 1n \u2211n i=1 L(zi, \u03b8) minimize some loss function L over training data z1, . . . , zn, where zi = (xi, yi) \u2208 X \u00d7 Y and we fold in regularization terms into L. Assume that the empirical risk 1 n \u2211n i=1 L(zi, \u03b8) is twice-differentiable and strongly convex in \u03b8 in a local neighborhood around \u03b8\u0302, i.e., the Hessian H\u03b8\u0302 = 1 n \u2211n i=1\u22072\u03b8L(zi, \u03b8\u0302) exists and is positive definite (PD); in section 4 we explore relaxing these assumptions.\nWe wish to compute \u03b8\u0302\u2212z\u2212 \u03b8\u0302 for all training points z, where \u03b8\u0302\u2212z , arg min\u03b8\u2208\u0398 \u2211 zi 6=z L(zi, \u03b8); this is the change in the model parameters when z is removed. However, retraining the model for each removed z is prohibitively slow.\nFortunately, influence functions give us an efficient approximation. The idea is to compute the parameter change if z were upweighted by some small , giving us new parameters \u03b8\u0302 ,z , arg min\u03b8\u2208\u0398(1\u2212 ) 1n \u2211n i=1 L(zi, \u03b8) + L(z, \u03b8). A classic result (Cook & Weisberg, 1982) tells us that the influence of upweighting z on the parameters \u03b8\u0302 is given by\nIup,params(z) , d\u03b8\u0302 ,z d \u2223\u2223\u2223 =0 = \u2212H\u22121 \u03b8\u0302 \u2207\u03b8L(z, \u03b8\u0302).x (1)\nIn essence, we form a quadratic approximation to the empirical risk around \u03b8\u0302 and take a single Newton step; see appendix A for a derivation. Since removing a point is the same as removing 1n weight from it, we can make the linear approximation \u03b8\u0302\u2212z \u2212 \u03b8\u0302 \u2248 \u2212 1nIup,params(z) without retraining the model.\nWe can apply the chain rule to measure how upweighting z changes functions of \u03b8\u0302. In particular, the influence of upweighting z on the loss at a test point ztest is\nIup,loss(z, ztest) , dL(ztest, \u03b8\u0302 ,z)\nd\n\u2223\u2223\u2223 =0\n(2)\n= \u2207\u03b8L(ztest, \u03b8\u0302)> d\u03b8\u0302 ,z d \u2223\u2223\u2223 =0 = \u2212\u2207\u03b8L(ztest, \u03b8\u0302)>H\u22121\u03b8\u0302 \u2207\u03b8L(z, \u03b8\u0302)."}, {"heading": "2.2. Perturbing the training input", "text": "We can develop a finer-grained notion of influence by studying a different counterfactual: how would the model\u2019s predictions change if a training input were modified?\nLet z = (x, y) be some training point and z\u03b4 , (x+ \u03b4, y).\nConsider the perturbation z 7\u2192 z\u03b4 , and let the resulting parameters be \u03b8\u0302z\u03b4,\u2212z . We can interpret this perturbation as removing 1n weight from z and adding it to a new \u201cphantom\u201d point z\u03b4 . To approximate its effects, consider moving infinitesimal weight from z onto z\u03b4 . Letting the resulting parameters be \u03b8\u0302 ,z\u03b4,\u2212z , eqn 1 gives us\nd\u03b8\u0302 ,z\u03b4,\u2212z d \u2223\u2223\u2223 =0 = Iup,params(z\u03b4)\u2212 Iup,params(z)\n= \u2212H\u22121 \u03b8\u0302\n( \u2207\u03b8L(z\u03b4, \u03b8\u0302)\u2212\u2207\u03b8L(z, \u03b8\u0302) ) . (3)\nAs before, we can make the linear approximation \u03b8\u0302z\u03b4,\u2212z \u2212 \u03b8\u0302 \u2248 d\u03b8\u0302 ,z\u03b4,\u2212zd \u2223\u2223 =0 \u00b7 1n , giving us a closed-form estimate of the effect of z 7\u2192 z\u03b4 on the model. Analogous equations also apply for changes in y. While influence functions might appear to only work for infinitesimal (therefore continuous) perturbations, it is important to note that this approximation holds for arbitrary \u03b4: the -upweighting scheme allows us to smoothly interpolate between z and z\u03b4 . This is particularly useful for working with discrete data (e.g., in NLP) or with discrete label changes.\nIf x is continuous and \u03b4 is small, we can further approximate eqn 3. Assume that the input domain X \u2286 Rd, the parameters \u0398 \u2286 Rp, and L is differentiable in \u03b8 and x. As \u2016\u03b4\u2016 \u2192 0,\u2207\u03b8L(z\u03b4, \u03b8\u0302)\u2212\u2207\u03b8L(z, \u03b8\u0302) \u2248 [\u2207x\u2207\u03b8L(z, \u03b8\u0302)]\u03b4, where\u2207x\u2207\u03b8L(z, \u03b8\u0302) \u2208 Rp\u00d7d. Substituting into eqn 3,\nd\u03b8\u0302 ,z\u03b4,\u2212z d \u2223\u2223\u2223 =0 = \u2212H\u22121 \u03b8\u0302 [\u2207x\u2207\u03b8L(z, \u03b8\u0302)]\u03b4. (4)\nWe thus have \u03b8\u0302z\u03b4,\u2212z \u2212 \u03b8\u0302 \u2248 \u2212 1nH \u22121 \u03b8\u0302 [\u2207x\u2207\u03b8L(z, \u03b8\u0302)]\u03b4. Differentiating w.r.t. \u03b4 and applying the chain rule gives us\nIpert,loss(z, ztest) , \u2207\u03b4L(ztest, \u03b8\u0302z\u03b4,\u2212z) \u2223\u2223\u2223 \u03b4=0\n(5)\n= \u2212\u2207\u03b8L(ztest, \u03b8\u0302)>H\u22121\u03b8\u0302 \u2207x\u2207\u03b8L(z, \u03b8\u0302).\n[Ipert,loss(z, ztest)]\u03b4 tells us the approximate effect that z 7\u2192 z+ \u03b4 has on the loss at ztest. By setting \u03b4 in the direction of Ipert,loss(z, ztest), we can construct local perturbations of z that maximally increase the loss at ztest. In section 5.2, we will use this to construct dataset poisoning attacks.\nIpert,loss also appears when identifying the features responsible for the influence of z on ztest. One way to do this is to pick the largest components of \u2207xIup,loss(z, ztest), i.e., the features that most change influence when perturbed. Comparing eqns 2 and 5, we see that \u2207xIup,loss(z, ztest) \u2248 Ipert,loss(z, ztest) up to an O( 1n ) term (since H\u03b8\u0302 = 1 n \u2211 i\u22072\u03b8L(zi, \u03b8\u0302) depends on z)."}, {"heading": "2.3. Relation to Euclidean distance", "text": "To find the training examples most relevant to a test example, it is common to look at its nearest neighbors in Euclidean space (e.g., Ribeiro et al. (2016)); with normalized\nvectors, this is equivalent to choosing x with large x \u00b7 xtest. For intuition, we compare this to Iup,loss(z, ztest) on a logistic regression model and show that influence is much more accurate at accounting for the effect of training.\nLet p(y | x) = \u03c3(y\u03b8>x), with y \u2208 {\u22121, 1} and \u03c3(t) = 1 1+exp(\u2212t) . We seek to maximize the probability of the training set. For a training point z = (x, y), L(z, \u03b8) = log(1 + exp(\u2212y\u03b8>x)), \u2207\u03b8L(z, \u03b8) = \u2212\u03c3(\u2212y\u03b8>x)yx, and H\u03b8 = 1 n \u2211n i=1 \u03c3(\u03b8\n>xi)\u03c3(\u2212\u03b8>xi)xix>i . From eqn 2, Iup,loss(z, ztest) is:\n\u2212ytesty \u00b7 \u03c3(\u2212ytest\u03b8>xtest) \u00b7 \u03c3(\u2212y\u03b8>xk) \u00b7 x>testH\u22121\u03b8\u0302 x.\nWe highlight two key differences from x \u00b7 xtest. First, \u03c3(\u2212y\u03b8>x) gives points with high training loss more influence, revealing that outliers can dominate the model parameters. Second, the weighted covariance matrix H\u22121 \u03b8\u0302 measures the \u201cresistance\u201d of the other training points to the removal of z; if\u2207\u03b8L(z, \u03b8\u0302) points in a direction where there is little variation, its influence will be higher since moving in that direction will not significantly increase the loss on other training points. As we show in Fig 1, these differences mean that influence functions capture the effect of model training much more accurately."}, {"heading": "3. Efficiently calculating influence", "text": "There are two challenges to efficiently computing Iup,loss(z, ztest) = \u2212\u2207\u03b8L(ztest, \u03b8\u0302)>H\u22121\u03b8\u0302 \u2207\u03b8L(z, \u03b8\u0302). First, it requires forming and inverting H\u03b8\u0302 = 1 n \u2211n i=1\u22072\u03b8L(zi, \u03b8\u0302), the Hessian of the empirical risk. With n training points and \u03b8 \u2208 Rp, this requires O(np2 + p3) operations, which\ncan be prohibitively expensive in models like deep neural networks with millions of parameters. Second, in a typical application we are not just interested in a single pair of (z, ztest); instead, we might want to calculate Iup,loss(zi, ztest) across all training points zi and a few test points of interest ztest.\nThe first problem is well-studied in second-order optimization. The idea is to avoid explicitly evaluating H\u22121\n\u03b8\u0302 ; in-\nstead, we use implicit Hessian-vector products (HVPs) to efficiently approximate stest , H\u22121\u03b8\u0302 \u2207\u03b8L(ztest, \u03b8\u0302) and then compute Iup,loss(z, ztest) = \u2212stest \u00b7 \u2207\u03b8L(z, \u03b8\u0302). This also gives a solution to the second problem: if we have a few test points of interest, we can precompute stest for each of them and then efficiently compute\u2212stest \u00b7\u2207\u03b8L(zi, \u03b8\u0302) for all training points zi.\nWe discuss two techniques for approximating stest, both relying on the fact that the HVP of a single term in H\u03b8\u0302, [\u22072\u03b8L(zi, \u03b8\u0302)]v, can be computed for arbitrary v in roughly the same amount of time that\u2207\u03b8L(zi, \u03b8\u0302) would take, which is typically O(p) (Pearlmutter, 1994). These HVPs can be computed via auto-differentiation: the idea is to calculate \u2207\u03b8L(zi, \u03b8\u0302) with one pass, symbolically form\u2207\u03b8L(zi, \u03b8\u0302)\u00b7v, and then differentiate it to get \u2207\u03b8(\u2207\u03b8L(zi, \u03b8\u0302) \u00b7 v) = [\u22072\u03b8L(zi, \u03b8\u0302)]v.\nConjugate gradients (CG). The first technique is a standard transformation of matrix inversion into an optimization problem. Since H\u03b8\u0302 0 by assumption, H \u22121 \u03b8\u0302 v \u2261 arg mint{t>H\u03b8\u0302t \u2212 v >t}. We can solve this with CG approaches that only require the evaluation of H\u03b8\u0302t, which takesO(np) time, without explicitly formingH\u03b8\u0302. While an\nexact solution takes p CG iterations, in practice we can get a good approximation with fewer iterations; see Martens (2010) for more details.\nStochastic estimation. With large datasets, standard CG can be slow; each iteration still goes through all n training examples. We use a method developed by Agarwal et al. (2016) to get a stochastic estimator that only samples a single point per iteration, achieving a significant speedup.\nDropping the \u03b8\u0302 subscript for clarity, let H\u22121j = \u2211j i=0(I \u2212 H)i, the first j terms in the Taylor expansion of H\u22121. Rewrite this recursively as H\u22121j = I + (I \u2212 H)H \u22121 j\u22121. From the validity of the Taylor expansion, H\u22121j \u2192 H\u22121 as j \u2192 \u221e.1 The key is that at each iteration, we can substitute the full H with a draw from any unbiased (and fasterto-compute) estimator of H to form H\u0303j . Since E[H\u0303\u22121j ] = H\u22121j , we still have E[H\u0303 \u22121 j ]\u2192 H\u22121.\nIn particular, we can use \u22072\u03b8L(zi, \u03b8\u0302), for any zi, as an unbiased estimator of H . This gives us the following procedure: uniformly sample t points zs1 , . . . , zst from the training data; define H\u0303\u221210 v = v; and recursively compute H\u0303\u22121j v = v + ( I \u2212 \u22072\u03b8L(zsj , \u03b8\u0302) ) H\u0303\u22121j\u22121v, taking H\u0303 \u22121 t v as our final unbiased estimate of H\u22121v. We pick t to be large enough such that H\u0303t stabilizes, and to reduce variance, we repeat this procedure r times and average results. Altogether, this is often significantly faster than CG.\nWe note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [\u22072\u03b8L(zi, \u03b8\u0302)]v can be decomposed into two rank-one matrix-vector products that can be efficiently computed in O(p) time. In our case, we rely on Pearlmutter (1994)\u2019s more general algorithm for fast HVPs, described above, to achieve the same time complexity.2\nWith these techniques, we can compute Iup,loss(zi, ztest) on all training points zi in O(np + rtp) time; we show in section 4.1 that we can choose rt < n and still have accurate results. Similarly, we can compute Ipert,loss(zi, ztest) = \u2212 1n\u2207\u03b8L(ztest, \u03b8\u0302) >H\u22121 \u03b8\u0302 \u2207x\u2207\u03b8L(zi, \u03b8\u0302) with two matrix-vector products: we first compute stest,\n1We assume w.l.o.g. that \u2200i,\u22072\u03b8L(zi, \u03b8\u0302) 4 I; if this is not true, we can scale the loss down without affecting the parameters. In some cases, we can get an upper bound on\u22072\u03b8L(zi, \u03b8\u0302) (e.g., for linear models and bounded input), which makes this easy. Otherwise, we treat the scaling as a separate hyperparameter and tune it such that the Taylor expansion converges.\n2One caveat of moving away from linear models is that we might not have\u22072\u03b8L(z, \u03b8\u0302) 0 even if H\u03b8\u0302 0, which may cause the recursive estimation of H\u0303j to diverge. To get around this, we can add a small \u03bbI damping term to \u22072\u03b8L(z, \u03b8\u0302) and also average \u22072\u03b8L(z, \u03b8\u0302) over mini-batches of training points at each recursive step, instead of relying on a single training point. Empirically, this seems to give accurate results.\nthen find s>test\u2207x\u2207\u03b8L(zi, \u03b8\u0302) with the same HVP trick.\nThese computations are easy to implement in auto-grad systems like TensorFlow (Abadi et al., 2015) and Theano (Theano D. Team, 2016), as users need only specify the loss; the fast HVPs and the required gradients are automatically handled with minimal human effort. We will release code that works for models already defined in TensorFlow (as well as code to replicate the experiments in this paper)."}, {"heading": "4. Validation and extensions", "text": "Recall that influence functions are asymptotic approximations of leave-one-out retraining, and further, the approximations are only valid in theory when (i) the model parameters \u03b8\u0302 minimize the empirical risk, and that (ii) the empirical risk is twice-differentiable and strongly convex in a local neighborhood around \u03b8\u0302. In this section, we empirically show that influence functions are accurate approximations (section 4.1) and that they provide useful information even when these assumptions are violated (sections 4.2, 4.3)."}, {"heading": "4.1. Influence functions vs. leave-one-out retraining", "text": "Influence functions assume that the weight on a training point is changed by an infinitesimally small . To investigate the accuracy of influence functions when = 1n , i.e., when we remove a training point and retrain the model, we compared \u2212 1nIup,loss(z, ztest) with actually doing leaveone-out retraining (\u03b8\u0302\u2212z \u2212 \u03b8\u0302). With a softmax model on 10- class MNIST,3 the predicted and actual changes matched closely (Fig 2-Left).\n3We trained with L-BFGS (Liu & Nocedal, 1989), with weight decay of 0.01, n = 55, 000, and p = 7, 840 parameters.\nThe stochastic estimator was also accurate with r = 10 repeats and t = 5, 000 iterations (Fig 2-Mid). Since each iteration only requires one HVP [\u22072\u03b8L(zi, \u03b8\u0302)]v, this runs quickly: in fact, we accurately estimated H\u22121v without even looking at every data point, since in this case n = 55, 000 > rt. Surprisingly, even setting r = 1 worked; it gave noisier but correlated results and was quite accurate at finding the most influential points."}, {"heading": "4.2. Non-convexity and non-convergence", "text": "In section 2, we took \u03b8\u0302 as the global minimum. In practice, if we obtain our model parameters \u03b8\u0303 through methods like SGD, we are not guaranteed that \u03b8\u0303 = \u03b8\u0302. Non-convexity could trap us in a local minimum; or if the model has not converged, \u03b8\u0303 might not be a local minimum and H\u03b8\u0303 could have negative eigenvalues. We show that influence functions applied to \u03b8\u0303 still give meaningful results in practice.\nIf \u03b8\u0303 is a local minimum, L(z, \u00b7) is still approximately quadratic around L(z, \u03b8\u0303). As such, influence functions are locally meaningful: they tell us what happens if we upweight a training point z and retrain with gradient descent starting from \u03b8\u0303, i.e., staying at the \u201csame\u201d local minimum.\nIf \u03b8\u0303 is close to a local minimum and H\u03b8\u0303 0, Iup,params(z) is correlated with the result of taking a single Newton step from \u03b8\u0303 after removing weight from z (see appendix B for a more precise statement). If H\u03b8\u0303 has non-positive eigenvalues, the Newton step is not well-defined. We can add a small damping term \u03bbI to H\u03b8\u0303 to control the negative curvature and make H\u03b8\u0303 0; this has the effect of regularizing the new \u03b8\u0303 ,z to be near the original \u03b8\u0303.4\n4In practice, we can efficiently find the smallest eigenvalue of H\u03b8\u0303 through methods like power iteration, which rely only on HVPs, and use this to set the amount of damping.\nWe checked the behavior of Iup,loss in a non-convergent, non-convex setting by training a convolutional neural network for 500k iterations.5 The model had not converged and H\u03b8\u0303 was not PD, so we added damping of 0.01I . Even in this difficult setting, the predicted and actual changes in loss were highly correlated (Pearson R = 0.90, Fig 2-Right)."}, {"heading": "4.3. Non-differentiable losses", "text": "What happens when the derivatives of the loss, \u2207\u03b8L and \u22072\u03b8L, do not exist? In this section, we show that influence functions computed on smooth approximations to non-differentiable losses can predict the behavior of the original, non-differentiable loss under leave-one-out retraining. The robustness of this approximation suggests that we can train non-differentiable models and swap out non-differentiable components for smoothed versions for the purposes of calculating influence.\nTo see this, we trained a linear SVM on the same 1s vs. 7s MNIST task in section 2.3. This involves minimizing Hinge(s) = max(0, 1 \u2212 s); this simple piecewise linear function is similar to ReLUs in neural networks, the most common source of their non-differentiability. We set the derivatives at the hinge to 0 and calculated Iup,loss. As one might expect, this was inaccurate (Fig 3b-Left): because the second derivative is 0 everywhere except for regularization, Iup,loss overestimates how much it can move the pa-\n5The network had 7 sets of convolutional layers with tanh(\u00b7) non-linearities, modeled after the all-convolutional network from (Springenberg et al., 2014). For speed, we used 10% of the MNIST training set and only 2,616 parameters, since repeatedly retraining the network was expensive. Training was done with mini-batches of 500 examples and the Adam optimizer (Kingma & Ba, 2014). The model had not converged after 500k iterations; training it for another 500k iterations, using a full training pass for each iteration, reduced train loss from 0.146 to 0.120.\nrameters and ends up predicting changes that are too large.\nWe approximated Hinge(s) with SmoothHinge(s, t) = t log(1 + exp( 1\u2212st )), which approaches the hinge loss as t \u2192 0 (Fig 3a). Using the same SVM weights, we found that calculating Iup,loss using SmoothHinge(s, 0.001) closely matched the predicted change in the original Hinge(s) (Pearson R = 0.95; Fig 3b-Mid) and remained accurate over a wide range of t (Fig 3b-Right)."}, {"heading": "5. Use cases of influence functions", "text": ""}, {"heading": "5.1. Understanding model behavior", "text": "By telling us the training points \u201cresponsible\u201d for a given prediction, influence functions reveal insights about how models rely on and extrapolate from the training data. In this section, we show that two models can make the same correct predictions but get there in very different ways.\nWe compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 and (b) an RBF SVM on a dog vs. fish image classification dataset we built from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class. Freezing neural networks in this way is not uncommon in computer vision applications and is equivalent to training a softmax on the bottleneck features (Donahue et al., 2014). We picked a test image that both models got correct (Fig 4-Top) and used SmoothHinge(\u00b7, 0.001) for the SVM\u2019s influence.\nAs expected, Iup,loss in the RBF SVM varied inversely with distance, with training images far from the test image in pixel space having almost no influence; the Inception influences were much less correlated with distance in pixel space (Fig 4-Left). Looking at the two most helpful images (most positive\u2212Iup,loss) for each model in Fig 4-Right, we see that the Inception network picked on the distinctive characteristics of clownfish, whereas the RBF SVM pattern matched against training images close in pixel space.\nMoreover, in the RBF SVM, fish (green points) close to the test image were mostly helpful, while dogs (red) were mostly harmful, with the RBF acting as a soft nearest neighbor function (Fig 4-Left). In contrast, in the Inception network, fish and dogs could be both helpful and harmful for correctly classifying the test image as a fish; in fact, the 5th most helpful training image was a dog that, to the model, looked very different from the test fish (Fig 4-Top)."}, {"heading": "5.2. Adversarial training examples", "text": "In this section, we show that models that place a lot of influence on a small number of points can be surprisingly vulnerable to training input perturbations, posing a serious\n6We used pre-trained weights from Keras (Chollet, 2015).\nsecurity risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011). Recent work has generated adversarial test images that are visually indistinguishable from real test images but completely fool a classifier (Goodfellow et al., 2015). We demonstrate that influence functions can be used to craft adversarial training images that are similarly visually-indistinguishable and can flip a model\u2019s prediction on a separate test image. To the best of our knowledge, this is the first proof-of-concept that visually-indistinguishable training attacks can be executed on otherwise highly-accurate neural networks.\nFor a target test image ztest, we can construct for each training image zi an adversarial perturbation \u03b4i = clip(sign (Ipert,loss(zi, ztest))), which makes a visuallyimperceptible change to zi (each pixel is perturbed by at most 1 and clipped to [0, 255]). This is a training-set analogue of the fast gradient sign method used by (Goodfellow et al., 2015) for test-set attacks.\nWe tested these adversarial training perturbations on the same Inception network on dogs vs. fish from Section 5.1, choosing this pair of animals to provide a stark contrast between the classes. As before, we froze all but the top layer for training; note that Ipert,loss still involves differentiating through the entire network. We targeted the test image in Fig 5. In Fig 5a, we plot the changes in test loss expected from each \u03b4i: most perturbations had little effect, but a few caused sharp increases in test loss. We applied the \u03b4i with the largest expected effect and retrained the model. Sur-\nprisingly, this small change to just 1 out of 1,800 training images flipped the prediction on the test image (Fig 5b).\nOther test images were also vulnerable. For each ztest, we computed \u03b4i for each training example i and greedily applied them in descending order of their expected effect. Originally, the model correctly classified 592 / 600 test images. Out of these 592, 20 predictions could be flipped by perturbing only 1 training image per prediction (choosing a different training example for each test image). 54% of the test predictions could be flipped if we allowed perturbations on 18 training images (1% of train data, again with a different 18 training images for each test image), and 87% of the test predictions could be flipped with perturbations on 36 training images (2% of train data) per test image.\nThis training-set attack extends to a broader class of targets (e.g., increasing test loss on an entire class) using the same approach. One could also increase its effectiveness by iterating gradient steps, resulting in larger perturbations. We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6. Biggio et al. (2012) constructed a dataset poisoning attack against a linear SVM on a two-class MNIST task, but had to modify the training points in an obviously distinguishable way to be effective. Measuring the magnitude of Ipert,loss gives model developers a way of quantifying how vulnerable their models are to training-set perturbations."}, {"heading": "5.3. Debugging domain mismatch", "text": "Domain mismatch \u2014 where the training distribution does not match the test distribution \u2014 can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010). We show that influence functions can identify the training examples most responsible for the errors, helping model developers to pinpoint a domain mismatch.\nAs a case study, we predicted whether a patient would be readmitted to a hospital. Domain mismatches are common in biomedical data; for example, different hospitals can serve very different populations, and readmission models trained on one population can do poorly on another (Kansagara et al., 2011). We used logistic regression to predict readmission with a balanced training dataset of 20K diabetic patients from 100+ US hospitals, each represented by 127 features (Strack et al., 2014).7\n3 out of the 24 children under age 10 in this dataset were re-admitted. To induce a domain mismatch, we filtered out 20 children who were not re-admitted, leaving 3 out of 4 readmitted. This caused the model to wrongly classify many children in the test set. Our aim is to identify the 4 children in the training set as being \u201cresponsible\u201d for these errors.\nAs a baseline, we looked at the learned parameters \u03b8\u0302 to see if the indicator variable for being a child was obviously different, but 11/127 features had a larger coefficient, e.g., whether the patient was referred to a different hospital.\n7Hospital readmission was defined as whether a patient would be readmitted within the next 30 days. Features were demographic (e.g., age, race, gender), administrative (e.g., length of hospital stay), or medical (e.g., test results).\nPicking a random child ztest that the model got wrong, we calculated\u2212Iup,loss(zi, ztest) for each training point zi. This clearly highlighted the 4 training children, each of whom was more than 100 times as influential as the next most influential examples. The 1 child in the training set who was not readmitted had a very positive influence, while the other 3 had very negative influences. Calculating Ipert,loss on these 4 children showed that a change in the \u2018child\u2019 indicator variable had by far the largest effect on Iup,loss."}, {"heading": "5.4. Fixing mislabeled examples", "text": "Labels in the real world are often noisy, especially if crowdsourced (Fre\u0301nay & Verleysen, 2014), and can even be adversarially corrupted, as in section 5.2. Even if a human expert could recognize wrongly labeled examples, it is impossible in many applications to manually review all of the training data. We show that influence functions can help human experts prioritize their attention, allowing them to inspect only the examples that actually matter.\nThe key idea is to flag the training points that exert the most influence on the model. Because we do not have access to the test set, we measure the influence of zi with \u2212 1nIup,loss(zi, zi), which approximates the leave-one-out error incurred on zi if we remove zi from the training set.\nOur case study is email spam classification, which relies on user-provided labels and is also vulnerable to adversarial attack (Biggio et al., 2011).8 We flipped the labels of a random 10% of the training data and then simulated manually inspecting a fraction of the training points, correcting them if they had been flipped. Using influence functions to prioritize the training points to inspect allowed us to repair the dataset (Fig 6, blue) without checking too many points, outperforming the baselines of checking points with the highest train loss (Fig 6, green) or at random (Fig 6, red). No method had access to the test data."}, {"heading": "6. Related work", "text": "The use of influence-based diagnostics originated in statistics in the 70s and 80s, driven by seminal papers by Cook and others (Cook, 1977; Cook & Weisberg, 1980; 1982), though similar ideas appeared even earlier in other forms, e.g., the infinitesimal jackknife (Jaeckel, 1972). Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998). Most of this prior work focused on experiments with small datasets, e.g., n = 24 and p = 10 in Cook & Weisberg\n8We divided the Enron1 spam dataset (Metsis et al., 2006) into training (n = 4147) and test (n = 1035) sets, training logistic regression on top of a bag-of-words representation.\n(1980), with special attention therefore paid to exact solutions, or if not possible, characterizations of the error terms.\nTo the best of our knowledge, influence functions have not been used much in the ML literature, with some exceptions. Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods. Wojnowicz et al. (2016) uses matrix sketching to estimate Cook\u2019s distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models.\nAs noted in section 5.2, our training-set attack is mathematically similar to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al., 2016a). These papers derived the attack directly from the KKT conditions without considering influence functions, though for continuous data the end result is equivalent to iterating Ipert,loss. Influence functions additionally allow us to consider attacks that change discrete data (section 2.2) and potentially models that are not convex or differentiable (sections 4.2, 4.3), though more work needs to be done to test the effectiveness of those attacks. Our work merges elements of this approach with the work of Goodfellow et al. (2015) and others on visually-imperceptible, adversarial test-set attacks in neural networks: we cast the dataset poisoning attack in the influence function framework and apply it in a larger-scale, neural network setting, creating a visually-imperceptible attack on the training set.\nIn contrast to training-set attacks, Cadamuro et al. (2016) consider the task of taking an incorrect test prediction and\nfinding a small subset of training data such that changing the labels on this subset makes the prediction correct. They provide a solution for OLS and Gaussian process models when the labels are continuous. Our work with influence functions allow us to solve this problem in a much larger range of models and in datasets with discrete labels."}, {"heading": "7. Discussion", "text": "We have discussed a variety of applications, from detecting training-set vulnerabilities to debugging models and fixing datasets. Underlying each of these applications is a common tool, influence functions, which are based on a simple idea \u2014 we can better understand model behavior by looking at how it was derived from its training data.\nAt their core, influence functions measure the effect of local changes: what happens when we upweight a point by an infinitesimally-small ? This locality allows us to derive efficient closed-form estimates, and as we show, they can be surprisingly effective. However, we might want to ask about more global changes, e.g., how does the set of all patients from this hospital affect the model? Since the accuracy of influence functions is based on the model not changing too much, how to tackle this is an open question.\nIt seems inevitable that high-performing, complex, blackbox models will become increasingly prevalent and important. We hope that the approach presented here \u2014 of looking at the training data through the lens of the model \u2014 will become a standard part of the toolkit of developing, understanding, and diagnosing these models.\nA. Deriving Iup,params Here, we give a standard derivation of the influence function Iup,params in the context of M-estimation; a much more thorough treatment can be found in (van der Vaart, 1998) and other statistics textbooks.\nAs a reminder, our setting is unconstrained empirical risk minimization. We assume that we are given parameters \u03b8\u0302 that are the global minimum of the empirical risk 1 n \u2211n i=1 L(zi, \u03b8). We further assume that the empirical risk is twice-differentiable and strongly convex in \u03b8 in a local neighborhood around \u03b8\u0302, i.e.,H\u03b8\u0302 , 1 n \u2211n i=1\u22072\u03b8L(zi, \u03b8\u0302) exists and is positive definite. This guarantees the existence of H\u22121\n\u03b8\u0302 , which we will use in the subsequent derivation.\nOur goal is to find \u03b8\u0302 ,z , the model parameters that result from upweighting a training point z by an infinitesimal . This perturbation results in a modified empirical risk R\u0303(\u03b8, , z) , (1 \u2212 ) 1n \u2211n i=1 L(zi, \u03b8) + L(z, \u03b8), with \u03b8\u0302 ,z , arg min\u03b8 R(\u03b8, , z).\nAt a high level, our strategy will be as follows: since is\nsmall and the empirical risk is strongly convex around \u03b8\u0302, we can assume that \u03b8\u0302 ,z is close to \u03b8\u0302. We can therefore write R\u0303(\u03b8\u0302 ,z, , z) in terms of \u03b8\u0302 by taking Taylor expansions of L around \u03b8\u0302; minimizing this expanded version of R\u0303 will give us the desired formula for Iup,params.\nFor convenience, define \u2206 = \u03b8 \u2212 \u03b8\u0302. The second-order Taylor expansion of L around \u03b8\u0302 at a training point zi is given by\nL(zi, \u03b8) = L(zi, \u03b8\u0302) +\u2207\u03b8L(zi, \u03b8\u0302)>\u2206+ 1\n2 \u2206>\u22072\u03b8L(zi, \u03b8\u0302)\u2206 + o(\u2016\u2206\u2016 2 2). (6)\nSince \u2206 ,z , \u03b8\u0302 ,z\u2212 \u03b8\u0302 is small, we can restrict our attention to values of \u03b8 that are close to \u03b8\u0302 and ignore the o(\u2016\u2206\u201622) term. Substituting this expansion into R\u0303 gives us:\nR\u0303(\u03b8, , z)\n= (1\u2212 ) 1 n n\u2211 i=1 L(zi, \u03b8) + L(z, \u03b8)\n\u2248 (1\u2212 ) 1 n n\u2211 i=1 L(zi, \u03b8\u0302) + L(z, \u03b8\u0302)\n+ (1\u2212 ) 1 n n\u2211 i=1 \u2207\u03b8L(zi, \u03b8\u0302)>\u2206 + \u2207\u03b8L(z, \u03b8\u0302)>\u2206\n+ 1\u2212 2n n\u2211 i=1 \u2206>\u22072\u03b8L(zi, \u03b8\u0302)\u2206 + 2 \u2206>\u22072\u03b8L(z, \u03b8\u0302)\u2206,\n(7)\nwhere the approximation comes from discarding the o(\u2016\u2206\u201622) term.\nNow, since \u03b8\u0302 ,z minimizes R\u0303(\u03b8, , z), we know that d d\u03b8 R\u0303(\u03b8\u0302 ,z, , z) = 0. Applying this to eqn 7 gives us\n\u2206 ,z = \u2212 [ (1\u2212 ) 1\nn n\u2211 i=1 \u22072\u03b8L(zi, \u03b8\u0302) + \u22072\u03b8L(z, \u03b8\u0302) ]\u22121 [\n(1\u2212 ) 1 n n\u2211 i=1 \u2207\u03b8L(zi, \u03b8\u0302) + \u2207\u03b8L(z, \u03b8\u0302)\n] .\n(8)\nWe can simplify this by using the fact that the original \u03b8\u0302 satisfies the optimality condition \u2211n i=1\u2207\u03b8L(zi, \u03b8\u0302) = 0, since \u03b8\u0302 is a local minimum of the empirical risk. Furthermore, for small and any positive-definite matrix A, (I + A)\u22121 = I \u2212 A+ o( ). Discarding the o( ) term, we thus have\n\u2206 ,z \u2248 \u2212\n[ 1\nn n\u2211 i=1 \u22072\u03b8L(zi, \u03b8\u0302)\n]\u22121 \u2207\u03b8L(z, \u03b8\u0302) + o( ).\n(9)\nFinally, differentiating this w.r.t. yields the desired result:\nd\u03b8\u0302 ,z d \u2223\u2223\u2223 =0 = d\u2206 ,z d \u2223\u2223\u2223 =0\n= \u2212\n[ 1\nn n\u2211 i=1 \u22072\u03b8L(zi, \u03b8\u0302)\n]\u22121 \u2207\u03b8L(z, \u03b8\u0302)\n= \u2212H\u22121 \u03b8\u0302 \u2207\u03b8L(z, \u03b8\u0302) = Iup,params(z). (10)\nBecause H\u03b8\u0302 0 by assumption, its inverse exists. Without this assumption (e.g., \u03b8\u0302 is still the global minimum but H\u03b8\u0302 has eigenvalues that are zero), our second-order Taylor expansion would fail and we would need to go to higherorder derivatives of L at \u03b8\u0302. One way to overcome this is by adding a small damping term \u03bbI to H\u03b8\u0302, which has the effect of regularizing \u03b8\u0302 ,z so that it stays close to \u03b8\u0302.\nIf the empirical risk is non-convex in \u03b8, then \u03b8\u0302 might not even be the global minimum. We study this in section 4.2 and appendix B.\nB. Influence at non-convergence Consider a training point z. When the model parameters \u03b8\u0303 are close to but not at a local minimum, Iup,params(z) is correlated with the change in parameters after upweighting z and then taking a single Newton step from \u03b8\u0303. The high-level idea is that even though the gradient of the empirical risk at \u03b8\u0303 is not 0, the Newton step from \u03b8\u0303 can be decomposed into a component following the existing gradient (which does not depend on the choice of z) and a second component responding to the upweighted z (which Iup,params(z) tracks).\nLet c , 1n \u2211n i=1\u2207\u03b8L(zi, \u03b8\u0303) be the gradient of the empirical risk at \u03b8\u0303; since \u03b8\u0303 is not a local minimum, c 6= 0. After upweighting z by , the gradient at \u03b8\u0303 goes from c 7\u2192 (1 \u2212 )c + \u2207\u03b8L(z, \u03b8\u0303), and the empirical Hessian goes from H\u03b8\u0303 7\u2192 (1\u2212 )H\u03b8\u0303 + \u22072\u03b8L(z, \u03b8\u0303). A Newton step from \u03b8\u0303 therefore changes the parameters by:\nN ,z , \u2212 [ (1\u2212 )H\u03b8\u0303 + \u2207 2 \u03b8L(z, \u03b8\u0303) ]\u22121 [ (1\u2212 )c+ \u2207\u03b8L(z, \u03b8\u0303) ] . (11)\nIf c \u2207\u03b8L(z, \u03b8\u0303), i.e., the model is far from convergence and still has a large gradient, then N ,z \u2248 \u2212H\u22121\u03b8\u0303 c. In this case, the effect of downweighting z is swamped by c.\nHowever, if c is on the same order as \u2207\u03b8L(z, \u03b8\u0303), then by ignoring terms in c, 2, and higher, we get N ,z \u2248 \u2212H\u22121\u03b8\u0303 ( c+ \u2207\u03b8L(z, \u03b8\u0303) ) . Since Iup,params(z) =\n\u2212H\u22121 \u03b8\u0303 \u2207\u03b8L(z, \u03b8\u0303), it is a scaled multiple ofN ,z plus a constant \u2212H\u22121 \u03b8\u0303 c."}, {"heading": "Acknowledgements", "text": "We thank Jacob Steinhardt, Zhenghao Chen, and Hongseok Namkoong for helpful discussions and comments. This work was supported by a Future of Life Research Award and a Microsoft Research Faculty Fellowship."}], "references": [{"title": "Auditing black-box models for indirect influence", "author": ["P. Adler", "C. Falk", "S.A. Friedler", "G. Rybeck", "C. Scheidegger", "B. Smith", "S. Venkatasubramanian"], "venue": "arXiv preprint arXiv:1602.07043,", "citeRegEx": "Adler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2016}, {"title": "Second order stochastic optimization in linear time", "author": ["N. Agarwal", "B. Bullins", "E. Hazan"], "venue": "arXiv preprint arXiv:1602.03943,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Modeltracker: Redesigning performance analysis tools for machine learning", "author": ["S. Amershi", "M. Chickering", "S.M. Drucker", "B. Lee", "P. Simard", "J. Suh"], "venue": "In Conference on Human Factors in Computing Systems (CHI),", "citeRegEx": "Amershi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amershi et al\\.", "year": 2015}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML, 20:97\u2013112,", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Debugging machine learning models", "author": ["G. Cadamuro", "R. Gilad-Bachrach", "X. Zhu"], "venue": "In ICML Workshop on Reliable Machine Learning in the Wild,", "citeRegEx": "Cadamuro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cadamuro et al\\.", "year": 2016}, {"title": "Influential observations, high leverage points, and outliers in linear regression", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "Statistical Science,", "citeRegEx": "Chatterjee and Hadi,? \\Q1986\\E", "shortCiteRegEx": "Chatterjee and Hadi", "year": 1986}, {"title": "On robustness properties of convex risk minimization methods for pattern recognition", "author": ["A. Christmann", "I. Steinwart"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Christmann and Steinwart,? \\Q2004\\E", "shortCiteRegEx": "Christmann and Steinwart", "year": 2004}, {"title": "Detection of influential observation in linear regression", "author": ["R.D. Cook"], "venue": "Technometrics, 19:15\u201318,", "citeRegEx": "Cook,? \\Q1977\\E", "shortCiteRegEx": "Cook", "year": 1977}, {"title": "Assessment of local influence", "author": ["R.D. Cook"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Cook,? \\Q1986\\E", "shortCiteRegEx": "Cook", "year": 1986}, {"title": "Characterizations of an empirical influence function for detecting influential cases in regression", "author": ["R.D. Cook", "S. Weisberg"], "venue": null, "citeRegEx": "Cook and Weisberg,? \\Q1980\\E", "shortCiteRegEx": "Cook and Weisberg", "year": 1980}, {"title": "Residuals and influence in regression", "author": ["R.D. Cook", "S. Weisberg"], "venue": null, "citeRegEx": "Cook and Weisberg,? \\Q1982\\E", "shortCiteRegEx": "Cook and Weisberg", "year": 1982}, {"title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems", "author": ["A. Datta", "S. Sen", "Y. Zick"], "venue": "In Security and Privacy (SP),", "citeRegEx": "Datta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Datta et al\\.", "year": 2016}, {"title": "Model selection in kernel based regression using the influence function", "author": ["M. Debruyne", "M. Hubert", "J.A. Suykens"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Debruyne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Debruyne et al\\.", "year": 2008}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Classification in the presence of label noise: a survey", "author": ["B. Fr\u00e9nay", "M. Verleysen"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Fr\u00e9nay and Verleysen,? \\Q2014\\E", "shortCiteRegEx": "Fr\u00e9nay and Verleysen", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "European union regulations on algorithmic decision-making and a \u201cright to explanation", "author": ["B. Goodman", "S. Flaxman"], "venue": "arXiv preprint arXiv:1606.08813,", "citeRegEx": "Goodman and Flaxman,? \\Q2016\\E", "shortCiteRegEx": "Goodman and Flaxman", "year": 2016}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "In Proceedings of the 4th ACM workshop on Security and artificial intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "The infinitesimal jackknife", "author": ["L.A. Jaeckel"], "venue": "Unpublished memorandum,", "citeRegEx": "Jaeckel,? \\Q1972\\E", "shortCiteRegEx": "Jaeckel", "year": 1972}, {"title": "Risk prediction models for hospital readmission: a systematic review", "author": ["D. Kansagara", "H. Englander", "A. Salanitro", "D. Kagen", "C. Theobald", "M. Freeman", "S. Kripalani"], "venue": null, "citeRegEx": "Kansagara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kansagara et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Data poisoning attacks on factorization-based collaborative filtering", "author": ["B. Li", "Y. Wang", "A. Singh", "Y. Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Understanding neural networks through representation erasure", "author": ["J. Li", "W. Monroe", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1612.08220,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "Efficient approximation of cross-validation for kernel methods using Bouligand influence function", "author": ["Y. Liu", "S. Jiang", "S. Liao"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "The security of latent Dirichlet allocation", "author": ["S. Mei", "X. Zhu"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Mei and Zhu,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu", "year": 2015}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Mei and Zhu,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu", "year": 2015}, {"title": "Spam filtering with naive bayes-which naive bayes", "author": ["V. Metsis", "I. Androutsopoulos", "G. Paliouras"], "venue": "In CEAS,", "citeRegEx": "Metsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metsis et al\\.", "year": 2006}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "Pearlmutter,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "In International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["A. Shrikumar", "P. Greenside", "A. Shcherbina", "A. Kundaje"], "venue": "arXiv preprint arXiv:1605.01713,", "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient", "author": ["B. Strack", "J.P. DeShazo", "C. Gennings", "J.L. Olmo", "S. Ventura", "K.J. Cios", "J.N. Clore"], "venue": "records. BioMed Research International,", "citeRegEx": "Strack et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strack et al\\.", "year": 2014}, {"title": "Rethinking the Inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano D. Team"], "venue": "arXiv preprint arXiv:1605.02688,", "citeRegEx": "Team.,? \\Q2016\\E", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Assessing influence on predictions from generalized linear models", "author": ["W. Thomas", "R.D. Cook"], "venue": null, "citeRegEx": "Thomas and Cook,? \\Q1990\\E", "shortCiteRegEx": "Thomas and Cook", "year": 1990}, {"title": "Asymptotic statistics", "author": ["A.W. van der Vaart"], "venue": null, "citeRegEx": "Vaart,? \\Q1998\\E", "shortCiteRegEx": "Vaart", "year": 1998}, {"title": "Generalized leverage and its applications", "author": ["B. Wei", "Y. Hu", "W. Fung"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Wei et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1998}, {"title": "Influence sketching\u201d: Finding influential samples in large-scale regressions", "author": ["M. Wojnowicz", "B. Cruz", "X. Zhao", "B. Wallace", "M. Wolff", "J. Luan", "C. Crable"], "venue": "arXiv preprint arXiv:1611.05923,", "citeRegEx": "Wojnowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wojnowicz et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "By understanding why a model does what it does, we can hope to improve the model (Amershi et al., 2015), discover new science (Shrikumar et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 36, "context": ", 2015), discover new science (Shrikumar et al., 2016), and provide end-users with explanations of actions that impact them (Goodman & Flaxman, 2016).", "startOffset": 30, "endOffset": 54}, {"referenceID": 23, "context": ", deep neural networks for image and speech recognition (Krizhevsky et al., 2012) \u2014 are complicated, blackbox models whose predictions seem hard to explain.", "startOffset": 56, "endOffset": 81}, {"referenceID": 34, "context": ", by locally fitting a simpler model around the test point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 43, "context": "Despite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is a recent paper from Wojnowicz et al. (2016), which introduced a method for approximating Cook\u2019s distance (a quantity related to influence) in generalized linear models.", "startOffset": 190, "endOffset": 214}, {"referenceID": 33, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 29, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 1, "context": "We address this challenge by efficiently approximating influence functions using techniques from second-order optimization (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016).", "startOffset": 123, "endOffset": 179}, {"referenceID": 17, "context": "We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior; debugging models; detecting dataset errors; and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015). ar X iv :1 70 3.", "startOffset": 327, "endOffset": 352}, {"referenceID": 34, "context": ", Ribeiro et al. (2016)); with normalized", "startOffset": 2, "endOffset": 24}, {"referenceID": 24, "context": "These results are from a logistic regression model trained to distinguish 1\u2019s from 7\u2019s in MNIST (LeCun et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 33, "context": "We discuss two techniques for approximating stest, both relying on the fact that the HVP of a single term in H\u03b8\u0302, [\u2207\u03b8L(zi, \u03b8\u0302)]v, can be computed for arbitrary v in roughly the same amount of time that\u2207\u03b8L(zi, \u03b8\u0302) would take, which is typically O(p) (Pearlmutter, 1994).", "startOffset": 249, "endOffset": 268}, {"referenceID": 29, "context": "exact solution takes p CG iterations, in practice we can get a good approximation with fewer iterations; see Martens (2010) for more details.", "startOffset": 109, "endOffset": 124}, {"referenceID": 1, "context": "We use a method developed by Agarwal et al. (2016) to get a stochastic estimator that only samples a single point per iteration, achieving a significant speedup.", "startOffset": 29, "endOffset": 51}, {"referenceID": 1, "context": "We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [\u2207\u03b8L(zi, \u03b8\u0302)]v can be decomposed into two rank-one matrix-vector products that can be efficiently computed in O(p) time.", "startOffset": 36, "endOffset": 58}, {"referenceID": 1, "context": "We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [\u2207\u03b8L(zi, \u03b8\u0302)]v can be decomposed into two rank-one matrix-vector products that can be efficiently computed in O(p) time. In our case, we rely on Pearlmutter (1994)\u2019s more general algorithm for fast HVPs, described above, to achieve the same time complexity.", "startOffset": 36, "endOffset": 275}, {"referenceID": 38, "context": "The network had 7 sets of convolutional layers with tanh(\u00b7) non-linearities, modeled after the all-convolutional network from (Springenberg et al., 2014).", "startOffset": 126, "endOffset": 153}, {"referenceID": 40, "context": "We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 and (b) an RBF SVM on a dog vs.", "startOffset": 58, "endOffset": 80}, {"referenceID": 35, "context": "fish image classification dataset we built from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class.", "startOffset": 57, "endOffset": 83}, {"referenceID": 15, "context": "Freezing neural networks in this way is not uncommon in computer vision applications and is equivalent to training a softmax on the bottleneck features (Donahue et al., 2014).", "startOffset": 152, "endOffset": 174}, {"referenceID": 19, "context": "security risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011).", "startOffset": 87, "endOffset": 107}, {"referenceID": 17, "context": "Recent work has generated adversarial test images that are visually indistinguishable from real test images but completely fool a classifier (Goodfellow et al., 2015).", "startOffset": 141, "endOffset": 166}, {"referenceID": 17, "context": "This is a training-set analogue of the fast gradient sign method used by (Goodfellow et al., 2015) for test-set attacks.", "startOffset": 73, "endOffset": 98}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6.", "startOffset": 111, "endOffset": 132}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6.", "startOffset": 111, "endOffset": 151}, {"referenceID": 4, "context": "We note that this attack is mathematically similar to the gradient-based dataset poisoning attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models; we elaborate on this connection in section 6. Biggio et al. (2012) constructed a dataset poisoning attack against a linear SVM on a two-class MNIST task, but had to modify the training points in an obviously distinguishable way to be effective.", "startOffset": 111, "endOffset": 265}, {"referenceID": 3, "context": "Domain mismatch \u2014 where the training distribution does not match the test distribution \u2014 can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010).", "startOffset": 160, "endOffset": 184}, {"referenceID": 21, "context": "Domain mismatches are common in biomedical data; for example, different hospitals can serve very different populations, and readmission models trained on one population can do poorly on another (Kansagara et al., 2011).", "startOffset": 194, "endOffset": 218}, {"referenceID": 39, "context": "We used logistic regression to predict readmission with a balanced training dataset of 20K diabetic patients from 100+ US hospitals, each represented by 127 features (Strack et al., 2014).", "startOffset": 166, "endOffset": 187}, {"referenceID": 4, "context": "Our case study is email spam classification, which relies on user-provided labels and is also vulnerable to adversarial attack (Biggio et al., 2011).", "startOffset": 127, "endOffset": 148}, {"referenceID": 9, "context": "The use of influence-based diagnostics originated in statistics in the 70s and 80s, driven by seminal papers by Cook and others (Cook, 1977; Cook & Weisberg, 1980; 1982), though similar ideas appeared even earlier in other forms, e.", "startOffset": 128, "endOffset": 169}, {"referenceID": 20, "context": ", the infinitesimal jackknife (Jaeckel, 1972).", "startOffset": 30, "endOffset": 45}, {"referenceID": 10, "context": "Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).", "startOffset": 160, "endOffset": 236}, {"referenceID": 44, "context": "Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).", "startOffset": 160, "endOffset": 236}, {"referenceID": 32, "context": "We divided the Enron1 spam dataset (Metsis et al., 2006) into training (n = 4147) and test (n = 1035) sets, training logistic regression on top of a bag-of-words representation.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods.", "startOffset": 31, "endOffset": 73}, {"referenceID": 12, "context": "Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods. Wojnowicz et al. (2016) uses matrix sketching to estimate Cook\u2019s distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models.", "startOffset": 31, "endOffset": 198}, {"referenceID": 4, "context": "2, our training-set attack is mathematically similar to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "2, our training-set attack is mathematically similar to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al., 2016a). These papers derived the attack directly from the KKT conditions without considering influence functions, though for continuous data the end result is equivalent to iterating Ipert,loss. Influence functions additionally allow us to consider attacks that change discrete data (section 2.2) and potentially models that are not convex or differentiable (sections 4.2, 4.3), though more work needs to be done to test the effectiveness of those attacks. Our work merges elements of this approach with the work of Goodfellow et al. (2015) and others on visually-imperceptible, adversarial test-set attacks in neural networks: we cast the dataset poisoning attack in the influence function framework and apply it in a larger-scale, neural network setting, creating a visually-imperceptible attack on the training set.", "startOffset": 86, "endOffset": 861}, {"referenceID": 6, "context": "In contrast to training-set attacks, Cadamuro et al. (2016) consider the task of taking an incorrect test prediction and", "startOffset": 37, "endOffset": 60}], "year": 2017, "abstractText": "How can we explain the predictions of a blackbox model? In this paper, we use influence functions \u2014 a classic technique from robust statistics \u2014 to trace a model\u2019s prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to highdimensional black-box models, even in nonconvex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessianvector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.", "creator": "LaTeX with hyperref package"}}}