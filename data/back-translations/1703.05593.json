{"id": "1703.05593", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Convolutional neural network architecture for geometric matching", "abstract": "We address the problem of determining similarities between two images in accordance with a geometric model, such as affine or thin plate spline transformation, and estimating their parameters. The contributions to this paper are threefold: First, we propose a revolutionary neural network architecture for geometric alignment, based on three main components that mimic the standard steps of feature extraction, matching, and simultaneous recognition and estimation of model parameters, while simultaneously being traceable throughout. Second, we show that network parameters can be trained from synthetically generated images without manual annotation, and that our matching layer significantly increases generalization capabilities on never-before-seen images. Finally, we show that the same model can perform alignments at both instance and category levels, delivering state-of-the-art results on the demanding Proposal Flow dataset.", "histories": [["v1", "Thu, 16 Mar 2017 13:03:54 GMT  (4307kb,D)", "http://arxiv.org/abs/1703.05593v1", "To appear in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)"], ["v2", "Thu, 13 Apr 2017 22:32:43 GMT  (9428kb,D)", "http://arxiv.org/abs/1703.05593v2", "In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)"]], "COMMENTS": "To appear in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ignacio rocco", "relja arandjelovi\\'c", "josef sivic"], "accepted": false, "id": "1703.05593"}, "pdf": {"name": "1703.05593.pdf", "metadata": {"source": "CRF", "title": "Convolutional neural network architecture for geometric matching", "authors": ["Ignacio Rocco", "Relja Arandjelovi\u0107", "Josef Sivic"], "emails": ["josef.sivic}@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45]. Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33]. This approach works well in many cases but fails in situations that (i) exhibit large changes of depicted appearance due to e.g. intra-class variation [18], or (ii) large changes of scene layout or non-rigid deformations that require complex geometric models with\n\u2020WILLOW project, Departement d\u2019Informatique de l\u2019E\u0301cole Normale Supe\u0301rieure, ENS/INRIA/CNRS UMR 8548. \u2217Now at DeepMind.\nmany parameters which are hard to estimate in a manner robust to outliers.\nIn this work we build on the traditional approach and develop a convolutional neural network architecture that mimics the standard matching process. First, we replace the standard local features with powerful trainable convolutional neural network features [27, 39], which allows us to cope with large changes of appearance between the matched images. Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].\nThe outcome is a convolutional neural network architecture trainable for the end task of geometric matching, which can cope with large appearance changes, and is therefore suitable for both instance-level and category-level matching problems.\n1\nar X\niv :1\n70 3.\n05 59\n3v 1\n[ cs\n.C V\n] 1\n6 M\nar 2\n01 7"}, {"heading": "2. Related work", "text": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37]. A limitation of the classical approach is is that, although it performs relatively well for instance-level matching, the feature detectors and descriptors lack the generalization ability for category-level matching.\nRecently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44]. However, these works still divide the image into a set of local patches and use the local patches for the descriptor computation. These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].\nIn this work, we take a different approach, treating the image as a whole, instead of a set of patches. Our approach has the advantage of capturing the interaction of the different parts of the image in a greater extent, which is not possible when the image is divided into local patches.\nRelated to us are also network architectures for estimating inter-frame motion in video [13] or instance-level homography estimation [10], however their goal is very different from ours targeting high-precision correspondence with very limited appearance variation and background clutter. Closer to us is the network architecture of [25] who, however, target a different problem of fine-grained category-level matching (different species of birds) with limited background clutter and small translations and scale changes, as their objects are centered in the image. They use different matching layer, which we show is inferior to ours.\nSome works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].\nOn the contrary, our approach is fully trainable in an endto-end manner and does not require a separate estimation of the geometric transformation (such as RANSAC), nor guidance by object proposals."}, {"heading": "3. Architecture for geometric matching", "text": "In this section, we introduce a new convolutional neural network (CNN) architecture for estimating the parameters of the geometric transformation between two input images. The architecture is designed to mimic the classical computer vision pipeline (e.g. [35]), while using differentiable modules so that it trainable end-to-end for the geometry estimation task. The classical approach consists of the fol-\nlowing stages: (i) local descriptors (e.g. SIFT) are extracted from both input images, (ii) the descriptors are matched across images to form a set of tentative correspondences, which are then used to (iii) robustly estimate the parameters of the geometric model using RANSAC or Hough voting.\nOur architecture, illustrated in Fig. 2, mimics this process by: (i) passing input images IA and IB through a siamese architecture consisting of convolutional layers, thus extracting feature maps fA and fB which are analogous to dense local descriptors, (ii) matching the feature maps (\u201cdescriptors\u201d) across images into a tentative correspondence map fAB , followed by a (iii) regression network which directly outputs the parameters of the geometric model, \u03b8\u0302, in a robust manner. The input to the network are the two images, and the outputs are the parameters of the chosen geometric model, e.g. a 6-D vector for an affine transformation.\nIn the following, we describe each of the three stages in detail."}, {"heading": "3.1. Feature extraction", "text": "The first stage of the pipeline is feature extraction, for which we use a standard CNN architecture. A CNN without fully connected layers takes an input image and produces a feature map f \u2208 Rh\u00d7w\u00d7d, which can be interpreted as a h\u00d7 w dense spatial grid of d-dimensional local descriptors. A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors. Thus, for feature extraction we use the VGG-16 network [39], cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization. As shown in Fig. 2, the feature extraction network is duplicated and arranged in a siamese configuration such that the two input images are passed through two identical networks which share parameters."}, {"heading": "3.2. Matching network", "text": "The image features produced by the feature extraction networks should be combined into a single tensor which will get used by the regressor network to estimate the geometric transformation. We first describe the classical approach for generating tentative correspondences, and then present our matching layer which mimics this process.\nTentative matches in classical geometry estimation. Classical methods start by computing the similarities between all pairs of descriptors across the two images. From this point on, the original descriptors are discarded as all the necessary information for geometry estimation is contained in the pairwise descriptor similarities and their spatial locations. Secondly, the pairs are pruned by either thresholding the similarity values, or, more commonly, only keeping the matches which involve the nearest (most similar) neighbors. Furthermore, the second nearest neighbor test [33] prunes the matches further by requiring that the match strength is significantly stronger than the second best match involving the same descriptor, which is very effective at discarding ambiguous matches. Matching layer. Our matching layer applies a similar procedure. Firstly, all pairs of similarities between descriptors are computed in a correlation layer. Secondly, similarity scores are processed and normalized such that ambiguous matches are strongly down-weighted.\nAnalogously to the classical approach, the geometry estimation should only consider descriptor similarities and their spatial locations, and not the original descriptors themselves. To achieve this, we propose to use a correlation layer. In more detail, given two features maps fA, fB \u2208 Rh\u00d7w\u00d7d, which we can interpret as an h\u00d7w dense grid of ddimensional descriptors fA and fB , we compute the output cAB as the scalar product between all pairs of descriptors across the two images. More formally, cAB \u2208 Rh\u00d7w\u00d7(h\u00d7w) is computed as:\ncAB(i, j, k) = fB(i, j) T fA(ik, jk)\nwhere ik \u2208 [1, 2, . . . , h], jk \u2208 [1, 2, . . . , w], k = h(jk \u2212 1) + ik\n(1) Intuitively we can think of this approach as finding how much each individual descriptor fA in fA correlates with the whole of fB , and computing a number of stacked correlation maps, as illustrated in Fig. 3.\nAs is done in the classical methods for tentative correspondence estimation, it is important to postprocess the pairwise similarity scores to remove ambiguous matches. To this end, we apply a channel-wise normalization of the correlation map at each spatial location to produce the final tentative correspondence map fAB . The normalization is performed by ReLU, to zero out the negative correlations, followed by L2-normalization, which has two desirable effects. Firstly, let us consider a case when the descriptor fA correlates well with only a single feature in fB . In this case, the normalization will amplify the score of the match, akin to the nearest neighbor matching in classical geometry estimation. Secondly, in the case of the descriptor fA matching multiple features in fB due to the existence of clutter or repetitive patterns, the matching scores will be downweighted similarly to the second nearest neighbor test [33]. However, note that both the correlation and the normalization operations are differentiable with respect to the input descriptors, which facilitates backpropagation thus enabling end-to-end learning.\nDiscussion. The first step of our matching layer, namely the correlation layer, is somewhat similar to layers used in DeepMatching [42] and FlowNet [13]. However, DeepMatching [42] only uses deep RGB patches and no part of their architecture is trainable. FlowNet [13] uses a spatially constrained correlation layer such that similarities are are only computed in a restricted spatial neighborhood thus limiting the range of geometric transformations that can be captured. This is acceptable for their task of learning to estimate optical flow, but is inappropriate for larger transformations that we consider in this work. Furthermore, neither of these methods performs score normalization, which we find to be crucial in dealing with cluttered scenes.\nPrevious works have used other matching layers to combine the descriptors across images, namely simple concatenation of descriptors or images themselves along the channel dimension [10] or subtraction [25]. However, these approaches suffer from two problems \u2013 firstly, as following layers are typically convolutional, these methods also struggle to handle large transformations as they are unable to detect long-range matches. Secondly, it is counter-intuitive to keep the information about the image and the local descriptors themselves, rather than only using pairwise descriptor similarities, as is commonly done in classical geometry estimation. Consider two pairs of images that are related with the same geometric transformation \u2013 the concatenation and subtraction strategies will produce different outputs for the two cases, making it hard for the regressor to deduce the geometric transformation. In contrast, the correlation layer output is likely to produce similar correlation maps for the two cases, regardless of the image content, thus simplifying the problem for the regressor. In line with this intuition, in Sec. 5.5 we show that the concatenation and subtraction\nmethods indeed have difficulties generalizing beyond the training set, while our correlation layer achieves generalization yielding superior results."}, {"heading": "3.3. Regression network", "text": "The normalized correlation map is passed through a regression network which directly estimates the parameters of the geometric transformation relating the two input images. In classical geometry estimation, this step consists of robustly estimating the transformation from the list of tentative correspondences. Local geometric constraints are often used to further prune the list of tentative matches [37, 40] by only retaining matches which are consistent with other matches in its spatial neighborhood. Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].\nWe again mimic the classical approach using a neural network, where we stack two blocks of convolutional layers, followed by batch-normalization [22] and the ReLU non-linearity, and add a final fully connected layer which regresses to the parameters of the transformation, as shown in Fig. 4. The intuition behind this architecture is that the estimation is performed in a bottom-up manner somewhat like Hough voting, where early convolutional layers vote for candidate transformations, and these are then processed by the later layers to aggregate the votes. The first convolutional layers can also enforce local neighborhood consensus [37, 40] by learning filters which only fire if nearby descriptors in image A are matched to nearby descriptors in image B, and we show qualitative evidence in Sec. 5.5 that this indeed does happen.\nDiscussion. A potential alternative to a convolutional regression network is to use fully connected layers. However, as the input correlation map size is quadratic in the number of image features, such a network would be hard to train due to a large number of parameters that would need to be learned, and it would not be scalable due to occupying too much memory and being too slow to use. It should be noted that even though the layers in our architecture are convolutional, the regressor can learn to estimate large transformations. This is because one spatial location in the input correlation map contains similarity scores between the corresponding feature in image B and all the features in image A (c.f. equation (1)), and not just the local neighborhood as in [13]."}, {"heading": "3.4. Hierarchy of transformations", "text": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35]. The motivation behind this method is that estimating a very complex transformation could be hard and computationally inefficient in the presence of clutter, so a robust and fast rough estimate of a simpler transformation can be used as a starting point, also regularizing the subsequent estimation of the more complex transformation.\nWe follow the same good practice and start by estimating an affine transformation, which is a 6 degree of freedom linear transformation capable of modeling translation, rotation, non-isotropic scaling and shear. The estimated affine transformation is then used to align image B to image A using an image resampling layer [23]. The aligned images are then passed through a second geometry estimation network which estimates the 18 parameters of the thin plate spline transformation. The final estimate of the geometric transformation is then obtained by composing the two transformations, which is also a thin plate spline. The process is illustrated in Fig. 5."}, {"heading": "4. Training framework", "text": "In order to train the parameters of our geometric matching CNN, it is necessary to design the appropriate loss function, and to use suitable training data. We address these two important points next."}, {"heading": "4.1. Loss function", "text": "We assume a fully supervised setting, where the training data consists of pairs of images and the desired outputs in the form of the parameters \u03b8GT of the ground-truth geometric transformation. The loss function L is designed in order to compare the estimated transformation \u03b8\u0302 with the groundtruth transformation \u03b8GT and, more importantly, compute the gradient of the loss function with respect to our estimates \u2202L\n\u2202\u03b8\u0302 . This gradient is then used in a standard manner\nto learn the network parameters which minimize the loss function by using backpropagation and Stochastic Gradient Descent.\nIt is desired for the loss to be general and not specific to a particular type of the geometric model, so that it can be used for estimating affine, homography, thin plate spline or any other geometric transformation. Furthermore, the loss should be independent of the parametrization of the transformation and thus should not directly operate on the parameter values themselves. We address all these design constraints by measuring loss on an imaginary grid of points which is being deformed by the transformation. Namely, we construct a grid of points in image A, transform it using\nthe ground truth and neural network estimated transformations T\u03b8GT and T\u03b8\u0302 with parameters \u03b8GT and \u03b8\u0302, respectively, and measure the discrepancy between the two transformed grids by summing the squared distances between the corresponding grid points:\nL(\u03b8\u0302, \u03b8GT ) = 1\nN N\u2211 i=1 d(T\u03b8\u0302(gi), T\u03b8GT (gi)) 2 (2)\nwhere G = {gi} = {(xi, yi)} is the uniform grid used, and N = |G|. We define the grid as having xi, yi \u2208 {s : s = \u22121 + 0.1 \u00d7 n, n \u2208 {0, 1, . . . , 20}}, that is to say, each coordinate belongs to a partition of [\u22121, 1] in equally spaced subintervals of steps 0.1. Note that we construct the coordinate system such that the center of the image is at (0, 0) and that the width and height of the image are equal to 2, i.e. the bottom left and top right corners have coordinates (\u22121,\u22121) and (1, 1), respectively.\nThe gradient of the loss function with respect to the transformation parameters, needed to perform backpropagation in order to learn network weights, can be computed easily if the location of the transformed grid points T\u03b8\u0302(gi) is differentiable with respect to \u03b8\u0302. This is commonly the case, for example, when T is an affine transformation, T\u03b8\u0302(gi) is linear in the parameters \u03b8\u0302 and therefore the loss can be differentiated in a straightforward manner."}, {"heading": "4.2. Training from synthetic transformations", "text": "For our training procedure we require fully supervised training data consisting of image pairs and a known geometric relation. Training CNNs usually requires a lot of data, and no public dataset exist that contain many image pairs annotated with their geometric transformation. Therefore, we opt for training from synthetically generated data, which gives us the flexibility to gather as many training examples as needed, for any 2-D geometric transformation of interest. We generate each example by sampling image A from a public image dataset, and generating image B by applying a random transformation T\u03b8GT to image A. More\nstrictly, image A is created from the central crop of the original image, while image B is created by transforming the original image with added symmetrical padding in order to avoid border artifacts; the procedure is shown in Fig. 6."}, {"heading": "5. Experimental results", "text": "In this section we give details on the training and evaluation datasets and implementation details, perform quantitative and qualitative evaluation of our method and compare it to baselines and the state-of-the-art. We also provide further insights into the components of our architecture."}, {"heading": "5.1. Evaluation dataset and performance measure", "text": "Quantitative evaluation of our method is performed on the Proposal Flow dataset of Ham et al. [18]. The dataset contains 900 image pairs depicting different instances of the same class, such as ducks and cars, but with large intraclass variations, e.g. the cars are often of different make, or the ducks can be of different subspecies. Furthermore, the images contain significant background clutter, as can be seen in Fig. 8. The task is to predict the locations of predefined keypoints from image A in image B. We do so by estimating a geometric transformation that warps image A into image B, and applying the same transformation to the\nkeypoint locations. We follow the standard evaluation metric used for this benchmark, i.e. the average probability of correct keypoint (PCK) [43], being the proportion of keypoints that are correctly matched. A keypoint is considered to be matched correctly if its predicted location is within a distance of \u03b1 \u00b7 max(h,w) of the target keypoint position, where \u03b1 = 0.1 and h and w are the height and width of the object bounding box, respectively."}, {"heading": "5.2. Training dataset", "text": "Two different training datasets are generated by sampling images from two publicly available image datasets: StreetView-synth is created from the training set of the Tokyo Time Machine [3] dataset which contains Google Street View images of Tokyo, and Pascal-synth is created from the training set of Pascal VOC 2011 [12] images. Each synthetically generated dataset contains 40k images, divided into 20k for training and 20k for validation. The ground truth transformation parameters were sampled independently from reasonable ranges, e.g. for the affine transformation we sample the relative scale change of up to 2\u00d7, while for thin plate spline we randomly jitter a 3 \u00d7 3 grid of control points by independently translating each point by up to one quarter of the image size in all directions.\nTo demonstrate the generalization capabilities of our approach, we use StreetView-synth for all experiments, as the street view images are very different in nature to the Proposal Flow images which are more Pascal-like. Pascalsynth is only used in Sec. 5.5 where we compare the effects of training with different data sources."}, {"heading": "5.3. Implementation details", "text": "We use the MatConvNet [41] library and train the networks with stochastic gradient descent, with learning rate 10\u22123, momentum 0.9, no weight decay and batch size of 16. There is no need for jittering as instead of data augmentation we can simply generate more synthetic training data. Input images are resized to 227 \u00d7 227 producing 15 \u00d7 15 feature maps that are passed into the matching layer. The networks are trained until convergence which typically occurs after 10 epochs, and takes 12 hours on a single GPU. Our final method for estimating affine transformations uses an ensemble of two networks that independently regress the parameters, which are then averaged to produce the final affine estimate. The two networks were trained on different ranges of affine transformations. As in Fig. 5, the estimated affine transformation is used to warp image A and pass it together with image B to a single network which estimates the thin plate spline transformation."}, {"heading": "5.4. Comparisons to state-of-the-art", "text": "We compare our method against SIFT Flow [30], Graphmatching kernels (GMK) [11], Deformable spatial pyramid\nmatching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18]. As shown in Tab. 1, our method outperforms all others and sets the new state-of-the-art by 1%. The best competing methods are based on Proposal Flow and make use of object proposals, which enables them to guide the matching towards regions of the images that contain objects. Their performance varies significantly with the choice of the object proposal method, illustrating the importance of this guided matching. On the contrary, our method does not use any guiding, but it still manages to outperform even the best Proposal Flow and object proposal combination.\nFurthermore, we also compare to affine transformations computed using RANSAC which uses the same descriptors as our method (VGG-16 pool4). Its parameters are tuned extensively to obtain the best result by adjusting the thresholds for the second nearest neighbor test and by pruning proposal transformations which are outside of the range of likely transformations. Our affine estimator proves to be more robust by beating RANSAC with 49% to 47%."}, {"heading": "5.5. Discussions and ablation studies", "text": "In this section we examine the importance of various components of our architecture. Apart from training on the StreetView-synth dataset, we also train on Pascal-synth which contains images that are more similar in nature to the images in the Proposal Flow benchmark. The results of these ablation studies are summarized in Tab. 2. Correlation versus concatenation and subtraction. Replacing our correlation-based matching layer with feature concatenation or subtraction, as proposed in [10] and [25], respectively, incurs a large performance drop. The behavior is expected as we designed the matching layer to only keep information on pairwise descriptor similarities rather than the descriptors themselves, as is good practice in classical\ngeometry estimation methods, while concatenation and subtraction do not follow this principle.\nGeneralization. Our method is relatively unaffected by the choice of training data as its performance is similar regardless whether it was trained with StreetView or Pascal im-\nages. We also attribute this to the design choice of operating on pairwise descriptor similarities rather than the raw descriptors. Furthermore, it can be seen that the performance of feature concatenation [10] and subtraction [25] is, apart from being significantly worse than ours, much more affected by the choice of the training set.\nNormalization. Tab. 2 also shows the importance of the correlation map normalization step, where the normalization improves results from 44% to 49%. The step mimics the second nearest neighbor test used in classical feature matching [33], as discussed in Sec. 3.2. Note that [13] also uses a correlation layer, but they do not normalize the map in any way, which is clearly suboptimal.\nWhat is being learned? We examine filters from the first convolutional layer of the regressor, which operate directly on the output of the matching layer, i.e. the tentative cor-\nrespondence map. Recall that each spatial location in the correspondence map contains all similarity scores between that feature in image B and all features in image A. Thus, a single slice through the weights of one convolutional filter at a particular spatial location can be visualized as an image, showing filter\u2019s preferences to features in image B that match to specific locations in image A. For example, if the central slice of a filter contains all zeros apart from a peak at the top-left corner, this filter responds positively to features in image B that match to the top-left of image A. Similarly, if many spatial locations of the filter produce similar images, then this filter is highly sensitive to spatially co-located features in image B that all match to the top-left of image A. For visualization, we pick the peaks from all slices of filter weights and average them together to produce a single image. Several filters shown in Fig. 7 confirm our hypothesis that this layer has learned to stimulate local neighborhood consensus as some filters respond strongly to spatially co-located features in image B that match to spatially consistent locations in image A. Furthermore, it can be observed that the size of the preferred spatial neighborhood varies across filters, thus showing that the filters are discriminative of the scale change."}, {"heading": "5.6. Qualitative results", "text": "Fig. 8 illustrates the effectiveness of our method in category-level matching, where quite challenging pairs of\nimages from the Proposal Flow dataset [18], containing large intra-class variations, are aligned correctly. The method is able to robustly, in the presence of clutter, estimate large translations, rotations, scale changes, as well as non-rigid transformations and some perspective changes.\nFig. 9 shows the quality of instance-level matching, where different images of the same scene are aligned correctly. The images are taken from the Tokyo Time Machine dataset [3] and are captured at different points in time which are months or years apart. Note that, by automatically highlighting the differences (in the feature space) between the aligned images, it is possible to detect changes in the scene, such as occlusions, changes in vegetation, structural differences e.g. new buildings being built."}, {"heading": "6. Conclusions", "text": "We have described a network architecture for geometric matching fully trainable from synthetic imagery without the need for manual annotations. Thanks to our matching layer, the network generalizes well to never seen before imagery, reaching state-of-the-art results on the challenging Proposal Flow dataset for category-level matching. This opens-up the possibility of applying our architecture to other difficult correspondence problems such as matching across depiction style. Acknowledgements. This work has been partly supported by the European Research Council (ERC grant LEAP\nno. 336845), Agence Nationale de la Recherche (Semapolis project, ANR-13-CORD-0003), the Inria CityLab IPL, CIFAR Learning in Machines&Brains program and ESIF, OP Research, development and education Project IMPACT No. CZ.02.1.01/0.0/0.0/15 003/0000468."}], "references": [{"title": "Building rome in a day", "author": ["S. Agarwal", "N. Snavely", "I. Simon", "S.M. Seitz", "R. Szeliski"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning to match aerial images with deep attentive architectures", "author": ["H. Altwaijry", "E. Trulls", "J. Hays", "P. Fua", "S. Belongie"], "venue": "In Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "NetVLAD: CNN architecture for weakly supervised place recognition", "author": ["R. Arandjelovi\u0107", "P. Gronat", "A. Torii", "T. Pajdla", "J. Sivic"], "venue": "In Proc. CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Factors of transferability from a generic ConvNet representation", "author": ["H. Azizpour", "A. Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "CoRR, abs/1406.5774,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Aggregating local deep features for image retrieval", "author": ["A. Babenko", "V. Lempitsky"], "venue": "In Proc. ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Pn-net: Conjoined triple deep network for learning local image descriptors", "author": ["V. Balntas", "E. Johns", "L. Tang", "K. Mikolajczyk"], "venue": "arXiv preprint arXiv:1601.05030,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "In Proc. ECCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Shape matching and object recognition using low distortion correspondence", "author": ["A. Berg", "T. Berg", "J. Malik"], "venue": "In Proc. CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Histogram of Oriented Gradients for Human Detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Deep image homography estimation", "author": ["D. DeTone", "T. Malisiewicz", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1606.03798,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A graph-matching kernel for object categorization", "author": ["O. Duchenne", "A. Joulin", "J. Ponce"], "venue": "In Proc. ICCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The PASCAL Visual Object Classes Challenge 2011", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "In Proc. ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Comm. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1981}, {"title": "Computer vision: a modern approach", "author": ["D.A. Forsyth", "J. Ponce"], "venue": "Prentice Hall Professional Technical Reference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In Proc. ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Non-rigid dense correspondence with applications for  image enhancement", "author": ["Y. HaCohen", "E. Shechtman", "D.B. Goldman", "D. Lischinski"], "venue": "Proc. ACM SIGGRAPH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Proposal flow", "author": ["B. Ham", "M. Cho", "C. Schmid", "J. Ponce"], "venue": "In Proc. CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Matchnet: Unifying feature and metric learning for patchbased matching", "author": ["X. Han", "T. Leung", "Y. Jia", "R. Sukthankar", "A.C. Berg"], "venue": "In Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A combined corner and edge detector", "author": ["C. Harris", "M. Stephens"], "venue": "In Alvey vision conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Multiple view geometry in computer vision", "author": ["R. Hartley", "A. Zisserman"], "venue": "Cambridge university press,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learned local descriptors for recognition and matching", "author": ["M. Jahrer", "M. Grabner", "H. Bischof"], "venue": "In Computer Vision Winter Workshop,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Warpnet: Weakly supervised matching for single-view reconstruction", "author": ["A. Kanazawa", "D.W. Jacobs", "M. Chandraker"], "venue": "In Proc. CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Deformable spatial pyramid matching for fast dense correspondences", "author": ["J. Kim", "C. Liu", "F. Sha", "K. Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Object recognition by affine invariant matching", "author": ["Y. Lamdan", "J.T. Schwartz", "H.J. Wolfson"], "venue": "In Proc. CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Robust object detection with interleaved categorization and segmentation", "author": ["B. Leibe", "A. Leonardis", "B. Schiele"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Sift flow: Dense correspondence across scenes and its applications", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE PAMI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Do convnets learn correspondence", "author": ["J.L. Long", "N. Zhang", "T. Darrell"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "In Proc. ICCV,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "An affine invariant interest point detector", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "In Proc. ECCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "In Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Deepmatching: Hierarchical deformable dense matching", "author": ["J. Revaud", "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Local grayvalue invariants for image retrieval", "author": ["C. Schmid", "R. Mohr"], "venue": "IEEE PAMI,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "In Proc. ICCV,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proc. ICLR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In Proc. ACMM,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Proc. ICCV,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Articulated human detection with flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "IEEE PAMI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "In Proc. CVPR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences", "author": ["T. Zhou", "Y.J. Lee", "S.X. Yu", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 95, "endOffset": 103}, {"referenceID": 0, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 165, "endOffset": 168}, {"referenceID": 16, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 191, "endOffset": 195}, {"referenceID": 44, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 222, "endOffset": 226}, {"referenceID": 32, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 8, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 204, "endOffset": 211}, {"referenceID": 17, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 204, "endOffset": 211}, {"referenceID": 36, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 286, "endOffset": 294}, {"referenceID": 39, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 286, "endOffset": 294}, {"referenceID": 13, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 384, "endOffset": 388}, {"referenceID": 27, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 28, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 32, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 17, "context": "intra-class variation [18], or (ii) large changes of scene layout or non-rigid deformations that require complex geometric models with", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "First, we replace the standard local features with powerful trainable convolutional neural network features [27, 39], which allows us to cope with large changes of appearance between the matched images.", "startOffset": 108, "endOffset": 116}, {"referenceID": 38, "context": "First, we replace the standard local features with powerful trainable convolutional neural network features [27, 39], which allows us to cope with large changes of appearance between the matched images.", "startOffset": 108, "endOffset": 116}, {"referenceID": 32, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 229, "endOffset": 233}, {"referenceID": 36, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 258, "endOffset": 266}, {"referenceID": 39, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 258, "endOffset": 266}, {"referenceID": 27, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 28, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 32, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 6, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 7, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 19, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 31, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 32, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 33, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 36, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 5, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 18, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 23, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 37, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 43, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 5, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 23, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 37, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 18, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 116, "endOffset": 124}, {"referenceID": 43, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 116, "endOffset": 124}, {"referenceID": 1, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 197, "endOffset": 200}, {"referenceID": 12, "context": "Related to us are also network architectures for estimating inter-frame motion in video [13] or instance-level homography estimation [10], however their goal is very different from ours targeting high-precision correspondence with very limited appearance variation and background clutter.", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Related to us are also network architectures for estimating inter-frame motion in video [13] or instance-level homography estimation [10], however their goal is very different from ours targeting high-precision correspondence with very limited appearance variation and background clutter.", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "Closer to us is the network architecture of [25] who, however, target a different problem of fine-grained category-level matching (different species of birds) with limited background clutter and small translations and scale changes, as their objects are centered in the image.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 29, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 30, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 17, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 268, "endOffset": 272}, {"referenceID": 34, "context": "[35]), while using differentiable modules so that it trainable end-to-end for the geometry estimation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 3, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 4, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 15, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 38, "context": "Thus, for feature extraction we use the VGG-16 network [39], cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 32, "context": "Furthermore, the second nearest neighbor test [33] prunes the matches further by requiring that the match strength is significantly stronger than the second best match involving the same descriptor, which is very effective at discarding ambiguous matches.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "Secondly, in the case of the descriptor fA matching multiple features in fB due to the existence of clutter or repetitive patterns, the matching scores will be downweighted similarly to the second nearest neighbor test [33].", "startOffset": 219, "endOffset": 223}, {"referenceID": 41, "context": "The first step of our matching layer, namely the correlation layer, is somewhat similar to layers used in DeepMatching [42] and FlowNet [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The first step of our matching layer, namely the correlation layer, is somewhat similar to layers used in DeepMatching [42] and FlowNet [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 41, "context": "However, DeepMatching [42] only uses deep RGB patches and no part of their architecture is trainable.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "FlowNet [13] uses a spatially constrained correlation layer such that similarities are are only computed in a restricted spatial neighborhood thus limiting the range of geometric transformations that can be captured.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "Previous works have used other matching layers to combine the descriptors across images, namely simple concatenation of descriptors or images themselves along the channel dimension [10] or subtraction [25].", "startOffset": 181, "endOffset": 185}, {"referenceID": 24, "context": "Previous works have used other matching layers to combine the descriptors across images, namely simple concatenation of descriptors or images themselves along the channel dimension [10] or subtraction [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "Local geometric constraints are often used to further prune the list of tentative matches [37, 40] by only retaining matches which are consistent with other matches in its spatial neighborhood.", "startOffset": 90, "endOffset": 98}, {"referenceID": 39, "context": "Local geometric constraints are often used to further prune the list of tentative matches [37, 40] by only retaining matches which are consistent with other matches in its spatial neighborhood.", "startOffset": 90, "endOffset": 98}, {"referenceID": 13, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 28, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 32, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 21, "context": "We again mimic the classical approach using a neural network, where we stack two blocks of convolutional layers, followed by batch-normalization [22] and the ReLU non-linearity, and add a final fully connected layer which regresses to the parameters of the transformation, as shown in Fig.", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "The first convolutional layers can also enforce local neighborhood consensus [37, 40] by learning filters which only fire if nearby descriptors in image A are matched to nearby descriptors in image B, and we show qualitative evidence in Sec.", "startOffset": 77, "endOffset": 85}, {"referenceID": 39, "context": "The first convolutional layers can also enforce local neighborhood consensus [37, 40] by learning filters which only fire if nearby descriptors in image A are matched to nearby descriptors in image B, and we show qualitative evidence in Sec.", "startOffset": 77, "endOffset": 85}, {"referenceID": 12, "context": "equation (1)), and not just the local neighborhood as in [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 31, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 34, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 22, "context": "The estimated affine transformation is then used to align image B to image A using an image resampling layer [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "the average probability of correct keypoint (PCK) [43], being the proportion of keypoints that are correctly matched.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "Two different training datasets are generated by sampling images from two publicly available image datasets: StreetView-synth is created from the training set of the Tokyo Time Machine [3] dataset which contains Google Street View images of Tokyo, and Pascal-synth is created from the training set of Pascal VOC 2011 [12] images.", "startOffset": 185, "endOffset": 188}, {"referenceID": 11, "context": "Two different training datasets are generated by sampling images from two publicly available image datasets: StreetView-synth is created from the training set of the Tokyo Time Machine [3] dataset which contains Google Street View images of Tokyo, and Pascal-synth is created from the training set of Pascal VOC 2011 [12] images.", "startOffset": 317, "endOffset": 321}, {"referenceID": 40, "context": "We use the MatConvNet [41] library and train the networks with stochastic gradient descent, with learning rate 10\u22123, momentum 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "We compare our method against SIFT Flow [30], Graphmatching kernels (GMK) [11], Deformable spatial pyramid Methods PCK (%)", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "We compare our method against SIFT Flow [30], Graphmatching kernels (GMK) [11], Deformable spatial pyramid Methods PCK (%)", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "All the numbers apart from ours and RANSAC are taken from [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Replacing our correlation-based matching layer with feature concatenation or subtraction, as proposed in [10] and [25], respectively, incurs a large performance drop.", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "Replacing our correlation-based matching layer with feature concatenation or subtraction, as proposed in [10] and [25], respectively, incurs a large performance drop.", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "Concatenation [10] 26 29 Subtraction [25] 18 21 Ours without normalization 44 \u2013 Ours 49 45", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Concatenation [10] 26 29 Subtraction [25] 18 21 Ours without normalization 44 \u2013 Ours 49 45", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "Furthermore, it can be seen that the performance of feature concatenation [10] and subtraction [25] is, apart from being significantly worse than ours, much more affected by the choice of the training set.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Furthermore, it can be seen that the performance of feature concatenation [10] and subtraction [25] is, apart from being significantly worse than ours, much more affected by the choice of the training set.", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "The step mimics the second nearest neighbor test used in classical feature matching [33], as discussed in Sec.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Note that [13] also uses a correlation layer, but they do not normalize the map in any way, which is clearly suboptimal.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "8 illustrates the effectiveness of our method in category-level matching, where quite challenging pairs of images from the Proposal Flow dataset [18], containing large intra-class variations, are aligned correctly.", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "The images are taken from the Tokyo Time Machine dataset [3] and are captured at different points in time which are months or years apart.", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate-spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.", "creator": "LaTeX with hyperref package"}}}