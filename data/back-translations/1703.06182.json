{"id": "1703.06182", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability", "abstract": "Many tasks in the real world involve multiple actors with partial observability and limited communication. In this environment, learning is a challenge due to local perceptions of actors who perceive the world as non-stationary due to simultaneous team research. Approaches that learn specialized strategies for individual tasks face major problems in the real world: Not only do actors have to learn and store their own strategy for each task, but in practice the identity of the task is often unobservable, making these algorithms inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent enhanced learning with partial observability. We introduce a decentralized learning approach that is robust to simultaneous team interactions, and present an approach to distilling individual task strategies into a unified policy that works well across multiple related tasks without explicitly providing a task identity.", "histories": [["v1", "Fri, 17 Mar 2017 19:32:38 GMT  (5933kb,D)", "https://arxiv.org/abs/1703.06182v1", null], ["v2", "Sat, 25 Mar 2017 15:54:36 GMT  (5933kb,D)", "http://arxiv.org/abs/1703.06182v2", null], ["v3", "Wed, 14 Jun 2017 16:09:39 GMT  (6467kb,D)", "http://arxiv.org/abs/1703.06182v3", null], ["v4", "Thu, 13 Jul 2017 17:34:34 GMT  (6467kb,D)", "http://arxiv.org/abs/1703.06182v4", "Accepted to ICML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.MA", "authors": ["shayegan omidshafiei", "jason pazis", "christopher amato", "jonathan p how", "john vian"], "accepted": true, "id": "1703.06182"}, "pdf": {"name": "1703.06182.pdf", "metadata": {"source": "META", "title": "Deep Decentralized Multi-task Multi-Agent Reinforcement Learningunder Partial Observability", "authors": ["Shayegan Omidshafiei", "Jason Pazis", "Christopher Amato", "Jonathan P. How", "John Vian"], "emails": ["<shayegan@mit.edu>."], "sections": [{"heading": "1. Introduction", "text": "In multi-task reinforcement learning (MTRL) agents are presented several related target tasks (Taylor & Stone, 2009; Caruana, 1998) with shared characteristics. Rather than specialize on a single task, the objective is to generalize performance across all tasks. For example, a team of autonomous underwater vehicles (AUVs) learning to detect and repair faults in deep-sea equipment must be able to do so in many settings (varying water currents, lighting, etc.), not just under the circumstances observed during training.\nMany real-world problems involve multiple agents with\n1Laboratory for Information and Decision Systems (LIDS), MIT, Cambridge, MA, USA 2College of Computer and Information Science (CCIS), Northeastern University, Boston, MA, USA 3Boeing Research & Technology, Seattle, WA, USA. Correspondence to: Shayegan Omidshafiei <shayegan@mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\npartial observability and limited communication (e.g., the AUV example) (Oliehoek & Amato, 2016), but generating accurate models for these domains is difficult due to complex interactions between agents and the environment. Learning is difficult in these settings due to partial observability and local viewpoints of agents, which perceive the environment as non-stationary due to teammates\u2019 actions. Efficient learners must extract knowledge from past tasks to accelerate learning and improve generalization to new tasks. Learning specialized policies for individual tasks can be problematic, as not only do agents have to store a distinct policy for each task, but in practice face scenarios where the identity of the task is often non-observable.\nExisting MTRL methods focus on single-agent and/or fully observable settings (Taylor & Stone, 2009). By contrast, this work considers cooperative, independent learners operating in partially-observable, stochastic environments, receiving feedback in the form of local noisy observations and joint rewards. This setting is general and realistic for many multi-agent domains. We introduce the multitask multi-agent reinforcement learning (MT-MARL) under partial observability problem, where the goal is to maximize execution-time performance on a set of related tasks, without explicit knowledge of the task identity. Each MTMARL task is formalized as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002), a general formulation for cooperative decision-making under uncertainty. MT-MARL poses significant challenges, as each agent must learn to coordinate with teammates to achieve good performance, ensure policy generalization across all tasks, and conduct (implicit) execution-time inference of the underlying task ID to make sound decisions using local noisy observations. As typical in existing MTRL approaches, this work focuses on average asymptotic performance across all tasks (Caruana, 1998; Taylor & Stone, 2009) and sample-efficient learning.\nWe propose a two-phase MT-MARL approach that first uses cautiously-optimistic learners in combination with Deep Recurrent Q-Networks (DRQNs) (Hausknecht & Stone, 2015) for action-value approximation. We introduce Concurrent Experience Replay Trajectories (CERTs), a decentralized extension of ex-\nar X\niv :1\n70 3.\n06 18\n2v 4\n[ cs\n.L G\n] 1\n3 Ju\nl 2 01\n7\nperience replay (Lin, 1992; Mnih et al., 2015) targeting sample-efficient and stable MARL. This first contribution enables coordination in single-task MARL under partial observability. The second phase of our approach distills each agent\u2019s specialized action-value networks into a generalized recurrent multi-task network. Using CERTs and optimistic learners, well-performing distilled policies (Rusu et al., 2015) are learned for multi-agent domains. Both the single-task and multi-task phases of the algorithm are demonstrated to achieve good performance on a set of multi-agent target capture Dec-POMDP domains. The approach makes no assumptions about communication capabilities and is fully decentralized during learning and execution. To our knowledge, this is the first formalization of decentralized MT-MARL under partial observability."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Reinforcement Learning", "text": "Single-agent RL under full observability is typically formalized using Markov Decision Processes (MDPs) (Sutton & Barto, 1998), defined as tuple \u3008S,A, T ,R, \u03b3\u3009. At timestep t, the agent with state s \u2208 S executes action a \u2208 A using policy \u03c0(a|s), receives reward rt = R(s) \u2208 R, and transitions to state s\u2032 \u2208 S with probability P (s\u2032|s, a) = T (s, a, s\u2032). Denoting discounted return as Rt = \u2211H t\u2032=t \u03b3\nt\u2032\u2212trt, with horizon H and discount factor \u03b3 \u2208 [0, 1), the action-value (or Q-value) is defined as Q\u03c0(s, a) = E\u03c0[Rt|st = s, at = a]. Optimal policy \u03c0\u2217 maximizes the Q-value function, Q\u03c0 \u2217 (s, a) = max\u03c0 Q(s, a). In RL, the agent interacts with the environment to learn \u03c0\u2217 without explicit provision of the MDP model. Model-based methods first learn T andR, then use a planner to find Q\u03c0 \u2217 . Model-free methods typically directly learn Q-values or policies, so can be more space and computation efficient.\nQ-learning (Watkins & Dayan, 1992) iteratively estimates the optimal Q-value function using backups, Q(s, a) = Q(s, a) + \u03b1[r + \u03b3maxa\u2032 Q(s\n\u2032, a\u2032) \u2212 Q(s, a)], where \u03b1 \u2208 [0, 1) is the learning rate and the term in brackets is the temporal-difference (TD) error. Convergence to Q\u03c0 \u2217 is guaranteed in the tabular (no approximation) case provided sufficient state/action space exploration; however, tabulated learning is unsuitable for problems with large state/action spaces. Practical TD methods instead use function approximators (Gordon, 1995) such as linear combinations of basis functions or neural networks, leveraging inductive bias to execute similar actions in similar states. Deep Q-learning is a state-of-the-art approach using a Deep Q-Network (DQN) for Q-value approximation (Mnih et al., 2015). At each iteration j, experience tuple \u3008s, a, r, s\u2032\u3009 is sampled from replay memory M and DQN parameters \u03b8 are updated to minimize loss Lj(\u03b8j) = E(s,a,r,s\u2032)\u223cM[(r +\n\u03b3maxa\u2032 Q(s \u2032, a\u2032; \u03b8\u0302j)\u2212Q(s, a; \u03b8j))2]. Replay memoryM is a first-in first-out queue containing the set of latest experience tuples from -greedy policy execution. Target network parameters \u03b8\u0302j are updated less frequently and, in combination with experience replay, are critical for stable deep Q-learning.\nAgents in partially-observable domains receive observations of the latent state. Such domains are formalized as Partially Observable Markov Decision Processes (POMDPs), defined as \u3008S,A, T ,R,\u2126,O, \u03b3\u3009 (Kaelbling et al., 1998). After each transition, the agent observes o \u2208 \u2126 with probability P (o|s\u2032, a) = O(o, s\u2032, a). Due to noisy observations, POMDP policies map observation histories to actions. As Recurrent Neural Networks (RNNs) inherently maintain an internal state ht to compress input history until timestep t, they have been demonstrated to be effective for learning POMDP policies (Wierstra et al., 2007). Recent work has introduced Deep Recurrent QNetworks (DRQNs) (Hausknecht & Stone, 2015), combining Long Short-Term Memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) with DQNs for RL in POMDPs. Our work extends this single-task, single-agent approach to the multi-task, multi-agent setting."}, {"heading": "2.2. Multi-agent RL", "text": "Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Bus\u0327oniu et al., 2010). Our work focuses on cooperative settings, where agents share a joint return. Claus & Boutilier (1998) dichotomize MARL agents into two classes: Joint Action Learners (JALs) and Independent Learners (ILs). JALs observe actions taken by all agents, whereas ILs only observe local actions. As observability of joint actions is a strong assumption in partially observable domains, ILs are typically more practical, despite having to solve a more challenging problem (Claus & Boutilier, 1998). Our approach utilizes ILs that conduct both learning and execution in a decentralized manner.\nUnique challenges arise in MARL due to agent interactions during learning (Bus\u0327oniu et al., 2010; Matignon et al., 2012). Multi-agent domains are non-stationary from agents\u2019 local perspectives, due to teammates\u2019 interactions with the environment. ILs, in particular, are susceptible to shadowed equilibria, where local observability and nonstationarity cause locally optimal actions to become a globally sub-optimal joint action (Fulda & Ventura, 2007). Effective MARL requires each agent to tightly coordinate with fellow agents, while also being robust against destabilization of its own policy due to environmental nonstationarity. Another desired characteristic is robustness to alter-exploration, or drastic changes in policies due to exploratory actions of teammates (Matignon et al., 2012)."}, {"heading": "2.3. Transfer and Multi-Task Learning", "text": "Transfer Learning (TL) aims to generalize knowledge from a set of source tasks to a target task (Pan & Yang, 2010). In single-agent, fully-observable RL, each task is formalized as a distinct MDP (i.e., MDPs and tasks are synonymous) (Taylor & Stone, 2009). While TL assumes sequential transfer, where source tasks have been previously learned and even may not be related to the target task, Multi-Task Reinforcement Learning (MTRL) aims to learn a policy that performs well on related target tasks from an underlying task distribution (Caruana, 1998; Pan & Yang, 2010). MTRL tasks can be learned simultaneously or sequentially (Taylor & Stone, 2009). MTRL directs the agent\u2019s attention towards pertinent training signals learned on individual tasks, enabling a unified policy to generalize well across all tasks. MTRL is most beneficial when target tasks share common features (Wilson et al., 2007), and most challenging when the task ID is not explicitly specified to agents during execution \u2013 the setting addressed in this paper."}, {"heading": "3. Related Work", "text": ""}, {"heading": "3.1. Multi-agent RL", "text": "Bus\u0327oniu et al. (2010) present a taxonomy of MARL approaches. Partially-observable MARL has received limited attention. Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al., 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al., 2012). Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).\nOur approach is most related to IL algorithms that learn Qvalues, as their representational power is more conducive to transfer between tasks, in contrast to policy tables or FSCs. The majority of existing IL approaches assume full observability. Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning. This simple approach has some empirical success (Matignon et al., 2012). Distributed Q-learning (Lauer & Riedmiller, 2000) is an optimal algorithm for deterministic domains; it updates Q-values only when they are guaranteed to increase, and the policy only for actions that are no longer greedy with respect to Q-values. Bowling & Veloso (2002) conduct Policy Hill Climbing using the Win-or-Learn Fast heuristic to decrease (increase) each agent\u2019s learning rate when it performs well (poorly). Frequency Maximum Q-Value heuristics (Kapetanakis & Kudenko, 2002) bias action selection towards those con-\nsistently achieving max rewards. Hysteretic Q-learning (Matignon et al., 2007) addresses miscoordination using cautious optimism to stabilize policies while teammates explore. Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach. Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing. They also evaluate a model combining Decentralized Q-learning with DRQNs, which they call Reinforced Inter-Agent Learning. Given the decentralized nature of this latter model (called Dec-DRQN herein for clarity), we evaluate our method against it. Concurrent to our work, Foerster et al. (2017) investigated an alternate means of stabilizing experience replay for the centralized learning case."}, {"heading": "3.2. Transfer and Multi-task RL", "text": "Taylor & Stone (2009) and Torrey & Shavlik (2009) provide excellent surveys of transfer and multi-task RL, which almost exclusively target single-agent, fully-observable settings. Tanaka & Yamamura (2003) use first and secondorder statistics to compute a prioritized sweeping metric for MTRL, enabling an agent to maximize lifetime reward over task sequences. Ferna\u0301ndez & Veloso (2006) introduce an MDP policy similarity metric, and learn a policy library that generalizes well to tasks within a shared domain. Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP. They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008). Brunskill & Li (2013) introduce an MDP clustering approach that reduces negative transfer in MTRL, and prove reduction of sample complexity of exploration using transfer. Taylor et al. (2013) introduce parallel transfer to accelerate multi-agent learning using interagent transfer. Recent work extends the notion of neural network distillation (Hinton et al., 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al., 2015). The efficacy of the distillation technique for single-agent MDPs with large state spaces leads our work to use it as a foundation for the proposed MT-MARL under partial observability approach."}, {"heading": "4. Multi-task Multi-agent RL", "text": "This section introduces MT-MARL under partial observability. We formalize single-task MARL using the Decentralized Partially Observable Markov Decision Process (Dec-POMDP), defined as \u3008I,S,A, T ,R,\u2126,O, \u03b3\u3009, where I is a set of n agents, S is the state space, A =\n\u00d7iA(i) is the joint action space, and \u2126 = \u00d7i\u2126(i) is the joint observation space (Bernstein et al., 2002).1 Each agent i executes action a(i) \u2208 A(i), where joint action a = \u3008a(1), . . . , a(n)\u3009 causes environment state s \u2208 S to transition with probability P (s\u2032|s,a) = T (s,a, s\u2032). At each timestep, each agent receives observation o(i) \u2208 \u2126(i), with joint observation probability P (o|s\u2032,a) = O(o, s\u2032,a), where o = \u3008o(1), . . . , o(n)\u3009. Let local observation history at timestep t be ~ot (i) = (o (i) 1 , . . . , o (i) t ), where ~ot\n(i) \u2208 ~Ot (i) . Single-agent policy \u03c0(i) : ~Ot (i) 7\u2192 A(i) conducts action selection, and the joint policy is denoted \u03c0 = \u3008\u03c0(1), . . . , \u03c0(n)\u3009. For simplicity, we consider only pure joint policies, as finite-horizon Dec-POMDPs have at least one pure joint optimal policy (Oliehoek et al., 2008). The team receives a joint reward rt = R(st,at) \u2208 R at each timestep t, the objective being to maximize the value (or expected return), V = E[ \u2211H t=0 \u03b3\ntrt]. While Dec-POMDP planning approaches assume agents do not observe intermediate rewards, we make the typical RL assumption that they do. This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).\nILs provide a scalable way to learn in Dec-POMDPs, as each agent\u2019s policy maps local observations to actions. However, the domain appears non-stationary from the perspective of each Dec-POMDP agent, a property we formalize by extending the definition by Laurent et al. (2011). Definition 1. Let a\u2212(i) = a \\ {a(i)}. Local decision process for agent i is stationary if, for all timesteps t, u \u2208 N,\n\u2211 a \u2212(i) t \u2208A \u2212(i) P (s\u2032|s,\u3008a(i),a\u2212(i)t \u3009) = \u2211 a\u2212(i)u \u2208A \u2212(i) P (s\u2032|s,\u3008a(i),a\u2212(i)u \u3009), (1) and\u2211 a \u2212(i) t \u2208A \u2212(i) P (o(i)|s\u2032,\u3008a(i),a\u2212(i)t \u3009) = \u2211 a\u2212(i)u \u2208A \u2212(i) P (o(i)|s\u2032,\u3008a(i),a\u2212(i)u \u3009).\n(2)\nLetting \u03c0\u2212(i) = \u03c0 \\ {\u03c0(i)}, non-stationarity from the local perspective of agent i follows as in general a\u2212(i)t = \u03c0\u2212(i)(~ot) 6= \u03c0\u2212(i)(~ou) = a\u2212(i)u , which causes violation of (1) and (2). Thus, MARL extensions of single-agent algorithms that assume stationary environments, such as Dec-DRQN, are inevitably ill-fated. This motivates our decision to first design a single-task, decentralized MARL approach targeting non-stationarity in Dec-POMDP learning.\nThe MT-MARL problem in partially observable settings is now introduced by extending the single-agent, fullyobservable definition of Ferna\u0301ndez & Veloso (2006). Definition 2. A partially-observable MT-MARL DomainD is a tuple \u3008I,S,A,\u2126, \u03b3\u3009, where I is the set of agents, S is\n1Superscripts indicate local parameters for agent i \u2208 I.\nthe environment state space, A is the joint action space, \u2126 is the joint observation space, and \u03b3 is the discount factor.\nDefinition 3. A partially-observable MT-MARL Task Tj is a tuple \u3008D, Tj ,Rj ,Oj\u3009, where D is a shared underlying domain; Tj , Rj , Oj are, respectively, the task-specific transition, reward, and observation functions.\nIn MT-MARL, each episode e \u2208 {1, . . . , E} consists of a randomly sampled Task Tj from domain D. The team observes the task ID, j, during learning, but not during execution. The objective is to find a joint policy that maximizes average empirical execution-time return in all E episodes, V\u0304 = 1E \u2211E e=0 \u2211He t=0 \u03b3\ntRe(st,at), where He is the time horizon of episode e."}, {"heading": "5. Approach", "text": "This section introduces a two-phase approach for partiallyobservable MT-MARL; the approach first conducts singletask specialization, and subsequently unifies task-specific DRQNs into a joint policy that performs well in all tasks."}, {"heading": "5.1. Phase I: Dec-POMDP Single-Task MARL", "text": "As Dec-POMDP RL is notoriously complex (and solving for the optimal policy is NEXP-complete even with a known model (Bernstein et al., 2002)), we first introduce an approach for stable single-task MARL. This enables agents to learn coordination, while also learning Q-values needed for computation of a unified MT-MARL policy."}, {"heading": "5.1.1. DECENTRALIZED HYSTERETIC DEEP RECURRENT Q-NETWORKS (DEC-HDRQNS)", "text": "Due to partial observability and local non-stationarity, model-based Dec-POMDP MARL is extremely challenging (Banerjee et al., 2012). Our approach is model-free and decentralized, learning Q-values for each agent. In contrast to policy tables or FSCs, Q-values are amenable to the multi-task distillation process as they inherently measure quality of all actions, rather than just the optimal action.\nOverly-optimistic MARL approaches (e.g., Distributed Qlearning (Lauer & Riedmiller, 2000)) completely ignore low returns, which are assumed to be caused by teammates\u2019 exploratory actions. This causes severe overestimation of Q-values in stochastic domains. Hysteretic Qlearning (Matignon et al., 2007), instead, uses the insight that low returns may also be caused by domain stochasticity, which should not be ignored. This approach uses two learning rates: nominal learning rate, \u03b1, is used when the TD-error is non-negative; a smaller learning rate, \u03b2, is used otherwise (where 0 < \u03b2 < \u03b1 < 1). The result is hysteresis (lag) of Q-value degradation for actions associated with positive past experiences that occurred due to successful\ncooperation. Agents are, therefore, robust against negative learning due to teammate exploration and concurrent actions. Notably, unlike Distributed Q-learning, Hysteretic Q-learning permits eventual degradation of Q-values that were overestimated due to outcomes unrelated to their associated action.\nHysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches. Encouraged by these results, we introduce Decentralized Hysteretic Deep Recurrent Q-Networks (Dec-HDRQNs) for partially-observable domains. This approach exploits the robustness of hysteresis to non-stationarity and alterexploration, in addition to the representational power and memory-based decision making of DRQNs. As later demonstrated, Dec-HDRQN is well-suited to Dec-POMDP MARL, as opposed to non-hysteretic Dec-DRQN."}, {"heading": "5.1.2. CONCURRENT EXPERIENCE REPLAY TRAJECTORIES (CERTS)", "text": "Experience replay (sampling a memory bank of experience tuples \u3008s, a, r, s\u2032\u3009 for TD learning) was first introduced by Lin (1992) and recently shown to be crucial for stable deep Q-learning (Mnih et al., 2015). With experience replay, sampling cost is reduced as multiple TD updates can be conducted using each sample, enabling rapid Q-value propagation to preceding states without additional environmental interactions. Experience replay also breaks temporal correlations of samples used for Q-value updates\u2014crucial for reducing generalization error, as the stochastic optimization algorithms used for training DQNs typically assume i.i.d. data (Bengio, 2012).\nDespite the benefits in single-agent settings, existing MARL approaches have found it necessary to disable experience replay (Foerster et al., 2016). This is due to the non-concurrent (and non-stationary) nature of local experiences when sampled independently for each agent, despite the agents learning concurrently. A contributing factor is that inter-agent desynchronization of experiences compounds the prevalence of earlier-mentioned shadowed equilibria challenges, destabilizing coordination. As a motivating example, consider a 2 agent game where A(1) = A(2) = {a1, a2}. Let there be two optimal joint actions: \u3008a1, a1\u3009 and \u3008a2, a2\u3009 (e.g., only these joint actions have positive, equal reward). Given independent experience samples for each agent, the first agent may learn action a1 as optimal, whereas the second agent learns a2, resulting in arbitrarily poor joint action \u3008a1, a2\u3009. This motivates a need for concurrent (synchronized) sampling of experiences across the team in MARL settings. Concurrent experiences induce correlations in local policy updates, so\nthat given existence of multiple equilibria, agents tend to converge to the same one. Thus, we introduce Concurrent Experience Replay Trajectories (CERTs), visualized in Fig. 1a. During execution of each learning episode e \u2208 N+, each agent i collects experience tuple \u3008o(i)t , a (i) t , rt, o (i) t+1\u3009 at timestep t, where ot, at, and rt are current observation, action, and reward, and ot+1 is the subsequent observation. Fig. 1a visualizes each experience tuple as a cube. Experiences in each episode are stored in a sequence (along time axis t of Fig. 1a), as Dec-HDRQN assumes an underlying RNN architecture that necessitates sequential samples for each training iteration. Importantly, as all agents are aware of timestep t and episode e, they store their experiences concurrently (along agent index axis i of Fig. 1a). Upon episode termination, a new sequence is initiated (a new row along episode axis e of Fig. 1a). No restrictions are imposed on terminal conditions (i.e., varying trajectory lengths are permitted along axis t of Fig. 1a). CERTs are a first-in first-out circular queue along the episode axis e, such that old episodes are eventually discarded."}, {"heading": "5.1.3. TRAINING DEC-HDRQNS USING CERTS", "text": "Each agent i maintains DRQN Q(i)(o(i)t , h (i) t\u22121, a (i); \u03b8(i)), where o(i)t is the latest local observation, h (i) t\u22121 is the RNN hidden state, a(i) is the action, and \u03b8(i) are the local DRQN parameters. DRQNs are trained on experience sequences (traces) with tracelength \u03c4 . Figure 1b visualizes the minibatch sampling procedure for training, with \u03c4 = 4. In each training iteration, agents first sample a concurrent minibatch of episodes. All agents\u2019 sampled traces have the same starting timesteps (i.e., are coincident along agent axis i in Fig. 1b). Guaranteed concurrent sampling merely requires a one-time (offline) consensus of agents\u2019 random number generator seeds prior to initiating learning. This ensures our approach is fully decentralized and assumes no explicit communication, even during learning. Fig. 1b shows a minibatch of 3 episodes, e, sampled in red. To train DRQNs, Hausknecht & Stone (2015) suggest randomly\nsampling a timestep within each episode, and training using \u03c4 backward steps. However, this imposes a bias where experiences in each episode\u2019s final \u03c4 timesteps are used in fewer recurrent updates. Instead, we propose that for each sampled episode e, agents sample a concurrent start timestep t0 for the trace from interval {\u2212\u03c4 + 1, . . . ,He}, where He is the timestep of the episode\u2019s final experience. For example, the three sampled (red) traces in Fig. 1b start at timesteps +1, \u22121, and +2, respectively. This ensures all experiences have equal probability of being used in updates, which we found especially critical for fast training on tasks with only terminal rewards.\nSampled traces sometimes contain elements outside the episode interval (indicated as \u2205 in Fig. 1b). We discard \u2205 experiences and zero-pad the suffix of associated traces (to ensure all traces have equal length \u03c4 , enabling seamless use of fixed-length minibatch optimizers in standard deep learning libraries). Suffix (rather than prefix) padding ensures RNN internal states of non-\u2205 samples are unaffected. In training iteration j, agent i uses the sampling procedure to collect a minibatch of traces from CERT memoryM(i),\nB = {\u3008\u3008obt0 , a b t0 , r b t0 , o b t0+1\u3009, . . . , (3)\n\u3008obt0+\u03c4\u22121, a b t0+\u03c4\u22121, r b t0+\u03c4\u22121, o b t0+\u03c4 \u3009\u3009}b={1,...,B},\nwhere t0 is the start timestep for each trace, b is trace index, andB is number of traces (minibatch size).2 Each trace b is used to calculate a corresponding sequence of target values,\n{\u3008\u3008ybt0\u3009, . . . , \u3008y b t0+\u03c4\u22121\u3009\u3009}b={1,...,B}, (4)\nwhere ybt = r b t + \u03b3maxa\u2032 Q(o b t+1, h b t , a \u2032; \u03b8\u0302 (i) j ). Target network parameters \u03b8\u0302(i)j are updated less frequently, for stable learning (Mnih et al., 2015). Loss over all traces is,\nLj(\u03b8 (i) j ) = E(obt ,abt ,rbt ,obt+1)\u223cM(i) [(\u03b4 b t ) 2], (5)\nwhere \u03b4bt = y b t \u2212 Q(obt , hbt\u22121, abt ; \u03b8 (i) j ). Loss contributions of suffix \u2205-padding elements are masked out. Parameters are updated via gradient descent on Eq. (5), with the caveat of hysteretic learning rates 0 < \u03b2 < \u03b1 < 1, where learning rate \u03b1 is used if \u03b4bt \u2265 0, and \u03b2 is used otherwise."}, {"heading": "5.2. Phase II: Dec-POMDP MT-MARL", "text": "Following task specialization, the second phase involves distillation of each agent\u2019s set of DRQNs into a unified DRQN that performs well in all tasks without explicit provision of task ID. Using DRQNs, our approach extends the single-agent, fully-observable MTRL method proposed by Rusu et al. (2015) to Dec-POMDP MT-MARL. Specifically, once Dec-HDRQN specialization is conducted for\n2For notational simplicity, agent superscripts (i) are excluded from local experiences \u3008o(i), a(i), r, o\u2032(i)\u3009 in Eqs. (3) to (6).\neach task, multi-task learning can be treated as a regression problem over Q-values. During multi-task learning, our approach iteratively conducts data collection and regression.\nFor data collection, agents use each specialized DRQN (from Phase I) to execute actions in corresponding tasks, resulting in a set of regression CERTs {MR} (one per task), each containing sequences of regression experiences \u3008o(i)t , Q (i) t \u3009, where Q (i) t = Q (i) t (~ot\n(i); \u03b8(i)) is the specialized DRQN\u2019s Q-value vector for agent i at timestep t. Supervised learning of Q-values is then conducted. Each agent samples experiences from its local regression CERTs to train a single distilled DRQN with parameters \u03b8 (i) R . Given a minibatch of regression experience traces BR = {\u3008\u3008obt0 , Q b t0\u3009, . . . , \u3008o b t0+\u03c4\u22121, Q b t0+\u03c4\u22121\u3009\u3009}b={1,...,B}, the following tempered Kullback-Leibler (KL) divergence loss is minimized for each agent,\nLKL(BR, \u03b8(i)R ;T ) (6) = E(obt ,Qbt)\u223c{MR(i)} |A(i)|\u2211 a=1 softmaxa( Qbt T ) ln softmaxa( Qbt T ) softmaxa(Qbt,R) ,\nwhere Qbt,R = Q b t,R(~ot b; \u03b8 (i) R ) is the vector of actionvalues predicted by distilled DRQN given the same input as the specialized DRQN, T is the softmax temperature, and softmaxa refers to the a-th element of the softmax output. The motivation behind loss function (6) is that low temperatures (0 < T < 1) lead to sharpening of specialized DRQN action-values, Qbt , ensuring that the distilled DRQN ultimately chooses similar actions as the specialized policy it was trained on. We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss. Note that concurrent sampling is not necessary during the distillation phase, as it is entirely supervised; CERTs are merely used for storage of the regression experiences."}, {"heading": "6. Evaluation", "text": ""}, {"heading": "6.1. Task Specialization using Dec-HDRQN", "text": "We first evaluate single-task performance of the introduced Dec-HDRQN approach on a series of increasingly challenging domains. Domains are designed to support a large number of task variations, serving as a useful MT-MARL benchmarking tool. All experiments use DRQNs with 2 multi-layer perceptron (MLP) layers, an LSTM layer (Hochreiter & Schmidhuber, 1997) with 64 memory cells, and another 2 MLP layers. MLPs have 32 hidden units each and rectified linear unit nonlinearities are used throughout, with the exception of the final (linear) layer. Experiments use \u03b3 = 0.95 and Adam optimizer (Kingma & Ba, 2014) with base learning rate 0.001. Dec-HDRQNs use hysteretic learning rate \u03b2 = 0.2 to 0.4. All results are reported for batches of 50 randomly-initialized episodes.\nPerformance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009). Agents i \u2208 {1, . . . , n} in an m \u00d7 m toroidal grid receive +1 terminal reward only when they simultaneously capture moving targets (1 target in MAST, and n targets in MAMT). Each agent always observes its own location, but only sometimes observes targets\u2019 locations. Target dynamics are unknown to agents and vary across tasks. Similar to the Pong POMDP domain of Hausknecht & Stone (2015), our domains include observation flickering: in each timestep, observations of targets are sometimes obscured, with probability Pf . In MAMT, each agent is assigned a unique target to capture, yet is unaware of the assignment (which also varies across tasks). Agent/target locations are randomly initialized in each episode. Actions are \u2018move north\u2019,\n\u2018south\u2019, \u2018east\u2019, \u2018west\u2019, and \u2018wait\u2019, but transitions are noisy (0.1 probability of moving to an unintended adjacent cell).\nIn the MAST domain, each task is specified by a unique grid size m; in MAMT, each task also has a unique agenttarget assignment. The challenge is that agents must learn particular roles (to ensure coordination) and also discern aliased states (to ensure quick capture of targets) using local noisy observations. Tasks end after H timesteps, or upon simultaneous target capture. Cardinality of local policy space for agent i at timestep t is O(|A(i)| |\u2126(i)|t\u22121 |\u2126(i)|\u22121 ) (Oliehoek et al., 2008), where |A(i)| = 5, |\u2126(i)| = m4 for MAST, and |\u2126(i)| = m2(n+1) for MAMT. Across all tasks, non-zero reward signals are extremely sparse, appearing in the terminal experience tuple only if targets are simultaneously captured. Readers are referred to the supplementary material for domain visualizations.\nThe failure point of Dec-DRQN is first compared to DecHDRQN in the MAST domain with n = 2 and Pf = 0 (full observability) for increasing task size, starting from 4\u00d7 4. Despite the domain simplicity, Dec-DRQN fails to match Dec-HDRQN at the 8\u00d78 mark, receiving value 0.05\u00b10.16 in contrast to Dec-HDRQN\u2019s 0.76\u00b10.11 (full results reported in supplementary material). Experiments are then scaled up to a 2 agent, 2 target MAMT domain with Pf = 0.3. Empirical returns throughout training are shown in Fig. 2. In the MAMT tasks, a well-coordinated policy induces agents to capture targets simultaneously (yielding joint +1 reward). If any agent strays from this strategy during learning (e.g., while exploring), teammates receive no reward even while executing optimal local policies, leading them to deviate from learned strategies. Due to lack of robustness against alter-exploration/non-stationarity, the Dec-DRQN becomes unstable in the 5 \u00d7 5 task, and fails to learn altogether in the 6 \u00d7 6 and 7 \u00d7 7 tasks (Fig. 2a). Hysteresis affords Dec-HDRQN policies the stability necessary to consistently achieve agent coordination (Fig. 2b). A centralized-learning variation of Dec-DRQN with interagent parameter sharing (similar to RIAL-PS in Foerster et al. (2016)) was also tested, but was not found to improve performance (see supplementary material). These re-\nsults further validate that, despite its simplicity, hysteretic learning significantly improves the stability of MARL in cooperative settings. Experiments are also conducted for the n = 3 MAMT domain (Fig. 3). This domain poses significant challenges due to reward sparsity. Even in the 4 \u00d7 4 task, only 0.02% of the joint state space has a nonzero reward signal. Dec-DRQN fails to find a coordinated joint policy, receiving near-zero return after training. DecHDRQN successfully coordinates the 3 agents. Note the high variance in empirical return for the 3 \u00d7 3 task is due to flickering probability being increased to Pf = 0.6.\nSensitivity of Dec-HDRQN empirical performance to hysteretic learning rate \u03b2 is shown in Fig. 4, where lower \u03b2 corresponds to higher optimism; \u03b2 = 0 causes monotonic increase of approximated Q-values during learning, whereas \u03b2 = 1 corresponds to Dec-DRQN. Due to the optimistic assumption, anticipated returns at the initial timestep, Q(o0, a0), overestimate true empirical return. Despite this, \u03b2 \u2208 [0.1, 0.6] consistently enables learning of a well-coordinated policy, with \u03b2 \u2208 [0.4, 0.5] achieving best performance. Readers are referred to the supplementary material for additional sensitivity analysis of convergence trends with varying \u03b2 and CERT tracelength \u03c4 ."}, {"heading": "6.2. Multi-tasking using Distilled Dec-HDRQN", "text": "We now evaluate distillation of specialized Dec-HDRQN policies (as learned in Section 6.1) for MT-MARL. A first approach is to forgo specialization and directly learn a Dec-HDRQN using a pool of experiences from all tasks. This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings. In Fig. 5, we compare these approaches (where we label ours as \u2018Distilled\u2019, and MultiHDRQN as \u2018Multi\u2019). Both approaches were trained to perform multi-tasking on 2-agent MAMT tasks ranging from 3\u00d73 to 6\u00d76, with Pf = 0.3. Our distillation approach uses no task-specific MLP layers, unlike Rusu et al. (2015), due to our stronger assumptions on task relatedness and lack of\nexecution-time observability of task identity.\nIn Fig. 5, our MT-MARL approach first performs DecHDRQN specialization training on each task for 70K epochs, and then performs distillation for 100K epochs. A grid search was conducted for temperature hyperparameter in Eq. (6) (T = 0.01 was found suitable). Note that performance is plotted only for the 4\u00d7 4 and 6\u00d7 6 tasks, simply for plot clarity (see supplementary material for MT-MARL evaluation results on all tasks). Multi-HDRQN exhibits poor performance across all tasks due to the complexity involved in concurrently learning over multiple DecPOMDPs (with partial observability, transition noise, nonstationarity, varying domain sizes, varying target dynamics, and random initializations). We experimented with larger and smaller network sizes for Multi-HDRQN, with no major difference in performance (we also include training results for 500K Multi-HDRQN iterations in the supplementary). By contrast, our proposed MT-MARL approach achieves near-nominal execution-time performance on all tasks using a single distilled policy for each agent \u2013 despite not explicitly being provided the task identity."}, {"heading": "7. Contribution", "text": "This paper introduced the first formulation and approach for multi-task multi-agent reinforcement learning under partial observability. Our approach combines hysteretic learners, DRQNs, CERTs, and distillation, demonstrably achieving multi-agent coordination using a single joint policy in a set of Dec-POMDP tasks with sparse rewards, despite not being provided task identities during execution. The parametric nature of the capture tasks used for evaluation (e.g., variations in grid size, target assignments and dynamics, sensor failure probabilities) makes them good candidates for ongoing benchmarks of multi-agent multitask learning. Future work will investigate incorporation of skills (macro-actions) into the framework, extension to domains with heterogeneous agents, and evaluation on more complex domains with much larger numbers of tasks."}, {"heading": "Acknowledgements", "text": "The authors thank the anonymous reviewers for their insightful feedback and suggestions. This work was supported by Boeing Research & Technology, ONR MURI Grant N000141110688 and BRC Grant N000141712072."}, {"heading": "A. Multi-agent Multi-target (MAMT) Domain Overview", "text": ""}, {"heading": "B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain", "text": "Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation flickering disabled). These results mainly illustrate that Dec-DRQN sometimes has some empirical success in low-noise domains with small state-space. Note that in the 8\u00d78 task, Dec-HDRQN significantly outperforms Dec-DRQN, which converges to a sub-optimal policy despite domain simplicity."}, {"heading": "C. Empirical Results: Learning on MAMT Domain", "text": "Multi-agent Single-target (MAMT) domain results, with 2 agents and Pf = 0.3 (observation flickering disabled). We also evaluated performance of inter-agent parameter sharing (a centralized approach) in Dec-DRQN (which we called Dec-DRQN-PS). Additionally, performance of a Double-DQN was deemed to have negligible impacts (Dec-DDRQN).\n0.0 10K 20K 30K 40K\nTraining Epoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE m\np ir\nic a l\nR e tu\nrn\n3\u00d7 3 (Dec-DRQN) 4\u00d7 4 (Dec-DRQN) 3\u00d7 3 (Dec-HDRQN) 4\u00d7 4 (Dec-HDRQN)\n(a) Empirical returns during training. For batch of 50 randomlyinitialized games.\n0.0 10K 20K 30K 40K\nTraining Epoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nQ (s\n0 , a\n0 ) 3\u00d7 3 (Dec-DRQN)\n4\u00d7 4 (Dec-DRQN) 3\u00d7 3 (Dec-HDRQN) 4\u00d7 4 (Dec-HDRQN)\n(b) Anticipated values during training. For specific starting states and actions undertaken in the same 50 randomly-initialized games as Fig. 10a.\nFigure 10. MAMT domain results for Dec-DRQN and Dec-HDRQN, with n = 3 agents. Pf = 0.6 for the 3\u00d7 3 task, and Pf = 0.1 for the 4\u00d7 4 task."}, {"heading": "E. Empirical Results: Learning Sensitivity to Dec-HDRQN Recurrent Training Tracelength Parameter \u03c4", "text": ""}, {"heading": "D. Empirical Results: Learning Sensitivity to Dec-HDRQN Negative Learning Rate \u03b2", "text": ""}, {"heading": "F. Empirical Results: Multi-tasking Performance Comparison", "text": "The below plots show multi-tasking performance of both the distillation and Multi-HDRQN approaches. Both approaches were trained on the 3 \u00d7 3 through 6 \u00d7 6 MAMT tasks. Multi-DRQN failed to achieve specialized-level performance on all tasks, despite 500K training epochs. By contrast, the proposed MT-MARL distillation approach achieves nominal performance after 100K epochs."}], "references": [{"title": "Incremental policy generation for finitehorizon DEC-POMDPs", "author": ["Amato", "Christopher", "Dibangoye", "Jilles Steeve", "Zilberstein", "Shlomo"], "venue": "In ICAPS,", "citeRegEx": "Amato et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2009}, {"title": "Sample bounded distributed reinforcement learning for decentralized POMDPs", "author": ["Banerjee", "Bikramjit", "Lyle", "Jeremy", "Kraemer", "Landon", "Yellamraju", "Rajesh"], "venue": "In AAAI,", "citeRegEx": "Banerjee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2012}, {"title": "A robust approach for multi-agent natural resource allocation based on stochastic optimization algorithms", "author": ["Barbalios", "Nikos", "Tzionas", "Panagiotis"], "venue": "Applied Soft Computing,", "citeRegEx": "Barbalios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barbalios et al\\.", "year": 2014}, {"title": "Practical recommendations for gradientbased training of deep architectures", "author": ["Bengio", "Yoshua"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "Bengio and Yoshua.,? \\Q2012\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2012}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["Bernstein", "Daniel S", "Givan", "Robert", "Immerman", "Neil", "Zilberstein", "Shlomo"], "venue": "Mathematics of operations research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Multiagent learning using a variable learning rate", "author": ["Bowling", "Michael", "Veloso", "Manuela"], "venue": "Artificial Intelligence,", "citeRegEx": "Bowling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2002}, {"title": "Sample complexity of multi-task reinforcement learning", "author": ["Brunskill", "Emma", "Li", "Lihong"], "venue": "arXiv preprint arXiv:1309.6821,", "citeRegEx": "Brunskill et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Brunskill et al\\.", "year": 2013}, {"title": "Multi-agent reinforcement learning: An overview", "author": ["Bu\u015foniu", "Lucian", "Babu\u0161ka", "Robert", "De Schutter", "Bart"], "venue": "In Innovations in multi-agent systems and applications-1,", "citeRegEx": "Bu\u015foniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bu\u015foniu et al\\.", "year": 2010}, {"title": "Multitask learning. In Learning to learn, pp. 95\u2013133", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1998\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1998}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["Claus", "Caroline", "Boutilier", "Craig"], "venue": "AAAI/IAAI, 1998:746\u2013752,", "citeRegEx": "Claus et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Claus et al\\.", "year": 1998}, {"title": "Multi-agent systems by incremental gradient reinforcement learning", "author": ["Dutech", "Alain", "Buffet", "Olivier", "Charpillet", "Fran\u00e7ois"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Dutech et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dutech et al\\.", "year": 2001}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["Fern\u00e1ndez", "Fernando", "Veloso", "Manuela"], "venue": "In Proc. of the fifth international joint conf. on Autonomous agents and multiagent sys.,", "citeRegEx": "Fern\u00e1ndez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2006}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Foerster", "Jakob", "Nardelli", "Nantas", "Farquhar", "Gregory", "Torr", "Philip", "Kohli", "Pushmeet", "Whiteson", "Shimon"], "venue": "arXiv preprint arXiv:1702.08887,", "citeRegEx": "Foerster et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2017}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks", "author": ["Foerster", "Jakob N", "Assael", "Yannis M", "de Freitas", "Nando", "Whiteson", "Shimon"], "venue": "arXiv preprint arXiv:1602.02672,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Predicting and preventing coordination problems in cooperative Q-learning systems", "author": ["Fulda", "Nancy", "Ventura", "Dan"], "venue": "In IJCAI,", "citeRegEx": "Fulda et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fulda et al\\.", "year": 2007}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey J"], "venue": "In Proc. of the twelfth international conf. on machine learning,", "citeRegEx": "Gordon and J.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and J.", "year": 1995}, {"title": "Deep recurrent Qlearning for partially observable MDPs", "author": ["Hausknecht", "Matthew", "Stone", "Peter"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural comp.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning of coordination in cooperative multi-agent systems", "author": ["Kapetanakis", "Spiros", "Kudenko", "Daniel"], "venue": null, "citeRegEx": "Kapetanakis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kapetanakis et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "An algorithm for distributed reinforcement learning in cooperative multiagent systems", "author": ["Lauer", "Martin", "Riedmiller"], "venue": "In Proc. of the Seventeenth International Conf. on Machine Learning. Citeseer,", "citeRegEx": "Lauer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lauer et al\\.", "year": 2000}, {"title": "The world of independent learners is not markovian", "author": ["Laurent", "Guillaume J", "Matignon", "La\u00ebtitia", "Fort-Piat", "Le"], "venue": "International Journal of Knowledge-based and Intelligent Engineering Systems,", "citeRegEx": "Laurent et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laurent et al\\.", "year": 2011}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "Long-Ji"], "venue": "Machine learning,", "citeRegEx": "Lin and Long.Ji.,? \\Q1992\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1992}, {"title": "Stick-breaking policy learning in Dec-POMDPs", "author": ["Liu", "Miao", "Amato", "Christopher", "Liao", "Xuejun", "Carin", "Lawrence", "How", "Jonathan P"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning for decentralized control of multiagent systems in large partially observable stochastic environments", "author": ["Liu", "Miao", "Amato", "Christopher", "Anesta", "Emily", "Griffith", "J. Daniel", "How", "Jonathan P"], "venue": "In AAAI,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Hysteretic Q-learning: an algorithm for decentralized reinforcement learning in cooperative multiagent teams", "author": ["Matignon", "La\u00ebtitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "In IROS,", "citeRegEx": "Matignon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2007}, {"title": "Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems", "author": ["Matignon", "Laetitia", "Laurent", "Guillaume J", "Le Fort-Piat", "Nadine"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "Matignon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matignon et al\\.", "year": 2012}, {"title": "A Concise Introduction to Decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Amato", "Christopher"], "venue": null, "citeRegEx": "Oliehoek et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2016}, {"title": "Optimal and approximate q-value functions for decentralized POMDPs", "author": ["Oliehoek", "Frans A", "Spaan", "Matthijs TJ", "Vlassis", "Nikos A"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Oliehoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oliehoek et al\\.", "year": 2008}, {"title": "A survey on transfer learning", "author": ["Pan", "Sinno Jialin", "Yang", "Qiang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Learning to cooperate via policy search", "author": ["Peshkin", "Leonid", "Kim", "Kee-Eung", "Meuleau", "Nicolas", "Kaelbling", "Leslie Pack"], "venue": "In Proc. of the Sixteenth conf. on Uncertainty in artificial intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Policy distillation", "author": ["Rusu", "Andrei A", "Colmenarejo", "Sergio Gomez", "Gulcehre", "Caglar", "Desjardins", "Guillaume", "Kirkpatrick", "James", "Pascanu", "Razvan", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Tan", "Ming"], "venue": "In Proc. of the tenth international conf. on machine learning,", "citeRegEx": "Tan and Ming.,? \\Q1993\\E", "shortCiteRegEx": "Tan and Ming.", "year": 1993}, {"title": "Multitask reinforcement learning on the distribution of mdps", "author": ["Tanaka", "Fumihide", "Yamamura", "Masayuki"], "venue": "In Computational Intelligence in Robotics and Automation,", "citeRegEx": "Tanaka et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tanaka et al\\.", "year": 2003}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Taylor", "Matthew E", "Stone", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Transfer learning. Handbook of Research on Machine Learning Applications and Trends: Algs", "author": ["Torrey", "Lisa", "Shavlik", "Jude"], "venue": "Methods, and Techniques,", "citeRegEx": "Torrey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Torrey et al\\.", "year": 2009}, {"title": "Solving deep memory POMDPs with recurrent policy gradients", "author": ["Wierstra", "Daan", "Foerster", "Alexander", "Peters", "Jan", "Schmidhuber", "Juergen"], "venue": "In International Conf. on Artificial Neural Networks,", "citeRegEx": "Wierstra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2007}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["Wilson", "Aaron", "Fern", "Alan", "Ray", "Soumya", "Tadepalli", "Prasad"], "venue": "In Proc. of the 24th international conf. on Machine learning,", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Learning and transferring roles in multi-agent reinforcement", "author": ["Wilson", "Aaron", "Fern", "Alan", "Ray", "Soumya", "Tadepalli", "Prasad"], "venue": "In Proc. AAAI-08 Workshop on Transfer Learning for Complex Tasks,", "citeRegEx": "Wilson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2008}, {"title": "Rollout sampling policy iteration for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Chen", "Xiaoping"], "venue": "arXiv preprint arXiv:1203.3528,", "citeRegEx": "Wu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2012}, {"title": "Monte-carlo expectation maximization for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Jennings", "Nicholas R"], "venue": "In Proc. of the International Joint Conf. on Artificial Intelligence,", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Multiagent-based reinforcement learning for optimal reactive power dispatch", "author": ["Xu", "Yinliang", "Zhang", "Wei", "Liu", "Wenxin", "Ferrese", "Frank"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Each MTMARL task is formalized as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002), a general formulation for cooperative decision-making under uncertainty.", "startOffset": 107, "endOffset": 131}, {"referenceID": 33, "context": "Using CERTs and optimistic learners, well-performing distilled policies (Rusu et al., 2015) are learned for multi-agent domains.", "startOffset": 72, "endOffset": 91}, {"referenceID": 19, "context": "Such domains are formalized as Partially Observable Markov Decision Processes (POMDPs), defined as \u3008S,A, T ,R,\u03a9,O, \u03b3\u3009 (Kaelbling et al., 1998).", "startOffset": 118, "endOffset": 142}, {"referenceID": 39, "context": "As Recurrent Neural Networks (RNNs) inherently maintain an internal state ht to compress input history until timestep t, they have been demonstrated to be effective for learning POMDP policies (Wierstra et al., 2007).", "startOffset": 193, "endOffset": 216}, {"referenceID": 7, "context": "Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Bu\u015foniu et al., 2010).", "startOffset": 126, "endOffset": 148}, {"referenceID": 7, "context": "Multi-agent RL (MARL) involves a set of agents in a shared environment, which must learn to maximize their individual returns (Bu\u015foniu et al., 2010). Our work focuses on cooperative settings, where agents share a joint return. Claus & Boutilier (1998) dichotomize MARL agents into two classes: Joint Action Learners (JALs) and Independent Learners (ILs).", "startOffset": 127, "endOffset": 252}, {"referenceID": 7, "context": "Unique challenges arise in MARL due to agent interactions during learning (Bu\u015foniu et al., 2010; Matignon et al., 2012).", "startOffset": 74, "endOffset": 119}, {"referenceID": 28, "context": "Unique challenges arise in MARL due to agent interactions during learning (Bu\u015foniu et al., 2010; Matignon et al., 2012).", "startOffset": 74, "endOffset": 119}, {"referenceID": 28, "context": "Another desired characteristic is robustness to alter-exploration, or drastic changes in policies due to exploratory actions of teammates (Matignon et al., 2012).", "startOffset": 138, "endOffset": 161}, {"referenceID": 40, "context": "MTRL is most beneficial when target tasks share common features (Wilson et al., 2007), and most challenging when the task ID is not explicitly specified to agents during execution \u2013 the setting addressed in this paper.", "startOffset": 64, "endOffset": 85}, {"referenceID": 32, "context": "Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 10, "context": "Works include model-free gradient-ascent based methods (Peshkin et al., 2000; Dutech et al., 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al.", "startOffset": 55, "endOffset": 98}, {"referenceID": 42, "context": ", 2001), simulator-supported methods to improve policies using a series of linear programs (Wu et al., 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al.", "startOffset": 91, "endOffset": 108}, {"referenceID": 1, "context": ", 2012), and modelbased approaches where agents learn in an interleaved fashion to reduce destabilization caused by concurrent learning (Banerjee et al., 2012).", "startOffset": 136, "endOffset": 159}, {"referenceID": 43, "context": "Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).", "startOffset": 101, "endOffset": 142}, {"referenceID": 25, "context": "Recent scalable methods use Expectation Maximization to learn finite state controller (FSC) policies (Wu et al., 2013; Liu et al., 2015; 2016).", "startOffset": 101, "endOffset": 142}, {"referenceID": 28, "context": "This simple approach has some empirical success (Matignon et al., 2012).", "startOffset": 48, "endOffset": 71}, {"referenceID": 27, "context": "Hysteretic Q-learning (Matignon et al., 2007) addresses miscoordination using cautious optimism to stabilize policies while teammates explore.", "startOffset": 22, "endOffset": 45}, {"referenceID": 44, "context": "Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach.", "startOffset": 62, "endOffset": 129}, {"referenceID": 28, "context": "Its track record of empirical success against complex methods (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014) leads us to use it as a foundation for our MT-MARL approach.", "startOffset": 62, "endOffset": 129}, {"referenceID": 25, "context": "Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning.", "startOffset": 0, "endOffset": 23}, {"referenceID": 25, "context": "Matignon et al. (2012) survey these approaches, the most straightforward being Decentralized Qlearning (Tan, 1993), where each agent performs independent Q-learning. This simple approach has some empirical success (Matignon et al., 2012). Distributed Q-learning (Lauer & Riedmiller, 2000) is an optimal algorithm for deterministic domains; it updates Q-values only when they are guaranteed to increase, and the policy only for actions that are no longer greedy with respect to Q-values. Bowling & Veloso (2002) conduct Policy Hill Climbing using the Win-or-Learn Fast heuristic to decrease (increase) each agent\u2019s learning rate when it performs well (poorly).", "startOffset": 0, "endOffset": 511}, {"referenceID": 12, "context": "Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Foerster et al. (2016) present architectures to learn communication protocols for DecPOMDP RL, noting best performance using a centralized approach with inter-agent backpropagation and parameter sharing. They also evaluate a model combining Decentralized Q-learning with DRQNs, which they call Reinforced Inter-Agent Learning. Given the decentralized nature of this latter model (called Dec-DRQN herein for clarity), we evaluate our method against it. Concurrent to our work, Foerster et al. (2017) investigated an alternate means of stabilizing experience replay for the centralized learning case.", "startOffset": 0, "endOffset": 499}, {"referenceID": 41, "context": "They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008).", "startOffset": 80, "endOffset": 101}, {"referenceID": 17, "context": "Recent work extends the notion of neural network distillation (Hinton et al., 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 33, "context": ", 2015) to DQNs for single-agent, fully-observable MTRL, first learning a set of specialized teacher DQNs, then distilling teachers to a single multi-task network (Rusu et al., 2015).", "startOffset": 163, "endOffset": 182}, {"referenceID": 37, "context": "Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP.", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": "Wilson et al. (2007) consider TL for MDPs, learning a Dirichlet Process Mixture Model over source MDPs, used as an informative prior for a target MDP. They extend the work to multi-agent MDPs by learning characteristic agent roles (Wilson et al., 2008). Brunskill & Li (2013) introduce an MDP clustering approach that reduces negative transfer in MTRL, and prove reduction of sample complexity of exploration using transfer.", "startOffset": 0, "endOffset": 276}, {"referenceID": 35, "context": "Taylor et al. (2013) introduce parallel transfer to accelerate multi-agent learning using interagent transfer.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "\u00d7iA is the joint action space, and \u03a9 = \u00d7i\u03a9 is the joint observation space (Bernstein et al., 2002).", "startOffset": 74, "endOffset": 98}, {"referenceID": 30, "context": "For simplicity, we consider only pure joint policies, as finite-horizon Dec-POMDPs have at least one pure joint optimal policy (Oliehoek et al., 2008).", "startOffset": 127, "endOffset": 150}, {"referenceID": 1, "context": "This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).", "startOffset": 54, "endOffset": 99}, {"referenceID": 32, "context": "This assumption is consistent with prior work in MARL (Banerjee et al., 2012; Peshkin et al., 2000).", "startOffset": 54, "endOffset": 99}, {"referenceID": 23, "context": "However, the domain appears non-stationary from the perspective of each Dec-POMDP agent, a property we formalize by extending the definition by Laurent et al. (2011). Definition 1.", "startOffset": 144, "endOffset": 166}, {"referenceID": 4, "context": "As Dec-POMDP RL is notoriously complex (and solving for the optimal policy is NEXP-complete even with a known model (Bernstein et al., 2002)), we first introduce an approach for stable single-task MARL.", "startOffset": 116, "endOffset": 140}, {"referenceID": 1, "context": "Due to partial observability and local non-stationarity, model-based Dec-POMDP MARL is extremely challenging (Banerjee et al., 2012).", "startOffset": 109, "endOffset": 132}, {"referenceID": 27, "context": "Hysteretic Qlearning (Matignon et al., 2007), instead, uses the insight that low returns may also be caused by domain stochasticity, which should not be ignored.", "startOffset": 21, "endOffset": 44}, {"referenceID": 44, "context": "Hysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches.", "startOffset": 91, "endOffset": 158}, {"referenceID": 28, "context": "Hysteretic Q-learning has enjoyed a strong empirical track record in fully-observable MARL (Xu et al., 2012; Matignon et al., 2012; Barbalios & Tzionas, 2014), exhibiting similar performance as more complex approaches.", "startOffset": 91, "endOffset": 158}, {"referenceID": 13, "context": "Despite the benefits in single-agent settings, existing MARL approaches have found it necessary to disable experience replay (Foerster et al., 2016).", "startOffset": 125, "endOffset": 148}, {"referenceID": 33, "context": "Using DRQNs, our approach extends the single-agent, fully-observable MTRL method proposed by Rusu et al. (2015) to Dec-POMDP MT-MARL.", "startOffset": 93, "endOffset": 112}, {"referenceID": 33, "context": "We refer readers to Rusu et al. (2015) for additional analysis of the distillation loss.", "startOffset": 20, "endOffset": 39}, {"referenceID": 0, "context": "Performance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009).", "startOffset": 183, "endOffset": 203}, {"referenceID": 0, "context": "Performance is evaluated on both multi-agent single-target (MAST) and multi-agent multi-target (MAMT) capture domains, variations of the existing meeting-in-a-grid DecPOMDP benchmark (Amato et al., 2009). Agents i \u2208 {1, . . . , n} in an m \u00d7 m toroidal grid receive +1 terminal reward only when they simultaneously capture moving targets (1 target in MAST, and n targets in MAMT). Each agent always observes its own location, but only sometimes observes targets\u2019 locations. Target dynamics are unknown to agents and vary across tasks. Similar to the Pong POMDP domain of Hausknecht & Stone (2015), our domains include observation flickering: in each timestep, observations of targets are sometimes obscured, with probability Pf .", "startOffset": 184, "endOffset": 596}, {"referenceID": 30, "context": "icy space for agent i at timestep t is O(|A| |\u03a9(i)|t\u22121 |\u03a9(i)|\u22121 ) (Oliehoek et al., 2008), where |A| = 5, |\u03a9| = m for MAST, and |\u03a9| = m for MAMT.", "startOffset": 66, "endOffset": 89}, {"referenceID": 12, "context": "A centralized-learning variation of Dec-DRQN with interagent parameter sharing (similar to RIAL-PS in Foerster et al. (2016)) was also tested, but was not found to improve performance (see supplementary material).", "startOffset": 102, "endOffset": 125}, {"referenceID": 33, "context": "This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings.", "startOffset": 35, "endOffset": 54}, {"referenceID": 33, "context": "This approach, called Multi-DQN by Rusu et al. (2015), is susceptible to convergence issues even in single-agent, fully-observable settings. In Fig. 5, we compare these approaches (where we label ours as \u2018Distilled\u2019, and MultiHDRQN as \u2018Multi\u2019). Both approaches were trained to perform multi-tasking on 2-agent MAMT tasks ranging from 3\u00d73 to 6\u00d76, with Pf = 0.3. Our distillation approach uses no task-specific MLP layers, unlike Rusu et al. (2015), due to our stronger assumptions on task relatedness and lack of execution-time observability of task identity.", "startOffset": 35, "endOffset": 447}], "year": 2017, "abstractText": "Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrentlyexploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.", "creator": "LaTeX with hyperref package"}}}