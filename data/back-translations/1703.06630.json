{"id": "1703.06630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process", "abstract": "The number of documents available on the Internet is increasing with each passing day. Therefore, effective and meaningful processing of this amount of information is becoming a major concern for companies and scientists. Methods that represent a text document through a topic representation are widely used in the Information Retrieval (IR) for processing big data such as Wikipedia articles. One of the biggest difficulties in using topic models to capture huge amounts of data is related to the material resources (CPU time and memory) needed for model estimation. To address this problem, we propose to build topic spaces from combined documents. In this paper, we present a study on topic space representation in the context of big data. The behavior of topic space representation is analyzed in different languages. Experiments show that topic spaces estimated from text collections are just as relevant as those estimated from the complete documents. The real benefit of such an approach is the gain in time during document processing: We have shown that the processing time can be reduced by 60% (by reducing the processing time in general).", "histories": [["v1", "Mon, 20 Mar 2017 08:19:43 GMT  (547kb,D)", "http://arxiv.org/abs/1703.06630v1", "16 pages, 4 tables, 8 figures"]], "COMMENTS": "16 pages, 4 tables, 8 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["mohamed morchid", "juan-manuel torres-moreno", "richard dufour", "javier ram\\'irez-rodr\\'iguez", "georges linar\\`es"], "accepted": false, "id": "1703.06630"}, "pdf": {"name": "1703.06630.pdf", "metadata": {"source": "CRF", "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process", "authors": ["Mohamed Morchid", "Juan-Manuel Torres-Moreno", "Richard Dufour", "Javier Ram\u0131\u0301rez-Rod\u0155\u0131guez", "Georges Linar\u00e8s"], "emails": ["firstname.lastname@univ-avignon.fr", "jararo@azc.uam.mx"], "sections": [{"heading": "1 Introduction", "text": "The number of documents available into Internet moves each day up in an exponential way. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. An important part of the information is conveyed through textual documents such as blogs or micro-blogs, general or advertise websites, and encyclopedic documents. This last type of textual data increases each day with new articles, which convey large and heterogenous information. The most famous and used collaborative Internet encyclopedia is Wikipedia, enriched by worldwide volunteers. It is the 12th most visited website in the USA, with around 10.65 million users visiting\n1 Preprint of International Journal of Computational Linguistics and Applications, 7(2):87-109, 2016.\nar X\niv :1\n70 3.\n06 63\n0v 1\n[ cs\n.I R\n] 2\n0 M\nthe site daily, and a total reaching 39 millions of the estimated 173.3 million Internet users in the USA2 3.\nThe massive number of documents provided by Wikipedia is mainly exploited by Natural Language Processing (NLP) scientists in various tasks such as keyword extraction, document clustering, automatic text summarization. . . Different classical representations of a document, such as term-frequency based representation [1], have been proposed to extract word-level information from this large amount of data in a limited time. Nonetheless, these straightforward representations obtain poor results in many NLP tasks with respect to more abstract and complex representations. Indeed, the classical term-frequency representation reveals little in way of intra- or inter-document statistical structure, and does not allow us to capture possible and unpredictable context dependencies. For these reasons, more abstract representations based on latent topics have been proposed. The most known and used one is the latent Dirichlet allocation (LDA) [2] approach which outperforms classical methods in many NLP tasks. The main drawback of this topic-based representation is the time needed to learn LDA latent variables. This massive waste of time that occurs during the LDA learning process, is mainly due to the documents size along with the number of documents, which is highly visible in the context of big data such as Wikipedia.\nThe solution proposed in this article is to summarize documents contained into a big data corpus (here Wikipedia) and then, learn a LDA topic space. This should answer the these three raised difficulties:\n\u2022 reducing the processing time during the LDA learning process, \u2022 retaining the intelligibility of documents, \u2022 maintaining the quality of LDA models.\nWith this summarization approach, the size of documents will be drastically reduced, the intelligibility of documents will be preserved, and we make the assumption that the LDA model quality will be conserved. Moreover, for all these reasons, the classical term-frequency document reduction is not considered in this paper. Indeed, this extraction of a subset of words to represent the document content allows us to reduce the document size, but does not keep the document structure and then, the intelligibility of each document.\nThe main objective of the paper is to compare topic space representations using complete documents and summarized ones. The idea behind is to show the effectiveness of this document representation, in terms of performance and timeprocessing reduction, when summarized documents are used. The topic space representation behavior is analyzed on different languages (English, French and Spanish). In the series of proposed experiments, the topic models built from complete and summarized documents are evaluated using the Jensen-Shannon (JS) divergence measure as well as the perplexity measure. To the best of our knowledge, this is the most extensive set of experiments interpreting the evaluation of topic spaces built from complete and summarized documents without human models. 2 http://www.alexa.com 3 http://www.metrics2.com\nThe rest of the paper is organized in the following way: first, Section 2 introduces related work in the areas of topic modeling and automatic text summarization evaluations. Then, Section 3 describes the proposed approach, including the topic representation adopted in our work and the different summarization systems employed. Section 4 presents the topic space quality measures used for the evaluation. Experiments carried out along with with the results presented in Section 5. A discussion is finally proposed in Section 6 before concluding in Section 7."}, {"heading": "2 Related work", "text": "Several methods were proposed by Information Retrieval (IR) researchers to process large corpus of documents such as Wikipedia encyclopedia. All these methods consider documents as a bag-of-words [1] where the word order is not taken into account.\nAmong the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and documents) to a vector of numeral values represented by the word counts (number of occurrences) in the document named TF-IDF [4]. This approach showed its effectiveness in different tasks, and more precisely in the basic identification of discriminative words for a document [5]. However, this method has many weaknesses such as the small amount of reduction in description length, or the weak of inter- or intra-statistical structure of documents in the text corpus.\nTo substantiate the claims regarding TF-IDF method, IR researchers have proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a singular value decomposition (SVD) to reduce the space dimension.\nThis method was improved by [8] which proposed a Probabilistic LSA (PLSA). PLSA models each word in a document as a sample from a mixture model, where the mixture components are multinomial random variables that can be viewed as representations of topics. This method demonstrated its performance on various tasks, such as sentence [9] or keyword [10] extraction. In spite of the effectiveness of the PLSA approach, this method has two main drawbacks. The distribution of topics in PLSA is indexed by training documents. Thus, the number of its parameters grows with the training document set size and then, the model is prone to overfitting which is a main issue in an IR task such as documents clustering. However, to address this shortcoming, a tempering heuristic is used to smooth the parameter of PLSA models for acceptable predictive performance: the authors in [11] showed that overfitting can occur even if tempering process is used.\nTo overcome these two issues, the latent Dirichlet allocation (LDA) [2] method was proposed. Thus, the number of LDA parameters does not grow with the size of the training corpus and LDA is not candidate for overfitting. Next section describes more precisely the LDA approach that will be used in our experimental study.\nThe authors in [12] evaluated the effectiveness of the Jensen-Shannon (JS) theoretic measure [13] in predicting systems ranks in two summarization tasks: query-focused and update summarization. They have shown that ranks produced by Pyramids and those produced by JS measure correlate. However, they did not investigate the effect of the measure in summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English.\nNext section describes the proposed approach followed in this article, including the topic space representation with the LDA approach and its evaluation with the perplexity and the Jensen-Shannon metrics."}, {"heading": "3 Overview of the proposed approach", "text": "Figure 1 describes the approach proposed in this paper to evaluate the quality of a topic model representation with and without automatic text summarization systems. The latent Dirichlet allocation (LDA) approach, described in details in the next section, is used for topic representation, in conjunction with different state-of-the-art summarization systems presented in Section 3.2."}, {"heading": "3.1 Topic representation: latent Dirichlet allocation", "text": "LDA is a generative model which considers a document, seen as a bag-of-words [1], as a mixture of latent topics. In opposition to a multinomial mixture model, LDA considers that a theme is associated to each occurrence of a word composing the document, rather than associate a topic with the complete document. Thereby, a document can change of topics from a word to another. However, the word occurrences are connected by a latent variable which controls the global respect of the distribution of the topics in the document. These latent topics are characterized by a distribution of word probabilities associated with them. PLSA and LDA models have been shown to generally outperform LSA on IR tasks [14]. Moreover, LDA provides a direct estimate of the relevance of a topic knowing a word set.\nFigure 2 shows the LDA formalism. For every document d of a corpus D, a first parameter \u03b8 is drawn according to a Dirichlet law of parameter \u03b1. A second parameter \u03c6 is drawn according to the same Dirichlet law of parameter \u03b2. Then, to generate every word w of the document c, a latent topic z is drawn from a multinomial distribution on \u03b8. Knowing this topic z, the distribution of the words is a multinomial of parameters \u03c6. The parameter \u03b8 is drawn for all the documents from the same prior parameter \u03b1. This allows to obtain a parameter binding all the documents together [2].\nSeveral techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16]. Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [17] and gives a simple algorithm to approximate inference in high-dimensional models such as LDA [18]. This overcomes the difficulty to directly and exactly estimate parameters that maximize the likelihood of the whole data collection defined as: p(W |\u2212\u2192\u03b1 , \u2212\u2192 \u03b2 ) = \u220fM m=1 p( \u2212\u2192wm|\u2212\u2192\u03b1 , \u2212\u2192 \u03b2 ) for the whole data collection W = {\u2212\u2192wm}Mm=1 knowing the Dirichlet parameters \u2212\u2192\u03b1 and \u2212\u2192 \u03b2 .\nThe first use of Gibbs Sampling for estimating LDA is reported in [16] and a more comprehensive description of this method can be found in [18].\nThe next section describes the income of the LDA technique. The input of the LDA method is an automatic summary of each document of the train corpus. These summaries are built with different systems."}, {"heading": "3.2 Automatic Text Summarization systems", "text": "Various text summarization systems have been proposed over the years [19]. Two baseline systems as well as the ARTEX summarization system, that reaches state-of-the-art performance [20], are presented in this section.\nBaseline first (BF) The Baseline first (or leadbase) selects the n first sentences of the documents, where n is determined by a compression rate. Although very simple, this method is a strong baseline for the performance of any automatic summarization system [21, 22]. This very old and very simple sentence weighting heuristic does not involve any terms at all: it assigns highest weight to the first sentences of the text. Texts of some genres, such as news reports or scientific papers, are specifically designed for this heuristic: e.g., any scientific paper contains a ready summary at the beginning. This gives a baseline [23] that proves to be very hard to beat on such texts. It is worth noting that in Document Understanding Conference (DUC) competitions [23] only five systems performed above this baseline, which does not demerit the other systems because this baseline is genre-specific.\nBaseline random (BR) The Baseline random [21] randomly selects n sentences of the documents, where n is also determined by a compression rate. This method is the classic baseline for measuring the performance of automatic text summarization systems.\nARTEX AnotheR TEXt (ARTEX) algorithm [20] is another simple extractive algorithm. The main idea is to represent the text in a suitable space model (VSM). Then, an average document vector that represents the average (the \u201cglobal topic\u201d) of all sentence vectors is constructed. At the same time, the \u201clexical weight\u201d for each sentence, i.e. the number of words in the sentence, is obtained. After that, the angle between the average document and each sentence is calculated. Narrow angles \u03b1 indicate that the sentences near the \u201cglobal topic\u201d should be important and are therefore extracted. See Figure 3 for the VSM of words: p vector sentences and the average \u201cglobal topic\u201d are represented in a N dimensional space of words. The angle \u03b1 between the sentence \u2212\u2192s\u00b5 and the global topic \u2212\u2192 b is processed as follow:\ncos(\u03b1) = \u2212\u2192 b \u00d7\u2212\u2192s\u00b5 || \u2212\u2192 b ||.||\u2212\u2192s\u00b5||\n(1)\nNext, a weight for each sentence is calculated using their proximity with the \u201cglobal topic\u201d and their \u201clexical weight\u201d. In Figure 4, the \u201clexical weight\u201d is\nVSM of words\nrepresented in a VSM of p sentences. Narrow angles indicate that words closest to the \u201clexical weight\u201d should be important. Finally, the summary is generated concatenating the sentences with the highest scores following their order in the original document. Formally, ARTEX algorithm computes the score of each sentence by calculating the inner product between a sentence vector, an average pseudo-sentence vector (the \u201cglobal topic\u201d) and an average pseudo-word vector(the\u201clexical weight\u201d). Once the pre-processing is complete, a matrix S[pN ] (N words and p sentences) is created. Let \u2212\u2192s\u00b5 = (s\u00b5,1, s\u00b5,2, ..., s\u00b5,N ) be a vector of the sentence \u00b5 = 1, 2, ..., p. The average pseudo-word vector \u2212\u2192a = [a\u00b5] was defined as the average number of occurrences of N words used in the sentence \u2212\u2192s\u00b5:\na\u00b5 = 1\nN \u2211 j s\u00b5,j (2)\nVSM of sentences\nand the average pseudo-sentence vector \u2212\u2192 b = [bj ] as the average number of\noccurrences of each word j used through the p sentences:\nbj = 1\np \u2211 \u00b5 s\u00b5,j (3)\nThe weight of a sentence \u2212\u2192s\u00b5 is calculated as follows:\nw(\u2212\u2192s\u00b5) = (\u2212\u2192s \u00d7 \u2212\u2192 b )\u00d7\u2212\u2192a\n= 1\nNp  N\u2211 j=1 s\u00b5,j \u00d7 bj \u00d7 a\u00b5 ;\u00b5 = 1, 2, . . . , p (4) The w(\u2022) computed by Equation 4 must be normalized between the interval [0, 1]. The calculation of (\u2212\u2192s \u00d7 \u2212\u2192 b ) indicates the proximity between the sentence \u2212\u2192s\u00b5 and the average pseudo-sentence \u2212\u2192 b . The product (\u2212\u2192s \u00d7 \u2212\u2192 b )\u00d7\u2212\u2192a weight this proximity using the average pseudo-word \u2212\u2192a . If a sentence \u2212\u2192s\u00b5 is near \u2212\u2192 b and their corresponding element a\u00b5 has a high value, therefore \u2212\u2192s\u00b5 will have a high score. Moreover, a sentence \u2212\u2192s\u00b5 far from a main topic (i.e. \u2212\u2192s\u00b5 \u00d7 \u2212\u2192 b is near 0) or their corresponding element amu has a low value, (i.e. amu are near 0), therefore \u2212\u2192s\u00b5 will have a low score. It is not really necessary to divide the scalar product by the constant 1Np , because the angle \u03b1 between \u2212\u2192 b and \u2212\u2192s\u00b5 is the same if \u2212\u2192 b = \u2212\u2192 b\u2032 = \u2211 \u00b5 s\u00b5,j . The element a\u00b5 is only a scale factor that does not modify \u03b1 [20]:\nw(\u2212\u2192s\u00b5)\u2217 = 1\u221a N5p3  N\u2211 j=1 s\u00b5,j \u00d7 bj \u00d7 a\u00b5 ;\u00b5 = 1, 2, . . . , p (5) The term 1/ \u221a N5p3 is a constant value, and then w(\u2022) (Equation 4) and w(\u2022)\u2217 (Equation 5) are both equivalent. This summarization system outperforms the CORTEX [24] one with the FRESA [25] measure. ARTEX is evaluated with several corpus such as the Medecina Clinica [20]. ARTEX performance is then better than CORTEX on English, Spanish or French, which are the targeted languages in this study."}, {"heading": "4 Evaluation of LDA model quality", "text": "The previous section described different summarization systems to reduce the size of train corpus and to retain only relevant information contained into the train documents. This section proposes a set of metrics to evaluate the quality of topic spaces generated from summaries of the train documents. The first one is the perplexity. This score is the most popular one. We also propose to study another measure to evaluate the dispersion of each word into a given topic space. This measure is called the Jensen-Shannon (JS) divergence."}, {"heading": "4.1 Perplexity", "text": "Perplexity is a standard measure to evaluate topic spaces, and more generally a probabilistic model. A topic model Z is effective if it can correctly predict an unseen document from the test collection. The perplexity used in language modeling is monotonically decreasing in the likelihood of the test data, and is algebraically equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity score indicates better generalization performance [2]:\nperplexity(B) = exp { \u2212 1 NB M\u2211 d=1 logP (w) } (6)\nwith\nNB = M\u2211 d=1 Nd (7)\nwhere NB is the combined length of all M testing terms and Nd is the number of words in the document d; P (w) is the likelihood that the generative model will be assigned to an unseen word w of a document d in the test collection. The quantity inside the exponent is called the entropy of the test collection. The logarithm enables to interpret the entropy in terms of bits of information.\n4.2 Jensen-Shannon (JS) divergence\nThe perplexity evaluates the performance of a topic space. Another important information is the distribution of words in each topic. The Kullback-Leibler divergence (KL) estimates how much a topic is different from the N topics contained in the topic model. This distribution is defined hereafter:\nKL(zi, zj) = \u2211 w\u2208A pi log pi pj\n(8)\nwhere pi = P (w|zi) and pj = P (w|zj) are the probabilities that the word w is generated by the topic zi or zj . Thus, the symmetric KL divergence is named Jensen-Shannon (JS) divergence metric. It is the mid-point measure between KL(zi, zj) and KL(zj , zi). JS is then defined with equation 8 as the mean of the divergences between (zi, zj) and (zj , zi) as:\nJS(zi, zj) = 1\n2 (KL(zi, zj) +KL(zj , zi))\n= 1\n2 \u2211 w\u2208A ( pi log pi pj + pj log pj pi ) . (9)\nThe JS divergence for the entire topic space is then defined as the divergence between each pair of topics composing the topic model Z, defined in equation 9 as:\nJS(Z) = \u2211 zi\u2208Z \u2211 zj\u2208Z JS(zi, zj)\n= 1\n2 \u2211 zi\u2208Z \u2211 zj\u2208Z \u2211 w\u2208A pi log pi pj + pj log pj pi . (10)\nif i = j \u21d2 log pjpi = 0 (log1 = 0). After defining the metrics to evaluate the quality of the model, the next section describes the experiment data sets and the experimental protocol."}, {"heading": "5 Experiments", "text": "These summarization systems are used to compress and retain only relevant information into train text collection in each language. This section presents the experiments processed to evaluate the relevance and the effectiveness of the proposed system of fast and robust topic space building. First of all, the experimental protocol is presented, and then a qualitative analysis of obtained results is performed using evaluation metrics described in Section 4."}, {"heading": "5.1 Experimental protocol", "text": "In order to train topic spaces, a large corpus of documents is required. Three corpus was used. Each corpus C is in a particular language (English, Spanish and French), and is composed of a training set A and a testing set B. The corpus are composed of articles from Wikipedia. Thus, for each of the three languages, a set of 100,000 documents is collected. 90% of the corpus is summarized and used to build topic spaces, while 10% is used to evaluate each model (no need to be summarized).\nTable 1 shows that the latin languages (French and Spanish) have a similar size (a difference of less than 4% is observed), while the English one is bigger than the others (English text corpus is 1.37 times bigger than French or Spanish corpus). In spite of the size difference of corpus, both of them have more or less the same number of words and sentences in an article. We can also note that the English vocabulary size is roughly the same (15%) than the latin languages. Same observations can be made in Table 2, that presents statistics at document level (mean on the whole corpus). In next section, the outcome of this fact is seen during the perplexity evaluation of topic spaces built from English train text collection.\nAs set of topic spaces is trained to evaluate the perplexity and the JensenShannon (JS) scores for each language, as well as the processing time to summarize and compress documents from the train corpus. Following a classical study of LDA topic spaces quality [26], the number of topics by model is fixed to {5, 10, 50, 100, 200, 400}. These topic spaces are built with the MALLET toolkit [27]."}, {"heading": "5.2 Results", "text": "The experiments conducted in this paper are topic-based concern. Thus, each metric proposed in Section 4 (Perplexity and JS) are applied for each language (English, Spanish and French), for each topic space size ({5, 10, 50, 100, 200, 400}), and finally, for each compression rate during the summarization process (10% to 50% of the original size of the documents). Figures 5 and 6 present results obtained by varying the number of topics (Figure (a) to (c)) and the percentage of summary (Figure 6), respectively for perplexity and Jensen-Shannon (JS) measures. Results are computed with a mean among the various topic spaces size and a mean among the different reduced summaries size. Moreover, each language was study separately to point out differences of topic spaces quality depending on the language."}, {"heading": "6 Discussions", "text": "The results reported in Figures 5 and 6 allow us to point out a first general remark, already observed in section 5.1: the two latin languages have more or less the same tendencies. This should be explained by the root of these languages, which are both latins.\nFigure ?? shows that the Spanish and French corpus obtain a perplexity between 3,000 and 6,100 when the number of classes in the topic space varies. Another observation is that, for these two languages, topic spaces obtained with summarized documents, outperform the ones obtained with complete documents when at least 50 topics are considered (Figures 5-b and -c). The best system for all languages is ordered in the same way. Systems are ordered from the best to the worst in this manner: ARTEX, BF (this fact is explained in the next part and is noted into JS measure curves in Figures 7 and 8), and then BR. If we considerer a number of topics up to 50, we can note that the topic spaces, from full text documents (i.e. not summarized) with an English text corpus, obtain a better perplexity (smaller) than documents processed with a summarization system, that is particularly visible into Figures 6.\nTo address the shortcoming due to the size of the English corpus (1.37 times bigger than latin languages), the number of topics contained into the thematic space has to be increased to effectively disconnect words into topics. In spite of moving up, the number of topics move down the perplexity of topic spaces for all summarization systems (except random baseline (RB)), the perplexity obtained with the English corpus being higher than those obtained from the Spanish and French corpus.\nAmong all summarization systems used to reduce the documents from the train corpus, the baseline first (BF) obtains good results for all languages. This performance is due to the fact that BF selects the first paragraph of the document as a summary: when a Wikipedia content provider writes a new article, he exposes the main idea of the article in the first sentences. Furthermore, the rest of\nthe document relates different aspects of the article subject, such as historical or economical details, which are not useful to compose a relevant summary. Thus, this baseline is quite hard to outperform when the documents to summarize are from encyclopedia such as Wikipedia.\nThe random baseline (RB) composes its summary by randomly selecting a set of sentences in an article. This kind of system is particularly relevant when the main ideas are disseminated in the document such as a blog or a website. This is the main reason why this baseline did not obtain good results except for JS divergence measure (see Figures 7 and 8). This can be explained by the fact that this system selects sentences at different places, and then, selects\na variable set of words. Thus, topic spaces from these documents contain a variable vocabulary. The JS divergence evaluates how much a word contained in a topic is discriminative, and allows to distinguish this topic from the others that compose the thematic representation.\nFigures 7 and 8 also show that Jensen-Shannon (JS) divergence scores between topics obtained a similar performance order of summarization systems for all languages corpus. Moreover, full text documents always outperform all topic spaces representation for all languages and all summary rates. The reason is that full text documents contain a larger vocabulary, and JS divergence is sensitive to the vocabulary size, especially when the number of topics is equal for summarized and full text documents. This observation is pointed out by Figures 8-b and -c where the means among topic spaces for each summary rate of full text documents are beyond all summarization systems. Last points of the curves show that topic spaces, with a high number of topics and estimated from summaries, do not outperform those estimated from full text documents, but become more and more closer to these ones: this confirms the original idea that have motivated this work.\nTables 3 and 4 finally present the processing time, in seconds, by varying the number of topics for each language corpus, respectively with the full text and the summarized documents. We can see that processing time is saved when topic spaces are learned from summarized documents. Moreover, tables show that the processing times follow an exponential curve, especially for the full text context. For this reason, we can easily imagine the processing time that can be saved using summaries instead of the complete documents, which inevitably contain non informative and irrelevant terms.\nA general remark is that the best summarization system is ARTEX, but if we take into account the processing time during the topic space learning, the baseline first (BF) is the best agreement. Indeed, if one want to find a common ground between a low perplexity, a high JS divergence between topics and a fast learning process, the BF method should be chosen."}, {"heading": "7 Conclusions", "text": "In this paper, a qualitative study of the impact of documents summarization in topic space learning is proposed. The basic idea that learning topic spaces from compressed documents is less time consuming than learning topic spaces from the full documents is noted. The main advantage to use the full text document in text corpus to build topic space is to move up the semantic variability into each topic, and then increase the divergence between these ones. Experiments show that topic spaces with enough topics size have more or less (roughly) the same divergence.\nThus, topic spaces with a large number of topics, i.e. suitable knowing the size of the corpus (more than 200 topics in our case), have a lower perplexity, a better divergence between topics and are less time consuming during the LDA learning process. The only drawback of topic spaces learned from text corpus of summarized documents disappear when the number of topics comes up suitable for the size of the corpus whatever the language considered."}], "references": [{"title": "Automatic text processing: the transformation", "author": ["G. Salton"], "venue": "Analysis and Retrieval of Information by Computer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "The Journal of Machine Learning Research 3", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Modern information retrieval", "author": ["R. Baeza-Yates", "B Ribeiro-Neto"], "venue": "Volume 463. ACM press New York", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Introduction to modern information retrieval", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "On the specification of term values in automatic indexing", "author": ["G. Salton", "C.S. Yang"], "venue": "Journal of documentation 29", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science 41", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "A latent semantic analysis framework for large-span language modeling", "author": ["J. Bellegarda"], "venue": "Fifth European Conference on Speech Communication and Technology.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proc. of Uncertainty in Artificial Intelligence, UAI \u2019 99, Citeseer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting latent semantic information in statistical language modeling", "author": ["J. Bellegarda"], "venue": "Proceedings of the IEEE 88", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Keyword extraction using term-domain interdependence for dictation of radio news", "author": ["Y. Suzuki", "F. Fukumoto", "Y. Sekiguchi"], "venue": "17th international conference on Computational linguistics. Volume 2., ACL", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments", "author": ["A. Popescul", "D.M. Pennock", "S. Lawrence"], "venue": "Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, Morgan Kaufmann Publishers Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatically Evaluating Content Selection in Summarization without Human Models", "author": ["A. Louis", "A. Nenkova"], "venue": "Empirical Methods in Natural Language Processing, Singapore", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Divergence Measures based on the Shannon Entropy", "author": ["J. Lin"], "venue": "IEEE Transactions on Information Theory 37", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning 42", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Expectation-propagation for the generative aspect model", "author": ["T. Minka", "J. Lafferty"], "venue": "Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, Morgan Kaufmann Publishers Inc.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National academy of Sciences of the United States of America 101", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1984}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Web: http://www. arbylon. net/publications/text-est. pdf", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic Text Summarization", "author": ["J.M. Torres-Moreno"], "venue": "Wiley and Sons", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Artex is another text summarizer", "author": ["J.M. Torres-Moreno"], "venue": "arxiv:1210.3312 [cs.ir]", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Terms derived from frequent sequences for extractive text summarization", "author": ["Y. Ledeneva", "A. Gelbukh", "R.A. Gar\u0107\u0131a-Hern\u00e1ndez"], "venue": "Computational Linguistics and Intelligent Text Processing. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press, Cambridge, Massachusetts", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Cortex : un algorithme pour la condensation automatique des textes", "author": ["J.M. Torres-Moreno", "P. Vel\u00e1zquez-Morales", "J.G. Meunier"], "venue": "ARCo\u201901. Volume 2., Lyon, France", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Summary evaluation with and without references", "author": ["J.M. Torres-Moreno", "H. Saggion", "Cunha", "I.d.", "E. SanJuan", "P. Vel\u00e1zquez-Morales"], "venue": "Polibits", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence, AUAI Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Mallet: A machine learning for language toolkit", "author": ["A.K. McCallum"], "venue": "http://mallet.cs.umass.edu", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Different classical representations of a document, such as term-frequency based representation [1], have been proposed to extract word-level information from this large amount of data in a limited time.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "The most known and used one is the latent Dirichlet allocation (LDA) [2] approach which outperforms classical methods in many NLP tasks.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "All these methods consider documents as a bag-of-words [1] where the word order is not taken into account.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Among the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and documents) to a vector of numeral values represented by the word counts (number of occurrences) in the document named TF-IDF [4].", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Among the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and documents) to a vector of numeral values represented by the word counts (number of occurrences) in the document named TF-IDF [4].", "startOffset": 234, "endOffset": 237}, {"referenceID": 4, "context": "This approach showed its effectiveness in different tasks, and more precisely in the basic identification of discriminative words for a document [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "To substantiate the claims regarding TF-IDF method, IR researchers have proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a singular value decomposition (SVD) to reduce the space dimension.", "startOffset": 160, "endOffset": 166}, {"referenceID": 6, "context": "To substantiate the claims regarding TF-IDF method, IR researchers have proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a singular value decomposition (SVD) to reduce the space dimension.", "startOffset": 160, "endOffset": 166}, {"referenceID": 7, "context": "This method was improved by [8] which proposed a Probabilistic LSA (PLSA).", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "This method demonstrated its performance on various tasks, such as sentence [9] or keyword [10] extraction.", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "This method demonstrated its performance on various tasks, such as sentence [9] or keyword [10] extraction.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "However, to address this shortcoming, a tempering heuristic is used to smooth the parameter of PLSA models for acceptable predictive performance: the authors in [11] showed that overfitting can occur even if tempering process is used.", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "To overcome these two issues, the latent Dirichlet allocation (LDA) [2] method was proposed.", "startOffset": 68, "endOffset": 71}, {"referenceID": 11, "context": "The authors in [12] evaluated the effectiveness of the Jensen-Shannon (JS) theoretic measure [13] in predicting systems ranks in two summarization tasks: query-focused and update summarization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "The authors in [12] evaluated the effectiveness of the Jensen-Shannon (JS) theoretic measure [13] in predicting systems ranks in two summarization tasks: query-focused and update summarization.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "LDA is a generative model which considers a document, seen as a bag-of-words [1], as a mixture of latent topics.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "PLSA and LDA models have been shown to generally outperform LSA on IR tasks [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "This allows to obtain a parameter binding all the documents together [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 94, "endOffset": 97}, {"referenceID": 14, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [17] and gives a simple algorithm to approximate inference in high-dimensional models such as LDA [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [17] and gives a simple algorithm to approximate inference in high-dimensional models such as LDA [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "The first use of Gibbs Sampling for estimating LDA is reported in [16] and a more comprehensive description of this method can be found in [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The first use of Gibbs Sampling for estimating LDA is reported in [16] and a more comprehensive description of this method can be found in [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "Various text summarization systems have been proposed over the years [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Two baseline systems as well as the ARTEX summarization system, that reaches state-of-the-art performance [20], are presented in this section.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "Although very simple, this method is a strong baseline for the performance of any automatic summarization system [21, 22].", "startOffset": 113, "endOffset": 121}, {"referenceID": 21, "context": "Although very simple, this method is a strong baseline for the performance of any automatic summarization system [21, 22].", "startOffset": 113, "endOffset": 121}, {"referenceID": 20, "context": "Baseline random (BR) The Baseline random [21] randomly selects n sentences of the documents, where n is also determined by a compression rate.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "ARTEX AnotheR TEXt (ARTEX) algorithm [20] is another simple extractive algorithm.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The w(\u2022) computed by Equation 4 must be normalized between the interval [0, 1].", "startOffset": 72, "endOffset": 78}, {"referenceID": 19, "context": "The element a\u03bc is only a scale factor that does not modify \u03b1 [20]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "This summarization system outperforms the CORTEX [24] one with the FRESA [25] measure.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "This summarization system outperforms the CORTEX [24] one with the FRESA [25] measure.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "ARTEX is evaluated with several corpus such as the Medecina Clinica [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "A lower perplexity score indicates better generalization performance [2]:", "startOffset": 69, "endOffset": 72}, {"referenceID": 24, "context": "Following a classical study of LDA topic spaces quality [26], the number of topics by model is fixed to {5, 10, 50, 100, 200, 400}.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "These topic spaces are built with the MALLET toolkit [27].", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.", "creator": "TeX"}}}