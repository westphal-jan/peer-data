{"id": "1703.07055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems", "abstract": "Language comprehension is a key component in a spoken dialogue system. In this paper, we examine how the speech comprehension module affects the performance of the dialogue system by conducting a series of systematic experiments on a task-oriented neural dialogue system in a learning environment based on amplification. Empirical study shows that slot-level errors under different types of speech comprehension errors can have a greater impact on the overall performance of a dialogue system than errors at the intention level. Furthermore, our experiments show that the reinforcement-based dialogue system is able to learn when and what needs to be confirmed in order to achieve better performance and greater robustness.", "histories": [["v1", "Tue, 21 Mar 2017 04:56:14 GMT  (127kb,D)", "http://arxiv.org/abs/1703.07055v1", "5 pages, 5 figures"]], "COMMENTS": "5 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["xiujun li", "yun-nung chen", "lihong li", "jianfeng gao", "asli celikyilmaz"], "accepted": false, "id": "1703.07055"}, "pdf": {"name": "1703.07055.pdf", "metadata": {"source": "CRF", "title": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems", "authors": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz"], "emails": ["xiul@microsoft.com", "lihongli@microsoft.com", "jfgao@microsoft.com", "y.v.chen@ieee.org", "asli@ieee.org"], "sections": [{"heading": "1. Introduction", "text": "Task-oriented dialogue systems, such as Microsoft\u2019s Cortana, Apple\u2019s Siri, Amazon\u2019s Echo, Google\u2019s Home, etc., assist users in completing specific tasks such as booking movie tickets, setting up calendar items or finding restaurants through natural language interactions. A traditional dialogue system consists of the following components: 1) a natural language understanding (NLU) module, which receives utterances of free texts (typed or spoken), and maps them into a structured semantic frame; usually there are three key tasks in such a NLU module: domain classification, intent determination and slot filling. Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5]. 2) a dialogue manager (DM), which consists of a state tracker and a policy learner: the state tracker offers the ability to access the external database or knowledge base, tracks the evolving state of the dialogue, and constructs the state estimation, whereas the policy learner takes the state estimation as input and chooses a dialogue action; and 3) a natural language generation (NLG) module, which translates the structured dialogue action representation into a natural language form.\nThere exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11]. In a typical modular pipeline, each component is trained separately, and processed in sequence to form a pipelined dialog system. The biggest problem of such dialogue systems is that the error in an upstream module is propagated to downstream components in the pipeline, making it challenging for the downstream components (e.g., policy learner) to adapt to the errors accumulated from the upstream components (e.g., LU), and eventually degrading the overall dialogue system performance. Recently,\nend-to-end learning approaches offer a potential solution to this issue. For instance, the policy learner can be adapted to the noise trickling down from the LU component, as well as the error from a downstream component (e.g., from policy learner or NLG) can be back-propagated to fine tune the LU component [12, 13]. This eventually yields a dialogue system that is more robust to individual component errors.\nDespite the widespread interest in building task oriented dialogue systems, there has been few work that investigated the relationship and mutual influence of these components, and their impact on the overall dialogue system performance. For instance, Lemon et al. [14] compared the policy transfer properties under different environments, showing that policies trained in high-noise conditions have better transfer properties than those trained in low-noise conditions. Su et al. [15] briefly investigated the effect of dialogue action level semantic error rates (SER) on the dialogue performance. In this work, with extensive quantitative analysis on a fine-grained level of NLU errors, our goal is to provide meaningful insights on how the language understanding component impacts the overall performance of the dialogue system. Our contributions are three-folds:\n\u2022 Our work is the first systematic analysis to investigate the impact of different types of noise in the natural language understanding component on the dialogue systems.\n\u2022 We show that slot-level errors have a greater impact on the performance of dialogue systems, compared to intent-level noises.\n\u2022 Our findings shed some light on how to design multi-task natural language understanding models (intent classification, slot labeling) in the dialogue systems."}, {"heading": "2. Approach", "text": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13]. A dialogue policy is often sensitive to the noise or other types of errors (e.g., mis-classification of a domain or dialog intent) accumulated from the NLU module, especially in modular pipeline based dialogue systems, where NLU and policy learning are trained separately. Recently, end-to-end learning approaches to building dialog systems with varying optimization objective functions offer many benefits for both NLU [12] and policy learning, in which the policy learning can be adapted to the noise in the NLU component, and the NLU part can be fine tuned in a way that is guided by the policy learner\u2019s performance.\nIn this work, we thoroughly investigate the real impact of the NLU on the performance of a dialogue system. Leveraging the influence of NLU (the most upstream component) to the\nar X\niv :1\n70 3.\n07 05\n5v 1\n[ cs\n.C L\n] 2\n1 M\nar 2\n01 7\ndialogue system will have a huge impact to either the development of the understanding module, policy learning and natural language generation etc. downstream tasks. All experiments are conducted in a user simulation environment [18]."}, {"heading": "2.1. User Simulation", "text": "In the dialogue community, researchers typically seek to optimize dialogue policies with either supervised learning (SL) or reinforcement learning (RL) methods. In SL approaches, a policy is trained to imitate the observed actions of an expert. Supervised learning approaches often require a large amount of expert-labeled data for training. For task-specific domains, intensive domain knowledge is usually required for collecting and annotating actual human-human or human-machine conversations, and is often expensive and time-consuming. Additionally, even with a large amount of training data, parts of the dialogue state space may not be well-covered in the training data, due to lack of sufficient exploration, which prevents a supervised learner finding an optimal policy.\nIn contrast, RL approaches allow an agent to learn without expert-generated examples. Given only a reward signal, the agent can optimize a dialogue policy through interaction with users. Unfortunately, RL can require many samples from an environment, making learning from scratch with real users impractical. To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].\nThe goal of user simulation is to generate natural and reasonable conversations, allowing the RL agent to explore the policy space. The simulation-based approach allows an agent to explore trajectories which may not exist in previously observed data, overcoming a central limitation of imitation-based approaches. Dialogue agents trained on these simulators can then serve as an effective starting point, after which they can be deployed against real humans to improve further via reinforcement learning. To understand the impact of NLU to the dialogue system and draw a convincing conclusion, it is hard to control all possible NLU variations in the real user setting, and also the requirement of large data makes this impossible. While in user simulation, it is much easier to control each variable to directly analyze the importance of NLU in the dialogue system.\nIn the task-completion dialogue setting, the user simulator first generates a user goal. The agent does not know the user goal, but tries to help the user accomplish it in the course of conversations. Hence, the entire conversation exchange is around this implicit goal. A user goal generally consists of two parts: inform slots for slot-value pairs that serve as constraints from the user, and request slots for slots whose value the user has no information about, but wants to get the values from the agent during the conversation. The user goals are generated using a labeled set of conversational data [18]. During the course of a dialogue, the user simulator maintains a compact, stack-like representation called user agenda [23]."}, {"heading": "2.2. Error Model Controller", "text": "When training or testing a policy based on semantic frames of user actions, an error model [24] is introduced to simulate the noise from the NLU component, and noisy communication between the user and agent. Here, we introduce different levels of noise in the error model: one at the intent level, the other the slot level. For each level, there are more fine-grained noise."}, {"heading": "2.2.1. Intent Error", "text": "At the intent level, we categorize the intent into three groups: \u2022 Group 1: general greeting, thanks, closing, etc. \u2022 Group 2: user may inform, to tell the slot val-\nues (or constraints) to the agent, for example, inform(moviename=\u2018Titanic\u2019, starttime=\u20187pm\u2019).\n\u2022 Group 3: user may request information for some specific slots. In a movie-booking scenario, user might ask \u201crequest(starttime;moviename=\u2018Titanic\u2019)\u201d.\nIn one specific task, for example, movie-booking scenario, there are multiple inform and request intents, like request theater, request starttime, request moviename etc. are different intents, but in the same group.\nBased on the above intent categorization, there are three types of intent errors:\n\u2022 Random error (0): Random noisy intent from same category (within group error) or other categories (between group error).\n\u2022 Within group error (1): the noisy intent is from the same group with the real intent, for example, the real intent is request theater, but the predicted intent from NLU might be request moviename.\n\u2022 Between group error (2): the noisy intent is from the different group, for example, a real request moviename might be predicted as inform moviename intent."}, {"heading": "2.2.2. Slot Error", "text": "At the slot level, there are four kinds of error types: \u2022 Random error (0): to simulate noise that is randomly set\nto the following three types. \u2022 Slot deletion (1): to simulate the scenario where the slot\nwas not recognized by the NLU; \u2022 Incorrect slot value (2): to simulate the scenario where\nthe slot name was recognized correctly, but the slot value was not, e.g., wrong word segmentation;\n\u2022 Incorrect slot (3): to simulate the scenario where neither the slot or its value was recognized correctly."}, {"heading": "2.3. Dialogue Manager", "text": "The symbolic dialogue act form from NLU will be passed on to the dialogue manager (DM). A classic DM is charge of both state tracking and policy learning. The state tracker will keep tracking the evolving slot value pairs from both agent and user, and based on the conversation history, a query may be formed to interact with an external database to retrieve the available result. In every turn of the dialogue, the state tracker is updated based on the retrieved results from the database and the latest user dialogue action, and outputs a dialogue state (in some compact representation). The dialogue state often includes the latest user action, latest agent action, database results, turn information, and conversation history, etc. Conditioned on the dialogue state, the dialogue policy is to generate the next available agent action \u03c0(a|s). To optimize this policy, we apply reinforcement learning in an end-to-end fashion. In this work, we represent the policy using a deep Q-network (DQN) [25], which takes the state st from the state tracker as input, and outputs Q(st, a; \u03b8) for all actions a using network parameter \u03b8. Given a state s, the policy chooses the action with the highest Q-value: argmaxaQ(s, a; \u03b8). Two important DQN tricks, target network and experience replay are applied [25]."}, {"heading": "3. Experiments", "text": "The experiments are performed on a neural task-completion dialogue system that helps users book movie tickets. The system gathers information about the customers\u2019 desires over multiturn conversations and ultimately books the intended movie tickets. The environment then assesses a binary outcome (success or not) at the end of the conversation: it is a success if a movie is booked and the booked movie satisfies all the users constraints. To measure the quality of the agent, there are three evaluation metrics: {success rate1, average reward, average turns}. Each of them provides different information about the quality of agents. Three metrics are strongly correlated: generally, a good policy should have a higher success rate, higher average reward, and lower average turns. We train the reinforcement learning based agents by interacting with a simulated user in an end-to-end fashion under different error settings, and report success rate and average turns for analysis. Table 1 summarizes all settings for investigating the impact of different elements (intent and slot errors from NLU) to the dialogue systems, where the learning curves are averaged over 10 runs."}, {"heading": "3.1. Datasets", "text": "The data were collected via Amazon Mechanical Turk and annotated with an internal schema. There are 11 intents (i.e., inform, request, confirm question, confirm answer, etc.), and 29 slots (i.e., moviename, starttime, theater, numberofpeople, etc.). Most slots are informable slots, which users can use to constrain the search, and some are requestable slots, of which users can ask values from the agent. For example, numberofpeople cannot be requestable, since arguably user knows how many tickets he or she wants to buy. There are a total of 280 labeled dialogues in the movie domain, and the average number of turns per dialogue is approximately 11."}, {"heading": "3.2. Basic Experiments", "text": "The group of basic experiments (from B1 to B3) are in the settings that combine the noise from both intent and slot: 1)\n1Success rate is sometimes known as task completion rate \u2014 the fraction of dialogues that are completed successfully.\nFor both intent and slot, the error types are random, and the error rates are in {0.00, 0.10, 0.20}. The rule-based agent reports 41%, 21%, and 12% success rates under 0.00, 0.10, and 0.20 error rates respectively. In constrast, the RL-based agent achieves 91%, 79%, and 76% success rate under the same error rates, respectively. We compare the performance between two types of agents and find that the RL-based agent has greater robustness and is less sensitive to noisy inputs. Therefore, the following experiments are performed using a RL dialogue agent due to robustness consideration. From Fig. 1, the dialogue agents degrade remarkably when the error rate increases (leading to lower success rates and higher average turns)."}, {"heading": "3.3. Intent Experiments", "text": "To further understand the impact of intent-level noises to dialogue systems, two experimental groups are performed: the first group (I0\u2013I2) focuses on the difference among all intent error types; the second group (I3\u2013I5) focuses on the impact of intent error rates. Other factors are identical for the two groups, with the random slot error type and a 5% slot error rate."}, {"heading": "3.3.1. Intent Error Type", "text": "Experiments with the settings of I0\u2013I2 are under the same slot errors and same intent error rate (10%), but with different intent error types: I1 includes the noisy intents from the same categories, I2 includes the noisy intents from different categories, and I0 includes both via random selection. Fig. 2(a) shows the learning curves for all intent error types, where the difference among three curves is insignificant, indicating that the incorrect intents have similar impact no matter what categories they belong to."}, {"heading": "3.3.2. Intent Error Rate", "text": "Experiments with the settings I3\u2013I5 investigate the difference among different intent error rates. When the intent error rate increases, the dialogue agent performs slightly worse, but the difference is subtle. It suggests that the RL-based agent has better robustness to noisy intents. As shown in Fig. 2(a,b), all RL agents can converge to a similar success rate in both intent error type and intent error rate settings."}, {"heading": "3.4. Slot Experiments", "text": "We further conducted two groups of experiments to investigate the impact of slot-level noises, where other factors are fixed, with the random intent error type and a 10% intent error rate."}, {"heading": "3.4.1. Slot Error Type", "text": "Experiments with the settings from S0\u2013S3 investigate the impact of different slot error types. Corresponding learning curves are given in Fig. 2(c). Among single error types (S1\u2013S3), incorrect slot value (S2) performs worst, which means that the slot name is recognized correctly, but a wrong value is extracted with the slot (such as wrong word segmentation); in this case, the agent receives a wrong value for the slot, and eventually books a wrong ticket or fails to book it. The probable reason is that the dialogue agent has difficulty identifying the mistakes based on the RL-based belief tracking, and using the incorrect slot values for the following dialogue actions could significantly degrade the performance. Between slot deletion (S1) and incorrect slot (S3), the difference is limited, indicating that the RL agent has similar capability of handling these two kinds of slotlevel noises."}, {"heading": "3.4.2. Slot Error Rate", "text": "Experiments with the settings from S4\u2013S6 focus on different slot error rates (0%, 10%, and 20%) and report the results in Fig. 2(d). It is clear from Fig. 2(d) that the dialogue agent performs worse as the slot error rate increases (the curve of the success rate drops and the curve of average turns rises). Comparing with Fig. 2(b), the dialogue system performance is more sensitive to the slot error rate than the intent error rate."}, {"heading": "3.5. Discussion", "text": "An important finding suggested by our empirical results is that slot-level errors are more important than intent-level errors. A possible explanation is related to our dialogue action representa-\ntion, intent(slot-value pairs). If an intent is predicted wrong, for example, inform was predicted incorrectly as request ticket, the dialogue agent can handle the unreliable situation and decide to make confirmation in order to keep the correct information for the following conversation. In contrast, if a slot moviename is predicted wrong, or a slot value is not identified correctly, this dialogue turn might directly pass the wrong information to the agent, which might lead the agent to book a wrong ticket. Another reason is that the dialogue agent can still maintain a correct intent based on slot information even though the predicted intent is wrong. In order to verify the hypotheses, further experiments are needed, which we leave as future work.\nFinally, it should be noted that the experiments in this paper are based on a task-completion dialogue setting, but chit-chat is another setting with different optimization goals [26]. It is interesting to conduct similar experiments to see the impact of language understanding errors on a chit-chat dialogue system\u2019s performance."}, {"heading": "4. Conclusion", "text": "In this paper, we conduct a series of extensive experiments to understand the impact of natural language understanding errors on the performance of a reinforcement learning based, taskcompletion neural dialogue system. Our results suggest several interesting conclusions: 1) slot-level errors have a greater impact than intent-level errors; 2) different slot error types have different impacts on the RL agents; 3) RL agents are more robust to certain types of slot-level errors \u2014 the agents can learn to double-check or confirm with users, at the cost of slightly longer conversations."}, {"heading": "5. References", "text": "[1] G. Tur and R. De Mori, Spoken language understanding: Systems\nfor extracting semantic information from speech. John Wiley & Sons, 2011.\n[2] P. Xu and R. Sarikaya, \u201cConvolutional neural network based triangular crf for joint intent detection and slot filling,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78\u201383.\n[3] D. Hakkani-Tu\u0308r, G. Tur, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng, and Y.-Y. Wang, \u201cMulti-domain joint semantic frame parsing using bi-directional rnn-lstm,\u201d in Proceedings of The 17th Annual Meeting of the International Speech Communication Association, 2016.\n[4] B. Liu and I. Lane, \u201cAttention-based recurrent neural network models for joint intent detection and slot filling,\u201d Interspeech, pp. 685\u2013689, 2016.\n[5] Y.-N. Chen, D. Hakkani-Tu\u0308r, G. Tur, A. Celikyilmaz, J. Gao, and L. Deng, \u201cSyntax or semantics? knowledge-guided joint semantic frame parsing,\u201d in Proceedings of 6th IEEE Workshop on Spoken Language Technology, 2016.\n[6] A. I. Rudnicky, E. H. Thayer, P. C. Constantinides, C. Tchou, R. Shern, K. A. Lenzo, W. Xu, and A. Oh, \u201cCreating natural dialogs in the carnegie mellon communicator system.\u201d in Eurospeech, 1999.\n[7] V. Zue, S. Seneff, J. R. Glass, J. Polifroni, C. Pao, T. J. Hazen, and L. Hetherington, \u201cJUPITER: a telephone-based conversational interface for weather information,\u201d IEEE Transactions on speech and audio processing, vol. 8, no. 1, pp. 85\u201396, 2000.\n[8] V. W. Zue and J. R. Glass, \u201cConversational interfaces: Advances and challenges,\u201d Proceedings of the IEEE, vol. 88, no. 8, pp. 1166\u20131180, 2000.\n[9] J. D. Williams and G. Zweig, \u201cEnd-to-end lstm-based dialog control optimized with supervised and reinforcement learning,\u201d arXiv preprint arXiv:1606.01269, 2016.\n[10] T. Zhao and M. Eskenazi, \u201cTowards end-to-end learning for dialog state tracking and management using deep reinforcement learning,\u201d arXiv preprint arXiv:1606.02560, 2016.\n[11] X. Li, Y.-N. Chen, L. Li, and J. Gao, \u201cEnd-to-end task-completion neural dialogue systems,\u201d arXiv preprint arXiv:1703.01008, 2017.\n[12] X. Yang, Y.-N. Chen, D. Hakkani-Tur, P. Crook, X. Li, J. Gao, and L. Deng, \u201cEnd-to-end joint learning of natural language understanding and dialogue manager,\u201d arXiv preprint arXiv:1612.00913, 2016.\n[13] B. Dhingra, L. Li, X. Li, J. Gao, Y.-N. Chen, F. Ahmed, and L. Deng, \u201cEnd-to-end reinforcement learning of dialogue agents for information access,\u201d arXiv preprint arXiv:1609.00777, 2016.\n[14] O. Lemon and X. Liu, \u201cDialogue policy learning for combinations of noise and user simulation: transfer results,\u201d in Proc. SIGdial, 2007.\n[15] P.-H. Su, M. Gasic, N. Mrksic, L. Rojas-Barahona, S. Ultes, D. Vandyke, T.-H. Wen, and S. Young, \u201cContinuously learning neural dialogue management,\u201d arXiv preprint arXiv:1606.02689, 2016.\n[16] J. Williams, A. Raux, and M. Henderson, \u201cThe dialog state tracking challenge series: A review,\u201d Dialogue & Discourse, vol. 7, no. 3, pp. 4\u201333, 2016.\n[17] Z. C. Lipton, J. Gao, L. Li, X. Li, F. Ahmed, and L. Deng, \u201cEfficient exploration for dialogue policy learning with bbq networks & replay buffer spiking,\u201d arXiv preprint arXiv:1608.05081, 2016.\n[18] X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao, and Y.-N. Chen, \u201cA user simulator for task-completion dialogues,\u201d arXiv preprint arXiv:1612.05688, 2016.\n[19] W. Eckert, E. Levin, and R. Pieraccini, \u201cUser modeling for spoken dialogue system evaluation,\u201d in Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on. IEEE, 1997, pp. 80\u201387.\n[20] K. Georgila, J. Henderson, and O. Lemon, \u201cLearning user simulations for information state update dialogue systems.\u201d in INTERSPEECH, 2005, pp. 893\u2013896.\n[21] O. Pietquin, \u201cConsistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation,\u201d in 2006 IEEE International Conference on Multimedia and Expo. IEEE, 2006.\n[22] J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young, \u201cA survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies,\u201d The knowledge engineering review, 2006.\n[23] J. Schatzmann and S. Young, \u201cThe hidden agenda user simulation model,\u201d IEEE transactions on audio, speech, and language processing, vol. 17, no. 4, pp. 733\u2013747, 2009.\n[24] J. Schatzmann, B. Thomson, and S. Young, \u201cError simulation for training statistical dialogue systems,\u201d in IEEE Workshop on Automatic Speech Recognition & Understanding, 2007.\n[25] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHumanlevel control through deep reinforcement learning,\u201d Nature, vol. 518, pp. 529\u2013533, 2015.\n[26] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, \u201cDeep reinforcement learning for dialogue generation,\u201d arXiv preprint arXiv:1606.01541, 2016."}], "references": [{"title": "Spoken language understanding: Systems for extracting semantic information from speech", "author": ["G. Tur", "R. De Mori"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 78\u201383.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm", "author": ["D. Hakkani-T\u00fcr", "G. Tur", "A. Celikyilmaz", "Y.-N. Chen", "J. Gao", "L. Deng", "Y.-Y. Wang"], "venue": "Proceedings of The 17th Annual Meeting of the International Speech Communication Association, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "author": ["B. Liu", "I. Lane"], "venue": "Interspeech, pp. 685\u2013689, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Syntax or semantics? knowledge-guided joint semantic frame parsing", "author": ["Y.-N. Chen", "D. Hakkani-T\u00fcr", "G. Tur", "A. Celikyilmaz", "J. Gao", "L. Deng"], "venue": "Proceedings of 6th IEEE Workshop on Spoken Language Technology, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "JUPITER: a telephone-based conversational interface for weather information", "author": ["V. Zue", "S. Seneff", "J.R. Glass", "J. Polifroni", "C. Pao", "T.J. Hazen", "L. Hetherington"], "venue": "IEEE Transactions on speech and audio processing, vol. 8, no. 1, pp. 85\u201396, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Conversational interfaces: Advances and challenges", "author": ["V.W. Zue", "J.R. Glass"], "venue": "Proceedings of the IEEE, vol. 88, no. 8, pp. 1166\u20131180, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["J.D. Williams", "G. Zweig"], "venue": "arXiv preprint arXiv:1606.01269, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning", "author": ["T. Zhao", "M. Eskenazi"], "venue": "arXiv preprint arXiv:1606.02560, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end task-completion neural dialogue systems", "author": ["X. Li", "Y.-N. Chen", "L. Li", "J. Gao"], "venue": "arXiv preprint arXiv:1703.01008, 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end joint learning of natural language understanding and dialogue manager", "author": ["X. Yang", "Y.-N. Chen", "D. Hakkani-Tur", "P. Crook", "X. Li", "J. Gao", "L. Deng"], "venue": "arXiv preprint arXiv:1612.00913, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y.-N. Chen", "F. Ahmed", "L. Deng"], "venue": "arXiv preprint arXiv:1609.00777, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Dialogue policy learning for combinations of noise and user simulation: transfer results", "author": ["O. Lemon", "X. Liu"], "venue": "Proc. SIGdial, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuously learning neural dialogue management", "author": ["P.-H. Su", "M. Gasic", "N. Mrksic", "L. Rojas-Barahona", "S. Ultes", "D. Vandyke", "T.-H. Wen", "S. Young"], "venue": "arXiv preprint arXiv:1606.02689, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The dialog state tracking challenge series: A review", "author": ["J. Williams", "A. Raux", "M. Henderson"], "venue": "Dialogue & Discourse, vol. 7, no. 3, pp. 4\u201333, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking", "author": ["Z.C. Lipton", "J. Gao", "L. Li", "X. Li", "F. Ahmed", "L. Deng"], "venue": "arXiv preprint arXiv:1608.05081, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A user simulator for task-completion dialogues", "author": ["X. Li", "Z.C. Lipton", "B. Dhingra", "L. Li", "J. Gao", "Y.-N. Chen"], "venue": "arXiv preprint arXiv:1612.05688, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "User modeling for spoken dialogue system evaluation", "author": ["W. Eckert", "E. Levin", "R. Pieraccini"], "venue": "Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on. IEEE, 1997, pp. 80\u201387.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning user simulations for information state update dialogue systems.", "author": ["K. Georgila", "J. Henderson", "O. Lemon"], "venue": "in INTER- SPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Consistent goal-directed user model for realisitc man-machine task-oriented spoken dialogue simulation", "author": ["O. Pietquin"], "venue": "2006 IEEE International Conference on Multimedia and Expo. IEEE, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey of statistical user simulation techniques for reinforcementlearning of dialogue management strategies", "author": ["J. Schatzmann", "K. Weilhammer", "M. Stuttle", "S. Young"], "venue": "The knowledge engineering review, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "The hidden agenda user simulation model", "author": ["J. Schatzmann", "S. Young"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 17, no. 4, pp. 733\u2013747, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Error simulation for training statistical dialogue systems", "author": ["J. Schatzmann", "B. Thomson", "S. Young"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, pp. 529\u2013533, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1606.01541, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 2, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 3, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 4, "context": "Typically, there exist two kinds of recipes: single-task learning with a concatenated approach [1] and multi-task learning with a joint approach [2, 3, 4, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 5, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 118, "endOffset": 127}, {"referenceID": 6, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 118, "endOffset": 127}, {"referenceID": 7, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 8, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 9, "context": "There exist in the literature two main approaches to building dialogue systems: modular pipeline based dialog systems [6, 7, 8] and end-to-end dialogue systems [9, 10, 11].", "startOffset": 160, "endOffset": 171}, {"referenceID": 10, "context": ", from policy learner or NLG) can be back-propagated to fine tune the LU component [12, 13].", "startOffset": 83, "endOffset": 91}, {"referenceID": 11, "context": ", from policy learner or NLG) can be back-propagated to fine tune the LU component [12, 13].", "startOffset": 83, "endOffset": 91}, {"referenceID": 12, "context": "[14] compared the policy transfer properties under different environments, showing that policies trained in high-noise conditions have better transfer properties than those trained in low-noise conditions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] briefly investigated the effect of dialogue action level semantic error rates (SER) on the dialogue performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 11, "context": "Natural language understanding (NLU) is a fundamental component to many downstream tasks in a dialogue system, such as state tracking [16] and policy learning [17, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 10, "context": "Recently, end-to-end learning approaches to building dialog systems with varying optimization objective functions offer many benefits for both NLU [12] and policy learning, in which the policy learning can be adapted to the noise in the NLU component, and the NLU part can be fine tuned in a way that is guided by the policy learner\u2019s performance.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "All experiments are conducted in a user simulation environment [18].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 18, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 19, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 20, "context": "To overcome this limitation, many dialogue researchers train RL agents using simulated users [19, 20, 21, 22].", "startOffset": 93, "endOffset": 109}, {"referenceID": 16, "context": "The user goals are generated using a labeled set of conversational data [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "During the course of a dialogue, the user simulator maintains a compact, stack-like representation called user agenda [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 22, "context": "When training or testing a policy based on semantic frames of user actions, an error model [24] is introduced to simulate the noise from the NLU component, and noisy communication between the user and agent.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "In this work, we represent the policy using a deep Q-network (DQN) [25], which takes the state st from the state tracker as input, and outputs Q(st, a; \u03b8) for all actions a using network parameter \u03b8.", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Two important DQN tricks, target network and experience replay are applied [25].", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Finally, it should be noted that the experiments in this paper are based on a task-completion dialogue setting, but chit-chat is another setting with different optimization goals [26].", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.", "creator": "LaTeX with hyperref package"}}}