{"id": "1703.07381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Improving Statistical Multimedia Information Retrieval Model by using Ontology", "abstract": "A typical IR system that delivers and stores information is affected by the problem of comparing user queries with available content on the Web. Ontology presents the extracted terms in the form of a network graph consisting of nodes, edges, index terms, etc. The above-mentioned IR approaches provide relevance and thus satisfy user queries. Work also focuses on the analysis of multimedia documents and performs calculations for extracted terms using various statistical formulas. The developed model reduces semantic gaps and efficiently meets user needs.", "histories": [["v1", "Tue, 21 Mar 2017 18:29:05 GMT  (997kb)", "http://arxiv.org/abs/1703.07381v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["gagandeep singh narula", "vishal jain"], "accepted": false, "id": "1703.07381"}, "pdf": {"name": "1703.07381.pdf", "metadata": {"source": "META", "title": "Improving Statistical Multimedia Information Retrieval (MIR) Model by using Ontology", "authors": ["Gagandeep Singh Narula", "Vishal Jain"], "emails": [], "sections": [{"heading": null, "text": "collection of documents, either multimedia or text documents is still a cumbersome task. Multimedia documents include various elements of different data types including visible and audible data types (text, images and video documents), structural elements as well as interactive elements. In this paper, we have proposed a statistical high level multimedia IR model that is unaware of the shortcomings caused by classical statistical model. It involves use of ontology and different statistical IR approaches (Extended Boolean Approach, Bayesian Network Model etc) for representation of extracted text-image terms or phrases.\nA typical IR system that delivers and stores information is affected by problem of matching between user query and available content on web. Use of Ontology represents the extracted terms in form of network graph consisting of nodes, edges, index terms etc. The above mentioned IR approaches provide relevance thus satisfying user\u201fs query.\nThe paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas. The proposed model developed reduces semantic gap and satisfies user needs efficiently."}, {"heading": "Index Terms", "text": "Information Retrieval (IR), OWL, Statistical Approaches (BI model, Extended Boolean Approach, Bayesian Network Model), Query Expansion and Refinement."}, {"heading": "State of Art", "text": "Research on multimedia information retrieval seems to be gargantuan and challenging task. Its areas are so diversified that it has lead to independent research in its own components. Firstly, there used to be human centered systems that focus on user\u201fs behavior and needs. Various experiments and studies were conducted in lieu of these systems. The users were asked to present a set of valuable things in daily life. It was done on similarity of users. Some of choices are same while some are different. Few of them prefer to use images instead of text caption.\nIn further experiments, it was noticed that new users were taking feedback from previous users. It leads to concept of relevance feedback module in information model. In early years, most research was done on content- based image retrieval. The existing models are of different level and scope. These models are semantically unambiguous. For e.g.: IPTC model [1] uses location fields that focus on location of data but this model also failed due to lack of statistical approach. Another metadata model was developed i.e. EXIF\n[2] to support features of images but it did not tell anything about relationship and associations between different contents of image. It also resulted in vain. The third model developed was Dublin Core [3] that deals with semantic as well as structural content of image and text but it failed to depict relationship between text and image.\nWith advancement in technology and predictions, some probabilistic and futuristic models were also developed. In following paper, statistical multimedia IR model has been proposed and compared with classical multimedia IR model."}, {"heading": "1. INTRODUCTION", "text": "Human knowledge is richest multimedia storage system. There are various mechanisms like vision, language that expresses knowledge and information obtained from them must be processed by system efficiently. There must be systems designed that interprets and process human queries, thus producing relevant results. It is often seen that users get baffled while searching results of their queries. The reasons behind this are:\n The content of information is unclear and needs user to refine that information.\n The data stored on systems may or may not be updated regularly.\n There lies lower level of interaction between user request and stored information on systems. The low-\nlevel links are called Semantic Gap.\nStatistical approaches involves retrieved documents that matches query closely in terms of statistics i.e. it must have statistical model, calculations and analysis. These approaches break given query into TERMS. Terms are words that occur in collection of documents and are extracted automatically. For reducing inconsistencies and semantic gap in multimedia information, it is necessary to remove different forms of same word because it makes user confused in choosing specific terms that lies close to query. Some IR systems extract phrases from documents. A phrase is a combination of two or more words that is found in document.\nWe have used approaches like extended Boolean approach, network model that performs structural analysis for retrieving text or image pairs. They also assign weights to given term. The weight is defined as measure of effectiveness of given term in distinguishing one document from other documents. The paper has following sections: Section 2 describes architecture of classical multimedia model. Section 3 lets reader go through proposed IR model that is implemented using statistical approaches with the use of ontology. It also requires conversion of low level features to high level features\nusing multimedia analysis. Section 4 deals with experimental analysis and calculations depicting the relevance of proposed model. Finally, Section 5 concludes about paper."}, {"heading": "2. CONCEPT OF MULTIMEDIA IR SYSTEM", "text": "The classical multimedia IR system has not proven effective in extraction of relevant terms from document collections. Traditional IR systems are not intelligent that they are able to produce accurate results. These systems use human perception to process query and returns results. The results may be relevant or non- relevant because these systems match query with information stored in information database.\nThe syntax of multimedia document is different from text documents. Multimedia documents do not contain any information symbols or keywords that help in expressing information. They consist of:\n Visible and Audible Data Types: - It includes text, images, graphs, videos and audio.\n Structural Elements: - They are not visible. They describe the organization of other data types.\nThe salient features of multimedia information [4] are given below:\n The information stored in document that is to be searched can be audio, visual, videos etc. They\ncommunicate variety of messages and emotions that helps to understand easily.\n Structure information gives organization and usability in performing communications.\n There is communicational gap between user and system. It is known that some systems are fast in\nprocessing of calculations whereas human is not. So, it leads to communication gap."}, {"heading": "2.1 Layout of Classical Multimedia Ir Model", "text": "Since multimedia documents do not contain keywords or symbols that facilitates easy process of searching through document. Keeping this in mind, this classical model consists of Query Processing Module that translates the multimedia information tokens into symbols / keywords which are easily understood by system. The model has following modules:\n Analysis Module: - IR system firstly analysis multimedia documents and extract features from\nthem. The features include low- level as well as high- level features.\n Indexing Module: - The module that stores features or terms retrieved from multimedia documents is\ncalled Indexing Module.\n Query Processing Module: - This module translates multimedia information tokens like audio, text-\npairs, videos etc into information symbols that are now understood by system.\n Retrieval Module: - It finds rank of stored documents on basis of similar terms used in query.\nAfter ranking of documents, the results satisfying query are presented to user.\nQuery Query"}, {"heading": "Results Documents", "text": "User\nFigure1: A Classical Multimedia IR Model [5]"}, {"heading": "2.2 Shortcomings of Classical Multimedia", "text": ""}, {"heading": "IR Model", "text": "They are explained below:\n The classical model deals with terms or information symbols instead of maintaining relationships\nbetween them. It does not give any information about concepts used in extracted terms or image pairs.\n It creates semantic gap [6] between user and system due to availability of irrelevant and superfluous\ninformation terms stored in information database of IR system.\n It does not involve concept of ontology and semantic associations for representing concepts\nassociated with terms in document.\n The terms which are relevant and similar to each other are identified at the end of phase by\nRETRIEVAL Module. The good model is one that has capability to distinguish between relevant and non relevant terms in the middle of phase in order to prevent any confusion.\n The model does not involve the concept of re-use of queries. Once the query is expanded, it will not\nstore in system for future use. Again, it has to analyze large collection of documents and retrieve terms from them.\nMultimedia Analysis Query processing\nRetrieval Application Indexing\nMultimedia\ndocument\nIndexer\n It does not employ any statistical or probabilistic approaches for determining relevance of IR system."}, {"heading": "3. PROPOSED HIGH LEVEL", "text": "MULTIMEDIA IR MODEL A model is being designed that employ use of statistical IR approaches for extracting terms from multimedia documents. Ontology Module has been introduced that serves the task of representing concepts and relationships among retrieved\nterms. In order to overcome this problem, the model includes only those approaches that perform extraction of terms like images, video, and text from multimedia documents as well as text documents.\nThe block diagram of proposed model containing several modules is shown below:\nStructure Analysis Terms and information (Text, Image pairs) symbols are extracted. Terms are phrases or Collections (video, audio)\nOntology Module\nFigure2 (a): Proposed High- level Statistical Multimedia IR Model\nContains large amount of\ninformation that may be\nrelevant or irrelevant\nINDEXING\nExtraction of Relevant terms / phrases / concepts\nfrom analyzed / retrieved documents.\nUse of Statistical approach (low- level features) as\nwell as Semantic Approach (high-level features)\nStatistical Approach (Extended\nBoolean Approach and Bayesian\nInference network Model)\nSemantic Approach (NLP\napproaches and knowledge\ndiscovery)\nExtended Boolean\nApproach: It gives\nn relevant terms\nin less time.\nBayesian Network model: It takes\nmultiple queries at same time. It\ncreates a graph that has nodes\nconnected by edges. Nodes are\nTrue/False statements.\nP-norm model model BI Model\nWEB\nMultimedia\nDocuments\n(images, videos,\ntext etc)\nIR Systems\n(SMART,\nINQUERY)\nIndexer (stores info\nsymbols)\nOntology Module\nQuery Processing Module\nResults User\nFigure2 (b): Proposed High- level Statistical Multimedia IR Model\nThe model has following aspects:\n Improves user expressiveness: - It analyses terms that have close meaning to user\u201fs query and\nexpressive results are presented to user.\n Supports different modules: - Several modules like Ontology Module, Extraction Module, Query\nExpansion and Refinement have been introduced in proposed model.\n Low Computation and Cost: - The approaches that are used to extract terms from documents are so\nefficient that they takes into account only relevant terms and discards non relevant terms. Only relevant terms are expanded and it leads to saving of time and work.\nRepresentation of Extracted Concepts / Terms in hierarchical manner\nCreation of Ontology for\ngiven document Di.\nGathering information of\nrelevant extracted terms.\nOntology Builder\n(Classification\nAlgorithm).\nGenerate classes from\nInference Network graph using\nOWL and XML classes.\nCreation of new\ndocuments\nExtraction of new relevant\nterms and maintaining\nsemantic associations\nOntology Phrase Extractor\nINDEXING\nQuery Processing Query Expansion Query Transformation\nRules 1\u2026\u2026\u2026\u2026\u2026\u2026\u2026n\nTransformed queries 1\u2026\u2026\u2026\u2026..n Uses methods namely Sketch Retrieval, Search by\nkeyword, Search by example, Adaptive retrieval,\nLocal Context Analysis (LCA)\nQuery Refinement Calculation of new\nand old weights\nDummy document\nRetrieval Module\nRanks document according\nto similarity metrics Application\nRe-Use of queries. The queries that are captured are stored in\nquery database for future use.\n Good Retrieval Accuracy: - The model retrieves only those terms from documents that satisfies\nuser\u201fs information needs.\n Pipelining Facility: - Pipelining means dividing of complex tasks into certain number of independent\nsub tasks that performs parallel to each other. It helps in extraction of text- image documents by dividing into smaller segments. Each segment holds some information. As soon as each part is analyzed, terms from different segments are retrieved and combined to produce full document.\nOriginal Image\nImage terms (Segment 1) Image terns (Segment 2) Image terms (Segment 3)\nFigure3: Extraction of Image Terms [7]"}, {"heading": "3.1 Multimedia Document Analysis Module", "text": "There is large number of multimedia documents consisting of video text collections on web. The IR systems used in model performs structural analysis of documents and extracts textimage terms from them [8]. At this stage, it is not possible to\nfully determine that random chosen documents are relevant or non relevant. The classical model works on low level multimedia analysis. The proposed multimedia model works on high level multimedia analysis algorithm rather than lowlevel analysis because of following reasons:\nTable1: Features of High \u2013 Level Multimedia Analysis\nLow- Level Multimedia Analysis High- Level Multimedia Analysis\n1. It produces low level features like text and image terms. 1. It produces high level features like it describes concepts\nassociated with extracted low level text terms.\n2. It only extracts relevant terms from information that is stored in information database on system. 2. It extracts terms from derived documents even if information is not stored on system. 3. It uses information symbols to build the index of multimedia documents. These symbols may or may not specify given concepts. 3. It uses keywords that are related to document and always show presence of concepts that are described by terms in given document."}, {"heading": "3.2 Indexing Module", "text": "The terms and information symbols are extracted in previous module. The storage location of these extracted terms (relevant/ irrelevant) is decided by Indexing Module. It is done with the help of Indexer that stores the generated terms. This module has capability to store high dimensional information i.e. it can also solve structured indexes or trees along with information symbols."}, {"heading": "3.3 Extraction Module", "text": "This module uses two or more statistical approaches for extraction of relevant terms or phrases from retrieved documents. It is a module that determines the relevance of IR system. It is able to provide distinction between relevant and non relevant terms on basis of results produced by statistical approaches. The approached are discussed in following sub sections:"}, {"heading": "3.3.1 Extended Boolean Approach", "text": "Problem: - The classical Boolean condition i.e. True/ False produces both relevant and non relevant results. They supply given solution in response to whole document. If some text or image terms are relevant in document and some are not relevant, then Boolean condition leads to irrelevant results because it considers whole document.\nSolution: - Extended Boolean Approach\nAnalysis: - A number of extended Boolean models have been developed to provide ranked output of results i.e. the documents that satisfies user\u201fs query. These models use extended Boolean operators that are called as Soft Boolean Operators for finding relevant text- image pairs. This approach assigned different weights to different terms and computes relevance.\nThe classical Boolean operators are different from Soft operators as follows:\nTable2: Classical Operators vs. Soft Operators\nClassical Boolean Operator Extended Operators (Soft Operators)\nIt evaluates its terms to return two values only i.e. True/\nFalse. The values are represented by zero (False) and 1\n(True) respectively. It is represented by truth tables\ngraphically.\nIt evaluates its items to number on basis of degree to\nwhich condition matches document i.e. If condition\nmatches document, then it returns 1 else 0. If some part\nsatisfies condition while other part does not, then the\nvalue is in fraction. It means soft operators do not leave\ndocument as irrelevant.\nExample of Extended Boolean approach is p-norm model.\nP-Norm Model: - The model performs evaluation if and only if terms satisfy user\u201fs query in accordance with user\u201fs views. The model uses two functions AND, OR for finding similar documents and terms. Consider a query that has n terms given by q1, q2, q3 \u2026.qn-1, qn with corresponding weights wq1, wq2, wq3\u2026\u2026..wqn-1, wqn in a given document Di. The document is also assigned weights as wd1, wd2, wd3\u2026\u2026\u2026..wdn-1, wdn.\nFirstly, the extended Boolean function AND finds similar documents by combining (AND) query terms together. Then, terms are retrieved from those documents that satisfy user needs. AND function follows condition that all components must be present in order to return relevant (non zero) terms. If any component is absent, then it will give zero values.\n(1) SAND (d (q1, wq1) AND \u2026\u2026\u2026\u2026. AND (qn, wqn)) = 1 \u2013 [(\u2211(1-wdi) p * (wq1) p) / (\u2211 (wq) p)] 1/p\nWhere 1\u2264p\u2264\u221e and SAND = Similar documents retrieved using AND function.\nThe Extended Boolean OR function finds similar documents with query that add (OR) the query terms together.\n(2) SOR (d (q1, wq1) OR \u2026\u2026\u2026\u2026. OR (qn, wqn)) = [(\u2211(wdi) p * (wq1) p) / (\u2211 (wq) p)] 1/p\nWhere 1\u2264p\u2264\u221e and SOR = Similar documents retrieved using OR function\nSo, we conclude that p- norm model returns n relevant multimedia terms instead of binary terms. It reduces system time and increases performance.\nDrawbacks: Extended Boolean approach fails in extracting relevant terms from given n terms. P- norm model assigns weights to query terms as well as document terms. Both queries are treated equally because p \u2013 norm functions evaluate all term weights in a same way. It cannot distinguish between relevant and non- relevant terms. The solution to this problem lies in usage of probabilistic statistical IR approaches."}, {"heading": "3.3.2 Bayesian Probability Models / Conditional Probability Models", "text": "Bayesian models give relationship between probability of random selected documents and probability that given document is relevant. In such case, we are aware of features of document (image terms, text, statistics, phrases etc) and then calculate its probability. Following are features of probabilistic models: -\n They are related to prior and posterior probabilities. Prior means finding probability as earliest as\npossible without knowing features of document. Posterior means finding probability after examining\nthe features of document. Prior Probability + Posterior Probability = 1\n Conditional Models are also called as Probability Kinematics model that is defined as flow of\nprobabilities of relevant terms to non relevant terms in whole document.\n It uses concept of Inverse Document Frequency (idf) for determining number of relevant terms by\nusing formula as:\nIdf = ln N/n where N= No of total documents, n = No of relevant documents\n Probabilistic models helps in achieving relevance on basis of values estimated for different documents.\nThe statistical probabilistic models [9] are categorized into two parts:\n(a) Binary Independence Model (BI): - The model in which each text- image term (relevant/ irrelevant) is independent of other text- image pairs in collection of documents is called BI model. So, the probability of any relevant/ irrelevant term is independent of probability of any other terms in documents.\nBI model is also called Relevance Weighting Theory. It says that each term is given weight that is used to rank documents on basis of relevance, thus extracting relevant terms. Weights are assigned by product of Term Frequency and Inverse Term Frequency i.e. (tf * idf) when we are taking random collection of documents. Term Frequency (tf) means number of terms occurred in document. So, tf varies from one document to another whereas Inverse Document Frequency (idf) measures how many times the given term occurs in document. It gives probability of terms occurred in a document.\nConsider number of finite terms tk in document di. Each term is assigned different weights wk that is to be calculated according to given formula:\nWk = log [Pk (1- Uk) / Uk (1 \u2013 Pk)] (When we are given set of data terms)\nWhere Pk = Probability of term tk occurring in relevant documents\nUk = Probability of term tk occurring in non relevant documents\nWk = Weight to each term. It is defined as measure of distinguishing relevant terms from non relevant terms. It is also called as Term Relevance Weight or Log Odds Function.\nOdd ratio is calculated on basis of likelihood of terms in relevant documents as well as in non relevant documents. Let likelihood of terms in relevant documents is X = (Pk / 1- Pk) and in non relevant documents Y = (Uk / 1-Uk). Then Wk is given by X / Y. Wk is zero if Pk = UK, Wk > 0 if Pk > Uk\nThe model concludes that the terms which occur many times in single document is relevant but if same terms occur in large\nnumber of large number of documents , then it is not relevant. So, a weight function is developed that varies from idf to Wk formula.\nLimitation of this model: - It is not able to distinguish between low frequency terms and high frequency terms in context of weights. It gives weight of low frequency terms as same as those of high frequency terms. It does not able to extract terms from multiple queries also. So, to overcome these problems, we have used Inference Network Model.\n(b) Bayesian Inference Network Model It is one of statistical approach for extraction of terms from multimedia documents with the help of constructing graph called as Inference Network Graph. Besides computing probabilities for different nodes, this model also determines concepts between various retrieved terms. It provides surety that user needs are fulfilled because it also combines multiple sources of evidence regarding relevance of document to user query.\nGraph Structure: - Inference Network is a graph that has nodes connected by edges. Nodes represent True/ false\nstatements describing term is relevant or not. A graph has following elements:\n Document Nodes (Dn) : - They are called Root Nodes\n Text Nodes (Tn): - They are child nodes of document nodes. It may include audio, video nodes,\ntext image nodes etc. So, child nodes have multiple representations of document.\n Concept Representation Nodes (CRn): - They are child of text nodes. The concepts used in terms that\nare in text nodes are represented by CR nodes. These nodes are index terms or keywords that are matched in document and retrieves relevant terms.\n Document Network: - It is network consisting of Document nodes, Text nodes, and CR nodes. It is\nnot tree as it has multiple roots and nodes. Document Network is Directed Acyclic Graph (DAG) since it has no loop. The representation of document network for different documents from D1 to Dn is shown as:\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\nFigure4: Document Network (It describes concepts used in multiple terms from different documents)\n Query Network: - Since we have extracted concepts in Document Network, it is possible that different\nconcepts are used in same query nodes or different concepts in different nodes. The concepts that\ndescribe relevant terms are shown in form of results and presented to user.\nThe representation of query network for different query nodes from Q1 to Qn is shown as:\n\u2026\u2026\u2026.............\u2026\u2026\u2026\u2026\u2026.\u2026\u2026\u2026..\nQuery Nodes\nLeaf Nodes (Results)\nFigure5: Query Network (It describes generation of results (leaf nodes))\nWhen we combine document network and query network, we get inference graph. This graph computes probabilities of terms contained in child nodes of document nodes and so on. It is done by using LINK MATRIX. Each node is assigned with its weight in each row of matrix. The column represents number of possible combinations a node can have.\nIn link matrix, Number of parents = 2n Number of columns. If node has 3 parents, then there will be 8 columns. Then, probabilities and weight function are computed for all 8 columns of matrix. Each probability is multiplied by its weight and then all eight probabilities are added to get total\nprobability of their respective parents\u201f node. Consider combination of 110 (1 stands for True, 0 for False). The probability for combination is calculated as P1 * P2 * (1-P3). Weight function for such combination is (W1 + W2) / (W1 + W2 + W3). Total probability is calculated as P1 * P2 * (1-P3) * (W1 + W2) / (W1 + W2 + W3)."}, {"heading": "3.4 Ontology Module", "text": "This module is used to represent concepts and conceptual relationships among nodes that are described by inference network graph in previous module using concept of ontology. Ontology is defined as Formal, Explicit, and Shared\nD1\nT1\nCR1\nD2\nT2\nCR2\nDn-1\nTn-1\nCRn-1\nDn\nTn\nCRn"}, {"heading": "CR1 CR2", "text": "Q1\nr2\nCRn-1\nQn-1\nrn\nQn\nCRn\nr1\nConceptualization of concepts, thus organizing them in hierarchical fashion [10]. Various phases of ontology module are described below: - (a) Creation of Ontology or Ontology Representation: - Inference Graph consists of document nodes ( root nodes).\nEach document node has concept nodes that are treated as Vertices. An edge from one node to other node represents relationship among concepts.\nDi has concept nodes as CRi. Edges represent relationship between them\n(b) Ontology Building: - It uses an algorithm for developing ontology for inference graph. It requires use of OWL (Ontology Web Language) that is used for writing ontology. It is used for creating objects of each class.\nBEGIN\nFor each vertices V of inference graph G\nClass C = new (owl: class)\nC.Id = C.label // each concept has its unique identification and name//\nDatatypeProperty DP = new (owl: DatatypeProperty) // DatatypeProperty of parent node means what should be type of values in child nodes. It is also called Content Description//\nDP.Id = DP.Name, DP.Value;\nDP.AddDomain (C); // It adds values of child nodes to given concept node C//\nFor each edge E of Graph G\nDP.AddDomain (B.getClass ()) // getClass is used to show relationship between concepts//\nEnd for\nEnd begin\n(c) Generation of OWL class\nClass Result = new (owl: class) // Result represents leaf nodes//\nResult.Id = Result. Name\nDatatypeProperty ResultDP = new (owl: DatatypeProperty) // to show value of leaf nodes//\nResultDP.Id = Result.Name, Result. Value; // Leaf nodes have name and value//\nResult.AddDomain (Result)\nFor each edge E of Graph G\nClass Relationship = new (owl: class)"}, {"heading": "Relationship.Id= \u201c \u201c", "text": "For each vertices of graph\nRelationship.Id= Relationship.Id + C.label;\nEnd for\nResultDP.AddDomain (Relationship)\nEnd for"}, {"heading": "3.5 Query Processing Module", "text": "A query is called information need. It is final result with optimal and effective terms. This module deals with expansion and refinement of query either automatically or manually with user interaction. It analyze query according to query language, extract information symbols from it and pass it to Retrieval Module for searching index terms.\nQuery Expansion through manual methods: It includes:\n Sketch Retrieval: - It is one of methods to query a multimedia database. With t is, user query is visual\nsketch given by user, and then system processes this drawing to extract its features and searches the index for similar images.\n Search by Example: - In this, user gives query as an example of image that he tends to find. A query\nthen extracts low level features.\n Search by Keyword: - It is most popular method. User describes information with set of relevant\nterms and system searches it in documents.\nQuery Expansion through automatic method: - It includes Local Context Analysis (LCA) approach.\nIt is one of best methods for automatic query expansion. It expands terms from query, rank and weights them by using certain formula."}, {"heading": "LCA = Local Feedback Analysis + Global Analysis", "text": "It is local because concept relevant terms are only retrieved from globally retrieved documents. It is global because documents related to given query topic are selected randomly from huge collection of documents present on web (like we have selected three documents related to semantic web from web). When we put query in Google and press ENTER, query is executed and it retrieves some documents. It is global activity. LCA is concept based fixed length scheme. It expands user query and retrieves top n relevant terms that closely satisfies query. It returns only fixed number of terms.\nThe retrieved terms are ranked accordingly as:\nBelief (Q, C) =  [ + log (af(c, ta)) idfc / log (n)] idfa\nWhere C= Concepts related to query Q\nBelief (Q, C) = Ranking Function\nDi\nTi\nCRi\nta = Query Term\naf(c, ta) = fta1d1 * fc1d1 + fta2d2 * fc2d2 + fta3d3 * fc3d3 + \u2026\u2026\u2026\u2026\u2026 ft an, dn * f cn, dn\nd=n\naf(c, ta) = \u2211 ftad fcd\nd=1\nWhere d = documents from 1 to n\nftad = Frequency of occurrence of query term ta in document d\nfcd = Frequency of concepts (terms related to query) in document d\nidfc = It measures importance of concepts related to query terms i.e. how many times the same concept is used in document\nidfa = It measures importance of query terms ta.\n = It is constant used for distinguishing between relevant and non relevant terms. It stores non relevant terms that are treated as constant."}, {"heading": "3.5.1 Query Refinement", "text": "A tern can have different weights in each relevant document, so there is need to refine query. Query Refinement means calculation of old weights of expanded query terns in order to produce new weights of same query terns. These query terms are transformed into dummy document that is used for Indexing.\nHere is formula used that calculates new weights of query terms and produces optimal results by discarding non relevant terms. It is called Rocchio Formula.\nAim: - The aim of this formula is to increase weights of terms that occur in relevant documents and decrease the weights of terms occurring in non relevant documents.\nEquation: -\nQa (new) = x * Qa (old) + y * 1/ (RD) * \u2211 wtaRD \u2013 z * 1/ (NRD) * \u2211 wtaNRD\nWhere Qa (new) = New weight of query term a\nQa (old) = old weights of tern s\nRD = Relevant documents judged by user\nNRD = Non- Relevant documents judged by user\nwtaRD = Weights of terms in relevant documents\nwtaNRD = Weights of terms in non relevant documents\n\u2211 wtaRD = All weights of RD are added together\n\u2211 wtaNRD = All weights of NRD are added together\ny = It is constant that gives average of weights of terms in RD\nz = It is constant that gives average of weights of terms in NRD. The result is that we get negative weights and they will be discarded automatically."}, {"heading": "3.6 Retrieval Module", "text": "It is module that retrieves final results/optimal queries that have been extracted after going through various phases. It ranks document according to similar queries and maintains index according to information symbols contained in that query."}, {"heading": "3.6.1 Re-Use of Queries", "text": "Need for Re-Use of Queries: - The queries that were already expanded and refined according to user\u201fs requirements are optimized and stored anywhere. If user needs some information ion future, then what is way to retrieve those documents that satisfies query?\nSolution: - Re-Use of queries.\nAnalysis: - The expanded and refined queries are stored in database that is called as Query database. The query base contains queries related to previously retrieved documents. These queries are called Persistent Queries."}, {"heading": "How to Use Persistent Queries with new Query?", "text": "(a) If a new query is somewhat similar to persistent query, then result of new query is related to persistent query.\n(b) If user new query is not similar to persistent query in any way, then system has to find persistent query from database that satisfies new query to some extent."}, {"heading": "How to check similar queries?", "text": "Using concept of Solution Region: - When search for an optimal query begins, system retrieves number of queries instead of only one query. All those queries are described in query space. The region containing that query space is called Solution Region.\nWe can check similarity between queries as the new queries are compared with queries in solution region and if they get matched, then both queries are said to be similar."}, {"heading": "4. EXPERIMENTAL ANALYSIS AND CALCULATIONS", "text": "Consider a given sets of data. We have to compute probabilities of relevant and non relevant terms and hence calculate weight function for each term.\nGiven data: Total number of relevant documents (R) = 10\nDocuments with Documents without\nTerm tk ( r) = 4 Term tk (R \u2013 r) = 6\nTotal no. of documents having terms tk\nn-r + r = 9\nn = 9\nTotal number of Non relevant documents (N-r) = 15\nDocuments with Documents without Term tk (n-r) = 5 Term tk (N-r) \u2013 (n-r) = 10\nTotal no. of documents without term tk (N-r) \u2013 (n-r) + (Rr) = 16\nAccording to BI model, Total number of documents N= 25 Total number of documents with term tk (n) = 9 Total number of relevant documents (R) = 10 Total number of relevant documents with term tk (r) = 4 From above data, Pk = Probability of term tk occurring in relevant documents\n= 4/10 = 2/5\nUk = Probability of term tk occurring in non - relevant documents\n= 5/15 = 1/3\nX = Pk / (1- Pk) = (2/5)/ (3/5) = 2/3 Y = Uk / (1 \u2013 Uk)\n= (1/3) / (2/3) = \u00bd\nOdd Ratio or Weighting Function Wk = X/Y = 4/3 Ranking function W = log (X/Y) = log (4/3) = 0.20068\nFigure6: - Computation of Probabilities of terms graphically\nOn the basis of above graph and probability values, we can find new weight function for terms from old weight function by using Rocchio Formula.\nQa (new) = x * Qa (old) + y * 1/ (RD) * \u2211 wtaRD \u2013 z * 1/ (NRD) * \u2211 wtaNRD\nHere Qa (old) = 4/3\nRelevant Documents (RD) = 10\nNon relevant documents (NRD) = 15\n\u2211 wtaRD = 4 + 6 = 10\n\u2211 wtaNRD = 5 + 10 = 15\nX = 1, y = (4+6) / 2 = 5, z = (5+10) / 2 = 15/2 = 7.5\nSo, Qa (new) = 1 * (4/3) + 5 * (1/10) * 10 \u2013 7.5 * (1/15) * 15\n= 4/3 + 5 \u2013 7.5 = - 3.7\nSince new weight function is negative, so it is discarded and old function is considered as relevance function.\nCatchy Concept: - The proposed High-Level Statistical Multimedia IR Model deals with the queries that have been expanded and refined according to user's requirements. In this way the queries can be reused. It is good idea if given queries is short. HOW THIS MODEL CAN BECOME SUITABLE FOR LONG QUERIES ALSO?\nCatchy Answer: - The answer to above question is Use of Random Variables. These variables may be continuous as well as Discrete. The terms that are found in multimedia text documents can be treated as variables. If the terms are short or finite, then it is solved using concept of Discrete Random Variables. It simply means adding product of probabilities of various terms/queries used in document. In this way, expected value [E] of term function is calculated and we can determine its relevance.\nFor long queries, the concept of Continuous Random Variables can be used. Further, long queries may have some limit or they are infinite. For queries having limit, approximation is used. The terms are integrated to particular interval and produce results proximity to user\u201fs requirements.\nFor infinite long queries, various methods of calculating expected value [E] like Poisson distribution, Binomial distribution are employed.\nIn this way, both short queries as well as long queries can be reused and expanded."}, {"heading": "5. CONCLUSION", "text": "The paper illustrates the working of proposed high level multimedia IR model consisting of various modules. Each module is described separately. This module provides extraction of relevant terms from huge collection of multimedia documents. Since multimedia documents produce information tokens that are different from text tokens, so those statistical approaches are shown in paper that analyses multimedia document and retrieves multimedia terms (text, images, and videos) from them.\nThe new model can replace ambiguities of traditional multimedia IR model that deals with information symbols only instead of maintaining relationships between them. It is beneficial in various aspects like there is module introduced in it for maintaining conceptual relationships between extracted terms and represents them using ontology. The model uses probabilistic approaches for calculating ranking of documents and retrieves optimal queries. The results are then presented to user."}, {"heading": "6. REFERENCES", "text": "[1] International Press Telecommunications Council: \\IPTC\nCore\" Schema for XMP Version 1.0 Specification document (2005)\n[2] Technical Standardization Committee on AV & IT Storage Systems and Equipment: Exchangeable image\n_le format for digital still cameras: Exif Version 2.2. Technical Report JEITA CP-3451 (April 2002)\n[3] Borgo, S., Masolo, C.: Foundational choices in DOLCE. In: Handbook on Ontologies. 2nd edn. Springer (2009)\n[4] Joao Miguel Costa Magalhaes: \u201eStatistical Models for Semantic \u2013 Multimedia Information Retrieval\u201f,\nSeptember 2008.\n0\n10\n20\n30\n0 5 10 15\nN o\nn R\nel ev\na n t D o cu m en ts\nRelevant Documents\nGraph for BI model\nTotal no. of relevant documents\nTotal no. of non relevant documents\nTotal no of documents\n[5] Meghini C, Sebastiani F, and Straccia U: \u201eA model of multimedia information retrieval\u201f Journal of ACM\n(JACM), 48(5), pages 909\u2013970, 2001.\n[6] Grosky, W.I., Zhao, R.: \u201eNegotiating the semantic gap: From feature maps to semantic landscape\u201f, Lecture Notes\nin Computer Science 2234 (2001).\n[7] Adams, W. H., Iyengart, G., Lin, C. Y., Naphade, M. R., Neti, C., Nock, H. J., and Smith, J.:\u201f Semantic indexing\nof multimedia content using visual, audio and text cues\u201f EURASIP Journal on Applied Signal Processing 2003 (2), pages 170-185.\n[8] Datta, R., Joshi, D., Li, J., and Wang, J. Z.: \u201eImage retrieval: ideas, influences, and trends of the new age\u201f\nACM Computing Surveys, 2008.\n[9] Hofmann, T., and Puzicha: \u201eStatistical models for cooccurrence data. Technical Report\u201f, Massachusetts\nInstitute of Technology, 1998\n[10] M. Preethi, Dr. J. Akilandeswari,: \u201eCombining Retrieval with Ontology Browsing\u201f, International Journal of\nInternet Computing, Vol.1, Issue-1\u201d, 2011\n[11] Croft, W. B., Turtle, H. R., and Lewis, D. D.: \u201eThe use of phrases and structured queries in information retrieval\u201f,\nIn ACM SIGIR Conf. on research and development in information retrieval, Chicago, Illinois, United States 2004\n[12] Rifat Ozcan, Y. Alp: \u201eConcept Based Information Access using Ontologies and Latent Semantic Analysis\u201f,\nTechnical Report, 2004-08.\n[13] F. Crestani, M. Lalmas, C.J. van Rijsbergen, and I. Campbell: \u201eIs this document relevant? . . . Probably: A\nsurvey of probabilistic models in information retrieval\u201f, ACM Computing Surveys, 30(4), pages 528- 552, December 1998.\n[14] Manning C.D., Raghavan P., and Schu\u00a8tze H: \u201eAn Introduction to Information Retrieval\u201f, Cambridge\nUniversity Press, Cambridge, 2007.\n[15] CAI, D., Yu, S. Wen, J.-R., and Ma, W.-Y: \u201eExtracting content structure for Web pages based on visual\nRepresentation\u201f. In Asia Pacific Web Conference 2003\n[16] Metzler, D. Manmatha, R: \u201eAn inference network approach to image retrieval\u201f, In Enser, P.G.B.,\nKompatsiaris, Y., O\u201fConnor, N.E. Smeaton, A.F. Smeulders, A.W.M., eds.: CIVR. Volume 3115 of Lecture Notes in Computer Science. Springer (2004) 42\u2013 50.\n[17] Faloutsos C., Barber R., Flickner M., Hafner J., and Niblack W: \u201eEfficient and effective querying by image\ncontent\u201f, J. Intell. Inform. Syst., 3:231\u2013262, 1994.\n[18] Ed Greengrass: \u201eInformation Retrieval: A Survey\u201f, November 2000\n[19] O.S. Al- Kadi: \u201eCombined Statistical and Model based texture features for improved image classification\u201f,\n4th IET International Conference on Advances in Medical, Signal and Information Processing (MEDSIP 2008), January 2008 page 314.\n[20] S.Vigneshwari, M.Aramudhan: \u201eAn Ontological Approach for effective knowledge engineering\u201f,\nInternational Conference on Software Engineering and Mobile Application Modeling and Development (ICSEMA 2012), January 2012 page 5.\n[21] M.A. Moraga, C.Calero, and M.F. Bertoa: \u201eImproving interpretation of component-based systems quality\nthrough visualization techniques\u201f, IET Software, Volume 4, Issue 1, February 2010, p. 79 \u2013 90, DOI: 10.1049/ietsen.2008.0056,Print ISSN 1751-8806, Online ISSN 1751-8814.\n[22] Michael S.Lew, Nicu Sebu, Chabane Djeraba and Ramesh Jain: \u201eContent-based Multimedia Information\nRetrieval: State of Art and Challenges\u201f, In ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), Feb 2006.\n[23] Alberto Del Bimbo, Pietro Pala: \u201eContent- based retrieval of 3D Models\u201f, In ACM Transactions on Multimedia\nComputing, Communications, and Applications (TOMCCAP), Vol. 2 Issue 1, Feb 2006, Pages 20-43.\n[24] Carlo Meghini, Fabrizio Sebastiani and Umberto Straccia: \u201eA model of multimedia information retrieval\u201f,\nJournal of ACM (JACM), Vol 48, Issue 5 September 2001, Pages 909-970.\n[25] Simone Sanitini: \u201eEfficient Computation of queries on feature streams\u201f, In ACM Transactions on Multimedia\nComputing, Communications, and Applications (TOMCCAP), Vol. 7 Issue 4, November 2011, Article No. 38\n[26] Graham Bennett, Falk Scholer and Alexandra: \u201eA comparative study of probabilistic and language models\nfor information retrieval\u201f, In Proceedings of nineteenth conference on Australian database ADC\u201f08, Vol.75 ISBN: 978-1-920682-56-9, Pages 65-74."}, {"heading": "ABOUT THE AUTHORS", "text": "Gagandeep Singh has completed his B.Tech (CSE) from GTBIT affiliated to Guru Gobind Singh Indraprastha University, Delhi. His Research areas include Semantic Web, Information Retrieval, Data Mining, Remote Sensing (GIS) and Knowledge Engineering.\nVishal Jain has completed his M.Tech (CSE) from USIT, Guru Gobind Singh Indraprastha University, Delhi and doing PhD in Computer Science and Engineering Department, Lingaya\u201fs University, Faridabad. Presently, He is working as Assistant Professor in Bharati Vidyapeeth\u201fs Institute of Computer Applications and Management, (BVICAM), New Delhi. His research area includes Web Technology, Semantic Web and Information Retrieval. He is also associated with CSI, ISTE.\n.\nIJCATM : www.ijcaonline.org"}], "references": [{"title": "Foundational choices in DOLCE", "author": ["S. Borgo", "C. Masolo"], "venue": "Handbook on Ontologies. 2nd edn. Springer", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Statistical Models for Semantic \u2013 Multimedia Information Retrieval", "author": ["Joao Miguel Costa Magalhaes"], "venue": "Total no of documents  International Journal of Computer Applications", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A model of multimedia information retrieval", "author": ["C Meghini", "F Sebastiani", "U Straccia"], "venue": "Journal of ACM (JACM),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Negotiating the semantic gap: From feature maps to semantic landscape", "author": ["W.I. Grosky", "R. Zhao"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Semantic indexing of multimedia content using visual, audio and text cues", "author": ["W.H. Adams", "G. Iyengart", "C.Y. Lin", "M.R. Naphade", "C. Neti", "H.J. Nock", "Smith"], "venue": "EURASIP Journal on Applied Signal Processing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Image retrieval: ideas, influences, and trends of the new age", "author": ["R. Datta", "D. Joshi", "J. Li", "J.Z. Wang"], "venue": "ACM Computing Surveys,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Statistical models for cooccurrence data", "author": ["T. Hofmann", "Puzicha"], "venue": "Technical Report\u201f, Massachusetts Institute of Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Combining Retrieval with Ontology Browsing", "author": ["M. Preethi", "Dr. J. Akilandeswari"], "venue": "International Journal of Internet Computing, Vol.1,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The use of phrases and structured queries in information retrieval", "author": ["W.B. Croft", "H.R. Turtle", "D.D. Lewis"], "venue": "In ACM SIGIR Conf. on research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Concept Based Information Access using Ontologies and Latent Semantic Analysis", "author": ["Rifat Ozcan", "Y. Alp"], "venue": "Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Is this document relevant", "author": ["F. Crestani", "M. Lalmas", "C.J. van Rijsbergen", "I. Campbell"], "venue": "ACM Computing Surveys,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "An Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H Schu \u0308tze"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Extracting content structure for Web pages based on visual Representation", "author": ["D. CAI", "Yu", "S. Wen", "J.-R", "Ma", "W.-Y"], "venue": "In Asia Pacific Web Conference", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "An inference network approach to image retrieval", "author": ["Metzler", "R D. Manmatha"], "venue": "In Enser,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Efficient and effective querying by image content", "author": ["C. Faloutsos", "R. Barber", "M. Flickner", "J. Hafner", "W Niblack"], "venue": "J. Intell. Inform. Syst.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Kadi: \u201eCombined Statistical and Model based texture features for improved image classification", "author": ["O.S. Al"], "venue": "IET International Conference on Advances in Medical, Signal and Information Processing (MEDSIP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Bertoa: \u201eImproving interpretation of component-based systems quality through visualization techniques", "author": ["M.A. Moraga", "C.Calero", "M.F"], "venue": "IET Software,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Content-based Multimedia Information Retrieval: State of Art and Challenges", "author": ["Michael S.Lew", "Nicu Sebu", "Chabane Djeraba", "Ramesh Jain"], "venue": "In ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Content- based retrieval of 3D Models", "author": ["Alberto Del Bimbo", "Pietro Pala"], "venue": "In ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Straccia: \u201eA model of multimedia information retrieval", "author": ["Carlo Meghini", "Fabrizio Sebastiani", "Umberto"], "venue": "Journal of ACM (JACM),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Efficient Computation of queries on feature streams", "author": ["Simone Sanitini"], "venue": "In ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A comparative study of probabilistic and language models for information retrieval", "author": ["Graham Bennett", "Falk Scholer", "Alexandra"], "venue": "Proceedings of nineteenth conference on Australian database ADC\u201f08,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2068}], "referenceMentions": [{"referenceID": 0, "context": "The third model developed was Dublin Core [3] that deals with semantic as well as structural content of image and text but it failed to depict relationship between text and image.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "The salient features of multimedia information [4] are given below:", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Figure1: A Classical Multimedia IR Model [5]", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "\uf0b7 It creates semantic gap [6] between user and system due to availability of irrelevant and superfluous information terms stored in information database of IR system.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Figure3: Extraction of Image Terms [7]", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "The IR systems used in model performs structural analysis of documents and extracts textimage terms from them [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "The statistical probabilistic models [9] are categorized into two parts:", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "42 Conceptualization of concepts, thus organizing them in hierarchical fashion [10].", "startOffset": 79, "endOffset": 83}], "year": 2014, "abstractText": "The process of retrieval of relevant information from massive collection of documents, either multimedia or text documents is still a cumbersome task. Multimedia documents include various elements of different data types including visible and audible data types (text, images and video documents), structural elements as well as interactive elements. In this paper, we have proposed a statistical high level multimedia IR model that is unaware of the shortcomings caused by classical statistical model. It involves use of ontology and different statistical IR approaches (Extended Boolean Approach, Bayesian Network Model etc) for representation of extracted text-image terms or phrases. A typical IR system that delivers and stores information is affected by problem of matching between user query and available content on web. Use of Ontology represents the extracted terms in form of network graph consisting of nodes, edges, index terms etc. The above mentioned IR approaches provide relevance thus satisfying user\u201fs query. The paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas. The proposed model developed reduces semantic gap and satisfies user needs efficiently. Index Terms Information Retrieval (IR), OWL, Statistical Approaches (BI model, Extended Boolean Approach, Bayesian Network Model), Query Expansion and Refinement. State of Art Research on multimedia information retrieval seems to be gargantuan and challenging task. Its areas are so diversified that it has lead to independent research in its own components. Firstly, there used to be human centered systems that focus on user\u201fs behavior and needs. Various experiments and studies were conducted in lieu of these systems. The users were asked to present a set of valuable things in daily life. It was done on similarity of users. Some of choices are same while some are different. Few of them prefer to use images instead of text caption. In further experiments, it was noticed that new users were taking feedback from previous users. It leads to concept of relevance feedback module in information model. In early years, most research was done on contentbased image retrieval. The existing models are of different level and scope. These models are semantically unambiguous. For e.g.: IPTC model [1] uses location fields that focus on location of data but this model also failed due to lack of statistical approach. Another metadata model was developed i.e. EXIF [2] to support features of images but it did not tell anything about relationship and associations between different contents of image. It also resulted in vain. The third model developed was Dublin Core [3] that deals with semantic as well as structural content of image and text but it failed to depict relationship between text and image. With advancement in technology and predictions, some probabilistic and futuristic models were also developed. In following paper, statistical multimedia IR model has been proposed and compared with classical multimedia IR model.", "creator": "Microsoft\u00ae Office Word 2007"}}}