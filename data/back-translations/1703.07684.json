{"id": "1703.07684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Predicting Deeper into the Future of Semantic Segmentation", "abstract": "The ability to predict and thus anticipate the future is an important feature of intelligence. It is also of utmost importance in real-time systems, such as robotics or autonomous driving, which depend on visual understanding of the scene when making decisions. While predicting the raw RGB pixel values in future video images has already been studied in previous work, here we are focusing on predicting semantic segmentation of future frames. More specifically: Given a sequence of semantically segmented video images, our goal is to predict segmentation maps of previously unobserved video images that lie up to a second or further in the future. We are developing an authoregressive revolutionary neural network that learns to iteratively generate multiple frames. Our results on cityscapes datasets show that the direct prediction of future segmentation is significantly better than predicting the segmentation of two recent segments and the subsequent segmentation of the second half of the most recent segments (and the subsequent segmentation of the second half of the most recent cars).", "histories": [["v1", "Wed, 22 Mar 2017 14:45:15 GMT  (3838kb,D)", "http://arxiv.org/abs/1703.07684v1", null], ["v2", "Tue, 28 Mar 2017 13:54:24 GMT  (3839kb,D)", "http://arxiv.org/abs/1703.07684v2", null], ["v3", "Tue, 8 Aug 2017 10:02:36 GMT  (3926kb,D)", "http://arxiv.org/abs/1703.07684v3", "Accepted to ICCV 2017. Supplementary material available on the authors' webpages"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["pauline luc", "natalia neverova", "camille couprie", "jakob verbeek", "yann lecun"], "accepted": false, "id": "1703.07684"}, "pdf": {"name": "1703.07684.pdf", "metadata": {"source": "CRF", "title": "Predicting Deeper into the Future of Semantic Segmentation", "authors": ["Natalia Neverova", "Pauline Luc", "Camille Couprie", "Yann LeCun"], "emails": ["neverova@fb.com", "paulineluc@fb.com", "coupriec@fb.com", "jakob.verbeek@inria.fr", "yann@fb.com"], "sections": [{"heading": "1. Introduction", "text": "Prediction and anticipation of future events is a key component to intelligent decision-making [27]. Building smarter robotic systems and autonomous vehicles implies making decisions based on the analysis of the current situ-\n\u2217These authors contributed equally\nation and hypotheses made on what could happen next [7]. While humans can predict vehicle or pedestrian trajectories effortlessly and at the reflex level, it remains an open challenge for current computer vision systems.\nThe task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22]. Modeling the raw RGB intensities, however, might be a task\n1\nar X\niv :1\n70 3.\n07 68\n4v 1\n[ cs\n.C V\n] 2\nthat is overly complicated as compared to predicting future high-level scene properties, while the latter is sufficient for many applications. Such high-level future prediction has been studied in various forms, e.g . by explicitly forecasting trajectories of people and other objects in future video frames [1, 17]. In our work we do not explicitly model objects or other scene elements, but instead model the dynamics of semantic segmentation maps of object categories implicitly using convolutional neural neural networks.\nSemantic segmentation is one of the most complete forms of visual scene understanding, where the goal is to label each pixel with the corresponding semantic label (e.g ., tree, pedestrian, car, etc .). In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames. See Figure 1 for an illustration.\nThe pixel-level annotations needed for semantic segmentation are expensive to acquire, and this is even worse if we need annotations for each video frame to learn models to predict future semantic segmentations. To alleviate this issue we rely on state-of-the-art semantic image segmentation models to label all frames in videos, and then learn our future segmentation prediction models from these automatically generated annotations.\nWe systematically study the effect of using RGB frames and/or segmentations as inputs and outputs for our models, and the impact of various loss functions. Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these. By moving away from raw RGB predictions and modeling pixel-level object labels instead, we believe that more of the network\u2019s modeling capacity is allocated to learn basic physics and object interaction dynamics. Moreover, artifacts in the predicted RGB frames can lead to poor performance of still image semantic segmentation models.\nIn this work we make three contributions:\n\u2022 Our approach is the first to predict future semantic segmentations without requiring extremely costly temporally dense video annotation. Its genericity allows different architectures for still-image segmentation and future segmentation prediction to be swapped in.\n\u2022 We propose an autoregressive training strategy that outperforms predicting multiple future frames in a single feed-forward batch model.\n\u2022 Our model convincingly predicts segmentations up to 0.5 seconds into the future, the mean IoU of our predictions reaches two thirds of the one obtained by a state-of-the-art semantic segmentation model [36]."}, {"heading": "2. Related work", "text": "Here we discuss the most relevant related work on video forecasting and on disambiguating learning under uncertainty, in particular using adversarial training.\nVideo forecasting. Several authors developed methods to improve the temporal stability of semantic video segmentation. Jin et al . [14] train a model to predict the semantic segmentation of the immediate next image from the preceding input frames, and fuse this prediction with the segmentation computed from the next input frame. Nilsson and Sminchisescu [23] use a convolutional RNN model with a spatial transformer component [13] to accumulate the information from past and future frames in order to improve prediction of the current frame segmentation. In a similar spirit, Patraucean et al . [24] employ a convolutional RNN to explicitly predict the optical flow, and use these to warp and aggregate per-frame segmentations. In contrast, our work is focused on predicting future segmentations without seeing the corresponding frames. Most importantly, we target a longer time horizon than a single frame.\nA second line of related work focuses on generative models for future video frame forecasting. Ranzato et al . [25] introduced the first baseline of next video frame prediction. Srivastava et al . [26] developed a Long Short Term Memory (LSTM) [12] architecture for the task, and demonstrated a gain in action classification using the learned features. Mathieu et al . [22] improved the predictions using a multi-scale convolutional architecture, adversarial training [10], and a gradient difference loss. To reduce the number of parameters to estimate, several authors re-parameterize the problem to predict frame transformations instead of raw pixels [9, 28]. Luo et al . [21] employ a convolutional LSTM architecture to predict sequences of up to eight frames of optical flow in RGBd videos. The video pixel network of Kalchbrenner et al . [15] also uses LSTMs, and factorizes the temporal and spatial/color dimensions. Rather than predicting pixels or flows, Vondrick et al . [29] instead predict features in future frames. They predict the activations in the penultimate layer of the AlexNet in future frames, and use these to predict the presence of human actions in these frames.\nLearning with uncertainty. Generative Adversarial Networks (GAN) [10] and Variational Auto-Encoders (VAE) [16] are recent unsupervised learning methods that can be used to deal with the inherent uncertainty in futureprediction tasks. GAN training has recently been improved using the Wasserstein GAN approach of Arjovsky and Bottou [2], and the energy based approach of Zhao et al . [37]. An interesting approach using GANs for unsupervised image representation learning is proposed in [5], where the generative model is trained along with an inference model that maps images to their latent representations. Vondrick\net al . [30] showed that GANs can be applied to video generation. They use a two-stream generative model: one stream generates a static background, while the other generates a dynamic foreground sequence which is pasted on the background. Yang et al . [35] use similar ideas to develop an iterative image generation model where objects are sequentially pasted on the image canvas using a recurrent GAN. Xue et al . [34] predict future video frames from a single given frame using a VAE approach. Similarly, Walker et al . [31] perform forecasting with a VAE, predicting feature point trajectories from still images."}, {"heading": "3. Predicting future frames and segmentations", "text": "This section first presents the different scenarios that we investigated to predict RGB pixel values and/or segmentations of the next frame. Then we describe two extensions of the single frame prediction model to predict further into the future."}, {"heading": "3.1. Single frame prediction models", "text": "Manual pixel-level supervision is laborious to acquire for still image semantic segmentation, and even more so for all frames in semantic video segmentation. To circumvent the need for datasets with per-frame annotations, we use the state-of-the-art multi-scale Dilated-10 semantic image segmentation network [36] to provide target semantic segmentations for all frames in each video. Then, we use the resulting temporally dense sequences of segmentations to learn our models.\nLet us denote withXi the i-th frame of a video sequence, and denote the sequences of frames fromXt toXT asXt:T . We denote by Si the semantic segmentation of frame Xi given the Dilated-10 network. We represent the segmentations Si using log-probabilities rather than the output of the softmax probabilities. This is motivated by recent observations in network distillation that the log-probabilities carry more information when training one network on the outputs of another network [3, 11]. For single-frame future prediction, we consider five different models that differ in whether they take RGB frames and/or segmentations as their input and output. We list these models in Table 1.\nArchitectures. Model X2X is a next-frame prediction\nmodel, for which we use the multi-scale network of Mathieu et al . [22], with two spatial scales. Noting C the number of output channels, each scale module corresponds to a simple four-layer convolutional network alternating convolutions and ReLU operations, containing feature maps with 128, 256, 128, C channels each, and convolution kernels of size 3 for the smaller scale, and of 5, 3, 3, 5 for the largest scale. No pooling is employed. The last non-linear function is a hyperbolic tangent, to ensure that the (normalized) RGB values lie in the range [\u22121, 1]. The output at a coarser scale is up-sampled, and used in input to the next scale module together with a copy of the input at that scale.\nFor models that output segmentations, we removed the last hyperbolic tangent non-linearities for the corresponding output channels, since the class log-probabilities are not limited to a fixed range. Apart from this difference, the S2S model, that predicts the future segmentation from past ones, has the same architecture as the X2X model. The multiscale architecture of the S2S model is illustrated in Figure 2. The other models (XS2X, XS2S, and XS2XS), which take both RGB frames and segmentation maps as input, also use the same internal architecture, and just vary in the number of input and output channels.\nLoss function. Following [22], for all models the loss function between the model output Y\u0302 and the target output Y is the sum of an `1 loss and a gradient difference loss:\nL(Y\u0302 , Y ) = L`1(Y\u0302 , Y ) + Lgdl(Y\u0302 , Y ). (1)\nUsing Yij to denote the pixel elements in Y , and similarly for Y\u0302 , the losses are defined as:\nL`1(Y\u0302 , Y ) = \u2211 i,j \u2223\u2223Yij \u2212 Y\u0302ij\u2223\u2223, (2) Lgdl(Y\u0302 , Y ) =\n\u2211 i,j \u2223\u2223\u2223\u2223\u2223Yi,j \u2212 Yi\u22121,j\u2223\u2223\u2212 \u2223\u2223Y\u0302i,j \u2212 Y\u0302i\u22121,j\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223Yi,j\u22121 \u2212 Yi,j\u2223\u2223\u2212 \u2223\u2223Y\u0302i,j\u22121 \u2212 Y\u0302i,j\u2223\u2223\u2223\u2223\u2223, (3)\nwhere | \u00b7 | denotes the absolute value function. The `1 loss tries to match all pixel predictions independently to their corresponding target values. The gradient difference loss [22], instead, penalizes errors in the gradients of the target image and the predicted one. This loss is relatively insensitive to low-frequency mismatches between prediction and target (e.g ., adding a constant to all pixels does not affect the loss), and is more sensitive to high-frequency mismatches that are perceptually more significant (e.g . blurring the contours of an object). We present a comparison of this loss with a multiclass cross entropy loss in Section 4.\nAdversarial training. As shown by Mathieu et al . [22] in the context of raw images, introducing an adversarial loss term allows the model to disambiguate between modes corresponding to different turns of events, and reduces blur associated with this uncertainty. A recent study by Luc et al . [20] has demonstrated the positive influence of the adversarial training for semantic image segmentation, and its effectiveness in detecting higher-order spatial inconsistencies in the produced outputs.\nOur formulation of the adversarial loss term is based on the principles of recently introduced Wasserstein GAN [2], with some modifications for the semantic segmentation application. In the case of the S2S model, the discriminator D\u0398 is trained to maximize the `1 distance between ground truth sequences (S1:t, St+1) and sequences (S1:t, S\u0302t+1) predicted by our model:\nmax \u0398 \u2223\u2223\u03c3(D\u0398(S1:t, St+1))\u2212 \u03c3(D\u0398(S1:t, S\u0302t+1))\u2223\u2223, (4) where \u0398 is the set of parameters of the discriminator D. The outputs produced by the predictive model are logprobability maps with unbounded values. In the Wasserstein GAN setting, they are encouraged to grow indefinitely. To avoid this and stabilize training, we employ an additional sigmoid non-linearity \u03c3 at the output of the discriminator, and set explicit targets for two kinds of outputs: 0 for the sequences produced by the generator and \u03b1 for real training sequences (S1:t, St+1), where \u03b1=1\u2212 to avoid saturation.\nThe adversarial regularization term for our predictive model (i.e . the \u201cgenerator\u201d) then takes the following form:\nLadv(S\u0302t+1, St+1) = \u03bb|\u03c3(D\u0398(S1:t, S\u0302t+1))\u2212 \u03b1|. (5)\nThe structure of the discriminator network is derived from the two-scale architecture described above. The coarse-scale subnetwork has a single convolutional layer 128\u00d73\u00d73, followed by three fully connected layers with 512, 256 and 1 hidden units respectively. The finescale subnetwork consists of three convolutional layers (128\u00d73\u00d73, 128\u00d73\u00d73, 256\u00d73\u00d73) and three fully connected layers with 512, 256, 1 hidden units.\nFollowing [2], we employ clipping of the discriminator weights \u0398 to the range [\u22120.01, 0.01] after each gradient update, and set the target coefficient \u03b1=0.9 to prevent saturation. In our setting, every iteration of the discriminator training is followed by a single update of the generator parameters. We found that \u03bb=0.1 provides the optimal balance between the loss terms."}, {"heading": "3.2. Predicting deeper into the future", "text": "We consider two extensions of the previous models to predict further into the future than a single frame. The first is to expand the output of the network to comprise a batch\nof m frames, i.e . to output Xt+1:t+m and/or St+1:t+m. We refer to this as the \u201cbatch\u201d approach. The drawback of this approach is that it ignores the recurrence structure of the problem. That is, the fact that St+1 depends on S1:t in the same manner as St+2 depends on S2:t+1. As a result, the capacity of the model is split to predict them output frames, and the number of parameters in the last layer scales linearly with the number of output frames.\nIn our second approach, we leverage the recurrence property, and iteratively apply a model that predicts a single step into the future, using its prediction for time t+1 as an input to predict at time t + 2. This allows us to predict arbitrarily far into the future in an autoregressive manner, without resources scaling with the number of time-steps we want to predict. We refer to this approach as \u201cautoregressive\u201d. See Figure 3 for a schematic illustration of the single time-step model, and the two extensions for longer term prediction.\nFor the autoregressive mode, we either use the models as trained to predict one time-step ahead as they are, or we fine-tune these models by taking into account the impact the autoregressive approach has on predictions farther away than a single frame. In the latter case, during training we first make a forward pass, predicting one frame ahead at a time, and using the most recent outputs to predict the next time step. We then back-propagate the gradients through time [33], where the gradients w.r.t. the prediction at time t are based on the loss at time t, and the impact on the losses at later time steps."}, {"heading": "4. Experiments", "text": "Before presenting our experimental results, we first describe the dataset and evaluation metrics in Section 4.1. We then present results on single-frame prediction, mid-term prediction (0.5 sec.), and long-term prediction (10 sec.)."}, {"heading": "4.1. Dataset and evaluation metrics", "text": "The Cityscapes dataset [4] contains 2,975 training, 500 validation and 500 testing video sequences of 1.8 second. Each sequence consists of 30 frames, and a ground-truth semantic segmentation is available for the 20-th frame. We measure the performance of our models on the Cityscape validation set, and refer to the supplementary material for results on the test set.\nWe assess performance using the standard mean Intersection over Union (IoU) measure, computed w.r.t. the ground truth segmentation of the 20-th frame in each sequence (IoU GT). We also compute the IoU measure w.r.t. the segmentation produced using the Dilated-10 network [36] for the 20-th frame (IoU SEG). The segmentation outputs of the Dilated-10 network are produced at a resolution of 256 \u00d7 128 pixels. The IoU SEG metric allows us to validate our models w.r.t. the target segmentations from which they are trained. Finally, we compute the mean IoU\nacross categories that can move in the scene: person, rider, car, truck, bus, train, motorcycle, and bicycle (IoU-MO, for \u201cmoving objects\u201d).\nTo evaluate the quality of the frame RGB predictions, we compute the Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) measures [32]. The SSIM measures similarity between two images, ranging between -1 for very dissimilar inputs to 1 when the inputs are the same. It is based on comparing local patterns of pixel intensities normalized for luminance and contrast.\nUnless specified otherwise, we train our models using a frame rate of 3, and taking 4 frames and/or segmentations as input. That is, the input sequence consists of frames {Xt\u22129, Xt\u22126, Xt\u22123, Xt}, and similarly for segmentations. We performed patch-wise training with 64\u00d7 64 patches for the largest scale resolution, enabling equal class frequency sampling as in [8], using mini-batches of four patches and a learning rate pf 0.01."}, {"heading": "4.2. Short-term prediction", "text": "In our first set of experiments we used frames 8, 11, 14, and 17 to predict frame 20, for which we have ground truth segmentation available. In Table 2 we compare our five models. For models that do not directly predict future segmentations, we generate segmentations using the Dilated10 network based on the predicted RGB frames. We also include two baselines. The first baseline copies the last input frame to the output. For the second baseline we use FlowNet [6] to estimate the optical flow between the last two inputs, and warp the last input using the estimated flow.\nFrom the result we can make several observations. First, in terms of RGB frame prediction (PSNR and SSIM), the performance is comparable for the three models X2X, XS2X, and XS2XS, and substantially better than the two baselines. This shows that our models learn about the scene dynamics in a non-trivial manner, and that adding semantic segmentations either at input and/or output does not have a substantial impact on this ability.\nSecond, in terms of segmentation prediction (IoU measures), the models that directly predict future segmentations (S2S, XS2S, XS2XS) perform much better than the models that only predict the RGB frames. This suggests that artifacts in the RGB frame predictions propagate into the Dilated-10 network, which then in turn gives degraded segmentations.\nThird, the XS2XS model, which predicts both segmentations and RGB frames performs somewhat worse than the models that only predict segmentations (S2S and XS2S), suggesting that some of the modeling capacity is compromised by jointly predicting the RGB frames.\nFinally, we find that fine-tuning the S2S model using adversarial training (S2S-adv) gives overall the best results, be it with a small margin over normal training. In Figure 4,\nwe show qualitative results of the predictions for one of the validation sequences.\nTable 3 presents results of an ablation study of the S2S model, assessing the impact of the differnt loss functions, as well as the impact of using one or two scales. We include the results obtained using the Dilated-10 model as an \u201coracle\u201d, that predicts the future segmentation based on the future RGB frame, which is not accessible to our other models. This oracle result gives an idea of the maximum performance that could be expected, as it removes the difficulty of\nfuture prediction and reduces to a classic semantic segmentation problem. All variants of the S2S model were trained during about 960,000 iterations, taking about four days of training on a single GPU. The results show that using two scales improves the performance, as does the addition of the gradient difference loss. Training with the `1 and/or gdl loss on the log-probabilities gives better results as compared to training using the Multi-class Cross-Entropy (MCE) loss on the segmentation labels. This is in line with observations made in network distillation [3, 11]."}, {"heading": "4.3. Mid-term prediction", "text": "We now address the more challenging task of predicting the mid-term future, i.e . the next 0.5 second. In these experiments we take in input frames 2, 5, 8, and 11, and predict outputs for frame 14, 17 and 20. We compare different strategies: batch models, autoregressive models (AR), and models with autoregressive fine-tuning (AR fine-tune). We compare these strategies to our two baselines consisting in copying the last input, and the second one relying on optical flow. For the optical flow baseline, after the first prediction, we also warp the flow field so that the flow is applied to the correct locations at the next time step, and so on. Qualitative prediction results are shown in Figure 5. For models XS2X and XS2S, the auto-regressive training mode is not employed because either the frame or the segmentation in-\nput are missing for predicting from the second output. The results for RGB frame prediction in Table 4, show that for frame 14 all models give comparable results, and consistently improve over the copy and warping baseline results. For frame 20, the batch models perform somewhat better than the autoregressive models. When predicting segmentations, c.f . Table 5, we find that the autoregressive models are better than the batch models. This is probably due to the fact that the single-step predictions are more accurate for segmentation, which makes them more suitable for autoregressive modeling. For RGB frame prediction, er-\nrors accumulate quickly leading to degraded autoregressive predictions. Among the batch models, using the images as input (XS2S model) helps slightly. Predicting both the images and segmentation (XS2XS model) performs worst, the image prediction task presumably takes up resources otherwise available for the segmentation task.\nThe S2S is the most effective, as it can be applied in autoregressive mode, and outperforms the XS2XS in this\n1\nsetting. In Figure 5 we compare different versions of this model. Visually, the first sequence shows some improvements using the autoregressive fine-tuned model, by more accurately matching contours of the moving cars than the other strategies. The second sequence displays typical failures of the optical flow baseline, as well as some improvements of the adversarial fine-tuning mode on the car contours. More examples are present in the supplementary material, where we can observe that the most difficult cases for our method are dealing with occlusions and situations where the video recording vehicle is turning."}, {"heading": "4.4. Long-term prediction", "text": "Finally, we consider what happens if we run our best autoregressive model to predict longer sequences of up to 10 seconds into the future. In this experiment we applied our S2S model in autoregressive mode on ten sequences of 238 frames from the Frankfurt long movie of the Cityscapes validation dataset. Given four frame segmentations with a frame rate of 17 images, the model predicts the ten next\nones. Thus, in this setting the images are sampled roughly at 1 Hz. In Figure 7 we report the IoU SEG performance as a function of time. In this extremely challenging setting the predictive performance quickly drops over time. Finetuning the model in autoregressive mode improves its performance, but only gives a clear advantage over the inputcopy baseline for predictions at one and two seconds ahead. We also applied our model with a frame rate of 3 to predict up to 55 steps ahead, but found this to perform much worse. Figure 6 shows an example of predictions compared to the actual future segmentations. The visualization shows that our models average the different classes into an average future, which is perhaps not entirely surprising. Sampling different possible futures using a GAN or VAE approach could be an interesting way to resolve this issue."}, {"heading": "5. Conclusion", "text": "We introduced a new visual understanding task of predicting future semantic segmentations. We explored five different models for this task relying on RGB and/or segmentations from previous frames. For prediction beyond a single future frame, we considered batch models that predict all future frames at once, and autoregressive models that sequentially predict the future frames. We found that autoregressive training produces the best results for our problem, and that models predicting in the segmentation space work better than those relying on the RGB frames.\nWhile our results are encouraging, there is still room for improvement. Where the Dilated-10 network for semantic image segmentation gives around 69% IoU, this drops to about 58% when predicting 3 frames ahead (0.18 sec.), and to about 47% when for 9 frames (0.54 sec.). Most predicted object trajectories are reasonable, but simply do not always correspond to the actual observed trajectories. To improve results, our relatively basic convolutional network architectures can easily be extended to more sophisticated architectures such as residual nets, LSTMs, and dilated convolutions. Furthermore, GANs or VAE models with stochastic inputs may be useful to address the inherent uncertainty in the prediction of future segmentations.\nWe plan to open-source our Torch-based implementation upon paper acceptance, and invite the reader to watch videos of the predictions in the supplementary material."}], "references": [{"title": "Socially-aware large-scale crowd forecasting", "author": ["A. Alahi", "V. Ramanathan", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Do deep nets really need to be deep", "author": ["L. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The Cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Krahenbuhl", "T. Darrell"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2017}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. Hausser", "C. Hazirbas", "V. Golkov"], "venue": "v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to act by predicting the future", "author": ["A. Dosovitskiy", "V. Koltun"], "venue": "ICLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915\u2013 1929", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial networks", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Video scene parsing with predictive feature learning", "author": ["X. Jin", "X. Li", "H. Xiao", "X. Shen", "Z. Lin", "J. Yang", "Y. Chen", "J. Dong", "L. Liu", "Z. Jie", "J. Feng", "S. Yan"], "venue": "arXiv:1612.00119", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A", "author": ["N. Kalchbrenner"], "venue": "van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. arXiv:1610.00527", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational Bayes", "author": ["D. Kingma", "M. Welling"], "venue": "ICLR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity forecasting", "author": ["K. Kitani", "B. Ziebart", "J. Bagnell", "M. Hebert"], "venue": "ECCV", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Refinenet: Multi-path refinement networks for high-resolution semantic segmentation", "author": ["G. Lin", "A. Milan", "C. Shen", "I. Reid"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic segmentation using adversarial networks", "author": ["P. Luc", "C. Couprie", "S. Chintala", "J. Verbeek"], "venue": "NIPS Workshop on Adversarial Training", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of long-term motion dynamics for videos", "author": ["Z. Luo", "B. Peng", "D.-A. Huang", "A. Alahi", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["V. Patraucean", "A. Handa", "R. Cipolla"], "venue": "ICLR Workshop", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": "arXiv:1412.6604", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Transformation-based models of video sequences", "author": ["J. Van Amersfoort", "A. Kannan", "M. Ranzato", "A. Szlam", "D. Tran", "S. Chintala"], "venue": "arXiv:1701.08435", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Anticipating the future by watching unlabeled video", "author": ["C. Vondrick", "P. Hamed", "A. Torralba"], "venue": "CVPR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating videos with scene dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "NIPS", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "ECCV", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions on Image Processing, 13(4):600\u2013612", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P. Werbos"], "venue": "Neural Networks, 1(4):339\u2013356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1988}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K. Bouman", "W. Freeman"], "venue": "NIPS", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "LR-GAN: Layered recursive generative adversarial networks for image generation", "author": ["J. Yang", "A. Kannan", "D. Batra", "D. Parikh"], "venue": "ICLR", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Energy-based generative adversarial networks", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 25, "context": "Prediction and anticipation of future events is a key component to intelligent decision-making [27].", "startOffset": 95, "endOffset": 99}, {"referenceID": 6, "context": "ation and hypotheses made on what could happen next [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 23, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 24, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 21, "context": "The task of predicting future RGB video frames given preceding ones is an interesting one to assess if current vision systems are able to reason about future events, and it has recently received significant attention [25, 26, 22].", "startOffset": 217, "endOffset": 229}, {"referenceID": 0, "context": "by explicitly forecasting trajectories of people and other objects in future video frames [1, 17].", "startOffset": 90, "endOffset": 97}, {"referenceID": 16, "context": "by explicitly forecasting trajectories of people and other objects in future video frames [1, 17].", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 18, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 17, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 22, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 34, "context": "In our work, we build upon the recent progress in this area [8, 19, 18, 23, 24, 36], and develop models to predict the semantic segmentation of future video frames, given the segmentation of several preceding frames.", "startOffset": 60, "endOffset": 83}, {"referenceID": 3, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 42, "endOffset": 45}, {"referenceID": 21, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 221, "endOffset": 229}, {"referenceID": 23, "context": "Our experiments on the Cityscapes dataset [4] suggest that it is advantageous to directly predict future frames at the more abstract semantic-level, rather than to predict at the low-level RGB appearance of future frames [22, 25] and then to apply a semantic segmentation model to these.", "startOffset": 221, "endOffset": 229}, {"referenceID": 34, "context": "5 seconds into the future, the mean IoU of our predictions reaches two thirds of the one obtained by a state-of-the-art semantic segmentation model [36].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "[14] train a model to predict the semantic segmentation of the immediate next image from the preceding input frames, and fuse this prediction with the segmentation computed from the next input frame.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Nilsson and Sminchisescu [23] use a convolutional RNN model with a spatial transformer component [13] to accumulate the information from past and future frames in order to improve prediction of the current frame segmentation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "[24] employ a convolutional RNN to explicitly predict the optical flow, and use these to warp and aggregate per-frame segmentations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] introduced the first baseline of next video frame prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] developed a Long Short Term Memory (LSTM) [12] architecture for the task, and demonstrated a gain in action classification using the learned features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[26] developed a Long Short Term Memory (LSTM) [12] architecture for the task, and demonstrated a gain in action classification using the learned features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "[22] improved the predictions using a multi-scale convolutional architecture, adversarial training [10], and a gradient difference loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[22] improved the predictions using a multi-scale convolutional architecture, adversarial training [10], and a gradient difference loss.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "To reduce the number of parameters to estimate, several authors re-parameterize the problem to predict frame transformations instead of raw pixels [9, 28].", "startOffset": 147, "endOffset": 154}, {"referenceID": 26, "context": "To reduce the number of parameters to estimate, several authors re-parameterize the problem to predict frame transformations instead of raw pixels [9, 28].", "startOffset": 147, "endOffset": 154}, {"referenceID": 20, "context": "[21] employ a convolutional LSTM architecture to predict sequences of up to eight frames of optical flow in RGBd videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also uses LSTMs, and factorizes the temporal and spatial/color dimensions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] instead predict features in future frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Generative Adversarial Networks (GAN) [10] and Variational Auto-Encoders (VAE) [16] are recent unsupervised learning methods that can be used to deal with the inherent uncertainty in futureprediction tasks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "Generative Adversarial Networks (GAN) [10] and Variational Auto-Encoders (VAE) [16] are recent unsupervised learning methods that can be used to deal with the inherent uncertainty in futureprediction tasks.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "GAN training has recently been improved using the Wasserstein GAN approach of Arjovsky and Bottou [2], and the energy based approach of Zhao et al .", "startOffset": 98, "endOffset": 101}, {"referenceID": 35, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "An interesting approach using GANs for unsupervised image representation learning is proposed in [5], where the generative model is trained along with an inference model that maps images to their latent representations.", "startOffset": 97, "endOffset": 100}, {"referenceID": 28, "context": "[30] showed that GANs can be applied to video generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] use similar ideas to develop an iterative image generation model where objects are sequentially pasted on the image canvas using a recurrent GAN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] predict future video frames from a single given frame using a VAE approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] perform forecasting with a VAE, predicting feature point trajectories from still images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "To circumvent the need for datasets with per-frame annotations, we use the state-of-the-art multi-scale Dilated-10 semantic image segmentation network [36] to provide target semantic segmentations for all frames in each video.", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "This is motivated by recent observations in network distillation that the log-probabilities carry more information when training one network on the outputs of another network [3, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 10, "context": "This is motivated by recent observations in network distillation that the log-probabilities carry more information when training one network on the outputs of another network [3, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 21, "context": "[22], with two spatial scales.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Following [22], for all models the loss function between the model output \u0176 and the target output Y is the sum of an `1 loss and a gradient difference loss:", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "The gradient difference loss [22], instead, penalizes errors in the gradients of the target image and the predicted one.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "[22] in the context of raw images, introducing an adversarial loss term allows the model to disambiguate between modes corresponding to different turns of events, and reduces blur associated with this uncertainty.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] has demonstrated the positive influence of the adversarial training for semantic image segmentation, and its effectiveness in detecting higher-order spatial inconsistencies in the produced outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Our formulation of the adversarial loss term is based on the principles of recently introduced Wasserstein GAN [2], with some modifications for the semantic segmentation application.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Following [2], we employ clipping of the discriminator weights \u0398 to the range [\u22120.", "startOffset": 10, "endOffset": 13}, {"referenceID": 31, "context": "We then back-propagate the gradients through time [33], where the gradients w.", "startOffset": 50, "endOffset": 54}, {"referenceID": 3, "context": "The Cityscapes dataset [4] contains 2,975 training, 500 validation and 500 testing video sequences of 1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 34, "context": "the segmentation produced using the Dilated-10 network [36] for the 20-th frame (IoU SEG).", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "To evaluate the quality of the frame RGB predictions, we compute the Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) measures [32].", "startOffset": 163, "endOffset": 167}, {"referenceID": 7, "context": "We performed patch-wise training with 64\u00d7 64 patches for the largest scale resolution, enabling equal class frequency sampling as in [8], using mini-batches of four patches and a learning rate pf 0.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "For the second baseline we use FlowNet [6] to estimate the optical flow between the last two inputs, and warp the last input using the estimated flow.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "This is in line with observations made in network distillation [3, 11].", "startOffset": 63, "endOffset": 70}, {"referenceID": 10, "context": "This is in line with observations made in network distillation [3, 11].", "startOffset": 63, "endOffset": 70}], "year": 2017, "abstractText": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we focus on predicting semantic segmentations of future frames. More precisely, given a sequence of semantically segmented video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Our models predict trajectories of cars and pedestrians much more accurately (25%) than baselines that copy the most recent semantic segmentation or warp it using optical flow. Prediction results up to half a second in the future are visually convincing, the mean IoU of predicted segmentations reaching two thirds of the real future segmentations.", "creator": "LaTeX with hyperref package"}}}