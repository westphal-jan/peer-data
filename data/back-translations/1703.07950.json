{"id": "1703.07950", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Failures of Gradient-Based Deep Learning", "abstract": "In recent years, deep learning has become a state-of-the-art solution for a wide range of applications. However, it is important for both theorists and practitioners to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four problem families where some of the commonly used existing algorithms fail or experience significant difficulties. We illustrate the errors through practical experiments and give theoretical insights into how they arise and how they could be corrected.", "histories": [["v1", "Thu, 23 Mar 2017 07:16:37 GMT  (316kb,D)", "http://arxiv.org/abs/1703.07950v1", null], ["v2", "Wed, 26 Apr 2017 05:23:26 GMT  (406kb,D)", "http://arxiv.org/abs/1703.07950v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["shai shalev-shwartz", "ohad shamir", "shaked shammah"], "accepted": true, "id": "1703.07950"}, "pdf": {"name": "1703.07950.pdf", "metadata": {"source": "CRF", "title": "Failures of Deep Learning", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few. The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.\nThe current theoretical understanding of deep learning is far from being sufficient for a rigorous analysis of the difficulties faced by practitioners. Progress must be made from both parties: from a practitioner\u2019s perspective, emphasizing the difficulties provides practical insights to the theoretician, which in turn, supplies theoretical insights and guarantees, further strengthening and sharpening practical intuitions and wisdom. In particular, understanding failures of existing algorithms is as important as understanding where they succeed.\nOur goal in this paper is to present and discuss families of simple problems for which commonly used methods do not show as exceptional a performance as one might expect. We use empirical results and insights as a ground on which to build a theoretical analysis, characterising the sources of failure. Those understandings are aligned, and sometimes lead to, different approaches, either for an architecture, loss function, or an optimization scheme, and explain their superiority when applied to members of those families. The code for running the experiments is found in https://github.com/shakedshammah/failures_of_DL. See command lines at Appendix C.\nWe start off in Section 2 by discussing a class of simple learning problems for which the gradient information, central to deep learning algorithms, provably carries negligible information on the target function which we attempt to learn. This result is a property of the learning problems themselves, and holds for any specific network architecture one may choose for tackling the learning problem, implying that no gradient-based method is likely to succeed. Our analysis builds upon tools from the Statistical Query literature, and underscores one of the main deficiencies of Deep Learning: its reliance on local properties of the loss function, with the objective being of a global nature.\nNext, in Section 3, we tackle the ongoing dispute between two common approaches to learning. Most, if not all, learning and optimization problems can be viewed as some structured set of sub-problems. The first approach, which we refer to as the \u201cEnd-to-end\u201d approach, will tend to solve all of the sub-problems together in one shot, by optimizing a single primary objective. The second approach, which we refer to as the \u201cDecomposition\u201d one, will tend to handle these sub-problems separately, solving each one by defining and optimizing additional objectives, and not rely solely on the primary objective. The benefits of the End-to-end approach, both in terms of requiring a smaller\nar X\niv :1\n70 3.\n07 95\n0v 1\n[ cs\n.L G\n] 2\n3 M\nar 2\n01 7\namount of labeling and prior knowledge, and perhaps enabling more expressive architectures, cannot be ignored. On the other hand, intuitively and empirically, the extra supervision injected through decomposition is helpful in the optimization process. We experiment with two setups in which application of the two approaches is possible, and the distinction between them is clear and intuitive. We observe, in both experiments, that an \u201cEnd-to-end\u201d approach can be much slower than a \u201cDecomposition\u201d method, and sometimes even does not show any progress. We analyze this gap by comparing, theoretically and empirically, the signal-to-noise ratio of the stochastic gradient estimate used by Stochastic Gradient Descent (SGD) for the two approaches.\nIn Section 4, we demonstrate the importance of both the network\u2019s architecture and the optimization algorithm on the training time. While the choice of architecture is usually studied in the context of its expressive power, we show that even when two architectures have the same expressive power for a given task, there may be a tremendous difference in the ability to optimize them. We analyze the required runtime of gradient descent for the two architectures through the lens of the condition number of the problem. We further show that conditioning techniques can yield additional orders of magnitude speedups. The experimental setup in this section is around a seemingly simple problem \u2014 encoding a piece-wise linear one-dimensional curve. Despite the simplicity of this problem, we show that following the common rule of \u201cperhaps I should use a deeper/wider network1\u201d does not significantly help here.\nFinally, in Section 5, we question deep learning\u2019s reliance on \u201cvanilla\u201d gradient information for the optimization process. We previously discussed the deficiency of using a local property of the objective in directing global optimization. Here, we focus on a simple case in which it is possible to solve the optimization problem based on local information, but not in the form of a gradient. We experiment with architectures that contain activation functions with flat regions, which leads to the well known vanishing gradient problem. Practitioners take great care when working with such activation functions, and many heuristic tricks are applied in order to initialize the network\u2019s weights in non-flat areas of its activations. Here, we show that by using a different update rule, we manage to solve the learning problem efficiently. Convergence guarantees exist for a family of such functions. This shows the power of non-gradient-based optimization schemes in overcoming the limitations of gradient-based learning."}, {"heading": "2 Failure due to Non-Informative Gradients", "text": "Most existing deep learning algorithms are gradient-based methods; namely, algorithms which optimize an objective through access to its gradient w.r.t. some weight vector w, or estimates of the gradient. We consider a setting where the goal of this optimization process is to learn some underlying hypothesis classH, of which one member, h \u2208 H, is responsible for labelling the data. This yields an optimization problem of the form\nmin w Fh(w).\nThe underlying assumption is that the gradient of the objective w.r.t. w,\u2207Fh(w), contains useful information regarding the target function h, and will help us make progress.\nBelow, we discuss a family of problems for which with high probability, at any fixed point, the gradient,\u2207Fh(w), will be essentially the same regardless of the underlying target function h. Furthermore, we prove that this holds independently of the choice of architecture or parametrization, and using a deeper/wider network will not help. The family we study is that of compositions of linear and periodic functions, and we experiment with the classical problem of learning parities. Our empirical and theoretical study shows that indeed, if there\u2019s little information in the gradient, using it for learning cannot succeed."}, {"heading": "2.1 Experiment", "text": "We begin with the simple problem of learning random parities: After choosing some v\u2217 \u2208 {0, 1}d uniformly at random, our goal is to train a predictor mapping x \u2208 {0, 1}d to y = (\u22121)\u3008x,v\u2217\u3009, where x is uniformly distributed. In words, y indicates whether the number of 1\u2019s in a certain subset of coordinates of x (indicated by v\u2217) is odd or even.\n1See http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/ for the inspiration behind this humoristic quote.\nFor our experiments, we use the hinge loss, and a simple network architecture of one fully connected layer of width 10d > 3d2 with ReLU activations, and a fully connected output layer with linear activation and a single unit. Note that this class realizes the parity function corresponding to any v\u2217 (see Lemma 3 in the appendix).\nEmpirically, as the dimension d increases, so does the difficulty of learning, which can be measured in the accuracy we arrive at after a fixed number of training iterations, to the point where around d = 30, no advance beyond random performance is observed after reasonable time. Figure 1 illustrates the results."}, {"heading": "2.2 Analysis", "text": "To formally explain the failure from a geometric perspective, consider the stochastic optimization problem associated with learning a target function h,\nmin w Fh(w) := E x [`(pw(x), h(x))] , (1)\nwhere ` is a loss function, x are the stochastic inputs (assumed to be vectors in Euclidean space), and pw is some predictor parametrized by a parameter vector w (e.g. a neural network of a certain architecture). We will assume that F is differentiable. A key quantity we will be interested in studying is the variance of the gradient of F with respect to h, when h is drawn uniformly at random from a collection of candidate target functionsH:\nVar(H, F,w) = E h \u2225\u2225\u2225\u2225\u2207Fh(w)\u2212 Eh\u2032\u2207Fh\u2032(w) \u2225\u2225\u2225\u22252\nIntuitively, this measures the expected amount of \u201csignal\u201d about the underlying target function contained in the gradient. As we will see later, this variance correlates with the difficulty of solving (1) using gradient-based methods2.\nThe following theorem bounds this variance term.\nTheorem 1 Suppose that\n\u2022 H consists of real-valued functions h satisfying Ex[h2(x)] \u2264 1, such that for any two distinct h, h\u2032 \u2208 H, Ex[h(x)h\u2032(x)] = 0.\n\u2022 pw(x) is differentiable w.r.t. w, and satisfies Ex [ \u2016 \u2202\u2202wpw(x)\u2016 2 ] \u2264 G(w)2 for some scalar G(w).\n\u2022 The loss function ` in (1) is either the square loss `(y\u0302, y) = 12 (y\u0302 \u2212 y) 2 or a classification loss of the form\n`(y\u0302, y) = r(y\u0302 \u00b7 y) for some 1-Lipschitz function r, and the target function h takes values in {\u00b11}.\nThen\nVar(H, F,w) \u2264 G(w) 2\n|H| .\n2This should not be confused with the variance of gradient estimates used by SGD, which we discuss in Section 3.\nThe proof is given in Appendix A.1. The theorem implies that if we try to learn an unknown target function, possibly coming from a large collection of uncorrelated functions, then the sensitivity of the gradient to the target function at any point decreases linearly with |H|.\nBefore we make a more general statement, let us return to the case of parities, and study it through the lens of this framework. Suppose that our target function is some parity function chosen uniformly at random, i.e. a random element from the set of 2d functions H = {x 7\u2192 (\u22121)\u3008x,v\u2217\u3009 : v\u2217 \u2208 {0, 1}d}. These are binary functions, which are easily seen to be mutually orthogonal: Indeed, for any v,v\u2032,\nE x\n[ (\u22121)\u3008x,v\u3009(\u22121)\u3008x,v \u2032\u3009 ]\n= E x\n[ (\u22121)\u3008x,v+v \u2032\u3009 ]\n= d\u220f i=1 E [ (\u22121)xi(vi+v \u2032 i) ] = d\u220f i=1 (\u22121)vi+v\u2032i + (\u22121)\u2212(vi+v\u2032i) 2\nwhich is non-zero if and only if v = v\u2032. Therefore, by Theorem 1, we get that Var(H, F,w) \u2264 G(w)2/2d \u2013 that is, exponentially small in the dimension d. By Chebyshev\u2019s inequality, this implies that the gradient at any point w will be extremely concentrated around a fixed point independent of h.\nThis phenomenon of exponentially-small variance can also be observed for other distributions, and learning problems other than parities. Indeed, in [26], it was shown that this also holds in a more general setup, when the output y corresponds to a linear function composed with a periodic one, and the input x is sampled from a smooth distribution:\nTheorem 2 (Shamir 2016) Let \u03c8 be a fixed periodic function, and let H = {x 7\u2192 \u03c8(v\u2217>x) : \u2016v\u2217\u2016 = r} for some r > 0. Suppose x \u2208 Rd is sampled from an arbitrary mixture of distributions with the following property: The square root of the density function \u03d5 has a Fourier transform \u03d5\u0302 satisfying \u222b x:\u2016x\u2016>r \u03d5\u0302\n2(x)dx\u222b x \u03d5\u03022(x)dx\n\u2264 exp(\u2212\u2126(r)). Then if F denotes the objective function with respect to the squared loss,\nVar(H, F,w) \u2264 O (exp(\u2212\u2126(d)) + exp(\u2212\u2126(r))) .\nThe condition on the Fourier transform of the density is generally satisfied for smooth distributions (e.g. arbitrary Gaussians whose covariance matrices are positive definite, with all eigenvalues at least \u2126(1/r)). Thus, the bound is extremely small as long as the norm r and the dimension d are moderately large, and indicates that the gradients contains little signal on the underlying target function.\nBased on these bounds, one can also formally prove that a gradient-based method will fail in returning a reasonable predictor, unless the number of iterations is exponentially large in r, d: By Chebyshev\u2019s inequality, these bounds imply that with overwhelming probability, at any point w (say, the point at which our gradient-based method is initialized at), the gradient \u2207F (w) will be fixed independent of the underlying target function (up to a deviation which for moderate r, d is much smaller than machine precision, hence can be assumed non-existent on any realistic computing platform). As a result, even if we perform exact gradient descent, the update in the first iteration will be independent of the underlying target function3 . Repeating the argument and using a union bound, it follows that as long as the number of iterations is sub-exponential, the trajectory of the gradient descent procedure will be independent of the underlying target function, so we cannot expect it to achieve a reasonable result. Overall, this gives strong evidence that gradient-based methods indeed cannot learn random parities and linear-periodic functions.\nWe emphasize that these results hold regardless of which class of predictors we use (e.g. they can be arbitrarily complex neural networks) \u2013 the problem lies in using a gradient-based method to train them. Also, we note that the difficulty lies in the random choice of v\u2217, and the problem is not difficult if v\u2217 is known and fixed in advance (for example, for a full parity v\u2217 = (1, . . . , 1), this problem is known to be solvable with an appropriate LSTM network [14]).\nFinally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [18, 1]. This refers to algorithms which are constrained to interact with data by receiving estimates of the expected value of some query over the underlying\n3Formally, this requires an oracle-based model, where given a point w, the algorithm receives the gradient at w up to some arbitrary error much smaller than machine precision. See [26, Theorem 4] for details.\ndistribution (e.g. the expected value of the first coordinate), and it is well-known that parities cannot be learned with such algorithms. Recently, [6] have shown that gradient-based methods with an approximate gradient oracle can be implemented as a statistical query algorithm, which implies that gradient-based methods are indeed unlikely to solve learning problems which are known to be hard in the statistical queries framework, in particular parities. In the discussion on random parities above, we have simply made the connection between gradient-based methods and parities more explicit, by direct examination of gradients\u2019 variance w.r.t. the target function."}, {"heading": "3 Decomposition vs. End-to-end", "text": "Many practical learning problems, and more generally, algorithmic problems, can be viewed as a structured composition of sub-problems. Applicable approaches for a solution can either be tackling the problem in an End-to-end manner, or by decomposition. Whereas for a traditional algorithmic solution, the \u201cDivide-and-conquer\u201d strategy is an obvious choice, the ability of deep learning to utilize big data and expressive architectures have made \u201cEnd-to-end training\u201d an attractive alternative. Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions. We try to answer the following questions: what is the price of the rather appealing \u201cEnd-to-end\u201d approach? Is letting a network \u201clearn by itself\u201d such a bad idea? When is it necessary, or worth the effort, to \u201chelp\u201d it?\nThere are various aspects which can be considered in this context. For example, [25] analyzed the difference between the approaches from the sample complexity point of view. Here, we focus on the optimization aspect, showing that training SGD with the \u201cEnd-to-end\u201d approach might suffer from a low signal-to-noise ratio of the gradient estimates, which may significantly affect the training time. Helping the SGD process by decomposing the problem leads to much faster training. We present two experiments, motivated by questions each and every practitioner must answer when facing a new, non trivial problem; what exactly is the needed data, what architecture is planned to be used, and what is the right distribution of development efforts, are all correlated questions with no clear answer. Our experiments and analysis show that taking the wrong choice may be expensive."}, {"heading": "3.1 Experiment I", "text": "Our first experiment compares the two approaches in a computer vision setting, where Convolutional Neural Networks (CNN) had become the most widely used and successful algorithmic architectures. We define a family of problems, parameterized by k \u2208 N, and show that as grows k, grows the gap between an \u201cEnd-to-end\u201d approach and a \u201cDecomposition\u201d one.\nLet X1 denote the space of 28 \u00d7 28 binary images, with a distribution D defined by the following sampling procedure:\n\u2022 Sample \u03b8 \u223c U([0, \u03c0]), l \u223c U([5, 28\u2212 5]), (x, y) \u223c U([0, 27])2.\n\u2022 The image x\u03b8,l,(x,y) associated with the above sample is set to 0 everywhere, except for a straight line of length l, centered at (x, y), and rotated at an angle \u03b8. Note that as the images space is discrete, we round the values corresponding to the points on the lines to the closest integer coordinate.\nLet y : X1 \u2192 {\u00b11} be defined by\ny(x\u03b8,l,(x,y)) = { 1 if \u03b8 < \u03c0/2 \u22121 otherwise .\nFigure 2 shows a few examples of instances. We can now define the problem for each k. The sample space for our problem is X := Xk1 , namely, the k-tuples of elements from X1, where each of the tuple\u2019s entries is sampled i.i.d. by the above procedure. We denote such tuples by v := (x(1), ...,x(k)). Each such tuple is associated with a vector in {\u00b11}k, defined by the corresponding y values for the tuple\u2019s entries, and denoted y(v) := (y(x(1)), ..., y(x(k))). The labeling function, y\u0303 : X \u2192 {\u00b11}, is defined to be the parity function over y(v), namely, y\u0303(v) = \u220f j=1...k y(x\n(j)). Many architectures of DNN can be used for predicting y\u0303(v) given v. A natural \u201chigh-level\u201d choice can be:\n\u2022 Feed each of the images, separately, to a single CNN (of some standard specific architecture, for example, LeNet-like), denoted N (1)w1 and parameterized by its weights vector w1, outputting a single scalar, which can be regarded as a \u201cscore\u201d.\n\u2022 Concatenate the \u201cscores\u201d of a tuple\u2019s entries, transform them to the range [0, 1] using a sigmoid function, and feed the resulting vector into another network, N (2)w2 , of a similar architecture to the one defined in Section 2, outputting a single \u201ctuple-score\u201d, which can then be thresholded for obtaining the binary prediction.\nLet the whole architecture be denoted Nw. Assuming that N (1) is expressive enough to provide, at least, a weak learner for y (a reasonable assumption), and using Lemma 3 to obtain the sufficiency ofN (2) in terms of expressibility for the parity function, we obtain that this architecture has the potential for good performance.\nThe final piece of the experimental setting is the choice of a loss function. Clearly, the \u201cprimary\u201d loss which we\u2019d like to minimize is the expected zero-one loss over the prediction, Nw(v), and the label, y\u0303(v), namely:\nL\u03030\u22121(w) := E v [Nw(v) 6= y\u0303(v)]\nA \u201csecondary\u201d loss which may be considerable, is the zero-one loss over the prediction of N (1)w1 (x) and the respective y(x) value:\nL0\u22121(w1) := E x [ N (1)w1 (x) 6= y(x) ] Let L\u0303, L be some differentiable surrogates for L\u03030\u22121, L0\u22121. A classical \u201cEnd-to-end\u201d approach will be to minimize L\u0303, and only it; this is our \u201cprimary\u201d objective. We have no explicit desire for N (1) to output any specific value, and hence L is, a priori, irrelevant. A \u201cDecomposition\u201d approach would be to minimize both losses, under the assumption that L can \u201cdirect\u201d w1 towards an \u201carea\u201d in which we know that the resulting outputs of N (1) can be separated by N (2). Note that using L is only possible when the y values are known to us.\nEmpirically, when comparing based on the \u201cprimary\u201d objective, we see significant inferiority of \u201cEnd-to-end\u201d with respect to \u201cDecomposition\u201d. Using \u201cDecomposition\u201d, we quickly arrive at a good solution, regardless of the tuple\u2019s length, k (as long as k is in the range where perfect input to N (2) is solvable by SGD, as described in Section 2). However, using \u201cEnd-to-end\u201d works only for very small k values, and completely fails for k \u2265 3. This may be somewhat surprising, as the \u201cEnd-to-end\u201d approach optimizes exactly the comparison criterion, with no additional irrelevant objectives. Figure 3 illustrates the results."}, {"heading": "3.2 Experiment II", "text": "We now consider a more synthetic setup, which will later allow us crisp analysis and explanation for the empirical results. Consider the problem of training a predictor, which given a \u201cpositive media reference\u201d x to a certain stock option, will distribute our assets between the k = 500 stocks in the S&P500 index in some manner. One can, again, come up with two rather different strategies for solving the problem.\n\u2022 An \u201cEnd-to-end\u201d approach: train a deep network Nw that given x outputs a distribution over the k stocks. The objective for training is maximizing the gain obtained by allocating our money according to this distribution.\n\u2022 A \u201cDecomposition\u201d approach: train a deep network Nw that given x outputs a single stock, y \u2208 [k], whose future gains are the most positively correlated to x. Of course, we may need to gather extra labeling for training Nw based on this criterion.\nTo make a crisp analysis and experiment, we make the (non-realistic) assumption that every instance of media reference is strongly and positively correlated to a single stock y \u2208 [k], and it has no correlation with future performance of other stocks. This obviously makes our problem rather toyish; the stock exchange and media worlds have highly complicated correlations. However, it indeed rises from, and is motivated by, practical problems.\nTo examine the problem in a simple and theoretically clean manner, we design a synthetic experiment defined by the following optimization problem: Let X \u00d7 Z \u2282 Rd \u00d7 {\u00b11}k be the sample space, and let y : X \u2192 [k] be some labelling function. We would like to learn a mapping Nw : X \u2192 Sk\u22121, with the objective being:\nmin w L(w) := E x,z\u223cX\u00d7Z\n[ \u2212z>Nw(x) ] .\nTo connect this to our story, Nw(x) is our asset distribution, z indicates the future performance of the stocks, and thus, we are seeking minimization of our expected future negative gains, or in other words, maximization of expected profit. We further assume that given x, the coordinate zy(x) equals 1, and the rest of the coordinates are sampled i.i.d from the uniform distribution over {\u00b11}.\nWhereas in the previous experiment, the difference between the \u201cEnd-to-end\u201d and \u201cDecomposition\u201d approaches could be summarized by a different loss function choice, in this experiment, the difference will boil down to the different gradient estimators we would use, where we are again taking as a given fact that exact gradient computations are expensive for large-scale problems, implying the method of choice to be SGD. For the purpose of the experimental discussion, let us write the two estimators explicitly as two unconnected update rules. We will later analyze their (equal) expectation.\nFor an \u201cEnd-to-end\u201d approach, we sample a pair (x, z), and use \u2207w(\u2212z>Nw(x)) as a gradient estimate. It is clear that this is an unbiased estimator of the gradient.\nFor a \u201cDecomposition\u201d approach, we sample a pair (x, z), completely ignore z, and instead, pay the extra costs and gather the required labelling to get y(x). We will then use \u2207w(\u2212e>y(x)Nw(x)) as a gradient estimate. It will be shown later that this too is an unbiased estimator of the gradient.\nFigure 4 clearly shows that optimizing using the \u201cEnd-to-end\u201d estimator is inferior to working with the \u201cDecomposition\u201d one, in terms of training time and final accuracy, to the extent that for large k, the \u201cEnd-to-end\u201d estimator cannot close the gap in performance in reasonable time."}, {"heading": "3.3 Analysis", "text": "We approach the analysis through examination of the Signal to Noise Ratio (SNR) of the two stochastic gradient estimators, showing that the \u201cEnd-to-end\u201d one is significantly inferior in that regard.\nLet us first gain intuition with regard to the first experiment, by empirically measuring:\n\u2022 The signal\u2019s strength, namely, the squared norm of the gradients\u2019 mean (w.r.t. the losses L\u0303, L), which we estimate by SGD, denoted \u2016\u2207L\u0303\u20162, \u2016\u2207L\u20162.\n\u2022 The signal\u2019s variance, namely, Ev \u2016\u2207\u0302L \u2212 \u2207L\u20162, where \u2207\u0302L is the estimate of \u2207L obtained by differentiating w.r.t. to a single sample v. We define this similarly for L\u0303.\nFigure 5 shows an estimation of the SNR for the last layer of N (1), for the two approaches, as a function of k, where our estimation is over the random initialization of the network (clearly, there are weights w for which all gradients are 0, no matter what approach is used). It is easy to see that the \u201cEnd-to-end\u201d approach suffers from significantly lower SNR, and more importantly, it shows dependence on k, quickly falling below machine precision, whereas the \u201cDecomposition\u201d approach\u2019s SNR is constant. This gives strong empirical evidence for a connection between slow convergence and low SNR.\nEquipped with the above empirical observation, we turn on to analytically examine the second experiment. First, let us show that indeed, both estimators are unbiased estimators of the true gradient. As stated above, it is clear, by definition of L, that the \u201cEnd-to-end\u201d estimator is an unbiased estimator of \u2207wL(w). To observe this is also the case for the \u201cDecomposition\u201d estimator, we write:\n\u2207wL(w) = \u2207w E x,z [\u2212z>Nw(x)]\n=E x [ E z|x\n[\u2207w(\u2212z>Nw(x))]]\n(1) = E\nx [ E z|x\n[\u2212z>\u2207w(Nw(x))]] (2) = E x [\u2212e>y(x)\u2207w(Nw(x))]\nwhere (1) follows from the chain rule, and (2) from the assumption on the distribution of z given x. It is now easy to see that the \u201cDecomposition\u201d estimator is indeed a (different) unbiased estimator of the gradient.\nIntuition says that when a choice between two unbiased estimators is presented, we should choose the one with the lower variance. In our context, [7] showed that when running SGD (even on non-convex objectives), arriving at a point where \u2016\u2207wL(w)\u20162 \u2264 requires order of \u03bd\u03042/ 2 iterations, where\n\u03bd\u03042 = max t E x,q \u2016\u2207tw(x, q)\u20162 \u2212 \u2016\u2207wL(w(t))\u20162,\nwt is the weight vector at time t, q is sampled along with x (where it can be replaced by z or y(x), in our experiment), and \u2207tw is the unbiased estimator for the gradient. This serves as a motivation for analyzing the problem through this lens.\nWe examine the quantity Ex,q \u2016\u2207tw(x, q)\u20162 explicitly. For the \u201cEnd-to-end\u201d estimator, this quantity equals\nE x,z \u2016 \u2212 z>\u2207wNw(x)\u20162 = E x,z \u2016 \u2212 k\u2211 i=1 zi\u2207wNw(x)i\u20162\nDenoting by Gi := \u2207wNw(x)i, we get:\n= E x E z|x \u2016 \u2212 k\u2211 i=1 ziGi\u20162 = E x k\u2211 i=1 \u2016Gi\u20162 (2)\nwhere the last equality follows from expanding the squared sum, and taking expectation over z, while noting that mixed terms cancel out (from independence of z\u2019s coordinates), and that z2i = 1 for all i.\nAs for the \u201cDecomposition\u201d estimator, it is easy to see that\nE x \u2016 \u2212 e>y(x)\u2207wNw(x)\u2016 2 = E x \u2016Gy(x)\u20162. (3)\nObserve that in 2 we are summing up, per x, k summands, compared to the single element in 3. When randomly initializing a network it is likely that the values of \u2016Gi\u20162 are similar, hence we obtain that at the beginning of training, the variance of the \u201cEnd-to-end\u201d estimator is roughly k times larger than that of the \u201cDecomposition\u201d estimator. To summarize, we get further evidence for the benefits of direct supervision, whenever it is applicable to a problem."}, {"heading": "4 Architecture and Conditioning", "text": "Network architecture choice is a crucial element in the success of deep learning. New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [11, 29]. When choosing an architecture, one consideration is how to inject prior knowledge on the problem at hand, improving the network\u2019s expressiveness for that problem, while not dramatically increasing sample complexity. Another aspect involves improving the computational complexity of training. In this section we formally show how the choice of architecture affects the training time through the lens of the condition number of the problem.\nThe study and practice of conditioning techniques, for convex and non-convex problems, gained much attention recently (e.g., [15, 19, 5, 24]). Here we show how architectural choice may have a dramatic effect on the applicability of better conditioning techniques.\nThe learning problem we consider in this section is that of encoding one-dimensional, piecewise linear curves. We show how different architectures, all of them of sufficient expressive power for solving the problem, have orders-ofmagnitude difference in their condition numbers. In particular, this becomes apparent when considering convolutional vs. fully connected layers. This sheds a new light over the success of convolutional neural networks, which is generally attributed to their sample complexity benefits. Moreover, we show how conditioning, applied in conjunction with a better architecture choice, can further decrease the condition number by orders of magnitude. The direct effect on the convergence rate is analyzed, and is aligned with the significant performance gaps observed empirically. We also demonstrate how performance may not significantly improve by employing deeper and more powerful architectures, as well as the price that comes with choosing a sub-optimal architecture."}, {"heading": "4.1 Experiments and Analysis", "text": "We experiment with various deep learning solutions for encoding the structure of one-dimensional, continuous, piecewise linear (PWL) curves. Any PWL curve with k pieces can be written as: f(x) = b+ \u2211k i=1 ai[x\u2212 \u03b8i]+, where ai is the difference between the slope at the i\u2019th segment and the (i \u2212 1)\u2019th segment. For example, the curve below can be parametrized by b = 1, a = (1,\u22122, 3), \u03b8 = (0, 2, 6).\nThe problem we consider is that of receiving a vector of the values of f at x \u2208 {0, 1, . . . , n \u2212 1}, namely f := (f(0), f(1), . . . , f(n \u2212 1)), and outputting the values of b, {ai, \u03b8i}ki=1. We can think of this problem as an encoding problem, since we would like to be able to rebuild f from the values of b, {ai, \u03b8i}ki=1. Observe that b = f(0), so from now on, let us assume without loss of generality that b = 0.\nThroughout our experiments, we use n = 100, k = 3. We sample {\u03b8i}i\u2208[k] uniformly without replacement from {0, 1, . . . , n\u2212 1}, and sample each ai i.i.d. uniformly from [\u22121, 1]."}, {"heading": "4.1.1 Convex Problem, Large Condition Number", "text": "As we assume that each \u03b8i is an integer in {0, 1, . . . , n \u2212 1}, we can represent {ai, \u03b8i}ki=1 as a vector p \u2208 Rn such that pj = 0 unless there is some i such that \u03b8i = j, and in this case we set pj = ai. That is, pj = \u2211k i=1 ai 1[\u03b8i=j\u22121].\nThis allows us to formalize the problem as a convex optimization problem. Define a matrix W \u2208 Rn,n such that Wi,j = [i \u2212 j + 1]+. It is not difficult to show that f = Wp. Moreover, W can be shown to be invertible, so we can extract p from f by p = W\u22121f .\nWe hence start by attempting to learn this linear transformation directly, using a connected architecture of one layer, with n output channels. Let the weights of this layer be denoted U\u0302 . We therefore minimize the objective:\nmin U\u0302 E f\n[ 1\n2 (W\u22121f \u2212 U\u0302 f)2\n] (4)\nwhere f is sampled according to some distribution. As a convex, realizable (by U\u0302 = W\u22121) problem, convergence is guaranteed, and we can explicitly analyze its rate. However, perhaps unexpectedly, we observe a very slow rate of convergence to a satisfactory solution, where significant inaccuracies are present at the non-smoothness points. Figure 6a illustrates the results.\nTo analyze the convergence rate of this approach, and to benchmark the performance of the next set of experiments, we start off by giving an explicit expression for W\u22121:\nLemma 1 The inverse of W is the matrix U s.t. Ui,i = Ui+2,i = 1, Ui+1,i = \u22122, and the rest of the coordinates of U are zero.\nThe proof is given in Appendix A.2. Next, we analyze the iteration complexity of SGD for learning the matrix U . To that end, we give an explicit expression for the expected value of the learned weight matrix at each iteration t, denoted as U\u0302 t:\nLemma 2 Assume U\u03020 = 0, and that Ef [Uff>U>] = \u03bbI for some \u03bb. Then, running SGD with learning rate \u03b7 over objective 4 for t iterations yields:\nE U\u0302t = \u03b7\u03bbW> t\u22121\u2211 i=0 (I \u2212 \u03b7\u03bbWW>)i\nThe proof is given at Appendix A.3. Note that the assumption that Ef [Uff>U>] = \u03bbI holds under the distributional assumption over the curves, as changes of direction in the curve are independent, and are sampled each time from the same distribution. The following theorem establishes a lower bound on \u2016E U\u0302t+1 \u2212 U\u2016, which by Jensen\u2019s inequality, implies a lower bound on E \u2016U\u0302t+1\u2212U\u2016, the expected distance of U\u0302t+1 from U . Note that the lower bound holds even if we use all the data for updating (that is, gradient descent and not stochastic gradient descent).\nTheorem 3 Let W = QSV > be the singular value decomposition of W . If \u03b7 \u03bbS21,1 \u2265 1 then E U\u0302t+1 diverges. Otherwise, we have\nt+ 1 \u2264 S21,1\n2S2n,n \u21d2 \u2016E U\u0302t+1 \u2212 U\u2016 \u2265 0.5 ,\nwhere the norm is the spectral norm. Furthermore, the condition number S21,1 S2n,n (where S1,1, Sn,n are the top and bottom singular values of W ) is \u2126(n3.5).\nThe proof is given at Appendix A.4. The theorem implies that the condition number of W, and hence, the number of GD iterations required for convergence, scales quite poorly with n. In the next subsection, we will try to decrease the condition number of the problem."}, {"heading": "4.1.2 Improved Condition Number through Convolutional Architecture", "text": "Examining the explicit expression for U given in Lemma 1, we see that U f can be written as a one-dimensional convolution of f with the kernel [1,\u22122, 1]. Therefore, the mapping from f to p is realizable using a convolutional layer.\nEmpirically, convergence to an accurate solution is faster using this architecture. Figure 6b illustrates a few examples. To theoretically understand the benefit of using a convolution, from the perspective of the required number of iterations for training, we will consider the new problem\u2019s condition number, providing understanding of the gap in training time. In the previous section we saw that GD requires \u2126(n3.5) iterations to learn the full matrix U . In the appendix (sections A.5 and A.6) we show that under some mild assumptions, the condition number is only \u0398(n3), and GD requires only that order of iterations to learn the optimal filter [1,\u22122, 1]."}, {"heading": "4.1.3 Additional Improvement through Explicit Conditioning", "text": "In Section 4.1.2, despite observing an improvement from the fully connected architecture, we saw that GD still requires \u2126(n3) iterations even for the simple problem of learning the filter [1,\u22122, 1]. This motivates an application of additional conditioning techniques, in the hope for extra performance gains.\nFirst, let us explicitly represent the convolutional architecture as a linear regression problem. We perform Vec2Row operation on f as follows: given a sample f , construct a matrix, F , of size n \u00d7 3, such that the t\u2019th row of F is [ft\u22121, ft, ft+1]. Then, we obtain a vanilla linear regression problem in R3, with the filter [1,\u22122, 1] as its solution. Given a sample f , we can now approximate the correlation matrix of F , denoted C \u2208 R3,3, by setting Ci,j = Ef ,t[ft\u22122+ift\u22122+j ]. We then calculate the matrixC\u22121/2 and replace every instance (namely, a row of F ) [ft\u22121, ft, ft+1] by the instance [ft\u22121, ft, ft+1]C\u22121/2. By construction, the correlation matrix of the resulting instances is approximately the identity matrix, hence the condition number is approximately 1. It follows (see again Appendix A.5) that SGD converges using order of log(1/ ) iterations, independently of n. Empirically, we quickly converge to extremely accurate results, illustrated in Figure 6c.\nWe note that the use of a convolution architecture is crucial for the efficiency of the conditioning; had the dimension of the problem not been reduced so dramatically, the difficulty of estimating a large n \u00d7 n correlation matrix scales strongly with n, and furthermore, its inversion becomes a costly operation. The combined use of a better architecture and of conditioning is what allows us to gain this dramatic improvement."}, {"heading": "4.1.4 Perhaps I should use a deeper network?", "text": "The solution arrived at in Section 4.1.3 indicates that a suitable architecture choice and conditioning scheme can provide training time speedups of multiple orders of magnitude. Moreover, the benefit of reducing the number of parameters, in the transition from a fully connected architecture to a convolutional one, is shown to be helpful in terms of convergence time. However, we should not rule out the possibility that a deeper, wider network will not suffer from the deficiencies analyzed above for the convex case.\nMotivated by the success of deep auto-encoders, we experiment with a deeper architecture for encoding f . Namely, we minimize minv1,v2 Ef [(f \u2212Mv2(Nv1(f)))2], Where Nv1 ,Mv2 are deep networks parametrized by their weight\nvectors v1,v2, with the output of N being of dimension 2k, enough for realization of the encoding problem. Each of the two networks has three layers with ReLU activations, except for the output layer of M having a linear activation. The dimensions of the layers are, 500, 100, 2k for N , and 100, 100, n for M .\nAligned with the intuition gained through the previous experiments, we observe that additional expressive power, when unnecessary, does not solve inherent optimization problems, as this stronger Auto-Encoder fails to capture the fine details of f at its non-smooth points. See Figure 6d for examples."}, {"heading": "5 Flat Activations", "text": "We now examine a different aspect of gradient-based learning which poses difficulties for optimization: namely, flatness of the loss surface due to saturation of the activation functions, leading to vanishing gradients and a slow-down of the training process. This problem is amplified in deeper architectures, since it is likely that the backpropagated message to lower layers in the architecture would vanish due to a saturated activation somewhere along the way. This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [10, 3].\nWhile non-local search-based optimization for large scale problems seems to be beyond reach, variants on the gradient update, whether by adding momentum, higher order methods, or normalized gradients, are quite successful, leading to consideration of update schemes deviating from \u201cvanilla\u201d gradient updates.\nIn this section, we consider a family of activation functions which amplify the \u201cvanishing gradient due to saturated activation\u201d problem; they are piecewise flat. Using such activations in a neural network architecture will result in\na gradient equal to 0, which will be completely useless. We consider different ways to implement, approximate or learn such activations, such that the error will effectively propagate through them. Using a different variant of a local search-based update, based on [17, 16] , we arrive at an efficient solution. Convergence guarantees exist for a one-layer architecture. We leave further study of deeper networks to future work."}, {"heading": "5.1 Experimental Setup", "text": "Consider the following optimization setup. The sample space X \u2282 Rd is symmetrically distributed. The target function y : Rd \u2192 R is of the form y(x) = u(v\u2217>x + b\u2217), where v\u2217 \u2208 Rd, b\u2217 \u2208 R, and u : R \u2192 R is a monotonically non-decreasing function. The objective of the optimization problem is given by:\nmin w E x [` (u(Nw(x)), y(x))]\nwhereNw is some neural network parametrized by w, and ` is some loss function (for example, the squared or absolute difference).\nFor the experiments, we use u of the form: u(r) = z0 + \u2211 i\u2208[55] 1[r>zi] \u00b7 (zi \u2212 zi\u22121) ,\nwhere z0 < z1 < . . . < z55 are known. In words, given r, the function rounds down to the nearest zi. We also experiment with normally distributed X . Our theoretical analysis is not restricted to u of this specific form, nor to normal X . All figures are found in Figure 7.\nOf course, applying gradient-based methods to solve this problem directly, is doomed to fail as the derivative of u is identically 0. Is there anything which can be done instead?"}, {"heading": "5.2 Non-Flat Approximation Experiment", "text": "We start off by trying to approximate u using a non flat function u\u0303 defined by\nu\u0303(r) = z0 + \u2211 i\u2208[55] (zi \u2212 zi\u22121) \u00b7 \u03c3(c \u00b7 (r \u2212 zi)),\nwhere c is some constant, and \u03c3 is the sigmoid function \u03c3(z) = (1 + exp(\u2212z))\u22121. Intuitively, we approximate the \u201csteps\u201d in u using a sum of sigmoids, each of amplitude corresponding to the step\u2019s height, and centered at the step\u2019s position. This is similar to the motivation for using sigmoids as activation functions and as gates in LSTM cells \u2014 a non-flat approximation of the step function. Below is an example for u, and its approximation u\u0303.\nu u\u0303\nThe objective is the expected squared loss, propagated through u\u0303, namely\nmin v,b E x\n[( u\u0303(v>x + b)\u2212 y(x) )2] .\nAlthough the objective is not completely flat, and is continuous, it suffers from the flatness and non continuity deficiencies of the original u, and training using this objective is much slower, and sometimes completely failing. In particular, sensitivity to the initialization of bias term is observed, where the wrong initialization can cause the starting point to be in a very wide flat region of u, and hence a very flat region of u\u0303."}, {"heading": "5.3 End-to-End Experiment", "text": "Next, we attempt to solve the problem using improper learning, with the objective now being:\nmin w E x\n[ (Nw(x)\u2212 y(x))2 ] where Nw is a network parametrized by its weight vector w. We use a simple architecture of four fully connected layers, the first three with ReLU activations and 100 output channels, and the last, with only one output channel and no activation function.\nAs covered in Section 4, difficulty arises when regressing to non smooth functions. In this case, with u not even being continuous, the inaccuracies in capturing the non continuity points are brought to the forefront. Moreover, this solution has its extra price in terms of sample complexity, training time, and test time, due to the use of a much larger than necessary network. An advantage is of course the minimal prior knowledge about u which is required. While this approach manages to find a reasonable solution, it is far from being perfect."}, {"heading": "5.4 Multi-Class Experiment", "text": "In this experiment, we approach the problem as a general multi-class classification problem, with each value of the image of u is treated as a separate class. We use a similar architecture to that of the End-to-end experiment, with one less hidden layer, and with the final layer outputting 55 outputs, each corresponding to one of the steps defined by the zis. A problem here is the inaccuracies at the boundaries between classes, due to the lack of structure imposed over the predictor. The fact that the linear connection between x and the input to u is not imposed through the architecture results in \u201cblurry\u201d boundaries. In addition, the fact that we rely on an \u201cimproper\u201d approach, in the sense that we ignore the ordering imposed by u, results in higher sample complexity."}, {"heading": "5.5 The \u201cForward-Only\u201d Update Rule", "text": "Let us go back to a direct formulation of the problem, in the form of the objective function\nmin w F (w) = E x\n[ (u(w>x)\u2212 y(x))2 ] where y(x) = u(v\u2217>x). The gradient update rule in this case is w(t+1) = w(t)\u2212\u03b7\u2207F (w(t)), where for our objective we have\n\u2207F (w) = E x\n[ (u(w>x)\u2212 y(x)) \u00b7 u\u2032(w>x) \u00b7 x ] Since u\u2032 is zero a.e., the gradient update is meaningless. [17, 16] proposed to replace the gradient with the following:\n\u2207\u0303F (w) = E x\n[ (u(w>x)\u2212 y(x)) \u00b7 x ] (5)\nIn terms of the backpropagation algorithm, this kind of update can be interpreted as replacing the backpropagation message for the activation function u with an identity message. For notation simplicity, we omitted the bias terms b, b\u2217, but the same Forward-only concept is applied to them too.\nThis method empirically achieves the best results, both in terms of final accuracy, training time, and test time cost. As mentioned before, the method is due to [17, 16], where it is proven to converge to an -optimal solution in O(L2/ 2), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm. For completeness, we provide a short proof in Appendix A.7.\nAcknowledgements: This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}, {"heading": "A Proofs", "text": "A.1 Proof of Theorem 1 Proof Given two square-integrable functions f, g on an Euclidean space Rn, let \u3008f, g\u3009L2 = Ex[f(x)g(x)] and \u2016f\u2016L2 = \u221a Ex[f2(x)] denote inner product and norm in the L2 space of square-integrable functions (with respect to the relevant distribution). Also, define the vector-valued function\ng(x) = \u2202\n\u2202w pw(x),\nand let g(x) = (g1(x), g2(x), . . . , gn(x)) for real-valued functions g1, . . . , gn. Finally, let Eh denote an expectation with respect to h chosen uniformly at random fromH. Let |H| = d.\nWe begin by proving the result for the squared loss. To prove the bound, it is enough to show that Eh \u2016\u2207Fh(w)\u2212 a\u20162 \u2264 G 2\n|H| for any vector a independent of h. In particular, let us choose a = Ex [pw(x)g(x)]. We thus bound the following:\nE h \u2016\u2207Fh(w)\u2212 E x [pw(x)g(x)] \u20162 = E h \u2016E x [(pw(x)\u2212 h(x)) g(x)]\u2212 E x [pw(x)g(x)] \u20162\n= E h \u2016E x [h(x)g(x)] \u20162 = E h n\u2211 j=1 ( E x [h(x)gj(x)] )2\n= E h n\u2211 j=1 \u3008h, gj\u30092L2 = n\u2211 j=1\n( 1\n|H| d\u2211 i=1 \u3008hi, gj\u30092L2 ) (\u2217) \u2264\nn\u2211 j=1 ( 1 |H| \u2016gj\u20162L2 ) = 1 |H| n\u2211 j=1 E x [g2j (x)]\n= 1\n|H| E x\n[ \u2016g(x)\u20162 ] \u2264 G(w) 2\n|H| ,\nwhere (\u2217) follows from the functions inH being mutually orthogonal, and satisfying \u2016h\u2016L2 \u2264 1 for all h \u2208 H. To handle a classification loss, note that by its definition and the fact that h(x) \u2208 {\u22121,+1},\n\u2207Fh(w) = E x\n[ r\u2032(h(x)pw(x)) \u00b7 \u2202\n\u2202w pw(x) ] = E\nx\n[( r\u2032(pw(x)) + r\n\u2032(\u2212pw(x)) 2 + h(x) \u00b7 r \u2032(pw(x))\u2212 r\u2032(\u2212pw(x)) 2 ) \u00b7 \u2202 \u2202w pw(x) ] = E\nx\n[ r\u2032(pw(x)) + r\n\u2032(\u2212pw(x)) 2 \u00b7 \u2202 \u2202w pw(x)\n] + E\nx\n[ h(x) \u00b7 ( r\u2032(pw(x))\u2212 r\u2032(\u2212pw(x))\n2\n) \u00b7 \u2202 \u2202w pw(x) ] .\nLetting g(x) = ( r\u2032(pw(x))\u2212r\u2032(\u2212pw(x))\n2\n) \u00b7 \u2202\u2202wpw(x) (which satisfies Ex[\u2016g(x)\u2016 2] \u2264 G2 since r is 1-Lipschitz) and\na = Ex [ r\u2032(pw(x))+r \u2032(\u2212pw(x)) 2 \u00b7 \u2202 \u2202wpw(x) ] (which does not depend on h), we get that\nE h \u2016\u2207Fh(w)\u2212 a\u20162 = E h \u2016E x [h(x)g(x)]\u20162.\nProceeding now exactly in the same manner as the squared loss case, the result follows.\nA.2 Proof of lemma 1 Proof\n(UW )i,j = \u2211 t Ui,tWt,j = Wi,j \u2212 2Wi\u22121,j +Wi\u22122,j\nIf i \u2265 j + 1 then Wi,j+Wi\u22122,j2 = Wi\u22121,j and therefore the above is clearly zero. If i < j then all the values of W are zeros. Finally, if i = j we obtain 1. This concludes our proof.\nA.3 Proof of lemma 2 Proof Given a sample f , and that our current weight matrix is U\u0302 , let p = W\u22121f . The loss function on f is given by\n1 2 \u2016U\u0302 f \u2212 p\u20162\nThe gradient w.r.t. U\u0302 is \u2207 = (U\u0302 f \u2212 p)f> = U\u0302ff> \u2212 pf>\nWe obtain that the update rule is U\u0302t+1 = U\u0302t \u2212 \u03b7 ( U\u0302tff > \u2212 pf> ) = U\u0302t(I \u2212 \u03b7ff>) + \u03b7pf>\nTaking expectation with respect to the random choice of the pair, using again f = Wp, and assuming Epp> = \u03bbI , we obtain that the stochastic gradient update rule satisfies\nE U\u0302t+1 = U\u0302t(I \u2212 \u03b7\u03bbWW>) + \u03b7\u03bbW>\nContinuing recursively, we obtain\nE U\u0302t+1 = E U\u0302t(I \u2212 \u03b7\u03bbWW>) + \u03b7\u03bbW> = [ E U\u0302t\u22121(I \u2212 \u03b7\u03bbWW>) + \u03b7\u03bbW> ] (I \u2212 \u03b7\u03bbWW>) + \u03b7\u03bbW>\n= E U\u0302t\u22121(I \u2212 \u03b7\u03bbWW>)2 + \u03b7\u03bbW>(I \u2212 \u03b7\u03bbWW>) + \u03b7\u03bbW>\n= U\u03020(I \u2212 \u03b7\u03bbWW>)t + \u03b7\u03bbW> t\u2211 i=0 (I \u2212 \u03b7\u03bbWW>)i\nWe assume that U\u03020 = 0, and thus\nE U\u0302t+1 = \u03b7\u03bbW> t\u2211 i=0 (I \u2212 \u03b7\u03bbWW>)i\nA.4 Proof of Theorem 3 Proof Fix some i, we have that\n(I \u2212 \u03b7 \u03bbWW>)i = (QIQ> \u2212 \u03b7 \u03bbQSV >V SQ>)i = Q(I \u2212 \u03b7 \u03bbSS)iQ> = Q\u039biQ>\nwhere \u039bi is diagonal with \u039bij,j = (1\u2212 \u03b7\u03bbS2j )i. Therefore, by the properties of geometric series, E U\u0302t+1 converges if and only if \u03b7 \u03bbS21,1 < 1. When this condition holds we have that\nU\u0302\u221e = \u03b7 \u03bbW > \u221e\u2211 i=0 (I \u2212 \u03b7 \u03bbWW>)i\n= \u03b7 \u03bbW>(\u03b7 \u03bbWW>)\u22121 = W>(WW>)\u22121\n= V SQ>(QSV >V SQ>)\u22121 = V SQ>QS\u22122Q> = V S\u22121Q> = U .\nTherefore,\nE U\u0302t+1 \u2212 U = \u03b7 \u03bbW> \u221e\u2211\ni=t+1\n(I \u2212 \u03b7 \u03bbWW>)i\n= \u03b7 \u03bbV SQ> \u221e\u2211\ni=t+1\nQ\u039biQ>\n= V [ \u221e\u2211 i=t+1 (\u03b7 \u03bbS)\u039bi ] Q> .\nThe matrix in the parentheses is diagonal, where the j\u2019th diagonal element is\n\u03b7\u03bbSj,j \u00b7 (1\u2212 \u03b7\u03bbS2j,j)t+1\n\u03b7\u03bbS2j,j = S\u22121j,j (1\u2212 \u03b7\u03bbS 2 j,j) t+1\nIt follows that \u2016E U\u0302t+1 \u2212 U\u2016 = max\nj S\u22121j,j (1\u2212 \u03b7\u03bbS 2 j,j) t+1\nUsing the inequality (1\u2212 a)t+1 \u2265 1\u2212 (t+ 1)a, that holds for every a \u2208 (\u22121, 1), we obtain that\n\u2016E U\u0302t+1 \u2212 U\u2016 \u2265 S\u22121n,n(1\u2212 (t+ 1)\u03b7\u03bbS2n,n) \u2265 S\u22121n,n(1\u2212 (t+ 1) S2n,n S21,1 ) .\nIt follows that whenever,\nt+ 1 \u2264 S21,1\n2S2n,n ,\nwe have that \u2016E U\u0302t+1 \u2212 U\u2016 \u2265 0.5S\u22121n,n. Finally, observe that\nS2n,n = min x:\u2016x\u2016=1 x>WW>x \u2264 e>1 WW>e1 = 1 ,\nhence S\u22121n,n \u2265 1.\nWe now prove the second part of the theorem, regarding the condition number of W>W , namely, S21,1\n2S2n,n \u2265\n\u2126(n3.5). We note that the condition number of W>W can be calculated through its inverse matrix\u2019s, namely, U>U \u2019s condition number, as those are equal.\nIt is easy to verify that Uen = en. Therefore, the maximal eigenvalue of U>U is at least 1. To construct an upper bound on the minimal eigenvalue of U>U , consider v \u2208 Rn s.t. for i \u2264 \u221a n we have vi = \u2212 12 (i/n) 2 and for i > \u221a n we have vi = 12n \u2212 i/n\u221a n . We have that |vi| = O(1/n) for i < \u221a n + 2 and v is linear for i \u2265 \u221a n. This implies that (Uv)i = 0 for i \u2265 \u221a n + 2. We also have (Uv)1 = v1 = \u22120.5/n2, (Uv)2 = \u22122v1 + v2 \u2248 \u22121/n2, and for i \u2208 {3, . . . , \u221a n} we have\n(Uv)i = vi\u22122 \u2212 2vi\u22121 + vi = \u22123/n2\nFinally, for i = \u221a n+ 1 we have\n(Uv)i = vi\u22122 \u2212 2vi\u22121 + vi = vi \u2212 vi\u22121 \u2212 (vi\u22121 \u2212 vi\u22122)\n= 1 n \u221a n \u2212 1 2n2 ( (i\u2212 1)2 \u2212 (i\u2212 2)2 ) = 1 n \u221a n \u2212 1 2n2 (2i\u2212 3) = 1 2n2 .\nThis yields \u2016Uv\u20162 \u2248 \u0398( \u221a n/n4) = \u0398(n\u22123.5)\nIn addition,\n\u2016v\u20162 \u2265 n\u2211\ni=n/2\nv2i \u2265 n\n2 v2n/2 =\nn\n2\n( 1\n2n \u2212 1 2 \u221a n\n)2 = \u2126(1) .\nTherefore, \u2016Uv\u20162\n\u2016v\u20162 = O(n\u22123.5) ,\nwhich implies that the minimal eigenvalue of U>U is at most O(n\u22123.5). All in all, we have shown that the condition number of U>U is \u2126(n\u22123.5), implying the same over W>W .\nA.5 Gradient Descent for Linear Regression The loss function is\nE x,y\n1 2 (x>w \u2212 y)2\nThe gradient at w is \u2207 = E\nx,y x(x>w \u2212 y) = ( E x xx> ) w \u2212 E x,y xy := Cw \u2212 z\nFor the optimal solution we have Cw\u2217 \u2212 z = 0, hence z = Cw\u2217. The update is therefore\nwt+1 = wt \u2212 \u03b7(Cwt \u2212 z) = (I \u2212 \u03b7C)wt + \u03b7z = . . . = t\u2211 i=0 (I \u2212 \u03b7C)i\u03b7z = t\u2211 i=0 (I \u2212 \u03b7C)i\u03b7Cw\u2217\nLet C = V DV > be the eigenvalue decomposition of C. Observe that\nwt+1 = V t\u2211 i=0 \u03b7(I \u2212 \u03b7D)iDV >w\u2217\nHence\n\u2016w\u2217 \u2212 wt+1\u2016 = \u2016(V V > \u2212 V t\u2211 i=0 \u03b7(I \u2212 \u03b7D)iDV >)w\u2217\u2016\n= \u2016(I \u2212 t\u2211 i=0 \u03b7(I \u2212 \u03b7D)iD)V >w\u2217\u2016 = \u2016(I \u2212 \u03b7D)t+1 V >w\u2217\u2016 ,\nwhere the last equality is because for every j we have\n(I \u2212 t\u2211 i=0 \u03b7(I \u2212 \u03b7D)iD)j,j = 1\u2212 t\u2211 i=0 \u03b7(1\u2212 \u03b7Dj,j)iDj,j = (1\u2212 \u03b7Dj,j)t+1 .\nDenote v\u2217 = V >w\u2217. We therefore obtain that\n\u2016w\u2217 \u2212 wt+1\u20162 = n\u2211 j=1 ( (1\u2212 \u03b7Dj,j)t+1v\u2217j )2 To obtain an upper bound, choose \u03b7 = 1/D1,1 and t + 1 \u2265 D1,1Dn,n log(\u2016w\n\u2217\u2016/ ), and then, using 1 \u2212 a \u2264 e\u2212a, we get that\n\u2016w\u2217 \u2212 wt+1\u20162 \u2264 n\u2211 j=1 ( exp(\u2212\u03b7Dj,j(t+ 1))v\u2217j )2 \u2264 2 \u2016w\u2217\u20162 n\u2211 j=1 ( v\u2217j )2 = 2 .\nTo obtain a lower bound, observe that if v\u22171 is non-negligible then \u03b7 must be at most 1/D1,1 (otherwise the process will diverge). If in addition v\u2217n is a constant (for simplicity, say v \u2217 n = 1), then\n\u2016w\u2217 \u2212 wt+1\u2016 \u2265 (1\u2212 \u03b7Dn,n)t+1 \u2265 (1\u2212Dn,n/D1,1)t+1 \u2265 1\u2212 (t+ 1)Dn,n/D1,1 ,\nwhere we used (1\u2212 a)t+1 \u2265 1\u2212 (t+ 1)a for a \u2208 [\u22121, 1]. It follows that if t+ 1 < 0.5Dn,n/D1,1 then we have that \u2016w\u2217 \u2212 wt+1\u2016 \u2265 0.5.\nA.6 The Covariance Matrix of Section 4.1.2 Denote by C \u2208 R3,3 the covariance matrix, and let \u03bbi(C) denote the i\u2019th eigenvalue of C (in a decreasing order). The condition number of C is \u03bb1(C)/\u03bb3(C). Below we derive lower and upper bounds on the condition number under some assumptions.\nLower bound We assume that Ef,t f2t = \u2126(n2) (this would be the case in many typical cases, for example when the allowed slopes are in {\u00b11}), that k (the number of pieces of the curve) is constant, and that the changes of slope are in [\u22121, 1].\nNow, take v = [1, 1, 1]>, then\nv>Cv = E f,t (ft\u22121 + ft + ft+1) 2 = \u2126(n2)\nThis yields \u03bb1(C) \u2265 \u2126(n2). Next, take v = [1,\u22122, 1]> we obtain\nv>Cv = E f,t (ft\u22121 \u2212 ft + ft+1)2 = O(k/n)\nThis yields \u03bb3(C) \u2264 O(k/n). All in all, we obtain that the condition number of C is \u2126(n3).\nUpper bound We consider distribution over f s.t. for every f , at exactly k indices f changes slope from 1 to \u22121 or from \u22121 to 1 (with equal probability over the indices), and at the rest of the indices we have that f is linear with a slope of 1 or \u22121. Denote p = k/n. Take any unit vector v, and denote v\u0304 = v1 + v2 + v3. Then\nv>Cv = E f,t (v1ft\u22121 + v2ft + v3ft+1) 2\n= E f\n[ 0.5(1\u2212 p) ( (v\u0304ft + v3 \u2212 v1)2 + (v\u0304ft + v1 \u2212 v3)2 ) + 0.5p ( (v\u0304ft + v3 + v1) 2 + (v\u0304ft \u2212 v3 \u2212 v1)2 )]\n= v\u03042 E f f2t + (1\u2212 p)(v1 \u2212 v3)2 + p(v1 + v3)2 .\nSince Ef,t f2t = \u0398(n2), it is clear that v>Cv = O(n2). We next establish a lower bound of \u2126(1/n). Observe that if v\u03042 \u2265 \u2126(1/n3), we are done. If this is not the case, then \u2212v2 \u2248 v1 + v3. If |v1 + v3| > 0.1, we are done. Otherwise, 0.9 \u2264 1\u2212 v22 = v21 + v23 , so we must have that v1 and v3 has opposite signs, and one of them is large, hence (v1\u2212 v3)2 is larger than a constant. This concludes our proof.\nA.7 Proof of Update Rule 5 Convergence in the Lipschitz Case Proof Let B be an upper bound over \u2016w(t)\u2016 for all time step t, and over \u2016v\u2217\u2016. Moreover, assume |u| \u2264 c for some constant c. We denote the update rule by w(t) = w(t\u22121) + \u03b7\u2207\u0303, and bound from below:\n\u2016w(t) \u2212 v\u2217\u20162 \u2212 \u2016w(t+1) \u2212 v\u2217\u20162 = 2\u3008w(t) \u2212 v\u2217, \u03b7\u2207\u0303\u3009 \u2212 \u03b72\u2016\u2207\u0303\u20162 = 2\u03b7 E [ (u((w(t))>x)\u2212 u((v\u2217)>x))(w(t) \u2212 v\u2217)x ] \u2212 \u03b72\u2016\u2207\u0303\u20162\n(1) \u2265 2\u03b7 L\nE [ (u((w(t))>x)\u2212 u((v\u2217)>x))2 ] \u2212 \u03b72\u2016\u2207\u0303\u20162\n(2) \u2265 2\u03b7 L\nE [ (u((w(t))>x)\u2212 u((v\u2217)>x))2 ] \u2212 \u03b72B2c2\nwhere (1) follows from L-Lipschitzness and monotonicity of u, (2) follows from bounding \u2016w\u2016, \u2016x\u2016 and u. Let the expected error of the regressor parametrized by wt be denoted et. We separate into cases:\n\u2022 If \u2016wt \u2212 v\u2217\u20162 \u2212 \u2016wt+1 \u2212 v\u2217\u20162 \u2265 \u03b72B2c2, we can rewrite \u2016wt \u2212 v\u2217\u20162 \u2212 \u03b72B2c2 \u2265 \u2016wt+1 \u2212 v\u2217\u20162, and note that since \u2016wt+1\u2212v\u2217\u20162 \u2265 0, and \u2016w0\u2212v\u2217\u20162 \u2264 B2, there can be at most B 2\n\u03b72B2c2 = 1 \u03b72c2 iterations where this condition will hold.\n\u2022 Otherwise, we get that et \u2264 \u03b7B2c2L.\nTherefore, for a given , by taking T = c 4B4L2 2 , and setting \u03b7 = \u221a 1 Tc2 , we obtain that after T iterations, the first case is not holding anymore, and the second case implies eT \u2264 ."}, {"heading": "B Technical Lemmas", "text": "Lemma 3 Any parity function over d variables is realizable by a network with one fully connected layer of width d\u0303 > 3d2 with ReLU activations, and a fully connected output layer with linear activation and a single unit.\nProof Let the weights entering each of the first 3d2 hidden units be set to v \u2217, and the rest to 0. Further assume that for i \u2208 [d/2], the biases of the first 3i + {1, 2, 3} units are set to \u2212(2i \u2212 12 ), \u22122i, \u2212(2i + 1 2 ) respectively, and that their weights in the output layer are 1, \u22122, and 1. It is not hard to see that the weighted sum of those triads of neurons is 1 2 if \u3008x,v\n\u2217\u3009 = 2i, and 0 otherwise. Observe that there\u2019s such a triad defined for each even number in the range [d]. Therefore, the output of this net is 0 if y = \u22121, and 12 otherwise. It is easy to see that scaling of the output layer\u2019s weights by 4, and introduction of a \u22121 bias value to it, results in a perfect predictor."}, {"heading": "C Command Lines for Experiments", "text": "Our experiments are implemented in a simple manner in python. We use the tensorflow package for optimization. The following command lines can be used for viewing all optional arguments:\nTo run experiment 2.1, use:\npython ./parity.py --help\nTo run experiment 3.1, use:\npython ./tuple_rect.py --help\nFor SNR estimations, use:\npython ./tuple_rect_SNR.py --help\nTo run experiment 3.2, use:\npython ./dec_vs_e2e_stocks.py --help\nFor Section 4\u2019s experiments, given below are the command lines used to generate the plots. Additional arguments can be viewed by running:\npython PWL_fail1.py --help\nTo run experiment 4.1.1, use:\npython PWL_fail1.py --FtoK\nTo run experiment 4.1.2, use:\npython PWL_fail1.py --FtoKConv\nTo run experiment 4.1.3, use:\npython PWL_fail1.py --FtoKConvCond --batch_size 10 --number_of_iterations 500 --learning_rate 0.99\nTo run experiment 4.1.4, use:\npython PWL_fail1.py --FAutoEncoder\nTo run Section 5\u2019s experiments, run:\npython step_learn.py --help"}], "references": [{"title": "Weakly learning dnf and characterizing statistical query learning using fourier analysis", "author": ["Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich"], "venue": "In Proceedings of the twentysixth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Multitask learning. In Learning to learn, pages 95\u2013133", "author": ["Rich Caruana"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Statistical query algorithms for stochastic convex optimization", "author": ["Vitaly Feldman", "Cristobal Guzman", "Santosh Vempala"], "venue": "arXiv preprint arXiv:1512.09170,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I Jordan", "Philipp Moritz"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Shai Shalev-Shwartz", "Alon Gonen", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1106.1622,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "On the sample complexity of end-to-end training vs. semantic abstraction training", "author": ["Shai Shalev-Shwartz", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1604.06915,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton"], "venue": "ICML (3),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 10, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 21, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 29, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 167, "endOffset": 183}, {"referenceID": 3, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 11, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 8, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 238, "endOffset": 252}, {"referenceID": 20, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 274, "endOffset": 282}, {"referenceID": 22, "context": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-the-art performances, ranging the fields of computer vision [20, 11, 22, 30], audio and natural language processing and generation [4, 12, 9, 31], as well as robotics [21, 23], to name just a few.", "startOffset": 274, "endOffset": 282}, {"referenceID": 26, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 164, "endOffset": 172}, {"referenceID": 18, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 164, "endOffset": 172}, {"referenceID": 7, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 197, "endOffset": 200}, {"referenceID": 27, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 223, "endOffset": 227}, {"referenceID": 19, "context": "The list of success stories can be matched and surpassed by a list of practical \u201ctips and tricks\u201d, from different optimization algorithms, parameter tuning methods [27, 19], initialization schemes [8], architecture designs [28], loss functions, data augmentation [20] and so on.", "startOffset": 263, "endOffset": 267}, {"referenceID": 25, "context": "Indeed, in [26], it was shown that this also holds in a more general setup, when the output y corresponds to a linear function composed with a periodic one, and the input x is sampled from a smooth distribution: Theorem 2 (Shamir 2016) Let \u03c8 be a fixed periodic function, and let H = {x 7\u2192 \u03c8(v\u2217>x) : \u2016v\u2217\u2016 = r} for some r > 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": ", 1), this problem is known to be solvable with an appropriate LSTM network [14]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Finally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [18, 1].", "startOffset": 188, "endOffset": 195}, {"referenceID": 0, "context": "Finally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [18, 1].", "startOffset": 188, "endOffset": 195}, {"referenceID": 5, "context": "Recently, [6] have shown that gradient-based methods with an approximate gradient oracle can be implemented as a statistical query algorithm, which implies that gradient-based methods are indeed unlikely to solve learning problems which are known to be hard in the statistical queries framework, in particular parities.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 28, "endOffset": 35}, {"referenceID": 8, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 28, "endOffset": 35}, {"referenceID": 12, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 28, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 1, "context": "Prior results of End-to-end [21, 9] and decomposition and added feedback [13, 29, 2] approaches show success in both directions.", "startOffset": 73, "endOffset": 84}, {"referenceID": 24, "context": "For example, [25] analyzed the difference between the approaches from the sample complexity point of view.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Let X1 denote the space of 28 \u00d7 28 binary images, with a distribution D defined by the following sampling procedure: \u2022 Sample \u03b8 \u223c U([0, \u03c0]), l \u223c U([5, 28\u2212 5]), (x, y) \u223c U([0, 27]).", "startOffset": 171, "endOffset": 178}, {"referenceID": 0, "context": "\u2022 Concatenate the \u201cscores\u201d of a tuple\u2019s entries, transform them to the range [0, 1] using a sigmoid function, and feed the resulting vector into another network, N (2) w2 , of a similar architecture to the one defined in Section 2, outputting a single \u201ctuple-score\u201d, which can then be thresholded for obtaining the binary prediction.", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "In our context, [7] showed that when running SGD (even on non-convex objectives), arriving at a point where \u2016\u2207wL(w)\u2016 \u2264 requires order of \u03bd\u0304/ 2 iterations, where \u03bd\u0304 = max t E x,q \u2016\u2207w(x, q)\u2016 \u2212 \u2016\u2207wL(w)\u2016,", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [11, 29].", "startOffset": 116, "endOffset": 124}, {"referenceID": 28, "context": "New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [11, 29].", "startOffset": 116, "endOffset": 124}, {"referenceID": 14, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 18, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 4, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 23, "context": ", [15, 19, 5, 24]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 9, "context": "This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [10, 3].", "startOffset": 119, "endOffset": 126}, {"referenceID": 2, "context": "This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [10, 3].", "startOffset": 119, "endOffset": 126}, {"referenceID": 16, "context": "Using a different variant of a local search-based update, based on [17, 16] , we arrive at an efficient solution.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Using a different variant of a local search-based update, based on [17, 16] , we arrive at an efficient solution.", "startOffset": 67, "endOffset": 75}, {"referenceID": 16, "context": "[17, 16] proposed to replace the gradient with the following: \u2207\u0303F (w) = E x [ (u(w>x)\u2212 y(x)) \u00b7 x ] (5)", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[17, 16] proposed to replace the gradient with the following: \u2207\u0303F (w) = E x [ (u(w>x)\u2212 y(x)) \u00b7 x ] (5)", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "As mentioned before, the method is due to [17, 16], where it is proven to converge to an -optimal solution in O(L/ ), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm.", "startOffset": 42, "endOffset": 50}, {"referenceID": 15, "context": "As mentioned before, the method is due to [17, 16], where it is proven to converge to an -optimal solution in O(L/ ), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm.", "startOffset": 42, "endOffset": 50}], "year": 2017, "abstractText": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "creator": "LaTeX with hyperref package"}}}