{"id": "1703.08434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Linear classifier design under heteroscedasticity in Linear Discriminant Analysis", "abstract": "Among assumptions about normality and homoscedasticity, Linear Discrimination Analysis (LDA) is known as optimal for minimizing the Bayes error for binary classification. In the heteroscedastic case, there is no guarantee that LDA will minimize this error directly. In addition, assuming heteroscedasticity, we derive a linear classifier, the Gaussian Linear Discriminant (GLD), which directly minimizes the Bayes error for binary classification. In addition, we also propose a local neighborhood search algorithm (LNS) to obtain a more robust classifier when the data is known to have an abnormal distribution. We evaluate the proposed classifiers against two artificial and ten real datasets covering a wide range of application areas, including handwriting, medical diagnosis and remote sensing, and compare our algorithm with existing LDA approaches and other linear classifiers.", "histories": [["v1", "Fri, 24 Mar 2017 14:45:12 GMT  (20kb)", "http://arxiv.org/abs/1703.08434v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kojo sarfo gyamfi", "james brusey", "rew hunt", "elena gaura"], "accepted": false, "id": "1703.08434"}, "pdf": {"name": "1703.08434.pdf", "metadata": {"source": "CRF", "title": "Linear classifier design under heteroscedasticity in Linear Discriminant Analysis", "authors": ["Kojo Sarfo Gyamfi", "James Brusey", "Andrew Hunt", "Elena Gaura"], "emails": ["gyamfik@uni.coventry.ac.uk", "j.brusey@coventry.ac.uk", "ab8187@coventry.ac.uk", "csx216@coventry.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n08 43\n4v 1\n[ cs\n.L G\n] 2\n4 M\nUnder normality and homoscedasticity assumptions, Linear Discriminant Analysis (LDA) is known to be optimal in terms of minimising the Bayes error for binary classification. In the heteroscedastic case, LDA is not guaranteed to minimise this error. Assuming heteroscedasticity, we derive a linear classifier, the Gaussian Linear Discriminant (GLD), that directly minimises the Bayes error for binary classification. In addition, we also propose a local neighbourhood search (LNS) algorithm to obtain a more robust classifier if the data is known to have a non-normal distribution. We evaluate the proposed classifiers on two artificial and ten real-world datasets that cut across a wide range of application areas including handwriting recognition, medical diagnosis and remote sensing, and then compare our algorithm against existing LDA approaches and other linear classifiers. The GLD is shown to outperform the original LDA procedure in terms of the classification accuracy under heteroscedasticity. While it compares favourably with other existing heteroscedastic LDA approaches, the GLD requires as much as 60 times lower training time on some datasets. Our comparison with the support vector machine (SVM) also shows that, the GLD, together with the LNS, requires as much as 150 times lower training time to achieve an equivalent classification accuracy on some of the datasets. Thus, our algorithms can provide a cheap and reliable option for classification in a lot of expert systems.\nKeywords: LDA, Heteroscedasticity, Bayes Error, Linear Classifier\n\u2217Corresponding author: Email addresses: gyamfik@uni.coventry.ac.uk (Kojo Sarfo Gyamfi),\nj.brusey@coventry.ac.uk (James Brusey), ab8187@coventry.ac.uk (Andrew Hunt), csx216@coventry.ac.uk (Elena Gaura)\nPreprint submitted to Expert Systems with Applications March 27, 2017"}, {"heading": "1. Introduction", "text": "In many applications one encounters the need to classify a given object under one of a number of distinct groups or classes based on a set of features known as the feature vector. A typical example is the task of classifying a machine part under one of a number of health states. Other applications that involve classification include face detection, object recognition, medical diagnosis, credit card fraud prediction and machine fault diagnosis.\nA common treatment of such classification problems is to model the conditional density functions of the feature vector (Ng & Jordan, 2002). Then, the most likely class to which a feature vector belongs can be chosen as the class that maximises the a posteriori probability of the feature vector. This is known as the maximum a posteriori (MAP) decision rule.\nLet K be the number of classes, Ck be the kth class, x be a feature vector and Dk be training samples belonging to the kth class (k \u2208 {1, 2, ..., K}). The MAP decision rule for the classification task is then to choose the most likely class of x, C\u2217(x) given as:\nC\u2217(x) = argmax Ck p(Ck|x), k \u2208 {1, 2, ..., K} (1)\nWe assume for the moment that there are only K = 2 classes, i.e. binary classification (we consider multi-class classification in a later section). Then, using Bayes\u2019 rule, the two posterior probabilities can be expressed as:\np(Ck|x) = p(x|Ck)\u00d7 p(Ck)\np(x) , k \u2208 {1, 2} (2)\nIt is often the case that the prior probabilities p(C1) and p(C2) are known, or else they may be estimable from the relative frequencies of D1 and D2 in D where D = D1 \u222a D2. Let these priors be given by \u03c01 and \u03c02 respectively for class C1 and C2. Then, the likelihood ratio defined as:\n\u03bb(x) = p(x|C1) p(x|C2)\n(3)\nis compared against a threshold defined as \u03c4 = \u03c02/\u03c01 so that one decides on class C1 if \u03bb(x) \u2265 \u03c4 and class C2 otherwise.\nLinear Discriminant Analysis (LDA) proceeds from here with two basic assumptions (Izenman, 2009, Chapter 8):\n1. The conditional probabilities p(x|C1) and p(x|C2) have multivariate normal distributions.\n2. The two classes have equal covariance matrices, an assumption known as homoscedasticity.\nLet x\u03041, \u03a31 be the mean and covariance matrix of D1 and x\u03042, \u03a32 be the mean and covariance of D2 respectively. Then, for k \u2208 {1, 2},\np(x|Ck) = 1 \u221a\n(2\u03c0)d det(\u03a3k) exp\n[\n\u2212 1 2 (x\u2212 x\u0304k)T\u03a3\u22121k (x\u2212 x\u0304k)\n]\n(4)\nwhere d is the dimensionality of X , which is the feature space of x. Given the above definitions of the conditional probabilities, one may obtain a loglikelihood ratio given as:\nln\u03bb(x)\n= 1\n2 ln det\u03a32 det\u03a31 + 1 2\n[ (x\u2212 x\u03042)T\u03a3\u221212 (x\u2212 x\u03042)\u2212 (x\u2212 x\u03041)T\u03a3\u221211 (x\u2212 x\u03041) ] (5)\nwhich is then compared against ln \u03c4 so that C1 is chosen if ln\u03bb(x) \u2265 ln \u03c4 , and C2 otherwise. Thus, the decision rule for classifying a vector x under class C1 can be rewritten as:\n(x\u2212 x\u03042)T\u03a3\u221212 (x\u2212 x\u03042)\u2212 (x\u2212 x\u03041)T\u03a3\u221211 (x\u2212 x\u03041) \u2265 ln \u03c4 2 det\u03a31 det\u03a32\n(6)\nIn general, this result is a quadratic discriminant. However, a linear classifier is often desired for the following reasons:\n1. A linear classifier is robust against noise since it tends not to overfit (Mika et al., 1999).\n2. A linear classifier has relatively shorter training and testing times (Yuan et al., 2012).\n3. Many linear classifiers allow for a transformation of the original feature space into a higher dimensional feature space using the kernel trick for better classification in the case of a non-linear decision boundary (Bishop, 2006, Chapter 6).\nBy calling on the assumption of homoscedasticity, i.e. \u03a31 = \u03a32 = \u03a3x, the original quadratic discriminant given by (6) for classifying a given vector\nx decomposes into the following linear decision rule:\nxT\u03a3\u22121x (x\u03041 \u2212 x\u03042) C1\nR C2\nln \u03c4 + 1\n2 (x\u0304T1\u03a3 \u22121 x x\u03041 \u2212 x\u0304T2\u03a3\u22121x x\u03042) (7)\nHere, \u03a3\u22121x (x\u03041\u2212x\u03042) is a vector of weights denoted byw and ln \u03c4+ 12(x\u0304T1\u03a3\u22121x x\u03041\u2212 x\u0304T2\u03a3 \u22121 x x\u03042) is a threshold denoted by w0. This linear classifier is also known as Fishers Linear Discriminant. If only the weight vector w is required for dimensionality reduction, w may be obtained by maximising Fishers criterion (Fisher, 1936), given by:\nS = wT (x\u03041 \u2212 x\u03042)(x\u03041 \u2212 x\u03042)Tw\nwT\u03a3xw (8)\nwhere \u03a3x = n1\u03a31 + n2\u03a32 and n1, n2 are the cardinalities of D1 and D2 respectively.\nLDA is the optimal Bayes\u2019 classifier for binary classification if the normality and homoscedasticity assumptions hold (Hamsici & Martinez, 2008) (Izenman, 2009, Chapter 8). It demands only the computation of the dot product between w and x, which is a relatively computationally inexpensive operation.\nAs a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al., 1999). LDA has been applied to several problems such as medical diagnosis e.g. (Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.g. (Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.g. (Mahmoudi & Duman, 2015). The widespread use of LDA in these areas is not because the datasets necessarily satisfy the normality and homoscedasticity assumptions, but mainly due to the robustness of LDA against noise, being a linear model (Mika et al., 1999). Since the linear Support Vector Machine (SVM) can be quite expensive to train, especially for large values of K or n (n = n1 + n2), LDA is often relied upon (Hariharan et al., 2012).\nYet, practical implementation of LDA is not without problems. Of note is the small sample size (SSS) problem that LDA faces with high-dimensional data and much smaller training data (Sharma & Paliwal, 2015; Lu et al.,\n2003). When d \u226b n, the scatter matrix \u03a3x is not invertible, as it is not full-rank. Since the decision rule as given by (7) requires the computation of the inverse of \u03a3x, the singularity of \u03a3x makes the solution infeasible. In works by, for example, (Liu et al., 2007; Paliwal & Sharma, 2012), this problem is overcome by taking the Moore-Penrose pseudo-inverse of the scatter matrix, rather than the ordinary matrix inverse. Sharma & Paliwal (2008) use a gradient descent approach where one starts from an initial solution of w and moves in the negative direction of the gradient of Fisher\u2019s criterion (8). This method avoids the computation of an inverse altogether. Another approach to solving the SSS problem involves adding a scalar multiple of the identity matrix to the scatter matrix to make the resulting matrix nonsingular, a method known as regularised discriminant analysis (Friedman, 1989; Lu et al., 2003).\nHowever, for a given dataset that does not satisfy the homoscedasticity or normality assumption, one would expect that modifications to the original LDA procedure accounting for these violations would yield an improved performance. One such modification, in the case of a non-normal distribution, is the mixture discriminant analysis (Hastie & Tibshirani, 1996; McLachlan, 2004; Ju et al., 2003) in which a non-normal distribution is modelled as a mixture of Gaussians. However, the parameters of the mixture components or even the number of mixture components, are usually not known a priori. Other non-parametric approaches to LDA that remove the normality assumption involve using local neighbourhood structures (Cai et al., 2007; Fukunaga & Mantock, 1983; Li et al., 2009) to construct a similarity matrix instead of the scatter matrix \u03a3x used in LDA. However, these approaches aim at linear dimensionality reduction, rather than linear classification. Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008). KFD maps the original feature space X into some other space Y (usually higher dimensional) via the kernel trick (Mika et al., 1999). While the main utility of the kernel is to guarantee linear separability in the transformed space, the kernel may also be employed to transform non-normal data into one that is near-normal.\nOur proposed method differs from the above approaches in that we primarily consider violation of the homoscedasticity assumption, and do not address the SSS problem. We seek to provide a linear approximation to the quadratic boundary given by (6) under heteroscedasticity without any kernel transformation; we note that several heteroscedastic LDA approaches have\nbeen proposed to this effect. Nevertheless, for reasons which we highlight in the next section, our contributions in this paper are stated explicitly as follows:\n1. We propose a novel linear classifier, which we term the Gaussian Linear Discriminant (GLD), that directly minimises the Bayes error under heteroscedasticity via an efficient optimisation procedure. This is presented in Section 3. 2. We propose a local neighbourhood search method to provide a more robust classifier if the data has a non-normal distribution (Section 4)."}, {"heading": "2. Related Work", "text": "Under the heteroscedasticity assumption, many LDA approaches have been proposed among which we mention (Fukunaga, 2013, Chapter 4),(Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002). As it is known that Fisher\u2019s criterion (whose maximisation is equivalent to the LDA derivation described in the Introduction section) only takes into account the difference in the projected class means, existing heteroscedastic LDA approaches tend to obtain a generalisation on Fisher\u2019s criterion. In the work of (Loog & Duin, 2002), for instance, a directed distance matrix (DDM) known as the Chernoff distance, which takes into account the difference in covariance matrices between the two classes as well as the projected class means, is maximised instead of Fisher\u2019s criterion (8). The same idea employing the Chernoff criterion is used by (Duin & Loog, 2004). A wider class of Bregman divergences including the Bhattacharya distance (Decell Jr & Marani, 1976) and the Kullback-Leibler divergence (Decell & Mayekar, 1977) have also been used for heteroscedastic LDA, as Fisher\u2019s criterion can be considered a special case of these measures when the covariance matrices of the classes are equal.\nHowever, most of these approaches aim at linear dimensionality reduction, which involves finding a linear transformation that transforms the original data into one of reduced dimensionality, while at the same time maximising the discriminatory information between the classes. Our focus with this paper, however, is not on dimensionality reduction, but on obtaining a Bayes optimal linear classifier for binary classification assuming that the covariance matrices are not equal. As far as we know, the closest work to ours in this regard are the works by (Marks & Dunn, 1974; Anderson & Bahadur, 1962; Peterson & Mattson, 1966; Fukunaga, 2013)\nObtaining the Bayes optimal linear classifier involves minimising the probability of misclassification pe as given by:\npe = \u03c01p(y < w0|C1) + \u03c02p(y \u2265 w0|C2) (9)\nwhere y = wTx. Unfortunately, there is no closed-form solution to the minimisation of (9) (Anderson & Bahadur, 1962). Thus, an iterative procedure is inevitable in order to obtain the Bayes optimal linear classifier.\nIn the work of (Marks & Dunn, 1974), for example, the iterative procedure described is to solve for w and w0 as given by\nw = [ s1\u03a31 + s2\u03a32 ]\u22121 (x\u03041 \u2212 x\u03042) w0 = \u00b51 \u2212 s1\u03c321 = \u00b52 + s2\u03c322 (10)\nby obtaining the optimal values of s1 and s2 via systematic trial and error. We denote this heteroscedastic LDA procedure by R-HLD-2, for the reason that the two parameters s1 and s2 are chosen at random.\n(Anderson & Bahadur, 1962) makes the observation that if the weight vector w and the threshold w0 are both multiplied by the same positive scalar, the decision boundary remains unchanged. Therefore, by multiplying (10) through by the scalar s1 + s2, w and w0 can be put in the form of:\nw = [ s\u03a32 + (1\u2212 s)\u03a31 ]\u22121 (x\u03041 \u2212 x\u03042) w0 = \u00b51 \u2212 (1\u2212 s)\u03c321 = \u00b52 + s\u03c322 (11)\nStill, the optimal value of s has to be chosen by systematic trial and error. We denote this heteroscedastic LDA approach by R-HLD-1, for the reason that only one parameter s is chosen at random. As we show in the next section, s is unbounded. Therefore, the difficulty faced by this approach is that s has to be chosen from the interval (\u2212\u221e,\u221e), so that the probability of finding the optimal s for a given dataset is low, without extensive trial and error to limit the choice of s to some finite interval [a, b].\nTo avoid the unguided trial and error procedure in (Marks & Dunn, 1974; Anderson & Bahadur, 1962), (Peterson & Mattson, 1966) and (Fukunaga, 2013, Chapter 4) propose a theoretical approach described below:\n1. Change s from 0 to 1 with small step increments \u2206s.\n2. Evaluate w as given by:\nw = [ s\u03a31 + (1\u2212 s)\u03a32 ]\u22121 (x\u03041 \u2212 x\u03042) (12)\n3. Evaluate w0 as given by:\nw0 = s\u00b52\u03c3\n2 1 + (1\u2212 s)\u00b51\u03c322\ns\u03c321 + (1\u2212 s)\u03c322 (13)\n4. Compute the probability of misclassification pe. 5. Choose w and w0 that minimise pe.\nWe refer to this procedure as C-HLD, for the reason that the optimal s is constrained in the interval [0, 1].\nHowever, we highlight two main problems with the above C-HLD procedure:\n1. There is no obvious choice of the step rate \u2206s. Too small a value of \u2206s will demand too many matrix inversions in Step 2, as there will be too many s values. On the other hand, if \u2206s is too large, the optimal s may not be refined enough, and the w obtained may not be optimal. Specifically, the change in w that results from a small change in s is given as:\ndw = ( s\u03a32+(1\u2212s)\u03a31 )\u22121 (\u03a31\u2212\u03a32) ( s\u03a32+(1\u2212s)\u03a31 )\u22121 (x\u03041\u2212x\u03042)ds (14) which can affect the classification accuracy. 2. The solution obtained this way is only locally optimal as s is bounded in the interval [0, 1]. As we show in the next section, s is actually unbounded. When there is a class imbalance (Xue & Titterington, 2008), the optimal s may be found outside the interval [0, 1] which can lead to poor classification accuracy.\nOur proposed algorithm, which is described in the next section, unlike the trial and error approach by (Marks & Dunn, 1974; Anderson & Bahadur, 1962), has a principled optimisation procedure, and unlike (Fukunaga, 2013; Peterson & Mattson, 1966) does not encounter the problem of choosing an inappropriate \u2206s, nor restricts s to the interval [0, 1]. Consequently, our proposed algorithm achieves a far lower training time than the C-HLD, RHLD-1 and R-HLD-2, for roughly the same classification accuracy."}, {"heading": "3. Gaussian Linear Discriminant", "text": "Let w \u2208 Rd be a vector of weights, and w0 \u2208 R, a threshold such that:\nC\u2217(x) = { C1 if y = wTx \u2265 w0 C2 if y = wTx < w0\n(15)\nSince x is assumed to have a multivariate normal distribution in classes C1 and C2, y has a mean of \u00b51 and a variance of \u03c321 for class C1 and a mean of \u00b52 and a variance of \u03c3 2 2 for class C2 given as:\n\u00b51 = w T x\u03041 \u00b52 = w T x\u03042 \u03c3 2 1 = w T\u03a31w \u03c3 2 2 = w T\u03a32w (16)\nWith reference to the Bayes error of (9), the individual misclassification probabilities can be expressed as:\np(y < w0|C1)\n=\n\u222b w0\n\u2212\u221e 1\u221a 2\u03c0\u03c31 exp\n[\n\u2212 (\u03b6 \u2212 \u00b51) 2\n2\u03c321\n] d\u03b6 = 1\u2212Q ( w0 \u2212 \u00b51 \u03c31 )\n(17)\nand\np(y \u2265 w0|C2) = \u222b \u221e\nw0 1\u221a 2\u03c0\u03c32 exp\n[\n\u2212 (\u03b6 \u2212 \u00b52) 2\n2\u03c322\n]\nd\u03b6 = Q\n(\nw0 \u2212 \u00b52 \u03c32\n)\n(18)\nwhere Q(.) is the Q-function. Therefore, the Bayes error to be minimised may be rewritten as:\npe = \u03c01 [ 1\u2212Q(z1) ] + \u03c02 [ Q(z2) ]\n(19)\nwhere\nz1 = w0 \u2212 \u00b51\n\u03c31 and z2 = w0 \u2212 \u00b52 \u03c32\n(20)\nOur aim is to find a local minimum of pe. A necessary condition is for the gradient of pe to be zero, i.e.,\n\u2207pe(w, w0) = [ \u2202pe \u2202wT , \u2202pe \u2202w0 ]T = 0 (21)\nFrom (9), it can be shown that:\n\u2202pe \u2202w = \u03c01\n(\n1\u221a 2\u03c0 e\u2212z 2 1 /2\u2202z1 \u2202w\n) \u2212 \u03c02 ( 1\u221a 2\u03c0 e\u2212z 2 2 /2\u2202z2 \u2202w )\n(22)\nFrom (20), however, we obtain the following:\n\u2202z1 \u2202w = \u2212\u03c31x\u03041 \u2212 z1\u03a31w \u03c321 and \u2202z2 \u2202w = \u2212\u03c32x\u03042 \u2212 z2\u03a32w \u03c322 (23)\nTherefore,\n\u2202pe \u2202w = 1\u221a 2\u03c0\n[\n\u2212 \u03c01e\u2212z 2 1 /2\n(\n\u03c31x\u03041 + z1\u03a31w\n\u03c321\n)\n+ \u03c02e \u2212z2 2 /2\n(\n\u03c32x\u03042 + z2\u03a32w\n\u03c322\n)]\n(24) It can similarly be shown from (9) that,\n\u2202pe \u2202w0 = \u03c01\n(\n1\u221a 2\u03c0 e\u2212z 2 1 /2 \u2202z1 \u2202w0\n) \u2212 \u03c02 ( 1\u221a 2\u03c0 e\u2212z 2 2 /2 \u2202z2 \u2202w0 )\n(25)\nAgain, from (20), \u2202z1 \u2202w0 = 1 \u03c31 and \u2202z2 \u2202w0 = 1 \u03c32 (26)\nTherefore, \u2202pe \u2202w0 = \u03c01\u221a 2\u03c0 ( 1 \u03c31 e\u2212z 2 1 /2 ) \u2212 \u03c02\u221a 2\u03c0 ( 1 \u03c32 e\u2212z 2 2 /2 )\n(27)\nNow, equating the gradient \u2207pe(w, w0) to zero, the following set of equations are obtained: (\n\u03c02z2 \u03c322 e\u2212z 2 2 /2\u03a32 \u2212 \u03c01z1 \u03c321 e\u2212z 2 1 /2\u03a31\n)\nw =\n(\n\u03c01 \u03c31 e\u2212z 2 1 /2\n) x\u03041 \u2212 ( \u03c02 \u03c32 e\u2212z 2 2 /2 ) x\u03042 (28)\n\u03c01 \u03c31 e\u2212z 2 1 /2 = \u03c02 \u03c32 e\u2212z 2 2 /2 (29)\nSubstituting (29) into (28) yields: (\nz2 \u03c32 \u03a32 \u2212 z1 \u03c31 \u03a31\n)\nw = (x\u03041 \u2212 x\u03042) (30)\nThen the vector w can be given by:\nw =\n(\nz2 \u03c32 \u03a32 \u2212 z1 \u03c31 \u03a31\n)\u22121\n(x\u03041 \u2212 x\u03042) (31)\nIt will be noted however that (31) is still in terms of w0, so that an explicit representation of w0 in terms of w is needed from (29) to substitute in z1 and z2 in (31). This is where our approach most significantly differs from (Fukunaga, 2013). Solving for w0 from (29) results in the following quadratic:\nz22 2 \u2212 z 2 1 2 \u2212 ln\n(\n\u03c4\u03c31 \u03c32\n)\n= 0 (32)\nwhich can be simplified to:\n(\nw0 \u2212 \u00b52 \u03c32 )2 \u2212 ( w0 \u2212 \u00b51 \u03c31 )2 \u2212 2 ln \u03c4\u03c31 \u03c32 = 0, (33)\nwhere \u03c4 is given as before as \u03c4 = \u03c02/\u03c01. If \u03c4 is defined and not equal to zero, and \u03c321 6= \u03c322 (since \u03a31 6= \u03a32 for heteroscedastic LDA), (33) can be shown to have the following solutions:\nw0 = \u00b52\u03c3\n2 1 \u2212 \u00b51\u03c322 \u00b1 \u03c31\u03c32\n\u221a\n(\u00b51 \u2212 \u00b52)2 + 2(\u03c321 \u2212 \u03c322) ln ( \u03c4\u03c31 \u03c32 )\n\u03c321 \u2212 \u03c322 (34)\nNevertheless, since there are two solutions to w0 in (34), a choice has to be made as to which of them is substituted into (31). To eliminate one of the solutions, we consider the second-order partial derivative of pe with respect to w0 evaluated at w0 as given by (34), and determine under what condition it is greater than or equal to zero. This is a second-order necessary condition for pe to be a local minimum. From (27), it can be shown that:\n\u22022pe \u2202w20 = \u03c01\u221a 2\u03c0\n(\n\u2212 z1 \u03c321 e\u2212z 2 1 /2\n)\n+ \u03c02\u221a 2\u03c0\n(\nz2 \u03c322 e\u2212z 2 2 /2\n)\n(35)\nWe denote this second-order derivative by h. We then consider all possibilities of z1 and z2 (which are the variables in (35) that depend on w0) under three cases, and analyse the sign of h in each.\nCase 1\nz2 \u2264 0 and z1 \u2265 0: then h is trivially non-positive.\nCase 2\nz2 \u2265 0 and z1 \u2264 0: then h is trivially non-negative.\nCase 3\nz2 > 0 and z1 > 0 or z2 < 0 and z1 < 0: then h is non-negative if and only if\nln\n(\n\u03c02z2 \u03c322\n)\n\u2212 z 2 2\n2 \u2265 ln\n(\n\u03c01z1 \u03c321\n)\n\u2212 z 2 1\n2 (36)\ni.e.,\nln\n(\nz2 \u03c32\n/\nz1 \u03c31\n)\n\u2265 z 2 2 2 \u2212 z 2 1 2 \u2212 ln\n(\n\u03c4\u03c31 \u03c32\n)\n(37)\nIt will be noted that the right-hand side of the inequality (37) is identically zero, as can be seen from (32). Therefore, the condition under which h is greater than or equal to zero is when:\nz2 \u03c32 \u2265 z1 \u03c31\n(38)\nNote also that Case 2 necessarily satisfies (38) so that we consider (38) as the general inequality for the non-negativity of h for all cases, and thus for w0 to be a local minimum.\nNow, when one considers the two solutions of w0 in (35), only the solution given by:\nw0 = \u00b52\u03c3\n2 1 \u2212 \u00b51\u03c322 + \u03c31\u03c32\n\u221a\n(\u00b51 \u2212 \u00b52)2 + 2(\u03c321 \u2212 \u03c322) ln ( \u03c4\u03c31 \u03c32 )\n\u03c321 \u2212 \u03c322 (39)\nsatisfies the inequality of (38), i.e., only this choice of w0 corresponds to a local minimum. The proof of this is given in the appendix.\nWe may then substitute this expression of w0 into (31) so that (31) is in terms of w only. Even so, w has to be solved for iteratively. This is because (31) has no closed-form solution since \u00b51, \u00b52, \u03c31, \u03c32 are themselves functions of w. As the iterative procedure requires an initial choice of w, we use Fisher\u2019s choice of the weight vector as given by:\nw = (n1\u03a31 + n2\u03a32) \u22121(x\u03041 \u2212 x\u03042) (40)\nas our initial solution. Again, we mention that n1 and n2 are the cardinalities of D1 and D2. After a number of such iterative updates, the optimal w0 is then solved for from (39). This algorithm, known as the Gaussian Linear Discriminant (GLD), is described in detail in Algorithm 1.\nNote that by multiplying both w of (31) and w0 proportionally by c = (\u03c31z2\u2212\u03c32z1)/\u03c31\u03c32 (due to (38), c is non-negative and hence the discrimination criterion given by (15) is not changed), the GLD may be viewed in terms of the optimal solution of (11), where\ns = \u2212\u03c32z1/(\u03c31z2 \u2212 \u03c32z1). (41) which is unbounded given the inequality of (38). However, unlike (Anderson & Bahadur, 1962; Marks & Dunn, 1974), s is not chosen by systematic trial and error, and unlike (Fukunaga, 2013), s is not varied between 0 and 1 at small step increments. Instead, since s is a function of w and w0, our algorithm may be interpreted as obtaining increasingly refined values of s by improving upon w and w0 starting from Fisher\u2019s solution, as is described in Algorithm 1.\nAlgorithm 1 GLD\n1: Input: D1 and D2 2: Evaluate x\u03041, x\u03042,\u03a31,\u03a32 3: Initialise w: w = (n1\u03a31 + n2\u03a32)\n\u22121(x\u03041 \u2212 x\u03042) 4: Evaluate \u00b51, \u00b52, \u03c3 2 1, \u03c3 2 2, z1, z2. 5: while Stopping criteria are not satisfied do 6: Solve for w0 from (39) 7: Evaluate z1, z2 8: Evaluate the Bayes error pe\n9: Update w as w =\n(\nz2 \u03c32 \u03a32 \u2212 z1\u03c31\u03a31\n)\u22121\n(x\u03041 \u2212 x\u03042) 10: Evaluate \u00b51, \u00b52, \u03c31, \u03c32. 11: end while"}, {"heading": "3.1. Stopping Criteria", "text": "The GLD algorithm may be terminated under any of the following conditions:\n1. When the change in the objective function pe remains within a certain tolerance \u01eb1 for a number of consecutive iterations. 2. When the change in the norm of w remains within a certain tolerance \u01eb2 for a number of consecutive iterations. 3. When the gradient of pe as given by (21) remains within a certain tolerance \u01eb3 for a number of consecutive iterations. 4. After a fixed number of iterations I, if convergence is slow.\nAt the end of the algorithm, the final solution may be chosen either as the solution to which the iterations converge, or the solution corresponding to the minimum pe found in the iterative updates."}, {"heading": "3.2. Multiclass Classification", "text": "Suppose now that there are K > 2 classes in the dataset D, then the classification problem may be reduced to a number of binary classification problems. The two main approaches usually taken for this reduction are the Onevs-All (OvA) and One-vs-One (OvO) strategies (Bishop, 2006; Hsu & Lin, 2002)."}, {"heading": "3.2.1. One-vs-All (OvA)", "text": "In OvA, one trains a classifier to discriminate between one class and all other classes. Thus, there are K different classifiers. An unknown vector x is then tested on all K classifiers so that the class corresponding to the classifier with the highest discriminant score is chosen. However, with respect to the proposed GLD algorithm, this is an ill-suited approach. This is because the collection of all other classes on one side of the discriminant will not necessarily have a normal distribution, and could in fact be multimodal, if the means are well-separated. Since our algorithm is built on strong normality assumptions of the data on each side of the discriminant, the GLD, as has been formulated, is expected to perform poorly."}, {"heading": "3.2.2. One-vs-One", "text": "In OvO, a classifier is trained to discriminate between every pair of classes in the dataset, ignoring the other K\u22122 classes. Thus, there are K(K\u22121)/2 unique classifiers that may be constructed. Again, an unknown vector x is tested on allK(K\u22121)/2 classifiers. The predicted classes for all the classifiers are then tallied so that the class that occurs most frequently is chosen. This is equivalent to a majority vote decision. In a lot of cases, however, there is no clear-cut winner, as more than one class may have the highest number of votes. In such a case, the most likely class is often chosen randomly between those most frequently occurring classes. The GLD provides a more appropriate means for breaking such ties, by making use of the minimised Bayes error pe for each classifier. Specifically, one may instead use a weighted voting system, where the count of every predicted class is weighted by 1\u2212pe, since pe provides an appropriate measure of uncertainty associated with each classifier output. Thus, the decision rule is reduced to choosing the maximum weighted vote among the K classes.\nNote that even though the GLD minimises the Bayes error for each classifier, the overall Bayes error for a multiclass problem may not be minimised by using multiple binary classifiers."}, {"heading": "4. Non-normal Distributions", "text": "So far, the fundamental assumption that has been used to derive the GLD is that the data in each class has a normal distribution. Thus, for an unknown non-normal distribution, the linear classifier we have obtained does not minimise the Bayes error for that unknown distribution. We argue, however, that\nif this unknown distribution is nearly-normal (Mudholkar & Hutson, 2000), then a more robust linear classifier may be found in some neighbourhood of the GLD. For this reason, we use a local neighbourhood search algorithm to explore the region in Rd+1 around the GLD to obtain the classifier that minimises the number of misclassifications on the training dataset. We do this by perturbing each of the d+1 vector elements in the optimal w\u0303 = [w0,w T ]T obtained from the GLD procedure by a small amount \u03b4w\u0303i. After every perturbation, the resulting classifier is evaluated on the test dataset. This procedure is repeated as described in Algorithm 2 until the stopping criterion is satisfied.\nAlgorithm 2 Local Neighbourhood Search (LNS)\n1: Input: Optimal w\u0303 = [w0,w T ]T obtained from the GLD. 2: while Stopping criterion is not satisfied do 3: Let w\u0303 be the current solution. 4: for i \u2190 1 to d do 5: v+ \u2190 w\u0303, v\u2212 \u2190 w\u0303. 6: v+ \u2190 v+i + \u03b4v+i 7: Evaluate the misclassifications on the training set using v+ 8: v\u2212 \u2190 v\u2212i \u2212 \u03b4v\u2212i 9: Evaluate the misclassifications on the training set using v\u2212\n10: end for 11: Set the classifier with the minimum number of misclassifications as the current solution w\u0303.\n12: end while 13: Choose the classifier with the smallest number of misclassifications.\nThe algorithm is terminated after a certain maximum number of iterations R is reached. Additionally, one may perform an early termination if after a predefined number of iterations rmax, there is no improvement in the minimum number of misclassifications on the training dataset that has been found in the search."}, {"heading": "5. Experimental Validation", "text": "We validate our proposed algorithm on two artificial datasets denoted by D1 and D2, as well as on ten real-world datasets taken from the University of California, Irvine (UCI) Machine Learning Repository. These datasets\nare shown in Table 1, and cut across a wide range of applications including handwriting recognition, medical diagnosis, remote sensing and spam filtering. D1 and D2 are normally distributed with different covariance matrices. For D1, we generate 1000 samples for class C1 and 2000 samples for class C2 using the following Gaussian parameters:\nx\u03042 = [3.86, 3.10, 0.84, 0.84, 1.64, 1.08, 0.26, 0.01] T,\n\u03a32 = diag(8.41, 12.06, 0.12, 0.22, 1.49, 1.77, 0.35, 2.73)\nx\u03041 = x\u03042 \u2212 0.3, \u03a31 = I (42) For D2, we generate 2000 samples for class C1 and 4000 samples for class C2 using the following Gaussian parameters:\nx\u03042 = [\u22121.5,\u22120.75, 0.75, 1.5]T , \u03a32 = diag(0.25, 0.75, 1.25, 1.75)\nx\u03041 = x\u03042 \u2212 0.75, \u03a31 = I (43) The above Gaussian parameters are slightly modified from the two class data used by (Fukunaga, 2013) and (Xue & Titterington, 2008) in order to make the sample means less separated.\nFor each dataset in Table 1, we perform 10-fold cross validation. We run 20 different trials. On each training dataset, we evaluate the minimum Bayes error achievable by our proposed algorithm averaged over all 10 folds and 20 trials. If there are more than two classes, we use OvO, and calculate the mean Bayes error over all K(K \u2212 1)/2 discriminants. As we are interested only in linear classification, we compare the performance of the GLD with the original LDA as well as the heteroscedastic LDA procedures by (Fukunaga, 2013),(Anderson & Bahadur, 1962) and (Marks & Dunn, 1974) as described in Section 2 in terms of the Bayes error (9). For the sake of brevity, we denote these three heteroscedastic LDA algorithms by the annotations earlier introduced: C-HLD, R-HLD-1 and R-HLD-2 respectively. These results are shown in Table 2.\nMoreover, for each of the test datasets, we evaluate the average classification accuracy for each of LDA, C-HLD, R-HLD-1, R-HLD-2, GLD and GLD with local neighbourhood search (LNS). We also compare the performance of these LDA approaches to the SVM. These results are shown in Table 3, while the average training times of the algorithms are shown in Table 4.\nWe estimate the prior probabilities based on the relative frequencies of the data in each class in the dataset, and the stopping criterion for the GLD\nLetters (l) 20000 16 26 This table lists the datasets used in the experimental section. K is the number of classes, d is the dimensionality of the dataset, and n is the number of data points in the dataset.\nis thus: we stop if the gradient of w change is less than or equal to \u01eb3 = 10 \u22126, or else we terminate our algorithm after I = 20 iterations and choose the solution corresponding to the minimum pe. Also, for the LNS procedure, we perturb each vector element by 10% of its absolute value, i.e. \u03b4w\u0303i = 0.1|w\u0303i|, and we run for R=1000 iterations, terminating prematurely if rmax = 0.1R. We use a step size of \u2206s = 0.001 for the C-HLD algorithm, and run 1000 trials for R-HLD-1 and R-HLD-2. All the parameters used in the experiments are optimised via cross-validation. Note that if the sample covariance matrix is singular, we use the Moore-Penrose pseudo-inverse."}, {"heading": "6. Results and Discussion", "text": "For real-world datasets, the covariance matrices of the classes are rarely equal, therefore the homoscedasticity assumption in LDA does not hold. Our results in Table 2 confirm that LDA does not minimise the Bayes error under heteroscedasticity, as none of the datasets used has equal covariance matrices. With the exception of datasets (d), (j) and (l), where LDA achieves an equal Bayes error as the other heteroscedastic LDA approaches, LDA is outperformed by the GLD on all remaining datasets in terms of minimising the Bayes error. It will be noted that the other three heteroscedastic LDA\napproaches algorithms achieve a performance comparable to the GLD on all the datasets in terms of the Bayes error. However, R-HLD-1 and RHLD-2 require a lot of trials (1000 in our experiments) in order to obtain the optimal parameters s and s1, s2 respectively, while C-HLD requires a step size of \u2206s = 0.001 which translates to 1001 trials. Consequently, the training time for these algorithms far exceed that of the GLD, as can be seen in Table 4. For example, the gain in training time of the GLD over C-HLD, R-HLD-1 and R-HLD-2 is over 62 folds for dataset (g), and about 20 folds for dataset (l). Moreover, since C-HLD, R-HLD-1 and R-HLD-2 all require matrix inversions, performing a matrix inversion for each of the 1000 trials can be a computationally demanding task especially for high-dimensional data, which have large covariance matrices. Instead, since the GLD follows a principled optimisation procedure, the number of matrix inversions required is far lower. For example, on dataset (f), which has a dimensionality of 47, the GLD requires over 60 times less time to train than the other heteroscedastic LDA approaches.\nIt is conceivable that the minimisation of the Bayes error would translate into a good performance in terms of the classification accuracy, if the normality assumption of LDA holds. For this reason, it can be seen in Table 3 that the GLD achieves the best classification accuracy on datasets (a) and (b),\nwhich are generated from known normal distributions. Thus, the proposed GLD algorithm is particularly suited for applications with datasets that tend to be normally distributed in each class e.g. in machine fault diagnosis, or accelerometer-based human activity recognition (Ojetola et al., 2015), as it also requires far less training time than the existing heteroscedastic LDA approaches.\nHowever, for datasets (c) through to (l), the classes do not have any known normal distribution. Therefore, minimising the Bayes error under the normality assumption would not necessarily result in a classifier that has the best classification accuracy, even if the difference in covariance matrices has been accounted for. For this reason, it is not surprising that LDA achieves a superior classification accuracy than C-HLD, R-HLD-1, R-HLD-2 and the GLD on datasets (c) and (h) as can be seen in Table 3. However, by searching around the neighbourhood of the GLD, the Local Neighbourhood Search (LNS) algorithm is able to account for the non-normality and obtain a more robust classifier. Thus, the GLD, together with the LNS procedure, achieves a higher classification accuracy than all the LDA approaches on all the realworld datasets (i.e. (c) to (l)) with the exception of dataset (f) which has the GLD showing superior classification accuracy.\nWhile the SVM outperforms the LDA approaches on half of the datasets, its training time can be rather long for large datasets. For instance, for dataset (d) which has 58000 elements, the SVM takes about 1.3 hours to train whereas the GLD with LNS, which achieves the best classification accuracy on this dataset, takes 43 seconds to train, representing over 100 fold savings in computational time over the SVM. Similar patterns can be seen in other datasets like (i), where the GLD with LNS achieves a superior classification accuracy with over 150 times shorter training time than the SVM. This suggests that for such large datasets, the GLD with Local Neighbourhood Search is a low-complexity alternative to the SVM, as it requires far less computational time than the SVM.\nWe, however, make note of two weaknesses our proposed algorithms have. For the GLD, the procedure as described in Algorithm 1, may converge to a saddle point, instead of a local minimum. Even if it were to converge to a local minimum, there is no guarantee that is the global optimum solution due to the fact that the objective function pe is known to be non-convex Anderson & Bahadur (1962). Also, since the Local Neighbourhood Search involves evaluating the misclassification rate on the training set for every perturbation, the procedure does not scale well with large amounts of training\ndata. Because of this, it is important to have a good initial solution like the GLD, so that an early termination may be performed if there is no improvement after some number of iterations."}, {"heading": "7. Conclusion", "text": "In this paper, we have presented the Gaussian Linear Discriminant (GLD), a novel and computationally efficient method for obtaining a linear discriminant for heteroscedastic Linear Discriminant Analysis (LDA) for the purpose of binary classification. Our algorithm minimises the Bayes error via an iterative optimisation procedure that uses Fisher\u2019s Linear Discriminant as the initial solution. Moreover, the GLD does not require any parameter adjustments. We have also proposed a local neighbourhood search method by which a more robust linear classifier may be obtained for non-normal distributions. Our experimental results on two artificial and ten real world applications show that when the covariance matrices of the classes are unequal, LDA is unable to minimise the Bayes error. Thus, under heteroscedasticity, our proposed algorithm achieves superior classification accuracy to the LDA for normally distributed classes. While the proposed GLD algorithm compares favourably with other heteroscedastic LDA approaches, the GLD requires a far less training time. Moreover, the GLD, together with the LNS, has been shown to be particularly robust, comparing favourably with the SVM, but requiring far less training time on our datasets. Thus, for expert systems like machine fault diagnosis or human activity monitoring that require linear classification, the proposed algorithms provide a low-complexity, high-accuracy solution.\nWhile this work has focused on linear classification, on-going work is focused on modifying the GLD procedure for the purpose of linear dimensionality reduction. Moreover, it is of particular interest to us to be able to derive the Bayes error for some known non-normal distributions. An alternative to this is to be able to obtain a kernel that implicitly transforms some data of a known non-normal distribution into a feature space where the classes are normally distributed. Finally, like all local search algorithms, the performance and complexity of the LNS procedure depends on the choice of the initial solution. Therefore, further work that explores the use initial solutions (including the heteroscedastic LDA approaches discussed) other than the GLD for the LNS procedure is being done."}, {"heading": "Appendix A.", "text": "Theorem 1. Let w+0 and w \u2212 0 be the two distinct solutions of (34), then w + 0 and w\u22120 cannot both satisfy (38) given that \u03c31 6= \u03c32. Proof. Let\n\u03b2 =\n\u221a\n(\u00b51 \u2212 \u00b52)2 + 2(\u03c321 \u2212 \u03c322) ln ( \u03c4\u03c31 \u03c32 )\n(A.1)\nand let\nw+0 = \u00b52\u03c3\n2 1 \u2212 \u00b51\u03c322 + \u03c31\u03c32\u03b2\n\u03c321 \u2212 \u03c322 (A.2)\nThen\nz2 \u03c32 = (\u00b52 \u2212 \u00b51)\u03c32 + \u03b2\u03c31 \u03c32(\u03c321 \u2212 \u03c322) , z1 \u03c31 = (\u00b52 \u2212 \u00b51)\u03c31 + \u03b2\u03c32 \u03c31(\u03c321 \u2212 \u03c322) (A.3)\nSuppose that w+0 satisfies (38), then\n(\u00b52 \u2212 \u00b51)\u03c32 + \u03b2\u03c31 \u03c32(\u03c321 \u2212 \u03c322) \u2265 (\u00b52 \u2212 \u00b51)\u03c31 + \u03b2\u03c32 \u03c31(\u03c321 \u2212 \u03c322)\n(A.4)\ni.e., \u03b2\u03c321\n\u03c321 \u2212 \u03c322 \u2265 \u03b2\u03c3 2 2 \u03c321 \u2212 \u03c322 (A.5)\nThis implies that \u03c321/(\u03c3 2 1 \u2212 \u03c322) > \u03c322/(\u03c321 \u2212 \u03c322) since \u03b2 is a positive scalar.\nConsider now w\u22120 given as:\nw\u22120 = \u00b52\u03c3\n2 1 \u2212 \u00b51\u03c322 \u2212 \u03c31\u03c32\u03b2\n\u03c321 \u2212 \u03c322 (A.6)\nThen\nz2 \u03c32 = (\u00b52 \u2212 \u00b51)\u03c32 \u2212 \u03b2\u03c31 \u03c32(\u03c321 \u2212 \u03c322) , z1 \u03c31 = (\u00b52 \u2212 \u00b51)\u03c31 \u2212 \u03b2\u03c32 \u03c31(\u03c321 \u2212 \u03c322) (A.7)\nIn order for (38) to be satisfied, it can be shown, similar to (A.5), that\n\u2212\u03b2\u03c321 \u03c321 \u2212 \u03c322 \u2265 \u2212\u03b2\u03c3 2 2 \u03c321 \u2212 \u03c322 (A.8)\nwhich can be simplified to give 1 \u2264 0. Since this conclusion is false, only w+0 satisfies (26)."}], "references": [{"title": "Classification into two multivariate normal distributions with different covariance matrices. The annals of mathematical statistics", "author": ["T.W. Anderson", "R. Bahadur"], "venue": null, "citeRegEx": "Anderson and Bahadur,? \\Q1962\\E", "shortCiteRegEx": "Anderson and Bahadur", "year": 1962}, {"title": "Bayesian reasoning and machine learning", "author": ["D. Barber"], "venue": "Cambridge University Press.", "citeRegEx": "Barber,? 2012", "shortCiteRegEx": "Barber", "year": 2012}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning , 128 , 1\u201358.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Toward Bayes-optimal linear dimension reduction", "author": ["L.J. Buturovic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 16 , 420\u2013424.", "citeRegEx": "Buturovic,? 1994", "shortCiteRegEx": "Buturovic", "year": 1994}, {"title": "Locality sensitive discriminant analysis", "author": ["D. Cai", "X. He", "K. Zhou", "J. Han", "H. Bao"], "venue": "In Proceedings of the 20th international joint conference on Artifical intelligence (pp", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "A new LDA-based face recognition system which can solve the small sample size problem", "author": ["Chen", "L.-F", "Liao", "H.-Y. M", "Ko", "M.-T", "Lin", "J.-C", "Yu", "G.-J"], "venue": "Pattern recognition,", "citeRegEx": "Chen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2000}, {"title": "The application of linear discriminant analysis in the diagnosis of thyroid diseases", "author": ["D. Coomans", "M. Jonckheer", "D.L. Massart", "I. Broeckaert", "P. Blockx"], "venue": "Analytica chimica acta,", "citeRegEx": "Coomans et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Coomans et al\\.", "year": 1978}, {"title": "Feature combinations and the divergence criterion", "author": ["H.P. Decell", "S.M. Mayekar"], "venue": "Computers & Mathematics with Applications ,", "citeRegEx": "Decell and Mayekar,? \\Q1977\\E", "shortCiteRegEx": "Decell and Mayekar", "year": 1977}, {"title": "Feature combinations and the Bhattacharyya criterion", "author": ["H.P. Decell Jr.", "S.K. Marani"], "venue": "Communications in Statistics-Theory and Methods ,", "citeRegEx": "Jr and Marani,? \\Q1976\\E", "shortCiteRegEx": "Jr and Marani", "year": 1976}, {"title": "Linear dimensionality reduction via a heteroscedastic extension of LDA: the Chernoff criterion", "author": ["R. Duin", "M. Loog"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Duin and Loog,? \\Q2004\\E", "shortCiteRegEx": "Duin and Loog", "year": 2004}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of eugenics , 7 , 179\u2013188. 23", "citeRegEx": "Fisher,? 1936", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Regularized discriminant analysis", "author": ["J.H. Friedman"], "venue": "Journal of the American statistical association, 84 , 165\u2013175.", "citeRegEx": "Friedman,? 1989", "shortCiteRegEx": "Friedman", "year": 1989}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press.", "citeRegEx": "Fukunaga,? 2013", "shortCiteRegEx": "Fukunaga", "year": 2013}, {"title": "Nonparametric discriminant analysis", "author": ["K. Fukunaga", "J. Mantock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Fukunaga and Mantock,? \\Q1983\\E", "shortCiteRegEx": "Fukunaga and Mantock", "year": 1983}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["O.C. Hamsici", "A.M. Martinez"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Hamsici and Martinez,? \\Q2008\\E", "shortCiteRegEx": "Hamsici and Martinez", "year": 2008}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "In European Conference on Computer Vision (pp", "citeRegEx": "Hariharan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2012}, {"title": "Discriminant analysis by Gaussian mixtures", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Hastie and Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1996}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["Hsu", "C.-W", "Lin", "C.-J"], "venue": "IEEE transactions on Neural Networks ,", "citeRegEx": "Hsu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2002}, {"title": "Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning", "author": ["A.J. Izenman"], "venue": "Springer Science & Business Media.", "citeRegEx": "Izenman,? 2009", "shortCiteRegEx": "Izenman", "year": 2009}, {"title": "Gaussian mixture discriminant analysis and sub-pixel land cover characterization in remote sensing", "author": ["J. Ju", "E.D. Kolaczyk", "S. Gopal"], "venue": "Remote Sensing of Environment ,", "citeRegEx": "Ju et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ju et al\\.", "year": 2003}, {"title": "Nonparametric discriminant analysis for face recognition", "author": ["Z. Li", "D. Lin", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Efficient pseudoinverse linear discriminant analysis and its nonlinear form for face recognition", "author": ["J. Liu", "S. Chen", "X. Tan", "D. Zhang"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Non-iterative heteroscedastic linear dimension reduction for two-class data. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR) (pp. 508\u2013517)", "author": ["M. Loog", "R.P. Duin"], "venue": null, "citeRegEx": "Loog and Duin,? \\Q2002\\E", "shortCiteRegEx": "Loog and Duin", "year": 2002}, {"title": "Regularized discriminant analysis for the small sample size problem in face recognition", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Pattern Recognition Letters ,", "citeRegEx": "Lu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2003}, {"title": "Detecting credit card fraud by modified Fisher discriminant analysis", "author": ["N. Mahmoudi", "E. Duman"], "venue": "Expert Systems with Applications ,", "citeRegEx": "Mahmoudi and Duman,? \\Q2015\\E", "shortCiteRegEx": "Mahmoudi and Duman", "year": 2015}, {"title": "On an extended Fisher criterion for feature selection", "author": ["W. Malina"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, (pp. 611\u2013614).", "citeRegEx": "Malina,? 1981", "shortCiteRegEx": "Malina", "year": 1981}, {"title": "Discriminant functions when covariance matrices are unequal", "author": ["S. Marks", "O.J. Dunn"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Marks and Dunn,? \\Q1974\\E", "shortCiteRegEx": "Marks and Dunn", "year": 1974}, {"title": "Discriminant analysis and statistical pattern recognition volume 544", "author": ["G. McLachlan"], "venue": "John Wiley & Sons.", "citeRegEx": "McLachlan,? 2004", "shortCiteRegEx": "McLachlan", "year": 2004}, {"title": "Fisher discriminant analysis with kernels", "author": ["S. Mika", "G. Ratsch", "J. Weston", "B. Scholkopf", "Mullers", "K.-R"], "venue": "In Neural Networks for Signal Processing IX,", "citeRegEx": "Mika et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mika et al\\.", "year": 1999}, {"title": "The epsilon\u2013skew\u2013normal distribution for analyzing near-normal data", "author": ["G.S. Mudholkar", "A.D. Hutson"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Mudholkar and Hutson,? \\Q2000\\E", "shortCiteRegEx": "Mudholkar and Hutson", "year": 2000}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "Advances in neural information processing systems ,", "citeRegEx": "Ng and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2002}, {"title": "Data set for fall events and daily activities from inertial sensors", "author": ["O. Ojetola", "E. Gaura", "J. Brusey"], "venue": "In Proceedings of the 6th ACM Multimedia Systems Conference (pp. 243\u2013248)", "citeRegEx": "Ojetola et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ojetola et al\\.", "year": 2015}, {"title": "Improved pseudoinverse linear discriminant analysis method for dimensionality reduction", "author": ["K.K. Paliwal", "A. Sharma"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Paliwal and Sharma,? \\Q2012\\E", "shortCiteRegEx": "Paliwal and Sharma", "year": 2012}, {"title": "A method of finding linear discriminant functions for a class of performance criteria", "author": ["D. Peterson", "R. Mattson"], "venue": "IEEE Transactions on Information Theory ,", "citeRegEx": "Peterson and Mattson,? \\Q1966\\E", "shortCiteRegEx": "Peterson and Mattson", "year": 1966}, {"title": "A cascade learning system for classification of diabetes disease: Generalized discriminant analysis and least square support vector machine. Expert systems with applications", "author": ["K. Polat", "S. G\u00fcne\u015f", "A. Arslan"], "venue": null, "citeRegEx": "Polat et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Polat et al\\.", "year": 2008}, {"title": "An expert system based on linear discriminant analysis and adaptive neuro-fuzzy inference system to diagnosis heart valve diseases", "author": ["A. Sengur"], "venue": "Expert Systems with Applications , 35 , 214\u2013222.", "citeRegEx": "Sengur,? 2008", "shortCiteRegEx": "Sengur", "year": 2008}, {"title": "Cancer classification by gradient LDA technique using microarray gene expression data", "author": ["A. Sharma", "K.K. Paliwal"], "venue": "Data & Knowledge Engineering ,", "citeRegEx": "Sharma and Paliwal,? \\Q2008\\E", "shortCiteRegEx": "Sharma and Paliwal", "year": 2008}, {"title": "Linear discriminant analysis for the small sample size problem: an overview", "author": ["A. Sharma", "K.K. Paliwal"], "venue": "International Journal of Machine Learning and Cybernetics ,", "citeRegEx": "Sharma and Paliwal,? \\Q2015\\E", "shortCiteRegEx": "Sharma and Paliwal", "year": 2015}, {"title": "A parameterized direct LDA and its application to face recognition", "author": ["F. Song", "D. Zhang", "J. Wang", "H. Liu", "Q. Tao"], "venue": "Neurocomputing ,", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "Do unbalanced data have a negative effect on LDA", "author": ["Xue", "J.-H", "D.M. Titterington"], "venue": "Pattern Recognition,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "A direct LDA algorithm for high-dimensional datawith application to face recognition", "author": ["H. Yu", "J. Yang"], "venue": "Pattern recognition,", "citeRegEx": "Yu and Yang,? \\Q2001\\E", "shortCiteRegEx": "Yu and Yang", "year": 2001}, {"title": "Recent advances of large-scale linear classification", "author": ["Yuan", "G.-X", "Ho", "C.-H", "Lin", "C.-J"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "An equalized heteroscedastic linear discriminant analysis algorithm", "author": ["Zhang", "W.-Q", "J. Liu"], "venue": "IEEE Signal Processing Letters ,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Multiclass probabilistic kernel discriminant analysis", "author": ["Z. Zhao", "L. Sun", "S. Yu", "H. Liu", "J. Ye"], "venue": "In Proceedings of the 21st international jont conference on Artifical intelligence (pp", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 28, "context": "A linear classifier is robust against noise since it tends not to overfit (Mika et al., 1999).", "startOffset": 74, "endOffset": 93}, {"referenceID": 41, "context": "A linear classifier has relatively shorter training and testing times (Yuan et al., 2012).", "startOffset": 70, "endOffset": 89}, {"referenceID": 10, "context": "If only the weight vector w is required for dimensionality reduction, w may be obtained by maximising Fishers criterion (Fisher, 1936), given by:", "startOffset": 120, "endOffset": 134}, {"referenceID": 3, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al.", "startOffset": 154, "endOffset": 204}, {"referenceID": 35, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al.", "startOffset": 154, "endOffset": 204}, {"referenceID": 18, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al., 1999).", "startOffset": 291, "endOffset": 325}, {"referenceID": 28, "context": "As a supervised learning algorithm, LDA is performed either for dimensionality reduction (usually followed by classification) (Barber, 2012, Chapter 16), (Buturovic, 1994; Duin & Loog, 2004; Sengur, 2008), or directly for the purpose of statistical classification (Fukunaga, 2013, Chapter 4)(Izenman, 2009; Mika et al., 1999).", "startOffset": 291, "endOffset": 325}, {"referenceID": 6, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 35, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 34, "context": "(Sharma & Paliwal, 2008; Coomans et al., 1978; Sengur, 2008; Polat et al., 2008), face and object recognition e.", "startOffset": 0, "endOffset": 80}, {"referenceID": 38, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 5, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 21, "context": "(Song et al., 2007; Chen et al., 2000; Liu et al., 2007; Yu & Yang, 2001) and credit card fraud prediction e.", "startOffset": 0, "endOffset": 73}, {"referenceID": 28, "context": "The widespread use of LDA in these areas is not because the datasets necessarily satisfy the normality and homoscedasticity assumptions, but mainly due to the robustness of LDA against noise, being a linear model (Mika et al., 1999).", "startOffset": 213, "endOffset": 232}, {"referenceID": 15, "context": "Since the linear Support Vector Machine (SVM) can be quite expensive to train, especially for large values of K or n (n = n1 + n2), LDA is often relied upon (Hariharan et al., 2012).", "startOffset": 157, "endOffset": 181}, {"referenceID": 21, "context": "In works by, for example, (Liu et al., 2007; Paliwal & Sharma, 2012), this problem is overcome by taking the Moore-Penrose pseudo-inverse of the scatter matrix, rather than the ordinary matrix inverse.", "startOffset": 26, "endOffset": 68}, {"referenceID": 11, "context": "Another approach to solving the SSS problem involves adding a scalar multiple of the identity matrix to the scatter matrix to make the resulting matrix nonsingular, a method known as regularised discriminant analysis (Friedman, 1989; Lu et al., 2003).", "startOffset": 217, "endOffset": 250}, {"referenceID": 23, "context": "Another approach to solving the SSS problem involves adding a scalar multiple of the identity matrix to the scatter matrix to make the resulting matrix nonsingular, a method known as regularised discriminant analysis (Friedman, 1989; Lu et al., 2003).", "startOffset": 217, "endOffset": 250}, {"referenceID": 27, "context": "One such modification, in the case of a non-normal distribution, is the mixture discriminant analysis (Hastie & Tibshirani, 1996; McLachlan, 2004; Ju et al., 2003) in which a non-normal distribution is modelled as a mixture of Gaussians.", "startOffset": 102, "endOffset": 163}, {"referenceID": 19, "context": "One such modification, in the case of a non-normal distribution, is the mixture discriminant analysis (Hastie & Tibshirani, 1996; McLachlan, 2004; Ju et al., 2003) in which a non-normal distribution is modelled as a mixture of Gaussians.", "startOffset": 102, "endOffset": 163}, {"referenceID": 4, "context": "Other non-parametric approaches to LDA that remove the normality assumption involve using local neighbourhood structures (Cai et al., 2007; Fukunaga & Mantock, 1983; Li et al., 2009) to construct a similarity matrix instead of the scatter matrix \u03a3x used in LDA.", "startOffset": 121, "endOffset": 182}, {"referenceID": 20, "context": "Other non-parametric approaches to LDA that remove the normality assumption involve using local neighbourhood structures (Cai et al., 2007; Fukunaga & Mantock, 1983; Li et al., 2009) to construct a similarity matrix instead of the scatter matrix \u03a3x used in LDA.", "startOffset": 121, "endOffset": 182}, {"referenceID": 28, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 43, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 34, "context": "Another modification, in the case of a non-linear decision boundary between D1 and D2, is the Kernel Fisher Discriminant (KFD) (Mika et al., 1999; Zhao et al., 2009; Polat et al., 2008).", "startOffset": 127, "endOffset": 185}, {"referenceID": 28, "context": "KFD maps the original feature space X into some other space Y (usually higher dimensional) via the kernel trick (Mika et al., 1999).", "startOffset": 112, "endOffset": 131}, {"referenceID": 15, "context": "In works by, for example, (Liu et al., 2007; Paliwal & Sharma, 2012), this problem is overcome by taking the Moore-Penrose pseudo-inverse of the scatter matrix, rather than the ordinary matrix inverse. Sharma & Paliwal (2008) use a gradient descent approach where one starts from an initial solution of w and moves in the negative direction of the gradient of Fisher\u2019s criterion (8).", "startOffset": 27, "endOffset": 226}, {"referenceID": 27, "context": "Under the heteroscedasticity assumption, many LDA approaches have been proposed among which we mention (Fukunaga, 2013, Chapter 4),(Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002).", "startOffset": 131, "endOffset": 269}, {"referenceID": 25, "context": "Under the heteroscedasticity assumption, many LDA approaches have been proposed among which we mention (Fukunaga, 2013, Chapter 4),(Zhang & Liu, 2008; McLachlan, 2004; Duin & Loog, 2004; Decell Jr & Marani, 1976; Decell & Mayekar, 1977; Malina, 1981; Loog & Duin, 2002).", "startOffset": 131, "endOffset": 269}, {"referenceID": 12, "context": "As far as we know, the closest work to ours in this regard are the works by (Marks & Dunn, 1974; Anderson & Bahadur, 1962; Peterson & Mattson, 1966; Fukunaga, 2013)", "startOffset": 76, "endOffset": 164}, {"referenceID": 12, "context": "Our proposed algorithm, which is described in the next section, unlike the trial and error approach by (Marks & Dunn, 1974; Anderson & Bahadur, 1962), has a principled optimisation procedure, and unlike (Fukunaga, 2013; Peterson & Mattson, 1966) does not encounter the problem of choosing an inappropriate \u2206s, nor restricts s to the interval [0, 1].", "startOffset": 203, "endOffset": 245}, {"referenceID": 12, "context": "This is where our approach most significantly differs from (Fukunaga, 2013).", "startOffset": 59, "endOffset": 75}, {"referenceID": 12, "context": "However, unlike (Anderson & Bahadur, 1962; Marks & Dunn, 1974), s is not chosen by systematic trial and error, and unlike (Fukunaga, 2013), s is not varied between 0 and 1 at small step increments.", "startOffset": 122, "endOffset": 138}, {"referenceID": 2, "context": "The two main approaches usually taken for this reduction are the Onevs-All (OvA) and One-vs-One (OvO) strategies (Bishop, 2006; Hsu & Lin, 2002).", "startOffset": 113, "endOffset": 144}, {"referenceID": 12, "context": "75, \u03a31 = I (43) The above Gaussian parameters are slightly modified from the two class data used by (Fukunaga, 2013) and (Xue & Titterington, 2008) in order to make the sample means less separated.", "startOffset": 100, "endOffset": 116}, {"referenceID": 12, "context": "As we are interested only in linear classification, we compare the performance of the GLD with the original LDA as well as the heteroscedastic LDA procedures by (Fukunaga, 2013),(Anderson & Bahadur, 1962) and (Marks & Dunn, 1974) as described in Section 2 in terms of the Bayes error (9).", "startOffset": 161, "endOffset": 177}, {"referenceID": 31, "context": "in machine fault diagnosis, or accelerometer-based human activity recognition (Ojetola et al., 2015), as it also requires far less training time than the existing heteroscedastic LDA approaches.", "startOffset": 78, "endOffset": 100}, {"referenceID": 31, "context": "in machine fault diagnosis, or accelerometer-based human activity recognition (Ojetola et al., 2015), as it also requires far less training time than the existing heteroscedastic LDA approaches. However, for datasets (c) through to (l), the classes do not have any known normal distribution. Therefore, minimising the Bayes error under the normality assumption would not necessarily result in a classifier that has the best classification accuracy, even if the difference in covariance matrices has been accounted for. For this reason, it is not surprising that LDA achieves a superior classification accuracy than C-HLD, R-HLD-1, R-HLD-2 and the GLD on datasets (c) and (h) as can be seen in Table 3. However, by searching around the neighbourhood of the GLD, the Local Neighbourhood Search (LNS) algorithm is able to account for the non-normality and obtain a more robust classifier. Thus, the GLD, together with the LNS procedure, achieves a higher classification accuracy than all the LDA approaches on all the realworld datasets (i.e. (c) to (l)) with the exception of dataset (f) which has the GLD showing superior classification accuracy. While the SVM outperforms the LDA approaches on half of the datasets, its training time can be rather long for large datasets. For instance, for dataset (d) which has 58000 elements, the SVM takes about 1.3 hours to train whereas the GLD with LNS, which achieves the best classification accuracy on this dataset, takes 43 seconds to train, representing over 100 fold savings in computational time over the SVM. Similar patterns can be seen in other datasets like (i), where the GLD with LNS achieves a superior classification accuracy with over 150 times shorter training time than the SVM. This suggests that for such large datasets, the GLD with Local Neighbourhood Search is a low-complexity alternative to the SVM, as it requires far less computational time than the SVM. We, however, make note of two weaknesses our proposed algorithms have. For the GLD, the procedure as described in Algorithm 1, may converge to a saddle point, instead of a local minimum. Even if it were to converge to a local minimum, there is no guarantee that is the global optimum solution due to the fact that the objective function pe is known to be non-convex Anderson & Bahadur (1962). Also, since the Local Neighbourhood Search involves evaluating the misclassification rate on the training set for every perturbation, the procedure does not scale well with large amounts of training", "startOffset": 79, "endOffset": 2314}], "year": 2017, "abstractText": "Under normality and homoscedasticity assumptions, Linear Discriminant Analysis (LDA) is known to be optimal in terms of minimising the Bayes error for binary classification. In the heteroscedastic case, LDA is not guaranteed to minimise this error. Assuming heteroscedasticity, we derive a linear classifier, the Gaussian Linear Discriminant (GLD), that directly minimises the Bayes error for binary classification. In addition, we also propose a local neighbourhood search (LNS) algorithm to obtain a more robust classifier if the data is known to have a non-normal distribution. We evaluate the proposed classifiers on two artificial and ten real-world datasets that cut across a wide range of application areas including handwriting recognition, medical diagnosis and remote sensing, and then compare our algorithm against existing LDA approaches and other linear classifiers. The GLD is shown to outperform the original LDA procedure in terms of the classification accuracy under heteroscedasticity. While it compares favourably with other existing heteroscedastic LDA approaches, the GLD requires as much as 60 times lower training time on some datasets. Our comparison with the support vector machine (SVM) also shows that, the GLD, together with the LNS, requires as much as 150 times lower training time to achieve an equivalent classification accuracy on some of the datasets. Thus, our algorithms can provide a cheap and reliable option for classification in a lot of expert systems.", "creator": "LaTeX with hyperref package"}}}