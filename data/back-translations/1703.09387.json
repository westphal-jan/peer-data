{"id": "1703.09387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples", "abstract": "These approaches include either the direct calculation of image pixel gradients or the direct solution of image pixel optimization. In this paper, we present a fundamentally new method for generating adversary examples that can be executed quickly and offers an exceptional variety of output. ATNs are trained to generate adverse examples in a self-monitored manner to generate adverse examples against a target network or a series of networks. We call such a network the Adversarial Transformation Network (ATN). ATNs are trained to generate adverse examples that minimally modify the outputs of the classifier in relation to the original input, while limiting the new classification to an adverse target class. We present methods to train ATNs and analyze their effectiveness that target a variety of MNIST classifiers and are aimed at the state of the art.", "histories": [["v1", "Tue, 28 Mar 2017 03:24:33 GMT  (3353kb,D)", "http://arxiv.org/abs/1703.09387v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV", "authors": ["shumeet baluja", "ian fischer"], "accepted": false, "id": "1703.09387"}, "pdf": {"name": "1703.09387.pdf", "metadata": {"source": "META", "title": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples ", "authors": ["Shumeet Baluja", "Ian Fischer"], "emails": [], "sections": [{"heading": "1. Introduction and Background", "text": "With the resurgence of deep neural networks for many realworld classification tasks, there is an increased interest in methods to generate training data, as well as to find weaknesses in trained models. An effective strategy to achieve both goals is to create adversarial examples that trained models will misclassify. Adversarial examples are small perturbations of the inputs that are carefully crafted to fool the network into producing incorrect outputs. These small perturbations can be used both offensively, to fool models into giving the \u201cwrong\u201d answer, and defensively, by providing training data at weak points in the model. Seminal work by Szegedy et al. (2013) and Goodfellow et al.\n(2014b), as well as much recent work, has shown that adversarial examples are abundant, and that there are many ways to discover them.\nGiven a classifier f(x) : x \u2208 X \u2192 y \u2208 Y and original inputs x \u2208 X , the problem of generating untargeted adversarial examples can be expressed as the optimization: argminx\u2217 L(x,x\n\u2217) s.t. f(x\u2217) 6= f(x), where L(\u00b7) is a distance metric between examples from the input space (e.g., the L2 norm). Similarly, generating a targeted adversarial attack on a classifier can be expressed as argminx\u2217 L(x,x\n\u2217) s.t. f(x\u2217) = yt, where yt \u2208 Y is some target label chosen by the attacker.1\nUntil now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as proposed in Szegedy et al. (2013) and Carlini & Wagner (2016). Such optimizer-based approaches tend to be much slower and more powerful than the other approaches. (2) By approximation with single-step gradient-based techniques like fast gradient sign (Goodfellow et al., 2014b) or fast least likely class (Kurakin et al., 2016a). These approaches are fast, requiring only a single forward and backward pass through the target classifier to compute the perturbation. (3) By approximation with iterative variants of gradient-based techniques (Kurakin et al., 2016a; MoosaviDezfooli et al., 2016a;b). These approaches use multiple forward and backward passes through the target network to more carefully move an input towards an adversarial classification.\n1Another axis to compare when considering adversarial attacks is whether the adversary has access to the internals of the target model. Attacks without internal access are possible by transferring successful attacks on one model to another model, as in Szegedy et al. (2013); Papernot et al. (2016a), and others. A more challenging class of blackbox attacks involves having no access to any relevant model, and only getting online access to the target model\u2019s output, as explored in Papernot et al. (2016b); Baluja et al. (2015); Trame\u0300r et al. (2016). See Papernot et al. (2015) for a detailed discussion of threat models.\nar X\niv :1\n70 3.\n09 38\n7v 1\n[ cs\n.N E\n] 2\n8 M\nar 2"}, {"heading": "2. Adversarial Transformation Networks", "text": "In this work, we propose Adversarial Transformation Networks (ATNs). An ATN is a neural network that transforms an input into an adversarial example against a target network or set of networks. ATNs may be untargeted or targeted, and trained in a black-box2 or white-box manner. In this work, we will focus on targeted, white-box ATNs.\nFormally, an ATN can be defined as a neural network:\ngf,\u03b8(x) : x \u2208 X \u2192 x\u2032 (1)\nwhere \u03b8 is the parameter vector of g, f is the target network which outputs a probability distribution across class labels, and x\u2032 \u223c x, but argmax f(x) 6= argmax f(x\u2032).\nTraining. To find gf,\u03b8, we solve the following optimization:\nargmin \u03b8 \u2211 xi\u2208X \u03b2LX (gf,\u03b8(xi),xi)+LY(f(gf,\u03b8(xi)), f(xi)) (2) where LX is a loss function in the input space (e.g., L2 loss or a perceptual similarity loss like Johnson et al. (2016)), LY is a specially-formed loss on the output space of f (described below) to avoid learning the identity function, and \u03b2 is a weight to balance the two loss functions. We will omit \u03b8 from gf when there is no ambiguity.\nInference. At inference time, gf can be run on any input x without requiring further access to f or more gradient computations. This means that after being trained, gf can generate adversarial examples against the target network f even faster than the single-step gradient-based approaches, such as fast gradient sign, so long as ||gf || / ||f ||.\nLoss Functions. The input-space loss function, LX , would ideally correspond closely to human perception. However, for simplicity, L2 is sufficient. LY determines whether or not the ATN is targeted; the target refers to the class for which the adversary will cause the classifier to output the maximum value. In this work, we focus on the more challenging case of creating targeted ATNs, which can be defined similarly to Equation 1:\ngf,t(x) : x \u2208 X \u2192 x\u2032 (3)\nwhere t is the target class, so that argmax f(x\u2032) = t. This allows us to target the exact class the classifier should mistakenly believe the input is.\nIn this work, we define LY,t(y\u2032,y) = L2(y\u2032, r(y, t)), where y = f(x), y\u2032 = f(gf (x)), and r(\u00b7) is a reranking function that modifies y such that yk < yt,\u2200 k 6= t.\n2E.g., using Williams (1992) to generate training gradients for the ATN based on a reward signal computed on the result of sending the generated adversarial examples to the target network.\nNote that training labels for the target network are not required at any point in this process. All that is required is the target network\u2019s outputs y and y\u2032. It is therefore possible to train ATNs in a self-supervised manner, where they use unlabeled data as the input and make argmax f(gf,t(x)) = t.\nReranking function. There are a variety of options for the reranking function. The simplest is to set r(y, t) = onehot(t), but other formulations can make better use of the signal already present in y to encourage better reconstructions. In this work, we look at reranking functions that attempt to keep r(y, t) \u223c y. In particular, we use r(\u00b7) that maintains the rank order of all but the targeted class in order to minimize distortions when computing x\u2032 = gf,t(x).\nThe specific r(\u00b7) used in our experiments has the following form:\nr\u03b1(y, t) = norm {\u03b1 \u2217maxy if k = t yk otherwise } k\u2208y  (4) \u03b1 > 1 is an additional parameter specifying how much larger yt should be than the current max classification. norm(\u00b7) is a normalization function that rescales its input to be a valid probability distribution."}, {"heading": "2.1. Adversarial Example Generation", "text": "There are two approaches to generating adversarial examples with an ATN. The ATN can be trained to generate just the perturbation to x, or it can be trained to generate an adversarial autoencoding of x.\n\u2022 Perturbation ATN (P-ATN): To just generate a perturbation, it is sufficient to structure the ATN as a variation on the residual block (He et al., 2015): gf (x) = tanh(x+G(x)), where G(\u00b7) represents the core function of gf . With small initial weight vectors, this structure makes it easy for the network to learn to generate small, but effective, perturbations.\n\u2022 Adversarial Autoencoding (AAE): AAE ATNs are similar to standard autoencoders, in that they attempt to accurately reconstruct the original input, subject to regularization, such as weight decay or an added noise signal. For AAE ATNs, the regularizer is LY . This imposes an additional requirement on the AAE to add some perturbation p to x such that r(f(x\u2032)) = y\u2032.\nFor both ATN approaches, in order to enforce that x\u2032 is a plausible member of X , the ATN should only generate values in the valid input range of f . For images, it suffices to set the activation function of the last layer to be the tanh function; this constrains each output channel to [\u22121, 1]."}, {"heading": "2.2. Related Network Architectures", "text": "This training objective resembles standard Generative Adversarial Network training (Goodfellow et al., 2014a) in that the goal is to find weaknesses in the classifier. It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations. Unlike standard network training in which the gradients are used to modify the weights of the network, here, the network weights are frozen and the input itself is changed. In subsequent work, Ulyanov et al. (2016) created a method to approximate the results of the gradient descent procedure through the use of an off-line trained neural network. Ulyanov et al. (2016) removed the need for a gradient descent procedure to operate on every source image to which a new artistic style was to be applied, and replaced it with a single forward pass through a separate network. Analagously, we do the same for generating adverarial examples: a separately trained network approximates the usual gradient descent procedure done on the target network to find adversarial examples."}, {"heading": "3. MNIST Experiments", "text": "To begin our empirical exploration, we train five networks on the standard MNIST digit classification task (LeCun et al., 1998). The networks are trained and tested on the same data; they vary only in the weight initialization and architecture, as shown in Table 1. Each network has a mix of convolution (Conv) and Fully Connected (FC) layers. The input to the networks is a 28x28 grayscale image and the output is 10 logit units. Classifierp and Classifiera0 use the same architecture, and only differ in the initialization of the weights. We will primarily use Classifierp for the experiments in this section. The other networks will be used\nlater to analyze the generalization capabilities of the adversaries. Table 1 shows that all of the networks perform well on the digit recognition task.3\nWe attempt to create an Adversarial Autoencoding ATN that can target a specific class given any input image. The ATN is trained against a particular classifier as illustrated in Figure 1. The ATN takes the original input image, x, as input, and outputs a new image, x\u2032, that the target classifier should erroneously classify as t. We also add the constraint that the ATN should maintain the ordering of all the other classes as initially output by the classifier. We train ten ATNs against Classifierp \u2013 one for each target digit, t.\nAn example is provided to make this concrete. If a classifier is given an image, x3, of the digit 3, a successful ordering of the outputs (from largest to smallest) may be as follows: Classifierp(x3) \u2192 [3, 8, 5, 0, 4, 1, 9, 7, 6, 2]. If ATN7 is applied to x3, when the resulting image, x\u20323, is fed into the same classifier, the following ordering of outputs is desired (note that the 7 has moved to the highest output): Classifierp(ATN7(x3))\u2192 [7, 3, 8, 5, 0, 4, 1, 9, 6, 2].\nTraining for a single ATNt proceeds as follows. The weights of Classifierp are frozen and never change during ATN training. Every training image, x, is passed through Classifierp to obtain output y. As described in Equation 4, we then compute r\u03b1(y, t) by copying y to a new value, y\u2032,\n3It is easy to get better performance than this on MNIST, but for these experiments, it was more important to have a variety of architectures that achieved similar accuracy, than to have state-ofthe-art performance.\nsetting y\u2032t = \u03b1 \u2217max(y), and then renormalizing y\u2032 to be a valid probability distribution. This sets the target class, t, to have the highest value in y\u2032 while maintaining the relative order of the other original classifications. In the MNIST experiments, we empirically set \u03b1 = 1.5.\nGiven y\u2032, we can now train ATNt to generate x\u2032 by minimizing \u03b2 \u2217LX = \u03b2 \u2217L2(x,x\u2032) and LY = L2(y,y\u2032) using Equation 2. Though the weights of Classifierp are frozen, error derivatives are still passed through them to train the ATN. We explore several values of \u03b2 to balance the two loss functions. The results are shown in Table 2.\nExperiments. We tried three ATN architectures for the AAE task, and each was trained with three values of \u03b2 against all ten targets, t. The full 3 \u00d7 3 set of experiments are shown in Table 2. The accuracies shown are the ability of ATNt to transform an input image x into x\u2032 such that Classifierp mistakenly classifies x\u2032 as t.4 Each measurement in Table 2 is the average of the 10 networks, ATN0\u22129.\nResults. In Figure 2(top), each row represents the transformation that ATNt makes to digits that were initially correctly classified as 0-9 (columns). For example, in the top row, the digits 1-9 are now all classified as 0. In all cases, their second highest classification is the original correct classification (0-9).\nThe reconstructions shown in Figure 2(top) have the largest \u03b2; smaller \u03b2 values are shown in the bottom row. The fidelity to the underlying digit diminishes as \u03b2 is reduced. However, by loosening the constraints to stay similar to the original input, the number of trials in which the trans-\n4Images that were originally classified as t were not counted in the test as no transformation on them was required.\nformer network is able to successfully \u201cfool\u201d the classification network increases dramatically, as seen in Table 2. Interestingly, with \u03b2 = 0.010, in Figure 2(second row), where there should be a \u20180\u2019 that is transformed into a \u20181\u2019, no digit appears. With this high \u03b2, no example was found that could be transformed to successfully fool Classifierp. With the two smaller \u03b2 values, this anomaly does not occur.\nIn Figure 3, we provide a closer look at examples of x and x\u2032 for ATNc with \u03b2 = 0.005. A few points should be noted:\n\u2022 The transformations maintain the large, empty regions of the image. Unlike many previous studies in attacking classifiers, the addition of salt-and-pepper type noise did not appear (Nguyen et al., 2014; MoosaviDezfooli et al., 2016b).\n\u2022 In the majority of the generated examples, the shape of the digit does not dramatically change. This is the desired behavior: by training the networks to maintain the order beyond the top-output, only minimal changes should be made to the image. The changes that are often introduced are patches where the light strokes have become darker.\n\u2022 Vertical-linear components of the original images are emphasized in several digits; it is especially noticeable in the digits transformed to 1. With other digits (e.g., 8), it is more difficult to find a consistent pattern of what is being (de)emphasized to cause the classification network to be fooled.\nA novel aspect of ATNs is that though they cause the target classifier to output an erroneous top-class, they are also trained to ensure that the transformation preserves the existing output ordering of the target-classifier (other than the top-class). For the examples that were successfully transformed, Table 3 gives the average rank-difference of the outputs with the pre-and-post transformed images (excluding the intentional targeted misclassification)."}, {"heading": "4. A Deeper Look into ATNs", "text": "This section explores three extensions to the basic ATNs: increasing the number of networks the ATNs can attack, using hidden state from the target network, and using ATNs in serial and parallel."}, {"heading": "4.1. Adversarial Transfer to Other Networks", "text": "So far, we have examined ATNs in the context of attacking a single classifier. Can ATNs create adversarial exam-\nTable 4. ATNb with \u03b2 = 0.005 trained to defeat Classifierp. Tested on 5 classifiers, without further training, to measure transfer. 1st place is the percentage of times t was the top classification. 2nd place measures how many times the original top class (argmaxy) was correctly placed into 2nd place, conditioned on the 1st place being correct (Conditional) or unconditioned on 1st place (Unconditional).\nClassifierp* Classifiera0 Classifiera1 Classifiera2 Classifiera3 1st Place Correct 82.5% 15.7% 16.1% 7.7% 28.9%\n2nd Place Correct (Conditional) 96.6% 84.7% 89.3% 85.0% 81.8% 2nd Place Correct (Unconditional) 79.7% 15.6% 16.1% 8.4% 26.2%\nples that generalize to other classifiers? Much research has studied adversarial transfer for traditional adversaries, including the recent work of Moosavi-Dezfooli et al. (2016a); Liu et al. (2016).\nTargeting multiple networks. To test transfer, we take the adversarial examples from the previously trained ATNs and test them against Classifiera0,a1,a2,a3 (described in Table 1).\nThe results in Table 4 clearly show that the transformations made by the ATN are not general; they are tied to the network it is trained to attack. Even Classifiera0, which has the same architecture as Classifierp, is not more susceptible to the attacks than those with different architectures. Looking at the second place correctness scores (in the same Table 4), it may, at first, seem counter-intuitive that the conditional probability of a correct second-place classification remains high despite a low first-place classification. The reason for this is that in the few cases in which the ATN was able to successfully change the classifier\u2019s top choice, the second choice (the real classification) remained a close second (i.e., the image was not transformed in a large manner), thereby maintaining the high performance in the conditional second rank measurement.\nTraining against multiple networks. Is it possible to create a network that will be able to create a single transform that can attack multiple networks? Will such an ATN generalize better to unseen networks? To test this, we created an ATN that receives training signals from multiple networks, as shown in Figure 4. As with the earlier training, the LX reconstruction error remains.\nThe new ATN was trained with classification signals from three networks: Classifierp, and Classifiera1,2. The training proceeds in exactly the same manner as described earlier, except the ATN attempts to minimize LY for all three target networks at the same time. The results are shown in Table 5. First, examine the columns corresponding to the networks that were used in the training (marked with an *). Note that the success rates of attacking these three classifiers are consistently high, comparable with those when\nFigure 4. The ATN now has to fool three networks (of various architectures), while also minimizing LX , the reconstruction error.\nthe ATN was trained with a single network. Therefore, it is possible to learn a transformation network that modifies images such that perturbation defeats multiple networks.\nNext, we turn to the remaining two networks to which the adversary was not given access during training. There is a large increase in success rates over those when the ATN was trained with a single target network (Table 4). However, the results do not match those of the networks used in training. It is possible that training against larger numbers of target networks at the same time could further increase the transferability of the adversarial examples.\nFinally, we look at the success rates of image transformations. Do the same images consistenly fool the networks, or are the failure cases of the networks different? As shown in Figure 5, for the 3 networks the ATN was trained to defeat, the majority of transformations attacked all three networks successfully. For the unseen networks, the results were mixed; the majority of transformations successfully attacked only a single network.\nTable 5. ATNb retrained with 3 networks (marked with *).\n\u03b2 Classifierp* Classifiera0 Classifiera1* Classifiera2* Classifiera3\n0.010 1st Place Correct 89.9% 37.9% 83.9% 78.7% 70.2%\n2nd Place Correct (Conditional) 96.1% 88.1% 96.1% 95.2% 79.1% 2nd Place Correct (Unconditional) 86.4% 34.4% 80.7% 74.9% 55.9%\n0.005 1st Place Correct 93.6% 34.7% 88.1% 82.7% 64.1%\n2nd Place Correct (Conditional) 96.8% 88.3% 96.9% 96.4% 73.1% 2nd Place Correct (Unconditional) 90.7% 31.4% 85.3% 79.8% 47.2%\nFigure 5. Do the same transformed examples work well on all the networks? (Top) Percentage of examples that worked on exactly 0-3 training networks. (Bottom) Percentage of examples that worked on exactly 0-2 unseen networks. Note: these are all measured on independent test set images."}, {"heading": "4.2. \u201cInsider\u201d Information", "text": "In the experiments thus far, the classifier, C, was treated as a white box. From this box, two pieces of information were needed to train the ATN. First, the actual outputs of C were used to create the new target vector. Second, the error derivatives from the new target vector were passed through C and propagated into the ATN.\nIn this section, we examine the possibility of \u201copening\u201d the classifier, and accessing more of its internal state. From C, the actual hidden unit activations for each example are used as additional inputs to the ATN. Intuitively, because the goal is to maintain as much similarity as possible to the original image and to maintain the same order of the non-top-most classifications as the original image, access to these activations may convey usable signals.\nBecause of the very large number of hidden units that accompany convolution layers, in practice, we only use the penultimate fully-connected layer from C. The results of training the ATNs with this extra information are shown in Table 6. Interestingly, the most salient difference does\nnot come from the ability of the ATN to attack the networks in the first-position. Rather, when looking at the conditional-successes of the second-position, the numbers are improved (compare to Table 2). We speculate that this is because the extra hints provided by the classifier\u2019s internal activations (with the unmodified image) could be used to also ensure that the second-place classification, after input modification, was also correctly maintained."}, {"heading": "4.3. Serial and Parallel ATNs", "text": "Separate ATNs are created for each digit (0-9). In this section, we examine whether the ATNs can be used in parallel (can the same original image be transformed by each of the ATNs successfully?) and in serial (can the same image be transformed by one ATN then that resulting image be transformed by another, successfully?).\nIn the first test, we started with 1000 images of digits from the test set. Each was passed through all 10 ATNs (ATNc, \u03b2 = 0.005); the resulting images were then classified with Classifierp. For each image, we measured how many ATNs were able to successfully transform the image (success is defined for ATNt as causing the classifier to output t as the top-class). Out of the 1000 trials, 283 were successfully\ntransformed by all 10 of the ATNs. Samples results and a histogram of the results are shown in Figure 6.\nA second experiment is constructed in which the 10 ATNs are applied serially, one-after-the-other. In this scenario, first ATN0 is applied to image x, yielding x\u2032. Then ATN1 is applied to x\u2032 yielding x\u2032\u2032 ... to ATN9. The goal is to see whether the transformations work on previously transformed images. The results of chaining the ATNs together in this manner are shown in Figure 6(right). The more transformations that are applied, the larger the image degradation. As expected, by the ninth transformation (rightmost column in Figure 6) the majority of images are severely degraded and usually not recognizable. Though we expected the degradation in images, there were two additional, surprising, findings. First, in the parallel application of ATNs (the first experiment described above), out of 1000 images, 283 of them were successfully transformed by 10 of the ATNs. In this experiment, 741 images were successfully transformed by 10 ATNs. The improvement in the number of all-10 successes over applying the ATNs in parallel occurs because each transformation effectively diminishes the underlying original image (to remove the real classification from the top-spot). Meanwhile, only a few new pixels are added by the ATN to cause the misclassification as it is also trained to minimize the reconstruction error. The overarching effect is a fading of the image through chaining ATNs together.\nSecond, it is interesting to examine what happens to the second-highest classifications that the networks were also trained to preserve. Order preservation did not occur in this test. Had the test worked perfectly, then for an input-image, x (e.g., of the digit 8), after ATN0 was applied, the first\nand second top classifications of x\u2032 should be 0,8, respectively. Subsequently, after ATN1 is then applied to x\u2032, the classifications of x\u2032\u2032 should be 1,0,8, etc. The reason this does not hold in practice is that though the networks were trained to maintain the high classification (8) of the original digit, x, they were not trained to maintain the potentially small perturbations that ATN0 made to x to achieve a top-classification of 0. Therefore, when ATN1 is applied, the changes that ATN0 made may not survive the transformation. Nonetheless, if chaining adversaries becomes important, then training the ATNs with images that have been previously modified by other ATNs may be a sufficient method to address the difference in training and testing distributions. This is left for future work."}, {"heading": "5. ImageNet Experiments", "text": "We explore the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.2 million natural images categorized into 1 of 1000 classes. The target classifier, f , used in these experiments is a pre-trained state-of-the-art classifier, Inception ResNet v2 (IR2), that has a top-1 single-crop error rate of 19.9% on the 50,000 image validation set, and a top-5 error rate of 4.9%. It is described fully in Szegedy et al. (2016)."}, {"heading": "5.1. Experiment Setup", "text": "We trained AAE ATNs and P-ATNs as described in Section 2 to attack IR2. Training an ATN against IR2 follows the process described in Section 3.\nIR2 takes as input images scaled to 299 \u00d7 299 pixels of 3 channels each. To autoencode images of this size for the\nAAE task, we use three different fully convolutional architectures (Table 7):\n\u2022 IR2-Base-Deconv, a small architecture that uses the first few layers of IR2 and loads the pre-trained parameter values at the start of training the ATN, followed by deconvolutional layers;\n\u2022 IR2-Resize-Conv, a small architecture that avoids checkerboard artifacts common in deconvolutional layers by using bilinear resize layers to downsample and upsample between stride 1 convolutions; and\n\u2022 IR2-Conv-Deconv, a medium architecture that is a tower of convolutions followed by deconvolutions.\nFor the perturbation approach, we use IR2-Base-Deconv and IR2-Conv-FC, which has many more parameters than the other architectures due to two large fully-connected layers. The use of fully-connected layers cause the network to learn too slowly for the autoencoding approach (AAE ATN), but can be used to learn perturbations quickly (PATN).\nHyperparameter search. All five architectures across both tasks are trained with the same hyperparameters. For each architecture and task, we trained four networks, one for each target class: binoculars, soccer ball, volcano, and zebra. In total, we trained 20 different ATNs to attack IR2.\nTo find a good set of hyperparameters for these networks, we did a series of grid searches through reasonable parameter values for learning rate, \u03b1, and \u03b2, using only Volcano as the target class. Those training runs were terminated after 0.025 epochs, which is only 1600 training steps with a batch size of 20. Based on the parameter search, for the results reported here, we set the learning rate to 0.0001, \u03b1 = 1.5, and \u03b2 = 0.01. All runs were trained for 0.1 epochs (6400 steps) on shuffled training set images, using the Adam optimizer and the TensorFlow default settings.\nIn order to avoid cherrypicking the best results after the networks were trained, we selected four images from the unperturbed validation set to use for the figures in this paper prior to training. Once training finished, we evaluated the ATNs by passing 1000 images from the validation set through the ATN and measuring IR2\u2019s accuracy on those adversarial examples."}, {"heading": "5.2. Results Overview", "text": "Table 8 shows the top-1 adversarial accuracy for each of the 20 model/target combinations. The AAE approach is superior to the perturbation approach, both in terms of top-1 adversarial accuracy, and in terms of training success. Nonetheless, the results in Figures 9 and 7 show\nthat using an architecture like IR2-Conv-FC can provide a qualitatively different type of adversary from the AAE approach.The examples generated using the perturbation approach preserve more pixels in the original image, at the expense of a small region of large perturbations.\nIn contrast to the perturbation approaches, the AAE architectures distribute the differences across wider regions of the image. However, IR2-Base-Deconv and IR2-Conv-Deconv tend to exhibit checkerboard patterns, which is a common problem in image generation with deconvolutions (Odena et al. (2016)). The checkerboarding led us to try IR2-Resize-Conv, which avoids the checkerboard pattern, but gives smooth outputs (Figure 9). Interestingly, in all three AAE networks, many of the original high-frequency patterns are replaced with high frequencies that encode the adversarial signal.\nThe results from IR2-Base-Deconv show that the same network architectures perform substantially differently when trained as P-ATNs and AAE ATNs. Since P-ATNs are only learning to perturb the input, these networks are much better at preserving the original image, but the perturbations end up being focused along the edges or in the corners of the image. The form of the perturbations often manifests itself as \u201cDeepDream\u201d-like images, as in Figure 8. Approximately the same perturbation, in the same place, is used across all input examples. Placing the perturbations in that manner is less likely to disrupt the other top classifications, thereby keeping LY lower. This is in stark contrast to the AAE ATNs, which creatively modify the input, as seen in Figures 9 and 7."}, {"heading": "5.3. Detailed Discussion", "text": "Adversarial diversity. Figure 7 shows that ATNs are capable of generating a wide variety of adversarial perturbations targeting a single network. Previous approaches to generating adversarial examples often produced qualitatively uniform results \u2013 they add various amounts of \u201cnoise\u201d to the image, generally concentrating the noise at pixels with large gradient magnitude for the particular adversarial loss function. Indeed, Hendrik Metzen et al. (2017) recently showed that it may be possible to train a detector for previous adversarial attacks. From the perspective of an attacker, then, adversarial examples produced by ATNs may provide a new way past defenses in the cat-andmouse game of security, since this somewhat unpredictable diversity will likely challenge such approaches to defense. Perhaps a much more interesting consequence of this diversity is its potential application for more comprehensive adversarial training, as described below.\nAdversarial Training with ATNs. In Kurakin et al. (2016b), the authors show the current state-of-the-art in\nusing adveraries for improving training. With single step and iterative gradient methods, they find that it is possible to increase a network\u2019s robustness to adversarial examples, while suffering a small loss of accuracy on clean inputs. However, it works only for the adversary the network was trained against. It appears that ATNs could be used in their adversarial training architecture, and could provide substantially more diversity to the trained model than current adversaries. This adversarial diversity might improve model test-set generalization and adversarial robustness.\nBecause ATNs are quick to train relative to the target network (in the case of IR2, hours instead of weeks), reliably produce diverse adversarial examples, and can be automatically checked for quality (by checking their success rate against the target network and the LX magnitude of the adversarial examples), they could be used as follows: Train a set of ATNs targeting a random subset of the output classes on a checkpoint of the target network. Once the ATNs are trained, replace a fraction of each training batch with corresponding adversarial examples, subject to two constraints: the current classifier incorrectly classifies the adversarial example as the target class, and the LX loss of the adversarial example is below a threshold that indicates it is similar to the original image. If a given ATN stops producing successful adversarial examples, replace it with a newly trained ATN targeting another randomly selected class. In this manner, throughout training, the target network would be exposed to a shifting set of diverse adversaries from ATNs that can be trained in a fully-automated manner.5,6\nDeepDream perturbations. IR2-Conv-FC exhibits interesting behavior not seen in any of the other architectures. The network builds a perturbation that generally contains spatially coherent, recognizable regions of the target class. For example, in Figure 8, a consistent soccer-ball \u201cghost\u201d image appears in all of the transformed images. While the methods and goals of these perturbations are quite different from those generated by DeepDream (Mordvintsev et al., 2015), the qualitative results appear similar. IR2-Conv-FC seems to learn to distill the target network\u2019s representation of the target class in a manner that can be drawn across a large fraction of the image.7 This result hints at a direct\n5This procedure conceptually resembles GAN training (Goodfellow et al., 2014a) in many ways, but the goal is different: for GANs, the focus is on using an easy-to-train discriminator to learn a hard-to-train generator; for this adversarial training system, the focus is on using easy-to-train generators to learn a hard-to-train multi-class classifier.\n6Note also that we can run the adversarial example generation in this algorithm on unlabeled data, as described in Section 2. Miyato et al. (2016) also describe a method for using unlabeled data in a manner conceptually similar to adversarial training.\n7This is likely due to the final fully-connected layer, which has one weight for each pixel and channel, allowing the network to specify a particular output at each pixel.\nrelationship between DeepDream-style techniques and adversarial examples that may improve our ability to find and correct weaknesses in our models.\nHigh frequency data. The AAE ATNs all remove high frequency data from the images when building their reconstructions. This is likely to be due to limitations of the underlying architectures. In particular, all three convolutional architectures have difficulty exactly recreating edges from the input image, due to spatial data loss introduced when downsampling and padding. Consequently, the LX loss penalizes high confidence predictions of edge locations, leading the networks to learn to smooth out boundaries in the reconstruction. This strategy minimizes the overall loss, but it also places a lower bound on the error imposed by pixels in regions with high frequency information.\nThis lower bound on the loss in some regions provides the network with an interesting strategy when generating an AAE output: it can focus the adversarial perturbations in regions of the input image that have high-frequency noise. This strategy is visible in many of the more interesting images in Figure 7. For example, many of the networks make minimal modification to the sky in the dog image, but add substantial changes around the edges of the dog\u2019s face, exactly where the LX error would be high in a nonadversarial reconstruction."}, {"heading": "6. Conclusions and Future Work", "text": "Current methods for generating adversarial samples involve a gradient descent procedure on individual input ex-\namples. We have presented a fundamentally different approach to finding examples by training neural networks to convert inputs into adversarial examples. Our method is efficient to train, fast to execute, and produces remarkably diverse, successful adversarial examples.\nFuture work should explore the possibility of using ATNs in adversarial training. A successful ATN-based system may pave the way towards models with better generaliza-\ntion and robustness.\nHendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries. It may be possible to train such detectors on ATN output. If so, using that signal as an additional loss for the ATN may improve the outputs. Similarly, exploring the use of a GAN discriminator during training may improve the realism of the ATN outputs. It would be interesting to explore the impact of ATNs on generative models, rather than just classifiers, similar to work in Kos et al. (2017). Finally, it may also be possible to train ATNs in a black-box manner, similar to recent work in Trame\u0300r et al. (2016); Baluja et al. (2015), or using REINFORCE (Williams, 1992) to compute gradients for the ATN using the target network simply as a reward signal."}], "references": [{"title": "The virtues of peer pressure: A simple method for discovering high-value mistakes", "author": ["Baluja", "Shumeet", "Covell", "Michele", "Sukthankar", "Rahul"], "venue": "In Int. Conf. on Computer Analysis of Images and Patterns,", "citeRegEx": "Baluja et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baluja et al\\.", "year": 2015}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Carlini", "Nicholas", "Wagner", "David"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carlini et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "A neural algorithm of artistic", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "style. CoRR,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "On Detecting Adversarial Perturbations", "author": ["J. Hendrik Metzen", "T. Genewein", "V. Fischer", "B. Bischoff"], "venue": "ArXiv e-prints,", "citeRegEx": "Metzen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Metzen et al\\.", "year": 2017}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Johnson", "Justin", "Alahi", "Alexandre", "Fei-Fei", "Li"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Adversarial examples for generative models", "author": ["Kos", "Jernej", "Fischer", "Ian", "Song", "Dawn"], "venue": "arXiv preprint arXiv:1702.06832,", "citeRegEx": "Kos et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kos et al\\.", "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian J", "Bengio", "Samy"], "venue": "CoRR, abs/1607.02533,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Adversarial machine learning at scale", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian J", "Bengio", "Samy"], "venue": "CoRR, abs/1611.01236,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Inversion of multilayer nets", "author": ["Linden", "Alexander", "J. Kindermann"], "venue": "In Neural Networks,", "citeRegEx": "Linden et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Linden et al\\.", "year": 1989}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Liu", "Yanpei", "Chen", "Xinyun", "Chang", "Song", "Dawn"], "venue": "CoRR, abs/1611.02770,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Miyato", "Takeru", "Maeda", "Shin-ichi", "Koyama", "Masanori", "Nakae", "Ken", "Ishii", "Shin"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Universal adversarial perturbations", "author": ["Moosavi-Dezfooli", "Seyed-Mohsen", "Fawzi", "Alhussein", "Omar", "Frossard", "Pascal"], "venue": "CoRR, abs/1610.08401,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Moosavi-Dezfooli", "Seyed-Mohsen", "Fawzi", "Alhussein", "Frossard", "Pascal"], "venue": "In Proceedings of the IEEE CVPR,", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Nguyen", "Anh Mai", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "images. CoRR,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Deconvolution and checkerboard artifacts. Distill, 2016", "author": ["Odena", "Augustus", "Dumoulin", "Vincent", "Olah", "Chris"], "venue": null, "citeRegEx": "Odena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Jha", "Somesh", "Fredrikson", "Matt", "Celik", "Z Berkay", "Swami", "Ananthram"], "venue": "In Proceedings of the 1st IEEE European Symposium on Security and Privacy,", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Goodfellow", "Ian"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Goodfellow", "Ian", "Jha", "Somesh", "Celik", "Z Berkay", "Swami", "Ananthram"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Stealing machine learning models via prediction apis", "author": ["Tram\u00e8r", "Florian", "Zhang", "Fan", "Juels", "Ari", "Reiter", "Michael K", "Ristenpart", "Thomas"], "venue": "In USENIX Security,", "citeRegEx": "Tram\u00e8r et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tram\u00e8r et al\\.", "year": 2016}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["Ulyanov", "Dmitry", "Lebedev", "Vadim", "Vedaldi", "Andrea", "Lempitsky", "Victor S"], "venue": "CoRR, abs/1603.03417,", "citeRegEx": "Ulyanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulyanov et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 22, "context": "Seminal work by Szegedy et al. (2013) and Goodfellow et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 4, "context": "(2013) and Goodfellow et al. (2014b), as well as much recent work, has shown that adversarial examples are abundant, and that there are many ways to discover them.", "startOffset": 11, "endOffset": 37}, {"referenceID": 20, "context": "Until now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as proposed in Szegedy et al. (2013) and Carlini & Wagner (2016).", "startOffset": 175, "endOffset": 197}, {"referenceID": 20, "context": "Until now, these optimization problems have been solved using three broad approaches: (1) By directly using optimizers like L-BFGS or Adam (Kingma & Ba, 2015), as proposed in Szegedy et al. (2013) and Carlini & Wagner (2016). Such optimizer-based approaches tend to be much slower and more powerful than the other approaches.", "startOffset": 175, "endOffset": 225}, {"referenceID": 20, "context": "Attacks without internal access are possible by transferring successful attacks on one model to another model, as in Szegedy et al. (2013); Papernot et al.", "startOffset": 117, "endOffset": 139}, {"referenceID": 20, "context": "(2013); Papernot et al. (2016a), and others.", "startOffset": 8, "endOffset": 32}, {"referenceID": 20, "context": "(2013); Papernot et al. (2016a), and others. A more challenging class of blackbox attacks involves having no access to any relevant model, and only getting online access to the target model\u2019s output, as explored in Papernot et al. (2016b); Baluja et al.", "startOffset": 8, "endOffset": 239}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al. (2016). See Papernot et al.", "startOffset": 9, "endOffset": 52}, {"referenceID": 0, "context": "(2016b); Baluja et al. (2015); Tram\u00e8r et al. (2016). See Papernot et al. (2015) for a detailed discussion of threat models.", "startOffset": 9, "endOffset": 80}, {"referenceID": 8, "context": ", L2 loss or a perceptual similarity loss like Johnson et al. (2016)), LY is a specially-formed loss on the output space of f (described below) to avoid learning the identity function, and \u03b2 is a weight to balance the two loss functions.", "startOffset": 47, "endOffset": 69}, {"referenceID": 6, "context": "\u2022 Perturbation ATN (P-ATN): To just generate a perturbation, it is sufficient to structure the ATN as a variation on the residual block (He et al., 2015): gf (x) = tanh(x+G(x)), where G(\u00b7) represents the core function of gf .", "startOffset": 136, "endOffset": 153}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al.", "startOffset": 178, "endOffset": 198}, {"referenceID": 27, "context": ", 2015)(Ulyanov et al., 2016).", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations.", "startOffset": 179, "endOffset": 242}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations. Unlike standard network training in which the gradients are used to modify the weights of the network, here, the network weights are frozen and the input itself is changed. In subsequent work, Ulyanov et al. (2016) created a method to approximate the results of the gradient descent procedure through the use of an off-line trained neural network.", "startOffset": 179, "endOffset": 707}, {"referenceID": 3, "context": "It is interesting to note the similarity to work outside the adversarial training paradigm \u2014 the recent use of feed-forward neural networks for artistic style transfer in images (Gatys et al., 2015)(Ulyanov et al., 2016). Gatys et al. (2015) originally proposed a gradient descent procedure based on \u201cback-driving networks\u201d (Linden & Kindermann, 1989) to modify the inputs of a fully-trained network to find a set of inputs that maximize a desired set of outputs and hidden unit activations. Unlike standard network training in which the gradients are used to modify the weights of the network, here, the network weights are frozen and the input itself is changed. In subsequent work, Ulyanov et al. (2016) created a method to approximate the results of the gradient descent procedure through the use of an off-line trained neural network. Ulyanov et al. (2016) removed the need for a gradient descent procedure to operate on every source image to which a new artistic style was to be applied, and replaced it with a single forward pass through a separate network.", "startOffset": 179, "endOffset": 862}, {"referenceID": 13, "context": "To begin our empirical exploration, we train five networks on the standard MNIST digit classification task (LeCun et al., 1998).", "startOffset": 107, "endOffset": 127}, {"referenceID": 19, "context": "Unlike many previous studies in attacking classifiers, the addition of salt-and-pepper type noise did not appear (Nguyen et al., 2014; MoosaviDezfooli et al., 2016b).", "startOffset": 113, "endOffset": 165}, {"referenceID": 16, "context": "ples that generalize to other classifiers? Much research has studied adversarial transfer for traditional adversaries, including the recent work of Moosavi-Dezfooli et al. (2016a); Liu et al.", "startOffset": 148, "endOffset": 180}, {"referenceID": 15, "context": "(2016a); Liu et al. (2016).", "startOffset": 9, "endOffset": 27}, {"referenceID": 2, "context": "We explore the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": "We explore the effectiveness of ATNs on the ImageNet dataset (Deng et al., 2009), which consists of 1.2 million natural images categorized into 1 of 1000 classes. The target classifier, f , used in these experiments is a pre-trained state-of-the-art classifier, Inception ResNet v2 (IR2), that has a top-1 single-crop error rate of 19.9% on the 50,000 image validation set, and a top-5 error rate of 4.9%. It is described fully in Szegedy et al. (2016).", "startOffset": 62, "endOffset": 453}, {"referenceID": 20, "context": "However, IR2-Base-Deconv and IR2-Conv-Deconv tend to exhibit checkerboard patterns, which is a common problem in image generation with deconvolutions (Odena et al. (2016)).", "startOffset": 151, "endOffset": 171}, {"referenceID": 7, "context": "Indeed, Hendrik Metzen et al. (2017) recently showed that it may be possible to train a detector for previous adversarial attacks.", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "In Kurakin et al. (2016b), the authors show the current state-of-the-art in", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "This procedure conceptually resembles GAN training (Goodfellow et al., 2014a) in many ways, but the goal is different: for GANs, the focus is on using an easy-to-train discriminator to learn a hard-to-train generator; for this adversarial training system, the focus is on using easy-to-train generators to learn a hard-to-train multi-class classifier. Note also that we can run the adversarial example generation in this algorithm on unlabeled data, as described in Section 2. Miyato et al. (2016) also describe a method for using unlabeled data in a manner conceptually similar to adversarial training.", "startOffset": 52, "endOffset": 498}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries.", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries. It may be possible to train such detectors on ATN output. If so, using that signal as an additional loss for the ATN may improve the outputs. Similarly, exploring the use of a GAN discriminator during training may improve the realism of the ATN outputs. It would be interesting to explore the impact of ATNs on generative models, rather than just classifiers, similar to work in Kos et al. (2017). Finally, it may also be possible to train ATNs in a black-box manner, similar to recent work in Tram\u00e8r et al.", "startOffset": 8, "endOffset": 536}, {"referenceID": 6, "context": "Hendrik Metzen et al. (2017) recently showed that it is possible to detect when an input is adversarial, for current types of adversaries. It may be possible to train such detectors on ATN output. If so, using that signal as an additional loss for the ATN may improve the outputs. Similarly, exploring the use of a GAN discriminator during training may improve the realism of the ATN outputs. It would be interesting to explore the impact of ATNs on generative models, rather than just classifiers, similar to work in Kos et al. (2017). Finally, it may also be possible to train ATNs in a black-box manner, similar to recent work in Tram\u00e8r et al. (2016); Baluja et al.", "startOffset": 8, "endOffset": 654}, {"referenceID": 0, "context": "(2016); Baluja et al. (2015), or using REINFORCE (Williams, 1992) to compute gradients for the ATN using the target network simply as a reward signal.", "startOffset": 8, "endOffset": 29}], "year": 2017, "abstractText": "Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier\u2019s outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "creator": "LaTeX with hyperref package"}}}