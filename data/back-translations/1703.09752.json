{"id": "1703.09752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Collective Anomaly Detection based on Long Short Term Memory Recurrent Neural Network", "abstract": "It has an important role for organizations, governments and our society because of its valuable resources in computer networks. Traditional abuse detection strategies are unable to detect new and unknown intruders. Furthermore, detection of network security anomalies aims to distinguish between illegal or malicious events and the normal behavior of network systems. Detection of anomalies can be considered a classification problem, in which it builds models of normal network behavior that it uses to detect new patterns that differ significantly from the model. Most current research on detection of anomalies is based on learning of normal and anomaly behavior, not taking into account previous, recurring events to detect the new incoming one. In this paper, we propose a collective real-time anomaly based on the neural network detection model and functioning.", "histories": [["v1", "Tue, 28 Mar 2017 19:04:11 GMT  (454kb)", "http://arxiv.org/abs/1703.09752v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["loic bontemps", "van loi cao", "james mcdermott", "nhien-an le-khac"], "accepted": false, "id": "1703.09752"}, "pdf": {"name": "1703.09752.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ie@ucd.ie", "an.lekhac@ucd.ie"], "sections": [{"heading": null, "text": " \u00a0 Keywords: Long Short-Term Memory, Recurrent Neural Network, Collective Anomaly Detection  \u00a0  \u00a0 1 Introduction  \u00a0\nNetwork anomaly detection refers to the problem of detecting illegal or malicious activities or events from normal connections or expected behavior of network systems [4, 5]. It has become one of the most concerned subjects in network security domain due to the fact that organizations or governments are now seeking for\n \u00a0  \u00a0  \u00a0  \u00a0\ngood solutions to protect their valuable resources on computer networks from unauthorized and illegal accesses, network attacks or malware. Over the last three decades, machine learning techniques are known as a common approach for developing network anomaly detection models [3, 4]. Network anomaly detection is usually posed as a type of classification problem: given a dataset representing normal and anomalous examples, the goal is to build a learning classifier which is capable of signaling when a new anomalous data sample is encountered [5].\n \u00a0\nHowever, most of the existing approaches consider an anomaly as a single point: cases when they occur \u201cindividually\u201d and \u201cseparately\u201d [6, 7, 16]. In such approaches, anomaly detection models do not have the ability to represent the information from previous data or events for evaluating a current point. In network security, some kinds of attacks, Denial of Service (DoS), usually occur for a long period of time (several minutes) [10], and are often represented by a set of single points. An attack will be indicated only if a set of single points are considered as attack. In order to detect this kinds of attacks, anomaly detection models should be capable of remembering the information from a number of previous events, and representing the relationship between them and current event. To avoid important mistakes, one must always consider every outcome: in this sense a highly anomalous value may still be linked to a perfectly normal condition, and conversely. In this work, we aim to build an anomaly detection model for this kinds of attacks (known as collective anomaly mentioned in [5]).\n \u00a0\nCollective anomaly is the term to refer to a collection of related anomalous data instances with respect to the whole dataset [5]. The single data points in a collective anomaly may not be considered as anomalies by themselves, but the occurrence of these single points together indicates an anomaly. Long Short Term Memory Recurrent Neural Network (LSTM RNN) is known as one of powerful techniques to represent the relationship between current event and previous events, and handles time series problems [12, 14]. Thus, it is employed to develop anomaly detection model in this paper.\n \u00a0\nIn this paper, we will propose a collective anomaly detection model by using the predictive power of LSTM RNN [8]. Firstly, LSTM RNN is applied as a time series anomaly detection model. The prediction of a current event will depend on both the current event and its previous events. Secondly, the model will be adapted to detect collective anomaly by proposing a circular array. The circular array contains the prediction errors from a certain number of latest time steps. If the prediction errors in the circular array are higher than predeterminer threshold and last for a certain time steps, it will indicate a collective anomaly. More details will be described in Section 4.\n \u00a0\nThe rest of the paper is organized as follows. We briefly review some work related to anomaly detection and LSTM RNN. In Section 3, we give a short introduction to LSTM RNN. This is followed by a section proposing the collective anomaly detection model using LSTM RNN. Experiments, Results and Discussion are presented in Section 5 and Section 6 respectively. The paper concludes with highlights and future directions.\n \u00a0  \u00a0  \u00a0  \u00a0 2 Related Work  \u00a0  \u00a0\nWhen considering a time series dataset, point anomalies are often directly linked to the value of the considered sample. However, attempting real time collective anomaly detection implies always being aware of previous samples, and more precisely their behavior. This means that every time step should include an evaluation of the current value combined with the evolution of precedent information. In this section, we briefly describe work related applying LSTM RNN to time series and collective anomaly detection problems [12, 14, 15].\nOlsson et al. [15] proposed an unsupervised approach for detecting collective anomaly. In order to detect a group of the anomalous examples, the \u201canomalous score\u201d of the group of data points was probabilistically aggregated from the contribution of each individual examples. Obtaining the collective anomalous was processed under unsupervised manner, thus it is suitable for both unsupervised and supervised learning anomaly techniques to scoring individual anomalies. The model was evaluated on an artificial dataset and two industrial datasets, detecting anomalies in moving cranes and anomalies in fuel consumption.\nIn [12], Malhotra et al. applied LSTM network for addressing the problem of time series anomaly detection. The stacked LSTM network trained on only normal data was used to predict over a number of time steps. They assumed that the resulting prediction errors has a Gaussian distribution, which was used to assess the likelihood of anomaly behavior. Their model was demonstrated performing well on four datasets.\nMarchi et al. [14, 13] presented a novel approach by combining non-linear predictive denoising autoencoders (DA) with LSTM for identifying abnormal acoustic signals. Firstly, LSTM Recurrent DA was employed to predict auditory spectral features of the next short-term frame from its previous frames. The network trained on normal acoustic recorders tends to behave well on normal data, and yields small reconstruction errors whereas the reconstruction errors from abnormal acoustic signals are high. The reconstruction errors of the autoencoder was used as \u201canomaly score\u201d, the reconstruction error above a pre-determine threshold indicates an novel acoustic event. The model was trained on a public dataset containing in-home sound events, and evaluated on a dataset including new more anomaly events. The results demonstrated that their model performed significantly better than exsiting methods.\nThe idea is also used in a practical acoustic example [14, 13], where LSTM RNNs are used to predict short-term frames. The core idea of this paper is to combine the previous methods, to adapt Long Short-Term Memory to collective anomaly detection. By labelling testing LSTM RNN outputs at every time step with a standardized error value, we shall propose an algorithm to detect collective anomalies. This will prove very useful in our example : First, we will train normal data on an LSTM RNN in order to estimate the behaviour of a normal day of traffic. Then, we will use a classifier inspired by [15] to rate the level of anomaly of each time sample. We will apply this method to a network security problem (KDD 1999 cup), aiming to raise an alarm in the case of DoS Neptune attacks.\n \u00a0  \u00a0  \u00a0  \u00a0 3 Preliminaries  \u00a0  \u00a0  \u00a0\nIn this section, we briefly describe a specific type of Recurrent Neural Network: Long Short Term Memory. The structure was proposed by Hochreiter et al. [8] in 1997, and has already proven as a powerful technique for addressing the problem of time series prediction.\nThe difference initiated by LSTM regarding other types of RNN resides in its \u201csmart\u201d nodes presented in Fig. 1. Each of these cells contains three gates, input gate, forget gate and output gate, which decide how to react to an input. Depending on the strength of the information each node receives, it will decide to block it or pass it on. The information is also filtered with the set of weights associated with the cells when it is transferred through these cells.\n \u00a0  \u00a0  \u00a0\nThe LSTM node structure enables a phenomenon called backpropagation through time. By calculating for each hidden layer the partial derivatives of the output, weight and input values, the system can move backwards to trace the evolving error between real output and predicted output. Afterwards, the network uses the derivative of this evolution to adapt its weights and decrease prediction error. This learning method is named Gradient Descent.\nAs mentioned before, Long Short-Term Memory has the power to incorporate a behaviour into a network by training it with normal data. The system becomes representative of the variations of the data. In other words, a prediction is made focusing on two features: the value of a sample and its position at specific time. This means that two input samples at different times may have the same value, but their outputs will very probably differ. It is because a LSTM RNN is able to use Back Propagation through time to consider the past of a sample, and therefore understand its context better.\n \u00a0  \u00a0  \u00a0  \u00a0 4 Proposed Approach  \u00a0\nIn this section, we are going to describe a new approach to address the problem of collective anomaly detection. Firstly, we show the LSTM RNNs ability to impregnate itself with the behaviour of a training set, and in this stage it acts like a time series anomaly detection model. We will then adapt it for collective anomaly detection by introducing terms that measure its prediction errors in a period of time steps. Finally, we shall describe how to seek a collective anomaly by combining a LSTM RNN with a circular array method.\n \u00a0  \u00a0 4.1 LSTM RNN as a predictive vector  \u00a0\nThe first step inspires the idea presented in [12]: when trained correctly, LSTM RNNs have the ability to impregnate themselves with the behavior of a training set. Intuitively meaning that when given a certain input samples, they have the ability to remember the context of the value of the samples, and to predict a coherent output in agreement with the context of the sample. In our work, we will use a simple LSTM RNN, in opposition to stacked LSTM in [12]. This does not change the core principle of the method: when given sufficient training, a LSTM RNN adapts its weights, which become representatives of the training data.\n \u00a0  \u00a0 4.2 Definitions  \u00a0\nIn order to adapt a LSTM RNN for time series data to detect collective anomalies, we introduce terms to measure prediction errors at each time step or in a period of time steps. These terms are defined as below.\n \u00a0 \u2013 Relative Error (RE): the Relative Error between two real values x and\ny is given by Eq. 1:\nRE (x, y) = |x \u2212 \u00a0y| \u00a0 x\n \u00a0 (1)\n\u2013 Relative Error Threshold (RET): Relative Error value above a predetermined threshold indicates an anomaly. This threshold, RET , is determined by using labeled normal and attacks from a validation set. \u2013 Minimum Attack Time (MAT): The minimum amount of recent time steps that is used to define a collective attack. \u2013 Danger Coefficient (DC): The density of anomalous points within the last M AT time steps. Let N be the number of anomalous points over the last M AT time steps, DC is defined as in Eq. 2.\n \u00a0  \u00a0  \u00a0  \u00a0\nNB: 0 < DC < 1\n \u00a0 DC = N\nM AT\n \u00a0 (2)\n \u00a0  \u00a0  \u00a0  \u00a0\n\u2013 The Averaged Relative Error (ARE): The Average Relative Error over a M AT is given by Eq. 3:\n \u00a0  \u00a0\nThe values of two terms, Danger Coefficient and Average Relative Error, are the keys factors that will help the model to decide whether a set of inputs within a number of the latest time steps is collective anomaly or not as described in 4.3. These values will be estimated by using validation set.\n \u00a0  \u00a0 4.3 Degree of error evaluation  \u00a0\nAt each time step, the sample predicted by the LSTM RNN is compared with the real future sample. This comparison is computed as a RE value. In this sense, a \u201cRelative Error time series\u201d is built in live. Based on the observation of a validation set, we can initialize value for the RET .\nAt this stage, our system is theoretically capable of detecting point anomalies at each time step. In order to adapt the model from a individually anomaly to collective anomaly, we must consider simultaneously an ensemble of points. To do this, we propose a circular array containing the M AT latest error values to represent the level of anomaly of the latest time steps as shown in Fig. 2. By analyzing the circular array at every time step, we evaluate the possibility of facing a collective anomaly. A collective anomaly will be identified if both Danger Coefficient and Average Relative Error are higher than predefined thresholds, \u03b1 and \u03b2, respectively. It is formulated in Eq. 4 and 5 as below.\n \u00a0 DC > \u03b1 (4)  \u00a0  \u00a0\nARE > \u03b2 (5)\n \u00a0  \u00a0  \u00a0  \u00a0\nwhere \u03b1 and \u03b2 will be estimated by using the validation set. Once training is terminated, the thresholds and parameters of the LSTM network will be adjusted correctly (in order to obtain a satisfactory error decrease). The model should be able to determine with good efficiency whether a set of points represents a collective anomaly.\n \u00a0  \u00a0 5 Experiments  \u00a0 5.1 Datasets  \u00a0\nIn order to demonstrate the efficient performance of the proposed model, we choose a dataset related to network security domain, the KDD 1999 dataset [2, 9], for our experiments. The dataset in tcpdump format was collected from a simulated military-like environment over a period of 5 weeks. There are four main groups of attacks in the dataset, but we restrict our experiments on a specific attack, Neptune, in Denial-of-Service (DoS). The dataset is also converted into time series version before feeding into the model. More details about how to obtain a time series version from the original dataset, and how to choose training, validation and testing sets are presented in the following paragraphs.\nThe first crucial step is to build a conveniently usable time series dataset out of the tcpdump data, and selecting the features we wish to use. We use terminal commands and a python program to convert the original tcpdump records in the KDD 1999 dataset into a time dependant function. This method is a development of the proposed transformation in [11] that acts directly on the tcpdump to obtain real time statistics of the data. Our scheme follows this step by step transition as described below:\ntcpdump \u21d2 \u00a0pcap \u21d2 \u00a0csv  \u00a0\nEach day of records can be time-filtered and input into a new .pcap file. This also has the advantage of giving a first approach on visualizing the data by using Wireshark functionalities (IO graphs and filters). Once this is done, the tshark command is adapted to select and transfer the relevant information from the records into a .csv file. We may note that doing this is a first step towards faster computation and better system efficiency, since all irrelevant pcap columns can be ignored. There are two major steps for the conversion processing.\n \u00a0 1. Store the information of a .tcpdump file into a newly generated .pcap file. From the terminal, we use the editcap command:  \u00a0\neditcap -A \u20191999-03-11 08:00:00\u2019 -B\u20191999-03-11 18:00:00\u2019 Thursday2outside.tcpdump Thursday2.pcap\n2. Convert from .pcap file into .csv file by tshark command. From the terminal again, type the command below:\n \u00a0 tshark -r Thursday2.pcap -T fields -e frame.number -e frame.len -e frame.time -e ip.proto -E header=y -E separator=, -E quote=d -E occurrence=f -i netstat -f tcp[13]==12 > Thursday2.csv\n \u00a0  \u00a0  \u00a0  \u00a0\ntshark is a simple but powerful command, enabling the selection of columns of interest in a .pcap file, and their output in a newly generated .csv. Once the data is in the .csv format, python code can be implemented from the XX library to store it and use with our classifier.\nProcessing the tcpdump with this method enables quick and easy manipulation of the data. For example, Neptune and Smurf are both DoS attacks caracterized by a high flow of specific packets in networks (eg. SYN ACK and ICMP echo replies). By using this simple fact, the needed records can be filtered and counted at every time step. If we aim to detect Neptune attack, the thark command can be implemented with the \u2212i netstat \u2212f tcp[13] == 2 filter, so only SYN ACK packets from servers are counted. We observe in the case of KDD 1999 that a Neptune attack can be sought by looking for an anomalously high number of these packets.\nThe KDD1999 time series is composed of a two-weeks training set n1 (weeks 1 & 3, normal data), one week of validation set v1 (week 2, both labeled normal and anomaly data), and a two-weeks of testing set t1 (weeks 4 & 5). The protocol will be the following: training the network with n1 , using v1 to determine our error threshold(s), and evaluating the proposed model on t1 .\n \u00a0  \u00a0  \u00a0  \u00a0  \u00a0 5.2 Experimental Settings  \u00a0\nIn this work, we conduct two experiments, one preliminary experiment and one main experiment. The preliminary experiment is aim to estimate the parameters\n \u00a0  \u00a0  \u00a0  \u00a0\nfor the models and set its thresholds by using the validation set whereas the main experiment is to evaluate the proposed model.\nPreliminary Experiment: This experiment is aim to select the best parameters of our LSTM RNN model with respect to minimize its prediction error, and determine the thresholds, \u03b1 and \u03b2. Firstly, we determine how many the previous time steps should be used for predicting the current event. The hyper-parameters of LSTM RNN, hidden size and learning rate, is then estimated. Finally, the two thresholds, \u03b1 and \u03b2, will be estimated with respect to a good classification performance of the model on the validation set.\nIn order to optimize the proposed model for the main experiment, we proceed to a preliminary test to measure the influence of the number of inputs on the prediction error of LSTM. We first focus on how many inputs will influence the prediction of an LSTM [12]. More specifically, they show that a sample xt input at time t will be predicted with reasonable accuracy yt+1 . We emit the hypothesis that inserting more values in our system may help decrease prediction errors, but it leads to more time consuming. Thus, we investigate the relationship between the prediction value yt+1 to three sets of the previous input examples (xt ), (xt , xt\u22121 ), (xt , xt\u22121 , xt\u22122 ). They are formulated in equations 6, 7 and 8 below:\nyt+1 = f (xt ) (6)  \u00a0  \u00a0 yt+1 = f (xt , xt\u22121 ) (7)  \u00a0  \u00a0\nyt+1 = f (xt , xt\u22121 , xt\u22122 ) (8)\nwhere xt , xt\u22121 and xt\u22122 are the input samples at times t, t \u2212 \u00a01 and t \u2212 \u00a02 respectively, and yt+1 is the predicted value for the input xt .\nThe number of hidden nodes and the learning rate are the final two parameters that can highly influence the performance of a LSTM RNN. On the one hand, the strength of a LSTM RNN resides in its hidden layer. Each synapse of a network is weighted differently, and can be considered as a unique interpretation of the input data. Each node of the hidden layer is storage space for these interpretations. Theoretically, the higher number of hidden nodes, the more information the network can contain. This also means more computation, and may lead to over-fitting.\nUsing the LSTM RNN error evolution curve empirically, we concluded that the optimum number of nodes in our hidden layer to obtain good memorization is approximatively 23. The learning rate is other factor directly linked to the speed at which a LSTM RNN can improve its predictions. For a time step t during training, the synapse weights of our neural network are updated. The learning rate defines how much we wish a weight to be modified at each instant. In our experiment, we choose learning rate equal to 0.01 that gives us a convenient error curve.\n \u00a0  \u00a0  \u00a0  \u00a0\nFinally, the classifier that is trained on ten days of normal data is used to determine \u03b1 and \u03b2. We observe the reaction of the system on labeled Neptune attacks from the validation set, and set the thresholds. The values of these thresholds is shown in Section 6.\nMain Experiment: Our task is to use the potential speed and accuracy of LSTM RNN to detect a disproportionate durable change in a time series. Once the preliminary experiment is complete, we choose the most performant LSTM RNN, and train it with the normal training set n1 . The classifier is then evaluated on testing set t1 containing both normal and attack data to investigate how efficiently our proposed classifier performs.\n \u00a0  \u00a0 6 Results and Discussion  \u00a0\nThis section presents our experimental results. First, the preliminary experiment evaluates two factors: computation cost and LSTM prediction error when using one input, two inputs and three inputs respectively. Then, the general performance in terms of classification accuracy is measured.\n \u00a0  \u00a0\nThe Table 1 illustrates that the model with three inputs had less computational time than those with one or two inputs. Moreover, the Fig. 3 shows that\n \u00a0  \u00a0  \u00a0  \u00a0\nthe model with three inputs achieve a lower training error in comparison to two others. Thus, we use the model with three inputs for our main experiment.\n \u00a0  \u00a0\nThe results from the main experiment are shown in Table 2. The experiment is done with M AT = 12, and \u03b1 = 0.66, and we also report the results on four values of \u03b2, \u03b2 = 0.69, 0.66, 0.62 and 0.52. We observe that it is possible to obtain 100% collective anomaly detection rate, but this infers triggering a high amount of false alarms. Conversely, it is possible to avoid false alarms, but less correct alarms will be detected. Ultimately, detecting more real attacks results in triggering more false alarms as shown in Table 2.\n \u00a0  \u00a0\nIn this paper, we have proposed a model for collective anomaly detection based on Long Short-Term Memory Recurrent Neural Network. We have motivated this method through investigating LSTM RNN in the problem of time series, and adapted it to detect collective anomaly by proposing the measurements in 4.2. We investigated the hyper-parameters, the suitable number of inputs and some thresholds by using the validation set.\nThe proposed model is evaluated by using the time series version of the KDD 1999 dataset. The results suggest that proposed model is efficiently capable of detecting collective anomalies the dataset. However, they must be used with caution. The training data fed into a network must be organized in a coherent manner to guarantee the stability of the system. In future work, we will focus on how to improve the classification accuracy of the model. We also observed that implementing variations in a LSTM RNNs number of inputs might trigger different output reactions.\n \u00a0  \u00a0  \u00a0  \u00a0 8 Acknowledgements  \u00a0\nThe experiments in this paper is performed by Loic Bontemps when he is doing the final year project in School of Computer Science, University College Dublin.\n \u00a0 References  \u00a0 1. LSTM networks for sentiment analysis. In: LSTM networks for sentiment analy-\nsis deeplearning 0.1 documentation, http://deeplearning.net/tutorial/lstm. html#lstm, [Accessed 25 Jun 2016] 2. DARPA intrusion detection evaluation. (n.d.). (Retrieved June 30, 2016), http: //www.ll.mit.edu/ideval/data/1999data.html 3. Ahmed, M., Mahmood, A.N., Hu, J.: A survey of network anomaly detection techniques. Journal of Network and Computer Applications 60, 19\u201331 (2016) 4. Bhattacharyya, D.K., Kalita, J.K.: Network anomaly detection: A machine learning perspective. CRC Press (2013) 5. Chandola, V., Banerjee, A., Kumar, V.: Anomaly detection: A survey. ACM computing surveys (CSUR) 41(3), 15 (2009) 6. Chmielewski, A., Wierzchon, S.T.: V-detector algorithm with tree-based structures. In: Proc. of the International Multiconference on Computer Science and Information Technology, Wis/la (Poland). pp. 9\u201314. Citeseer (2006) 7. Hawkins, S., He, H., Williams, G., Baxter, R.: Outlier detection using replicator neural networks. In: International Conference on Data Warehousing and Knowledge Discovery. pp. 170\u2013180. Springer (2002) 8. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735\u20131780 (1997) 9. KDD Cup Dataset (1999), available at the following website http://kdd.ics.uci. edu/databases/kddcup99/kddcup99.html\n10. Lee, W., Stolfo, S.J.: A framework for constructing features and models for intrusion detection systems. ACM transactions on Information and system security (TiSSEC) 3(4), 227\u2013261 (2000) 11. Lu, W., Ghorbani, A.A.: Network anomaly detection based on wavelet analysis. EURASIP Journal on Advances in Signal Processing 2009, 4 (2009) 12. Malhotra, P., Vig, L., Shroff, G., Agarwal, P.: Long short term memory networks for anomaly detection in time series. In: Proceedings. p. 89. Presses universitaires de Louvain (2015) 13. Marchi, E., Vesperini, F., Eyben, F., Squartini, S., Schuller, B.: A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional lstm neural networks. In: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1996\u20132000. IEEE (2015) 14. Marchi, E., Vesperini, F., Weninger, F., Eyben, F., Squartini, S., Schuller, B.: Non-linear prediction with lstm recurrent neural networks for acoustic novelty detection. In: 2015 International Joint Conference on Neural Networks (IJCNN). pp. 1\u20137. IEEE (2015) 15. Olsson, T., Holst, A.: A probabilistic approach to aggregating anomalies for unsupervised anomaly detection with industrial applications. In: FLAIRS Conference. pp. 434\u2013439 (2015) 16. Salama, M.A., Eid, H.F., Ramadan, R.A., Darwish, A., Hassanien, A.E.: Hybrid intelligent intrusion detection scheme. In: Soft computing in industrial applications, pp. 293\u2013303. Springer (2011)"}], "references": [{"title": "A survey of network anomaly detection techniques", "author": ["M. Ahmed", "A.N. Mahmood", "J. Hu"], "venue": "Journal of Network and Computer Applications 60, 19\u201331", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Network anomaly detection: A machine learning perspective", "author": ["D.K. Bhattacharyya", "J.K. Kalita"], "venue": "CRC Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM computing surveys (CSUR) 41(3), 15", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "V-detector algorithm with tree-based structures", "author": ["A. Chmielewski", "S.T. Wierzchon"], "venue": "Proc. of the International Multiconference on Computer Science and Information Technology, Wis/la (Poland). pp. 9\u201314. Citeseer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Outlier detection using replicator neural networks", "author": ["S. Hawkins", "H. He", "G. Williams", "R. Baxter"], "venue": "International Conference on Data Warehousing and Knowledge Discovery. pp. 170\u2013180. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "A framework for constructing features and models for intrusion detection systems", "author": ["W. Lee", "S.J. Stolfo"], "venue": "ACM transactions on Information and system security (TiSSEC) 3(4), 227\u2013261", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Network anomaly detection based on wavelet analysis", "author": ["W. Lu", "A.A. Ghorbani"], "venue": "EURASIP Journal on Advances in Signal Processing 2009, 4", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "Proceedings. p. 89. Presses universitaires de Louvain", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel approach for automatic acoustic novelty detection using a denoising autoencoder with bidirectional lstm neural networks", "author": ["E. Marchi", "F. Vesperini", "F. Eyben", "S. Squartini", "B. Schuller"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1996\u20132000. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-linear prediction with lstm recurrent neural networks for acoustic novelty detection", "author": ["E. Marchi", "F. Vesperini", "F. Weninger", "F. Eyben", "S. Squartini", "B. Schuller"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN). pp. 1\u20137. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A probabilistic approach to aggregating anomalies for unsupervised anomaly detection with industrial applications", "author": ["T. Olsson", "A. Holst"], "venue": "FLAIRS Conference. pp. 434\u2013439", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid intelligent intrusion detection scheme", "author": ["M.A. Salama", "H.F. Eid", "R.A. Ramadan", "A. Darwish", "A.E. Hassanien"], "venue": "Soft computing in industrial applications, pp. 293\u2013303. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Keywords: Long Short-Term Memory, Recurrent Neural Network, Collective Anomaly Detection 1 Introduction Network anomaly detection refers to the problem of detecting illegal or malicious activities or events from normal connections or expected behavior of network systems [4, 5].", "startOffset": 271, "endOffset": 277}, {"referenceID": 2, "context": "Keywords: Long Short-Term Memory, Recurrent Neural Network, Collective Anomaly Detection 1 Introduction Network anomaly detection refers to the problem of detecting illegal or malicious activities or events from normal connections or expected behavior of network systems [4, 5].", "startOffset": 271, "endOffset": 277}, {"referenceID": 0, "context": "Over the last three decades, machine learning techniques are known as a common approach for developing network anomaly detection models [3, 4].", "startOffset": 136, "endOffset": 142}, {"referenceID": 1, "context": "Over the last three decades, machine learning techniques are known as a common approach for developing network anomaly detection models [3, 4].", "startOffset": 136, "endOffset": 142}, {"referenceID": 2, "context": "Network anomaly detection is usually posed as a type of classification problem: given a dataset representing normal and anomalous examples, the goal is to build a learning classifier which is capable of signaling when a new anomalous data sample is encountered [5].", "startOffset": 261, "endOffset": 264}, {"referenceID": 3, "context": "However, most of the existing approaches consider an anomaly as a single point: cases when they occur \u201cindividually\u201d and \u201cseparately\u201d [6, 7, 16].", "startOffset": 134, "endOffset": 144}, {"referenceID": 4, "context": "However, most of the existing approaches consider an anomaly as a single point: cases when they occur \u201cindividually\u201d and \u201cseparately\u201d [6, 7, 16].", "startOffset": 134, "endOffset": 144}, {"referenceID": 12, "context": "However, most of the existing approaches consider an anomaly as a single point: cases when they occur \u201cindividually\u201d and \u201cseparately\u201d [6, 7, 16].", "startOffset": 134, "endOffset": 144}, {"referenceID": 6, "context": "In network security, some kinds of attacks, Denial of Service (DoS), usually occur for a long period of time (several minutes) [10], and are often represented by a set of single points.", "startOffset": 127, "endOffset": 131}, {"referenceID": 2, "context": "In this work, we aim to build an anomaly detection model for this kinds of attacks (known as collective anomaly mentioned in [5]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Collective anomaly is the term to refer to a collection of related anomalous data instances with respect to the whole dataset [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Long Short Term Memory Recurrent Neural Network (LSTM RNN) is known as one of powerful techniques to represent the relationship between current event and previous events, and handles time series problems [12, 14].", "startOffset": 204, "endOffset": 212}, {"referenceID": 10, "context": "Long Short Term Memory Recurrent Neural Network (LSTM RNN) is known as one of powerful techniques to represent the relationship between current event and previous events, and handles time series problems [12, 14].", "startOffset": 204, "endOffset": 212}, {"referenceID": 5, "context": "In this paper, we will propose a collective anomaly detection model by using the predictive power of LSTM RNN [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "In this section, we briefly describe work related applying LSTM RNN to time series and collective anomaly detection problems [12, 14, 15].", "startOffset": 125, "endOffset": 137}, {"referenceID": 10, "context": "In this section, we briefly describe work related applying LSTM RNN to time series and collective anomaly detection problems [12, 14, 15].", "startOffset": 125, "endOffset": 137}, {"referenceID": 11, "context": "In this section, we briefly describe work related applying LSTM RNN to time series and collective anomaly detection problems [12, 14, 15].", "startOffset": 125, "endOffset": 137}, {"referenceID": 11, "context": "[15] proposed an unsupervised approach for detecting collective anomaly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In [12], Malhotra et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[14, 13] presented a novel approach by combining non-linear predictive denoising autoencoders (DA) with LSTM for identifying abnormal acoustic signals.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[14, 13] presented a novel approach by combining non-linear predictive denoising autoencoders (DA) with LSTM for identifying abnormal acoustic signals.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "The idea is also used in a practical acoustic example [14, 13], where LSTM RNNs are used to predict short-term frames.", "startOffset": 54, "endOffset": 62}, {"referenceID": 9, "context": "The idea is also used in a practical acoustic example [14, 13], where LSTM RNNs are used to predict short-term frames.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "Then, we will use a classifier inspired by [15] to rate the level of anomaly of each time sample.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "[8] in 1997, and has already proven as a powerful technique for addressing the problem of time series prediction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "1 LSTM RNN as a predictive vector The first step inspires the idea presented in [12]: when trained correctly, LSTM RNNs have the ability to impregnate themselves with the behavior of a training set.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "In our work, we will use a simple LSTM RNN, in opposition to stacked LSTM in [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "This method is a development of the proposed transformation in [11] that acts directly on the tcpdump to obtain real time statistics of the data.", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "-E occurrence=f -i netstat -f tcp[13]==12 > Thursday2.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "If we aim to detect Neptune attack, the thark command can be implemented with the \u00ad\u2212i netstat \u00ad\u2212f tcp[13] == 2 filter, so only SYN ACK packets from servers are counted.", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "We first focus on how many inputs will influence the prediction of an LSTM [12].", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "Intrusion detection for computer network systems becomes one of the most critical tasks for network administrators today. It has an important role for organizations, governments and our society due to its valuable resources on computer networks. Traditional misuse detection strategies are unable to detect new and unknown intrusion. Besides, anomaly detection in network security is aim to distinguish between illegal or malicious events and normal behavior of network systems. Anomaly detection can be considered as a classification problem where it builds models of normal network behavior, which it uses to detect new patterns that significantly deviate from the model. Most of the current research on anomaly detection is based on the learning of normally and anomaly behaviors. They do not take into account the previous, recent events to detect the new incoming one. In this paper, we propose a real time collective anomaly detection model based on neural network learning and feature operating. Normally a Long Short-Term Memory Recurrent Neural Network (LSTM RNN) is trained only on normal data and it is capable of predicting several time steps ahead of an input. In our approach, a LSTM RNN is trained with normal time series data before performing a live prediction for each time step. Instead of considering each time step separately, the observation of prediction errors from a certain number of time steps is now proposed as a new idea for detecting collective anomalies. The prediction errors from a number of the latest time steps above a threshold will indicate a collective anomaly. The model is built on a time series version of the KDD 1999 dataset. The experiments demonstrate that it is possible to offer reliable and efficient for collective anomaly detection.", "creator": "Word"}}}