{"id": "1703.09817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2017", "title": "Learning Similarity Functions for Pronunciation Variations", "abstract": "A significant source of error in Automatic Speech Recognition (ASR) systems is based on pronunciation variations that occur in spontaneous and conversational speech. Typically, ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this essay, we focus on learning a similarity function between two pronunciations. Pronunciation can be the canonical and superficial pronunciation of the same word, or it can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning to match words and their possible pronunciations) and the definition of word neighbors. It can also be used to dynamically increase the size of the pronunciation lexicon or predict ASR errors. We propose two methods based on recurring neural networks to learn the similarity function.", "histories": [["v1", "Tue, 28 Mar 2017 21:47:16 GMT  (208kb,D)", "https://arxiv.org/abs/1703.09817v1", null], ["v2", "Sun, 28 May 2017 19:05:38 GMT  (209kb,D)", "http://arxiv.org/abs/1703.09817v2", null], ["v3", "Sun, 18 Jun 2017 11:52:54 GMT  (209kb,D)", "http://arxiv.org/abs/1703.09817v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["einat naaman", "yossi adi", "joseph keshet"], "accepted": false, "id": "1703.09817"}, "pdf": {"name": "1703.09817.pdf", "metadata": {"source": "CRF", "title": "Learning Similarity Functions for Pronunciation Variations", "authors": ["Einat Naaman", "Yossi Adi", "Joseph Keshet"], "emails": ["jkeshet@cs.biu.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Spontaneous and conversational speech are significantly different both acoustically and linguistically from read speech. One of the key differences is the vast pronunciation variations in spontaneous and conversational speech, as the speaking rate is accelerated and consequently the pronunciation becomes reduced or coarticulated. Other factors, such as the neighboring words and the speaker\u2019s style, also influence the way words are produced.\nWe distinguish between two types of pronunciations of a word. The typical pronunciation found in a dictionary is called canonical pronunciation, whereas the actual way in which speakers produce the word is called surface pronunciation. Spontaneous speech often includes pronunciations that differ from the one found in the dictionary. For example, pronunciations of the word \u201cprobably\u201d in the Switchboard conversational speech corpus include [p r aa b iy], [p r aa l iy], [p r ay], etc. [1]. Fewer than half of the word productions are pronounced canonically in the phonetically transcribed portion of Switchboard [2].\nIn this work we propose to learn a similarity function between two pronunciation variations. The function should score the similarity between any type of pronunciations. The input can be, for example, canonical and surface pronunciations or it can be two surface pronunciations. We expect such a function to output a high number if the inputs are canonical and surface pronunciations of the same word or if the input consists of two surface pronunciations of the same word.\nSuch a similarity measure can be utilized in many tasks, including lexical access, word neighborhood and pronunciation\nscoring. In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5]. The problem of lexical access can be handled using a pronunciation similarity function as follows: the input surface pronunciation is compared to all of the (canonical) pronunciations in the dictionary using the similarity function, and the one with the maximal similarity is predicted as the articulated word.\nIn the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6]. It has been suggested as an explanatory variable for quantifying errors in ASR, but is also used in linguistic studies. In most works in the psycholinguistics literature, word neighborhood is defined to be the set of words which differ by a single phone from the given word. Here we propose a different definition which is closer to the definition suggested in [6]. The word neighborhood can be defined as all the words in the dictionary in proximity to the given word, where proximity is measured using the similarity function between pronunciations.\nASR systems are based on a finite dictionary which provides each word with one or more pronunciations. The variance of pronunciations in spontaneous and conversational speech leads to a high rate of errors in ASR systems [7, 8]. The standard approach to this problem is to expand the dictionary, either by adding alternate pronunciations with probabilities, or with phonetic substitution, insertion and deletion rules derived from linguistic knowledge and learned from data. A similarity function can be used to add pronunciation variations to each word either statically or dynamically during the dictionary lookup.\nMapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5]. Note that this problem is different from the grapheme-to-phoneme problem, in which pronunciations are predicted from a word\u2019s spelling, whereas in lexical access we assume a dictionary of canonical pronunciations like the one used in speech recognition. Learning word neighborhood automatically was proposed by [6].\nOur work is restricted to proposing algorithms for learning the pronunciation similarity score. Specifically we propose two different deep network architectures to tackle this problem. The first is based on binary classification of two recurrent neural networks (RNNs), while the second is based on triplet networks designed to rank the pronunciations.\nThe paper is organized as follows. Section 2 introduces our notation and the problem definition. In Section 3 we propose two deep network architectures to learn similarity between two pronunciations. In Section 4 we present a set of experiments on the Switchboard conversational speech corpus, and in Section 5 we analyze the experimental results. We conclude the paper in Section 6.\nar X\niv :1\n70 3.\n09 81\n7v 3\n[ cs\n.C L\n] 1\n8 Ju\nn 20\n17"}, {"heading": "2. Problem settings", "text": "We denote a word pronunciation by a sequence of phones, p = (p1, . . . , pN ) where pn \u2208 P for all 1 \u2264 n \u2264 N and P is the set of all sub-word units (all phones). Naturally, N is not fixed since the number of phonemes varies across different words. We denote the set of all finite-length phone streams as P\u2217.\nGiven two pronunciation sequences, our goal is to find a similarity function between these two sequences. Formally, our goal is to learn a function f : P\u2217\u00d7P\u2217 \u2192 R which gets as input two pronunciation sequences and returns the similarity between the two. That is if two pronunciations p1,p2 \u2208 P\u2217 are similar, then the function f(p1,p2) will be high. Otherwise it will be low.\nThis type of function can be utilized in various tasks. For example, in the lexical access task [5] the goal is to predict which word w from a finite dictionary V is associated with a given surface pronunciation ps. Assume that the dictionary V is a list of pairs, where each pair is composed of a word w and its canonical pronunciation pc, namely (w,pc) \u2208 V . Define sortk as a function that gets a set of unordered scores and returns a vector of the top k maximal ordered scores. Similarly define arg sortk to be the function the returns the indices of the ordered scores. Then the best k-words can be found to be those whose canonical pronunciations get the highest similarity to the input surface pronunciation:\nw = arg sortk (w,pc)\u2208V f(ps,pc), (1)\nwhere w is a list of k-best words in the dictionary. Either the word with the highest unigram probability can be predicted or it can be combined to generated a prediction based on the n-gram probability.\nIn the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6]. This can be achieved by comparing the similarity of a canonical pronunciation of a given word to the set of all canonical pronunciations in the dictionary. Formally we propose to define word neighborhood N (w) for a given word w as the set of all words u in the dictionary V for which the similarity function is less than some threshold \u03b8:\nN (w) = {u | \u2200(u,pu) \u2208 V, f(pw,pu) < \u03b8}, (2)\nwhere pw,pu \u2208 P\u2217 are the canonical pronunciations of the word w and u respectively."}, {"heading": "3. Network architecture", "text": "Is this section we suggest two network architectures to learn the similarity function. Both architectures are based on Recurrent Neural Networks (RNNs) [10]. The first one is based on Siamese RNNs which were designed as a binary classifier and were trained to minimize the negative log likelihood loss function. The second architecture is based on three identical RNNs, which are combined together to create a ranking architecture and optimized with a ranking loss function. We also tried several sequence-to-sequence architectures [11, 12], but since all of them led to poor results in terms of Word Error Rate (WER), we will not discuss them in the paper."}, {"heading": "3.1. Binary loss function", "text": "The first architecture is a network that is designed to learn a mapping between two pronunciations using a binary classifier,\nwhich is trained to predict whether two pronunciations are of the same word. Each example in the training set ofm examples, S = {(psi ,pci , yi)}mi=1, is composed of a surface pronunciation ps \u2208 P\u2217, a canonical pronunciation pc \u2208 P\u2217 and a binary label y \u2208 {\u22121,+1}, which indicates if both pronunciations are of the same word or not.\nWe would like to train a neural network to learn this binary mapping. In order to do so, we encode both pronunciations using two RNNs with a shared set of parameters. The input to each of the RNNs is a sequence of phones, which represents either surface or canonical pronunciations, and the output is a real vector. Denote by vc and vs the output of the RNNs for the canonical and the surface representations, respectively. Then, both vectors are concatenated [vc,vs] and are fed to three fully connected layers with shared parameters over time. The output is a series of vectors, one for each time step. These are all concatenated1 and are fed to a fully-connected layer followed by a softmax layer. The similarity function f is the output of the softmax layer, and it can be interpreted as a probability function. The whole network is trained so as to minimize the negative log likelihood loss function. The network architecture is depicted schematically in Figure 1."}, {"heading": "3.2. Ranking loss function", "text": "A different approach to learning the similarity function is to score the similarity between a given pronunciation and other pronunciations according to their ranking. In order to do so we use a slightly different training set than the one used to train the binary network. Each example in the new training set is composed of a triplet: a surface pronunciation ps \u2208 P\u2217, a positive canonical pronunciation p+ \u2208 P\u2217, and a negative canonical pronunciation p\u2212 \u2208 P\u2217. The positive canonical pronunciation is the canonical pronunciation associated with the surface pronunciation ps, and the negative canonical pronunciation is a canonical pronunciation of a different word. Overall the training set of m examples is denoted S = {(psi ,p+i ,p \u2212 i )} m i=1. As in the previous model, we represent each of the three pronunciations using an RNN. The output of the RNN is fed into two fully-connected layers and is considered to be the pronunciation embedding. Pronunciation embedding is a function g that maps a pronunciation p \u2208 P\u2217 to a fixed size vector, u \u2208 Rn, where u = g(p). We measure the closeness between the two embeddings, u \u2208 Rn and v \u2208 Rn, using the cosine distance\n1We apply sequence padding when needed.\nscore [13]:\ndcos(u,v) = 1\n2 ( 1\u2212 u \u00b7 v\u2016u\u2016\u2016v\u2016 ) .\nThe cosine distance score between two embeddings is close to 0 if the vectors u and v are close, and close to 1 if they are far apart. Formally, the similarity function between two pronunciations p1 and p2 is defined as 1 minus the cosine distance\nf(p1,p2) = 1\u2212 dcos(g(p1), g(p2)). (3)\nDuring training we minimize the hinge loss over the cosine distance so that the score of the related canonical-surface pronunciations is higher than the score of the unrelated canonical-surface pronunciation by a margin of at least \u03b3, where \u03b3 \u2208 R+ is a positive scalar. Formally, the loss function is defined as [14, 13]:\n`(ps,p+,p\u2212) = max{0, \u03b3 \u2212 f(ps,p+) + f(ps,p\u2212)}, (4)\nwhere f is the similarity function defined in Eq. (3), and \u03b3 is the margin parameter. The network architecture is depicted schematically in Figure 2.\nThis architecture has three advantages over the binary architecture: (i) the ranking optimization criterion is closer to the notion of similarity function: related pronunciations get higher score than unrelated ones; (ii) as a by-product we introduce the pronunciation embedding, which maps a sequence of phones to a fixed size vector; and (iii) we use many negative surface pronunciations for each positive surface pronunciation and increase our training data. We found that it has a great impact on the model\u2019s performance, and we analyze it in Section 5."}, {"heading": "4. Experiments", "text": "We evaluated the proposed architectures on the lexical access task, where we would like to predict the word in the dictionary that is associated with a given surface pronunciation. All experiments are conducted on a subset of Switchboard conversational speech corpus that has been labeled at a fine phonetic level [1]; these phonetic transcriptions are the input to our similarity models. The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5]. The dictionary contains 5,117 words, consisting of the most frequent words in Switchboard. The base-form uses a similar, slightly smaller phone set (lacking, e.g., nasalization). We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.\nResults are presented in terms of the word error rate when the top k predictions are considered. This is denoted by\nWER@k. Table 1 summarizes the results for all our architectures. For each architecture we tested three types of RNNs: LSTM with one layer, LSTM with two layers (2-LSTM), and bidirectional LSTM with two layers (BI-2-LSTM). We optimize all our models using Adagrad [15] with learning rate value of 0.01. We use ReLU [16] as an activation function after each fully connected layer. For the ranking models we use a margin value of \u03b3 = 0.3. All hyper-parameters were tuned on a validation set.\nFor comparison we added to Table 1 the word error rate of other algorithms for lexical access: a dictionary lookup with and without Levenshtein distance [9], a dynamic Bayesian network (Jyothi et al., 2011 [4]), and discriminative structured prediction model (Hao et al., 2012 [5]). It can be seen from the table that both of our models outperform the dictionary lookup approaches and the model which is based on dynamic Bayesian networks [4]. However the discriminative structured prediction model [5] performs much better than any of our models. The discriminative model was trained specifically for the lexical access task with a unique set of feature maps, but it cannot be used as a similarity score between two pronunciations.\nThe performance of the binary loss model and the ranking model are almost the same, with the binary loss performing slightly better than the rank loss. The reason is likely that it was trained to maximize the probability that two pronunciations are related and not to match a similarity score."}, {"heading": "5. Analysis", "text": "In this section we analyze the performance of the ranking model. We investigate the effect of the number of negative examples, the effect of the embedding size and present some of the model\u2019s outputs and the way it errs."}, {"heading": "5.1. The effect of the number of negative samples", "text": "Recall that the ranking loss model was trained on triplet made of a canonical pronunciation of a word, a surface pronunciation of the word (positive sample), and a surface pronunciation which is not associated with the word (negative sample). In our experiments the negative surface pronunciation was a surface pronunciation of a random word.\nDeep neural networks require a lot of training data in order to converge to a good local minimum. We expanded the training set by using many different negative samples for each positive samples. In order to examine if this approach leads to a better performance, we trained the network several times with\na different number of negative samples per positive sample and evaluated the performance on the test set.\nFigure 3 shows WER@1 and WER@2 of the lexical access task, as a function of the number of negative samples per positive one. Notice that when adding more examples the error rate keeps decreasing until the limit of 50 negative samples, from this point the error rate stays roughly the same."}, {"heading": "5.2. The effect of the embedding size", "text": "Next, we investigated the effect of the pronunciation embedding size n, i.e., the size of the output of the last layer after the RNN. We tried different embeddings sizes, and evaluated their performance on the validation and test sets. Table 2 summarizes the results. From the table we see that embedding of size 40 is too small, but the performance of embeddings of size 80 and above are all good. We found the embedding of size 120 to yield the best performance."}, {"heading": "5.3. Visualization", "text": "Lastly, we performed a few visualizations in order to get a sense of what the model learned. In Figure 4 we visualized a subset from the embedding space of the canonical pronunciations\nin the dictionary, using t-SNE [17] for dimensionality reduction. The words which have a similar pronunciation appear to be close in the embeddings space.\nIn Table 3 we present the 4 most similar words according to the learned similarity function for the words sense, write, die, male, and their. Again, we can see that the most similar words in the embedding space are the words which contain a similar phone sequence.\nIn Table 4 we illustrate the predictions of the model for the lexical access task for hard cases of surface pronunciations. The table shows the first three predictions of the model, ordered (left to right) from most similar to least similar. The table is separated into two panels: the upper panel shows the correct predictions made by the model and the lower panel shows incorrect predictions. It can be seen that both the correct and the incorrect prediction are hard to classify, even for a human. In the lower panel, it seems that there might be an error in the transcription (e.g., [s, tcl, t, ao, r] is more similar to the predicted word store rather than to labeled word start), or that we have reached the limit of the possible discrimination in this task (e.g., [wn, ahn, n] can equally be predicted as either want or won."}, {"heading": "6. Conclusion", "text": "We presented two very different architectures to learn similarity between two phone streams. The first architecture learns the alignment between phone streams that represent close pronunciations. The second architecture is designed to learn a mapping of a phone stream to a vector space, such that close pronunciations will have close representation in the output vector space.\nFuture work will explore similarity between other sub-word units. Specifically we would like to propose a similarity function between articulatory features, and analyze it in the light of articulatory phonology [18]."}, {"heading": "7. Acknowledgment", "text": "We would like to thank Karen Livescu, Preethi Jyothi and the anonymous reviewers for their helpful comments."}, {"heading": "8. References", "text": "[1] S. Greenberg, J. Hollenback, and D. Ellis, \u201cInsights into spoken\nlanguage gleaned from phonetic transcription of the Switchboard corpus,\u201d in Proceeding of the 4th International Conference on Spoken Language Processing (ICSLP), 1996.\n[2] K. Livescu, \u201cFeature-based pronunciation modeling for automatic speech recognition,\u201d Ph.D. dissertation, Massachusetts Institute of Technology, 2005.\n[3] L. Fissore, P. Laface, G. Micca, and R. Pieraccini, \u201cLexical access to large vocabularies for speech recognition,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 8, pp. 1197\u20131213, 1989.\n[4] P. Jyothi, K. Livescu, and E. Fosler-Lussier, \u201cLexical access experiments with context-dependent articulatory feature-based models,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2011.\n[5] H. Tang, J. Keshet, and K. Livescu, \u201cDiscriminative pronunciation modeling: A large-margin, feature-rich approach,\u201d in The 50th Annual Meeting of the Association of Computational Linguistics (ACL), 2012.\n[6] P. Jyothi and K. Livescu, \u201cRevisiting word neighborhoods for speech recognition,\u201d in ACL MORPHFSM Workshop, 2014.\n[7] D. Jurafsky, W. Ward, Z. Jianping, K. Herold, Y. Xiuyang, and Z. Sen, \u201cWhat kind of pronunciation variation is hard for triphones to model?\u201d in Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2001.\n[8] M. Sarac\u0327lar and S. Khudanpur, \u201cPronunciation change in conversational speech and its implications for automatic speech recognition,\u201d Computer Speech and Language, vol. 18, no. 4, 2004.\n[9] K. Livescu and J. R. Glass, \u201cFeature-based pronunciation modeling with trainable asynchrony probabilities.\u201d in Proceeding of the 8th International Conference on Spoken Language Processing (ICSLP), 2004.\n[10] J. L. Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[11] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[12] K. Yao and G. Zweig, \u201cSequence-to-sequence neural net models for grapheme-to-phoneme conversion,\u201d arXiv preprint arXiv:1506.00196, 2015.\n[13] H. Kamper, W. Wang, and K. Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.\n[14] J. Keshet, D. Grangier, and S. Bengio, \u201cDiscriminative keyword spotting,\u201d Speech Communication, vol. 51, no. 4, pp. 317\u2013329, 2009.\n[15] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121\u20132159, 2011.\n[16] V. Nair and G. E. Hinton, \u201cRectified linear units improve restricted boltzmann machines,\u201d in Proceedings of the 27th international conference on machine learning (ICML), 2010, pp. 807\u2013814.\n[17] L. van der Maaten and G. Hinton, \u201cVisualizing data using t-SNE,\u201d Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u2013 2605, 2008.\n[18] C. P. Browman and L. Goldstein, \u201cArticulatory phonology: An overview,\u201d Phonetica, vol. 49, no. 3-4, pp. 155\u2013180, 1992."}], "references": [{"title": "Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus", "author": ["S. Greenberg", "J. Hollenback", "D. Ellis"], "venue": "Proceeding of the 4th International Conference on Spoken Language Processing (ICSLP), 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Feature-based pronunciation modeling for automatic speech recognition", "author": ["K. Livescu"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Lexical access to large vocabularies for speech recognition", "author": ["L. Fissore", "P. Laface", "G. Micca", "R. Pieraccini"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 37, no. 8, pp. 1197\u20131213, 1989.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Lexical access experiments with context-dependent articulatory feature-based models", "author": ["P. Jyothi", "K. Livescu", "E. Fosler-Lussier"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative pronunciation modeling: A large-margin, feature-rich approach", "author": ["H. Tang", "J. Keshet", "K. Livescu"], "venue": "The 50th Annual Meeting of the Association of Computational Linguistics (ACL), 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting word neighborhoods for speech recognition", "author": ["P. Jyothi", "K. Livescu"], "venue": "ACL MORPHFSM Workshop, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "What kind of pronunciation variation is hard for triphones to model?", "author": ["D. Jurafsky", "W. Ward", "Z. Jianping", "K. Herold", "Y. Xiuyang", "Z. Sen"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Pronunciation change in conversational speech and its implications for automatic speech recognition", "author": ["M. Sara\u00e7lar", "S. Khudanpur"], "venue": "Computer Speech and Language, vol. 18, no. 4, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Feature-based pronunciation modeling with trainable asynchrony probabilities.", "author": ["K. Livescu", "J.R. Glass"], "venue": "Proceeding of the 8th International Conference on Spoken Language Processing (ICSLP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["K. Yao", "G. Zweig"], "venue": "arXiv preprint arXiv:1506.00196, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative keyword spotting", "author": ["J. Keshet", "D. Grangier", "S. Bengio"], "venue": "Speech Communication, vol. 51, no. 4, pp. 317\u2013329, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML), 2010, pp. 807\u2013814.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u2013 2605, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Articulatory phonology: An overview", "author": ["C.P. Browman", "L. Goldstein"], "venue": "Phonetica, vol. 49, no. 3-4, pp. 155\u2013180, 1992.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Fewer than half of the word productions are pronounced canonically in the phonetically transcribed portion of Switchboard [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 3, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 4, "context": "In the task of lexical access, the goal is to predict which word in the dictionary was uttered given its pronunciation in terms of sub-word units [3, 4, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 5, "context": "In the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "Here we propose a different definition which is closer to the definition suggested in [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "The variance of pronunciations in spontaneous and conversational speech leads to a high rate of errors in ASR systems [7, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "The variance of pronunciations in spontaneous and conversational speech leads to a high rate of errors in ASR systems [7, 8].", "startOffset": 118, "endOffset": 124}, {"referenceID": 8, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 3, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 4, "context": "Mapping between words and their possible pronunciations in terms of sub-word units was explored in light of the lexical access task [9, 4, 5].", "startOffset": 132, "endOffset": 141}, {"referenceID": 5, "context": "Learning word neighborhood automatically was proposed by [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "For example, in the lexical access task [5] the goal is to predict which word w from a finite dictionary V is associated with a given surface pronunciation p.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "In the word neighborhood task the goal is to find the acoustic neighborhood density of a given word [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Both architectures are based on Recurrent Neural Networks (RNNs) [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "We also tried several sequence-to-sequence architectures [11, 12], but since all of them led to poor results in terms of Word Error Rate (WER), we will not discuss them in the paper.", "startOffset": 57, "endOffset": 65}, {"referenceID": 11, "context": "We also tried several sequence-to-sequence architectures [11, 12], but since all of them led to poor results in terms of Word Error Rate (WER), we will not discuss them in the paper.", "startOffset": 57, "endOffset": 65}, {"referenceID": 12, "context": "score [13]:", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Formally, the loss function is defined as [14, 13]:", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Formally, the loss function is defined as [14, 13]:", "startOffset": 42, "endOffset": 50}, {"referenceID": 0, "context": "All experiments are conducted on a subset of Switchboard conversational speech corpus that has been labeled at a fine phonetic level [1]; these phonetic transcriptions are the input to our similarity models.", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 3, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 4, "context": "The data subsets, phone set P , and dictionary V are the same as those previously used in [9, 4, 5].", "startOffset": 90, "endOffset": 99}, {"referenceID": 8, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 3, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 4, "context": "We used the same partition of the corpus as in [9, 4, 5] into 2,942 words in the training set, 165 words in the development set, and 236 words in the test set.", "startOffset": 47, "endOffset": 56}, {"referenceID": 14, "context": "We optimize all our models using Adagrad [15] with learning rate value of 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "We use ReLU [16] as an activation function after each fully connected layer.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "dictionary lookup [2] 59.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": ", 2011 [4] 29.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": ", 2012 [5] 15.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "For comparison we added to Table 1 the word error rate of other algorithms for lexical access: a dictionary lookup with and without Levenshtein distance [9], a dynamic Bayesian network (Jyothi et al.", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": ", 2011 [4]), and discriminative structured prediction model (Hao et al.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": ", 2012 [5]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "It can be seen from the table that both of our models outperform the dictionary lookup approaches and the model which is based on dynamic Bayesian networks [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "However the discriminative structured prediction model [5] performs much better than any of our models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 16, "context": "In Figure 4 we visualized a subset from the embedding space of the canonical pronunciations in the dictionary, using t-SNE [17] for dimensionality reduction.", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "Specifically we would like to propose a similarity function between articulatory features, and analyze it in the light of articulatory phonology [18].", "startOffset": 145, "endOffset": 149}], "year": 2017, "abstractText": "A significant source of errors in Automatic Speech Recognition (ASR) systems is due to pronunciation variations which occur in spontaneous and conversational speech. Usually ASR systems use a finite lexicon that provides one or more pronunciations for each word. In this paper, we focus on learning a similarity function between two pronunciations. The pronunciations can be the canonical and the surface pronunciations of the same word or they can be two surface pronunciations of different words. This task generalizes problems such as lexical access (the problem of learning the mapping between words and their possible pronunciations), and defining word neighborhoods. It can also be used to dynamically increase the size of the pronunciation lexicon, or in predicting ASR errors. We propose two methods, which are based on recurrent neural networks, to learn the similarity function. The first is based on binary classification, and the second is based on learning the ranking of the pronunciations. We demonstrate the efficiency of our approach on the task of lexical access using a subset of the Switchboard conversational speech corpus. Results suggest that on this task our methods are superior to previous methods which are based on graphical Bayesian methods.", "creator": "LaTeX with hyperref package"}}}