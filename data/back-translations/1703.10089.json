{"id": "1703.10089", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Position-based Content Attention for Time Series Forecasting with Sequence-to-sequence RNNs", "abstract": "In this paper, we examine the use of recurrent neural networks (RNNs) to model and predict time series. First, we illustrate the fact that standard sequence-to-sequence RNs do not capture time series well or handle well missing values, even though many real time series are periodic and contain missing values. Then, we propose an advanced attention mechanism that can be used on any RNN and is designed to capture time periods and make the RNN more robust against missing values. We demonstrate the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "histories": [["v1", "Wed, 29 Mar 2017 15:11:16 GMT  (248kb,D)", "http://arxiv.org/abs/1703.10089v1", null], ["v2", "Mon, 21 Aug 2017 12:36:58 GMT  (192kb,D)", "http://arxiv.org/abs/1703.10089v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["yagmur g cinar", "hamid mirisaee", "parantapa goswami", "eric gaussier", "ali ait-bachir", "vadim strijov"], "accepted": false, "id": "1703.10089"}, "pdf": {"name": "1703.10089.pdf", "metadata": {"source": "META", "title": "Time Series Forecasting using RNNs: an Extended Attention Mechanism to Model Periods and Handle Missing Values", "authors": ["Yagmur Gizem Cinar", "Hamid Mirisaee"], "emails": ["YAGMUR.CINAR@IMAG.FR", "HAMID.MIRISAEE@IMAG.FR", "PARANTAPA.GOSWAMI@IMAG.FR", "ERIC.GAUSSIER@IMAG.FR", "A.AIT-BACHIR@COSERVIT.COM", "STRIJOV@CCAS.RU"], "sections": [{"heading": "1. Introduction", "text": "Forecasting future values of temporal variables is termed as time series prediction or time series forecasting and has applications in a variety of fields, as finance, economics, meteorology, or customer support center operations. A considerable number of stochastic (De Gooijer & Hyndman, 2006) and machine learning based (Bontempi et al., 2013) approaches have been proposed for this problem. A partic-\nular class of approaches that has recently received much attention for modeling sequences is based on sequenceto-sequence Recurrent Neural Networks (RNNs) (Graves, 2013). Sequence-to-sequence RNNs constitute a flexible class of methods particularly well adapted to time series forecasting when one aims at predicting a sequence of future values on the basis of a sequence of past values. They have furthermore led to state-of-the-art results in different applications (as machine translation or image captioning). We explore here how to adapt them to general time series.\nIn real life scenarios, time series data often display periods, due for example to environmental factors as seasonality or to the patterns underlying the activities measured (the workload on professional email servers for example has both weekly and daily periods). A first question one can ask is thus: Can sequence-to-sequence RNNs model periods in time series? Furthermore, missing observations and gaps in the data are also common, due to e.g. lost records and/or mistakes during data entry or faulty sensors. In addition, for multivariate time series, underlying variables can have mixed sampling rates, i.e. different variables may have different sampling frequencies, resulting again in values missing at certain times when comparing the different variables. The standard strategy to tackle missing values is to apply numerical or deterministic approaches, such as interpolation and data imputation methods, to explicitly estimate or infer the missing values, and then apply classical methar X\niv :1\n70 3.\n10 08\n9v 1\n[ cs\nods for forecasting. In the context of sequence-to-sequence RNNs, a standard technique, called padding, consists in repeating the hidden state of the last observed value over the missing values. All these methods however make strong assumptions about the functional form of the data, and imputed values, especially for long gaps, and can introduce sources of errors and biases. The second question we address here is thus: Do sequence-to-sequence RNNs handle well missing values?\nTo answer the above questions, we first conduct in Section 2 simple experiments to illustrate how state-of-the-art sequence-to-sequence RNNs behave on time series. Even though our approach is general in the sense that it can be applied to any sequence-to-sequence RNN, we consider in this study bidirectional RNNs (Schuster & Paliwal, 1997) based on Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) with peephole connections (Gers et al., 2002), known to perform very well in practice (Graves, 2013), and we also make use of the attention mechanism recently introduced in (Bahdanau et al., 2014). We then present in Section 3 two extensions of the attention mechanism to better handle periods and missing values, prior to describe their application to multivariate time series (Section 4) and to evaluate their impact on several univariate and multivariate time series in Section 5. We finally discuss the related work in Section 6. In the remainder, we use the term RNNs to refer to sequence-to-sequence RNNs."}, {"heading": "2. How do RNNs behave on time series? A study of periodicity and missing values", "text": "We focus at first on univariate time series even though most of the elements we discuss apply directly to multivariate time series (we will consider multivariate time series in a second step). As mentioned before, time series forecasting consists in predicting future values from past, observed values. The time span of the past values, denoted by T , is termed as history, whereas the time span of the future values to be predicted, denoted by T \u2032, is termed as forecast horizon (in multi-step ahead prediction, which we consider here, T \u2032 > 1). The prediction is modeled as a regression-like problem where the goal is to learn the relation y = r(x) where y = (xT+1, . . . , xT+i, . . . , xT+T \u2032) is the output sequence and x = (x1, . . . , xj , . . . , xT ) is the input sequence. Both input and output sequences are ordered and indexed by time instants.\nFor clarity\u2019s sake, and without loss of generality, for the input sequence x = (x1, . . . , xj , . . . , xT ), the output sequence y is rewritten as y = (y1, . . . , yi, . . . , yT \u2032).\nRNNs rely on three parts, one dedicated to encoding the input, and referred to as encoder, one dedicated to construct-\ning a summary of the encoding, which we will refer to as summarizer, and one dedicated to generating the output, and referred to as decoder. The encoder represents each input xj , 1 \u2264 j \u2264 T as a hidden state \u2212\u2192 hj = f(xj ,\n\u2212\u2212\u2192 hj\u22121),\u2212\u2192 hj \u2208 Rn, where the function f corresponds here to the non-linear transformation implemented in LSTM with peephole connections (Gers et al., 2002). The equations of LSTM with peephole connections are given in Eq. 1. For bidirectional RNNs (Schuster & Paliwal, 1997), the input is read both forward and backward, leading to two vectors\u2212\u2192 h j = f(xj , \u2212\u2192 h j\u22121) and \u2190\u2212 h j = f(xj , \u2190\u2212 h j+1). The final hidden state for any input xj is constructed simply by concatenating the corresponding forward and backward hidden states, i.e. hj = [ \u2212\u2192 h j ; \u2190\u2212 h j ] T, where now hj \u2208 R2n.\nij = \u03c3(Wixj + Uihj\u22121 + C \u2032 ic \u2032 j\u22121) fj = \u03c3(Wfxj + Ufhj\u22121 + C \u2032 fc \u2032 j\u22121) c\u2032j = fjc \u2032 j\u22121 + ij tanh(Wc\u2032xj + Uc\u2032hj\u22121) oj = \u03c3(Woxj + Uohj\u22121 + C \u2032 oc \u2032 j\u22121) hj = oj tanh(c\u2032j)\n(1)\nThe summarizer builds, from the sequence of input hidden states hj , 1 \u2264 j \u2264 T , a context vector c = q({h1, . . . ,hj , . . . ,hT }). In its most simple form, the function q just selects the last hidden state (Graves, 2013): q({h1, . . . ,hj , . . . ,hT }) = hT . More recently, in (Bahdanau et al., 2014), an attention mechanism is used to construct different context vectors ci for different outputs yi (1 \u2264 i \u2264 T \u2032) as a weighted sum of the hidden states of the encoder representing the input history: ci = \u2211T j=1 \u03b1ijhj , where \u03b1ij are the attention weights. They are computed as:\n\u03b1ij = exp(eij)\u2211T\nj\u2032=1 exp(eij\u2032) (2)\nwhere\neij = a(si\u22121,hj) = v T a tanh(Wasi\u22121 + Uahj) (3)\nwith a being a feedforward neural network with weights Wa, Ua and va trained in conjunction with the entire encoder-decoder framework. si\u22121 is the hidden state obtained by the decoder (see below) at time (i-1)One can note that a scores the importance of the input at time j (specifically its representation hj by the encoder) for the output at time i, given the previous hidden state si\u22121 of the decoder. This allows the model to concentrate or put attention on certain parts of the input history to predict each output.\nThe decoder parallels the encoder by associating each output yi, 1 \u2264 i \u2264 T \u2032 to a hidden state vector si = g(yi\u22121, si\u22121, ci) where si \u2208 Rn and yi\u22121 denotes the output at time i \u2212 1. The function g corresponds again to an\nLSTM with peephole connections, with an explicit context added (Graves, 2013). The equations of LSTM with peephole connections and context are given in Eq. 4. Each output is then predicted in sequence through:\nyi = Woutsi + bout,\nwhere bout is a scalar and Wout \u2208 Rn.\nii = \u03c3(Wiyi\u22121 + Uisi\u22121 + C \u2032 ic \u2032 i\u22121 + Cici) fi = \u03c3(Wfyi\u22121 + Ufsi\u22121 + C \u2032 fc \u2032 i\u22121 + Cfci) c\u2032i = fic \u2032 i\u22121 + iitanh(Wc\u2032yi\u22121 + Uc\u2032si\u22121 + Cc\u2032ci) oi = \u03c3(Woyi\u22121 + Uosi\u22121 + C \u2032 oc \u2032 i\u22121 + Coci) si = oitanh(c\u2032i) (4)"}, {"heading": "2.1. RNNs and periodicity", "text": "To illustrate how RNNs behave w.r.t. periods in time series, we retained two periodic time series, fully described in Section 5 and representing respectively (a) the electrical consumption load in Poland over the period of 10 years (1/1/2002-7/31/2012), and (b) the maximum daily temperature, again in Poland, over the period of 12 years (1/1/2002- 7/31/2014). The first time series, referred to as PSE, has two main periods, a daily one and a weekly one, whereas the second time series, referred to as PW, has a yearly period. Figure 1 (left and middle) displays the autocorrelation (Chatfield, 2016) for each time series and shows these different periods (note that the term original indicates that we used the dataset as it is, without any modification). We ran the RNN described above, with (RNN-A) and without (RNN) the attention mechanism (in the latter case, the context is taken to be the last hidden state). The experimental setting retained is the one described in Section 5.\nWe also added to the input sequence the time stamp (or position) information j, to help the RNNs capture potential periods. When the positions are added, each input at time j contains two values: xj and j, 1 \u2264 j \u2264 T . The position is of course not predicted in the output. The results we obtained, evaluated in terms of mean squared error (MSE) and displayed in Figure 1 (right), show that adding the time stamps in the input does not improve the RNNs: the results slightly degrade on PSE (from 0.039 to 0.053) and are almost the same on PW. Furthermore, on these time series, the use of the attention mechanism has almost no impact: the MSE remains almost the same when the attention mechanism is used.\nTo see whether standard RNNs are able to capture periods in time series, we plot, in Figure 2, the weights obtained by the attention mechanism. As mentioned before, the attention mechanism aims at capturing the importance of the input at time j for predicting the output at time i. Were\nstandard RNNs able to capture periods, we should see that the corresponding weights in the attention mechanism are higher. However, as one can note, this is not the case: for RNNs with time stamp information, a higher weight is put on the first instances, which do not correspond to the actual periods of the times series. Higher weights are put with standard RNNs on the more recent history on PSE. This seems more reasonable, and leads to better results, even though the period at one day is missed. On PW, the weights with standard RNNs are uniformly distributed on the different time stamps. This shows that current RNNs do not always detect (and make us of) the periods underlying the data."}, {"heading": "2.2. RNNs and missing values", "text": "We now turn to the problem of missing values and use this time a degraded version of the above time series in which 15% of the original values are missing, either in consecutive sequences (to form gaps in the data) or randomly (we refer again the reader to Section 5 for further detail on these datasets). As mentioned before, a standard technique to handle such missing values in RNNs, called padding, consists in repeating the hidden state of the last observed value over the missing values. Another standard approach on temporal data is to reconstruct missing values using interpolation. Among the existing interpolation methods, we experimented with three methods from three different categories, namely linear interpolation, non-linear spline interpolation and kernel based Fourier transform interpolation (Meijering, 2002). We measured their reconstruction ability on all four datasets we used for experiments (described in Section 5.1) in terms of MSE, calculated between the interpolated and original datasets. In all the cases, we found that linear interpolation provided the minimum reconstruction error. We thus compare here the use of linear interpolation and padding on RNNs, with and without the attention mechanism. The results obtained, displayed in Figure 3 (left), show that the linear interpolation tends to outperform the padding technique on both datasets1, particularly on PSE15. Furthermore, as before, the attention mechanism brings here no improvement.\nLooking at the weights obtained by the attention mechanism (Figure 3 (middle and right)) for both PSE and PW, one can see that, not surprisingly, the missing input data sometimes get high weights and are primarily used to predict output values, despite their lack of reliability. In particular, for PW, the missing values corresponding to the gap comprised in between 12 and 8 months in the history get very high values. This indicates that current RNNs are not entirely robust to missing values as an important part of\n1 We obtained the same results on all collections, with different levels of missing values.\ntheir decision can be based on them."}, {"heading": "3. Extended attention mechanism", "text": "We present here two simple extensions of the attention mechanism that allow one to model periods and better handle missing values in RNNs."}, {"heading": "3.1. Modeling periodicity", "text": "With a history size T and a forecast horizon T \u2032, the possible periods to rely on for prediction range in the set {1, \u00b7 \u00b7 \u00b7 , T + T \u2032 \u2212 1}. We here explicitly model all possible periods throughout a real vector, which we will refer to as \u03c4 , of dimension T + T \u2032 \u2212 1. This vector is used to encode the importance of all possible periods, and to decrease or increase the importance of the corresponding input to predict the current output. To this end, we modify the original attention mechanism to reweigh the attention weights as follows:\neij,\u03c4 = v T a tanh(Wasi\u22121 + Uahj)\u00d7 (\u03c4T\u2206(i, j)) (5)\nwhere \u2206(i, j) \u2208 RT+T \u2032\u22121 is a binary vector that is 1 on dimension (i \u2212 j) and 0 elsewhere. We will refer to this model as RNN-\u03c4 .\nAs one can note, \u03c4(i\u2212j) will either decrease or increase the weight eij computed by the original attention mechanism. Since \u03c4 is learned along with the other parameters of the RNN, we expect that \u03c4(i\u2212j) will be high for those values of (i\u2212 j) that correspond to periods of the time series. We will see in Section 5 that this is indeed the case."}, {"heading": "3.2. Handling missing values", "text": "Provided their proportion is not too important, otherwise the forecasting task is less relevant, missing values in time series can be easily identified by considering that the most common interval between consecutive points corresponds to the sampling rate of the time series. Points not present at the expected intervals are then considered as missing. This strategy works well in practice, provided the sampling rate of the time series does not change much over time. It is thus possible to identify missing values in time series, and then use padding or interpolation techniques to represent them. Padded or interpolated inputs should however not be treated as standard inputs as they are less reliable than other inputs. Furthermore, be it with padding or interpolation techniques, when the size of a gap, i.e. consecutive missing values, is important, the further away the missing value is from the observed values (the last one in case of padding, the last and next ones in case of interpolation), the less confident one is in the padded hidden states or interpolated values. It is thus desirable to decrease the importance of missing values according to their position in gaps.\nWe propose to do so by another modification of the attention mechanism that again reweighs the attention weights. We consider two reweighing scheme: a first one that penalizes missing values further away from the last observed value through a decaying function, a priori adapted to padding, and a second one that penalizes missing values depending on whether they are at the beginning, middle or end of a gap, a priori adapted to interpolation methods that rely on values before and after the gap. The first reweighing scheme corresponds to the following equation, that extends Eq. 5 and relies on an exponential decaying function:\neij,\u03c4\u00b5,1 = eij,\u03c4 \u00d7 f1(\u00b5, j) (6)\nwith:\nf1(\u00b5, j) = { exp(\u2212\u00b5(j \u2212 jlast)) if j is missing 1 otherwise\nwhere jlast denotes the last observed value before j and \u00b5 is an additional parameter that is learned with the other parameters of the RNN. eij,\u03c4 is given by Eq. 5. We will refer to this model as RNN-\u03c4\u00b5-1.\nSimilarly, the second reweighing scheme is defined as follows:\neij,\u03c4\u00b5,2 = eij,\u03c4 \u00d7 (1 + \u00b5TPos(j; \u03b8g)) (7)\nwhere Pos(j; \u03b8g) is a three-dimensional vector which is null if j is not missing; otherwise, denoting the length of the gap by |g|, if j\u2212jlast|g| < \u03b8 g 1 , then the first coordinate of Pos(j; \u03b8g) is set to 1, if \u03b8g1 < j\u2212jlast |g| < \u03b8 g 2 then the second coordinate is set to 1 and if j\u2212jlast|g| > \u03b8 g 2 then the third coordinate is set to 1. Note that if only one value is missing, then |g| = 1. \u00b5 is now a three-dimensional vector, learned with the other parameters of the RNN, where each of the coordinates aim at reweighing the impact of missing values on the prediction task according to their position in gaps. The values \u03b8g1 and \u03b8 g 2 are hyper-parameters; they are set here in such a way that they divide each gap g into three equal parts corresponding to the beginning of the gap, its middle and its end. We will refer to this model as RNN-\u03c4\u00b5-2. As one can note, if there is no gap in the data (i.e. no missing values), then both Eq. 6 and Eq. 7 reduce to Eq. 5.\nFigure 4 illustrates the modifications brought to the attention mechanism to model periods and handle missing values (the reweighing scheme of RNN-\u03c4\u00b5-1 is used but can nevertheless be replaced by the one of RNN-\u03c4\u00b5-2)."}, {"heading": "4. Multivariate Extension", "text": "We now consider the multivariate extension of the above model. As each variable in aK multivariate time series can\nhave its own periods, we propose here to apply the RNN with the extended attention mechanism described above on each variable k, 1 \u2264 k \u2264 K, of the time series. We thus construct, for each variable k, context vectors on the basis of the following equations:\n\u03b1 (k) ij =\nexp ( e (k) ij )\n\u2211T j\u2032=1 exp ( e (k) ij\u2032 ) (8)\nwhere ekij is given by Eqs. 3, 5, 6 or 7. The context vector for the ith output of the kth variable is then defined by c (k) i = \u2211T j=1 \u03b1 (k) ij h (k) j , where h (k) j is the encoder hidden state at time stamp j for the kth variable.\nLastly, to predict the output in the multivariate case while taking into account potential dependencies between different variables, we concatenate the context vectors from the different variables into a single context vector ci that is used as input to the decoder, the rest of the decoder architecture being unchanged:\nci = [c (1)T i \u00b7 \u00b7 \u00b7 c (K)T i ] T\nAs each c(k)i is of dimension 2n (that is the dimension of the encoder hidden states), ci is of dimension 2Kn.\nWe now turn to the experimental validation of the proposed RNN."}, {"heading": "5. Experiments", "text": "In this section, we first describe the datasets used in this study and explain the experimental settings prior to presenting the results we obtain using the proposed methods."}, {"heading": "5.1. Datasets and settings", "text": "We retained four widely used and publicly available (Datasets) that are described in Table 1. The values for the history size were set so as they encompass the known periods of the datasets. They can of course be tuned by crossvalidation if one does not want to identify the potential periods by checking the autocorrelation curves. In general, the forecast horizon should reflect the nature of the data and the application one has in mind, with of course a trade off between long forecast horizon and prediction quality. For this purpose, the forecast horizons of PSE, PW, AQ and HPC are chosen as 8 hours, 1 week, 6 hours and 8 hours respectively. In Table 1, one can also find the sampling rate for each dataset.\nNote that for PW, we selected the Warsaw metropolitan area which covers only one recording station. For the univariate experiments, from PW we selected the max temperature series, from AQ we picked PT08.S4(NO2) and from HPC we selected the global active power variable.\nTo implement the RNN models discussed before, we used theano2 (Theano Development Team, 2016) and Lasagne 3 on a Linux system with 256GB of memory and 32-core Intel Xeon @2.60GHz. All parameters are regularized and learned through stochastic backpropagation with adaptive learning rate for each parameter (Kingma & Ba, 2014), the objective function being the MSE on the output. The hyperparameters we considered for the different RNNs are given in Table 2. To tune the hyperparameters, we performed a two-level grid search, where in the first level we tuned the mini-batch size by using 10 random combinations of"}, {"heading": "2 http://deeplearning.net/software/theano/", "text": "3 https://lasagne.readthedocs.io\nthe hyperparameters. Within the set {1, 32, 64, 128, 256, 512}, we retained the mini-batch size of 64 as it performs best. Then, in the second level, we tuned all the hyperparameters shown in Table 2 and picked the top 4 settings according to the RNN model with standard attention mechanism. We use these top 4 settings for all the RNN-based models we are evaluating, selecting the best setting for each model on the validation set. Note that in all those top 4 settings, the learning rate, regularization coefficient and regularization type are the same (shown in the bold face in the upper part Table 2). In other words, the top 4 settings differ in number of attention units and number of RNN units, all the other hyperparameters being the same. Lastly, the objective function for all RNN-based models is the MSE.\nIn order to assess whether the proposed methods are robust to missing values, we introduced different levels of missing values in the datasets with the following strategy: for each dataset, we added 5%, 10%, 15% and 20% percent of missing values. As the datasets should contain both random missing values and gaps, we introduced half gaps and half random missing values. For instance, to add 10% of missing values, we add 5% of random missing values and 5% of gaps. Furthermore, as the gaps could be of different sizes, we let the length of gaps vary from 5 to 100, picked at random. Note that for HPC and AQ which already contain missing values, we first introduced new missing values until we reach half of the desired percentage, then we introduce the gaps. In our experiments, the percentage of missing values introduced are shown as a number along with the dataset name; the original dataset is named original. Also note that the time series AQ already has 5% of missing values, so we discard introducing the 5% missing values and we start with 10%.\nWe compare the following methods discussed before, namely standard RNN, RNN-A, RNN-\u03c4 , RNN-\u03c4\u00b5-1/2, with gradient boosted trees (GBT) and random forests (RF) as these ensemble methods have been shown to provide state-of-the-art results on various forecasting problems and to outperform moving-average methods (e.g. (Kane et al., 2014a)). We apply grid search over the number of trees {500, 1000, 2000} and the number of features {nfeatures,\u221a nfeatures, log2(nfeatures)} for RF, and over learning rate {0.01, 0.05, 0.1, 0.25} for GBT. As stated in Section 2, we use linear interpolation and padding for the datasets with missing values.\nFor evaluation, we use MSE and symmetric mean absolute percentage error (SMAPE). MSE is defined as\nMSE = 1\nN\n\u2211\ni\n(yi \u2212 y\u0302i)2\nand gives the average L2 distance between the predicted value y\u0302i and the true value yi, where N is the total number\nof predictions, including all horizons. SMAPE is define as\nSMAPE = 1\nN\nN\u2211\ni=1\n|y\u0302i \u2212 yi| (|yi|+ |y\u0302i|)/2\nand calculates the L1 error divided by the average of predicted and true value. SMAPE is a scaled L1 error.\nLastly, we divided the datasets by retaining the first 75% of each dataset for training-validation and the last 25% for testing. We applied 5-fold cross validation on the trainingvalidation sets for RF and GBT. For RNN-based methods, we divided the training-validation sets into first 75% for training (56.25% of the data) and last 25% for validation (18.75% of the data)."}, {"heading": "5.2. Overall results on univariate time series", "text": "Figure 5 and Figure 6 illustrate the univariate experiments with the interpolation and padding methods, respectively, using the MSE and the SMAPE as evaluation measures. We nevertheless focus our discussion on MSE as it is the metric based on which the problem is optimized.\nAs one can observe in Figure 5, for the AQ dataset, the three proposed models, i.e. RNN-\u03c4 and RNN-\u03c4\u00b5-1/2, yield better MSE results than the other methods. The same conclusion could be drawn from the SMAPE figure of the same dataset. Furthermore, as conjectured in Section 3, RNN\u03c4\u00b5-2 tends to perform better than RNN-\u03c4\u00b5-1 on these interpolated data. This fact can be seen in Figure 6 where the MSE and SMAPE results are illustrated for the padded data. One can observe in Figure 6 that, in general, RNN\u03c4\u00b5-1 performs better in terms of MSE, as anticipated in Section 3, and is very close to RNN-\u03c4\u00b5-2 in terms of SMAPE. Note that, in Figure 6, we removed RF and GBT as padding cannot be used (they are always interpolated with missing values). One can also observe that on the original dataset, with only 5% missing values, RNN-\u03c4 has the lowest MSE both in padded and interpolated data.\nIn Figure 5, similar results, both in terms of MSE and SMAPE, are observed on PSE, with again a slight advantage for RNN-\u03c4\u00b5-2 over RNN-\u03c4\u00b5-1, the difference with the other methods being important when the proportion of missing values increases, e.g. for PSE20. On the other hand, one can see in Figure 6 that when the data is\npadded and there is considerable amount of missing values, i.e. PSE20, RNN-\u03c4\u00b5-1 outperforms RNN-\u03c4\u00b5-2, both with MSE and SMAPE, as conjectured in Section 3. The baselines, RF and GBT, do not behave well on AQ and PSE ,and yield worse results than the RNN-based methods. Moreover, RNN-A yields better results w.r.t. RNN for interpolated data when we have the maximum amount of missing values, both in terms of MSE and SMAPE.\nThe situation is more contrasted on HPC, where there is no clear difference between the methods for the interpolated data. In this case, the proposed methods are comparable to the standard attention mechanism. When there are many missing values (HPC15 and HPC20), RNN-\u03c4\u00b5-2 is slightly better than the other methods. However, in case of padded data, all three proposed methods outperform RNN and RNN-A, particularly when we increase the missing values.\nSimilar to HPC there is no clear difference for PW between the methods in case of interpolated data. However, if we compare the RNN-based methods, we see that except for PW10, the proposed methods outperform RNN and RNNA, particularly in case of PW20. The situation is a bit better in case of padded data; for instance, there is a clear improvement over the MSE on PW10 and slight improvement on SMAPE on PW15."}, {"heading": "5.3. RNN-\u03c4 and periodicity", "text": "As mentioned before, RNN-\u03c4 aims at detecting the periodicity of the data, which is done via the \u03c4 vector explained in Section. 3.1. Here, we show the effectiveness of this approach by illustrating how the attention weights of the original attention model behave compared to those of RNN-\u03c4 . Obviously, we expect RNN-\u03c4 to put higher weights on the inputs corresponding to periods. To illustrate this point, we\nchose the PSE dataset which has two periods, weekly and daily (see Section 2). To observe how RNN-\u03c4 behaves, we average all the attention weights of all test examples and all forecast horizons. Figure 7 (left) shows the average attention weights of PSE with RNN-A and RNN-\u03c4 . As one can observe, RNN-A fails to capture both periods (1 day and 7 days) while RNN-\u03c4 can effectively spot those two periods by giving them higher weights. We also illustrate the learned \u03c4 values of PSE in Figure 7 (right), which shows that the highest values are assigned to the period of the data, i.e. 7 days and 1 day."}, {"heading": "5.4. RNN-\u03c4\u00b5 and missing values", "text": "The RNN-\u03c4\u00b5-1/2 models are designed to capture periodicity and to prevent the attention mechanism from assigning high weights to the missing instances. As noted before, RNN-\u03c4\u00b5-2 with interpolated data is overall the best per-\nforming method. To show the effectiveness of this method in handling the missing values, we investigated the attention weights it assigned to missing instances. For each dataset, we averaged the attention weights of RNN-\u03c4\u00b5-2 and compared them to those of RNN-A. Figure 8 illustrates the average attention weight and the confidence intervals for all datasets.\nAs one can observe, in 11 cases out of 15, RNN-\u03c4\u00b5-2 provides attention weights which are lower on missing values than those of RNN-A. In average, the attention weights that RNN-\u03c4\u00b5 put on missing values is 48% smaller than that of RNN-A, which shows how the \u00b5 vector can efficiently take care of missing values inside the data, regardless of its position, by not putting much weight on them."}, {"heading": "5.5. Results on multivariate time series", "text": "As mentioned in Section 4, the proposed methods can be extended to multivariate time series. Here, we show how this extension can effectively outperform the state-ofthe-art methods as well as the standard attention mechanism. To illustrate that, we pick all four global time series of HPC, namely global active power, global reactive power, voltage and global intensity. We predict the first one which is important for e.g. monthly electricity bills. For AQ, we selected the four variables associated to real sensors (thus excluding nominal sensors), namely C6H6(GT), NO2(GT), CO(GT) and NOx(GT) and predict the first one, C6H6(GT), as it is the most important for NMHC-related air pollution (Vito et al., 2009). Note that this differs from the univariate case in which we focused on the nominal sensor PT08.S4(NO2) to observe the effect of different amounts of missing values and gaps. Also note that all the time series selected from AQ have 18% of missing values; for HPC we consider 5, 10 and 15% of missing values.\nFigure 9 (left) shows the results of our experiments on these multivariate series. We only show GBT here since it generally outperforms RF. As one can see in this figure, RNN\u03c4 and RNN-\u03c4\u00b5-1/2 provide better results than RNN in all cases. Compared to RNN-A, this is also the true in three cases out of five, the methods being comparable in the last\ntwo cases. In all cases, RNN-\u03c4 outperforms GBT and, except for AQ dataset, RNN-\u03c4\u00b5-1/2 always provides better results than the other approaches.\nSimilarly with what is happening on the univariate case, RNN-\u03c4\u00b5-1/2 behave better when the amount of missing values increases, with again a slight preference for RNN\u03c4\u00b5-2, as conjectured in Section 3. Furthermore, relying on several variables for the prediction improves the results over the univariate case. As illustrated in Figure 9 (right), the results obtained with the multivariate extension are systematically and significantly better than the ones obtained with the univariate case (the same variable is predicted in both cases)."}, {"heading": "6. Related Work", "text": "The notion of stochasticity of time series modeling and prediction was introduced long back (Yule, 1927). Since then various stochastic models have been developed, notable among these are autoregressive (AR) (Walker, 1931) and moving averages (MA) (Slutzky, 1937) methods. These two models were combined in a more general and more effective framework, known as autoregressive moving average (ARMA), or autoregressive integrated moving average (ARIMA) when the differencing is included in the modelling (Box & Jenkins, 1968). Vector ARIMA or VARIMA (Tiao & Box, 1981) is the multivariate extension of the univariate ARIMA models where each time series instance is represented using a vector.\nNeural networks are regarded as a promising tool for time series prediction (Zhang, 2001; Crone et al., 2011) due to their data-driven and self-adaptive nature, their ability to approximate any continuous function and their inherent non-linearity. The idea of using neural networks for prediction dates back to 1964 where an adaptive linear network was used for weather forecasting (Hu, 1964). But the research was quite limited due to the lack of a training algorithm for general multilayer networks at the time. Since the introduction of backpropagation algorithm (Rumelhart\net al., 1988), there had been much development in the use of neural networks for forecasting. It was shown in a simulated study that neural networks can be used for modeling and forecasting nonlinear time series (Laepes & Farben, 1987). Several studies (Werbos, 1988; Sharda & Patil, 1990) have found that neural networks are able to outperform the traditional stochastic models such as ARIMA or Box-Jenkins approaches (Box & Jenkins, 1968).\nWith the advent of SVM, it has been used to formulate time series prediction as a regression estimation using Vapnik\u2019s insensitive loss function and Huber\u2019s loss function (Mu\u0308ller et al., 1997). The authors showed that SVM based prediction outperforms traditional neural network based methods. Since then different versions of SVMs are applied for time series prediction and many different SVM forecasting algorithms have been derived (Raicharoen et al., 2003; Fan et al., 2006). More recently, random forests (Breiman, 2001) are used for time series predictions due to their performance accuracy and ease of execution. Random forest regression is used for prediction in the field of finance (Creamer & Freund, 2004) and bioinformatics (Kusiak et al., 2013), and are shown to outperform ARIMA (Kane et al., 2014b).\nTraditional neural networks allow only feedforward connections among the neurons of a layer and the neurons in the following layer. In contrast, recurrent neural networks (RNN) (Jordan, 1986; Elman, 1990) allow both forward and feedback or recurrent connections between the neurons of different layers. Hence, RNNs are able to incorporate contextual information from past inputs which makes them an attractive choice for predicting general sequence-to-sequence data, including time series (Graves, 2013). In this present study we use RNNs for modeling time series. Early work (Connor et al., 1991) has shown that RNNs (a) are a type of nonlinear autoregressive moving average (NARMA) model and (b) outperform feedforward networks and various types of linear statistical models\non time series. Subsequently, various RNN-based models were developed for different time series, as noisy foreign exchange rate prediction (Giles et al., 2001), chaotic time series prediction in communication engineering (Jaeger & Haas, 2004) or stock price prediction (Hsieh et al., 2011). A detailed review can be found in (La\u0308ngkvist et al., 2014) on the applications of RNNs along with other deep learning based approaches for different time series prediction tasks.\nRNNs based on LSTMs (Hochreiter & Schmidhuber, 1997), which we consider in our study, alleviate the vanishing gradient problem of the traditional RNNs proposed. They have furthermore been shown to outperform traditional RNNs on various temporal tasks (Gers et al., 2001; 2002). More recently, they have been used for predicting the next frame in a video and for interpolating intermediate frames (Ranzato et al., 2014), for forecasting the future rainfall intensity in a region (Xingjian et al., 2015), or for modeling clinical data consisting of multivariate time series of observations (Lipton et al., 2015).\nAdding attention mechanism on the decoder side of an encoder-decoder RNN framework enabled the network to focus on the interesting parts of the encoded sequence (Bahdanau et al., 2014). Attention mechanism is used in many applications such as image description (Xu et al., 2015), image generation (Karol et al., 2015), phoneme recognition (Chorowski et al., 2015), heart failure prediction (Choi et al., 2016), as well as time series prediction (Riemer et al., 2016) and classification (Choi et al., 2016). Many studies also apply attention mechanism on external memory (Graves et al., 2014; 2016).\nThe previous work (Riemer et al., 2016) uses attention mechanism to determine the importance of a factor among other factors that affect time series. However, we use extended attention mechanism to model period and emphasize the interesting parts of input sequence with missing values. Our approach is applicable for both univariate and\nmultivariate time series prediction.\nHowever, as far we know, no work has focused on analyzing the adequacy of RNNs (based or not on LSTMs) for time series, in particular w.r.t. their ability to model periods and handle missing values (for this latter case, some methods based on e.g. Gaussian processes have been proposed to handle irregularly sampled data and missing values (Ghassemi et al., 2015), but none related to RNNs). To our knowledge, this study is the first one to address this problem."}, {"heading": "7. Conclusion", "text": "In this paper we studied the abilities of RNNs for modeling and forecasting time series. We used state-of-the-art RNNs based on bidirectional LSTM encoder-decoder with attention mechanism, and illustrated their deficiencies in capturing the periodicity of the data and in handling properly missing values. To alleviate this, we proposed two architectural modifications over the traditional attention mechanism: a first one to learn and exploit the periodicity in the temporal data (RNN-\u03c4 ) and a second one to handle both random missing values and long gaps in the data (RNN-\u03c4\u00b51/2). We further extended the entire framework for multivariate time series.\nThe experiments we conducted over multiple univariate and multivariate time series demonstrate the effectiveness of these modifications. RNN-\u03c4 and RNN-\u03c4\u00b5-1/2 are able to perform not only better than traditional RNNs and RNNs with original attention mechanism, but also better than two state-of-the-art baselines based on random forests and gradient boosted trees. Furthermore, by design, RNN-\u03c4 can capture several periods in a time series and make use of all of them to forecast new values. We provided experimental illustration to this fact on a time series with both weekly and daily periods. Regarding missing values, the two variants we propose, RNN-\u03c4\u00b5-1/2, are well adapted to different techniques used in RNNs to handle missing values: padding and interpolation. We showed that these variants rely more on actual values and less on padded or interpolated values than the original RNNs, thus making the proposed RNN more robust to missing values."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Some recent advances in forecasting and control", "author": ["Box", "George EP", "Jenkins", "Gwilym M"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "Box et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Box et al\\.", "year": 1968}, {"title": "The analysis of time series: an introduction", "author": ["Chatfield", "Chris"], "venue": "CRC press,", "citeRegEx": "Chatfield and Chris.,? \\Q2016\\E", "shortCiteRegEx": "Chatfield and Chris.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Recurrent networks and narma modeling", "author": ["Connor", "Jerome", "Atlas", "Les E", "Martin", "Douglas R"], "venue": "In NIPS,", "citeRegEx": "Connor et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Connor et al\\.", "year": 1991}, {"title": "Predicting performance and quantifying corporate governance risk for latin american adrs and banks", "author": ["Creamer", "Germ\u00e1n G", "Freund", "Yoav"], "venue": null, "citeRegEx": "Creamer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Creamer et al\\.", "year": 2004}, {"title": "Advances in forecasting with neural networks? empirical evidence from the nn3 competition on time series prediction", "author": ["Crone", "Sven F", "Hibon", "Michele", "Nikolopoulos", "Konstantinos"], "venue": "International Journal of Forecasting,", "citeRegEx": "Crone et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Crone et al\\.", "year": 2011}, {"title": "25 years of time series forecasting", "author": ["De Gooijer", "Jan G", "Hyndman", "Rob J"], "venue": "International journal of forecasting,", "citeRegEx": "Gooijer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gooijer et al\\.", "year": 2006}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Dynamic least squares support vector machine", "author": ["Fan", "Yugang", "Li", "Ping", "Song", "Zhihuan"], "venue": "In Intelligent Control and Automation,", "citeRegEx": "Fan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2006}, {"title": "Applying lstm to time series predictable through timewindow approaches", "author": ["Gers", "Felix A", "Eck", "Douglas", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2001}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "Journal of machine learning research,", "citeRegEx": "Gers et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2002}, {"title": "Noisy time series prediction using recurrent neural networks and grammatical inference", "author": ["Giles", "C Lee", "Lawrence", "Steve", "Tsoi", "Ah Chung"], "venue": "Machine learning,", "citeRegEx": "Giles et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Giles et al\\.", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm", "author": ["Hsieh", "Tsung-Jung", "Hsiao", "Hsiao-Fen", "Yeh", "WeiChang"], "venue": "Applied soft computing,", "citeRegEx": "Hsieh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2011}, {"title": "Application of the adaline system to weather forecasting", "author": ["Hu", "Michael Jen-Chao"], "venue": null, "citeRegEx": "Hu and Jen.Chao.,? \\Q1964\\E", "shortCiteRegEx": "Hu and Jen.Chao.", "year": 1964}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["Jaeger", "Herbert", "Haas", "Harald"], "venue": null, "citeRegEx": "Jaeger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 2004}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Jordan", "Michael I"], "venue": "Technical Report 8604,", "citeRegEx": "Jordan and I.,? \\Q1986\\E", "shortCiteRegEx": "Jordan and I.", "year": 1986}, {"title": "Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks", "author": ["Kane", "Michael J", "Price", "Natalie", "Scotch", "Matthew", "Rabinowitz", "Peter"], "venue": "BMC bioinformatics,", "citeRegEx": "Kane et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kane et al\\.", "year": 2014}, {"title": "Comparison of arima and random forest time series models for prediction of avian influenza h5n1 outbreaks", "author": ["Kane", "Michael J", "Price", "Natalie", "Scotch", "Matthew", "Rabinowitz", "Peter"], "venue": "BMC bioinformatics,", "citeRegEx": "Kane et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kane et al\\.", "year": 2014}, {"title": "Draw: a recurrent neural network for image generation", "author": ["G Karol", "I Danihelka", "A Graves", "D Rezende", "D. Wierstra"], "venue": null, "citeRegEx": "Karol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karol et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A data-mining approach to predict influent quality", "author": ["Kusiak", "Andrew", "Verma", "Anoop", "Wei", "Xiupeng"], "venue": "Environmental monitoring and assessment,", "citeRegEx": "Kusiak et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kusiak et al\\.", "year": 2013}, {"title": "Nonlinear signal processing using neural networks: prediction and system modelling", "author": ["A Laepes", "R. Farben"], "venue": "Technical report,", "citeRegEx": "Laepes and Farben,? \\Q1987\\E", "shortCiteRegEx": "Laepes and Farben", "year": 1987}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["L\u00e4ngkvist", "Martin", "Karlsson", "Lars", "Loutfi", "Amy"], "venue": "Pattern Recognition Letters,", "citeRegEx": "L\u00e4ngkvist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "L\u00e4ngkvist et al\\.", "year": 2014}, {"title": "Learning to diagnose with lstm recurrent neural networks", "author": ["Lipton", "Zachary C", "Kale", "David C", "Elkan", "Charles", "Wetzell", "Randall"], "venue": "arXiv preprint arXiv:1511.03677,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "A chronology of interpolation: From ancient astronomy to modern signal and image processing", "author": ["Meijering", "Erik"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Meijering and Erik.,? \\Q2002\\E", "shortCiteRegEx": "Meijering and Erik.", "year": 2002}, {"title": "Predicting time series with support vector machines", "author": ["M\u00fcller", "K-R", "Smola", "Alexander J", "R\u00e4tsch", "Gunnar", "Sch\u00f6lkopf", "Bernhard", "Kohlmorgen", "Jens", "Vapnik", "Vladimir"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "M\u00fcller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 1997}, {"title": "Application of critical support vector machine to time series prediction. circuits and systems. iscas03", "author": ["T Raicharoen", "C Lursinsap", "P. Sanguanbhoki"], "venue": "In Proceedings of the 2003 International Symposium on,", "citeRegEx": "Raicharoen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Raicharoen et al\\.", "year": 2003}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["Ranzato", "MarcAurelio", "Szlam", "Arthur", "Bruna", "Joan", "Mathieu", "Michael", "Collobert", "Ronan", "Chopra", "Sumit"], "venue": "arXiv preprint arXiv:1412.6604,", "citeRegEx": "Ranzato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2014}, {"title": "Correcting forecasts with multifactor neural attention", "author": ["Riemer", "Matthew", "Vempaty", "Aditya", "Calmon", "Flavio P", "Heath III", "Fenno F", "Hull", "Richard", "Khabiri", "Elham"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Riemer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riemer et al\\.", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Mike", "Paliwal", "Kuldip K"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Neural networks as forecasting experts: an empirical test", "author": ["Sharda", "Ramesh", "R. Patil"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "Sharda et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Sharda et al\\.", "year": 1990}, {"title": "The summation of random causes as the source of cyclic processes", "author": ["Slutzky", "Eugen"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "Slutzky and Eugen.,? \\Q1937\\E", "shortCiteRegEx": "Slutzky and Eugen.", "year": 1937}, {"title": "Modeling multiple time series with applications", "author": ["Tiao", "George C", "Box", "George EP"], "venue": "journal of the American Statistical Association,", "citeRegEx": "Tiao et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Tiao et al\\.", "year": 1981}, {"title": "Co, {NO2} and {NOx} urban pollution monitoring with on-field calibrated electronic nose by automatic bayesian regularization", "author": ["Vito", "Saverio De", "Piga", "Marco", "Martinotto", "Luca", "Francia", "Girolamo Di"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "Vito et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vito et al\\.", "year": 2009}, {"title": "On periodicity in series of related terms", "author": ["Walker", "Gilbert"], "venue": "Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character,", "citeRegEx": "Walker and Gilbert.,? \\Q1931\\E", "shortCiteRegEx": "Walker and Gilbert.", "year": 1931}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["SHI Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "DitYan", "Wong", "Wai-Kin", "Woo", "Wang-chun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xingjian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xingjian et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "S et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S et al\\.", "year": 2015}, {"title": "An investigation of neural networks for linear time-series forecasting", "author": ["Zhang", "Guoqiang Peter"], "venue": "Computers & Operations Research,", "citeRegEx": "Zhang and Peter.,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Peter.", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "Even though our approach is general in the sense that it can be applied to any sequence-to-sequence RNN, we consider in this study bidirectional RNNs (Schuster & Paliwal, 1997) based on Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) with peephole connections (Gers et al., 2002), known to perform very well in practice (Graves, 2013), and we also make use of the attention mechanism recently introduced in (Bahdanau et al.", "startOffset": 284, "endOffset": 303}, {"referenceID": 0, "context": ", 2002), known to perform very well in practice (Graves, 2013), and we also make use of the attention mechanism recently introduced in (Bahdanau et al., 2014).", "startOffset": 135, "endOffset": 158}, {"referenceID": 11, "context": "The encoder represents each input xj , 1 \u2264 j \u2264 T as a hidden state \u2212\u2192 hj = f(xj , \u2212\u2212\u2192 hj\u22121), \u2212\u2192 hj \u2208 R, where the function f corresponds here to the non-linear transformation implemented in LSTM with peephole connections (Gers et al., 2002).", "startOffset": 221, "endOffset": 240}, {"referenceID": 0, "context": "More recently, in (Bahdanau et al., 2014), an attention mechanism is used to construct different context vectors ci for different outputs yi (1 \u2264 i \u2264 T \u2032) as a weighted sum of the hidden states of the encoder representing the input history: ci = \u2211T j=1 \u03b1ijhj , where \u03b1ij are the attention weights.", "startOffset": 18, "endOffset": 41}, {"referenceID": 38, "context": "For AQ, we selected the four variables associated to real sensors (thus excluding nominal sensors), namely C6H6(GT), NO2(GT), CO(GT) and NOx(GT) and predict the first one, C6H6(GT), as it is the most important for NMHC-related air pollution (Vito et al., 2009).", "startOffset": 241, "endOffset": 260}, {"referenceID": 6, "context": "Neural networks are regarded as a promising tool for time series prediction (Zhang, 2001; Crone et al., 2011) due to their data-driven and self-adaptive nature, their ability to approximate any continuous function and their inherent non-linearity.", "startOffset": 76, "endOffset": 109}, {"referenceID": 29, "context": "With the advent of SVM, it has been used to formulate time series prediction as a regression estimation using Vapnik\u2019s insensitive loss function and Huber\u2019s loss function (M\u00fcller et al., 1997).", "startOffset": 171, "endOffset": 192}, {"referenceID": 30, "context": "Since then different versions of SVMs are applied for time series prediction and many different SVM forecasting algorithms have been derived (Raicharoen et al., 2003; Fan et al., 2006).", "startOffset": 141, "endOffset": 184}, {"referenceID": 9, "context": "Since then different versions of SVMs are applied for time series prediction and many different SVM forecasting algorithms have been derived (Raicharoen et al., 2003; Fan et al., 2006).", "startOffset": 141, "endOffset": 184}, {"referenceID": 24, "context": "Random forest regression is used for prediction in the field of finance (Creamer & Freund, 2004) and bioinformatics (Kusiak et al., 2013), and are shown to outperform ARIMA (Kane et al.", "startOffset": 116, "endOffset": 137}, {"referenceID": 4, "context": "Early work (Connor et al., 1991) has shown that RNNs (a) are a type of nonlinear autoregressive moving average (NARMA) model and (b) outperform feedforward networks and various types of linear statistical models on time series.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "Subsequently, various RNN-based models were developed for different time series, as noisy foreign exchange rate prediction (Giles et al., 2001), chaotic time series prediction in communication engineering (Jaeger & Haas, 2004) or stock price prediction (Hsieh et al.", "startOffset": 123, "endOffset": 143}, {"referenceID": 16, "context": ", 2001), chaotic time series prediction in communication engineering (Jaeger & Haas, 2004) or stock price prediction (Hsieh et al., 2011).", "startOffset": 117, "endOffset": 137}, {"referenceID": 26, "context": "A detailed review can be found in (L\u00e4ngkvist et al., 2014) on the applications of RNNs along with other deep learning based approaches for different time series prediction tasks.", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "They have furthermore been shown to outperform traditional RNNs on various temporal tasks (Gers et al., 2001; 2002).", "startOffset": 90, "endOffset": 115}, {"referenceID": 31, "context": "More recently, they have been used for predicting the next frame in a video and for interpolating intermediate frames (Ranzato et al., 2014), for forecasting the future rainfall intensity in a region (Xingjian et al.", "startOffset": 118, "endOffset": 140}, {"referenceID": 41, "context": ", 2014), for forecasting the future rainfall intensity in a region (Xingjian et al., 2015), or for modeling clinical data consisting of multivariate time series of observations (Lipton et al.", "startOffset": 67, "endOffset": 90}, {"referenceID": 27, "context": ", 2015), or for modeling clinical data consisting of multivariate time series of observations (Lipton et al., 2015).", "startOffset": 94, "endOffset": 115}, {"referenceID": 0, "context": "Adding attention mechanism on the decoder side of an encoder-decoder RNN framework enabled the network to focus on the interesting parts of the encoded sequence (Bahdanau et al., 2014).", "startOffset": 161, "endOffset": 184}, {"referenceID": 22, "context": ", 2015), image generation (Karol et al., 2015), phoneme recognition (Chorowski et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 3, "context": ", 2015), phoneme recognition (Chorowski et al., 2015), heart failure prediction (Choi et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 32, "context": ", 2016), as well as time series prediction (Riemer et al., 2016) and classification (Choi et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "Many studies also apply attention mechanism on external memory (Graves et al., 2014; 2016).", "startOffset": 63, "endOffset": 90}, {"referenceID": 32, "context": "The previous work (Riemer et al., 2016) uses attention mechanism to determine the importance of a factor among other factors that affect time series.", "startOffset": 18, "endOffset": 39}], "year": 2017, "abstractText": "In this paper, we study the use of recurrent neural networks (RNNs) for modeling and forecasting time series. We first illustrate the fact that standard sequence-to-sequence RNNs neither capture well periods in time series nor handle well missing values, even though many real life times series are periodic and contain missing values. We then propose an extended attention mechanism that can be deployed on top of any RNN and that is designed to capture periods and make the RNN more robust to missing values. We show the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "creator": "LaTeX with hyperref package"}}}