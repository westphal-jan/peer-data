{"id": "1704.00045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Comparison of ontology alignment algorithms across single matching task via the McNemar test", "abstract": "The results require the alignment produced by a method and the reference alignment that contains the underlying actual matches of the given ontologies. Furthermore, the current trend in alignment evaluation is to present a new value and compare different alignments by comparing their performance values. However, it is much more provocative to select one performance value among others for comparison. Moreover, the assertion that one method performs better than another cannot be supported solely by comparing the values. In this paper, we propose the statistical procedures that allow us to theoretically favor one method over another. McNemar test is considered a reliable and appropriate means of comparing two ontology alignment methods over a suitable task.", "histories": [["v1", "Wed, 29 Mar 2017 15:20:01 GMT  (387kb)", "http://arxiv.org/abs/1704.00045v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["majid mohammadi", "amir ahooye atashin", "wout hofman", "yaohua tan"], "accepted": false, "id": "1704.00045"}, "pdf": {"name": "1704.00045.pdf", "metadata": {"source": "META", "title": "Comparison of ontology alignment algorithms across single matching task via the McNemar test", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 04\n5v 1\n[ cs\n.A I]\n2 9\nM ar\n2 01\n7\nComparison of ontology alignment algorithms across single matching task via the McNemar test\nMAJID MOHAMMADI, Del University of Technology AMIR AHOOYE ATASHIN, Ferdowsi University of Mashhad WOUT HOFMAN, TNO YAOHUA TAN, Del University of Technology\nOntology alignment is widely used to nd the correspondences between di erent ontologies in diverse elds. A er discovering the alignment by methods, several performance scores are available to evaluate them. e scores require the produced alignment by amethod and the reference alignment containing the underlying actual correspondences of the given ontologies.\ne current trend in alignment evaluation is to put forward a new score and to compare various alignments by juxtaposing their performance scores. However, it is substantially provocative to select one performance score among others for comparison. On top of that, claiming if one method has a be er performance than one another can not be substantiated by solely comparing the scores. In this paper, we propose the statistical procedures which enable us to theoretically favor one method over one another. e McNemar test is considered as a reliable and suitable means for comparing two ontology alignment methods over one matching task. e test applies to a 2 \u00d7 2 contingency table which can be constructed in two di erent ways based on the alignments, each of which has their own merits/pitfalls. e ways of the contingency table construction and various apposite statistics from the McNemar test are elaborated in minute detail. In the case of having more than two alignment methods for comparison, the family-wise error rate is expected to happen. us, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar test in the presence of multiple alignment methods. From this graph, it is readily understood if one method is be er than one another or if their di erences are imperceptible. Our investigation on the methods participated in the anatomy track of OAEI 2016 demonstrates that AML and CroMatcher are the top two methods and DKP-AOM and Alin are the bo om two ones. Moreover, the Levenstein and N-gram string-based distances discover the most correspondences while SMOA and Hamming distance are the ones with the least found correspondences.\nAdditional Key Words and Phrases: ontology alignment; McNemar test; family-wise error rate; anatomy;\nACM Reference format: Majid Mohammadi, Amir Ahooye Atashin, Wout Hofman, and Yaohua Tan. 2017. Comparison of ontology alignment algorithms across single matching task via the McNemar test. ACM Trans. Knowl. Discov. Data. 0, 0, Article 0 ( 2017), 16 pages. DOI: 0000001.0000001"}, {"heading": "1 INTRODUCTION", "text": "With the boom in information technology, data these days come from various sources. Such data have multiple salient but unwelcome features: they are big, dynamic and heterogeneous. ere are solutions to cope with any of these features, and ontology alignment (or mapping/matching) is the remedy to data heterogeneity (Euzenat et al. 2007). Given the source and target ontologies for alignment, a correspondence is de ned as the mapping of one concept\nACM acknowledges that this contribution was authored or co-authored by an employee, or contractor of the national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. Permission to make digital or hard copies for personal or classroom use is granted. Copies must bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. To copy otherwise, distribute, republish, or post, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2017 ACM. 1556-4681/2017/0-ART0 $15.00 DOI: 0000001.0000001\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nin the source to one concept in the target ontology. For discovering correspondences, it is typical to utilize a similarity measure. ere are three di erent similarity measure categories. e rst category is the string-based measure which only considers the text of concepts to declare their similarities (Cohen et al. 2003; Levenshtein 1966; Stoilos et al. 2005). Another category is the linguistic-based similarity measures which consider the linguistic relations, e.g. synonym, antonym, hypernym, etc., between the strings of two concepts. e linguistic-based similarity measures usually take advantages of WordNet (Miller 1995) to discover the similarity. e third category is the structural-based measures which take into account the position of the concepts in their ontologies. Traditionally, the challenge of ontology alignment was to come up with a new similarity measure and then to\nnd the interrelation between the ontologies (Stoilos et al. 2005). However, this focus has moved to take advantages of various similarity measures and try to reason the correspondences based on outcomes of di erent similarity measures (Jan et al. 2012; Nagy et al. 2006). An alignment, which is the result of any standard ontology matching algorithm, comprises a set of correspondences, mapping various concepts of one ontology to one another. It is the common practice to nd the goodness of an alignment method by comparing its output with the actual alignment which is in hand. However, it is controversial to select the appropriate performance score in di erent cases. On top of that, claiming the superiority of method against one another cannot be substantiated by merely comparing the obtained scores. In this article, the appropriate procedures are put forward to statistically opt for one method if it has actually be er performance than the other. However, the claim of superior performance must be treated with caution as the no free lunch theorem suggests. According to the no free lunch theorem (Wolpert 2012;Wolpert and Macready 1997), there is no context-independent reason to favor one strategy (or optimization method) over one another. e average performances of all strategies over all possible problems are the same. It is drawn, as a result, that the superior performance of one method over one another is due to its be er tness to the nature of the problem, not because of its inherent features. Any claim of performing the best in a general sense must be questioned and faced with doubts.\ne no free lunch theorem is rstly introduced in the supervised machine learning realm (Wolpert 1996), but it is generalized to any optimization problem a erward (Wolpert and Macready 1997). erefore, the results of the no free lunch theorem are also correct for the ontology alignment algorithms, and the preferred alignment method can be only discovered in one particular context. To date, the a empt of claiming if one method is be er than one another has been solely concentrated on employing a new performance score (e.g. precision, recall, etc.) (Ehrig and Sure 2004; Euzenat 2007; Ritze et al. 2013). If there are multiple pairs of ontologies for comparison, the superiority of a method is concluded only if its average performance across multiple pairs of ontologies is higher than the rest. Statistically speaking, the average performance is unsafe and inappropriate: it is highly sensitive to the outliers and having higher average performance does not necessarily indicate the superiority (the di erence might be imperceptible and insigni - cant) (Dems\u030car 2006). In the case of existing only one pair of ontologies, on the other hand, the comparison is merely performed by the juxtaposition of the performance scores of the methods. As a complement to the no free lunch theorem, this article aims to consider the statistical hypothesis testing to nd the best ontology alignment on a particular task. Employing the appropriate statistical test, one can determine if one alignment method outperforms one another with substantial statistical evidence. Instead of comparing one alignment with the reference one, the methodology proposed here takes the reference alignment along with two alignments under comparison as the inputs and states if one alignment statistically outperforms the other. us, the outcome of the methodology in this article is not a score but the statement of superiority of an alignment in comparison with one another.\ne McNemar test is the statistical means by which the various methods of alignment over one matching task can be compared. is test can be applied to the paired nominal data summarized in a contingency table with a dichotomous trait. Summing up the results of alignments in a contingency table would be challenging andmight\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nerupt discussions. We present two ways to build such a contingency table whose applicabilities is conceptually similar to those of recall and F-measure. Further, four statistics from the McNemar family are considered, and their advantages and pitfalls are discussed. In the case of having two methods for comparison, the McNemar test can be simply applied. If more than two alignments are available, all pairwise comparisons must be performed. In this case, the family-wise error rate (FWER) is likely to happen and must be controlled. e appropriate procedures for the FWER prevention are elaborated as well. We leverage the proposed methodology across the anatomy track 2016, and the corresponding results are visualized by a directed graph. is graph indicates if the di erence between each pair of methods are signi cant or not. AML and Cromatcher have shown the best performances, and DKP-AOM and Alin are the ones with reduced accomplishment. We further compare the string-based similarity measures over this track because many correspondences can be easily discovered by comparing the strings. e N-gram and Levenstein distances are the ones with the maximum discovery with respect to others.\nis paper is organized as follows. e ways of the contingency table construction are brought in Section II, and the appropriate statistics from the McNemar test are discussed in Section III. e family wise error rate and the ways of adjusting the p-values are studied in Section IV. Section V dedicates to the experiments of the statistical procedures over the anatomy track, and the paper is concluded in Section VI."}, {"heading": "2 CONTINGENCY TABLE CONSTRUCTION", "text": "e McNemar test is applicable when there are two experiments over N samples (McNemar 1947). Let the\noutcome of each test be either positive or negative; then a simple contingency table would be as Table 1.\nIn this table, n00 and n11 are called the accordant pair and are respectively the number of experiments when both experiments produce positive and negative outcomes. e discordant pair, i.e. n01 and n10, are the number of experiments when the results of experiments are in contradiction; n01 is the number of experiments which the rst outcome is negative while the second one is positive and n10 is the other way around. In ontology matching case, the positive or negative outcome for experiments can be de ned in two ways, each of which has its own merits and is suitable for particular situations. For given two ontologies, let R be the reference alignment containing a set of correct correspondences and A1 and A2 be two alignments retrieved by two di erent methods. In the rst approach of the contingency table construction, the focus is on the truly discovered alignments, thereby ignoring the concepts which have not correctly mapped. Hence, n00 andn11 are respectively the number of correspondences in the reference alignment R which are not in both alignments A1 and A2 and the number of correspondences which are in the reference and both alignments. n01 (and similarly n10) is the number of correspondences truly discovered by A2, but not by A1. ese elements can be wri en as\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\n  n00 = |R \u2212 (A1 \u222aA2)| n01 = |(A2 \u2229 R) \u2212A1 | n10 = |(A1 \u2229 R) \u2212A2 | n11 = |A1 \u2229 A2 \u2229 R |\n(1)\nwhere |.| indicates the cardinality operator. is approach is conceptually similar to recall as it does not consider the number of correspondences which are falsely discovered by methods. We again accent that the approach of this article is di erent than the performance scores, including recall, as we compare two alignments and do not produce any score indicating the neness of the alignments. An example elaborates the issue of this approach. Assume that both methods can discover the complete reference alignment, i.e. A1 = A2 = R. In this case, n01 = n10 = 0 which means that the both methods have performed equally well (it is discussed in further sections that n01 and n10 are the only important pair for the McNemar test). Now, suppose that A1 = R and A2 = R + B, where B is a set of correspondences which are not in R (falsely discovered by A2). In this case, n01 is the same as n10 which again indicates that the two methods perform equally well. However, it is plain to grasp that A1 is a more reliable method as it does not mistakenly discover any correspondences. Statistically speaking, this approach does not take into account the false positive and only considers the true positive. Nonetheless, such an approach is suitable for occasions where the goal is to have as many correspondences as possible so that the false discovery does not have a profound impact.\ne second approach of building the contingency table avoids the aforementioned pitfall and consider the false discovery as well. As it considers the truly unmapped pairs of concepts, obtaining the elements of the contingency table is of higher complexity in comparison with the previous approach. erefore, it is necessary to explain how to obtain each element of the table individually. n00 is the number of correspondences which are wrongly discovered by both alignments. Hence it includes the correspondences which are in R but not in A1 or A2 plus the correspondences which are in both A1 and A2 but not in R, i.e. n00 = |R \u2212 (A1 \u222a A2)| + |(A1 \u2229 A2) \u2212 R |. n10 is the number of truly discovered correspondences by A1 which are not in A2 plus the correspondences which are falsely identi ed only by A2 and not by A1, i.e. n10 = |(A1\u2229R) \u2212A2 | + |A2 \u2212A1 \u2212R |. With the same token, n01 can also be obtained. n11 is a bit more challenging as the total number of possible correspondences between two ontologies is required. Let this number beT , one possibility for T is to multiply the number of concepts of two ontologies, i.e. T = n \u00d7m where n andm are the numbers of candidate concepts for matching in two ontologies. us, n11 = |A1\u2229A2\u2229R |+ |T \u2212(A1\u222aA2\u2212R)|. e statistics considered in this paper only need the discordant pair; therefore the value of n11 and subsequently T is not taken into account. e elements as mentioned earlier of the contingency table from the second approach can be summarized as:\n  n00 = |R \u2212 (A1 \u222a A2)| + |(A1 \u2229 A2) \u2212 R | n01 = |(A2 \u2229 R) \u2212A1 | + |A1 \u2212A2 \u2212 R | n10 = |(A1 \u2229 R) \u2212A2 | + |A2 \u2212A1 \u2212 R | n11 = |A1 \u2229 A2 \u2229 R | + |T \u2212 (A1 \u222a A2 \u2212 R)|\n(2)\nis way of the contingency table construction considers the falsely discovered correspondences of methods.\nNote that this calculation is relative to the other method. In other words, it does not consider all the incorrectly identi ed correspondences, but the false correspondences are considered which are not in the rival method. As the goal is to compare two alignments together, it is entirely rational to nd the relative false positive. is approach can be guratively viewed as similar to F-measure due to its consideration of both true and false discoveries.\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017."}, {"heading": "3 MCNEMAR TEST", "text": "Having built the contingency table, it is time to run the McNemar test. Before elaborating the McNemar test, we digress a li le to explain the null hypothesis testing. To leverage any statistical test, the null and alternative hypotheses are required. e null hypothesis H0 states that the di erence between two populations is insigni cant, and this di erence is due to the sampling or experimental errors (Sheskin 2003). e alternative hypothesis, on the other hand, states the contrary: the di erence between two populations is signi cant and not random. To reject or retain H0, we need to compute the p-value and compare it with signi cant level \u03b1 which must be determined before running the test. e p-value is the probability of obtaining a result equal to, or even more extreme than the observations (Sheskin 2003). If the p-value is less than the nominal signi cant level \u03b1 , then the null hypothesis is rejected, and it is drawn that the di erence between populations is signi cant. In the comparison of ontology alignment methods, the populations mentioned above are the outcomes of two methods. erefore, the null hypothesis is that the di erence between the outcomes of alignment methods is random and insigni cant. e null hypothesis in the McNemar test states that the two marginal probabilities of the contingency table are the same, i.e.\np(n00) + p(n01) = p(n00) + p(n10)\np(n10) + p(n11) = p(n01) + p(n11) (3)\nwhere p(a) indicates the probability of occurring the cell of Table 1 with the label a. A er canceling out the p(n00) and p(n11) from the aforementioned equations, the null and alternative hypotheses are obtained as\nH0 : p(n01) = p(n10)\nHa : p(n01) , p(n10). (4)\nTo obtain the p-value of the null hypothesis (4), we consider four statistics from the McNemar family and discuss their advantages and pitfalls in the hypothesis testing. e statistics studied here only work with the accordant pair of the contingency table. However, there is also an exact unconditional McNemar test which takes into account the discordant pair of the contingency table (Suissa and Shuster 1991). e exact unconditional test is way more intricate than the McNemar tests put forward here, but its power is approximately the same as other tests (Fagerland et al. 2013). erefore, this test is ignored in this paper."}, {"heading": "3.1 The asymptotic McNemar test", "text": "easymptoticMcNemar test assumes thatn01 is binomially distributed withp = 0.5 and parametersn = n01+n10 under the null hypothesis (McNemar 1947). e asymptotic McNemar test statistic\n\u03c72 = (n01 \u2212 n10)\n2\nn01 \u2212 n10\nis distributed according to \u03c72 with one degree of freedom. is test is unde ned for n01 = n10 = 0. To reject the null hypothesis, this test requires a su cient number of data (n01 + n10 \u2265 25) because it might violate the nominal signi cant level \u03b1 for the small sample size."}, {"heading": "3.2 The McNemar exact test", "text": "It is traditionally advised to use the McNemar exact test when a small sample size is available in order not to exceed the nominal signi cant level. In this test, n01 is compared to a binomial distribution with parameter\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nn = n01 + n10 and p = 0.5. us, the p-value for this test is obtained as\nexact-p-value =\nn\u2211\nx=n01\n( n\nx\n) ( 1\n2\n)2\ne one-sided p-value can be multiplied by two to obtain the two-sided p-value. is test guarantees to have\ntype I error rate below the nominal signi cant level \u03b1 ."}, {"heading": "3.3 The asymptotic McNemar test with continuity correction", "text": "emain drawback of the McNemar exact test, though preserving the nominal signi cant level, is conservatism: it unnecessarily generates large p-values so that the null hypothesis cannot be rejected. As a remedy to conservatism, Edwards (Edwards 1948) approximated the exact p-value by the following continuity corrected statistic\n\u03c72 = (|b \u2212 c | \u2212 1)2\nb + c\nwhich is \u03c72-distributed with one degree of freedom. is test is also unde ned for n01 = n10 = 0."}, {"heading": "3.4 The McNemar mid-p test", "text": "e continuity corrected method is not as conservative as the McNemar exact test, but it does not guarantee to preserve the nominal signi cant level. e mid-p approach propounds a way to trade o between the conservatism of the exact test and the signi cant level transgression of the continuity correction (Lancaster 1961). To obtain the mid-p-value, a simple modi cation is required: the mid-p-value equals the exact p-value minus half the point probability of the observed test statistic (Fagerland et al. 2013). Hence, the p-value is obtained as\nmid-p-value = 2-sided exact p-value \u2212\n( n\nn01\n) 0.5n .\ne McNemar mid-p test resolves the conservatism of the exact test, but it does not theoretically guarantee to preserve the nominal signi cant level. In a recent study, however, it is investigated that the mid-p test has low type I error and does not violate the signi cant level. e continuity corrected test, in contrast, indicated the high type I error, coming from the nature of asymptotic tests, as well as high type II error, inherited from the exact test. us, it is rational not to use the continuity corrected test for the alignment comparison."}, {"heading": "4 FAMILY-WISE ERROR RATE AND P-VALUE ADJUSTMENT", "text": "When there are two methods for comparison, the null hypothesis will be rejected if the obtained p-value is below the nominal signi cant level \u03b1 . If more than two alignment algorithms are available for comparison, the well-known family-wise error rate (FWER) might happen. FWER refers to the increase in the probability of type I error which is likely to violate the nominal signi cant level \u03b1 when multiple populations are to be compared. To explain what FWER is, assume that there are 5 methods for comparison and the signi cant level is \u03b1 = 0.05. If it is desired to do all the pairwise comparisons, then there are k = 5 \u00d7 4/2 = 10 hypotheses overall. For each of the null hypothesis, the probability of rejection without occurring the type I error is 1\u2212\u03b1 = 0.95. For all comparisons, on the other hand, the probability of not having any type I error in all the hypotheses is (0.95)10 = 0.6. As a result, the probability of occurring type I error increases to 1 \u2212 0.6 = 0.4, which is way higher than the nominal \u03b1 = 0.05. is phenomenon is the so-called family-wise error rate. To prevent this error, there are two primary approaches. Akin to the above example, the rst approach is applicable when all the pairwise comparisons are desired. Conducting all pairwise comparisons are suitable when a comparison study of the existing methods in the literature or a comparison of methods in a competition like\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nOAEI is desired. Another approach to control FWER is convenient when a new alignment method is proposed and it is to be compared with other existing algorithms. For the sake of simplicity, the former approach is called N \u00d7 N comparisons and the la er is called N \u00d7 1 comparisons."}, {"heading": "4.1 Controlling FWER in N \u00d7 1 comparison", "text": "When a new alignment method is proposed, it is usually compared with other state-of-the-art alignments. For comparing n methods (including the proposed one) in this case, k = n \u2212 1 comparisons must be performed.\nere are four methods which can control the family-wise error rate in N \u00d7 1 comparison. ese methods can be viewed as the p-value adjustment procedures which modify the p-values in a way that the adjusted p-values (APV) can be directly compared with the signi cant level while the nominal signi cant level is also preserved.\nus, a null hypothesis is rejected if its corresponding adjusted p-value be below the nominal \u03b1 .\nLetHi , i = 1, ...,k be all hypotheses for nmethods andpi , i = 1, ...,k be their corresponding p-values. e Bonferroni\u2019s method (Dunn 1961) is the most straightforward way to prevent FWER. In this procedure, all the p-values are compared with the nominal signi cant level \u03b1 divided by the total number of comparisons. In other words, the hypothesis Hi is rejected if pi < \u03b1/k . Based on this equation, the adjusted p-value for the Hi hypothesis is obtained by multiplying both sides of above inequality by k , i.e. APVi = min{k \u00d7 pi , 1}. us, Hi is rejected if APVi < \u03b1 . is procedure, though simple, is too conservative: it retains the hypotheses which must be rejected by generating high APV. Contrary to the single step Bonferroni correction, there are step-up and step-down procedures which sequentially reject the null hypothesis. It is necessary to order p-values for sequential rejective procedures and we denote the ordered p-values as p1 \u2264 p2 \u2264 ... \u2264 pk and their corresponding hypotheses are H1,H2, ...,Hk .\ne Holm\u2019s procedure (Holm 1979) is a step-down method which starts with the most signi cant (or the small-\nest) p-value p1. If p1 \u2264 \u03b1 k , then H1 is rejected, and p2 is compared with \u03b1 k\u22121 . If p2 \u2264 \u03b1 k\u22121 , then H2 is rejected, and p3 is compared with \u03b1\nk\u22122 . is procedure continues until an hypothesis is retained. In other words, each\npi in the Holm\u2019s method is compared with \u03b1\nk+1\u2212i and it is rejected if it is below this value, otherwise it is not\nrejected and the rest hypotheses are retained as well. e Holm adjusted p-value is APVi = min{vi , 1} where vi =max{(k \u2212 j)pj : 1 \u2264 j \u2264 i}. Similar to the Holm\u2019s procedure, the Holland\u2019s procedure (Holland and Copenhaver 1987) is also a step-down method. Instead of comparing the p-values with \u03b1 k+1\u2212i , it compares each pi with 1\u2212(1\u2212\u03b1) k\u2212i . us, the adjusted p-value is APVi = min{vi , 1} where vi = max{1 \u2212 (1 \u2212 pj ) k+1\u2212j : 1 \u2264 j \u2264 i}. e Finner\u2019s procedure (Finner 1993) is almost the same as the Holland\u2019s procedure and compares each pi with 1\u2212(1\u2212\u03b1) k i . e Finner adjusted p-value is APVi = min{vi , 1} where vi = max{1 \u2212 (1 \u2212 pj ) k j : 1 \u2264 j \u2264 i}.\ne Hochberg\u2019s procedure (Hochberg 1988) works in the opposite direction and starts with the largest p-value. It compares the largest p-value with \u03b1 , the next largest with \u03b1/2 and it is terminated until a hypothesis can be rejected. All the hypotheses with the smaller p-values are then rejected as well. e Hochberg adjusted p-value is APVi = max{(k \u2212 j)pj : (k \u2212 1) \u2265 j \u2265 i}."}, {"heading": "4.2 Controlling FWER in N \u00d7 N comparison", "text": "For performing all the pairwise comparisons when n methods are available, there are k = n(n \u2212 1)/2 hypotheses overall. e Nemenyi\u2019s method (Nemenyi 1963) is the Bonferroni\u2019s methodwith k is set to theN \u00d7N comparison, i.e. k = n(n \u2212 1)/2. us, it has high type II error which results in not detecting the di erence among the population when there is a de facto di erence. e same modi cation of k must be applied to other methods so that they are suitable for N \u00d7 N comparison case.\nere is also another sequential-rejective null hypothesis approach which is suitable for N \u00d7N comparison and\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\ntakes into account the logical relations between hypotheses. Sha er (Sha er 1986) discovered that the Holm\u2019s procedure could be improved when hypotheses are logically interrelated. In many scenarios, it is not feasible to get any combination of true and false hypotheses. In the pairwise comparison among means, for instance, it is not possible to have \u00b51 = \u00b52 and \u00b52 = \u00b53 but \u00b51 , \u00b53. us, this case need not be protected against FWER. Correction procedures which take into account the logical relations are similar to the Holm\u2019s correction: they start with the most signi cant (or the smallest) p-value but compare it with \u03b1/t1, where t1 is the maximum number of hypotheses which can be retained at the rst step. If p1 < \u03b1/t1, then the corresponding hypothesis H1 is rejected, and p2 is compared with \u03b1/t2 . If H2 is rejected, then p3 is compared with \u03b1/t3 and so on. e procedure terminates at the stage j if Hj cannot be rejected. e remaining hypotheses with smaller p-values than pj are also retained. e adjusted p-value for the sequential corrective methods isAPVi = min{vi , 1} where vi = min{ti \u00d7 pi , 1}.\nere are two major methods which consider the logical relations of hypothesis: Sha er\u2019s and Bergmann\u2019s procedures. ese methods di er in their way to obtain the maximum number of true hypotheses at each level.\neHolm\u2019s procedure simply set the maximum number of true hypothesis at the stage j as the rest of hypotheses\na er the jth stage, i.e. tj = k \u2212 j + 1. In the Sha er\u2019s method (Sha er 1986), the possible numbers for true hypothesis and consequently tj is obtained by the following recursive formula\nS(k) =\nk\u22c3\nj=1\n{\n( 2\nj\n) + x : x \u2208 S(k \u2212 j)}\nwhere S(k) is the set of all possible numbers of true hypotheses when there are k alignments for comparison and S(0) = S(1) = 0. tj is simply computed based on the set S(k). Similar to the Sha er\u2019s method, the Bergmann\u2019s method (Bergmann and Hommel 1988) use the logical interrelations between the hypotheses but dynamically estimates the maximum number of true hypotheses at the stage j , given that j \u2212 1 hypotheses are rejected. To do so, they de ned the exhaustive which is an index set of hypotheses I \u2286 {1, ...,m} where exactly all the hypothesesHj , j \u2208 I can be true. en, any hypothesisHj is rejected if j < AwhereA is the acceptance set which is retained and de ned as\nA = \u22c3\n{I: I exhaustive,min{Pi : i \u2208 I } > \u03b1/|I |} (5)\ne Bergmann\u2019s method is the most powerful procedures when N \u00d7 N comparison is required. However,\nbuilding the exhaustive set is very time-consuming especially if more than nine methods are available for comparison."}, {"heading": "5 RESULTS", "text": "In this section, the aforementioned statistical procedures are applied to the anatomy track of OAEI 2016, and the corresponding results are reported. Further, di erent string similarity measures are compared and ranked according to the number of correct discovery. We have two ways of obtaining the contingency table, four McNemar tests and four ways to prevent the FWER.\nerefore there are totally 32 states for comparison. For the sake of simplicity (and probably for the exclusion of duplication), we only consider four states: the two ways of building the contingency table compared with the McNemar mid-p test and controlling FWER by the Nemenyi\u2019s and Bergmann\u2019s correction, the most conservative and the most robust methods. e underlying reason behind the mid-p test selection is that it is not as conservative as the exact test and it is less likely to violate the nominal signi cant level \u03b1 .\ne anatomy track has been a part of OAEI since 2011 and its aim is to nd the alignment between the Adult\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nMouse Anatomy and a part of the NCI esaurus related to the human anatomy. We select 10 methods participated in OAEI 2016 for conducting the comparison: Alin (da Silva 2016), AML (Faria et al. 2013), CroMatcher (Achichi et al. 2016a), DKP-AOM (Amrouch et al. 2016), FCA-Map (Zhao and Zhang 2016), Lily (Wang and Xu 2008), LogMapLite (Jime\u0301nez-Ruiz and Grau 2011), LPHOM (Megdiche et al. 2016), LYAM (Achichi et al. 2016b) and XMap (Djeddi and Khadir 2010).\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\ne contingency table is built by the two aforementioned methodologies. e values for n01 and n10 for the\nway of ignoring the false positive and considering it are brought in Tables 2 and 3. For the sake of simplicity, n01 and n10 are tabulated in one single table for each perspective (below and upper diagonal). To compare the ith and jth methods in each approach, (i, j) and (j, i) elements of this table is taken as n01 and n10, where (i, j) is the element at the ith row and jth column. For instance, let\u2019s compare the Alin and AML methods. In the\nrst perspective, n01 = 911 which means that there are 911 correspondences discovered by AML but not by Alin. And, n10 = 0 indicates that there are no correspondences which are in the Alin but are not in the AML alignment. In the second perspective, on the other hands, n01 = 917 and n10 = 72. Comparing with the previous view, n10 changes from 0 to 72 which means that AML has discovered 72 wrong correspondences while Alin has not. e li le increase in n01 is due to the false discovery rate of Alin (6 correspondences) in comparison to AML. As a result, it is grasped that the false discovery rate of Alin is less than AML while the true discovery rate of AML is way higher that Alin. If the McNemar test rejects the null hypothesis, then AML is concluded to have a be er performance than Alin due to its higher true discovery rate than Alin. e comparison of other methods can be conducted likewise which clari es the di erence between the two perspectives.\nFor the comparison study, we conduct all the pairwise comparisons. We take advantage of the Nemenyi\u2019s correction, the most conservative one, and Bergman\u2019s correction, the most powerful one, to control the family-wise error rate. We visualize the results using weighted graphs. Four di erent weighted graphs correspond to each perspective, and each correction methods are brought in Figures (1 - 4). e nodes in these graphs are the methods under study and any directed edge A\u2192 B means that the method A is signi cantly be er than the method B. If there is no such an edge, there is no signi cant di erence between the corresponding methods.\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nFirst, we compare the results obtained from the Nemenyi\u2019s and Bergman\u2019s correction from each perspective of the contingency table construction. Figures 2 and 1 are the weighted graphs corresponds to the pairwise comparisons of alignment methods obtained by applying respectively the Nemenyi\u2019s and Bergmann\u2019s correction under the rst perspective of contingency table construction. e results of these two correction methods are di erent in only one comparison: the Bergmann\u2019s correction indicates the signi cant di erence between Cromatcher and LYAM while the Nemenyi\u2019s correction method can not detect it. us, the Bergmann\u2019s correction is more powerful than the Nemenyi\u2019s correction as the theory suggests.\nIn the second approach which also considers the false positive, the Bergmann\u2019s correction method indicates its power in comparison with the Nemenyi\u2019s correction. It declares the di erence between FCA-Map LYAM and between LYAM and LogMapLite signi cant while the Nemenyi\u2019s correction cannot detect such di erences as signi cant. Now, we compare the two perspectives on the contingency table construction. To do so, the Bergmann\u2019s correction method is considered due to its ability to detect more di erences. Considering Fig. 2, it is readily seen that the LYAM and XMAP methods are not declared signi cant, but both of them are declared signi cant in comparison to FCA-MAP. If the false positive rate is taken into account, as in Fig. 4, FCA-MAP is replaced LYAM. To investigate such a replacement, Tables 2 and 3 must be considered. While the false positive rate is not considered, FCA-Map has 51 correct correspondences which are not in LYAM, and LYAM has 110 true correspondences which do not exist in FCA-MAP. However, when the false positive is also considered, the number of truly discovered correspondences by FCA-MAP which are not in the LYAM alignment increases to 220 while the number of truly discovered correspondences by LYAM which are not in FCA-MAP is 160. As a result, the LYAM ontology mapping is be er than FCA-MAP from the rst point of view, but FCA-MAP outperforms LYAM in the second approach because it has a lower false discovery rate in comparison with LYAM. e same\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nargument is also valid for the comparison of FCA-MAP and XAMP: if the falsely discovered correspondences are not taken into account, XAMP outperforms FCA-MAP while they are declared insigni cant when the false discovery error is considered as well. Another di erence between two perspectives on the contingency table construction is about the LogMapLite method. When the false discovery rate does not ma er, Lily outperforms LogMapLite, which is further declared insigni cant compared with LPHOM. If the false positive error is heeded, however, LogMapLite outperforms LPHOM and it is declared insigni cant with Lily. is indicates that LogMapLite has a lower false discovery rate than Lily and LPHOM. We nally rank the methods participated in OAEI 2016 in Table 4 based on the Bergmann\u2019s correction. e columns with labels IFP and CFP correspond to the contingency table construction with ignoring the false discovery (IFP) and considering (CFP) it. In this table, the methods in higher rows are ones which are signi cantly be er than the methods in lower rows. If two methods are not signi cantly di erent, they are placed in the same cell. It can be readily seen that AML and DKP-AOM are the best and the worst methods from two perspectives.\ne string-based similarity measures are of utmost importance in the anatomy track because many correspon-\ndences can be discovered if the right string-based similarity is employed. To compare various measures, we take advantage of Shiva framework (Mathur et al. 2014) which convert the ontology mapping into an assignment problem. In this framework, the similarity between each concept from the source ontology is gauged with all the concepts of the target ontology. e similarity score from the concepts of two ontologies construct a matrix, which can be given to the Hungarian algorithm (Munkres 1957) to nd the best match for each concept. We use nine string-based similarity measure to construct the matrix: Levenstein (Levenshtein 1966), N-gram (Kondrak 2005), Hamming (Euzenat et al. 2007), Jaro (Jaro 1995), JaroWinkler (Winkler 1999), SMOA (Stoilos et al. 2005), NeedlemanWunsch2 (Needleman and Wunsch 1970), Substring distance (Euzenat et al. 2007) and equivalence\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nmeasure. e Hungarian method applies to the resultant matrix to nd the best correspondence for each concept.\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nWe consider the case when the false positive is not taken into account for two reason. First, the string-based alignment is generally exploited to discover a rst-line matching (Marie and Gal 2007). e result of this measure is then improved by various methodologies to discover the so-called second-line matching. us, the goal of the\nrst-line matcher would be to nd as much true correspondences as possible. Second, we have not used any threshold to invalidate some correspondences. e Hungarian method surely assigns a concept from the source\nACM Transactions on Knowledge Discovery from Data, Vol. 0, No. 0, Article 0. Publication date: 2017.\nontology to one in the target ontology. erefore, there are many correspondences with low con dence which might invalidate even if a small threshold is applied. For these two reasons, we report the outcomes of the stringbased similarity measure with the neglect of the false positive. Similar to the previous ones, Table 5 tabulates n01 and n10 corresponding to di erent string-based similarity measures while the false positive is ignored. e results are visualized by a directed graph shown in Fig. 5. From this gure, N-gram has shown the best performances and is followed by Levenstein. Further, SMOA and Hamming distances are the ones with the least retrieved correspondences but they are be er than Substring and Equivalence measures as expected."}, {"heading": "6 CONCLUSION", "text": "is paper proposed the utilization the McNemar test to compare various ontology alignment methods over one single task. e current approach for the alignment comparison is to rstly select a performance score and then compare two methods by obtaining their performance scores on a task with reference alignment. In this article, the alignment produced by two methods as well as the reference alignment are given, and the outcome is if two methods are signi cantly di erent. us, the output is not a score, but to / not to declare the signi cance between two ontology matching algorithms. Further, e ways for preventing family-wise error rate, which is likely to happen in the comparison of multiple (> 2) alignment methods, are explored in minute detail. e proposed methodologies are applied to the anatomy track of ontology alignment initiative evaluation (OAEI) 2016. It is indicated that the AML and CroMatcher are the top two algorithms, and Alin and DKP-AOM are the worst alignment methods. For string-based measures, N-gram and Levenstein outperform other methods while SMOA and Hamming distance have shown poor performances."}], "references": [{"title": "2016a. Results of the Ontology Alignment Evaluation Initiative 2016", "author": ["Valentina Ivanova", "others"], "venue": "In 11th ISWC workshop on ontology", "citeRegEx": "Ivanova and others.,? \\Q2016\\E", "shortCiteRegEx": "Ivanova and others.", "year": 2016}, {"title": "2016b. Results of the Ontology Alignment Evaluation Initiative 2016", "author": ["Valentina Ivanova", "others"], "venue": "In 11th ISWC workshop on ontology", "citeRegEx": "Ivanova and others.,? \\Q2016\\E", "shortCiteRegEx": "Ivanova and others.", "year": 2016}, {"title": "Decision trees in automatic ontology matching", "author": ["Siham Amrouch", "Sihem Mostefai", "Muhammad Fahad"], "venue": null, "citeRegEx": "Amrouch et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amrouch et al\\.", "year": 2016}, {"title": "Multiple Hypothesenpr\u00fcfung/Multiple Hypotheses Testing", "author": ["William Cohen", "Pradeep Ravikumar", "Stephen Fienberg"], "venue": "Kdd", "citeRegEx": "Cohen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2003}, {"title": "ALIN Results for OAEI 2016", "author": ["Warith Eddine Djeddi", "Mohammed Tarek Khadir"], "venue": "workshop on data cleaning and object consolidation,", "citeRegEx": "Djeddi and Khadir.,? \\Q2016\\E", "shortCiteRegEx": "Djeddi and Khadir.", "year": 2016}, {"title": "Multiple comparisons among means", "author": ["Olive Jean Dunn"], "venue": "Machine and Web Intelligence (ICMWI),", "citeRegEx": "Dunn.,? \\Q2010\\E", "shortCiteRegEx": "Dunn.", "year": 2010}, {"title": "Ontology mapping\u2013an integrated approach", "author": ["Marc Ehrig", "York Sure"], "venue": "Psychometrika 13,", "citeRegEx": "Ehrig and Sure.,? \\Q1948\\E", "shortCiteRegEx": "Ehrig and Sure.", "year": 1948}, {"title": "be\u008aer than exact conditional", "author": ["Daniel Faria", "Catia Pesquita", "Emanuel Santos", "Ma\u008aeo Palmonari", "Isabel F Cruz", "Francisco M Couto"], "venue": "BMC medical research methodology 13,", "citeRegEx": "Faria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Faria et al\\.", "year": 2013}, {"title": "On a monotonicity problem in step-down multiple test procedures", "author": ["M. Mohammadi"], "venue": "Finner", "citeRegEx": "Mohammadi,? \\Q1993\\E", "shortCiteRegEx": "Mohammadi", "year": 1993}, {"title": "A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics", "author": ["417\u2013423. Sture Holm"], "venue": null, "citeRegEx": "Holm.,? \\Q1979\\E", "shortCiteRegEx": "Holm.", "year": 1979}, {"title": "Ma\u008ahew A Jaro", "author": ["Ernesto Jim\u00e9nez-Ruiz", "Bernardo Cuenca Grau"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42,", "citeRegEx": "Jim\u00e9nez.Ruiz and Grau.,? \\Q2012\\E", "shortCiteRegEx": "Jim\u00e9nez.Ruiz and Grau.", "year": 2012}, {"title": "Signi\u0080cance tests in discrete distributions", "author": ["115\u2013126. HO Lancaster"], "venue": "J. Amer. Statist. Assoc", "citeRegEx": "Lancaster.,? \\Q1961\\E", "shortCiteRegEx": "Lancaster.", "year": 1961}, {"title": "Managing uncertainty in schema matcher ensembles", "author": ["707\u2013710. Anan Marie", "Avigdor Gal"], "venue": "In International Conference on Scalable Uncertainty", "citeRegEx": "Marie and Gal.,? \\Q2007\\E", "shortCiteRegEx": "Marie and Gal.", "year": 2007}, {"title": "Shiva: A Framework for Graph Based Ontology Matching", "author": ["Management. Springer", "60\u201373. Iti Mathur", "Nisheeth Joshi", "Hemant Darbari", "Ajai Kumar"], "venue": null, "citeRegEx": "Springer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2014}, {"title": "LPHOM results for OAEI 2016", "author": ["Imen Megdiche", "Olivier Teste", "Cassia Trojahn"], "venue": "Ontology Matching", "citeRegEx": "Megdiche et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Megdiche et al\\.", "year": 2016}, {"title": "Dssim-ontology mapping with uncertainty", "author": ["Miklos Nagy", "Maria Vargas-Vera", "Enrico Mo\u008aa"], "venue": null, "citeRegEx": "Nagy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nagy et al\\.", "year": 2006}, {"title": "Distribution-free multiple comparisons", "author": ["Dominique Ritze", "Heiko Paulheim", "Kai Eckert"], "venue": "Journal of molecular biology 48,", "citeRegEx": "Ritze et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Ritze et al\\.", "year": 1970}, {"title": "Modi\u0080ed sequentially rejective multiple test procedures", "author": ["Press. Giorgos Stoilos", "Giorgos Stamou", "Stefanos Kollias"], "venue": "International Semantic Web Conference", "citeRegEx": "Stoilos et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Stoilos et al\\.", "year": 1986}, {"title": "\u008ce 2 x 2 matched-pairs trial: Exact unconditional design and analysis", "author": ["Springer", "624\u2013637. Samy Suissa", "Jonathan J Shuster"], "venue": "Biometrics", "citeRegEx": "Springer et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Springer et al\\.", "year": 1991}, {"title": "\u008ce lack of a priori distinctions between learning algorithms", "author": ["Citeseer. David H Wolpert."], "venue": "Neural computation 8, 7 (1996), 1341\u20131390. David H Wolpert. 2012. What the no free lunch theorems really mean; how to improve search algorithms. In Santa fe Institute Working", "citeRegEx": "Wolpert.,? 1996", "shortCiteRegEx": "Wolpert.", "year": 1996}, {"title": "No free lunch theorems for optimization", "author": ["Paper. 12. David H Wolpert", "William G Macready"], "venue": "IEEE transactions on evolutionary computation", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "FCA-Map Results for OAEI 2016", "author": ["Mengyi Zhao", "Songmao Zhang"], "venue": "Ontology Matching", "citeRegEx": "Zhao and Zhang.,? \\Q1997\\E", "shortCiteRegEx": "Zhao and Zhang.", "year": 1997}], "referenceMentions": [{"referenceID": 3, "context": "\u008ce \u0080rst category is the string-based measure which only considers the text of concepts to declare their similarities (Cohen et al. 2003; Levenshtein 1966; Stoilos et al. 2005).", "startOffset": 117, "endOffset": 175}, {"referenceID": 15, "context": "However, this focus has moved to take advantages of various similarity measures and try to reason the correspondences based on outcomes of di\u0082erent similarity measures (Jan et al. 2012; Nagy et al. 2006).", "startOffset": 168, "endOffset": 203}, {"referenceID": 7, "context": "We select 10 methods participated in OAEI 2016 for conducting the comparison: Alin (da Silva 2016), AML (Faria et al. 2013), CroMatcher (Achichi et al.", "startOffset": 104, "endOffset": 123}, {"referenceID": 2, "context": "2016a), DKP-AOM (Amrouch et al. 2016), FCA-Map (Zhao and Zhang 2016), Lily (Wang and Xu 2008), LogMapLite (Jim\u00e9nez-Ruiz and Grau 2011), LPHOM (Megdiche et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "2016), FCA-Map (Zhao and Zhang 2016), Lily (Wang and Xu 2008), LogMapLite (Jim\u00e9nez-Ruiz and Grau 2011), LPHOM (Megdiche et al. 2016), LYAM (Achichi et al.", "startOffset": 110, "endOffset": 132}], "year": 2017, "abstractText": "Ontology alignment is widely used to \u0080nd the correspondences between di\u0082erent ontologies in diverse \u0080elds. A\u0089er discovering the alignment by methods, several performance scores are available to evaluate them. \u008ce scores require the produced alignment by amethod and the reference alignment containing the underlying actual correspondences of the given ontologies. \u008ce current trend in alignment evaluation is to put forward a new score and to compare various alignments by juxtaposing their performance scores. However, it is substantially provocative to select one performance score among others for comparison. On top of that, claiming if one method has a be\u008aer performance than one another can not be substantiated by solely comparing the scores. In this paper, we propose the statistical procedures which enable us to theoretically favor one method over one another. \u008ce McNemar test is considered as a reliable and suitable means for comparing two ontology alignment methods over one matching task. \u008ce test applies to a 2 \u00d7 2 contingency table which can be constructed in two di\u0082erent ways based on the alignments, each of which has their own merits/pitfalls. \u008ce ways of the contingency table construction and various apposite statistics from the McNemar test are elaborated in minute detail. In the case of having more than two alignment methods for comparison, the family-wise error rate is expected to happen. \u008cus, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar test in the presence of multiple alignment methods. From this graph, it is readily understood if one method is be\u008aer than one another or if their di\u0082erences are imperceptible. Our investigation on the methods participated in the anatomy track of OAEI 2016 demonstrates that AML and CroMatcher are the top two methods and DKP-AOM and Alin are the bo\u008aom two ones. Moreover, the Levenstein and N-gram string-based distances discover the most correspondences while SMOA and Hamming distance are the ones with the least found correspondences.", "creator": "LaTeX with hyperref package"}}}