{"id": "1704.00115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Ontological Multidimensional Data Models and Contextual Data Qality", "abstract": "Data quality evaluation and data cleansing are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD Model), which can be used to model and display contexts as logical ontologies. The data to be evaluated is included in the context to enable additional analysis, processing and quality data extraction. The resulting contexts allow the representation of dimensions and a multidimensional evaluation of data quality. At the core of a multidimensional context, we include a generalized multidimensional data model and a data + / - ontology with demonstrably good properties in terms of answering queries. These main components are used to represent dimensional hierarchies, dimensional constraints, dimensional rules and to define predicates the specification of quality data. Answering queries relies on and resolves the navigation through multidimensional data archives, making multidimensional data from quality data and the basic tool of OMD interesting to the beyond.", "histories": [["v1", "Sat, 1 Apr 2017 03:50:53 GMT  (1097kb,D)", "https://arxiv.org/abs/1704.00115v1", "Journal submission. Extended version of RuleML'15 paper"], ["v2", "Sun, 13 Aug 2017 21:11:37 GMT  (1103kb,D)", "http://arxiv.org/abs/1704.00115v2", "Journal submission (revised version addressing reviewers' observations) Extended version of RuleML'15 paper"]], "COMMENTS": "Journal submission. Extended version of RuleML'15 paper", "reviews": [], "SUBJECTS": "cs.DB cs.AI", "authors": ["leopoldo bertossi", "mostafa milani"], "accepted": false, "id": "1704.00115"}, "pdf": {"name": "1704.00115.pdf", "metadata": {"source": "META", "title": "Ontological Multidimensional Data Models and Contextual Data Quality", "authors": ["LEOPOLDO BERTOSSI", "MOSTAFA MILANI"], "emails": [], "sections": [{"heading": null, "text": "1 Ontological Multidimensional Data Models and Contextual Data ality\nLEOPOLDO BERTOSSI, Carleton University MOSTAFA MILANI, McMaster University\nData quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. e data under assessment is mapped into the context, for additional analysis, processing, and quality data extraction. e resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context we include a generalized multidimensional data model and a Datalog\u00b1 ontology with provably good properties in terms of query answering. ese main components are used to represent dimension hierarchies, dimensional constraints, dimensional rules, and de ne predicates for quality data speci cation. ery answering relies upon and triggers navigation through dimension hierarchies, and becomes the basic tool for the extraction of quality data. e OMD model is interesting per se, beyond applications to data quality. It allows for a logic-based, and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.\nCCS Concepts: \u2022Information systems\u2192 Data cleaning; General Terms: Database Management, Multidimensional data, Contexts, Data quality, Data cleaning\nAdditional Key Words and Phrases: Ontology-based data access, Datalog\u00b1, Weakly-sticky programs, ery answering\nACM Reference format: Leopoldo Bertossi and Mostafa Milani. 2017. Ontological Multidimensional Data Models and Contextual Data ality. 1, 1, Article 1 (March 2017), 35 pages. DOI: 0000001.0000001"}, {"heading": "1 INTRODUCTION", "text": "Assessing the quality of data and performing data cleaning when the data are not up to the expected standards of quality have been and will continue being common, di cult and costly problems in data management [12, 35, 83]. is is due, among other factors, to the fact that there is no uniform, general de nition of quality data. Actually, data quality has several dimensions. Some of them are [12]: (1) Consistency, which refers to the validity and integrity of data representing real-world entities, typically identi ed with satisfaction of integrity constraints. (2) Currency (or timeliness), which aims to identify the current values of entities represented by tuples in a (possibly stale) database, and to answer queries with the current values. (3) Accuracy, which refers to the closeness of values in a database to the true values for the entities that the data in the database is work is supported by NSERC Discovery Grant 2016-06148, and the NSERC Strategic Network on Business Intelligence (BIN). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. XXXX-XXXX/2017/3-ART1 $15.00 DOI: 0000001.0000001\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nar X\niv :1\n70 4.\n00 11\n5v 2\n[ cs\n.D B\n] 1\n3 A\nug 2\n01 7\nrepresents; and (4) Completeness, which is characterized in terms of the presence/absence of values. (5) Redundancy, e.g. multiple representations of external entities or of certain aspects thereof. Etc. (Cf. also [39, 40, 57] for more on quality dimensions.)\nIn this work we consider data quality as referring to the degree to which the data ts or ful lls a form of usage [12], relating our data quality concerns to the production and the use of data. We will elaborate more on this a er the motivating example in this introduction.\nIndependently from the quality dimension we may consider, data quality assessment and data cleaning are context-dependent activities. is is our starting point, and the one leading our research. In more concrete terms, the quality of data has to be assessed with some form of contextual knowledge; and whatever we do with the data in the direction of data cleaning also depends on contextual knowledge. For example, contextual knowledge can tell us if the data we have is incomplete or inconsistent. In the la er case, the context knowledge is provided by explicit semantic constraints.\nIn order to address contextual data quality issues, we need a formal model of context. In very general terms, the big picture is as follows. A database can be seen as a logical theory, T , and a context for it, as another logical theory, T c , into which T is mapped by means of a set, m, of logical mappings, as shown in Figure 1. e image of T in T c is T \u2032 = m(T ), which could be seen as an interpretation of T in T c .1 e contextual theory T c provides extra knowledge about T , as a logical extension of its image T \u2032. For example, T c may contain additional semantic constraints on elements of T (or their images in T c ) or extensions of their de nitions. In this way, T c conveys more semantics or meaning about T , contributing to making more sense of T \u2019s elements. T c may also contain data and logical rules that can be used for further processing or using knowledge in T . e embedding of T into T c can be achieved via predicates in common or, more complex logical formulas.\nIn this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11]. In this work, the mappings turn out to be quite simple: e ontology contains, among other predicates, nicknames for the predicates in R (i.e. copies of them), so that each predicate P in R is directly mapped to its copy P \u2032 in Oc .\nOnce the data in D is mapped into Oc , i.e. put in context, the extra elements in it can be used to de ne alternative versions of D, in our case, clean or quality versions, Dq , of D in terms of data quality. e data quality criteria are imposed within Oc . is may determine a class of possible quality versions of D, virtual or material. e existence of several quality versions re ects the uncertainty that emerges from not having only quality data in D. 1 Interpretations between logical theories have been investigated in mathematical logic [36, sec. 2.7] and used, e.g. to obtain (un)decidability results [82].\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\ne whole class, al(D,Oc ), of quality versions of D determines or characterizes the quality data in D, as the data that are certain with respect to al(D,Oc ). One way to go in this direction consists in keeping only the data that are found in the intersection of all the instances in al(D,Oc ). A more relaxed alternative consists in considering as quality data those that are obtained as certain answers to queries posed to D, but answered through al(D,Oc ): e query is posed to each of the instances in al(D,Oc ) (which essentially have the same schema as D), but only those answers that are shared by those instances are considered to be certain [55].2 ese answers become the quality-answers in our se ing. e main question is about the kind of contextual ontologies that are appropriate for our tasks. ere are several basic conditions to satisfy. First of all, Oc has to be wri en in a logical language. As a theory it has to be expressive enough, but not too much so that computational problems, such as (quality) data extraction via queries becomes intractable, if not impossible. It also has to combine well with relational data. And, as we emphasize and exploit in our work, it has to allow for the representation and use of dimensions of data, i.e. conceptual axes along which data are represented and analyzed. ey are the basic elements in multidimensional databases and data warehouses [56], where we usually nd time, location, product, as three dimensions that give context to numerical data, e.g. sales. Dimensions are almost essential elements of contexts, in general, and crucial if we want to analyze data from di erent perspectives or points of view. We use dimensions as (possibly partially ordered) hierarchies of categories.3 For example, the location dimension could have categories, city, province, country, continent, in this hierarchical order of abstraction. e language of choice for the contextual ontologies will be Datalog\u00b1 [28]. As an extension of Datalog, a declarative query language for relational databases [33], it provides declarative extensions of relational data by means of expressive rules and semantic constraints. Certain classes of Datalog\u00b1 programs have non-trivial expressive power and good computational properties at the same time. One of those good classes is that of weakly-sticky Datalog\u00b1 [29]. Programs in that class allow us to represent a logic-based, relational reconstruction and extension of the Hurtado-Mendelzon multidimensional data model [53, 54], which allows us to bring data dimensions into contexts.\nEvery contextual ontology Oc contains its multidimensional core ontology, OM , which is wri en in Datalog\u00b1 and represents what we will call the ontological multidimensional data model (OMD model, in short), plus a quality-oriented sub-ontology, Oq , containing extra relational data (shown as instance E in Figure 5), Datalog rules, and possibly additional constraints. Both sub-ontologies are application dependent, but OM follows a relatively xed format, and contains the dimensional structure and data that extend and supplement the data in the input instance D, without any explicit 2 ose familiar with database repairs and consistent query answering [14, 17], would notice that both can be formulated in this general ste ing. Instance D would be the inconsistent database, the ontology would provide the integrity constraints and the speci cation of repairs, say in answer set programming [32], the class al(D, Oc ) would contain the repairs, and the general certain answers would become the consistent answers. 3 Data dimensions were not considered in [16, 19].\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nquality concerns in it. e OMD model is interesting per se in that it considerably extends the usual multidimensional data models (more on this later). Ontology Oq contains as main elements de nitions of quality predicates that will be used to produce quality versions of the original tables, and to compute quality query answers. Notice that the la er problem becomes a case of ontologybased data access (OBDA), i.e. about indirectly accessing underlying data through queries posed to the interface and elements of an ontology [81].\nstandard intensive terminal\nW4 W3 W2 W1\nH1\nallHospital\nH2\nAllHospital\nInstitution\nUnit\nWard\nFig. 3. The Hospital dimension\nTable 1. Temperatures\nTime Patient Value Nurse\n1 12:10-Sep/1/2016 Tom Waits 38.2 Anna 2 11:50-Sep/6/2016 Tom Waits 37.1 Helen 3 12:15-Nov/12/2016 Tom Waits 37.7 Alan 4 12:00-Aug/21/2016 Tom Waits 37.0 Sara 5 11:05-Sep/5/2016 Lou Reed 37.5 Helen 6 12:15-Aug/21/2016 Lou Reed 38.0 Sara\nTable 2. Temperaturesq\nTime Patient Value Nurse 1 12:15-Nov/12/2016 Tom Waits 37.7 Alan 2 12:00-Aug/21/2016 Tom Waits 37.0 Sara 3 12:15-Aug/21/2016 Lou Reed 38.0 Sara\nExample 1.1. e relational table Temperatures (Table 1) shows body temperatures of patients in an institution. A doctor wants to know \u201c e body temperatures of Tom Waits for August 21 taken around noon with a thermometer of brand B1\u201d (as he expected). Possibly a nurse, unaware of this requirement, used a thermometer of brand B2, storing the data in Temperatures. In this case, not all the temperature measurements in the table are up to the expected quality. However, table Temperatures alone does not discriminate between intended values (those taken with brand B1) and the others.\nFor assessing the quality of the data or for extracting quality data in/from the table Temperatures according to the doctor\u2019s quality requirement, extra contextual information about the thermometers in use may help. In this case, we may have contextual information in the form of a guideline prescribing that: \u201cNurses in intensive care unit use thermometers of Brand B1\u201d. We still cannot combine this guideline with the data in the table. However, if we know that nurses work in wards, and those wards are associated to units, then we may be in position to combine the table with the given contextual information. Actually, as shown in Figure 4, the context contains dimensional data, in categorical relations linked to dimensions.\nIn it we nd two dimensions, Hospital, on the le -hand side, and Temporal, on the right-hand side. For example, the Hospital dimension\u2019s instance is found in Figure 3. In the middle of Figure 4 we nd categorical relations (shown as solid tables and initially excluding the two rows shaded in gray at the bo om of the top table). ey are associated to categories in the dimensions.\nNow we have all the necessary information to discriminate between quality and non-quality entries in Table 1: Nurses appearing in it are associated to wards, as shown in table Shi s; and the wards are associated to units, as shown in Figure 3. Table WorkSchedules may be incomplete, and new -possibly virtual- entries can be produced for it, showing Helen and Sara working for the Standard and Intensive units, resp. ( ese correspond to the two (potential) extra, shaded tuples in Figure 4.) is is done by upward navigation and data propagation through the dimension hierarchy. At this point we are in position to take advantage of the guideline, inferring that Alan and Sara used thermometers of brand B1, as expected by the physician.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nAs expected, in order to do upward navigation and use the guideline, they have to be represented in our multidimensional contextual ontology. Accordingly, the la er contains, in addition to the data in Figure 4, the two rules, for upward data propagation and the guideline, resp.:\n\u03c31 : Shi s(w,d ;n, s),WardUnit(w,u) \u2192 \u2203t WorkSchedules(u,d ;n, t). (1) WorkTimes(intensive, t ;n,y) \u2192 TakenWith erm(t ,n, b1). (2)\nHere, WorkTimes is a categorical relation linked to the Time category in the Temporal dimension. It contains the schedules as in relation WorkSchedules, but at the time of the day level, say \u201c14:30 on Feb/08, 2017\u201d, rather than the day level.\nRule (1) tells that: \u201cIf a nurse has shi s in a ward on a speci c day, he/she has a work schedule in the unit of that ward on the same day\u201d. Notice that the use of (1) introduces unknown, actually null, values in a ribute Specialization, which is due to the existential variable ranging over the a ribute domain. Existential rules of this kind already make us depart from classic Datalog, taking us to Datalog\u00b1.\nAlso notice that in (1) we are making use of the binary dimensional predicate WardUnit that represents in the ontology the child-parent relation between members of the Ward and Unit categories.4\nRule (1) properly belong to a contextual, multidimensional, core ontology OM in the sense that it describes properly dimensional information. Now, rule (2), the guideline, could also belong to OM , but it is less clear that it convey strictly dimensional information. Actually, in our case we intend to use it for data quality purposes (cf. Example 1.2 below), and as such we will place it in the quality-oriented ontology Oq . In any case, the separation is always application dependent. However, under certain conditions on the contents of OM , we will be able to guarantee (in Section 4) that the la er has good computational properties.\ne contextual ontology Oc can be used to support the speci cation and extraction of quality data, as shown in Figure 5. A database instance D for a relational schema R = {R1, ...,Rn} is mapped into Oc for quality data speci cation and extraction. e ontology contains a copy, R \u2032 = {R\u20321, ...,R\u2032n}, of\n4 In the WorkSchedules(Unit, Day;Nurse, Speciality) predicate, a ributes Unit and Day are called categorical a ributes, because they take values from categories in dimension. ey are separated by a semi-colon (;) from the non-categorical Nurse and Speciality.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nschema R with predicates that are nicknames for those in R. e nickname predicates are directly populated with the data in the corresponding relations (tables) in D.\nIn addition to the multidimensional (MD) ontology, OM , the contextual ontology contains, in ontology Oq , de nitions of application-dependent quality predicates P, those in \u03a3P . Together with application-dependent, not directly dimensional rules, e.g. capturing guidelines as in Example 1.1, they capture data quality concerns. Figure 5 also shows E as a possible extra contextual database, with schema RE , whose data could be used at the contextual level in combination with the data strictly associated to the multidimensional ontology (cf. Section 5 for more details).\nData originally obtained fromD is processed through the contextual ontology, producing, possibly virtual, extensions for copies, Rq , of the original predicates R in R. Predicates Rq \u2208 Rq are the \u201cquality versions\u201d of predicates R \u2208 R. e following example shows the gist.\nExample 1.2. (ex. 1.1 cont.) Temperatures\u2032, the nickname for predicate Temperatures in the original instance, is de ned by the rule:\nTemperatures(t ,p,v,n) \u2192 Temperatures\u2032(t ,p,v,n). (3) Furthermore, Oq contains rule (2) as a de nition of quality predicate TakenWith erm. Now, Temperaturesq , the quality-version of predicate Temperatures, is de ned by means of:\nTemperatures\u2032(t ,p,v,n), TakenWith erm(t ,n, b1) \u2192 Temperaturesq(t ,p,v,n). (4) e extension of Temperaturesq can be computed, and is shown in Table 2. It contains \u201cquality\ndata\u201d from the original relation Temperatures. e second and the third tuples in Temperaturesq are obtained through the fact that Sara was in the intensive care unit on Aug/21, as reported by the last in-gray shaded tuple in WorkSchedules in Figure 4, which was created by upward data propagation with the dimensional ontology.\nIt is not mandatory to materialize relation Temperaturesq . Actually, the doctor\u2019s query: Q(v) : \u2203n \u2203t (Temperatures(t , tom waits,v,n) \u2227 11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016) (5)\ncan be answered by, (a) replacing Temperatures by Temperaturesq , (b) unfolding the de nition of Temperaturesq in (4), obtaining a query in terms of TakenWith erm and Temperatures\u2032; and (c) using (2) and (3) to compute the answers through OM and D. e quality answer is the second tuple in Table 2. ( is procedure is described in general in Section 5.2).\nDue to the simple ontological rules and the use of them in the example above, we obtain a single quality instance. In other cases, we may obtain several of them, and quality query answering amounts to doing certain query answering (QA) on Datalog\u00b1 ontologies, in particular on the the MD ontologies. ery answering on Datalog\u00b1 ontologies has been investigated in the literature for di erent classes of Datalog\u00b1 programs. For some of them, query answering is tractable and\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nthere are e cient algorithms. For others, the problem is know to be tractable, but still practical algorithms are needed. For some classes, the problem is known to be intractable. For this reason, it becomes important to characterize the kind of Datalog\u00b1 ontologies used for the OMD model. e promising application of the OMD model that we investigate in this work is related to data quality concerns as pertaining to the use and production of data [12]. By this we mean that the available data are not bad or good a priori or prima facie, but their quality depends on how they were created or how they will be used, and this information is obtained from a contextual ontology. is form of data quality has been mentioned in the literature. For example, in [90] contextual data quality dimensions are described as those quality dimensions that are relevant to the context of data usage. In [51] and [59], quality is characterized as \u201c tness for use\u201d.\nOur motivating example already shows the gist of our approach to this form of data quality: Nothing looks wrong with the data in Table 1 (the data source), but in order to assess the quality of the source\u2019s data or to extract quality data from it, we need to provide additional data and knowledge that do not exist at the source; and they are both provided by the context. From this point of view, we are implicitly addressing a problem of incomplete data, one of the common data quality dimensions [12]. However, this is not the form of explicit incompleteness that we face with null or missing values in a table [64]. (Cf. Section 6.3 for an additional discussion on the data quality dimensions addressed by the OMD model.)\nAs we pointed out before (cf. Footnote 2), our contextual approach can be used, depending on the elements we introduce in a contextual ontology, to address other data quality concerns, such as inconsistency, redundancy,5 and the more typical and direct form of incompleteness, say obtaining from the context data values for null or missing values in tables.\nIn this work we concentrate mostly on the OMD model by itself, but also on its combination and use with quality-oriented ontologies for quality QA. We do not go into data quality assessment, which is also an interesting subject.6 Next, we summarize the main contributions of this work.\n(A) We propose and formalize the Ontological Multidimensional Data Model (OMD model), which is based on a relational extension via Datalog\u00b1 of the HM model for multidimensional data. e OMD allows for: (a) Categorical relations linked to dimension categories (at any level), which go beyond the bo om-level, numerical fact tables found in data warehouses. (b) Incomplete data (and complete data as usual). (c) A logical integration and simultaneous representation of dimensional data and metadata, the la er by means of semantic dimensional constrains and dimensional rules. (d) Dimensional navigation and data generation, both upwards and downwards (the examples above show only the upward case). (B) We establish that, under natural assumptions that MD ontologies belong to the class of weaklysticky (WS) Datalog\u00b1 programs [29], for which conjunctive QA is tractable (in data). e class of WS programs is an extension of sticky Datalog\u00b1[29] and weakly-acyclic programs [37]. Actually, WS Datalog\u00b1 is de ned through restrictions on join variables occurring in in nite-rank positions, as introduced in [37].\nIn this work, we do not provide algorithms for (tractable) QA on weakly-sticky Datalog\u00b1 programs. However, in [74] a practical algorithm was proposed, together with a methodology for magic-setbased query optimization.\n5 In the case of duplicate records in a data source, the context could contain an answer set program or a Datalog program to enforce matching dependencies for entity resolution [10]. 6 e quality of D can be measured in terms of how much D departs from (its quality versions in) Dq : dist(D, Dq ). Of course, di erent distance measures may be used for this purpose [16, 19].\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\n(C) We analyze the interaction between dimensional constraints and the dimensional rules, and their e ect on QA. Most importantly, the combination of constraints that are equality-generating dependencies (egds) and the rules, which are tuple-generating dependencies (tgds) [27], may lead to undecidability of QA. Separability [29] is a semantic condition on egds and tgds that guarantees the interaction between them does not harm the tractability of QA. Separability is an applicationdependent issue. However, we show that, under reasonable syntactic conditions on egds in MD ontologies, separability holds. (D) We propose a general ontology-based approach to contextual quality data speci cation and extraction. e methodology takes advantage of a MD ontology and a process of dimensional navigation and data generation that is triggered by queries about quality data. We show that under natural conditions the elements of the quality-oriented ontology Oq , in form of additional Datalog\u00b1rules and constraints, do not a ect the good computational properties of the core MD ontology OM .\ne closest work related to our OMD model can be found in the dimensional relational algebra proposed in [71], which is subsumed by the OMD model [76, chap. 4]. e contextual and dimensional data representation framework in [25] is also close to our OMD model in that it uses dimensions for modeling context. However, in their work dimensions are di erent from the dimensions in the HM data model. Actually, they use the notion of context dimension trees (CDTs) for modeling multidimensional contexts. Section 6.5 includes more details on related work. is paper is structured as follows. Section 2 contains a review of databases, Datalog\u00b1, and the HM data model. Section 3 formalizes the OMD data model. Section 4 analyzes the computational properties of the OMD model. Section 5 extends the OMD model with additional contextual elements for specifying and extracting quality data, and show how to use the extension for this task. Section 6 discusses additional related work, draws some nal conclusions, and includes a discussion of possible extensions of the OMD model. is paper considerably extends results previously reported in [73]."}, {"heading": "2 BACKGROUND", "text": "In this section, we brie y review relational databases and the multidimensional data model."}, {"heading": "2.1 Relational Databases", "text": "We always start with a relational schema R with two disjoint domains: C, with possibly in nitely many constants, and N , of in nitely many labeled nulls. R also contains predicates of xed nite arities. If P is an n-ary predicate (i.e. with n arguments) and 1 \u2264 i \u2264 n, P[i] denotes its i-th position. R gives rise to a language L(R) of rst-order (FO) predicate logic with equality (=). Variables are usually denoted with x ,y, z, ..., and sequences thereof by x\u0304 , .... Constants are usually denoted with a,b, c, ...; and nulls are denoted with \u03b6 , \u03b61, .... An atom is of the form P(t1, . . . , tn), with P an n-ary predicate and t1, . . . , tn terms, i.e. constants, nulls, or variables. e atom is ground (aka. a tuple) if it contains no variables. An instance I for schema R is a possibly in nite set of ground atoms; this set I is also called an extension for the schema. In particular, the extension of a predicate P in an instance I , denoted by P(I ), is the set of atoms in I whose predicate is P . A database instance is a nite instance that contains no nulls. e active domain of an instance I , denoted Adom(I ), is the set of constants or nulls that appear in atoms of I . Instances can be used as interpretation structures for language L(R).\nAn instance I may be closed or incomplete (a.k.a. open or partial). In the former case, one makes the meta-level assumption, the so-called closed-world-assumption (CWA) [1, 84], that the only positive ground atoms that are true w.r.t. I are those explicitly given as members of I . In the la er\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\ncase, those explicit atoms may form only a proper subset of those positive atoms that could be true w.r.t. I .7\nA homomorphism is a structure-preserving mapping, h: C \u222aN\u2192C \u222aN , between two instances I and I \u2032 for schema R such that: (a) t \u2208 C implies h(t) = t , and (b) for every ground atom P(t\u0304): if P(t\u0304) \u2208 I , then P(h(t\u0304)) \u2208 I \u2032.\nA conjunctive query (CQ) is an FO formula, Q(x\u0304), of the form:\n\u2203y\u0304 (P1(x\u03041) \u2227 \u00b7 \u00b7 \u00b7 \u2227 Pn(x\u0304n)), (6) with Pi \u2208 R, and (distinct) free variables x\u0304 := \u22c3 x\u0304i r y\u0304. If Q hasm (free) variables, for an instance I , t\u0304 \u2208 (C \u222a N)m is an answer to Q if I |= Q[t\u0304], meaning that Q[t\u0304] becomes true in I when the variables in x\u0304 are componentwise replaced by the values in t\u0304 . Q(I ) denotes the set of answers to Q in I . Q is a boolean conjunctive query (BCQ) when x\u0304 is empty, and if it is true in I , in which case Q(I ) := {true}. Otherwise, Q(I ) = \u2205, and we say it is false.\nA tuple-generating dependency (tgd), also called a rule, is an implicitly universally quanti ed sentence of L(R) of the form:\n\u03c3 : P1(x\u03041), . . . , Pn(x\u0304n) \u2192 \u2203y\u0304 P(x\u0304 , y\u0304), (7) with Pi \u2208 R, and x\u0304 \u2286 \u22c3 i x\u0304i , and the dots in the antecedent standing for conjunctions. e variables in y\u0304 (that could be empty) are the existential variables. We assume y\u0304 \u2229 \u222ax\u0304i = \u2205. With head(\u03c3 ) and body(\u03c3 ) we denote the atom in the consequent and the set of atoms in the antecedent of \u03c3 , respectively.\nA constraint is an equality-generating dependency (egd ) or a negative constraint (nc), which are also sentences of L(R), respectively, of the forms:\nP1(x\u03041), . . . , Pn(x\u0304n) \u2192 x = x \u2032, (8) P1(x\u03041), . . . , Pn(x\u0304n) \u2192 \u22a5, (9)\nwith Pi \u2208 R, and x ,x \u2032 \u2208 \u22c3\ni x\u0304i , and \u22a5 is a symbol that denotes the Boolean constant (propositional variable) that is always false. Satisfaction of constraints by an instance is as in FO logic. In Section 3 we will use ncs with negated body atoms (i.e. negative literals), in a limited manner. eir semantics is also as in FO logic, i.e. the body cannot be made true in a consistent instance, for any data values for the variables in it. Tgds, egds, and ncs are particular kinds of relational integrity constraints (ICs) [1]. In particular, egds include key constraints and functional dependencies (FDs). ICs also include inclusion dependencies (IDs): For an n-ary predicate P and an m-ary predicate S , the ID P[j] \u2286 S[k], with j \u2264 n, k \u2264 m, means that -in the extensions of P and S in an instance- the values appearing in the jth position (a ribute) of P must also appear in the kth position of S .\nRelational databases work under the CWA, i.e. ground atoms not belonging to a database instance are assumed to be false. As a consequence, an IC is true or false when checked for satisfaction on a (closed) database instance, never undetermined. However, as we will see below, if instances are allowed to be incomplete, i.e. with undetermined or missing ground atoms, ICs may not be false, but only undetermined in relation to their truth status. Actually, they can be used, by enforcing them, to generate new tuples for the (open) instance.\nDatalog is a declarative query language for relational databases that is based on the logic programming paradigm, and allows to de ne recursive views [1, 33]. A Datalog program \u03a0 for 7 In the most common scenario one starts with a ( nite) open database instance D that is combined with an ontology whose tgds are used to create new tuples. is process may lead to an in nite instance I . Hence the distinction between database instances and instances.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nschema R is a nite set of non-existential rules, i.e. as in (7) but without \u2203-variables. Some of the predicates in \u03a0 are extensional, i.e. they do not appear in rule heads, and their complete extensions are given by a database instance D (for a subschema of R), that is called the program\u2019s extensional database. e program\u2019s intentional predicates are those that are de ned by the program by appearing in tgds\u2019 heads. e program\u2019s extensional database D may give to them only partial extensions (additional tuples for them may be computed by the application of the program\u2019s tgds). However, without loss of generality, it is common with Datalog to make the assumption that intensional predicates do not have an explicit extension, i.e. explicit ground atoms in D. e minimal-model semantics of a Datalog program w.r.t. an extensional database instance D is given by a x-point semantics [1]: the extensions of the intentional predicates are obtained by, starting from D, iteratively enforcing the rules and creating tuples for the intentional predicates, i.e. whenever a ground (or instantiated) rule body becomes true in the extension obtained so far, but not the head, the corresponding ground head atom is added to the extension under computation. If the set of initial ground atoms is nite, the process reaches a x-point a er a nite number of steps. e database instance obtained in this way turns out to be the unique minimal model of the Datalog program: it extends the extensional database D, makes all the rules true, and no proper subset has the two previous properties. Notice that the constants in a minimal model of a Datalog program are those already appearing in the active domain of D or in the program rules; no new data values of any kind are introduced.\nOne can pose a CQ to a Datalog program by evaluating it on the minimal model of the program, seen as a database instance. However, it is common to add the query to the program, and the minimal model of the combined program gives us the set of answers to the query. In order to do this, a CQ as in (6) is expressed as a Datalog rule of the form:\nP1(x\u03041), ..., Pn(x\u0304n) \u2192 ansQ(x\u0304), (10)\nwhere ansQ(\u00b7) is an auxiliary, answer-collecting predicate. e answers to query Q form the extension of predicate ansQ(\u00b7) in the minimal model of the original program extended with the query rule. When Q is a BCQ, ansQ is a propositional atom; and Q is true in the undelying instance exactly when the atom ansQ belongs to the minimal model of the program.\nExample 2.1. A Datalog program \u03a0 containing the rules P(x ,y) \u2192 R(x ,y), and P(x ,y),R(y, z) \u2192 R(x , z) recursively de nes, on top of an extension for predicate P , the intentional predicate R as the transitive closure of P . With D = {P(a,b), P(b,d)} as the extensional database, the extension of R can be computed by iteratively adding tuples enforcing the program rules, which results in the instance I = {P(a,b), P(b,d),R(a,b),R(b,d),R(a,d)}, which is the minimal model of the program. e CQQ(x) : R(x ,b)\u2227R(x ,d) can be expressed by the rule R(x ,b),R(x ,d) \u2192 ansQ(x). e set of answers is the computed extension for ansQ(x) on instance D, namely {a}. Equivalently, the query rule can be added to the program, and the minimal model of the resulting program will contain the extension for the auxiliary predicate ansQ : I \u2032 = {P(a,b), P(b,d),R(a,b),R(b,d),R(a,d), ansQ(a)}."}, {"heading": "2.2 Datalog\u00b1", "text": "Datalog\u00b1 is an extension of Datalog. e \u201c+\u201d stands for the extension, and the \u201c\u2212\u201d, for some syntactic restrictions on the program that guarantee some good computational properties. We will refer to some of those restrictions in Section 4. Accordingly, until then we will consider Datalog+ programs.\nA Datalog+ program may contain, in addition to (non-existential) Datalog rules, also existential rules rules of the form (7), and constraints of the forms (8) and (9). A Datalog+ program has\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nan extensional database D. In a Datalog+ program \u03a0, unlike plain Datalog, predicates are not necessarily partitioned into extensional and intentional ones: any predicate may appear in the head of a rule. As a consequence, some predicates may have partial extensions in the extensional database D, and their extensions will be completed via rule enforcements. e semantics of a Datalog+ program \u03a0 with an extensional database instance D is modeltheoretic, and given by the class Mod(\u03a0,D) of all, possibly in nite, instances I for the program\u2019s schema (in particular, with domain contained in C \u222a N ) that extend D and make \u03a0 true. Notice that, and in contrast to Datalog, the combination of the \u201copen-world assumption\u201d and the use of \u2203-variables in rule heads makes us consider possibly in nite models for a Datalog+program, actually with domains that go beyond the active domain of the extensional database.\nIf a Datalog\u00b1 program \u03a0 has an extensional database instance D, a set \u03a0R of tgds, and a set \u03a0C of constraints of the forms (8) or (9), then \u03a0 is consistent if Mod(\u03a0,D) is non-empty, i.e. the program has at least one model.\nGiven a Datalog+program \u03a0 with database instance D and an n-ary CQ Q(x\u0304), t\u0304 \u2208 (C \u222a N)n is an answer w.r.t. \u03a0 i I |= Q[t\u0304] for every I \u2208 Mod(\u03a0,D), which is equivalent to \u03a0 \u222a D |= Q[t\u0304]. Accordingly, this is certain answer semantics. In particular, a BCQ Q is true w.r.t. \u03a0 if it is true in every I \u2208 Mod(\u03a0,D). In the rest of this paper, unless otherwise stated, CQs are BCQs, and CQA is the problem of deciding if a BCQ is true w.r.t. a given program.8\nWithout any syntactic restrictions on the program, and even for programs without constraints, conjunctive query answering (CQA) may be undecidable [13]. CQA appeals to all possible models of the program. However, the chase procedure [70] can be used to generate a single, possibly in nite, instance that represents the class Mod(\u03a0,D) for this purpose. We show it by means of an example.\nExample 2.2. Consider a program \u03a0 with the set of rules \u03c3 : R(x ,y) \u2192 \u2203z R(y, z), and \u03c3 \u2032 : R(x ,y),R(y, z) \u2192 S(x ,y, z), and an extensional database instance D = {R(a,b)}, providing an incomplete extension for the program\u2019s schema. With the instance I0 := D, the pair (\u03c3 ,\u03b81), with (value) assignment (for variables) \u03b81 : x 7\u2192 a,y 7\u2192 b, is applicable: \u03b81(body(\u03c3 )) = {R(a,b)} \u2286 I0. e chase enforces \u03c3 by inserting a new tuple R(b, \u03b61) into I0 (\u03b61 is a fresh null, i.e. not in I0), resulting in instance I1.\nNow, (\u03c3 \u2032,\u03b82), with \u03b82 : x 7\u2192 a,y 7\u2192 b, z 7\u2192 \u03b61, is applicable, because \u03b82(body(\u03c3 \u2032)) = {R(a,b),R(b, \u03b61)} \u2286 I1. e chase adds S(a,b, \u03b61) into I1, resulting in I2. e chase continues, without stopping, creating an in nite instance, usually called the chase (instance): chase(\u03a0,D) = {R(a,b),R(b, \u03b61), S(a,b, \u03b61), R(\u03b61, \u03b62),R(\u03b62, \u03b63), S(b, \u03b61, \u03b62), . . .}.\nFor some programs an instance obtained through the chase may be nite. Di erent orders of chase steps may result in di erent sequences and instances. However, it is possible to de ne a canonical chase procedure that determines a canonical sequence of chase steps, and consequently, a canonical chase instance [31].\nGiven a program \u03a0 and extensional database D, its chase (instance) is a universal model [37]: For every I \u2208 Mod(\u03a0,D), there is a homomorphism from the chase into I . For this reason, the (certain) answers to a CQ Q under \u03a0 and D can be computed by evaluating Q over the chase instance (and discarding the answers containing nulls) [37]. Universal models of Datalog programs are nite and coincide with the minimal models. However, the universal model of a Datalog+ program may be in nite, this is when the chase procedure does not stop, as shown in Example 2.2. is is a consequence of the OWA underlying Datalog+ programs and the presence of existential variables.\n8 For Datalog+ programs, CQ answering, i.e. checking if a tuple is an answer to a CQ query, can be reduced to BCQ answering as shown in [31], and they have the same data complexity.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nIf a program \u03a0 consists of a set of tgds \u03a0R and a set of ncs \u03a0C , then CQA amounts to deciding if D \u222a \u03a0R \u222a \u03a0C |= Q. However, this is equivalent to deciding if: (a) D \u222a \u03a0R |= Q, or (b) for some \u03b7 \u2208 \u03a0C , D \u222a\u03a0R |= Q\u03b7 , where Q\u03b7 is the BCQ obtained as the existential closure of the body of \u03b7 [29, theo. 6.1]. In the la er case, D \u222a \u03a0 is inconsistent, and Q becomes trivially true. is shows that CQA evaluation under ncs can be reduced to the same problem without ncs, and the data complexity of CQA does not change. Furthermore, ncs may have an e ect on CQA only if they are mutually inconsistent with the rest of the program, in which case every BCQ becomes trivially true.\nIf \u03a0 has egds, they are expected to be satis ed by a modi ed (canonical) chase [31] that also enforces the egds. is enforcement may become impossible at some point, in which case we say the chase fails (cf. Example 2.3). Notice that consistency of a Datalog+ program is de ned independently from the chase procedure, but can be characterized in terms of the chase. Furthermore, if the canonical chase procedure terminates ( nitely or by failure) the result can be used to decide if the program is consistent. e next example shows that egds may have an e ect on CQA even with consistent programs.\nExample 2.3. Consider a program \u03a0 with D = {R(a,b)} with two rules and an egd:\nR(x ,y) \u2192 \u2203z \u2203w S(y, z,w). (11) S(x ,y,y) \u2192 P(x ,y). (12) S(x ,y, z) \u2192 y = z. (13)\ne chase of \u03a0 rst applies (11) and results in I1 = {R(a,b), S(b, \u03b61, \u03b62)}. ere are no more tgd/assignment applicable pairs. But, if we enforce the egd (13), equating \u03b61 and \u03b62, we obtain I2 = {R(a,b), S(b, \u03b61, \u03b61)}. Now, (12) and \u03b8 \u2032 : x 7\u2192 b,y 7\u2192 \u03b61 are applicable, so we add P(b, \u03b61) to I2, generating I3 = {R(a,b), S(b, \u03b61, \u03b61), P(b, \u03b61)}. e chase terminates (no applicable tgds or egds), obtaining chase(\u03a0,D) = I3.\nNotice that the program consisting only of (11) and (12) produces I1 as the chase, which makes the BCQ \u2203x\u2203y P(x ,y) evaluate to false. With the program also including the egd (13) the answer is now true, which shows that consistent egds may a ect CQ answers. is is in line with the use of a modi ed chase procedure that applies them along with the tgds.\nNow consider program \u03a0\u2032 that is \u03a0 with the extra rule R(x ,y) \u2192 \u2203z S(z,x ,y), which enforced on I3 results in I4 = {R(a,b), S(b, \u03b61, \u03b61), P(b, \u03b61), S(\u03b63,a,b)}. Now (13) is applied, which creates a chase failure as it tries to equate constants a and b. is is case where the set of tgds and the egd are mutually inconsistent."}, {"heading": "2.3 The Hurtado-Mendelzon Multidimensional Data Model", "text": "According to the Hurtado-Mendelzon multidimensional data model (in short, the HMmodel) [53], a dimension schema, H = \u3008K ,\u2197\u3009, consists of a nite set K of categories, and an irre exive, binary relation\u2197, called the child-parent relation, between categories (the rst category is a child and the second category is a parent). e transitive and re exive closure of\u2197 is denoted by\u2197\u2217, and is a partial order (a la ice) with a top category, All, which is reachable from every other category: K\u2197\u2217 All, for every category K \u2208 K . ere is a unique base category, Kb , that has no children. ere are no \u201cshortcuts\u201d, i.e. if K \u2197 K \u2032, there is no category K \u2032\u2032, di erent from K and K \u2032, with K \u2197\u2217 K \u2032\u2032, K \u2032\u2032\u2197\u2217 K \u2032.\nA dimension instance for schema H is a structure L = \u3008U, <,m \u3009, whereU is a non-empty, nite set of data values called members, < is an irre exive binary relation between members, also called a child-parent relation (the rst member is a child and the second member is a parent),9 and 9 ere are two child-parent relations in a dimension:\u2197, between categories; and <, between category members.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nm : U \u2192 K is the total membership function. Relation < parallels (is consistent with) relation \u2197: e < e \u2032 implies m(e) \u2197 m(e \u2032). e statement m(e) = K is also expressed as e \u2208 K . <\u2217 is the transitive and re exive closure of <, and is a partial order over the members. ere is a unique member all, the only member of All, which is reachable via <\u2217 from any other member: e <\u2217 all, for every member e . A child member in < has only one parent member in the same category: for members e , e1, and e2, if e < e1, e < e2 and e1, e2 are in the same category (i.e. m(e1) = m(e2)), then e1 = e2. <\u2217 is used to de ne the roll-up relations for any pair of distinct categories K \u2197\u2217 K \u2032: LK \u2032\nK (L ) = {(e, e \u2032) | e \u2208 K , e \u2032 \u2208 K \u2032 and e <\u2217 e \u2032}.\nPatientsDiseases\nWard Disease Day Count\nW4 Lung Cancer Jan/10, 2016 1\nW3 Malaria Feb/16, 2016 2\nW1 Coronary Artery Mar/25, 2016 5\nDay Temporal dimension\nW4W3W2W1\nWard Hospital dimension\nDisorder dimension\nDisease\nFig. 6. An HM model\nExample 2.4. e HM model in Figure 6 includes three dimension instances: Temporal and Disorder (at the top) and Hospital (at the bo om). ey are not shown in full detail, but only their base categories Day, Disease, and Ward, resp. We will use four di erent dimensions in our running example, the three just mentioned and also Instrument (cf. Example 3.2). For the Hospital dimension, shown in detail in Figure 3, K = {Ward,Unit, Institution,AllHospital}, with base category Ward and top category AllHospital . e child-parent relation\u2197 contains (Institution ,AllHospital), (Unit, Institution), and (Ward,Unit). e category of each member is speci ed by m, e.g. m(H1) = Institution. e child-parent relation < between members contains (W1, standard), (W2, standard), (W3, intensive), (W4, terminal), (standard,H1), (intensive,H1), (terminal,H2), (H1, allHospital), and (H2, allHospital). Finally, LInstitutionWard is one of the roll-up relations and contains (W1,H1), (W2,H1), (W3,H1), and (W4,H2).\nIn the rest of this section we show how to represent an HM model in relational terms. is representation will be used in the rest of this paper, in particular to extend the HM model. We introduce a relational dimension schemaH = K \u222aL, whereK is a set of unary category predicates, and L (for \u201cla ice\u201d) is a set of binary child-parent predicates, with the rst a ribute as the child and the second as the parent. e data domain of the schema isU (the set of category members). Accordingly, a dimension instance is a database instance DH for H that gives extensions to predicates inH . e extensions of the category predicates form a partition ofU.\nIn particular, for each category K \u2208 K there is a category predicate K(\u00b7) \u2208 K , and the extension of the predicate contains the members of the category. Also, for every pair of categories K , K \u2032 with K \u2197 K \u2032, there is a corresponding child-parent predicate, say KK \u2032(\u00b7, \u00b7), in L, whose extension contains the child-parent, <-relationships between members of K and K \u2032. In other words, each child-parent predicate in L stands for a roll-up relation between two categories in child-parent relationship.\nExample 2.5. (ex 2.4 cont.) In the relational representation of the Hospital dimension (cf. Figure 3), schema K contains unary predicates Ward(\u00b7), Unit(\u00b7), Institution(\u00b7) and AllHospital(\u00b7). e instance\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nDH gives them the extensions: Ward = {W1,W2,W3,W4}, Unit = {standard, intensive, terminal}, Institution = {H1,H2} and AllHospital = {allHospital}. L contains binary predicates: WardUnit(\u00b7, \u00b7), UnitInstitution(\u00b7, \u00b7), and InstitutionAllHospital(\u00b7, \u00b7), with the following extensions: WardUnit = {(W1, standard), (W2, standard), (W3, intensive), (W4, terminal)}, UnitInstitution = {(standard,H1), (intensive,H1), (terminal,H2)}, and InstitutionAllHospital = {(H1, allHospital), (H2, allHospital)}.\nIn order to recover the hierarchy of a dimension in its relational representation, we have to impose some ICs. First, inclusion dependencies (IDs) associate the child-parent predicates to the category predicates. For example, the following IDs: WardUnit[1] \u2286 Ward[1], and WardUnit[2] \u2286 Unit[1]. We need key constraints for the child-parent predicates, with the rst a ribute (child) as the key. For example, WardUnit[1] is the key a ribute for WardUnit(\u00b7, \u00b7), which can be represented as the egd: WardUnit(x ,y),WardUnit(x , z) \u2192 y = z.\nAssume H is the relational schema with multiple dimensions. A fact-table schema over H is a predicate T (C1, ...,Cn ,M), where C1, ...,Cn are a ributes with domains Ui for subdimensions Hi , and M is the measure a ribute with a numerical domain. A ribute Ci is associated with base-category predicate Kbi (\u00b7) \u2208 Ki through the ID: T [i] \u2286 Kbi [1]. Additionally, {C1, ...,Cn} is a key forT , i.e. each point in the base multidimensional space is mapped to at most one measurement. A fact-table provides an extension (or instance) for T . For example, in the center of Figure 6, the fact table PatientsDiseases is linked to the base categories of the three participating dimensions through its a ributes Ward, Disease, and Day, upon which its measure a ribute Count functionally depends. is multidimensional representation enables aggregation of numerical data at di erent levels of granularity, i.e. at di erent levels of the hierarchies of categories. e roll-up relations can be used for aggregation."}, {"heading": "3 THE ONTOLOGICAL MULTIDIMENSIONAL DATA MODEL", "text": "In this section, we present the OMD model as an ontological, Datalog+-based extension of the HM model. In this section we will be referring to the working example from Section 1, extending it along the way when necessary to illustrate elements of the OMD model.\nAn OMD model has a database schema RM = H \u222a Rc , where H is a relational schema with multiple dimensions, with sets K of unary category predicates, and sets L of binary, child-parent predicates (cf. Section 2.3); and Rc is a set of categorical predicates, whose categorical relations can be seen as extensions of the fact-tables in the HM model.\nA ributes of categorical predicates are either categorical, whose values are members of dimension categories, or non-categorical, taking values from arbitrary domains. Categorical predicate are represented in the form R(C1, . . . ,Cm ;N1, . . . ,Nn), with categorical a ributes (the Ci ) all before the semi-colon (\u201c;\u201d), and non-categorical a ributes (the Ni ) all a er it. e extensional data, i.e the instance for the schema RM , is IM = DH \u222a I c , where DH is a complete database instance for subschemaH containing the dimensional predicates (i.e. category and child-parent predicates); and sub-instance I c contains possibly partial, incomplete extensions for the categorical predicates, i.e. those in Rc .\nEvery schema RM = H\u222aRc for an OMD model comes with some basic, application-independent semantic constraints. We list them next, represented as ICs.\n1. Dimensional child-parent predicates must take their values from categories. Accordingly, if child-parent predicate P \u2208 L is associated to category predicates K ,K \u2032 \u2208 K , in this order, we introduce IDs P[1] \u2286 K[1] and P[2] \u2286 K \u2032[1]), as ncs:\nP(x ,x \u2032),\u00acK(x) \u2192 \u22a5, and P(x ,x \u2032),\u00acK \u2032(x \u2032) \u2192 \u22a5. (14)\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nWe do not represent them as the tgds P(x ,x \u2032) \u2192 K(x), etc., because we reserve the use of tgds for predicates (in their right-hand sides) that may be incomplete. is is not the case for K or K \u2032, which have complete extensions in every instance. For this same reason, as mentioned right a er introducing ncs in (8), we use here ncs with negative literals: they are harmless in the sense that they are checked against complete extensions for predicates that do not appear in rule heads. en, this form of negation is the simplest case of strati ed negation [1].10 Checking any of these constraints amounts to posing a non-conjunctive query to the instance at hand (we retake this issue in Section 4.3).\n2. Key constraints on dimensional child-parent predicates P \u2208 K , as egds: P(x ,x1), P(x ,x2) \u2192 x1 = x2. (15)\n3. e connections between categorical a ributes and the category predicates are speci ed by means of IDs represented as ncs. More precisely, for the ith categorical position of predicate R taking values in category K , the ID R[i] \u2286 K[1] is represented by:\nR(x\u0304 ; y\u0304),\u00acK(x) \u2192 \u22a5, (16) where x is the ith variable in the list x\u0304 .\nExample 3.1. (ex. 1.1 cont.) e categorical a ributes Unit and Day of categorical predicate WorkSchedules(Unit,Day;Nurse, Speciality) in Rc are connected to the Hospital and Temporal dimensions, resp., which is captured by the IDs WorkSchedules[1] \u2286 Unit[1], and WorkSchedules[2] \u2286 Day[1]. e former is wri en in Datalog+ as in (16):\nWorkSchedules(u, d; n, t),\u00acUnit(u) \u2192 \u22a5. (17) For the Hospital dimension, one of the two IDs for the child-parent predicate WardUnit is\nWardUnit[2] \u2286 Unit[1], which is expressed by an nc of the form (14): WardUnit(w, u),\u00acUnit(u) \u2192 \u22a5.\ne key constraint of WardUnit is captured by an egd of the form (15):\nWardUnit(w, u),WardUnit(w, u\u2032) \u2192 u = u \u2032.\ne OMD model allows us to build multidimensional ontologies, OM . Each of them, in addition to an instance IM for a schema RM , includes a set \u2126M of basic constraints as in 1.-3. above, a set \u03a3M of dimensional rules, and a set \u03baM of dimensional constraints. All these rules and constraints are expressed in the Datalog+ language associated to schema RM . Below we introduce the general forms for dimensional rules in \u03a3M (those in 4.) and the dimensional constraints in \u03baM (in 5.), which are all application-dependent.\n4. Dimensional rules as Datalog+ tgds:\nR1(x\u03041; y\u03041), ...,Rn(x\u0304n ; y\u0304n), P1(x1,x \u20321), ..., Pm(xm ,x \u2032m) \u2192 \u2203y\u0304 \u2032 R\u2032(x\u0304 \u2032; y\u0304). (18) Here, Ri (x\u0304i ; y\u0304i ) and R\u2032(x\u0304 \u2032; y\u0304) are categorical predicates, the Pi are child-parent predicates, y\u0304 \u2032 \u2286 y\u0304, x\u0304 \u2032 \u2286 x\u03041 \u222a ... \u222a x\u0304n \u222a {x1, ...,xm ,x \u20321, ...,x \u2032m}, y\u0304ry\u0304 \u2032 \u2286 y\u03041 \u222a ... \u222a y\u0304n ; repeated variables in bodies (join variables) appear only in categorical positions in the categorical relations and a ributes in child-parent predicates.11\nNotice that existential variables appear only in non-categorical a ributes. e main reason for this condition is that in some applications we may have an existing, xed and closed-world 10 Datalog+with strati ed negation, i.e. that is not intertwined with recursion, is considered in [31]. 11 is is a natural restriction to capture dimensional navigation as captured by the joins (cf. Example 3.2).\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nmultidimensional database providing the multidimensional structure and data. In particular, we may not want to create new category elements via value invention, but only values for non-categorical a ributes, which do not belong to categories. We will discuss this condition in more detail and its possible relaxation in Section 6.4.\n5. Dimensional constraints, as egds or ncs, of the forms:\nR1(x\u03041; y\u03041), ...,Rn(x\u0304n ; y\u0304n), P1(x1,x \u20321), ..., Pm(xm ,x \u2032m) \u2192 z = z \u2032. (19) R1(x\u03041; y\u03041), ...,Rn(x\u0304n ; y\u0304n), P1(x1,x \u20321), ..., Pm(xm ,x \u2032m) \u2192 \u22a5. (20)\nHere, Ri \u2208 Rc , Pj \u2208 L, and z, z \u2032 \u2208 \u22c3 x\u0304i \u222a \u22c3 y\u0304j .\nSome of the lists in the bodies of (18)-(19) may be empty, i.e. n = 0 orm = 0. is allows us to represent, in addition to properly \u201cdimensional\u201d constraints, also classical constraints on categorical relations, e.g. keys or FDs."}, {"heading": "W4 Sep/5/2016 Cathy Noon", "text": ""}, {"heading": "W3 Aug/21/2016 Sara Noon", "text": "A general tgd of the form (18) can be used for upward- or downward-navigation (or, more precisely, upward or downward data generation) depending on the joins in the body. e direction is determined by both the di erence of category levels in a dimension of the categorical variables that appear in the body joins, and the value propagation to the rule head. To be more precise, consider the simplest case where (18) is of the form\nR(x\u0304 ; y\u03041), P(x1,x \u20321) \u2192 \u2203y\u0304 \u2032 R\u2032(x\u0304 \u2032; y\u0304), with a join between R(x\u0304 ; y\u03041) and P(x1,x \u20321) (via a categorical variable in x\u0304 ). When x1 \u2208 x\u0304 and x \u20321 \u2208 x\u0304 \u2032, one-step upward-navigation is enabled, from (the level of) x1 to (the level of) x \u20321. An example is \u03c31 in (1). Now, when x \u20321 \u2208 x\u0304 and x1 \u2208 x\u0304 \u2032, one-step downward-navigation is enabled. An example is \u03c32 in (22). More generally, multi-step navigation, between a category and an ancestor or descendant category, can be captured through a chain of joins with adjacent child-parent dimensional predicates in the body of a tgd (an example is (23) below). However, a general dimensional rule of the form (18) may contain joins in mixed directions, even on the same dimension.\nExample 3.2. (ex. 3.1 cont.) e le -hand-side of Figure 7 shows a dimensional constraint \u03b7 categorical relation WorkSchedules, which is linked to the Temporal dimension via the Day category.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nIt tells us (possibly because the Intensive care unit was closed during January) that: \u201cNo personnel was working in the Intensive care unit in January\u201d. It is a constraint of the form (20):\n\u03b7 : WorkSchedules(intensive,d ;n, s),DayMonth(d, jan) \u2192 \u22a5. (21)\ne dimensional rule \u03c31 in Figure 7 and given in (1) (as a tgd of the general form (18)) can be used to generate new tuples for relation WorkSchedules. en, constraint \u03b7 is expected to be satis ed both by the initial extensional tuples for WorkSchedules and its tuples generated through \u03c31, i.e. by its non-shaded tuples and shaded tuples in Figure 7, resp. In this example, \u03b7 is satis ed.\nNotice that WorkSchedules refers to the Day a ribute of the Temporal dimensions, whereas \u03b7 involves the Month a ribute. en, checking \u03b7 requires upward navigation through the Temporal dimension. Also the Hospital dimension is involved in the satisfaction of \u03b7: e tgd \u03c31 in may generate new tuples for WorkSchedules, by upward navigation from Ward to Unit.\nFurthermore, we have an additional tgd:\n\u03c32 : WorkSchedules(u,d ;n, t),WardUnit(w,u) \u2192 \u2203s Shi s(w,d ;n, s) (22) that can be used with WorkSchedules to generate data for categorical relation Shi s. e shaded tuple in it is one of those. is tgd re ects the institutional guideline stating that \u201cIf a nurse works in a unit on a speci c day, he/she has shi s in every ward of that unit on the same day\u201d. Accordingly, \u03c32 relies on downward navigation for tuple generation, from the Unit category level down to the Ward category level.\nHere, \u03c31 and \u03c32 in (1) and (22) are examples of tgds enabling upward and downward, onestep dimension navigation, resp. e following dimensional rule enables multi-step navigation, propagating doctors at the unit level all the way up to the hospital level:\nWardDoc(ward; na, sp),WardUnit(ward, unit),UnitInst(unit, ins) \u2192 HospDoc(ins; na, sp). (23)\nAssuming the ontology also has a categorical relation, erm(Ward, ertype;Nurse), with Ward and ertype categorical a ributes, the la er for an Instrument dimension, the following should be an egd of the form (19) saying that \u201cAll thermometers in a unit are of the same type\u201d:\nerm(w, t; n), erm(w \u2032, t \u2032; n\u2032),WardUnit(w, u),WardUnit(w \u2032, u) \u2192 t = t \u2032. (24)\nNotice that our ontological language allows us to impose a condition at the Unit level without having it as an a ribute in the categorical relation.12\nNotice that existential variables in dimensional rules, such as t and s as in (1) and (22), resp., make up for the missing, non-categorical a ributes Speciality and Shi in WorkSchedules and Shi s, resp.\nExample 3.3. (ex. 3.2 cont.) Rule \u03c32 supports downward tuple-generation. When enforcing it on a tuple WorkSchedules(u,d ;n, t), via category member u (for Unit), a tuple for Shi s is generated for each childw ofu in the Ward category for which the body of \u03c32 is true. For example, chasing \u03c32 with the third tuple inWorkSchedules generates two new tuples in Shi s: Shi s(W2, sep/6/2016, helen, \u03b6 ) and Shi s(W1, sep/6/2016, helen, \u03b6 \u2032), with fresh nulls, \u03b6 and \u03b6 \u2032. e la er tuple is not shown in Figure 7 since it is dominated by the third tuple, Shi s(W1, sep/6/2016, helen,morning), in Shi s (i.e. the existing tuple is more general or informative than the one that would be introduced with a null value, and also it already serves as a witness for the existential statement).13 With the old and new tuples we can obtain the answers to the query about the wards of Helen on Sep/6/2016: Q \u2032(w) : \u2203s Shi s(w, sep/6/2016, helen, s). ey areW1 andW2. 12 If we have that relation, then (24) could be replaced by a \u201cstatic\u201d, non-dimensional FD. 13 Eliminating those dominated tuples does not have any impact on certain query answering.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nIn contrast, the join between Shi s and WardUnit in \u03c31 enables upward-dimensional navigation; and the generation of only one tuple for WorkSchedules from each tuple in Shi s, because each Ward member has at most one Unit parent."}, {"heading": "4 COMPUTATIONAL PROPERTIES OF THE OMDMODEL", "text": "As mentioned before, without any restrictions Datalog+ programs conjunctive query answering (CQA) may be undecidable, even without constraints [27]. Accordingly, it is important to identify classes of programs for which CQA is decidable, and hopefully in polynomial time in the size of the underlying database, i.e. in data complexity. Some classes of this kind have been identi ed. In the rest of this section we introduce some of them that are particularly relevant for our research. We show that under natural assumptions or OMD ontologies belong to those classes. In general, those program classes do not consider constraints. At the end of the section we consider the presence of them in terms of their e ect on QA."}, {"heading": "4.1 Weakly-Acyclic, Sticky and Weakly-Sticky Programs", "text": "Weakly-acyclicDatalog\u00b1 programs (without constraints) form a syntactic class of Datalog+ programs that is de ned appealing to the notion of dependency graph [37]. e dependency graph (DG) of a Datalog+ program \u03a0 is a directed graph whose vertices are the positions of the program\u2019s schema. Edges are de ned as follows. For every \u03c3 \u2208 \u03a0 and universally quanti ed variable (\u2200-variable) x in head(\u03c3 ) and position p in body(\u03c3 ) where x appears: (a) Create an edge from p to position p \u2032 in head(\u03c3 ) where x appears (representing the propagation of a value from a position in the body of a rule to a position in its head). (b) Create a special edge from p to position p \u2032\u2032 in head(\u03c3 ) where an \u2203-variable z appears (representing a value invention in the position of an existential variable in the rule head). e rank of a position p, rank(p), is the maximum number of special edges on ( nite or in nite) paths ending at p. \u03c0F (\u03a0) denotes the set of nite-rank positions in \u03a0. A program is WeaklyAcyclic (WA) if all of the positions have nite-rank.\ne chase for these programs stops in polynomial time in the size of the extensional data, making CQA ptime-complete in data complexity [37], but 2exptime-complete in combined complexity, i.e. in the combined size of the program, query and data [60].\nSticky Datalog+ programs (without constraints) are characterized through a marking procedure on body variables program rules. For a program \u03a0, the procedure has two steps: (a) Preliminary step: For every \u03c3 \u2208 \u03a0 and variable x in body(\u03c3 ), if there is an atom in head(\u03c3 )\nwhere x does not appear, mark every occurrence x in body(\u03c3 ). (b) Propagation step: For every \u03c3 \u2208 \u03a0, if a marked variable in body(\u03c3 ) appears in position p, then\nfor every \u03c3 \u2032 \u2208 \u03a0, mark every occurrence of a variable in body(\u03c3 \u2032) that also appears in head(\u03c3 \u2032) in position p.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nExample 4.2. Consider program \u03a0 on the le -hand side below., with its second rule already showing marked variables (with a hat) a er the preliminary step. e set of rules on the right-hand side show the result of whole marking procedure.\nR(x ,y), P(x , z) \u2192 S(x ,y, z). R(x\u0302 ,y), P(x\u0302 , z\u0302) \u2192 S(x ,y, z). S(x\u0302 ,y, z\u0302) \u2192U (y). S(x\u0302 ,y, z\u0302) \u2192U (y).\nU (x) \u2192 \u2203y R(y,x). U (x) \u2192 \u2203y R(y,x). For example, x is marked in S[1] in the body of the second rule (a er the preliminary step). For the propagation step, we nd S[1] in the head of the rst rule, containing x (it could have been a di erent variable). en the occurrences of x in the body of the rst rule have to be marked too, in positions R[1] and P[1].\nA Datalog+ program \u03a0 is sticky when, a er applying the marking procedure, there is no rule with a marked variable appearing more than once in its body. (Notice that a variable never appears both marked and unmarked in a same body.) Accordingly, the program in Example 4.2 is not sticky: marked variable x in the rst rule\u2019s body appears in a join (in R[1] and P[1]). e stickiness property for a program guarantees that, given a CQ, a nite initial fragment of the possibly in nite chase can be used for answering the query; actually, a fragment of polynomial size in that of the extensional data (cf. [29] and [74] for a more detailed discussion). As a consequence, CQA on sticky programs is in ptime in data. (It is exptime-complete in combined complexity [29].) Even more, CQA over sticky programs enjoys rst-order rewritable [49], that is, a CQ posed to the program can be rewri en into an FO query that can be evaluated directly on the extensional data.\nNone of the well-behaved classes of weakly-acyclic and sticky programs contain the other, but they can be combined into a new syntactic class of weakly-sticky (WS) programs that extends both original classes. Again, its characterization does not depend on the extensional data, and uses the already introduced notions of nite-rank and marked variable: A program \u03a0 (without constraints) is weakly-sticky if every repeated variable in a rule body is either non-marked or appears in some position in \u03c0F (\u03a0) (in that body).\nExample 4.3. Consider program \u03a0 already showing the marked variables:\nR(x\u0302 , y\u0302) \u2192 \u2203z R(y, z). R(x\u0302 , y\u0302),U (y\u0302),R(y\u0302, z\u0302) \u2192 R(x , z).\nHere, \u03c0F (\u03a0) = {U [1]}. e only join variable is y in the second rule, which appears in U [1]. Since U [1] \u2208 \u03c0F (\u03a0), \u03a0 is WS. Now, let \u03a0\u2032 be obtained from \u03a0 by replacing the second rule by (the already marked) rule: R(x\u0302 , y\u0302),R(y\u0302, z\u0302) \u2192 R(x , z). Now, \u03c0F (\u03a0\u2032) = \u2205, and the marked join variable y in the second rule appears in R[1] and R[2], both non- nite (i.e. in nite) positions. en, \u03a0\u2032 is not WS.\ne WS conditions basically prevent marked join variables from appearing only in in nite (i.e. in nite-rank) positions. With WS programs the chase may not terminate, due to an in nite generation and propagation of null values, but in nite positions only nitely many nulls may appear, which restricts the values that the possibly problematic variables, i.e. those marked in joins, may take (cf. [74] for a discussion). For WS programs CQA is tractable. Actually, CQA can be done on initial, query-dependent fragments of the chase of polynomial size in data. CQA is tractable, but ptime-complete in data, and 2exptime-complete in combined complexity [29]. In the following and as usual with a Datalog\u00b1 program \u03a0, we say \u03a0 is weakly-acyclic, sticky or weakly-sticky, etc., if its set of tgds has those properties.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017."}, {"heading": "4.2 OMD Ontologies as Weakly-Sticky Datalog\u00b1 Programs", "text": "In this section we investigate the ontologies OM used by the OMD model as Datalog\u00b1 programs. We start by considering only their subontologies \u03a3M formed by their tgds. e impact of the set \u03baM of constraints in OM is analyzed in Section 4.3.\nIt turns out that the MD ontologies are weakly-sticky. Intuitively, the main reason is that the join variables in the dimensional tgds are in the categorical positions, where nitely many members of dimensions can appear during the chase, because no existential variable (\u2203-variable) occurs in a categorical position; so, no new values are invented in them positions during the chase.\nProposition 4.4. MD ontologies are weakly-sticky Datalog\u00b1 programs.\nProof of Proposition 4.4: e tgds are of the form (18):\nR1(x\u03041; y\u03041), ...,Rn(x\u0304n ; y\u0304n), P1(x1,x \u20321), ..., Pm(xm ,x \u2032m) \u2192 \u2203y\u0304 \u2032 R\u2032(x\u0304 \u2032; y\u0304), where: (a) y\u0304 \u2032 \u2286 y\u0304, (b) x\u0304 \u2032 \u2286 x\u03041 \u222a ... \u222a x\u0304n \u222a {x1, ...,xm ,x \u20321, ...,x \u2032m}, (c) y\u0304ry\u0304 \u2032 \u2286 y\u03041 \u222a ... \u222a y\u0304n , and (d) repeated (i.e. join) variables in bodies are only in positions of categorical a ributes.\nWe have to show that every join variable in such a tgd either appears at least once in a nite-rank position or it is not marked. Actually, the former always holds, because, by condition (d), join variables appear only in categorical positions; and categorical positions, as we will show next, have nite, actually 0, rank (so, no need to investigate marked positions).14\nIn fact, condition (a) guarantees that there is no special edge in the dependency graph of a set of dimensional rules \u03a3M that ends at a categorical position. Also, (b) ensures that there is no path from a non-categorical position to a categorical position, i.e. categorical positions are connected only to categorical positions. Consequently, every categorical position has a nite-rank, namely 0.\ne proof establishes that every position involved in join in the body of a tgd has nite rank. However, non-join body variables in a tgd might still have in nite rank.\nExample 4.5. (ex. 3.2 cont.) For the MD ontology with \u03c31 and \u03c32, WorkSchedules[4] and Shi s[4] have in nite rank; and all the other positions have nite rank.\nCorollary 4.6. Conjunctive query answering on MD ontologies (without constraints) can be done in polynomial-time in data complexity.\ne tractability (in data) of CQA under WS programs was established in [29] on theoretical grounds, without providing a practical algorithm. An implementable, polynomial-time algorithm for CQA under WS programs is presented in [74]. Given a CQ posed to the program, they apply a query-driven chase of the program, generating a nite initial portion of the chase instance that su ces to answer the query at hand. Actually, the algorithm can be applied to a class that not only extends WS, but is also closed under magic-set rewriting of Datalog+ programs [4], which allows for query-dependent optimizations of the program [74].\nUnlike sticky programs, for complexity-theoretic reasons,WS programs do not allow FO rewritability for CQA. However, a hybrid algorithm is proposed in [75]. It is based on partial grounding of the tgds using the extensional data, obtaining a sticky program, and a subsequent rewriting of the query. ese algorithms can be used for CQA under our MD ontologies. However, presenting the details of these algorithms is beyond the scope of this paper.\n14 Actually, we could extend the MD ontologies by relaxing the condition on join variables in the dimensional rules, i.e. condition (d), while still preserving the weakly-sticky condition. It is by allowing non-marked joins variables in non-categorical positions.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017."}, {"heading": "4.3 OMD Ontologies with Constraints", "text": "In order to analyze the impact of the constraints on MD ontologies, i.e. those in 1.-3., 5. in Section 3, on CQA, we have to make and summarize some general considerations on constraints in Datalog+ programs. First, the whole discussion on constraints of Section 2.2 apply here. In particular, the presence of constraints may make the ontology inconsistent, in which case CQA becomes trivial. Furthermore, in comparison to a program without constraints, the addition of the la er to the same program may change query answers, because some models of the ontology may be discarded. Furthermore, CQA under ncs can be reduced to CQA without them.\nFirst, those in (14) and (16) are ncs with negative literals in their bodies. e former capture the structure of the underlying multidimensional data model (as opposed to the ontological one). ey can be checked against the extensional database D. If they are satis ed, they will stay as such, because the dimensional tgds in (18) do not invent category members. If the underlying multidimensional database has been properly created, those constraints will be satis ed and preserved as such. e same applies to the negative constraints in (16): the dimensional tgds may invent only non-categorical values in categorical relations. (cf. Section 6.4 for a discussion.)\nAs discussed in Section 3, egds may be more problematic since there may be interactions between egds and tgds during the chase procedure: the enforcement of a tgd may activate an egd, which in turn may make some tgds applicable, etc. (cf. Section 2.2). Actually, these interactions between tgds and egds, make it in general impossible to postpone egd checking or enforcement until all tgds have been applied: tgd-chase steps and egd-chase steps may have to be interleaved. When the (combined) chase does not fail, the result is a possibly in nite universal model that satis es both the tgds and egds [31]. e interaction of tgds and egds may lead to undecidability of CQA [27, 34, 58, 72]. However, a separability property of the combination of egds and tgds guarantees a harmless interaction that makes CQA decidable and preserves CQA [29]: For a program \u03a0 with extensional database D, a set of tgds \u03a0R , and a set of egds \u03a0C , \u03a0R and \u03a0C are separable if either (a) the chase with \u03a0 fails, or (b) for any BCQ Q, \u03a0 |= Q if and only if \u03a0R \u222a D |= Q.\nIn Example 2.3, the tgds and the egd are not separable as the chase does not fail, and the egd changes CQ answers (in that case, \u03a0 6 |= Q and \u03a0\u2032 |= Q).\nSeparability is a semantic property, relative to the chase, and depends on a program\u2019s extensional data. If separability holds, combined chase failure can be detected by posing BCQs (with ,, and obtained from the egds\u2019 bodies) to the program without the egds [30, theo. 1]. However, separability is undecidable [30]. Hence the need for an alternative, syntactic, decidable, su cient condition for separability. Such a condition has been identi ed for egds that are key constraints [31]; it is that of non-con icting interaction.15 Intuitively, the condition guarantees that the tgds can only generate tuples with new key values, so they cannot violate the key dependencies.\nBack to our OMD ontologies, it is easy to check that the egds of the form (15) in 2., actually key constraints, are non-con icting, because they satisfy the rst of the conditions for non-con icting interaction. en, they are separable from the dimensional constraints as egds.\nMore interesting and crucial are the dimensional constraints under 5.. ey are applicationdependent ncs or egds. Accordingly, the discussion in Section 4.3 applies to them, and not much can be said in general. However, for the combination of dimensional tgds and dimensional egds in OMD ontologies, separability holds when the egds satisfy a simple condition.\n15 e notion has been extended to FDs in [29]: A set of tgds \u03a0R and a set \u03a0C of FDs are non-con icting if, for every tgd \u03c3 , with set U\u03c3 of non-existential(lly quanti ed variables for) a ributes in head(\u03c3 ), and FD \u03f5 of the form R : A\u0304\u2192 B\u0304 , at least one of the following holds: (a) head(\u03c3 ) is not an R-atom, (b) U\u03c3 + A\u0304, or (c) U\u03c3 = A\u0304 and each \u2203-variable in \u03c3 occurs just once in the head of \u03c3 .\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nProposition 4.7. For an MD ontology OM with a set \u03a3M of tgds as in (18) and set \u03baM of egds as in (19), if for every egd in \u03baM the variables in the head occur in categorical positions in the body, then separability holds.\nProof of Proposition 4.7: Let DM be the ontology\u2019s extensional data. We have to show that if chase(OM) does not fail, then for every BCQQ, chase(OM) |= Q if and only if chase(\u03a3M ,DM) |= Q.\nLet\u2019s assume that the chase with OM does not fail. As we argued before Proposition 4.4, no null value replaces a variable in a categorical position during the chase with OM . For this reason, the variables in the heads of egds are never replaced by nulls. As a result, the egds can only equate constants, leading to chase failure if they are di erent. Since we assumed the chase with OM does not fail, the egds are never applicable during the chase or they do not produce anything new (when the two constants are indeed the same), so they can be ignored, and the same result for the chase with or without egds.\nAn example of dimensional egd as in Proposition 4.7 is (24). Also the key constraints in (15) satisfy the syntactic condition. In combination with Proposition 4.4, we obtain:\nCorollary 4.8. Under the hypothesis of Proposition 4.7, CQA from an MD ontology can be done in polynomial-time in data.\nProof of Corollary 4.8: From the proof of Proposition 4.7, we have that the chase(OM) never fails, and the egds can be eliminated. en CQA can be correctly done with the extensional database and the tgds, which can de done in polynomial time in data."}, {"heading": "5 CONTEXTUAL DATA QUALITY SPECIFICATION AND EXTRACTION", "text": "e use of the OMD model for quality data speci cation and extraction generalizes a previous approach to- and work on context-based data quality assessment and extraction [16, 19], which was brie y described in Section 1. e important new element in comparison to previous work is the presence in an ontological context Oc as in Figure 5 of the core multi-dimensional (MD) ontology OM represented by an OMD model as introduced in Section 3.\nIn the rest of this section we show in detail the components and use of an MD context in quality data speci cation and extraction, for which we refer to Figure 5. For motivation and illustration we use a running example that extends those in Sections 1 and 3.\nOn the LHS of Figure 5, we nd a database instance, D, for a relational schema R = {R1, ...,Rn}. e goal is to specify and extract quality data from D. For this we use the contextual ontology Oc shown in the middle, which contains the following elements and components: (a) Nickname predicates R\u2032 in a nickname schema R \u2032 for predicates R in R. ese are copies of the\npredicates for D and are populated exactly as in D, by means of the simple mappings (rules) forming a set \u03a3\u2032 of tgds, of the form:\nR(x\u0304) \u2192 R\u2032(x\u0304). (25) whose enforcement producing a material or virtual instance D \u2032 within Oc . (b) e core MD ontology, OM , as in Section 3, with an instance IM = DH \u222a I c , a set \u03a3M of dimensional tgds, and a set \u03baM of dimensional constraints, among them egds and ncs. (c) ere can be, for data quality use, extra contextual data forming an instance E, with schema RE , that is not necessarily part of (or related to) the OMD ontology OM . It is shown in Figure 5 on the RHS of the middle box. (d) A set of quality predicates, P, with their de nitions as Datalog rules forming a set \u03a3P of tgds. ey may be de ned in terms of predicates in RE , built-ins, and dimensional predicates in RM .\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nWe will assume that quality predicates in P do not appear in the core dimensional ontology OM that de nes the dimensional predicates in RM . As a consequence, the program de ning quality predicates can be seen as a \u201ctop layer\u201d, or top sub-program, that can be computed a er the core (or base) ontological program has been computed.16 For a quality predicate P \u2208 P, its de nition of the form:\n\u03c6EP (x\u0304),\u03c6MP (x\u0304) \u2192 P(x\u0304). (26)\nHere, \u03c6EP (x\u0304) is a conjunction of atoms with predicates in RE or plus built-ins, and \u03c6 M P (x\u0304) is a conjunction of atoms with predicates in RM .17 Due to their de nitions, quality predicates in the context can be syntactically told apart from dimensional predicates. ality predicate re ect application dependent, speci c quality concerns.\nExample 5.1. (ex. 1.1 and 3.1 cont.) Predicate for Temperatures \u2208 R, the initial schema, has Temperatures\u2032 \u2208 R \u2032 as a nickname, and de ned by Temperatures(x\u0304) \u2192 Temperatures\u2032(x\u0304). e former has Table 1 as extension in instance D, which is under quality assessment, and with this rule, the data are copied into the context. e core MD ontology OM has WorkSchedules and Shi s as categorical relations, linked to the Hospital and Temporal dimensions (cf. Figure 4). OM has a set of dimensional tgds, \u03a3M , that includes \u03c31 and \u03c32, and also a dimensional rule de ning a categorical relation WorkTimes, as a view in terms of WorkSchedules the TimeDay child-parent dimensional relation, to create data from the day level down to the time (of the day) level:\nWorkSchedules(u,d ;n, s), TimeDay(t ,d) \u2192 WorkTimes(u, t ;n, s). (27)\nOM also has a set \u03baM of dimensional constraints, including the dimensional nc and egd, (21) and (24), resp.\nNow, in order to address data quality concerns, e.g. about certi ed nurses or thermometers, we introduce quality predicates, e.g. TakenWith erm, about times at which nurses use certain thermometers, with a de nition of the form (26):\nWorkTimes(intensive, t ;n,y) \u2192 TakenWith erm(t ,n, b1), (28) which captures the guideline about thermometers used in intensive care units; and becomes a member of \u03a3P (cf. Figure 5).\nIn this case, we are not using any contextual database E outside the MD ontology, but we could have an extension for a predicate Supply(Ins, ) \u2208 RE , showing thermometer brands ( ) supplied to hospital institutions (Ins), in the Hospital dimension.18 It could be used to de ne (or supplement the previous de nition of) TakenWith erm(t,n,th):\nSupply(ins, th),UnitInstitution(u, ins),WorkTimes(u, t ;n,y) \u2192 TakenWith erm(t, n, th). (29)\n16 is assumption does not guarantee that the resulting, combined ontology has the same syntactic properties of the core MD ontology, e.g. being WS (cf. Example 5.3), but the analysis of the combined ontology becomes easier, and in some cases it allows us to establish that the combination inherits the good computational properties from the MD ontology. We could allow de nitions of quality predicates in Datalog with strati ed negation (not) or even in Datalog+. In the former case, the complexity of CQA would not increase, but in the la er we cannot say anything general about the complexity of CQA. 17 We could also have predicates from P in the body if we allow mutual or even recursive dependencies between quality predicates. 18 E could represent data brought from external sources, possible at query answering time [16, 19]. In this example, it governmental data about hospital supplies.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nNow the main idea consists in using the data brought into the context via the nickname predicates and all the contextual elements to specify quality data for the original schema R, as a quality alternative to instance D.\n(e) We introduce a \u201cquality schema\u201d, Rq , a copy of schema R, with a predicate Rq for each predicate R \u2208 R. ese are quality versions of the original predicates. ey are de ned, and populated if needed, through quality data extraction rules that form a set, \u03a3q (cf. Figure 5), of Datalog rules of the form:\nR\u2032(x\u0304),\u03c8 PR\u2032(x\u0304) \u2192 R q(x\u0304). (30)\nHere,\u03c8 PR\u2032(x\u0304) is an ad hoc for predicate R conjunction of quality predicates (in P) and builtins. e connection with the data in the corresponding original predicate is captured with the join with its nickname predicate R\u2032.19\nDe nitions of the initial predicates\u2019 quality versions impose conditions corresponding to user\u2019s data quality pro les, and their extensions form the quality data (instance).\nExample 5.2. (ex. 5.1 cont.) e quality version of the original predicate Temperatures is Temperaturesq \u2208 Rq , de ned by:\nTemperatures\u2032(t ,p,v,n), TakenWith erm(t ,n, b1) \u2192 Temperaturesq(t ,p,v,n), (31) imposing extra quality conditions on the former. is is a de nition of the form (30) in \u03a3q (cf. also Example 1.2)."}, {"heading": "5.1 Computational Properties of the Contextual Ontology", "text": "In Section 4, we studied the computational properties of MD ontologies without considering additional rules de ning quality predicates and quality versions of tables in D. In this regard, it may happen that the combination of Datalog\u00b1 ontologies that enjoy good computational properties may be an ontology without such properties [8, 9]. Actually, in our case, the contextual ontology may not preserve the syntactic properties of the core MD ontology.\nExample 5.3. (ex. 4.5 and 5.2 cont.) To theWSMD ontology containing the dimensional rules\u03c31, \u03c32. we can add a non-recursive Datalog rule de ning a quality predicate SameShi (Ward,Day;Nurse1, Nurse2) saying that Nurse1 and Nurse2 have the same shi s at the same ward and on the same day:\n\u03c33 : Shi s(w,d ;n, s), Shi s(w,d ;n\u2032, s) \u2192 SameShi (w,d ;n,n\u2032). Now, \u03a3 = {\u03c31,\u03c32,\u03c33} is not WS since variable s in the body of \u03c33 is a repeated marked body variable only appearing in in nite-rank position Shi s[4]. is shows that the even the de nition of a quality predicates in plain Datalog may break the WS property.\nUnder our layered (or modular) approach (cf. item (d) at the beginning of this section), according to which de nitions in \u03a3P and \u03a3q belong to Datalog programs that call predicates de ned in the MD ontology OM as extensional predicates, we can guarantee that the good computational properties of the core MD ontology still hold for the contextual ontology Oc . In fact, the top Datalog program can be computed in terms of CQs and iteration starting from extensions for the dimensional predicates. In the end, all this can be done in polynomial time in the size of the initial extensional database. e data in the non-dimensional, contextual, relational instance E are also called as extensional data by \u03a3P and \u03a3q . Consequently, this is not a source of additional complexity. 19 As in the previous item, these de nitions could be made more general, but we keep them like this to x ideas. In particular, Rq could be de ned not only in terms of R (or its nickname R\u2032), but also from other predicates in the original (or, be er, nickname) schema.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nus, even when weak-stickiness does not hold for the combined contextual ontology, CQA is still tractable."}, {"heading": "5.2 ery-Based Extraction of ality Data", "text": "In this section we present a methodology to obtain quality data through the context on the basis of data that has origin in the initial instance D. e approach is query based, i.e. queries are posed to the contextual ontology Oc , and in its language. In principle, any query can be posed to this ontology, assuming one knows its elements. However, most typically a user will know about D\u2019s schema R only, and the (conjunctive) query, Q, will be expressed in language L(R), but (s)he will still expect quality answers. For this reason, Q is rewri en into a query Qq , the quality version of Q, that is obtained by replacing every predicate R \u2208 R in it by its quality version Rq (notice that Qq is also conjunctive). is idea leads as to the following notion of quality answer to a query.\nDe nition 5.4. Given instance D of schema R and a conjunctive query Q \u2208 L(R), a sequence of constants c\u0304 is a quality answer to Q from D via Oc i Oc |= Qq[c\u0304], where Qq is the quality version of Q, and Oc is the contextual ontology containing the MD ontology OM , and into which D is mapped via rules (25). QAns(Q,D,Oc ) denotes the set of quality answers to Q from D via Oc .\nA particular case of this de nition occurs when the query is an open atomic query, say Q : R(x\u0304), with R \u2208 R. We could de ne the core quality version of D, denoted by Coreq(D), as the database instance for schema R obtained by collecting the quality answers for these queries:\nCoreq(D) := {R(c\u0304) | Oc |= Rq[c\u0304] and R \u2208 R}. (32) We just gave a model-theoretic de nition of quality answer. Actually, a clean answer to a query holds in every quality instance in the class al(D,Oc ) (cf. Figure 2). is semantic de nition has a computational counterpart: quality answers can be obtained by conjunctive query answering from ontology Oc, a process that in general will inherit the good computational properties of the MD ontology OM , as discussed earlier in this section.\nIn the rest of this section, we describe the alityQA algorithm (cf. Algorithm 1), given a CQ Q \u2208 L(R) and a contextual ontology Oc that imports data from instance D, computes QAns(Q,D,Oc ). e assumption is that we have an algorithm for CQA from the MD ontology OM . If it is a weaklysticky Datalog\u00b1 ontology, we can use the chase-based algorithm introduced in [74].20 We also assume that a separability check takes place before calling the algorithm (cf. Sections 4.3 and 6.2).\nFor the unfolding-based steps 2 and 3, we are assuming the predicate de nitions in \u03a3P and \u03a3q are given in non-recursive Datalog.21 Starting from the CQ Qq , unfolding results into a union of conjunctive queries (UCQs) (a union in the case predicates are de ned by more than one Datalog rule). Next, according to Step 4, each (conjunctive) disjunct of QM can be answered by the given algorithm for CQA from OM with extensional data in E and D \u2032 (the la er obtained by importing D into context Oc ). e algorithm can be applied in particular to compute the core clean version, Coreq(D), of D.\nExample 5.5. (ex. 5.2 cont.) e initial query in (5), asking for (quality) values for Tom Waits\u2019 temperature, is, according to Step 1 of alityQA, rst rewri en into:\nQq (v) : \u2203n \u2203t (Temperaturesq(t, tom waits, v, n) \u2227 11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016), 20Actually the algorithm applies to a larger class of Datalog\u00b1 ontologies, that of join-weakly-sticky programs that is closed under magic-sets optimizations [74]. 21 If they are more general, but under the modularity assumption of Section 5.1, we do not unfold, but do rst CQA on the Datalog programs de ning the top, non-dimensional predicates, and next, when the \u201cextensional\u201d dimensional predicates have to be evaluated, we call the algorithm for CQA for the MD ontology.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nALGORITHM 1: e alityQA algorithm Step 1: Replace each predicate R in Q with its corresponding quality version Rq , obtaining a CQ Qq over schema Rq .\nStep 2: Unfold in Qq the de nitions of quality-version predicates Rq given by the rules (30) in \u03a3q . Obtain a UCQ Qc in terms of predicates in R \u2032 \u222a P and built-ins.\nStep 3: Unfold in Qc the de nitions of quality predicates given by the rules (26) in \u03a3P . Obtain a UCQ QM in terms of predicates in R \u2032 \u222a RE \u222a RM , and built-ins.\nStep 4: Answer QM by CQA (for each of QM \u2019s disjuncts) over the extensional database E \u222a D \u2032 and the MD ontology OM .\nwhich can be answered using (31) to unfold according to Step 2 of alityQA, obtaining:\nQc (v) : \u2203n \u2203t (Temperatures\u2032(t , tom waits,v,n) \u2227 TakenWith erm(t ,n, b1) \u2227 11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016).\nStep 3 of alityQA uses the quality predicate de nition (28) for unfolding, obtaining the query: QM (v) : \u2203n \u2203t \u2203y (Temperatures\u2032(t , tom waits,v,n) \u2227WorkTimes(intensive, t ;n,y) \u2227\n11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016),\nexpressed in terms of Temperatures\u2032, predicates in RM , and built-ins. Finally, at Step 4 of alityQA, QM is answered as a CQ over OM and database D \u2032,22 using, for example, the QA algorithms in [74, 75]. Predicate unfolding may produce a UCQ rather than a CQ. For example, if we unfold predicate TakenWith erm according to both de nitions (28) and (29), we obtain the following UCQ: QM (v) : \u2203n \u2203t \u2203y (Temperatures\u2032(t , tom waits,v,n) \u2227WorkTimes(intensive, t ;n,y) \u2227\n11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016) \u2228 \u2203i \u2203n \u2203t \u2203u \u2203y (Temperatures\u2032(t , tom waits,v,n) \u2227 Supply(i, b1) \u2227 UnitInstitution(u, i) \u2227\nWorkTimes(u, t ;n,y) \u2227 11 :45-aug/21/2016 \u2264 t \u2264 12 :15-aug/21/2016)."}, {"heading": "6 DISCUSSION AND CONCLUSIONS", "text": "In this paper, we started from the idea that data quality is context-dependent. As a consequence, we needed a formal model of context for context-based data quality assessment and quality data extraction. For that, we followed and extended the approach in [16, 19], by proposing ontological contexts, and embedding multidimensional (MD) data models in them. For the la er, we took advantage of our relational reconstruction of the HM data model [53, 54]. e MD data model was extended with categorical relations, which are linked to categories at di erent levels of dimension hierarchies, and also with dimensional constraints rules. e la er add the capability of navigating multiple dimensions in both upward and downward directions. Although not shown here (but cf. [76]), it is possible to include in the ontological contexts semantic constraints usually present in the HM model, such as strictness and homogeneity,23 which guarantee summarizability (or aggregation) for the correct computation of cube views [53].\n22 e predicates in the nickname schema R\u2032 act as extensional predicates at this point, without creating any computational problems. 23 A dimension is strict when every category member has at most one parent in each higher category. It is homogeneous (a.k.a. covering) when every category member has at least one parent in each parent category.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nWe represented MD ontologies using the Datalog\u00b1 ontological language, and we showed that they fall in the syntactic class of WS Datalog\u00b1 programs, for which CQA is tractable. We also unveiled conditions under which separability of egds and rules holds.\nWe used and extended the MD ontologies with rules for data quality speci cation and extraction, and proposed a general methodology for quality data extraction via query answering. Our underlying approach to data quality is that the given database instance does not have all the necessary elements to assess the quality of data or to extract quality data. e context is used for that purpose, and provides additional information about the origin and intended use of data. Notice that from this point of view, our contexts can also be seen as enabling tailoring and focusing of given data for a particular application. is is a idea that deserves additional investigation.\nOur approach to quality data speci cation and extraction is declarative [18, 39]. It uses logicbased languages, namely relational calculus, Datalog and Datalog\u00b1to specify quality data. ese languages have a precise and clear semantics and their scope of applicability can be easily analyzed. It is also independent of any procedural mechanism for quality data extraction and data cleaning, but computational methods can be extracted from (or be based on) the speci cations e implementation of the alityQA algorithm and experiments to evaluate its performance correspond to ongoing work. e algorithm and its optimization is based on our work on QA under WS programs [74, 75].\nSome important possible extensions of- and issues about our OMD data model that deserve further investigation, have to do with: (a) Having categorical a ributes in categorical relations forming a key. (b) Adopting and using a repair semantics when the MD ontology becomes inconsistent. (c) Analyzing and implementing data quality extraction as a data cleaning or repair problem. (d) Allowing some predicates to be closed and the related problem of non-deterministic or uncertain value invention, mainly for downwards navigation. We brie y elaborate on each of them in Sections 6.1, 6.2, 6.3 and 6.4, respectively. ey correspond all to open areas of research. Hence the speculative style of the discussion."}, {"heading": "6.1 Categorical Keys", "text": "In our running example, the categorical relation WorkSchedules(Unit,Day;Nurse,Speciality), does not have {Unit,Day} as a key: multiple nurses might have work schedules in the same unit and on the same day. However, in many applications it may make sense to have the categorical a ributes forming a key for a categorical relation. (For example, in the HM model, the non-measure a ributes in a fact-table form a key.) is is not required by the basic OMD model, and such a key constraint has to be added.\nIf we assume that in a categorical relation R(C1, ...,Cn ;A1, ...,Am), {C1, ...,Cn} is a key for R, we have to include egds in the MD ontology, one for each pair yi \u2208 y\u0304, y \u2032i \u2208 y\u0304 \u2032:\nR(x\u0304 ; y\u0304),R(x\u0304 ; y\u0304 \u2032) \u2192 y \u2032i = yi . (33)\nWe can use our running example to show that dimensional rules and categorical keys of the form (33) may not be separable (cf. Section 4.3).\nExample 6.1. Consider the categorical relation InstitutionBoard(Institution;Chair, President,CEO) with Institution as a key. In particular, we have the egd:\nInstitutionBoard(i; c,p, e), InstitutionBoard(i; c \u2032,p \u2032, e \u2032) \u2192 c \u2032 = c .\nWe also have the dimensional rules (they di er on the underlined \u2203-variables on the RHS):\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nPatientUnit(u,d ;p),UnitInstiution(u, i) \u2192 \u2203c \u2203n InstitutionBoard(i; c, c,n). (34) PatientUnit(u,d ;p),UnitInstiution(u, i) \u2192 \u2203c \u2203n InstitutionBoard(i; c,n,n). (35)\nLet (standard, sep/5; tom waits) be the only tuple in the extension of PatientUnit. e egds de ning Institution as a key are not separable from the dimensional tgds (34) and (35) (cf. Section 4.3), because: (a) the chase does not fail since the egds only equate nulls invented by (34) and (35), and (b) the BCQ Q : \u2203i \u2203c InstitutionBoard(i, c, c, c) has a negative answer without the categorical key, but a positive answer with the categorical key. Actually, the combination of tgds and egds here is con icting (cf. Section 4.3), because no \u2203-variable in (34) or (35) appears in a key position, and then the tgds may generate di erent tuples with the same key values.\nDespite the possible non-separability introduced by categorical keys, CQA is still in ptime in data complexity, because no null value appears in categorical positions. As a consequence, there are polynomially many (in the size of data) key categorical values. ere are also polynomially many tgd-chase steps, including those that are applicable a er the egd-chase steps or due to nonseparability. is shows that the chase procedure, with tgd- and egd-chase steps (as we explained in Sections 2.2) runs in polynomial time for an MD ontology under categorical keys, and CQA can be done on the resulting chase instance.\nProposition 6.2. e data complexity of CQA on MD ontologies with categorical keys is in ptime."}, {"heading": "6.2 Inconsistent MD Ontologies", "text": "We discussed in Section 4.3 the presence of dimensional ncs and egds may lead to an inconsistent MD ontology (cf. Section 2.2). In this case, the ontology can be repaired according to an inconsistencytolerant semantics, so that it still gives semantically meaningful and non-trivial answers to queries under inconsistency. A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.\nAccording to this semantics, a repair of an inconsistent ontology O including an extensional instance I , is a consistent ontology with the same rules and constraints as O, but with an extensional instance I \u2032 that is maximally contained in I . e consistent answers to a query posed to O are those answers shared by all the repairs of the la er. QA under this semantics is np-hard in the size of I , already for DL [62] or Datalog\u00b1 ontologies [65, 66] with relatively low expressive power.\nRepairing the inconsistent ontology by changing the extensional instance amounts, in the case of an MD ontology OM , to possibly changing the MD instance. In this regard, we might want to respect the MD structure of data, in particular, semantic constraints that apply at that level, e.g. enforcing summarizability constraints mentioned earlier in this section. Repairs and consistent answers from MD databases have been investigated in [6, 7, 15], and also in [91], which proposes the path schema for MD databases as a be er relational schema for dealing with the kinds of inconsistencies that appear in them."}, {"heading": "6.3 ality Data Extraction as Inconsistency Handling", "text": "As pointed out in Section 1, context-based quality data extraction is reminiscent of database repairing and consistent query answering [14, 17]. Actually, we can reproduce from our context-based approach to data cleaning a scenario where cleaning can be seen as consistent query answering.\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\ne initial database D may not be subject to integrity constraints.24 However, as we can see in Example 5.2, the rule (31) could be seen as a rewriting of the queryQ(t ,p,v,n) : Temperatures\u2032(t ,p,v,n), performed to obtain the consistent answers (or consistent contents of Temperatures\u2019 in this case) w.r.t. the contextual inclusion dependency \u03c8 : Temperatures\u2032(t ,p,v,n) \u2192 TakenWith erm(t ,n, b1). e rewriting re ects a repair semantics based on deletions of tuples from Temperatures\u2032 when the constraint is not satis ed [17]. at is, predicate TakenWith erm(t ,n, b1) acts as a lter on predicate Temperatures\u2032. e quality version Coreq(D) of the initial instance D, de ned in (32), can be seen then as the intersection of all repairs of D w.r.t. these contextual constraints (more precisely, as the intersection of the instances in al(D,Oc )). Doing quality (or consistent) query answering directly from the intersection of all repairs is sound, but possibly incomplete. However, this has been a predominant approach to OBDA with inconsistent ontologies [62, 65, 66]: the ontology is repaired by repairing the extensional instance and considering the intersection of its repairs (cf. also Section 6.2).\nIn Section 1 we characterized our context-based approach to data quality mainly as one confronting incompleteness of data. However, we can also see it as addressing inconsistency w.r.t. constraints imposed at the contextual level rather than directly at the database level."}, {"heading": "6.4 Categorical Value Invention and Closed Predicates", "text": "We assumed in Section 3 that tgds do not have existential quanti ers on variables for categorical a ributes. is has two important consequences. First, the OMD programs become weakly-sticky (cf. Proposition 4.4); second, we can apply the CWA to categories and categorical a ributes (actually, without existential quanti cations on categorical a ributes, making the CWA or the OWA does not ma er for CQA). Relaxing this condition has two immediate e ects on the MD ontology: (a) We cannot make the CWA on dimension categories (and categorical a ributes) anymore (without violating the ncs in (16)); and (b) e set of tgds of an OMD ontology may not be weakly-sticky anymore. e following example shows both issues.\nTable 3. DischargePatients\nInst. Day Patient 1 H1 Sep/9 Tom Waits 2 H1 Sep/6 Lou Reed 3 H2 Oct/5 Elvis Costello 4 H1 Dec/16 Elvis Costello\nTable 4. PatientUnit\nUnit Day Patient 1 Standard Sep/5 Tom Waits 2 Standard Sep/9 Tom Waits 3 Intensive Sep/6 Lou Reed\nExample 6.3. Consider categorical relations DischargePatients (Table 3) and PatientUnit (Table 4), containing data on patients leaving an institution and on locations of patients, resp. Since, a patient was in a unit when discharged, we can use DischargePatient to generate data for PatientUnit, at the Unit level, down from the Institution level, through the tgd (with a conjunction in the head that can be eliminated),25\nDischargePatients(i,d ;p) \u2192 \u2203u (UnitInstitution(u, i) \u2227 PatientUnit(u, d; p)), (36) which invents values downwards, in the categorical position (for units) PatientUnit[1] and in the child-parent predicate UnitInstitution in its head. is may invent new category members, which 24 We have developed this case, but in principle we could have constraints on D , satis ed or not, and they could be mapped into the context for combination with the other elements there. 25 E.g. with DischargePatients(i, d ;p) \u2192 \u2203u TempPatient(i, u, d; p), TempPatient(i, u, d; p) \u2192 UnitInstitution(u, i), and TempPatient(i, u, d; p) \u2192 PatientUnit(u, d; p).\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\ncould be in con ict with the a CWA applied to category predicates, child-parent predicates, and indirectly via the ncs, to categorical a ributes.\nLet\u2019s now add the following tgds, respectively, saying that every patient eventually leaves the hospital, and de ning the patients\u2019 relationships of being on a day in the same unit.\nPatientUnit(u, d; p),UnitInstitution(u, i) \u2192 \u2203d \u2032DischargePatients(i,d \u2032;p), (37) PatientUnit(u, d; p), PatientUnit(u, d; p\u2032) \u2192 SameDay(d ;p,p \u2032), (38)\ne set of rules (36)-(38) is not weakly-sticky.26\nIf we accept value invention in OMD ontologies, then their weak-stickiness cannot be guaranteed, and has to be analyzed for each particular ontology. However, the issue raised by the example in relation to the invention of category members still persists.\nSometimes an existential quanti er is used to refer to an unspeci ed element in a speci ed set or domain, as a disjunction over its elements. is interpretation of quanti ers is possible if we have a metalevel CWA assumption or a domain closure axioms [84] over (some) predicates, none of which is part of Datalog\u00b1.\nRecent work in OBDA addresses this problem, allowing the combination of open and closed predicates in Datalog\u00b1 ontologies, but previously tractable CQA may become intractable [2]. Similar extensions and results hold for light-weight DLs [44, 67, 68, 87].\nIn our case, if we accept value invention for category members, the natural candidates to be declared as closed in the new se ing are the unary category predicates and the child-parent predicates: we do not want to create new category members or new children for a given parent, nor, under upward data propagation, a new parent for a given child since parents are unique, as captured by the \u201cupward\u201d egds (15), which will force the invented nulls to take the given parent values. More problematic becomes downward data propagation with existential quanti ers over categorical positions. Even under a closure assumption on child-parent predicates, we may end up creating new children (we stand for existing do not have any \u201cdownward\u201d egds).\nIf we accept tgds such as (36) and we consider category predicates and child-parent predicates as closed, then we start departing from the usual Datalog\u00b1 semantics, and some of the results we have for weakly-sticky programs (with OWA semantics) have to be reconsidered.\nHaving existential variables over categorical predicates may lead to new forms of inconsistency, involving category values. Adopting a repair semantic based on changes on the extensional data leads to repairs of a MD database, which should be treated as such and not as an ordinary relational database (cf. Section 6.2).\nAs an alternative to existential categorical variables as choices from given (possibly closed) sets of values, we could think of using disjunctive Datalog\u00b1, with disjunctive tgds [3, 26], in particular for downward navigation. However, CQA under disjunctive sticky-Datalog\u00b1 may be undecidable in some cases [50, 77]. Furthermore, disjunctive rule heads may become data dependant."}, {"heading": "6.5 Related Work", "text": "As a logical extension of a multidimensional data model, our model is similar in spirit to the data warehouse conceptual data model [43] that extends the entity-relationship data model with dimensions by means of the expressive description logic (DL) ALCFI [52]. ey concentrate on the model and reasoning about the satis ability of the MD schema and implied statements, but not OBDA. In [69], preliminary work motivated by data quality on specifying MD ontologies in light-weight DLs is reported, without going much into quality aspects or query answering. 26According to their dependency graph (cf. Section 4.1), PatientUnit[1] has in nite rank. Rule (38) breaks weak-stickiness, because u is a repeated marked variable that appears only in PatientUnit[1].\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\ne existing declarative approaches to data quality [18] mostly use classic ICs, such as FDs and IDs, and denial constraints (i.e. the ncs of Datalog\u00b1). Newer classes of dependencies have been introduced to capture data quality conditions and directly support data cleaning processes [38]. Examples are conditional dependencies (conditional FDs and IDs), and matching dependencies [41, 42]. We claim that more expressive contexts are necessary in real life databases to express stronger conditions and semantics of (quality) data.\nModels of context [22] have been proposed and investigated in the data management and knowledge representation literature. Concentrating mostly on the former, in the following we brie y review, in an itemized manner, some of those models. A er describing them, we make comparisons with our model of context and its use. A. Multi Context Systems (MCS) [48] and Local Models Semantics (LMS) [46, 47] are related logicbased frameworks for formalizing contexts and reasoning with them. MCS provides a prooftheoretic framework with a hierarchy of FO languages, each of them for knowledge representation and reasoning within a speci c context. LMS is a model-theoretic framework based on the principles of locality, i.e. reasoning uses only part (the context) of what is potentially available; and compatibility of the kinds of reasoning performed in di erent contexts. B. In [78, 79], a general framework is proposed based on the concept of viewing for decomposing information bases into possibly overlapping fragments, called contexts, in order to be able to be er manage and customize information. Viewing refers to giving partial information on conceptual entities by observing them from di erent viewpoints or situations. C. In [5, 89], a model of contexts in information bases is proposed. A context is de ned as a set of objects, each of them with possibly several names. Operations, such as create, copy, merge, and browse, are de ned for manipulating and using contexts. Contextual objects can be structured through traditional abstraction mechanisms, i.e. classi cation, generalization, and a ribution. A theory of contextualized information bases is introduced. It includes a set of validity constraints for contexts, a model theory, as well as a set of sound and complete inference rules. D. In [45], ideas from [46], specially LMS, are applied to information integration and federated database management, where each database may have its own local semantics. LMS for federated databases is used, as an extension of LMS. A notion of logical consequence between formulae (queries) in di erent databases is de ned, and becomes the theoretical basis for the implementation of algorithms for query answering and their optimization. E. Context-aware data tailoring [24] proposes context dimension trees (CDTs) for modeling multidimensional aspects of context. It allows sub-dimensions with values of ner granularity. A user\u2019s context is modeled as a \u201cchunk con guration\u201d, consisting of a set of dimension labels and their assigned values, and is used to specify the relevant portion of a target database for the user. is user\u2019s view is computed by combining the sub-views linked to dimension values [23, 25]. F. In [71] dimensions, as in multidimensional databases, are used for modeling contexts. A contextaware data model is proposed in which the notion of context is implicit and indirectly captured by contextual a ributes, i.e. relational a ributes that take as values members of dimension categories. In particular, in a contextual relation the context of a tuple is captured by its values in dimensions, while the categories of these members specify the granularity level of the context. ey present a query language that extends the relational algebra, by introducing new operators for manipulating the granularity of contextual a ributes. G. In [80, 88] contexts are used in preference database systems to support context-aware queries whose results depend on the context at the time of their submission. Data cubes are used to store the dependencies between context-dependent preferences, database relations, and OLAP techniques\n, Vol. 1, No. 1, Article 1. Publication date: March 2017.\nfor processing context-aware queries. is allows for the manipulation of the captured context data at various levels of abstraction. H. e context relational model (CR) model [86] extends the relational model with contexts, which are treated as rst-class citizens, at the level of database models and query languages. A relation in this model has di erent schemas and extensions in di erent contexts. A set of basic operations is introduced that extend relational algebra so as to take context into account.\nIn the following we compare our OMD contexts with the context models in D.-H., which have been used in data management and are relatively close to ours.\nOur model of context is relational in that data are represented as relations only. However, the relational context models described above are not completely relational: they use an extension of relations with new data entities. In H., a collection of relations represents a contextual relation. Accordingly, creating, manipulating and querying those contextual relations requires additional tools and care. In E.-G., no relational representation of dimensions is given. e formalizations of context in F.-G. use a MD data model for modeling dimensions, and those in E. propose CDTs and chunk con gurations, which are not represented by relational terms.\nWith respect to languages for querying context, E.-H. use extensions of relational algebra, from which shortcomings are inherited, in particular, the lack of recursion queries and the inability to capture incomplete data. Both features are supported the OMD model. e work under D. studies and formalizes the problem of querying federated databases using the notion of logical consequence. OMD extends the work in F. and its query language (cf. [76, chap. 4]).\nConcerning the applications of these context models, the context model in G. can be used in particular for context-aware preference databases. Context-aware data tailoring E. is a methodology for managing small databases (possibly obtained from larger sources) aimed at being hosted by portable devices. e work in D. focuses on using LMS for federated databases. It is not clear how these models can be adapted for other purposes. e work on context-aware databases in F. is fairly general and can be applied in many applications in data management. Our MD context model is not restricted to the problem of data quality speci cation and extraction, and can have wide applicability. However, it is an open problem to nd ways to provide and include in OMD the speci c applications and tools that those other models provide."}], "references": [{"title": "Polynomial Datalog Rewritings for Ontology Mediated \u008beries with Closed Predicates", "author": ["S. Ahmetaj", "M. Ortiz", "M. \u0160imkus"], "venue": "In Proc. of the Alberto Mendelzon International Workshop on Foundations of Data Management (AMW), CEUR-WS Proc", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Disjunctive Datalog with Existential \u008banti\u0080ers: Semantics, Decidability, and Complexity Issues", "author": ["M. Alviano", "W. Faber", "N. Leone", "M. Manna"], "venue": "\u008aeory and Practice of Logic Programming (TPLP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Magic-Sets for Datalog with Existential \u008banti\u0080ers", "author": ["M. Alviano", "N. Leone", "M. Manna", "G. Terracina", "P. Veltri"], "venue": "In Proc. of the Int. Conference on Datalog in Academia and Industry", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Contextualization as an Independent Abstraction Mechanism for Conceptual Modeling", "author": ["A. Anality", "M. \u008ceodorakis", "N. Spyratos", "P. Constantopoulos"], "venue": "Information Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Structural Repairs of Multidimensional Databases", "author": ["S. Ariyan", "L. Bertossi"], "venue": "In Proc. of the Alberto Mendelzon International WS of Foundations of Data Management (AMW),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A Multidimensional Data Model with Subcategories for Flexibly Capturing Summarizability", "author": ["S. Ariyan", "L. Bertossi"], "venue": "In Proc. of the International Conference on Scienti\u0080c and Statistical Database Management (SSDBM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "On Rules with Existential Variables: Walking the Decidability Line", "author": ["J.F. Baget", "M. Lecl\u00e9re", "M.L. Mugnier", "E. Salvat"], "venue": "Arti\u0080cial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Combining Existential Rules and Transitivity: Next Steps", "author": ["J.F. Baget", "M. Bienvenu", "M.L. Mugnier", "S. Rocher"], "venue": "In Proc. of the International Joint Conference on Arti\u0080cial Intelligence (IJCAI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Declarative Entity Resolution via Matching Dependencies and Answer Set Programs", "author": ["Z. Bahmani", "L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "In Proc. of the International Conference on Principles of Knowledge Represena\u0088ion and Reasoning (KR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Logical Foundations of Relational Data Exchange", "author": ["P. Barcelo"], "venue": "ACM SIGMOD Record,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Data \u0089ality: Concepts, Methodologies and Techniques", "author": ["C. Batini", "M. Scannapieco"], "venue": "Second edition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "\u008ce Implication Problem for Data Dependencies", "author": ["C. Beeri", "M.Y. Vardi"], "venue": "In Proc. of the Colloquium on Automata, Languages and Programming (ICALP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1981}, {"title": "Consistent \u008bery Answering in Databases", "author": ["L. Bertossi"], "venue": "ACM Sigmod Record,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Consistent \u008bery Answering in Data Warehouses", "author": ["L. Bertossi", "L. Bravo", "M. Caniupan"], "venue": "In Proc. of the Alberto Mendelzon International Workshop on Foundations of Data Management (AMW),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Data \u008bality is Context Dependent", "author": ["L. Bertossi", "F. Rizzolo", "J. Lei"], "venue": "In Proc. of the Workshop on Enabling Real-Time Business Intelligence (BIRTE) Collocated with the International Conference on Very Large Data Bases (VLDB), Springer LNBIP", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Database Repairing and Consistent \u0089ery Answering", "author": ["L. Bertossi"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Generic and Declarative Approaches to Data \u008bality Management", "author": ["L. Bertossi", "L. Bravo"], "venue": "In Handbook of Data \u0089ality - Research and Practice,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Contexts and Data \u008bality Assessment", "author": ["L. Bertossi", "F. Rizzolo"], "venue": "Corr Arxiv Paper cs.DB/1608.04142,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "\u008berying Inconsistent Description Logic Knowledge Bases under Preferred Repair Semantics", "author": ["Bienvenu M", "C. Bourgaux", "F. Goasdou\u00e8"], "venue": "In Proc. of the National Conference on Arti\u0080cial Intelligence (AAAI),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Explaining Inconsistency-tolerant \u008bery Answering over Description Logic Knowledge Bases", "author": ["Bienvenu M", "C. Bourgaux", "F. Goasdou\u00e8"], "venue": "In Proc. of the National Conference on Arti\u0080cial Intelligence (AAAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A Data-Oriented Survey of Context Models", "author": ["C. Bolchini", "C.A. Curino", "E. \u008bintarelli", "F.A. Schreiber", "L. Tanca"], "venue": "ACM SIGMOD Record,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Using Context for the Extraction of Relational Views", "author": ["C. Bolchini", "E. \u008bintarelli", "R. Rossato", "L. Tanca"], "venue": "In Proc. of the International and Interdisciplinary Conference on Modeling and Using Context,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Context Information for Knowledge Reshaping", "author": ["C. Bolchini", "C.A. Curino", "E. \u008bintarelli", "F.A. Schreiber", "L. Tanca"], "venue": "International Journal of Web Engineering and Technology,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "CARVE: Context-Aware Automatic View De\u0080nition over Relational Databases", "author": ["C. Bolchini", "E. \u008bintarelli", "L. Tanca"], "venue": "Information Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Guarded-Based Disjunctive Tuple-Generating Dependencies", "author": ["P. Bourhis", "M. Manna", "M. Morak", "A. Pieris"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "On the Decidability and Complexity of \u008bery Answering over Inconsistent and Incomplete Databases", "author": ["A. Cal\u0300\u0131", "D. Lembo", "R. Rosati"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Datalog\u00b1: A Uni\u0080ed Approach to Ontologies and Integrity Constraints", "author": ["A. Cal\u0300\u0131", "G. Go\u008alob", "T. Lukasiewicz"], "venue": "In Proc. of the International Conference on Database \u008aeory (ICDT),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Towards More Expressive Ontology Languages: \u008ce \u008bery Answering Problem", "author": ["A. Cal\u0300\u0131", "G. Go\u008alob", "A. Pieris"], "venue": "Arti\u0080cial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "On Separability of Ontological Constraints", "author": ["A. Cal\u0300\u0131", "M. Console", "R. Frosini"], "venue": "In Proc. of the Alberto Mendelzon International Workshop on Foundations of Data Management (AMW),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Taming the In\u0080nite Chase: \u008bery Answering under Expressive Relational Constraints", "author": ["A. Cal\u0300\u0131", "G. Go\u008alob", "M. Kifer"], "venue": "Journal of Arti\u0080cial Intelligence Research (JAIR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "\u008ce Consistency Extractor System: Answer Set Programs for Consistent \u008bery Answering in Databases", "author": ["M. Caniupan-Marileo", "L. Bertossi"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "\u008ce Implication Problem for Functional and Inclusion Dependencies", "author": ["A.K. Chandra", "M.Y. Vardi"], "venue": "SIAM Journal of Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1985}, {"title": "Data \u008bality and the Bo\u008aom Line: Achieving Business Success \u008crough a Commitment to High \u008bality Data", "author": ["W. Eckerson"], "venue": "Report of the Data Warehousing Institute,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "A Mathematical Introduction to Logic. 2nd Edition", "author": ["H.B. Enderton"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Data Exchange: Semantics and \u008bery Answering", "author": ["R. Fagin", "P.G. Kolaitis", "R.J. Miller", "L. Popa"], "venue": "\u008aeoretical Computer Science (TCS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Dependencies Revisited for Improving Data \u008bality", "author": ["W. Fan"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Foundations of Data \u0089ality Management", "author": ["W. Fan", "F. Geerts"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Data \u008bality: From \u008ceory to Practice", "author": ["W. Fan"], "venue": "Article 1. Publication date: March 2017", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Reasoning about Record Matching Rules", "author": ["W. Fan", "X. Jia", "J. Li", "S. Ma"], "venue": "In Proc. VLDB Endowment (PVLDB),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Dynamic Constraints for Record Matching", "author": ["W. Fan", "H. Gao", "X. Ji", "J. Li", "S. Ma"], "venue": "\u008ae International Journal on Very Large Data Bases (VLDBJ),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "A DataWarehouse Conceptual Data Model for Multidimensional Aggregation", "author": ["E. Franconi", "I. Sa\u008aler"], "venue": "In Proc. of the International Workshop on Design and Management of Data Warehouses (DMDW),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "\u008bery Answering with DBoxes is Hard", "author": ["E. Franconi", "Y. Garcia", "I. Seylan"], "venue": "Electronic Notes in \u008aeoretical Computer Science (ENTCS),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Model \u008ceoretic Semantics for Information Integration", "author": ["C. Ghidini", "L. Sera\u0080ni"], "venue": "In Proc. of the International Conference on Arti\u0080cial Intelligence, Methodology, Systems, and Applications (AIMSA),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}, {"title": "Local Models Semantics, or Contextual Reasoning = Locality + Compatibility", "author": ["C. Ghidini", "F. Giunchiglia"], "venue": "Arti\u0080cial Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Multi-Context Logics - A General Introduction", "author": ["C. Ghidini", "L. Sera\u0080ni"], "venue": "In Context in Computing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Multilanguage Hierarchical Logics, or: How We Can Do without Modal Logics", "author": ["F. Giunchiglia", "L. Sera\u0080ni"], "venue": "Arti\u0080cial Intelligence,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1994}, {"title": "Ontological \u008beries: Rewriting and Optimization", "author": ["G. Go\u008alob", "G. Orsi", "A. Pieris"], "venue": "In Proc. of the International Conference on Data Engineering (ICDE),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Recent Advances in Datalog\u00b1", "author": ["G. Go\u008alob", "M. Morak", "A. Pieris"], "venue": "Reasoning Web 2015, Springer LNCS 9203,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "A Description Logic with Transitive and Inverse Roles and Role Hierarchies", "author": ["I. Horrocks", "S. Sa\u008aler"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1999}, {"title": "OLAP Dimension Constraints", "author": ["C. Hurtado", "A. Mendelzon"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Capturing Summarizability with Integrity Constraints in OLAP", "author": ["C. Hurtado", "C. Gutierrez", "A. Mendelzon"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2005}, {"title": "Incomplete Information in Relational Databases", "author": ["T. Imielinski", "W. Lipski"], "venue": "Journal of the ACM,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1984}, {"title": "Multidimensional Databases and Data Warehousing", "author": ["Jensen", "Ch. S", "T. Bach Pedersen", "\u008comsen", "Ch"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2010}, {"title": "Towards a Compositional Semantic Account of Data \u008bality A\u008aributes", "author": ["L. Jiang", "A. Borgida", "J. Mylopoulos"], "venue": "In Proc. International Conference on Conceptual Modeling (ER),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2008}, {"title": "Testing Containment of Conjunctive \u008beries under Functional and Inclusion Dependencies", "author": ["D.S. Johnson", "A. Klug"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1984}, {"title": "Juran\u2019s \u0089ality Handbook, Fi\u0087h Edition", "author": ["J.M. Juran", "A.M. Godfrey"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1999}, {"title": "\u008ce Complexity of Data Exchange", "author": ["P.G. Kolaitis", "W.C. Tan", "J. Pan\u008aaja"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2006}, {"title": "Inconsistency-Tolerant Semantics for Description Logics", "author": ["D. Lembo", "Lenzerini M", "R. Rosati", "M. Ruzzi", "D.F. Savo"], "venue": "In Proc. of the International Conference on Web Reasoning and Rule Systems (RR),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Inconsistency-tolerant \u008bery Answering in Ontology- Based Data Access", "author": ["D. Lembo", "Lenzerini M", "R. Rosati", "M. Ruzzi", "D.F. Savo"], "venue": "Journal of Web Semantics,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Data Integration: A \u008ceoretical Perspective", "author": ["M. Lenzerini"], "venue": "In Proc. of the ACM SIGMOD-SIGACT Symposium on Principles of Database Systems (PODS),", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2002}, {"title": "Inconsistency Handling in Datalog+/- Ontologies", "author": ["T. Lukasiewicz", "M. Martinez", "A. Pieris", "G. Simari"], "venue": "In Proc. of the European Conference on Arti\u0080cial Intelligence (ECAI),", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2012}, {"title": "From Classical to Consistent \u008bery Answering under Existential Rules", "author": ["T. Lukasiewicz", "M. Martinez", "A. Pieris", "G. Simari"], "venue": "In Proc. of the National Conference on Arti\u0080cial Intelligence (AAAI),", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2015}, {"title": "Ontology-Based Data Access with Closed Predicates is Inherently Intractable (Sometimes)", "author": ["C. Lutz", "I. Seylan", "F. Wolter"], "venue": "In Proc. of the International Joint Conference on Arti\u0080cial Intelligence (IJCAI),", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2013}, {"title": "Ontology-Mediated \u008beries with Closed Predicates", "author": ["C. Lutz", "I. Seylan", "F. Wolter"], "venue": "In Proc. of the International Joint Conference on Arti\u0080cial Intelligence (IJCAI),", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2015}, {"title": "Multidimensional Contexts for Data \u008bality Assessment", "author": ["A. Malaki", "L. Bertossi", "F. Rizzolo"], "venue": "In Proc. of the Alberto Mendelzon International Workshop on Foundations of Data Management (AMW),", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "Testing Implications of Data Dependencies", "author": ["D. Maier", "A. Mendelzon", "Y. Sagiv"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1979}, {"title": "Taxonomy-Based Relaxation of \u008bery Answering in Relational Databases", "author": ["D. Martinenghi", "R. Torlone"], "venue": "\u008ae International Journal on Very Large Data Bases (VLDBJ),", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2014}, {"title": "\u008ce Implication Problem for Functional and Inclusion Dependencies", "author": ["J. Mitchell"], "venue": "Information and Control,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1983}, {"title": "Ontology-Based Multidimensional Contexts with Applications to \u008bality Data Speci\u0080cation and Extraction", "author": ["M. Milani", "L. Bertossi"], "venue": "In Proc. of the International Symposium on Rules and Rule Markup Languages for the Semantic Web (RuleML),", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Extending Weakly-Sticky Datalog\u00b1: \u008bery-Answering Tractability and Optimizations", "author": ["M. Milani", "L. Bertossi"], "venue": "In Proc. of the International Conference on Web Reasoning and Rule Systems (RR), Springer LNCS", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "A Hybrid Approach to \u008bery Answering under Expressive Datalog\u00b1", "author": ["M. Milani", "L. Bertossi", "A. Cal\u0300\u0131"], "venue": "In Proc. of the International Conference on Web Reasoning and Rule Systems (RR), Springer LNCS", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2016}, {"title": "Multidimensional Ontologies for Contextual \u0089ality Data Speci\u0080cation and Extraction", "author": ["M. Milani"], "venue": "PhD in Computer Science \u008cesis, Carleton University,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2017}, {"title": "\u008ae Impact of Disjunction on Reasoning under Existential Rules", "author": ["M. Morak"], "venue": "PhD in Computer Science \u008cesis, University of Oxford,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "An Integrating View on the Viewing Abstraction: Contexts and Perspectives in So\u0089ware Development, AI, and Databases", "author": ["R. Motschnig-Pitrik"], "venue": "Systems Integration,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1995}, {"title": "A Generic Framework for the Modeling of Contexts and its Applications", "author": ["R. Motschnig-Pitrik"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2000}, {"title": "Contextual Database Preferences", "author": ["E. Pitoura", "K. Stefanidis", "P. Vassiliadis"], "venue": "IEE Data Engineering Bulletin,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2011}, {"title": "Linking Data to Ontologies", "author": ["A. Poggi", "D. Lembo", "D. Calvanese", "G. De Giacomo", "M. Lenzerini", "R. Rosati"], "venue": "Data Semantics,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2008}, {"title": "A Simple Method for Undecidability Proofs and Some Applications", "author": ["M.O. Rabin"], "venue": "In Logic, Methodology and Philosophy of Science, Proceedings of the 1964 International Congress, Bar-Hillel, Y. (ed.). Studies in Logic and the Foundations of Mathematics", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1965}, {"title": "\u008ce Impact of Poor Data \u008bality on the Typical Enterprise", "author": ["T. Redman"], "venue": "Communications of the ACM,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 1998}, {"title": "Towards a Logical Reconstruction of Relational Database \u008ceory", "author": ["R. Reiter"], "venue": "In On Conceptual Modelling,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 1984}, {"title": "On the Complexity of Dealing with Inconsistency in Description Logic Ontologies", "author": ["R. Rosati"], "venue": "In Proc. of the International Joint Conference on Arti\u0080cial Intelligence (IJCAI),", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2011}, {"title": "Towards a Context-Aware Relational Model", "author": ["Y. Rousoss", "Y. Stavrakas", "V. Pavlaki"], "venue": "In Proc. International Workshop on Context Representation and Reasoning,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2005}, {"title": "E\u0082ective \u008bery Rewriting with Ontologies over DBoxes", "author": ["I. Seylan", "E. Franconi", "J. De Bruijn"], "venue": "In Proc. of the International Joint Conference on Arti\u0080cial Intelligence (IJCAI),", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2009}, {"title": "Beyond Accuracy: What Data \u008bality Means to Data Consumers", "author": ["Y. Wang R", "M. Strong D"], "venue": "Journal of Management Information Systems,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 1996}, {"title": "Repair-Oriented Relational Schemas for Multidimensional Databases", "author": ["M. Yaghmaie", "L. Bertossi", "S. Ariyan"], "venue": "In Proc. of the International Conference on Extending Database Technology (EDBT),", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Assessing the quality of data and performing data cleaning when the data are not up to the expected standards of quality have been and will continue being common, di\u0081cult and costly problems in data management [12, 35, 83].", "startOffset": 210, "endOffset": 222}, {"referenceID": 32, "context": "Assessing the quality of data and performing data cleaning when the data are not up to the expected standards of quality have been and will continue being common, di\u0081cult and costly problems in data management [12, 35, 83].", "startOffset": 210, "endOffset": 222}, {"referenceID": 78, "context": "Assessing the quality of data and performing data cleaning when the data are not up to the expected standards of quality have been and will continue being common, di\u0081cult and costly problems in data management [12, 35, 83].", "startOffset": 210, "endOffset": 222}, {"referenceID": 10, "context": "Some of them are [12]: (1) Consistency, which refers to the validity and integrity of data representing real-world entities, typically identi\u0080ed with satisfaction of integrity constraints.", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "also [39, 40, 57] for more on quality dimensions.", "startOffset": 5, "endOffset": 17}, {"referenceID": 37, "context": "also [39, 40, 57] for more on quality dimensions.", "startOffset": 5, "endOffset": 17}, {"referenceID": 53, "context": "also [39, 40, 57] for more on quality dimensions.", "startOffset": 5, "endOffset": 17}, {"referenceID": 10, "context": ") In this work we consider data quality as referring to the degree to which the data \u0080ts or ful\u0080lls a form of usage [12], relating our data quality concerns to the production and the use of data.", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "In this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 17, "context": "In this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 79, "context": "In this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11].", "startOffset": 332, "endOffset": 336}, {"referenceID": 59, "context": "In this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11].", "startOffset": 513, "endOffset": 517}, {"referenceID": 9, "context": "In this work, building upon and considerably extending the framework in [16, 19], context-based data quality assessment, quality data extraction and data cleaning on a relational database D for a relational schema R are approached by creating a context model where D is the theory T above (it could be expressed as a logical theory [84]), the theory T c is a (logical) ontology Oc ; and, considering that we are using theories around data, the mappings can be logical mappings as used in virtual data integration [63] or data exchange [11].", "startOffset": 535, "endOffset": 539}, {"referenceID": 77, "context": "to obtain (un)decidability results [82].", "startOffset": 35, "endOffset": 39}, {"referenceID": 51, "context": "A more relaxed alternative consists in considering as quality data those that are obtained as certain answers to queries posed to D, but answered through \u0089al(D,Oc ): \u008ce query is posed to each of the instances in \u0089al(D,Oc ) (which essentially have the same schema as D), but only those answers that are shared by those instances are considered to be certain [55].", "startOffset": 357, "endOffset": 361}, {"referenceID": 52, "context": "\u008cey are the basic elements in multidimensional databases and data warehouses [56], where we usually \u0080nd time, location, product, as three dimensions that give context to numerical data, e.", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "\u008ce language of choice for the contextual ontologies will be Datalog\u00b1 [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "One of those good classes is that of weakly-sticky Datalog\u00b1 [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 49, "context": "Programs in that class allow us to represent a logic-based, relational reconstruction and extension of the Hurtado-Mendelzon multidimensional data model [53, 54], which allows us to bring data dimensions into contexts.", "startOffset": 153, "endOffset": 161}, {"referenceID": 50, "context": "Programs in that class allow us to represent a logic-based, relational reconstruction and extension of the Hurtado-Mendelzon multidimensional data model [53, 54], which allows us to bring data dimensions into contexts.", "startOffset": 153, "endOffset": 161}, {"referenceID": 12, "context": "2 \u008cose familiar with database repairs and consistent query answering [14, 17], would notice that both can be formulated in this general ste\u008aing.", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": "2 \u008cose familiar with database repairs and consistent query answering [14, 17], would notice that both can be formulated in this general ste\u008aing.", "startOffset": 69, "endOffset": 77}, {"referenceID": 30, "context": "Instance D would be the inconsistent database, the ontology would provide the integrity constraints and the speci\u0080cation of repairs, say in answer set programming [32], the class \u0089al(D, Oc ) would contain the repairs, and the general certain answers would become the consistent answers.", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "3 Data dimensions were not considered in [16, 19].", "startOffset": 41, "endOffset": 49}, {"referenceID": 17, "context": "3 Data dimensions were not considered in [16, 19].", "startOffset": 41, "endOffset": 49}, {"referenceID": 76, "context": "about indirectly accessing underlying data through queries posed to the interface and elements of an ontology [81].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "\u008ce promising application of the OMD model that we investigate in this work is related to data quality concerns as pertaining to the use and production of data [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 83, "context": "For example, in [90] contextual data quality dimensions are described as those quality dimensions that are relevant to the context of data usage.", "startOffset": 16, "endOffset": 20}, {"referenceID": 55, "context": "In [51] and [59], quality is characterized as \u201c\u0080tness for use\u201d.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "From this point of view, we are implicitly addressing a problem of incomplete data, one of the common data quality dimensions [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "(B) We establish that, under natural assumptions that MD ontologies belong to the class of weaklysticky (WS) Datalog\u00b1 programs [29], for which conjunctive QA is tractable (in data).", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "\u008ce class of WS programs is an extension of sticky Datalog\u00b1[29] and weakly-acyclic programs [37].", "startOffset": 58, "endOffset": 62}, {"referenceID": 34, "context": "\u008ce class of WS programs is an extension of sticky Datalog\u00b1[29] and weakly-acyclic programs [37].", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "Actually, WS Datalog\u00b1 is de\u0080ned through restrictions on join variables occurring in in\u0080nite-rank positions, as introduced in [37].", "startOffset": 125, "endOffset": 129}, {"referenceID": 69, "context": "However, in [74] a practical algorithm was proposed, together with a methodology for magic-setbased query optimization.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "5 In the case of duplicate records in a data source, the context could contain an answer set program or a Datalog program to enforce matching dependencies for entity resolution [10].", "startOffset": 177, "endOffset": 181}, {"referenceID": 14, "context": "Of course, di\u0082erent distance measures may be used for this purpose [16, 19].", "startOffset": 67, "endOffset": 75}, {"referenceID": 17, "context": "Of course, di\u0082erent distance measures may be used for this purpose [16, 19].", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "Most importantly, the combination of constraints that are equality-generating dependencies (egds) and the rules, which are tuple-generating dependencies (tgds) [27], may lead to undecidability of QA.", "startOffset": 160, "endOffset": 164}, {"referenceID": 27, "context": "Separability [29] is a semantic condition on egds and tgds that guarantees the interaction between them does not harm the tractability of QA.", "startOffset": 13, "endOffset": 17}, {"referenceID": 66, "context": "\u008ce closest work related to our OMD model can be found in the dimensional relational algebra proposed in [71], which is subsumed by the OMD model [76, chap.", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "\u008ce contextual and dimensional data representation framework in [25] is also close to our OMD model in that it uses dimensions for modeling context.", "startOffset": 63, "endOffset": 67}, {"referenceID": 68, "context": "\u008cis paper considerably extends results previously reported in [73].", "startOffset": 62, "endOffset": 66}, {"referenceID": 79, "context": "In the former case, one makes the meta-level assumption, the so-called closed-world-assumption (CWA) [1, 84], that the only positive ground atoms that are true w.", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "8 Without any syntactic restrictions on the program, and even for programs without constraints, conjunctive query answering (CQA) may be undecidable [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 65, "context": "However, the chase procedure [70] can be used to generate a single, possibly in\u0080nite, instance that represents the class Mod(\u03a0,D) for this purpose.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "However, it is possible to de\u0080ne a canonical chase procedure that determines a canonical sequence of chase steps, and consequently, a canonical chase instance [31].", "startOffset": 159, "endOffset": 163}, {"referenceID": 34, "context": "Given a program \u03a0 and extensional database D, its chase (instance) is a universal model [37]: For every I \u2208 Mod(\u03a0,D), there is a homomorphism from the chase into I .", "startOffset": 88, "endOffset": 92}, {"referenceID": 34, "context": "For this reason, the (certain) answers to a CQ Q under \u03a0 and D can be computed by evaluating Q over the chase instance (and discarding the answers containing nulls) [37].", "startOffset": 165, "endOffset": 169}, {"referenceID": 29, "context": "checking if a tuple is an answer to a CQ query, can be reduced to BCQ answering as shown in [31], and they have the same data complexity.", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "If \u03a0 has egds, they are expected to be satis\u0080ed by a modi\u0080ed (canonical) chase [31] that also enforces the egds.", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "According to the Hurtado-Mendelzon multidimensional data model (in short, the HMmodel) [53], a dimension schema, H = \u3008K ,\u2197\u3009, consists of a \u0080nite set K of categories, and an irre\u0083exive, binary relation\u2197, called the child-parent relation, between categories (the \u0080rst category is a child and the second category is a parent).", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "For example, the following IDs: WardUnit[1] \u2286 Ward[1], and WardUnit[2] \u2286 Unit[1].", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "Accordingly, if child-parent predicate P \u2208 L is associated to category predicates K ,K \u2032 \u2208 K , in this order, we introduce IDs P[1] \u2286 K[1] and P[2] \u2286 K \u2032[1]), as ncs: P(x ,x \u2032),\u00acK(x) \u2192 \u22a5, and P(x ,x \u2032),\u00acK \u2032(x \u2032) \u2192 \u22a5.", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": ", which is captured by the IDs WorkSchedules[1] \u2286 Unit[1], and WorkSchedules[2] \u2286 Day[1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "For the Hospital dimension, one of the two IDs for the child-parent predicate WardUnit is WardUnit[2] \u2286 Unit[1], which is expressed by an nc of the form (14):", "startOffset": 98, "endOffset": 101}, {"referenceID": 29, "context": "that is not intertwined with recursion, is considered in [31].", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "As mentioned before, without any restrictions Datalog programs conjunctive query answering (CQA) may be undecidable, even without constraints [27].", "startOffset": 142, "endOffset": 146}, {"referenceID": 34, "context": "1 Weakly-Acyclic, Sticky and Weakly-Sticky Programs Weakly-acyclicDatalog\u00b1 programs (without constraints) form a syntactic class of Datalog programs that is de\u0080ned appealing to the notion of dependency graph [37].", "startOffset": 208, "endOffset": 212}, {"referenceID": 0, "context": "P[2] R[1]", "startOffset": 1, "endOffset": 4}, {"referenceID": 0, "context": "R[2] U [1]", "startOffset": 1, "endOffset": 4}, {"referenceID": 0, "context": "R[2] and P[2] have rank 1.", "startOffset": 1, "endOffset": 4}, {"referenceID": 0, "context": "R[2] and P[2] have rank 1.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "\u008cen, \u03c0F (\u03a0) = {U [1],R[1], P[1],R[2], P[2]}, and \u03a0 is WA.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "\u008cen, \u03c0F (\u03a0) = {U [1],R[1], P[1],R[2], P[2]}, and \u03a0 is WA.", "startOffset": 39, "endOffset": 42}, {"referenceID": 34, "context": "\u008ce chase for these programs stops in polynomial time in the size of the extensional data, making CQA ptime-complete in data complexity [37], but 2exptime-complete in combined complexity, i.", "startOffset": 135, "endOffset": 139}, {"referenceID": 56, "context": "in the combined size of the program, query and data [60].", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "[29] and [74] for a more detailed discussion).", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[29] and [74] for a more detailed discussion).", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "(It is exptime-complete in combined complexity [29].", "startOffset": 47, "endOffset": 51}, {"referenceID": 46, "context": ") Even more, CQA over sticky programs enjoys \u0080rst-order rewritable [49], that is, a CQ posed to the program can be rewri\u008aen into an FO query that can be evaluated directly on the extensional data.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Now, \u03c0F (\u03a0\u2032) = \u2205, and the marked join variable y in the second rule appears in R[1] and R[2], both non-\u0080nite (i.", "startOffset": 89, "endOffset": 92}, {"referenceID": 69, "context": "[74] for a discussion).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "CQA is tractable, but ptime-complete in data, and 2exptime-complete in combined complexity [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": ") For the MD ontology with \u03c31 and \u03c32, WorkSchedules[4] and Shi\u0087s[4] have in\u0080nite rank; and all the other positions have \u0080nite rank.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": ") For the MD ontology with \u03c31 and \u03c32, WorkSchedules[4] and Shi\u0087s[4] have in\u0080nite rank; and all the other positions have \u0080nite rank.", "startOffset": 64, "endOffset": 67}, {"referenceID": 27, "context": "\u008ce tractability (in data) of CQA under WS programs was established in [29] on theoretical grounds, without providing a practical algorithm.", "startOffset": 70, "endOffset": 74}, {"referenceID": 69, "context": "An implementable, polynomial-time algorithm for CQA under WS programs is presented in [74].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "Actually, the algorithm can be applied to a class that not only extends WS, but is also closed under magic-set rewriting of Datalog programs [4], which allows for query-dependent optimizations of the program [74].", "startOffset": 141, "endOffset": 144}, {"referenceID": 69, "context": "Actually, the algorithm can be applied to a class that not only extends WS, but is also closed under magic-set rewriting of Datalog programs [4], which allows for query-dependent optimizations of the program [74].", "startOffset": 208, "endOffset": 212}, {"referenceID": 70, "context": "However, a hybrid algorithm is proposed in [75].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "When the (combined) chase does not fail, the result is a possibly in\u0080nite universal model that satis\u0080es both the tgds and egds [31].", "startOffset": 127, "endOffset": 131}, {"referenceID": 25, "context": "\u008ce interaction of tgds and egds may lead to undecidability of CQA [27, 34, 58, 72].", "startOffset": 66, "endOffset": 82}, {"referenceID": 31, "context": "\u008ce interaction of tgds and egds may lead to undecidability of CQA [27, 34, 58, 72].", "startOffset": 66, "endOffset": 82}, {"referenceID": 54, "context": "\u008ce interaction of tgds and egds may lead to undecidability of CQA [27, 34, 58, 72].", "startOffset": 66, "endOffset": 82}, {"referenceID": 67, "context": "\u008ce interaction of tgds and egds may lead to undecidability of CQA [27, 34, 58, 72].", "startOffset": 66, "endOffset": 82}, {"referenceID": 27, "context": "However, a separability property of the combination of egds and tgds guarantees a harmless interaction that makes CQA decidable and preserves CQA [29]: For a program \u03a0 with extensional database D, a set of tgds \u03a0R , and a set of egds \u03a0C , \u03a0R and \u03a0C are separable if either (a) the chase with \u03a0 fails, or (b) for any BCQ Q, \u03a0 |= Q if and only if \u03a0R \u222a D |= Q.", "startOffset": 146, "endOffset": 150}, {"referenceID": 28, "context": "However, separability is undecidable [30].", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Such a condition has been identi\u0080ed for egds that are key constraints [31]; it is that of non-con\u0083icting interaction.", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "15 \u008ce notion has been extended to FDs in [29]: A set of tgds \u03a0R and a set \u03a0C of FDs are non-con\u0083icting if, for every tgd \u03c3 , with set U\u03c3 of non-existential(lly quanti\u0080ed variables for) a\u008aributes in head(\u03c3 ), and FD \u03b5 of the form R : \u0100\u2192 B\u0304 , at least one of the following holds: (a) head(\u03c3 ) is not an R-atom, (b) U\u03c3 + \u0100, or (c) U\u03c3 = \u0100 and each \u2203-variable in \u03c3 occurs just once in the head of \u03c3 .", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "\u008ce use of the OMD model for quality data speci\u0080cation and extraction generalizes a previous approach to- and work on context-based data quality assessment and extraction [16, 19], which was brie\u0083y described in Section 1.", "startOffset": 170, "endOffset": 178}, {"referenceID": 17, "context": "\u008ce use of the OMD model for quality data speci\u0080cation and extraction generalizes a previous approach to- and work on context-based data quality assessment and extraction [16, 19], which was brie\u0083y described in Section 1.", "startOffset": 170, "endOffset": 178}, {"referenceID": 14, "context": "18 E could represent data brought from external sources, possible at query answering time [16, 19].", "startOffset": 90, "endOffset": 98}, {"referenceID": 17, "context": "18 E could represent data brought from external sources, possible at query answering time [16, 19].", "startOffset": 90, "endOffset": 98}, {"referenceID": 6, "context": "In this regard, it may happen that the combination of Datalog\u00b1 ontologies that enjoy good computational properties may be an ontology without such properties [8, 9].", "startOffset": 158, "endOffset": 164}, {"referenceID": 7, "context": "In this regard, it may happen that the combination of Datalog\u00b1 ontologies that enjoy good computational properties may be an ontology without such properties [8, 9].", "startOffset": 158, "endOffset": 164}, {"referenceID": 2, "context": "Now, \u03a3 = {\u03c31,\u03c32,\u03c33} is not WS since variable s in the body of \u03c33 is a repeated marked body variable only appearing in in\u0080nite-rank position Shi\u0087s[4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 69, "context": "If it is a weaklysticky Datalog\u00b1 ontology, we can use the chase-based algorithm introduced in [74].", "startOffset": 94, "endOffset": 98}, {"referenceID": 69, "context": "20Actually the algorithm applies to a larger class of Datalog\u00b1 ontologies, that of join-weakly-sticky programs that is closed under magic-sets optimizations [74].", "startOffset": 157, "endOffset": 161}, {"referenceID": 69, "context": "Finally, at Step 4 of \u0083alityQA, QM is answered as a CQ over OM and database D \u2032,22 using, for example, the QA algorithms in [74, 75].", "startOffset": 124, "endOffset": 132}, {"referenceID": 70, "context": "Finally, at Step 4 of \u0083alityQA, QM is answered as a CQ over OM and database D \u2032,22 using, for example, the QA algorithms in [74, 75].", "startOffset": 124, "endOffset": 132}, {"referenceID": 14, "context": "For that, we followed and extended the approach in [16, 19], by proposing ontological contexts, and embedding multidimensional (MD) data models in them.", "startOffset": 51, "endOffset": 59}, {"referenceID": 17, "context": "For that, we followed and extended the approach in [16, 19], by proposing ontological contexts, and embedding multidimensional (MD) data models in them.", "startOffset": 51, "endOffset": 59}, {"referenceID": 49, "context": "For the la\u008aer, we took advantage of our relational reconstruction of the HM data model [53, 54].", "startOffset": 87, "endOffset": 95}, {"referenceID": 50, "context": "For the la\u008aer, we took advantage of our relational reconstruction of the HM data model [53, 54].", "startOffset": 87, "endOffset": 95}, {"referenceID": 71, "context": "[76]), it is possible to include in the ontological contexts semantic constraints usually present in the HM model, such as strictness and homogeneity,23 which guarantee summarizability (or aggregation) for the correct computation of cube views [53].", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[76]), it is possible to include in the ontological contexts semantic constraints usually present in the HM model, such as strictness and homogeneity,23 which guarantee summarizability (or aggregation) for the correct computation of cube views [53].", "startOffset": 244, "endOffset": 248}, {"referenceID": 16, "context": "Our approach to quality data speci\u0080cation and extraction is declarative [18, 39].", "startOffset": 72, "endOffset": 80}, {"referenceID": 36, "context": "Our approach to quality data speci\u0080cation and extraction is declarative [18, 39].", "startOffset": 72, "endOffset": 80}, {"referenceID": 69, "context": "\u008ce algorithm and its optimization is based on our work on QA under WS programs [74, 75].", "startOffset": 79, "endOffset": 87}, {"referenceID": 70, "context": "\u008ce algorithm and its optimization is based on our work on QA under WS programs [74, 75].", "startOffset": 79, "endOffset": 87}, {"referenceID": 60, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 167, "endOffset": 187}, {"referenceID": 19, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 167, "endOffset": 187}, {"referenceID": 57, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 167, "endOffset": 187}, {"referenceID": 58, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 167, "endOffset": 187}, {"referenceID": 80, "context": "A common approach to DL or Datalog\u00b1 ontology repair has been based on repairing the extensional database in the case of Datalog\u00b1 [65], and the A-Box in the case of DL [20, 21, 61, 62, 85] ontologies.", "startOffset": 167, "endOffset": 187}, {"referenceID": 58, "context": "QA under this semantics is np-hard in the size of I , already for DL [62] or Datalog\u00b1 ontologies [65, 66] with relatively low expressive power.", "startOffset": 69, "endOffset": 73}, {"referenceID": 60, "context": "QA under this semantics is np-hard in the size of I , already for DL [62] or Datalog\u00b1 ontologies [65, 66] with relatively low expressive power.", "startOffset": 97, "endOffset": 105}, {"referenceID": 61, "context": "QA under this semantics is np-hard in the size of I , already for DL [62] or Datalog\u00b1 ontologies [65, 66] with relatively low expressive power.", "startOffset": 97, "endOffset": 105}, {"referenceID": 4, "context": "Repairs and consistent answers from MD databases have been investigated in [6, 7, 15], and also in [91], which proposes the path schema for MD databases as a be\u008aer relational schema for dealing with the kinds of inconsistencies that appear in them.", "startOffset": 75, "endOffset": 85}, {"referenceID": 5, "context": "Repairs and consistent answers from MD databases have been investigated in [6, 7, 15], and also in [91], which proposes the path schema for MD databases as a be\u008aer relational schema for dealing with the kinds of inconsistencies that appear in them.", "startOffset": 75, "endOffset": 85}, {"referenceID": 13, "context": "Repairs and consistent answers from MD databases have been investigated in [6, 7, 15], and also in [91], which proposes the path schema for MD databases as a be\u008aer relational schema for dealing with the kinds of inconsistencies that appear in them.", "startOffset": 75, "endOffset": 85}, {"referenceID": 84, "context": "Repairs and consistent answers from MD databases have been investigated in [6, 7, 15], and also in [91], which proposes the path schema for MD databases as a be\u008aer relational schema for dealing with the kinds of inconsistencies that appear in them.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "As pointed out in Section 1, context-based quality data extraction is reminiscent of database repairing and consistent query answering [14, 17].", "startOffset": 135, "endOffset": 143}, {"referenceID": 15, "context": "As pointed out in Section 1, context-based quality data extraction is reminiscent of database repairing and consistent query answering [14, 17].", "startOffset": 135, "endOffset": 143}, {"referenceID": 15, "context": "\u008ce rewriting re\u0083ects a repair semantics based on deletions of tuples from Temperatures\u2032 when the constraint is not satis\u0080ed [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 58, "context": "However, this has been a predominant approach to OBDA with inconsistent ontologies [62, 65, 66]: the ontology is repaired by repairing the extensional instance and considering the intersection of its repairs (cf.", "startOffset": 83, "endOffset": 95}, {"referenceID": 60, "context": "However, this has been a predominant approach to OBDA with inconsistent ontologies [62, 65, 66]: the ontology is repaired by repairing the extensional instance and considering the intersection of its repairs (cf.", "startOffset": 83, "endOffset": 95}, {"referenceID": 61, "context": "However, this has been a predominant approach to OBDA with inconsistent ontologies [62, 65, 66]: the ontology is repaired by repairing the extensional instance and considering the intersection of its repairs (cf.", "startOffset": 83, "endOffset": 95}, {"referenceID": 79, "context": "\u008cis interpretation of quanti\u0080ers is possible if we have a metalevel CWA assumption or a domain closure axioms [84] over (some) predicates, none of which is part of Datalog\u00b1.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "Recent work in OBDA addresses this problem, allowing the combination of open and closed predicates in Datalog\u00b1 ontologies, but previously tractable CQA may become intractable [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 41, "context": "Similar extensions and results hold for light-weight DLs [44, 67, 68, 87].", "startOffset": 57, "endOffset": 73}, {"referenceID": 62, "context": "Similar extensions and results hold for light-weight DLs [44, 67, 68, 87].", "startOffset": 57, "endOffset": 73}, {"referenceID": 63, "context": "Similar extensions and results hold for light-weight DLs [44, 67, 68, 87].", "startOffset": 57, "endOffset": 73}, {"referenceID": 82, "context": "Similar extensions and results hold for light-weight DLs [44, 67, 68, 87].", "startOffset": 57, "endOffset": 73}, {"referenceID": 1, "context": "As an alternative to existential categorical variables as choices from given (possibly closed) sets of values, we could think of using disjunctive Datalog\u00b1, with disjunctive tgds [3, 26], in particular for downward navigation.", "startOffset": 179, "endOffset": 186}, {"referenceID": 24, "context": "As an alternative to existential categorical variables as choices from given (possibly closed) sets of values, we could think of using disjunctive Datalog\u00b1, with disjunctive tgds [3, 26], in particular for downward navigation.", "startOffset": 179, "endOffset": 186}, {"referenceID": 47, "context": "However, CQA under disjunctive sticky-Datalog\u00b1 may be undecidable in some cases [50, 77].", "startOffset": 80, "endOffset": 88}, {"referenceID": 72, "context": "However, CQA under disjunctive sticky-Datalog\u00b1 may be undecidable in some cases [50, 77].", "startOffset": 80, "endOffset": 88}, {"referenceID": 40, "context": "As a logical extension of a multidimensional data model, our model is similar in spirit to the data warehouse conceptual data model [43] that extends the entity-relationship data model with dimensions by means of the expressive description logic (DL) ALCFI [52].", "startOffset": 132, "endOffset": 136}, {"referenceID": 48, "context": "As a logical extension of a multidimensional data model, our model is similar in spirit to the data warehouse conceptual data model [43] that extends the entity-relationship data model with dimensions by means of the expressive description logic (DL) ALCFI [52].", "startOffset": 257, "endOffset": 261}, {"referenceID": 64, "context": "In [69], preliminary work motivated by data quality on specifying MD ontologies in light-weight DLs is reported, without going much into quality aspects or query answering.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "\u008ce existing declarative approaches to data quality [18] mostly use classic ICs, such as FDs and IDs, and denial constraints (i.", "startOffset": 51, "endOffset": 55}, {"referenceID": 35, "context": "Newer classes of dependencies have been introduced to capture data quality conditions and directly support data cleaning processes [38].", "startOffset": 131, "endOffset": 135}, {"referenceID": 38, "context": "Examples are conditional dependencies (conditional FDs and IDs), and matching dependencies [41, 42].", "startOffset": 91, "endOffset": 99}, {"referenceID": 39, "context": "Examples are conditional dependencies (conditional FDs and IDs), and matching dependencies [41, 42].", "startOffset": 91, "endOffset": 99}, {"referenceID": 20, "context": "Models of context [22] have been proposed and investigated in the data management and knowledge representation literature.", "startOffset": 18, "endOffset": 22}, {"referenceID": 45, "context": "Multi Context Systems (MCS) [48] and Local Models Semantics (LMS) [46, 47] are related logicbased frameworks for formalizing contexts and reasoning with them.", "startOffset": 28, "endOffset": 32}, {"referenceID": 43, "context": "Multi Context Systems (MCS) [48] and Local Models Semantics (LMS) [46, 47] are related logicbased frameworks for formalizing contexts and reasoning with them.", "startOffset": 66, "endOffset": 74}, {"referenceID": 44, "context": "Multi Context Systems (MCS) [48] and Local Models Semantics (LMS) [46, 47] are related logicbased frameworks for formalizing contexts and reasoning with them.", "startOffset": 66, "endOffset": 74}, {"referenceID": 73, "context": "In [78, 79], a general framework is proposed based on the concept of viewing for decomposing information bases into possibly overlapping fragments, called contexts, in order to be able to be\u008aer manage and customize information.", "startOffset": 3, "endOffset": 11}, {"referenceID": 74, "context": "In [78, 79], a general framework is proposed based on the concept of viewing for decomposing information bases into possibly overlapping fragments, called contexts, in order to be able to be\u008aer manage and customize information.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "In [5, 89], a model of contexts in information bases is proposed.", "startOffset": 3, "endOffset": 10}, {"referenceID": 42, "context": "In [45], ideas from [46], specially LMS, are applied to information integration and federated database management, where each database may have its own local semantics.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "In [45], ideas from [46], specially LMS, are applied to information integration and federated database management, where each database may have its own local semantics.", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "Context-aware data tailoring [24] proposes context dimension trees (CDTs) for modeling multidimensional aspects of context.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "\u008cis user\u2019s view is computed by combining the sub-views linked to dimension values [23, 25].", "startOffset": 82, "endOffset": 90}, {"referenceID": 23, "context": "\u008cis user\u2019s view is computed by combining the sub-views linked to dimension values [23, 25].", "startOffset": 82, "endOffset": 90}, {"referenceID": 66, "context": "In [71] dimensions, as in multidimensional databases, are used for modeling contexts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": "In [80, 88] contexts are used in preference database systems to support context-aware queries whose results depend on the context at the time of their submission.", "startOffset": 3, "endOffset": 11}, {"referenceID": 81, "context": "\u008ce context relational model (CR) model [86] extends the relational model with contexts, which are treated as \u0080rst-class citizens, at the level of database models and query languages.", "startOffset": 39, "endOffset": 43}], "year": 2017, "abstractText": "Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. \u008ce data under assessment is mapped into the context, for additional analysis, processing, and quality data extraction. \u008ce resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context we include a generalized multidimensional data model and a Datalog\u00b1 ontology with provably good properties in terms of query answering. \u008cese main components are used to represent dimension hierarchies, dimensional constraints, dimensional rules, and de\u0080ne predicates for quality data speci\u0080cation. \u008bery answering relies upon and triggers navigation through dimension hierarchies, and becomes the basic tool for the extraction of quality data. \u008ce OMD model is interesting per se, beyond applications to data quality. It allows for a logic-based, and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.", "creator": "LaTeX with hyperref package"}}}