{"id": "1704.00559", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "abstract": "Input into a neural sequence-to-sequence model is often determined by an upstream system, such as a word segmentator, part of a language tag, or speech recognition model. These upstream models are potentially error prone. Presenting input through word grids makes it possible to make this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we expand the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is capable of consuming word lattices and can be used as an encoder in an attentive encoder decoder model. We integrate rear grid numbers into this architecture by extending the child sum of the TreeLSTM and forgetting gates and introducing a distorted term into the attention mechanism. We experiment with language translation lattices and report consistent improvements over baseline values, either the best or the best hypothesis.", "histories": [["v1", "Mon, 3 Apr 2017 13:03:40 GMT  (415kb,D)", "http://arxiv.org/abs/1704.00559v1", null], ["v2", "Fri, 21 Jul 2017 13:31:07 GMT  (420kb,D)", "http://arxiv.org/abs/1704.00559v2", "EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["matthias sperber", "graham neubig", "jan niehues", "alex waibel"], "accepted": true, "id": "1704.00559"}, "pdf": {"name": "1704.00559.pdf", "metadata": {"source": "CRF", "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "authors": ["Matthias Sperber", "Graham Neubig", "Jan Niehues", "Alex Waibel"], "emails": ["matthias.sperber@kit.edu"], "sections": [{"heading": "1 Introduction", "text": "In many natural language processing tasks, we will require a down-stream system to consume the input of an up-stream system, such as word segmenters, part of speech taggers, or automatic speech recognizers. Among these, one of the most prototypical and widely used examples is speech translation, where a down-stream translation system must consume the output of an up-stream automatic speech recognition (ASR) system.\nPrevious research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step\n0: <s>\n1: ah 5: qu\u00e9 8: </s>\n2: hay 4: qu\u00e9\n3: que\n6: bueno\n7: bueno\n(Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008).\nRecently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works.\nAs a remedy, Su et al. (2016) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model. This is achieved by extending the encoder\u2019s Gated Recurrent Units (GRUs) (Cho et al., 2014) to be conditioned on multiple predecessor paths. The authors demonstrate improvements in Chineseto-English translation by translating lattices that combine the output of multiple word segmenters, rather than a single segmented sequence.\nar X\niv :1\n70 4.\n00 55\n9v 1\n[ cs\n.C L\n] 3\nA pr\n2 01\n7\nHowever, this model does not address one aspect of lattices that we argue is critical to obtaining good translation results: their ability to encode the certainty or uncertainty of the paths through the use of posterior scores. Specifically, we postulate that these scores are essential for tasks that require handling lattices with a considerable amount of erroneous content, such as those produced by ASR systems. In this paper, we propose a lattice-tosequence model that accounts for this uncertainty. Specifically, our contributions are as follows:\n\u2022 We employ the popular child-sum TreeLSTM (Tai et al., 2015) to derive a lattice encoder that replaces the sequential encoder in an attentional encoder-decoder model. We show empirically that this approach yields only minor improvements compared to a baseline that is fine-tuned on sequential recognition outputs. This finding stands in contrast to the positive results by Su et al. (2016), as well as by Ladhak et al. (2016) on a lattice classification task, and suggests higher learning complexity of our speech translation task.\n\u2022 We hypothesize that lattice scores are crucial in aiding training and inference, and propose several techniques for integrating lattice scores into the model: (1) We compute weighted child-sums,1 where hidden units in the lattice encoder are conditioned on their predecessor hidden units such that predecessors with low probability are less influential on the current hidden state. (2) We bias the TreeLSTM\u2019s forget gates for each incoming connection toward being more forgetful for predecessors with low probability, such that their cell states become relatively less influential. (3) We bias the attention mechanism to put more focus on source embeddings belonging to nodes with high lattice scores. We demonstrate empirically that the third proposed technique is particularly effective and produces strong gains over the baseline. According to our knowledge, this is the first attempt of integrating lattice scores at the training stage of a machine translation model.\n\u2022 We exploit the fact that our lattice encoder is a strict generalization of a sequential encoder by pre-training on sequential data, ob-\n1This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification.\ntaining faster and better training convergence on large corpora of parallel sequential data.\nWe conduct experiments on the Fisher and Callhome Spanish\u2013English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.4 BLEU points on Fisher and 0.8 BLEU points on Callhome, compared to a strong baseline optimized for translating 1-best ASR outputs. We find that the proposed integration of lattice scores is crucial for achieving these improvements."}, {"heading": "2 Background", "text": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.\nGiven an input sequence x = (x1, x2, . . . , xN ), the goal is to generate an appropriate output sequence y = (y1, y2, . . . , yM ). The conditional probability p(y | x) is estimated using parameters trained on a parallel corpus, e.g. of sentences in the source and target language in a translation task. This probability is factorized as the product of conditional probabilities of each token to be generated:\np(y | x) = M\u220f t=1 p(yt | y<t,x).\nTraining is performed in an end-to-end fashion. The training objective is to estimate parameters \u03b8 that maximize the log-likelihood of the sentence pairs in a given parallel training set D:\nJ(\u03b8) = \u2211\n(x,y)\u2208D\nlog p(y | x; \u03b8)."}, {"heading": "2.1 Encoder", "text": "In our baseline model, the encoder is a bidirectional recurrent neural network (RNN), following (Bahdanau et al., 2015). Here, the source sentence is processed in both the forward and backward directions with two separate RNNs. For every input xi, two hidden states are generated as\n\u2212\u2192 h i = LSTM ( Efwd(xi), \u2212\u2192 h i\u22121 ) (1) \u2190\u2212 h i = LSTM ( Ebwd(xi), \u2190\u2212 h i+1 ) , (2)\nwhere Efwd and Ebwd are source embedding lookup tables. We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997)\nrecurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015). We stack multiple LSTM layers and concatenate the final layer into the final source hidden state hi = \u2212\u2192 h i | \u2190\u2212 h i, where layer indices are omitted for simplicity."}, {"heading": "2.2 Attention", "text": "We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a fixed-size representation. At each decoding time step j, a context vector cj is computed as a weighted average of the source hidden states:\ncj = N\u2211 i=1 \u03b1ijhi.\nThe normalized attentional weights \u03b1ij measure the relative importance of the source words for the current decoding step and are computed as a softmax with normalization factor Z summing over i:\n\u03b1ij = 1\nZ exp\n( s ( sj\u22121,hi )) (3)\ns(\u00b7) is a feed-forward neural network with a single layer that estimates the importance of source hidden state hi for producing the next target symbol yj , conditioned on the previous decoder state sj\u22121."}, {"heading": "2.3 Decoder", "text": "The decoder creates output symbols one by one, conditioned on the encoder states via the attention mechanism. It contains another LSTM, initialized using the final encoder hidden state: s0 = hN . The decoding at step j takes place as follows, assuming a special start-of-sequence symbol y0:\nsj = LSTM ( Etrg(yj\u22121), sj\u22121 ) s\u0303t = tanh(Whs[sj ; cj ] + bhs)\nThe conditional probability that the j-th target word is generated is:\np(yj | y<j ,x) = softmax(Wsos\u0303t + bso)\nHere, Etrg is the target embedding lookup table, Whs and Wso are weight matrices, and bhs and bso are bias vectors.\nDuring decoding beam search is used to find an output sequence with high conditional probability."}, {"heading": "3 Attentional Lattice-to-Sequence Model", "text": "The seq2seq model described above assumes sequential inputs and is therefore limited to taking a single output of an up-stream model as input. Instead, we wish to consume lattices to carry over uncertainties from an up-stream model."}, {"heading": "3.1 Lattices", "text": "Lattices (e.g. Figure 1) represent multiple ambiguous or competing sequences in a compact form. They are a more efficient alternative to enumerating all hypotheses as an n-best list, as they allow for avoiding redundant computation over subsequences shared between multiple hypotheses. Lattices can either be produced directly, e.g. by an ASR dumping its pruned search space (Post et al., 2013), or can be obtained by merging several nbest sequences (Dyer et al., 2008; Su et al., 2016).\nA word lattice G = \u3008V,E\u3009 is a directed, connected, and acyclic graph with nodes V and edges E. V\u2282N is a node set, and (k, i)\u2208E denotes an edge connecting node k to node i. C(i) denotes the set of predecessor nodes for node i. We assume that all nodes follow a topological ordering, such that k<i \u2200 k\u2208C(i). Each node i is assigned a word label w(i).2 Every lattice contains exactly one start-of-sequence node with only outgoing edges, and exactly one end-of-sequence node with only incoming edges."}, {"heading": "3.2 Lattices and the TreeLSTM", "text": "One thing to notice here is that lattice nodes can have multiple predecessor states. In contrast, hidden states in LSTMs and other sequential RNNs are conditioned on only one predecessor state (h\u0303j in left column of Table 1), rendering standard RNNs unsuitable for the modeling of lattices. Luckily Tai et al. (2015)\u2019s TreeLSTM, which was designed to compose encodings in trees, is also straightforward to apply to lattices; the TreeLSTM composes multiple child states into a parent state, which can also be applied to lattices to compose multiple predecessor states into a successor state. Table 1, middle column, shows the TreeLSTM in its child-sum variant that supports an arbitrary number of predecessors. Conditioning on multiple predecessor hidden states is achieved by simply taking their sum as h\u0303i. Cell states from multi-\n2It is perhaps more common to think of each edge representing a word, but we will motivate why we instead assign word labels to nodes in Section 3.3.\n<s> ah hay que qu\u00e9 qu\u00e9 bueno bueno </s>\nple predecessor are each passed through their own forget gates fjk and then summed.\nEncoding a lattice results in one hidden state for each lattice node. Our lat2seq framework uses this network as encoder, computing the attention over all lattice nodes.3 In other words we replace (1) by the following:\n\u2212\u2192 h i = LatticeLSTM ( xi, { \u2212\u2192 h k | k\u2208C(i)} ) (4)\nSimilarly, we encode the lattice in backward direction and replace (2) accordingly. Figure 2 illustrates the result. The computational complexity of the encoder is O(|V | + |E|), i.e. linear in the number of nodes plus number of edges in the graph. The complexity of the attention mechanism is O(|V |M), where M is the output sequence length. |V | depends on both the expected input sentence length and the lattice density."}, {"heading": "3.3 Node-labeled Lattices", "text": "At this point we take a step back to motivate our choice of assigning word labels to lattice nodes, which is in contrast to the prior work by Ladhak et al. (2016) and Su et al. (2016) who assign word labels to edges. Recurrent states in edge-labeled lattice encoders are conditioned not only on multiple predecessor states, but must also aggregate words from multiple incoming edges. This implies that hidden units may represent more than one word in the lattice. Moreover, in the edge-labeled case hidden units that are in the same position in forward and backward encoders represent different words, but are nevertheless concatenated and attended to jointly. For these reasons we find our approach of encoding word-labeled lattices more\n3This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model.\n0: <s>\n1: ah 5: qu\u00e9 8: </s>\n2: hay 4: qu\u00e9\n3: que\n6: bueno\n7: bueno\nintuitively appealing when used as input to an attentional decoder, although empirical justification is beyond the scope of this paper. We also note that it is easy to convert an edge-labeled lattice into a node-labeled lattice using the line-graph algorithm (Hemminger and Beineke, 1978), which we utilize in this work."}, {"heading": "4 Integration of Lattice Scores", "text": "This section describes the key technical contribution of our work: integration of lattice scores encoding input uncertainty into the lat2seq framework. These lattice scores assign different probabilities to competing paths, and are often provided by up-stream statistical models. For example, an ASR may attach posterior probabilities that capture acoustic evidence and linguistic plausibility of words in the lattice. In this section, we describe our method, first explaining how we normalize scores to a format that is easily usable in our method, then presenting our methods for incorporating these scores into our encoder calculations."}, {"heading": "4.1 Lattice Score Normalization", "text": "Lattice scores that are obtained from upstream systems (such as ASR) are typically given in forwardnormalized fashion, interpreted as the probability of a node given its predecessor. Here, outgoing edges sum up to one, as illustrated by the left-most scores in Figure 3. However, in some of our methods it will be necessary that scores be normalized in the backward direction, so that the weights from incoming connections sum up to one, or globally normalized, so that the probability of the node is the marginal probability of all the paths containing that node.\nLetwf ,i,wm,i,wb,i denote forward-normalized, marginal, and backward-normalized scores for node i respectively, illustrated in Figure 3. Given wf ,i, we can compute marginal probabilities recur-\nsively as\nwm,i = \u2211\nk\u2208C(i)\nwm,k\u00b7wf ,i\nby using the forward algorithm (Rabiner, 1989). Then, we can normalize backward using\nwb,i = wm,i\u2211\nk\u2208C\u2032(i)wm,k ,\nwhere C \u2032(i) denotes the successors of node i. All 3 forms are employed in the sections below.\nFurthermore, when integrating these scores into the lat2seq framework, it is desirable to maintain flexibility over how strongly they should impact the model. For this purpose, we introduce a peakiness coefficient S. Given a lattice score wb,i in backward direction, we compute wSb,i/Zi. Zi= \u2211 k\u2208C(i)wk is a re-normalization term to ensure that incoming connections still sum up to one. In the forward direction, we compute wSf,i/Zi and normalize analogously over outgoing connections. Setting S=0 amounts to ignoring the scores by flattening their distribution, while letting S\u2192\u221e puts emphasis solely on the strongest nodes. S can be optimized jointly with the other model parameters via back-propagation during model training."}, {"heading": "4.2 Integration Approaches", "text": "We suggest three methods to integrate these scores into our lat2seq model, with equations shown in\nthe right column of Table 1. These methods can optionally be combined, and we conduct an ablation study to assess the effectivity of each method in isolation (\u00a75.3).\nThe first method consists of computing a weighted child-sum (WCS), using lattice scores as weights when composing the hidden state h\u0303i. This is based on the intuition that predecessor hidden states with high lattice weights should have a higher influence on their successor than states with low weights. The precise formulas for WCS are shown in (5).\nThe second method biases the forget gate fik for each predecessor cell state such that predecessors with high lattice score are more likely to pass through the forget gate (BFG). The intuition for this is similar to WCS; the composed cell state is more highly influenced by cell states from predecessors with high lattice score. BFG is implemented by introducing a bias term inside the sigmoid as in (6).\nIn the cases of both WCS and BFG, all hidden units have their own independent peakiness. Thus Sh and Sf are vectors, applied elementwise after broadcasting the lattice score. The renormalization terms Zh,k and Zf,k are also vectors and are applied element-wise. We use backwardnormalized scores wb,i for the forward-directed encoder, and forward-normalized scores wf ,i for the backward-directed encoder.\nIn the third and final method, we bias the attentional weights (BATT) to put more focus on lattice nodes with high lattice scores. This can potentially mitigate the problem of having multiple contradicting lattice nodes that may confuse the attentional decoder. BATT is computed by introducing a bias term to the attention as in (7). Attentional weights are scalars, so here the peakiness Sa is also a scalar. We drop the normalization term, relying instead on the softmax normalization. Both BFG and BATT use the logarithm of lattice scores so that values will still be in the probability domain after the softmax or sigmoid is computed."}, {"heading": "4.3 Pre-training", "text": "Finally, to reduce the computational burden, we perform a two-step training process where the model is first pre-trained on sequential data, then fine-tuned on lattice data. The pre-training, like standard training for neural machine translation (NMT), allows for efficient training using minibatches, and also allows for training on standard text corpora for which we might not have lattices available. The fine-tuning is then performed on parallel data with lattices on the source side. This is much slower4 than the pre-training because the network structure changes from sentence to sentence, preventing us from using efficient minibatched calculations. However, fine-tuning for only a small number of iterations is generally sufficient, as the model is already relatively accurate in the first place. In practice we found it important to use minibatches when fine-tuning, accumulating gradients over several examples before performing parameter updates. This provided negligible speedups but greatly improved optimization stability.\nAt test time, the model is able to translate both sequential and lattice inputs and can therefore be used even in cases where no lattices are available, at potentially diminished accuracy."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setting", "text": "We conduct experiments on the Fisher and Callhome Spanish\u2013English Speech Translation Corpus\n4With our implementation, we could process sequential inputs about 75 times faster than lattice inputs during training, and overall fine-tuning convergence was 15 times faster. Decoding time was only slightly increased by a factor of 1.2 when using lattice inputs.\n(Post et al., 2013). This is a corpus of Spanish telephone conversations that includes automatic transcripts and lattices. The Fisher portion consists of telephone conversations between strangers, while the Callhome portion contains telephone conversations between relatives or friends. The training data size is 138,819 sentences (Fisher/Train), and 15,000 sentences (Callhome/Train). Heldout testing data is shown in Table 2. ASR word error rates (WER) are relatively high, due to the spontaneous speaking style and challenging acoustic conditions. Lattice contain on average 3.4 (Fisher/Train) or 4.5 (Callhome/Train) times more words than their corresponding reference transcripts.\nFor preprocessing, we tokenized and lowercased source and target sides. We removed punctuation from the reference transcripts on the source side for consistency with the automatic transcripts and lattices. All models are pre-trained and finetuned on Fisher/Train unless otherwise noted. Our source-side vocabulary contains all words from the automatic transcripts for Fisher/Train, replacing singletons by an unknown word token, totaling 14,648 words. Similarly, on the target side we used all words from the reference translations of Fisher/Train, replacing singletons by the unknown word, yielding 10,800 words in total.\nOur implementation is based on lamtram (Neubig, 2015) and DyNet (Neubig et al., 2017). We use the implemented attentional model with default parameters: a layer size of 256 per encoder direction and 512 for the decoder, word embedding size of 512. We used 2 encoder and decoder layers for better baseline performance. For the sequential baselines, the LSTM variant in the left column of Table 1 was employed. Forget gates were initialized to 1, following Jozefowicz et al. (2015).\nWe used Adam (Kingma and Ba, 2014) for training, with an empirically determined initial learning rate of 0.001 for pre-training and 0.0001 for fine-tuning. We halve the learning rate when the dev perplexity (on Fisher/Dev) gets worse. Pre-training and fine-tuning on 1-best sequences is performed until convergence, and training on lattices is performed for 2 epochs to keep experimental effort manageable. On Fisher/Train, this took 3-4 days on a fast CPU.5 Minibatch size was 1000 words for pre-training, and 20 sentences for lattice training. Unless otherwise noted, peakiness coefficients were jointly optimized during training. We repeat training 3 times with different random seeds for parameter initialization and data shuffling, and report averaged results. The decoding beam size is 5."}, {"heading": "5.2 Main Results", "text": "We compare 4 systems: Performing pre-training on the sequential reference transcripts only (R), fine-tuning on 1-best transcripts (R+1), fine-tuning on lattices without scores (R+L), and fine-tuning on lattices including lattice scores (R+L+S). At test time, we try references, lattice oracles,6 1-best transcripts, and lattices as inputs to all 4 systems. The former 2 experiments give upper bounds on achievable translation accuracy, while the latter 2 correspond to a realistic setting. Table 3 shows the results on Fisher/Dev2 and Fisher/Test.\nBefore even considering lattices, we can see that 1-best fine-tuning boosted BLEU scores quite impressively (1-best/R vs. 1-best/R+1), with gains of 1.3 and 0.7 BLEU points. This stands in contrast to Post et al. (2013) who find the 1-best transcripts not to be helpful for training a hierarchical machine translation system. Possible explanations are learning from repeating error patterns, and improved robustness to erroneous inputs. On top of these gains, our proposed set-up (lattice/R+L+S) improve BLEU scores by another 1.4. Removing the lattice scores (lattice/R+L) diminishes the results and performs worse than the 1-best baseline (1-best/R+1), indicating that the proposed lattice score integration is crucial for good performance. This demonstrates a clear advantage of our proposed method over that of Su et al. (2016).\n5For comparison, we tried training on lattices from scratch, which took 9 days (6 epochs) to converge at a dev perplexity that was 10% worse than with the pre-training plus fine-tuning strategy.\n6The path through the lattice with the best WER.\nAs can be seen in the table, models finetuned on lattices show reasonable performance for both lattice and sequential inputs (1-best/R+L, lattice/R+L, 1-best/R+L+S, lattice/R+L+S). This is not surprising, given that the lattice training data includes lattices of varying density, including lattices with very few paths or even only one path. On the other hand, without fine-tuning on lattices, using lattices as input performs poorly (lattice/R and lattice/R+1). A closer look revealed that translations were often too long, potentially because implicitly learned mechanisms for length control were not ready to handle lattice inputs.\nTable 3 reports perplexities for Fisher/Dev2. Unlike the corresponding BLEU scores, the lattice encoder appears stronger than the 1-best baseline in terms of perplexity even without lattice scores (lattice/R+L vs. 1-best/R+1). To understand this better, we computed the average entropy of the decoder softmax, a measure of how much confusion there is in the decoder predictions, independent of whether it selects the correct answer or not. Over the first 100 sentences, this value was 2.24 for 1-best/R+1, 2.39 for lattice/R+L, and 2.15 for lattice/R+L+S. This indicates that the decoder is more confused for lattices without scores, while integrating lattice scores removes this problem. These numbers also suggest that it may be possible to obtain further gains using methods that stabilize the decoder."}, {"heading": "5.3 Ablation Experiments", "text": "Next, we conduct an ablation study to assess the impact of the three proposed extension for integrating lattice scores (Section 4.2). We train models with different peakiness coefficients S, either ignoring lattices scores by fixing S=0, using lattice scores as-is by fixing S=1, or optimizing S during training. Table 4 shows the results. Overall, joint training of S gives similar results as fixing S=1, but both clearly outperform fixing S=0. Removing confidences (setting S=0) in one place at a time reveals that the attention mechanism is clearly the most important point of integration, while gains from the integration into child-sum and forget gate are smaller and not always consistent.\nWe also analyzed what peakiness values were actually learned. We found that all 3 models that we trained for the averaging purposes converged to Sa=0.62. Sh and Sf had per-vector means\nbetween 0.92 and 1.0, at standard deviations between 0.02 and 0.04. We conclude that while the peakiness coefficients were not particularly helpful in our experiments, stable convergence behavior makes them safe to use, and they might be helpful on other data sets that may contain lattice scores of higher or lower reliability."}, {"heading": "5.4 Callhome Experiments", "text": "In this experiment, we test a situation in which we have a reasonable amount of sequential data available for pre-training, but only a limited amount of lattice training data for the fine-tuning step. This may be a more realistic situation, because speech translation corpora are scarce. To investigate in this scenario, we again pre-train our mod-\nels on Fisher/Train, but then fine-tune them on the 9 times smaller Callhome/Train portion of the corpus. We fine-tune for 10 epochs, all other settings are as before. We use Callhome/Evltest for testing. Table 5 shows the results. The trends are consistent to Section 5.2: The proposed model (lattice/R+L+S) outperforms the 1-best baseline (1- best/R+1) by 0.8 BLEU points, which in turn beats the pre-trained system (1-best/R) by 1.5 BLEU points. Including the lattice scores is clearly beneficial, although lattices without scores also improve over 1-best inputs in this experiment."}, {"heading": "5.5 Impact of Lattice Quality", "text": "Next, we analyze the impact of using lattices and lattice scores as the ASR WER changes. For this purpose, we concatenate all test data sets from Table 2 and divide the result into bins according to the 1-best WER. We sample 1000 sentences from each bin, and compare BLEU scores between several models.\nThe results are shown in Figure 4. For very good WERs, lattices do not improve over 1-best inputs, which is unsurprising. In all other cases, lattices are helpful. Lattice scores are most beneficial for moderate WERs, and not beneficial for very high WERs. We speculate that for high WERs, the lattice scores tend to be less reliable than for lower WERs."}, {"heading": "6 Conclusion", "text": "We investigated translating uncertain inputs from an error-prone up-stream component using a neural lattice-to-sequence model. Our proposed model takes word lattices as input and is able to take advantage of lattice scores. In our experiments in a speech translation task we find consistent improvements over translating 1-best transcriptions and that consideration of lattice scores, especially in the attention mechanism, is crucial for obtaining these improvements."}, {"heading": "Acknowledgments", "text": "We thank Paul Michel and Austin Matthews for their helpful comments on earlier drafts of this paper."}, {"heading": "A Appendix: Cherry-Picked Examples", "text": "A.1 Erroneous 1-best \u201dideal\u201d is contained in lattice but not 1-best transcript, and correctly translated in the lattice/R+L+S setting.\n1-best/R+1 input: y y eso es algo que a mi me parece contraproducente verdad porque uno piensa y cuando ya a todos uno quisiera tal vez un mundo ya el de que una vez que cadena cuerpos trabajara\u0301n por el bienestar de de todos\n1-best/R+1 output: and , and that \u2019s something that seems to me , right ? because one thinks , and when you think , and when everyone would like perhaps a world already , the one time that the chain changes for the\nlattice/R+L+S input:\nmundo\nideal\nel\nde\nya\n\u2026 \u2026\nlattice/R+L+S output: and , and that \u2019s something that seems to me , right ? because one thinks , and when you see , when you go to a ideal world , you see that they are illegals for the , well , they are all foreigners\nA.2 Redundant Lattice Content Another frequent pattern was a word appearing (once) in the 1-best transcript, but multiple times in the lattice, and only the lattice/R+L+S model translating this word.\n1-best/R+1 input: los que van porque que es un d\u0131\u0301a los que van porque no tiene alicia derrita jugar y los que s\u0131\u0301 caray profesionalmente porque hay ciertos counselor bueno creo que soy jose\u0301 playa que\n1-best/R+1 output: the ones that go , because it \u2019s a day that they go , because they don \u2019t have alicia , play and the ones that are italian , because there are some <unk> , well , i think i \u2019m jose\nlattice/R+L+S input: yo\npor que\n\u2026\n\u2026 caray \u2026\ncaballo\nvaya\n\u2026\n\u2026\nprofesionalmente\nprofesionalmente\nprofesionalmente\nprofesionalmente\nlattice/R+L+S output: the ones that go , because it \u2019s a day that they go because you don \u2019t\nwant to play and play , and the ones that influenced professionally , because there are certain things , well , i think that i \u2019m jose\nA.3 Redundant Lattice Content In this example, the lattice/R+L+S system correctly produced <unk> as translation to the unknown input word \u201dmo\u0301viles\u201d, while the 1- best/R+1 system produced a seemingly random word instead. A possible explanation is the added context helping the lat2seq system to know when to be unsure.\n1-best/R+1 input: s\u0131\u0301 s\u0131\u0301 bueno contar otro que usaban los tele\u0301fonos sate\u0301lite los los mo\u0301viles no funcionaban bien porque pero a veces si funcionaban s\u0131\u0301\n1-best/R+1 output: yes , well , tell me that i used to use satellite phones , the kids didn \u2019t work well , because sometimes it worked , yes\nlattice/R+L+S input:\nno \u2026 los \u2026 los m\u00f3viles\nlattice/R+L+S output: yes , yes , well , with the other that i used to use the satellite phones , the , the \u3008unk\u3009 didn \u2019t work well because , but sometimes it worked , yes\nA.4 Counter Example In this example, the lattice (but not the 1-best transcript) contained then word \u201dsan\u201d, which tricked the lat2seq decoder to produce \u201dsan francisco\u201d.\n1-best/R+1 input: pero los dema\u0301s aqu\u0131\u0301 esta\u0301n tambie\u0301n s\u0131\u0301 esta\u0301 bien esta\u0301 tranquilo para aca\u0301\n1-best/R+1 output: but the rest here are also , yes , it \u2019s ok , it \u2019s quiet for here\nlattice/R+L+S input:\ntambi\u00e9n \u2026\nen \u2026 san\nest\u00e1n \u2026\ntambi\u00e9n\ns\u00ed\nlattice/R+L+S output: but the rest here in san francisco is very quiet here"}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Representation Learning (ICLR). San Diego, USA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Some approaches to statistical and finite-state speech-to", "author": ["F. Casacuberta", "H. Ney", "F.J. Och", "E. Vidal", "J.M. Vilar", "S. Barrachina", "I. Garcia-Varea", "D. Llorens", "C. Martinez", "S. Molau", "F. Nevado", "M. Pastor", "D. Picco", "A. Sanchis", "C. Tillmann"], "venue": null, "citeRegEx": "Casacuberta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Casacuberta et al\\.", "year": 2004}, {"title": "Learning Phrase Representations Using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generalizing Word Lattice Translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "Technical Report LAMP-TR-149, University of Maryland, Institute For Advanced Computer Studies.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Tree-to-Sequence Attentional Neural Machine Translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Association for Computational Linguistic (ACL). Berlin, Germany, pages 823\u2013833.", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Line graphs and line digraphs", "author": ["R.L. Hemminger", "L.W. Beineke."], "venue": "Selected Topics in Graph Theory, Academic Press Inc., pages 271\u2013305.", "citeRegEx": "Hemminger and Beineke.,? 1978", "shortCiteRegEx": "Hemminger and Beineke.", "year": 1978}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "International Conference on Machine Learning (ICML). Lille, France.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Seattle, Washington, USA, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "LatticeRnn: Recurrent Neural Networks over Lattices", "author": ["Faisal Ladhak", "Ankur Gandhe", "Markus Dreyer", "Lambert Mathias", "Ariya Rastrow", "Bj\u00f6rn Hoffmeister."], "venue": "Annual Conference of the International Speech Communication Association (Inter-", "citeRegEx": "Ladhak et al\\.,? 2016", "shortCiteRegEx": "Ladhak et al\\.", "year": 2016}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). Lisbon, Portugal, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "ASR word lattice translation with exhaustive reordering is possible", "author": ["Evgeny Matusov", "Bj\u00f6rn Hoffmeister", "Hermann Ney."], "venue": "Annual Conference of the International Speech Communication Association (InterSpeech). Brisbane, Australia, pages", "citeRegEx": "Matusov et al\\.,? 2008", "shortCiteRegEx": "Matusov et al\\.", "year": 2008}, {"title": "lamtram: A Toolkit for Language and Translation Modeling using Neural Networks", "author": ["Graham Neubig."], "venue": "http://www.github.com/neubig/lamtram.", "citeRegEx": "Neubig.,? 2015", "shortCiteRegEx": "Neubig.", "year": 2015}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Speech Translation: Coupling of Recognition and Translation", "author": ["Hermann Ney."], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Phoenix, USA, pages 517\u2013520.", "citeRegEx": "Ney.,? 1999", "shortCiteRegEx": "Ney.", "year": 1999}, {"title": "Improved Speech-to-Text Translation with the Fisher and Callhome Spanish\u2013English Speech Translation Corpus", "author": ["Matt Post", "Gaurav Kumar", "Adam Lopez", "Damianos Karakos", "Chris Callison-Burch", "Sanjeev Khudanpur."], "venue": "International Work-", "citeRegEx": "Post et al\\.,? 2013", "shortCiteRegEx": "Post et al\\.", "year": 2013}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner."], "venue": "Proceedings of the IEEE .", "citeRegEx": "Rabiner.,? 1989", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Using Word Lattice Information for a Tighter Coupling in Speech Translation Systems", "author": ["Shirin Saleem", "Szu-Chen (Stan) Jou", "Stephan Vogel", "Tanja Schultz"], "venue": "In International Conference of Spoken Language Processing (ICSLP). Jeju Island, Korea,", "citeRegEx": "Saleem et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Saleem et al\\.", "year": 2004}, {"title": "Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation", "author": ["Jinsong Su", "Zhixing Tan", "Deyi Xiong", "Yang Liu."], "venue": "arXiv preprint arXiv:1609.07730 .", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS). Montr\u00e9al, Canada, pages 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Association for Computational Linguistic (ACL). Beijing, China, pages 1556\u20131566.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A Decoding Algorithm for Word Lattice Translation in Speech Translation", "author": ["Ruiqiang Zhang", "Genichiro Kikui", "Hirofumi Yamamoto", "Wai-Kit Lo."], "venue": "International Workshop on Spoken Language Translation (IWSLT). Pittsburgh, USA, pages", "citeRegEx": "Zhang et al\\.,? 2005", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoderdecoder model.", "startOffset": 37, "endOffset": 55}, {"referenceID": 15, "context": "(Ney, 1999; Casacuberta et al., 2004).", "startOffset": 0, "endOffset": 37}, {"referenceID": 1, "context": "(Ney, 1999; Casacuberta et al., 2004).", "startOffset": 0, "endOffset": 37}, {"referenceID": 18, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 22, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 12, "context": "Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al.", "startOffset": 67, "endOffset": 130}, {"referenceID": 3, "context": ", 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008).", "startOffset": 69, "endOffset": 88}, {"referenceID": 8, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 20, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 0, "context": "Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "startOffset": 55, "endOffset": 134}, {"referenceID": 2, "context": "This is achieved by extending the encoder\u2019s Gated Recurrent Units (GRUs) (Cho et al., 2014) to be conditioned on multiple predecessor paths.", "startOffset": 73, "endOffset": 91}, {"referenceID": 18, "context": "As a remedy, Su et al. (2016) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model.", "startOffset": 13, "endOffset": 30}, {"referenceID": 21, "context": "\u2022 We employ the popular child-sum TreeLSTM (Tai et al., 2015) to derive a lattice encoder that replaces the sequential encoder in an attentional encoder-decoder model.", "startOffset": 43, "endOffset": 61}, {"referenceID": 18, "context": "This finding stands in contrast to the positive results by Su et al. (2016), as well as by Ladhak et al.", "startOffset": 59, "endOffset": 76}, {"referenceID": 10, "context": "(2016), as well as by Ladhak et al. (2016) on a lattice classification task, and suggests higher learning complexity of our speech translation task.", "startOffset": 22, "endOffset": 43}, {"referenceID": 10, "context": "This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification.", "startOffset": 56, "endOffset": 77}, {"referenceID": 16, "context": "We conduct experiments on the Fisher and Callhome Spanish\u2013English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 20, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 0, "context": "Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section.", "startOffset": 72, "endOffset": 151}, {"referenceID": 0, "context": "In our baseline model, the encoder is a bidirectional recurrent neural network (RNN), following (Bahdanau et al., 2015).", "startOffset": 96, "endOffset": 119}, {"referenceID": 6, "context": "We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997)", "startOffset": 41, "endOffset": 75}, {"referenceID": 21, "context": "recurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015).", "startOffset": 113, "endOffset": 131}, {"referenceID": 11, "context": "We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a fixed-size representation.", "startOffset": 30, "endOffset": 50}, {"referenceID": 16, "context": "by an ASR dumping its pruned search space (Post et al., 2013), or can be obtained by merging several nbest sequences (Dyer et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": ", 2013), or can be obtained by merging several nbest sequences (Dyer et al., 2008; Su et al., 2016).", "startOffset": 63, "endOffset": 99}, {"referenceID": 19, "context": ", 2013), or can be obtained by merging several nbest sequences (Dyer et al., 2008; Su et al., 2016).", "startOffset": 63, "endOffset": 99}, {"referenceID": 21, "context": "Luckily Tai et al. (2015)\u2019s TreeLSTM, which was designed to compose encodings in trees, is also straightforward to apply to lattices; the TreeLSTM composes multiple child states into a parent state, which can also be applied to lattices to compose multiple predecessor states into a successor state.", "startOffset": 8, "endOffset": 26}, {"referenceID": 10, "context": "At this point we take a step back to motivate our choice of assigning word labels to lattice nodes, which is in contrast to the prior work by Ladhak et al. (2016) and Su et al.", "startOffset": 142, "endOffset": 163}, {"referenceID": 10, "context": "At this point we take a step back to motivate our choice of assigning word labels to lattice nodes, which is in contrast to the prior work by Ladhak et al. (2016) and Su et al. (2016) who assign word labels to edges.", "startOffset": 142, "endOffset": 184}, {"referenceID": 4, "context": "This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model.", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "We also note that it is easy to convert an edge-labeled lattice into a node-labeled lattice using the line-graph algorithm (Hemminger and Beineke, 1978), which we utilize in this work.", "startOffset": 123, "endOffset": 152}, {"referenceID": 21, "context": "Table 1: Formulas for sequential and TreeLSTM encoders according to Tai et al. (2015), the proposed LatticeLSTM encoder, and conventional vs.", "startOffset": 68, "endOffset": 86}, {"referenceID": 17, "context": "by using the forward algorithm (Rabiner, 1989).", "startOffset": 31, "endOffset": 46}, {"referenceID": 16, "context": "(Post et al., 2013).", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Our implementation is based on lamtram (Neubig, 2015) and DyNet (Neubig et al.", "startOffset": 39, "endOffset": 53}, {"referenceID": 7, "context": "Forget gates were initialized to 1, following Jozefowicz et al. (2015).", "startOffset": 46, "endOffset": 71}, {"referenceID": 9, "context": "We used Adam (Kingma and Ba, 2014) for training, with an empirically determined initial learning rate of 0.", "startOffset": 13, "endOffset": 34}, {"referenceID": 16, "context": "This stands in contrast to Post et al. (2013) who find the 1-best transcripts not to be helpful for training a hierarchical machine translation system.", "startOffset": 27, "endOffset": 46}, {"referenceID": 16, "context": "This stands in contrast to Post et al. (2013) who find the 1-best transcripts not to be helpful for training a hierarchical machine translation system. Possible explanations are learning from repeating error patterns, and improved robustness to erroneous inputs. On top of these gains, our proposed set-up (lattice/R+L+S) improve BLEU scores by another 1.4. Removing the lattice scores (lattice/R+L) diminishes the results and performs worse than the 1-best baseline (1-best/R+1), indicating that the proposed lattice score integration is crucial for good performance. This demonstrates a clear advantage of our proposed method over that of Su et al. (2016).", "startOffset": 27, "endOffset": 658}], "year": 2017, "abstractText": "The input to a neural sequence-tosequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoderdecoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM\u2019s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.", "creator": "LaTeX with hyperref package"}}}