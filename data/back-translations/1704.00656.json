{"id": "1704.00656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Detection and Resolution of Rumours in Social Media: A Survey", "abstract": "Despite the increasing use of social media platforms for gathering information and news, their unmoderated nature often leads to the emergence and dissemination of rumors, i.e. information that is not verified at the time of posting. At the same time, the openness of social media platforms provides opportunities to investigate how users share and discuss rumors, and how natural language processing and data mining techniques can be used to find ways to determine their veracity. In this survey, we present and discuss two types of rumors circulating on social media: longstanding rumors circulating over long periods of time, and newly emerging rumors that emerge during fast-paced events such as breaking news, where reports are published piecemeal and often with an unverified status in their early stages. We offer an overview of social media rumor research with the ultimate goal of developing a rumor classification system consisting of four components: detection of rumors, tracking of rumors, classification of rumors, and classification of rumors.", "histories": [["v1", "Mon, 3 Apr 2017 15:57:44 GMT  (77kb)", "http://arxiv.org/abs/1704.00656v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.IR cs.SI", "authors": ["arkaitz zubiaga", "ahmet aker", "kalina bontcheva", "maria liakata", "rob procter"], "accepted": false, "id": "1704.00656"}, "pdf": {"name": "1704.00656.pdf", "metadata": {"source": "CRF", "title": "Detection and Resolution of Rumours in Social Media: A Survey", "authors": ["Arkaitz Zubiaga", "Ahmet Aker", "Maria Liakata"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 65\n6v 1\n[ cs\n.C L\n] 3\nA pr\n2 01\n7\nDetection and Resolution of Rumours in Social Media: A Survey\nArkaitz Zubiaga, University of Warwick, Coventry, UK Ahmet Aker, University of Sheffield, Sheffield, UK and University of Duisburg-Essen, Duisburg/Essen, Germany Kalina Bontcheva, University of Sheffield, Sheffield, UK Maria Liakata, University of Warwick, Coventry, UK Rob Procter, University of Warwick, Coventry, UK\nDespite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e. pieces of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how natural language processing and data mining techniques may be used to find ways of determining their veracity. In this survey we introduce and discuss two types of rumours that circulate on social media; long-standing rumours that circulate for long periods of time, and newly-emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far towards the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for detection and resolution of rumours."}, {"heading": "1. INTRODUCTION", "text": "Social media platforms are increasingly being used as a tool for gathering information about, for example, societal issues [Lazer et al. 2009] and to find out about the latest developments during breaking news stories [Phuvipadawat and Murata 2010]. This is possible because these platforms enable anyone with an Internet-connected device to share in real-time their thoughts and/or to post an update about an unfolding event that they may be witnessing. Hence, social media has become a powerful tool for journalists [Diakopoulos et al. 2012; Tolmie et al. 2017a] but also for ordinary citizens [Hermida 2010]. However, while social media provides access to an unprecedented source of information, the absence of systematic efforts by platforms to moderate posts also leads to the spread of misinformation [Procter et al. 2013b; Webb et al. 2016], which then requires extra effort to establish their provenance and veracity. Updates associated with breaking news stories are often released piecemeal, which gives rise to a significant proportion of those updates being unverified at the time of posting, some of which may later be proven to be false [Silverman 2015a]. In the absence of an authoritative statement corroborating or debunking an ongoing rumour, it is observed that social media users will often share their own thoughts on its veracity via a process of collective, inter-subjective sense-making [Tolmie et al. 2017b] that may lead to the exposure of the truth behind the rumour [Procter et al. 2013a; Li and Sakamoto 2015]. Nevertheless, despite this apparent robustness of social media, its increasing tendency to give rise to rumours motivates the development of systems that, by gathering and analysing the collective judgements of users [Lukasik et al. 2016], are able to reduce the spread of rumours by accelerating the sense-making process [Derczynski and Bontcheva 2014]. A rumour detection system that identifies, in its early stages, postings whose veracity status is unverified, can be effectively used to warn users that the information in them may turn out to be false [Zhao et al. 2015]. Likewise, a rumour classification system that aggregates the evolving, collective judgements posted by users can help track the veracity status of a rumour as it is exposed to this process\nof collective sense-making [Metaxas et al. 2015]. In this paper we present an overview of the components needed to develop such a rumour classification system and discuss the success so far of the efforts towards building it."}, {"heading": "1.1. Defining and Characterising Rumours", "text": "Rumour definition. Recent publications in the research literature have used definitions of rumours that differ from one another. For example, some recent work has mis-defined a rumour as a piece of information that is deemed false (e.g. [Cai et al. 2014; Liang et al. 2015]), while the majority of the literature defines rumours instead as \u201cunverified and instrumentally relevant information statements in circulation\u201d [DiFonzo and Bordia 2007]. In our work, we have adopted as the defining characteristic of rumours that they are unverified at the time of posting, which is consistent with the definition given by major dictionaries, such as the Oxford English Dictionary, which defines a rumour as \u201ca currently circulating story or report of uncertain or doubtful truth\u201d1 or the Merriam Webster dictionary, which defines it as \u201ca statement or report current without known authority for its truth\u201d2. This unverified information may turn out to be true, or partly or entirely false; alternatively, it may also remain unresolved. Hence, throughout this paper we adhere to this prevailing definition of rumour that classifies it as \u201ca piece of circulating information whose veracity status is yet to be verified at the time of posting\u201d. A rumour can be understood as a piece of information that has not yet been verified, and hence its truth value remains unresolved while it is circulating. Rumour types. One may rely on many different factors when classifying rumours by type, including its eventual veracity value (true, false or unresolved) [Zubiaga et al. 2016] or its degree of credibility (e.g. high or low) [Jaeger et al. 1980]. Another attempt at classifying rumours by type is that by [Knapp 1944], who introduced a taxonomy of three types of rumours: (1) \u2018pipe-dream\u2019 rumours: i.e. rumours that lead to wishful thinking; (2) \u2018bogy\u2019 rumours: i.e. those that increase anxiety or fear; and (3) \u2018wedgedriving\u2019 rumours: i.e. those that generate hatred. When it comes to the development of a rumour classification system, the factor that largely determines approaches to be utilised is their temporal characteristics:\n(1) New rumours that emerge during breaking news. Rumours that emerge in the context of breaking news are generally rumours that have not been observed before. Therefore, rumours need to be automatically detected and a rumour classification system needs to be able to deal with new, unseen rumours, considering that the training data available to the system may differ from what will later be observed by the system. An example of a rumour that emerges during breaking news is when the identity of a suspected terrorist is reported. A rumour classification system may have observed other similar cases of suspected terrorists, but the case and the names involved will most likely differ. Therefore, the design of a rumour classifier in these cases will need to consider the emergence of new cases, with the new vocabulary that they will likely bring. (2) Long-standing rumours that are discussed for long periods of time. Some rumours may circulate for long periods of time without their veracity being established with certainty. These rumours provoke significant, ongoing interest, despite (or perhaps because of) the difficulty in establishing the actual truth. This is, for example, the case of the rumour stating that Barack Obama is muslim. While this statement is unsubstantiated, it appears that there is no evidence that helps debunk it to the\n1https://en.oxforddictionaries.com/definition/rumour 2http://www.merriam-webster.com/dictionary/rumor\nsatisfaction of everyone.3 For rumours like these, a rumour classification system may not need to detect the rumour, as it might be known a priori. Moreover, the system can make use of historical discussions about the rumour to classify ongoing discussions, where the vocabulary is much less likely to differ and therefore the classifier built on old data can still be used for new data.\nThroughout the paper we refer to these two types of rumours, describing how different approaches can deal with each of them."}, {"heading": "1.2. Studying Rumours: From Early Studies to Social Media", "text": "A brief history. Rumours and related phenomena have been studied from many different perspectives [Donovan 2007], ranging from psychological studies [Rosnow and Foster 2005] to computational analyses [Qazvinian et al. 2011]. Traditionally, it has been very difficult to study people\u2019s reactions to rumours, given that this would involve real-time collection of reaction as rumours unfold, assuming that participants had already been recruited. To overcome this obstacle, Allport undertook early investigations [Allport and Postman 1946; 1947] in the context of wartime rumours. He posited the importance of studying rumours, emphasising that \u201cnewsworthy events are likely to breed rumors\u201d and that \u201cthe amount of rumor in circulation will vary with the importance of the subject to the individuals involved times the ambiguity of the evidence pertaining to the topic at issue\u201d. This led him to set forth a motivational question which is yet to be answered: \u201cCan rumors be scientifically understood and controlled?\u201d [Allport and Postman 1946]. His 1947 experiment [Allport and Postman 1947] reveals an interesting fact about rumour circulation and belief. He looked at how US President Franklin D. Roosevelt allayed rumours about losses sustained by the US Navy at the Japanese attack on Pearl Harbor in 1941. The study showed that before the President made his address, 69% of a group of undergraduate students believed that losses were greater than officially stated; but five days later, the President having spoken in the meantime, only 46% of an equivalent group of students believed this statement to be true. This study revealed the importance of an official announcement by a reputable person in shaping society\u2019s perception of the accuracy of a rumour. Early research focused on different objectives. Some work has looked at the factors that determine the diffusion of a rumour, including, for instance, the influence of the believability of a rumour on its subsequent circulation, where believability refers to the extent to which a rumour is likely to be perceived as truthful. Early research by Prasad [Prasad 1935] and Sinha [Sinha 1952] posited that believability was not a factor affecting rumour mongering in the context of natural disasters. More recently, however, Jaeger et al. [Jaeger et al. 1980] found that rumours were passed on more frequently when the believability level was high. Moreover, Jaeger et al. [Jaeger et al. 1980] and Scanlon [Scanlon 1977] found the importance of a rumour as perceived by recipients to be a factor that determineswhether or not it is spread, the least important rumours being spread more. Rumours on the Internet. The widespread adoption of the Internet gave rise to a new phase in the study of rumour in naturalistic settings [Bordia 1996] and has taken on particular importance with the advent of social media, which not only provides powerful new tools for sharing information but also facilitates data collection from large numbers of participants. For instance, Takayasu et al. [Takayasu et al. 2015] used social media to study the diffusion of a rumour in the context of the 2011 Japan Earthquake, which stated that rain in the aftermath might include harmful chemical\n3Arguably, such rumours survive because they are a vehicle for those inclined to believe in conspiracy theories, where by definition, nothing is as it seems.\nsubstances and led to people being warned to carry an umbrella. The authors looked at retweets of early tweets reporting the rumour, as well as later tweets reporting that it was false. While their study showed that the appearance of later correction tweets diminished the diffusion of tweets reporting the false rumour, the analysis was limited to a single rumour and does not provide sufficient insight into understanding the nature of rumours in social media. Their case study, however, does show an example of a rumour with important consequences for society, as citizens were following the latest updates with respect to the earthquake in order to stay safe. Rumours in social media. Social media as a source for researching rumours has gained ground in recent years, both because it is an interesting source for gathering large datasets associated with rumours and also because it is a type of platform that gives rise to even more rumours from its many participants. Researchers have used social media, among others, to study how users orient to rumours. It has been generally suggested that Twitter does well in debunking inaccurate information thanks to self-correcting properties of crowdsourcing as users share opinions, conjectures, and evidence. For example, Castillo et al. [Castillo et al. 2013] found that the ratio between tweets supporting and debunking false rumours was 1:1 (one supporting tweet per debunking tweet) in the case of a 2010 earthquake in Chile. Procter et al. [Procter et al. 2013b] came to similar conclusions in their analysis of false rumours during the 2011 riots in England, but they noted that any self-correction can be slow to take effect. In contrast, in their study of the 2013 Boston Marathon bombings, Starbird et al. [Starbird et al. 2014] found that Twitter users did not do so well in distinguishing between the truth and hoaxes. Examining three different rumours, they found the equivalent ratio to be 44:1, 18:1 and 5:1 in favour of tweets supporting false rumours. Delving further into temporal aspects of rumour diffusion and support, [Zubiaga et al. 2016] describe the analysis of rumours circulating during nine breaking news events. This study concludes that, while the overall tendency is for users to support unverified rumours in the early stages, there is a shift towards supporting true rumours and debunking false rumours as time goes on. The ability of social media to aggregate the judgements of a large community of users [Li and Sakamoto 2015] thus motivates further study of machine learning approaches to improve rumour classification systems. Despite the challenges that the spread of rumours and misinformation pose for the development of data mining tools, breaking down the development process into smaller components and making use of suitable techniques is showing encouraging progress towards developing effective systems that can assist people in making decisions towards assessing the veracity of information gathered from social media."}, {"heading": "1.3. Scope and Organisation", "text": "This survey is motivated by the increasing use of social media platforms such as Facebook or Twitter to post and discover information. While we acknowledge their unquestionable usefulness for gathering often exclusive information, their openness, lack of moderation, and the ease with which information can be posted from anywhere and at anytime undoubtedly leads to major problems for information quality assurance. Given the unease that the spread of rumours can produce and the potential for harm, the incentive for the development of data mining tools for dealing with rumours has increased in recent years. This survey aims to delve into these challenges posed by rumours to the development of data mining applications for gathering information from social media, as well as to summarise the efforts so far in this direction. We continue this survey in Section 2 by examining the opportunities social media brings to numerous domains, while also introducing the new challenge of having to deal with rumours. Moving on to the analysis of rumour classification systems, we first describe different approaches to putting together a dataset of rumours that enables\nfurther experimentation; the generation of datasets is described in Section 3, beginning with ways for accessing social media APIs, to outlining approaches for collecting and annotating data collected from social media. We summarise findings from studies looking at characterisation and understanding of diffusion and dynamics of rumours in social media in Section 4. After that, we describe the components that form a rumour classification system in Section 5. These components are then further described and existing approaches discussed in subsequent sections; rumour detection systems in Section 6, rumour tracking systems in Section 7, rumour stance classification in Section 8 and veracity classification in Section 9. We continue in Section 10 listing and describing existing applications that deal with the classification of rumours and related applications. To conclude, we summarise the achievements to date and outline future research directions in Section 11."}, {"heading": "2. SOCIAL MEDIA AS AN INFORMATION SOURCE: CHALLENGES POSED BY RUMOURS", "text": "Social media is being increasingly leveraged by both a range of professionals as well as end users as an information source to learn about the latest developments and current affairs [Van Dijck 2013; Fuchs 2013]. The use of social media has been found useful in numerous different domains; we describe some of the most notable uses below: News gathering. Social media platforms have shown great potential for news diffusion, occasionally even outpacing professional news outlets in breaking news reporting [Kwak et al. 2010]. This enables, among others, access to updates from eyewitnesses and a broad range of users who have access to potentially exclusive information [Diakopoulos et al. 2012; Starbird et al. 2012]. Aiming to exploit this feature of social media platforms, researchers have looked into the development of tools for news gathering [Zubiaga et al. 2013; Diakopoulos et al. 2012; Marcus et al. 2011], analysed the use of user-generated content (UGC) for news reporting [Hermida and Thurman 2008; Tolmie et al. 2017a], and explored the potential of social media to give rise to collaborative and citizen journalism, including collaborative verification of reports posted in social media [Hermida 2012; Spangenberg and Heise 2014]. Emergencies and crises. The use of social media during emergencies and crises has also increased substantially in recent years [Imran et al. 2015; Castillo 2016; Procter et al. 2013a], with applications such as getting reports from eyewitnesses or finding help-seekers. Social media has been found useful for information gathering and coordination in different situations, including emergencies [Yates and Paquette 2011; Yin et al. 2012; Procter et al. 2013a], protests [Trottier and Fuchs 2014; Agarwal et al. 2014] and natural hazards [Vieweg et al. 2010; Middleton et al. 2014]. Public opinion. Social media is also being used by researchers to collect perceptions of users on a range of social issues, which can then be aggregated to measure public opinion [Murphy et al. 2014]. Researchers attempt to clean social media data [Gao et al. 2014] and try to get rid of population biases [Olteanu et al. 2016] to understand how social media shapes society\u2019s perceptions on issues, products, people, etc. [Goodman et al. 2011]. Social media have been found useful to measure public opinion during elections [Anstead and O\u2019Loughlin 2015], and the effect of online opinions on the offline world is being analysed, for instance, towards the reputation of organisations [Sung and Lee 2015] or towards different policies [Shi et al. 2014]. Financial/stock markets. Social media has also become an important information source to stay abreast of the latest development in the financial world and in stock markets. For instance, sentiment expressed in tweets has been used to predict stock market reactions [Azar and Lo 2016], to collect opinions that investors post in social media [Chen et al. 2014] or to analyse the effect that social media posts can have on brands and products [Lee et al. 2015].\nDespite the increasing potential of social media as an information source, its propensity to the spread of misinformation and unsubstantiated claims has given rise to numerous studies on social media. Studies have looked at credibility perceptions of users [Westerman et al. 2014] and have also assessed the degree to which users rely on social media to gather information such as news [Gottfried and Shearer 2016]. The difficulties arising from the presence of rumours and questionable claims in social media has hence led to the study of techniques to build rumour classification systems and to alleviate the problem by facilitating the gathering of accurate information for users. When it comes to the development of rumour classification systems, there are two main use cases to be considered:\n\u2014Dealing with long-standing rumours. Where the rumours being tracked are known a priori and social media is being mined as a source for collecting opinions. This use case may be applicable, for instance, when one wants to track public opinion, or when rumours such as potential buyouts are being discussed for long periods in the financial domain. \u2014Dealing with emerging rumours. Where new rumours emerge suddenly while certain events or topics are being tracked. This use case may apply in the case of news gathering and emergencies, where information is released piecemeal and needs to be verified, or other suddenly emerging rumours, such as those anticipating political decisions that are expected to have an impact on stock markets."}, {"heading": "3. DATA COLLECTION AND ANNOTATION", "text": "This section describes different strategies used to collect social media data that enables researching rumours, as well as approaches for collecting annotations for the data."}, {"heading": "3.1. Access to Social Media APIs", "text": "The best way to access, collect and store data from social media platforms is generally through Application Programming Interfaces (APIs) [Lomborg and Bechmann 2014]. APIs are easy-to-use interfaces that are usually accompanied by documentation that describes how to request the data that one is interested in. They are designed to be accessed by other applications as opposed to web interfaces which are designed for people; APIs provide a set of well-defined methods that an application can invoke to request data. For instance, in a social media platform, one may want to retrieve all data posted by a specific user or all the posts containing a certain keyword. Before using an API, a crucial first step is to read its documentation and to understand its methods and limitations. Indeed, every social media platform has its own limitations and this is key when one wants to develop a rumour classification system that utilises social media data. Three of the key platforms used for the study of rumours are Twitter, Sina Weibo and Facebook; here we briefly discuss the features and limitations of these three platforms:\n\u2014Twitter provides detailed documentation4 of ways to use its API, which gives access to a REST API to harvest data from its database as well as a streaming API to harvest data in real-time. After registering a Twitter application5 that will generate a set of keys for accessing the API through OAuth authentication, the developer will then have access to a range of methods (\u2019endpoints\u2019) to collect Twitter data. The most generous of these endpoints gives access to a randomly sampled 1% of the whole tweet stream; getting access to a larger percentage usually requires payment of a fee. To make sure that one gathers a comprehensive collection of tweets, it is\n4https://dev.twitter.com/docs 5https://apps.twitter.com/\nadvisable to collect tweets in real-time through the streaming API; again, there is a limit of 1% on the number of tweets that can be collected for free from this API. The main advantage of using Twitter\u2019s API is that it is the most open and this may partly explain why it is the most widely used for research; the main caveat is that it is mainly designed to collect either real-time or recent data, and so it is more challenging to collect data that is older than the last few weeks. \u2014Sina Weibo, the most popular microblogging platform in China, provides an API6\nthat has many similarities to that of Twitter. However, access to some of its methods is not openly available. For example the search API requires contacting the administrator to get approval first. Moreover, the range of methods provided by Sina Weibo are only accessible through its REST API and it lacks an official streaming API to retrieve real-time data. To retrieve real-time data from Sina Weibo through its streaming API, one needs to make use of third party providers such as Socialgist78. \u2014Facebook provides a documented API9 with a set of software development kits (SDKs) for multiple programming languages and platforms that make it easy to develop applications with its data. Similar to Twitter API, Facebook also requires registering an application10 to generate the keys needed to access the API. In contrast to Twitter, most of the content posted by Facebook users is private and therefore there is no access to specific content posted, unless the users are friends with the authenticated account. The workaround to get access to posts on Facebook is usually to collect data from so-called Facebook Pages, which are open pages created by organisations, governments, groups or associations. Unlike with Twitter, one can then get access to historical data from those Facebook Pages, however, one is limited to content that has been posted in those pages.\nIn recent years Twitter has become the data source par excellence for collection and analysis of rumours, thanks to the openness of its API, as well as its prominence as a source of early reports during breaking news. Most of the research surveyed in this study, as well as the applications described in Section 10, make use of Twitter for this reason."}, {"heading": "3.2. Rumour Data Collection Strategies", "text": "Collection of social media data that is relevant for the development of rumour classifiers is not straightforward a priori and one needs to define a careful data collection strategy to come up with good datasets. For other applications in social media mining it might just suffice to define filters that are already implemented in the APIs of social media platforms, such as: (1) filtering by keyword to collect data related to an event [Driscoll and Walker 2014]; (2) defining a bounding box to collect data posted from predefined geographical locations [Frias-Martinez et al. 2012]; or (3) listing a set of users of interest to track their posts [Li and Cardie 2014]. Collection of rumours requires combining one of those implemented approaches with expertise to retrieve data that is applicable to the rumour classification scenario. We classify the different data collection strategies employed in the literature in two different levels. On the one hand, researchers have used different strategies to collect long-standing rumours or newly emerging rumours and, on the other hand, re-\n6http://open.weibo.com/wiki/API\\%E6\\%96\\%87\\%E6\\%A1\\%A3/en 7http://www.socialgist.com/ 8http://www.socialgist.com/press/socialgist-emerges-as-the-first-official-provider-of-social-data-fromchinese-microblogging-platform-sina-weibo/ 9https://developers.facebook.com/docs/ 10https://developers.facebook.com/docs/apps/register\nsearchers have relied on different top-down and bottom-up strategies for sampling rumour-related data from their collections. Collection of long-standing rumours vs collection of emerging rumours. The methodology for collecting rumour data from social media can have remarkable differences depending on whether the aim is to collect long-standing or newly emerging rumours.\n\u2014Collection of long-standing rumours is performed for a rumour or rumours that are known in advance. For instance, posts can be collected for the rumour discussing whether Obama is muslim or not by using keywords like Obama and muslim to filter the posts [Qazvinian et al. 2011]. Since such rumours have, by definition, been running for a while, there is no need to have a system that detects those rumours and the list of rumours is manually input. This type of collection is useful when one wants to track opinion shifts over a long period of time and the ease with which one can define keywords to collect posts enables collection of large-scale datasets. One has to be careful when defining the keywords, so that as many relevant posts as possible are collected. \u2014Collection of emerging rumours tends to be more challenging. Given that data collection is usually done from a stream of posts in real-time, one needs to make sure that tweets associated with a rumour will be collected before it occurs. Since the keywords are not known beforehand, alternative solutions are generally used for performing a broader data collection to then sample the subset of interest. In closed scenarios where one wants to make sure to collect rumours that emerge during an event or news story, one can simply collect as many posts as possible for those events. Once the posts for an event are collected, one can then filter the tweets that are associated with rumours [Procter et al. 2013b; Zubiaga et al. 2015]; this can be done in two different ways by following top-down or bottom-up strategies, as we explain below. Alternatively, one may want to collect emerging rumours in an open scenario that is not restricted to events or news stories, but in a broader context. A solution for this is to use alternative API endpoints to collect posts through a less restrictive stream of data, such as Twitter\u2019s streaming API sampling a random 1% of the whole, or a filter of posts by geolocation, where available, to collect posts coming from a country or region [Han et al. 2014]. A caveat to be taken into account is that, since the data collection has not been specifically set up for a rumour but for a wider collection, the sampled subset associated with rumours may not lead to comprehensive representations of the rumours as keywords different to those initially predefined can be used. The identification of changes in vocabulary during an event for improved data collection is, however, an open research issue [Earle et al. 2012; Wang et al. 2015].\nTop-down vs bottom-up data sampling strategies. Where one collects a broad collection, as can be the case when attempting to discover newly emerging rumours, for instance, collecting posts related to an event or by following an unfiltered stream of posts, it is then often necessary to sample the data to extract the posts associated with rumours. This sampling can be performed by using either a top-down or a bottom-up strategy:\n\u2014Top-down sampling strategies prevailed in early work on social media rumours, i.e., sampling posts related to rumours identified in advance. This can apply to longstanding rumours, where one can define keywords to sample posts related a rumour known to have been circulating for a long time [Qazvinian et al. 2011], for retrospective sampling of rumours known to have emerged during an event [Procter et al. 2013b], or using rumour repositories like Snopes.com to collect posts associ-\nated with those rumours [Hannak et al. 2014]. The main caveat of this approach is that sampling of data is limited to the rumours listed and other rumours may be missed. \u2014Bottom-up sampling strategies have emerged more recently in studies that aimed at collecting a wider range of rumours, i.e., sifting through data to identify rumours, rather than rumours that are already known. Instead of listing a set of known rumours and filtering tweets related to those, the bottom-up collection consists in displaying a timeline of tweets so that an annotator can go through those tweets, identifying the ones that are deemed rumourous. This is an approach that was used first by [Zubiaga et al. 2016] and subsequently by [Giasemidis et al. 2016]. The benefit of this approach is that it leads to a wider range of rumours than the top-down strategy, as it is more likely to find new rumours that would not have been found otherwise. The main caveat of this approach is that generally leads to a few tweets associated with each rumour, rather than a comprehensive collection of tweets linked to each rumour as with the top-down strategy."}, {"heading": "3.3. Annotation of Rumour Data", "text": "The annotation of rumour data can be carried out at different levels, depending on the task and the purpose. Here we present previous efforts on rumour annotation for different purposes. The first step is to identify the rumourous subset within the collected data. This is sometimes straightforward as only rumourous data is collected using topdown sampling strategies, and hence no further annotation is needed to identify what is a rumour and what is not. However, if one wants to use a bottom-up sampling strategy, then manual annotation work is needed to identify what constitutes a rumour and what a non-rumour [Zubiaga et al. 2015]. Manual distinction of what is a rumour may not always be straightforward, as it is largely dependent on the context and on one\u2019s judgement as to whether the underlying information was verified or not at the moment of posting. However, well established definitions of rumour exist to help in this regard [DiFonzo and Bordia 2007] and people with a professional interest in veracity, such as journalists, have put together detailed guides to help determine what is a rumour [Silverman 2015a] and when further verification is needed [Silverman 2013]. Annotation work distinguishing rumours and non-rumours is described in [Zubiaga et al. 2016b] as a task to determine when a piece of information has not sufficient evidence to be verified or lacks confirmation from an authoritative source. Once rumours and non-rumours have been manually classified, further annotation is usually useful to do additional classification and resolution work on the rumours. It is usually the case that no additional annotations are collected for non-rumours, as the rumours are the ones that need to be further dealt with; different annotations that have been done on rumours including the following:\n\u2014Rumour veracity:Manually determining the veracity of a rumour is a challenging task, usually requiring an annotator with expertise who performs careful analysis of claims and additional evidence, context and reports from authoritative sources before making a decision. This annotation process has been sometimes operationalised by enlisting the help of journalists with expertise in verification [Zubiaga et al. 2016]. In this example, journalists analysed rumourous claims spreading on social media during breaking news to determine, where possible, if a rumour had later been confirmed as true or debunked and proven false; this is, however, not always possible and some rumours weremarked as unverified as no reliable resolution could be found. While this approach requires expertise that can be hard to resource, others have used online sources to determine the veracity of rumours. For instance, [Hannak et al. 2014] used Snopes.com as a database that provides ground truth an-\nnotations of veracity for rumours put together by experts. While some online sources like Truth-O-Meter and PolitiFact provide finer-grained labels for veracity, such as mostly true, half true and mostly false, these are usually reduced to three labels [Popat et al. 2016], namely true, false and optionally unverified. While annotation is increasingly being performed through crowdsourcing platforms for many natural language processing and data mining tasks [Doan et al. 2011; Wang et al. 2013], it is not as suitable for more challenging annotation tasks such as rumour veracity. Crowdsourcing annotations for veracity will lead to collection of credibility perceptions rather than ground truth veracity values, given that verification will often require an exhaustive work of checking additional sources for validating the accuracy of information, which may be beyond the expertise of average crowd workers. This is what [Zubiaga and Ji 2014] found in their study, suggesting that verification work performed by crowd workers tends to favour selection of true labels for inaccurate information. \u2014Stance towards rumours: Annotation of stance involves determining how a social media post is oriented towards a target, the target being a rumour in this case. This has been operationalised by [Qazvinian et al. 2011] annotating tweets as supporting, denying or querying a rumour, while later work by [Procter et al. 2013b] suggested the inclusion of an additional label, commenting to make it a scheme with four labels. \u2014Rumour relevance:Annotation of relevance involves determining if a social media post is related to a rumour or not. This is performed as binary classification marking a post as relevant or not. [Qazvinian et al. 2011] annotated rumours by relevance, where for instance for a rumour saying that Obama is muslim, a post that says Obama does seem to be muslim would be marked as relevant, while a post saying that Obama had a meeting with muslims would be marked as not relevant. \u2014Other factors: Some work has performed annotation of additional factors that can be of help in the assessment of the veracity of rumours. For example, [Zubiaga et al. 2016] annotated rumourous tweets for certainty (certain, somewhat certain, uncertain) and evidentiality (first-hand experience, inclusion of URL, quotation of person/organisation, link to an image, quotation of unverifiable source, employment of reasoning, no evidence), along with support. [Lendvai et al. 2016a] annotated relations between claims associated with rumours, intended for automated identification of entailment and contradiction between claims. Annotation of credibility perceptions has also been done in previous work, determining how credible claims appear to people [Mitra and Gilbert 2015; Zubiaga and Ji 2014]; this could be useful in the context of rumours to identify those that are likely to be misleading for people, however, it has not yet been applied in the context of rumours."}, {"heading": "4. CHARACTERISING RUMOURS: UNDERSTANDING RUMOUR DIFFUSION AND FEATURES", "text": "Numerous recent studies have looked at characterising the emergence and spread of rumours in social media. Insights from these studies can in turn be useful to inform the development of rumour classification systems. Some of this research has focused on extensive analyses of a specific rumour, whereas others have looked into larger sets of rumours to perform broader analyses. Studies of discourse surrounding rumours have been conducted to examine discussions around \u2013 and the evolution of \u2013 rumours over time. Some studies have looked at defining a scheme to categorise types of reactions expressed towards rumours. [Maddock et al. 2015] looked at the origins and changes of rumours over time, which led to the identification of seven behavioural reactions to rumours: misinformation, speculation, correction, question, hedge, unrelated, or neutral/other. Similarly, [Procter et al. 2013b] suggested that reactions to rumours could be categorised into four types,\nnamely support, denial, appeal for more information and comment. Others have looked into rumours to understand how people react to them. By looking at rumours spreading in the Chinese microblogging platform Sina Weibo, [Liao and Shi 2013] identified interventions of seven types of users (celebrity, certified, mass media, organisation, website, internet star and ordinary), who contributed in seven different ways (providing information, giving opinions, emotional statements, sense-making statements, interrogatory statements, directive statements and digressive statements). In another study looking at conversations sparked by rumourous reports on Twitter, [Zubiaga et al. 2016] found that the prevalent tendency of social media users is to support and spread rumours, irrespective of their veracity value. This includes users of high reputation, such as news organisations, who tend to favour rumour support in the early stages of rumours, issuing a correction statement later where needed. In an earlier study, [Mendoza et al. 2010] had found strong correlations between rumour support and veracity, showing that a majority of users support true rumours, while a higher number of users denies false rumours. Despite the apparent contradiction between these studies, it is worth noting that [Mendoza et al. 2010] looked at the entire life cycle of a rumour and hence the aggregation leads to good correlations; in contrast, [Zubiaga et al. 2016] focused on the early reactions to rumours, showing that users have problems in determining veracity in the early stages of a rumour. Using rumour data from Reddit, differences across users have also been identified, suggesting three different user groups: those who generally support a false rumour; those who generally refute a false rumour; and those who generally joke about false rumours [Dang et al. 2016a]. It has also been suggested that corrections are usually issued by news organisations and they can be sometimes widely spread [Takayasu et al. 2015; Arif et al. 2016; Andrews et al. 2016], especially if those corrections come from like-minded accounts [Hannak et al. 2014] and occasionally even leading to deletion or unsharing of the original post [Frias-Martinez et al. 2012]. However, corrections do not always have the same effect as the original rumours [Lewandowsky et al. 2012; Shin et al. 2016; Starbird et al. 2014], which reinforces the need to develop rumour classification systems that deal with newly emerging rumours. Other studies have looked at factors motivating the diffusion of rumours. Rumour diffusion is often dependent on the strength of ties between users, where rumours are more likely to be spread across strong ties in a network [Cheng et al. 2013]. Other studies looking at temporal patterns of rumours have suggested that their popularity tends to fluctuate over time in social media [Kwon et al. 2013; Kwon and Cha 2014; Lukasik et al. 2015b] and other platforms on the Internet [Jo 2002], but with a possibility of being discussed again later in time after rumour popularity fades. Studies have also looked at the emergence of rumours. By using rumour theoretic approaches to examine factors that lead to expression of interest in tracking a rumour, [Oh et al. 2013] identified the lack of an official source and personal involvement as the most important factors, whereas other factors, such as anxiety, were not as important. The poster\u2019s credibility and attractiveness of the rumour are also believed to be factors contributing to the propagation of rumours [Petty and Cacioppo 2012]. [Liu et al. 2014] reinforced these findings suggesting that personal involvement was the most important factor. Analysing specific rumour messages on Twitter, [Chua et al. 2016] identified that tweets from established users with a larger follower network were spread the most. While many studies have explored the diffusion of rumours, an exhaustive analysis of these studies is not within the scope of this survey, which focuses instead on research concerning development of approaches to detect and resolve rumours. To read more about studies looking at the diffusion of rumours, we recommend the surveys by [Serrano et al. 2015] and [Walia and Bhatia 2016]."}, {"heading": "5. RUMOUR CLASSIFICATION: SYSTEM ARCHITECTURE", "text": "The architecture of a rumour classification system can have slight variations depending on the specific use case. Here we define a typical architecture for a rumour classification system, which includes all the components needed for a complete system; however, as we point out in the descriptions below, depending on requirements, some of these components can be omitted. A rumour classification system usually begins with identifying that a piece of information is not confirmed (i.e., rumour detection) and ends by determining the estimated veracity value of that piece of information (i.e., veracity classification). The entire process from rumour detection to veracity classification is performed through the following four components (see Figure 1):\n(1) Rumour detection: In the first instance, a rumour classification system has to identify whether a piece of information constitutes a rumour. A typical input to a rumour detection component can be a stream of social media posts, whereupon a binary classifier has to determine if each post is deemed a rumour or a non-rumour. The output of this component is the stream of posts, where each post is labelled as rumour or non-rumour. This component is useful for identifying emerging rumours, however, it is not necessary when one needs to deal with rumours that are known a priori. (2) Rumour tracking:Once a rumour is identified, either because it is known a priori or because it is identified by the rumour detection component, the rumour tracking component collects and filters posts discussing the rumour. Having a rumour as input, which can be a post or a sentence describing it, or a set of keywords, this component monitors social media to find posts discussing the rumour, while eliminating irrelevant posts. The output of this component is a collection of posts discussing the rumour. (3) Stance classification: While the rumour tracking component retrieves posts related to a rumour, the stance classification component determines how each post is orienting to the rumour\u2019s veracity. Having a set of posts associated with the same rumour as input, it outputs a label for each of those posts, where the labels are chosen from a generally predefined set of types of stances. This component can be useful to facilitate the task of the subsequent component dealing with veracity classification. However, it can be omitted where the stance of the public is not considered useful, e.g., cases solely relying on input from experts or validation from authoritative sources. (4) Veracity classification: The final, veracity classification component attempts to determine the actual truth value of the rumour. It can use as input the set of posts collected in the rumour tracking component, as well as the stance labels produced in the stance classification component. It can optionally try to collect additional data from other sources such as news media, or other websites and databases.\nThe output of the component can be just the predicted truth value, but it can also include context such as URLs or other data sources that help the end user assess the reliability of the classifier by double checking with relevant sources.\nIn the following sections, we explore these four components in more detail, the approaches that have been used so far to implement them and the achievements to date."}, {"heading": "6. RUMOUR DETECTION", "text": ""}, {"heading": "6.1. Definition of the Task and Evaluation", "text": "The rumour detection task is that in which a system has to determine, from a set of social media posts, which ones are reporting rumours, and hence are spreading information that is yet to be verified. Note that the fact that a tweet constitutes a rumour does not imply that it will later be deemed true or false, but instead that it is unverified at the time of posting. Formally, the task takes a timeline of social media posts TL = {t1, ..., t|TL|} as input, and the classifier has to determine whether each of these posts, ti, is a rumour or a non-rumour by assigning a label from Y = {R,NR}. Hence, the task is usually formulated as a binary classification problem, whose performance is evaluated by computing the precision, recall and F1 scores for the target category, i.e., rumours."}, {"heading": "6.2. Datasets", "text": "The only publicly available dataset is the PHEME dataset of rumours and nonrumours11, which includes a collection of 1,972 rumours and 3,830 non-rumours associated with five breaking news stories [Zubiaga et al. 2016b]."}, {"heading": "6.3. Approaches to Rumour Detection", "text": "Despite the increasing interest in analysing rumours in social media and building tools to deal with rumours that had been previously identified [Seo et al. 2012; Takahashi and Igata 2012], there has been very little work in automatic rumour detection. Some of the work in rumour detection [Qazvinian et al. 2011; Hamidian and Diab 2015; 2016] has been limited to finding rumours known a priori. They feed a classifier with a set of predefined rumours (e.g. Obama is muslim), which classifies new tweets as being related to one of the known rumours or not (e.g. I think Obama is not muslim would be about the rumour, while Obama was talking to a group of muslimswouldn\u2019t). An approach like this can be useful for long-standing rumours, where one wants to identify relevant tweets to track the rumours that have already been identified; in this survey we refer to this task as rumour tracking, as one is aware of the rumours that are being monitored, however, the stream of posts needs to be filtered. Solely relying on a rumour tracker would not suffice for fast-paced contexts such as breaking news, where new, unseen rumours emerge and one does not know a priori the specific keywords linked to a rumour, which is yet to be detected. To deal with this, a classifier will need to learn generalisable patterns that will help identify rumours during emerging events. The first work that tackled the detection of new rumours is that by [Zhao et al. 2015]. Their approach builds on the assumption that rumours will provoke tweets from skeptic users who question or enquire about their veracity; the fact that a piece of information has a number of enquiring tweets associated would then imply that the information is rumourous. The authors created a manually curated list of five regular expressions (e.g., \u201cis (that | this | it) true\u201d) that are used to identify enquiring tweets. These enquiring tweets are then clustered by similarity, each cluster being ultimately\n11https://figshare.com/articles/PHEME dataset of rumours and non-rumours/4010619\ndeemed a candidate rumour. It was not viable for them to evaluate by recall, but their best approach achieved 52% and 28% precision for two datasets. In contrast, [Zubiaga et al. 2016b] suggested an alternative approach that learns context throughout a breaking news story to determine if a tweet constitutes a rumour. They build on the hypothesis that a tweet alone may not suffice to know if its underlying story is a rumour, due to the lack of context. Moreover, they avoid the reliance on enquiring tweets, which they argue that not all rumours may trigger and hence may lead to low recall, as rumours not provoking enquiring tweets would be missed. Their context-learning approach relied on Conditional Random Fields (CRF) as a sequential classifier that learns the reporting dynamics during an event, so that the classifier can determine, for each new tweet, whether it is or not a rumour based on what has been seen so far during the event. Their approach led to improved performance over the baseline classifier by [Zhao et al. 2015], improving also a number of non-sequential classifiers compared as baselines, with a performance of 0.607 in terms of F1 score. The classifier achieved 0.667 in precision and 0.556 in recall, compared to 0.410 and 0.065 respectively for the classifier by [Zhao et al. 2015]. Work by [Tolosi et al. 2016] using feature analysis on rumours across different events found it difficult to distinguish rumours and non-rumours as features change dramatically across events. These findings at the tweet level were then resolved by [Zubiaga et al. 2016b] showing that generalisability can be achieved by leveraging context of the events. [McCreadie et al. 2015] studied the feasibility of using a crowdsourcing platform to identify rumours and non-rumours in social media, finding that the annotators achieve high inter-annotator agreement. They also categorised rumours into six different types: Unsubstantiated information, disputed information, misinformation/disinformation, reporting, linked dispute and opinionated. However, their work was limited to crowdsourced annotation of rumours and non-rumours and they did not study the development of an automated rumour detection system. The dataset from this research is not publicly available. Yet other work has been labelled as rumour detection, focusing on determining if information posted in social media was true or false, rather than on early detection of unverified information and hence we discuss this in Section 9 on veracity classification."}, {"heading": "7. RUMOUR TRACKING", "text": ""}, {"heading": "7.1. Definition of the Task and Evaluation", "text": "The rumour tracking component is triggered once a rumour is detected and consists in identifying subsequent posts associated with the rumour being monitored. The input is usually a stream of posts, which can be tailored to the rumour in question after filtering for relevant keywords, or it can be broader by including posts related to a bigger event or even an unrestricted stream of posts. The task is generally framed as a binary classification task that consists in determining whether each of the posts is related to the rumour or not. The output will be a labelled version of the stream of posts, where labels define if each post is related or unrelated. Traditional evaluation methods for binary classification are used for this task, namely precision, recall and F1 score, where the positive class is the set of related posts."}, {"heading": "7.2. Datasets", "text": "The most widely used dataset for rumour tracking is that by [Qazvinian et al. 2011], which includes over 10,000 tweets, associated with 5 different rumours, each tweet annotated for relevance towards the rumour as related or unrelated. Unrelated tweets\nhave similar characteristics to those related, such as overlapping keywords, and therefore the classification is more challenging. While not specifically intended for rumour tracking, the dataset produced by [Zubiaga et al. 2016] provides over 4,500 tweets categorised by rumour. This dataset is different as it does not include tweets with similar characteristics that are actually unrelated. Instead, it provides tweets that are associated with different rumours and tweets that have been grouped by rumour."}, {"heading": "7.3. Approaches to Rumour Tracking", "text": "Research in rumour tracking is scarce in the scientific literature. Despite early work by [Qazvinian et al. 2011] performing automated rumour tracking, few studies have subsequently followed their line of research when it comes to determining the relevance of tweets to rumours. [Qazvinian et al. 2011] use a manually generated Twitter data set containing 10K tweets to guide a supervised machine learning approach. The authors use different features categorised as \u201ccontent\u201d, \u201cnetwork\u201d and \u201cTwitter specific memes\u201d. The content category contains unigrams, bigrams and their part-of-speech (POS) tags as features. In the network category the authors look at retweets (RT) as a feature. Finally, the Twitter specific memes include content features inferred from hashtags and URLs. A Bayesian classifier is used as the machine learning approach. The best performance was achieved by using content-based features, with a mean average precision of 96.5%. Later work by [Hamidian and Diab 2015] also focused on a rumour tracker, using the dataset produced by [Qazvinian et al. 2011]. They used an approach called Tweet Latent Vector (TLV), which creates a latent vector representative of a tweet to overcome the limited length and context of a tweet. Their approach relies on the Semantic Textual Similarity (STS) model proposed by [Guo and Diab 2012], which exploits WordNet [Miller 1995], Wiktionary12 and Brown clusters [Brown et al. 1992] to enhance the shortage of semantic meaning of a tweet. This approach led to a precision score of 97.2%, outperforming the baseline score established earlier by [Qazvinian et al. 2011]. Rumour tracking has not been studied for emerging rumours. The most relevant work to that of tracking newly emerging rumours is that conducted for event detection and tracking in social media [Jaidka et al. 2016]. For instance, [Sayyadi et al. 2009] describe an event detection and tracking approach based on keyword graphs. They build a graph of keywords to detect communities and subsequently newly emerging events. They then use the set of keywords associated with an event to track new incoming tweets. Similar approaches to event tracking have been introduced by others, such as using a bipartite graph for topical word selection [Long et al. 2011], using text classification techniques to determine whether incoming data is related to a previously identified event or to a new one [Reuter and Cimiano 2012], and using similarity metrics [Tzelepis et al. 2016]. However, these approaches have not been directly applied to rumours and hence their applicability needs to be further studied with a suitable rumour dataset."}, {"heading": "8. RUMOUR STANCE CLASSIFICATION", "text": ""}, {"heading": "8.1. Definition of the Task and Evaluation", "text": "The rumour stance classification task consists in determining the type of orientation that each individual post expresses towards the disputed veracity of a rumour. The task is especially interesting in the context of social media, where unverified reports are continually being posted and discussed, both on breaking news stories as\n12https://www.wiktionary.org/\nthey unfold as well as on long-standing rumours. A rumour stance classifier usually takes a set of rumours D = {R1, ..., Rn}, where each rumour is composed of a collection of posts discussing it. Each rumour has a variably sized set of posts ti discussing it so that Ri = {t1, ..., t|Ri|}; the task consists in determining the stance of each of the posts tj pertaining to a rumour Ri. The classification scheme to determine the stance of each post varies across different studies; while early work [Qazvinian et al. 2011] performed 2-way classification of Y = {supporting, denying}, later work performed 3-way classification [Lukasik et al. 2015a] involving Y = {supporting, denying, querying} as well as 4-way classification [Zubiaga et al. 2016a] into Y = {supporting, denying, querying, commenting}. The evaluation of the rumour stance classifier is usually based on micro-averaged precision, recall and F1 scores, as well as accuracy scores. However, as research has progressed into a 4-way classification, which generally shows a skewed distribution of labels, evaluation is now also focusing onmacro-averaged scores for a fairer evaluation, rewarding the classifiers that perform well across the different labels. More details on the rumour stance classification task can be found in the report of the RumourEval shared task [Derczynski et al. 2017]."}, {"heading": "8.2. Datasets", "text": "Work on stance classification has made use of different datasets, although the only dataset that is publicly available is the PHEME stance dataset13, which provides tweet-level annotations of stance (support, deny, query, comment) for tweets associated with nine events. Other datasets used in previous work include a dataset with over 10,000 tweets annotated for stance as support, deny or query by [Qazvinian et al. 2011] and the dataset annotated as affirm, deny, neutral, uncodable or unrelated by [Andrews et al. 2016]; however, the latter two are not publicly available. A dataset released for the Fake News Challenge14 is also annotated for stance (agrees, disagrees, discusses, unrelated). This dataset is however made of news articles instead of social media posts."}, {"heading": "8.3. Approaches to Rumour Stance Classification", "text": "Stance classification is well studied in online debates where the aim is to classify the user entries as \u201cfor\u201d or \u201cagainst\u201d. Studies in this respect define stance as an overall position held by a person towards an object, idea or position [Somasundaran and Wiebe 2009; Walker et al. 2012]. Unlike stance classification in online debates, the aim of rumour stance classification is to classify user contributions as, e.g., \u201csupporting\u201d, \u201cdenying\u201d, \u201cquerying\u201d or \u201ccommenting\u201d. However, in the literature the querying and commenting categories are sometimes omitted or replaced by the \u201cneutral\u201d label that encompasses everything that is not supporting or denying. The rumour stance classification task has attracted many studies over the past few years. All studies follow a supervised approach, and mainly differ in the way they represent a post and how this representation is used to generate a predictive model, i.e. in the features and in the machine learning approaches used to learn predictive models. One of the pioneering studies in this task is reported by [Mendoza et al. 2010]. The study involved a human-labelled, non-automated analysis of rumours with established veracity levels to understand the stance that Twitter users express with respect to true and false rumours. The authors looked at fourteen rumours, seven of which turned out to be true and the other seven were proven false. They manually labelled the tweets associated with those rumours with the stance categories \u201caffirms\u201d (supports), \u201cdenies\u201d\n13https://figshare.com/articles/PHEME rumour scheme dataset journalism use case/2068650 14http://www.fakenewschallenge.org/\nand \u201cquestions\u201d. They found that over 95% of tweets associated with true rumours were \u201caffirms\u201d, whereas only 4% were \u201cquestions\u201d and only 0.4% were \u201cdenies\u201d. This suggested that true rumours are largely supported by other Twitter users. On the other hand, 38% of the tweets associated with false rumours were identified as \u201cdenies\u201d and 17% as \u201cquestions\u201d. While false rumours are not denied as often as true rumours are supported, both of these figures suggest that there is indeed a difference in the stances expressed by users towards true and false rumours and that user stances can be indicative of rumour veracity. This study aims to understand the stance categories by manual classification and human analysis, so it does not propose any solution to perform the stance classification task automatically. The first study that tackles the stance classification automatically is reported by [Qazvinian et al. 2011]. In addition to stance classification the authors also perform automatic rumour tracking, as we reported in Section 7. The supervised approach developed for the rumour tracking task is also adopted for the stance classification task. In the rumour stance classification task the tweets are classified as supporting, denying, questioning or neutral. In terms of results, observations similar to the ones obtained for the rumour tracker are reported. When all features are used an accuracy of 93.5%, a precision of 94.4% and recall of 90.6% are achieved. Similarly to the rumour tracking task, the best performing features are those belonging to the content category. Similar to [Qazvinian et al. 2011], the work by [Hamidian and Diab 2015] reports rumour tracking and rumour stance classification by applying supervisedmachine learning using the dataset created by [Qazvinian et al. 2011]. However, instead of Bayesian classifiers the authors use the J48 decision tree implemented within the Weka platform [Hall et al. 2009]. The features from [Qazvinian et al. 2011] are adopted and extended with time related information and the hashtag itself as a token instead of the semantic content of the hashtag as used by [Qazvinian et al. 2011]. In addition to these features, Hamidian and Diab introduce another feature category: \u201cpragmatics\u201d. The pragmatic features include named entities, events, sentiment and emoticons. The evaluation of the performance is cast either as a 1-step problem containing a 6- way classification task (unrelated to rumour, 4 classes of stance and not determined) or as a 2-step problem containing first a 3-way classification task (related to rumour, unrelated to rumour, not determined) and then 4 class classification task (stance classification). The highest performance scores are achieved using the 2-step approach leading to 82.9% F-1 measure compared to 74% with the 1-step approach. The authors also report that the best performing features were the content based features and the least performing ones the network and Twitter specific features. In their recent paper, [Hamidian and Diab 2016] introduce the Tweet Latent Vector (TLV) approach that is obtained by applying the Semantic Textual Similarity model proposed by [Guo and Diab 2012]. The authors compare the TLV approach to their own earlier system as well as to original features of [Qazvinian et al. 2011] and show that the TLV approach outperforms both baselines. [Liu et al. 2015] follow the resulting investigations of stances in rumours by [Mendoza et al. 2010] and use stance as an additional feature to those reported in related work to tackle the veracity classification problem (see Section 9). For stance classification the authors adopt the approach of [Qazvinian et al. 2011] and compare it with a rule-based method briefly outlined by the authors. They claim that their rule-based approach performed better than the one adopted by previous work and thus use the rule-based stance classification as an additional component in the veracity problem (see Section 9). The experimentswere performed on the dataset reported by [Qazvinian et al. 2011]. Unfortunately, the authors do not provide detailed analysis of the performance of the stance classifier.\nMore recently, [Zeng et al. 2016] enrich the feature sets investigated by earlier studies by features determined through Linguistic Inquiry and Word Count (LIWC) dictionaries [Tausczik and Pennebaker 2010]. They investigate supervised approaches to stance classification using Logistic Regression, Na\u0131\u0308ve Bayes and Random Forest classification. The authors use their own manually annotated data to classify tweets for stance. However, unlike previous studies, Zeng et al. consider only two classes: affirm and deny. The best results are achieved using a Random Forest classifier, which leads to a performance of 87% in precision, 96.9% in recall, 91.7% in F1-measure and 88.4% in accuracy. [Lukasik et al. 2016] investigated Gaussian Processes as rumour stance classifier. For the first time the authors also use Brown Clusters to extract the features for each tweet. The authors work on rumour data released by [Zubiaga et al. 2016] and report an accuracy of 67.7%. This result is achieved when the classifier is trained on n\u2212 1 rumours and tested on the nth rumour. However, the authors achieve substantially better results when a small proportion from the in-domain data (data from the nth rumour) is included in the training leading to almost 68.6% accuracy. Performance scores differ substantially from those in the studies described above, given that Lukasik et al. tackled classification of stance in new rumours that differ from those in the training set. Subsequent work has also tackled stance classification for new, unseen rumours. [Zubiaga et al. 2016a] moved away from the classification of tweets in isolation, focusing instead on Twitter \u2019conversations\u2019 [Tolmie et al. 2017b] initiated by rumours. They looked at tree-structured conversations initiated by a rumour and followed by tweets responding to it by supporting, denying, querying or commenting on it. To mine the conversational nature of the data, they used Conditional Random Fields (CRF) as a sequential classifier in two different settings: Linear-Chain CRFs and Tree CRFs. Their objective with CRFs was to exploit the discursive nature of the argumentation produced collaboratively by users. Their experiments on eight different datasets of rumours spread during breaking news showed that the discursive characteristics of conversations can be indeed exploited with a sequential classifier to improve on the performance that equivalent, non-sequential classifiers can achieve. Rumour stance classification for tree structured conversations has also been studied in the RumourEval shared task at SemEval 2017 [Derczynski et al. 2017]. The subtask A consisted of stance classification of individual tweets discussing a rumour within a conversational thread as one of support, deny, query, or comment. Eight participants submitted results to this task. Most of the systems viewed this task as a 4-way single tweet classification task, with the exception of the best performing system by [Kochkina et al. 2017], as well as the systems by [Wang et al. 2017] and [Singh et al. 2017]. The winning system addressed the task as a sequential classification problem, where the stance of each tweet takes into consideration the features and labels of the previous tweets. The system by [Singh et al. 2017] takes as input pairs of source and reply tweets, whereas [Wang et al. 2017] addressed class imbalance by decomposing the problem into a two step classification task, first distinguishing between comments and non-comments, to then classify non-comment tweets as one of support, deny or query. Half of the systems employed ensemble classifiers, where classification was obtained through majority voting [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Bahuleyan and Vechtomova 2017; Srivastava et al. 2017]. In some cases the ensembles were hybrid, consisting both of machine learning classifiers and manually created rules, with differential weighting of classifiers for different class labels [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Srivastava et al. 2017]. Three systems used deep learning, with [Kochkina et al. 2017] employing LSTMs for sequential classification, [Chen et al. 2017] using convolutional neural networks (CNN) for obtaining the repre-\nsentation of each tweet, assigned a probability for a class by a softmax classifier and [Garc\u0131\u0301a Lozano et al. 2017] using CNN as one of the classifiers in their hybrid conglomeration. The remaining two systems by [Enayet and El-Beltagy 2017] and [Singh et al. 2017] used support vector machines with a linear and polynomical kernel respectively. Half of the systems invested in elaborate feature engineering, including cue words and expressions denoting Belief, Knowledge, Doubt and Denial [Bahuleyan and Vechtomova 2017] as well as Tweet domain features, including meta-data about users, hashtags and event specific keywords [Wang et al. 2017; Bahuleyan and Vechtomova 2017; Singh et al. 2017; Enayet and El-Beltagy 2017]. The systems with the least elaborate features were [Chen et al. 2017] and [Garc\u0131\u0301a Lozano et al. 2017] for CNNs (word embeddings), [Srivastava et al. 2017] (sparse word vectors as input to logistic regression) and [Kochkina et al. 2017] (average word vectors, punctuation, similarity between word vectors in current tweet, source tweet and previous tweet, presence of negation, picture, URL). Five out of the eight systems used pre-trained word embeddings, mostly Google News word2vec embeddings15, whereas [Wang et al. 2017] used four different types of embeddings. Other related studies that have looked into stance classification not directly applicable to rumour stance classification. While [Zhao et al. 2015] did not study stance classification per se, they developed an approach to look for querying tweets, which is one of the reaction types considered in stance classification. However, the other stance types were not considered and querying tweets were found by matching with manually defined regular expressions, which may not be directly applicable to other stance types. While not focused on rumours, classification of stance towards a target on Twitter was addressed in SemEval-2016 task 6 [Mohammad et al. 2016]. Task A had to determine the stance of tweets towards five targets as \u2018favor\u2019, \u2018against\u2019 or \u2018none\u2019. Task B tested stance detection towards an unlabelled target, which required a weakly supervised or unsupervised approach. Researchers have also studied the identification of agreement and disagreement in on-line conversations. To classify agreement between question-answer (Q-A) message pairs in fora, [Abbott et al. 2011] used Naive Bayes as the classifier and [Rosenthal and McKeown 2015] used a logistic regression classifier. A sequential classifier like CRF has also been used to detect agreement and disagreement between speakers in broadcast debates [Wang et al. 2011]. It is also worthwhile emphasising that stance classification is different to agreement/disagreement detection, given that in stance classification one has to determine the orientation of a user towards a rumour. Instead, in agreement/disagreement detection, one has to determine if a pair of posts share the same view. In stance classification, one might agree with another user who is denying a rumour and hence they are denying the rumour as well, irrespective of the pairwise agreement."}, {"heading": "9. RUMOUR VERACITY CLASSIFICATION", "text": ""}, {"heading": "9.1. Definition of the Task and Evaluation", "text": "The veracity classification task aims to determine whether a given rumour can be confirmed as true, debunked as false, or its truth value is still to be resolved. Given a set of posts associated with a rumour and, optionally, additional sources related to the rumour, the task consists in assigning one of the following labels to the rumour, Y \u2208 {true, false, unverified}. Some work has limited the classification to a binary task of determining if a rumour is true or false; however, it is likely that veracity value will remain uncertain for some rumours. Optionally, the classifier can also output, along\n15https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nwith the veracity label, the confidence with which the label has been assigned, usually ranging from 0 to 1. The outcome of the veracity classification task is usually evaluated either using the accuracy measure that computes the ratio of correct classifications, or using a combination of precision, recall and F1 score for the three categories."}, {"heading": "9.2. Datasets", "text": "The dataset produced for RumourEval 2017 [Derczynski et al. 2017], a shared task that took place at SemEval 2017, includes over 300 rumours annotated for veracity as one of true, false or unverified. Another dataset suitable for veracity classification is the one released by [Kwon et al. 2017], which includes 51 true rumours and 60 false rumours. Each rumour includes a stream of tweets associated with it. Other datasets, such as that by [Qazvinian et al. 2011], are not suitable for veracity classification, as all the rumours are false."}, {"heading": "9.3. Approaches to Rumour Veracity Classification", "text": "The vast majority of research dealing with social media rumours has focused on veracity classification, which is the crucial and ultimate goal of determining the truth value of a circulating rumour. This work generally assumes that rumours have already been identified or input by a human. Therefore, most of the previous work skips the preceding components of a rumour classification system, especially the rumour detection component that identifies candidate rumours to be input to the veracity classification system. Work by [Castillo et al. 2011] initiated research in this direction of determining the veracity of social media content. Although the authors did not directly tackle the veracity of rumours but rather their credibility perceptions, i.e., determination of the believability or authority of its source [Zhang et al. 2015]. However, others report that veracity is related to authority [Association et al. 2001; Oh et al. 2013] and hence Castillo et al.\u2019s work has been considered as a reference by many in subsequent work on veracity classification. To study credibility perceptions, [Castillo et al. 2011] distinguish two types of microblog posts: \u2018NEWS\u2019, which report an event or fact that can be of interest to others, and \u2018CHAT\u2019, which is a message that is purely based on personal/subjective opinions and/or conversations among friends. Microblogs categorised as NEWS are analysed for rumour credibility. Decision trees based on J48 are used to train classifiers and these are used to classify microblogs into NEWS and CHAT categories. The microblogs in the NEWS category are further analysed for credibility \u2013 for this task the authors report that they used various other machine learning approaches, such as Bayesian networks and SVM classifiers, but mention that decision trees based on J48 were superior. In the experiments microblogs collected from Twitter are used. These are manually annotated using the Mechanical Turk. The authors use four categories of features: message-based, user-based, topic-based and propagation-based features. The message-based features consider characteristics of messages such as the length of a message, whether the message contains exclamation/question marks, number of positive/negative sentiment words, whether the message contains a hashtag, and whether it is a retweet. User-based features entail information about the user such as registration age, number of followers, number of followees and the number of tweets the user has authored in the past. Topic-based features aggregate information from the previous two feature types, such as the fraction of tweets that contain URLs, the fraction of tweets with hashtags, etc. Finally, the propagation-based features consider characteristics related to the messaging tree, such as depth of the retweet tree or the number of initial tweets on a topic. In the NEWS/CHAT classification task the authors report\n89.2% accuracy, 89.1% precision, 89.1% recall and 89.1% F1-measure. In the credibility classification task, an accuracy of 86% is reported (the same figure is reported for precision/recall and F-1 measure). Despite the fact that this approach was designed for classification of credibility perceptions, the features utilised in this work have subsequently been exploited for veracity classification too. Most research, however, has dealt with veracity classification. [Kwon et al. 2013] propose new set of feature categories: temporal, structural and linguistic. The temporal features aim to capture how rumours spread over time. The structural features model the connectivity between users who posted about the rumour. Finally, the linguistic features are obtained through the Linguistic Inquiry and Word Count (LIWC) dictionaries [Tausczik and Pennebaker 2010]. As a baseline classifier, features proposed by [Castillo et al. 2011] are adopted. Using Random Forest and Logistic Regression, the authors perform feature selection to find the most significant ones. Using these features and three different classifiers (Decision tree, Random Forest and SVM) they then perform rumour veracity classification. The results show that for both selection of significant features and subsequent classification, a Random Forest classifier performed best in terms of accuracy (90%), precision (93.5%), recall (89.2%) and F1measure (89.3%). The best results using the baseline features adopted from [Castillo et al. 2011] are obtained using a SVM with 81.1% accuracy, 89.1% precision, 75.3% recall and 78.8% F1-measure. The authors also show that a combination of significant features identified by Random Forest and baseline features lead to performance degradation. More recently, [Kwon et al. 2017] analyse feature stability over time and report that structural and temporal features distinguish true from false rumours over a long-term window. However, the authors also report that these are not available in early stage of rumour propagation but only later on. In contrast, user and linguistic features are an alternative when the task is to determine rumour veracity as early as possible. [Yang et al. 2012] tackle the veracity of microblogs on the Chinese microblogging platform Sina Weibo. The authors adopt features from earlier studies discussed above and extend themwith two more features: client-based and location-based features. The client-based features include information about the software that was used to perform the messaging. The location-based features include information relating to whether the message was sent from within the same country where the event happened or not. The authors report that adding these two features on top of earlier reported features leads to a substantial boost of accuracy. For instance, adding the two features on top of the propagation-based features reported by [Castillo et al. 2011] leads to an increase of 6.3% in accuracy. However, the authors do not combine all the features and report results on them. For the classification task, the authors use SVM with the RBF kernel [Buhmann 2003]. Another study that tackles rumours in Sina Weibo is reported by [Yang et al. 2015]. Unlike the others, the authors make use of the reviews or comments attached to the source tweet. Traditional features discussed so far are used, but they also incorporate network features (creating a social network based on the comment providers) derived from comments to perform the rumour veracity classification task. It is shown that when the network feature is added to the traditional features the results improve substantially. [Liu et al. 2015] use approaches reported by [Yang et al. 2012] and [Castillo et al. 2011] as baseline systems and compare them against their proposed approach that make use of so called \u201cverification features\u201d. These features are determined based on insights from journalists and include source credibility, source identification, source diversity, source and witness location, event propagation and belief identification. In belief identification results of rumour stance classification are used as features. The authors show that the proposed approach outperforms the two baselines. They also\nshow that, when adding the belief identification to the other features, the results are significantly better than when not adding them to the feature set. The best results range from 78% to 89% accuracy depending on how many tweets are used to verify the rumour (5 tweets to 400 tweets). The experiments are performed on the author\u2019s own dataset using SVM classification. The authors also report having investigated Random Forest and Decision Trees but SVM gave the best results, although no further details are provided on this comparison. [Ma et al. 2015] proposed to model features over time. The authors adopt features from earlier studies as well as machine learning approaches used in those studies (J48, SVM with the RBF kernel). Experiments are performed on datasets from both Twitter and Sina Weibo. Ma et al. use SVM with linear kernel and report that this linear SVM combined with the proposed approach that models the features over time leads to best performance. On the Twitter dataset reported by [Castillo et al. 2011] the authors report 89% accuracy. For Sina Weibo, the authors collect their own dataset and run existing approaches on them. The proposed setting reaches an accuracy of 84.6% where decision trees with J48 leads to 77.4%, SVM with the RBF kernel to 77.9% and random forests to 81.5%. [Wu et al. 2015] extract features from message propagation trees. Three categories of features are considered: message-based, user-based and report-based. Two methods reported by earlier studies [Castillo et al. 2011] and [Yang et al. 2012] are adopted for the evaluation. The machine learning approach chosen by the authors is an SVM with a hybrid kernel technique consisting of random walk kernel [Borgwardt et al. 2005] and an RBF kernel. The results reported are in favour of the proposed hybrid approach leading to 91% accuracy. The baselines achieve 85.4% [Castillo et al. 2011] and 77.2% accuracy [Yang et al. 2012]. In the experiments the authors use rumours with at least 100 reposts. This opens the question as to how well the proposed approach will perform when it is applied to newly emerging rumours where there are only few posts available. The idea of message propagation is also investigated by [Wang and Terano 2015] in combination with pattern matching. In addition, the information inferred from a stance classifier is integrated in the classification process. They propose social graphs to model the interaction between users and so identify influential rumour spreaders. The graph entails information about familiarity measured by the number of contacts such as retweets, replies and comments between two users, activeness measured by the number of days a user has sent out messages, similarity measured by gender and location similarity between two users and trustworthiness measured by whether the user is verified or not. These four factors are merged in a linear model and hence the model is used to weight the link between two users in the social graph. Using a new proposed metric influential spreaders are used to determine rumours. [Vosoughi 2015] tackles veracity classification task using three categories of features (linguistic, user oriented and temporal propagation related features) and speech recognition inspired machine learning approaches, such as Dynamic Time Wrapping (DTW) and HiddenMarkov Models (HMMs). Evaluations are performed on Twitter data gathered by the author. Results show that HMMs with 75% accuracy are superior to DTWs that lead to only 71% accuracy. The authors also report that the best performing features are those in the temporal propagation category leading to 70% accuracy. The linguistic features lead to 64% accuracy and the user oriented features to 65% accuracy. It is also worthwhile noting that the authors, like [Wu et al. 2015], work with rumours that have at least a good volume of tweets, in this case 1,000 tweets. [Giasemidis et al. 2016] report experiments run on 100 million tweets associated with 72 different rumours. Features and machine learning classifiers used in previous work are adopted in this case. The authors report 96.6% classification accuracy and 97% F1 score which are achieved using decision trees.\n[Chen et al. 2016] approach the rumour veracity classification from a different angle. The authors treat it as an anomaly detection problem where false rumours are regarded as anomalies. Several features related to the content, crowd opinion and post propagation are used along with a factor analysis. Euclidean distance and cosine similarity are proposed to describe the deviation degree and posts with high deviation degree are marked as rumours. Comparisons are made with respect to well-known clustering approaches, such as K-means, and the reported results show significantly improved performance. [Chang et al. 2016] put the emphasis on the characteristics of users who post the rumours to determine the veracity. The authors focus on tweets discussing news. Such tweets are first clustered using simple heuristics, e.g., all posts linking to the same news article are grouped together. Based on rules and heuristics, \u2019extreme users\u2019 are determined \u2013 users who match the heuristics, such as number of followers, etc., of users that are likely to post false rumours. If a cluster of posts contains a number of extreme users exceeding a predefined threshold then it is marked as a false rumour cluster. Authors report rules which led to 80% precision and recall. [Chua and Banerjee 2016] published an analysis of various features on the tweet veracity classification task. The authors analysed six categories of features: comprehensibility, sentiment, time-orientation, quantitative details, writing style and topic. Rumours gathered by the authors are used along with the binomial logistic regression to tackle the task in a supervised fashion. Unlike previous studies, Chua et al. only report features that are significantly important rather than an indication of the overall performance of the classifier. These features are: negation words (comprehensibility category), past, present, future POS in the tweets (time-orientation category), discrepancy, sweat and exclusion features (writing style category) and, finally, home, leisure, religion and sex topic features (topic category). In a similar vein, [Ma et al. 2017] investigated the performance difference between bag-of-words (BoW) and word embedding representation of post contents and conclude that the BoW representation was superior to the embedding variant. [Zhang et al. 2015] investigate rumour veracity classification within the health domain. Using data obtained from liuyanbaike.com (a Chinese rumour debunking platform), the authors investigate the correlation between features and veracity of rumours (true or false) based on logistic regression. They report that features like mention of numbers, the source the rumour originated from and hyperlinks, positively correlate with true rumours and rumours containing some wishes are positively correlated with false rumours. If images are included in the rumours then those were negatively correlated with true rumours. Finally, the authors report that the foreign source feature (whether a foreign source was used to support the rumour or not) was not correlated at all with rumour veracity. [Qin et al. 2016] aim to detect new rumours and propose two new feature categories to achieve this. In the first category, posts containing new pieces of information that are unconfirmed with respect to some news event are considered as rumours. In the second feature category, later posts that repeat the same information as the earlier ones marked as rumour are also considered as rumours. The authors report significantly better results than baseline approaches when the aim is to determine veracity of rumours early on (in their particular case, in less than 12 hours). Unlike the previous studies, [Tong et al. 2017] aim at blocking rumours rather than detecting them or marking tweets as true or false. Motivated by the fact that later corrections are not as effective, the authors argue that the first post seen by a user is influential for their future opinion and thus it is important to show users rumours only once they are confirmed to be true. Based on this they propose a reverse-tuple based\nrandomised algorithm to block rumours. The algorithm aims at producing positive seeds to be shown to users first. Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 [Derczynski et al. 2017]. Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Five participants submitted results to this subtask. Participants viewed the task either as either a three-way [Enayet and El-Beltagy 2017; Wang et al. 2017; Singh et al. 2017] or two-way [Chen et al. 2017; Srivastava et al. 2017] single tweet classification task. The methods used mostly the same features and classifiers as used in subtask A (see Section 8), although some added features more specific to the distribution of stance labels in the tweets replying to the source tweet (for example, the best performing system in this task [Enayet and El-Beltagy 2017] considered the percentage of reply tweets classified as either support, deny or query)."}, {"heading": "10. APPLICATIONS", "text": "There have been numerous efforts by both industry and the scientific community to deal with social media rumour detection and verification, ranging from ongoing research projects to fully-fledged applications. The following are some notable examples:\n\u2014PHEME16 [Derczynski and Bontcheva 2014] is a 3-year research project funded by the European Commission, which ran from 2014-2017, studying natural language processing techniques for dealing with rumour detection and resolution. Publications produced as part of this project include rumour detection [Zubiaga et al. 2016b], stance classification [Lukasik et al. 2015a; Lukasik et al. 2016; Zubiaga et al. 2016a], contradiction detection [Lendvai et al. 2016a; Lendvai and Reichel 2016], ontological modelling of rumours [Declerck et al. 2015], visualisation [Lendvai et al. 2016b], analysis of social media rumours [Zubiaga et al. 2016] and studies of journalistic practices of the use of user-generated content [Tolmie et al. 2017a]. \u2014Emergent17 is a data-driven, real-time, web-based rumour tracker. The system automatically tracks social media mentions of URLs associated rumours, however, the identification of rumours and selection of URLs associated with those requires human input and has not been automated. It is part of a research project led by Craig Silverman, partnering with the Tow Center for Digital Journalism at Columbia University, which focuses on how unverified information and rumour are reported in the media. The outcome of this project was published in a report on best practices for debunking misinformation [Silverman 2015a]. \u2014RumorLens18 [Resnick et al. 2014] is a one year research project that ran in 2014, funded by Google. It focused on building a tool to aid journalists in finding posts that spread or correct a particular rumour on Twitter, by exploring the size of the audiences that those posts have reached. More details on the rumour detection system developed in this project were published in [Zhao et al. 2015]. \u2014TwitterTrails19 [Finn et al. 2014] is a project in the Social Informatics Lab at Wellesley College. Twitter Trails is an interactive, web-based tool that allows users to investigate the origin and propagation characteristics of a rumour and its refutation, if any, on Twitter. Visualisations of burst activity, propagation timeline, retweet\n16http://www.pheme.eu/ 17http://www.emergent.info/ 18https://www.si.umich.edu/research/research-projects/rumorlens 19http://twittertrails.com/\nand co-retweeted networks help its users trace the spread of a story. It collects relevant tweets and automatically answers several important questions regarding a rumour: its originator, burst characteristics, propagators and main actors according to the audience. In addition, it computes and reports the rumour\u2019s level of visibility and, as an example of the power of crowdsourcing, the audience\u2019s skepticism towards it which correlates with the rumour\u2019s credibility. The project has produced a number of publications (cf. [Finn et al. 2015; Metaxas et al. 2015]) exploring and characterising the diffusion of rumours. \u2014RumourFlow [Dang et al. 2016b] is a framework that designs, adopts and implements multiple visualisations and modelling tools that can be integrated to reveal rumour contents and participants activity, both within a rumour and across different rumours. The approach supports analysts in drawing hypotheses regarding rumour propagation. \u2014Hoaxy20 [Shao et al. 2016] is a platform for the collection, detection and analysis of online misinformation and its related fact checking efforts. \u2014REVEAL21 is a 3 year project (2013\u20132016) funded by the European Commission. It is concerned with verification of social media content from a journalistic and enterprise perspective, especially focusing on image verification. The project has produced a number of publications on journalistic verification practices concerning social media [Brandtzaeg et al. 2016], social media verification approaches [Andreadou et al. 2015] and approaches to track down the location of social media users [Middleton and Krivcovs 2016]. \u2014 InVID22 (In Video Veritas) is a Horizon 2020 project, funded by the European Commission (2017-2020), which will build a platform providing services to detect, authenticate and check the reliability and accuracy of newsworthy video files and video content spread via social media. \u2014CrossCheck23 is a collaborative verification project led by First Draft and Google News Lab, in collaboration with a number of newsrooms in France, to fight misinformation, with an initial focus on the French presidential election. \u2014De\u0301codex24 is an online database by the French news organisation Le Monde, which allows to check the reliability of news in an Internet domain, warning about a number of satire news and unreliable sites. \u2014Check25 is a verification platform that offers newsrooms the possibility to verify breaking news content online. The platform is not openly available yet, but there is a form to register interest. \u2014ClaimBuster26 is a project aiming to perform live fact-checking. The demo application shows check-worthy claims identified by the system for the 2016 US election, and it allows the user to input their own text to find factual claims. Details of the project have been published in [Hassan et al. 2015]. \u2014UnaHakika27 is a Kenyan project dealing withmisinformation and disinformation. It offers a search engine to look for rumours, as well as an API for data collection. It is manually updated with new stories.\n20http://hoaxy.iuni.iu.edu/ 21http://revealproject.eu/ 22http://www.invid-project.eu/ 23https://firstdraftnews.com/crosscheck-launches/ 24http://www.lemonde.fr/verification/ 25https://meedan.com/en/check/ 26http://idir-server2.uta.edu/claimbuster 27http://www.unahakika.org/\n\u2014Seriously Rapid Source Review28 [Diakopoulos et al. 2012] is a system that incorporates a number of advanced aggregations, computations and cues that can be helpful for journalists to find and assess sources in Twitter around breaking news events, such as finding eyewitnesses on the ground. Finding eyewitnesses can be helpful to get first hand reports that provide evidence to either confirm or debunk rumours. \u2014TweetCred29 [Gupta and Kumaraguru 2012; Gupta et al. 2014] is a real-time, webbased system to assess credibility of content on Twitter. While the system does not determine the veracity of stories, it provides a credibility rating between 1 to 7 for each tweet on the Twitter timeline."}, {"heading": "11. DISCUSSION: SUMMARY AND FUTURE RESEARCH DIRECTIONS", "text": "Research on the development of rumour detection and verification tools has become increasingly popular as social media penetration has increased, enabling both ordinary users and professional practitioners to gather news and facts in a real-time fashion, but with the problematic side effect of the diffusion of information of unverified nature. This survey has summarised studies reported in the scientific literature towards the development of rumour classification systems, defining and characterising social media rumours and has described the different approaches to the development of their four main components: (1) rumour detection, (2) rumour tracking, (3) rumour stance classification, and (4) rumour veracity classification. In so doing, the survey provides a guide to the state-of-the-art in the development of these components. In what follows we review the progress achieved so far, the shortcomings of existing systems, outline suggestions for future research, and comment on the applicability and generalisability of rumour classification systems to other kinds of misleading information that also spread in social media. Research in detection and resolution of rumours has progressed substantially since the proliferation of social media as a platform for information and news gathering. A range of studies have taken very different approaches to understanding and characterising social rumours and this diversity helps to shed light on the future development of rumour classification systems. Research has been conducted in all four of the components that comprise a rumour classification system, although most have focused on the two last components of the pipeline, namely rumour stance classification and veracity classification. Despite substantial progress in the research field, as shown in this survey, we also show that this is still an open research problem that needs further study. We examine the main open research challenges in the next section."}, {"heading": "11.1. Open Challenges and Future Research Directions", "text": "In recent years, research in rumour classification has largely focused on the later stages of the pipeline, namely rumour stance classification and veracity classification. These are crucial stages, however, they cannot be used without performing the preceding tasks of detecting rumours and tracking posts associated with those rumours. The latter has generally been skipped in previous work, either leaving the development of those components for future work or assuming that rumours and associated posts are input by a human. Aiming to alleviate these initial tasks by avoiding relying entirely on the human-in-the-loop, we argue that future research should focus on rumour detection and tracking. An important limitation towards the development of rumour classification systems has been the dearth of publicly available datasets. Along with the recently published\n28http://www.nickdiakopoulos.com/2012/01/24/finding-news-sources-in-social-media/ 29http://twitdigest.iiitd.edu.in/TweetCred/\ndatasets that we have listed in this survey, we encourage researchers to release their own datasets so as to enable further research over different datasets and so permit the scientific community to compare their approaches with one another. While many have attempted to automatically determine the veracity value of a rumour, a system that simply outputs the final decision on veracity may not always be sufficient, given that the classifier will inevitably make errors. To make the output of a veracity classifier more reliable, we argue that the system needs to provide a richer output that also includes the reason for the decision [Procter et al. 2013b]. A veracity classifier that outputs not only the automatically determined veracity score, but also links to sources where this decision can be corroborated, will be more robust in that it will enable the user to assess the reliability of the classifier\u2019s decision and \u2013 if found wanting \u2013 to ignore it. The output of a veracity classifier can be enriched, for instance, by using the output of the stance classifier to choose a few supporting and opposing views that can be presented to the user as a summary. Given that achieving a perfectly accurate veracity classifier is an unlikely goal, we argue that research in this direction should focus especially on finding information sources that facilitate the end user to make their own judgement of rumour veracity. Another caveat of existing veracity classification systems is that they have focused on determining veracity regardless of rumours being resolved. Where rumours have not yet been resolved, the veracity classification task becomes then a prediction task, which may not be reliable for an end user given the lack of evidence to support the system\u2019s decision. As rumours have an unverified status in which determining veracity is hard or requires involvement of authoritative sources, future research should look into temporality of rumour veracity determination, potentially attempting to determine veracity soon after evidence can be found."}, {"heading": "11.2. Rumours, Hoaxes, Misinformation, Disinformation, Fake News", "text": "In this survey we have covered previous efforts towards the development of a rumour classification system that can detect, and resolve the veracity of, rumours. As defined in Section 1.1, rumours refer to pieces of information that start of as unverified statements. A rumour\u2019s veracity value is unverifiable in the early stages, while being subsequently resolved as true or false in a relatively short period of time, or it can also remain unverified for a long time. A number of similar terms are also used in related literature, which have distinct characteristics but also commonalities with rumours. The term misinformation is used to refer to circulating information that is accidentally false as a consequence of an honest mistake, while disinformation refers to information that is deliberately false [Hernon 1995]. Rumours can fall in either of these two categories, depending on the intent of the source; however, the main difference is that rumours are not necessarily false, but may turn out to be true. A rumour that is eventually debunked can then be categorised into misinformation or disinformation depending on the intent of the source. Unlike rumours, hoaxes and fake news are, by definition, always false and can be seen as specific types of disinformation. While it is usually used to refer to any fabricated falsehood indistinctly, a hoax is more rigorously defined as a false story used to masquerade the truth, originating from the verb hocus, meaning \u201cto cheat\u201d [Nares 1822]. Fake news are a specific type of hoax, usually spread through news outlets that are intended to gain politically or financially [Hunt 2016]. However, terms like fake news are widely being used to refer to different types of inaccurate information, while not necessarily adhering to any specific type of misinformation. As [Wardle 2017] suggested, the term fake news is being used to refer to seven types of misinformation: false connection, false context, manipulated content, satire or parody, misleading content, imposter content and fabricated content.\nThe approaches described in this survey are designed to tackle the problem of rumour. Further research is needed to study their applicability to other phenomena, such as hoaxes and fake news. However, we believe that some of the underlying commonalities between rumours, hoaxes and fake news suggest that rumour research has an important contribution to make the new challenges posed by these more recent phenomena."}, {"heading": "11.3. Further Reading", "text": "For more discussion on the issues we cover in this survey, we recommend the special issue on trust and veracity of information on social media of the ACM TOIS journal [Papadopoulos et al. 2016; Rijke 2016], Full Fact\u2019s report on The State of Automated Factchecking [Babakar and Moy 2016], reports and discussion on rumours on Snopes30 and Craig Silverman\u2019s books on rumours and journalistic verification practices [Silverman 2013; 2015a; 2015b]. We also recommend keeping track of ongoing initiatives by the Knight Center for Journalism in the Americas31 and the European Journalism Centre32, as well as signing up for relevant newsletters such as Craig Silverman\u2019s on online rumours, fake news and misinformation33 and Poynter and American Press Institute\u2019s \u2018The Week in Fact-Checking\u201934."}, {"heading": "Acknowledgments", "text": "This work has been supported by the PHEME FP7 project (grant No. 611233)."}], "references": [{"title": "How can you say such things?!?: Recognizing disagreement in informal political argument", "author": ["Rob Abbott", "Marilyn Walker", "Pranav Anand", "Jean E Fox Tree", "Robeson Bowmani", "Joseph King"], "venue": "In Proceedings of the Workshop on Languages in Social Media", "citeRegEx": "Abbott et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbott et al\\.", "year": 2011}, {"title": "A model of crowd enabled organization: Theory and methods for understanding the role of twitter in the occupy protests", "author": ["Sheetal D Agarwal", "W Lance Bennett", "Courtney N Johnson", "Shawn Walker"], "venue": "International Journal of Communication", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "An analysis of rumor", "author": ["Gordon W Allport", "Leo Postman"], "venue": "Public Opinion Quarterly 10,", "citeRegEx": "Allport and Postman.,? \\Q1946\\E", "shortCiteRegEx": "Allport and Postman.", "year": 1946}, {"title": "Media REVEALr: A Social Multimedia Monitoring and Intelligence System for Web Multimedia Verification", "author": ["Katerina Andreadou", "Symeon Papadopoulos", "Lazaros Apostolidis", "Anastasia Krithara", "Yiannis Kompatsiaris"], "venue": "In Pacific-Asia Workshop on Intelligence and Security Informatics", "citeRegEx": "Andreadou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreadou et al\\.", "year": 2015}, {"title": "Keeping Up with the Tweet-Dashians: The Impact of \u2019Official\u2019Accounts on Online Rumoring", "author": ["Cynthia Andrews", "Elodie Fichet", "Yuwei Ding", "Emma S Spiro", "Kate Starbird"], "venue": "In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing", "citeRegEx": "Andrews et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2016}, {"title": "Social media analysis and public opinion: The 2010 UK general election", "author": ["Nick Anstead", "Ben O\u2019Loughlin"], "venue": "Journal of Computer-Mediated Communication 20,", "citeRegEx": "Anstead and O.Loughlin.,? \\Q2015\\E", "shortCiteRegEx": "Anstead and O.Loughlin.", "year": 2015}, {"title": "How information snowballs: Exploring the role of exposure in online rumor propagation", "author": ["Ahmer Arif", "Kelley Shanahan", "Fang-Ju Chou", "Yoanna Dosouto", "Kate Starbird", "Emma S Spiro"], "venue": "In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing", "citeRegEx": "Arif et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arif et al\\.", "year": 2016}, {"title": "The wisdom of twitter crowds: Predicting stock market reactions to fomc meetings via twitter feeds", "author": ["Pablo D Azar", "Andrew W Lo"], "venue": "The Journal of Portfolio Management 42,", "citeRegEx": "Azar and Lo.,? \\Q2016\\E", "shortCiteRegEx": "Azar and Lo.", "year": 2016}, {"title": "The State of Automated Factchecking", "author": ["Mevan Babakar", "Will Moy"], "venue": "Full Fact", "citeRegEx": "Babakar and Moy.,? \\Q2016\\E", "shortCiteRegEx": "Babakar and Moy.", "year": 2016}, {"title": "UWaterloo at SemEval-2017 Task 8: Detecting Stance towards Rumours with Topic Independent Features", "author": ["Hareesh Bahuleyan", "Olga Vechtomova"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Bahuleyan and Vechtomova.,? \\Q2017\\E", "shortCiteRegEx": "Bahuleyan and Vechtomova.", "year": 2017}, {"title": "Studying verbal interaction on the Internet: The case of rumor transmission research", "author": ["Prashant Bordia"], "venue": "Behavior Research Methods, Instruments, & Computers 28,", "citeRegEx": "Bordia.,? \\Q1996\\E", "shortCiteRegEx": "Bordia.", "year": 1996}, {"title": "Protein function prediction via graph kernels", "author": ["Karsten M Borgwardt", "Cheng Soon Ong", "Stefan Sch\u00f6nauer", "SVN Vishwanathan", "Alex J Smola", "HansPeter Kriegel"], "venue": "Bioinformatics", "citeRegEx": "Borgwardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2005}, {"title": "Emerging journalistic verification practices concerning social media", "author": ["Petter Bae Brandtzaeg", "Marika L\u00fcders", "Jochen Spangenberg", "Linda Rath-Wiggins", "Asbj\u00f8rn F\u00f8lstad"], "venue": "Journalism Practice 10,", "citeRegEx": "Brandtzaeg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brandtzaeg et al\\.", "year": 2016}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": "Computational linguistics 18,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Radial basis functions: theory and implementations", "author": ["Martin D Buhmann"], "venue": "Cambridge Monographs on Applied and Computational Mathematics", "citeRegEx": "Buhmann.,? \\Q2003\\E", "shortCiteRegEx": "Buhmann.", "year": 2003}, {"title": "Rumors detection in chinese via crowd responses", "author": ["Guoyong Cai", "Hao Wu", "Rui Lv"], "venue": "In Advances in Social Networks Analysis andMining (ASONAM),", "citeRegEx": "Cai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2014}, {"title": "Big Crisis Data: Social Media in Disasters and Time-Critical Situations", "author": ["Carlos Castillo"], "venue": null, "citeRegEx": "Castillo.,? \\Q2016\\E", "shortCiteRegEx": "Castillo.", "year": 2016}, {"title": "Information credibility on twitter", "author": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete"], "venue": "In Proceedings of the 20th international conference on World wide web", "citeRegEx": "Castillo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 2011}, {"title": "Predicting information credibility in timesensitive social media", "author": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete"], "venue": "Internet Research 23,", "citeRegEx": "Castillo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 2013}, {"title": "Extreme User and Political Rumor Detection on Twitter", "author": ["Cheng Chang", "Yihong Zhang", "Claudia Szabo", "Quan Z Sheng"], "venue": "In Advanced Data Mining and Applications: 12th International Conference,", "citeRegEx": "Chang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2016}, {"title": "Wisdom of crowds: The value of stock opinions transmitted through social media", "author": ["Hailiang Chen", "Prabuddha De", "Yu Jeffrey Hu", "Byoung-Hyoun Hwang"], "venue": "Review of Financial Studies 27,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Behavior deviation: An anomaly detection view of rumor preemption", "author": ["Weiling Chen", "Chai Kiat Yeo", "Chiew Tong Lau", "Bu Sung Lee"], "venue": "In Information Technology, Electronics and Mobile Communication Conference (IEMCON),", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "IKM at SemEval-2017 Task 8: Convolutional Neural Networks for Stance Detection and Rumor Verification", "author": ["Yi-Chin Chen", "Zhao-Yand Liu", "Hung-Yu Kao"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Chen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "An epidemic model of rumor diffusion in online social networks", "author": ["Jun-Jun Cheng", "Yun Liu", "Bo Shen", "Wei-Guo Yuan"], "venue": "The European Physical Journal B", "citeRegEx": "Cheng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2013}, {"title": "Linguistic Predictors of Rumor Veracity on the Internet", "author": ["Alton YK Chua", "Snehasish Banerjee"], "venue": "In Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "Chua and Banerjee.,? \\Q2016\\E", "shortCiteRegEx": "Chua and Banerjee.", "year": 2016}, {"title": "The Retransmission of Rumorrelated Tweets: Characteristics of Source and Message", "author": ["Alton YK Chua", "Cheng-Ying Tee", "Augustine Pang", "Ee-Peng Lim"], "venue": "In Proceedings of the 7th 2016 International Conference on Social Media & Society", "citeRegEx": "Chua et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chua et al\\.", "year": 2016}, {"title": "What is in a rumour: Combined visual analysis of rumour flow and user activity", "author": ["Anh Dang", "Abidalrahman Moh\u2019d", "Evangelos Milios", "Rosane Minghim", "others"], "venue": "Computer Graphics International,", "citeRegEx": "Dang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dang et al\\.", "year": 2016}, {"title": "Toward understanding how users respond to rumours in social media", "author": ["Anh Dang", "Michael Smit", "Abidalrahman Moh\u2019d", "Rosane Minghim", "Evangelos Milios"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "Dang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dang et al\\.", "year": 2016}, {"title": "Ontological modelling of rumors", "author": ["Thierry Declerck", "Petya Osenova", "Georgi Georgiev", "Piroska Lendvai"], "venue": "In Workshop on Social Media and the Web of Linked Data", "citeRegEx": "Declerck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Declerck et al\\.", "year": 2015}, {"title": "Pheme: Veracity in Digital Social Networks", "author": ["Leon Derczynski", "Kalina Bontcheva"], "venue": "In UMAP Workshops", "citeRegEx": "Derczynski and Bontcheva.,? \\Q2014\\E", "shortCiteRegEx": "Derczynski and Bontcheva.", "year": 2014}, {"title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours", "author": ["Leon Derczynski", "Kalina Bontcheva", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Arkaitz Zubiaga"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Derczynski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2017}, {"title": "Finding and assessing social media information sources in the context of journalism", "author": ["Nicholas Diakopoulos", "MunmunDe Choudhury", "andMor Naaman"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems", "citeRegEx": "Diakopoulos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Diakopoulos et al\\.", "year": 2012}, {"title": "Rumor, gossip and urban legends", "author": ["Nicholas DiFonzo", "Prashant Bordia"], "venue": "Diogenes 54,", "citeRegEx": "DiFonzo and Bordia.,? \\Q2007\\E", "shortCiteRegEx": "DiFonzo and Bordia.", "year": 2007}, {"title": "Crowdsourcing systems on the world-wide web", "author": ["Anhai Doan", "Raghu Ramakrishnan", "Alon Y Halevy"], "venue": "Commun. ACM 54,", "citeRegEx": "Doan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Doan et al\\.", "year": 2011}, {"title": "How idle is idle talk? One hundred years of rumor research", "author": ["Pamela Donovan"], "venue": "Diogenes 54,", "citeRegEx": "Donovan.,? \\Q2007\\E", "shortCiteRegEx": "Donovan.", "year": 2007}, {"title": "Big data, big questions\u2014working within a black box: Transparency in the collection and production of big twitter data", "author": ["Kevin Driscoll", "ShawnWalker"], "venue": "International Journal of Communication", "citeRegEx": "Driscoll and ShawnWalker.,? \\Q2014\\E", "shortCiteRegEx": "Driscoll and ShawnWalker.", "year": 2014}, {"title": "Twitter earthquake detection: earthquake monitoring in a social world", "author": ["Paul S Earle", "Daniel C Bowden", "Michelle Guy"], "venue": "Annals of Geophysics 54,", "citeRegEx": "Earle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Earle et al\\.", "year": 2012}, {"title": "NileTMRG at SemEval-2017 Task 8: Determining Rumour and Veracity Support for Rumours on Twitter", "author": ["Omar Enayet", "Samhaa R. El-Beltagy"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Enayet and El.Beltagy.,? \\Q2017\\E", "shortCiteRegEx": "Enayet and El.Beltagy.", "year": 2017}, {"title": "Spread and Skepticism: Metrics of Propagation on Twitter", "author": ["Samantha Finn", "Panagiotis Takis Metaxas", "Eni Mustafaraj"], "venue": "In Proceedings of the ACM Web Science Conference", "citeRegEx": "Finn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2015}, {"title": "TRAILS: A system for monitoring the propagation of rumors on twitter", "author": ["Samantha Finn", "Panagiotis Takis Metaxas", "Eni Mustafaraj", "Megan OKeefe", "Lindsay Tang", "Susan Tang", "Laura Zeng"], "venue": "In Computation and Journalism Symposium,", "citeRegEx": "Finn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2014}, {"title": "Characterizing urban landscapes using geolocated tweets", "author": ["Vanessa Frias-Martinez", "Victor Soto", "Heath Hohwald", "Enrique Frias-Martinez"], "venue": "In International Conference on Privacy, Security, Risk and Trust and International Conference on Social Computing", "citeRegEx": "Frias.Martinez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Frias.Martinez et al\\.", "year": 2012}, {"title": "Social media: A critical introduction", "author": ["Christian Fuchs"], "venue": null, "citeRegEx": "Fuchs.,? \\Q2013\\E", "shortCiteRegEx": "Fuchs.", "year": 2013}, {"title": "Brand data gathering from live social media streams", "author": ["Yue Gao", "Fanglin Wang", "Huanbo Luan", "Tat-Seng Chua"], "venue": "In Proceedings of International Conference on Multimedia Retrieval", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Mama Edha at SemEval-2017 Task 8: Stance Classification with CNN and Rules", "author": ["Marianela Garc\u0131\u0301a Lozano", "Hanna Lilja", "Edward Tj\u00f6rnhammar", "Maja Maja Karasalo"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Lozano et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lozano et al\\.", "year": 2017}, {"title": "Determining the veracity of rumours on Twitter", "author": ["Georgios Giasemidis", "Colin Singleton", "Ioannis Agrafiotis", "Jason RC Nurse", "Alan Pilgrim", "Chris Willis", "DV Greetham"], "venue": "In International Conference on Social Informatics", "citeRegEx": "Giasemidis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Giasemidis et al\\.", "year": 2016}, {"title": "Mapping and leveraging influencers in social media to shape corporate brand perceptions", "author": ["Michael B Goodman", "Norman Booth", "Julie Ann Matic"], "venue": "Corporate Communications: An International Journal 16,", "citeRegEx": "Goodman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2011}, {"title": "News Use Across Social Media Platforms 2016", "author": ["Jeffrey Gottfried", "Elisa Shearer"], "venue": null, "citeRegEx": "Gottfried and Shearer.,? \\Q2016\\E", "shortCiteRegEx": "Gottfried and Shearer.", "year": 2016}, {"title": "Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 864\u2013872", "author": ["Weiwei Guo", "Mona Diab"], "venue": null, "citeRegEx": "Guo and Diab.,? \\Q2012\\E", "shortCiteRegEx": "Guo and Diab.", "year": 2012}, {"title": "Credibility ranking of tweets during high impact events", "author": ["Aditi Gupta", "PonnurangamKumaraguru"], "venue": "In Proceedings of the 1st Workshop on Privacy and Security in Online Social Media. ACM,", "citeRegEx": "Gupta and PonnurangamKumaraguru.,? \\Q2012\\E", "shortCiteRegEx": "Gupta and PonnurangamKumaraguru.", "year": 2012}, {"title": "Tweetcred: Real-time credibility assessment of content on twitter", "author": ["Aditi Gupta", "Ponnurangam Kumaraguru", "Carlos Castillo", "Patrick Meier"], "venue": "In International Conference on Social Informatics", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD explorations newsletter 11,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Rumor Detection and Classification for Twitter Data", "author": ["Sardar Hamidian", "Mona T Diab"], "venue": "In Proceedings of the Fifth International Conference on Social Media Technologies,", "citeRegEx": "Hamidian and Diab.,? \\Q2015\\E", "shortCiteRegEx": "Hamidian and Diab.", "year": 2015}, {"title": "Rumor Identification and Belief Investigation on Twitter", "author": ["Sardar Hamidian", "Mona T Diab"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Hamidian and Diab.,? \\Q2016\\E", "shortCiteRegEx": "Hamidian and Diab.", "year": 2016}, {"title": "Text-based twitter user geolocation prediction", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Han et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Han et al\\.", "year": 2014}, {"title": "Get back! you dont know me like that: The social mediation of fact checking interventions in twitter conversations", "author": ["Aniko Hannak", "Drew Margolin", "Brian Keegan", "Ingmar Weber"], "venue": "In Proceedings of the AAAI Conference on Weblogs and Social Media", "citeRegEx": "Hannak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannak et al\\.", "year": 2014}, {"title": "The quest to automate fact-checking", "author": ["Naeemul Hassan", "Bill Adair", "James T Hamilton", "Chengkai Li", "Mark Tremayne", "Jun Yang", "Cong Yu"], "venue": "In Computation and Journalism Symposium,", "citeRegEx": "Hassan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2015}, {"title": "Twittering the news: The emergence of ambient journalism", "author": ["Alfred Hermida"], "venue": "Journalism practice 4,", "citeRegEx": "Hermida.,? \\Q2010\\E", "shortCiteRegEx": "Hermida.", "year": 2010}, {"title": "Tweets and truth: Journalism as a discipline of collaborative verification", "author": ["Alfred Hermida"], "venue": "Journalism Practice", "citeRegEx": "Hermida.,? \\Q2012\\E", "shortCiteRegEx": "Hermida.", "year": 2012}, {"title": "A clash of cultures: The integration of user-generated content within professional journalistic frameworks at British newspaper websites", "author": ["Alfred Hermida", "Neil Thurman"], "venue": "Journalism practice", "citeRegEx": "Hermida and Thurman.,? \\Q2008\\E", "shortCiteRegEx": "Hermida and Thurman.", "year": 2008}, {"title": "Disinformation and misinformation through the Internet: Findings of an exploratory study", "author": ["Peter Hernon"], "venue": "Government Information Quarterly 12,", "citeRegEx": "Hernon.,? \\Q1995\\E", "shortCiteRegEx": "Hernon.", "year": 1995}, {"title": "What is fake news? How to spot it and what you can do to stop it", "author": ["Elle Hunt"], "venue": null, "citeRegEx": "Hunt.,? \\Q2016\\E", "shortCiteRegEx": "Hunt.", "year": 2016}, {"title": "Processing social media messages in mass emergency: A survey", "author": ["Muhammad Imran", "Carlos Castillo", "Fernando Diaz", "Sarah Vieweg"], "venue": "ACM Computing Surveys (CSUR) 47,", "citeRegEx": "Imran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Imran et al\\.", "year": 2015}, {"title": "Who Hears What from Whom and with What Effect A Study of Rumor", "author": ["Marianne E Jaeger", "Susan Anthony", "Ralph L Rosnow"], "venue": "Personality and Social Psychology Bulletin", "citeRegEx": "Jaeger et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 1980}, {"title": "SocialStories: Segmenting Stories within Trending Twitter Topics", "author": ["Kokil Jaidka", "Kaushik Ramachandran", "Prakhar Gupta", "Sajal Rustagi"], "venue": "In Proceedings of the 3rd IKDD Conference on Data Science,", "citeRegEx": "Jaidka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaidka et al\\.", "year": 2016}, {"title": "Diffusion of Rumors on the Internet. The Information Society Review", "author": ["Dong-Gi Jo"], "venue": null, "citeRegEx": "Jo.,? \\Q2002\\E", "shortCiteRegEx": "Jo.", "year": 2002}, {"title": "A psychology of rumor", "author": ["Robert H Knapp"], "venue": "Public Opinion Quarterly 8,", "citeRegEx": "Knapp.,? \\Q1944\\E", "shortCiteRegEx": "Knapp.", "year": 1944}, {"title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM", "author": ["Elena Kochkina", "Maria Liakata", "Isabelle Augenstein"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Kochkina et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kochkina et al\\.", "year": 2017}, {"title": "What is Twitter, a social network or a news media", "author": ["Haewoon Kwak", "Changhyun Lee", "Hosung Park", "Sue Moon"], "venue": "In Proceedings of the 19th international conference on World wide web", "citeRegEx": "Kwak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kwak et al\\.", "year": 2010}, {"title": "Modeling Bursty Temporal Pattern of Rumors", "author": ["Sejeong Kwon", "Meeyoung Cha"], "venue": "In ICWSM", "citeRegEx": "Kwon and Cha.,? \\Q2014\\E", "shortCiteRegEx": "Kwon and Cha.", "year": 2014}, {"title": "Rumor Detection over Varying Time Windows", "author": ["Sejeong Kwon", "Meeyoung Cha", "Kyomin Jung"], "venue": "PLOS ONE 12,", "citeRegEx": "Kwon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kwon et al\\.", "year": 2017}, {"title": "Prominent features of rumor propagation in online social media", "author": ["Sejeong Kwon", "Meeyoung Cha", "Kyomin Jung", "Wei Chen", "Yajun Wang"], "venue": "IEEE 13th International Conference on Data Mining", "citeRegEx": "Kwon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwon et al\\.", "year": 2013}, {"title": "The role of social media in the capital market: evidence from consumer product recalls", "author": ["Lian Fen Lee", "Amy P Hutton", "Susan Shu"], "venue": "Journal of Accounting Research 53,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Monolingual social media datasets for detecting contradiction and entailment", "author": ["Piroska Lendvai", "Isabelle Augenstein", "Kalina Bontcheva", "Thierry Declerck"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC), Portoroz, Slovenia. European Language Resources Association (ELRA)", "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "Contradiction Detection for Rumorous Claims", "author": ["Piroska Lendvai", "Uwe D Reichel"], "venue": "In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics", "citeRegEx": "Lendvai and Reichel.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai and Reichel.", "year": 2016}, {"title": "Factuality drift assessment by lexical markers in resolved rumors", "author": ["Piroska Lendvai", "Uwe D Reichel", "Thierry Declerck"], "venue": "In Joint Proceedings of the Posters and Demos Track of the 12th International Conference on Semantic Systems (SEMANTiCS 2016) and the 1st International Workshop on Semantic Change & Evolving Semantics,", "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "Misinformation and its correction continued influence and successful debiasing", "author": ["Stephan Lewandowsky", "Ullrich KH Ecker", "Colleen M Seifert", "Norbert Schwarz", "John Cook"], "venue": "Psychological Science in the Public Interest 13,", "citeRegEx": "Lewandowsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lewandowsky et al\\.", "year": 2012}, {"title": "Computing the Veracity of Information through Crowds: A Method for Reducing the Spread of False Messages on Social Media", "author": ["Huaye Li", "Yasuaki Sakamoto"], "venue": "In System Sciences (HICSS),", "citeRegEx": "Li and Sakamoto.,? \\Q2015\\E", "shortCiteRegEx": "Li and Sakamoto.", "year": 2015}, {"title": "Timeline generation: Tracking individuals on twitter", "author": ["Jiwei Li", "Claire Cardie"], "venue": "In Proceedings of the 23rd international conference on World wide web", "citeRegEx": "Li and Cardie.,? \\Q2014\\E", "shortCiteRegEx": "Li and Cardie.", "year": 2014}, {"title": "Rumor Identification in Microblogging Systems Based on Users Behavior", "author": ["Gang Liang", "Wenbo He", "Chun Xu", "Liangyin Chen", "Jinquan Zeng"], "venue": "IEEE Transactions on Computational Social Systems", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "She gets a sports car from our donation: rumor transmission in a chinese microblogging community", "author": ["Qinying Liao", "Lei Shi"], "venue": "In Proceedings of the 2013 conference on Computer supported cooperative work", "citeRegEx": "Liao and Shi.,? \\Q2013\\E", "shortCiteRegEx": "Liao and Shi.", "year": 2013}, {"title": "Rumors on Social Media in disasters: Extending Transmission to Retransmission", "author": ["Fang Liu", "Andrew Burton-Jones", "Dongming Xu"], "venue": "In PACIS", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Real-time rumor debunking on twitter", "author": ["Xiaomo Liu", "Armineh Nourbakhsh", "Quanzhi Li", "Rui Fang", "Sameena Shah"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Using APIs for data collection on social media", "author": ["Stine Lomborg", "Anja Bechmann"], "venue": "The Information Society 30,", "citeRegEx": "Lomborg and Bechmann.,? \\Q2014\\E", "shortCiteRegEx": "Lomborg and Bechmann.", "year": 2014}, {"title": "Towards effective event detection, tracking and summarization on microblog data", "author": ["Rui Long", "Haofen Wang", "Yuqiang Chen", "Ou Jin", "Yong Yu"], "venue": "In International Conference on Web-Age Information", "citeRegEx": "Long et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Long et al\\.", "year": 2011}, {"title": "Classifying Tweet Level Judgements of Rumours in Social Media", "author": ["Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lukasik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "2015b. Point Process Modelling of Rumour Dynamics in Social Media", "author": ["Michal Lukasik", "Trevor Cohn", "Kalina Bontcheva"], "venue": "In ACL", "citeRegEx": "Lukasik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2015}, {"title": "2016. Hawkes Processes for continuous time sequence classification: an application to rumour stance classification in Twitter", "author": ["Michal Lukasik", "PK Srijith", "Duy Vu", "Kalina Bontcheva", "Arkaitz Zubiaga", "Trevor Cohn"], "venue": "In Proceedings of 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "Lukasik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2016}, {"title": "Content Representation for Microblog Rumor Detection", "author": ["Ben Ma", "Dazhen Lin", "Donglin Cao"], "venue": "In Advances in Computational Intelligence", "citeRegEx": "Ma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2017}, {"title": "Detect rumors using time series of social context information on microblogging websites", "author": ["Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Characterizing online rumoring behavior using multi-dimensional signatures", "author": ["JimMaddock", "Kate Starbird", "Haneen J Al-Hassani", "Daniel E Sandoval", "Mania Orand", "Robert MMason"], "venue": "In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing", "citeRegEx": "JimMaddock et al\\.,? \\Q2015\\E", "shortCiteRegEx": "JimMaddock et al\\.", "year": 2015}, {"title": "Twitinfo: aggregating and visualizing microblogs for event exploration", "author": ["Adam Marcus", "Michael S Bernstein", "Osama Badar", "David R Karger", "Samuel Madden", "Robert C Miller"], "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems", "citeRegEx": "Marcus et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 2011}, {"title": "Crowdsourced rumour identification during emergencies", "author": ["Richard McCreadie", "Craig Macdonald", "Iadh Ounis"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "McCreadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McCreadie et al\\.", "year": 2015}, {"title": "Twitter under crisis: can we trust what we RT", "author": ["Marcelo Mendoza", "Barbara Poblete", "Carlos Castillo"], "venue": "In Proceedings of the first workshop on social media analytics", "citeRegEx": "Mendoza et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mendoza et al\\.", "year": 2010}, {"title": "Using twittertrails. com to investigate rumor propagation", "author": ["Panagiotis Takas Metaxas", "Samantha Finn", "Eni Mustafaraj"], "venue": "In Proceedings of the 18th ACMConference Companion on Computer Supported Cooperative Work & Social Computing", "citeRegEx": "Metaxas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Metaxas et al\\.", "year": 2015}, {"title": "Geoparsing and geosemantics for social media: spatiotemporal grounding of content propagating rumours to support trust and veracity analysis during breaking news", "author": ["Stuart E Middleton", "Vadims Krivcovs"], "venue": "ACM Transactions on Information Systems", "citeRegEx": "Middleton and Krivcovs.,? \\Q2016\\E", "shortCiteRegEx": "Middleton and Krivcovs.", "year": 2016}, {"title": "Real-time crisis mapping of natural disasters using social media", "author": ["Stuart E Middleton", "Lee Middleton", "Stefano Modafferi"], "venue": "IEEE Intelligent Systems 29,", "citeRegEx": "Middleton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Middleton et al\\.", "year": 2014}, {"title": "WordNet: a lexical database for English", "author": ["George A Miller"], "venue": "Commun. ACM 38,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "CREDBANK: A Large-Scale Social Media Corpus With Associated Credibility Annotations", "author": ["Tanushree Mitra", "Eric Gilbert"], "venue": "In ICWSM", "citeRegEx": "Mitra and Gilbert.,? \\Q2015\\E", "shortCiteRegEx": "Mitra and Gilbert.", "year": 2015}, {"title": "Semeval2016 task 6: Detecting stance in tweets", "author": ["Saif MMohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry"], "venue": "In Proceedings of the International Workshop on Semantic Evaluation, SemEval,", "citeRegEx": "MMohammad et al\\.,? \\Q2016\\E", "shortCiteRegEx": "MMohammad et al\\.", "year": 2016}, {"title": "Social Media in Public Opinion Research Executive Summary of the Aapor Task Force on Emerging Technologies in Public Opinion Research", "author": ["Joe Murphy", "Michael W Link", "Jennifer Hunter Childs", "Casey Langer Tesfaye", "Elizabeth Dean", "Michael Stern", "Josh Pasek", "Jon Cohen", "Mario Callegaro", "Paul Harwood"], "venue": "Public Opinion Quarterly 78,", "citeRegEx": "Murphy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 2014}, {"title": "Community intelligence and social media services: A rumor theoretic analysis of tweets during social crises", "author": ["Onook Oh", "Manish Agrawal", "H Raghav Rao"], "venue": "Mis Quarterly 37,", "citeRegEx": "Oh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2013}, {"title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries", "author": ["Alexandra Olteanu", "Carlos Castillo", "Fernando Diaz", "Emre Kiciman"], "venue": null, "citeRegEx": "Olteanu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Olteanu et al\\.", "year": 2016}, {"title": "Overview of the Special Issue on Trust and Veracity of Information in Social Media", "author": ["Symeon Papadopoulos", "Kalina Bontcheva", "Eva Jaho", "Mihai Lupu", "Carlos Castillo"], "venue": "ACM Transactions on Information Systems (TOIS) 34,", "citeRegEx": "Papadopoulos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2016}, {"title": "Communication and persuasion: Central and peripheral routes to attitude change", "author": ["Richard Petty", "John Cacioppo"], "venue": null, "citeRegEx": "Petty and Cacioppo.,? \\Q2012\\E", "shortCiteRegEx": "Petty and Cacioppo.", "year": 2012}, {"title": "Breaking news detection and tracking in Twitter", "author": ["Swit Phuvipadawat", "Tsuyoshi Murata"], "venue": "In Web Intelligence and Intelligent Agent Technology (WI-IAT),", "citeRegEx": "Phuvipadawat and Murata.,? \\Q2010\\E", "shortCiteRegEx": "Phuvipadawat and Murata.", "year": 2010}, {"title": "Credibility Assessment of Textual Claims on the Web", "author": ["Kashyap Popat", "Subhabrata Mukherjee", "Jannik Str\u00f6tgen", "Gerhard Weikum"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "Popat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Popat et al\\.", "year": 2016}, {"title": "The psychology of rumour: A study relating to the great Indian earthquake of 1934", "author": ["Jamuna Prasad"], "venue": "British Journal of Psychology. General Section", "citeRegEx": "Prasad.,? \\Q1935\\E", "shortCiteRegEx": "Prasad.", "year": 1935}, {"title": "Cantijoch. 2013a. Reading the riots: What were the Police doing on Twitter", "author": ["Rob Procter", "Jeremy Crump", "Susanne Karstedt", "Alex Voss", "Marta"], "venue": "Policing and society 23,", "citeRegEx": "Procter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Reading the riots on Twitter: methodological innovation for the analysis of big data", "author": ["Rob Procter", "Farida Vis", "Alex Voss"], "venue": "International journal of social research methodology 16,", "citeRegEx": "Procter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Procter et al\\.", "year": 2013}, {"title": "Rumor Has It: Identifying Misinformation in Microblogs", "author": ["Vahed Qazvinian", "Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Qazvinian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "Spotting Rumors via Novelty Detection", "author": ["Yumeng Qin", "Dominik Wurzer", "Victor Lavrenko", "Cunchen Tang"], "venue": "arXiv preprint arXiv:1611.06322", "citeRegEx": "Qin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qin et al\\.", "year": 2016}, {"title": "Rumorlens: A system for analyzing the impact of rumors and corrections in social media", "author": ["Paul Resnick", "Samuel Carton", "Souneil Park", "Yuncheng Shen", "Nicole Zeffer"], "venue": "In Proc. Computational Journalism Conference", "citeRegEx": "Resnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Resnick et al\\.", "year": 2014}, {"title": "Event-based classification of social media streams", "author": ["Timo Reuter", "Philipp Cimiano"], "venue": "In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval", "citeRegEx": "Reuter and Cimiano.,? \\Q2012\\E", "shortCiteRegEx": "Reuter and Cimiano.", "year": 2012}, {"title": "Special Issue on Trust and Veracity of Information in Social Media", "author": ["Maarten de Rijke"], "venue": "ACM Transactions on Information Systems 34,", "citeRegEx": "Rijke.,? \\Q2016\\E", "shortCiteRegEx": "Rijke.", "year": 2016}, {"title": "I couldnt agree more: The role of conversational structure in agreement and disagreement detection in online discussions", "author": ["Sara Rosenthal", "Kathleen McKeown"], "venue": "In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "citeRegEx": "Rosenthal and McKeown.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal and McKeown.", "year": 2015}, {"title": "Rumor and gossip research", "author": ["Ralph L Rosnow", "Eric K Foster"], "venue": "Psychological Science Agenda 19,", "citeRegEx": "Rosnow and Foster.,? \\Q2005\\E", "shortCiteRegEx": "Rosnow and Foster.", "year": 2005}, {"title": "Event Detection and Tracking in Social Streams", "author": ["Hassan Sayyadi", "Matthew Hurst", "Alexey Maykov"], "venue": "In Proceedings of the AAAI Conference on Weblogs and Social Media", "citeRegEx": "Sayyadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sayyadi et al\\.", "year": 2009}, {"title": "Post-disaster rumor chains: A case study", "author": ["T Joseph Scanlon"], "venue": "Mass Emergencies", "citeRegEx": "Scanlon.,? \\Q1977\\E", "shortCiteRegEx": "Scanlon.", "year": 1977}, {"title": "Identifying rumors and their sources in social networks. In SPIE defense, security, and sensing. International Society for Optics and Photonics, 83891I\u201383891I", "author": ["Eunsoo Seo", "Prasant Mohapatra", "Tarek Abdelzaher"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2012}, {"title": "A survey of Twitter rumor spreading simulations", "author": ["Emilio Serrano", "Carlos A Iglesias", "Mercedes Garijo"], "venue": null, "citeRegEx": "Serrano et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serrano et al\\.", "year": 2015}, {"title": "Hoaxy: A platform for tracking online misinformation", "author": ["Chengcheng Shao", "Giovanni Luca Ciampaglia", "Alessandro Flammini", "Filippo Menczer"], "venue": "In Proceedings of the 25th International Conference Companion on World Wide Web", "citeRegEx": "Shao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2016}, {"title": "Effects of online comments on smokers\u2019 perception of antismoking public service announcements", "author": ["Rui Shi", "Paul Messaris", "Joseph N Cappella"], "venue": "Journal of Computer-Mediated Communication 19,", "citeRegEx": "Shi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2014}, {"title": "Political rumoring on Twitter during the 2012 US presidential election: Rumor diffusion and correction. new media", "author": ["Jieun Shin", "Lian Jian", "Kevin Driscoll", "Fran\u00e7ois Bar"], "venue": null, "citeRegEx": "Shin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2016}, {"title": "Lies, Damn Lies, and Viral Content: How News Websites Spread (and Debunk) Online Rumors, Unverified Claims and Misinformation", "author": ["Craig Silverman"], "venue": "Tow Center for Digital Journalism", "citeRegEx": "Silverman.,? \\Q2015\\E", "shortCiteRegEx": "Silverman.", "year": 2015}, {"title": "Verification Handbook for Investigative Reporting: A Guide to Online Search and Research Techniques for Using UGC and Open Source Information in Investigations", "author": ["Craig Silverman"], "venue": "European Journalism Centre", "citeRegEx": "Silverman.,? \\Q2015\\E", "shortCiteRegEx": "Silverman.", "year": 2015}, {"title": "IITP at SemEval-2017 Task 8: A Supervised Approach for Rumour Evaluation", "author": ["Vikram Singh", "Sunny Narayan", "Md Shad Akhtar", "Asif Ekbal", "Pushpak Bhattacharya"], "venue": "Proceedings of SemEval", "citeRegEx": "Singh et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2017}, {"title": "Behaviour in a catastrophic situation: A psychological study of reports and rumours", "author": ["Durganand Sinha"], "venue": "British Journal of Psychology. General Section 43,", "citeRegEx": "Sinha.,? \\Q1952\\E", "shortCiteRegEx": "Sinha.", "year": 1952}, {"title": "Recognizing stances in online debates", "author": ["Swapna Somasundaran", "Janyce Wiebe"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "Somasundaran and Wiebe.,? \\Q2009\\E", "shortCiteRegEx": "Somasundaran and Wiebe.", "year": 2009}, {"title": "News from the crowd: Grassroots and collaborative journalism in the digital age", "author": ["Jochen Spangenberg", "Nicolaus Heise"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web", "citeRegEx": "Spangenberg and Heise.,? \\Q2014\\E", "shortCiteRegEx": "Spangenberg and Heise.", "year": 2014}, {"title": "DFKI-DKT at SemEval-2017 Task 8: Rumour Detection and Classification using Cascading Heuristics", "author": ["Ankit Srivastava", "Rehm Rehm", "Julian Moreno Schneider"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Srivastava et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2017}, {"title": "Rumors, false flags, and digital vigilantes: Misinformation on twitter after the 2013 boston marathon bombing", "author": ["Kate Starbird", "JimMaddock", "Mania Orand", "Peg Achterman", "Robert MMason"], "venue": "iConference 2014 Proceedings", "citeRegEx": "Starbird et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Starbird et al\\.", "year": 2014}, {"title": "Learning from the crowd: collaborative filtering techniques for identifying on-the-ground Twitterers during mass disruptions", "author": ["Kate Starbird", "Grace Muzny", "Leysia Palen"], "venue": "In Proceedings of 9th International Conference on Information Systems for Crisis Response and Management, ISCRAM", "citeRegEx": "Starbird et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Starbird et al\\.", "year": 2012}, {"title": "Do Online Comments Influence the Public\u2019s Attitudes Toward an Organization? Effects of Online Comments Based on Individuals Prior Attitudes", "author": ["Kang Hoon Sung", "Moon J Lee"], "venue": "The Journal of psychology 149,", "citeRegEx": "Sung and Lee.,? \\Q2015\\E", "shortCiteRegEx": "Sung and Lee.", "year": 2015}, {"title": "Rumor detection on twitter", "author": ["Tetsuro Takahashi", "Nobuyuki Igata"], "venue": "In Soft Computing and Intelligent Systems (SCIS) and 13th International Symposium on Advanced Intelligent Systems (ISIS),", "citeRegEx": "Takahashi and Igata.,? \\Q2012\\E", "shortCiteRegEx": "Takahashi and Igata.", "year": 2012}, {"title": "Rumor diffusion and convergence during the 3.11 earthquake: a Twitter case study", "author": ["Misako Takayasu", "Kazuya Sato", "Yukie Sano", "Kenta Yamada", "Wataru Miura", "Hideki Takayasu"], "venue": "PLoS one 10,", "citeRegEx": "Takayasu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Takayasu et al\\.", "year": 2015}, {"title": "The psychological meaning of words: LIWC and computerized text analysis methods", "author": ["Yla R Tausczik", "James W Pennebaker"], "venue": "Journal of language and social psychology 29,", "citeRegEx": "Tausczik and Pennebaker.,? \\Q2010\\E", "shortCiteRegEx": "Tausczik and Pennebaker.", "year": 2010}, {"title": "Supporting the Use of User Generated Content in Journalistic Practice", "author": ["Peter Tolmie", "Rob Procter", "Dave Randall", "Mark Rouncefield", "Christian Burger", "Geraldine Wong Sak Hoi", "Arkaitz Zubiaga", "Maria Liakata"], "venue": "In ACM Conference on Human Factors and Computing Systems", "citeRegEx": "Tolmie et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tolmie et al\\.", "year": 2017}, {"title": "An Analysis of Event-Agnostic Features for Rumour Classification in Twitter", "author": ["Laura Tolosi", "Andrey Tagarev", "Georgi Georgiev"], "venue": "In ICWSM Workshop on Social Media in the Newsroom", "citeRegEx": "Tolosi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tolosi et al\\.", "year": 2016}, {"title": "An Efficient Randomized Algorithm for Rumor Blocking in Online Social Networks", "author": ["Guangmo Tong", "Weili Wu", "Ling Guo", "Deying Li", "Cong Liu", "Bin Liu", "Ding-Zhu Du"], "venue": null, "citeRegEx": "Tong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2017}, {"title": "Social media, politics and the state: protests, revolutions, riots, crime and policing in the age of Facebook, Twitter and YouTube", "author": ["Daniel Trottier", "Christian Fuchs"], "venue": null, "citeRegEx": "Trottier and Fuchs.,? \\Q2014\\E", "shortCiteRegEx": "Trottier and Fuchs.", "year": 2014}, {"title": "Event-based media processing and analysis: A survey of the literature", "author": ["Christos Tzelepis", "ZhigangMa", "Vasileios Mezaris", "Bogdan Ionescu", "Ioannis Kompatsiaris", "Giulia Boato", "Nicu Sebe", "Shuicheng Yan"], "venue": "Image and Vision Computing", "citeRegEx": "Tzelepis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tzelepis et al\\.", "year": 2016}, {"title": "The culture of connectivity: A critical history of social media", "author": ["Jos\u00e9 Van Dijck"], "venue": null, "citeRegEx": "Dijck.,? \\Q2013\\E", "shortCiteRegEx": "Dijck.", "year": 2013}, {"title": "Microblogging during two natural hazards events: what twitter may contribute to situational awareness", "author": ["Sarah Vieweg", "Amanda L Hughes", "Kate Starbird", "Leysia Palen"], "venue": "In Proceedings of the SIGCHI conference on human factors in computing systems", "citeRegEx": "Vieweg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vieweg et al\\.", "year": 2010}, {"title": "Automatic detection and verification of rumors on Twitter", "author": ["Soroush Vosoughi"], "venue": "Ph.D. Dissertation. Citeseer", "citeRegEx": "Vosoughi.,? \\Q2015\\E", "shortCiteRegEx": "Vosoughi.", "year": 2015}, {"title": "Modeling Rumors in Twitter: An Overview", "author": ["Rhythm Walia", "MPS Bhatia"], "venue": "International Journal of Rough Sets and Data Analysis (IJRSDA) 3,", "citeRegEx": "Walia and Bhatia.,? \\Q2016\\E", "shortCiteRegEx": "Walia and Bhatia.", "year": 2016}, {"title": "That is your evidence?: Classifying stance in online political debate", "author": ["Marilyn A Walker", "Pranav Anand", "Rob Abbott", "Jean E Fox Tree", "Craig Martell", "Joseph King"], "venue": "Decision Support Systems 53,", "citeRegEx": "Walker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Perspectives on crowdsourcing annotations for natural language processing", "author": ["Aobo Wang", "Cong Duy Vu Hoang", "Min-Yen Kan"], "venue": "Language resources and evaluation 47,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "ECNU at SemEval-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models", "author": ["Feixiang Wang", "Man Lan", "Yuanbin Wu"], "venue": "In Proceedings of SemEval. ACL", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Detecting rumor patterns in streaming social media", "author": ["Shihan Wang", "Takao Terano"], "venue": "In Big Data (Big Data),", "citeRegEx": "Wang and Terano.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Terano.", "year": 2015}, {"title": "Detection of agreement and disagreement in broadcast conversations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Association for Computational Linguistics, 374\u2013378", "author": ["Wen Wang", "Sibel Yaman", "Kristin Precoda", "Colleen Richey", "Geoffrey Raymond"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Adaptive identification of hashtags for real-time event data collection", "author": ["Xinyue Wang", "Laurissa Tokarchuk", "Felix Cuadrado", "Stefan Poslad"], "venue": "In Recommendation and Search in Social Networks", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Digital Wildfires: Propagation, Verification, Regulation, and Responsible Innovation", "author": ["H. Webb", "P. Burnap", "R. Procter", "O. Rana", "B.C. Stahl", "M. Williams", "W. Housley", "A. Edwards", "M. Jirotka"], "venue": "ACM Transactions on Information Systems 34,", "citeRegEx": "Webb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2016}, {"title": "Social media as information source: Recency of updates and credibility of information", "author": ["David Westerman", "Patric R Spence", "Brandon Van Der Heide"], "venue": "Journal of Computer-Mediated Communication", "citeRegEx": "Westerman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Westerman et al\\.", "year": 2014}, {"title": "False rumors detection on sina weibo by propagation structures", "author": ["KeWu", "Song Yang", "Kenny Q Zhu"], "venue": "In 2015 IEEE 31st International Conference on Data Engineering", "citeRegEx": "KeWu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "KeWu et al\\.", "year": 2015}, {"title": "Automatic detection of rumor on Sina Weibo", "author": ["Fan Yang", "Yang Liu", "Xiaohui Yu", "Min Yang"], "venue": "In Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics. ACM,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Exploiting the topology property of social network for rumor detection", "author": ["YeKang Yang", "Kai Niu", "ZhiQiang He"], "venue": "In Computer Science and Software Engineering (JCSSE),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Emergency knowledge management and social media technologies: A case study of the 2010", "author": ["Dave Yates", "Scott Paquette"], "venue": "Haitian earthquake. International journal of information management 31,", "citeRegEx": "Yates and Paquette.,? \\Q2011\\E", "shortCiteRegEx": "Yates and Paquette.", "year": 2011}, {"title": "Using social media to enhance emergency situation awareness", "author": ["Jie Yin", "Andrew Lampert", "Mark Cameron", "Bella Robinson", "Robert Power"], "venue": "IEEE Intelligent Systems 27,", "citeRegEx": "Yin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2012}, {"title": "Unconfirmed: Classifying Rumor Stance in Crisis-Related Social Media Messages", "author": ["Li Zeng", "Kate Starbird", "Emma S Spiro"], "venue": "In Tenth International AAAI Conference on Web and Social Media", "citeRegEx": "Zeng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}, {"title": "Predictors of the authenticity of Internet health rumours", "author": ["Zili Zhang", "Ziqiong Zhang", "Hengyun Li"], "venue": "Health Information & Libraries Journal 32,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Enquiring minds: Early detection of rumors in social media from enquiry posts", "author": ["Zhe Zhao", "Paul Resnick", "Qiaozhu Mei"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Tweet, but verify: epistemic study of information verification on twitter", "author": ["Arkaitz Zubiaga", "Heng Ji"], "venue": "Social Network Analysis and Mining", "citeRegEx": "Zubiaga and Ji.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga and Ji.", "year": 2014}, {"title": "Curating and contextualizing twitter stories to assist with social newsgathering", "author": ["Arkaitz Zubiaga", "Heng Ji", "Kevin Knight"], "venue": "In Proceedings of the 2013 international conference on Intelligent user interfaces", "citeRegEx": "Zubiaga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2013}, {"title": "Stance classification in rumours as a sequential task exploiting the tree structure of social media conversations", "author": ["Arkaitz Zubiaga", "Elena Kochkina", "Maria Liakata", "Rob Procter", "Michal Lukasik"], "venue": "In Proceedings of the 26th International Conference on Computational Linguistics", "citeRegEx": "Zubiaga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}, {"title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter"], "venue": null, "citeRegEx": "Zubiaga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}, {"title": "Crowdsourcing the annotation of rumourous conversations in social media", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Kalina Bontcheva", "Peter Tolmie"], "venue": "In Proceedings of the 24th International Conference on World Wide Web Companion", "citeRegEx": "Zubiaga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}, {"title": "Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads", "author": ["Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie"], "venue": "PLoS ONE 11,", "citeRegEx": "Zubiaga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 31, "context": "Hence, social media has become a powerful tool for journalists [Diakopoulos et al. 2012; Tolmie et al. 2017a] but also for ordinary citizens [Hermida 2010].", "startOffset": 63, "endOffset": 109}, {"referenceID": 151, "context": "However, while social media provides access to an unprecedented source of information, the absence of systematic efforts by platforms to moderate posts also leads to the spread of misinformation [Procter et al. 2013b; Webb et al. 2016], which then requires extra effort to establish their provenance and veracity.", "startOffset": 195, "endOffset": 235}, {"referenceID": 86, "context": "Nevertheless, despite this apparent robustness of social media, its increasing tendency to give rise to rumours motivates the development of systems that, by gathering and analysing the collective judgements of users [Lukasik et al. 2016], are able to reduce the spread of rumours by accelerating the sense-making process [Derczynski and Bontcheva 2014].", "startOffset": 217, "endOffset": 238}, {"referenceID": 160, "context": "A rumour detection system that identifies, in its early stages, postings whose veracity status is unverified, can be effectively used to warn users that the information in them may turn out to be false [Zhao et al. 2015].", "startOffset": 202, "endOffset": 220}, {"referenceID": 93, "context": "of collective sense-making [Metaxas et al. 2015].", "startOffset": 27, "endOffset": 48}, {"referenceID": 15, "context": "[Cai et al. 2014; Liang et al. 2015]), while the majority of the literature defines rumours instead as \u201cunverified and instrumentally relevant information statements in circulation\u201d [DiFonzo and Bordia 2007].", "startOffset": 0, "endOffset": 36}, {"referenceID": 78, "context": "[Cai et al. 2014; Liang et al. 2015]), while the majority of the literature defines rumours instead as \u201cunverified and instrumentally relevant information statements in circulation\u201d [DiFonzo and Bordia 2007].", "startOffset": 0, "endOffset": 36}, {"referenceID": 163, "context": "One may rely on many different factors when classifying rumours by type, including its eventual veracity value (true, false or unresolved) [Zubiaga et al. 2016] or its degree of credibility (e.", "startOffset": 139, "endOffset": 160}, {"referenceID": 62, "context": "high or low) [Jaeger et al. 1980].", "startOffset": 13, "endOffset": 33}, {"referenceID": 109, "context": "Rumours and related phenomena have been studied from many different perspectives [Donovan 2007], ranging from psychological studies [Rosnow and Foster 2005] to computational analyses [Qazvinian et al. 2011].", "startOffset": 183, "endOffset": 206}, {"referenceID": 62, "context": "[Jaeger et al. 1980] found that rumours were passed on more frequently when the believability level was high.", "startOffset": 0, "endOffset": 20}, {"referenceID": 62, "context": "[Jaeger et al. 1980] and Scanlon [Scanlon 1977] found the importance of a rumour as perceived by recipients to be a factor that determineswhether or not it is spread, the least important rumours being spread more.", "startOffset": 0, "endOffset": 20}, {"referenceID": 134, "context": "[Takayasu et al. 2015] used social media to study the diffusion of a rumour in the context of the 2011 Japan Earthquake, which stated that rain in the aftermath might include harmful chemical", "startOffset": 0, "endOffset": 22}, {"referenceID": 18, "context": "[Castillo et al. 2013] found that the ratio between tweets supporting and debunking false rumours was 1:1 (one supporting tweet per debunking tweet) in the case of a 2010 earthquake in Chile.", "startOffset": 0, "endOffset": 22}, {"referenceID": 130, "context": "[Starbird et al. 2014] found that Twitter users did not do so well in distinguishing between the truth and hoaxes.", "startOffset": 0, "endOffset": 22}, {"referenceID": 163, "context": "Delving further into temporal aspects of rumour diffusion and support, [Zubiaga et al. 2016] describe the analysis of rumours circulating during nine breaking news events.", "startOffset": 71, "endOffset": 92}, {"referenceID": 67, "context": "Social media platforms have shown great potential for news diffusion, occasionally even outpacing professional news outlets in breaking news reporting [Kwak et al. 2010].", "startOffset": 151, "endOffset": 169}, {"referenceID": 31, "context": "This enables, among others, access to updates from eyewitnesses and a broad range of users who have access to potentially exclusive information [Diakopoulos et al. 2012; Starbird et al. 2012].", "startOffset": 144, "endOffset": 191}, {"referenceID": 131, "context": "This enables, among others, access to updates from eyewitnesses and a broad range of users who have access to potentially exclusive information [Diakopoulos et al. 2012; Starbird et al. 2012].", "startOffset": 144, "endOffset": 191}, {"referenceID": 162, "context": "Aiming to exploit this feature of social media platforms, researchers have looked into the development of tools for news gathering [Zubiaga et al. 2013; Diakopoulos et al. 2012; Marcus et al. 2011], analysed the use of user-generated content (UGC) for news reporting [Hermida and Thurman 2008; Tolmie et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 31, "context": "Aiming to exploit this feature of social media platforms, researchers have looked into the development of tools for news gathering [Zubiaga et al. 2013; Diakopoulos et al. 2012; Marcus et al. 2011], analysed the use of user-generated content (UGC) for news reporting [Hermida and Thurman 2008; Tolmie et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 90, "context": "Aiming to exploit this feature of social media platforms, researchers have looked into the development of tools for news gathering [Zubiaga et al. 2013; Diakopoulos et al. 2012; Marcus et al. 2011], analysed the use of user-generated content (UGC) for news reporting [Hermida and Thurman 2008; Tolmie et al.", "startOffset": 131, "endOffset": 197}, {"referenceID": 61, "context": "The use of social media during emergencies and crises has also increased substantially in recent years [Imran et al. 2015; Castillo 2016; Procter et al. 2013a], with applications such as getting reports from eyewitnesses or finding help-seekers.", "startOffset": 103, "endOffset": 159}, {"referenceID": 157, "context": "Social media has been found useful for information gathering and coordination in different situations, including emergencies [Yates and Paquette 2011; Yin et al. 2012; Procter et al. 2013a], protests [Trottier and Fuchs 2014; Agarwal et al.", "startOffset": 125, "endOffset": 189}, {"referenceID": 1, "context": "2013a], protests [Trottier and Fuchs 2014; Agarwal et al. 2014] and natural hazards [Vieweg et al.", "startOffset": 17, "endOffset": 63}, {"referenceID": 142, "context": "2014] and natural hazards [Vieweg et al. 2010; Middleton et al. 2014].", "startOffset": 26, "endOffset": 69}, {"referenceID": 95, "context": "2014] and natural hazards [Vieweg et al. 2010; Middleton et al. 2014].", "startOffset": 26, "endOffset": 69}, {"referenceID": 99, "context": "Social media is also being used by researchers to collect perceptions of users on a range of social issues, which can then be aggregated to measure public opinion [Murphy et al. 2014].", "startOffset": 163, "endOffset": 183}, {"referenceID": 42, "context": "Researchers attempt to clean social media data [Gao et al. 2014] and try to get rid of population biases [Olteanu et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 101, "context": "2014] and try to get rid of population biases [Olteanu et al. 2016] to understand how social media shapes society\u2019s perceptions on issues, products, people, etc.", "startOffset": 46, "endOffset": 67}, {"referenceID": 45, "context": "[Goodman et al. 2011].", "startOffset": 0, "endOffset": 21}, {"referenceID": 121, "context": "Social media have been found useful to measure public opinion during elections [Anstead and O\u2019Loughlin 2015], and the effect of online opinions on the offline world is being analysed, for instance, towards the reputation of organisations [Sung and Lee 2015] or towards different policies [Shi et al. 2014].", "startOffset": 288, "endOffset": 305}, {"referenceID": 20, "context": "For instance, sentiment expressed in tweets has been used to predict stock market reactions [Azar and Lo 2016], to collect opinions that investors post in social media [Chen et al. 2014] or to analyse the effect that social media posts can have on brands and products [Lee et al.", "startOffset": 168, "endOffset": 186}, {"referenceID": 71, "context": "2014] or to analyse the effect that social media posts can have on brands and products [Lee et al. 2015].", "startOffset": 87, "endOffset": 104}, {"referenceID": 152, "context": "Studies have looked at credibility perceptions of users [Westerman et al. 2014] and have also assessed the degree to which users rely on social media to gather information such as news [Gottfried and Shearer 2016].", "startOffset": 56, "endOffset": 79}, {"referenceID": 40, "context": "For other applications in social media mining it might just suffice to define filters that are already implemented in the APIs of social media platforms, such as: (1) filtering by keyword to collect data related to an event [Driscoll and Walker 2014]; (2) defining a bounding box to collect data posted from predefined geographical locations [Frias-Martinez et al. 2012]; or (3) listing a set of users of interest to track their posts [Li and Cardie 2014].", "startOffset": 342, "endOffset": 370}, {"referenceID": 109, "context": "For instance, posts can be collected for the rumour discussing whether Obama is muslim or not by using keywords like Obama and muslim to filter the posts [Qazvinian et al. 2011].", "startOffset": 154, "endOffset": 177}, {"referenceID": 165, "context": "Once the posts for an event are collected, one can then filter the tweets that are associated with rumours [Procter et al. 2013b; Zubiaga et al. 2015]; this can be done in two different ways by following top-down or bottom-up strategies, as we explain below.", "startOffset": 107, "endOffset": 150}, {"referenceID": 53, "context": "A solution for this is to use alternative API endpoints to collect posts through a less restrictive stream of data, such as Twitter\u2019s streaming API sampling a random 1% of the whole, or a filter of posts by geolocation, where available, to collect posts coming from a country or region [Han et al. 2014].", "startOffset": 286, "endOffset": 303}, {"referenceID": 36, "context": "The identification of changes in vocabulary during an event for improved data collection is, however, an open research issue [Earle et al. 2012; Wang et al. 2015].", "startOffset": 125, "endOffset": 162}, {"referenceID": 150, "context": "The identification of changes in vocabulary during an event for improved data collection is, however, an open research issue [Earle et al. 2012; Wang et al. 2015].", "startOffset": 125, "endOffset": 162}, {"referenceID": 109, "context": "This can apply to longstanding rumours, where one can define keywords to sample posts related a rumour known to have been circulating for a long time [Qazvinian et al. 2011], for retrospective sampling of rumours known to have emerged during an event [Procter et al.", "startOffset": 150, "endOffset": 173}, {"referenceID": 54, "context": "ated with those rumours [Hannak et al. 2014].", "startOffset": 24, "endOffset": 44}, {"referenceID": 163, "context": "This is an approach that was used first by [Zubiaga et al. 2016] and subsequently by [Giasemidis et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 44, "context": "2016] and subsequently by [Giasemidis et al. 2016].", "startOffset": 26, "endOffset": 50}, {"referenceID": 165, "context": "However, if one wants to use a bottom-up sampling strategy, then manual annotation work is needed to identify what constitutes a rumour and what a non-rumour [Zubiaga et al. 2015].", "startOffset": 158, "endOffset": 179}, {"referenceID": 163, "context": "This annotation process has been sometimes operationalised by enlisting the help of journalists with expertise in verification [Zubiaga et al. 2016].", "startOffset": 127, "endOffset": 148}, {"referenceID": 54, "context": "For instance, [Hannak et al. 2014] used Snopes.", "startOffset": 14, "endOffset": 34}, {"referenceID": 105, "context": "While some online sources like Truth-O-Meter and PolitiFact provide finer-grained labels for veracity, such as mostly true, half true and mostly false, these are usually reduced to three labels [Popat et al. 2016], namely true, false and optionally unverified.", "startOffset": 194, "endOffset": 213}, {"referenceID": 33, "context": "While annotation is increasingly being performed through crowdsourcing platforms for many natural language processing and data mining tasks [Doan et al. 2011; Wang et al. 2013], it is not as suitable for more challenging annotation tasks such as rumour veracity.", "startOffset": 140, "endOffset": 176}, {"referenceID": 146, "context": "While annotation is increasingly being performed through crowdsourcing platforms for many natural language processing and data mining tasks [Doan et al. 2011; Wang et al. 2013], it is not as suitable for more challenging annotation tasks such as rumour veracity.", "startOffset": 140, "endOffset": 176}, {"referenceID": 109, "context": "This has been operationalised by [Qazvinian et al. 2011] annotating tweets as supporting, denying or querying a rumour, while later work by [Procter et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 109, "context": "[Qazvinian et al. 2011] annotated rumours by relevance, where for instance for a rumour saying that Obama is muslim, a post that says Obama does seem to be muslim would be marked as relevant, while a post saying that Obama had a meeting with muslims would be marked as not relevant.", "startOffset": 0, "endOffset": 23}, {"referenceID": 163, "context": "For example, [Zubiaga et al. 2016] annotated rumourous tweets for certainty (certain, somewhat certain, uncertain) and evidentiality (first-hand experience, inclusion of URL, quotation of person/organisation, link to an image, quotation of unverifiable source, employment of reasoning, no evidence), along with support.", "startOffset": 13, "endOffset": 34}, {"referenceID": 163, "context": "In another study looking at conversations sparked by rumourous reports on Twitter, [Zubiaga et al. 2016] found that the prevalent tendency of social media users is to support and spread rumours, irrespective of their veracity value.", "startOffset": 83, "endOffset": 104}, {"referenceID": 92, "context": "In an earlier study, [Mendoza et al. 2010] had found strong correlations between rumour support and veracity, showing that a majority of users support true rumours, while a higher number of users denies false rumours.", "startOffset": 21, "endOffset": 42}, {"referenceID": 92, "context": "Despite the apparent contradiction between these studies, it is worth noting that [Mendoza et al. 2010] looked at the entire life cycle of a rumour and hence the aggregation leads to good correlations; in contrast, [Zubiaga et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 163, "context": "2010] looked at the entire life cycle of a rumour and hence the aggregation leads to good correlations; in contrast, [Zubiaga et al. 2016] focused on the early reactions to rumours, showing that users have problems in determining veracity in the early stages of a rumour.", "startOffset": 117, "endOffset": 138}, {"referenceID": 134, "context": "It has also been suggested that corrections are usually issued by news organisations and they can be sometimes widely spread [Takayasu et al. 2015; Arif et al. 2016; Andrews et al. 2016], especially if those corrections come from like-minded accounts [Hannak et al.", "startOffset": 125, "endOffset": 186}, {"referenceID": 6, "context": "It has also been suggested that corrections are usually issued by news organisations and they can be sometimes widely spread [Takayasu et al. 2015; Arif et al. 2016; Andrews et al. 2016], especially if those corrections come from like-minded accounts [Hannak et al.", "startOffset": 125, "endOffset": 186}, {"referenceID": 4, "context": "It has also been suggested that corrections are usually issued by news organisations and they can be sometimes widely spread [Takayasu et al. 2015; Arif et al. 2016; Andrews et al. 2016], especially if those corrections come from like-minded accounts [Hannak et al.", "startOffset": 125, "endOffset": 186}, {"referenceID": 54, "context": "2016], especially if those corrections come from like-minded accounts [Hannak et al. 2014] and occasionally even leading to deletion or unsharing of the original post [Frias-Martinez et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 40, "context": "2014] and occasionally even leading to deletion or unsharing of the original post [Frias-Martinez et al. 2012].", "startOffset": 82, "endOffset": 110}, {"referenceID": 75, "context": "However, corrections do not always have the same effect as the original rumours [Lewandowsky et al. 2012; Shin et al. 2016; Starbird et al. 2014], which reinforces the need to develop rumour classification systems that deal with newly emerging rumours.", "startOffset": 80, "endOffset": 145}, {"referenceID": 122, "context": "However, corrections do not always have the same effect as the original rumours [Lewandowsky et al. 2012; Shin et al. 2016; Starbird et al. 2014], which reinforces the need to develop rumour classification systems that deal with newly emerging rumours.", "startOffset": 80, "endOffset": 145}, {"referenceID": 130, "context": "However, corrections do not always have the same effect as the original rumours [Lewandowsky et al. 2012; Shin et al. 2016; Starbird et al. 2014], which reinforces the need to develop rumour classification systems that deal with newly emerging rumours.", "startOffset": 80, "endOffset": 145}, {"referenceID": 23, "context": "Rumour diffusion is often dependent on the strength of ties between users, where rumours are more likely to be spread across strong ties in a network [Cheng et al. 2013].", "startOffset": 150, "endOffset": 169}, {"referenceID": 70, "context": "Other studies looking at temporal patterns of rumours have suggested that their popularity tends to fluctuate over time in social media [Kwon et al. 2013; Kwon and Cha 2014; Lukasik et al. 2015b] and other platforms on the Internet [Jo 2002], but with a possibility of being discussed again later in time after rumour popularity fades.", "startOffset": 136, "endOffset": 195}, {"referenceID": 100, "context": "By using rumour theoretic approaches to examine factors that lead to expression of interest in tracking a rumour, [Oh et al. 2013] identified the lack of an official source and personal involvement as the most important factors, whereas other factors, such as anxiety, were not as important.", "startOffset": 114, "endOffset": 130}, {"referenceID": 80, "context": "[Liu et al. 2014] reinforced these findings suggesting that personal involvement was the most important factor.", "startOffset": 0, "endOffset": 17}, {"referenceID": 25, "context": "Analysing specific rumour messages on Twitter, [Chua et al. 2016] identified that tweets from established users with a larger follower network were spread the most.", "startOffset": 47, "endOffset": 65}, {"referenceID": 119, "context": "To read more about studies looking at the diffusion of rumours, we recommend the surveys by [Serrano et al. 2015] and [Walia and Bhatia 2016].", "startOffset": 92, "endOffset": 113}, {"referenceID": 118, "context": "Despite the increasing interest in analysing rumours in social media and building tools to deal with rumours that had been previously identified [Seo et al. 2012; Takahashi and Igata 2012], there has been very little work in automatic rumour detection.", "startOffset": 145, "endOffset": 188}, {"referenceID": 109, "context": "Some of the work in rumour detection [Qazvinian et al. 2011; Hamidian and Diab 2015; 2016] has been limited to finding rumours known a priori.", "startOffset": 37, "endOffset": 90}, {"referenceID": 160, "context": "The first work that tackled the detection of new rumours is that by [Zhao et al. 2015].", "startOffset": 68, "endOffset": 86}, {"referenceID": 160, "context": "Their approach led to improved performance over the baseline classifier by [Zhao et al. 2015], improving also a number of non-sequential classifiers compared as baselines, with a performance of 0.", "startOffset": 75, "endOffset": 93}, {"referenceID": 160, "context": "065 respectively for the classifier by [Zhao et al. 2015].", "startOffset": 39, "endOffset": 57}, {"referenceID": 137, "context": "Work by [Tolosi et al. 2016] using feature analysis on rumours across different events found it difficult to distinguish rumours and non-rumours as features change dramatically across events.", "startOffset": 8, "endOffset": 28}, {"referenceID": 91, "context": "[McCreadie et al. 2015] studied the feasibility of using a crowdsourcing platform to identify rumours and non-rumours in social media, finding that the annotators achieve high inter-annotator agreement.", "startOffset": 0, "endOffset": 23}, {"referenceID": 109, "context": "The most widely used dataset for rumour tracking is that by [Qazvinian et al. 2011], which includes over 10,000 tweets, associated with 5 different rumours, each tweet annotated for relevance towards the rumour as related or unrelated.", "startOffset": 60, "endOffset": 83}, {"referenceID": 163, "context": "While not specifically intended for rumour tracking, the dataset produced by [Zubiaga et al. 2016] provides over 4,500 tweets categorised by rumour.", "startOffset": 77, "endOffset": 98}, {"referenceID": 109, "context": "Despite early work by [Qazvinian et al. 2011] performing automated rumour tracking, few studies have subsequently followed their line of research when it comes to determining the relevance of tweets to rumours.", "startOffset": 22, "endOffset": 45}, {"referenceID": 109, "context": "[Qazvinian et al. 2011] use a manually generated Twitter data set containing 10K tweets to guide a supervised machine learning approach.", "startOffset": 0, "endOffset": 23}, {"referenceID": 109, "context": "Later work by [Hamidian and Diab 2015] also focused on a rumour tracker, using the dataset produced by [Qazvinian et al. 2011].", "startOffset": 103, "endOffset": 126}, {"referenceID": 13, "context": "Their approach relies on the Semantic Textual Similarity (STS) model proposed by [Guo and Diab 2012], which exploits WordNet [Miller 1995], Wiktionary and Brown clusters [Brown et al. 1992] to enhance the shortage of semantic meaning of a tweet.", "startOffset": 170, "endOffset": 189}, {"referenceID": 109, "context": "2%, outperforming the baseline score established earlier by [Qazvinian et al. 2011].", "startOffset": 60, "endOffset": 83}, {"referenceID": 63, "context": "The most relevant work to that of tracking newly emerging rumours is that conducted for event detection and tracking in social media [Jaidka et al. 2016].", "startOffset": 133, "endOffset": 153}, {"referenceID": 116, "context": "For instance, [Sayyadi et al. 2009] describe an event detection and tracking approach based on keyword graphs.", "startOffset": 14, "endOffset": 35}, {"referenceID": 83, "context": "Similar approaches to event tracking have been introduced by others, such as using a bipartite graph for topical word selection [Long et al. 2011], using text classification techniques to determine whether incoming data is related to a previously identified event or to a new one [Reuter and Cimiano 2012], and using similarity metrics [Tzelepis et al.", "startOffset": 128, "endOffset": 146}, {"referenceID": 140, "context": "2011], using text classification techniques to determine whether incoming data is related to a previously identified event or to a new one [Reuter and Cimiano 2012], and using similarity metrics [Tzelepis et al. 2016].", "startOffset": 195, "endOffset": 217}, {"referenceID": 109, "context": "The classification scheme to determine the stance of each post varies across different studies; while early work [Qazvinian et al. 2011] performed 2-way classification of Y = {supporting, denying}, later work performed 3-way classification [Lukasik et al.", "startOffset": 113, "endOffset": 136}, {"referenceID": 30, "context": "More details on the rumour stance classification task can be found in the report of the RumourEval shared task [Derczynski et al. 2017].", "startOffset": 111, "endOffset": 135}, {"referenceID": 109, "context": "Other datasets used in previous work include a dataset with over 10,000 tweets annotated for stance as support, deny or query by [Qazvinian et al. 2011] and the dataset annotated as affirm, deny, neutral, uncodable or unrelated by [Andrews et al.", "startOffset": 129, "endOffset": 152}, {"referenceID": 4, "context": "2011] and the dataset annotated as affirm, deny, neutral, uncodable or unrelated by [Andrews et al. 2016]; however, the latter two are not publicly available.", "startOffset": 84, "endOffset": 105}, {"referenceID": 145, "context": "Studies in this respect define stance as an overall position held by a person towards an object, idea or position [Somasundaran and Wiebe 2009; Walker et al. 2012].", "startOffset": 114, "endOffset": 163}, {"referenceID": 92, "context": "One of the pioneering studies in this task is reported by [Mendoza et al. 2010].", "startOffset": 58, "endOffset": 79}, {"referenceID": 109, "context": "The first study that tackles the stance classification automatically is reported by [Qazvinian et al. 2011].", "startOffset": 84, "endOffset": 107}, {"referenceID": 109, "context": "Similar to [Qazvinian et al. 2011], the work by [Hamidian and Diab 2015] reports rumour tracking and rumour stance classification by applying supervisedmachine learning using the dataset created by [Qazvinian et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 109, "context": "2011], the work by [Hamidian and Diab 2015] reports rumour tracking and rumour stance classification by applying supervisedmachine learning using the dataset created by [Qazvinian et al. 2011].", "startOffset": 169, "endOffset": 192}, {"referenceID": 50, "context": "However, instead of Bayesian classifiers the authors use the J48 decision tree implemented within the Weka platform [Hall et al. 2009].", "startOffset": 116, "endOffset": 134}, {"referenceID": 109, "context": "The features from [Qazvinian et al. 2011] are adopted and extended with time related information and the hashtag itself as a token instead of the semantic content of the hashtag as used by [Qazvinian et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 109, "context": "2011] are adopted and extended with time related information and the hashtag itself as a token instead of the semantic content of the hashtag as used by [Qazvinian et al. 2011].", "startOffset": 153, "endOffset": 176}, {"referenceID": 109, "context": "The authors compare the TLV approach to their own earlier system as well as to original features of [Qazvinian et al. 2011] and show that the TLV approach outperforms both baselines.", "startOffset": 100, "endOffset": 123}, {"referenceID": 81, "context": "[Liu et al. 2015] follow the resulting investigations of stances in rumours by [Mendoza et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 92, "context": "2015] follow the resulting investigations of stances in rumours by [Mendoza et al. 2010] and use stance as an additional feature to those reported in related work to tackle the veracity classification problem (see Section 9).", "startOffset": 67, "endOffset": 88}, {"referenceID": 109, "context": "For stance classification the authors adopt the approach of [Qazvinian et al. 2011] and compare it with a rule-based method briefly outlined by the authors.", "startOffset": 60, "endOffset": 83}, {"referenceID": 109, "context": "The experimentswere performed on the dataset reported by [Qazvinian et al. 2011].", "startOffset": 57, "endOffset": 80}, {"referenceID": 158, "context": "More recently, [Zeng et al. 2016] enrich the feature sets investigated by earlier studies by features determined through Linguistic Inquiry and Word Count (LIWC) dictionaries [Tausczik and Pennebaker 2010].", "startOffset": 15, "endOffset": 33}, {"referenceID": 86, "context": "[Lukasik et al. 2016] investigated Gaussian Processes as rumour stance classifier.", "startOffset": 0, "endOffset": 21}, {"referenceID": 163, "context": "The authors work on rumour data released by [Zubiaga et al. 2016] and report an accuracy of 67.", "startOffset": 44, "endOffset": 65}, {"referenceID": 30, "context": "Rumour stance classification for tree structured conversations has also been studied in the RumourEval shared task at SemEval 2017 [Derczynski et al. 2017].", "startOffset": 131, "endOffset": 155}, {"referenceID": 66, "context": "Most of the systems viewed this task as a 4-way single tweet classification task, with the exception of the best performing system by [Kochkina et al. 2017], as well as the systems by [Wang et al.", "startOffset": 134, "endOffset": 156}, {"referenceID": 147, "context": "2017], as well as the systems by [Wang et al. 2017] and [Singh et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 125, "context": "2017] and [Singh et al. 2017].", "startOffset": 10, "endOffset": 29}, {"referenceID": 125, "context": "The system by [Singh et al. 2017] takes as input pairs of source and reply tweets, whereas [Wang et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 147, "context": "2017] takes as input pairs of source and reply tweets, whereas [Wang et al. 2017] addressed class imbalance by decomposing the problem into a two step classification task, first distinguishing between comments and non-comments, to then classify non-comment tweets as one of support, deny or query.", "startOffset": 63, "endOffset": 81}, {"referenceID": 147, "context": "Half of the systems employed ensemble classifiers, where classification was obtained through majority voting [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Bahuleyan and Vechtomova 2017; Srivastava et al. 2017].", "startOffset": 109, "endOffset": 210}, {"referenceID": 129, "context": "Half of the systems employed ensemble classifiers, where classification was obtained through majority voting [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Bahuleyan and Vechtomova 2017; Srivastava et al. 2017].", "startOffset": 109, "endOffset": 210}, {"referenceID": 147, "context": "In some cases the ensembles were hybrid, consisting both of machine learning classifiers and manually created rules, with differential weighting of classifiers for different class labels [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Srivastava et al. 2017].", "startOffset": 187, "endOffset": 257}, {"referenceID": 129, "context": "In some cases the ensembles were hybrid, consisting both of machine learning classifiers and manually created rules, with differential weighting of classifiers for different class labels [Wang et al. 2017; Garc\u0131\u0301a Lozano et al. 2017; Srivastava et al. 2017].", "startOffset": 187, "endOffset": 257}, {"referenceID": 66, "context": "Three systems used deep learning, with [Kochkina et al. 2017] employing LSTMs for sequential classification, [Chen et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 22, "context": "2017] employing LSTMs for sequential classification, [Chen et al. 2017] using convolutional neural networks (CNN) for obtaining the repre-", "startOffset": 53, "endOffset": 71}, {"referenceID": 125, "context": "The remaining two systems by [Enayet and El-Beltagy 2017] and [Singh et al. 2017] used support vector machines with a linear and polynomical kernel respectively.", "startOffset": 62, "endOffset": 81}, {"referenceID": 147, "context": "Half of the systems invested in elaborate feature engineering, including cue words and expressions denoting Belief, Knowledge, Doubt and Denial [Bahuleyan and Vechtomova 2017] as well as Tweet domain features, including meta-data about users, hashtags and event specific keywords [Wang et al. 2017; Bahuleyan and Vechtomova 2017; Singh et al. 2017; Enayet and El-Beltagy 2017].", "startOffset": 280, "endOffset": 376}, {"referenceID": 125, "context": "Half of the systems invested in elaborate feature engineering, including cue words and expressions denoting Belief, Knowledge, Doubt and Denial [Bahuleyan and Vechtomova 2017] as well as Tweet domain features, including meta-data about users, hashtags and event specific keywords [Wang et al. 2017; Bahuleyan and Vechtomova 2017; Singh et al. 2017; Enayet and El-Beltagy 2017].", "startOffset": 280, "endOffset": 376}, {"referenceID": 22, "context": "The systems with the least elaborate features were [Chen et al. 2017] and [Garc\u0131\u0301a Lozano et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 129, "context": "2017] for CNNs (word embeddings), [Srivastava et al. 2017] (sparse word vectors as input to logistic regression) and [Kochkina et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 66, "context": "2017] (sparse word vectors as input to logistic regression) and [Kochkina et al. 2017] (average word vectors, punctuation, similarity between word vectors in current tweet, source tweet and previous tweet, presence of negation, picture, URL).", "startOffset": 64, "endOffset": 86}, {"referenceID": 147, "context": "Five out of the eight systems used pre-trained word embeddings, mostly Google News word2vec embeddings, whereas [Wang et al. 2017] used four different types of embeddings.", "startOffset": 112, "endOffset": 130}, {"referenceID": 160, "context": "While [Zhao et al. 2015] did not study stance classification per se, they developed an approach to look for querying tweets, which is one of the reaction types considered in stance classification.", "startOffset": 6, "endOffset": 24}, {"referenceID": 0, "context": "To classify agreement between question-answer (Q-A) message pairs in fora, [Abbott et al. 2011] used Naive Bayes as the classifier and [Rosenthal and McKeown 2015] used a logistic regression classifier.", "startOffset": 75, "endOffset": 95}, {"referenceID": 149, "context": "A sequential classifier like CRF has also been used to detect agreement and disagreement between speakers in broadcast debates [Wang et al. 2011].", "startOffset": 127, "endOffset": 145}, {"referenceID": 30, "context": "The dataset produced for RumourEval 2017 [Derczynski et al. 2017], a shared task that took place at SemEval 2017, includes over 300 rumours annotated for veracity as one of true, false or unverified.", "startOffset": 41, "endOffset": 65}, {"referenceID": 69, "context": "Another dataset suitable for veracity classification is the one released by [Kwon et al. 2017], which includes 51 true rumours and 60 false rumours.", "startOffset": 76, "endOffset": 94}, {"referenceID": 109, "context": "Other datasets, such as that by [Qazvinian et al. 2011], are not suitable for veracity classification, as all the rumours are false.", "startOffset": 32, "endOffset": 55}, {"referenceID": 17, "context": "Work by [Castillo et al. 2011] initiated research in this direction of determining the veracity of social media content.", "startOffset": 8, "endOffset": 30}, {"referenceID": 159, "context": ", determination of the believability or authority of its source [Zhang et al. 2015].", "startOffset": 64, "endOffset": 83}, {"referenceID": 100, "context": "However, others report that veracity is related to authority [Association et al. 2001; Oh et al. 2013] and hence Castillo et al.", "startOffset": 61, "endOffset": 102}, {"referenceID": 17, "context": "To study credibility perceptions, [Castillo et al. 2011] distinguish two types of microblog posts: \u2018NEWS\u2019, which report an event or fact that can be of interest to others, and \u2018CHAT\u2019, which is a message that is purely based on personal/subjective opinions and/or conversations among friends.", "startOffset": 34, "endOffset": 56}, {"referenceID": 70, "context": "[Kwon et al. 2013] propose new set of feature categories: temporal, structural and linguistic.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "As a baseline classifier, features proposed by [Castillo et al. 2011] are adopted.", "startOffset": 47, "endOffset": 69}, {"referenceID": 17, "context": "The best results using the baseline features adopted from [Castillo et al. 2011] are obtained using a SVM with 81.", "startOffset": 58, "endOffset": 80}, {"referenceID": 69, "context": "More recently, [Kwon et al. 2017] analyse feature stability over time and report that structural and temporal features distinguish true from false rumours over a long-term window.", "startOffset": 15, "endOffset": 33}, {"referenceID": 154, "context": "[Yang et al. 2012] tackle the veracity of microblogs on the Chinese microblogging platform Sina Weibo.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "For instance, adding the two features on top of the propagation-based features reported by [Castillo et al. 2011] leads to an increase of 6.", "startOffset": 91, "endOffset": 113}, {"referenceID": 155, "context": "Another study that tackles rumours in Sina Weibo is reported by [Yang et al. 2015].", "startOffset": 64, "endOffset": 82}, {"referenceID": 81, "context": "[Liu et al. 2015] use approaches reported by [Yang et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 154, "context": "2015] use approaches reported by [Yang et al. 2012] and [Castillo et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 17, "context": "2012] and [Castillo et al. 2011] as baseline systems and compare them against their proposed approach that make use of so called \u201cverification features\u201d.", "startOffset": 10, "endOffset": 32}, {"referenceID": 88, "context": "[Ma et al. 2015] proposed to model features over time.", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "On the Twitter dataset reported by [Castillo et al. 2011] the authors report 89% accuracy.", "startOffset": 35, "endOffset": 57}, {"referenceID": 17, "context": "Two methods reported by earlier studies [Castillo et al. 2011] and [Yang et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 154, "context": "2011] and [Yang et al. 2012] are adopted for the evaluation.", "startOffset": 10, "endOffset": 28}, {"referenceID": 11, "context": "The machine learning approach chosen by the authors is an SVM with a hybrid kernel technique consisting of random walk kernel [Borgwardt et al. 2005] and an RBF kernel.", "startOffset": 126, "endOffset": 149}, {"referenceID": 17, "context": "4% [Castillo et al. 2011] and 77.", "startOffset": 3, "endOffset": 25}, {"referenceID": 154, "context": "2% accuracy [Yang et al. 2012].", "startOffset": 12, "endOffset": 30}, {"referenceID": 44, "context": "[Giasemidis et al. 2016] report experiments run on 100 million tweets associated with 72 different rumours.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "[Chen et al. 2016] approach the rumour veracity classification from a different angle.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "[Chang et al. 2016] put the emphasis on the characteristics of users who post the rumours to determine the veracity.", "startOffset": 0, "endOffset": 19}, {"referenceID": 87, "context": "In a similar vein, [Ma et al. 2017] investigated the performance difference between bag-of-words (BoW) and word embedding representation of post contents and conclude that the BoW representation was superior to the embedding variant.", "startOffset": 19, "endOffset": 35}, {"referenceID": 159, "context": "[Zhang et al. 2015] investigate rumour veracity classification within the health domain.", "startOffset": 0, "endOffset": 19}, {"referenceID": 110, "context": "[Qin et al. 2016] aim to detect new rumours and propose two new feature categories to achieve this.", "startOffset": 0, "endOffset": 17}, {"referenceID": 138, "context": "Unlike the previous studies, [Tong et al. 2017] aim at blocking rumours rather than detecting them or marking tweets as true or false.", "startOffset": 29, "endOffset": 47}, {"referenceID": 30, "context": "Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 [Derczynski et al. 2017].", "startOffset": 99, "endOffset": 123}, {"referenceID": 147, "context": "Participants viewed the task either as either a three-way [Enayet and El-Beltagy 2017; Wang et al. 2017; Singh et al. 2017] or two-way [Chen et al.", "startOffset": 58, "endOffset": 123}, {"referenceID": 125, "context": "Participants viewed the task either as either a three-way [Enayet and El-Beltagy 2017; Wang et al. 2017; Singh et al. 2017] or two-way [Chen et al.", "startOffset": 58, "endOffset": 123}, {"referenceID": 22, "context": "2017] or two-way [Chen et al. 2017; Srivastava et al. 2017] single tweet classification task.", "startOffset": 17, "endOffset": 59}, {"referenceID": 129, "context": "2017] or two-way [Chen et al. 2017; Srivastava et al. 2017] single tweet classification task.", "startOffset": 17, "endOffset": 59}, {"referenceID": 86, "context": "2016b], stance classification [Lukasik et al. 2015a; Lukasik et al. 2016; Zubiaga et al. 2016a], contradiction detection [Lendvai et al.", "startOffset": 30, "endOffset": 95}, {"referenceID": 28, "context": "2016a; Lendvai and Reichel 2016], ontological modelling of rumours [Declerck et al. 2015], visualisation [Lendvai et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 163, "context": "2016b], analysis of social media rumours [Zubiaga et al. 2016] and studies of journalistic practices of the use of user-generated content [Tolmie et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 111, "context": "\u2014RumorLens [Resnick et al. 2014] is a one year research project that ran in 2014, funded by Google.", "startOffset": 11, "endOffset": 32}, {"referenceID": 160, "context": "More details on the rumour detection system developed in this project were published in [Zhao et al. 2015].", "startOffset": 88, "endOffset": 106}, {"referenceID": 39, "context": "\u2014TwitterTrails [Finn et al. 2014] is a project in the Social Informatics Lab at Wellesley College.", "startOffset": 15, "endOffset": 33}, {"referenceID": 38, "context": "[Finn et al. 2015; Metaxas et al. 2015]) exploring and characterising the diffusion of rumours.", "startOffset": 0, "endOffset": 39}, {"referenceID": 93, "context": "[Finn et al. 2015; Metaxas et al. 2015]) exploring and characterising the diffusion of rumours.", "startOffset": 0, "endOffset": 39}, {"referenceID": 120, "context": "\u2014Hoaxy [Shao et al. 2016] is a platform for the collection, detection and analysis of online misinformation and its related fact checking efforts.", "startOffset": 7, "endOffset": 25}, {"referenceID": 12, "context": "The project has produced a number of publications on journalistic verification practices concerning social media [Brandtzaeg et al. 2016], social media verification approaches [Andreadou et al.", "startOffset": 113, "endOffset": 137}, {"referenceID": 3, "context": "2016], social media verification approaches [Andreadou et al. 2015] and approaches to track down the location of social media users [Middleton and Krivcovs 2016].", "startOffset": 44, "endOffset": 67}, {"referenceID": 55, "context": "Details of the project have been published in [Hassan et al. 2015].", "startOffset": 46, "endOffset": 66}, {"referenceID": 31, "context": "\u2014Seriously Rapid Source Review [Diakopoulos et al. 2012] is a system that incorporates a number of advanced aggregations, computations and cues that can be helpful for journalists to find and assess sources in Twitter around breaking news events, such as finding eyewitnesses on the ground.", "startOffset": 31, "endOffset": 56}, {"referenceID": 49, "context": "\u2014TweetCred [Gupta and Kumaraguru 2012; Gupta et al. 2014] is a real-time, webbased system to assess credibility of content on Twitter.", "startOffset": 11, "endOffset": 57}, {"referenceID": 102, "context": "For more discussion on the issues we cover in this survey, we recommend the special issue on trust and veracity of information on social media of the ACM TOIS journal [Papadopoulos et al. 2016; Rijke 2016], Full Fact\u2019s report on The State of Automated Factchecking [Babakar and Moy 2016], reports and discussion on rumours on Snopes and Craig Silverman\u2019s books on rumours and journalistic verification practices [Silverman 2013; 2015a; 2015b].", "startOffset": 167, "endOffset": 205}], "year": 2017, "abstractText": "Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e. pieces of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how natural language processing and data mining techniques may be used to find ways of determining their veracity. In this survey we introduce and discuss two types of rumours that circulate on social media; long-standing rumours that circulate for long periods of time, and newly-emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far towards the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for detection and resolution of rumours.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}