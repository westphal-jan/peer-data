{"id": "1704.01889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2017", "title": "Conformative Filtering for Implicit Feedback Data", "abstract": "Implicit feedback is the simplest form of user feedback that can be used for product recommendations. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing work circumvents this problem by making various assumptions regarding non-consumed items that do not apply if the user has not consumed a product because he did not know it. In this post, we propose conformative filtering (CoF) as a new method to address the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them have consumed a product, it is most likely irrelevant to that taste. We use hierarchical latent tree analysis (HLTA) to identify flavor-based user groups and make recommendations for a user based on their membership in the groups. Experiments with real-world datasets from different areas of code 10 show that compared to other coelines, more than 10% performance is observed in Recelgene @.", "histories": [["v1", "Thu, 6 Apr 2017 15:31:48 GMT  (3275kb)", "http://arxiv.org/abs/1704.01889v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["farhan khawar", "nevin l zhang", "jinxing yu"], "accepted": false, "id": "1704.01889"}, "pdf": {"name": "1704.01889.pdf", "metadata": {"source": "CRF", "title": "Conformative Filtering for Implicit Feedback Data", "authors": ["Farhan Khawar", "Nevin L. Zhang", "Jinxing Yu"], "emails": ["fkhawar@cse.ust.hk", "lzhang@cse.ust.hk", "jyuat@cse.ust.hk", "Recall@5", "Recall@10"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n01 88\n9v 1\n[ cs\n.I R\n] 6\nA pr\nImplicit feedback is the simplest form of user feedback that can be used for item recommendation. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing works circumvent this problem by making various assumptions regarding the unconsumed items, which fail to hold when the user did not consume an item because she was unaware of it. In this paper we propose Conformative Filtering (CoF) as a novel method for addressing the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them consumed an item, then it is highly likely that the item is irrelevant to this taste. We use Hierarchical Latent Tree Analysis (HLTA) to identify taste-based user groups, and make recommendations for a user based on her memberships in the groups. Experiments on real-world datasets from different domains show that CoF has superior performance compared to other baselines and more than 10% improvement in Recall@5 and Recall@10 is observed."}, {"heading": "1 Introduction", "text": "With the advent of the online marketplace, an average user is presented with an un-ending choice of items to consume. These items can be the books bought, webpages clicked, songs listened, or the movies watched etc. Online stores and content providers no longer have to worry about the limited shelf space to display their items. However, too much choice is not always a luxury. It can often be an unwanted distraction or it can make it difficult\n\u2217Corresponding authors\nfor a user to find the items she desires. Therefore, from the space of all items we need to filter relevant items for a user.\nCollaborative filtering (CF) [Goldberg et al., 1992] is one commonly used technique to deal with the problem. Most research work on CF focuses on explicit feedback data, where users provide explicit ratings for items [Koren and Bell, 2015]. Items with high ratings are preferred over those with low ratings. In other words, items with high ratings are positive examples, while those with low ratings are negative examples. Unrated items are treated as missing data.\nIn practice, one often encounters implicit feedback data, where users do not explicitly rate items [Nichols, 1997]. Recommendations need to be made based on user\u2019s activities such as clicks, page views, and purchase actions. Those are positive-only data and contain information regarding which items were consumed. There is no information about the unconsumed items. In other words, there are no negative examples.\nIn comparison with CF with explicit feedback, CF with implicit feedback has received considerably less attention. A key issue with implicit feedback is how to deal with the lack of negative examples [Ricci et al., 2015]. We are unsure whether the user didn\u2019t consume an item because she didn\u2019t like it or because she never saw it. In this paper, we propose a novel method for addressing this issue. We start by identifying user groups with the same tastes. By a taste we mean the tendency to consume a certain collection of items such as comedy movies, pop songs, or spicy food. Those taste-based groups give us a nice way to deal with the lack of negative examples. While it is not justifiable to assume that non-consumption means disinterest for an individual user, it is relatively more reasonable to make that assumption for a group of users with the same taste: If many users share a taste and none of them have consumed an item before, then it is likely that the group is\nnot interested in the item.\nWe use HLTA [Chen et al., 2016; Liu et al., 2014] to identify taste-based user groups. When applied to implicit feedback data, HLTA obtains a hierarchy of binary latent variables by: (1) Identifying item co-consumption patterns (groups of items that tend to be consumed by the same customers, not necessarily at the same time) and introducing a latent variable for each pattern; (2) Identifying co-occurrence patterns of those patterns and introducing a latent variable; (3) Repeating step 2 until termination. Each of the latent variables identifies a soft group of users, just as the concept \u201cintelligence\u201d denotes a class of people.\nTo make recommendations, we choose the user groups from a certain level of the hierarchy and characterize each group by aggregating recent behaviors of its members. For a particular user, we perform inference on the learned model to determine her memberships in the groups, and predict her preferences by combining her memberships in the groups and the group characteristics. We call this method Conformative Filtering because a user is expected to conform to the behaviors of the groups she belongs to.\nThe rest of the paper is organized as follows: We start by reviewing existing methods for dealing with lack of negative feedback in Section 2 and the basics of latent tree models in Section 3. In Section 4 we show how latent tree models can be used to identify the taste-based user groups, and in Section 5 we present our conformative filtering method. Empirical results are given in Section 6, followed by concluding remarks."}, {"heading": "2 Related Work", "text": "CoF is similar to user-kNN [Ricci et al., 2010] in that they both predict a user\u2019s preferences based on past behaviors of similar users. There are two important differences. First, when a user belongs to multiple taste groups, as is usually the case, CoF uses information from all the users in those groups, while user-kNN uses information only from the users who are in all groups. To put it another way, CoF uses the union of the groups, while user-kNN uses their intersection. This is illustrated in Figure 1. Second, user-kNN is not model-based whereas CoF is. More specifically, the taste groups are the latent factors. An item is characterized by the frequencies it was consumed by members of the groups, and a user is characterized by her memberships in the groups. In comparison with matrix factorization, the latent factors in CoF are more interpretable. They also offer more flexibilities. For example, we can use recent behaviors of group members, instead of their entire consumption his-\ntories, when predict future behavior of the group.\nSeveral model-based methods have been previously proposed to deal with the lack of negative examples in implicit feedback. One idea is to have two parts in the loss function for matrix factorization. Let us call a user-item pair (u, i) a consumption pair if user u has consumed item i before, and a non-consumption pair otherwise. The first part of the loss function encourages the predicted score r\u0302ui to be close to 1 for consumption pairs, while the second part encourages the predicted score r\u0302ui to be close to 0 for nonconsumption pairs. The second part is associated with lower weights than the first part. Thus, non-consumption is essentially viewed as disinterest with low confidence [Hu et al., 2008; Pan et al., 2008].\nAlternatively, one can assign the same weight to the two parts of the loss function but include in the second part a subset of non-consumption pairs that are randomly sampled [Pan et al., 2008]. Heuristics are used to maximize the chance of the sampled non-consumption pairs being truly disinterest pairs. For example, one can assign higher sampling probabilities to non-consumption pairs (u, i) where the item i is unpopular or the user u is active.\nAnother idea is to assume that a user prefers consumed items over unconsumed items, as done in Bayesian personalized ranking (BPR)[Rendle et al., 2009]. The idea is realized by employing a loss function that depend negatively on the difference r\u0302ui \u2212 r\u0302uj between predicted scores for consumption pairs (u, i) and non-consumption pairs (u, j) for the same user u. Consequently, minimiz-\ning the loss function leads to high scores for consumption pairs and low scores for non-consumption pairs.\nThe assumptions behind all these methods are about the preferences of individual users. They fail to hold when a user u would have liked an item i but did not consume it only because she was unaware of it. In that case, it is incorrect to assume user u is not interested in item i, even with low confidence; it would be a mistake if the pair (u, i) is sampled as a negative example; and it is wrong to assume u prefers all her consumed items to item i. In contrast, the assumption behind CoF is about the preferences of groups of users. If a group is large enough, it is relatively safe to assume that most of the items have come to the attention of at least one of the group members, and hence relatively reasonable to assume that the items not consumed by any group members are not of interest to the group."}, {"heading": "3 Basics of Latent Tree Models", "text": "In this section, we briefly review the basics of latent tree models. The materials are borrowed from [Zhang and Poon, 2017].\nA latent tree model (LTM) is a tree-structured Bayesian network [Pearl, 1988], where the leaf nodes represent observed variables and the internal nodes represent latent variables. An example is shown in Figure 1 (a). All variables are assumed to be binary in this paper. The model parameters include a marginal distribution for the root Y1 and a conditional distribution for each of the other nodes given its parent. The product of the distributions defines a joint distribution over all the variables.\nBy changing the root from Y1 to Y2 in Figure 1 (a), we get another model shown in (b). The two models are equivalent in the sense that they represent the same set of distributions over the observed variables X1, . . . , X5 [Zhang, 2004]. It is not possible to distinguish between equivalent models based on data. This implies that edge orientations in LTMs are unidentifiable. It therefore makes more sense to talk about undirected LTMs, which is what we do in this paper. One example is shown in Figure 1 (c). It represents an equivalent class of directed models, which includes the two models shown in (a) and (b) as members. In implementation, an undirected model is represented using an arbitrary directed model in the equivalence class it represents.\nTo learn an LTM from a dataset, one needs to determine: (1) the number of latent variables, (2) the connections among all the variables, and (3) the probability distributions. A host of algorithms have been proposed. In particular, [Chen et al., 2016; Liu et al., 2014] have developed an algorithm for learning hierarchical latent tree\nmodels (HLTMs), in the context to topic detection, that has a layer of observed variables at the bottom and multiple layers of latent variables on the top. Their algorithm is called HLTA 1."}, {"heading": "4 Taste Group Detection", "text": "HLTA can be used to analyze implicit feedback data if we regard the items as words and the consumption history of a user as a document. We have run it on the Movielens1M dataset. A part of the resulting model is shown in Figure 3. The variables at the leaves are binary variables that indicate whether movies are consumed. The other variables are latent variables. Each latent variable partitions the users into two clusters. Hence, multiple partitions of the users are obtained [Chen et al., 2012; Liu et al., 2015]. Users in the same clusters have the tendency to consume same subsets of the items. So, multiple tastes are detected, with one identified by each latent variable.\nFor example, the movies \u201cArmageddon\u201d, \u201cGolden Eye\u201d and \u201cCon Air\u201d are grouped under the latent variable Z13, which reveals the pattern that the three movies tend to be co-consumed, i.e., watched by the same viewers. Z13 partitions all the users into two soft clusters. The relevant numerical information is shown in Table 1. The first cluster Z13 = s1 consists of 21% of the users. Users in this cluster have high probabilities of watching these movies. So, they have a taste for the three movies, which is a sub-collection of action-adventure-thrillermovies. In contrast, users in the second cluster Z13 = s0 have low\n1An implementation of HLTA is available at https://github.com/kmpoon/hlta.\nprobabilities of watching these movies and hence they do not possess the taste.\nSimilarly, Z14 identifies a group of users with a taste for the movies \u201cThe Seige\u201d, \u201cMask of Zorro\u201d, \u201cDaylight\u201d and \u201cThe River Wild\u201d. It is grouped together with Z13 under Z22, which indicates that the two tastes tend to be co-possessed, and Z22 identifies the users who tend to have both tastes.\nDifferent parts of the model represent different genres and a variety of tastes are detected. For example, we see that Z1148 represents users who consume childrenanimation movies like \u201cTarzan\u201d,\u201cRugrats\u201d,\u201cMulan\u201d and \u201cWinnie the Pooh and the Blustery Day\u201d and Z1147 represents users with taste for the children-comedy movies \u201cThe Great Muppet Caper\u201d, \u201cPete\u2019s Dragon\u201d and \u201cThe Muppets take Manhattan\u201d. Similarly, other parts of the model represent users who have taste for different genres like horror, documentary, sci-fi, noir etc."}, {"heading": "5 Conformative filtering", "text": "Suppose we have learned an HLTM m from implicit feedback data and suppose there are K latent variables\non the l-th level of the model, each with two states s0 and s1. Denote the latent variables as Zl1, . . . , ZlK . They give us K taste-based user groups Zl1 = s1, . . . , ZlK = s1, which will sometimes be denoted as G1, . . . , GK for simplicity. In this section, we show how these taste groups can be used for item recommendation."}, {"heading": "5.1 Taste Group Characterization", "text": "A natural way to characterize a user group is to aggregate past behaviors of the group members. The issue is somewhat complex for us because our user groups are soft clusters. Let I(i|u,D) be the indicator function which takes value 1 if user u has consumed item i before, according to the dataset D, and 0 otherwise. We determine the preference of a taste groupGk (i.e., Zlk = s1) on an item i as follows:\np(i|Gk,D) =\n\u2211\nu I(i|u,D)P (Zlk = s1|u,m) \u2211\nu P (Zlk = s1|u,m) , (1)\nwhere P (ZlK = s1|u,m) is the probability of user u being in the soft cluster Zlk = s1 according to the model m, and the summations are over all users.\nNote that p(i|Gk,D) = 0 if no users in Gk have consumed the item i before. In other words, we assume that a group is not interested in an item if none of the group members have consumed the item before. There is a risk of treating an item as a negative example because none of the group members have seen it. We argue that the probability of this happening is low when the group is large.\nTheorem 1. Let there be m people in a group and N items in total. Suppose each person picks n items with replacement randomly and independently. Let X be the total number of different items picked. For any two numbers q, p \u2208 [0, 1], we have\nP (X \u2265 qN) \u2265 p,\nif m \u2265 log\n(\n1\u2212q\u2212\n\u221a\n\u2212 log(1\u2212p) 2N\n)\nn log(1\u22121/N) .\nFor example, if each user randomly picks 20 items from a total of 1000, then with a group of only 136 users we can ensure that the probability of at least 90% of the items being seen by at least one person is at least 0.9.\nAlso note that the reason we determine the preferences of a group Gk is that we want to predict future behavior of the groupmembers. As such, we might want to use recent behaviors of the group members instead of their entire consumption histories. For example, we can choose to use the latest H consumptions for each user. The resulting subset of data is denoted as DH . As will be seen later,H has an impact on the quality of item recommendations."}, {"heading": "5.2 User Characterization", "text": "A user u is characterized using her memberships in the K clusters, i.e.,\nu = (P (Zl1 = s1|u,m), . . . , P (ZlK = s1|u,m)). (2)\nNote that m is a tree-structure model. All the K posterior probability values can be calculated by propagating messages over the tree twice [Pearl, 1988]. It takes time linear in the number of variables in the model, and hence linear in the number of items."}, {"heading": "5.3 Item Recommendation", "text": "To make recommendations, we first compute a score for each user-item pair, and, for each user, we recommend the items with the highest scores. The score r\u0302ui for a user-item pair (u, i) is computed by combining the\ntaste group characterizations and the memberships of u in those groups:\nr\u0302ui =\nK \u2211\nk=1\np(i|Gk,DH)P (Zlk = s1|u,m). (3)\nThe score is the inner product of two vectors \u2014 the user characterization vector u and the item characterization vector i = (p(i|G1,DH), . . . , p(i|GK ,DH)); that characterizes item i in terms of the preference scores of the K taste groups for i."}, {"heading": "6 Experiments", "text": "We performed experiments on three publicly available datasets from different domains. Each dataset comprised only of (user, item, time-stamp) tuples. Following [Wu et al., 2017], we split a dataset into training, validation and test subsets by time. This is so that all the training instances came before all the testing instances, which matches real-world scenarios better than splits that do not consider time. We tested on several splits of the datasets and the results were similar. In the following, we will only report the results on the split with 70% of the data for training, 15% for validation and 15% for test."}, {"heading": "6.1 Datasets", "text": "Here are the three datasets used in our experiments:\n\u2022 Movilens1M 2 is a dataset that contains the ratings given by users to the movies they watched.\n\u2022 Amazon baby [McAuley et al., 2015] is a dataset that consists of users providing ratings for the baby\nproducts they bought from Amazon. Following the suggestion of the dataset providers, we retained only users and items with more than 5 ratings.\n2 https://grouplens.org/datasets/movielens/1m/\n\u2022 Ta-feng 3 is an implicit feedback supermarket dataset which consists of buy events. Where a user\nbuying an item from the supermarket is an event.\nThe first two are explicit feedback data and were converted to implicit feedback data by ignoring the explicit ratings. Then, all the events of a user rating an item indicate that a user consumed (watched, bought) this item. Statistics about the datasets are found in Table 2."}, {"heading": "6.2 Setup", "text": "We compared CoF against the four popular baselines: weighted-user-KNN (W-UkNN), weighted-itemKNN (W-IkNN), weighted regularized matrix factorization (WRMF) [Hu et al., 2008], and Bayesian personalized ranking matrix factorization (BPRMF) [Rendle et al., 2009]. As mentioned in Section 2, WRMF and BPRMF were specifically proposed to deal with the lack of negative examples in implicit feedback. MyMediaLite [Gantner et al., 2011] implementation was used for all baselines.\nParameters of all methods were tuned based on the validation set. After parameter tuning we retrained all the models on the train and validation set and tested on the test set. For WRMF and BPRMF, we performed grid search for the latent factors over F \u2208 {10, 20, 40, 80, 160}, and for regularization over \u03bb = {10\u22124, 10\u22123, . . . , 102}. For WUkNN and W-IkNN, k was searched from the set {10, 20, 40, 80, 160, 320, 500}. For CoF we searched for H over {2, 3, 5, 10, 20, . . . , 100} and l over the number of levels inm. The number of latent factors on each level are automatically determined during model construction."}, {"heading": "6.3 Evaluation Measures", "text": "We used two popular metrics to evaluate the quality of the recommended lists for implicit feedback. They are briefly outlined below:\n\u2022 Recall@R: It is the fraction, among all items consumed by a user, of those that are placed at the top\nR positions in a recommended list.\n\u2022 NDCG : Discounted cumulative gain is defined as DCG =\n\u2211P i=1 reli log2(i+1) , where P is the total num-\nber of recommended items and reli \u2208 {0, 1} indicates whether the item at position i is consumed by the user. NDCG is DCG normalized by the ideal DCG of the list.\n3 http://www.bigdatalab.ac.cn/benchmark/bm/dd?data=Ta-Feng\nWhile Recall@R was calculated on the top R items, NDCG was calculated over the entire recommendation list that involved all unconsumed items.\nNote that Precision is known to be unsuitable for implicit data [Wang and Blei, 2011], because it treats all unconsumed items as negative examples. Therefore, Precision based metrics (Precision@R and Mean Average Precision) are not used in this paper."}, {"heading": "6.4 Main Results", "text": "The result are shown in Table 3. Recall@R is arguably the most important metric for implicit feedback. In the real-world scenario, one can recommend only a few items to a user, and one would hope the list contains as many items of interest to the user as possible. This is exactly what Recall@R measures.\nIn terms of Recall@R, CoF performed drastically better than all the baselines. For example, the Recall@5 and Recall@10 scores of CoF are more than 10% higher than those of the baselines on all the datasets. Note that the second best method is user-kNN, which is not a modelbased method. If we compare CoF to only model-based alternatives, the improvement is more than 20%.\nMoreover, for Movilens1M, CoF outperforms all competing methods by a large margin. This shows that when the data is less sparse, CoF is able to extract meaningful information much more effectively than other methods.\nIn terms of NDCG, CoF also performed better than the baselines in all cases. NDCG gives high score to recommendation lists in which relevant items have higher rank in the list. This indicates that the recommend lists (over all items) produced by CoF are of higher quality than those given by the baselines.\nUser-kNN comes the second in most cases and beats CoF in one case. It should be noted that CoF is model-based. The user and item characterization vectors can be computed offline. Hence, it scales up better than user-kNN."}, {"heading": "6.5 Impact of Parameters", "text": "There are two parameters in CoF: l and H . The first parameter l determines which level of the hierarchical model produced by HLTA to use. The larger the l, the fewer the number of latent factors. The second parameter H denotes the number of latest consumptions of a user to be used when characterizing user groups. Although both parameters are selected using the validation, it would be interesting to gain some insight of their impact on performance.\nFigure 4(a) shows the Recall@5 scores on Ta-feng as a\nTable 3: Performance evaluation on test-set: boldface denotes the best performance, underline denotes the second\nbest performance and parentheses contain the percentage improvement by CoF over the second best result. CoF outperforms other methods in all but one test case. Substantial improvement in Recall@5 and Recall@10 is observed.\nTa-feng Recall@5 Recall@10 Recall@20 NDCG CoF(l=1,H=40,K=2004) 0.05582(13.4%) 0.06627(12.8%) 0.07661(6.1%) 0.23359(17.4%) WRMF(F=10,\u03bb=10) 0.01854 0.02970 0.04632 0.17277 BPRMF(F=80,\u03bb=10\u22124) 0.04385 0.05154 0.06262 0.19468 W-UkNN(k=500) 0.04924 0.05875 0.07224 0.19897 W-IkNN(k=10) 0.02165 0.03319 0.04660 0.15752 Movilens1M CoF(l=1,H=10,K=1040) 0.04285(45.4%) 0.06952(41.2%) 0.10583(24%) 0.53959(0.7%) WRMF(F=20,\u03bb=1.0) 0.02840 0.04923 0.07713 0.52301 BPRMF(F=160,\u03bb=0.01) 0.02948 0.04916 0.08537 0.53589 W-UkNN(k=80) 0.02634 0.04577 0.07402 0.52251 W-IkNN(k=500) 0.02446 0.04298 0.07267 0.52027 Amazon Baby CoF(l=1,H=3,K=54) 0.02461(25%) 0.03953(11%) 0.05924(\u22121.8%) 0.17624(0.8%) WRMF(F=10,\u03bb=10) 0.01523 0.03272 0.05703 0.17480 BPRMF(F=80,\u03bb=0.1) 0.00985 0.02222 0.03902 0.16425 W-UkNN(k=500) 0.01969 0.03561 0.06031 0.17284 W-IkNN(k=500) 0.01616 0.02797 0.04341 0.16643\nfunction of H . The optimal value is 40. The score is low for H = 2 because the data used for user group characterization is too sparse. The score is also low for H = 200 because too much history is included and the data does not reflect the current interests of the groups.\nIn CoF, we can use the whole dataset for identifying user tastes and use only recent histories for characterizing user groups and hence the items. We consider it a major advantage and it is not shared by matrix factorization. If one uses the whole dataset in MF, the item characterization vectors would be influenced by transactions long ago that do not necessarily reflect users\u2019 current interests. If one use only recent history, valuable information is lost and the data, which is typically sparse, becomes even sparser.\nThe choice of H also influences global diversity[Adomavicius and Kwon, 2012] i.e., the total number of distinct items recommended. As can be seen in Figure 4(b), the diversity decreases with H . The reason is that, as H increases, group characteristics are influenced more and more by the most active users, and consequently the algorithm becomes more and more likely to recommend items favored by those users.\nFigure 4(c) and 4(d) show the Recall@5 and Diversity@5 scores respectively on Ta-feng as a function of l. We see that the scores decrease with l. As l increases, the number of latent variables decrease and the user tastes that they represent become more and more general. These results indicate that it is better to use more specific user tastes for group characterization than to use more general ones.\nJust because higher levels of the hierarchy do not yield better recommendations does not mean they are useless. They can be used to define broader categories of items and be used for category-aware recommendation. For example, if a user has watched many action movies and only a small number of comedies, then we should recommend more action movies than comedies. Obviously, only a small number of categories can be used in such a strategy. Further exploration along this line is planned for future work."}, {"heading": "7 Conclusion", "text": "Conformative filtering (CoF) is proposed as a novel method for dealing with the lack of negative examples in implicit feedback. The key assumption is that a user with a certain taste would be interested in the items that other users with the same taste have consumed before. The items not consumed by those users are hence negative examples for that taste. Taste user groups are detected using hierarchical latent tree analysis.\nThe assumption behind CoF is more justifiable than those behind existing alternative methods. In addition, it has several other advantages. In comparison with the userkNN, CoF is model-based while the latter is not. When predicting the interests of a target user, CoF uses past behaviors of other users who share at least one taste with the target user, while user-kNN relies on past behaviors of only those users who share all tastes with the target user. In comparison with matrix factorization (MF), CoF uses more interpretable latent factors. Moreover, it iden-\ntifies taste user groups using the entire consumption histories, while relying on only recent behaviors of group members to predict future interests of the group. MF does not allow such flexibility.\nIt has been empirically shown that CoF method substantially outperforms the alternative methods on realworld datasets from different domains. In particular, its Recall@5 and Recall@10 scores are 10% higher than those of user-kNN, and 20% higher than those of modelbased methods specifically proposed to deal with implicit feedback data."}, {"heading": "Acknowledgements", "text": "We thank Peixian Chen and Zhourong Chen for valuable discussions. Research on this article was supported by Hong Kong Research Grants Council under grants 16202515 and 16212516."}, {"heading": "Appendix: Proof of Theorem 1", "text": "Let xi = 1 when item i is picked by at least one of the m people, and 0 otherwise, and X =\n\u2211N i=1 xi. Note\nE(xi) = P (xi = 1) = 1\u2212 (1\u2212 1 N ) mn, so\nE(X) = E(\nN \u2211\ni=1\nxi) =\nN \u2211\ni=1\nE(xi) = N\n(\n1\u2212 (1\u2212 1\nN )mn\n)\n.\n(4)\nSince x1, x2, \u00b7 \u00b7 \u00b7 , xN are independent, by Hoeffding\u2019s inequality [Hoeffding, 1963], for \u2200t > 0\nP (X \u2264 E(X)\u2212 t) \u2264 exp\n(\n\u2212 2t2\nN\n)\n. (5)\nChoose t = \u221a\n\u2212N log(1\u2212p) 2 , then exp(\u2212 2t2 N ) = 1\u2212 p.\nIfm \u2265 log\n(\n1\u2212q\u2212\n\u221a\n\u2212 log(1\u2212p) 2N\n)\nn log(1\u22121/N) , we haveE(X)\u2212t \u2265 qN ,\nP (X \u2264 qN) \u2264 P (X \u2264 E(X)\u2212 t) \u2264 1\u2212 p (6)\nQED"}], "references": [{"title": "on Knowl", "author": ["Gediminas Adomavicius", "YoungOk Kwon. Improving aggregate recommendation diversity using ranking-based techniques. IEEE Trans"], "venue": "and Data Eng., 24(5):896\u2013911,May", "citeRegEx": "Adomavicius and Kwon. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Artificial Intelligence", "author": ["Tao Chen", "Nevin L Zhang", "Tengfei Liu", "Kin Man Poon", "Yi Wang. Model-based multidimensional clustering of categorical data"], "venue": "176(1):2246\u20132269,", "citeRegEx": "Chen et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Progressive em for latent tree models and hierarchical topic detection", "author": ["Chen et al", "2016] Peixian Chen", "Nevin L. Zhang", "Leonard K.M. Poon", "Zhourong Chen"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Mymedialite: A free recommender system library", "author": ["Gantner et al", "2011] Zeno Gantner", "Steffen Rendle", "Christoph Freudenthaler", "Lars Schmidt-Thieme"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "35(12):61\u201370", "author": ["David Goldberg", "David Nichols", "Brian M. Oki", "Douglas Terry. Using collaborative filtering to weave an information tapestry. Commun. ACM"], "venue": "December", "citeRegEx": "Goldberg et al.. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Journal of the American Statistical Association", "author": ["Wassily Hoeffding. Probability inequalities for sums of bounded random variables"], "venue": "58(301):13\u201330,March", "citeRegEx": "Hoeffding. 1963", "shortCiteRegEx": null, "year": 1963}, {"title": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky. Collaborative filtering for implicit feedback datasets"], "venue": "ICDM \u201908, pages 263\u2013272, Washington, DC, USA,", "citeRegEx": "Hu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 77\u2013118", "author": ["Yehuda Koren", "Robert Bell. Advances in Collaborative Filtering"], "venue": "Springer US, Boston, MA,", "citeRegEx": "Koren and Bell. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 256\u2013272", "author": ["Tengfei Liu", "Nevin L Zhang", "Peixian Chen. Hierarchical latent tree analysis for topic detection. In Joint European Conference on Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Liu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning", "author": ["Teng-Fei Liu", "Nevin L Zhang", "Peixian Chen", "April Hua Liu", "Leonard KM Poon", "Yi Wang. Greedy learning of latent tree models for multidimensional clustering"], "venue": "98(12):301\u2013330,", "citeRegEx": "Liu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Image-based recommendations on styles and substitutes", "author": ["McAuley et al", "2015] Julian McAuley", "Christopher Targett", "Qinfeng Shi", "Anton Van Den Hengel"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In Proceedings of the 5th DELOS Workshop on Filtering and Collaborative Filtering", "author": ["David M. Nichols. Implicit ratings", "filtering"], "venue": "Budapaest: ERCIM, volume 12,", "citeRegEx": "Nichols. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "One-class collaborative filtering", "author": ["Pan et al", "2008] Rong Pan", "Yunhong Zhou", "Bin Cao", "Nathan N. Liu", "Rajan Lukose", "Martin Scholz", "Qiang Yang"], "venue": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,", "citeRegEx": "Pearl. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "SchmidtThieme. Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Rendle et al", "2009] Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "USA", "author": ["Francesco Ricci", "Lior Rokach", "Bracha Shapira", "Paul B. Kantor. Recommender Systems Handbook. Springer-Verlag New York", "Inc.", "NY New York"], "venue": "1st edition,", "citeRegEx": "Ricci et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Recommender Systems: Introduction and Challenges", "author": ["Francesco Ricci", "Lior Rokach", "Bracha Shapira"], "venue": "pages 1\u201334. Springer US, Boston, MA,", "citeRegEx": "Ricci et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["Wang", "Blei", "2011] ChongWang", "David M. Blei"], "venue": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Recurrent recommender networks", "author": ["Wu et al", "2017] Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola", "How Jing"], "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2017\\E", "shortCiteRegEx": "al. et al\\.", "year": 2017}, {"title": "February 4-9", "author": ["Nevin L. Zhang", "Leonard K.M. Poon. Latent tree analysis. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"], "venue": "2017, San Francisco, California, USA., pages 4891\u20134898,", "citeRegEx": "Zhang and Poon. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research, 5(6):697\u2013723", "citeRegEx": "Zhang. 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "Collaborative filtering (CF) [Goldberg et al., 1992] is one commonly used technique to deal with the problem.", "startOffset": 29, "endOffset": 52}, {"referenceID": 7, "context": "Most research work on CF focuses on explicit feedback data, where users provide explicit ratings for items [Koren and Bell, 2015].", "startOffset": 107, "endOffset": 129}, {"referenceID": 11, "context": "In practice, one often encounters implicit feedback data, where users do not explicitly rate items [Nichols, 1997].", "startOffset": 99, "endOffset": 114}, {"referenceID": 16, "context": "A key issue with implicit feedback is how to deal with the lack of negative examples [Ricci et al., 2015].", "startOffset": 85, "endOffset": 105}, {"referenceID": 8, "context": "We use HLTA [Chen et al., 2016; Liu et al., 2014] to identify taste-based user groups.", "startOffset": 12, "endOffset": 49}, {"referenceID": 15, "context": "CoF is similar to user-kNN [Ricci et al., 2010] in that they both predict a user\u2019s preferences based on past behaviors of similar users.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Thus, non-consumption is essentially viewed as disinterest with low confidence [Hu et al., 2008; Pan et al., 2008].", "startOffset": 79, "endOffset": 114}, {"referenceID": 19, "context": "The materials are borrowed from [Zhang and Poon, 2017].", "startOffset": 32, "endOffset": 54}, {"referenceID": 13, "context": "A latent tree model (LTM) is a tree-structured Bayesian network [Pearl, 1988], where the leaf nodes represent observed variables and the internal nodes represent latent variables.", "startOffset": 64, "endOffset": 77}, {"referenceID": 20, "context": ", X5 [Zhang, 2004].", "startOffset": 5, "endOffset": 18}, {"referenceID": 8, "context": "In particular, [Chen et al., 2016; Liu et al., 2014] have developed an algorithm for learning hierarchical latent tree Table 1: Information about the latent variables Z13, that represents the taste for three action-adventure-thriller movies, and Z1147, that represents the taste for three children-animation movies.", "startOffset": 15, "endOffset": 52}, {"referenceID": 1, "context": "Hence, multiple partitions of the users are obtained [Chen et al., 2012; Liu et al., 2015].", "startOffset": 53, "endOffset": 90}, {"referenceID": 9, "context": "Hence, multiple partitions of the users are obtained [Chen et al., 2012; Liu et al., 2015].", "startOffset": 53, "endOffset": 90}, {"referenceID": 13, "context": "All the K posterior probability values can be calculated by propagating messages over the tree twice [Pearl, 1988].", "startOffset": 101, "endOffset": 114}, {"referenceID": 6, "context": "We compared CoF against the four popular baselines: weighted-user-KNN (W-UkNN), weighted-itemKNN (W-IkNN), weighted regularized matrix factorization (WRMF) [Hu et al., 2008], and Bayesian personalized ranking matrix factorization (BPRMF) [Rendle et al.", "startOffset": 156, "endOffset": 173}, {"referenceID": 0, "context": "The choice of H also influences global diversity[Adomavicius and Kwon, 2012] i.", "startOffset": 48, "endOffset": 76}], "year": 2017, "abstractText": "Implicit feedback is the simplest form of user feedback that can be used for item recommendation. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing works circumvent this problem by making various assumptions regarding the unconsumed items, which fail to hold when the user did not consume an item because she was unaware of it. In this paper we propose Conformative Filtering (CoF) as a novel method for addressing the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them consumed an item, then it is highly likely that the item is irrelevant to this taste. We use Hierarchical Latent Tree Analysis (HLTA) to identify taste-based user groups, and make recommendations for a user based on her memberships in the groups. Experiments on real-world datasets from different domains show that CoF has superior performance compared to other baselines and more than 10% improvement in Recall@5 and Recall@10 is observed.", "creator": "LaTeX with hyperref package"}}}