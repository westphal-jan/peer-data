{"id": "1704.02147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2017", "title": "Hierarchical Clustering: Objective Functions and Algorithms", "abstract": "Hierarchical clustering is a recursive division of a dataset into clusters of ever finer granularity. Motivated by the fact that most work on hierarchical clustering relied on the provision of algorithms rather than optimizing a particular goal, Dasgupta formulated similarity-based hierarchical clustering as a combinatorial optimization problem, in which a \"good\" hierarchical clustering minimizes a cost function. He showed that this cost function has certain desirable properties.", "histories": [["v1", "Fri, 7 Apr 2017 09:14:28 GMT  (71kb)", "http://arxiv.org/abs/1704.02147v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["vincent cohen-addad", "varun kanade", "frederik mallmann-trenn", "claire mathieu"], "accepted": false, "id": "1704.02147"}, "pdf": {"name": "1704.02147.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Clustering: Objective Functions and Algorithms", "authors": ["Vincent Cohen-Addad", "Varun Kanade", "Frederik Mallmann-Trenn"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n02 14\n7v 1\n[ cs\n.D S]\nWe take an axiomatic approach to defining \u2018good\u2019 objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a \u2018natural\u2019 ground-truth hierarchical clustering, the ground-truth clustering has an optimal value.\nEquipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog3{2 nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation. This aims at explaining the success of this heuristics in practice. Finally, we consider \u2018beyond-worst-case\u2019 scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta\u2019s cost function also has desirable properties for these inputs and we provide a simple algorithm that for graphs generated according to this model yields a 1 + o(1) factor approximation.\n1Charikar and Chatziafratis (2017) independently proved that the sparsest-cut based approach achieves a Op ? log nq approximation.\nContents"}, {"heading": "1 Introduction 2", "text": "1.1 Summary of Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Preliminaries 6", "text": "2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Ultrametrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "3 Quantifying Output Value: An Axiomatic Approach 9", "text": "3.1 Admissible Cost Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Characterizing Admissible Cost Functions . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2.1 Characterizing g that satisfy conditions of Theorem 3.4 . . . . . . . . . . . . 13 3.2.2 Characterizing Objective Functions for Dissimilarity Graphs . . . . . . . . . . 14"}, {"heading": "4 Similarity-Based Inputs: Approximation Algorithms 15", "text": ""}, {"heading": "5 Admissible Objective Functions and Algorithms for Random Inputs 17", "text": "5.1 A Random Graph Model For Hierarchical Clustering . . . . . . . . . . . . . . . . . . 17 5.2 Objective Functions and Ground-Truth Tree . . . . . . . . . . . . . . . . . . . . . . . 19 5.3 Algorithm for Clustering in the HSBM . . . . . . . . . . . . . . . . . . . . . . . . . . 21"}, {"heading": "6 Dissimilarity-Based Inputs: Approximation Algorithms 25", "text": "6.1 Average-Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 6.2 A Simple and Better Approximation Algorithm for Worst-Case Inputs . . . . . . . . 27"}, {"heading": "7 Perfect Ground-Truth Inputs and Beyond 29", "text": "7.1 Perfect Ground-Truth Inputs are Easy . . . . . . . . . . . . . . . . . . . . . . . . . . 29 7.2 A Near-Linear Time Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 7.3 Beyond Structured Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n8 Worst-Case Analysis of Common Heuristics 36"}, {"heading": "1 Introduction", "text": "A hierarchical clustering is a recursive partitioning of a dataset into successively smaller clusters. The input is a weighted graph whose edge weights represent pairwise similarities or dissimilarities between datapoints. A hierarchical clustering is represented by a rooted tree where each leaf represents a datapoint and each internal node represents a cluster containing its descendant leaves. Computing a hierarchical clustering is a fundamental problem in data analysis; it is routinely used to analyze, classify, and pre-process large datasets. A hierarchical clustering provides useful information about data that can be used, e.g., to divide a digital image into distinct regions of different granularities, to identify communities in social networks at various societal levels, or to determine the ancestral tree of life. Developing robust and efficient algorithms for computing hierarchical clusterings is of importance in several research areas, such as machine learning, big-data analysis, and bioinformatics.\nCompared to flat partition-based clustering (the problem of dividing the dataset into k parts), hierarchical clustering has received significantly less attention from a theory perspective. Partitionbased clustering is typically framed as minimizing a well-defined objective such as k-means, kmedians, etc. and (approximation) algorithms to optimize these objectives have been a focus of study for at least two decades. On the other hand, hierarchical clustering has rather been studied at a more procedural level in terms of algorithms used in practice. Such algorithms can be broadly classified into two categories, agglomerative heuristics which build the candidate cluster tree bottom up, e.g., average-linkage, single-linkage, and complete-linkage, and divisive heuristics which build the tree top-down, e.g., bisection k-means, recursive sparsest-cut etc. Dasgupta (2016) identified the lack of a well-defined objective function as one of the reasons why the theoretical study of hierarchical clustering has lagged behind that of partition-based clustering.\nDefining a Good Objective Function.\nWhat is a \u2018good\u2019 output tree for hierarchical clustering? Let us suppose that the edge weights represent similarities (similar datapoints are connected by edges of high weight)2. Dasgupta (2016) frames hierarchical clustering as a combinatorial optimization problem, where a good output tree is a tree that minimizes some cost function; but which function should that be? Each (binary) tree node is naturally associated to a cut that splits the cluster of its descendant leaves into the cluster of its left subtree on one side and the cluster of its right subtree on the other, and Dasgupta defines the objective to be the sum, over all tree nodes, of the total weight of edges crossing the cut multiplied by the cardinality of the node\u2019s cluster. In what sense is this good? Dasgupta argues that it has several attractive properties: (1) if the graph is disconnected, i.e., data items in different connected components have nothing to do with one another, then the hierarchical clustering that minimizes the objective function begins by first pulling apart the connected components from one another; (2) when the input is a (unit-weight) clique then no particular structure is favored and all binary trees have the same cost; and (3) the cost function also behaves in a desirable manner for data containing a planted partition. Finally, an attempt to generalize the cost function leads to functions that violate property (2).\nIn this paper, we take an axiomatic approach to defining a \u2018good\u2019 cost function. We remark that in many application, for example in phylogenetics, there exists an unknown \u2018ground truth\u2019 hierarchical clustering\u2014 the actual ancestral tree of life\u2014from which the similarities are generated (possibly with noise), and the goal is to infer the underlying ground truth tree from the available data. In this sense, a cluster tree is good insofar as it is isomorphic to the (unknown) ground-truth\n2This entire discussion can equivalently be phrased in terms of dissimilarities without changing the essence.\ncluster tree, and thus a natural condition for a \u2018good\u2019 objective function is one such that for inputs that admit a \u2018natural\u2019 ground-truth cluster tree, the value of the ground-truth tree is optimal. We provide a formal definition of inputs that admit a ground-truth cluster tree in Section 2.2.\nWe consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)). In Section 3 we characterize the \u2018good\u2019 objective functions in this class and call them admissible objective functions. We prove that for any objective function, for any ground-truth input, the ground-truth tree has optimal cost (w.r.t to the objective function) if and only if the objective function (1) is symmetric (independent of the left-right order of children), (2) is increasing in the cardinalities of the child clusters, and (3) for (unit-weight) cliques, has the same cost for all binary trees (Theorem 3.4). Dasgupta\u2019s objective function is admissible in terms of the criteria described above.\nIn Section 5, we consider random graphs that induce a natural clustering. This model can be seen as a noisy version of our notion of ground-truth inputs and a hierarchical stochastic block model. We show that the ground-truth tree has optimal expected cost for any admissible objective function. Furthermore, we show that the ground-truth tree has cost at most p1 ` op1qqOPT with high probability for the objective function introduced by Dasgupta (2016).\nAlgorithmic Results\nThe objective functions identified in Section 3 allow us to (1) quantitatively compare the performances of algorithms used in practice and (2) design better and faster approximation algorithms.3\nAlgorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive \u03c6-approximate sparsest cut algorithm, that recursively splits the input graph using a \u03c6-approximation to the sparsest cut problem, outputs a tree whose cost is at most Op\u03c6 log n \u00a8 OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive \u03c6-sparsest cut algorithm of Dasgupta gives an Op\u03c6q-approximation. In Section 4, we obtain an independent proof showing that the \u03c6-approximate sparsest cut algorithm is an Op\u03c6q-approximation (Theorem 4.1)4. Our proof is quite different from the proof of Charikar and Chatziafratis (2017) and relies on a charging argument. Combined with the celebrated result of Arora et al. (2009), this yields an Op ? log nq-approximation. The results stated here apply to Dasgupta\u2019s objective function; the approximation algorithms extend to other objective functions, though the ratio depends on the specific function being used. We conclude our analysis of the worst-case setting by showing that all the linkage-based algorithms commonly used in practice can perform rather poorly on worst-case inputs (see Sec. 8).\nAlgorithms for Dissimilarity Graphs: Many of the algorithms commonly used in practice, e.g., linkage-based methods, assume that the input is provided in terms of pairwise dissimilarity (e.g., points that lie in a metric space). As a result, it is of interest to understand how they fare when compared using admissible objective functions for the dissimilarity setting. When the edge\n3For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard. This directly applies to the admissible objective functions for the dissimilarity setting as well. Thus, the focus turns to developing approximation algorithms.\n4Our analysis shows that the algorithm achieves a 6.75\u03c6-approximation and the analysis of Charikar and Chatziafratis (2017) yields a 8\u03c6-approximation guarantee. This minor difference is of limited impact since the best approximation guarantee for sparsest-cut is Op ? log nq.\nweights of the input graph represent dissimilarities, the picture is considerably different from an approximation perspective. For the analogue of Dasgupta\u2019s objective function in the dissimilarity setting, we show that the average-linkage algorithm (see Algorithm 3) achieves a 2-approximation (Theorem 6.2). This stands in contrast to other practical heuristic-based algorithms, which may have an approximation guarantee as bad as \u2126pn1{4q (Theorem 8.6). Thus, using this objectivefunction based approach, one can conclude that the average-linkage algorithm is the more robust of the practical algorithms, perhaps explaining its success in practice. We also provide a new, simple, and better algorithm, the locally densest-cut algorithm,5 which we show gives a 3{2-approximation (Theorem 6.5). Our results extend to any admissible objective function, though the exact approximation factor depends on the specific choice.\nStructured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.2). Thus, we consider inputs that admit a \u2018natural\u2019 ground-truth cluster tree. For such inputs, we show that essentially all the practical algorithms do the right thing, in that they recover the ground-truth cluster tree. Since real-world inputs might exhibit a noisy structure, we consider more general scenarios:\n\u2022 We consider a natural generalization of the classic stochastic block model that generates random graphs with a hidden ground-truth hierarchical clustering. We provide a simple algorithm based on singular value decomposition (SVD) and agglomerative methods that achieves a p1 ` op1qq-approximation for Dasgupta\u2019s objective function (in fact, it recovers the ground-truth tree) with high probability. Interestingly, this algorithm is very similar to approaches used in practice for hierarchical clustering.\n\u2022 We introduce the notion of a \u03b4-adversarially perturbed ground-truth input, which can be viewed as being obtained from a small perturbation to an input that admits a natural ground truth cluster tree. This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012). We provide an algorithm that achieves a \u03b4-approximation in both the similarity and dissimilarity settings, independent of the objective function used as long as it is admissible according to the criteria used in Section 3."}, {"heading": "1.1 Summary of Our Contributions", "text": "Our work makes significant progress towards providing a more complete picture of objectivefunction based hierarchical clustering and understanding the success of the classic heuristics for hierarchical clustering.\n\u2022 Characterization of \u2018good\u2019 objective functions. We prove that for any ground-truth input, the ground-truth tree has strictly optimal cost for an objective function if and only if, the objective function (1) is symmetric (independent of the left-right order of children), (2) is monotone in the cardinalities of the child clusters, and (3) for unit-weight cliques, gives the same weight to all binary trees (Theorem 3.4). We refer to such objective functions as admissible; according to these criteria Dasgupta\u2019s objective function is admissible.\n\u2022 Worst-case approximation. First, for similarity-based inputs, we provide a new proof that the recursive \u03c6-approximate sparsest cut algorithm is an Op\u03c6q-approximation (hence an Op ? log nq-approximation) (Theorem 4.1) for Dasgupta\u2019s objective function. Second, for\n5We say that a cut pA,Bq is locally dense if moving a vertex from A to B or from to B to A does not increase the density of the cut. One could similarly define locally-sparsest-cut.\ndissimilarity-based inputs, we show that the classic average-linkage algorithm is a 2-approximation (Theorem 6.2), and provide a new algorithm which we prove is a 3{2-approximation (Theorem 6.5). All those results extend to other cost functions but the approximation ratio is function-dependent.\n\u2022 Beyond worst-case. First, stochastic models. We consider the hierarchical stochastic block model (Definition 5.1). We give a simple algorithm based on SVD and classic agglomerative methods that, with high probability, recovers the ground-truth tree and show that this tree has cost that is p1` op1qqOPT with respect to Dasgupta\u2019s objective function (Theorem 5.8). Second, adversarial models. We introduce the notion of \u03b4-perturbed inputs, obtained by a small adversarial perturbation to ground-truth inputs, and give a simple \u03b4-approximation algorithm (Theorem 7.8).\n\u2022 Perfect inputs, perfect reconstruction. For ground-truth inputs, we note that the algorithms used in practice (the linkage algorithms, the bisection 2-centers, etc.) correctly reconstruct a ground truth tree (Theorems 7.1, 7.3, 7.5). We introduce a simple, faster algorithm that is also optimal on ground-truth inputs (Theorem 7.7)."}, {"heading": "1.2 Related Work", "text": "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive \u03c6-sparsest-cut algorithm achieves an Op\u03c6 log nqapproximation. Dasgupta\u2019s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta\u2019s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation.\nCharikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis. They also proved that the recursive \u03c6-sparsest cut algorithm produces a hierarchical clustering with cost at most Op\u03c6OPTq; their techniques appear to be significantly different from ours. Additionally, Charikar and Chatziafratis (2017) introduce a spreading metric SDP relaxation for the hierarchical clustering problem introduced by Dasgupta that has integrality gap Op ? log nq and a spreading metric LP relaxation that yields an Oplog nq-approximation to the problem.\nOn hierarchical clustering more broadly. There is an extensive literature on hierarchical clustering and its applications. It will be impossible to discuss most of it here; for some applications the reader may refer to e.g., (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004). Algorithms for hierarchical clustering have received a lot of attention from a practical perspective. For a definition and overview of agglomerative algorithms (such as average-linkage, complete-linkage, and single-linkage) see e.g., (Friedman et al., 2001) and for divisive algorithms see e.g., Steinbach et al. (2000).\nMost previous theoretical work on hierarchical clustering aimed at evaluating the cluster tree output by the linkage algorithms using the traditional objective functions for partition-based clustering, e.g., considering k-median or k-means cost of the clusters induced by the top levels of the\ntree (see e.g., (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)). Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions.\nIn Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and Me\u0301moli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms.\nOur condition for inputs to have a ground-truth cluster tree, and especially their \u03b4-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal. It bears some similarities with the \u201cstrict separation\u201d condition of Balcan et al. (2008), while we do not require the separation to be strict, we do require some additional hierarchical constraints. There are a variety of stability conditions that aim at capturing some of the structure that realworld inputs may exhibit (see e.g., (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)). Some of them induce a condition under which an underlying clustering can be mostly recovered (see e.g., (Bilu and Linial, 2012; Balcan et al., 2009a, 2013) for deterministic conditions and e.g., Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions). Imposing other conditions allows one to bypass hardness-of-approximation results for classical clustering objectives (such as k-means), and design efficient approximation algorithms (see, e.g., (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)). Eldridge et al. (2016) also investigate the question of understanding hierarchical cluster trees for random graphs generated from graphons. Their goal is quite different from ours\u2014they consider the \u201csingle-linkage tree\u201d obtained using the graphon as the ground-truth tree and investigate how a cluster tree that has low merge distortion with respect to this be obtained.6 This is quite different from the approach taken in our work which is primarily focused on understanding performance with respect to admssible cost functions."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Notation", "text": "An undirected weighted graph G \u201c pV,E,wq is defined by a finite set of vertices V , a set of edges E \u010e ttu, vu | u, v P V u and a weight function w : E \u00d1 R`, where R` denotes non-negative real numbers. We will only consider graphs with positive weights in this paper. To simplify notation (and since the graphs are undirected) we let wpu, vq \u201c wpv, uq \u201c wptu, vuq. When the weights on the edges are not pertinent, we simply denote graphs as G \u201c pV,Eq. When G is clear from the context, we denote |V | by n and |E| by m. We define GrU s to be the subgraph induced by the nodes of U .\n6This is a simplistic characterization of their work. However, a more precise characterization would require introducing a lot of terminology from their paper, which is not required in this paper.\nA cluster tree or hierarchical clustering T for graph G is a rooted binary tree with exactly |V | leaves, each of which is labeled by a distinct vertex v P V .7 Given a graph G \u201c pV,Eq and a cluster tree T for G, for nodes u, v P V we denote by LCAT pu, vq the lowest common ancestor (furthest from the root) of u and v in T .\nFor any internal node N of T , we denote the subtree of T rooted at N by TN . 8 Moreover, for any node N of T , define V pNq to be the set of leaves of the subtree rooted at N . Additionally, for any two trees T1, T2, define the union of T1, T2 to be the tree whose root has two children C1, C2 such that the subtree rooted at C1 is T1 and the subtree rooted at C2 is T2. Finally, given a weighted graph G \u201c pV,E,wq, for any set of vertices A \u010e V , let wpAq \u201c\u0159 a,bPAwpa, bq and for any set of edges E0, let wpE0q \u201c \u0159 ePE0\nwpeq. Finally, for any sets of vertices A,B \u010e V , let wpA,Bq \u201c \u0159aPA,bPB wpa, bq."}, {"heading": "2.2 Ultrametrics", "text": "Definition 2.1 (Ultrametric). A metric space pX, dq is an ultrametric if for every x, y, z P X, dpx, yq \u010f maxtdpx, zq, dpy, zqu.\nSimilarity Graphs Generated from Ultrametrics\nWe say that a weighted graph G \u201c pV,E,wq is a similarity graph generated from an ultrametric, if there exists an ultrametric pX, dq, such that V \u010e X, and for every x, y P V, x \u2030 y, e \u201c tx, yu exists, and wpeq \u201c fpdpx, yqq, where f : R` \u00d1 R` is a non-increasing function.9\nDissimilarity Graphs Generated from Ultrametrics\nWe say that a weighted graph G \u201c pV,E,wq is a dissimilarity graph generated from an ultrametric, if there exists an ultrametric pX, dq, such that V \u010e X, and for every x, y P V, x \u2030 y, e \u201c tx, yu exists, and wpeq \u201c fpdpx, yqq, where f : R` \u00d1 R` is a non-decreasing function.\nMinimal Generating Ultrametric\nFor a weighted undirected graph G \u201c pV,E,wq generated from an ultrametric (either similarity or dissimilarity), in general there may be several ultrametrics and the corresponding function f mapping distances in the ultrametric to weights on the edges, that generate the same graph. It is useful to introduce the notion of a minimal ultrametric that generates G.\nWe focus on similarity graphs here; the notion of minimal generating ultrametric for dissimilarity graphs is easily obtained by suitable modifications. Let pX, dq be an ultrametric that generates G \u201c pV,E,wq and f the corresponding function mapping distances to similarities. Then we consider the ultrametric pV, rdq defined as follows: (i) rdpu, uq \u201c 0 and (ii) for u \u2030 v,\nrdpu, vq \u201c rdpv, uq \u201c max u1,v1 tdpu1, v1q | fpdpu1, v1qq \u201c fpdpu, vqqu (1)\nIt remains to be seen that pV, rdq is indeed an ultrametric. First, notice that by definition, rdpu, vq \u011b dpu, vq and hence clearly rdpu, vq \u201c 0 if and only if u \u201c v as d is the distance in an ultrametric.\n7In general, one can look at trees that are not binary. However, it is common practice to use binary trees in the context of hierarchical trees. Also, for results presented in this paper nothing is gained by considering trees that are not binary.\n8For any tree T , when we refer to a subtree T 1 (of T ) rooted at a node N , we mean the connected subgraph containing all the leaves of T that are descendant of N .\n9In some cases, we will say that e \u201c tx, yu R E, if wpeq \u201c 0. This is fine as long as fpdpx, yqq \u201c 0.\nThe fact that rd is symmetric is immediate from the definition. The only part remaining to check is the so called isosceles triangles with longer equal sides conditions\u2014the ultrametric requirement that for any u, v, w, dpu, vq \u010f maxtdpu,wq, dpv,wqu implies that all triangles are isosceles and the two sides that are equal are at least as large as the third side. Let u, v, w P V , and assume without loss of generality that according to the distance d of pV, dq, dpu,wq \u201c dpv,wq \u011b dpu, vq. From (1) it is clear that rdpu,wq \u201c rdpv,wq \u011b dpu,wq. Also, from (1) and the non-increasing nature of f it is clear that if dpu, vq \u010f dpu1, v1q, then rdpu, vq \u010f rdpu1, v1q. Thence, pV, rdq is an ultrametric. The advantage of considering the minimal ultrametric is the following: if D \u201c trdpu, vq | u, v P V, u \u2030 vu and W \u201c twpu, vq | u, v P V, u \u2030 vu, then the restriction of f from D \u00d1 W is actually a bijection. This allows the notion of a generating tree to be defined in terms of distances in the ultrametric or weights, without any ambiguity. Applying an analogous definition and reasoning yields a similar notion for the dissimilarity case.\nDefinition 2.2 (Generating Tree). Let G \u201c pV,E,wq be a graph generated by a minimal ultrametric pV, dq (either a similarity or dissimilarity graph). Let T be a rooted binary tree with |V | leaves and |V | \u00b4 1 internal nodes; let N denote the internal nodes and L the set of leaves of T and let \u03c3 : L \u00d1 V denote a bijection between the leaves of T and nodes of V . We say that T is a generating tree for G, if there exists a weight function W : N \u00d1 R`, such that for N1, N2 P N , if N1 appears on the path from N2 to the root, W pN1q \u010f W pN2q. Moreover for every x, y P V , wptx, yuq \u201c W pLCAT p\u03c3\u00b41pxq, \u03c3\u00b41pyqqq.\nThe notion of a generating tree defined above more or less corresponds to what is referred to as a dendogram in the machine learning literature (see e.g., (Carlsson and Me\u0301moli, 2010)). More formally, a dendogram is a rooted tree (not necessarily binary), where the leaves represent the datapoints. Every internal node in the tree has associated with it a height function h which is the distance between any pairs of datapoints for which it is the least common ancestor. It is a well-known fact that a set of points in an ultrametric can be represented using a dendogram (see e.g., (Carlsson and Me\u0301moli, 2010)). A dendogram can easily be modified to obtain a generating tree in the sense of Definition 2.2: an internal node with k children is replace by an arbitrary binary tree with k leaves and the children of the nodes in the dendogram are attached to these k leaves. The height h of this node is used to give the weight W \u201c fphq to all the k\u00b4 1 internal nodes added when replacing this node. Figure 1 shows this transformation.\nGround-Truth Inputs\nDefinition 2.3 (Ground-Truth Input.). We say that a graph G is a ground-truth input if it is a similarity or dissimilarity graph generated from an ultrametric. Equivalently, there exists a tree T\nthat is generating for G.\nMotivation. We briefly describe the motivation for defining graphs generated from an ultrametric as ground-truth inputs. We\u2019ll focus the discussion on similarity graphs, though essentially the same logic holds for dissimilarity graphs. As described earlier, there is a natural notion of a generating tree associated with graphs generated from ultrametrics. This tree itself can be viewed as a cluster tree. The clusters obtained using the generating tree have the property that any two nodes in the same cluster are at least as similar to each other as they are to points outside this cluster; and this holds at every level of granularity. Furthermore, as observed by Carlsson and Me\u0301moli (2010), many practical hierarchical clustering algorithms such as the linkage based algorithms, actually output a dendogram equipped with a height function, that corresponds to an ultrametric embedding of the data. While their work focuses on algorithms that find embeddings in ultrametrics, our work focuses on finding cluster trees. We remark that these problems are related but also quite different.\nFurthermore, our results show that the linkage algorithms (and some other practical algorithms), recover a generating tree when given as input graphs that are generated from an ultrametric. Finally, we remark that relaxing the notion further leads to instances where it is hard to define a \u2018natural\u2019 ground-truth tree. Consider a similarity graph generated by a tree-metric rather than an ultrametric, where the tree is the caterpillar graph on 5 nodes (see Fig. 2(a)). Then, it is hard to argue that the tree shown in Fig. 2(b) is not a more suitable cluster tree. For instance, D and E are more similar to each other than D is to B or A. In fact, it is not hard to show that by choosing a suitable function f mapping distances from this tree metric to similarities, Dasgupta\u2019s objective function is minimized by the tree shown in Fig. 2(b), rather than the \u2018generating\u2019 tree in Fig. 2(a)."}, {"heading": "3 Quantifying Output Value: An Axiomatic Approach", "text": ""}, {"heading": "3.1 Admissible Cost Functions", "text": "Let us focus on the similarity case; in this case we use cost and objective interchangeably. Let G \u201c pV,E,wq be an undirected weighted graph and let T be a cluster tree for graph G. We want to consider cost functions for cluster trees that capture the quality of the hierarchical clustering produced by T . Following the recent work of Dasgupta (2016), we adopt an approach in which a cost is assigned to each internal node of the tree T that corresponds to the quality of the split at that node.\nThe Axiom. A natural property we would like the cost function to satisfy is that a cluster tree T has minimum cost if and only if T is a generating tree for G. Indeed, the objective function can then be used to indicate whether a given tree is generating and so, whether it is an underlying ground-truth hierarchical clustering. Hence, the objective function acts as a \u201cguide\u201d for finding the correct hierarchical classification. Note that there may be multiple trees that are generating for the same graph. For example, if G \u201c pV,E,wq is a clique with every edge having the same weight then every tree is a generating tree. In these cases, all the generating tree are valid ground-truth hierarchical clusterings.\nFollowing Dasgupta (2016), we restrict the search space for such cost functions. For an internal node N in a clustering tree T , let A,B \u010e V be the leaves of the subtrees rooted at the left and right child of N respectively. We define the cost \u0393 of the tree T as the sum of the cost at every internal node N in the tree, and at an individual node N we consider cost functions \u03b3 of the form\n\u0393pT q \u201c \u00ff\nN\n\u03b3pNq, (2)\n\u03b3pNq \u201c \u02dc \u00ff\nxPA,yPB\nwpx, yq \u00b8 \u00a8 gp|A|, |B|q (3)\nWe remark that Dasgupta (2016) defined gpa, bq \u201c a` b.\nDefinition 3.1 (Admissible Cost Function). We say that a cost function \u03b3 of the form (2,3) is admissible if it satisfies the condition that for all similarity graphs G \u201c pV,E,wq generated from a minimal ultrametric pV, dq, a cluster tree T for G achieves the minimum cost if and only if it is a generating tree for G.\nRemark 3.2. Analogously, for the dissimilarity setting we define admissible value functions to be the functions of the form (2,3) that satisfy: for all dissimilarity graph G generated from a minimal ultrametric pV, dq, a cluster tree T for G achieves the maximum value if and only if it is a generating tree for G.\nRemark 3.3. The RHS of (3) has linear dependence on the weight of the cut pA,Bq in the subgraph of G induced by the vertex set AYB as well as on an arbitrary function of the number of leaves in the subtrees of the left and right child of the internal node creating the cut pA,Bq. For the purpose of hierarchical clustering this form is fairly natural and indeed includes the specific cost function introduced by Dasgupta (2016). We could define the notion of admissibility for other forms of the cost function similarly and it would be of interest to understand whether they have properties that are desirable from the point of view of hierarchical clustering."}, {"heading": "3.2 Characterizing Admissible Cost Functions", "text": "In this section, we give an almost complete characterization of admissible cost functions of the form (3). The following theorem shows that cost functions of this form are admissible if and only if they satisfy three conditions: that all cliques must have the same cost, symmetry and monotonicity.\nTheorem 3.4. Let \u03b3 be a cost function of the form (3) and let g be the corresponding function used to define \u03b3. Then \u03b3 is admissible if and only if it satisfies the following three conditions.\n1. Let G \u201c pV,E,wq be a clique, i.e., for every x, y P V , e \u201c tx, yu P E and wpeq \u201c 1 for every e P E. Then the cost \u0393pT q for every cluster tree T of G is identical.\n2. For every n1, n2 P N, gpn1, n2q \u201c gpn2, n1q.\n3. For every n1, n2 P N, gpn1 ` 1, n2q \u0105 gpn1, n2q.\nProof. We first prove the only if part and then the if part.\nOnly If Part: Suppose that \u03b3 is indeed an admissible cost function. We prove that all three conditions must be satisfied by \u03b3. 1. All cliques have same cost. We observe that a clique G \u201c pV,E,wq can be generated from an ultrametric. Indeed, let X \u201c V and let dpu, vq \u201c dpv, uq \u201c 1 for every u, v P X such that u \u2030 v and dpu, uq \u201c 0. Clearly, for f : R` \u00d1 R` that is non-increasing and satisfying fp1q \u201c 1, pV, dq is a minimal ultrametric generating G.\nLet T be any binary rooted tree with leaves labeled by V , i.e., a cluster tree for graph G. For any internal node N of T define W pNq \u201c 1 as the weight function. This satisfies the definition of generating tree (Defn. 2.2). Thus, every cluster tree T for G is generating and hence, by the definition of admissibility all of them must be optimal, i.e., they all must have exactly the same cost. 2. gpn1, n2q \u201c gpn2, n1q. This part follows more or less directly from the previous part. Let G be a clique on n1 ` n2 nodes. Let T be any cluster tree for G, with subtrees T1 and T2 rooted at the left and right child of the root respectively, such that T1 contains n1 leaves and T2 contains n2 leaves. The number of edges, and hence the total weight of the edges, crossing the cut induced by the root node of T is n1 \u00a8 n2. Let rT be a tree obtained by making T2 be rooted at the left child of the root and T1 at the right child. Clearly rT is also a cluster tree for G and induces the same cut at the root node, hence using the property that all cliques have the same cost, \u0393pT q \u201c \u0393p rT q. But \u0393pT q \u201c n1 \u00a8 n2 \u00a8 gpn1, n2q ` \u0393pT1q ` \u0393pT2q and \u0393p rT q \u201c n1 \u00a8 n2 \u00a8 gpn2, n1q ` \u0393pT1q ` \u0393pT2q. Thence, gpn1, n2q \u201c gpn2, n1q. 3. gpn1`1, n2q \u0105 gpn1, n2q. Consider a graph on n1`n2`1 nodes generated from an ultrametric as follows. Let V1 \u201c tv1, . . . , vn1u, V2 \u201c tv11, . . . , v1n2u and consider the ultrametric pV1 Y V2 Y tv\u02dau, dq defined by dpx, yq \u201c 1 if x \u2030 y and x, y P V1 or x, y P V2, dpx, yq \u201c 2 if x \u2030 y and x P V1, y P V2 or x P V2, y P V1, dpv\u02da, xq \u201c dpx, v\u02daq \u201c 3 for x P V1 YV2, and dpu, uq \u201c 0 for u P V1 YV2 Ytv\u02dau. It can be checked easily by enumeration that this is indeed an ultrametric. Furthermore, if f : R` \u00d1 R` is non-increasing and satisfies fp1q \u201c 2, fp2q \u201c 1 and fp3q \u201c 0, i.e., wptu, vuq \u201c 2 if u and v are both either in V1 or V2, wptu, vuq \u201c 1 if u P V1 and v P V2 or the other way around, and wptv\u02da, uuq \u201c 0 for u P V1 Y V2, then pV1 Y V2, tv\u02dau, dq is a minimal ultrametric generating G.\nNow consider two possible cluster trees defined as follows: Let T1 be an arbitrary tree on nodes V1, T2 and arbitrary tree on nodes V2. T is obtained by first joining T1 and T2 using internal node N and making this the left subtree of the root node \u03c1 and the right subtree of the root node is just the singleton node v\u02da. T 1 is obtained by first creating a tree by joining T1 and the singleton node v\u02da using internal node N 1, this is the left subtree of the root node \u03c11 and T2 is the right subtree of the root node. (See Figures 3a and 3b.)\nNow it can be checked that T is generating by defining the following weight function. For every internal node M of T1, let W pMq \u201c 1, similarly for every internal node M of T2, let W pMq \u201c 1, define W pNq \u201c 2 and W p\u03c1q \u201c 3. Now, we claim that T 1 cannot be a generating tree. This follows from the fact that for a node u P V1, v P V2, the root node \u03c11 \u201c LCAT 1pu, vq, but it is also the case that \u03c11 \u201c LCAT 1pv\u02da, vq. Thus, it cannot possibly be the case that W p\u03c1q \u201c wptu, vuq and W p\u03c1q \u201c wptv\u02da, vuq as wptu, vuq \u2030 wptv\u02da, vuq. By definition of admissibility, it follows that \u0393pT q \u0103 \u0393pT 1q, but \u0393pT q \u201c \u0393pT1q ` \u0393pT2q ` n1 \u00a8 n2 \u00a8 gpn1, n2q. The last term arises from the cut at node N ; the root makes no contribution as the cut at the root node \u03c1 has weight 0. On the other hand \u0393pT 1q \u201c \u0393pT1q ` \u0393pT2q ` n1 \u00a8 n2 \u00a8 gpn1 ` 1, n2q. There is no cost at the node N 1, since the cut\nhas size 0; however, at the root node the cost is now n1 \u00a8 n2 \u00a8 gpn1 ` 1, n2q as the left subtree at the root contains n1 ` 1 nodes. It follows that gpn1 ` 1, n2q \u0105 gpn1, n2q. If Part: For the other direction, we first use the following observation. By condition 2 in the statement of the theorem, every clique on n nodes has the same cost irrespective of the tree used for hierarchical clustering; let \u03bapnq denote said cost. Let n1, n2 \u011b 1, then we have,\nn1 \u00a8 n2 \u00a8 gpn1, n2q \u201c \u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2q (4)\nWe will complete the proof by induction on |V |. The minimum number of nodes required to have a cluster tree with at least one internal node is 2. Suppose |V | \u201c 2, then there is a unique (up to interchanging left and right children) cluster tree; this tree is also generating and hence by definition any cost function is admissible. Thus, the base case is covered rather easily.\nNow, consider a graph G \u201c pV,E,wq with |V | \u201c n \u0105 2. Let T \u02da be a tree that is generating. Suppose that T is any other tree. Let \u03c1\u02da and \u03c1 be the root nodes of the trees respectively. Let V \u02daL and V \u02daR be the nodes on the left subtree and right subtree of \u03c1\n\u02da; similarly VL and VR in the case of \u03c1. Let A \u201c V \u02daL X VL, B \u201c V \u02daL X VR, C \u201c V \u02daR X VL, D \u201c V \u02daR X VR. Let a, b, c and d denote the sizes of A, B, C and D respectively.\nWe will consider the case when all of a, b, c, d \u0105 0; the proof is similar and simpler in case some of them are 0. Let rT be a tree with root r\u03c1 that has the following structure: Both children of the root are internal nodes, all of A appears as leaves in the left subtree of the left child of the root, B as leaves in the right subtree of the left child of the root, C as leaves in the left subtree of the right child of the root and D as leaves in the right subtree of the right child of the root. We assume that all four subtrees for the sets A, B, C, D are generating and hence by induction optimal. We claim that the cost of rT is at least as much as the cost of T \u02da. To see this note that V \u02daL \u201c AYB. Thus, the left subtree of \u03c1\u02da is optimal for the set V \u02daL (by induction), whereas that of r\u03c1 may or may not be. Similarly for all the nodes in V \u02daR . The only other thing left to account for is the cost at the root. But since \u03c1\u02da and r\u03c1 induce exactly the same cut on V , the cost at the root is the same. Thus, \u0393prT q \u011b \u0393pT \u02daq. Furthermore, equality holds if and only if rT is also generating for G.\nLet W \u02da denote the weight function for the generating tree T \u02da such that for all u, v P V , W \u02dapLCAT\u02dapu, vqq \u201c wptu, vuq. Let \u03c1\u02daL and \u03c1\u02daR denote the left and right children of the root \u03c1\u02da of T \u02da. For all ua P A, ub P B, wptua, ubuq \u011b W \u02dap\u03c1\u02daLq. Let\nx \u201c 1 ab\n\u00ff\nuaPA,ubPB\nwptua, ubuq\ndenote the average weight of the edges going between A and B; it follows that x \u011b W \u02dap\u03c1\u02daLq. Similarly for all uc P C, ud P D, wptuc, uduq \u011b W \u02dap\u03c1\u02daRq. Let\ny \u201c 1 cd\n\u00ff\nucPC,udPD\nwptuc, uduq\ndenote the average weight of the edges going between C and D; it follows that y \u011b W \u02dap\u03c1\u02daRq. Finally for every u P A Y B,u1 P C Y D, wptu, u1uq \u201c W \u02dap\u03c1\u02daq; denote this common value by z. By the definition of generating tree, we know that x \u011b z and y \u011b z.\nNow consider the tree T . Let TL and TR denote the left and right subtrees of \u03c1. By induction, it must be that TL splits A and C as the first cut (or at least that\u2019s one possible tree, if multiple cuts exist), similarly TR first cuts B and D. Both, T and rT have subtrees containing only nodes from A, B, C and D. The costs for these subtrees are identical in both cases (by induction). Thus, we have\n\u0393pT q \u00b4 \u0393prT q \u201c zac \u00a8 gpa, cq ` zbd \u00a8 gpb, dq ` pxab` ycd` zpad` bcqq \u00a8 gpa` c, b` dq \u00b4 xab \u00a8 gpa, bq ` y \u00a8 cdgpc, dq \u00b4 zpa` bqpc` dq \u00a8 gpa` b, c` dq\n\u201c px\u00b4 zqabpgpa ` c, b` dq \u00b4 gpa, bqq ` py \u00b4 zqcdpgpa ` c, b` dq \u00b4 gpc, dqq ` zppa` cqpb ` dq \u00a8 gpa` c, b` dq ` ac \u00a8 gpa, cq ` bd \u00a8 gpb, dqq \u00b4 zppa` bqpc ` dq \u00a8 gpa` b, c` dq ` ab \u00a8 gpa, bq ` cd \u00a8 gpc, dqq\nUsing (4), we get that the last two expressions above both evaluate to zp\u03bapa` b` c` dq \u00b4 \u03bapaq \u00b4 \u03bapbq \u00b4 \u03bapcq \u00b4 \u03bapdqq, but have opposite signs. Thus, we get\n\u0393pT q \u00b4 \u0393prT q \u201c px\u00b4 zqabpgpa ` c, b` dq \u00b4 gpa, bqq ` py \u00b4 zqcdpgpa ` c, b` dq \u00b4 gpc, dqq\nIt is clear that the above expression is always non-negative and is 0 if and only if x \u201c z and y \u201c z. If it is the latter case and it is also the case that \u0393p rT q \u201c \u0393pT \u02daq, then it must actually be the case that T is a generating tree."}, {"heading": "3.2.1 Characterizing g that satisfy conditions of Theorem 3.4", "text": "Theorem 3.4 give necessary and sufficient conditions on g for cost functions of the form (3) be admissible. However, it leaves open the question of the existence of functions satisfying the criteria and also characterizing the functions g themselves. The fact that such functions exist already follows from the work of Dasgupta (2016), who showed that if gpn1, n2q \u201c n1 ` n2, then all cliques have the same cost. Clearly, g is monotone and symmetric and thus satisfies the condition of Theorem 3.4.\nIn order to give a more complete characterization, we define g as follows: Suppose gp\u00a8, \u00a8q is symmetric, we define gpn, 1q for all n \u011b 1 so that gpn, 1q{pn ` 1q is non-decreasing.10 We consider a particular cluster tree for a clique that is defined using a caterpillar graph, i.e., a cluster tree where the right child of any internal node is a leaf labeled by one of the nodes of G and the left child is another internal node, except at the very bottom. Figure 4 shows a caterpillar cluster tree for a clique on 4 nodes. The cost of the clique on n nodes, say \u03bapnq, using this cluster tree is given by\n\u03bapnq \u201c n\u00b41\u00ff\ni\u201c0\ni \u00a8 gpi, 1q\n10The function proposed by Dasgupta (2016) is gpn, 1q \u201c n` 1, so this ratio is always 1.\nNow, we enforce the condition that all cliques have the same cost by defining gpn1, n2q for n1, n2 \u0105 1 suitably, in particular,\ngpn1, n2q \u201c \u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2q\nn1 \u00a8 n2 (5)\nThus it only remains to be shown that g is strictly increasing. We show that for n2 \u010f n1, gpn1 ` 1, n2q \u0105 gpn1, n2q. In order to show this it suffices to show that,\nn1p\u03bapn1 ` n2 ` 1q \u00b4 \u03bapn1 ` 1q \u00b4 \u03bapn2qq \u00b4 pn1 ` 1qp\u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2qq \u0105 0\nThus, consider\nn1p\u03bapn1 ` n2 ` 1q \u00b4 \u03bapn1 ` 1q \u00b4 \u03bapn2qq \u00b4 pn1 ` 1qp\u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2qq \u201c n1p\u03bapn1 ` n2 ` 1q \u00b4 \u03bapn1 ` n2q \u00b4 \u03bap1q \u00b4 \u03bapn1 ` 1q ` \u03bapn1q ` \u03bap1qq \u00b4 p\u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2qq \u201c n1pn1 ` n2qgpn1 ` n2, 1q \u00b4 n21gpn1, 1q \u00b4 p\u03bapn1 ` n2q \u00b4 \u03bapn1q \u00b4 \u03bapn2qq\n\u011b n1pn1 ` n2qgpn1 ` n2, 1q \u00b4 n21gpn1, 1q \u00b4 n1`n2\u00b41\u00ff\ni\u201cn1\ni \u00a8 gpi, 1q\n\u011b gpn1 ` n2, 1q n1 ` n2 ` 1\n\u00a8 \u02dc n1pn1 ` n2qpn1 ` n2 ` 1q \u00b4 n21pn1 ` 1q \u00b4 n1`n2\u00b41\u00ff\ni\u201cn1\nipi ` 1q \u00b8 \u0105 0\nAbove we used the fact that gpn, 1q{pn`1q is non-decreasing in n and some elementary calculations. This shows that the objective function proposed by Dasgupta (2016) is by no means unique. Only in the last step, do we get an inequality where we use the condition that gpn, 1q{pn ` 1q is increasing. Whether this requirement can be relaxed further is also an interesting direction."}, {"heading": "3.2.2 Characterizing Objective Functions for Dissimilarity Graphs", "text": "When the weights of the edges represent dissimilarities instead of similarities, one can consider objective functions of the same form as (3). As mentioned in Remark 3.2, the difference in this case is that the goal is to maximize the objective function and hence the definition of admissibility now requires that generating trees have a value of the objective that is strictly larger than any tree that is not generating.\nThe characterization of admissible objective functions as given in Theorem 3.4 for the similarity case continues to hold in the case of dissimilarities. The proof follows in the same manner by appropriately switching the direction of the inequalities when required."}, {"heading": "4 Similarity-Based Inputs: Approximation Algorithms", "text": "In this section, we analyze the recursive \u03c6-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q \u201c \u0159NPT costpNq where for each node N of T with children N1, N2, costpNq \u201c wpV pN1q, V pN2qq \u00a8 V pNq. We show that the \u03c6-sparsest-cut algorithm achieves a 6.75\u03c6-approximation. (Charikar and Chatziafratis (2017) also proved an Op\u03c6q approximation for Dasgupta\u2019s function.) Our proof also yields an approximation guarantee not just for Dasgupta\u2019s cost function but more generally for any admissible cost function, but the approximation ratio depends on the cost function.\nThe \u03c6-sparsest-cut algorithm (Algorithm 1) constructs a binary tree top-down by recursively finding cuts using a \u03c6-approximate sparsest cut algorithm, where the sparsest-cut problem asks for a set A minimizing the sparsity wpA,V zAq{p|A||V zA|q of the cut pA,V zAq.\nAlgorithm 1 Recursive \u03c6-Sparsest-Cut Algorithm for Hierarchical Clustering\n1: Input: An edge weighted graph G \u201c pV,E,wq. 2: tA,V zAu \u00d0 cut with sparsity \u010f \u03c6 \u00a8 min\nS\u0102V wpS, V zSq{p|S||V zS|q\n3: Recurse on GrAs and on GrV zAs to obtain trees TA and TV zA 4: return the tree whose root has two children, TA and TV zA.\nTheorem 4.1. 11 For any graph G \u201c pV,Eq, and weight function w : E \u00d1 R`, the \u03c6-sparsest-cut algorithm (Algorithm 1) outputs a solution of cost at most 27\n4 \u03c6OPT.\nProof. Let G \u201c pV,Eq be the input graph and n denote the total number of vertices of G. Let T denote the tree output by the algorithm and T \u02da be any arbitrary tree. We will prove that costpT q \u010f 27\n4 \u03c6costpT \u02daq. 12\nRecall that for an arbitrary tree T0 and node N of T0, the vertices corresponding to the leaves of the subtree rooted at N is denoted by V pNq. Consider the node N0 of T \u02da that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T \u02da rooted at N0 contains fewer than 2n{3 leaves. The balanced cut (BC) of T \u02da is the cut pV pN0q, V \u00b4 V pN0qq. For a given node N with children N1, N2, we say that the cut induced by N is the sum of the weights of the edges between that have one extremity in V pN1q and the other in V pN2q.\nLet pAYC,BYDq be the cut induced by the root node u of T , where A,B,C,D are such that pAYB,C YDq is the balanced cut of T \u02da. Since pAY C,B YDq is a \u03c6-approximate sparsest cut:\nwpA Y C,B YDq |A YC| \u00a8 |B YD| \u010f \u03c6 wpA YB,C YDq |AYB| \u00a8 |C YD| .\nBy definition of N0, A Y B and C YD both have size in rn{3, 2n{3s, so the product of their sizes is at least pn{3qp2n{3q \u201c 2n2{9; developing wpAYB,C YDq into four terms, we obtain\nwpAY C,B YDq \u010f \u03c6 9 2n2 |AY C||B YD|pwpA,Cq ` wpA,Dq ` wpB,Cq `wpB,Dqq\n\u010f \u03c69 2 r |B YD| n wpA,Cq ` wpA,Dq `wpB,Cq ` |AY C| n wpB,Dqs,\n11For Dasgupta\u2019s function, this was already proved in Charikar and Chatziafratis (2017) with a different constant. The present, independent proof, uses a different method.\n12The following paragraph bears similarities with the first part of the analysis of (Dasgupta, 2016, Lemma 11) but we obtain a more fine-grained analysis by introducing a charging scheme.\nand so the cost induced by node u of T \u02da satisfies\nn \u00a8 wpAY C,B YDq \u010f 9 2 \u03c6|B YD|wpA,Cq ` 9 2 \u03c6|AYC|wpB,Dq ` 9 2 \u03c6npwpA,Dq ` wpB,Cqq.\nTo account for the cost induced by u, we thus assign a charge of p9{2q\u03c6|B YD|wpeq to each edge e of pA,Cq, a charge of p9{2q\u03c6|A Y C|wpeq to each edge e of pB,Dq, and a charge of p9{2q\u03c6nwpeq to each edge e of pA,Dq or pB,Cq.\nWhen we do this for every node u of T , how much does each edge get charged?\nLemma 4.2. Let G \u201c pV,Eq be a graph on n nodes. We consider the above charging scheme for T and T \u02da. Then, an edge pv1, v2q P E gets charged at most p9{2q\u03c6minpp3{2q|V pLCAT\u02dapv1, v2qq|, nqwpeq overall, where LCAT\u02dapv1, v2q denotes the lowest common ancestor of v1 and v2 in T \u02da.\nWe temporarily defer the proof and first see how Lemma 4.2 implies the theorem. Observe (as in Dasgupta (2016)) that costpT \u02daq \u201c \u0159tu,vuPE |V pLCAT\u02dapu, vqq|wpu, vq. Thanks to Lemma 4.2, when we sum charges assigned because of every node N of T , overall we obtain\ncostpT q \u010f 9 2 \u03c6\n\u00ff\ntv1,v2uPE\n3 2 |V pLCAT\u02dapv1, v2qq|wpv1, v2q \u201c 27 4 \u03c6costpT \u02daq.\nProof of Lemma 4.2. The lemma is proved by induction on the number of nodes of the graph. (The base case is obvious.) For the inductive step, consider the cut pAYC,B YDq induced by the root node u of T .\n\u2022 Consider the edges that cross the cut. First, observe that edges of pA,Bq or of pC,Dq never get charged at all. Second, an edge e \u201c tv1, v2u of pA,Dq or of pB,Cq gets charged p9{2q\u03c6nwpeq when considering the cost induced by node u, and does not get charged when considering any other node of T . In T \u02da, edge e is separated by the cut pAYB,C YDq induced by N0, so the least common ancestor of v1 and v2 is the parent node of N0 (or above), and by definition of N0 we have |V pLCAT\u02dapv1, v2qq| \u011b 2n{3, hence the lemma holds for e.\n\u2022 An edge e \u201c tv1, v2u of GrAsYGrCs does not get charged when considering the cut induced by node u. Apply Lemma 4.2 to GrAYCs for the tree T \u02daAYC defined as the subtree of T \u02da induced by the vertices of A Y C13. By induction, the overall charge to e due to the recursive calls for GrAYCs is at most p9{2q\u03c6minpp3{2q|V pLCAT\u02da AYC pv1, v2qq|, |AYC|qwpeq. By definition of\nT \u02daAYC , we have |V pLCAT\u02daAYC pv1, v2qq| \u010f |V pLCAT\u02dapv1, v2qq|, and |A Y C| \u010f n, so the lemma holds for e.\n\u2022 An edge tv1, v2u of pA,Cq gets a charge of p9{2q\u03c6|BYD|wpeq plus the total charge to e coming from the recursive calls for GrAY Cs and the tree T \u02daAYC . By induction the latter is at most\np9{2q\u03c6minpp3{2q|V pLCAT\u02da AYC pv1, v2qq|, |A Y C|qwpeq \u010f p9{2q\u03c6|A Y C|wpeq.\nOverall the charge to e is at most p9{2q\u03c6nwpeq. Since the cut induced by node u0 of T \u02da separates v1 from v2, we have |V pLCAT\u02dapv1, v2qq| \u011b 2n{3, hence the lemma holds for e. For edges of pB,Dq or of GrBs YGrDs, a symmetrical argument applies.\n13note that T\u02daAYC is not necessarily the optimal tree for GrAYCs, which is why the lemma was stated in terms of every tree T\u02da, not just on the optimal tree.\nRemark 4.3. The recursive \u03c6-sparsest-cut algorithm achieves an Opfn\u03c6q-approximation for any admissible cost function f , where fn \u201c maxn fpnq{fprn{3sq. Indeed, adapting the definition of the balanced cut as in Dasgupta (2016) and rescaling the charge by a factor of fn imply the result.\nWe complete our study of classical algorithms for hierarchical clustering by showing that the standard agglomerative heuristics can perform poorly (Theorems 8.1, 8.3). Thus, the sparsest-cutbased approach seems to be more reliable in the worst-case. To understand better the success of the agglomerative heuristics, we restrict our attention to ground-truth inputs (Section 7), and random graphs (Section 5), and show that in these contexts these algorithms are efficient."}, {"heading": "5 Admissible Objective Functions and Algorithms for Random", "text": "Inputs\nIn this section, we initiate a beyond-worst-case analysis of the hierarchical clustering problem (see also Section 7.3). We study admissible objective functions in the context of random graphs that have a natural hierarchical structure; for this purpose, we consider a suitable generalization of the stochastic block model to hierarchical clustering.\nWe show that, for admissible cost functions, an underlying ground-truth cluster tree has optimal expected cost. Additionally, for a subfamily of admissible cost functions (called smooth, see Defn. 5.4) which includes the cost function introduced by Dasgupta, we show the following: The cost of the ground-truth cluster tree is with high probability sharply concentrated (up to a factor of p1`op1qq around its expectation), and so of cost at most p1`op1qqOPT. This is further evidence that optimising admissible cost functions is an appropriate strategy for hierarchical clustering.\nWe also provide a simple algorithm based on the SVD based approach of McSherry (2001) followed by a standard agglomerative heuristic yields a hierarchical clustering which is, up to a factor p1` op1qq, optimal with respect to smooth admissible cost functions."}, {"heading": "5.1 A Random Graph Model For Hierarchical Clustering", "text": "We describe the random graph model for hierarchical clustering, called the hierarchical block model. This model has already been studied earlier, e.g., Lyzinski et al. (2017). However, prior work has mostly focused on statistical hypothesis testing and exact recovery in some regimes. We will focus on understanding the behaviour of admissible objective functions and algorithms to output cluster trees that have almost optimal cost in terms of the objective function.\nWe assume that there are k \u201cbottom\u201d-level clusters that are then arranged in a hierarchical fashion. In order to model this we will use a similarity graph on k nodes generated from an ultrametric (see Sec. 2.2). There are n1, . . . , nk nodes in each of the k clusters. Each edge is present in the graph with a probability that is a function of the clusters in which their endpoints lie and the underlying graph on k nodes generated from the ultrametric. The formal definition follows.\nDefinition 5.1 (Hierarchical Stochastic Block Model (HSBM)). A hierarchical stochastic block model with k bottom-level clusters is defined as follows:\n\u2022 Let rGk \u201c prVk, rEk, wq be a graph generated from an ultrametric (see Sec. 2.2), where |rVk| \u201c k\nfor each e P rEk, wpeq P p0, 1q.14 Let rTk be a tree on k leaves, let rN denote the internal nodes of rT and rL denote the leaves; let r\u03c3 : rL \u00d1 rks be a bijection. Let rT be generating for rGk with weight function \u0102W : rN \u00d1 r0, 1q (see Defn. 2.2).\n\u2022 For each i P rks, let pi P p0, 1s be such that pi \u0105 \u0102W pNq, if N denotes the parent of r\u03c3\u00b41piq in rT . \u2022 For each i P rks, there is a fixed constant fi P p0, 1q; furthermore \u0159k i\u201c1 fi \u201c 1.\nThen a random graph G \u201c pV,Eq on n nodes with sparsity parameter \u03b1n P p0, 1s is defined as follows: pn1, . . . , nkq is drawn from the multinomial distribution with parameters pn, pf1, . . . , fkqq. Each vertex i P rns is assigned a label \u03c8piq P rks, so that exactly nj nodes are assigned the label j for j P rks. An edge pi, jq is added to the graph with probability \u03b1np\u03c8piq if \u03c8piq \u201c \u03c8pjq and with probability \u03b1n\u0102W pNq if \u03c8piq \u2030 \u03c8pjq and N is the least common ancestor of r\u03c3\u00b41piq and r\u03c3\u00b41pjq in rT . The graph G \u201c pV,Eq is returned without any labels.\nAs the definition is rather long and technical, a few remarks are in order.\n\u2022 Rather than focusing on an arbitrary hierarchy on n nodes, we assume that there are k clusters (which exhibit no further hierarchy) and there is a hierarchy on these k clusters. The model assumes that k is fixed, but in future work, it may be interesting to study models where k itself may be a (modestly growing) function of n. The condition pi \u0105 \u0102W pNq (where N is the parent of r\u03c3\u00b41piq ) ensures that nodes in cluster i are strictly more likely to connect to each other than to node from any other cluster.\n\u2022 The graphs generated can be of various sparsity, depending on the parameter \u03b1n. If \u03b1n P p0, 1q is a fixed constant, we will get dense graphs (with \u2126pn2q edges), however if \u03b1n \u00d1 0 as n \u00d1 8, sparser graphs may be achieved. This is similar to the approach taken by Wolfe and Olhede (2013) when considering random graph models generated according to graphons.\nWe define the expected graph, G\u0304, which is a complete graph where an edge pi, jq has weight pi,j where pi,j is the probability with which it appears in the random graph G. In order to avoid ambiguity, we denote by \u0393pT ;Gq and \u0393pT ; G\u0304q the costs of the cluster tree T for the unweighted (random) graph G and weighted graph G\u0304 respectively. Observe that due to linearity (see Eqns. (3) and (2)), for any tree T and any admissible cost function, \u0393pT ; G\u0304q \u201c E r\u0393pT ;Gq s, where the expectation is with respect to the random choices of edges in G (in particular this holds even when conditioning on n1, . . . , nk).\nFurthermore, note that G\u0304 itself is generated from an ultrametric and the generating trees for G\u0304 are obtained as follows: Let rTk be any generating tree for rGk, let T\u03021, T\u03022, . . . , T\u0302k be any binary trees with n1, . . . , nk leaves respectively. Let the weight of every internal node of T\u0302i be pi and replace each leaf l in rTk by T\u0302r\u03c3plq. In particular, this last point allows us to derive Proposition 5.3. We refer to any tree that is generating for the expected graph G\u0304 as a ground-truth tree for G.\nRemark 5.2. Although it is technically possible to have ni \u201c 0 for some i under the model, we will assume in the rest of the section that ni \u0105 0 for each i. This avoids getting into the issue of degenerate ground-truth trees; those cases can be handled easily, but add no expository value.\n14In addition to rGk being generated from an ultrametric, we make the further assumption that the function f : R` \u00d1 R`, that maps ultrametric distances to edge weights, has range p0, 1q, so that the weight of an edge can be interpreted as a probability of an edge being present. We rule out wpeq \u201c 0 as in that case the graph is disconnected and each component can be treated separately."}, {"heading": "5.2 Objective Functions and Ground-Truth Tree", "text": "In this section, we assume that the graphs represent similarities. This is clearly more natural in the case of unweighted graphs; however, all our results hold in the dissimilarity setting and the proofs are essentially identical.\nProposition 5.3. Let \u0393 be an admissible cost function. Let G be a graph generated according to an HSBM (See Defn. 5.1). Let \u03c8 be the (hidden) function mapping the nodes of G to rks (the bottom-level clusters). Let T be a ground-truth tree for G Then,\nE r\u0393pT q | \u03c8 s \u010f min T 1\nE \u201c \u0393pT 1q | \u03c8 \u2030 .\nMoreover, for any tree T 1, E r\u0393pT q | \u03c8 s \u201c E r\u0393pT 1q | \u03c8 s if and only if T 1 is a ground-truth tree.\nProof. As per Remark 5.2, we\u2019ll assume that each ni \u0105 0 to avoid degenerate cases. Let G\u0304 be a the expected graph, i.e., G\u0304 is complete and an edge pi, jq has weight pij, the probability that the edge pi, jq is present in the random graph G generated according to the hierarchical model. Thus, by definition of admissibility \u0393pT ; G\u0304q \u201c minT 1 \u0393pT 1; G\u0304q if an only if T is generating (see Defn. 3.1). As ground-truth trees for G are precisely the generating trees for G\u0304; the result follows by observing that for any tree T (not necessarily ground-truth) E r\u0393pT ;Gq | \u03c8 s \u201c \u0393pT ; G\u0304q, where the expectation is taken only over the random choice of the edges, by linearity of expectation and the definition of the cost function (Eqns. 3 and 2).\nDefinition 5.4. Let \u03b3 be a cost function defined using the function gp\u00a8, \u00a8q (see Defn. 3.1). We say that the cost function \u0393 (as defined in Eqn. 2) satisfies the smoothness property if\ngmax :\u201c maxtgpn1, n2q | n1 ` n2 \u201c nu \u201c O \u02c6 \u03bapnq n2 \u02d9 ,\nwhere \u03bapnq is the cost of a unit-weight clique of size n under the cost function \u0393.\nFact 5.5. The cost function introduced by Dasgupta (2016) satisfies the smoothness property. Theorem 5.6. Let \u03b1n \u201c \u03c9p a\nlog n{nq. Let \u0393 be an admissible cost function satisfying the smoothness property (Defn. 5.4). Let k be a fixed constant and G be a graph generated from an HSBM (as per Defn. 5.1) where the underlying graph rGk has k nodes and the sparsity factor is \u03b1n. Let \u03c8 be the (hidden) function mapping the nodes of G to rks (the bottom-level clusters). For any binary tree T with n leaves labelled by the vertices of G, the following holds with high probability:\n|\u0393pT q \u00b4 E r\u0393pT q | \u03c8 s| \u010f opE r\u0393pT q | \u03c8 sq.\nThe expectation is taken only over the random choice of edges. In particular if T \u02da is a ground-truth tree for G, then, with high probability,\n\u0393pT \u02daq \u010f p1` op1qqmin T 1 \u0393pT 1q \u201c p1` op1qqOPT.\nProof. Our goal is to show that for any fixed cluster tree T 1 the cost is sharply concentrated around its expectation with an extremely high probability. We then apply the union bound over all possible cluster trees and obtain that in particular the cost of OPT is sharply concentrated around its expectation. Note that there are at most 2c\u00a8n logn possible cluster trees (including labellings of\nthe leaves to vertices of G), where c is a suitably large constant. Thus, it suffices to show that for any cluster tree T 1 we have\nP \u201c \u02c7\u030c \u0393pT 1q \u00b4 E \u201c \u0393pT 1q | \u03c8 \u2030\u02c7\u030c \u011b opE \u201c \u0393pT 1q | \u03c8 \u2030 q \u2030 \u010f exp p\u00b4c\u02dan log nq ,\nwhere c\u02da \u0105 c. Recall that for a given node N of T 1 with children N1, N2, we have \u03b3pNq \u201c wpV pN1q, V pN2qq \u00a8\ngp|V pN1q|, |V pN2q|q and \u0393pT 1q \u201c \u0159\nNPT 1 \u03b3pNq (see Eqns. (3) and (2)). Let Yi,j \u201c 1pi,jqPE for all 1 \u010f i, j \u010f n and observe that tYi,j|i \u0103 ju are independent and Yi,j \u201c Yj,i. Furthermore, let Zi,j \u201c gp|V pchild1pN i,jqq|, |V pchild2pN i,jqq|q \u00a8 Yi,j, where N i,j is the node in T 1 separating nodes i and j and child1pN i,jq and child2pN i,jq are the two children of N i,j. We can thus write\n\u0393pT 1q \u201c \u00ff\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q \u00ff\niPV pchild1pNqq jPV pchild2pNqq\nYi,j (6)\n\u201c \u00ff\nNPT 1\n\u00ff\niPV pchild1pNqq jPV pchild2pNqq\nZi,j (7)\n\u201c \u00ff\ni\u0103j\nZi,j, (8)\nwhere we used that every potential edge i, j, i \u2030 j appears in exactly one cut and that Zi,j \u201c Zj,i. Observe that \u0159 i\u0103j Zi,j is a sum of independent random variables. Assume that the following claim holds. Claim 5.7. Let wmin \u201c \u2126p1q be the minimum weight in rTk, the tree generating tree for rGk (see Defn. 5.1), i.e., wmin \u201c minNP rTk\n\u0102W pNq) and recall that gmax \u201c maxtgpn1, n2q | n1 ` n2 \u201c nu. We have\n1. E r\u0393pT 1q | \u03c8 s \u011b \u03bapnq \u00a8 \u03b1n \u00a8 wmin 2. \u0159\ni\u0103j gp|V pchild1pN i,jqq|, |V pchild2pN i,jqq|q2 \u010f gmax \u00a8 \u03bapnq\nWe defer the proof to later and first finish the proof of Theorem 5.6. We will make use of the slightly generalized version of Hoeffding bounds (see Hoeffding (1963)). For X1,X2, . . . ,Xm independent random variables satisfying ai \u010f Xi \u010f bi for i P rns. Let X \u201c \u0159m i\u201c1 Xi, then for any t \u0105 0\nP r |X \u00b4 E rX s | \u011b t s \u010f exp \u02c6 \u00b4 2t 2\n\u0159m i\u201c1pbi \u00b4 aiq2\n\u02d9 . (9)\nBy assumption, there exists a function yn : N \u00d1 R` such that \u03b1n \u201c \u03c9 \u02c6 yn \u00a8 b logn n \u02d9 with yn \u201c \u03c9p1q\nfor all n. We apply (9) with t \u201c E r\u0393pT 1q | \u03c8 s \u00a8 yn\u00a8 b log n n\n\u03b1n \u201c opE r\u0393pT 1q | \u03c8 sq and derive\nP \u00bb \u2013 \u02c7\u030c\u0393pT 1q \u00b4 E \u201c \u0393pT 1q | \u03c8 \u2030\u02c7\u030c \u010f E \u201c \u0393pT 1q | \u03c8 \u2030 \u00a8 yn \u00a8 b logn n\n\u03b1n\nfi fl \u011b\n\u011b 1\u00b4 exp \u00a8 \u02da\u030a \u02da\u030a \u02da\u030b\u00b4 2 \u02dc E r\u0393pT 1q | \u03c8 s \u00a8 yn\u00a8 b log n n \u03b1n \u00b82\n\u0159 i\u0103j gp|V pN i,j 1 q|, |V pN i,j 2 q|q2 \u02db \u2039\u2039\u2039\u2039\u2039\u201a\n\u011b 1\u00b4 exp \u02c6 \u00b42 \u00a8 \u03bapnq \u00a8 w 2 min \u00a8 y2n \u00a8 log n\ngmax \u00a8 n\n\u02d9\n\u011b 1\u00b4 exp p\u00b4c\u02da \u00a8 n log nq ,\nwhere the last inequality follows by assumption of the lemma and since yn \u201c \u03c9p1q.\nWe now turn to the proof of Claim 5.7.\nProof of Claim 5.7. Note that for any two vertices i, j of G, the edge pi, jq exists in G with probability at least \u03b1n \u00a8 wmin. Thus, we have\nE \u201c \u0393pT 1q | \u03c8 \u2030 \u201c \u00ff\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q \u00ff\niPV pchild1pNqq jPV pchild2pNqq\n\u03b1n \u00a8 wpi, jq\n\u011b wmin \u00a8 \u03b1n \u00a8 \u00ff\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q|V pchild1pNqq| \u00a8 |V pchild2pNqq|\n\u201c wmin \u00a8 \u03b1n \u00a8 \u03bapnq,\nwhere we made use of Eqn. (5). Furthermore, we have \u00ff\ni\u0103j\ngp|V pchild1pN i,jqq|,|V pchild2pN i,jqq|q2 \u201c\n\u201c \u00ff\nNPT 1\ngp|V pchild1pNqq|, |V pchild2pNqq|q2 \u00a8 |V pchild1pNqq| \u00a8 |V pchild2pNqqq|\n\u010f gmax \u00ff\nNPT 1\ngp|V pchild1pNqq|, |V pchild1pNqq|q \u00a8 |V pchild1pNqq| \u00a8 |V pchild2pNqqq|\n\u201c gmax \u00a8 \u03bapnq,\nwhere we made use of Eqn. (5).\n5.3 Algorithm for Clustering in the HSBM\nIn this section, we provide an algorithm for obtaining a hierarchical clustering of a graph generated from an HSBM. The algorithm is quite simple and combines approaches that are used in practice for hierarchical clustering: SVD projections and agglomerative heuristics. See Algorithm 2 for a complete description.\nTheorem 5.8. Let \u03b1n \u201c \u03c9p a\nlog n{nq. Let \u0393 be an admissible cost function (Defn. 3.1) satisfying the smoothness property (Defn 5.4). Let k be a fixed constant and G be a graph generated from an HSBM (as per Defn. 5.1) where the underlying graph rGk has k nodes and the sparsity factor is \u03b1n. Let T be a ground-truth tree for G. With high probability, Algorithm 2 with parameter k on graph G outputs a tree T 1 that satisfies \u0393pT q \u010f p1` op1qqOPT.\nAlgorithm 2 Agglomerative Algorithm for Recovering Ground-Truth Tree of an HSBM Graph\n1: Input: Graph G \u201c pV,Eq generated from an HSBM. 2: Parameter: A constant k. 3: Apply (SVD) projection algorithm of (McSherry, 2001, Thm. 12) with parameters G, k, \u03b4 \u201c\n|V |\u00b42, to get \u03b6p1q, . . . , \u03b6p|V |q P R|V | for vertices in V , where dimpspanp\u03b6p1q, . . . , \u03b6p|V |qqq \u201c k. 4: Run the single-linkage algorithm on the points t\u03b6p1q, . . . , \u03b6p|V |qu until there are exactly k clus-\nters. Let C \u201c tC\u03b61 , . . . , C \u03b6 ku be the clusters (of points \u03b6piq) obtained. Let Ci \u010e V denote the set\nof vertices corresponding to the cluster C\u03b6i . 5: while there are at least two clusters in C do 6: Take the pair of clusters Ci, Cj of C that maximizes\ncutpCi,Cjq |Ci|\u00a8|Cj|\n7: C \u00d0 C z tCiu z tCju Y tCi Y Cju 8: end while 9: The sequence of merges in the while-loop (Steps 5 to 8) induces a hierarchical clustering tree on tC1, . . . , Cku, say T 1k with k leaves (represented by C1, . . . , Ck). Replace each leaf of T 1k by an arbitrary binary tree on |Ck| leaves labelled according to the vertices Ck to obtain T . 10: Repeat the algorithm k1 \u201c 2k log n times. Let T 1, . . . T k1 be the corresponding hierarchical clustering trees. 11: Output: Tree T i (out of the k1 candidates) that minimises \u0393pTiq.\nRemark 5.9. In an HSBM, k is a fixed constant. Thus, even if k is not known in advance, one can simply run the Algorithm 2 with all possible different values (constantly many) and return the solution with the minimal cost \u0393pT q.\nLet G \u201c pV,Eq be the input graph generated according to an HSBM. Let T be the tree output by Algorithm 2. We divide the proof into two claims that correspond to the outcome of Step 3 and the while-loop (Steps 5 to 8) of Algorithm 2.\nWe use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here. The difference is that there is no hierarchical structure on top of the k clusters in his setting; on the other hand, his goal is also simply to identify the k clusters and not any hierarchy upon them. The following theorem is derived from McSherry (2001) (Observation 11 and a simplification of Theorem 12).\nTheorem 5.10 (McSherry (2001)). Let s be the size of the smallest cluster (of the k clusters) and \u03b4 be the confidence parameter. Assume that for all u, v belonging to different clusters with with adjacency vectors u,v (i.e., ui is 1 if the edge pu, iq exists in G and 0 otherwise) satisfy\n}E ru s \u00b4 E rv s }22 \u011b c \u00a8 k \u00a8 pn{s` logpn{\u03b4qq for a large enough constant c, where E ru s is the entry-wise expectation. Then, the algorithm of McSherry (2001, Thm. 12) with parameters G, k, \u03b4 projects the columns of the adjacency matrix of G to points t\u03b6p1q, . . . , \u03b6p|V |qu in a k-dimensional subspace of R|V | such that the following holds w.p. at least 1 \u00b4 \u03b4 over the random graph G and with probability 1{k over the random bits of the algorithm. There exists \u03b7 \u0105 0 such that for any u in the ith cluster and v in the jth cluster:\n1. if i \u201c j then }\u03b6puq \u00b4 \u03b6pvq}22 \u010f \u03b7;\n2. if i \u2030 j then }\u03b6puq \u00b4 \u03b6pvq}22 \u0105 2\u03b7,\nRecall that \u03c8 : V \u00d1 rks is the (hidden) labelling assigning each vertex of G to one of the k bottom-level clusters. Let C\u02dai \u201c tv P V | \u03c8pvq \u201c iu. Recall that ni \u201c |V pC\u02dai q|. Note that the algorithm of (McSherry, 2001, Thm. 12) might fail for two reasons. The first reason is that the random choices by the algorithm yield an incorrect clustering. This happens w.p. at most 1{k and we can simply repeat the algorithm sufficiently many times to be sure that at least once we get the desired result, i.e., the projections satisfy the conclusion of Thm. 5.10. Claims 5.11 and 5.12 show that in this case, Steps 5 to 8 of Alg. 2 produce a tree that has cost close to optimal. Ultimately, the algorithm simply outputs a tree that has the least cost among all the ones produced (and one of them is guaranteed to have cost p1` op1qqOPT) with high probability.\nThe second reason why the McSherry\u2019s algorithm may fail is that the generated random graph G might \u201cdeviate\u201d too much from its expectation. This is controlled by the parameter \u03b4 (which we set to 1{|V |2). Deviations from expected behaviour will cause our algorithm to fail as well. We bound this failure probability in terms of two events. Let E\u03041 be the event that there exists i, such that ni \u0103 fin{2, i.e., at least one of the bottom-level clusters has size that is not representative. This event occurs with a very low probability which is seen by a simple application of the ChernoffHoeffding bound, as E rni s \u201c fin. Note that E\u03041 depends only on the random choices that assign labels to the vertices according to \u03c8 (and not on random choice of the edges). Let E1 be the complement of E\u03041. If E1 holds the term n{s that appears in Thm. 5.10 is a constant. The second bad event is that McSherry\u2019s algorithm fails due to the random choice of edges. This happens with probability at most \u03b4 which we set at \u03b4 \u201c 1\n|V |2 . We denote the complement of this event E2. Thus,\nfrom now on we assume that both \u201cgood\u201d events E1 and E2 occur, allowing Alg. 2 to fail if either of them don\u2019t occur.\nIn order to prove Theorem 5.8 we establish the following claims.\nClaim 5.11. Let \u03b1n \u201c \u03c9p a\nlog n{nq. Let G be generated by an HSBM. Let C\u02da1 , . . . , C\u02dak be the hidden bottom-level clusters, i.e., C\u02dai \u201c tv | \u03c8pvq \u201c iu. Assume that events E1 and E2 occur. With probability at least 1{k, the clusters obtained after Step 4 correspond to the assignment \u03c8, i.e., there exists a permutation \u03c0 : rks \u00d1 rks, such that Cj \u201c C\u02da\u03c0pjq.\nProof. The proof relies on Theorem 5.10. As we know that the event E1 occurs, we may conclude that s \u201c mini ni \u011b n2 mini fi. Thus, n{s \u010f 2fmin , where fmin \u201c mini fi and hence n{s is bounded by some fixed constant.\nLet u, v be two nodes such that i \u201c \u03c8puq \u2030 \u03c8pvq \u201c j. Let u and v denote the random variables corresponding to the columns of u and v in the adjacency matrix of G. Let q \u201c \u0102W pNq where N is the LCA rTkpr\u03c3\n\u00b41piq, r\u03c3\u00b41pjqq in rTk, the generating tree for rGk used in defining the HSBM. Assuming E1 and taking expectations only with respect to the random choice of edges, we have:\n}E ru | \u03c8, E1 s \u00b4 E rv | \u03c8, E1 s }22 \u011b ni\u03b12nppi \u00b4 qq2 ` nj\u03b12nppj \u00b4 qq2 \u201c \u2126p\u03b12nnq \u201c \u03c9plog nq\nAbove we used that pi \u00b4 q \u0105 0 and ni \u201c \u2126pnq for each i. Note that for \u03b4 \u201c 1\nn2 , this satisfies the condition of Theorem 5.10. Since, we are already\nassuming that E2 holds, the only failure arises from the random coins of the algorithm. Thus, with probability at least 1{k the conclusions of Theorem 5.10 hold. In the rest of the proof we assume that the following holds: There exists \u03b7 \u0105 0 such that for any pair of nodes u, v we have\n1. if \u03c8puq \u201c \u03c8pvq then }\u03b6puq \u00b4 \u03b6pvq}22 \u010f \u03b7;\n2. if \u03c8puq \u2030 \u03c8pvq then }\u03b6puq \u00b4 \u03b6pvq}22 \u0105 2\u03b7.\nTherefore, any linkage algorithm, e.g., single linkage (See Alg. 6), performing merges starting from the set t\u03b6p1q, . . . , \u03b6pnqu until there are k clusters will merge clusters at a distance of at most \u03b7 and hence, the clusters obtained after Step 4 correspond to the assignment \u03c8. This yields the claim. Claim 5.12. Let \u03b1n \u201c \u03c9p a\nlog n{nq. Let G be generated according to an HSBM and let T \u02da be a ground-truth tree for G. Assume that events E1 and E2 occur, and that furthermore, the clusters obtained after Step 4 correspond to the assignment \u03c8, i.e., there exists a permutation \u03c0 : rks \u00d1 rks such that for each v P Ci, \u03c8pvq \u201c \u03c0piq. Then, the sequence of merges in the while-loop (Steps 5 to 8) followed by Step 9 produces w.h.p. a tree T such that \u0393pT q \u010f p1` op1qqOPT .\nProof. For simplicity of notation, we will assume that \u03c0 is the identity permutation, i.e., the algorithm has not only identified the true clusters correctly but also guessed the correct label. This only makes the notation less messy, though the proof is essentially unchanged.\nLet N be some internal node of any generating tree rTk and let S1 \u201c V pchild1pNqq and S2 \u201c V pchild2pNqq. Note that both S1 and S2 are a disjoint union of some of the clusters tC1, . . . , Cku. Then notice that for any u P S1 and v P S2, the probability that the edge pu, vq exists in G is \u03b1n\u0102W pNq. Thus, conditioned on E1 and \u03c8, we have\nE \u201e cutpSi, Sjq |Si||Sj |  \u201c \u03b1n\u0102W pNq\nFor now, let us assume that Alg. 2 makes merges in Steps 5 to 8 based on the true expectations instead of the empirical estimates. Then essentially, the algorithm is performing any linkage algorithm (i.e., average, single, or complete-linkage) on a ground-truth input and hence is guaranteed to recover the generating tree (see Theorem. 7.1).\nTo complete the proof, we will show the following: For any partition C1, . . . , Ck of V satisfying mini |Ci| \u011b n2 mini fi, and for any S1, S2, S11, S12, where S1 and S2 (and S11, S12) are disjoint and are both unions of some cluster tC1, . . . , Cku. and i1 \u2030 j1, with probability at least 1 \u00b4 1{n3, the following holds:\n\u02c7\u030c \u02c7\u030cE \u201e cutpS1, S2q |S1| \u00a8 |S2|  \u00b4 wpS1, S2q|S1| \u00a8 |S2| \u02c7\u030c \u02c7\u030c \u010f 1 log n (10)\nNote that as the probability of any edge existing is \u2126p\u03b1nq, it must be the case that E \u201d\nwpS1,S2q |S1||S2|\n\u0131 \u201c\n\u2126p\u03b1nq. Furthermore, since |S1|, |S2| \u201c \u2126pnq by the Chernoff-Hoeffding bound for a single pair pS1, S2q the above holds with probability expp\u00b4cn2\u03b1n{ log nq. Thus, even after taking a union bound over Opknq possible partitions and 2Opkq possible pairs of sets pS1, S2q that can be derived from said partition, Eqn. 10 holds with high probability.\nTo complete the proof, we will show the following: We will assume that the algorithm of McSherry has identified the correct clusters at the bottom level, i.e., E2 holds, and that Eqn. 10 holds.\nWe restrict our attention to sets S1, S2 that are of the form that S1 \u201c V pchild1pNqq and S2 \u201c V pchild2pNqq for some internal node N of some generating tree rTk of the graph rGk used to generate G. Then for pairs pS1, S2q and pS11, S12q both of this form, the following holds: there exists 3\nlogn \u0105 \u03b7 \u0105 0 such that\n1. If E \u201d\nwpS1,S2q |S1|\u00a8|S2|\n\u0131 \u201c E \u201d wpS11,S 1 2q\n|S11|\u00a8|S 1 2|\n\u0131 , then \u02c7\u030c \u02c7wpS1,S2q|S1|\u00a8|S2| \u00b4 wpS11,S 1 2q\n|S11|\u00a8|S 1 2|\n\u02c7\u030c \u02c7 \u010f \u03b7\n2. If E \u201d\nwpS1,S2q |S1|\u00a8|S2|\n\u0131 \u2030 E \u201d wpS11,S 1 2q\n|S11|\u00a8|S 1 2|\n\u0131 , then \u02c7\u030c \u02c7wpS1,S2q|S1|\u00a8|S2| \u00b4 wpS11,S 1 2q\n|S11|\u00a8|S 1 2|\n\u02c7\u030c \u02c7 \u0105 2\u03b7\nNote that the above conditions are enough to ensure that the algorithm performs the same steps as with perfect inputs, up to an arbitrary choice of tie-breaking rule. Since Theorem 7.1 is true no matter the tie breaking rule chosen, the proof follows since the two above conditions hold with probability at least expp\u00b4cn2\u03b1n{ log nq.\nWe are ready to prove Theorem 5.8.\nProof of Theorem 5.8. Conditioning on E1 and E2 which occur w.h.p. we get from Claims 5.11 and 5.12 that w.p. at least 1{k the tree Ti obtain in step 9 fulfills \u0393pTiq \u010f p1` op1qqOPT . It is possible to boost this probability by running Algorithm 2 multiple times. Running it \u2126pk log nq times and taking the tree with the smallest \u0393pTiq yields the result.\nRemark 5.13. It is worth mentioning that one of the trees Ti computed by the algorithm is w.h.p. the ground-truth tree T \u02da. If one desires to recover that tree, then this is possible by verifying for each candidate tree with minimal Ti whether is is indeed generating."}, {"heading": "6 Dissimilarity-Based Inputs: Approximation Algorithms", "text": "In this section, we consider general dissimilarity inputs and admissible objective for these inputs. For ease of exposition, we focus on an particular admissible objective function for dissimilarity inputs. Find T maximizing the value function corresponding to Dasgupta\u2019s cost function of Section 4: valpT q \u201c \u0159NPT valpNq where for each node N of T with children N1, N2, valpNq \u201c wpV pN1q, V pN2qq \u00a8V pNq. This optimization problem is NP-Hard Dasgupta (2016), hence we focus on approximation algorithms.\nWe show (Theorem 6.2) that average-linkage achieves a 2 approximation for the problem. We then introduce a simple algorithm based on locally-densest cuts and show (Theorem 6.5) that it achieves a 3{2` \u03b5 approximation for the problem.\nWe remark that our proofs show that for any admissible objective function, those algorithms have approximation guarantees, but the approximation guarantee depends on the objective function.\nWe start with the following elementary upper bound on OPT.\nFact 6.1. For any graph G \u201c pV,Eq, and weight function w : E \u00d1 R`, we have OPT \u010f n \u00a8\u0159 ePE wpeq."}, {"heading": "6.1 Average-Linkage", "text": "We show that average-linkage is a 2-approximation in the dissimilarity setting.\nTheorem 6.2. For any graph G \u201c pV,Eq, and weight function w : E \u00d1 R`, the average-linkage algorithm (Algorithm 3) outputs a solution of value at least n \u0159 ePE wpeq{2 \u011b OPT{2.\nWhen two trees are chosen at Step 4 of Algorithm 3, we say that they are merged. We say that all the trees considered at the beginning of an iteration of the while loop are the trees that are candidate for the merge or simply the candidate trees.\nWe first show the following lemma and then prove the theorem.\nAlgorithm 3 Average-Linkage Algorithm for Hierarchical Clustering (dissimilarity setting)\n1: Input: Graph G \u201c pV,Eq with edge weights w : E \u00de\u00d1 R` 2: Create n singleton trees. 3: while there are at least two trees do 4: Take trees roots N1 and N2 minimizing\n\u0159 xPV pN1q,yPV pN2q wpx, yq{p|V pN1q||V pN2q|q\n5: Create a new tree with root N and children N1 and N2 6: end while 7: return the resulting binary tree T\nLemma 6.3. Let T be the output tree and A,B be the children of the root. We have,\nwpV pAq, V pBqq |V pAq| \u00a8 |V pBq| \u011b wpV pAqq |V pAq| \u00a8 p|V pAq| \u00b4 1q ` wpV pBqq |V pBq| \u00a8 p|V pBq| \u00b4 1q .\nProof. Let a \u201c |V pAq|p|V pAq| \u00b4 1q{2 and b \u201c |V pBq|p|V pBq| \u00b4 1q{2. For any node N0 of T , let child1pN0q and child2pN0q be the two children of N0. We first consider the subtree TA of T rooted at A. We have #\nwpV pAqq \u201c \u0159A0PTA wpV pchild1pA0qq, V pchild2pA0qqq, a \u201c \u0159A0PTA |V pchild1pA0qq| \u00a8 |V pchild2pA0qq|.\nBy an averaging argument, there exists A1 P TA with children A1, A2 such that\nwpV pA1q, V pA2qq |V pA1q| \u00a8 |V pA2q| \u011b wpV pAqq a . (11)\nWe now consider the iteration of the while loop at which the algorithm merged the trees A1 and A2. Let A1, A2, . . . , Ak and B1, B2, . . . , B\u2113 be the trees that were candidate for the merge at that iteration, and such that V pAiq X V pBq \u201c H and V pBiq X V pAq \u201c H. Observe that the leaves sets of those trees form a partition of the sets V pAq and V pBq, so\n# wpA,Bq \u201c \u0159i,j wpV pAiq, V pBjqq, |V pAq| \u00a8 |V pBq| \u201c \u0159i,j |V pAiq| \u00a8 |V pBjq|.\nBy an averaging argument again, there exists Ai, Bj such that\nwpV pAiq, V pBjqq |V pAiq| \u00a8 |V pBjq| \u010f wpV pAq, V pBqq|V pAq| \u00a8 |V pBq| . (12)\nNow, since the algorithm merged A1, A2 rather than Ai, Bj , by combining Eq. 11 and 12, we have\nwpV pAqq a \u010f wpV pA1q, V pA2qq|V pA1q| \u00a8 |V pA2q| \u010f wpV pAiq, V pBjqq|V pAiq| \u00a8 |V pBjq| \u010f wpV pAq, V pBqq|V pAq| \u00a8 |V pBq| .\nApplying the same reasoning to B and taking the sum yields the lemma.\nProof of Theorem 6.2. We proceed by induction on the number of the nodes n in the graph. Let A,B be the children of the root of the output tree T . By induction,\nvalpT q \u011b p|V pAq| ` |V pBq|q \u00a8 wpV pAq, V pBqq ` |V pAq| 2 wpV pAqq ` |V pBq| 2 wpV pBqq. (13)\nLemma 6.3 implies p|V pAq|`|V pBq|q\u00a8wpV pAq, V pBqq \u011b |V pBq|wpV pAqq`|V pAq|wpV pBqq. Dividing both sides by 2 and plugging it into (13) yields\nvalpT q \u011b |V pAq| ` |V pBq| 2 wpV pAq, V pBqq ` |V pAq| ` |V pBq| 2 pwpV pAqq ` wpV pBqqq.\nObserving that n \u201c |V pAq| ` |V pBq| and combining \u0159ePE wpeq \u201c wpV pAq, V pBqq ` wpV pAqq ` wpV pBqq with Fact 6.1 completes the proof."}, {"heading": "6.2 A Simple and Better Approximation Algorithm for Worst-Case Inputs", "text": "In this section, we introduce a very simple algorithm (Algorithm 5) that achieves a better approximation guarantee. The algorithm follows a divisive approach by recursively computing locallydensest cuts using a local search heuristic (see Algorithm 4). This approach is similar to the recursive-sparsest-cut algorithm of Section 4. Here, instead of trying to solve the densest cut problem (and so being forced to use approximation algorithms), we solve the simpler problem of computing a locally-densest cut. This yields both a very simple local-search-based algorithm and a good approximation guarantee.\nWe use the notation A\u2018x to mean the set obtained by adding x to A if x R A, and by removing x from A if x P A. We say that a cut pA,Bq is a \u03b5{n-locally-densest cut if for any x,\nwpA\u2018 x,B \u2018 xq |A\u2018 x| \u00a8 |B \u2018 x| \u010f\n\u00b4 1` \u03b5\nn \u00af wpA,Bq |A||B| .\nThe following local search algorithm computes an \u03b5{n-locally-densest cut.\nAlgorithm 4 Local Search for Densest Cut\n1: Input: Graph G \u201c pV,Eq with edge weights w : E \u00de\u00d1 R` 2: Let pu, vq be an edge of maximum weight 3: A \u00d0 tvu, B \u00d0 V ztvu 4: while Dx: wpA\u2018x,B\u2018xq|A\u2018x|\u00a8|B\u2018x| \u0105 p1` \u03b5{nq wpA,Bq |A||B| do 5: A \u00d0 A\u2018 x, B \u00d0 B \u2018 x 6: end while 7: return pA,Bq\nTheorem 6.4. Algorithm 4 computes an \u03b5{n-locally-densest cut in time rOpnpn`mq{\u03b5q.\nProof. The proof is straightforward and given for completeness. By definition, the algorithm computes an \u03b5{n-locally densest cut so we only need to argue about the running time. The weight of the cut is initially at least wmax, the weight of the maximum edge weight, and in the end at most nwmax. Since the weight of the cut increases by a factor of p1 ` \u03b5{nq at each iteration, the total number of iterations of the while loop is at most log1`\u03b5{npnwmax{wmaxq \u201c rOpn{\u03b5q. Each iteration takes time Opm` nq, so the running time of the algorithm is rOpnpm` nq{\u03b5q.\nTheorem 6.5. Algorithm 5 returns a tree of value at least\n2n\n3 p1\u00b4 \u03b5q\n\u00ff\ne\nwpeq \u011b 2 3 p1\u00b4 \u03b5qOPT,\nin time rOpn2pn `mq{\u03b5q.\nAlgorithm 5 Recursive Locally-Densest-Cut for Hierarchical Clustering\n1: Input: Graph G \u201c pV,Eq, with edge weights w : E \u00de\u00d1 R`, \u03b5 \u0105 0 2: Compute an \u03b5{n-locally-densest cut pA,Bq using Algorithm 4 3: Recurse on GrAs and GrBs to obtain rooted trees TA and TB . 4: Return the tree T whose root node has two children, TA and TB .\nThe proof relies on the following lemma.\nLemma 6.6. Let pA,Bq be an \u03b5{n-locally-densest cut. Then,\np|A| ` |B|qwpA,Bq \u011b 2p1\u00b4 \u03b5qp|B|wpAq ` |A|wpBqq.\nProof. Let v P A. By definition of the algorithm,\np1` \u03b5{nqwpA,Bq|A||B| \u011b wpAztvu, B Y tvuq p|A| \u00b4 1qp|B| ` 1q .\nRearranging,\np|A| \u00b4 1qp|B| ` 1q |A||B| p1` \u03b5{nqwpA,Bq \u011b wpAztvu, B Y tvuq \u201c wpA,Bq ` wpv,Aq \u00b4 wpv,Bq\nSumming over all vertices of A, we obtain\n|A| p|A| \u00b4 1qp|B| ` 1q|A||B| p1` \u03b5{nqwpA,Bq \u011b |A|wpA,Bq ` 2wpAq \u00b4 wpA,Bq.\nRearranging and simplifying,\np|A| \u00b4 1qp|B| ` 1q \u03b5 n wpA,Bq ` p|A| \u00b4 1qp1 ` \u03b5{nqwpA,Bq \u011b 2|B|wpAq.\nSince |B| ` 1 \u010f n, this gives |A|wpA,Bq \u011b 2p1\u00b4 \u03b5q|B|wpAq.\nProceeding similarly with B and summing the two inequalities yields the lemma.\nProof of Theorem 6.5. We first show the approximation guarantee. We proceed by induction on the number of vertices. The base case is trivial. By inductive hypothesis,\nvalpT q \u011b nwpA,Bq ` 2 3 \u00a8 p1\u00b4 \u03b5qp|A|wpAq ` |B|wpBqq,\nwhere n \u201c |A| ` |B|. Lemma 6.6 implies\nnwpA,Bq \u201c p|A| ` |B|qwpA,Bq \u011b 2p1 \u00b4 \u03b5qp|B|wpAq ` |A|W pBqq.\nHence, |A| ` |B|\n3 wpA,Bq \u011b 2 3 p1\u00b4 \u03b5qp|B|wpAq ` |A|wpBqq.\nTherefore,\nvalpT q \u011b 2n 3 p1\u00b4 \u03b5qpwpA,Bq ` wpAq ` wpBqq \u201c p1\u00b4 \u03b5q2n 3\n\u00ff\ne\nwpeq.\nTo analyze the running time, observe that by Theorem 6.4, a recursive call on a graph G1 \u201c pV 1, E1q takes time rOp|V 1|p|V 1| ` |E1|q{\u03b5q and that the depth of the recursion is Opnq.\nRemark 6.7. The average-linkage and the recursive locally-densest-cut algorithms achieve an Opgnq- and Ophnq-approximation respectively, for any admissible cost function f , where gn \u201c maxn fpnq{fprn{2sq. hn \u201c maxn fpnq{fpr2n{3sq. An almost identical proof yields the result.\nRemark 6.8. In Section 8, we show that other commonly used algorithms, such as completelinkage, single-linkage, or bisection 2-Center, can perform arbitrarily badly (see Theorem 8.6). Hence average-linkage is more robust in that sense."}, {"heading": "7 Perfect Ground-Truth Inputs and Beyond", "text": "In this section, we focus on ground-truth inputs. We state that when the input is a perfect groundtruth input, commonly used algorithms (single linkage, average linkage, and complete linkage; as well as some divisive algorithms \u2013 the bisection k-Center and sparsest-cut algorithms) yield a tree of optimal cost, hence (by Definition 3.1) a ground-truth tree. Some of those results are folklore (and straightforward when there are no ties), but we have been unable to pin down a reference, so we include them for completeness (Section 7.1). We also introduce a faster optimal algorithm for \u201cstrict\u201d ground-truth inputs (Section 7.2). The proofs present no difficulty. The meat of this section is Subsection 7.3, where we go beyond ground-truth inputs; we introduce \u03b4-adversariallyperturbed ground-truth inputs and design a simple, more robust algorithm that, for any admissible objective function, is a \u03b4-approximation."}, {"heading": "7.1 Perfect Ground-Truth Inputs are Easy", "text": "Algorithm 6 Linkage Algorithm for Hierarchical Clustering (similarity setting)\n1: Input: A graph G \u201c pV,Eq with edge weights w : E \u00de\u00d1 R` 2: Create n singleton trees. Root labels: C \u201c ttv1u, . . . , tvnuu\n3: Define dist : C \u02c6 C \u00de\u00d1 R`: distpC1, C2q \u201c $ \u2019\u2019& \u2019\u2019% 1 |C1||C2| \u0159 xPC1,yPC2 wppx, yqq Average Linkage\nminxPC1,yPC2 wppx, yqq Single Linkage maxxPC1,yPC2 wppx, yqq Complete Linkage\n.\n4: while there are at least two trees do 5: Take the two trees with root labels C1, C2 such that distpC1, C2q is maximum 6: Create a new tree by making those two tree children of a new root node labeled C1 Y C2 7: Remove C1, C2 from C, add C1 Y C2 to C, and update dist 8: end while 9: return the resulting binary tree T\nIn the following, we refer to the tie breaking rule of Algorithm 6 as the rule followed by the algorithm for deciding which of Ci, Cj or Ck, C\u2113 to merge, when maxC1,C2PC distpC1, C2q \u201c distpCi, Cjq \u201c distpCk, C\u2113q.\nTheorem 7.1. 15 Assume that the input is a (dissimilarity or similarity) ground-truth input. Then, for any admissible objective function, the agglomerative heuristics average-linkage, single-linkage, and complete-linkage (see Algorithm 6) return an optimal solution. This holds no matter the tie breaking rule of Algorithm 6.\n15This Theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.\nProof. We focus on the similarity setting; the proof for the dissimilarity setting is almost identical. We define the candidate trees after t iterations of the while loop to be sets of trees in C at that time. The theorem follows from the following statement, which we will prove by induction on t: If Ct \u201c tC1, . . . , Cku denotes the set of clusters after t iterations, then there exists a generating tree T t for G, such that the candidate trees are subtrees of T t.\nFor the base case, initially each candidate tree contains exactly one vertex and the statement holds. For the general case, let C1, C2 be the two trees that constitute the t\nth iteration. By induction, there exists a generating tree T t\u00b41 for G, and associated weights W t\u00b41 (according to Definition 2.2) such that C1 and C2 are subtrees of T t\u00b41, rooted at nodes N1 and N2 of T t\u00b41 respectively. To define T t, we start from T t\u00b41. Consider the path P \u201c tN1, N1, N2, . . . , Nk, N2u joining N1 to N2 in T t\u00b41 and let Nr \u201c LCAT t\u00b41pN1, N2qq. If Nr is the parent of N1 and N2, then T t \u201c tt\u00b41, else do the following transformation: remove the subtrees rooted at N1 and at N2; create a new node N\u02da as second child of Nk, and let N1 and N2 be its children. This defines T\nt. To define W t, extend W t\u00b41 by setting W tpN\u02daq \u201c W pNrq. Claim 7.2. For any Ni, Nj P P , W t\u00b41pNiq \u201c W t\u00b41pNjq.\nThanks to the inductive hypothesis, with Claim 7.2 it is easy to verify that W t certifies that T t\nis generating for G.\nProof of Claim 7.2. Fix a node Ni on the path from Nr to N 1 (the argument for nodes on the path from Nr to N 2 is similar). By induction W t\u00b41pNiq \u011b W t\u00b41pNrq. We show that since the linkage algorithms merge the trees C1 and C2, we also have W t\u00b41pNiq \u010f W t\u00b41pNrq and so W t\u00b41pNiq \u201c W t\u00b41pNrq, hence the claim. Let w0 \u201c W t\u00b41pNrq. By induction, for all u P C1, v P C2, wpu, vq \u201c w0, and thus distpC1, C2q \u201c w0 in the execution of all the algorithms. Fix a candidate tree C 1 P Ct, C 1 \u2030 C1, C2 and C 1 \u010e V pNiq. Since C is a partition of the vertices of the graph and since candidate trees are subtrees of T t\u00b41, such a cluster exists. Thus, for u P C1, v P C 1 wpu, vq \u201c W t\u00b41pLCAT t\u00b41pu, vqq \u201c W t\u00b41pNiq \u011b w0 since Ni is a descendant of Nr.\nIt is easy to check that by their definitions, for any of the linkage algorithms, we thus have that distpC1, C 1q \u011b w0 \u201c distpC1, C2q. But since the algorithms merge the clusters at maximum distance, it follows that distpC1, C 1q \u010f distpC1, C2q \u201c w0 and therefore, W t\u00b41pNiq \u010f W t\u00b41pNrq and so, W t\u00b41pNiq \u201c W t\u00b41pNrq and the claim follows. This is true no matter the tie breaking chosen for the linkage algorithms.\nDivisive Heuristics. In this section, we focus on two well-known divisive heuristics: (1) the bisection 2-Center which uses a partition-based clustering objective (the k-Center objective) to divide the input into two (non necessarily equal-size) parts (see Algorithm 7), and (2) the recursive sparsest-cut algorithm, which can be implemented efficiently for ground-truth inputs (Lemma 7.6).\nAlgorithm 7 Bisection 2-Center (similarity setting)\n1: Input: A graph G \u201c pV,Eq and a weight function w : E \u00de\u00d1 R` 2: Find tu, vu \u010e V that maximizes minxmaxyPtu,vu wpx, yq 3: A \u00d0 tx | wpx, uq \u011b maxyPtu,vu wpx, yqu 4: B \u00d0 V zA. 5: Apply Bisection 2-Center on GrAs and GrBs to obtain trees TA,TB respectively 6: return The union tree of TA, TB .\nLoosely speaking, we show that this algorithm computes an optimal solution if the optimal solution is unique. More precisely, for any similarity graph G, we say that a tree T is strictly generating for G if there exists a weight function W such that for any nodes N1, N2, if N1 appears on the path from N2 to the root, then W pN1q \u0103 W pN2q and for every x, y P V , wpx, yq \u201c W pLCAT px, yqq. In this case we say that the input is a strict ground-truth input. In the context of dissimilarity, an analogous notion can be defined and we obtain a similar result.\nTheorem 7.3. 16 For any admissible objective function, the bisection 2-Center algorithm returns an optimal solution for any similarity or dissimilarity graph G that is a strict ground-truth input.\nProof. We proceed by induction on the number of nodes in the graph. Consider a strictly generating tree T and the corresponding weight function W . Consider the root node Nr of T and let N1, N2 be the children of the root. Let p\u03b1, \u03b2q be the cut induced by the root node of T (i.e., \u03b1 \u201c V pN1q, \u03b2 \u201c V pN2q). Define w0 to be the weight of an edge between u P \u03b1 and v P \u03b2 for any u, v (recall that since T is strictly generating all the edges between \u03b1 and \u03b2 are of same weight). We show that the bisection 2-Centers algorithm divides the graph into \u03b1 and \u03b2. Applying the inductive hypothesis on both subgraphs yields the result.\nSuppose that the algorithm locates the two centers in \u03b2. Then, minxmaxyPtu,vu wpx, yq \u201c w0 since the vertices of \u03b1 are connected by an edge of weight w0 to the centers. Thus, the value of the clustering is w0. Now, consider a clustering consisting of a center c0 in \u03b1 and a center c1 in \u03b2. Then, for each vertex u, we have maxcPtc0,c1uwpu, cq \u011b minpW pN1q,W pN2qq \u0105 W pNrq \u201c w0 since T and W are strictly generating; Hence a strictly better clustering value. Therefore, the algorithm locates x P \u03b1 and y P \u03b2. Finally, it is easy to see that the partitioning induced by the centers yields parts A \u201c \u03b1 and B \u201c \u03b2.\nRemark 7.4. To extend our result to (non-strict) ground-truth inputs, one could consider the following variant of the algorithm (which bears similarities with the popular elbow method for partition-based clustering): Compute a k-Center clustering for all k P t1, . . . , nu and partition the graph according to the k-Center clustering of the smallest k \u0105 1 for which the value of the clustering increases. Mimicking the proof of Theorem 7.3, one can show that the tree output by the algorithm is generating.\nWe now turn to the recursive sparsest-cut algorithm (i.e., the recursive \u03c6-sparsest-cut algorithm of Section 4, for \u03c6 \u201c 1). The recursive sparsest-cut consists in recursively partitioning the graph according to a sparsest cut of the graph. We show (1) that this algorithm yields a tree of optimal cost and (2) that computing a sparsest cut of a similarity graph generated from an ultrametric can be done in linear time. Finally, we observe that the analogous algorithm for the dissimilarity setting consists in recursively partitioning the graph according to the densest cut of the graph and achieves similar guarantees (and similarly the densest cut of a dissimilarity graph generated from an ultrametric can be computed in linear time).\nTheorem 7.5. 17 For any admissible objective function, the recursive sparsest-cut (respectively densest-cut) algorithm computes a tree of optimal cost if the input is a similarity (respectively dissimilarity) ground-truth input.\nProof. The proof, by induction, has no difficulty and it may be easier to recreate it than to read it. Let T be a generating tree and W be the associated weight function. Let Nr be the root of T , N1, N2 the children of Nr, and p\u03b1 \u201c V pN1q, \u03b2 \u201c V pN2qq the induced root cut. Since T is strictly 16This Theorem may be folklore, but we have been unable to find a reference. 17This Theorem may be folklore, at least when there are no ties, but we have been unable to find a reference.\ngenerating, all the edges between \u03b1 and \u03b2 are of same weight w, which is therefore also the sparsity of p\u03b1, \u03b2q. For every edge pu, vq of the graph, wpu, vq \u201c W pLCAT pu, vqq \u011b w, so every cut has sparsity at least w, so p\u03b1, \u03b2q has minimum sparsity.\nNow, consider the tree T \u02da computed by the algorithm, and let p\u03b3, \u03b4q denote the sparsest-cut used by the algorithm at the root (in case of ties it might not different from p\u03b1, \u03b2q). By induction the algorithm on Gr\u03b3s and Gr\u03b4s gives two generating trees T\u03b3 and T\u03b4 with associated weight functions W\u03b3 and W\u03b4. To argue that T\n\u02da is generating, we define W \u02da as follows, where N\u02dar denotes the root of T \u02da.\nW \u02dapNq \u201c $ \u2019& \u2019%\nW\u03b3pNq if N P T\u03b3 W\u03b4pNq if N P T\u03b4 w if N \u201c N\u02dar\nBy induction wpu, vq \u201c W pLCAT pu, vqq if either both u, v P \u03b3, or both u, v P \u03b4. For any u P \u03b3, v P \u03b4, we have wpu, vq \u201c w \u201c W pN\u02dar q \u201c W pLCAT pu, vqq. Finally, since w \u010f wpu, vq for any u, v, we have W pN\u02dar q \u201c w \u010f W pNq, for any N P T \u02da, and therefore T \u02da is generating.\nWe then show how to compute a sparsest-cut of a graph that is a ground-truth input.\nLemma 7.6. If the input graph is a ground-truth input then the sparsest cut is computed in Opnq time by the following algorithm: pick an arbitrary vertex u, let wmin be the minimum weight of edges adjacent to u, and partition V into A \u201c tx | wpu, xq \u0105 wminu and B \u201c V zA.\nProof. Let wmin \u201c wpu, vq. We show that wpA,Bq{p|A||B|q \u201c wmin and since wmin is the minimum edge weight of the graph, that the cut pA,Bq only contains edges of weight wmin. Fix a generating tree T . Consider the path from u to the root of T and let N0 be the first node on the (bottom-up) path such that W pN0q \u201c wmin. For any vertex x P A, we have that wpu, xq \u0105 wmin. Hence by definition, we have that N0 is an ancestor of LCAT pu, xq. Therefore, for any other node y such that wpu, yq \u201c wmin, we have LCAT pu, yq \u201c LCAT px, yq and so, wpx, yq \u201c W pLCAT px, yqq \u201c W pLCAT pu, yqq \u201c wmin. It follows that all the edges in the cut pA,Bq are of weight wmin and so, the cut is a sparsest cut."}, {"heading": "7.2 A Near-Linear Time Algorithm", "text": "In this section, we propose a simple, optimal, algorithm for computing a generating tree of a groundtruth input. For any graph G, the running time of this algorithm is Opn2q, and rOpnq if there exists a tree T that is strictly generating for the input. For completeness we recall that for any graph G, we say that a tree T is strictly generating for G if there exists a weight function W such that for any nodes N1, N2, if N1 appears on the path from N2 to the root, then W pN1q \u0103 W pN2q and for every x, y P V , wpx, yq \u201c W pLCAT px, yqq. In this case we say that the inputs is a strict ground-truth input.\nThe algorithm is described for the similarity setting but could be adapted to the dissimilarity case to achieve the same performances.\nTheorem 7.7. For any admissible objective function, Algorithm 8 computes a tree of optimal cost in time Opn log2 nq with high probability if the input is a strict ground-truth input or in time Opn2q if the input is a (non-necessarily strict) ground-truth input.\nProof. We proceed by induction on the number of vertices in the graph. Let p be the first pivot chosen by the algorithm and let B1, . . . , Bk be the sets defined by p at Step 4 of the algorithm, with wpp, uq \u0105 wpv, pq, for any u P Bi, v P Bi`1.\nAlgorithm 8 Fast and Simple Algorithm for Hierarchical Clustering on Perfect Data (similarity setting)\n1: Input: A graph G \u201c pV,Eq and a weight function w : E \u00de\u00d1 R` 2: p \u00d0 random vertex of V 3: Let w1 \u0105 . . . \u0105 wk be the edge weights of the edges that have p as an endpoint 4: Let Bi \u201c tv | wpp, vq \u201c wiu, for 1 \u010f i \u010f k. 5: Apply the algorithm recursively on each GrBis and obtain a collection of trees T1, . . . , Tk 6: Define T \u02da0 as a tree with p as a single vertex 7: For any 1 \u010f i \u010f k, define T \u02dai to be the union of T \u02dai\u00b41 and Ti 8: Return T \u02dak\nWe show that for any u P Bi, v P Bj, j \u0105 i, we have wpu, vq \u201c wpp, vq. Consider a generating tree T and define N1 \u201c LCAT pp, uq and N2 \u201c LCAT pp, vq. Since T, h, \u03c3 is generating and wpp, uq \u0105 wpp, vq, we have that N2 is an ancestor of N1, by Definition 2.2. Therefore, LCAT pu, vq \u201c N2, and so wpu, vq \u201c W pN2q \u201c wpp, vq. Therefore, combining the inductive hypothesis on any GrBis and by Definition 2.2 the tree output by the algorithm is generating.\nA bound of Opn2q for the running time follows directly from the definition of the algorithm. We now argue that the running time is Opn log2 nq with high probability if the input is strongly generated from a tree T . First, it is easy to see that a given recursive call on a subgraph with n0 vertices takes Opn0q time. Now, observe that if at each recursive call the pivot partitions the n0 vertices of its subgraph into buckets of size at most 2n0{3, then applying the master theorem implies a total running time of Opn log nq. Unfortunately, there are trees where picking an arbitrary vertex as a pivot yields a single bucket of size n\u00b4 1.\nThus, consider the node N of T that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T rooted at N contains fewer than 2n{3 but at least n{3 leaves. Since T is strongly generating we have that the partition into B1, . . . , Bk induced by any vertex v P V pNq is such that any Bi contains less than 2n{3 vertices. Indeed, for any u such that LCAT pu, vq is an ancestor of N and x P V pNq, we have that wpu, vq \u0103 wpx, vq, and so u and x belong to different parts of the partition B1, . . . , Bk.\nSince the number of vertices in V pNq is at least n{3, the probability of picking one of them is at least 1{3. Therefore, since the pivots are chosen independently, after c log n recursive calls, the probability of not picking a vertex of V pNq as a pivot is Op1{ncq. Taking the union bound yields the theorem."}, {"heading": "7.3 Beyond Structured Inputs", "text": "Since real-world inputs might sometimes differ from our definition of ground-truth inputs introduced in the Section 2, we introduce the notion of \u03b4-adversarially-perturbed ground-truth inputs. This notion aims at accounting for noise in the data. We then design a simple and arguably more reliable algorithm (a robust variant of Algorithm 8) that achieves a \u03b4-approximation for \u03b4-adversariallyperturbed ground-truth inputs in Opnpn `mqq time. An interesting property of this algorithm is that its approximation guarantee is the same for any admissible objective function.\nWe first introduce the definition of \u03b4-adversarially-perturbed ground-truth inputs. For any real \u03b4 \u011b 1, we say that a weighted graph G \u201c pV,E,wq is a \u03b4-adversarially-perturbed ground-truth input if there exists an ultrametric pX, dq, such that V \u010e X, and for every x, y P V, x \u2030 y, e \u201c tx, yu exists, and fpdpx, yqq \u010f wpeq \u010f \u03b4fpdpx, yqq, where f : R` \u00d1 R` is a non-increasing function. This defines \u03b4-adversarially-perturbed ground-truth inputs for similarity graphs and an analogous\ndefinition applies for dissimilarity graphs. We now introduce a robust, simple version of Algorithm 8 that returns a \u03b4-approximation if the input is a \u03b4-adversarially-perturbed ground-truth inputs. Algorithm 8 was partitioning the input graph based on a single, random vertex. In this slightly more robust version, the partition is built iteratively: Vertices are added to the current part if there exists at least one vertex in the current part or in the parts that were built before with which they share an edge of high enough weight (see Algorithm 9 for a complete description).\nAlgorithm 9 Robust and Simple Algorithm for Hierarchical Clustering on \u03b4-adversariallyperturbed ground-truth inputs (similarity setting)\n1: Input: A graph G \u201c pV,Eq and a weight function w : E \u00de\u00d1 R`, a parameter \u03b4 2: p \u00d0 arbitrary vertex of V 3: i \u00d0 0 4: rVi \u00d0 tpu 5: while rVi \u2030 V do 6: Let p1 P rVi, p2 P V zrVi s.t. pp1, p2q is an edge of maximum weight in the cut prVi, V zrViq 7: wi \u00d0 wpp1, p2q 8: Bi \u00d0 tu | wpp1, uq \u201c wiu 9: while Du P V zp rVi YBiq s.t. Dv P Bi Y rVi, wpu, vq \u011b wi do\n10: Bi \u00d0 Bi Y tuu. 11: end while 12: rVi`1 \u00d0 rVi YBi 13: i \u00d0 i` 1 14: end while 15: Let B1, . . . , Bk be the sets obtained 16: Apply the algorithm recursively on each GrBis and obtain a collection of trees T1, . . . , Tk 17: Define T \u02da0 as a tree with p as a single vertex 18: For any 1 \u010f i \u010f k, define T \u02dai to be the union of T \u02dai\u00b41 and Ti 19: Return T \u02dak\nTheorem 7.8. For any admissible objective function, Algorithm 9 returns a \u03b4-approximation if the input is a \u03b4-adversarially-perturbed ground-truth input.\nTo prove the theorem we introduce the following lemma whose proof is temporarily differed. The lemma states that the tree built by the algorithm is almost generating (up to a factor of \u03b4 in the edge weights).\nLemma 7.9. Let T be a tree output by Algorithm 9, let N be the set of internal nodes of T . For any node N with children N1, N2 there exists a function \u03c9 : N \u00de\u00d1 R`, such that for any u P V pN1q, v P V pN2q, \u03c9pNq \u010f wpu, vq \u010f \u03b4\u03c9pNq. Moreover, for any nodes N,N 1, if N 1 is an ancestor of N , we have that \u03c9pNq \u011b \u03c9pN 1q.\nAssuming Lemma 7.9, the proof of Theorem 7.8 is as follows.\nProof of Theorem 7.8. Let G \u201c pV,Eq, w : E \u00de\u00d1 R` be the input graph and T \u02da be a tree of optimal cost. By Lemma 7.9, the tree T output by the algorithm is such that for any node N with children N1, N2 there exists a real \u03c9pNq, such that for any u P V pN1q, v P V pN2q, \u03c9 \u010f wpu, vq \u010f \u03b4\u03c9. Thus, consider the slightly different input graph G1 \u201c pV,E,w1q, where w1 : E \u00de\u00d1 R` is defined as follows.\nFor any edge pu, vq, define w1pu, vq \u201c \u03c9pLCAT pu, vqq. Since by Lemma 7.9, for any nodes N,N 1 of T , if N 1 is an ancestor of N , we have that \u03c9pNq \u011b \u03c9pN 1q and by definition 2.2, T is generating for G1. Thus, for any admissible cost function, we have that for G1, costG1pT q \u010f costG1pT \u02daq.\nFinally, observe that for any edge e, we have w1peq \u010f wpeq \u010f \u03b4w1peq. It follows that costGpT q \u010f \u03b4costG1pT q for any admissible cost function and costG1pT \u02daq \u010f costGpT \u02daq. Therefore, costGpT q \u010f \u03b4costGpT \u02daq \u201c \u03b4OPT.\nProof of Lemma 7.9. We proceed by induction on the number of vertices in the graph (the base case is trivial). Consider the first recursive call of the algorithm. We show the following claim. Claim 7.10. For any 1 \u010f i \u010f k, for any y P rVi, x P Bi, wi \u011b wpx, yq \u011b wi{\u03b4. Additionally, for any x, y P Bi, wpx, yq \u011b wi{\u03b4.\nWe first argue that Claim 7.10 implies the lemma. Let T be the tree output by the algorithm. Consider the nodes on the path from p to the root of T ; Let Ni denote the node whose subtree is the union of T \u02dai\u00b41 and Ti. By definition, V pT \u02dai\u00b41q \u201c rVi and V pTiq \u201c Bi. Applying Claim 7.10 and observing that wi \u0105 wi`1 implies that the lemma holds for all the nodes on the path. Finally, since for any edge tu, vu, for u, v P Bi, we also have wpu, vq \u011b wi{\u03b4, combining with the inductive hypothesis on Bi implies the lemma for all the nodes of the subtree Ti.\nProof of Claim 7.10. Let pX, dq and f be a pair of ultrametric and function that is generating for G. Fix i P t1, . . . , ku. For any vertex x P Bi, let \u03c3pxq denote a vertex y that is in rVi or inserted to Bi before x and such that wpy, xq \u201c wi. For any vertex v, let \u03c3ipxq denotes the vertex obtained by applying \u03c3 i times to x (i.e., \u03c32pxq \u201c \u03c3p\u03c3pxqq). By definition of the algorithm that for any x P Bi, Ds \u011b 1, such that \u03c3spxq P rVi.\nFix x P Bi. For any y P rVi, we have that wpy, xq \u010f wi since otherwise, the algorithm would have added x before.\nNow, let y P rVi or y inserted to Bi prior to x. We aim at showing that wpy, xq \u011b wi{\u03b4. Observe that since X, d is an ultrametric, dpx, yq \u010f maxpdpx, \u03c3pxqq, dp\u03c3pxq, yqq.\nWe now \u201cfollow\u201d \u03c3 by applying the function \u03c3 to \u03c3pxq and repeating until we reach \u03c3pxq\u2113 \u201c z P rVi. Combining with the definition of an ultrametric, it follows that\ndpx, yq \u010f maxpdpx, \u03c3pxqq, dp\u03c3pxq, \u03c32pxqq, . . . , dp\u03c3pxq\u2113\u00b41, zq, dpz, yqq.\nIf y was in rVi, we define y\u0302 \u201c y. Otherwise y is also in Bi (and so was added to Bi before x). We then proceed similarly than for x and \u201cfollow\u201d \u03c3. In this case, let y\u0302 \u201c \u03c3kpyq P rVi. Applying the definition of an ultrametric again, we obtain\ndpx, yq \u010f maxpdpx, \u03c3pxqq, dp\u03c3pxq, \u03c32pxqq, . . . , dp\u03c3\u2113\u00b41, zq, dpz, y\u0302q, dpy, \u03c3pyqq, . . . , dp\u03c3k\u00b41pyq, y\u0302qq.\nAssume for now that dpz, y\u0302q is not greater than the others. Applying the definition of a \u03b4adversarially-perturbed input, we have that\n\u03b4wpx, yq \u011b minp. . . , wp\u03c3apxq, \u03c3a`1pxqq, . . . , wp\u03c3bpyq, \u03c3b`1pyqq, . . .q.\nFollowing the definition of \u03c3, we have wpv, \u03c3pvqq \u011b wi, @v. Therefore, we conclude \u03b4wpx, yq \u011b wi. We thus turn to the case where dpz, y\u0302q is greater than the others. Since both z, y\u0302 P rVi, we have that they belong to some Bj0 , Bj1 , where j0, j1 \u0103 i. We consider the minimum j such that a pair at distance at least dpz, y\u0302q was added to rVj . Consider such a pair u, v P rVj satisfying dpu, vq \u011b dpz, y\u0302q and suppose w.l.o.g that v P Bj\u00b41 (we could have either u P Bj\u00b41 or u P rVj\u00b41). Again, we follow the\npath \u03c3pvq, \u03c3p\u03c3pvqq, . . ., until we reach \u03c3r1pvq P rVj\u00b41 and similarly for u: \u03c3r2puq P rVj\u00b41. Applying the definition of an ultrametric this yields that\ndpu, vq \u010f maxp. . . , dp\u03c3apuq, \u03c3a`1puqq, . . . , dp\u03c3bpvq, \u03c3b`1pvqq, . . . , dp\u03c3r1pvq, \u03c3r2puqqq. (14)\nNow the difference is that rVj\u00b41 does not contain any pair at distance at least dpz, y\u0302q. Therefore, we have dp\u03c3r1pvq, \u03c3r2puqq \u0103 dpz, y\u0302q. Moreover, recall that by definition of u, v, dpz, y\u0302q \u010f dpu, vq. Thus, dp\u03c3r1pvq, \u03c3r2puqq is not the maximum in Equation 14 since it is smaller than the left-hand side. Simplifying Equation 14 yields\ndpx, yq \u0103 dpz, y\u0302q \u010f dpu, vq \u010f maxp. . . , dp\u03c3apuq, \u03c3a`1puqq, . . . , dp\u03c3bpvq, \u03c3b`1pvqq, . . .q.\nBy definition of a \u03b4-adversarially-perturbed input, we obtain \u03b4wpx, yq \u011b min\u2113 wp\u03c3\u2113pbq, \u03c3\u2113`1pbqq \u011b wj . Now, it is easy to see that for j \u0103 i, wi \u0103 wj and therefore \u03b4wpx, yq \u011b wi.\nWe conclude that for any y P rVi, x P Bi, wi \u011b wpx, yq \u011b wi{\u03b4 and for x, y P Bi, we have that wpx, yq \u011b wi{\u03b4, as claimed."}, {"heading": "8 Worst-Case Analysis of Common Heuristics", "text": "The results presented in this section shows that for both the similarity and dissimilarity settings, some of the widely-used heuristics may perform badly. The proofs are not difficult nor particularly interesting, but the results stand in sharp contrast to structured inputs and help motivate our study of inputs beyond worst case.\nSimilarity Graphs. We show that for very simple input graphs (i.e., unweighted trees), the linkage algorithms (adapted to the similarity setting, see Algorithm 6) may perform badly.\nTheorem 8.1. There exists an infinite family of inputs on which the single-linkage and completelinkage algorithms output a solution of cost \u2126pnOPT{ log nq.\nProof. The family of inputs consists of the graphs that represent paths of length n \u0105 2. More formally, Let Gn be a graph on n vertices such that V \u201c tv1, . . . , vnu and that has the following edge weights. Let wpvi\u00b41, viq \u201c wpvi, vi`1q \u201c 1, for all 1 \u0103 i \u0103 n and for any i, j, j R ti\u00b41, i, i`1u, define wpvi, vjq \u201c 0. Claim 8.2. OPTpGnq \u201c Opn log nq.\nProof. Consider the tree T \u02da that recursively divides the path into two subpaths of equal length. We aim at showing that costpT \u02daq \u201c Opn log nq. The cost induced by the root node is n (since there is only one edge joining the two subpaths). The cost induced by each child of the root is n{2 since there is again only one edge joining the two sub-subpaths and now only n{2 leaves in the two subtrees. A direct induction shows that for a descendant at distance i from the root, the cost induced by this node is n{2i. Since the number of children at distance i is 2i, we obtain that the total cost induced by all the children at distance i is n. Since the tree divides the graph into two subgraph of equal size, there are at most Oplog nq levels and therefore, the total cost of T (and so OPTpGnq) is Opn log nq.\nComplete-Linkage. We show that the complete-linkage algorithm could perform a sequence of merges that would induce a tree of cost \u2126pn2q. At start, each cluster contains a single vertex and so, the algorithm could merge any two clusters tviu, tvi`1u with 1 \u010f i \u0103 n since their distance\nare all 1 (and it is the maximum edge weight in the graph). Suppose w.l.o.g that the algorithm merges v1, v2. This yields a cluster C1 such that the maximum distance between vertices of C1 and v3 is 1. Thus, assume w.l.o.g that the second merge of the algorithm is C1, v3. Again, this yields a cluster C2 whose maximum distance to v4 is also 1. A direct induction shows that the algorithm output a tree whose root induces the cut pV ztvnu, vnq and one of the child induces the cut pV ztvn\u00b41, vnu, vn\u00b41q and so on. We now argue that this tree T\u0302 has cost \u2126pn2q. Indeed, for any 1 \u0103 i \u010f n, we have V pLCA\nT\u0302 pvi\u00b41, viqq \u201c i. Thus the cost is at least \u0159n i\u201c2 i \u201c \u2126pn2q.\nSingle-Linkage. We now turn to the case of the single-linkage algorithm. Recall that the algorithm merges the two candidate clusters Ci, Cj that minimize wpu, vq for u P Ci, v P Cj.\nAt start, each cluster contains a single vertex and so, the algorithm could merge any two clusters tviu, tvju for j R ti \u00b4 1, i, i ` 1u since the edge weight is 0 (and it is the minimum edge weight). Suppose w.l.o.g that the algorithm merges v1, v3. This yields a cluster C1 such that the distance between vertices of C1 and any vi for i \u201c 1 mod 2 is 0. Thus, assume w.l.o.g that the second merge of the algorithm is C1, v5. A direct induction shows that w.l.o.g the output tree contains a node rN such that V p rN q contains all the vertices of odd indices. Now observe that the cost of the tree is at least |V p rNq| \u00a8 wpV p rN q, V zV p rNqq \u201c \u2126pn2q.\nThus, by Claim 8.2 the single-linkage and complete-linkage algorithms can output a solution of cost \u2126pnOPT{ log nq.\nTheorem 8.3. There exists an infinite family of inputs on which the average-linkage algorithm output a solution of cost \u2126pn1{3OPTq.\nProof. For any n \u201c 2i for some integer i, we define a tree Tn \u201c pV,Eq as follows. Let k \u201c n1{3. Let P \u201c pu1, . . . , ukq be a path of length k (i.e., for each 1 \u010f i \u0103 k, we have an edge between ui and ui`1). For each ui, we define a collection Pi \u201c tP i1 \u201c pV i1 , Ei1q, . . . , P ik \u201c pV ik , Eikqu of k paths of length k and for each P ij we connect one of its extremities to ui. Define Vi \u201c tuiu \u0164 j V i j .\nClaim 8.4. OPTpTnq \u010f 3n4{3\nProof. Consider the following non-binary solution tree T \u02da: Let the root have children N1, . . . , Nk such that V pNiq \u201c Vi and for each child Ni let it have children N ji such that V pN j i q \u201c V j i . Finally, for each N ji let the subtree rooted at N j i be any tree.\nWe now analyze the cost of T \u02da. Observe that for each edge e in the path P , we have |V pLCAT\u02dapeqq| \u201c n. Moreover, for each edge e connecting a path P ij to ui, we have |V pLCAT\u02dapeqq| \u201c k2 \u201c n2{3. Finally, for each edge e whose both endpoints are in a path P ij , we have that |V pLCAT\u02dapeqq| \u010f k \u201c n1{3.\nWe now sum up over all edges to obtain the overall cost of Tn. There are k \u201c n1{3 edges in P ; They incur a cost of nk \u201c n4{3. There are k2 edges joining a vertex ui to a path P ij ; They incur a cost of k2 \u00a8 n2{3 \u201c n4{3. Finally, there are k3 edges whose both endpoints are in a path P ij ; They incur a cost of k3 \u00a8 n1{3 \u010f n4{3. Thus, the total cost of this tree is at most 3n4{3 \u011b OPTpTnq.\nWe now argue that there exists a sequence of merges done by the average-linkage algorithm that yield a solution of cost at least n5{3.\nClaim 8.5. There exists a sequence of merges and an integer t such that the candidate trees at time t have leaves sets ttu1, . . . , ukuu \u0164 i,jtV ij u.\nEquipped with this claim, we can finish the proof of the proposition. Since there is no edge between V ij and V i1 j1 for i 1 \u2030 i or j1 \u2030 j the distance between those trees in the algorithm will always be 0. However, the distance between the tree rT that has leaves set tu1, . . . , uku and any other tree\nis positive (since there is one edge joining those two sets of vertices in Tn). Thus, the algorithm will merge rT with some tree whose vertex set is exactly V ij for some i, j. For the same reasons, the resulting cluster will be merged to a cluster whose vertex set is exactly V i 1\nj1 , and so on. Hence, after\nn{2k \u201c k2{2 such merges, the tree rT has a leaves set of size k \u00a8 k2{2 \u201c n{2. However, the number of edges from this cluster to the other candidate clusters is k2{2 (since the other remaining clusters corresponds to vertex sets V ij for some i, j). For each such edge e we have |V pLCAT peqq| \u011b n{2. Since there are k2{2 of them, the resulting tree has cost \u2126pn5{3q. Combining with Claim 8.4 yields the theorem.\nWe thus turn to the proof of Claim 8.5.\nProof of Claim 8.5. Given a graph G a set of candidate trees C, defineG{C to be the graph resulting from the contraction of all the edges whose both endpoints belong to the same cluster. We show a slightly stronger claim. We show that for any graph G and candidate trees V such that\n1. All the candidate clusters in V have the same size; and\n2. There exists a bijection \u03c6 between vertices v P Tn and vertices in G{C;\nThere exists a sequence of merges and an integer t such that the candidate trees at time t have leaves sets tt\u03c6pu1q, . . . , \u03c6pukquu \u0164 i,jt\u03c6pV ij qu where \u03c6pV ij q \u201c t\u03c6pvq | v P V ij u.\nThis slightly stronger statement yields the claim by observing that Tn and the candidate trees at the start of the algorithm satisfies the conditions of the statement.\nWe proceed by induction on the number of vertices of the graph. Let V ij \u201c tvijp1q, . . . , vijpkqu such that pvijp\u2113q, vijp\u2113` 1qq P Eij for any 1 \u010f \u2113 \u0103 k, and pvijpkq, uiq P E.\nWe argue that the algorithm could perform a sequence of merges that results in the following set C of candidate trees. C contains candidate trees U i \u201c \u03c6pu2i\u00b41qY \u03c6pu2iq for 1 \u010f i \u0103 k{2, and for each i, j, candidate trees vi,j,\u2113 \u201c \u03c6pvijp2\u2113\u00b4 1q Y \u03c6pvijp2\u2113qq, for 1 \u010f \u2113 \u0103 k{2. Let s0 be the number of vertices in each candidate tree.\nAt first, all the trees contain a single vertex and so, for each adjacent vertices of the graph the distance between their corresponding trees in the algorithm is 1{s0. For any non-adjacent pair of vertices, the corresponding trees are at distance 0. Thus, w.l.o.g assume the algorithm first merges u1, u2. Then, the distance between the newly created tree U\n1 and any other candidate tree C is 0 if there is no edge between u1 and u2 and C or 1{p2s0q if there is one (since U1 contains now two vertices). For the other candidate trees the distance is unchanged. Thus, the algorithm could merge vertices u3, u4. Now, observe that the distance between U\n2 and U1 is at most 1{p4s0q. Thus, it is possible to repeat the argument and assume that the algorithm merges the candidate trees corresponding to u5, u6. Repeating this argument k{2 times yields that after k{2 merges, the algorithm has generated the candidates trees U1, . . . , Uk{2\u00b41. The other candidate trees still contain a single vertex. Thus, the algorithm is now forced to merge candidate trees that contains single vertices that are adjacent (since their distance is 1{s0 and any other distance is \u0103 1{s0). Assume, w.l.o.g, that the algorithm merges v11p1q, v11p2q. Again, applying a similar reasoning to each v11p2\u2113 \u00b4 1q, v11p2\u2113q yields the set of candidate clusters v1,1,1, . . . , v1,1,k{2\u00b41. Applying this argument to all sets V ij yields that the algorithm could perform a sequence of merges that results in the set C of candidate clusters described above.\nNow, all the clusters have size 2s0 and there exists a bijection between vertices of G{C and Tn{2. Therefore, combining with the induction hypothesis yields the claim.\nDissimilarity Graphs. We now show that single-linkage, complete-linkage, and bisection 2- Center might return a solution that is arbitrarily bad compared to OPT in some cases. Hence, since average-linkage achieves a 2-approximation in the worst-case it seems that it is more robust than the other algorithms used in practice.\nTheorem 8.6. For each of the single-linkage, complete-linkage, and bisection 2-Center algorithms, there exists a family of inputs for which the algorithm outputs a solution of value OpOPT{nq. Proof. We define the family of inputs as follow. For any n \u0105 2, the graph Gn consists of n vertices V \u201c tv1, . . . , vn\u00b41, uu and the edge weights are the following: For any i, j P t1, . . . , n\u00b4 1u, wpvi, vjq \u201c 1, for any 1 \u0103 i \u010f n \u00b4 1, wpvi, uq \u201c 1, and wpv1, uq \u201c W for some fixed W \u011b n3. Consider the tree T \u02da whose root induces a cut pV ztuu, tuuq. Then, the value of this tree (and so OPT) is at least nW , since |V pLCAT\u02dapv1, uqq| \u201c n.\nSingle-Linkage. At start, all the clusters are at distance 1 from each other except v1 and u that are at distance W . Thus, suppose that the first merge generates a candidate tree C1 whose leaves set is tv1, v2u. Now, since wpv2, uq \u201c 1, we have that all the clusters are at distance 1 from each other. Therefore, the next merge could possibly generate the cluster C2 with leaves sets tu, v1, v2u. Assume w.l.o.g that this is the case and let T be the tree output by the algorithm. We obtain |V pLCAT pu, v1qq| \u201c 3 and so, since for any vi, vj , |V pvi, vjq| \u010f n, valpT q \u010f n2 ` 3W \u010f 4W , since W \u0105 W 3. Hence, valpT q \u201c OpvalpT \u02daq{nq.\nComplete-Linkage. Again, at first all the clusters are at distance 1 from each other except v1 and u that are at distance W . Since the algorithm merges the two clusters that are at maximum distance, it merges u and v1. Again, let T be the tree output by the algorithm. We have valpT q \u010f n2 ` 2W \u010f 3W , since W \u0105 W 3. Hence, valpT q \u201c OpvalpT \u02daq{nq.\nBisection 2-Center. It is easy to see that for any location of the two centers, the cost of the clustering is 1. Thus, suppose that the algorithm locates centers in v2, v3 and that the induced partitioning is tv1, v2, uu, V ztv1, v2, uu. It follows that |V pLCAT pu, v1qq| \u010f 3 and so, valpT q \u010f n2 ` 3W \u010f 4W , since W \u0105 W 3. Again, valpT q \u201c OpvalpT \u02daq{nq.\nProposition 8.7. For any input I lying in a metric space, for any solution tree T for I, we have valpT q \u201c OpOPTq. Proof. Consider a solution tree T and the node u0 of T \u02da S that is the first node reached by the walk from the root that always goes to the child tree with the higher number of leaves, stopping when the subtree of T \u02daS rooted at u0 contains fewer than 2n{3 leaves. Let A \u201c V pu0q, B \u201c V zV pu0q. Note that the number of edges in GrAs is at most a \u201c ` |A| 2 \u02d8 , the number of edges in GrBs is at\nmost b \u201c ` |B| 2 \u02d8 , whereas the number of edges in the cut pA,Bq is |A| \u00a8 |B|. Recall that n{3 \u010f |A|, |B| \u010f 2n{3 and so a, b \u201c \u0398p|A| \u00a8 |B|q. Finally observe that for each edge pu, vq P GrAs, we have wpu, vq \u010f wpu, xq ` wpx, vq for any x P B. Thus, since a ` b \u201c \u0398p|A| \u00a8 |B|q, by a simple counting argument, we deduce valpT q \u201c \u2126pn \u0159 ewpeqq and by Fact 6.1, \u2126pOPTq."}, {"heading": "Acknowledgments", "text": "The authors are grateful to Sanjoy Dasgupta for sharing thoughtful comments at various stages of this project."}], "references": [{"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings on 33rd Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Arora and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2001}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani"], "venue": "J. ACM,", "citeRegEx": "Arora et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2009}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "Awasthi and Sheffet.,? \\Q2012\\E", "shortCiteRegEx": "Awasthi and Sheffet.", "year": 2012}, {"title": "Stability yields a PTAS for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Awasthi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "Awasthi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2012}, {"title": "Clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Yingyu Liang"], "venue": "SIAM J. Comput.,", "citeRegEx": "Balcan and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Balcan and Liang.", "year": 2016}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Agnostic clustering", "author": ["Maria-Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng"], "venue": "In Algorithmic Learning Theory, 20th International Conference,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "J. ACM,", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability & Computing,", "citeRegEx": "Bilu and Linial.,? \\Q2012\\E", "shortCiteRegEx": "Bilu and Linial.", "year": 2012}, {"title": "On the practically interesting instances of MAXCUT", "author": ["Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael E. Saks"], "venue": "In 30th International Symposium on Theoretical Aspects of Computer Science, STACS 2013,", "citeRegEx": "Bilu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bilu et al\\.", "year": 2013}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Characterization, stability and convergence of hierarchical clustering methods", "author": ["Gunnar Carlsson", "Facundo M\u00e9moli"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Carlsson and M\u00e9moli.,? \\Q2010\\E", "shortCiteRegEx": "Carlsson and M\u00e9moli.", "year": 2010}, {"title": "Likelihood based hierarchical clustering", "author": ["Rui M Castro", "Mark J Coates", "Robert D Nowak"], "venue": "IEEE Transactions on signal processing,", "citeRegEx": "Castro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2004}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "A cost function for similarity-based hierarchical clustering", "author": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Dasgupta.,? \\Q2016\\E", "shortCiteRegEx": "Dasgupta.", "year": 2016}, {"title": "Performance guarantees for hierarchical clustering", "author": ["Sanjoy Dasgupta", "Philip M Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Dasgupta and Long.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta and Long.", "year": 2005}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Graphons, mergeons, and so on", "author": ["Justin Eldridge", "Mikhail Belkin", "Yusu Wang"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Eldridge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eldridge et al\\.", "year": 2016}, {"title": "Inferring phylogenies, volume 2", "author": ["Joseph Felsenstein", "Joseph Felenstein"], "venue": "Sinauer Associates Sunderland,", "citeRegEx": "Felsenstein and Felenstein.,? \\Q2004\\E", "shortCiteRegEx": "Felsenstein and Felenstein.", "year": 2004}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Mathematical Taxonomy. Wiley series in probability and mathematical statistiscs", "author": ["N. Jardine", "R. Sibson"], "venue": null, "citeRegEx": "Jardine and Sibson.,? \\Q1972\\E", "shortCiteRegEx": "Jardine and Sibson.", "year": 1972}, {"title": "An impossibility theorem for clustering", "author": ["Jon Kleinberg"], "venue": "In NIPS,", "citeRegEx": "Kleinberg.,? \\Q2002\\E", "shortCiteRegEx": "Kleinberg.", "year": 2002}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Kumar and Kannan.,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Kannan.", "year": 2010}, {"title": "A general approach for incremental approximation and hierarchical clustering", "author": ["Guolong Lin", "Chandrashekhar Nagarajan", "Rajmohan Rajaraman", "David P Williamson"], "venue": "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "Lin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Community detection and classification in hierarchical stochastic blockmodels", "author": ["Vince Lyzinski", "Minh Tang", "Avanti Athreya", "Youngser Park", "Carey E Priebe"], "venue": "IEEE Transactions on Network Science and Engineering,", "citeRegEx": "Lyzinski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lyzinski et al\\.", "year": 2017}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2012}, {"title": "Approximation algorithms for hierarchical location problems", "author": ["C Greg Plaxton"], "venue": "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Plaxton.,? \\Q2003\\E", "shortCiteRegEx": "Plaxton.", "year": 2003}, {"title": "Hierarchical clustering via spreading metrics", "author": ["Aurko Roy", "Sebastian Pokutta"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Roy and Pokutta.,? \\Q2016\\E", "shortCiteRegEx": "Roy and Pokutta.", "year": 2016}, {"title": "A comparison of document clustering techniques", "author": ["Michael Steinbach", "George Karypis", "Vipin Kumar"], "venue": "KDD Workshop on Text Mining,", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Nonparametric graphon estimation", "author": ["Patrick J Wolfe", "Sofia C Olhede"], "venue": "arXiv preprint arXiv:1309.5936,", "citeRegEx": "Wolfe and Olhede.,? \\Q2013\\E", "shortCiteRegEx": "Wolfe and Olhede.", "year": 2013}, {"title": "A uniqueness theorem for clustering", "author": ["Reza Bosagh Zadeh", "Shai Ben-David"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "Zadeh and Ben.David.,? \\Q2009\\E", "shortCiteRegEx": "Zadeh and Ben.David.", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a \u2018good\u2019 hierarchical clustering is one that minimizes some cost function.", "startOffset": 144, "endOffset": 160}, {"referenceID": 15, "context": "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a \u2018good\u2019 hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in \u2018structureless\u2019 graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining \u2018good\u2019 objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a \u2018natural\u2019 ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs.", "startOffset": 144, "endOffset": 1203}, {"referenceID": 15, "context": "Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a \u2018good\u2019 hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in \u2018structureless\u2019 graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining \u2018good\u2019 objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a \u2018natural\u2019 ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation.", "startOffset": 144, "endOffset": 1513}, {"referenceID": 15, "context": "Dasgupta (2016) identified the lack of a well-defined objective function as one of the reasons why the theoretical study of hierarchical clustering has lagged behind that of partition-based clustering.", "startOffset": 0, "endOffset": 16}, {"referenceID": 15, "context": "Dasgupta (2016) frames hierarchical clustering as a combinatorial optimization problem, where a good output tree is a tree that minimizes some cost function; but which function should that be? Each (binary) tree node is naturally associated to a cut that splits the cluster of its descendant leaves into the cluster of its left subtree on one side and the cluster of its right subtree on the other, and Dasgupta defines the objective to be the sum, over all tree nodes, of the total weight of edges crossing the cut multiplied by the cardinality of the node\u2019s cluster.", "startOffset": 0, "endOffset": 16}, {"referenceID": 15, "context": "We consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)).", "startOffset": 294, "endOffset": 310}, {"referenceID": 15, "context": "We consider, as potential objective functions, the class of all functions that sum, over all the nodes of the tree, the total weight of edges crossing the associated cut times some function of the cardinalities of the left and right clusters (this includes the class of functions considered by Dasgupta (2016)). In Section 3 we characterize the \u2018good\u2019 objective functions in this class and call them admissible objective functions. We prove that for any objective function, for any ground-truth input, the ground-truth tree has optimal cost (w.r.t to the objective function) if and only if the objective function (1) is symmetric (independent of the left-right order of children), (2) is increasing in the cardinalities of the child clusters, and (3) for (unit-weight) cliques, has the same cost for all binary trees (Theorem 3.4). Dasgupta\u2019s objective function is admissible in terms of the criteria described above. In Section 5, we consider random graphs that induce a natural clustering. This model can be seen as a noisy version of our notion of ground-truth inputs and a hierarchical stochastic block model. We show that the ground-truth tree has optimal expected cost for any admissible objective function. Furthermore, we show that the ground-truth tree has cost at most p1 ` op1qqOPT with high probability for the objective function introduced by Dasgupta (2016).", "startOffset": 294, "endOffset": 1372}, {"referenceID": 14, "context": "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive \u03c6-approximate sparsest cut algorithm, that recursively splits the input graph using a \u03c6-approximation to the sparsest cut problem, outputs a tree whose cost is at most Op\u03c6 log n  \u0308 OPTq.", "startOffset": 34, "endOffset": 50}, {"referenceID": 14, "context": "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive \u03c6-approximate sparsest cut algorithm, that recursively splits the input graph using a \u03c6-approximation to the sparsest cut problem, outputs a tree whose cost is at most Op\u03c6 log n  \u0308 OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique.", "startOffset": 34, "endOffset": 285}, {"referenceID": 14, "context": "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive \u03c6-approximate sparsest cut algorithm, that recursively splits the input graph using a \u03c6-approximation to the sparsest cut problem, outputs a tree whose cost is at most Op\u03c6 log n  \u0308 OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive \u03c6-sparsest cut algorithm of Dasgupta gives an Op\u03c6q-approximation.", "startOffset": 34, "endOffset": 463}, {"referenceID": 14, "context": "Algorithms for Similarity Graphs: Dasgupta (2016) shows that the recursive \u03c6-approximate sparsest cut algorithm, that recursively splits the input graph using a \u03c6-approximation to the sparsest cut problem, outputs a tree whose cost is at most Op\u03c6 log n  \u0308 OPTq. Roy and Pokutta (2016) recently gave an Oplog nq-approximation by providing a linear programming relaxation for the problem and providing a clever rounding technique. Charikar and Chatziafratis (2017) showed that the recursive \u03c6-sparsest cut algorithm of Dasgupta gives an Op\u03c6q-approximation. In Section 4, we obtain an independent proof showing that the \u03c6-approximate sparsest cut algorithm is an Op\u03c6q-approximation (Theorem 4.1). Our proof is quite different from the proof of Charikar and Chatziafratis (2017) and relies on a charging argument.", "startOffset": 34, "endOffset": 775}, {"referenceID": 1, "context": "Combined with the celebrated result of Arora et al. (2009), this yields an Op ? log nq-approximation.", "startOffset": 39, "endOffset": 59}, {"referenceID": 15, "context": "For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard.", "startOffset": 49, "endOffset": 65}, {"referenceID": 15, "context": "For the objective function proposed in his work, Dasgupta (2016) shows that finding a cluster tree that minimizes the cost function is NP-hard. This directly applies to the admissible objective functions for the dissimilarity setting as well. Thus, the focus turns to developing approximation algorithms. Our analysis shows that the algorithm achieves a 6.75\u03c6-approximation and the analysis of Charikar and Chatziafratis (2017) yields a 8\u03c6-approximation guarantee.", "startOffset": 49, "endOffset": 428}, {"referenceID": 31, "context": "Structured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.", "startOffset": 69, "endOffset": 92}, {"referenceID": 31, "context": "Structured Inputs and Beyond-Worst-Case Analysis: The recent work of Roy and Pokutta (2016) and Charikar and Chatziafratis (2017) have shown that obtaining constant approximation guarantees for worst-case inputs is beyond current techniques (see Section 1.", "startOffset": 69, "endOffset": 130}, {"referenceID": 6, "context": "This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012).", "startOffset": 73, "endOffset": 94}, {"referenceID": 6, "context": "This approach bears similarity to the stability-based conditions used by Balcan et al. (2008) and Bilu and Linial (2012). We provide an algorithm that achieves a \u03b4-approximation in both the similarity and dissimilarity settings, independent of the objective function used as long as it is admissible according to the criteria used in Section 3.", "startOffset": 73, "endOffset": 121}, {"referenceID": 15, "context": "The recent paper of Dasgupta (2016) served as the starting point of this work.", "startOffset": 20, "endOffset": 36}, {"referenceID": 15, "context": "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem.", "startOffset": 20, "endOffset": 95}, {"referenceID": 15, "context": "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive \u03c6-sparsest-cut algorithm achieves an Op\u03c6 log nqapproximation. Dasgupta\u2019s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta\u2019s cost function.", "startOffset": 20, "endOffset": 489}, {"referenceID": 15, "context": "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive \u03c6-sparsest-cut algorithm achieves an Op\u03c6 log nqapproximation. Dasgupta\u2019s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta\u2019s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation. Charikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis.", "startOffset": 20, "endOffset": 1057}, {"referenceID": 15, "context": "The recent paper of Dasgupta (2016) served as the starting point of this work. Dasgupta (2016) defined an objective function for hierarchical clustering and thus formulated the question of constructing a cluster tree as a combinatorial optimization problem. Dasgupta also showed that the resulting problem is NP-hard and that the recursive \u03c6-sparsest-cut algorithm achieves an Op\u03c6 log nqapproximation. Dasgupta\u2019s results have been improved in two subsequent papers. Roy and Pokutta (2016) wrote an integer program for the hierarchical clustering problem using a combinatorial characterization of the ultrametrics induced by Dasgupta\u2019s cost function. They also provide a spreading metric LP and a rounding algorithm based on sphere/region-growing that yields an Oplog nq-approximation. Finally, they show that no polynomial size SDP can achieve a constant factor approximation for the problem and that under the Small Set Expansion (SSE) hypothesis, no polynomial-time algorithm can achieve a constant factor approximation. Charikar and Chatziafratis (2017) also gave a proof that the problem is hard to approximate within any constant factor under the Small Set Expansion hypothesis. They also proved that the recursive \u03c6-sparsest cut algorithm produces a hierarchical clustering with cost at most Op\u03c6OPTq; their techniques appear to be significantly different from ours. Additionally, Charikar and Chatziafratis (2017) introduce a spreading metric SDP relaxation for the hierarchical clustering problem introduced by Dasgupta that has integrality gap Op ? log nq and a spreading metric LP relaxation that yields an Oplog nq-approximation to the problem.", "startOffset": 20, "endOffset": 1420}, {"referenceID": 23, "context": ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).", "startOffset": 2, "endOffset": 107}, {"referenceID": 20, "context": ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).", "startOffset": 2, "endOffset": 107}, {"referenceID": 14, "context": ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004).", "startOffset": 2, "endOffset": 107}, {"referenceID": 21, "context": ", (Friedman et al., 2001) and for divisive algorithms see e.", "startOffset": 2, "endOffset": 25}, {"referenceID": 14, "context": ", (Jardine and Sibson, 1972; Sneath and Sokal, 1962; Felsenstein and Felenstein, 2004; Castro et al., 2004). Algorithms for hierarchical clustering have received a lot of attention from a practical perspective. For a definition and overview of agglomerative algorithms (such as average-linkage, complete-linkage, and single-linkage) see e.g., (Friedman et al., 2001) and for divisive algorithms see e.g., Steinbach et al. (2000). Most previous theoretical work on hierarchical clustering aimed at evaluating the cluster tree output by the linkage algorithms using the traditional objective functions for partition-based clustering, e.", "startOffset": 87, "endOffset": 429}, {"referenceID": 30, "context": ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).", "startOffset": 2, "endOffset": 60}, {"referenceID": 17, "context": ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).", "startOffset": 2, "endOffset": 60}, {"referenceID": 26, "context": ", (Plaxton, 2003; Dasgupta and Long, 2005; Lin et al., 2006)).", "startOffset": 2, "endOffset": 60}, {"referenceID": 6, "context": "Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)).", "startOffset": 168, "endOffset": 213}, {"referenceID": 5, "context": "Previous work also proved that average-linkage can be useful to recover an underlying partition-based clustering when it exists under certain stability conditions (see (Balcan et al., 2008; Balcan and Liang, 2016)).", "startOffset": 168, "endOffset": 213}, {"referenceID": 4, "context": ", (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 29, "context": ", (Awasthi et al., 2012; Balcan et al., 2013, 2008; Ostrovsky et al., 2012)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 10, "context": ", (Bilu and Linial, 2012; Balcan et al., 2009a, 2013) for deterministic conditions and e.", "startOffset": 2, "endOffset": 53}, {"referenceID": 3, "context": ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 25, "context": ", (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)).", "startOffset": 2, "endOffset": 75}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously.", "startOffset": 8, "endOffset": 460}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously.", "startOffset": 8, "endOffset": 497}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and M\u00e9moli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg.", "startOffset": 8, "endOffset": 674}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and M\u00e9moli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their \u03b4-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al.", "startOffset": 8, "endOffset": 1314}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and M\u00e9moli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their \u03b4-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal.", "startOffset": 8, "endOffset": 1336}, {"referenceID": 1, "context": ", 2008; Balcan and Liang, 2016)). The approach of this paper is different: we aim at associating a cost or a value to each hierarchical clustering and finding the best hierarchical clustering with respect to these objective functions. In Section 3, we take an axiomatic approach toward objective functions. Axiomatic approach toward a qualitative analysis of algorithms for clustering where taken before. For example, the celebrated result of Kleinberg (2002) (see also Zadeh and Ben-David (2009)) showed that there is no algorithm satisfying three natural axioms simultaneously. This approach was applied to hierarchical clustering algorithms by Carlsson and M\u00e9moli (2010) who showed that in the case of hierarchical clustering one gets a positive result, unlike the impossibility result of Kleinberg. Their focus was on finding an ultrametric (on the datapoints) that is the closest to the metric (in which the data lies) in terms of the Gromov-Hausdorf distance. Our approach is completely different as we focus on defining objective functions and use these for quantitative analyses of algorithms. Our condition for inputs to have a ground-truth cluster tree, and especially their \u03b4-adversarially perturbed versions, can be to be in the same spirit as that of the stability condition of Bilu and Linial (2012) or Bilu et al. (2013): the input induces a natural clustering to be recovered whose cost is optimal. It bears some similarities with the \u201cstrict separation\u201d condition of Balcan et al. (2008), while we do not require the separation to be strict, we do require some additional hierarchical constraints.", "startOffset": 8, "endOffset": 1505}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.", "startOffset": 2, "endOffset": 55}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.", "startOffset": 2, "endOffset": 85}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al.", "startOffset": 2, "endOffset": 102}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions).", "startOffset": 2, "endOffset": 125}, {"referenceID": 0, "context": ", Arora and Kannan (2001); Brubaker and Vempala (2008); Dasgupta and Schulman (2007); Dasgupta (1999); Balcan et al. (2009b) for probabilistic conditions). Imposing other conditions allows one to bypass hardness-of-approximation results for classical clustering objectives (such as k-means), and design efficient approximation algorithms (see, e.g., (Awasthi et al., 2010; Awasthi and Sheffet, 2012; Kumar and Kannan, 2010)). Eldridge et al. (2016) also investigate the question of understanding hierarchical cluster trees for random graphs generated from graphons.", "startOffset": 2, "endOffset": 449}, {"referenceID": 13, "context": ", (Carlsson and M\u00e9moli, 2010)).", "startOffset": 2, "endOffset": 29}, {"referenceID": 13, "context": ", (Carlsson and M\u00e9moli, 2010)).", "startOffset": 2, "endOffset": 29}, {"referenceID": 13, "context": "Furthermore, as observed by Carlsson and M\u00e9moli (2010), many practical hierarchical clustering algorithms such as the linkage based algorithms, actually output a dendogram equipped with a height function, that corresponds to an ultrametric embedding of the data.", "startOffset": 28, "endOffset": 55}, {"referenceID": 15, "context": "Following the recent work of Dasgupta (2016), we adopt an approach in which a cost is assigned to each internal node of the tree T that corresponds to the quality of the split at that node.", "startOffset": 29, "endOffset": 45}, {"referenceID": 15, "context": "Following Dasgupta (2016), we restrict the search space for such cost functions.", "startOffset": 10, "endOffset": 26}, {"referenceID": 15, "context": "We remark that Dasgupta (2016) defined gpa, bq \u201c a` b.", "startOffset": 15, "endOffset": 31}, {"referenceID": 15, "context": "For the purpose of hierarchical clustering this form is fairly natural and indeed includes the specific cost function introduced by Dasgupta (2016). We could define the notion of admissibility for other forms of the cost function similarly and it would be of interest to understand whether they have properties that are desirable from the point of view of hierarchical clustering.", "startOffset": 132, "endOffset": 148}, {"referenceID": 15, "context": "The fact that such functions exist already follows from the work of Dasgupta (2016), who showed that if gpn1, n2q \u201c n1 ` n2, then all cliques have the same cost.", "startOffset": 68, "endOffset": 84}, {"referenceID": 15, "context": "The function proposed by Dasgupta (2016) is gpn, 1q \u201c n` 1, so this ratio is always 1.", "startOffset": 25, "endOffset": 41}, {"referenceID": 15, "context": "This shows that the objective function proposed by Dasgupta (2016) is by no means unique.", "startOffset": 51, "endOffset": 67}, {"referenceID": 15, "context": "In this section, we analyze the recursive \u03c6-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q \u201c NPT costpNq where for each node N of T with children N1, N2, costpNq \u201c wpV pN1q, V pN2qq  \u0308 V pNq.", "startOffset": 118, "endOffset": 134}, {"referenceID": 15, "context": "In this section, we analyze the recursive \u03c6-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q \u201c NPT costpNq where for each node N of T with children N1, N2, costpNq \u201c wpV pN1q, V pN2qq  \u0308 V pNq.", "startOffset": 118, "endOffset": 209}, {"referenceID": 15, "context": "In this section, we analyze the recursive \u03c6-sparsest-cut algorithm (see Algorithm 1) that was described previously in Dasgupta (2016). For clarity, we work with the cost function introduced by Dasgupta (2016): The goal is to find a tree T minimizing costpT q \u201c NPT costpNq where for each node N of T with children N1, N2, costpNq \u201c wpV pN1q, V pN2qq  \u0308 V pNq. We show that the \u03c6-sparsest-cut algorithm achieves a 6.75\u03c6-approximation. (Charikar and Chatziafratis (2017) also proved an Op\u03c6q approximation for Dasgupta\u2019s function.", "startOffset": 118, "endOffset": 469}, {"referenceID": 15, "context": "For Dasgupta\u2019s function, this was already proved in Charikar and Chatziafratis (2017) with a different constant.", "startOffset": 4, "endOffset": 86}, {"referenceID": 15, "context": "Observe (as in Dasgupta (2016)) that costpT  \u030aq \u201c tu,vuPE |V pLCAT \u030apu, vqq|wpu, vq.", "startOffset": 15, "endOffset": 31}, {"referenceID": 15, "context": "Indeed, adapting the definition of the balanced cut as in Dasgupta (2016) and rescaling the charge by a factor of fn imply the result.", "startOffset": 58, "endOffset": 74}, {"referenceID": 15, "context": "4) which includes the cost function introduced by Dasgupta, we show the following: The cost of the ground-truth cluster tree is with high probability sharply concentrated (up to a factor of p1`op1qq around its expectation), and so of cost at most p1`op1qqOPT. This is further evidence that optimising admissible cost functions is an appropriate strategy for hierarchical clustering. We also provide a simple algorithm based on the SVD based approach of McSherry (2001) followed by a standard agglomerative heuristic yields a hierarchical clustering which is, up to a factor p1` op1qq, optimal with respect to smooth admissible cost functions.", "startOffset": 50, "endOffset": 469}, {"referenceID": 27, "context": ", Lyzinski et al. (2017). However, prior work has mostly focused on statistical hypothesis testing and exact recovery in some regimes.", "startOffset": 2, "endOffset": 25}, {"referenceID": 33, "context": "This is similar to the approach taken by Wolfe and Olhede (2013) when considering random graph models generated according to graphons.", "startOffset": 41, "endOffset": 65}, {"referenceID": 15, "context": "The cost function introduced by Dasgupta (2016) satisfies the smoothness property.", "startOffset": 32, "endOffset": 48}, {"referenceID": 22, "context": "We will make use of the slightly generalized version of Hoeffding bounds (see Hoeffding (1963)).", "startOffset": 56, "endOffset": 95}, {"referenceID": 28, "context": "We use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here.", "startOffset": 19, "endOffset": 35}, {"referenceID": 28, "context": "We use a result of McSherry (2001) who considers a random graph model with k clusters that is (slightly) more general than the HSBM considered here. The difference is that there is no hierarchical structure on top of the k clusters in his setting; on the other hand, his goal is also simply to identify the k clusters and not any hierarchy upon them. The following theorem is derived from McSherry (2001) (Observation 11 and a simplification of Theorem 12).", "startOffset": 19, "endOffset": 405}, {"referenceID": 28, "context": "10 (McSherry (2001)).", "startOffset": 4, "endOffset": 20}, {"referenceID": 15, "context": "Find T maximizing the value function corresponding to Dasgupta\u2019s cost function of Section 4: valpT q \u201c NPT valpNq where for each node N of T with children N1, N2, valpNq \u201c wpV pN1q, V pN2qq  \u0308V pNq. This optimization problem is NP-Hard Dasgupta (2016), hence we focus on approximation algorithms.", "startOffset": 54, "endOffset": 252}], "year": 2017, "abstractText": "Hierarchical clustering is a recursive partitioning of a dataset into clusters at an increasingly finer granularity. Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, Dasgupta (2016) framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a \u2018good\u2019 hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost disconnected components must be separated first and that in \u2018structureless\u2019 graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining \u2018good\u2019 objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a \u2018natural\u2019 ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similaritybased hierarchical clustering, Dasgupta (2016) showed that a simple recursive sparsest-cut based approach achieves an Oplog nq-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an Op ? lognq-approximation1. This improves upon the LP-based Oplog nq-approximation of Roy and Pokutta (2016). For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3{2 approximation. This aims at explaining the success of this heuristics in practice. Finally, we consider \u2018beyond-worst-case\u2019 scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta\u2019s cost function also has desirable properties for these inputs and we provide a simple algorithm that for graphs generated according to this model yields a 1 + o(1) factor approximation. Charikar and Chatziafratis (2017) independently proved that the sparsest-cut based approach achieves a Op ? log nq approximation.", "creator": "LaTeX with hyperref package"}}}