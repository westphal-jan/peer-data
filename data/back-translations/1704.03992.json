{"id": "1704.03992", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Fully Distributed and Asynchronized Stochastic Gradient Descent for Networked Systems", "abstract": "This paper looks at a general problem of data matching over a networked system where many computing nodes are connected by an undirected graph. This type of problem can find many applications in the real world and has been extensively studied in the literature. However, existing solutions either require a central controller for information exchange or require the synchronization of slots between different nodes, which increases the difficulty of practical implementations, especially for a very large and heterogeneous system.", "histories": [["v1", "Thu, 13 Apr 2017 04:58:54 GMT  (265kb,D)", "http://arxiv.org/abs/1704.03992v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.PF", "authors": ["ying zhang"], "accepted": false, "id": "1704.03992"}, "pdf": {"name": "1704.03992.pdf", "metadata": {"source": "CRF", "title": "Fully Distributed and Asynchronized Stochastic Gradient Descent for Networked Systems", "authors": ["Ying Zhang"], "emails": [], "sections": [{"heading": null, "text": "As a contrast, in this paper, we treat the data-fitting problem over the network as a stochastic programming problem with many constraints. By adapting the results in a recent paper [18], we design a fully distributed and asynchronized stochastic gradient descent (SGD) algorithm. We show that our algorithm can achieve global optimality and consensus asymptotically by only local computations and communications. Additionally, we provide a sharp lower bound for the convergence speed in the regular graph case. This result fits the intuition and provides guidance to design a \u2018good\u2019 network topology to speed up the convergence. Also, the merit of our design is validated by experiments on both synthetic and real-world datasets.\nI. INTRODUCTION As one of the most important optimization algorithms, stochastic gradient descent (SGD) and many of its variants have been proposed to solve different optimization problems, and are gaining their popularity in this \u2018big-data\u2019 era [4]. Popular examples include SVM, Logistic Regression for the convex cases [5], and deep neural networks for the non-convex cases [8]. It can be shown that, for the basic version of SGD, the asymptotic convergence rate is O(1/ \u221a T ) for the general convex problems and O(1/T ) for the strict convex problems, where T is the number of iterations.\nNowadays, data is generated and collected at a higher speed due to the wide use of Internet, mobile devices, sensor networks, etc. The basic version\nThe author is with Department of Information Engineering, the Chinese University of Hong Kong.\nof SGD suffers from its nature of sequentially processing the data and is still too slow. To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].\nIn most of the current literature, the \u2018distributed\u2019 SGD is usually performed by a \u2019one-Server multiple-Worker\u2019 mode, as shown in Fig. 1(a), where the workers access the data distributedly and the server acts as a coordinator among all workers. In practice, the \u2018server\u2019 and the workers can be many computers, connected by a computer network, like the design of the \u2018parameter-server\u2019 [12]; also, the \u2018server\u2019 can be the shared memory of one computer and the workers can be different threads, like the design of \u2019Hogwild!\u2019 [17]. To optimized the variable (training process), in each iteration, each worker will take some data samples to update their local variable and upload the information to the server; after receiving the new local variables from all workers, the server will perform a update, according to some requirements like budget limitation, and then broadcast the new variable to all workers. Intuitively speaking, the server exists so that all workers will be aware of the global information and the local variables of different workers will not deviate from each other too much.\nHowever, the performance of the \u2018server-worker\u2019 structure is inevitably limited by at least two factors. Firstly, due to the existence of \u2018central controller\u2019, the system is actually semi-distributed. It is uneasy to scale up to too many workers. For example, it may need complicated design and advanced systems to handle tens of thousands of workers. Also, this kind of system is not so robust. For example, if the server breaks down, the entire system also fails to work. Secondly, synchronization among all workers is needed in the process. However, synchronization is difficult to achieve considering the heterogeneous and time-varying performances of different workers, especially when the number of workers is large. In\nar X\niv :1\n70 4.\n03 99\n2v 1\n[ cs\n.L G\n] 1\n3 A\npr 2\n01 7\n2 practice, the late workers are simply ignored, which is equivalent to introducing noise and performance is thus degraded. In a recent work [13], the authors show that, for the \u2018server-worker\u2019 structure, an asynchronized dual-based algorithm can achieve O(1/ \u221a T ) convergence rate.\nIn this paper, we propose a SGD scheme for a general networked system, in which many nodes are connected by a general undirected graph, as shown in Fig. 1(b). In this system, each node can represent a computing unit, like one computer of a cluster, one user of online social network, or one sensor of wireless sensor network. We assume that each node can access the data and do some local computation, but communications can only happens between directly connected nodes (the node and its neighbors). We design an update method without relying on any controller to achieve global optimality and consensus. More specifically, the local variables stored by each node will asymptotically converge to the same value, which is the optimal value to minimize the total expected loss of all nodes.\nIn particular, we make the following contributions. \u2022 We reformulate the single-variable data-fitting\nproblem of a networked system as a multiplevariable problem with multiple constraints, which ensure global consensus. \u2022 By apply the result of the recent paper on stochastic optimization problem with many constraints [18], we design an algorithm for the reformulated optimization problem. The algorithm is essentially distributed and asynchronized. Both the global consensus and optimality can be asymptotically achieved. \u2022 We analyze the convergence speed of algorithm for a special structure: the regular graph. Our analysis indicates a not so surprising result: the algorithm will converge faster for a graph with better connectivity. \u2022 By simulations on synthetic and real-world datasets, we validate the merit of our design and test the impact of different system settings."}, {"heading": "II. PROBLEM DESCRIPTION", "text": "We consider a scenario in which N computing nodes are connected as a graph. We have one optimization variable \u03b2 as a vector, and the loss of Node i by \u03b2 is denoted as f\u0304i(\u03b2). One example is\nthat Node i has many training data samples, and f\u0304i(\u03b2) =\n1 Ki \u2211 k li(\u03b2, vk) is the averaged loss of\neach sample vk. We assume that f\u0304i(\u00b7) is a convex function. With one data sample being vk = (xk, yk), such a model can capture many popular examples as special cases, like \u2022 Logistic Regression:\nf\u0304i(\u03b2) = 1\nKi Ki\u2211 k=1\n[ yk\n1\n1 + e\u2212\u03b2T xk + (1\u2212 yk)\ne\u2212\u03b2 T xk\n1 + e\u2212\u03b2T xk\n] .\n\u2022 Support Vector Machine:\nf\u0304i(\u03b2) = 1\nKi Ki\u2211 k=1 max{0, 1\u2212 yk\u03b2Txk}+ \u03bb\u03b22.\n\u2022 Lasso:\nf\u0304i(\u03b2) = 1\n2Ki Ki\u2211 k=1 (yk \u2212 \u03b2Txk)2 + \u03bb\u2016\u03b2\u2016.\nOur interest is to find one optimal solution to minimize the total loss of all nodes, i.e., solving\nmin 1\nN N\u2211 i=1 f\u0304i(\u03b2). (1)\nIn this paper, instead of using the empirical loss, we consider minimizing a expected loss function fi(\u03b2) = E [li(\u03b2, Vi)], where the expectation is taken according to the distribution of Vi 1. Vi is the random variable to generate a data sample in Node i. So our optimization problem is further casted as as a stochastic programming in the following form,\nmin F (\u03b2) = 1\nN \u2211 i fi(\u03b2). (2)\nRemarks: (1) f\u0304i(\u03b2) can be viewed as an empirical replacement of fi(\u03b2). By law of large numbers, the difference between two functions will converge to 0 if the number of data samples in Node i goes to infinity. (2) Suppose we have one random variable V to generate all data samples. We can have another perspective to interpret the objective function F (\u03b2) of Problem. (2): V has probability 1\nN to take the\ndistribution of that of Vi, and at the same time, the loss function will take the form of li(\u00b7, \u00b7).\nTo summarize, we have an optimization problem with a single variable \u03b2 to minimize the expected\n1Note that we can assume that Vi and Vj can follow different distributions if i 6= j.\n3 (a) Server-Worker Structure (b) Networked Structure (discussed in this paper)\nFig. 1. Two Computing Structures to Perform SGD\nloss by data generated or stored in different nodes. The stochastic programming problem with convex objective functions has been extensively studied in the literature, and many off-the-shelf algorithm can be used, see [18] and the reference therein. However, most of works consider centralized design and are not applicable to the networked system. The results in [3], [14], [15] also consider to minimize an objective function over a networked system, but the algorithms require all nodes to perform update in each slot and synchronization is needed for realworld implementation, which, however, is difficult to achieve. In the next section, we will present our design to tackle this issue."}, {"heading": "III. ALGORITHM DESIGN", "text": ""}, {"heading": "A. Problem Reformulation", "text": "Before presenting our algorithm, we denote Ni as the set of the neighbors of Node i and reformulate original problem. (2) as follows,\nmin \u03b21,\u03b22,\u00b7\u00b7\u00b7 ,\u03b2N\n1\nN N\u2211 i=1 fi(\u03b2i) (3)\ns.t. \u03b2i = \u03b2k, \u2200k \u2208 Ni,\u2200i. (4)\nIn the new formulation, we increase the number of variables by assigning one local optimization variable to each node. We assume that this networked system is connected, so for any feasible solution, global consensus is guaranteed by Eq. (4). This reformulation is trivial at the first glance, but it is really helpful for the algorithm design and analysis, as we will show immediately."}, {"heading": "B. A Distributed Algorithm", "text": "In [18], the authors investigated a general stochastic programming problem in the following form,\nGenPro min X\nE[F (X)]\ns.t. X \u2208 X1 \u2229 X2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 XM ,\nand proposed several algorithms based on randomly sampling and projection. We recall one of them below,\nAlgorithm 1 An algorithm to solve GenPro Require: An oracle to generate data sample, k = 1, X0, stepsize \u03b1t, t = 1, 2, \u00b7 \u00b7 \u00b7 ,\u221e Ensure: An optimal solution X\u2217 to GenPro\n1: while not converge do 2: Generate one data sample vk and compute the\nsubgradient g(Xk, vk) with current variable Xk and data sample vk.\n3: Update current variable Xk+1 = Xk \u2212 \u03b1kg(Xk, vk). 4: Randomly pick one constraint Xm, and make the projection Xk+1 = \u03a0Xm(Xk+1). 5: k = k + 1 6: end while 7: X\u2217 = Xk 8: return X\u2217.\nIn Alg. 1, \u03a0Xm(\u00b7) is a projection operator and \u03a0Xm(x) = arg miny\u2208Xm \u2016y \u2212 x\u2016F . Even though only simple gradient descent and projection are performed in each iteration, it can be proved that the algorithm will converge to a optimal solution, i.e., the objective value will converge to the optimal\n4 value and the distance from the variable to the feasible region will converge to 0.\nWe denote the optimization variable \u03b2 as a concentration of original local variables, i.e.,\n\u03b2 = [\u03b21, \u03b22, \u00b7 \u00b7 \u00b7 , \u03b2N ]\nand Bn = {\u03b2|\u03b2n = \u03b2k,\u2200k \u2208 Nn} as one convex set. The optimization problem we are interested in can be written in a more compact fact as\nOurPro min \u03b2\n1\nN N\u2211 i=1 fi(\u03b2i)\ns.t. \u03b2 \u2208 B1 \u2229 B2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 BN .(5)\nNow our problem are put into a form that is suitable for Alg. 1, so we can directly apply the algorithm. Now we will examine the gradient descent and projection steps in details. a) Gradient Descent: To randomly generate a data sample, we can firstly randomly select a node i and generate a sample vki according to the distribution of Vi. Note that under the condition node i is selected, the loss of other nodes with vki is zero. So the sampled subgradient of the objective function is nonzero only for \u03b2i and can be written as 1\nN \u2202fi(\u03b2i)/\u2202\u03b2i. Thus the gradient descent can be\nperformed by{ \u03b2k+1i = \u03b2 k i \u2212 \u03b1k 1N \u2202li(\u03b2 k i , v k i )/\u2202\u03b2 k i ,\n\u03b2k+1n = \u03b2 k n, for n 6= i.\n(6)\nWe can see that if a data sample is generated locally in Node i, only local update is needed to perform gradient descent. b) Projection: If one convex set, say Bm, is randomly selected, we would need to project the current variable [ \u03b2k+11 , \u03b2 k+1 2 , \u00b7 \u00b7 \u00b7 , \u03b2k+1N ] onto it. Bm requires that the local variables stored in node m and its neighbors are equal. To find a new point satisfy this condition and minimize the distance between the new point and original point, the values of the variables stored in node m and its neighbors should take the averaged value among them and the values of other variables should stay the same. Thus the projection can be performed by{ \u03b2k+1i = 1 1+|Nm| \u2211 l\u2208{m}\u222aNm \u03b2 k l , \u2200i \u2208 {m} \u222a Nm,\n\u03b2k+1j = \u03b2 k j , for other node j. (7) We can also see that the projection process only involves one node and its neighbors. Specifically,\none node collects the local information from its neighbors, calculates the average and then makes a broadcast to its neighbors.\nWe summarize the above procedure to solve OurPro in Alg. 2 to end this part.\nAlgorithm 2 An algorithm to solve OurPro Require: An oracle to generate data sample, k = 1, \u03b20, stepsize \u03b1t, t = 1, 2, \u00b7 \u00b7 \u00b7 ,\u221e Ensure: An optimal solution X\u2217 to OurPro\n1: while not converge do 2: Randomly select a node i, and then generate a random variable r from 0 to 1. 3: if r \u2264 0.5 then 4: Generate one data sample vkm and up-\ndate the local variable by \u03b2k+1m = \u03b2 k m \u2212 \u03b1k 1 N \u2202lm(\u03b2 k m, v k m)/\u2202\u03b2 k m.\n5: else 6: Collect information from its neighbors\nand make a broadcast so that \u03b2k+1i = 1 1+|Nm| \u2211 l\u2208{m}\u222aNm \u03b2 k l , \u2200i \u2208 {m} \u222a Nm\n7: end if 8: k = k + 1 9: end while\n10: \u03b2\u2217 = \u03b2k 11: return \u03b2\u2217."}, {"heading": "C. Theoretical Results", "text": "In this part, we present some theoretical results related to the feasibility and optimality of Alg. 2. Unless otherwise mentioned, we will assume that the objective function satisfies Assumption 1 in [18] and stepsizes are set properly. For a concise presentation, we use DF (\u03b2k) as the distance from the current optimization variable to the feasible region B1 \u2229 B2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 BN 2 and DO(\u03b2k) as the distance from the current optimization variable to the optimal region.\nTheorem 1: (Feasibility and Optimality) [18] Both DF (\u03b2k) and DO(\u03b2k) will converge to zero almost surely.\nWe also provide the asymptotic convergence speed in the following theorem.\n2It should be noted that this region is a polyhedron and can be characterized by linear constraints.\n5 Theorem 2: (Convergence Speed) [18] We can have\nE[DF (\u03b2k+1)] \u2264 (1\u2212 C 4 )DF (\u03b2k) + \u03c3(5 + 4 C )\u03b12k,\n(8)\nand\nE[DO(\u03b2k+1)]\n\u2264DO(\u03b2k) + \u03c3(5 + 2 C )\u03b12k \u2212 2\u03b1k(F (\u03a0(\u03b2k)\u2212 F \u2217), (9)\nwhere \u03c3 is a constant determined by the objective function, \u03b1k is the stepsize, and C = \u03b7N (\u03b7 is the value for the linear regularity condition for the feasible region (5). 3).\nWe omit the proofs of Theorem 1 and 2 since they are established directly from the results in [18] (on Page 9 and Page 10), but we want to provide some results specifically related to our problem. Note from Theorem 2 that a larger value of C will lead to faster convergence. On the one hand, C will be larger if this network contains less nodes (N is smaller), which is intuitive. On the other hand, a larger value \u03b7 will also help the algorithm to converge faster. We provide some results for more insights below.\nFirstly, for a general graph, we define a matrix for local averaging A = [ai,j] where ai,j = 11+|Ni| . The meaning of A can be explained as it computes the new value for one node as the average of the original value of itself and its neighbors.\nLemma 1: For a system connected by a kregular graph, a lower bound for the value of \u03b7 in Theorem. 2 is given by\n(1\u2212 \u03c322) k + 1\nN ,\nwhere \u03c32 is a second largest eigenvalue of its averaging matrix A.\nThe proof is deferred in Appendix. B and we provide some remarks to end this part.\nRemarks: (a) Both a larger value of k and a smaller value of N will increase the value of \u03b7 and thus increase the convergence speed. This phenomenon can be intuitively understood because the algorithm will converge faster for a betterconnected and smaller-scale system. (b) If \u03c32 is\n3The definition of \u2018linear regularity condition\u2019 is provided in the appendix.\nsmaller, the algorithm will also converge faster. Essentially, the second largest singular value of A is another metric to evaluate the \u2018connectivity\u2019 of the system. Remember that the mixing rate of the Markov chain is also affected by the second largest eigenvalue of the transition probability matrix. The readers can refer to [6] for more information."}, {"heading": "IV. DISCUSSIONS OF PRACTICAL IMPLEMENTATIONS", "text": "In this part, we try to discuss some implementation details of Alg. 2."}, {"heading": "A. Node Selection", "text": "In Alg. 2, one node is randomly selected in each iteration, to perform gradient decent or local average. One trivial way to do this is to generate a random integer form 1 to N , and evoke the corresponding node according to this results. However, this method cannot be realized without a central controller. A distributed alternative would be that, we let each node generate a random variable according to a geometric distribution and then count backwards. It will be \u2018selected\u2019 when it counts to 0. We can even carefully design the parameter of geometric distributions for different nodes so that the probability for different nodes to be selected is preferred. The rationale behind this mechanism is similar to that of Monto Carlo Markov Chain Sampling [10], CSMA [7], etc."}, {"heading": "B. Communication Overhead", "text": "For the local average update, communication is needed between the selected node and its neighbors. A common understanding is that communication between nodes are much less efficient than local computation and is unpreferred. To decrease the communication overhead, we can decrease the probability to perform local average when one node is selected. But this mechanism will decrease the convergence speed to global consensus."}, {"heading": "C. Update Conflict", "text": "Since we select the nodes in a distributed manner, it is possible that two nodes are selected in the same time slot. (Imagine that two nodes generate the same value and count down to 0 at the same time.) If the two nodes are far away from each other, i.e., they\n6 share no neighbors, updating simultaneously does no harm and is even preferred since it is equivalent to do the update one by one. However, if two nodes are connected, such updates will introduce some conflicts, say, one node plans to do gradient descent but its neighbor tells him to update according to average. One proper way to handle this issue is that, if one node is \u2018selected\u2019, it sends a message to its neighbors for locking up. Such an mechanism can avoid update conflict but introduce additional communication overhead."}, {"heading": "V. SIMULATIONS", "text": "In this part, we try to evaluate the performance of Alg. 2 and the impact of different parameters. We describe the simulation setting in Sec. V-A. The results from Sec. V-B to Sec. V-D are based on synthetic data while the results in Sec. V-E are based on real-world data."}, {"heading": "A. Simulation Data and Settings", "text": "In our simulation, we perform a multiple class classification task by logistic regression (multinomial logistic regression). The objective of the training process is to minimize the cross entropy between the empirical distribution and predicted distribution, which is a convex function. From Sec. V-B to Sec. V-D, we let each node have its own distribution to generate data sample. We have 10 categories and 50 features. Also, we assume that the distributions for different nodes are different, so training with only one or several nodes will deviate from the global optimality. In Sec. V-E, we test the performance by a real-world dataset: notMNIST, the size of which is around 12G. For this dataset, we have 10 categories and 256 features.\nAll simulations are implemented by tensorflow, and the codes are available [2]."}, {"heading": "B. Global Consensus", "text": "Our first interest about Alg. 2 is to show whether it can guarantee global consensus and how much time it takes to achieve consensus. Towards this end, we define dk = \u2211N i=1 \u2016\u03b2ki \u2212 \u03b2\u0304k\u2016 as the \u2018distance of the variables from global consensus\u2019 in time slot k, where \u03b2\u0304k = \u2211N i=1 \u03b2 k i\nN . We test the performance of two\n30-node systems connected by a regular graph: one\nFig. 2. Distance to global consensus\nFig. 3. Prediction error\nby 4-regular graph and the other one by 15-regular graph. The result is depicted in Fig. 2.\nNote that the y axis is logarithmic, and we can see that dk converges very fast to zero. More specifically, the value of dk is below 10 after 10k updates. Considering the number of the features (50) and nodes (30), this result indicates that the global consensus is almost achieved. Another observation is that it converges faster for the 15-regular graph, which fits the result of Lemma. 1 and our intuition."}, {"heading": "C. Prediction Error", "text": "We also test the prediction error by \u03b2\u0304k, the averaged value of current variables on all nodes. We test two 30-node systems: one by 2-regular graph and the other one by 10-regular graph. The results are reported in Fig. 3.\nAs we can see, in both cases, the prediction error decreases with more iterations (more data feeding).\n7\nThe prediction errors will be under 0.4 after 40k iterations (a random guess will lead to 0.9 prediction error) and the prediction error decreases faster for the 10-regular graph. Considering the facts that we add noise to the generated data samples in training process and the distributions of different nodes are different, we can conclude that our mechanism is very effective."}, {"heading": "D. With Network Size Increasing", "text": "One motivation of this work is to build a scalable and robust system to enjoy the merit of \u2018big data\u2019. It is necessary to ask whether we can we predict more accurately when more nodes join the system? To answer this question, we test the final prediction error with the number of nodes increasing from 10 to 30, and each node generates 500 data samples on average. Also we set the number of neighbors for each node to be 4 and 10 respectively. The result is shown in Fig. 4.\nAs we can see from Fig. 4, the decreasing trend of the prediction error with more nodes is quite clear (not always because the stochastic nature of the algorithm). Also, the advantage of a better connected system is more clear for a larger system (the number of nodes is larger). The rationale behind this phenomenon can be explained as follows. When the size of the system is small, the distances between each two nodes are relatively small, so the information of one node can easily spread out to all nodes, even though the number of neighbors of each node is small."}, {"heading": "E. Simulation based on a real-world dataset", "text": "We also test the performance of Alg. 2 on a realworld dataset notMNIST. This dataset contains the images of characters in the alphabet and digits. A glace of the letter \u2018A\u2019 in the dataset is shown in Fig. 5.\nWe select 10 categories to form a dataset to perform a multinomial logistic regression task. The simulation setting is similar to that of Sec. V-C. And we report the result in Fig. 6.\nAs we can see, the prediction error converges to less than 0.1, which is almost the same result of a centralized version of SGD. Also, the performances of two systems with different connectivity converge to the same value. This result indicates that as long as the system is connected, optimality will be asymptotically achieved, even the convergence speed will be affected by the system topology."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a fully distributed and asynchronized stochastic gradient method for a networked system. We augment the variable size and\n8 reformulate a stochastic programming problem with many constraints. By adapting some results in recent literature, we can achieve global consensus and optimality by only local computation and communication. A sharp lower bound of the convergence speed for the regular graph case is characterized and some simulation results are also provided to validate the merit of our design. One possible future work is to implement this algorithm on real-world system, especially on the heterogeneous system including high-performance computing clusters and low-performance mobile devices."}, {"heading": "A. Definition", "text": "We say that the linear regularity condition holds for B = B1 \u2229 B2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 BN with a constant scalar \u03b7 \u2208 (0, 1) if for all x in the space, we can have\n\u03b7\u2016x\u2212 \u03a0B(x)\u20162 \u2264 max i=1,\u00b7\u00b7\u00b7 ,N \u2016x\u2212 \u03a0Bi(x)\u20162.\nSome explanations are provided below for a better understanding. \u2022 If the linear regularity condition hold for \u03b70,\nthen it also holds with \u03b7\u0304 \u2208 (0, \u03b70). \u2022 For the result in Theorem. 2, a larger \u03b7 will\nlead to faster convergence speed. \u2022 For the constraints satisfy the linear regularity\ncondition, projecting onto them sequentially with many rounds (a cyclic projection method) will converge to their intersection. \u2022 The linear regularity condition automatically hold when Bi are linear half spaces [9]. So it holds for Eq. (5)."}, {"heading": "B. Proof of Lemma. 1", "text": "Proof Recall that \u03b2 = [\u03b21, \u03b22, \u00b7 \u00b7 \u00b7 , \u03b2N ]. Let us suppose we have a random variable Y , uniformly distributed on {\u03b21, \u03b22, \u00b7 \u00b7 \u00b7 , \u03b2N}. Besides, we have another random variable I distributed on {1, 2, \u00b7 \u00b7 \u00b7 , N}. Conditioning on that I = i, Y will be uniformly distributed on {\u03b2k|k \u2208 {i} \u222a Ni}.\nBy law of total variance, we can have\nVar(Y ) = E [Var(Y |I)] + Var(E[Y |I]). Note that\nVar(Y ) = 1\nN \u2016\u03b2 \u2212 \u03a0B(\u03b2)\u2016\nand\nE [Var(Y |I)] = \u2211 i Pi 1 1 + |Ni| \u2016\u03b2 \u2212 \u03a0Bi(\u03b2)\u20162\n\u2264 \u2211 i Pi max i\n1\n1 + |Ni| \u2016\u03b2 \u2212 \u03a0Bi(\u03b2)\u20162\n\u2264 1 1 + k max i \u2016\u03b2 \u2212 \u03a0Bi(\u03b2)\u20162(for a k-regular graph)\n9 If we can prove that\nVar(E[Y |I]) \u2264 \u03c322Var(Y ), (10)\nwe can have\nVar(Y ) \u2264 1 1 + k max i \u2016\u03b2 \u2212 \u03a0Bi(\u03b2)\u20162 + \u03c322Var(Y ).\nThen\n(1\u2212 \u03c322)Var(Y ) = (1\u2212 \u03c322) 1\nN \u2016\u03b2 \u2212 \u03a0B(\u03b2)\u2016\n\u2264 1 1 + k max i \u2016\u03b2 \u2212 \u03a0Bi(\u03b2)\u20162,\nwhich is the result of Lemma. 1. To complete the proof, the remaining part is to show Eq. (10). We define another matrix A\u0304 = [ 1\nN ]N\u00d7N . Note that\nVar(Y )\n= 1\nN \u2016(I \u2212 A\u0304)\u03b2\u20162\n= 1\nN \u03b2T (I \u2212 A\u0304)T (I \u2212 A\u0304)\u03b2,\nand\nVar(E[Y |I])\n= 1\nN \u2016(A\u2212 A\u0304)\u03b2\u20162\n= 1\nN \u03b2T (A\u2212 A\u0304)T (A\u2212 A\u0304)\u03b2.\nNow it would be enough to show that \u03c322(I \u2212 A\u0304)T (I\u2212 A\u0304)\u2212 (A\u2212 A\u0304)T (A\u2212 A\u0304) is positive semidefinite. This statement holds with the following facts. \u2022 A is a doubly stochastic matrix. Its largest\nsingular value is 1, with singular vector being [ 1 N , 1 N , \u00b7 \u00b7 \u00b7 , 1 N ] . We denote its eigenvalue decomposition by A = V \u03a3V T , where \u03a3 is a diagonal matrix. \u2022 A\u0304 is of rank 1 and it can be decomposed as A\u0304 =\nV IndV T , where Indi,j = { 1 if i = j = 1, 0otherwise .\nThe proof is completed."}], "references": [{"title": "Distributed dual averaging in networks", "author": ["A. Agarwal", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Fastest mixing markov chain on a graph", "author": ["S. Boyd", "P. Diaconis", "L. Xiao"], "venue": "SIAM review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Markov approximation for combinatorial network optimization", "author": ["M. Chen", "S.C. Liew", "Z. Shao", "C. Kai"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The rate of convergence for the cyclic projections algorithm iii: regularity of convex sets", "author": ["F. Deutsch", "H. Hundal"], "venue": "Journal of Approximation Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Markov chain monte carlo", "author": ["W.R. Gilks"], "venue": "Wiley Online Library,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["C.-J. Hsieh", "H.-F. Yu", "I.S. Dhillon"], "venue": "arXiv preprint,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.-Y. Su"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convergence rate of distributed averaging dynamics and optimization in networks", "author": ["A. Nedich"], "venue": "Foundations and Trends R  \u00a9 in Systems and Control,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Dogwild!\u0142distributed hogwild for cpu & gpu", "author": ["C. Noel", "S. Osindero"], "venue": "In NIPS workshop on Distributed Machine Learning and Matrix Computations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Hogwild: A lockfree approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Random multi-constraint projection: Stochastic gradient methods for convex optimization with many constraints", "author": ["M. Wang", "Y. Chen", "J. Liu", "Y. Gu"], "venue": "arXiv preprint arXiv:1511.03760,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "By adapting the results in a recent paper [18], we design a fully distributed and asynchronized stochastic gradient descent (SGD) algorithm.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "As one of the most important optimization algorithms, stochastic gradient descent (SGD) and many of its variants have been proposed to solve different optimization problems, and are gaining their popularity in this \u2018big-data\u2019 era [4].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "Popular examples include SVM, Logistic Regression for the convex cases [5], and deep neural networks for the non-convex cases [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Popular examples include SVM, Logistic Regression for the convex cases [5], and deep neural networks for the non-convex cases [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": "To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "To handle this issue, distributed versions of SGD have been tried by many practioneer [8], [16] and analyzed by theoretical researchers [11], [13], [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "In practice, the \u2018server\u2019 and the workers can be many computers, connected by a computer network, like the design of the \u2018parameter-server\u2019 [12]; also, the \u2018server\u2019 can be the shared memory of one computer and the workers can be different threads, like the design of \u2019Hogwild!\u2019 [17].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "In practice, the \u2018server\u2019 and the workers can be many computers, connected by a computer network, like the design of the \u2018parameter-server\u2019 [12]; also, the \u2018server\u2019 can be the shared memory of one computer and the workers can be different threads, like the design of \u2019Hogwild!\u2019 [17].", "startOffset": 278, "endOffset": 282}, {"referenceID": 10, "context": "In a recent work [13], the authors show that, for the \u2018server-worker\u2019 structure, an asynchronized dual-based algorithm can achieve", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "\u2022 By apply the result of the recent paper on stochastic optimization problem with many constraints [18], we design an algorithm for the reformulated optimization problem.", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "The stochastic programming problem with convex objective functions has been extensively studied in the literature, and many off-the-shelf algorithm can be used, see [18] and the reference therein.", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "The results in [3], [14], [15] also consider to minimize an objective function over a networked system, but the algorithms require all nodes to perform update in each slot and synchronization is needed for realworld implementation, which, however, is difficult to achieve.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "The results in [3], [14], [15] also consider to minimize an objective function over a networked system, but the algorithms require all nodes to perform update in each slot and synchronization is needed for realworld implementation, which, however, is difficult to achieve.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "The results in [3], [14], [15] also consider to minimize an objective function over a networked system, but the algorithms require all nodes to perform update in each slot and synchronization is needed for realworld implementation, which, however, is difficult to achieve.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "In [18], the authors investigated a general stochastic programming problem in the following form,", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Unless otherwise mentioned, we will assume that the objective function satisfies Assumption 1 in [18] and stepsizes are set properly.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Theorem 1: (Feasibility and Optimality) [18] Both DF (\u03b2) and DO(\u03b2) will converge to zero almost surely.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Theorem 2: (Convergence Speed) [18] We can have", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "We omit the proofs of Theorem 1 and 2 since they are established directly from the results in [18] (on Page 9 and Page 10), but we want to provide some results specifically related to our problem.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "The readers can refer to [6] for more information.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "The rationale behind this mechanism is similar to that of Monto Carlo Markov Chain Sampling [10], CSMA [7], etc.", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "The rationale behind this mechanism is similar to that of Monto Carlo Markov Chain Sampling [10], CSMA [7], etc.", "startOffset": 103, "endOffset": 106}], "year": 2017, "abstractText": "This paper considers a general data-fitting problem over a networked system, in which many computing nodes are connected by an undirected graph. This kind of problem can find many real-world applications and has been studied extensively in the literature. However, existing solutions either need a central controller for information sharing or requires slot synchronization among different nodes, which increases the difficulty of practical implementations, especially for a very large and heterogeneous system. As a contrast, in this paper, we treat the data-fitting problem over the network as a stochastic programming problem with many constraints. By adapting the results in a recent paper [18], we design a fully distributed and asynchronized stochastic gradient descent (SGD) algorithm. We show that our algorithm can achieve global optimality and consensus asymptotically by only local computations and communications. Additionally, we provide a sharp lower bound for the convergence speed in the regular graph case. This result fits the intuition and provides guidance to design a \u2018good\u2019 network topology to speed up the convergence. Also, the merit of our design is validated by experiments on both synthetic and real-world datasets.", "creator": "LaTeX with hyperref package"}}}