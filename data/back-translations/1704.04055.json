{"id": "1704.04055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Land Cover Classification via Multi-temporal Spatial Data by Recurrent Neural Networks", "abstract": "Nowadays, modern Earth observation programs produce huge amounts of satellite imaging series (SITS) that can be useful for monitoring geographical areas over time. How such information can be efficiently analyzed is still an open question in the field of remote sensing. Recently, deep learning methods have been shown to be suitable to deal with remote sensing data primarily for classification of scenes (i.e., Convolutionary Neural Networks - CNNs - on single images), while very few studies exist that deal with time-low learning approaches (i.e. Recurrent Neural Networks - RNNNs) to deal with remote sensing time series. In this letter, we evaluate the ability of recurrent neural networks, especially the Long-Short Term Memory (LSTM) model, to perform the classification of land coverings taking into account multitemporal spatial data derived from a time series of satellite imagery. We evaluate the ability of recurrent neural networks, in particular the Long-Term Memory (LSTM) model, to perform the classification of land coverings taking into account multitemporal spatial data that are derived from a time series of satellite images.", "histories": [["v1", "Thu, 13 Apr 2017 09:47:12 GMT  (251kb,D)", "http://arxiv.org/abs/1704.04055v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dino ienco", "raffaele gaetano", "claire dupaquier", "pierre maurel"], "accepted": false, "id": "1704.04055"}, "pdf": {"name": "1704.04055.pdf", "metadata": {"source": "CRF", "title": "Land Cover Classification via Multi-temporal Spatial Data by Recurrent Neural Networks", "authors": ["Dino Ienco", "Raffaele Gaetano", "Claire Dupaquier", "Pierre Maurel"], "emails": [], "sections": [{"heading": null, "text": "In this letter we evaluate the ability of Recurrent Neural Networks, in particular the Long-Short Term Memory (LSTM) model, to perform land cover classification considering multi-temporal spatial data derived from a time series of satellite images. We carried out experiments on two different datasets considering both pixel-based and object-based classification. The obtained results show that Recurrent Neural Networks are competitive compared to state-of-the-art classifiers, and may outperform classical approaches in presence of low represented and/or highly mixed classes. We also show that using the alternative feature representation generated by LSTM can improve the performances of standard classifiers.\nIndex Terms\nRecurrent Neural Networks, Satellite Image time series, Land Cover classification, Deep Learning.\nI. INTRODUCTION\nMODERN earth observation programs produce huge volumes of remotely sensed data every day. Such information can beorganized in time series of satellite images that can be useful to monitor geographical zones through time. Efficiently manage and analyze remote sensing time series is still an open challenge in the remote sensing field [13].\nIn the context of land cover classification, exploiting time series of satellite images, instead that one single image, can be fruitful to distinguish among classes based on the fact they have different temporal profiles [1]. Despite the usefulness of temporal trends that can be derived from remote sensing time series, most of the proposed strategies [7], [12] directly apply standard machine learning approaches (i.e. Random Forest, SVM) on the stacked images. Since these approaches did not model temporal correlations, they manage features independently from each others, ignoring any temporal dependency which data may exhibit. Recently, the deep learning revolution [19] has shown that neural network models are well adapted tools to manage and automatically classify remote sensing data. While standard CNNs techniques are well suited to deal with spatial autocorrelation, the same approaches are not adapted to correctly manage long and complex temporal dependencies [3]. A family of deep learning methods especially tailored to cope with temporal correlations are Recurrent Neural Networks [3] and, in particular, Long Short Term Memory (LSTM) networks [10]. Such models explicitly capture temporal correlations by recursion and they have already proved to be effective in different domains such as speech recognition [9], natural language processing [14], image completion [18]. Only recently, in the remote sensing field, the work proposed in [15] performs preliminary experiments with LSTM model on a (small) time series composed of only two dates to perform supervised change detection. The task was modeled as a binary classification problem (change vs. no-change). To the best of our knowledge, RNNs (i.e. LSTM) have not yet been considered to deal with land cover classification of deeper time series. Like any other deep learning model [3], LSTM can be used as a classifier itself or employed to extract new discriminative features (or representation). In the latter case the extracted features are successively used to feed a standard learning algorithm that does not consider temporal dependencies (i.e. Random Forest, Naive Bayes, KNN, SVMs).\nIn this letter we evaluate the quality of RNN models - Long Short Term Memory - to deal with land cover classification via multi-temporal spatial data that are derived from SITS. More in detail, we perform experiments on two study areas: i) the THAU basin, a site located in the south of France, from which we obtain a time series of 3 dates, and ii) the REUNION ISLAND, a region of France located in the Indian Ocean (east of Madagascar) from which we derive a 23-date times series. While on the first area we conduct an object-oriented classification, on the second site we have performed a pixel-based prediction showing the general applicability of RNNs models to both object and pixel-level analysis. We also assess the RNN model (i.e. LSTM) as feature extractor evaluating the quality of the new generated features to feed the same baseline classifiers we have used as reference methods.\nD. Ienco is with UMR-TETIS laboratory, IRSTEA, Montpellier, France and with LIRMM laboratory, Montpellier, France (email: dino.ienco@irstea.fr). C. Dupaquier and P. Maurel are with UMR-TETIS laboratory, IRSTEA, Montpellier, France (email: claire.dupaquier@irstea.fr, pierre.maurel@irstea.fr). R. Gaetano is with UMR-TETIS laboratory, CIRAD, Montpellier, France (email: raffaele.gaetano@cirad.fr).\nar X\niv :1\n70 4.\n04 05\n5v 1\n[ cs\n.C V\n] 1\n3 A\npr 2\n01 7\n2 The rest of the paper is organized as follows: Section II introduces the LSTM unit and specifies the network architecture we propose, Section III describes the two datasets, the time series characteristics and the preprocessing we have performed on the source data. Experimental setting and results are discussed in Section IV. Conclusions are drawn in Section V."}, {"heading": "II. METHOD", "text": "We propose a neural architecture involving LSTM unit to deal with land cover classification via multi-temporal spatial data. We also assess the representation learned by the RNN model with standard classification strategies commonly used to perform prediction in the remote sensing field."}, {"heading": "A. Long-Short Term Memory", "text": "Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition [9], signal processing [17], natural language processing [14] and image completion [18]. Differently from standard feed forward networks (i.e. CNNs), RNNs explicitly manage temporal data dependencies since the output of the neuron at time t-1 is used, together with the next input, to feed the neuron itself at time t. A sketch of a typical RNN neuron is depicted in Figure 1.\nThe most well-known type of RNN is the Long-Short Term Memory (LSTM) [10] model. There are many variants of LSTM network [10] but here we refer to the architecture proposed in [8]. LSTM models were mainly introduced with the purpose to learn long term dependencies [10], since previous RNN models failed in this task due to the problem of vanishing and exploding gradients. The equations (1), (2), (3), (4), (5) and (6) formally describes the LSTM neuron while Figure 2 graphically depicts the LSTM unit. The symbol indicates an element-wise multiplication while \u03c3 and tanh represent Sigmoid and Hyperbolic Tangent respectively. The input of the LSTM is a sequence of variables (x1, ..., xN ) where a generic element xt is a feature vector and t refers to the corresponding timestamp. RNN models are able to manage variable-length data sequences.\nThe LSTM unit is composed of two cell states, the memory Ct and the hidden state ht, and three different gates, the input gate it, the forget gate ft and the output gate ot that are employed to control the flow of information. All the three gates combine the current input xt with the hidden state ht\u22121 coming from the previous timestamp. The gates have also two important functions: i) they regulate how much information have to be forgotten/reminded during the process; ii) they deal with the problem of vanishing/exploding gradients. We can observe that the gates are implemented by a sigmoid. This function returns values between 0 and 1 where, in this context, 0 indicates that the information is completely forgotten and 1 means that the information is completely retained.\n3 The LSTM unit uses also a temporary cell state yt that rescales the current input always taking into account the previous hidden state. This temporary cell is implemented by an hyperbolic tangent function that returns values between -1 and 1. Both sigmoid and hyperbolic tangent are applied element-wise.\nThe input gate it regulates how much of the current information needs to be maintained (it yt) while the forget gate ft indicates how much of the previous memory needs to be retained at the current step (ft ct\u22121). Finally, the output gate impacts on the new hidden state ht deciding how much information of the current memory will be outputted to the next step. The different W\u2217\u2217 matrices and bias coefficients b\u2217 are the parameters learned during the training of the model. Both, the memory Ct and the hidden state ht are forwarded to the next time step.\nit = \u03c3(Wixxt +Wihht\u22121 + bi) (1) ft = \u03c3(Wfxxt +Wfhht\u22121 + bf ) (2)\nyt = tanh(Wyxxt +Wyhht\u22121 + by) (3) ct = it yt + ft ct\u22121 (4)\not = \u03c3(Woxxt +Wohht\u22121 + bo) (5) ht = ot tanh(ct) (6)"}, {"heading": "B. LSTM-Based Time Series Classification", "text": "The LSTM neuron learns an internal representation of the input sequences (in our case objects or pixels time series) but it does not make any prediction by itself. To perform the classification task, we stack on top of the LSTM neuron a SoftMax layer [9] to accomplish the final multi-class prediction. The SoftMax layer has as many neurons as the number of the classes to predict. We choose SoftMax instead of Sigmoid function because the value of the SoftMax layer can be seen as a probability distribution over the classes that sum to 1 while each of the Sigmoid neurons can output a value between 0 and 1. This is due to the fact that, for the SoftMax neuron, the values are normalized per layer while no normalization is performed in the case of Sigmoid layer. This is why, in our context (multi-class prediction), we prefer the SoftMax instead of Sigmoid layer since we know that our samples exclusively belong to a single class. From an architectural point of view, the connection between the LSTM and the SoftMax layer is realized fully connecting the last hidden state vector produced by the LSTM unit with the SoftMax neurons."}, {"heading": "C. Representation Learning with LSTM for time series data", "text": "Standard deep learning approaches can also be seen as a way to produce a new, more discriminative representation of the original data [3]. Another way to assess the quality of LSTM unit for our land cover classification task is to use the features learned by the LSTM layer to feed a standard classifier. More in detail, we propose to employ the last hidden state vector, produced by the LSTM unit, as new data representation and, successively, train standard machine learning classifiers over such new set of features."}, {"heading": "III. DATA", "text": "In order to prove the generality of our proposal, it has been tested over two different remote-sensing based datasets. The first is a collection of spatial objects described by a set regional statistics extracted from very high spatial resolution imagery (VHSR), but with a limited temporal depth. The second one is a pixel-based dataset, more noisy but richer in both spectral and temporal resolution. Detailed descriptions are provided in the following subsections."}, {"heading": "A. THAU dataset", "text": "The first dataset has been generated using a time series of Ple\u0301iades VHSR images (2 m) acquired in the context of the Airbus DS/Spot Image distribution (July and September 2012, March 2013, c\u00a9 CNES). The study site is the THAU Basin located in the South of France, close to Montpellier. It covers an area of 42 000 ha with 70% of land area. The north is mainly composed of agricultural fields (i.e. vineyards) and natural spaces while the south is dominated by urban and industrial zones. For each date, two orthorectified, atmospherically corrected scenes are mosaicked.\nUsing the multi-temporal stack, a segmentation has been performed to extract a consistent multi-temporal object layer. Segmentation was performed using the Multiresolution Segmentation technique [2] available in the eCognition Developer software. Each object has been then featured using statistical mean and standard deviation using the four native bands (blue, green, red and near-infrared) and the NDVI. A total of 10 features are computed per object and per date (5 means and 5 standard deviations).\nThe so obtained segments have been subsequently filtered and labeled in 11 different classes by visual inspection. A total of 15 196 objects is retained. The set of classes with the relative cardinality is reported in Table I.\n4 ID Land Cover Class N. of Objects (1) Tree crops 600 (2) Forests and woods 2 445 (3) Water 556 (4) Summer crops 81 (5) Winter crops 677 (6) Grasslands 3 882 (7) Sclerophyll vegetation 2 457 (8) Truck farming 227 (9) Bare soils 299 (10) Salt marshes 236 (11) Vineyards 3 735\nTABLE I LAND COVER CLASSES AND THEIR CARDINALITY FOR THE THAU TIME SERIES DATASET"}, {"heading": "B. REUNION ISLAND dataset", "text": "The second dataset has been generated from an annual time series of 23 Landsat 8 images acquired in 2014 above the Reunion Island (2866 \u00d7 2633 pixels at 30 m spatial resolution), provided at level 2A1. Source data have been further processed to fill cloudy observations via pixel-wise multi-temporal linear interpolation on each multi-spectral band (OLI) independently, and compute complementary radiometric indices (NDVI, NDWI and brightness index - BI). A total of 10 features (7 surface reflectances plus 3 indices) are considered for each pixel at each timestamp.\nReference land cover data has been built using two publicly available dataset, namely the 2012 Corine Land Cover (CLC) map and the 2014 farmers\u2019 graphical land parcel registration (Re\u0301gistre Parcellaire Graphique - RPG). The most significant classes for the study area have been retained, and a spatial processing (aided by photo-interpretation) has also been performed to ensure consistency with image geometry. Finally, a pixel-based random sampling of this dataset has been applied to provide an almost balanced ground truth. The final reference dataset consists of a total of 37 900 pixels distributed over 9 classes as reported in Table II."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section we report the experimental settings and we discuss the results we obtained on the two SITS datasets we presented in Section III."}, {"heading": "A. Experimental Settings", "text": "We compare the LSTM-based Time Series Classification model to standard machine learning approaches commonly employed to perform land cover classification from multi-temporal spatial data [7], [12]. We also assess the value of the representation learned by the proposed model following the idea described in Section II-C.\nTo our purpose, we use Random Forest (RF) and Support Vector Machine (SVM) as standard classification strategies. For the RF model, we set the number of generated trees equals to 400 and we allow a maximum tree depth of 10. For the SVM model we use RBF kernel with complexity parameter and gamma equal to 100 and 0.01 respectively. For Random Forest we used the python implementation supplied by the Scikit-learn library [16] while for SVM we use the LibSVM implementation [4]. The same RF and SVM settings are used for both original data and the new representation learned by the LSTM-Based Time Series Classification model. For the latter, we set the number of hidden dimensions equal to 512, an initial learning rate equals to 5\u00d7 10\u22124 and a decay of 5\u00d7 10\u22125. We implement the model via the Keras python library [5] with Theano as back end. We used as optimization method the RMSprop strategy that is commonly employed to train LSTM units [6]. The model is trained for 200 epochs with a batch size equals to 20. We named RF(LSTM) (resp. SVM(LSTM)) the Random Forest (resp. SVM) learned over the new feature space induced by our RNN model. More in detail, each training and test instance is transformed\n1The source data are provided by the French Po\u0302le The\u0301matique Surfaces Continentales THEIA (www.theia-land.fr) and preprocessed by the Multi-sensor Atmospheric Correction and Cloud Screening (MACCS) level 2A processor [11] developed at the French National Space Agency (CNES) to provide accurate atmospheric, environmental and geometric corrections as well as precise cloud masks.\n5 in a 512 feature vector (the dimension of the hidden state of the LSTM neuron) and, successively, the classifiers are learned from this new representation instead of the original data.\nTo validate the different methods, we perform a 5-fold cross validation. Due to the unbalanced nature of the two time series datasets, in order to assess classification performances we use not only the Global Accuracy and Kappa measures, but we also provide average and per-class F-Measure."}, {"heading": "B. Results and Discussions", "text": "The Tables III and IV and Figures 3 and 4 summarize the results we have obtained on the two SITS datasets. Considering the THAU dataset, Table III depicts the average values of Accuracy, F-Measure and Kappa for the different methods. We can observe that the LSTM-based classifier outperforms both RF and SVM approaches regarding all the three metrics. The more important gain is reached when the average F-Measure is taken into account, the LSTM-based classifier obtaining a score of 74.63% while the second best method (SVM(LSTM)) attains a score equals to 73.31%. Interestingly, we can also highlight that, for the THAU dataset, the classifiers trained on the features (representation) learned by the Recurrent Neural Network RF(LSTM) and SVM(LSTM) exhibit better performances than the same classifiers coupled with the original time series data when the F-Measure is considered. In terms of Accuracy, the behavior is comparable considering the SVM vs SVM(LSTM) while the RF model clearly benefit of the new data representation.\nA more detailed assessment is provided in Figure 3 where the per-class F-Measure is reported. The first point we can highlight is that, for classes with few reference samples (i.e. (1),(4),(8), (9) and (10)), the LSTM network neatly outperforms standard approaches, which in some cases (i.e. (1) and (4)) completely miss the classes, while it obtains similar or slightly better results w.r.t. RF and SVM for well represented classes. The second point is related to the comparison between standard approaches trained on the original data and the same methods powered by features learned by our proposal. We can see that the use of the new learned representation improves the performances of both RF and SVM. Again, this fact is particularly evident on critical classes like (1), (4), (8) (for SVM) and (10) (for RF).\nTable IV summarizes the results on the REUNION ISLAND time series dataset. Similarly to the previous case, the LSTMbased classifier behaves better than the standard machine learning methods considering Accuracy and F-Measure; still in accord with the previous results, also the classifiers trained on the new features, obtained by the proposed network, outperform their counterparts trained on the original feature space. Conversely to the previous experiment, in this case the highest value of F-Measure is reached by the SVM(LSTM) but we can observe that the LSTM-based classifier still reaches competitive results. Figure 4 reports the per-class F-Measure results on the REUNION ISLAND dataset. Also in this case we can note that the LSTM-based approach works well for low represented and difficult classes (i.e. (8) - \u201dOther Crops\u201d) and it remains competitive on all the other classes.\nAs expected, the combined optimization and learning of a new feature representation along with classification, proper to all deep learning approaches, provides here a better support to discriminate among the different classes. In addition, all these results indicate that the LSTM model is well suited to capture long-short temporal dependencies as opposed to common classification approaches where all the information are managed at the same level forgetting temporal correlations. This is particularly evident on low represented and highly mixed classes: Tree Crops, Summer crops and Truck Farming (resp. Other Crops) for the THAU (resp. REUNION ISLAND) dataset. All these classes are related to agricultural activities whose time patterns are strongly varying due to the heterogeneity of practices, which make the corresponding classes detectable only considering short portions of the time series in which crop conditions are comparable.\nWe remind that our proposal uses only one LSTM layer while more layers can be stacked together to build more complex architectures [9]. This is out of our scope and we leave this point for future researches, since our main objective is to highlight the quality and the suitableness of RNNs methods to manage and analyze SITS data."}, {"heading": "V. CONCLUSION", "text": "In this letter we asses the benefit of using Recurrent Neural Network (LSTM) to perform land cover classification via multitemporal spatial data. We have validated the proposed model on two different SITS based datasets showing that the proposed framework efficiently deals with both pixel-based and object-based classification.\nThe proposed framework proved competitive, yet outperforming compared to classical approaches, with the remarkable advantage of improving the quality of the predictions on \u201cweak\u201d classes from unbalanced datasets. We also highlight that the\n6\nproposed LSTM-based classification model can be used as feature extractor to learn a new data representation that positively affect the performances of standard classification approaches on SITS data."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors acknowledge also the National Research Agency in the framework of the program \u201dInvestissements d\u2019Avenir\u201d for the GEOSUD project (ANR-10-EQPX-20) for the distribution of the Ple\u0301iades satellite images."}], "references": [{"title": "Comparative analysis of modis time-series classification using support vector machines and methods based upon distance and similarity measures in the brazilian cerrado-caatinga boundary", "author": ["N.A. Abade", "O. Ablio de Carvalho Jnior", "R. Fontes Guimares", "S.N. de Oliveira"], "venue": "Remote Sensing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multiresolution segmentation: an optimization approach for high quality multi-scale image segmentation", "author": ["M. Baatz", "A. Sch\u00e4pe"], "venue": "Angewandte Geographische Informationsverarbeitung XII,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "IEEE TPAMI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM TIST,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "CoRR, abs/1502.04390,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Analysis of multitemporal classification techniques for forecasting image time series", "author": ["R. Flamary", "M. Fauvel", "M. Dalla Mura", "S. Valero"], "venue": "IEEE Geosci. Remote Sensing Lett.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F.A. Cummins"], "venue": "Neural Comp.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G.E. Hinton"], "venue": "In ICASSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R. Kumar Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A multi-temporal and multi-spectral method to estimate aerosol optical thickness over land, for the atmospheric correction of formosat-2, landsat, vens and sentinel-2 images", "author": ["O. Hagolle", "M. Huc", "D. Villa Pascual", "G. Dedieu"], "venue": "Remote Sensing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Classification and monitoring of reed belts using dual-polarimetric terrasar-x time series", "author": ["I. Heine", "T. Jagdhuber", "S. Itzerott"], "venue": "Remote Sensing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Monitoring land-cover changes: A machine-learning perspective", "author": ["A. Karpatne", "Z. Jiang", "R.R. Vatsavai", "S. Shekhar", "V. Kumar"], "venue": "IEEE Geoscience and Remote Sensing Magazine,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Assessing the ability of lstms to learn syntax-sensitive", "author": ["T. Linzen", "E. Dupoux", "Y. Goldberg"], "venue": "dependencies. TACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning a transferable change rule from a recurrent neural network for land cover change detection", "author": ["H. Lyu", "H. Lu", "L. Mou"], "venue": "Remote Sensing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Simultaneous multichannel signal transfers via chaos in a recurrent neural network", "author": ["K. Soma", "R. Mori", "R. Sato", "N. Furumai", "S. Nara"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "K. Kavukcuoglu", "O. Vinyals", "A. Graves"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Deep learning for remote sensing data: A technical tutorial on the state of the art", "author": ["L. Zhang", "B. Du"], "venue": "IEEE Geoscience and Remote Sensing Magazine,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Efficiently manage and analyze remote sensing time series is still an open challenge in the remote sensing field [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "In the context of land cover classification, exploiting time series of satellite images, instead that one single image, can be fruitful to distinguish among classes based on the fact they have different temporal profiles [1].", "startOffset": 221, "endOffset": 224}, {"referenceID": 5, "context": "Despite the usefulness of temporal trends that can be derived from remote sensing time series, most of the proposed strategies [7], [12] directly apply standard machine learning approaches (i.", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "Despite the usefulness of temporal trends that can be derived from remote sensing time series, most of the proposed strategies [7], [12] directly apply standard machine learning approaches (i.", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Recently, the deep learning revolution [19] has shown that neural network models are well adapted tools to manage and automatically classify remote sensing data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "While standard CNNs techniques are well suited to deal with spatial autocorrelation, the same approaches are not adapted to correctly manage long and complex temporal dependencies [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "A family of deep learning methods especially tailored to cope with temporal correlations are Recurrent Neural Networks [3] and, in particular, Long Short Term Memory (LSTM) networks [10].", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "A family of deep learning methods especially tailored to cope with temporal correlations are Recurrent Neural Networks [3] and, in particular, Long Short Term Memory (LSTM) networks [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 7, "context": "Such models explicitly capture temporal correlations by recursion and they have already proved to be effective in different domains such as speech recognition [9], natural language processing [14], image completion [18].", "startOffset": 159, "endOffset": 162}, {"referenceID": 12, "context": "Such models explicitly capture temporal correlations by recursion and they have already proved to be effective in different domains such as speech recognition [9], natural language processing [14], image completion [18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 16, "context": "Such models explicitly capture temporal correlations by recursion and they have already proved to be effective in different domains such as speech recognition [9], natural language processing [14], image completion [18].", "startOffset": 215, "endOffset": 219}, {"referenceID": 13, "context": "Only recently, in the remote sensing field, the work proposed in [15] performs preliminary experiments with LSTM model on a (small) time series composed of only two dates to perform supervised change detection.", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "Like any other deep learning model [3], LSTM can be used as a classifier itself or employed to extract new discriminative features (or representation).", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition [9], signal processing [17], natural language processing [14] and image completion [18].", "startOffset": 154, "endOffset": 157}, {"referenceID": 15, "context": "Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition [9], signal processing [17], natural language processing [14] and image completion [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition [9], signal processing [17], natural language processing [14] and image completion [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Recurrent Neural Networks are well established machine learning techniques that demonstrate their quality in different domains such as speech recognition [9], signal processing [17], natural language processing [14] and image completion [18].", "startOffset": 237, "endOffset": 241}, {"referenceID": 8, "context": "The most well-known type of RNN is the Long-Short Term Memory (LSTM) [10] model.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "There are many variants of LSTM network [10] but here we refer to the architecture proposed in [8].", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "There are many variants of LSTM network [10] but here we refer to the architecture proposed in [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "LSTM models were mainly introduced with the purpose to learn long term dependencies [10], since previous RNN models failed in this task due to the problem of vanishing and exploding gradients.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "To perform the classification task, we stack on top of the LSTM neuron a SoftMax layer [9] to accomplish the final multi-class prediction.", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Standard deep learning approaches can also be seen as a way to produce a new, more discriminative representation of the original data [3].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Segmentation was performed using the Multiresolution Segmentation technique [2] available in the eCognition Developer software.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "We compare the LSTM-based Time Series Classification model to standard machine learning approaches commonly employed to perform land cover classification from multi-temporal spatial data [7], [12].", "startOffset": 187, "endOffset": 190}, {"referenceID": 10, "context": "We compare the LSTM-based Time Series Classification model to standard machine learning approaches commonly employed to perform land cover classification from multi-temporal spatial data [7], [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "For Random Forest we used the python implementation supplied by the Scikit-learn library [16] while for SVM we use the LibSVM implementation [4].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "For Random Forest we used the python implementation supplied by the Scikit-learn library [16] while for SVM we use the LibSVM implementation [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "We used as optimization method the RMSprop strategy that is commonly employed to train LSTM units [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "fr) and preprocessed by the Multi-sensor Atmospheric Correction and Cloud Screening (MACCS) level 2A processor [11] developed at the French National Space Agency (CNES) to provide accurate atmospheric, environmental and geometric corrections as well as precise cloud masks.", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "We remind that our proposal uses only one LSTM layer while more layers can be stacked together to build more complex architectures [9].", "startOffset": 131, "endOffset": 134}], "year": 2017, "abstractText": "Nowadays, modern earth observation programs produce huge volumes of satellite images time series (SITS) that can be useful to monitor geographical areas through time. How to efficiently analyze such kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification (i.e. Convolutional Neural Networks CNNs on single images) while only very few studies exist involving temporal deep learning approaches (i.e Recurrent Neural Networks RNNs) to deal with remote sensing time series. In this letter we evaluate the ability of Recurrent Neural Networks, in particular the Long-Short Term Memory (LSTM) model, to perform land cover classification considering multi-temporal spatial data derived from a time series of satellite images. We carried out experiments on two different datasets considering both pixel-based and object-based classification. The obtained results show that Recurrent Neural Networks are competitive compared to state-of-the-art classifiers, and may outperform classical approaches in presence of low represented and/or highly mixed classes. We also show that using the alternative feature representation generated by LSTM can improve the performances of standard classifiers.", "creator": "LaTeX with hyperref package"}}}