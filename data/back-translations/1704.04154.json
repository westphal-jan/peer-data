{"id": "1704.04154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation", "abstract": "Our hope is that a representation that is independent of the language in which a sentence is written will capture the underlying semantics. We look for and compare more than 1.4 million sentence representations in three different languages and examine the characteristics of narrow sentences. We provide experimental evidence that sentences that are closely embedded are indeed highly semantically related, but often have a very different structure and syntax. These relationships also apply when comparing sentences in different languages.", "histories": [["v1", "Thu, 13 Apr 2017 14:40:40 GMT  (130kb,D)", "http://arxiv.org/abs/1704.04154v1", "8 pages, 3 figures"], ["v2", "Tue, 8 Aug 2017 15:09:44 GMT  (134kb,D)", "http://arxiv.org/abs/1704.04154v2", "11 pages, 2 figures, published at ACL workshop RepL4NLP"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["holger schwenk", "matthijs douze"], "accepted": false, "id": "1704.04154"}, "pdf": {"name": "1704.04154.pdf", "metadata": {"source": "CRF", "title": "Learning Joint Multilingual Sentence Representations with Neural Machine Translation", "authors": ["Holger Schwenk", "Ke Tran", "Orhan Firat", "Matthijs Douze"], "emails": ["schwenk@fb.com", "m.k.tran@uva.nl", "matthijs@fb.com", "orhanf@google.com"], "sections": [{"heading": "1 Introduction", "text": "It is today common practice to use distributed representations of words, often called word embeddings, in almost all NLP applications. It has been shown that syntactic and semantic relations can be captured in this (high-dimensional) embedding space, see for instance (Mikolov et al., 2013). To process sequences of words, that is sentences or small paragraphs, these word embeddings need to be \u201ccombined\u201d into a representation of the whole sequence. Common approaches include: simple techniques like bag-of-words or some type of pooling, eg. (Arora et al., 2017), recursive neural networks, eg. (Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg. (Cho et al., 2014), convolutional neural networks, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg. (Zhao et al., 2015).\nIn some NLP applications, both the input and output are sentences. A very popular approach\n\u2217Work done while authors are at Facebook AI Research. \u2020 Now at Google orhanf@google.com\nto handle such tasks is the so-called \u201cencoderdecoder approach\u201d, also named \u201csequence-tosequence processing\u201d. The main idea is to first encode the input sentence into an internal representation, and then to generate the output sentence from this representation. A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Current best practice is to use recurrent neural networks for the encoder and decoder, but alternative architectures like convolutional networks are also being explored.\nThe performance of these vanilla sequence-tosequence models substantially degrades with the sequence length since it is difficult to encode long sequences into a single, fixed-size representation. A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015). The generation of each target word is conditioned on a weighted subset of source words, instead of the full sentence. NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).\nIn this work, we aim at learning multilingual sentence representations, i.e. which are independent of the language. Since we have to compare these representations among each other, for the same or between multiple languages, we only consider representations of fixed size.\nThere are many motivations to learn such a multilingual sentence representation, in particular:\n\u2022 it is likely to capture the underlying semantics of the sentence (since the meaning is the only common characteristic of a sentence formulated in several languages);\nar X\niv :1\n70 4.\n04 15\n4v 1\n[ cs\n.C L\n] 1\n3 A\npr 2\n01 7\n\u2022 it has the potential to transfer many sentence processing applications to other languages (classification, sentiment analysis, semantic similarity, etc), without the need for language specific training data.\n\u2022 it enables multilingual search;\n\u2022 such representation could be considered as sort of a continuous space interlingua.\nTo train these multilingual sentence embeddings we are using the framework of NMT with multiple encoders and decoders. In the next section, we describe our model in detail and relate it to existing research. We then present an experimental evaluation."}, {"heading": "2 Architecture", "text": "We propose to use multiple encoders and decoders, one for each source and target language respectively. The notion of multiple input languages can be also extended to different modalities, e.g. speech and images. One can also envision to add classification tasks, in addition to sequence generation. Our ultimate goal is to jointly train this generic architecture on many tasks at once, to obtain a universal multilingual and multi-modal representation. This ultimate goal is illustrated in Figure 1.\nTo ease the comparison and search, we are focusing on representations of fixed-size, independently of the length of the input (and output) sequence. This choice has certainly an impact on the performance for very long sequences, ie. in the order of more than fifty words, but we argue\nthat such long sentences are probably not very frequent in every day communication. We would also like to emphasize that the goal of our work is not to improve NMT (for multiple languages), but to use the NMT framework to learn multilingual sentence embeddings. Once the system is trained, the decoders are not used any more. This means in particular that the usual attention mechanism can\u2019t be used since the attention weights are usually conditioned on the decoder outputs. A possible solution could be to condition the attention on the inputs only, for instance so-called self-attention (Liu et al., 2016) or inner-attention (Lin et al., 2017).\nTo fix ideas, let us consider that we have corpora in L different languages which can be pairwise or N -way parallel, N \u2264 L. This means that our architecture is composed of L encoders and L decoders respectively. However, this does not mean that we always provide input to all encoders, or targets for all decoders, but we will change the used models at each mini-batch. One could for instance perform one mini-batch with two input languages and one output language (which requires an 3-way parallel corpus), and use one (different) input and output language in the next mini-batch (which require a bitext). We will call this partial training paths. Note that we can also use monolingual data in this framework, ie. the input and output language is identical.\nThere are many possibilities to define partial training paths, with 1 < M,N \u2264 L.\n1:1 i.e. translating from one source into one target language respectively.\nM:1 i.e. presenting simultaneously several source languages at the input.\n1:N i.e. translating from one source language into multiple target languages.\nM:N this is a combination of the preceding two strategies and the most general approach. Remember that not all inputs and outputs need to be present at each training step.\nOur goal is to learn joint sentence representations, which are as close as possible when sentences are presented in different languages at the input. If we use 1:1 training, changing the language pair at each mini-batch (input and output), it is quite unlikely that the system would learn a common joint representation which is independent of the source language. A variant of 1:1 training is to always use the same decoder, but many different encoders. Since the decoder is shared for all the input languages, and the capacity of the model is limited, there\u2019s an incentive for the system to use the same representations for all the encoders. This training strategy only requires bitexts with one common language (usually English). An important drawback, however, is that we won\u2019t obtain an embedding of this common language since it is never used at the input.1\nUsing multiple languages at the input at the same time and combining the corresponding sentence embeddings, ie. the M:1, strategy, has in principle the potential to learn joint sentence embeddings, if an appropriate technique is used to combine the individual embeddings. The most straightforward approach is to average the embeddings. This was used for instance in (Firat et al., 2016b) in a multilingual NMT system with attention. The joint embedding could be also enforced by some type of regularizer. Again, having one dedicated output language makes it impossible to learn a representation for it.\nThe 1:N strategy is an interesting extension of 1:1. The idea is translate from one input language simultaneously to all L-1 other languages, excluding the one at the input (ie. no autoencoder). The source and the set of target languages is changed at each mini-batch. By these means, every input language has at least one target language in common with the other input languages, and each target language has at least one input language in common. One hand hand, this\n1One could also use the common output language at the input. This corresponds to training an auto-encoder which is easier than a translation model and may have an negative impact.\nstrategy makes it possible to learn sentence embeddings for all languages, but one the other hand, it requires L-way parallel training data. Although bitexts are usually used in MT, there are also several corpora which can be aligned for more than two languages (eg. Eurpoarl, TED, UN).\nFinally, the N:M strategy is the most generic one which combines all above techniques."}, {"heading": "2.1 Related work", "text": "The use of multiple encoders and decoders was first studied in the context of neural MT. Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoders, i.e. M:1 training. It\u2019s not surprising that this complementarity improves MT quality, in comparison to one input language only. Many different configurations were explored by (Luong et al., 2015) for sequence-to-sequence models. Firat et al. (2016a) were the first to use multiple encoders and decoders with a shared attention mechanism. This approach was further refined to enable zeroresource NMT (Firat et al., 2016b). Alternatively, it was proposed to handle multiple source and target languages with one encoder and decoder only, using a special token to indicate the target language (Johnson et al., 2016) to enable zero-shot NMT. To best of our knowledge, all these works focus on the improvement and extensions of sequence to sequence modeling, and fixed-sized vector representations have not analyzed in depth in multilingual context.\nSeveral publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016). The usual approach is to optimize a distance or correlation between the two representations or predictive auto-encoders (Chandar et al., 2013). The same approach was applied to transliteration and captioning (Saha et al., 2016).\nThere is a large body of research on sentence representations. Common approaches include: simple techniques like bag-of-words or some type of pooling, eg (Arora et al., 2017), recursive neural networks, eg. (Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg. (Cho et al., 2014), convolutional neural networks, eg. (Collobert and Weston, 2008; Zhang et al., 2015) or\nhierarchical approaches, eg. (Zhao et al., 2015). In all these works, the sentence representations are learned for one language only. It is important to note that our multiple encoder/decoder architecture and the different training paths make no assumption on the type of encoder and decoder used. In principle, all these sentence representations methods could be used. This is left for future research.\nWe are only aware of a couple of works which aim in learning multilingual sentence representations. There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. Pham et al. (2015) directly learn a vector representations for sentences in the absence of compositional property. Zhou et al. (2016) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation. It is worth to point out that in those work, document representations are used for down stream classification tasks where the granularity at sentence levels is off the interest.\nFinally, many papers address the problem of learning bi- or multilingual word representations. Those are not discussed here since our goal is to learn sentence embeddings."}, {"heading": "3 Experimental evaluation", "text": ""}, {"heading": "3.1 Data", "text": "We have performed all our experiments with the freely available UN corpus (Ziemski et al., 2016). It contains about 12M sentences in six languages (En, Fr, Es, Ru, Ar and Zh). We have used the version which is 6-way parallel. This amounts to about 8.3M sentences (173M English words)2. This corpus comes with a predefined dev and test set (4000 words each). We used byte-pair encoding (BPE) as proposed by Sennrich et al. (2016). The code book is of size 20k. In contrast to to other works, we do not lower case the texts.3"}, {"heading": "3.2 Evaluation", "text": "An important question is how to evaluate multilingual joint sentence embeddings. Let us first define\n2After discarding sequences longer than 50 tokens 3Juncys-Dowmunt et al. (2016) use 30k BPE\nsome desired properties of such embeddings:\n\u2022 multilingual closeness: the representations of the same sentence in different languages should be as similar as possible;\n\u2022 semantic closeness: similar sentences should be also close in the embeddings space, ie. sentences conveying the same meaning, but not necessarily the syntactic structure and word choice;\n\u2022 preservation of content: sentence representations are usually used in the context of a task, eg. classification into several categories, multilingual NMT or semantic relatedness. This requires that enough information is preserved in the representations to perform the task;\n\u2022 scalability to many languages: it is desirable that the metric can be extended to many languages without important computational cost or need for human labeling of data.\nWe are aware of two approaches which have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus which was first described in (Klementiev et al., 2012); and 2) cross-lingual evaluation of semantic textual similarity (in short STS). This task was first introduced in the 2016 edition of SemEval (Agirre et al., 2016). Both tasks focus on the evaluation of sentences representations of two languages only. In the Reuters task, a document classifier is trained on English sentence representations and then applied to German texts, and in the opposite direction respectively. STS seeks to measure the degree of semantic equivalence between two sentences (or small paragraphs). Semantic similarity is expressed by a score between 0 (the two sentences are completely dissimilar) and 5 (the two sentences are completely equivalent). In 2016, a cross lingual task was introduced (Es/En) and extended to two more language pairs in 2017 (Ar-En and Tr-En).\nSimilarity search In this work, we propose an additional evaluation framework for multilingual joint representations, based on similarity search. Our metric can be automatically calculated without the need of new human-labeled data and scaled to many languages\nAlgorithm 1 Multilingual similarity search 1: L: number of languages 2: S: number of sentences 3: Epq: error between languages p and q 4: R(spi ): embedding of a sentence 5: D(): some distance metric 6: for p = 1 . . . L do 7: for q = 1 . . . L, q 6= p do 8: Epq = 0 9: for i = 1 . . . S do 10: if min j=1...S D(R(spi ), R(s q j)) 6= i then 11: Epq++ 12: end if 13: end for 14: end for 15: end for\nand large corpora. We only need collections of S sentences, and their translations in L different languages, ie. spi , i = 1 . . . S, p = 1 . . . L. Such L-way parallel corpora are freely available, for instance Europarl4 (20 languages), UN corpus, 6 languages (Ziemski et al., 2016), or TED, 23 languages, (Cettolo et al., 2012).\nThe details of our approach are given in algorithm 1. The basic idea is to search the closest sentence in all S sentences, and count an error if is not the reference translation. This requires the calculation on S2 distance metrics and makes only sense when there are are no duplicate sentences in the corpus. With increasing S it may be also likely that the corpus contains several valid translations which could be closer than the reference one. This is difficult to handle automatically at large scale and counted as error by our algorithm.\nSimilarity search mainly evaluates the multilingual closeness property and can be easily scaled to many languages. We will report results how the similarity error rate is influenced by the number of language pairs and the size of the corpus. One expects that the larger the corpus, the higher is the risk that sentences can be confused.\nWe have compared three distance metrics: L2, inner product and cosine. In general, cosine performed best. Note that all metrics are equivalent if the vectors are normalized.\n4http://www.statmt.org/europarl/"}, {"heading": "3.3 Results", "text": "In this work we only consider stacked LSTMs as encoders and decoders. The sentence representation is obtained from the last LSTM state. For each encoder and decoder, a three layer stacked LSTMs with 512 dimensional hidden layers was used. The word embeddings are of size 384. We use vertical dropout with a value of 0.2 and gradients are clipped at 2. The initial learning rate is set to 0.02 and decreased each time performance on the development data doesn\u2019t improve. Performance is measured by perplexity for the decoders and similarity error at the embedding layer for the encoders. It is important to note that the similarity error rate can be only calculated once the whole development set is processed. Therefore it isn\u2019t used to provide gradients to the encoders. Training is performed with SGD for up to ten epochs with a batch size of 96. For the smallest models, one iteration through the training data takes about 13h.\nIn the following, we provide a detailed comparison of the 1:1 and 1:N training strategies. Figure 2 details the training paths in the two configurations. We have also performed experiments with 2:1 and 2:N, but were not able to achieve further improvements. Table 1 summarizes the best results on the UN development corpus for several systems using the one-to-one and one-to-many partial training paths. In each column, we give the average similarity error over all n(n + 1)/2 language pairs. As an example, the system trained with English, French, Spanish and Russian at the input and Arabic at the output (\u201cefsr-a\u201d in the third line), achieves an average error of 2.25% over the 6 language pairs5, column \u201cefs\u201d, and 2.73% over 10\n5En-Es, En-Fr, Es-En, Es-Fr, Fr-En and Fr-Es.\nlanguages pairs6, column \u201cefsr\u201d. It is not obvious to draw clear conclusions, but we can make nevertheless the following observations. First, increasing the number of languages for which we seek a joint sentence embedding doesn\u2019t seem to make the task harder. On the contrary, our best results are obtained with systems trained on all available languages (error rate of 1.85% with respect to 2.14% when testing on the three language pairs en-fr-es). Second, the one-toone training strategy perform slightly better than one-to-many. This strategy has also have the advantage that only bilingual aligned data with one common language is needed, typically English. However, an important drawback is that we don\u2019t obtain a sentence representation for this common output language.\nIn our case, all the corpora have the same sentences, ie. during training the systems sees each sentence in all the input languages. This may be useful to quickly learn the joint sentence representation. On the other hand, the training data is shuffled and it is therefore quite unlikely that the same sentences is seen in close updates. In future work, we will investigate whether arbitrary bilingual corpora can be used, for instance UN Es-En, Europarl It-En and some other Fr-En corpus.\nDetailed similarity search error rates for all language pairs of the system \u201cefsraz-all\u201d are given in Table 2. Overall, the similarity error rates vary only slightly from the average of 2.7% although\n6En-Es, En-Fr, En-Ru, Es-En, Es-Fr, Es-Ru, Fr-En, Fr-Es, Fr-Ru, Ru-En, Ru-Es and Ru-Fr.\nthe six covered languages differ significantly with respect to morphology, inflection, word order, etc. In particular, Chinese is handled as well as the other languages. This is in nice contrast to many other NLP application, in particular NMT, for which the performances on Chinese are significantly below those of other languages. None of the error rates is higher than 4%."}, {"heading": "3.4 Large scale out-of domain similarity search", "text": "In this section, we evaluate our sentence representation on out-of domain data, and study the effect of the corpora size on the similarity error rate. We are not aware of another huge corpus which is 6- way parallel for the same languages than the UN corpus. Therefore, we have selected the Europarl corpus and limited our study on three languages, namely English, French and Spanish. After excluding duplicates and limiting the sentence length to fifty tokens, we dispose of almost 1.5 million 3-way parallel sentences.\nFigure 3 shows the similarity error rate in function of the corpus size, using the system \u201cefsra-z\u201d from Table1. Subsets of the various sizes were randomly sampled. The average error rate is 3.1% for a subset of 4000 words, ie. of the same size than the UN Dev set. We then observe a clear logarithmic increase of the similarity error rate in function of the corpus size, reaching 12.2% when searching the closest embedding in 1.5M sentences. This requires the calculation of 1.5M2 distances for each language pair. This can be very efficiently performed with the FAISS open-source toolkit (Johnson et al., 2017) which offers many\noptions to increase the speed of nearest neighbor search. Its implementation of brute-force L2 search was sufficient for our purposes."}, {"heading": "4 Conclusion", "text": "We have shown that the framework of neural MT with multiple encoders/decoders can be used to learn joint fixed-size sentence representations which exhibit interesting linguistic characteristics. We have explored several training paradigms which correspond to partial paths in the whole architecture. We have evaluated the joint sentence embeddings by cross-lingual similarity search between six languages7 which differ significantly with respect to morphology, inflection, word order, etc. We were able to obtain an average similarity error rate of 2.7% for all 21 languages pairs. We have also studied the evolution of the similarity error rate when scaling up to 1.5 million sentences, drawn from an out-of-domain corpus.\nThere are many directions of future research of this work. We have used stacked LSTMs for the encoders and decoders. There is a large body of research on fixed-size sentence embeddings, and we will investigate which approaches are best to learn joint multilingual sentence embeddings. When using many-to-one training, the individual sentence embeddings have to be merged. We will explore other possibilities, in addition to simple averaging. Finally, we will evaluate our multilingual sentence representations on other public tasks, namely the Reuters corpus for cross-lingual document classification, and cross-lingual evaluation of semantic textual similarity (SemEval STS task).\n7English, French, Spanish, Russian, Arabic and Chinese."}], "references": [{"title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "SemEval workshop.", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "ICLR.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "EAMT . pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Multilingual deep learning", "author": ["Sarath Chandar", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha."], "venue": "NIPS DL wshop.", "citeRegEx": "Chandar et al\\.,? 2013", "shortCiteRegEx": "Chandar et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "ICML. pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Huan Wu", "Wei He", "Dianhai Yu", "Haifeng wang."], "venue": "ACL. pages 1723\u20131732.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-way, multilingual neural machine translation with shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Choa", "Yoshua Bengio."], "venue": "NAACL.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."], "venue": "EMNLP.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "DeViSa:E a deep visual-semantic embedding model", "author": ["Andrea Frome", "Grep S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "marc\u2019Aurelio Ranzato", "Thomas Mikolov"], "venue": null, "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom."], "venue": "ACL. pages 58\u201368.", "citeRegEx": "Hermann and Blunsom.,? 2014", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Billion-scale similarity search with gpus", "author": ["Jeff Johnson", "Matthijs Douze", "Herv\u00e9 J\u00e9gou."], "venue": "arXiv preprint arXiv:1702.08734 .", "citeRegEx": "Johnson et al\\.,? 2017", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson"], "venue": "https://arxiv.org/ abs/1611.04558.", "citeRegEx": "Johnson,? 2016", "shortCiteRegEx": "Johnson", "year": 2016}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Juncys-Dowmunt", "Timasz Dwojak", "Hieu Hoang."], "venue": "https://arxiv.org/abs/ 1610.011108.", "citeRegEx": "Juncys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Juncys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai."], "venue": "Coling.", "citeRegEx": "Klementiev et al\\.,? 2012", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "A structured self-attentive sentence embedding", "author": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Lin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2017}, {"title": "Learning natural language inference using bidirectional lstm model and inner-attention", "author": ["Yang Liu", "Chenjie Sun", "Lei Lin", "Xiaolong Wang."], "venue": "https://arxiv.org/abs/1605.09090.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous word space representations", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig."], "venue": "NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot", "author": ["Hideki Nakayama", "Noriki Nishida."], "venue": "https://arxiv.org/abs/1611.04503.", "citeRegEx": "Nakayama and Nishida.,? 2016", "shortCiteRegEx": "Nakayama and Nishida.", "year": 2016}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng."], "venue": "ICML.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Learning distributed representations for multilingual text sequences", "author": ["Hieu Pham", "Minh-Thang Luong", "Christopher D. Manning."], "venue": "1st Workshop on Vector Space Modeling for NLP.", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "A correlational encoder decoder architecture for pivot based sequence generation", "author": ["Amrita Saha", "Mitesh M. Kharpa", "Sarath Chandar", "Janarthanan Rajendran", "Kyunghyun Cho."], "venue": "https://arxiv. org/abs/1606.04754.", "citeRegEx": "Saha et al\\.,? 2016", "shortCiteRegEx": "Saha et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "NIPS. pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."], "venue": "https://arxiv.org/abs/1504.05070.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Cross-lingual sentiment classification with bilingual document representation learning", "author": ["Xinjie Zhou", "Xiaojun Wan", "Jianguo Xiao."], "venue": "ACL.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M Ziemski", "Marcin Juncys-Dowmunt", "B. Pouliquen"], "venue": null, "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "NAACL. pages 30\u201334.", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "It has been shown that syntactic and semantic relations can be captured in this (high-dimensional) embedding space, see for instance (Mikolov et al., 2013).", "startOffset": 133, "endOffset": 155}, {"referenceID": 1, "context": "(Arora et al., 2017), recursive neural networks, eg.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "(Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Cho et al., 2014), convolutional neural networks, eg.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg.", "startOffset": 0, "endOffset": 48}, {"referenceID": 28, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or hierarchical approaches, eg.", "startOffset": 0, "endOffset": 48}, {"referenceID": 29, "context": "(Zhao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 5, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 27, "context": "A very successful application of this paradigm is neural machine translation (NMT), see for instance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 101, "endOffset": 175}, {"referenceID": 2, "context": "A plausible solution is the so-called attention mechanism (Bahdanau et al., 2015).", "startOffset": 58, "endOffset": 81}, {"referenceID": 7, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 32, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 19, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 8, "context": "NMT has been also extended to handle several source and/or target languages at once, with the goal of achieving better translation quality than with separately trained NMT systems, in particular for under resourced languages, see for instance (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2015; Firat et al., 2016a).", "startOffset": 243, "endOffset": 326}, {"referenceID": 18, "context": "A possible solution could be to condition the attention on the inputs only, for instance so-called self-attention (Liu et al., 2016) or inner-attention (Lin et al.", "startOffset": 114, "endOffset": 132}, {"referenceID": 17, "context": ", 2016) or inner-attention (Lin et al., 2017).", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "This was used for instance in (Firat et al., 2016b) in a multilingual NMT system with attention.", "startOffset": 30, "endOffset": 51}, {"referenceID": 19, "context": "Many different configurations were explored by (Luong et al., 2015) for sequence-to-sequence models.", "startOffset": 47, "endOffset": 67}, {"referenceID": 9, "context": "This approach was further refined to enable zeroresource NMT (Firat et al., 2016b).", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al.", "startOffset": 0, "endOffset": 122}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoders, i.", "startOffset": 0, "endOffset": 147}, {"referenceID": 7, "context": "Dong et al. (2015) used multiple decoders, i.e. 1:N training, to achieve improved NMT performance. Zoph and Knight (2016) and Firat et al. (2016b), on the other hand, used multiple encoders, i.e. M:1 training. It\u2019s not surprising that this complementarity improves MT quality, in comparison to one input language only. Many different configurations were explored by (Luong et al., 2015) for sequence-to-sequence models. Firat et al. (2016a) were the first to use multiple encoders and decoders with a shared attention mechanism.", "startOffset": 0, "endOffset": 441}, {"referenceID": 10, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 22, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 21, "context": "Several publications consider joint representations in a multimodal context, usually text and images, for instance (Frome et al., 2013; Ngiam et al., 2011; Nakayama and Nishida, 2016).", "startOffset": 115, "endOffset": 183}, {"referenceID": 4, "context": "The usual approach is to optimize a distance or correlation between the two representations or predictive auto-encoders (Chandar et al., 2013).", "startOffset": 120, "endOffset": 142}, {"referenceID": 24, "context": "The same approach was applied to transliteration and captioning (Saha et al., 2016).", "startOffset": 64, "endOffset": 83}, {"referenceID": 1, "context": "Common approaches include: simple techniques like bag-of-words or some type of pooling, eg (Arora et al., 2017), recursive neural networks, eg.", "startOffset": 91, "endOffset": 111}, {"referenceID": 26, "context": "(Socher et al., 2011), recurrent neural networks, in particular LSTMs, eg.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Cho et al., 2014), convolutional neural networks, eg.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or", "startOffset": 0, "endOffset": 48}, {"referenceID": 28, "context": "(Collobert and Weston, 2008; Zhang et al., 2015) or", "startOffset": 0, "endOffset": 48}, {"referenceID": 29, "context": "(Zhao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 30, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 23, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015).", "startOffset": 83, "endOffset": 148}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations.", "startOffset": 84, "endOffset": 177}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. Pham et al. (2015) directly learn a vector representations for sentences in the absence of compositional property.", "startOffset": 84, "endOffset": 333}, {"referenceID": 11, "context": "There are several work on learning multilingual representations at document levels (Hermann and Blunsom, 2014; Zhou et al., 2016; Pham et al., 2015). Hermann and Blunsom (2014) proposed a compositional vector model to learn document level representations. Their model is based on bag of words/bi-gram composition. Pham et al. (2015) directly learn a vector representations for sentences in the absence of compositional property. Zhou et al. (2016) learn bilingual document representation by minimizing Euclidean distance between document representations and their translation.", "startOffset": 84, "endOffset": 448}, {"referenceID": 31, "context": "We have performed all our experiments with the freely available UN corpus (Ziemski et al., 2016).", "startOffset": 74, "endOffset": 96}, {"referenceID": 25, "context": "We used byte-pair encoding (BPE) as proposed by Sennrich et al. (2016). The code book is of size 20k.", "startOffset": 48, "endOffset": 71}, {"referenceID": 14, "context": "After discarding sequences longer than 50 tokens Juncys-Dowmunt et al. (2016) use 30k BPE some desired properties of such embeddings:", "startOffset": 49, "endOffset": 78}, {"referenceID": 16, "context": "We are aware of two approaches which have been used in the literature to evaluate multilingual sentence embeddings: 1) cross-lingual document classification based on the Reuters corpus which was first described in (Klementiev et al., 2012); and 2) cross-lingual evaluation of semantic textual similarity (in short STS).", "startOffset": 214, "endOffset": 239}, {"referenceID": 0, "context": "This task was first introduced in the 2016 edition of SemEval (Agirre et al., 2016).", "startOffset": 62, "endOffset": 83}, {"referenceID": 31, "context": "Such L-way parallel corpora are freely available, for instance Europarl4 (20 languages), UN corpus, 6 languages (Ziemski et al., 2016), or TED, 23 languages, (Cettolo et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": ", 2016), or TED, 23 languages, (Cettolo et al., 2012).", "startOffset": 31, "endOffset": 53}, {"referenceID": 12, "context": "This can be very efficiently performed with the FAISS open-source toolkit (Johnson et al., 2017) which offers many", "startOffset": 74, "endOffset": 96}], "year": 2017, "abstractText": "In this paper, we use the framework of neural machine translation to learn joint sentence representations across different languages. Our hope is that a representation which is independent of the language a sentence is written in, is likely to capture the underlying semantics. We search and compare more than 1.4M sentence representations in three different languages and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.", "creator": "LaTeX with hyperref package"}}}