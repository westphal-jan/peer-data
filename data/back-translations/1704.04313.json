{"id": "1704.04313", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data", "abstract": "The extraction of single image functions using Convolutionary Neural Networks for real-time processing of video data currently takes place mainly on high-performance GPU-accelerated workstations and computing clusters. However, there are many applications such as smart surveillance cameras that require or would benefit from on-site processing. To this end, we propose a novel algorithm for modification-based evaluation of CNNs for video data recorded with a static camera setting, taking advantage of the spatio-temporal scarcity of pixel changes. We achieve an average speed of 8.6x across a cuDNN baseline to a realistic benchmark with a negligible loss of accuracy of less than 0.1% and without network retraining. The resulting energy efficiency is 10x higher than single image evaluation and achieves the equivalent of 328 GOp / s / W on the Tegra X1 platform.", "histories": [["v1", "Fri, 14 Apr 2017 00:36:55 GMT  (2331kb,D)", "http://arxiv.org/abs/1704.04313v1", null], ["v2", "Wed, 21 Jun 2017 09:27:14 GMT  (2333kb,D)", "http://arxiv.org/abs/1704.04313v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.PF", "authors": ["lukas cavigelli", "philippe degen", "luca benini"], "accepted": false, "id": "1704.04313"}, "pdf": {"name": "1704.04313.pdf", "metadata": {"source": "META", "title": "CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data", "authors": ["Lukas Cavigelli", "Philippe Degen", "Luca Benini"], "emails": ["cavigelli@iis.ee.ethz.ch", "degenp@ee.ethz.ch", "benini@iis.ee.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "Computer vision (CV) technology has become a key ingredient for automatized data analysis over a broad range of real-world applications: smart cameras for video surveillance, robotics, industrial quality assurance, medical diagnostics, and advanced driver assistance systems have recently become popular due the rising reliability of CV algorithms [13, 14, 35, 44]. This industry interest has fostered the procedure of a wealth of research projects yielding a fierce competition on many benchmarks datasets such as the ImageNet/ILSVRC [11, 39], MS COCO [31], and Cityscapes [9] benchmarks, on which scientists from academia and big industry players evaluate their latest algorithms.\nIn recent years, the most competitive approaches to address many CV challenges have relied on machine learning with complex, multi-layered, trained feature extractors commonly referred to as deep learning [18, 26, 41]. The most frequently used flavor of deep learning techniques for CV are convolutional neural networks (ConvNets, CNNs). Since their landslide success at the 2012 ILSVRC competition over hand-crafted features, their accuracy has further improved year-over-year even exceeding human performance on this complex dataset [19, 39]. CNNs keep on expanding to more areas of computer vision and data analytics in general [1, 14, 19, 32, 33, 43].\nUnfortunately, the high accuracy of CNNs comes with a high computational cost, requiring powerful GPU servers to train these networks for several weeks using hundreds of gigabytes of labeled data. While this effort is very costly, it is a one-time endeavour and can be done offline for many applications. However, the inference of state-of-the-art CNNs also requires several billions of multiplications and additions to classify even low resolution images by today\u2019s standards [5]. While in some cases offloading to centralized compute centers with powerful GPU servers is also possible for inference after deployment, it is extremely costly in terms of\ncompute infrastructure and energy. Furthermore, collecting large amounts of data at a central site raises privacy concerns and the required high-bandwidth communication channel causes additional reliability problems and potentially prohibitive cost of deployment and during operation [27].\nThe alternative, on-site near sensor embedded processing, largely solves the aforementioned issues by transmitting only the less sensitive, condensed information\u2014potentially only security alerts in case of a smart surveillance camera\u2014but imposes restrictions available computation resources and power. These push the evaluation of such networks for real-time semantic segmentation or object detection out of reach of even the most powerful embedded platforms available today for high-resolution video data [5]. However, exactly such systems are required for a wide range of applications limited in cost (CCTV/urban surveillance, perimeter surveillance, consumer behavior and highway monitoring) and latency (aerospace and UAV monitoring and defence, visual authentication) [27, 35].\nLarge efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43]. However, they either do not provide a strong enough performance boost, are already at the theoretical limit of what can be achieved on a given platform, are inflexible and not commercially available, or incur a considerable accuracy loss. It is thus essential to extend the available options to efficiently perform inference on CNNs.\nIn this paper, we propose a novel method to perform inference for convolutional neural networks on video data from a static camera with limited frame-to-frame changes. Evaluations on a Nvidia Tegra X11 show that an average speed-up of 8.6\u00d7 is possible with negligible accuracy loss over cuDNN-based per-frame evaluation on an urban video surveillance dataset. This pushes real-time CNN inference on high-resolution frames within the computation and power budget of current embedded platforms.\nOrganization of the Paper. In the next section we will discuss related work, before proposing our change-based convolution algorithm in Section 3. We present experimental results and discuss them in in Section 4. We then conclude the paper in Section 5."}, {"heading": "2 RELATEDWORK", "text": "In this section, wewill first discuss available datasets and CNNswith which we can evaluate our proposed algorithm. Then we describe existing optimized implementations for CNN inference and existing approximations trading accuracy for throughput. Finally, we survey related approaches exploiting the limited changes in video data to reduce the computational effort required to perform CNN inference. 1The Nvidia Tegra X1 is a system-on-chip available on an embedded board with an affordable power budget (<15W) for a stationary camera.\nar X\niv :1\n70 4.\n04 31\n3v 1\n[ cs\n.C V\n] 1\n4 A\npr 2\n01 7"}, {"heading": "2.1 Suitable Datasets and Neural Networks", "text": "For our evaluations we are interested in performing object detection or semantic segmentation, which are both often applied to high-resolution images and video streams with frame rates above 10 frame/s for meaningful applications. With still image object classification being considered solved by having achieved beyond human accuracy [18, 39], there is now a rapidly increasing interest in extracting information from video data, e.g. video tagging and action recognition on datasets that have recently become available (Sports-1M [25], Youtube-8M [1]).\nWe are specifically interested in video sequences obtained from a static camera. While some such dataset exist, most of them are specifically targeted at person tracking and/or re-identification and do not provide labeled data for multi-class object detection or segmentation. However, the dataset used in [4] provides ground truth labels for 10-class semantic segmentation from an urban street surveillance perspective, and while they work with individual images, several surrounding unlabeled frames and a trained convolutional network are available. An example image labeled with the provided CNN is shown in Figure 2, and a sample sequence of 3 images is visualized in Figure 3."}, {"heading": "2.2 Optimized Embedded System Implementations", "text": "The latest wave of interest in neural networks can be attributed to their sudden success driven by the availability of large datasets and the increasingly powerful computing platforms. One of the most economical and practicable solutions for training medium-sized CNNs is to use a workstation with GPUs. The available software frameworks to implement and train CNNs provide strong support for this kind of platform.\nThe massive amounts of compute time spent training CNNs has spurred the development of highly optimized GPU implementations. First, most widely used frameworks relied on their own custom implementations which have all converged to methods relying on matrix-multiplications [8, 23], leveraging the availability of highly optimized code in BLAS libraries and the fact that GPUs are capable of achieving a throughput within a few percent of their peak performance with this type of workload. Specialized libraries such as Nvidia\u2019s cuDNN and Nervana Systems\u2019 Neon provide some additional performance gains through assembly-level implementations [28] and additional algorithmic improvements such as Winograd and FFT-based convolution [29]. A specific implementation for nonbatched inference on an embedded platform building on a matrix multiplication is documented in [5], also showing that more than 90% of time is spent computing convolutions."}, {"heading": "2.3 Approximations Trading Accuracy for Throughput", "text": "Admitting limited accuracy losses in order to gain a higher throughput by approximating existing networks, inference algorithms, and arithmetic operations can help overcome the computational obstacles preventing widespread adoption of CNN-based algorithms on embedded and mobile platforms.\nOne such option is the reduction of the required arithmetic precision to evaluation NNs. Various methods from normal fixed-point\nanalysis to retraining networks to adapt for quantized weights and activations exist. While most fixed-point methods are of limited use on many off-the-shelf software programmable platforms, some can benefit from vectorization of lower-precision operations [15]. Extreme methods go as far as to enforce binary weights [2, 10], and in some cases also binary activations [36]. This means that multiplications can be dropped entirely, and in case of binary activations even collapse some of the add/subtract operations into XNOR and bit count operations. Many networks can be quantized with 8 bit without an increase in error rate, before there is a trade-off between precision and accuracy [3, 17]. Some methods try reducing the computational effort by pruning many very small weights to zero, making it possible to skip some operations [30]. More sophisticated quantization schemes such as vector quantization exist and can further compress a trained CNN model, but they require specialized hardware to bring an improvement in energy efficiency [16]. Once focusing on application-specific accelerators, also approximate arithmetic such as inaccuratemultipliers have been considered [45].\nFurther research has focused on optimizing semantic segmentation and object detection algorithms to better reuse already computed features by eliminating any non-convolutional elements from the network [32, 37, 38]. Simplifying the operations in a network, such as low-rank approximations of 2D convolutions or by simply designing smaller networks with state-of-the-art methods have been evaluated in [21, 22, 34].\nThe method we propose in this paper does not supersede these methods, but can be combined with the aforementioned approximation methods to further improve throughput."}, {"heading": "2.4 Video-based Computation Reduction", "text": "Obtaining per-frame features naturally seems like an easier task when these frames belong to a video sequence rather than a random collection of images. Limited movement of objects in a frame can be exploited in object tracking by working with a limited search window within the frame [20], not only reducing the problem size, but also simplifying the regression task\u2014up until the tracked target is occluded by a large object.\nFor object detection and semantic segmentation, the available work in this direction is limited to clockwork CNNs [40]. The authors of [32] have extended their work on fully convolutional networks for semantic segmentation, which presents a CNN with skip connections and deconvolution layers to refine the lower-resolution feature maps obtained deep within the network using the features extracted early in the network. They exploit the fact that lowerresolution feature maps within the network are more stable over time than the full-resolution input. They thus propose to reevaluate the first few layers and the last few affected through the skip connections more frequently than the more coarse grained feature maps. This is a strong limitation on the set of CNNs this method can be applied to. They present evaluations based on a static as well as a dynamic, content-adaptive reevaluation schedule, showing that they can reduce the number of full-frame convolutions by about 40% before the accuracy starts to drop on the Youtube-Objects dataset.\nHowever, this approach is limited to updating entire frames, whereas we exploit that often only small parts of the scene change and need to be reevaluated. We are not aware of any existing methods exploiting limited changes between frames, which we show to allow for much larger gains in throughput."}, {"heading": "3 METHODOLOGY", "text": "Differently from to previous work looking at reevaluating entire frames, we exploit the limited number of pixels changing frame-toframe to increase the throughput without loss in classification accuracy. The most straight-forward pixel-level approach is to detect\nchanging pixels at the input based on a threshold on the difference to the previous frame and then update all the pixels affected by them, increasing the number of pixels to be updated layer-after-layer due to the convolution operations. Thus for e.g. a 7 \u00d7 7 convolution a one-pixel change triggers an update of 49 pixels in the next layer and 169 pixels after another 7 \u00d7 7 convolution. Strided operations (often used with pooling layers) reduce this effect, but do not prevent it. This issue might seem prohibitive for multi-layer CNNs, particularly when considering that individual pixels might keep exceeding the threshold due to noise.\nHowever, the change is not only spatially local at the input, but also at the output. Furthermore, noise-like changes will likely not have strong impacts on feature maps deeper within the network. We thus propose to perform the change-detection not only at the input, but before each convolution layer\u2014relative to its previous input\u2014and to compute an updated value only for the affected output pixels. This can be done without modifications to the training of the CNN, can be applied to existing pre-trained networks, and is not specific to the CNN on which we evaluate the proposed algorithm.\nWe propose to replace all spatial convolution layers (conv layers) with change-based spatial convolution layers (CBconv layers). This means adapting the widely used, simple and well-performing matrix-generation and matrix-multiplication sequence of operations [5, 23]. The convolution layer computes\nyo (j, i) = bo + \u2211 c\u2208Cin \u2211 (\u2206j,\u2206i )\u2208Sk ko,c (\u2206j, \u2206i)xc (j \u2212 \u2206j, i \u2212 \u2206i), (1)\nwhere o indexes the output channels Cout and c indexes the input channels Cin . The pixel is identified by the tuple (j, i) and Sk denotes the support of the filters kernels k . This can be computed by performing a matrix multiplication\nY = KX, Y \u2208 R |CO |\u00d7ho \u00b7wo , (2)\nK \u2208 R |CO |\u00d7 |CI | \u00b7hk \u00b7wk , X \u2208 R |CI | \u00b7hk \u00b7wk\u00d7ho \u00b7wo . (3)\nThe image matrix X is constructed as X ((khk + j)wk + i,yowo + xo ) = x(k, j + yo , i + xo ) with k = 1, . . . , |Cin |, j = 1, . . . ,hk , i = 1, . . . ,wk and yo = 1, . . . ,ho , xo = 1, . . . ,wo . The filter matrix K is given byK(o, (chk + j)wk +i) = k(o, c, j, i) for o = 1, . . . , |Cout |, c = 1, . . . , |Cin |, j = 1, . . . ,hk and i = 1, . . . ,wk . The result matrix is stored as Y (o,yowo + xo ) = y(o,yo ,xo ). Zero-padding can be applied during the construction of the X matrix and an efficient strided convolution can be computed by dropping the unused rows.\nWe replace this matrix multiplication by the following sequence of processing steps, thereby drastically reducing the size of the matrix performing the main computation step."}, {"heading": "3.1 Processing Steps", "text": "We modify the standard approach and use a sequence of processing steps (cf. Figure 4): change detection, change indexes extraction, matrix generation, matrix multiplication, and output update. In the following, we will explain the individual steps.\nChange Detection. In this step, changed pixels are detected. We define a changed pixel as one where the absolute difference of the current to the previous input of any feature map/channel exceeds some threshold \u03c4 , i.e.\nm(j, i) = \u2228 c \u2208CI |x (t )(c, j, i) \u2212 x (t\u22121)(c, j, i)| > \u03c4 .\nThe computation effort of this step is crucial, since it is executed independently of whether any pixel changed. Each of these changes affects a region equal to the filter size, and these output pixels are marked for updating:\nm\u0303(j, i) = \u2228\n(\u2206j,\u2206i)\u2208Sk m(j + \u2206j, i + \u2206i),\nwhereSk is the filter kernel support, e.g.Sk = {\u22123, . . . , 3}2 for a 7\u00d7 7 filter. All of this is implemented on GPU by clearing the the change map to all-zero and having one thread per pixel, which\u2014if a change is detected\u2014sets the pixels of the filter support neighborhood in the resulting change map.\nChange Indexes Extraction. In this step, we condense the change map m\u0303 to 1) a list of pixel indexes where changes occurred and 2) count the number of changed pixels. This cannot easily be performed in parallel, so for our implementation we split the change map into blocks of pixels, compute the result for all the blocks in parallel, and reassemble the result. The computed index list is later on needed to access the right pixels to assemble the matrix for the convolution.\nMatrix Generation & Matrix Multiplication. Matrix multiplications are used in many applications, and highly optimized implementations such as the GEMM (general matrix multiplication) function provided by the Nvidia cuBLAS library come within a few percent of the peak FLOPS of which a GPU is capable to provide. Matrix multiplication-based implementations of the convolution layer relying on this are widely available and are highly efficient [5, 24] and is described earlier in this section. The X matrix in (2) is not generated full-sized, but instead only those columns corresponding to the relevant output pixels are assembled, resulting in a reduced width equal to the number of output pixels affected by the changes in the input image. The K matrix is made up of the filters trained using normal convolution layers and keeps the same dimensions, so the computation effort in this step is proportional to the number of changed pixels and the matrix multiplication is in the worst case only as time consuming as the full-frame convolution.\nOutput Updating. We use the previously stored results and the newly computed output values along with the change indexes list to provide the updated output feature maps. To maximize throughput, we also include the ReLU activation of the affected pixels in this step."}, {"heading": "3.2 Memory Requirements", "text": "The memory requirements of DNN frameworks are known to be very high, up to the point where it becomes a limiting factor for increasing the mini-batch size during learning and thus reducing the throughput when parallelizing across multiple GPUs. These requirements are very different when looking at embedded inference-only systems:\n(1) Inference is typically done on single frames and creating mini-batches would introduce often unacceptable latency and the benefit of doing so is limited to a few percent of additional performance [5]. (2) To maximize modularity and because it is required during training, each layer typically has memory allocated to store its output with the exception of ReLU activation layers which are often applied in-place. (3) To keep a high modularity, the memory to keep the matrix X is often not shared among layers, although its values are never reused after finishing the convolution computation. (4) Batch normalization layers (if present) are considered independent layers with their own output buffer, but they can be absorbed into the convolution layer for inference.\nTo obtain a baseline memory requirement, we compute the required memory of common DNN frameworks performing convolutions using matrix multiplication with a batch size of 1. We assume an optimized network minimizing the number of layers, e.g. by absorbing batch normalization layers into the convolution layers or using in-place activation layers. This way 30M values need to be stored for the intermediate results, 264M values for the X matrix, and 873k values for the parameters. This can further be optimized by sharing X among all convolution layers and by keeping only memory allocated to storing only the output of two layers and switching back-and-forth between them, layer-by-layer. This reduces the memory footprint to 9M, 93M, and 872k values, and a total of 103M values for our baseline.\nApplying our algorithm requires a little more memory, because we need to store additional intermediate results (cf. Figure 4) such as the change matrix, the changed indexes list, and the Y matrix, which can all again be shared between the layers. We also need to store the previous output to use it as a basis for the updated output and to use it as the previous input of the subsequent layer. For our sample network, this required another \u223c 60M values to a total of 163M values (+58%, total size \u223c 650MB)\u2014an acceptable increase and not a limitation, considering that modern graphics cards typically come with 8GB memory and even GPU-accelerated embedded platforms such as the Nvidia Jetson TX1 module provide 4GB of memory."}, {"heading": "3.3 Threshold Selection", "text": "The proposed algorithm adds one parameter to each convolution layer, the detection threshold, fixed offline after the training based on sample video sequences. A threshold of zero should yield identical results to the non-change-based implementation, which has been used for functional verification. For our evaluations we used the following procedure to select the thresholds: We start by setting all thresholds to zero. Then we iteratively step through them\nfrom the first to the last layer, sweeping the threshold parameter for each layer and keeping the maximum value before a clear performance degradation became noticeable when evaluating the entire validation set. The following evaluations will show that these thresholds need not be re-calibrated per video sequence and neither the accuracy nor the speed-up are overly sensitive to them."}, {"heading": "4 RESULTS & DISCUSSION", "text": "In this section, we will first present the evaluation environment and analysis the baseline compute time breakdown. We then show how the threshold parameters have been selected before discussing throughput measurements and the accuracy-throughput trade-off. Finally, we discuss the compute time breakdown and how changes propagate through the network to confirm the quality of our GPU implementation and justify design choices made during the construction of the algorithm."}, {"heading": "4.1 Evaluation Environment", "text": "While the algorithm is not limited to scene labeling/semantic segmentation, we perform our evaluations on the urban surveillance dataset described in [4] and using the corresponding scene labeling CNN, not using the multispectral imaging data. The dataset provides 51 training images and 6 validation images with 776 \u00d7 1040 pixel with the corresponding ground-truth scene labeling, classifying each pixel into one of the following 8 classes: building, road, tree, sky, tram, car/truck, water, distant background. For the validation set, the labeled images are part of short video sequences with 5 additional frames available before the frame for which the ground truth labeling is available. A trained network on this data is described in [4] and its parameters are reused unaltered for our evaluations. The procedure with which we perform our evaluation is visualized in Figure 5.\nWe have implemented the proposed algorithm using CUDA and wrapped them as modules for the Torch framework [8]. We have evaluated the performance on a Jetson TX1 board with JetPack 2.3.1 (Nov. 2016). Our performance baseline for the entire CNN and for the change-based implementation the pixel-wise classification is relying on Nvidia\u2019s cuDNN v5.1.5, which includes optimizations such\nas the Winograd algorithm and FFT-based convolutions mentioned in Section 2.2."}, {"heading": "4.2 Baseline Throughput and Computation Breakdown", "text": "Before we discuss the performance of the proposed algorithm, we analyze the baseline throughput and compute time breakdown in Table 1. Clearly, most time is spent performing convolutions, and the layers 1\u20133 performing 7 \u00d7 7 convolutions and belonging to the feature extraction part of the network are dominant with 91.9% (492ms) of the overall computation time (535ms or 1.87 frame/s). We thus specifically focus our analyses on these 3 layers, replacing only them with our CBconv layer."}, {"heading": "4.3 Threshold Selection", "text": "Our algorithm introduces a threshold parameter for each layer, for which we outline the selection process in Section 3.3. While we might want to leave them variable to investigate a throughput against accuracy trade-off, we also want to ensure that not a single layer\u2019s threshold is limiting the overall accuracy by aligning the tipping point where the accuracy starts to drop. We choose the thresholds conservatively, accepting very little accuracy drop, since any classification error will be focused around the moving objects which are our area of interest. We sweep the parameters of each layer to determine the increase in error (cf. Figure 6). We do so first for Layer 1 with \u03c42 = \u03c43 = 0 and select \u03c41 = 0.04, before repeating it for layers 2 and 3 after each other and using the already chosen thresholds for the previous layers, selecting \u03c42 = 0.3 and \u03c43 = 1.0.\nWith this selection of thresholds we can scale them jointly to analyze the trade-off against the classification accuracy more concisely (cf. Figure 7, left). The accuracy of the individual test sequences are visualized, and clearly show a similar behavior with a plateau up to a clear point where there is a steep increase in error rate."}, {"heading": "4.4 Throughput Evaluations", "text": "The motivation for the entire proposed algorithm was to increase throughput by focusing only on the frame-to-frame changes. We\nshow the performance gain in Figure 7 (right) with the indicated baseline analyzing the entire frame with the same network using cuDNN. In the extreme case of setting all thresholds to zero, the entire frame is updated, which results in a clear performance loss because of the change detection overhead as well as fewer optimization options such as less cache-friendly access patterns when generating the X matrix.\nWhen increasing the threshold factor, the throughput increases rapidly to about 16 frame/s, where it starts saturating because the change detection step as well as other non-varying components like the pooling and pixel classification layers are becoming dominant and the number of detected changed pixels does not further decrease. We almost reach this plateau already for a threshold factor of 1, where we have by construction almost no accuracy loss. The average frame rate over the different sequences is near 17 frame/s at this point\u2014an improvement of 8.6\u00d7 over the cuDNN baseline of 1.96 frame/s.\nOne sequence ( ) has\u2014while still being close to 5.1\u00d7 faster than the baseline\u2014a significantly lower throughput than the other sequences. While most of them show typical scenarios such as shown in Figure 3, this sequences shows a very busy situationwhere the entire road is full of vehicle and all of them are moving. The aggregate number of changed pixels across all 3 layers is visualized in Figure 7 (center). Most sequences trigger less than 3% of the maximum possible number of changes while the aforementioned exceptional case has a significantly higher share of around 9%.\nWe have repeated the same evaluations on a workstation with a Nvidia GTX Titan X GPU, obtaining an almost identical throughputthreshold trade-off and compute time breakdown up to a scaling\nfactor of 11.9\u00d7\u2014as can be expected for a largely very well parallelizable workload and a 12\u00d7 more powerful device with a similar architecture (TX1: 512 GFLOPS and 25.6 GB/s DRAM bandwidth, GTX Titan X: 6144 GFLOPS and 336 GB/s)."}, {"heading": "4.5 Accuracy-Throughput Trade-Off", "text": "While for some scenarios any drop in accuracy is unacceptable, many applications allow for some trade-off between accuracy and throughput\u2014after all choosing a specific CNN already implies selecting a network with an associated accuracy and computational cost.\nWe analyze the trade-off directly in Figure 8. The most extreme case is updating the entire frame every time resulting in the lowest\nthroughput at the same accuracy as full-frame inference. Increasing the threshold factor in steps of 0.25 immediately results in a significant throughput gain and for most sequences the trade-off only starts at frame rates close to saturation above 16 frame/s. The same frame sequence already deviate from the norm before behaves differently here as well. However, an adaptive selection of the threshold factor such as a simple control loop getting feedback about the number of changed pixels could allow for a guaranteed throughput by reducing the accuracy in such cases and is left to be explored in future work."}, {"heading": "4.6 Compute Time Breakdown", "text": "In Section 4.2 and specifically in Table 1, we already discussed the compute time breakdown of the entire network when using frameby-frame analysis. To gain more in-depth understanding of the limiting factors of our proposed algorithm, we show a detailed compute time breakdown of only the change-based convolution layers in Figure 9. The time spent on change detection is similar across all 3 conv layers, which aligns well with our expectations since the feature map volume at the input of nch \u00b7h \u00b7w values is identical for L2 and L3, and 25% smaller for L1. That this step already makes up for more than 23.4% of the overall time underlines the importance of a very simple change detection function: any increase in compute time for change detection has to be offset by time savings in the other steps by reducing the number of changes significantly. The change indexes extraction effort is linear to the number of pixels h \u00b7 w and the clear drop from L1 to L2 is as expected. However, since it is not well parallelizable, there is not much additional gain when comparing L2 to L3. The effort to generate the X matrix is very dependent on the number of changed pixels, the number of feature maps, and the filter size. It is however mostly important that the time spent on shuffling data around to generate X is significantly smaller than the actual matrix multiplication, which clearly makes up the largest share. The subsequent update of the output values including activation only uses a negligible part of the overall processing time.\nAn important aspect is not directly visible: The overall compute time for the critical part, the convolution and activation of L1-L3, has shrunk tremendously by more than 12.9\u00d7 from 512.8ms to about 39.7ms. The remaining steps like polling (total 6.6ms) and pixel-wise classification (L4, L5; total 16.0ms) now take 36% of the compute time, such that they move more into the target of future optimizations."}, {"heading": "4.7 Change Propagation", "text": "During the construction of the algorithm we argued that change detection should be performed for every convolution layer not only for modularity, but also justifying that the worst-case change propagation we had to assume otherwise would result in a higher computational effort. We have experimentally verified this and show an example case in Figure 10. For Layer 2, the number of changes is reduced by 6.8\u00d7 from 7.57% to 1.11% and for Layer 3 from 2.58% to 1.94% by 1.33\u00d7. While it clearly pays of for Layer 2, we analyze the situation for Layer 3 more closely. From the previous section, we know that the change detection makes up for only 22% of the overall compute time in Layer 3 and scaling up the time to generate the X matrix, perform the matrix multiplication and update the output clearly exceeds the overhead introduced by the change detection step. The change extraction step cannot be dropped, and in fact the change detection has to be replaced with a (though very quick to evaluate) change propagation kernel."}, {"heading": "4.8 Energy Efficiency", "text": "We have measured the current consumption of the entire Jetson TX1 board with only an Ethernet connection and no other off-board peripherals, running a continuous CNN workload. The average current when running the baseline cuDNN-based implementation was measured at 680mA (12.9W). The power consumption dropped to 10.92W when running CBinfer. When idling, we measured 1.7W under normal conditions, which raised to 2.5Wwhen enforcing the maximum clock frequency, as has been done to maximize throughput for the earlier measurements. The CNN we used has a computational complexity of\n210GOp/frame, where the number of operations (Ops) is the sum of additions and multiplications required for the convolution layers. We thus obtain 413GOp/s and 32.0 GOp/s/W with the cuDNN baseline. With the proposed CBinfer procedure, we obtain a perframe inference equivalent throughput of 3577GOp/s and an energy efficiency boost of about 10\u00d7 to 327.6 GOp/s/W."}, {"heading": "5 CONCLUSION", "text": "We have proposed and evaluated a novel algorithm for changebased evaluation of CNNs for video recorded with a static camera setting, exploiting the spatio-temporal sparsity of pixel changes. The results clearly show that even when choosing the change detection parameters conservatively to introduce no significant increase in misclassified pixels during semantic segmentation, an average speed-up of 8.6\u00d7 over a cuDNN baseline has been achieved using an optimized GPU implementation. An in-depth evaluation of the throughput-accuracy trade-off shows the aforementioned performance jump without loss and shows how the throughput can be further increased at the expense of accuracy. Analysis of the compute time split-up of the individual steps of the algorithm show that despite some overhead the GPU is fully loaded performing multiply-accumulate operations to update the changed pixels using the highly optimized cuBLAS matrix multiplication. An analysis of how changes propagate through the CNN further underline the optimality of the structure of the proposed algorithm. The resulting boost in energy efficiency over per-frame evaluation is an average of 10\u00d7, equivalent to 328GOp/s/W on the Tegra X1 platform."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank armasuisse Science & Technology for funding this research."}], "references": [{"title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "author": ["Lukas Cavigelli", "Luca Benini"], "venue": "IEEE TCSVT", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Accelerating Real-Time Embedded Scene Labeling with Convolutional Networks", "author": ["Lukas Cavigelli", "Michele Magno", "Luca Benini"], "venue": "In Proc. ACM/IEEE DAC", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Torch7: AMatlab-like Environment for Machine Learning", "author": ["Ronan Collobert"], "venue": "Advances in Neural Information Processing Systems Workshops", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Adv", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Hardwareoriented Approximation of Convolutional Neural Networks", "author": ["Philipp Gysel", "Mohammad Motamedi", "Soheil Ghiasi"], "venue": "In ICLR Workshops", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Learning to track at 100 FPS with deep regression networks", "author": ["David Held", "Sebastian Thrun", "Silvio Savarese"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "A Zisserman"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia"], "venue": "http://caffe.berkeleyvision.org", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Imagenet Classification With Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Adv. NIPS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "CCTV Surveillance: Video Practices and Technology", "author": ["Herman Kruegle"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs", "author": ["Andrew Lavin"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Fast Algorithms for Convolutional Neural Networks", "author": ["Andrew Lavin", "Scott Gray"], "venue": "In Proc. IEEE CVPR", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proc. IEEE CVPR", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "CNN-based in-loop filtering for coding efficiency improvement", "author": ["Woon-sung Park", "Munchurl Kim"], "venue": "In Proc. IEEE Image,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "In recent years, the most competitive approaches to address many CV challenges have relied on machine learning with complex, multi-layered, trained feature extractors commonly referred to as deep learning [18, 26, 41].", "startOffset": 205, "endOffset": 217}, {"referenceID": 12, "context": "CNNs keep on expanding to more areas of computer vision and data analytics in general [1, 14, 19, 32, 33, 43].", "startOffset": 86, "endOffset": 109}, {"referenceID": 13, "context": "CNNs keep on expanding to more areas of computer vision and data analytics in general [1, 14, 19, 32, 33, 43].", "startOffset": 86, "endOffset": 109}, {"referenceID": 1, "context": "However, the inference of state-of-the-art CNNs also requires several billions of multiplications and additions to classify even low resolution images by today\u2019s standards [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "Furthermore, collecting large amounts of data at a central site raises privacy concerns and the required high-bandwidth communication channel causes additional reliability problems and potentially prohibitive cost of deployment and during operation [27].", "startOffset": 249, "endOffset": 253}, {"referenceID": 1, "context": "These push the evaluation of such networks for real-time semantic segmentation or object detection out of reach of even the most powerful embedded platforms available today for high-resolution video data [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "However, exactly such systems are required for a wide range of applications limited in cost (CCTV/urban surveillance, perimeter surveillance, consumer behavior and highway monitoring) and latency (aerospace and UAV monitoring and defence, visual authentication) [27, 35].", "startOffset": 262, "endOffset": 270}, {"referenceID": 1, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 101, "endOffset": 123}, {"referenceID": 7, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 101, "endOffset": 123}, {"referenceID": 10, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 101, "endOffset": 123}, {"referenceID": 11, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 101, "endOffset": 123}, {"referenceID": 0, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 170, "endOffset": 187}, {"referenceID": 13, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 170, "endOffset": 187}, {"referenceID": 3, "context": "Large efforts have thus already been taken to develop optimized software for heterogeneous platforms [5, 7, 23, 28, 29, 42], to design specialized hardware architectures [2, 3, 6, 12, 33], and to adapt the networks to avoid expensive arithmetic operations or [10, 36, 43].", "startOffset": 259, "endOffset": 271}, {"referenceID": 2, "context": "First, most widely used frameworks relied on their own custom implementations which have all converged to methods relying on matrix-multiplications [8, 23], leveraging the availability of highly optimized code in BLAS libraries and the fact that GPUs are capable of achieving a throughput within a few percent of their peak performance with this type of workload.", "startOffset": 148, "endOffset": 155}, {"referenceID": 7, "context": "First, most widely used frameworks relied on their own custom implementations which have all converged to methods relying on matrix-multiplications [8, 23], leveraging the availability of highly optimized code in BLAS libraries and the fact that GPUs are capable of achieving a throughput within a few percent of their peak performance with this type of workload.", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "Specialized libraries such as Nvidia\u2019s cuDNN and Nervana Systems\u2019 Neon provide some additional performance gains through assembly-level implementations [28] and additional algorithmic improvements such as Winograd and FFT-based convolution [29].", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "Specialized libraries such as Nvidia\u2019s cuDNN and Nervana Systems\u2019 Neon provide some additional performance gains through assembly-level implementations [28] and additional algorithmic improvements such as Winograd and FFT-based convolution [29].", "startOffset": 240, "endOffset": 244}, {"referenceID": 1, "context": "A specific implementation for nonbatched inference on an embedded platform building on a matrix multiplication is documented in [5], also showing that more than 90% of time is spent computing convolutions.", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "While most fixed-point methods are of limited use on many off-the-shelf software programmable platforms, some can benefit from vectorization of lower-precision operations [15].", "startOffset": 171, "endOffset": 175}, {"referenceID": 3, "context": "Extreme methods go as far as to enforce binary weights [2, 10], and in some cases also binary activations [36].", "startOffset": 55, "endOffset": 62}, {"referenceID": 0, "context": "Many networks can be quantized with 8 bit without an increase in error rate, before there is a trade-off between precision and accuracy [3, 17].", "startOffset": 136, "endOffset": 143}, {"referenceID": 12, "context": "Further research has focused on optimizing semantic segmentation and object detection algorithms to better reuse already computed features by eliminating any non-convolutional elements from the network [32, 37, 38].", "startOffset": 202, "endOffset": 214}, {"referenceID": 6, "context": "Simplifying the operations in a network, such as low-rank approximations of 2D convolutions or by simply designing smaller networks with state-of-the-art methods have been evaluated in [21, 22, 34].", "startOffset": 185, "endOffset": 197}, {"referenceID": 5, "context": "Limited movement of objects in a frame can be exploited in object tracking by working with a limited search window within the frame [20], not only reducing the problem size, but also simplifying the regression task\u2014up until the tracked target is occluded by a large object.", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "The authors of [32] have extended their work on fully convolutional networks for semantic segmentation, which presents a CNN with skip connections and deconvolution layers to refine the lower-resolution feature maps obtained deep within the network using the features extracted early in the network.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "This means adapting the widely used, simple and well-performing matrix-generation and matrix-multiplication sequence of operations [5, 23].", "startOffset": 131, "endOffset": 138}, {"referenceID": 7, "context": "This means adapting the widely used, simple and well-performing matrix-generation and matrix-multiplication sequence of operations [5, 23].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "Matrix multiplication-based implementations of the convolution layer relying on this are widely available and are highly efficient [5, 24] and is described earlier in this section.", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "(1) Inference is typically done on single frames and creating mini-batches would introduce often unacceptable latency and the benefit of doing so is limited to a few percent of additional performance [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "We have implemented the proposed algorithm using CUDA and wrapped them as modules for the Torch framework [8].", "startOffset": 106, "endOffset": 109}], "year": 2017, "abstractText": "Extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful GPU-accelerated workstations and compute clusters. However, there are many applications such as smart surveillance cameras that require or would benefit from on-site processing. To this end, we propose and evaluate a novel algorithm for changebased evaluation of CNNs for video data recorded with a static camera setting, exploiting the spatio-temporal sparsity of pixel changes. We achieve an average speed-up of 8.6\u00d7 over a cuDNN baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1% and no retraining of the network. The resulting energy efficiency is 10\u00d7 higher than per-frame evaluation and reaches an equivalent of 328GOp/s/W on the Tegra X1 platform.", "creator": "LaTeX with hyperref package"}}}