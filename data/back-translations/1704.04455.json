{"id": "1704.04455", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Cardinal Virtues: Extracting Relation Cardinalities from Text", "abstract": "Information extraction (IE) from texts has largely focused on the relationships between individual entities, such as who won which award. However, some facts are never fully mentioned, and no IE method has a perfect memory. Therefore, it is beneficial to also tap into content about the cardinalities of these relationships, for example, how many awards someone has won. We present this novel problem of cardinalities extraction and discuss the specific challenges that distinguish them from the standard IE. We present a remote monitoring method using conditional random fields. A preliminary evaluation yields a precision between 3% and 55%, depending on the difficulty of the relationships.", "histories": [["v1", "Fri, 14 Apr 2017 15:28:06 GMT  (32kb)", "https://arxiv.org/abs/1704.04455v1", "5 pages, ACL 2017 (short paper)"], ["v2", "Sat, 27 May 2017 00:55:36 GMT  (32kb)", "http://arxiv.org/abs/1704.04455v2", "5 pages, ACL 2017 (short paper)"]], "COMMENTS": "5 pages, ACL 2017 (short paper)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paramita mirza", "simon razniewski", "fariz darari", "gerhard weikum"], "accepted": true, "id": "1704.04455"}, "pdf": {"name": "1704.04455.pdf", "metadata": {"source": "CRF", "title": "Cardinal Virtues: Extracting Relation Cardinalities from Text", "authors": ["Paramita Mirza", "Simon Razniewski", "Fariz Darari", "Gerhard Weikum"], "emails": ["weikum}@mpi-inf.mpg.de", "darari}@inf.unibz.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n04 45\n5v 2\n[ cs\n.C L\n] 2\n7 M\nay 2\n01 7\nlargely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discuss specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations."}, {"heading": "1 Introduction", "text": "Motivation Information extraction (IE) can infer relations between named entities from text (e.g., (Mitchell et al., 2015; Del Corro and Gemulla, 2013; Mausam et al., 2012)), yielding for example which awards an athlete has won, or instances of family relations like spouses, children, etc. These methods can be harnessed for summarization, question answering (QA), and more. For populating knowledge bases (KBs), the IE output is usually cast into subject-predicate-object (SPO) triples, such as \u3008BarackObama, hasChild, Malia\u3009, or sometimes n-ary tuples such as \u3008MichaelPhelps, hasWon, OlympicGold, 200mButterfly, 2016\u3009.\nIE has focused on capturing full SPO triples (or n-ary facts) with all arguments bound to entities for relation P. However, news, biographies or discussion forums often contain numeric expressions that reveal cardinalities of relations. Phrases such as \u201cher two children\u201d or \u201chis 28th medal\u201d are valu-\nable cues for quantifying the hasChild and hasWon relations. This can be harnessed in QA for cases like \u201cWho won the most Olympic medals?\u201d\nAn important application of relation cardinalities is KB curation. KBs are notoriously incomplete, contain erroneous triples, and are limited in keeping up with the pace of real-world changes. For example, a KB may contain only 10 of the 28 Olympic medals that Phelps has won, or may incorrectly list 3 children for Obama. Extracting the cardinalities of relations for given subject entities can address all of these issues.\nRelation cardinalities are disregarded by virtually all IE methods. Open IE methods (Mausam et al., 2012; Del Corro and Gemulla, 2013) capture triples (or quadruples) such as \u3008Obama, has, two children\u3009. However, there is no way to interpret the numeric expression in the O slot of this triple. While IE methods that hinge on pre-specified relations for KB population (e.g., NELL (Mitchell et al., 2015)) can already capture numeric values for explicitly stated attributes such as \u3008Berlin2016attack, hasNumOfVictims, 32\u3009, they are currently not able to learn them.\nThis paper addresses the novel task of extracting relation cardinalities. For a given subject entity s and predicate p, we aim to infer the cardinality |{\u3008S,P,O\u3009 | S = s, P = p}| directly from text, without having to observe anyO entities. This task poses several challenges: \u2022 IE Training. Most IE methods build on seedbased distant supervision. However, if the un-\nderlying KB is not complete, taking the counts of SPO triples for a given SP pair may result in wrong seeds, which can lead to poor patterns. \u2022 Compositionality. The cardinality of an SP pair for a relation may depend on several cardinality\nmentions. For example, when observing \u201cAngelina has two sons and three daughters\u201d, one could infer the children cardinality by summing.\n\u2022 Linguistic Variance. In addition to cardinal numbers, cardinality IE should also pay atten-\ntion to number-related terms, e.g., \u201cAngelina gives birth to twins\u201d, or ordinal information, e.g., \u201cAngelina\u2019s fourth child\u201d, which can reveal lower bounds on relation cardinalities.\nApproach and Contribution Our method learns patterns of phrases that contain cardinal numbers, relying on the distant supervision approach by counting facts for given SP pairs. Our technical contributions are as follows: (i) we provide a statistical analysis of numeric information in Wikipedia articles; (ii) we develop a CRF-based extraction method for relation cardinalities that achieves precision scores of up to 55%; (iii) we analyze further challenges in this research and outline possible solutions."}, {"heading": "2 Related Work", "text": "Knowledge Bases and Information Extraction Automated KB construction is a major effort for quite a while. Some approaches, such as YAGO (Suchanek et al., 2007) or DBpedia (Auer et al., 2007), focus on structured parts of Wikipedia, while other approaches such as OLLIE (Mausam et al., 2012), ClauseIE (Del Corro and Gemulla, 2013) or NELL (Mitchell et al., 2015), focus on unstructured contents across the whole Web. In the latter, usually the schema is also not predefined, thus such approaches are called Open IE. Most stateof-the-art systems now rely on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009).\nDespite all efforts, KBs are immensely incomplete. For instance, the average number of children per person in Wikidata (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014) is just 0.02 (Razniewski et al., 2016).\nNumbers and Relation Cardinalities Numbers in text are an important source of information. Much work has been done on understanding numbers that express temporal information (Ling and Weld, 2010; Stro\u0308tgen and Gertz, 2010), and more recently, on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016).\nIn contrast, numbers that express relation cardinalities have received little attention so far. State-\nof-the-art Open-IE systems either hardly extract cardinality information or fail to extract cardinalities at all. While NELL, for instance, knows 13 relations about the number of casualties and injuries in disasters, they all contain only seed facts and no learned facts. The only prior work we are aware of is of Mirza et al. (2016), who use manually created patterns to mine children cardinalities fromWikipedia. It is shown that with 30 manually crafted patterns and simple filters it is possible to extract 86,227 children-cardinality-assertions with a precision of 94.3%."}, {"heading": "3 Relation Cardinalities", "text": "Definition We define a mention that expresses relation cardinalities as the following: \u201cA cardinal number that states the number of objects that stand in a specific relation with a certain subject.\u201d\nUsing this definition, we analyzed how often relation cardinalities occur in Wikipedia. Relying on the part-of-speech (PoS) tagger of Stanford CoreNLP (Manning et al., 2014), we extracted numbers\u2013i.e., words tagged as CD (cardinal number)\u2013from 10,000 random Wikipedia articles. The distribution of their named-entity (NE) tags, according to Stanford NE-tagger, is shown in Table 1. While temporal-related numbers are the most frequent, around 40% are classified only as unspecific NUMBER. By manually checking 100 random NUMBERs, we observed that 47 are relation cardinalities,1 i.e., approximately 18.86% of all numbers in Wikipedia are relation cardinalities.\nWe also analyzed the nouns frequently modified by NUMBERs, based on their dependency paths, finding people, games, children, times, members and seasons among the top nouns. Coarse topicgrouping of the nouns shows that most NUMBERs are about sport (games, goals), followed by artwork (seasons, books), politics and organization (members, countries), and family (children)."}, {"heading": "4 Relation Cardinality Extraction", "text": "Ideally, we would like to make sense of all cardinality statements found in text. However, this would require us to resolve the meaning of a large set of vague predicates, which is in general a difficult task. We thus turn the problem around: given a well defined relation/predicate p, a subject s and a corresponding text about s, we now\n1Among the others are measures, age, or expressions like \u201cone of the...\u201d.\ntry to estimate the relation cardinality (i.e., the count of \u3008s, p, \u2217\u3009 triples), based on cardinality assertions found in the text. We chose four Wikidata predicates that span various domains, child (P40), spouse (P26), has part (P527) of a series of creative works (restricted to novel, book and film series), and contains administrative territorial entity (P150). As the text source for subjects of each predicate, we consider sentences containing numbers taken from their respective English Wikipedia articles.\nMethodology We approach the problem via sequence labelling, i.e., given a sentence containing at least one number, we aim to determine whether each number in the sentence corresponds to the cardinality of a certain relation. We build a Conditional Random Field (CRF) based model with CRF++ (Kudo, 2005) for each relation, taking as features the context lemmas (window size of 5) around the observed token t, along with bigrams and trigrams containing t.\nTo generate the training data, we rely on distant supervision, annotating candidate numbers2 in the text as correct cardinalities whenever they correspond to the exact triple count (count > 0) found in the knowledge base. Otherwise, they are labelled as O (for Others), like the rest of nonnumber tokens. Table 2 contains for each considered relation (p), the number of subjects (#s) in Wikidata, which have links to English Wikipedia pages and have at least one \u3008s, p, \u2217\u3009 triple.\nWe predict the relation cardinality of a given \u3008s, p\u3009 pair by selecting the number positively annotated with marginal probability\u2013resulting from forward-backward inference\u2013higher than 0.1, and choosing the one with the highest probability if there are several.\nExperiments Two experimental settings are considered: vanilla refers to the distant supervi-\n2Numbers that are not labelled as DATE, TIME, DURATION, SET, MONEY and PERCENT by Stanford NE-tagger.\nsion approach explained above, while for onlynummod, we only annotate a candidate number as correct cardinality if it modifies a noun, i.e., there is an incoming dependency relation of label nummod according to the Stanford Dependency Parser. This is to exclude numbers as in \u201cone of the reasons...\u201d from training examples. We also considered a naive baseline, which chooses a random number from a pool of numbers existing in each text about a certain subject.\nFurthermore, to estimate how well KB counts are suited as ground truth, we compare them on the the child relation with the manually-created number of children (P1971) property from Wikidata.\nEvaluation Results We manually annotated the evaluation data with the true relation counts, since the knowledge base is highly incomplete, and thus, the triple counts are often incorrect. Whenever the cardinality matches the true count, we also manually inspected how relevant the textual evidence\u2013 the context surrounding the cardinal number\u2013is for the observed relation. Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object.\nThe random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score. For spouse, the performance is significantly lower, reasons are discussed below. Furthermore, we can observe that using manual ground truth as training data for the child relation can boost performance considerably. Still, the performance is significantly below the stateof-the-art in fact extraction, where child triples can be extracted from Wikipedia text with 96% precision (Palomares et al., 2016)."}, {"heading": "5 Analysis", "text": "A qualitative analysis of the training data and evaluation results revealed three aspects that make extracting relation cardinalities difficult.\nQuality of Training Data Unlike training data for normal fact extraction, which is generally highly correct (e.g., YAGO claims 95% preci-\nsion (Suchanek et al., 2007)), taking triple counts found in knowledge bases as ground truth generally gives wrong results. For example, our manual evaluation of child shows that the triple count from Wikidata is 46% lower than what the texts assert.\nAs shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction. Unfortunately, manually curated data is generally difficult to obtain. We see two avenues to tackle training data quality:\n1. Filtering ground truth. Instead of taking the\ncounts of all entities as ground truth, one might trade size for quality, e.g., using popular entities only, as for these there are chances that KBs are more complete. 2. Incompleteness-resilient distant supervision.\nTriple counts in KBs are often lower than what is correct, but rarely too high. Thus, an avenue might be to label all numbers equal or higher than the KB count as correct, instead of only considering the equal ones. Given that different cardinalities could then be labelled as correct, this would require a postprocessing step in which conflicting counts are consolidated.\nCompositionality Around 16% of false positives in extracting child cardinalities can be attributed to failures in identifying the correct count for, e.g., \u201dThey have two sons and one daughter together; he has four children from an earlier relationship.\u201d This was also observed for other relations, e.g., \u201cThe Qidong county has 4 subdistricts, 17 towns and 3 townships under its juridiction.\u201d We see two avenues to tackle this problem:\n1. Aggregating numbers. In training data genera-\ntion, one could label a sequence of number as correct cardinalities if the sum of the numbers is equal to the relation count. In the prediction step, one might sum up all consecutive cardinalities that are labelled with sufficient confi-\ndence.\n2. Learning composition rules. One may try to\nlearn the composition of counts, for instance, that children are composed of sons and daughters, then try to extract the composing cardinalities.\nLinguistic Variance We observe that for the spouse relation, expressing the count with cardinal numbers (\u201cHe has married four times\u201d) is only found for 4% of subjects. It is more common to express the count with ordinal numbers, e.g., \u201cJohn\u2019s first wife, Mary, ...\u201d, which allows us to conclude that the spouse-count for John is at least\u2013and most probably more than\u2013one. An approach to such relations might be to identify ordinals numbers that express lower bounds of relations. Subsequently, one could reason over these bounds and try to infer relation counts.\nOur initial motivation was to make sense of the so far ignored large fraction of numbers that express relation cardinalities. However, we noticed quickly that relation cardinalities are frequently also expressed without numbers at all. This is especially true for the case of count zero, which is mostly expressed using negation (\u201cHe never married\u201d), and the count one, which is expressed using indefinite articles (\u201cThey have a child\u201d) or the signal-word only (\u201cTheir only child, James\u201d). Terms such as twins or trilogy are also ways to express domain-specific relation cardinalities. We see two avenues to approach this variance:\n1. Translation to numbers. For the 0\u2019s and 1\u2019s, a\npossible approach is to translate certain kinds of negation and indefinite articles into explicit numbers (e.g., \u201cdo not have any children\u201d \u2192 \u201chave 0 children\u201d). 2. Word similarity with cardinals. If a word bears\nhigh similarity with cardinal numbers, possibly also in other languages such as Latin or Greek, one might consider it as a candidate number."}, {"heading": "6 Conclusion", "text": "In this paper we have introduced the problem of relation cardinality extraction. We believe that relation cardinalities can be useful in a variety of tasks. Our next goal is to make distant supervision incompleteness-resilient and to deal with compositionality, hoping that these can improve the precision of our approach. We also aim to take ordinals into account and to experiment with linguistic transformation for the cases of cardinalities 0 and 1, hoping that these could boost the recall.\nA limitation of our work is also that we only focus on Wikipedia articles, assume that all statements are about the article\u2019s subject, and just take the statement with the highest confidence. In future work we aim to include a larger article base in combination with named entity recognition, coreference resolution and a truth consolidation step."}, {"heading": "Acknowledgments", "text": "We thank Werner Nutt and Sebastian Rudolph for their feedback on an earlier version of this work. We thank the anonymous reviewers for their helpful comments. This work has been partially supported by the project \u201cThe Call for Recall\u201d, funded by the Free University of Bozen-Bolzano."}], "references": [{"title": "DBpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."], "venue": "Springer.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "How much is 131 million dollars? putting numbers in perspective with compositional descriptions", "author": ["Arun Chaganty", "Percy Liang."], "venue": "ACL. pages 578\u2013587.", "citeRegEx": "Chaganty and Liang.,? 2016", "shortCiteRegEx": "Chaganty and Liang.", "year": 2016}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien."], "venue": "Proceedings of the Seventh InternationalConference on Intelligent Systems for Molecular Biology. pages 77\u201386.", "citeRegEx": "Craven and Kumlien.,? 1999", "shortCiteRegEx": "Craven and Kumlien.", "year": 1999}, {"title": "ClausIE: clause-based open information extraction", "author": ["Luciano Del Corro", "Rainer Gemulla."], "venue": "InWWW. ACM, pages 355\u2013366.", "citeRegEx": "Corro and Gemulla.,? 2013", "shortCiteRegEx": "Corro and Gemulla.", "year": 2013}, {"title": "Making sense of entities and quantities in web tables", "author": ["Yusra Ibrahim", "Mirek Riedewald", "Gerhard Weikum."], "venue": "CIKM. pages 1703\u20131712.", "citeRegEx": "Ibrahim et al\\.,? 2016", "shortCiteRegEx": "Ibrahim et al\\.", "year": 2016}, {"title": "CRF++: Yet another CRF toolkit", "author": ["Taku Kudo."], "venue": "Software available at http://crfpp. sourceforge.net .", "citeRegEx": "Kudo.,? 2005", "shortCiteRegEx": "Kudo.", "year": 2005}, {"title": "Temporal information extraction", "author": ["Xiao Ling", "Daniel S Weld."], "venue": "AAAI. volume 10, pages 1385\u20131390.", "citeRegEx": "Ling and Weld.,? 2010", "shortCiteRegEx": "Ling and Weld.", "year": 2010}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."], "venue": "ACL (System Demonstrations) pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Open language learning for information extraction", "author": ["Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni."], "venue": "EMNLP. pages 523\u2013534.", "citeRegEx": "Mausam et al\\.,? 2012", "shortCiteRegEx": "Mausam et al\\.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky."], "venue": "ACL. pages 1003\u2013 1011.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Expanding Wikidatas parenthood information by 178%, or how to mine relation cardinalities", "author": ["Paramita Mirza", "Simon Razniewski", "Werner Nutt."], "venue": "ISWC Posters & Demos .", "citeRegEx": "Mirza et al\\.,? 2016", "shortCiteRegEx": "Mirza et al\\.", "year": 2016}, {"title": "Multi-level semantic labelling of numerical values", "author": ["Sebastian Neumaier", "J\u00fcrgen Umbrich", "Josiane Xavier Parreira", "Axel Polleres."], "venue": "ISWC. pages 428\u2013445.", "citeRegEx": "Neumaier et al\\.,? 2016", "shortCiteRegEx": "Neumaier et al\\.", "year": 2016}, {"title": "Wikipedia knowledge graph with DeepDive", "author": ["Thomas Palomares", "Youssef Ahres", "Juhana Kangaspunta", "Christopher R\u00e9."], "venue": "ICWSM. pages 65\u201371.", "citeRegEx": "Palomares et al\\.,? 2016", "shortCiteRegEx": "Palomares et al\\.", "year": 2016}, {"title": "2016. But what do we actually know", "author": ["Simon Razniewski", "Fabian M. Suchanek", "Werner Nutt"], "venue": "Proceedings of AKBC", "citeRegEx": "Razniewski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Razniewski et al\\.", "year": 2016}, {"title": "Heideltime: High quality rule-based extraction and normalization of temporal expressions", "author": ["Jannik Str\u00f6tgen", "Michael Gertz."], "venue": "SemEval Workshop. pages 321\u2013324.", "citeRegEx": "Str\u00f6tgen and Gertz.,? 2010", "shortCiteRegEx": "Str\u00f6tgen and Gertz.", "year": 2010}, {"title": "YAGO: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."], "venue": "WWW pages 697\u2013706.", "citeRegEx": "Suchanek et al\\.,? 2007", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Commu-", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": ", (Mitchell et al., 2015; Del Corro and Gemulla, 2013; Mausam et al., 2012)), yielding for example which awards an athlete has won, or instances of family relations like spouses, children, etc.", "startOffset": 2, "endOffset": 75}, {"referenceID": 8, "context": "Open IE methods (Mausam et al., 2012; Del Corro and Gemulla, 2013) capture triples (or quadruples) such as \u3008Obama, has, two children\u3009.", "startOffset": 16, "endOffset": 66}, {"referenceID": 15, "context": "Some approaches, such as YAGO (Suchanek et al., 2007) or DBpedia (Auer et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 0, "context": ", 2007) or DBpedia (Auer et al., 2007), focus on structured parts of Wikipedia, while other approaches such as OLLIE (Mausam et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 8, "context": ", 2007), focus on structured parts of Wikipedia, while other approaches such as OLLIE (Mausam et al., 2012), ClauseIE (Del Corro and Gemulla, 2013) or NELL (Mitchell et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 2, "context": "Most stateof-the-art systems now rely on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009).", "startOffset": 61, "endOffset": 107}, {"referenceID": 9, "context": "Most stateof-the-art systems now rely on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009).", "startOffset": 61, "endOffset": 107}, {"referenceID": 16, "context": "For instance, the average number of children per person in Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) is just 0.", "startOffset": 68, "endOffset": 98}, {"referenceID": 13, "context": "02 (Razniewski et al., 2016).", "startOffset": 3, "endOffset": 28}, {"referenceID": 6, "context": "Much work has been done on understanding numbers that express temporal information (Ling and Weld, 2010; Str\u00f6tgen and Gertz, 2010), and more recently, on numbers that ex-", "startOffset": 83, "endOffset": 130}, {"referenceID": 14, "context": "Much work has been done on understanding numbers that express temporal information (Ling and Weld, 2010; Str\u00f6tgen and Gertz, 2010), and more recently, on numbers that ex-", "startOffset": 83, "endOffset": 130}, {"referenceID": 1, "context": "press physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al.", "startOffset": 64, "endOffset": 90}, {"referenceID": 4, "context": "press physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016).", "startOffset": 123, "endOffset": 168}, {"referenceID": 11, "context": "press physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016).", "startOffset": 123, "endOffset": 168}, {"referenceID": 10, "context": "The only prior work we are aware of is of Mirza et al. (2016), who use manually created patterns to mine children cardinalities fromWikipedia.", "startOffset": 42, "endOffset": 62}, {"referenceID": 7, "context": "Relying on the part-of-speech (PoS) tagger of Stanford CoreNLP (Manning et al., 2014), we extracted numbers\u2013i.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "We build a Conditional Random Field (CRF) based model with CRF++ (Kudo, 2005) for each relation, taking as features the context lemmas (window size of 5) around the observed token t, along with bigrams and trigrams containing t.", "startOffset": 65, "endOffset": 77}, {"referenceID": 12, "context": "Still, the performance is significantly below the stateof-the-art in fact extraction, where child triples can be extracted from Wikipedia text with 96% precision (Palomares et al., 2016).", "startOffset": 162, "endOffset": 186}, {"referenceID": 15, "context": "sion (Suchanek et al., 2007)), taking triple counts found in knowledge bases as ground truth generally gives wrong results.", "startOffset": 5, "endOffset": 28}], "year": 2017, "abstractText": "Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect recall. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel problem of extracting cardinalities and discuss specific challenges that set it apart from standard IE. We present a distant supervision method using conditional random fields. A preliminary evaluation results in precision between 3% and 55%, depending on the difficulty of relations.", "creator": "LaTeX with hyperref package"}}}