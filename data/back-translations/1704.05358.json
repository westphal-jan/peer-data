{"id": "1704.05358", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Representing Sentences as Low-Rank Subspaces", "abstract": "Sentences are important semantic units of natural language. A generic, distributive representation of sentences that can capture latent semantics is beneficial for several downstream applications. We observe simple sentence geometry - the word representations of a given sentence (an average of 10.23 words in all SemEval datasets with a standard deviation of 4.84) are roughly located in a subspace (approximately rank 4). Motivated by this observation, we represent a sentence by the subspace spanned by its word vectors. Such unattended representation is empirically confirmed by semantic text similarity tasks on 19 different datasets, in which it exceeds the highly developed neural network models, including skipped vectors, by an average of 15%.", "histories": [["v1", "Tue, 18 Apr 2017 14:30:32 GMT  (62kb,D)", "http://arxiv.org/abs/1704.05358v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiaqi mu", "suma bhat", "pramod viswanath"], "accepted": true, "id": "1704.05358"}, "pdf": {"name": "1704.05358.pdf", "metadata": {"source": "CRF", "title": "Representing Sentences as Low-Rank Subspaces", "authors": ["Jiaqi Mu", "Suma Bhat", "Pramod Viswanath"], "emails": ["pramodv}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015). Going beyond words, sentences capture much of the semantic information. Given the success of lexical representations, a natural question of great topical interest is how to extend the power of distributional representations to sentences.\nThere are currently two approaches to represent\nsentences. A sentence contains rich syntactic information and can be modeled through sophisticated neural networks (e.g., convolutional neural networks (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (Sutskever et al., 2014; Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) and recursive neural networks (Socher et al., 2013)). Another simple and common approach ignores the latent structure of sentences: a prototypical approach is to represent a sentence by summing or averaging over the vectors of the words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).\nRecently, Wieting et al. (2015); Adi et al. (2016) reveal that even though the latter approach ignores all syntactic information, it is simple, straightforward, and remarkably robust at capturing the sentential semantics. Such an approach successfully outperforms the neural network based approaches on textual similarity tasks in both supervised and unsupervised settings.\nWe follow the latter approach but depart from representing sentences in a vector space as in these prior works; we present a novel Grassmannian property of sentences. The geometry is motivated by (Gong et al., 2017; Mu et al., 2016) where an interesting phenomenon is observed \u2013 the local context of a given word/phrase can be well represented by a low rank subspace. We propose to generalize this observation to sentences: not only do the word vectors in a snippet of a sentence (i.e., a context for a given word defined as several words surrounding it) lie in a low-rank subspace, but the entire sentence (on average 10.23 words in all SemEval datasets with standard deviation 4.84) follows this geometric property as well:\nGeometry of Sentences: The word representations lie in a low-rank subspaces (rank 3-5) for all words in a target sen-\nar X\niv :1\n70 4.\n05 35\n8v 1\n[ cs\n.C L\n] 1\n8 A\npr 2\n01 7\ntence.\nThe observation indicates that the subspace contains most of the information about this sentence, and therefore motivates a sentence representation method: the sentences should be represented in the space of subspaces (i.e., the Grassmannian manifold) instead of a vector space; formally:\nSentence Representation: A sentence can be represented by a low-rank subspace spanned by its word representations.\nAnalogous to word representations of similar words being similar vectors, the principle of sentence representations is: similar sentences should have similar subspaces. Two questions arise: (a) how to define the similarity between sentences and (b) how to define the similarity between subspaces.\nThe first question has been already addressed by the popular semantic textual similarity (STS) tasks. Unlike textual entailment (which aims at inferring directional relation between two sentences) and paraphrasing (which is a binary classification problem), STS provides a unified framework of measuring the degree of semantic equivalence (Agirre et al., 2012, 2013, 2014, 2015) in a continuous fashion. Motivated by the cosine similarity between vectors being a good index for word similarity, we generalize this metric to subspaces: the similarity between subspaces defined in this paper is the `2-norm of the singular values between two subspaces; note that the singular values are in fact the cosine of the principal angles.\nThe key justification for our approach comes from empirical results that outperform state-ofthe-art in some cases, and being comparable in others. In summary, representing sentences by subspaces outperforms representing sentences by averaged word vectors (by 14% on average) and sophisticated neural networks (by 15%) on 19 different STS datasets, ranging over different domains (News, WordNet definition, and Twitter)."}, {"heading": "2 Geometry of Sentences", "text": "Assembling successful distributional word representations (for example, GloVe (Pennington et al., 2014)) into sentence representations is an active research topic. Different from previous studies (for example, doc2vec (Mikolov et al., 2013), skip-thought vectors (Kiros et al., 2015), Siamese\nCBOW (Kenter et al., 2016)), our main contribution is to represent sentences using non-vector space representations: a sentence can be well represented by the subspace spanned by the context word vectors \u2013 such a method naturally builds on any word representation method. Due to the widespread use of word2vec and GloVe, we use their publicly available word representations \u2013 word2vec(Mikolov et al., 2013) trained using Google News1 and GloVe (Pennington et al., 2014) trained using Common Crawl2 \u2013 to test our observations.\nObservation Let v(w) \u2208 Rd be the ddimensional word representation for a given word w \u2208 V , and s = (w1, . . . , wn) be a given sentence. Consider the following sentence where n = 32:\nThey would not tell me if there was any pension left here, and would only tell me if there was (and how much there was) if they saw I was entitled to it.\nAfter stacking the (non-functional) word vectors v(w) to form a d\u00d7n matrix, we observe that most energy (80% for GloVe and 72% for word2vec) of such a matrix is contained in a rank-N subspace, where N is much smaller than n (for comparison, we choose N to be 4 and therefore N/n \u2248 13%). Figure 1 provides a visual representation of this geometric phenomenon, where we have projected the d-dimensional word representations into 3- dimensional vectors and use these 3-dimensional word vectors to get the subspace for this sentence (we set N = 2 here for visualization), and plot the subspaces as 2-dimensional planes.\nGeometry of Sentences The example above generalizes to a vast majority of the sentences: the word representations of a given sentence roughly reside in a low-rank subspace, which can be extracted by principal component analysis (PCA).\nVerification We empirically validate this geometric phenomenon by collecting 53,396 sentences from the SemEval STS share tasks (Agirre et al., 2012, 2013, 2014, 2015) and plotting the fraction of energy being captured by the top N components of PCA in Figure 2 for N = 3, 4, 5,\n1https://code.google.com/archive/p/ word2vec/\n2http://nlp.stanford.edu/projects/ glove/\nfrom where we can observe that on average 70% of the energy is captured by a rank-3 subspace, and 80% for a rank-4 subspace and 90% for rank-5 subspace. For comparison, the fraction of energy of random sentences (generated i.i.d. from the unigram distribution) are also plotted in Figure 2.\nRepresentation The observation above motivates our sentence representation algorithm: since the words in a sentence concentrate on a few directions, the subspace spanned by these directions could in principle be a proper representation for this sentence. The direction and subspace in turn can be extracted via PCA as in Algorithm 1.\nSimilarity Metric The principle of sentence representations is that similar sentences should have similar representations. In this case, we expect the similarity between subspaces to be a good index for the semantic similarity of sentences. In our paper, we define the similarity between subspaces as follows: let u1(s), ..., uN (s) be the N orthonormal basis for a sentence s. After stacking the N vectors in a d \u00d7 N matrix\nAlgorithm 1: The algorithm for sentence representations. Input : a sentence s, word embeddings v(\u00b7),\nand a PCA rank N . 1 Compute the first N principle components of\nsamples v(w\u2032), w\u2032 \u2208 c,\nu1, ..., uN \u2190 PCA(v(w\u2032), w\u2032 \u2208 s),\nS \u2190\n{ N\u2211\nn=1\n: \u03b1nun, \u03b1n \u2208 R\n}\nOutput: N orthonormal basis u1, ..., uN and a subspace S.\nU(s) = (u1(s), ..., uN (s)), we define the corresponding cosine similarity as, CosSim(s1, s2) =\u221a\u2211N\nt=1 \u03c3 2 t , where \u03c3t is the t-th singular value of\nU(s1) TU(s2).\nNote that \u03c3t = cos(\u03b8t) where \u03b8t is the t-th \u201cprinciple angle\u201d between two subspaces. Such a metric is naturally related to the cosine similarity between vectors, which has been empirically validated to be a good measure of word similarity."}, {"heading": "3 Experiments", "text": "In this section we evaluate our sentence representations empirically on the STS tasks. The objective of this task is to test the degree to which the algorithm can capture the semantic equivalence between two sentences. For example, the similarity between \u201ca kid jumping a ledge with a bike\u201d and \u201ca black and white cat playing with a blanket\u201d is 0 and the similarity between \u201ca cat standing on tree branches\u201d and \u201ca black and white cat is high up on tree branches\u201d is 3.6. The algorithm is then evaluated in terms of Pearson\u2019s correlation between the predicted score and the human judgment.\nDatasets We test the performances of our algorithm on 19 different datasets, which include SemEval STS share tasks (Agirre et al., 2012, 2013, 2014, 2015), sourced from multiple domains (for example, News, WordNet definitions and Twitter).\nBaselines and Preliminaries Our main comparisons are with algorithms that perform unsupervised sentence representation: average of word representations (i.e., avg. (of GloVe and skipgram) where we use the average of word vectors), doc2vec (D2V) (Le and Mikolov, 2014) and\nsophisticated neural networks (i.e., skip-thought vectors (ST) (Kiros et al., 2015), Siamese CBOW (SC) (Kenter et al., 2016)). In order to enable a fair comparison, we use the Toronto Book Corpus (Zhu et al., 2015) to train word embeddings. In our experiment, we adapt the same setting as in (Kenter et al., 2016) where we use skip-gram (Mikolov et al., 2013) of and GloVe (Pennington et al., 2014) to train a 300-dimensional word vectors for the words that occur 5 times or more in the training corpus. The rank of subspaces is set to be 4 for both word2vec and GloVe.\nResults The detailed results are reported in Table 1, from where we can observe two phenomena: (a) representing sentences by its averaged word vectors provides a strong baseline and the performances are remarkably stable across different datasets; (b) our subspace-based method outperforms the average-based method by 14% on average and the neural network based approaches by 15%. This suggests that representing sentences by subspaces maintains more information than simply taking the average, and is more robust than highly-tuned sophisticated models.\nWhen we average over the words, the average\nvector is biased because of many irrelevant words (for example, function words) in a given sentence. Therefore, given a longer sentence, the effect of useful word vectors become smaller and thus the average vector is less reliable at capturing the semantics. On the other hand, the subspace representation is immune to this phenomenon: the word vectors capturing the semantics of the sentence tend to concentrate on a few directions which dominate the subspace representation."}, {"heading": "4 Conclusion", "text": "This paper presents a novel unsupervised sentence representation leveraging the Grassmannian geometry of word representations. While the current approach relies on the pre-trained word representations, the joint learning of both word and sentence representations and in conjunction with supervised datasets such the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) is left to future research. Also interesting is the exploration of neural network architectures that operate on subspaces (as opposed to vectors), allowing for downstream evaluations of our novel representation."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1608.04207 .", "citeRegEx": "Adi et al\\.,? 2016", "shortCiteRegEx": "Adi et al\\.", "year": 2016}, {"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "In* SEM 2013: The Second Joint Conference on Lexical and Computational Se-", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "Rand-walk: A latent variable model approach to word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."], "venue": "arXiv preprint arXiv:1502.03520 .", "citeRegEx": "Arora et al\\.,? 2015", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "journal of machine learning research 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Two step cca: A new spectral method for estimating vector models of words", "author": ["Paramveer Dhillon", "Jordan Rodu", "Dean Foster", "Lyle Ungar."], "venue": "arXiv preprint arXiv:1206.6403 .", "citeRegEx": "Dhillon et al\\.,? 2012", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of NAACL-", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Geometry of compositionality", "author": ["Hongyu Gong", "Suma Bhat", "Pramod Viswanath."], "venue": "Association for Advancement of Artificial Intelligence (AAAI) .", "citeRegEx": "Gong et al\\.,? 2017", "shortCiteRegEx": "Gong et al\\.", "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "arXiv preprint arXiv:1602.03483 .", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1404.2188 .", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."], "venue": "arXiv preprint arXiv:1606.04640 .", "citeRegEx": "Kenter et al\\.,? 2016", "shortCiteRegEx": "Kenter et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in neural information processing systems. pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, pages 641\u2013648.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Geometry of polysemy", "author": ["Jiaqi Mu", "Suma Bhat", "Pramod Viswanath."], "venue": "arXiv preprint arXiv:1610.07569 .", "citeRegEx": "Mu et al\\.,? 2016", "shortCiteRegEx": "Mu et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on empirical", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["Karl Stratos", "Michael Collins", "Daniel Hsu."], "venue": "Proceedings of ACL. pages 1282\u20131291.", "citeRegEx": "Stratos et al\\.,? 2015", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "arXiv preprint arXiv:1511.08198 .", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "arXiv preprint", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 21, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 20, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 7, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 12, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 8, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 19, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 23, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 18, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 5, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 25, "context": "Real-valued word representations have brought a fresh approach to classical problems in NLP, recognized for their ability to capture linguistic regularities: similar words tend to have similar representations; similar word pairs tend to have similar difference vectors (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010; Collobert et al., 2011; Huang et al., 2012; Dhillon et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Arora et al., 2015; Stratos et al., 2015).", "startOffset": 269, "endOffset": 515}, {"referenceID": 15, "context": ", convolutional neural networks (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (Sutskever et al.", "startOffset": 32, "endOffset": 70}, {"referenceID": 13, "context": ", convolutional neural networks (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (Sutskever et al.", "startOffset": 32, "endOffset": 70}, {"referenceID": 24, "context": ", 2016) and recursive neural networks (Socher et al., 2013)).", "startOffset": 38, "endOffset": 59}, {"referenceID": 27, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 0, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 14, "context": "words in this sentence (Wieting et al., 2015; Adi et al., 2016; Kenter et al., 2016).", "startOffset": 23, "endOffset": 84}, {"referenceID": 26, "context": "Recently, Wieting et al. (2015); Adi et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "(2015); Adi et al. (2016)", "startOffset": 8, "endOffset": 26}, {"referenceID": 10, "context": "The geometry is motivated by (Gong et al., 2017; Mu et al., 2016) where an interesting phenomenon is observed \u2013 the local context of a given word/phrase can be well represented by a low rank subspace.", "startOffset": 29, "endOffset": 65}, {"referenceID": 22, "context": "The geometry is motivated by (Gong et al., 2017; Mu et al., 2016) where an interesting phenomenon is observed \u2013 the local context of a given word/phrase can be well represented by a low rank subspace.", "startOffset": 29, "endOffset": 65}, {"referenceID": 23, "context": "Assembling successful distributional word representations (for example, GloVe (Pennington et al., 2014)) into sentence representations is an active research topic.", "startOffset": 78, "endOffset": 103}, {"referenceID": 19, "context": "Different from previous studies (for example, doc2vec (Mikolov et al., 2013), skip-thought vectors (Kiros et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 16, "context": ", 2013), skip-thought vectors (Kiros et al., 2015), Siamese CBOW (Kenter et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 14, "context": ", 2015), Siamese CBOW (Kenter et al., 2016)), our main contribution is to represent sentences using non-vector space representations: a sentence can be well represented by the subspace spanned by the context word vectors \u2013 such a method naturally builds on any word representation method.", "startOffset": 22, "endOffset": 43}, {"referenceID": 19, "context": "Due to the widespread use of word2vec and GloVe, we use their publicly available word representations \u2013 word2vec(Mikolov et al., 2013) trained using Google News1 and GloVe (Pennington et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 23, "context": ", 2013) trained using Google News1 and GloVe (Pennington et al., 2014) trained using Common Crawl2 \u2013 to test our observations.", "startOffset": 45, "endOffset": 70}, {"referenceID": 17, "context": "(of GloVe and skipgram) where we use the average of word vectors), doc2vec (D2V) (Le and Mikolov, 2014) and", "startOffset": 81, "endOffset": 103}, {"referenceID": 16, "context": ", skip-thought vectors (ST) (Kiros et al., 2015), Siamese CBOW", "startOffset": 28, "endOffset": 48}, {"referenceID": 14, "context": "(SC) (Kenter et al., 2016)).", "startOffset": 5, "endOffset": 26}, {"referenceID": 28, "context": "In order to enable a fair comparison, we use the Toronto Book Corpus (Zhu et al., 2015) to train word embeddings.", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "In our experiment, we adapt the same setting as in (Kenter et al., 2016) where we use skip-gram", "startOffset": 51, "endOffset": 72}, {"referenceID": 19, "context": "(Mikolov et al., 2013) of and GloVe (Pennington et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": ", 2013) of and GloVe (Pennington et al., 2014) to train a 300-dimensional word vectors for the words that occur 5 times or more in the training corpus.", "startOffset": 21, "endOffset": 46}, {"referenceID": 9, "context": "While the current approach relies on the pre-trained word representations, the joint learning of both word and sentence representations and in conjunction with supervised datasets such the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) is left to future research.", "startOffset": 216, "endOffset": 243}], "year": 2017, "abstractText": "Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences \u2013 the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.", "creator": "LaTeX with hyperref package"}}}