{"id": "1704.05693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Unsupervised Creation of Parameterized Avatars", "abstract": "We examine the problem of mapping an input image to a bound pair, consisting of a vector of parameters and an image generated from the vector of parameters using a graphical engine. The aim of mapping is to make the output image as similar to the input image as possible. No monitoring in the form of matching inputs and outputs is given during the training.", "histories": [["v1", "Wed, 19 Apr 2017 11:19:45 GMT  (7045kb,D)", "http://arxiv.org/abs/1704.05693v1", null], ["v2", "Sun, 9 Jul 2017 16:10:53 GMT  (7045kb,D)", "http://arxiv.org/abs/1704.05693v2", "v2 -- a change in the references due to a request from authors"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["lior wolf", "yaniv taigman", "adam polyak"], "accepted": false, "id": "1704.05693"}, "pdf": {"name": "1704.05693.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Creation of Parameterized Avatars", "authors": ["Lior Wolf", "Yaniv Taigman", "Adam Polyak"], "emails": [], "sections": [{"heading": null, "text": "This learning problem extends two literature problems: unsupervised domain adaptation and cross domain transfer. We define a generalization bound that is based on discrepancy, and employ a GAN to implement a network solution that corresponds to this bound. Experimentally, our method is shown to solve the problem of automatically creating avatars."}, {"heading": "1. Introduction", "text": "The artist Hanoch Piven creates caricatures by arranging household items and scrap material in a frame and photographing the result, see Fig. 1(a). How can a computer create such images? Given a training set consisting of Piven\u2019s images, Generative Adversarial Networks (GANs) can be used to create images that are as indistinguishable as possible from the training set. However, common sense tells us that for any reasonably sized training set, without knowledge about the physical world, the generated images would be easily recognized by humans as being synthetic. As a second motivating example, consider the problem of generating computer avatars based on the user\u2019s appearance. In order to allow the avatars to be easily manipulated, each avatar is represented by a set of \u201cswitches\u201d (parameters) that select, for example, the shape of the nose, the color of the eyes and the style of hair, all from a predefined set of options created by artists. Similar to the first example, the visual appearance of the avatar adheres to a set of constraints. Once the set of parameters is set, the avatar can be rendered\nin many variations (Fig. 1(b)). The goal of this work is to learn to map an input image to two tied outputs: a vector in some parameter space and the image generated by this vector. While it is sufficient to recover just the vector of parameters and then generate the image, a non-intuitive result of our work is that it is preferable to recover the analog image first. In any case, the mapping between the input image and either of the outputs should be learned in an unsupervised way due to the difficulty of obtaining supervised samples that map input images to parameterized representations. In avatar creation, it is time\n1\nar X\niv :1\n70 4.\n05 69\n3v 1\n[ cs\n.C V\n] 1\n9 A\npr 2\nconsuming for humans to select the parameters that represent a user, even after considerable training. The selected parameters are also not guaranteed to be the optimal depiction of that user. Therefore, using unsupervised methods is both more practical and holds the potential to lead to more accurate results.\nIn addition, humans can learn to create parameterized analogies without using matching samples. Understanding possible computational processes is, therefore, an objective of AI, and is the research question addressed. Our contributions are therefore as follows: (i) we present a highly applicable and, as far as we know, completely unexplored vision problem; (ii) the new problem is placed in the mathematical context of other domain shift problems; (iii) a generalization bound for the new problem is presented; (iv) an algorithm that matches the terms of the generalization bound is introduced; (v) the qualitative and quantitative success of the method further validates the non-intuitive path we take and (vi) the new method is shown to solve the parameterized avatar creation problem."}, {"heading": "1.1. Background", "text": "Generative Adversarial Networks GAN [8] methods train a generator network G that synthesizes samples from a target distribution, given noise vectors, by jointly training a second network d. The specific generative architecture we employ is based on the architecture of [20]. Since the image we create is based on an input and not on random noise, our method is related to Conditional GANs, which employ GANs in order to generate samples from a specific class [17], based on a textual description [21], or to invert mid-level network activations [3]. The CoGAN method [15], like our method, generates a pair of tied outputs. However, this method generates the two based on a random vector and not on an input image. More importantly, the two outputs are assumed to be similar and their generators (and GAN discriminators) share many of the layers. In our case, the two outputs are related in a different way: a vector of parameters and the resulting image. The solutions are also vastly different.\nA recent work, which studied the learning of 3D structure from images in an unsupervised manner, shares some of computational characteristics with our problem [11]. The most similar application to ours, involves a parametrization of a 3D computer graphics object with 162 vertices, each moving along a line, a black-box camera projecting from 3D to 2D and a set of 2D images without the corresponding 3D configuration. The system then learns to map 2D images to the set of vertices. This setting shares with us the existence of a fixed mapping from the vector of parameters to the image. In our case, this mapping is given as a neural network that will be termed e, in their case, it is given as a black box, which, as discussed in Sec. 5 is a solvable chal-\nlenge. A more significant difference is that in their case, the images generated by the fixed mapping are in the same domain as the input, while in our case it is from a different domain. The method employed in [11] completely differs from ours and is based on sequential generative models [9]. Distances between distributions In unsupervised learning, where one cannot match between an input sample and its output, many methods rely on measuring distances between distributions. Specifically, GANs were recently shown [6] to implement the theoretical notion of discrepancies.\nDefinition 1 (Discrepancy distance). Let C be a class of functions from A to B and let ` : B \u00d7 B \u2192 R+ be a loss function over B. The discrepancy distance discC between two distributions D1 and D2 over A is defined as discC(D1, D2) = supc1,c2\u2208C\n\u2223\u2223\u2223RD1 [c1, c2] \u2212 RD2 [c1, c2]\u2223\u2223\u2223, where RD[c1, c2] = Ex\u223cD [`(c1(x), c2(x))].\nImage synthesis with CNNs The supervised network of [4] receives as input a one-hot encoding of the desired model as well as view parameters and a 3D transformation and generates the desired view of a 3D object.\nDC-IGN [13] performs a similar task with less direct supervision. The training set of this method is stratified but not necessarily fully labeled and is used to disentangle the image representation in an encoder-decoder framework. Pix2pix [10] maps an image to another domain. This methods is fully supervised and requires pairs of matching samples from the two domains. Style transfer In these methods [7, 24, 12], new images are synthesized by minimizing the content loss with respect to one input sample and the style loss with respect to one or more input samples. The content loss is typically the encoding of the image by a network training for an image categorization task, similar to our work. The style loss compares the statistics of the activations in various layers of the neural network. We do not employ style losses in our method and more significantly, the problem that we solve differs. This is not only because style transfer methods cannot capture semantics [22], but also because the image we generate has to adhere to specific constraints. Similarly, the work that has been done to automatically generate sketches from images, e.g., [25, 27], does not apply to our problem since it does not produce a parameter vector in a semantic configuration space. The literature of face sketches also typically trains in a supervised manner that requires correspondences between sketches and photographs."}, {"heading": "2. Problem Formulation", "text": "Problems involving domain shift receive an increasing amount of attention, as the field of machine learning moves its focus away from the vanilla supervised learning scenarios to new combinations of supervised, unsupervised and\ntransfer learning. In this section, we formulate the new computational problem that we pose \u201cTied Output Synthesis\u201d (TOS) and put it within a theoretical context. In the next section, we redefine the problem as a concrete deep learning problem. In order to maximize clarity, the two sections are kept as independent as possible."}, {"heading": "2.1. Related Problems", "text": "In the unsupervised domain adaptation problem [2, 16, 1], the algorithm trains a hypothesis on a source domain and the hypothesis is tested on a different target domain. The algorithm is aided with a labeled dataset of the source domain and an unlabeled dataset of the target domain. The conventional approach to dealing with this problem is to learn a feature map that (i) enables accurate classification in the source domain and (ii) captures meaningful invariant relationships between the source and target domains.\nLet X be the input space and Y be the output space (the mathematical notation is also conveniently tabulated in the appendix). The source domain is a distribution DS over X along with a function yS : X \u2192 Y . Similarly, the target domain is specified by (DT , yT ). Given some loss function ` : Y\u00d7Y \u2192 R+ The goal is to fit a hypothesis h from some hypothesis space H, which minimizes the Target Generalization Risk, RDT [h, yT ]. Where a Generalization Risk is defined as RD[h1, h2] = Ex\u223cD [`(h1(x), h2(x))]. The distributions DS , DT and the target function yT : X \u2192 Y are unknown to the learning algorithm. Instead, the learning algorithm relies on a training set of labeled samples {(x, yS(x))}, where x is sampled from DS as well as on an unlabeled training set of samples x \u223c DT , see Fig. 2(a).\nIn the cross domain transfer problem, the task is to learn a function that maps samples from the input domain X to the output domain Y . It was recently presented in [22], where a GAN based solution was able to convincingly transform face images into caricatures from a specific domain.\nThe training data available to the learning algorithm in the cross domain transfer problem is illustrated in Fig. 2(b). The problem consists of two distributions, D1 and D2, and a target function, y. The algorithm has access to the following two unsupervised datasets: {xi\u223cD1}mi=1 and {y(xj)|xj\u223cD2}nj=1. The goal is to fit a function h = g \u25e6 f \u2208 H that optimizes infh\u2208HRD1 [h, y].\nIt is assumed that: (i) f is a fixed pre-trained feature map and, therefore, H = { g \u25e6 f \u2223\u2223g \u2208 H2} for some hypothesis class H2; and (ii) y is idempotent, i.e, y \u25e6 y \u2261 y. For example, in [22], f is the DeepFace representation [23] and y maps face images to emoji caricatures. In addition, applying y on an emoji gives the same emoji. Note that according to the terminology of [22], D1 and D2 are the source and target distributions respectively. However, the loss RD1 [h, y] is measured over D1, while in domain adaptation, it is measured over the target distribution.\nRecently [5], the cross domain transfer problem was analyzed using the theoretical term of discrepancy. Denoting, for example, y \u25e6D to be the distribution of the y mappings of samples x \u223c D, then the following bound is obtained.\nTheorem 1 (Domain transfer [5]). If ` satisfies the triangle inequality1 andH2 (the hypothesis class of g) is a universal Lipschitz hypothesis class2, then for all h = g \u25e6 f \u2208 H,\nRD1 [h, y] \u2264Ry\u25e6D2 [h, Id] +RD1 [f \u25e6 h, f ] + discH(y \u25e6D2, h \u25e6D1) + \u03bb\n(1)\nHere, \u03bb = minh\u2208H {Ry\u25e6D2 [h, Id] +RD1 [h, y]} and h\u2217 = g\u2217 \u25e6 f is the corresponding minimizer.\nThis theorem matches the method of [22], which is called DTN. It bounds the risk RD1 [h, y], i.e., the expected loss (using `) between the mappings by the ground truth function y and the mapping by the learned function h for samples x \u223c D1. The first term in the R.H.S Ry\u25e6D2 [h, Id] is the LTID part of the DTN loss, which, for the emoji generation application, states that emoji caricatures are mapped to themselves. The second term RD1 [f \u25e6 h, f ] corresponds to the LCONST term of DTN, which states that the DeepFace representations of the input face image and the resulting caricature are similar. The theorem shows that his constancy does not need to be assumed and is a result of the idempotency of y and the structure of h. The third term discH(y \u25e6 D2, h \u25e6 D1) is the GAN element of the DTN method, which compares generated caricatures (h \u25e6D1) to the training dataset of the unlabeled emoji (y \u25e6D2). Lastly, the \u03bb factor captures the complexity of the hypothesis class H, which depends on the chosen architecture of the neural network that instantiates g. A similar factor in the generalization bound of the unsupervised domain adaptation problem is presented in [1]."}, {"heading": "2.2. The Tied Output Synthesis Problem", "text": "The problem studied in this paper, is a third flavor of domain shift, which can be seen as a mix of the two problems: unsupervised domain adaptation and the cross domain transfer problem. Similar to the unsupervised domain transfer problem, we are given a set of supervised labeled samples. The samples cj are drawn i.i.d from some distribution D2 in the space Y2 and are given together with their mappings e(cj) \u2208 Y1. In addition, and similar to the cross domain transfer problem, we are given samples xi \u2208 X\n1For all y1, y2, y3 \u2208 Y it holds that `(y1, y3) \u2264 `(y1, y2) + `(y2, y3). This holds for the absolute loss, and can be relaxed to the square loss, where it holds up to a multiplicative factor of 3.\n2A function c \u2208 C is Lipschitz with respect to `, if there is a constant L > 0 such that: \u2200a1, a2 \u2208 A : `(c(a1), c(a2)) \u2264 L \u00b7 `(a1, a2). A hypothesis class C is universal Lipschitz with respect to ` if all functions c \u2208 C are Lipschitz with some universal constant L > 0. This holds, for example, for neural networks with leaky ReLU activations and weight matrices of bounded norms, under the squared or absolute loss.\ndrawn i.i.d from another distribution D1. The goal is to learn a mapping y : X \u2192 Y2 that satisfies the following condition y \u25e6 e \u25e6 y = y. The hypothesis class contains functions h of the form c \u25e6 g \u25e6 f for some known f for g \u2208 H2 and for c \u2208 H3. f is a pre-learned function that maps the input sample in X to some feature space, g maps from this feature space to the space Y1, and c maps from this space to the space of parameters Y2, see Fig. 2(c) and Fig. 3.\nOur approach assumes that e is prelearned from the matching samples (cj , e(cj)). However, c is learned together with g. This makes sense, since while e is a feedforward transformation from a set of parameters to an output, c requires the conversion of an input of the form g(f(x)) where x \u223c D1, which is different from the image of e for inputs in Y2. The theorem below describes our solution.\nTheorem 2 (Tied output bound). If ` satisfies the triangle inequality and H2 is a universal Lipschitz hypothesis class with respect to `, then for all h = c \u25e6 g \u25e6 f \u2208 H,\nRD1 [e \u25e6 h, e \u25e6 y] \u2264RD1 [e \u25e6 h, g \u25e6 f ] +Re\u25e6y\u25e6D2 [g \u25e6 f, Id] +RD1 [f \u25e6 g \u25e6 f, f ] + discH(e \u25e6 y \u25e6D2, g \u25e6 f \u25e6D1) + \u03bb, (2)\nwhere \u03bb = ming\u2208H2 {Re\u25e6y\u25e6D2 [g \u25e6 f, Id] +RD1 [g \u25e6 f, e \u25e6 y]} and g\u2217 is the corresponding minimizer.\nProof. By the triangle inequality, we obtain: RD1 [e \u25e6 h, e \u25e6 y] \u2264 RD1 [e \u25e6 h, g \u25e6 f ] +RD1 [g \u25e6 f, e \u25e6 y].\nApplying Thm. 1 completes the proof:\nRD1 [g \u25e6 f, e \u25e6 y] \u2264Re\u25e6y\u25e6D2 [g \u25e6 f, Id] +RD1 [f \u25e6 g \u25e6 f, f ] + discH(e \u25e6 y \u25e6D2, g \u25e6 f \u25e6D1) + \u03bb\nThm. 2 presents a recursive connection between the tied output synthesis problem and the cross domain transfer problem. This relation can be generalized for tying even more outputs to even more complex relations among parts of the training data. The importance of having a generalization bound to guide our solution stems from the plausibility of many other terms such as Re\u25e6y\u25e6D2 [e \u25e6 h, g \u25e6 f ] or RD1 [f \u25e6 g \u25e6 f, f \u25e6 e \u25e6 h]. Comparing to Unsupervised Cross Domain Transfer The tied output problem is a specific case of cross domain transfer with Y of the latter being Y1 \u00d7 Y2 of the former. However, this view makes no use of the network e. Comparing Thm. 1 and Tmm. 2, there is an additional term in the second bound: RD1 [e \u25e6 h, g \u25e6 f ]. It expresses the expected loss (over samples from D1) when comparing the result of applying the full cycle of encoding by f , generating an image by g, estimating the parameters in the space Y2 using c, and synthesizing the image that corresponds to these parameters using e, to the result of applying the subprocess that includes only f and g. Comparing to Unsupervised Domain Adaptation Consider the domain X \u222a Y1 and learn the function e\u22121 from this domain toY2, using the samples {(e(cj), cj)|cj \u223c D2}, adapted to xi \u223c D1. This is a domain adaptation problem with DS = e \u25e6 D2 and DT = D1. Our experiments show that applying this reduction leads to suboptimal results. This is expected, since this approach does not make use of the prelearned feature map f . This feature map is not to be confused with the feature network learned in [6], which we denote by p. The latter is meant to eliminate the differences between p \u25e6DS and p \u25e6DT . However, the prelearned f leads to easily distinguishable f \u25e6DS and f \u25e6DT .\nThe unsupervised domain adaptation and the TOS problem become more similar, if one identifies p with the conditional function that applies g \u25e6 f to samples from X and the identity to samples from Y1. In this case, the label predictor of [6] is identified with our c and the discrepancy terms (i.e., the GANs) are applied to the same pairs of distributions. However, the two solutions would still differ since (i) our solution minimizes RD1 [e \u25e6 h, g \u25e6 f ], while in unsupervised domain adaptation, the analog term is minimized over DS = e \u25e6D2 and (ii) the additional non-discrepancy terms\nwould not have analogs in the domain adaptation bounds."}, {"heading": "3. The Tied Output Synthesis Network", "text": "We next reformulate the problem as a neural network challenge. For clarity, this formulation is purposefully written to be independent of the mathematical presentation above. We study the problem of projecting an image in one domain to an image in another domain, in which the images follow a set of specifications. Given a domain, X , a mapping e and a function f , we would like to learn a generative function G such that f is invariant under G, i.e., f \u25e6G = f , and that for all samples x \u2208 X , there exists a configuration u \u2208 Y2 such that G(x) = e(u). Other than the functions f and e, the training data is unsupervised and consists of a set of samples from the source domain X and a second set from the target domain of e, which we call Y1.\nIn comparison to the domain transfer method presented in [22], the domain Y1 is constrained to be the image of a mapping e. DTN cannot satisfy this requirement, since presenting it with a training set t of samples generated by e is not a strong enough constraint. Furthermore, the real-world avataring applications require the recovery of the configuration u itself, which allows the synthesis of novel samples using an extended engine e\u2217 that generates new poses, expressions in the case of face images, etc."}, {"heading": "3.1. The interplay between the trained networks", "text": "In a general view of GANs, assume a loss function `(G, d, x), for some function d that receives inputs in the domainY1. G, which maps an input x to entities inY1, minimizes the following loss: LGAN = maxd\u2212Ex `(G, d, x). This optimization is successful, if for every function d, the expectation of `(G, d, x) is small for the learned G. It is done by maximizing this expectation with respect to d, and minimizing it with respect to G. The two learned networks d and G provide a training signal to each other.\nTwo networks can also provide a mutual signal by collaborating on a shared task. Consider the case in which G and a second function c work hand-in-hand in order to minimize the expectation of some other loss `(G, c, x). In this case, G \u201crelies\u201d on c and minimizes the following expression:\nLc = min c\nEx `(G, c, x). (3)\nThis optimization succeeds if there exists a function c for which, post-learning, the expectation Ex `(G, c, x) is small.\nIn the problem of tied output synthesis, the function e maps entities u in some configuration space Y2 to the target space Y1. c maps samples from Y1 to the configuration space, essentially inverting e. The suitable loss is:\n`e(G, c, x) = \u2016G(x)\u2212 e(c(G(x)))\u20162. (4)\nFor such a problem, the optimal c is given by c\u2217(z) = argminu \u2016z\u2212e(u)\u20162. This implicit function is intractable to compute, and c is learned instead as a deep neural network."}, {"heading": "3.2. The complete network solution", "text": "The learning algorithm is given, in addition to two mappings e and f , a training set s \u2282 X , and a training set t \u2282 Y1. Similar to [22], we define G to be composed out of f and a second function g that maps from the output space of f to T , i.e., G = g \u25e6 f . The e compliance term (Lc of Eq. 3 using `e of Eq. 4) becomes:\nLc = \u2211 x\u2208s \u2016g(f(x))\u2212 e(c(g(f(x))))\u20162 (5)\nIn addition, we minimize LCONST, which advocates that for every input x \u2208 s, f remains unchanged asGmaps it to Y1:\nLCONST = \u2211 x\u2208s \u2016f(x)\u2212 f(G(x))\u20162 (6)\nA GAN term is added to ensure that the samples generated by G are indistinguishable from the set t. The GAN employs a binary classifier network d, and makes use of the training set t. Specifically, the following form of ` is used in LGAN:\n`(G, d, x) = log[1\u2212 d(G(x))] + 1 |t| \u2211 x\u2032\u2208t log[d(x\u2032)]. (7)\nLike [22], the following term encourages G to be the identity mapping for samples from t.\nLTID = \u2211 x\u2208t \u2016x\u2212 g(f(x))\u20162 (8)\nTaken together, d maximizes LGAN , and both g and c minimize Lc+\u03b1LGAN+\u03b2LCONST+\u03b3LTID+\u03b4LTV for some nonnegative weights \u03b1, \u03b2, \u03b3, \u03b4, where LTV, is the total variation loss, which smooths the resulting image z = [zij ] = G(x):\nLTV (z) = \u2211\ni,j\n( (zi,j+1 \u2212 zij)2 + (zi+1,j \u2212 zij)2 ) 1 2\n. The method is illustrated in Fig. 4 and laid out in Alg. 1.\nAlgorithm 1 The TOS training algorithm. 1: Given the function e : Y2 \u2192 Y1, an embedding func-\ntion f , and S \u2282 X , T \u2282 Y1 training sets. 2: Initialize networks c, g and d 3: while iter < numiters do 4: Sample mini-batches s \u2282 S, t \u2282 T 5: Compute feed-forward d(t), d(g(f(s))) 6: Update d by minimizing `(G, d, x) for x \u2208 s . Eq. 7 7: Update g by maximizing `(G, d, x) for x \u2208 s . Eq. 7 8: Update g by minimizing LTID . Eq. 8 9: Update g by minimizing LCONST . Eq. 6 10: Update g by minimizing LTV 11: Compute e(c(z)) by feed-forwarding z := g(f(s)) 12: Update c and g by minimizing Lc . Eq. 5\nIn the context of Thm. 2, the term Lc corresponds to the risk term RD1 [e \u25e6 h, g \u25e6 f ] in the theorem and compares samples transformed by the mapping g \u25e6 f to the mapping of the same samples to a configuration in Y2 using c \u25e6 g \u25e6 f and then to Y1 using e. The term LTID corresponds to the risk Re\u25e6y\u25e6D2 [g \u25e6 f, Id], which is the expected loss over the distribution from which t is sampled, when comparing the samples in this training set to the result of mapping these by g \u25e6 f . The discrepancy term discH(e \u25e6 y \u25e6D2, g \u25e6 f \u25e6D1) matches the LGAN term, which as explained above, measures a distance between two distributions, in this case, e \u25e6 y \u25e6D2, which is the distribution from which the training set t is taken, and the distribution of mappings by g \u25e6 f of the samples s which are drawn from D1."}, {"heading": "4. Experiments", "text": "The Tied Output Synthesis (TOS) method is evaluated on a toy problem of inverting a polygon synthesizing engine and on avatar generation from a photograph for two different CG engines. The first problem is presented as a mere illustration of the method, while the second is an unsolved real-world challenge."}, {"heading": "4.1. Polygons", "text": "The first experiment studies TOS in a context that is independent of f constancy. Given a set of images t \u2208 Y1, and a mapping e from some vector space to Y1, learn a mapping c and a generative function G that creates random images in Y1 that are e-compliant (Eq. 4).\nWe create binary 64\u00d7 64 images of regular polygons by sampling uniformly three parameters: the number of vertices (3-6), the radius of the enclosing circle (15-30), and a rotation angle in the range [\u221210, 10]. Some polygons are shown in Fig. 5(a). 10,000 training images were created and used in order to train a CNN e that maps the three parameters to the output, with very little loss (MSE of 0.1).\nA training set t of a similar size is collected by sampling in the same way. As a baseline method, we employ DC-\nGAN [20], in which the generator function G has four deconvolution layers (the open code of https://github. com/soumith/dcgan.torch is used), and in which the input x is a random vector in [\u22121, 1]100. The results are shown in Fig. 5(b). While the generated images are similar to the class of generated polygons, they are not from this class and contain visible artifacts such as curved edges.\nA TOS is then trained by minimizing Eq. 4 with the additional GAN constraints. The optimization minimizes Lc + \u03b1LGAN, for \u03b1 = 1 (LCONST and LTID are irrelevant to this experiment), and with the input distribution D1 of random vectors sampled uniformly in the [\u22121, 1] hypercube in 100D. The results, as depicted in Fig. 5(c), show that TOS, which enjoys the additional supervision of e, produces results that better fit the polygon class."}, {"heading": "4.2. Face Emoji", "text": "The proposed TOS method is evaluated for the task of generating specification-compliant emoji. In this task, we transfer an \u201cin-the-wild\u201d facial photograph to a set of parameters that defines an emoji. As the unlabeled training data of face images (domain X ), we use a set s of one million random images without identity information. The set t consists of assorted facial avatars (emoji) created by an online service (bitmoji.com). The emoji images were processed by an automatic process that detects, based on a set of heuristics, the center of the irises and the tip of the nose [22]. Based on these coordinates, the emoji were centered and scaled into 152\u00d7 152 RGB images.\nThe emoji engine of the online service is mostly additive. In order to train the TOS, we mimic it and have created a neural network e that maps properties such as gender, length of hair, shape of eyes, etc. into an output image. The architecture is detailed in the appendix.\nAs the function f , we employ the representation layer of the DeepFace network [23]. This representation is 256- dimensional and was trained on a labeled set of four million\nimages that does not intersect the set s. Network c maps a 64\u00d764 emoji to a configuration vector. It contains five convolutional layers, each followed by batch normalization and a leaky ReLU with a leakiness coefficient of 0.2. Network g maps f \u2019s representations to 64 \u00d7 64 RGB images. Following [22], this is done through a network with 9 blocks, each consisting of a convolution, batch-normalization and ReLU. The odd blocks 1,3,5,7,9 perform upscaling convolutions. The even ones perform 1 \u00d7 1 convolutions [14]. Network d takes 152 \u00d7 152 RGB images (either natural or scaled-up emoji) and consists of 6 blocks, each containing a convolution with stride 2, batch normalization, and a leaky ReLU. We set \u03b1 = 0.01, \u03b2 = 100, \u03b3 = 1, \u03b4 = 0.0005 as the tradeoff hyperparameters, after eyeballing the results of the first epoch of a very limited set of experiments.\nFor evaluation purposes only, we employ the benchmark of [22], which contains manually created emoji of 118 random images from the CelebA dataset [26]. The benchmark was created by a team of professional annotators who used the web service that creates the emoji images. Fig. 6 shows side by side samples of the original image, the human generated emoji, the emoji generated by the generator function of DTN [22], and the emoji generated by both the generator G = g \u25e6f and the compound generator e\u25e6 c\u25e6G of our TOS method. As can be seen, the DTN emoji tend to be more informative, albeit less restrictive than the ones created manually. TOS respects the configuration space and creates emoji that are similar to the ones created by the human annotators, but which tend to carry more identity information.\nIn order to evaluate the identifiability of the resulting emoji, the authors of [22] have collected a second example for each identity in the set of 118 CelebA images and a set s\u2032 of 100,000 random face images (unsupervised, without identity), which were not included in s. The VGG face CNN descriptor [19] is then used in order to perform retrieval as follows. For each image x in the manually annotated set, a gallery s\u2032 \u222a x\u2032 is created, where x\u2032 is the other\nimage of the person in x. Retrieval is then performed using VGG faces and either the manually created emoji, G(x), or e(c(G(x))) as the probe.\nIn these experiments, the VGG network is used in order to avoid a bias that might be caused by using f both for training the DTN and the TOS methods and for evaluation. The results are reported in Tab. 1. As can be seen, the G(x) emoji generated by DTN are extremely discriminative and obtain a median rank of 16 in cross-domain identification out of 105 distractors. However, DTNs are not compatible with any configuration vector. In order to demonstrate this, we trained a network c\u0304 that maps emoji images to configurations. When applied to the emoji generated by DTN and transforming the results, using e, back to an emoji, the obtained images are less identifiable than the emoji created manually (Tab. 1, under e(..(x))). By comparison, the median rank of the emoji created by the configuration vector c(G(x)) of TOS is much better than the result obtained by the human annotators. As expected, DTN has more identifiable results than TOS when considering the output of g(f(x)) directly, since TOS has additional terms and the role of LCONST in TOS is naturally reduced.\nThe need to train c and G jointly, as is done in the TOS framework, is also verified in a second experiment, in which we fixed the network c of TOS to be the pretrained network c\u0304. The results of rendering the configuration vector were also not as good as those obtained by the unmodified TOS framework. As expected, querying by G(x) directly, produces results that are between DTN and TOS.\nIt should be noted that using the pretrained c\u0304 directly on inputs faces, leads to fixed configurations (modes), since c\u0304 was trained to map from Y1 and not from X . This is also true when performing the prediction based on f mappings of the input and when training a mapping from X to Y2 under the f distance on the resulting avatar. This situation calls for the use of unsupervised domain adaptation (Sec. 2) to learn a mapping from X to Y2 by adapting a mapping from Y1. Despite some effort, applying the domain adaptation method of [6] did not result in satisfactory results (Tab. 1 and appendix). The best architecture found for this network follows the framework of domain-adversarial neural networks [6]. Our implementation consists of a feature network p that resembles our network c - with 4 convolution layers, a label predictor l which consists of 3 fully connected layers, and a discriminative network d that consists of 2 fully connected layers. The latter is preceded by a gradient reversal layer to ensure that the feature distributions of both domains are made similar. In both l and d, each hidden layer is followed by batch normalization. Human rating Finally, we asked a group of 20 volunteers to select the better emoji, given a photo from celebA and two matching emoji: one created by the expert annotators and one created by TOS (e\u25e6c\u25e6G). The raters were told that\n(a) (b) (c)\n\u2190 Figure 6. Shown, side by side, are (a) sample images from the CelebA dataset. (b) emoji, from left to right: the images created manually using a web interface (for evaluation only), the result of DTN, and the two results of our TOS: G(x) and then e(c(G(x))). (c) VR avatar results: DTN, the two TOS results, and a 3D rendering of the resulting configuration file. See Tab. 1 for retrieval performance. The results of DANN [6] are not competitive and are shown in the appendix.\n\u2191Figure 7. Multi-image results on Facescrub. Shown, side by side, are (i) the image selected to create the TOS and the DTN emoji, (ii) the DTN emoji, and (iii) the TOS emoji, obtained by e \u25e6 c \u25e6 g \u25e6 f . See also appendix.\nthey are presented with the results of two algorithms for automatically generating emoji and are requested to pick their favorable emoji for each image. The images were presented printed out, in random order, and the raters were given an unlimited amount of time. In 39.53% of the answers, the TOS emoji was selected. This is remarkable considering that in a good portion of the celebA emoji, the TOS created very dark emoji in an unfitting manner (since f is invariant to illumination and since the configuration has many more dark skin tones than lighter ones).\nTOS, therefore, not only provides more identifiable emoji, but is also very close to be on par with professional annotators. It is important to note that we did not compare to DTN in this rating, since DTN does not create a configuration vector, which is needed for avatar applications (Fig 1(b)). Multiple Images Per Person Following [22], we evaluate the results obtained per person and not just per image on the Facescrub dataset [18]. For each person q, we considered the set of their images Xq , and selected the emoji that was most similar to their source image, i.e., the one for which: argminx\u2208Xq ||f(x) \u2212 f(e(c(G(x))))||. The qualitative re-\nsults are appealing and are shown in Fig. 9."}, {"heading": "4.3. VR Avatars", "text": "We next apply the proposed TOS method to a commercial avatar generator engine, see Fig. 6(c). We sample random parameterizations and automatically align their frontally-rendered avatars into 64\u00d764 RGB images to form the training set t. We then train a CNN e to mimic this engine and generate such images given their parameterization. Using the same architectures and configurations as in Sec. 4.2, including the same training set s, we train g and c to map natural facial photographs to their engine-compliant set of parameters. We also repeat the same identification experiment and report median rankings of the analog experiments, see Tab. 1(right). The 3D avatar engine is by design not as detailed as the 2D emoji one, with elements such as facial hair still missing and less part shapes available. In addition, the avatar model style is more generic and focused on real time puppeteering and not on cartooning. Therefore, the overall numbers are lower for all methods, as expected. TOS seems to be the only method that is able to produce identifiable configurations, while the other methods lead to\nranking that is close to random."}, {"heading": "5. Conclusions", "text": "With the advent of better computer graphics engines and the plethora of available models, and the ability of neural networks to compare cross-domain entities, the missing element for bridging between computer vision and computer graphics is the ability to link image data to a suitable parametrization. The previously presented DTN method showed a remarkable capability to create analogies without explicit supervision. For example, highly identifiable emoji were generated. However, emoji applications call for parametrized characters, which can then be transformed by artists to other views and new expressions, and the emoji created by DTN cannot be converted to a configuration. The TOS method that we present is able to generate identifiable emoji that are coupled with a valid configuration vector.\nWhile TOS was presented in a way that requires the rendering function e to be differentiable, working with blackbox renderers using gradient estimation techniques is a common practice, e.g., in Reinforcement Learning."}, {"heading": "A. Summary of Notations", "text": "Tab. 2 itemizes the symbols used in this work. Fig. 2,3,4 of the main text illustrate many of these symbols."}, {"heading": "B. DANN results", "text": "Fig. 8 shows side by side samples of the original image and the emoji generated by the method of [6]. As can be seen, these results do not preserve the identity very well, despite considerable effort invested in finding suitable architectures."}, {"heading": "C. Multiple Images Per Person", "text": "Following [22], we evaluate the visual quality that is obtained per person and not just per image, by testing TOS on the Facescrub dataset [18]. For each person p, we considered the set of their images Xp, and selected the emoji that was most similar to their source image, i.e., the one for which:"}, {"heading": "D. Detailed Architecture of the Various Networks", "text": "In this section we describe the architectures of the networks used in for the emoji and avatar experiments.\nD.1. TOS\nNetwork g maps DeepFace\u2019s 256-dimensional representation [23] into 64\u00d764 RGB emoji images. Following [22], this is done through a network with 9 blocks, each consisting of a convolution, batch-normalization and ReLU, except the last layer which employs Tanh activation. The odd blocks 1,3,5,7,9 perform upscaling convolutions with 512- 256-128-64-3 filters respectively of spatial size 4 \u00d7 4. The even ones perform 1\u00d7 1 convolutions [14]. The odd blocks use a stride of 2 and padding of 1, excluding the first one which does not use stride or padding.\nNetwork e maps emoji parameterization into the matching 64 \u00d7 64 RGB emoji. The parameterization is given as binary vectors in R813 for emojis; Avatar parameterization is in R354. While there are dependencies among the various dimensions (an emoji cannot have two hairstyles at once), the binary representation is chosen for its simplicity and generality. e is trained in a fully supervised way, using pairs of matching parameterization vectors and images in a supervised manner.\nThe architecture of e employs five upscaling convolutions with 512-256-128-64-3 filters respectively, each of spatial size 4 \u00d7 4. All layers except the last one are batch normalized followed by a ReLU activation. The last layer is followed by Tanh activation, generating an RGB image with values in range [\u22121, 1]. All the layers use a stride of 2 and padding of 1, excluding the first one which does not use stride or padding.\nNetwork d takes 152 \u00d7 152 RGB images (either natural or scaled-up emoji) and outputs log-probabilities predicting if the image is fake or real. It consists of 6 blocks, each containing a convolution with stride 2, batch normalization, and a leaky ReLU with leakiness coefficient of 0.2. Each block contains 64-128-256-512-512-3 filters respectively. As before, the last layer does not employ batch normalization and ReLU.\nNetwork cmaps a 64\u00d764 emoji to parameterization vector. It contains five convolutional layers, each followed by batch normalization and a leaky ReLU with a leakiness coefficient of 0.2. Each layer contains 64-128-256-512-813 filters respectively. The last layer is followed by Tanh activation, generating a parameterization vector with values in range [\u22121, 1].\nThe networks used for the synthetic polygon experiment are somewhat simpler: g has the same structure of as in the emoji experiment excluding the even convolutions i.e., it does not contain the 1\u00d71 convolutions. The architecture of d is unchanged. Finally, the architectures of e and c are updated to match the synthetic experiment parameterization. e is changed to map a parameterization vectors in R3 to RGB images, and c is trained to predict such a vector.\nD.2. DANN\nIn the domain adaptation experiments, network p extracts 2048-dimensional feature vectors from 64\u00d7 64 RGB images. It resembles the structure of network c - with 4 convolution layers. Each convolution is with 64-128-256-512 filters respectively. The last convolutional layer employs a stride of 1 instead of 2 and does not use batch-normalized or leaky ReLU. Finally, the network output is flattened to 1-dimensional feature vector.\nThe label prediction network l accepts as input feature vectors generated by p and outputs emoji parameterization vectors matching the input image. It consists of 3 fully connected layers. Each hidden layer is followed by batchnormalization and leaky ReLU activation. The last layer is followed by Tanh activation. The hidden layers contain 1024 and 512 units respectively.\nThe discriminator d predicts the input image domain given its feature vector. It consists of two fully connected layers with 512 hidden units. The hidden layer is followed by batch normalization and leaky ReLU activations. It is preceded by a gradient reversal layer to ensure that the feature distributions of both domains are similar. The last layer is followed by Sigmoid activation, predicting the input image domain."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study the problem of mapping an input image to a<lb>tied pair consisting of a vector of parameters and an image<lb>that is created using a graphical engine from the vector of<lb>parameters. The mapping\u2019s objective is to have the output<lb>image as similar as possible to the input image. During<lb>training, no supervision is given in the form of matching<lb>inputs and outputs. This learning problem extends two literature problems:<lb>unsupervised domain adaptation and cross domain trans-<lb>fer. We define a generalization bound that is based on dis-<lb>crepancy, and employ a GAN to implement a network so-<lb>lution that corresponds to this bound. Experimentally, our<lb>method is shown to solve the problem of automatically cre-<lb>ating avatars.", "creator": "LaTeX with hyperref package"}}}