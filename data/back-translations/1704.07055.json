{"id": "1704.07055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "k-FFNN: A priori knowledge infused Feed-forward Neural Networks", "abstract": "Recursive neural networks (RNN) are widely used via feed-forward neural networks (FFNN) because of their inherent ability to capture temporal relationships that exist in sequential data such as language. However, this aspect of RNN is particularly beneficial when there is no a priori knowledge of the temporal relationships within the data. However, RNNN require large amounts of data to capture these temporal relationships, limiting its advantage in scenarios with limited resources. The aim of this work is to explore k-FFNN, namely an FFNN architecture that can use the a priori temporal knowledge in an FFNN architecture (b) how FFNN behaves when this knowledge of temporal relationships within the data sequence (provided it is available) is made available during the training.", "histories": [["v1", "Mon, 24 Apr 2017 06:54:23 GMT  (169kb,D)", "http://arxiv.org/abs/1704.07055v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sri harsha dumpala", "rupayan chakraborty", "sunil kumar kopparapu"], "accepted": false, "id": "1704.07055"}, "pdf": {"name": "1704.07055.pdf", "metadata": {"source": "CRF", "title": "K-FFNN: USING A PRIORI KNOWLEDGE IN FEED-FORWARD NEURAL NETWORKS", "authors": ["Sri Harsha Dumpala", "Rupayan Chakraborty", "Sunil Kumar Kopparapu"], "emails": ["d.harsha@tcs.com", "rupayan.chakraborty@tcs.com", "sunilkumar.kopparapu@tcs.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "Artificial neural networks are extensively used in all types of classification problems [1]. Speech technologies and applications are not exception to that. Subsequent advancements in the field of neural networks was adapted to build better speech processing systems [2]. Recurrent neural network (RNN) is one such advancement in neural networks, which is being used extensively to solve various problems in speech processing [3, 4]. Among those, audio (speech) emotion recognition\nis one of the latest developments which is an integral part of Human Computer Interaction (HCI) system. RNN has been successfully applied in speech emotion recognition [5].\nIt is evident that a feed-forward neural network (FFNN) discriminatingly learns the patterns within the inputs, even from a low resource dataset [6]. But they are not designed to learn sequential relationships within the data. On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations. Figure 1(a) shows the general architecture of a FFNN and Figure 1(b) shows the structure of a RNN. The essential difference between the two is the self loop within the hidden layer which is useful in capturing the unseen temporal relationship that might exist in the training data. The general structure of an FFNN as shown in Figure 1(a), consists of three layers i.e., input layer, hidden layer and output layer. The input data or features are fed to the input layer which pass through the hidden layer to the output layer. Here the input features or the posterior probabilities pass from the input layer to the hidden layer and then to the output layer but never in backward direction. Hence, the name feed-forward neural network. In a FFNN, a mapping is obtained between the input features and the output values in a supervised learning condition. By design in FFNN, no information regarding the sequence in which the inputs are fed to the network is captured. In FFNN, all the input data sequences are considered independent of each other. On the other hand a RNN network (see Figure 1(b)) is similar to FFNN except for the feedback loop in the hidden layer. This ensures the capture of temporal information in the sequence of inputs along with the mapping between the input and output is also captured.\nIn this paper we explore the use of knowledge regarding the temporal relationships within the sequence of training data while using a FFNN architecture for a limited resource scenario. Knowing that an RNN architecture is capable of inherently learning the temporal relationships that exist in the sequential data, we use RNN to automatically capture that information. However, this aspect of RNN is useful especially when there is no a priori knowledge about the temporal correlations within the data. But to learn temporal correlations au-\nar X\niv :1\n70 4.\n07 05\n5v 1\n[ cs\n.L G\n] 2\n4 A\npr 2\n01 7\ntomatically, substantial amount of training data are required.\nWhat if a priori knowledge of the temporal relationships within the data sequences are known for limited samples, can a FFNN perform similar as an RNN?\nIn this work, a FFNN architecture which can use a priori knowledge of temporal correlationships has been explored, and we call it k-FFNN (short form of knowledge infused FFNN). In particular, we capture the relationship between FFNN and RNN, and then subsequently show through extensive experiments that the knowledge of temporal relationship can be infused to improve the performance of FFNN in a way that resembles RNN. Using MediaEval 2016 audio data (Emotional Impact of Movies task)[10]), we conduct several experiments to establish that the performance of k-FFNN is comparable to RNN and in some scenarios k-FFNN outperform RNN. The main contributions of the paper are (a) incorporation of a priori temporal/sequential knowledge in FFNN to construct a k-FFNN and (b) experimentally showing that not only the performance of k-FFNN is as good as RNN, but also better when there is small amount of training data. The paper is organized as follows. Section 2 presents the hypothesis we make with some theoretical representations. In Section 3, we discuss the dataset used to validate our hypothesis. Section 4 describes the experiments conducted with an analysis. We conclude in Section 5."}, {"heading": "2. HYPOTHESIS", "text": "We start off with the hypothesis\nFFNN infused with prior knowledge about temporal relationship between data is similar to RNN in terms of performance\nAs seen in Figure 1 the primary difference between an RNN and a FFNN is the presence of the hidden layer feedback self loop in RNN which adds memory to the RNN network over time. The question that we are addressing is if a regular FFNN fed with the sequence based a priori knowledge (i.e. k-FFNN) can perform as well as an RNN. In other words,\nif we had some a priori knowledge about the sequence can we use it without depending on RNN to learn it through its hidden layer feedback loop. This is very useful especially in the scenarios where the training data is sparse plus when we are aware of the sequential relationship between data a priori. We validate the hypothesis that the performance of k-FFNN and RNN are similar through an extensive experimentation using the MediaEval dataset."}, {"heading": "2.1. Background", "text": "We validate our hypothesis by considering a simple network configuration and derive a set of expressions that show the relationship between k-FFNN and RNN. For sake of simplicity, we consider a 4\u2212 2\u2212 1 (input-hidden-output nodes) network configuration. Additionally, we consider a data sequence of length 3. Consider the data shown in Table 1. More elaborately, if each ~gi,j was of dimension 4 then we would have Table 2 used as the input-output data to train RNN. We as-\nsume that there exists some temporal relationship between ~g1,1, ~g1,2, ~g1,3 and ~g2,1, ~g2,2, ~g2,3, which can be captured as shown in Table 3 . Namely, the output v1 associated with ~g1,1, ~g1,2, ~g1,3 is actually f(1)v1, f(2)v1, f(3)v1 instead of v1, v1, v1. This f() is the mode in which the a priori temporal knowledge existing between ~g1,1, ~g1,2, ~g1,3 is infused into the training set. Note that a FFNN that uses training data as shown in Table 3 is what we call k-FFNN.\nFor a 4\u22122\u22121 k-FFNN configuration, the model would be represented by a total of (4\u00d72) + (2\u00d71) = 10 variables that represent the network. Namely, [(ih)wij ]4\u00d72 the weights connecting the input to the hidden layer and [(ho)wij ]2\u00d71. While in case of RNN the model consists of not only [(ih)wij ]4\u00d72 the weights connecting the input to the hidden layer and [(ho)wij ]2\u00d71 the weights connecting the hidden and the output layer but also [(hh)wij ]2\u00d72 the feedback connection between the hidden layers. So in case of RNN, it is modeled by (4 \u00d7 2) + (2 \u00d7 2) + (2 \u00d7 1) = 14 variable. The input data remaining the same, the differences in k-FFNN and RNN is captured in Table 4.\nAssuming the same initial weights for both k-FFNN and RNN, we elaborate the process of how the weights (or the\nmodel) gets updated as it is trained. We assume the usual back-propagation based weights update. In case of FFNN Algorithm 1 FFNN Training\nGiven: Input-output pairs (Table 3) Given: The k-FFNN configuration (4\u2212 2\u2212 1) Given: The initial random weights; [(ih)w]4\u00d72, [(ho)w]2\u00d71\nfor i = 1 : 3, l = 1 : 2 pick the input-output pair (~gli, f(i)vl) do\nfor k = 1 : 2 do Compute (1), namely, hk = 1\n1+exp \u2212\u03bb(\n\u22114 j=1 g j il (ih)wjk)\nend for\nCompute (2), namely, oi = 1 1+exp\u2212\u03bb( \u22112 k=1 hk (ho)wk)\nCompute the error (3), namely, = (oi \u2212 f(i)vl)2\nCompute [\u2206(ho)w]2\u00d71and [\u2206(ih)w]4\u00d72 using (6) and (5)\nUpdate weights (4), namely, [(ho)w]2\u00d71 \u2190 [(ho)w]2\u00d71 + [\u2206(ho)w]2\u00d71 [(ih)w]4\u00d72 \u2190 [(ih)w]4\u00d72 + [\u2206(ih)w]4\u00d72\nend for [(ih)w]4\u00d72, [(ho)w]2\u00d71 represent the k-FFNN for the input data (Table 3).\n(see Figure 2), the output of the hidden node is given by\nhk = 1 1 + exp\u2212\u03bb( \u22114 j=1 g j 11 (ih)wjk) (1)\nassuming the sigmoid to be the squashing transfer function and ~g11 is the input and \u03bb is a constant which determines the steepness of the sigmoid. Similarly the output would be\no1 = 1 1 + exp\u2212\u03bb( \u22112 k=1 hk (ho)wk) (2)\nNow the error = (o1 \u2212 f(1)v1)2 (3)\nis used to modify the weights ((ih)w, (ho)w) such that when the same input ~g11 is given to k-FFNN it would reduce (called back propagation of error) generally using the steepest descent algorithm. The weight, (ho)wk for k = 1, 2 and the weight, (ih)wjk for j = 1, 2, 3, 4; k = 1, 2, for the hidden layer are modified as\n(ho)wk \u2190 (ho)wk + \u2206(ho)wk (ih)wjk \u2190 (ih)wjk + \u2206(ih)wjk (4)\nwhere \u2206(ho)wk = \u03b7(o1 \u2212 f(1)v1).hk (5)\n\u2206(ih)wjk = \u03b7(o1 \u2212 f(1)v1).hk(1\u2212 hk)(ih)wjkgj11 (6)\nThe next input, namely ~g12, is passed through the network to obtain hk (1) and o1 (2). Now o1 is used to compute the error (3) followed by weight update (4). This continues for other inputs (Table 3) as shown in Algorithm 1 to complete an epoch. Generally the update happen over several epochs. Algorithm 2 RNN Training\nGiven: Input-output pairs (Table 2) Given: The RNN configuration (4\u2212 2\u2212 1) Given: Initialize weights; [(ih)w]4\u00d72, [(ho)w]2\u00d71 and [(hh)w]2\u00d72\nfor l = 1 : 2 pick the input-output pair (~gl,1, ~gl,2, ~gl,3, vl) do\nfor t = 1 : T do\nfor k = 1 : 2 do Compute (7), namely, htk =\n1\n1+exp \u2212\u03bb(\n\u22114 j=1 (g j lt )((ih)wjk)+ \u22112 h\u2032=1 (h t\u22121 h\u2032 )((hh)w h\u2032k))\nend for end for\nCompute (8), namely, ol = 1 1+exp\u2212\u03bb( \u22112 k=1 (hT k )((ho)wk))\nCompute the error (9), namely, = (ol \u2212 vl)2\nCompute [\u2206(ho)w]2\u00d71,[\u2206(ih)w]4\u00d72 and [\u2206(hh)w]2\u00d72 using (13), (14) and (15)\nUpdate weights (19), namely, [(ho)w]2\u00d71 \u2190 [(ho)w]2\u00d71 + [\u2206(ho)w]2\u00d71 [(ih)w]4\u00d72 \u2190 [(ih)w]4\u00d72 + [\u2206(ih)w]4\u00d72 [(hh)w]2\u00d72 \u2190 [(hh)w]2\u00d72 + [\u2206(hh)w]2\u00d72\nend for [(ih)w]4\u00d72, [(ho)w]2\u00d71, [(hh)w]2\u00d72 represent the RNN for the input data (Table 2)."}, {"heading": "2.2. RNN", "text": "In RNNs, length of the input sequence (here T = 3) apart from the values of the input sequence at each time instant is considered to train the networks. The output of the hidden layer in case of RNNs is given as\nhtk = 1 1 + exp\u2212\u03bb( \u22114 j=1(g j 1t)( (ih)wjk)+ \u22112 h\u2032=1(h t\u22121 h\u2032 )( (hh)wh\u2032k))\n(7) The output of RNN is given by\no1 = 1 1 + exp\u2212\u03bb( \u22112 k=1(h T k )( (ho)wk)) (8)\nThe error in the output estimation is\n= (o1 \u2212 v1)2 (9)\nThe error is backpropagated through the length of the sequence i.e., back propagation through time (BPTT) is used to modify the weights of RNN.\nThe weight modification of RNNs in general is\n\u2206(ih)wij = \u2206 (ho)wij = T=3\u2211 t=1 \u03b4tja t i (10)\nand\n\u2206(hh)wij = T=3\u2211 t=1 \u03b4tja t\u22121 i (11)\nwhere ati is the activation function at the i th unit. So the weight modification for the output layer units is:\n\u2206(ho)wjk = T\u2211 t=1 \u03b4tkh t j (12)\nwhere \u03b4tk = (o t \u2212 vt)\nIn the network architecture considered for this analysis, the output is available only at t = T = 3 is an example. The weight modification at output layer gets modified as\n\u2206(ho)wjk = \u03b7(o1 \u2212 v1).hTj (13) The modification of the input to hidden layer weights is\nobtained as\n\u2206(ih)wij = T\u2211 t=1 \u03b4tjg i 1t (14)\nand the recursive/hidden weights are modified as\n\u2206(hh)wij = T\u2211 t=2 \u03b4tjh t\u22121 i (15)\nhere sigmoid activation function is considered for the hidden layers where \u03b4tk is generally defined as\n\u03b4tj = h t j(1\u2212 htj)\n( (\n1\u2211 k=1 \u03b4tk (ho)wjk + 2\u2211 h\u2032=1 \u03b4t+1h\u2032 (hh)wjh\u2032 ) (16)\nFor the network architecture considered, the above equation gets modified as\n\u03b4Tj = h T j (1\u2212 hTj )(o1 \u2212 v1)(ho)wj1 (17)\nand\n\u03b4tj = h t j(1\u2212 htj)\n( 2\u2211\nh\u2032=1\n\u03b4t+1h\u2032 (hh)wjh\u2032\n) (18)\nSo the weights are modified as\n(ho)wjk \u2190 (ho)wjk + \u2206(ho)wjk (ih)wij \u2190 (ih)wij + \u2206(ih)wij\n(hh)wij \u2190 (hh)wij + \u2206(hh)wij (19)\nRNN is trained using data shown in Table 2. Note that there is no f() in the output and there is an additional weights that represents the RNN (see Table 4). The training process is shown in Algorithm 2."}, {"heading": "3. WORKING SCENARIO AND DATASET PREPARATION", "text": "MediaEval 2016 dataset published for emotional impact of movies task is used in our analysis [10]. This dataset is part of the LIRIS-ACCEDE dataset [11, 12] and consists of video clips of duration 8-12 seconds which have been annotated by viewers for their perceived emotion, in terms of arousal and valance. Note that the perceived emotion annotation is for the entire video clip in terms of valance and arousal value in the range [0, 5].\nIn this paper, to test our hypothesis, the problem of predicting the perceived valence (arousal) value of the viewer after watching the video is considered. Note that, as shown in Figure 3 the valence (arousal) value represent the emotional state of the viewer after having watched the video. It is not immediately clear if the perceived emotion annotated by the viewer is something that is perceived uniformly for the entire duration of the video clip or if the perceived emotion is based on a smaller segment which is the subset of the video clip. According to the [13, 12], each video clip in the dataset has a fade in at the beginning of the video clip and and a fade out at the end of the video clip. This implies that there is a priori knowledge in terms of how the emotion has a temporal relationship within the video clip. This aspect, namely the perceived emotion of a video clip has a fade in and fade out and is not uniform for the entire duration of the video clip motivates us to use MediaEval 2016 dataset to evaluate our hypothesis.\nWe created a dataset of smaller 1 second video clips by segmenting the original video clip. Namely, a 10 second original video clip produced 10 1 second video clips, we retained the temporal relationship between the smaller video clips and the original video clip by naming the video clips appropriately. This enables us to incorporate the temporal correlations, in terms of the fade in and fade out, between the segments to test our hypothesis as mentioned in Section 2.\nIn our experiments we concentrate only on the audio obtained from video clips as the input and the corresponding annotated valence (arousal) values are the desired output. For testing the hypothesis, we first extracted the audio from the original video clip and then segmented the audio into smaller non-overlapping 1 second duration, so a movie clip of n seconds (n \u2208 [8, 12]) duration, resulted in n audio clips each of 1 second duration. For example, if ck is the audio extracted from the original kth video then,\nck = \u2295ni=1cki (20)\nwhere \u2295 represents the concatenation of the audio cki for\ni = 1, \u00b7 \u00b7 \u00b7 , n. Note that there is a temporal relationship between cki\u2019s because they are in a time sequence and are from a single video clip. This construction (20) helps us in building a dataset that can be used to analyze our hypothesis, namely, a FFNN infused with temporal knowledge can work as well as a RNN in terms of its overall performance when used for predicting the estimated emotion of a movie clip.\nLet {ck; ok} be the input output pair; where ok \u2208 [0, 5] can be either valence (vk) or arousal (ak) associated with the audio ck. As seen in Figure 4) the audio ck is made up of the ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn audio sequence. So for a RNN we have the input as ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn while the output is the associated vk (or ak). However, since the input ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn are temporally related, we assumed that the perceived valence vk (or arousal ak) has a bearing on cki. Namely,\nvki = f(i)vk (21)\nFor example, f(i) could be a linear function,\nf(i) = i\u2212 1 n\u2212 1\nsuch that vkn = vk and vk1 = 0. Then each of the audio clips ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn can be assigned a valence namely, (ck1; f(1)vk), (ck2; f(2)vk), \u00b7 \u00b7 \u00b7 , (ckn; f(n)vk). Note that f(i) captures the known a priori temporal knowledge.\nWe use (ck; vk) or equivalently (ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn; vk) to train RNN while we use (ck1, 09vk), (ck2, 1 9vk), \u00b7 \u00b7 \u00b7 , (ckn, 1 1vk) to train a FFNN. Notice that for both FFNN and RNN the input data is the same while the output in case of RNN is known (vk), we construct vki using the prior knowledge for use in FFNN."}, {"heading": "4. EXPERIMENTAL VALIDATION", "text": "In all our experiments, we used the audio extracted from 7571 video clips (MediaEval database) each of around n (n = 8 \u2212 10) seconds duration [14]. The database has a valence (and arousal) value in the range [0, 5] for all the 7571 videos, namely (ck; vk) for k = 1, 2, \u00b7 \u00b7 \u00b7 , 7571 is available. We constructed ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn each of 1 second duration from ck of n second duration for k = 1, 2, \u00b7 \u00b7 \u00b7 , 7571 (see (20)). For each {ckj}k=7571,j=nk=1,j=1 we extracted 384 features, which were used for Interspeech 2009 Emotion Challenge [15] using the openSMILE toolkit [16]. We used WEKA Toolkit [17] to reduce the feature dimension to 21 using feature selection method.\nIf ~gk,j represent the extracted features from ckj then the dataset used in our experiments for RNN training is shown in Table 5, and for FFNN set of experiments we constructed vki as mentioned in (21) resulting in a dataset as shown in Table 6. We used a variety of f(i)\u2019s to capture fade-in and fade-out in our experiments as shown in Table 7."}, {"heading": "4.1. Experimental Analysis", "text": "The performance of the proposed k-FFNN system is compared with the popular RNN architecture i.e., simple RNN, RNN with Long-short-term-memory (LSTM) units and bidirectional RNN with LSTM (BLSTM) units). In this analysis, all k-FFNN and RNN systems are implemented using Keras deep learning toolkit [18]. The architectures of the systems considered in this analysis are shown in Table 8. For all systems, only a single hidden layer is considered. The hidden layer size is selected by varying the number of units from 11 (half of the sum of number of input (i.e., 21) and output units (i.e., 1)) to 44 (twice the sum of number of input and output units) and selecting the number of nodes in the hidden layer which results in the best performance. Sigmoid (S in Table 8) is used as the non-linear activation function on the hidden units. The input layer has 21 linear (L) units and the output layer has a single linear (L) unit.\nThe system performance is evaluated in terms of Mean Squared Error (MSE) and Pearson Correlation Coefficient (PCC). PCC along with MSE is used as a performance metric as PCC provides a better evaluation of the performance of the systems trained on datasets with output values arousal)\ndistributed as shown in Figure 5. It can be observed from Figure 5 that the output values for arousal are concentrated more at a single value (at 1.5) compared to other values. For the considered metrics, lower the MSE values better is the performance of the system and higher the PCC values, better is the performance of the system.\nThe MSE and PCC values are computed at audio clip level for all the systems (k-FFNN and RNNs) to evaluate the performance. In case of RNNs, single output value vk is obtained for the given audio clip (ck) containing the sequence ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn. Subsequently, the computation of MSE and PCC is straight forward in case of RNNs. In case of a k-FFNN system (as shown in Table 6), for each subsegment ck1, ck2, \u00b7 \u00b7 \u00b7 , ckn corresponding to the audio clip ck, arousal/valence value are generated. To compute the MSE and PCC values for each audio clips ck, the output values obtained for each clips are scaled with a value depending on the function selected during training. Then the mean of the values obtained at all subsegments is computed and compared with the original value vk assigned to that audio clip to obtain the MSE and PCC values. If v \u2032 1, v \u2032 2, v \u2032 3, ..., v \u2032\nn are the output obtained for all the audio segment corresponding to the audio clip ck, then\nV \u2032 = n\u2211 i=1 v \u2032 i(1/f(i)) (22)\nis the defined arousal/valence value of the audio clip ck. The MSE and PCC values obtained by considering training sets of different sizes are shown in Figure 6 and Figure 7, respectively for different systems. It can be observed from Figure 6 that the MSE values obtained for k-FFNN (Fn1) is always lower or equal to that of the MSE values obtained for RNNs for all sizes of training set. k-FFNN system performs much better than RNN systems for smaller training dataset. It can be observed that there is a performance improvement of 0.05 (MSE) when 200 training samples are used. Note that the MSE of RNN is 0.977 compared to MSE of 0.927 for kFFNN (for 200 training samples) with function Fn1. However the performance of k-FFNN closer to that of RNN when 6814 (90% of dataset) samples are used for training. The MSE values are lower for FFNN compared to RNN for smaller training sets but are higher when the training set size is increased (MSE is 0.940 for FFNN and 0.953 for RNN when 500 samples are considered and 0.847 for FFNN and 0.820 for RNN when 6814 samples are considered). This shows that the performance of k-FFNN in terms of MSE is better than FFNN and RNN, especially for smaller training set.\nIt can be observed from Figure 7 that the PCC values are consistently higher for k-FFNN when compared to RNNs. Similar to MSE, the variation in PCC values between k-FFNN and RNN is larger for smaller training sets (difference = 0.08 (0.079 for RNN and 0.16 for Fn1) when 200 train samples are considered) and gradually decreases when the size of the training set is increased (difference = 0.048 (0.226 for RNN\nand 0.274 for k-FFNN), when 6814 train samples are considered). The PCC values obtained for FFNN are lower compared to k-FFNN for all sizes of training set. The PCC values are higher for FFNN compared to RNNs for smaller training sets but are lower when size of the training set is increased (PCC is 0.093 for FFNN and 0.079 for RNN when 200 samples are considered and 0.191 for FFNN and 0.226 for RNN when 6814 samples are considered).\nTable 10 shows the MSE and PCC values obtained on arousal values by considering different knowledge infused functions (shown in Table 7) to represent the temporal information. It can be observed from Table 9 that the performance of the k-FFNN system trained by considering Fn1 performs better than the systems trained using Fn2 and Fn3 (both in terms of MSE and PCC). It is to be observed that the performance of the k-FFNN systems developed by considering Fn2 and Fn3 is lower than FFNN. This shows that choosing a proper function (knowledge of sequential temporal correlations between data) to represent the temporal information is critical for the performance of the k-FFNN and any arbitrary function used to represent the temporal information will not improve the performance of k-FFNN but may even degrade the performance.\nTable 10 shows the MSE and PCC values obtained for the\ntask of estimating the valence values for different systems. The MSE and PCC values are listed for systems trained on 6814 utterances. It can be observed that the performance of k-FFNN system (using Fn1) is better than RNN and FFNN systems. But the performance of k-FFNN systems (using Fn2 and Fn3) are lower than RNN and even FFNN. Hence the observations made from the prediction of arousal values is further supported by the results obtained for prediction of valence values."}, {"heading": "5. CONCLUSIONS", "text": "FFNN architecture does not consider the temporal relationship that exits in a data sequence as in case of a speech signal. RNN architecture by its design is able to implicitly learn the temporal correlations that exists between the data sequence. While RNNs are advantageous when (a) one is not explicitly aware of the temporal relationship between the sequential data and (b) when there is a large amount of training data. In this paper, we address the scenario when there is insufficient training data and when a priori temporal knowledge about the training data is explicitly known.\nIn this paper, we have shown how one can infuse explicitly known a priori temporal knowledge about the sequential data to enhance the performance of FFNN architecture. We first compared the differences between a simple RNN and a FFNN and showed that the a priori knowledge can in some sense compensate for the hidden layer feedback weights that contribute in capturing temporal relationship in the training data. This observation, leads us to construct k-FFNN, a knowledge infused FFNN which exploits the known a priori\nsequential knowledge in the training data. This is one of the main contributions of this paper. Based on this observation, we hypothesized that k-FFNN performs as well as an RNN because k-FFNN are able to infuse known knowledge in the data sequence into its architecture. We further showed, experimentally, that the performance of k-FFNN especially for smaller training datasets exceeds the performance of RNN both in terms of the MSE and PCC and the performance of both k-FFNN and RNN level out when amount of training data increases. These experiments validate the hypothesis FFNN infused with prior knowledge about temporal relationship between data is similar to RNN in terms of performance. The essential contribution of this paper is the incorporation of known knowledge, when available, to learn a FFNN without depending on a deep architecture like RNN that requires more samples for better training."}, {"heading": "6. REFERENCES", "text": "[1] Teuvo Kohonen, \u201cAn introduction to neural computing,\u201d Neural Networks, vol. 1, no. 1, pp. 3 \u2013 16, 1988.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.\n[3] A. Graves, A. r. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2013, pp. 6645\u2013 6649.\n[4] Alex Graves and Navdeep Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks.,\u201d in ICML, 2014, vol. 14, pp. 1764\u20131772.\n[5] Felix Weninger, Fabien Ringeval, Erik Marchi, and Bjo\u0308rn W. Schuller, \u201cDiscriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio,\u201d in Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, 2016, pp. 2196\u20132202.\n[6] Simon Haykin, Neural Networks: A Comprehensive Foundation (3rd Edition), Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 2007.\n[7] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams, \u201cNeurocomputing: Foundations of research,\u201d chapter Learning Representations by Back-propagating Errors, pp. 696\u2013699. MIT Press, Cambridge, MA, USA, 1988.\n[8] Paul J. Werbos, \u201cGeneralization of backpropagation with application to a recurrent gas market model,\u201d Neural Networks, vol. 1, no. 4, pp. 339 \u2013 356, 1988.\n[9] Jeffrey L. Elman, \u201cFinding structure in time,\u201d Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[10] \u201cThe 2016 emotional impact of movies task,\u201d http:// www.multimediaeval.org/mediaeval2016/ emotionalimpact/index.html, 2016.\n[11] \u201cLiris-accede database,\u201d http://liris-accede. ec-lyon.fr/database.php, 2016.\n[12] Y. Baveye, E. Dellandra, C. Chamaret, and L. Chen, \u201cLiris-accede: A video database for affective content analysis,\u201d IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 43\u201355, 2015.\n[13] Y. Baveye, E. Dellandra, C. Chamaret, and L. Chen, \u201cFrom crowdsourced rankings to affective ratings,\u201d in 2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), 2014, pp. 1\u20136.\n[14] \u201cMediaeval 2016 proceedings,\u201d http://ceur-ws. org/Vol-1739/MediaEval_2016_paper_6. pdf, 2016.\n[15] Bjo\u0308rn W. Schuller, Stefan Steidl, and Anton Batliner, \u201cThe INTERSPEECH 2009 emotion challenge,\u201d in INTERSPEECH, 2009, pp. 312\u2013315.\n[16] openSMILE, ,\u201d http://www.audeering.com/ research/opensmile, 2014.\n[17] Toolkit WEKA, ,\u201d http://www.cs.waikato. ac.nz/ml/weka/, 2016.\n[18] \u201cFranc\u0327ois chollet \u201dkeras\u201d,\u201d https://github.com/ fchollet/keras/, 2015."}], "references": [{"title": "An introduction to neural computing", "author": ["Teuvo Kohonen"], "venue": "Neural Networks, vol. 1, no. 1, pp. 3 \u2013 16, 1988.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. r. Mohamed", "G. Hinton"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2013, pp. 6645\u2013 6649.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio", "author": ["Felix Weninger", "Fabien Ringeval", "Erik Marchi", "Bj\u00f6rn W. Schuller"], "venue": "Proceedings of the Twenty- Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, 2016, pp. 2196\u20132202.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Networks: A Comprehensive Foundation (3rd Edition), Prentice-Hall, Inc", "author": ["Simon Haykin"], "venue": "Upper Saddle River, NJ,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Neurocomputing: Foundations of research", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "chapter Learning Representations by Back-propagating Errors, pp. 696\u2013699. MIT Press, Cambridge, MA, USA, 1988.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J. Werbos"], "venue": "Neural Networks, vol. 1, no. 4, pp. 339 \u2013 356, 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Liris-accede: A video database for affective content analysis", "author": ["Y. Baveye", "E. Dellandra", "C. Chamaret", "L. Chen"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 1, pp. 43\u201355, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "From crowdsourced rankings to affective ratings", "author": ["Y. Baveye", "E. Dellandra", "C. Chamaret", "L. Chen"], "venue": "2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), 2014, pp. 1\u20136.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "The INTERSPEECH 2009 emotion challenge", "author": ["Bj\u00f6rn W. Schuller", "Stefan Steidl", "Anton Batliner"], "venue": "IN- TERSPEECH, 2009, pp. 312\u2013315.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Artificial neural networks are extensively used in all types of classification problems [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Subsequent advancements in the field of neural networks was adapted to build better speech processing systems [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Recurrent neural network (RNN) is one such advancement in neural networks, which is being used extensively to solve various problems in speech processing [3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 3, "context": "Recurrent neural network (RNN) is one such advancement in neural networks, which is being used extensively to solve various problems in speech processing [3, 4].", "startOffset": 154, "endOffset": 160}, {"referenceID": 4, "context": "RNN has been successfully applied in speech emotion recognition [5].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "It is evident that a feed-forward neural network (FFNN) discriminatingly learns the patterns within the inputs, even from a low resource dataset [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 7, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 8, "context": "On the other side, deep neural networks like RNN has an inherent characteristic of learning and exploiting temporal relationships amongst the sequences [7, 8, 9], however RNN requires large training data to capture those correlations.", "startOffset": 152, "endOffset": 161}, {"referenceID": 9, "context": "This dataset is part of the LIRIS-ACCEDE dataset [11, 12] and consists of video clips of duration 8-12 seconds which have been annotated by viewers for their perceived emotion, in terms of arousal and valance.", "startOffset": 49, "endOffset": 57}, {"referenceID": 4, "context": "Note that the perceived emotion annotation is for the entire video clip in terms of valance and arousal value in the range [0, 5].", "startOffset": 123, "endOffset": 129}, {"referenceID": 10, "context": "According to the [13, 12], each video clip in the dataset has a fade in at the beginning of the video clip and and a fade out at the end of the video clip.", "startOffset": 17, "endOffset": 25}, {"referenceID": 9, "context": "According to the [13, 12], each video clip in the dataset has a fade in at the beginning of the video clip and and a fade out at the end of the video clip.", "startOffset": 17, "endOffset": 25}, {"referenceID": 7, "context": "For testing the hypothesis, we first extracted the audio from the original video clip and then segmented the audio into smaller non-overlapping 1 second duration, so a movie clip of n seconds (n \u2208 [8, 12]) duration, resulted in n audio clips each of 1 second duration.", "startOffset": 197, "endOffset": 204}, {"referenceID": 9, "context": "For testing the hypothesis, we first extracted the audio from the original video clip and then segmented the audio into smaller non-overlapping 1 second duration, so a movie clip of n seconds (n \u2208 [8, 12]) duration, resulted in n audio clips each of 1 second duration.", "startOffset": 197, "endOffset": 204}, {"referenceID": 4, "context": "Let {ck; ok} be the input output pair; where ok \u2208 [0, 5] can be either valence (vk) or arousal (ak) associated with the audio ck.", "startOffset": 50, "endOffset": 56}, {"referenceID": 4, "context": "The database has a valence (and arousal) value in the range [0, 5] for all the 7571 videos, namely (ck; vk) for k = 1, 2, \u00b7 \u00b7 \u00b7 , 7571 is available.", "startOffset": 60, "endOffset": 66}, {"referenceID": 11, "context": "For each {ckj} k=1,j=1 we extracted 384 features, which were used for Interspeech 2009 Emotion Challenge [15] using the openSMILE toolkit [16].", "startOffset": 105, "endOffset": 109}], "year": 2017, "abstractText": "Recurrent neural network (RNN) are being extensively used over feed-forward neural networks (FFNN) because of their inherent capability to capture temporal relationships that exist in the sequential data such as speech. This aspect of RNN is advantageous especially when there is no a priori knowledge about the temporal correlations within the data. However, RNNs require large amount of data to learn these temporal correlations, limiting their advantage in low resource scenarios. It is not immediately clear (a) how a priori temporal knowledge can be used in a FFNN architecture (b) how a FFNN performs when provided with this knowledge about temporal correlations (assuming available) during training. The objective of this paper is to explore k-FFNN, namely a FFNN architecture that can incorporate the a priori knowledge of the temporal relationships within the data sequence during training and compare k-FFNN performance with RNN in a low resource scenario. We evaluate the performance of k-FFNN and RNN by extensive experimentation on MediaEval 2016 audio data (Emotional Impact of Movies task). Experimental results show that the performance of k-FFNN is comparable to RNN, and in some scenarios k-FFNN performs better than RNN when temporal knowledge is injected into FFNN architecture. The main contributions of this paper are (a) fusing a priori knowledge into FFNN architecture to construct a k-FFNN and (b) analyzing the performance of k-FFNN with respect to RNN for different size of training data.", "creator": "LaTeX with hyperref package"}}}