{"id": "1704.07434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Paying Attention to Descriptions Generated by Image Captioning Models", "abstract": "In order to bridge the gap between humans and machines in understanding and describing images, we need further understanding of how humans describe a perceived scene. In this essay, we examine the similarity between highlighted visual attention and object references in scene description constructs. We examine the characteristics of human-written and machine-generated descriptions, and then propose an highlighted captioning model to investigate the benefits of low-threshold references in language models. We learn that (1) humans mention more conspicuous objects earlier than less prominent objects in their descriptions, (2) the better a subtitling model performs, the better attention it has with human descriptions, (3) that the proposed highlighted highlighted highlighted, compared to its basic form, does not significantly improve the MS-COCO database, suggesting that explicit bottom-up appreciation does not help if the task is learned well to match and better data is observed in general terms (4).", "histories": [["v1", "Mon, 24 Apr 2017 19:51:16 GMT  (8097kb,D)", "http://arxiv.org/abs/1704.07434v1", null], ["v2", "Wed, 28 Jun 2017 10:13:45 GMT  (1069kb,D)", "http://arxiv.org/abs/1704.07434v2", null], ["v3", "Fri, 4 Aug 2017 11:24:45 GMT  (1140kb,D)", "http://arxiv.org/abs/1704.07434v3", "To appear in ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r tavakoli", "rakshith shetty", "ali borji", "jorma laaksonen"], "accepted": false, "id": "1704.07434"}, "pdf": {"name": "1704.07434.pdf", "metadata": {"source": "CRF", "title": "Can Saliency Information Benefit Image Captioning Models?", "authors": ["Hamed R. Tavakoli", "Rakshith Shetty", "Ali Borji", "Jorma Laaksonen"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2]. The main goal of these problems is an inference which ends in a human-like response. The nature of such responses often necessitates interaction between several low-level cognitive tasks, e.g., perception and sentence planning in describing images. Measuring the capability of a machine in replicating such interactions is challenging. Although the trivial assessment techniques facilitate understanding the average performance of the algorithms, we yet need more detailed studies to understand specific properties of the existing methods in comparison with a human baseline. In the domain of image description by machines, agreement with visual attention is one such case.\nThere exists various theories about human sentence con-\nstruction and formation. Wundt\u2019s theory of sentence production motivates the role of object\u2019s importance in sentence production. He proposes that, in a free word positioning scenario, not bound by any traditional rule, the words follow each other according to the degree of emphasis on the concepts [51]. This theory implicitly motivates the role of what is later on recognized as saliency. Griffin and Bock [20] found some empirical supporting evidence by showing that while describing scenes, speakers look at an object before naming it within their description. Besides, there exists numerous studies which have utilized attention to analyze human sentence planning and construction in different scenarios including scene describtion. Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56]. Encouraged enough, we lay the foundation of this study on the role of saliency in the construct of image descriptions, where the order of named objects is momentous in a sentence.\nContribution: In this paper, we address two intriguing questions: 1) How well do image descriptions, by humans or models, on a scene agree with saliency?, 2) Can saliency benefit image captioning by machine? Answering the questions, we learn not only about the role of attention in describing images, but also about the quality of human-written descriptions and machine-generated ones. We first study the textual statistics of the sentences by human and machine. Then we investigate the attention correlation in the structure of human-written and machine-generated descriptions. To further evaluate the contribution of low-level cues, we propose a saliency-boosted captioning model and compare it against a set of baseline captioning models."}, {"heading": "2. Related Work", "text": "Image description generation. There exists a wide range of captioning methods and models. They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14]. In-depth study of these different models is beyond the scope of this article and falls within the surveys such as [4]. We, however, briefly address the\n1\nar X\niv :1\n70 4.\n07 43\n4v 1\n[ cs\n.C V\n] 2\n4 A\npr 2\n01 7\nfour models utilized in this study. All the four models follow the popular encoder\u2013decoder approach to captioning, wherein the encoder converts the input image to a fixed size feature vector and the decoder is the language model which takes the feature vector as input to generate a caption. \u201cNeural Talk\u201d [29] utilizes a vanilla recurrent neural architecture for the language model while using CNNs for encoding. \u201cMicrosoft\u201d [17] employs multiple instance learning to learn a visual detector for words in order to utilize them in an exponential language model for sentence generation. The \u201cGoogle\u201d method [54] is a generative deep model based on recurrent architectures, more specifically long short-term memory (LSTM) networks. \u201cAalto\u201d [47] employs object detection to augment features in order to boost the results in a framework similar to \u201cNeural Talk\u201d, utilizing LSTM networks.\nAutomated metrics of description evaluation. The community often favors automated metrics over human evaluation due to their reduced cost, faster processing speed, and replicability. The current popular metrics of evaluation are mostly borrowed or inspired by machine translation. Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53]. Adopting an explicit word-to-word matching, METEOR addresses the weakness of BLEU caused by the lack of recall information. Recently, CIDEr was developed for image description evaluation. It is a similarity-based metric that computes the similarity of sentences by the occurrence of n-grams.\nAttention and language studies. The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28]. There exists numerous research in this area and surveying all of them is beyond the extent of this article. Instead, we focus on some of the most relevant ones. It has been demonstrated that the eye gaze and object descriptions highly correlate [57]. Further, in-depth analysis of gaze behavior in scene understanding and description reveals that people are often describing what they looked at [56], promoting the notion of importance. In [3], the importance of objects is studied in terms of their descriptions where object referral indicates the object\u2019s importance. On the other hand, obeying natural scene statistics, the importance and saliency of an object are equal [59]. The saliency,\nin the form of bottom-up attention, is reported to act as a facilitator whereby salient objects are more likely to be reported in scene descriptions [26]. The role of perception and attention is, however, more than the decisive role of referral and can even influence the order of mentioning objects [10]. Thus, we employ attention in order to analyze image descriptions written by humans and generated by machines.\nThe attention and language studies are affected by the difficulties of relating visual information to linguistic semantics. To date, most of the attention and language studies often use object bounding boxes, which introduces a degree of inaccuracy, in order to identify attention on objects. As an alternative to bounding boxes, [19] employed precise hand-labelled object masks to investigate the relation between objects and the scene context. Nonetheless, such annotations are often avoided due to cost. Thus, Zitnick et al. [58] proposed using abstract images in conjunction with written descriptions for semantic scene analysis, which is impossible for natural images. In this paper, we follow a procedure similar to [19] and rely on precise handlabelled object masks for natural images.\nHow are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human. Furthermore, building on top of the findings of such comparison, we study the contribution of saliency in image captioning models."}, {"heading": "3. Data", "text": "Human descriptions. Fig. 1 depicts examples of the data (images and their human-provided annotations) used in this study. There exist several famous datasets for the task of image description generation. At the time, the most popular dataset is the MS COCO [33]. It consists of over 200K images with at least 5 human-written sentences per image. Among large datasets, there exists Flicker8K [23] and its extention Flicker30K [55]. One of the earliest wellrecognized datasets is UIUC PASCAL sentences [45]. It consists of 1K images selected from the PASCAL-VOC dataset [16] and 5 human-written sentences for each. The same image set is used in PASCAL-50S [53], where 50 human-written sentences are provided. The use of the PASCAL-VOC images gives PASCAL-50S the advantage of having rich contextual annotation information [40]. Furthermore, the same image set is used by [57] for gazebased analysis of objects and descriptions, where the gaze is recorded during free-viewing separate from image descriptions. Combining the three data sets, i.e. [53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-\nject class categories, and gaze information. We call the new data as augmented PASCAL-50S (see Fig. 1).\nMachine-generated descriptions. The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17]. All the models were trained on the MS COCO data set [8] and generated descriptions for the image set of augmented PASCAL-50S. It is worth noting that the models are not necessarily the same as those reported on the COCO leader board information page [1] since the scores are updated by new submissions. Having machine-generated descriptions, we analyze the differences between machinegenerated sentences and human-written ones.\nPreprocessing. To exploit the full potential of the data, we conduct a preprocessing step and compute a visual object category to sentence\u2019s noun (VOS) mapping. VOS mapping is a key ingredient for analyzing sentence constructs in terms of attention. It associates the object categories and their corresponding hand-laballed masks with the nouns in the descriptions.\nThe database consist of images with 222 unique class categories like, \u2018person\u2019, \u2018airplane\u2019, etc. They are accompanied with hand-labelled object masks, useful to establish a visual object to description mapping. To obtain such a mapping, we first identified all the nouns in the sentences by running a part of speech (POS) tagging software [35]. All the unique nouns were extracted, which accounts for 3760 nouns. We listed the top 200 most similar (similarity score > 0.18) nouns to each object class label using word2vec [39]. Then, we manually checked the correspondence between the listed nouns and the category labels to establish a mapping from the visual domain to noun descriptions. During this process, we also considered minor issues such as misspellings and identified the synonyms."}, {"heading": "4. Analyzing Human Sentences", "text": "The quality of descriptions. The descriptions are written in a somewhat free from style. We, thus, studied their quality to gain a better understanding prior to any comparison with machine-generated descriptions. To this end, we looked into some factors, including correct syntactic grammar, the presence of a verb, and active and passive structures. The grammar checking was performed using a link grammar checking syntactic parser [50]. This process leaves 29646 sentences out of 50K (approx. 60%), that are not affected by grammatical errors. It is worth noting that we adopted conservative settings in the parser, which means the correct sentences can be slightly more than what is reported here. Among the grammatically correct sentences, using [35], we identified that only 19126 sentences (64%) have a verb, of which 17362 (90%) are active and 1764 (10%) are passive.\nIt is worthnoting that applying the same procedure to the machine generated sentences, we learn that the sentences are all in active voice. To understand the reason, by analyzing the training data, i.e., the MS COCO, we notice that the models are presumably not exposed to enough passive sentences during training, and are incapable of generating such sentences. Consequently, we report the comparative analysis between the machine-generated descriptions and active human-written sentences.\nObject noun statistics. Based on the information of VOS mapping for human descriptions, Fig. 2 summarizes the object noun statistics. On average 4.76 synonym terms are used to refer to one object class category (maximum 114). Some class categories have unusually high number of synonym terms as they can often be referred by specific type attributes, e.g., a \u2018person\u2019 can be identified as a \u2018boy\u2019, \u2018girl\u2019, \u2018man\u2019, etc. The top 20 words with the maximum number of synonyms are depicted in Fig. 2. The probability of object referrals in terms of noun orders is computed between\ndifferent groups of humans. Two random groups of human descriptions are selected (10 and 40) and the probability of the order of a noun is computed, revealing that people tend to refer to objects more often at the beginning of the sentences. There is also a tendency for naming more objects when the image is more populated, except when the image is overcrowded (see the scatter plot in Fig. 2).\nObject importance. We measured the object importance in terms of object size and attention. We computed the normalized object size, defined as nos = area of object/area of image, where the object area is obtained from the annotation mask. Then, the average over all object instances in all images is reported as the mean normalized object size. To measure the attention, knowing the mask of an object, we used the amount of fixations overlapping with the mask:\nattention = # of fixations on the object # of fixations on the image . (1)\nWe reported the mean of attention over all the instances of an object class category. Fig. 3 summarizes these statistics, signifying that some objects are often more attended in agreement with the findings of [57], providing us some idea about the importance and saliency of objects. We then computed the visual occurrence probability of an object with its description-based occurrence. Overall, highly-attended objects are more probable to be referenced in the descriptions, e.g., \u2018person\u2019. There are, however, some exceptions that are objects which are rarely referenced explicitly and still have high attention value, e.g., \u2018bird cage\u2019."}, {"heading": "5. Machine vs. Human", "text": "Object importance and its referrals. We compute object size and attention as a function of referral order to study the importance of objects in conjunction with their referrals in the descriptions. For each image and description pair, we first identify the annotation masks of the described objects\nusing the VOS mapping. Afterwards, the object size and attention are measured using the object annotation masks while considering the order of nouns in descriptions. Going though all the description and image pairs, the mean normalized object size (nos) for each noun order and average attention information are computed.\nFig. 4 visualizes the results. It reveals that the objects which are described second and third are on average larger than those which are described later on. The objects which are more attention-worthy are on average closer to the beginning of the sentence for both human-written and machine-generated descriptions. Overall, there exists a similar trend between the captioning methods and humans, in which the attention decreases as getting away from the beginning. Looking into individual objects, some class categories may not follow this trend, e.g., \u2018person\u2019. To conclude, despite small differences, both human and machine\nM e\na n\nO b\nje c\nt S\niz e\ntry to address the attention worthy objects as close to the beginning of a sentence as possible.\nAttention on described objects. We signify the role of attention in descriptions by measuring the attention on described objects. We follow the steps of [56] and extend to machine-generated descriptions to compare descriptions by human and machine. We compute the probability of an object being fixated, f , given it is described, d, and visually exists, e, denoted as p(f |d, e), and the probability of an object being described given it is fixated and exists, p(d|f, e). We also computed the probability of an object being described given it visually exists, p(d|e). Another interesting statistics is the probability of referrals to visually absent objects, i.e., p(d|\u00ace). The results are summarized in Table 1.\nWe learn that the human and the \u201cMicrosoft\u201d model perform above the chance level, showing a correlation between attention and description in terms of p(f |d, e). The low\np(d|f, e) is in agreement with the expectation of people looking around in order to describe something rather than describing something and looking around. We also learned that human describes existing objects more often compared to a machine and makes less referrals to non-existing objects. Intuitively, the small value of p(d|\u00ace) for human can be due to the use of nouns referring to concepts, scene schemes or an implicit piece of information.\nFor the sake of reproducibility, we here elaborate the small details of this computation. There are cases that a description refers to a visually existing object multiple times, e.g., \u201ca man and a boy . . . \u201d because an image may contain multiple objects of the same class category (in this example \u2018person\u2019). In such cases, we account the referral only once and consider it fixated if any of the corresponding handlabelled masks of that class category is fixated. The grounding between nouns and masks is validated and established by VOS.\nTo explain lower p(f |d, e) = 0.60 compared to the reported p(f |d, e) = 0.87 in [56], it is worth noting that, in this study, the object categories are obtained from contextual annotation, consisting of 222 classes compared to 20 in [56], and do not discriminate the background from the foreground objects. Also, a substantially higher number of descriptions are used to compute the human performance.\nThe attention agreement between human and machine. We quantified attention agreement by generating saliency maps from descriptions and checking their consistency with human attention on images. Given a description, we fetched all the referred objects and assigned them an attention value depending on their referral order. The attention value is obtained empirically from the average of human attention on object\u2019s referral order as in Fig 4. We put more weight to the centers of objects, as the center of objects are shown to allocate more fixations [6], and slightly smooth the maps. Some generated saliency map examples are provided in Fig. 5 for the captions given in Fig. 1.\nHaving a saliency map and fixation information, we employ the trivial fixation prediction evaluation criteria [7] for assessing a sentence in terms of attention. The average score over sentences of a model indicates the model\u2019s mean similarity with human in terms of attention. We utilized area under the curve (AUC) [27], correlation coefficient (CC), and normalized scanpath saliency (NSS) [43].\nFor all these measures larger values indicate better performance. To obtain an upper bound, we generated saliency maps for human-written descriptions and assessed their agreement with attention.\nThe results are summarized in Table 2. It is not surprising that the human-written sentences have the highest agreement with attention. However, there are cases in which the attention does not agree well for human-written captions. For example, consider the second image in Fig. 1 and its corresponding saliency in Fig. 5. As can be observed the human-description do not contain an explicit reference to the attended objects, but refer to a general concept.\nComparing attention score with description scores. From Table 2, we learn that the captioning methods differ with each other in terms of attention agreement with human. This motivates to gain further insight about the overall goodness of a model and its attention agreement with human. We thus evaluate the generated descriptions using Microsoft COCO caption evaluation code [8] and compared it with the AUC score of models. The results are summarized in Table 3. The results indicate that 1) the average ranking of methods by AUC agrees with the traditional metrics, and 2) the better a model is, the better attention agreement it has with human."}, {"heading": "6. Saliency-Boosted Captioning Model", "text": "To this point, we confirmed that there exists a degree of agreement between descriptions by human and machine in terms of attention. We learned that better captioning models have a higher attention agreement with human. For this purpose, we relied on fixations gathered from a free-viewing task. Thus, we build a captioning model with visual features\nboosted by a saliency model in order to investigate potential improvements using a bottom-up saliency model. In other words, we focus on answering: Can saliency benefit image captioning by machine?\nFor this purpose, we employ a standard captioning model, based on an LSTM network of three layers with residual connections [22] between the layers. We use the open implementation of [48], where we set both feature input channels of the LSTM model to visual features and avoid any contextual features for simplicity. Fig. 6 depicts a high-level illustration of the proposed captioning model. In the following paragraphs, we explain the feature extraction, saliency computation and feature boosting and linearization. We refer the readers to [48] for the details of the language model.\nImage features. We extract the image features using CNN features of the VGG network [49]. We follow the filter bank approach of [9] and compute the responses over the input image. In other words, the output of the last convolutional layer (pool5) is used. This results in a feature tensor of 7\u00d7 7\u00d7 512, i.e., a 7\u00d7 7 map of 512-dimensional feature vectors. These features are later boosted and linearized in order to be fed to the LSTM module.\nSaliency. We compute the saliency using the image features of VGG network. Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [25], following the saliency model of [52]. That is, an ensemble of saliency predictors are learnt to perform a regression from image features to the saliency space. The final saliency is the mean of the predicted saliencies from the members of the ensemble.\nBoosting image features and linearization. We boost the CNN features before feeding them to the language model with the saliency map of the image. The procedure is depicted in Fig 7.\nThe CNN feature maps and saliency maps are of size 7\u00d7 7. The saliency maps are normalized so that each pixel has a value between 0 and 1. We apply a 3 \u00d7 3 moving window with stride 1 on the 7 \u00d7 7 pool5 feature map and concatenate the features under the 3\u00d73 window to a feature vector of 4608 dimensions, denoted as Fl. A mean pooling\nis also performed on the 7 \u00d7 7 saliency map with a 3 \u00d7 3 moving window, denoted as Sal. Thus, a 5\u00d7 5 feature map of 4608-dimensional feature vectors and a 5\u00d7 5 aggregated saliency map are combined to produce a feature vector input to the language model.\nTo combine this saliency data with the image features, the local features corresponding to the feature maps are weighted by their corresponding saliency value. Then, the weighted feature vectors are averaged to produce a single feature vector, Fsal, which is input to the language model:\nFsal = 7\u2211 i=1 7\u2211 j=1 Sal\u03b1(i, j)Fl(i, j), (2)\nSal\u03b1(i, j) = 1 + SalL1(i, j) \u03b1, (3)\nwhere Fl is the image feature map, SalL1 is the L1 normalized saliency map, and \u03b1 is an attenuation factor to control compactness of the aggregated saliency. The value of \u03b1 = 2 was determined via cross validation during training on MS COCO.\nEvaluating saliency contribution. We evaluate the saliency boosted model on MS COCO [33] and augmented PASCAL50S, where the model is trained on MS COCO. To understand the contribution of saliency, we define a baseline using a uniform saliency where the saliency map is all ones. Then, we compare the proposed model and baseline. For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].\nThe results on the MS COCO evaluation set are reported in Table 4. Comparing the proposed baseline and the saliency-boosted model, there is no significant improvement by boosting the model using a saliency in this dataset.\nThe results on the augmented PASCAL50S are reported in Table 5. As depicted, contrary to the MS COCO re-\nsults, the proposed model outperforms the baseline, indicating that a better generalization is achieved by using saliency boosting. We should also note here that the automatic evaluation is much more reliable on the PASCAL50S dataset owing to the availability of 50 references per image.\nOverall, comparing between the results obtained on the MS COCO evaluation and augmented PASCAL50S datasets, the saliency boosting seems not contributing when we are training and testing a model on one database. The reason most likely lies on the fact that the model learns the underlying data very well and there is no need for further boosting. The performance results, however, shows that for across database and an unseen data with different visual characteristics, the saliency boosting contributes and improves the performance of the captioning model."}, {"heading": "7. Discussion and Conclusion", "text": "Let us get back to the two questions on the beginning of this paper and summarize our findings in regard to them. 1) How well do image descriptions, by humans or models, on a scene agree with saliency? In summary, all the captioning models shown a clear degree of agreement with human in capturing saliency.\nWe testified that humans describe fixated items rather than looking aimlessly, consistent with [56]. Then, we extended the analysis to include descriptions by machine. The \u201cMicrosoft\u201d model (the best among all models in this study) has the highest degree of agreement with human in terms of captioning metrics and attention. This indicates that the captioning models are becoming powerful enough to describe the visually existing elements similar to humans.\nCoinciding with the supporting evidence about the role\nof saliency, e.g. [56, 26], we confirmed that more salient objects appear on average closer to the beginning of the descriptions by human. Extending the analysis to automatic captioning models, we observered a similar phenomenon. A model, superior to its counterparts, agrees with humans better in terms of attention on the order of nouns.\nWe quantified the attention agreement between human and machine by generating saliency maps from descriptions. The analysis shows that all the captioning models have a degree of agreement in terms of attention. The descriptions by \u201cMicrosoft\u201d and \u201cAalto\u201d models, which employ some level of object detection, are in the highest degree of agreement with human in terms of attention. This promotes possible contributions from top-down attention and contextual factors.\n2) Can saliency benefit image captioning by machine? The answer is not straightforward. We investigated the role of bottom-up saliency by proposing a simple saliencyboosted captioning model. The use of bottom-up attention coincides with the role of saliency in language formation, which followers of Wundt\u2019s theory mostly back. We learned that the saliency-boosted model is not significantly different from the baseline on the validation set of the MS COCO database, which the model was trained on. But, more importantly, the saliency-boosted model has a better generalization performance on augmented PASCAL50S dataset, which is totally unseen during the training, compared to the baseline. Future studies. The future studies should target the topdown attention, which requires simultaneous recording of eye movements and scene descriptions and is not possible with the current data. Acknowledgements. The support of the Finnish Center of Excellence in Computational Inference Research (COIN) and generous gifts from Nvidia are gratefully acknowledged."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding and predicting importance in images", "author": ["A.C. Berg", "T.L. Berg", "H. Daum", "J. Dodge", "A. Goyal", "X. Han", "A. Mensch", "M. Mitchell", "A. Sood", "K. Stratos", "K. Yamaguchi"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic description generation from images: A survey", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "arXiv:1601.03896,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Minding the clock", "author": ["K. Bock", "D. Irwin", "D. Davidson", "W. Levelt"], "venue": "Journal of Memory and Language, 48,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations", "author": ["A. Borji", "J. Tanner"], "venue": "IEEE Trans Neural Netw Learn Syst., PP(99):1\u201313,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H.R. Tavakoli", "D.N. Sihite", "L. Itti"], "venue": "ICCV,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "T.-Y.L. Hao Fang", "R. Vedantam", "S. Gupta", "P. Dollr", "C.L. Zitnick"], "venue": "arXiv:1504.00325,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Giving good directions: order of mention reflects visual salience", "author": ["A.D.F. Clarke", "M. Elsner", "H. Rohde"], "venue": "Frontiers in Psychology, 6(1793),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Language models for image captioning: The quirks and what work", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "ACL,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing images using inferred visual dependency representations", "author": ["D. Elliott", "A.P. de Vries"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, 111(1):98\u2013136, Jan.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J. Platt", "L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistics of high-level scene context", "author": ["M.R. Greene"], "venue": "Frontiers in Psychology, 4(777),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "What the eyes say about speaking", "author": ["Z. Griffin", "K. Bock"], "venue": "Psychol Sci., 11(4),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Observing the what and when of language production for different age groups by monitoring speakers eye movements", "author": ["Z.M. Griffin", "D.H. Spieler"], "venue": "Brain and Language, 99(3):272 \u2013 288,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceeding of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation  metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013 899,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "How we focus attention in picture viewing, picture description, and during mental imagery, pages 291\u2013 313", "author": ["J. Holsanova"], "venue": "Bilder - sehen - denken : zum Verhltnis von begrifflichphilosophischen und empirisch-psychologischen Anstzen in der bildwissenschaftlichen Forschung. von Halem,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Extereme learning machine: Theory and applicatons", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomput., 70:489\u2013 501,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Action to Language via the Mirror Neuron System, chapter Attention and the Minimal Subscene", "author": ["L. Itti", "M.A. Arbib"], "venue": "Combridge Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Referential domains in spoken language comprehension: Using eye movements to bridge the product and action traditions. In The interface of language, vision, and action: Eye movements and visual world", "author": ["M. k. Tanenhaus", "C. Chambers", "J.E. Hanna"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "CoNLL, pages 220\u2013228,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "ACL,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "PAMI,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of eye tracking in studies of sentence generation", "author": ["A.S. Meyer"], "venue": "The interface of language, vision, and action: Eye movements and the visual world. Psychology Press,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Viewing and naming objects: eye movements during noun phrase production", "author": ["A.S. Meyer", "A.M. Sleiderink", "W.J. Levelt"], "venue": "Cognition, 66(2):B25 \u2013 B33,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.G. Cho", "S.W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR, pages 891\u2013898,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. jing Zhu"], "venue": "In ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Components of bottom-up gaze allocation in natural images", "author": ["R.J. Peters", "A. Iyer", "L. Itti", "C. Koch"], "venue": "Vision Research, 45:2397\u20132416,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Walking or talking?: Behavioral and neurophysiological correlates of action verb processing", "author": ["F. Pulverm\u00fcller", "M. Hrle", "F. Hummel"], "venue": "Brain and Language, 78(2),", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "The Long-Short Story of Movie Description", "author": ["A. Rohrbach", "M. Rohrbach", "B. Schiele"], "venue": "GCPR,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Video captioning with recurrent networks based on frame- and video-level features and visual content classification", "author": ["R. Shetty", "J. Laaksonen"], "venue": "CVPR Workshops,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting scene context for image captioning", "author": ["R. Shetty", "H. R-Tavakoli", "J. Laaksonen"], "venue": "ACMMM Vision and Language Integration Meets Multimedia Fusion Workshop,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv, abs/1409.1556,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing english with a link grammar", "author": ["D.D. Sleator", "D. Temperley"], "venue": "Third International Workshop on Parsing Technologies,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1991}, {"title": "Cognition and Sentence Production: A Cross- Linguistic Study, chapter Models of Sentence Production, pages 7\u201319", "author": ["S.N. Sridhar"], "venue": "Springer New York,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1988}, {"title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "author": ["H.R. Tavakoli", "A. Borji", "J. Laaksonen", "E. Rahtu"], "venue": "Neurocomputing,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2017}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL, 2:67\u201378,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring the role of gaze behavior and object detection in scene understanding", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G. Zelinsky", "T. Berg"], "venue": "Frontiers in Psychology, 4,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Studying relationships between human gaze, description, and computer vision", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G.J. Zelinsky", "T.L. Berg"], "venue": "CVPR. IEEE,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "PAMI, PP(99),", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Einhauser. Fixations on objects in natural scenes: dissociating importance from salience", "author": ["B.M. t Hart", "H.C.E.F. Schmidt", "C. Roth"], "venue": "Frontiers in Psychology,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 52, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 6, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 10, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 195, "endOffset": 210}, {"referenceID": 44, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 45, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 34, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 242, "endOffset": 254}, {"referenceID": 32, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 286, "endOffset": 293}, {"referenceID": 0, "context": "The recent advancements in machine learning, together with the increase in the availaible computational power, have increased the interest in solving high-level problems such as image-captioning [13, 54, 8, 12], scene and video understanding [46, 47, 36], and visual question answering [34, 2].", "startOffset": 286, "endOffset": 293}, {"referenceID": 49, "context": "He proposes that, in a free word positioning scenario, not bound by any traditional rule, the words follow each other according to the degree of emphasis on the concepts [51].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "Griffin and Bock [20] found some empirical supporting evidence by showing that while describing scenes, speakers look at an object before naming it within their description.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 35, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 19, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 22, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 54, "context": "Most of their findings provide supporting evidence that the sentence formation and attention correlate [5, 37, 21, 24, 56].", "startOffset": 103, "endOffset": 122}, {"referenceID": 16, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 39, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 21, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 28, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 45, "endOffset": 61}, {"referenceID": 29, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 83, "endOffset": 91}, {"referenceID": 52, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 15, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 12, "context": "They can be categorized into retrieval-based [18, 41, 23, 30], sentence generation [31, 15], and the models which combine the two paradigms [54, 17, 14].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "In-depth study of these different models is beyond the scope of this article and falls within the surveys such as [4].", "startOffset": 114, "endOffset": 117}, {"referenceID": 27, "context": "\u201cNeural Talk\u201d [29] utilizes a vanilla recurrent neural architecture for the language model while using CNNs for encoding.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "\u201cMicrosoft\u201d [17] employs multiple instance learning to learn a visual detector for words in order to utilize them in an exponential language model for sentence generation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 52, "context": "The \u201cGoogle\u201d method [54] is a generative deep model based on recurrent architectures, more specifically long short-term memory (LSTM) networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 45, "context": "\u201cAalto\u201d [47] employs object detection to augment features in order to boost the results in a framework similar to \u201cNeural Talk\u201d, utilizing LSTM networks.", "startOffset": 8, "endOffset": 12}, {"referenceID": 40, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 370, "endOffset": 374}, {"referenceID": 51, "context": "Some of these metrics are BiLingual Evaluation Understudy (BLEU) [42], which signifies the precision and neglects recall, Recall Oriented Understudy of Gisting Evaluation for the Longest common subsequence (ROUGE-L) [32], which is based on the statistics of the sentence level structure similarities, Metric for Evaluation of Translation with Explicit Ordering (METEOR) [11] and Consensus-based Image Description Evaluation (CIDEr) [53].", "startOffset": 432, "endOffset": 436}, {"referenceID": 18, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 126, "endOffset": 134}, {"referenceID": 36, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 126, "endOffset": 134}, {"referenceID": 42, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 215, "endOffset": 223}, {"referenceID": 26, "context": "The joint study of attention and language covers different perspectives either to understand the language development process [20, 38] or to investigate the role of language in scene understanding and comprehension [44, 28].", "startOffset": 215, "endOffset": 223}, {"referenceID": 55, "context": "It has been demonstrated that the eye gaze and object descriptions highly correlate [57].", "startOffset": 84, "endOffset": 88}, {"referenceID": 54, "context": "Further, in-depth analysis of gaze behavior in scene understanding and description reveals that people are often describing what they looked at [56], promoting the notion of importance.", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "In [3], the importance of objects is studied in terms of their descriptions where object referral indicates the object\u2019s importance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 57, "context": "On the other hand, obeying natural scene statistics, the importance and saliency of an object are equal [59].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "The saliency, in the form of bottom-up attention, is reported to act as a facilitator whereby salient objects are more likely to be reported in scene descriptions [26].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "The role of perception and attention is, however, more than the decisive role of referral and can even influence the order of mentioning objects [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "As an alternative to bounding boxes, [19] employed precise hand-labelled object masks to investigate the relation between objects and the scene context.", "startOffset": 37, "endOffset": 41}, {"referenceID": 56, "context": "[58] proposed using abstract images in conjunction with written descriptions for semantic scene analysis, which is impossible for natural images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In this paper, we follow a procedure similar to [19] and rely on precise handlabelled object masks for natural images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 54, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 55, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 24, "context": "How are we different? It is worth noting that what distinguishes the present work from aforemnetioned works like [3, 56, 57, 26] is that we consider the machine generaed sentences in conjunction with the human written ones, enabaling us to compare machine and human.", "startOffset": 113, "endOffset": 128}, {"referenceID": 31, "context": "At the time, the most popular dataset is the MS COCO [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Among large datasets, there exists Flicker8K [23] and its extention Flicker30K [55].", "startOffset": 45, "endOffset": 49}, {"referenceID": 53, "context": "Among large datasets, there exists Flicker8K [23] and its extention Flicker30K [55].", "startOffset": 79, "endOffset": 83}, {"referenceID": 43, "context": "One of the earliest wellrecognized datasets is UIUC PASCAL sentences [45].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "It consists of 1K images selected from the PASCAL-VOC dataset [16] and 5 human-written sentences for each.", "startOffset": 62, "endOffset": 66}, {"referenceID": 51, "context": "The same image set is used in PASCAL-50S [53], where 50 human-written sentences are provided.", "startOffset": 41, "endOffset": 45}, {"referenceID": 38, "context": "The use of the PASCAL-VOC images gives PASCAL-50S the advantage of having rich contextual annotation information [40].", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "Furthermore, the same image set is used by [57] for gazebased analysis of objects and descriptions, where the gaze is recorded during free-viewing separate from image descriptions.", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 55, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 38, "context": "[53, 57, 40], results in 1K images with 50 sentences, 222 precisely labelled ob-", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 45, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 52, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "The machine descriptions are generated by four captioning models, including: \u201cNeural Talk\u201d [29], \u201cAalto\u201d [47], \u201cGoogle\u201d [54], and \u201cMicrosoft\u201d [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "All the models were trained on the MS COCO data set [8] and generated descriptions for the image set of augmented PASCAL-50S.", "startOffset": 52, "endOffset": 55}, {"referenceID": 33, "context": "To obtain such a mapping, we first identified all the nouns in the sentences by running a part of speech (POS) tagging software [35].", "startOffset": 128, "endOffset": 132}, {"referenceID": 37, "context": "18) nouns to each object class label using word2vec [39].", "startOffset": 52, "endOffset": 56}, {"referenceID": 48, "context": "The grammar checking was performed using a link grammar checking syntactic parser [50].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "Among the grammatically correct sentences, using [35], we identified that only 19126 sentences (64%) have a verb, of which 17362 (90%) are active and 1764 (10%) are passive.", "startOffset": 49, "endOffset": 53}, {"referenceID": 55, "context": "3 summarizes these statistics, signifying that some objects are often more attended in agreement with the findings of [57], providing us some idea about the importance and saliency of objects.", "startOffset": 118, "endOffset": 122}, {"referenceID": 54, "context": "We follow the steps of [56] and extend to machine-generated descriptions to compare descriptions by human and machine.", "startOffset": 23, "endOffset": 27}, {"referenceID": 54, "context": "87 in [56], it is worth noting that, in this study, the object categories are obtained from contextual annotation, consisting of 222 classes compared to 20 in [56], and do not discriminate the background from the foreground objects.", "startOffset": 6, "endOffset": 10}, {"referenceID": 54, "context": "87 in [56], it is worth noting that, in this study, the object categories are obtained from contextual annotation, consisting of 222 classes compared to 20 in [56], and do not discriminate the background from the foreground objects.", "startOffset": 159, "endOffset": 163}, {"referenceID": 4, "context": "We put more weight to the centers of objects, as the center of objects are shown to allocate more fixations [6], and slightly smooth the maps.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Having a saliency map and fixation information, we employ the trivial fixation prediction evaluation criteria [7] for assessing a sentence in terms of attention.", "startOffset": 110, "endOffset": 113}, {"referenceID": 25, "context": "We utilized area under the curve (AUC) [27], correlation coefficient (CC), and normalized scanpath saliency (NSS) [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "We utilized area under the curve (AUC) [27], correlation coefficient (CC), and normalized scanpath saliency (NSS) [43].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "We thus evaluate the generated descriptions using Microsoft COCO caption evaluation code [8] and compared it with the AUC score of models.", "startOffset": 89, "endOffset": 92}, {"referenceID": 20, "context": "In other words, we focus on answering: Can saliency benefit image captioning by machine? For this purpose, we employ a standard captioning model, based on an LSTM network of three layers with residual connections [22] between the layers.", "startOffset": 213, "endOffset": 217}, {"referenceID": 46, "context": "We use the open implementation of [48], where we set both feature input channels of the LSTM model to visual features and avoid any contextual features for simplicity.", "startOffset": 34, "endOffset": 38}, {"referenceID": 46, "context": "We refer the readers to [48] for the details of the language model.", "startOffset": 24, "endOffset": 28}, {"referenceID": 47, "context": "We extract the image features using CNN features of the VGG network [49].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "We follow the filter bank approach of [9] and compute the responses over the input image.", "startOffset": 38, "endOffset": 41}, {"referenceID": 23, "context": "Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [25], following the saliency model of [52].", "startOffset": 101, "endOffset": 105}, {"referenceID": 50, "context": "Afterwards, we learn a regression to approximate the human fixations using extreme learning machines [25], following the saliency model of [52].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "We evaluate the saliency boosted model on MS COCO [33] and augmented PASCAL50S, where the model is trained on MS COCO.", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 112, "endOffset": 116}, {"referenceID": 45, "context": "For the sake of completeness, we also include the performance of \u201cNeural Talk\u201d [29], \u201cGoogle\u201d [54], \u201cMicrosoft\u201d [17], and \u201cAalto\u201d [47].", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "283 Microsoft [17] \u2013 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 45, "context": "257 Aalto [47] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 52, "context": "299 Google [54] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "277 Neural Talk [29] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 54, "context": "We testified that humans describe fixated items rather than looking aimlessly, consistent with [56].", "startOffset": 95, "endOffset": 99}, {"referenceID": 54, "context": "[56, 26], we confirmed that more salient objects appear on average closer to the beginning of the descriptions by human.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[56, 26], we confirmed that more salient objects appear on average closer to the beginning of the descriptions by human.", "startOffset": 0, "endOffset": 8}], "year": 2017, "abstractText": "To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliencyboosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization ability is, however, observed for the saliency-boosted model on unseen data.", "creator": "LaTeX with hyperref package"}}}