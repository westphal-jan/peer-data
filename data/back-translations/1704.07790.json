{"id": "1704.07790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "FWDA: a Fast Wishart Discriminant Analysis with its Application to Electronic Health Records Data Classification", "abstract": "However, the classical LDA for classifying EHR data suffers from two disadvantages: the insufficient estimation of LDA parameters (e.g. covariance matrix) and the \"linear inseparability\" of EHR data. In order to solve these two problems, we propose in this paper a new classifier FWDA - Fast Wishart Discriminant Analysis - that makes predictions on an ensemble basis. Specifically, FWDA first surrounds the distribution of inverse covariance matrices using a wishart distribution estimated from the training data, then \"weighted averages\" the classification results of several LDA classifiers, which are parameterized by the sampled inverse covariance matrices using a Bajian voting scheme. The weights for the voting data are optimally updated to adapt each new input to large LDA classifiers that have a high linear rate, enabling modal convergence of the HDA data.", "histories": [["v1", "Tue, 25 Apr 2017 17:11:57 GMT  (260kb,D)", "http://arxiv.org/abs/1704.07790v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haoyi xiong", "wei cheng", "wenqing hu", "jiang bian", "zhishan guo"], "accepted": false, "id": "1704.07790"}, "pdf": {"name": "1704.07790.pdf", "metadata": {"source": "CRF", "title": "FWDA: a Fast Wishart Discriminant Analysis with its Application to Electronic Health Records Data Classification", "authors": ["Haoyi Xiong", "Wei Cheng", "Wenqing Hu", "Jiang Bian", "Zhishan Guo"], "emails": [], "sections": [{"heading": null, "text": "Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data is widely-used for early detection of diseases. Classical LDA for EHR data classification, however, suffers from two handicaps: the ill-posed estimation of LDA parameters (e.g., covariance matrix), and the \u201clinear inseparability\u201d of EHR data. To handle these two issues, in this paper, we propose a novel classifier FWDA \u2014 Fast Wishart Discriminant Analysis, that makes predictions in an ensemble way. Specifically, FWDA first surrogates the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting scheme. The weights for voting are optimally updated to adapt each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that FWDA possesses a fast convergence rate and a robust performance on high dimensional data. Extensive experiments on large-scale EHR dataset show that our approach outperforms stateof-the-art algorithms by a large margin."}, {"heading": "1 Introduction", "text": "The ubiquity of Electronic Health Records (EHR) [1, 2] in healthcare systems provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records. Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques. Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].\nAmong these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8]. However, recent studies demonstrate the limitation of LDA under high dimension low sample size (HDLSS) settings [9], such as the EHR records [10]. Because it is difficult to recover\nar X\niv :1\n70 4.\n07 79\n0v 1\n[ cs\n.L G\n] 2\nthe \u201ctrue\u201d parameters, e.g., covariance matrix, from a relatively small number of training samples. When the number of dimensions of EHR data is larger than the number of samples, the sample covariance estimation used in classical LDA, is singular and not invertible. In this case, LDA cannot produce any valid prediction. Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the \u201ctrue\u201d (inverse) covariance matrix, with an inconsistent estimate of the largest eigenvalues and almost-orthogonal eigenvectors to the truth [11]. Such ill-posed estimation problem significantly degrades the performance of LDA. Moreover, EHR data is usually not linearly separable [1, 2].\nTo address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17]. Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed. In summary, these methods intend to improve LDA classification through optimizing the parameters of LDA, such as (inverse) covariance matrices, linear projection metrics, or kernel settings, in a so-called optimal model selection manner [22].\nInstead of \u201cbidding\u201d the optimal parameter in the full and usually unknown parameter space, in this work, we intend to improve LDA in an ensemble way [23], while adapting to the new input data. Specifically, we first sample a set of (inverse) covariance matrices from the both training data and the new input data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting Scheme [24]. Theoretical studies show that such Bayesian voting scheme can secure a wider margin and guarantee a good classification performance with a lower generalization error bound [24]. This theoretically guarantees that the proposed framework can \u201con average\u201d outperform those regularization-based LDA classifiers using only single (inverse) covariance matrix estimator [25]. More importantly, the sampled (inverse) covariance matrices used by different LDA classifiers are updated with each new input data instance. In this way, the proposed classifier enables nonlinear classification by leveraging local information of the input data.\nHowever, the aforementioned Input-Adaptive Bayesian Voting Scheme is not computationally efficient. As the sampled (inverse) covariance matrices are assumed to be updated to adapt each new input data for classification, the sampling complexity is very high. Especially, when the number of dimensions of data is high, it is quite time-consuming to sample the (inverse) covariance matrices, while ensuring each sampled matrix is positive-semidefinite. Thus, we propose a novel method FWDA \u2013 Fast Wishart Discriminant Analysis, which can approximate the optimal prediction results with minimal sampling efforts.\nSpecifically, FWDA first surrogates the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, then a set of inverse covariance matrices are sampled based on the distribution. The \u201cweighted-averaged\u201d result over the classification results from LDA classifiers parameterized by these sampled inverse covariance matrices are used for prediction. The \u201cweights\u201d are updated by each new input data for classification optimally in a Bayes manner. In this way, FWDA can approximate to the aforementioned input-adaptive Bayesian voting schema,\nwith proven convergence rate. Our theoretical analysis further proves that (1) the error of approximation could quickly converge with the increasing number of sampled inverse covariance matrices m in speed O(m\u2212 12 ); and (2) the error is not sensitive to the dimensions of the data, that means the performance of high dimensional data classification could be well-guaranteed.\nIn the rest of the paper, we first introduce the backgrounds, then we formulate the problem of research and elaborate the technical challenges in Section 2. In Section 3, we present the proposed algorithm FWDA, with the theoretical analysis on the approximation performance. In Section 4, we evaluate FWDA with other baseline algorithms for early detection of diseases using large-scale real EHR data. The results show that FWDA significantly outperforms baseline algorithms by a large margin."}, {"heading": "2 Background and Problem Formulation", "text": "In this section, we first introduce the preliminaries of our research, then formulate the research problem of this paper."}, {"heading": "2.1 Binary Classification for Early Detection of Diseases using EHR data", "text": "First of all, we introduce the EHR data representation using diagnosis-frequency vectors, and present settings of disease detection through binary classification of diagnosisfrequency vectors. Later, we briefly discuss the solution based on the typical LDA classifier.\nEHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6]. Among these approaches, the diagnosis-frequency is a common way to represent EHR data.\nGiven each patient\u2019s EHR data, this method first retrieves the diagnosis codes [26] recorded during each visit. Next, the frequency of each diagnosis appearing in all past visits are counted, followed by further transformation on the frequency of each diagnosis into a vector of frequencies. For example, \u30081, 0, . . . , 3\u3009, where 0 means the second diagnosis does not exist in all past visits. In this paper, we denote the dimension of diagnosis-frequency vectors as p. Note that the dimension p of original codes is usually larger than 15, 000. Even using clustered codes, p is usually larger than 250 [27], while the number of samples for training n is frequently smaller than p.\nEarly Detection by Binary Classification - Given m training samples (i.e., EHR frequency vectors) along with corresponding labels i.e., (x1, l1) . . . (xn, ln) where li \u2208 {\u22121,+1} refers to whether the patient i is diagnosed with the target disease or not, the early disease detection task is to determine if a new patient\u2019s data vector x would develop into the target disease by classifying the vector x to +1 (positive) or \u22121 (negative)."}, {"heading": "2.2 Linear Discriminant Analysis", "text": "To solve the binary classification problem aforementioned, we consider a simple LDA classifier f(x) \u2208 {\u00b11} based on the given p-dimensional data vector x and labeled samples x1, x2, ...xn\nf(x, \u03a3\u0302) = sign ( (x\u2212 x\u0304)T \u03a3\u0302\u22121 (x\u0304+1 \u2212 x\u0304\u22121) )\n(1)\nwhere x\u0304 refers to the mean vectors of all samples x1, x2, ...xn; x\u0304+1, x\u0304\u22121 refer to the mean vectors of the positive samples and negative samples receptively.\nThe \u03a3\u0302 is the covariance matrix estimated from data x1, x2, ...xn. The most common estimation of \u03a3\u0302 is the sample estimation:\n\u03a3\u0304 = 1 n\u2212 1 \u2211\n1\u2264j\u2264n (xj \u2212 x\u0304)T (xj \u2212 x\u0304) (2)\nThus, we write f(x, \u03a3\u0304) as the classical Fisher\u2019s Linear Discriminant Analysis."}, {"heading": "2.3 Bayesian Voting Scheme", "text": "Given a binary classifier h\u03c9(x) \u2208 {\u00b11}, which is parameterized by \u03c9, the Bayesian Voting Classification [24] of the classifier is:\nsign\n(\u222b\n\u03c9\nh\u03c9(x)p(\u03c9)d\u03c9\n) , (3)\nwhere the signal function sign(\u00b7) maps the non-negative input to +1 and the negative input to\u22121, and p(\u03c9) is the prior probability of the parameter \u03c9. As a binary classifier, the above classifier in Eq. 3 outputs the label with the highest weighted vote. The theoretical advantages of Bayesian voting scheme are addressed in [24]."}, {"heading": "2.4 Problem Formulation", "text": "To handle the uncertainty of (inverse-) covariance matrix estimates for LDA, through combining Bayesian Voting and LDA, we can consider a new classifier as:\nsign\n(\u222b\n\u03a3\u0302\u22650 f(x, \u03a3\u0302)P (\u03a3\u0302|x1, x2, ...xn, x)d\u03a3\u0302\n) , (4)\nwhere P (\u03a3\u0302|x1, x2, ...xn, x) is the probability of the covariance matrix \u03a3\u0302, given the n training samples x1, x2, ...xn as well as the new sample for prediction x. In our research, we named this pattern as Input Adaptive Bayesian Voting. Note that we take the new input vector x into account for generating the \u201chypothesis\u201d \u03a3\u0302 of Bayesian inference.\nWith all above backgrounds and settings in mind, the problem of this research is to compute Equation 4. However, there exists at least two major technical challenges:\nChallenge I: Fast Computation and Lazy Sampling - To compute the integral in Eq. 4, a common solution is to leverage a Monte-Carlo Integration algorithm [28] that\nfirst randomly samples a group of positive-semidefinite matrices e.g., \u03a31,\u03a32 . . .\u03a3m from the distribution with probability density function P (\u03a3\u0302|x1, x2, ...xn, x), then averages f(x, \u03a3\u0302) over the sampled positive-semidefinite matrices as 1/m \u2211m i=1 f(x,\u03a3i). This method can give an approximate result of Eq. 4. However, the density function of the sampled positive-semidefinite matrices P (\u03a3\u0302|x1, x2, ...xn, x) depends on the input x. That means, for each new testing sample x, we have to build a new probability distribution based on P (\u03a3\u0302|x1, x2, ...xm, x), then sample a new group of positivesemidefinite matrices and run the Monte-Carlo Integration accordingly. Obviously, the computational cost to re-sample a new group of positive-semidefinite matrices for each new input x is high. Thus, we need a \u201cLazy Sampling\u201d mechanism, which only samples a group of positive-semidefinite matrices once, then uses the same group of matrices for arbitrary input x.\nChallenge II: Approximation and Convergence - The accuracy of classification highly depends on whether the proposed algorithm can approximate to the Eq. 4 as well as the convergence rate. For the high-dimensional numeric integration [29], the approximation is usually bottle-necked by the number of dimensions (e.g., the dimensionality of positive-semidefinite matrices p \u00d7 p) and the sampling complexity (e.g., the number of sampled positive-semidefinite matrices m). Intuitively, the convergence of algorithms can be improved, with increasing sampling complexity and lower dimensionality. However, we aim at proposing an algorithm to approximate Eq. 4 with a low computational/sampling complexity while ensuring a fast convergence rate. Especially we require a convergence rate that is not sensitive to the dimensionality of the data p, so as to enable the high dimensional data classification.\nIn the rest of this paper, we present a novel classifier, Fast Wishart Discriminant Analysis \u2013 FWDA, which tackle the two research challenges, with low computational/sampling complexity and proven dimensionality-insensitive convergence rate.\n3 FWDA: Algorithms and Analysis In this section, we introduce our solution to compute Eq. 4 as follows: we first reformulate Eq. 4. Then, we introduce the algorithms of FWDA to compute the reformulation of Eq. 4. Finally, we analyze FWDA."}, {"heading": "3.1 Problem Reformulation", "text": "We first define P (x|\u03a3) as the probability of input vector x given the covariance matrix \u03a3, and P (\u03a3|x1, x2...xn) as the probability of the covariance matrix \u03a3, given the training samples x1, x2...xn. Then, we define a function:\ng(x) =\n\u222b\n\u0398\u22650 f(x,\u03a3)P (x|\u03a3)P (\u03a3|x1, x2...xn)d\u03a3. (5)\nTheorem 1. Eq. 4 is equivalent to the classification result of sign(g(x)).\nProof. Assuming all x1, x2, ...xn, x are i.i.d drawn from an unknown distribution, according to the Bayesian theorem, we decompose P (\u03a3|x1, ...xn, x) as\nP (\u03a3|x1, ...xn, x) = P (x|\u03a3)P (x1...xn|\u03a3)P (\u03a3)\nP (x)P (x1...xn) = P (x|\u03a3)P (\u03a3|x1, x2...xn) \u00b7 P (x)\u22121 (6)\nThus, Eq. 4 can be re-written as sign(p(x)\u22121 g(x)). As p(x)\u22121 is positive for \u2200x. Thus, we can conclude sign(g(x)) = sign(p(x)\u22121 g(x)) should be consistently equivalent to the Eq. 4.\nThus, the key of proposed research is to compute Eq. 5. We propose a straightforward method (FWDA): the algorithm consists of a probabilistic model that can generate m sampled (inverse) covariance matrices according to the density functionP (\u03a3|x1, x2...xn), then calculates Eq. 4 through Monte-Carlo Integration using the sampled (inverse) covariance matrices. The design of FWDA is described in the following."}, {"heading": "3.2 Wishart Distribution Model based on De-sparsified Graphical Lasso", "text": "To sample (inverse) covariance matrices according to P (\u03a3|x1, x2...xn), FWDA leverages a Wishart Distribution [30] namelyW(T\u0302 , v), where T\u0302 refers to the \u201cmean\u201d positivedefinite matrix for the Wishart distribution and v is the degree of freedom.\nGiven any p \u00d7 p positive definite matrix \u0398 (as the inverse of potential covariance matrix), we estimate the probability density of \u0398, based onW(T\u0302 , v), as:\nPw(\u0398|T\u0302 , v) = 1 2vp/2 \u2223\u2223\u2223T\u0302 \u2223\u2223\u2223 v/2 \u0393p ( v 2 ) |\u0398| (v\u2212p\u22121)/2 e\u2212(1/2) tr(T\u0302 \u22121\u0398) (7)\nwhere | \u00b7 | refers to the determinant and the multivariate gamma function is defined as:\n\u0393p (v 2 ) = \u03c0p(p\u22121)/4 p\u220f\nj=1\n\u0393\n( v\n2 + 1\u2212 j 2\n) .\nSpecifically, in our research, we set the degree of feedom v as v = n \u2212 1, and further estimate T\u0302 using De-sparsified Graphical Lasso [31]:\nT\u0302 = 2\u0398\u0302\u2212 \u0398\u0302\u03a3\u0304\u0398\u0302. (8)\nwhere \u0398\u0302 refers to the Graphical Lasso estimator\n\u0398\u0302 = argmin \u0398\u22650\n tr(\u03a3\u0304\u0398)\u2212 log |\u0398|+ \u03bb \u2211\nj 6=k |\u0398jk|\n  , (9)\nwhere \u03a3\u0304 refers to the sample covariance matrix on the samples x1, x2, ...xn, \u2211 j 6=k |\u0398jk| refers to the sum of absolute value of the non-diagonal elements in matrix \u0398."}, {"heading": "3.3 Binary Classification as Bayesian Inference via Regularized Wishart Prior", "text": "Using the typical inverse-wishart sampling algorithm [32], FWDA first randomly generated m inverse-covariance matrices \u03981,\u03982...\u0398m drawn from the Wishart Distribution W(T\u0302 , v). With the \u03981,\u03982...\u0398m, we approximate Eq. 4 as:\ng\u0304(x) = 1\nm\n\u2211\n1\u2264i\u2264m\n( f(x,\u0398\u22121i )P (x|\u0398\u22121i ) ) , (10)\nwhere P (x|\u0398\u22121i ) refers to the probability of the input vector x given the inverse covariance matrix \u0398i. In this paper, we characterize the probability as:\nP (x|\u0398\u22121i ) = 1\u221a 2\u03c0|\u0398\u22121i | e\u2212 1 2 (x\u2212x\u0304)T \u0398i(x\u2212x\u0304), (11)\nwhere x\u0304 = n\u22121 \u2211n\n1 xj refers to the mean vector of all training data. Thus, our algorithm FWDA uses sign(g\u0304(x)) as the classification result. The performance analysis of the proposed algorithm based on Eq. 10 to approximating the formulated problem expressed in Eq. 4 will be addressed in the following section."}, {"heading": "3.4 Approximation Analysis", "text": "In this section, we present how close g\u0304(x) used in FWDA can approximate the reformulated problem g(x).\nFirst of all, considering the fast convergence rate of De-Sparsified Graphical Lasso [31] i.e., ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]:\nAssumption 1. For any positive-semidefinite matrix \u03a3 i.e., \u2200\u03a3 \u2265 0 and \u0398 = \u03a3\u22121, there exists P (\u03a3|x1, x2...xn) = Pw(\u0398|T\u0302 , v), where Pw(\u0398|T\u0302 , v) refers to the Wishart probability of \u0398 based on the mean positive-semidefinite matrix T\u0302 and v = n \u2212 1. T\u0302 is an estimate of inverse covariance matrix on samples x1, x2...xn.\nWith Assumption 1., we can substitute P (\u03a3|x1, x2...xn) with Pw(\u0398|T\u0302 , v) i.e., the conjugate prior of inverse covariance matrix based on Wishart Distribution, to enable the Bayesian inference.\nTheorem 2. Under Assumption 1, for any \u03b7 > 0 sufficiently small, as the number of sampled inverse covariance matrices m \u2192 \u221e, our algorithm g\u0304(x) converges to g(x) with convergence rate Op( \u221a \u2212 log(\u03b7/2)/2m) with probability at least 1\u2212 \u03b7.\nProof. Sampled inverse covariance matrices \u03981,\u03982, ..., \u0398m are i.i.d and all drawn from the Wishart distributionW(T\u0302 , v) with probability density function Pw(\u0398|T\u0302 , v). By the classical Law of Large Numbers we know that as m\u2192\u221e we have\nlim m\u2192\u221e g\u0304(x) =\n\u222b\n\u0398\u22650\nf(x,\u0398\u22121)P (x|\u0398\u22121)Pw(\u0398|T\u0302 , v)d\u0398 = g(x),\nunder Assumption 1. Let \u03b42 be the variance of f(x|\u0398\u22121)P (x|\u0398\u22121) under the distribution given by Pw(\u0398|T\u0302 , v), so that\n\u03b42 = Varwf(x|\u0398\u22121)P (x|\u0398\u22121) = \u222b\n\u0398\u22650\n( f(x,\u0398\u22121)P (x|\u0398\u22121)\u2212 g(x) )2 Pw(\u0398|T\u0302 , v)d\u0398 .\nBy the Central Limit Theorem we know that for any \u03b3 > 0 we have\nlim m\u2192\u221e Pw\n( |g\u0304(x)\u2212 g(x)| \u2264 \u03b3 \u03b4\u221a\nm\n) =\n1\u221a 2\u03c0\n\u222b \u03b3\n\u2212\u03b3 e\u2212t 2/2dt .\nMoreover, based on Hoeffding\u2019s inequality [Hoeffding, 1963], we can conclude that for any \u03b7 > 0 sufficiently small, as m is large, with probability at least 1\u2212 \u03b7 we have\n|g(x)\u2212 g\u0304(x)| \u2264 \u221a \u2212 1\n2m \u00b7 log (\u03b7 2 ) .\nBased on Theorem. 2, we can conclude that the classification result of sign(g\u0304(x)) should be equivalent to Eq. 4, when the number of sampled inverse covariance matrices m is large. Our later experiments show that, with more than 100 sampled inverse covariance matrices m \u2265 100, FWDA can deliver decent performance and consistently outperform baseline algorithms, including SVM, Kernel SVM, Random Forest and AdaBoost."}, {"heading": "4 Evaluation", "text": "In this section, we first introduce the experimental design of our evaluation. Then we present the experimental results, including the performance comparison between the proposed FWDA algorithm, existing LDA baselines and other predictive models. Moreover, a comparison between FWDA and the method using a simple discretization strategy to support our theoretical analysis of FWDA."}, {"heading": "4.1 Experimental Setups", "text": "We use the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [36]. Among all diseases recorded in CHSN, we choose mental health disorders, including anxiety disorders, mood disorders, depression disorders, and other related disorders, as the targeted disease for early detection. We represent each patient using his/her diagnosis-frequency vector based on the clustered code set (the number of dimensions p = 295), where four clustered codes are considered to represent the diagnoses of mental health disorders and we do not predict these four types of mental disorders separately. Specifically, if a patient has any of\nthese four codes in his/her EHR, we say that he/she has been diagnosed with mental health disorders as ground truth.\nIn order to test the EARLY detection of diseases, for each patient with mental health disorders, we use his/her historical EHR data that was generated 90 days before he/she received the first mental health disorders diagnosis. Further, patients with less than two visits were excluded from the analysis.\nTo demonstrate the effectiveness of our method, we compare our method with baseline algorithms in terms of the following metrics: Accuracy and F1-Score. Specifically, the Accuracy metric characterizes the proportion of patients who are accurately classified in the early detection of mental disorders. The F1-Score measures both correctness and completeness of the early detection.\nBaseline Algorithms - To validate the superiority of FWDA over classical LDA, we use six baseline approaches for comparison:\n\u2022 LDA \u2013 This algorithm is based on the common implementation of generalized Fishier\u2019s discriminant analysis listed in Equation 1. Specifically, LDA uses the sample covariance estimation, and inverts the covariance matrix using pseudoinverse [37] when the matrix inverse is not available.\n\u2022 Linear Support Vector Machine (SVM-Linear) and SVM with Gaussian Kernels (SVM-G) - We compare our algorithm with both linear SVM (SVM-Linear) and nonlinear Kernel SVM with Gaussian Kernels (SVM-G). Both SVM classifiers are well-tuned among a wide range of parameters. Specifically, we report the performance of SVM-G classifiers with bandwidth parameter 0.1 and 1.0 in our research.\n\u2022 Decision Tree (D-Tree), Random Forest and AdaBoost - To compare our solution with the tree-based hypotheses, we use Decision Tree (D-Tree), and Random Forest for comparisons. Further, an AdaBoost classifier based on Logistic Regression is also used for comparison, as an advanced logistic regression baseline. Specifically, we report the performance of Random Forest and AdaBoost with 100 and 200 classifiers instances, respectively.\n\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].\nWe perform experiments with the following settings: to build the training sets, we randomly select 50 to 500 patients with mental health disorders as the positive training samples, and randomly select the same number of patients not been diagnosed with any mental health disorders as negative training samples. Thus the training set for the two classes is balanced (i.e., the number of dimensions p = 295 and training set size is 50 \u223c 500\u00d7 2). To build the testing sets, we randomly select 200 patients (not included in the training set) from both positive/negative groups. Also the testing set is balanced. For each setting, we execute the seven algorithms and repeat 30 times."}, {"heading": "4.2 Experimental Results", "text": ""}, {"heading": "4.2.1 Overall Comparison", "text": "Figure. 1 presents the performance of our approach, along with classical LDA, linear SVM, Kernel SVM and Decision Trees on 200 testing samples and varying training sample sizes. FWDA(200, 1.0) refers to the FWDA classifier based on 200 sampled inverse covariance matrices using De-Sparsified Graphical Lasso with \u03bb = 1.0 for Wishart mean matrix estimation. As can be seen from the results, FWDA clearly outperforms the baseline algorithms in terms of overall accuracy, and F1-score.\nDue to the space limit, we don\u2019t present the comparison results based on Two-stage LDA, Logistic Regression and LDA with shrinkage estimators in Figure. 1. FWDA(200, 1.0) outperforms Two-stage LDA by achieving on average 4.3% higher accuracy and 3.5% higher F1-score, it outperforms Logistic Regression with, on average, 13.2% higher accuracy and 70.4% higher F1-score, and also outperforms LDA with shrinkage estimators (with fined-tuned parameters) with, on average, 12.5% higher accuracy and 23.9% higher F1-score. It is clear that FWDA outperforms all these algorithms significantly."}, {"heading": "4.2.2 Comparison to Ensemble learners", "text": "As FWDA ensembles the classification results from multiple classifiers, we also compared FWDA to the existing ensemble learning algorithms, such as Random Forest and AdaBoost. To compare with ensemble learners with 100 and 200 basis classifiers, we use FWDA with 100 and 200 sampled inverse covariance matrices (i.e., ensemble with 100 and 200 LDA classifiers), with \u03bb = 1.0 for Wishart mean matrix regularization.\nThe performance comparison is illustrated in Figure. 2. It is obvious that FWDA outperforms these two algorithms in both 100-instance and 200-instance settings, while the performance of Random Forest is not quite stable. Moreover, Figure. 2 also shows the performance of FWDA classifiers with 100 and 200 sampled inverse covariance\nmatrices are very similar. Indeed, we tested FWDA with 50 to 2000 inverse covariance matrices, the prediction accuracy or F1-scores of FWDA is almost consistent on the varying number of sampled matrices. This indicates that FWDA can provide robust prediction performance, even when only a small number of inverse covariance matrices are sampled."}, {"heading": "4.2.3 Comparison on Discretization and Regularization", "text": "FWDA leverages importance sampling-alike method to improve the performance of approximation to the real integral in a discretization manner. We compare FWDA to a classifier namely \u201cDiscrete-FWDA\u201d based on the simple discretization strategy:\nsign\n  \u2211\n1\u2264i\u2264m f(x,\u0398\u22121i )P (x|\u0398\u22121i )Pw(\u0398i|T\u0302 , v)\n  .\nBoth two algorithms leverage a De-sparsified Graphical Lasso with \u03bb = 1.0, 10.0, 100.0 for the Wishart mean matrix estimation. Figure. 3 presents the performance comparison. It shows FWDA outperforms the simple discretization strategy using the same \u03bb significantly.\nIn Figure 3, we also demonstrate the performance improvement contributed by regularization (De-sparsified graphical lasso) for Wishart mean matrix estimation. The lines entitled \u201cSample-FWDA\u201d refer to a derived method using the sample inverse covariance matrix (pseudo-inverse when the covariance matrix is singular) as the Wishart mean matrix. It shows that FWDA can outperform Sample-FWDA significantly."}, {"heading": "4.3 Comparison on Time Consumption", "text": "In addition to the accuracy comparison, we also compare the time consumption of FWDA (based on \u201cLazy Sampling\u201d) with an alternative method that is based on the same regularized Wishart distribution. For each new data classification, it samples a group of inverse covariance matrices for LDA classification and voting. Given 400\u00d72\ntraining samples, FWDA(200,1.0), it on average consumes 149.5 seconds to train the model (including De-sparsified Graphical Lasso with \u03bb = 0.1 and sampling 200 inverse covariance matrices), and 0.037 seconds to classify 200\u00d72 samples for testing. We evaluate the alternative algorithm in the same settings \u2013 the alternative algorithm doesn\u2019t need to be trained, and it on average consumes 16.7 hours to classify the 200\u00d72 testing samples. The time consumption to train a FWDAmodel is almost the same as the time consumed to classify one sample by the alternative method. The experiments were all carried out using an iMac desktop with 3.1 GHz Intel Core i5 CPU, 16G memory and macOS v10.12."}, {"heading": "5 Conclusion and Discussion", "text": "In this paper, we proposed FWDA \u2013 a novel algorithm for early detection of diseases, based on EHR data and the diagnosis-frequency vector data representation. FWDA lowers the uncertainty of LDA parameter estimation and further enables the nonlinear classification, through the Input-Adaptive Bayesian voting scheme. The theoretical analysis shows that FWDA converges to the optimal Bayesian voting in a fast rate. The experimental results on real-world EHR dataset CHSN show that FWDA outperforms all baseline algorithms. In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction."}], "references": [{"title": "Supervised patient similarity measure of heterogeneous patient", "author": ["Jimeng Sun", "Fei Wang", "Jianying Hu", "Shahram Edabollahi"], "venue": "records. ACM SIGKDD Explorations Newsletter,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Personalized predictive modeling and risk factor identification using patient similarity", "author": ["Kenney Ng", "Jimeng Sun", "Jianying Hu", "Fei Wang"], "venue": "AMIA Summit on Clinical Research Informatics (CRI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Psf: A unified patient similarity evaluation framework through metric learning with weak supervision", "author": ["Fei Wang", "Jimeng Sun"], "venue": "Biomedical and Health Informatics, IEEE Journal of,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "MSEQ: Early detection of anxiety and depression via temporal orders of diagnoses in electronic health data", "author": ["Jinghe Zhang", "Haoyi Xiong", "Yu Huang", "Hao Wu", "Kevin Leach", "Laura E. Barnes"], "venue": "In Big Data (Workshop),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Mining medical data for predictive and sequential patterns: Pkdd", "author": ["Susan Jensen", "UK SPSS"], "venue": "In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework", "author": ["Chuanren Liu", "Fei Wang", "Jianying Hu", "Hui Xiong"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clinical risk prediction by exploring high-order feature correlations", "author": ["Fei Wang", "Ping Zhang", "Xiang Wang", "Jianying Hu"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["Onur C Hamsici", "Aleix M Martinez"], "venue": "TPAMI, 30(4):647\u2013657,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Random matrix theory in pattern classification: An application to error estimation", "author": ["Amin Zollanvari", "Edward R Dougherty"], "venue": "In 2013 Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization", "author": ["Joyce C Ho", "Joydeep Ghosh", "Jimeng Sun"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Estimating structured highdimensional covariance and precision matrices: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zhao Ren", "Harrison H Zhou"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "The use of shrinkage estimators in linear discriminant analysis", "author": ["Roger Peck", "John Van Ness"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1982}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["Juwei Lu", "Konstantinos N Plataniotis", "Anastasios N Venetsanopoulos"], "venue": "Pattern Recognition Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Covariance-regularized regression and classification for high dimensional problems", "author": ["Daniela M Witten", "Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse linear discriminant analysis by thresholding for high dimensional data", "author": ["Jun Shao", "Yazhen Wang", "Xinwei Deng", "Sijian Wang"], "venue": "The Annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Statistics for high-dimensional data: methods, theory and applications", "author": ["Peter Buhlmann", "Sara Van De Geer"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["Neil D Lawrence", "Bernhard Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Learning metrics via discriminant kernels and multidimensional scaling: Toward expected euclidean representation", "author": ["Zhihua Zhang"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Optimal kernel selection in kernel fisher discriminant analysis", "author": ["Seung-Jean Kim", "Alessandro Magnani", "Stephen Boyd"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Regularized discriminant analysis, ridge regression and beyond", "author": ["Zhihua Zhang", "Guang Dai", "Congfu Xu", "Michael I Jordan"], "venue": "JMLR, 11(Aug):2199\u20132228,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Model selection and multimodel inference: a practical information-theoretic approach", "author": ["Kenneth P Burnham", "David R Anderson"], "venue": "Springer Science & Business Media,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Bayesian model averaging: a tutorial", "author": ["Jennifer A Hoeting", "David Madigan", "Adrian E Raftery", "Chris T Volinsky"], "venue": "Statistical science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Bayesian voting schemes and large margin classifiers", "author": ["Nello Cristianini", "John Shawe-Taylor"], "venue": "Advances in Kernel MethodsSupport Vector Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm", "author": ["Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Jean-Francis Roy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Icd-9 codes and surveillance for clostridium difficile\u2013associated disease", "author": ["Erik R Dubberke", "Kimberly A Reske", "L Clifford McDonald", "Victoria J Fraser"], "venue": "Emerging infectious diseases,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Monte carlo methods", "author": ["John Hammersley"], "venue": "Springer Science & Business Media,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Methods of numerical integration", "author": ["Philip J Davis", "Philip Rabinowitz"], "venue": "Courier Corporation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Estimation of the inverse covariance matrix: Random mixtures of the inverse wishart matrix and the identity", "author": ["LR Haff"], "venue": "The Annals of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1979}, {"title": "Confidence intervals for high-dimensional inverse covariance estimation", "author": ["Jana Jankova", "Sara van de Geer"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wishart distributions and inverse-wishart sampling", "author": ["S Sawyer"], "venue": "URL: www. math. wustl. edu/sawyer/hmhandouts/Whishart. pdf,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Bayesian inference for a covariance matrix", "author": ["Tom Leonard", "John SJ Hsu"], "venue": "The Annals of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1992}, {"title": "Bayesian quadratic discriminant analysis", "author": ["Santosh Srivastava", "Maya R Gupta", "B\u00e9la A Frigyik"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Bayesian inference for a covariance matrix", "author": ["Ignacio Alvarez", "Jarad Niemi", "Matt Simpson"], "venue": "arXiv preprint arXiv:1408.4050,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "College Health Surveillance Network: Epidemiology and Health Care Utilization of College Students at U.S. 4-Year Universities", "author": ["James C. Turner", "Adrienne Keller"], "venue": "Journal of American college health: J of ACH,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "An optimization criterion for generalized discriminant analysis on undersampled problems", "author": ["Jieping Ye", "Ravi Janardan", "Cheong Hee Park", "Haesun Park"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Two-dimensional linear discriminant analysis", "author": ["Jieping Ye", "Ravi Janardan", "Qi Li"], "venue": "In NIPS,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "A two-stage linear discriminant analysis via qrdecomposition", "author": ["Jieping Ye", "Qi Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Efficient l \u0303 1 regularized logistic regression", "author": ["Su-In Lee", "Honglak Lee", "Pieter Abbeel", "Andrew Y Ng"], "venue": "In AAAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The ubiquity of Electronic Health Records (EHR) [1, 2] in healthcare systems provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 48, "endOffset": 54}, {"referenceID": 1, "context": "The ubiquity of Electronic Health Records (EHR) [1, 2] in healthcare systems provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 2, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 1, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 128, "endOffset": 134}, {"referenceID": 4, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 128, "endOffset": 134}, {"referenceID": 5, "context": "Existing researches on it first extract useful features, such as diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 2, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 1, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 6, "context": "Then, supervised learning techniques are adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, and Linear Discriminant Analysis (LDA) [1, 3, 2, 4, 7].", "startOffset": 192, "endOffset": 207}, {"referenceID": 3, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 6, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Among these methods, LDA is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "However, recent studies demonstrate the limitation of LDA under high dimension low sample size (HDLSS) settings [9], such as the EHR records [10].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "However, recent studies demonstrate the limitation of LDA under high dimension low sample size (HDLSS) settings [9], such as the EHR records [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the \u201ctrue\u201d (inverse) covariance matrix, with an inconsistent estimate of the largest eigenvalues and almost-orthogonal eigenvectors to the truth [11].", "startOffset": 285, "endOffset": 289}, {"referenceID": 0, "context": "Moreover, EHR data is usually not linearly separable [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 1, "context": "Moreover, EHR data is usually not linearly separable [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 11, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 12, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 13, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 188, "endOffset": 200}, {"referenceID": 14, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 224, "endOffset": 232}, {"referenceID": 15, "context": "To address the ill-posed problems and the linear inseparability of the data, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [12, 13, 14] or linear coefficients [15, 16] under high dimension and low sample size settings [17].", "startOffset": 283, "endOffset": 287}, {"referenceID": 16, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 17, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 18, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 19, "context": "Further, to handle the non-linearity, some kernel-based or nonparameteric LDA classifiers [18, 19, 20, 21] have been proposed.", "startOffset": 90, "endOffset": 106}, {"referenceID": 20, "context": "In summary, these methods intend to improve LDA classification through optimizing the parameters of LDA, such as (inverse) covariance matrices, linear projection metrics, or kernel settings, in a so-called optimal model selection manner [22].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "Instead of \u201cbidding\u201d the optimal parameter in the full and usually unknown parameter space, in this work, we intend to improve LDA in an ensemble way [23], while adapting to the new input data.", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "Specifically, we first sample a set of (inverse) covariance matrices from the both training data and the new input data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting Scheme [24].", "startOffset": 286, "endOffset": 290}, {"referenceID": 22, "context": "Theoretical studies show that such Bayesian voting scheme can secure a wider margin and guarantee a good classification performance with a lower generalization error bound [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 23, "context": "This theoretically guarantees that the proposed framework can \u201con average\u201d outperform those regularization-based LDA classifiers using only single (inverse) covariance matrix estimator [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 2, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 1, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 160, "endOffset": 169}, {"referenceID": 3, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 201, "endOffset": 207}, {"referenceID": 4, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 201, "endOffset": 207}, {"referenceID": 5, "context": "EHR Data Representation using Diagnosis-Frequency Vectors - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1, 3, 2], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 258, "endOffset": 261}, {"referenceID": 24, "context": "Given each patient\u2019s EHR data, this method first retrieves the diagnosis codes [26] recorded during each visit.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "3 Bayesian Voting Scheme Given a binary classifier h\u03c9(x) \u2208 {\u00b11}, which is parameterized by \u03c9, the Bayesian Voting Classification [24] of the classifier is:", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "The theoretical advantages of Bayesian voting scheme are addressed in [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "4, a common solution is to leverage a Monte-Carlo Integration algorithm [28] that", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "For the high-dimensional numeric integration [29], the approximation is usually bottle-necked by the number of dimensions (e.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "xn), FWDA leverages a Wishart Distribution [30] namelyW(T\u0302 , v), where T\u0302 refers to the \u201cmean\u201d positivedefinite matrix for the Wishart distribution and v is the degree of freedom.", "startOffset": 43, "endOffset": 47}, {"referenceID": 28, "context": "Specifically, in our research, we set the degree of feedom v as v = n \u2212 1, and further estimate T\u0302 using De-sparsified Graphical Lasso [31]: T\u0302 = 2\u0398\u0302\u2212 \u0398\u0302\u03a3\u0304\u0398\u0302.", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "3 Binary Classification as Bayesian Inference via Regularized Wishart Prior Using the typical inverse-wishart sampling algorithm [32], FWDA first randomly generated m inverse-covariance matrices \u03981,\u03982.", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "First of all, considering the fast convergence rate of De-Sparsified Graphical Lasso [31] i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 31, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 32, "context": ", ||T\u0302 \u2212 \u0398\u2217||\u221e = Op( \u221a log p /n), with a fixed number of dimensions p and an increasing number of samples n, we are more confident to follow an assumption frequently made in many of previous Bayesian inference studies [33, 34, 35]: Assumption 1.", "startOffset": 218, "endOffset": 230}, {"referenceID": 33, "context": "1 Experimental Setups We use the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [36].", "startOffset": 225, "endOffset": 229}, {"referenceID": 34, "context": "Specifically, LDA uses the sample covariance estimation, and inverts the covariance matrix using pseudoinverse [37] when the matrix inverse is not available.", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 140, "endOffset": 148}, {"referenceID": 36, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 140, "endOffset": 148}, {"referenceID": 37, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "\u2022 Two-stage LDA, Logistic Regression and LDA with Shrinkage Estimators - We also compared FWDA to other competitors including two-stage LDA [38, 39], logistic regression [40], and LDA with shrinkage estimators [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 0, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 2, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 1, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 3, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}, {"referenceID": 5, "context": "In our future work, we intend to integrate FWDA with advanced EHR data representation techniques [1, 3, 2, 4, 6], to enable the superior prediction.", "startOffset": 97, "endOffset": 112}], "year": 2017, "abstractText": "Linear Discriminant Analysis (LDA) on Electronic Health Records (EHR) data is widely-used for early detection of diseases. Classical LDA for EHR data classification, however, suffers from two handicaps: the ill-posed estimation of LDA parameters (e.g., covariance matrix), and the \u201clinear inseparability\u201d of EHR data. To handle these two issues, in this paper, we propose a novel classifier FWDA \u2014 Fast Wishart Discriminant Analysis, that makes predictions in an ensemble way. Specifically, FWDA first surrogates the distribution of inverse covariance matrices using a Wishart distribution estimated from the training data, then \u201cweightedaverages\u201d the classification results of multiple LDA classifiers parameterized by the sampled inverse covariance matrices via a Bayesian Voting scheme. The weights for voting are optimally updated to adapt each new input data, so as to enable the nonlinear classification. Theoretical analysis indicates that FWDA possesses a fast convergence rate and a robust performance on high dimensional data. Extensive experiments on large-scale EHR dataset show that our approach outperforms stateof-the-art algorithms by a large margin.", "creator": "LaTeX with hyperref package"}}}