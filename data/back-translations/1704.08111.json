{"id": "1704.08111", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2017", "title": "A Popperian Falsification of AI - Lighthill's Argument Defended", "abstract": "The area of computation called artificial intelligence (AI) is distorted by the description of an earlier distortion of AI by British applied mathematician James Lighthill in 1972. It is explained how Lighthill's arguments continue to apply to the current AI. It is argued that AI should apply the Popperian method to cell biology, where it is the duty of every scientist to falsify theories and when theories are falsified to replace or modify them. The paper then describes in detail the Popperian method and discusses Paul Nurse's application of the method to cell biology, which also includes questions of mechanism and behavior. Arguments used by Lighthill in his original 1972 report that falsified AI are discussed.", "histories": [["v1", "Sun, 23 Apr 2017 21:16:40 GMT  (34kb)", "http://arxiv.org/abs/1704.08111v1", "8 Pages"]], "COMMENTS": "8 Pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["steven meyer"], "accepted": false, "id": "1704.08111"}, "pdf": {"name": "1704.08111.pdf", "metadata": {"source": "CRF", "title": "A Popperian Falsification of AI - Lighthill\u2019s Argument Defended", "authors": ["Steven Meyer"], "emails": ["smeyer@tdl.com"], "sections": [{"heading": "1. Introduction", "text": "This paper applies the method of falsification discovered by Karl Popper to show that artificial intelligence (AI) programs are not intelligent and in fact are just normal computer programs in which programmers express their ideas by writing computer code. AI is meaningless metaphysics in the Popperian sense of metaphysics based on a number of incorrect assumptions and dogmas that was falsified by James Lighthill in his evaluation of AI for the British science funding agency (Lighthill[1972]). This paper defends Lighthill\u2019s 20th century falsification of AI and explains how it applies to current AI.\nThis paper presents material the author developed from being encouraged to criticize AI as a 1960s Stanford University undergraduate and from a talk given to Paul Feyerabend\u2019s philosophy of science seminar while the author was a computer science (CS) student at UC Berkeley. In order to understand why Lighthill\u2019s criticism falsifies AI and why his arguments still apply to AI now in the second decade of the 21st century in spite of vast improvements in computer speed and capacity, it is necessary to understand the development of modern computers primarily by physicists after WWII. The paper uses recent historical scholarship to explain Lighthill\u2019s background assumptions and shows how that background knowledge also falsifies current AI."}, {"heading": "2. What is Popperian falsification", "text": "Falsification is a method discovered by Karl Popper that argues general statements do not\nhave scientific merit. Only singular statements Popper calls basic statements that have simple structure have meaning. Such statements can be disproven either by scientific experiments or by logic (Popper[1968], p. 74). Popper\u2019s major contribution to the philosophy of science is to insist that it is the duty of every scientist to criticizes one\u2019s own theories to the fullest extent possible so that false theories can be modified or replaced. Popperians believe scientific method consists of numerous bold conjectures that are then tested and if falsified, eliminated or modified. Popper\u2019s method calls for bold conjecture followed by stringent criticism.\nPopper\u2019s original falsification theory developed in the late 1920s and early 1930s is called naive falsification (Lakatos[1999], pp. 64-85). The theory was improved and generalized by Popper and his colleagues during most of the 20th century. I am using the term Popperian philosophy in a sense that includes the modifications and improvement to Popper\u2019s theory mostly carried out at the London School of Economics not just by Popper but also by: Imre Lakatos, Paul Feyerabend and Thomas Kuhn. The other aspects of Popperian methodology is most clearly expressed by Imre Lakatos as the Methodology of Scientific Research Pro grammes (MSRP) (Lakatos[1970]). There were disagreements among the Popperians about questions of emphasis but not about methodology or importance of rationality in science. James Lighthill, as holder of the Lucasian chair in applied mathematics at Cambridge University, was familiar with and part of the milieu that developed Popperian theory.\nFalsification as a theory in the philosophy of science is usually discussed in terms of physics because the developers were trained as physicists. Physics is possibly not a good fit for study of AI methodology because there is no mechanism or functional explanation involved in attempting to understand physical reality (describe fields or particle interactions for example). The connection to cell biology that attempts to understand and utilize the mechanisms of cell behavior is closer. Paul Nurse in his 2016 Popper Memorial Lecture discusses the importance of bold conjectures and diligent attempts to eliminate incorrect theory by falsification (Nurse[2016]). Nurse also discussed data analysis in cell biology. For readers unfamiliar with Popperian falsification, the Nurse lecture provides an excellent introduction.\nFalsification of AI is important because it is claimed that computational intelligence is now so successful that discussions of ethical issues involving how inferior humans will deal with the superior intellect of AI robots are required. The author believes the primary obligation of scientists is to eliminate false theories."}, {"heading": "3. Lighthill\u2019s falsification of AI", "text": "Lighthill\u2019s falsification of AI is quite simple (Lighthill[1972]) and I claim continues to apply to AI in spite of changes mostly in vastly faster computers that execute machine instructions in parallel and new names for algorithms such as \"deep learning\" that replaces alpha beta heuristics to improve logic resolution algorithms to implement intelligence. Lighthill argues AI is just CS described using the language of human intelligence and views computers and computation as tools for expressing people\u2019s ideas.\nLighthill divides AI into three areas. Category A: Automation (feedback control engineering), Category C: computer based studies of the central nervous system, and Category B: the bridge area between A and B this is supposedly going to provide the magic synergy that allow creation of intelligent robots (p. 3). For example, current deep learning would fall into areas A and B. It falls into category B because it involves automatic logical deduction without any need for a person to program ideas into the algorithm, but also it is in category A because it \"looks beyond conventional data processing to the problems involved in large-scale data banking and retrieval.\" (p. 5) I think Lighthill is arguing here that AI studies normal computer science but\nrephrases problems in terms of human attributes (p. 7 paragraph 2).\nAccording to Lighthill for control engineering it should not matter how the engineering is accomplished. Lighthill writes in the section discussing category A: \"Nevertheless it (AI) must be looked at as a natural extension of previous work of automation of human activities, and be judged by essentially the same criteria.\" (p. 4 paragraph 4). After more than 40 years of computer development, programmable digital computers are usually the best choice for control engineering. In modern terms current feedback control engineering is based on improvements in camera technology allowing more precise location measurements and more complex feedback. Advances and cost reductions in computer and storage technology allow large amounts of data to be processed faster and at lower cost.\nIn criticizing AI\u2019s approach to area C since obviously it makes sense to study neurophysiology, Lighthill distinguishes syntactic automation as advocated currently by AI versus conceptual automation (p. 6). He asks if \"a device that mimics some human function somehow assists in studying and making a theory of the function of the central nervous system.\" (p. 6 paragraph 4)\nLighthill criticizes the use of mathematical logic in AI by arguing practical use runs into a combinatorial explosion (p. 10 paragraph 5) and argues there are difficulties in storing axioms favored by logicians versus heuristic knowledge favored by AI (p. 10, paragraph 6). In my view this is the crucial falsifier of AI. Namely, although Lighthill was attempting to provide a neutral assessment of AI, he did not believe in the Hilbert Programme that is the central tenet of AI.\nLighthill also discusses organization problems with AI methodology. He questions claims such as \"robots better than humans by 2000\" (p. 13) (now probably replace with 2030). Lighthill as an applied mathematician also discusses the combinatorial explosion that humans solve but can not be solved by formal algorithms."}, {"heading": "3.1 Understanding Lighthill\u2019s falsification in modern terms", "text": "In 1972 Lighthill falsified AI by showing its individual claims were false and by arguing there was no unified subject but rather just normal problems in the are of computation involving computer applications and study of data. AI researchers were not convinced at the time, I think, because Lighthill did not make his Popperian view of science clear. The remainder of this paper discusses how 1970s scientific background knowledge especially in the physics and applied mathematics areas falsifies current AI methods. The discussion is possible because of recent scholarship especially in the areas of Hilbert\u2019s philosophical programme and in the study of John von Neumann\u2019s thinking during the development of digital computers."}, {"heading": "4. Skepticism toward Hilbert\u2019s programme of truth as formal proof", "text": "In the 1920s, mathematician David Hilbert conjectured that knowledge and truth consists solely of all sentences that can be proven from axioms. Hilbert\u2019s original conjecture was a mathematical problem. However, it was interpreted as a philosophical theory in which truth became formal proof from axioms. A paradigmatic example is the Birkhoff and Von Neumann formalization of quantum mechanics as axiomatized logic (Birkhoff[1936], Popper]1968] attempted to falsify it). Hilbert\u2019s programme as the basic assumption of AI is that knowledge about the world can be expresses as formal sentences. Knowledge is then expressed as formulas that can be derived using logic (usually predicate calculus) from other sentences about the world that are true.\nIn addition to the belief that knowledge is formal sentences, the foundation of AI is the belief that the Church\u2019-Turing Thesis (Copeland[2015]) is true. Namely, that nothing can exist\noutside of formally proven sentences. proven from axioms. In the AI community this dogma is beyond criticism. However, the philosophical Hilbert programme was abandoned starting in the 1930s for various reasons. The reason most often given is that Goedel\u2019s incompleteness results showed the Hilbert programme could not succeed. The Hilbert programme is still believed in the logic area and AI seems to be grasping at the straw of attempts to mitigate the Goedel disproof by finding in practice areas where Goedel\u2019s results do not apply. Zach[2015] Stanford Encyclopedia of Philosophy article discusses some attempts to mitigate Goedel\u2019s results. See Detlefsen[2017]) for a more skeptical view of Hilbert\u2019s programme.\nThere were a number of other reasons Hilbert\u2019s philosophical programme was rejected. These other reasons explain why the AI argument that since people have intelligence computer programs can also have intelligence. In the view of AI, the problem is just building faster computers and developing better algorithms so that computers can discover and learn the formal sentences in people\u2019s heads. In fact the other reasons the Hilbert programme was abandoned show why Lighthill\u2019s falsification is correct and why AI is meaningless metaphysics."}, {"heading": "4.1 Von Neumann\u2019s argument automata and neural networks useless at high levels of complexity", "text": "During the second half of the 20th century, John von Neumann\u2019s work on computers and computations was widely accepted. Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]). However Lighthill as an applied mathematician was certainly familiar with Von Neumann\u2019s work.\nJohn Von Neumann studied automata and neural networks when he was developing his Von Neumann computer architecture. Von Neumann combined all his skepticism toward linguistics and automata as sources of AI algorithms in discussing problems with formal neural networks when he wrote:\nThe insight that a formal neuron network can do anything which you can describe in words a very important insight and simplifies matters enormously at low complication levels. It is by no means certain that it is a simplification on high complication levels. It is perfectly possible that on high complication levels the value of the theorem is in the reverse direction, namely, that you can express logics in terms of these efforts and the converse may not be true (Von Neumann[1966], quoted in Aspray[1990], note 94, p. 321).\nVon Neumann also considered and rejected current AI methodology when he developed the Von Neumann computer architecture. In a 1946 paper with Herman Goldstine on the design of a digital computer Von Neumann believed that some sort of intuition had to be built into programs instead of using brute force searching (Aspray[1990], p. 62). Edward Kohler (Kohler[2000]), p. 118) describes von Neumann\u2019s discovery in developing modern computer architecture in an article \"Why von Neumann Rejected Carnap\u2019s Duality of Information Concepts\" as:\nMost readers are tempted to regard the claim as trivial that automata can simulate arbitrarily complex behavior, assuming it is described exactly enough. But in fact, describing behavior exactly in the first place constitutes genuine scientific creativity. It is just such a prima facie superficial task which von Neumann achieved in his [1945] famous explication of the \"von Neumann machine\" regarded as the standard arc hitecture for most post World-War-II computers.\nThe problem context in the area of operations research solution space searching that influenced both von Neumann and Lighthill was pre computer algorithmic operations research experience (see Budiansky[2013] for the detailed story). Understanding the limitations of\ncombinatorial explosion arises naturally from that experience."}, {"heading": "4.2 Skepticism toward linguistics and formal languages in computing", "text": "Starting with Ludwig Wittgenstein in the late 1930s, skepticism toward linguistics and especially formal languages become prevalent. Wittgenstein\u2019s claim was that mathematical (and other) language was nothing more than pointing (Wittgenstein[1930]). The Popperians and English science in general were receptive to Wittgenstein and his \"pointing\" philosophy of mathematics. Popperians avoid linguistic philosophy because they viewed it as creating more problems than it solved. I read Lighthill\u2019s falsification as assuming this attitude toward language. Modern AI still claims knowledge and truth is limited to provable formal sentences."}, {"heading": "5. Physicist skepticism towards mathematics as axiomatized logic", "text": "In my view there was a more important reason for the rejection of Hilbert\u2019s programme. Physicists were always skeptical toward axiomatized mathematics. Albert Einstein in his 1921 lecture on geometry expresses this skepticism. Einstein believed that formal mathematics was incomplete and disconnected from physical reality. Einstein stated:\nThis view of axioms, advocated by modern axiomatics, purges mathematics of all extraneous elements. ... such an expurgated exposition of mathematics makes it also evident that mathematics as such cannot predicate anything about objects of our intuition or real objects (Einstein[1921]).\nNiels Bohr argued that first comes the conceptual theory then the calculation."}, {"heading": "6. Finsler\u2019s rejection of axiomatics and general 1926 inconsistency result", "text": "In addition to skepticism toward axiomatics, there was also skepticism toward set theory and its core claim that only sentences that are derivable from axioms (Zermelo Fraenkel probably) can exist. Swiss mathematician Paul Finsler believed that mathematics exists outside of language (formal sentences). Finsler claimed to have shown incompleteness in formal systems before Goedel in 1925 and that his proof was superior because it was not tied to Russell\u2019s logic as Goedel\u2019s was. See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p. 257 for discussion of Finsler\u2019s result on undecidability and formal proofs and its history (also Finsler[1996] and Finsler[1969]."}, {"heading": "7. Chess - elite human players response to chess programs", "text": "Superiority of chess programs over even the best human chess players is cited as evidence that in the future AI robots will be superior in all areas involving intelligence. In fact the situation is more complicated. The response by the world\u2019s best chess players shows that Lighthill\u2019s claims that even in a formal sentenced based toy world, combinatorial explosion limits problem solving ability of algorithms. Study of chess playing programs and evaluation of their efficacy show the problems with recent claims of AI successes in general.\nIn 1997, the Deep Blue chess program defeated then world champion Gary Kasperov. Since then the world\u2019s best chess players have adjusted to computer chess programs. In the December 31 Financial Times newspaper chess column, Leonard Barton referring to US champion Fabiano Caruana writes:\nThe US champion and world No. 2 unleashed a brilliant opening novelty, which incidentally showed the limitations of the most powerful computers (Barton[2016]).\nIt has taken two decades and Caruana was only five years old when Kasperov lost to Deep Blue, but it appears computer algorithms will run into the combinatorial explosion problems and more\nand more of the best players will defeat computers.\nPossibly more interesting is how the claims show problems with AI scientific methodology and emphasize the lack of diligent attempts to falsify AI theory. First, the financial incentive structure of the challenge meant that Kasperov made more money by losing rather than by winning. From Kasperov\u2019s viewpoint he could win and go back to collecting meager chess tournament prize money or lose and collect a large appearance fee plus receiving numerous other appearance fees as a marketing representative. Many AI claims of success involving human competition with computers follow this pattern. At a minimum, AI tests of this type need to use double blind protocols. A better method for determining if computers can defeat the best human players would be to use double blind tournaments where opponents may be humans or computers and participants and officials were not allowed to know who was who. Even better would be a system where chess player\u2019s natural competitiveness was utilized so that losing to a lower rated human player would result in a large deduction of rating points.\nFinally, progress in chess playing computer programs shows that chess programs are normal data processing applications in the Lighthill sense in which human knowledge of chess can be expressed and amplified by injecting it into a computer by writing a computer program."}, {"heading": "8. Turing Machine incorrect model for computation", "text": "The central argument for AI is based on the Church Turing thesis. Namely that Turing machines (TM) are universal and anything that involves intelligence can be calculated by TMs. Applying Lighthill\u2019s combinatorial explosion arguments, it seems to me that TMs are the wrong model of computation. Instead a different computational model called MRAMS (random access machines with unit multiply and a bounded number of unbounded size memory cells) is a better model of computation (Meyer[2016]). Von Neumann understood the need for random access memory in his design of the von Neumann architecture (ibid. pp. 5-6). For MRAM machines deterministic and non deterministic computations are both solvable in polynomial bound time so at least for some problems in the class NP, the combinatorial explosion is mitigated. This suggests that algorithms should be studied as normal data processing because AI\u2019s assumption that heuristics and guessing will somehow improve algorithms is problematic."}, {"heading": "9. Conclusion - suggestion to replace AI with Naur\u2019s Dataology", "text": "A problem with this paper is that people trained to perform advanced computational research before the 1970s primarily by physicists can\u2019t imagine AI as having any content, but people trained after CS became formalized as object oriented programming, computer programs verified by correctness proofs and axiomatized proofs of algorithm efficiency can\u2019t imagine anything but computation as formalized logic. Computation researchers trained after the 1970s are unable to imagine alternatives to the AI dogmas. My suggestion is to adopt the ideas of Danish computer scientist, who was trained as an astronomer, Peter Naur. Naur argued that computation should be studied as Dataology. Dataology is a theory neutral term for studying data. Naur wrote \"mental life during the twentieth century has become entirely misguided into an ideological position such that only discussions that adopt the computer inspired form\" are accepted. (Naur[2007], 87).\nIn the 1990s, Peter Naur, one of the founders of computer science, realized that CS had become too much formal mathematics separated from reality. Naur advocated the importance of programmer specific program development that does not use preconceptions. I would put it as computation allows people to express their ideas by writing computer programs.\nThe clearest explanation for Naur\u2019s method appears in the book Conversations - Pluralism\nin Software Engineering (Naur[2011]). This books amplifies the program development method Naur described in his 2005 Turing Award lecture (Naur[2007]). In Naur[2011] page 30, the interviewer asks \"... you basically say that there are no foundations, there is no such thing as computer science, and we must not formalize for the sake of formalization alone.\" Naur answers, \"I am not sure I see it this way. I see these techniques as tools which are applicable in some cases, but which definitely are not basic in any sense.\" Naur continues (p. 44) \"The programmer has to realize what these alternatives are and then choose the one that suits his understanding best. This has nothing to do with formal proofs.\" Dataology without preconceptions and predictions of imminent replacement of human intelligence by robots would improve the scientific study of computation. The next step for advocates of AI would be to try to falsify Naur\u2019s Dataology."}, {"heading": "10. References", "text": "Aspray[1990] Aspray, W. John von Neumann and The Origins of Modern Computing. MIT Press, 1990. Barton[2016] Barton, L. Chess column, Financial Times. Games page weekend life and style section, Dec. 30 and Jan. 1 edition, 2016. Birkhoff[1936] Birkhoff, G. and Von Neumann, J. The Logic of Quantum Mechanics. Annals of Math. 27, no. 4 (1936), 721-734. Breger[1992] Breger, H. A Restoration that failed: Paul Finsler\u2019s theory of sets. In Gillies, D. ed. Revolutions in Mathematics. Oxford, 1992, 249-264. Budiansky[2013] Budiansky, S. Blackett\u2019s War: The Men Who Defeated the Nazi U-Boats and Brought Sciene to the Art of Warfare. Knopf, 2013. Copeland[2015] Copeland, J. The Church-turing thesis. The Stanford Encyclopedia of Philosophy (Summer 2015 Edition), 2015. URL Feb. 2017: plato.stanford.edu/archives/sum2015/entries/church-turing Detlefsen[2017] Detlefsen, M. Hilbert\u2019s programme and formalism. Routledge Encyclopedia of Philosophy. Feb. 2017 URL: www.rep.routledge.com/articles/thematic/ Hilbert\u2019s-programme-andformalism/v-1 Einstein[1921] Einstein, A. Geometry and Experience. Lecture before Prussian Academy of Sciences. Berlin, January 27, 1921, Feb. 2017 URL: www.relativitycalculator.com/pdfs/einstein_geometry_and_experience_1921.pdf Finsler[1969] Finsler, P. Ueber die Unabhaengigkeit der Continuumshypothese. Dialectica 23, 1969, 67-78. Finsler[1996] Finsler, P. Finsler set theory: Platonism and circularity. D. Booth and R. Ziegler eds. Birkhauser, 1996. Kohler[2001] Kohler, E. Why Von Neumann Rejected Carnap\u2019s Duality of Information Concepts. In Redei, M. and Stoltzner, M. eds. John von Neumann and the Foundations of Quantum Physics. Vienna Circle Institute Yearbook 8, Kluwer, 2001, 97-134. Lakatos[1970] Lakatos, I. Falsification and the methodology of scientific research programmes. in I. Lakatos and A. Musgrave eds. Criticism and the growth of knowledge., Cambridge Press, 1970, 91-196. Lakatos[1999] Lakatos, I. and Feyerabend P. For and against method. M. Motterlini ed. University of Chicago Press, 1999. Lighthill[1973] Lighthill, J. \"Artificial Intelligence: A General Survey\" in Artificial Intelligence: a paper symposium. UK Science Research Council, 1973. URL Feb. 2017:\nwww.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf\nMeyer[2013] Meyer, S. Adding Methodological Testing to Naur\u2019s Anti-formalism. IACAP 2013 Proceedings, College Park Maryland, 2013. Meyer[2016] Meyer, S. Philosophical Solution to P=?NP: P is equal to NP, ArXiv:1603:06018, 2016, also solution No. 113 on the P-versus-NP page. Naur[1995] Naur, P. Knowing and the mystique of logic and rules. Kluwer Academic, 1995. Naur[2005] Naur, P., 2005. \"Computing as science\", in An anatomy of human mental life. naur.com Publishing, Appendix 2, 208-217, 2005. URL Feb. 2017: www.Naur.com/Nauranat-ref.html Naur[2007] Naur, P. Computing versus human thinking. Comm. ACM 50(1), 2007, 85-94. Naur[2011] Naur, P. Conversations - pluralism in software engineering. E. Daylight ed. Belgium:Lonely Scholar Publishing, 2011. Neumann[2005] Von Neumann, J. Redei, M. ed. John Von Neumann: Selected Letters. History of Mathematics Series, Vol. 27, American Mathematical Society, 2005. Nurse[2016] Nurse, P. \"How Philosophy Drives Discovery: A scientists view of Popper\" 2016 Popper Memorial Lecture, London School of Economics Podcast, 2016. URL Feb. 2017: http://richmedia.lse.ac.uk/publiclecturesandevents/20160928_1830_howPhilosophyDrivesDiscovery.mp3 Popper[1934] Popper, K. The Logic of Scientific Discovery. Harper Row, 1968 (original in german 1934). Popper[1968] Popper, K. Birkhoff and Von Neumann\u2019s Interpretation of Quantum Mechanics. Nature 219(1968), 682-685. Wittgenstein[1939] Wittgenstein, L. Wittgenstein\u2019s Lectures on the foundations of mathematics Cambridge 1939. C. Diamond ed. University of Chicago Press, 1939. Zach[2015] Zach, R. Hilberts Program. The Stanford Encyclopedia of Philosophy (Spring 2016 Edition), 2016. URL Feb. 2017: https://plato.stanford.edu/entries/hilbert-program/"}], "references": [{"title": "The Logic of Quantum Mechanics", "author": ["G. Birkhoff", "J. Von Neumann"], "venue": "Annals of Math. 27, no", "citeRegEx": "Birkhoff and Neumann,? \\Q1936\\E", "shortCiteRegEx": "Birkhoff and Neumann", "year": 1936}, {"title": "A Restoration that failed: Paul Finsler\u2019s theory of sets", "author": ["H. Breger"], "venue": "In Gillies, D. ed. Revolutions in Mathematics. Oxford,", "citeRegEx": "Breger,? \\Q1992\\E", "shortCiteRegEx": "Breger", "year": 1992}, {"title": "Blackett\u2019s War: The Men Who Defeated the Nazi U-Boats and Brought Sciene to the Art of Warfare", "author": ["S. Budiansky"], "venue": null, "citeRegEx": "Budiansky,? \\Q2013\\E", "shortCiteRegEx": "Budiansky", "year": 2013}, {"title": "The Church-turing thesis", "author": ["J. Copeland"], "venue": "The Stanford Encyclopedia of Philosophy (Summer 2015 Edition),", "citeRegEx": "Copeland,? \\Q2015\\E", "shortCiteRegEx": "Copeland", "year": 2015}, {"title": "Hilbert\u2019s programme and formalism", "author": ["M. Detlefsen"], "venue": "Routledge Encyclopedia of Philosophy. Feb", "citeRegEx": "Detlefsen,? \\Q2017\\E", "shortCiteRegEx": "Detlefsen", "year": 2017}, {"title": "Geometry and Experience", "author": ["A. Einstein"], "venue": "Lecture before Prussian Academy of Sciences. Berlin, January", "citeRegEx": "Einstein,? \\Q1921\\E", "shortCiteRegEx": "Einstein", "year": 1921}, {"title": "Finsler set theory: Platonism and circularity", "author": ["P. Finsler"], "venue": "D. Booth and R. Ziegler eds. Birkhauser,", "citeRegEx": "Finsler,? \\Q1996\\E", "shortCiteRegEx": "Finsler", "year": 1996}, {"title": "Why Von Neumann Rejected Carnap\u2019s Duality of Information Concepts", "author": ["E. Kohler"], "venue": "John von Neumann and the Foundations of Quantum Physics. Vienna Circle Institute Yearbook", "citeRegEx": "Kohler,? \\Q2001\\E", "shortCiteRegEx": "Kohler", "year": 2001}, {"title": "Falsification and the methodology of scientific research programmes", "author": ["I. Lakatos"], "venue": "Criticism and the growth of knowledge.,", "citeRegEx": "Lakatos,? \\Q1970\\E", "shortCiteRegEx": "Lakatos", "year": 1970}, {"title": "Artificial Intelligence: A General Survey\" in Artificial Intelligence: a paper symposium", "author": ["J. Lighthill"], "venue": "UK Science Research Council,", "citeRegEx": "Lighthill,? \\Q1973\\E", "shortCiteRegEx": "Lighthill", "year": 1973}, {"title": "Adding Methodological Testing to Naur\u2019s Anti-formalism", "author": ["S. Meyer"], "venue": null, "citeRegEx": "Meyer,? \\Q2013\\E", "shortCiteRegEx": "Meyer", "year": 2013}, {"title": "Philosophical Solution to P=?NP: P is equal to NP", "author": ["S. Meyer"], "venue": "IACAP", "citeRegEx": "Meyer,? \\Q2013\\E", "shortCiteRegEx": "Meyer", "year": 2013}, {"title": "Knowing and the mystique of logic and rules", "author": ["P. Naur"], "venue": "Kluwer Academic,", "citeRegEx": "Naur,? \\Q1995\\E", "shortCiteRegEx": "Naur", "year": 1995}, {"title": "Computing as science\", in An anatomy of human mental life", "author": ["P. Naur"], "venue": "naur.com Publishing,", "citeRegEx": "Naur,? \\Q2005\\E", "shortCiteRegEx": "Naur", "year": 2005}, {"title": "Computing versus human thinking", "author": ["P. Naur"], "venue": "Comm. ACM 50(1),", "citeRegEx": "Naur,? \\Q2007\\E", "shortCiteRegEx": "Naur", "year": 2007}, {"title": "Conversations - pluralism in software engineering", "author": ["P. Naur"], "venue": "E. Daylight ed. Belgium:Lonely Scholar Publishing,", "citeRegEx": "Naur,? \\Q2011\\E", "shortCiteRegEx": "Naur", "year": 2011}, {"title": "How Philosophy Drives Discovery: A scientists view of Popper\" 2016 Popper Memorial Lecture", "author": ["P. Nurse"], "venue": "London School of Economics Podcast,", "citeRegEx": "Nurse,? \\Q2016\\E", "shortCiteRegEx": "Nurse", "year": 2016}, {"title": "The Logic of Scientific Discovery", "author": ["K. Popper"], "venue": "(original in german 1934)", "citeRegEx": "Popper,? \\Q1968\\E", "shortCiteRegEx": "Popper", "year": 1968}, {"title": "Birkhoff and Von Neumann\u2019s Interpretation of Quantum Mechanics", "author": ["K. Popper"], "venue": "Nature", "citeRegEx": "Popper,? \\Q1968\\E", "shortCiteRegEx": "Popper", "year": 1968}, {"title": "Hilberts Program. The Stanford Encyclopedia of Philosophy (Spring 2016 Edition), 2016", "author": ["R. Zach"], "venue": "URL Feb", "citeRegEx": "Zach,? \\Q2017\\E", "shortCiteRegEx": "Zach", "year": 2017}], "referenceMentions": [{"referenceID": 9, "context": "AI is meaningless metaphysics in the Popperian sense of metaphysics based on a number of incorrect assumptions and dogmas that was falsified by James Lighthill in his evaluation of AI for the British science funding agency (Lighthill[1972]).", "startOffset": 150, "endOffset": 240}, {"referenceID": 17, "context": "Only singular statements Popper calls basic statements that have simple structure have meaning. Such statements can be disproven either by scientific experiments or by logic (Popper[1968], p.", "startOffset": 25, "endOffset": 188}, {"referenceID": 8, "context": "Popper\u2019s original falsification theory developed in the late 1920s and early 1930s is called naive falsification (Lakatos[1999], pp.", "startOffset": 114, "endOffset": 128}, {"referenceID": 8, "context": "Popper\u2019s original falsification theory developed in the late 1920s and early 1930s is called naive falsification (Lakatos[1999], pp. 64-85). The theory was improved and generalized by Popper and his colleagues during most of the 20th century. I am using the term Popperian philosophy in a sense that includes the modifications and improvement to Popper\u2019s theory mostly carried out at the London School of Economics not just by Popper but also by: Imre Lakatos, Paul Feyerabend and Thomas Kuhn. The other aspects of Popperian methodology is most clearly expressed by Imre Lakatos as the Methodology of Scientific Research Pro grammes (MSRP) (Lakatos[1970]).", "startOffset": 114, "endOffset": 655}, {"referenceID": 16, "context": "Paul Nurse in his 2016 Popper Memorial Lecture discusses the importance of bold conjectures and diligent attempts to eliminate incorrect theory by falsification (Nurse[2016]).", "startOffset": 5, "endOffset": 174}, {"referenceID": 3, "context": "In addition to the belief that knowledge is formal sentences, the foundation of AI is the belief that the Church\u2019-Turing Thesis (Copeland[2015]) is true.", "startOffset": 129, "endOffset": 144}, {"referenceID": 18, "context": "Zach[2015] Stanford Encyclopedia of Philosophy article discusses some attempts to mitigate Goedel\u2019s results.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "See Detlefsen[2017]) for a more skeptical view of Hilbert\u2019s programme.", "startOffset": 4, "endOffset": 20}, {"referenceID": 8, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 79, "endOffset": 145}, {"referenceID": 8, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 79, "endOffset": 160}, {"referenceID": 7, "context": "Publication of Von Neumann\u2019s work on computing did not occur until years after Lighthill\u2019s falsification was written (in particular Aspray[1990], Neumann[2005] and Kohler[2001]).", "startOffset": 164, "endOffset": 177}, {"referenceID": 7, "context": "Edward Kohler (Kohler[2000]), p.", "startOffset": 7, "endOffset": 28}, {"referenceID": 2, "context": "The problem context in the area of operations research solution space searching that influenced both von Neumann and Lighthill was pre computer algorithmic operations research experience (see Budiansky[2013] for the detailed story).", "startOffset": 192, "endOffset": 208}, {"referenceID": 5, "context": "such an expurgated exposition of mathematics makes it also evident that mathematics as such cannot predicate anything about objects of our intuition or real objects (Einstein[1921]).", "startOffset": 166, "endOffset": 181}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p.", "startOffset": 65, "endOffset": 78}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p. 257 for discussion of Finsler\u2019s result on undecidability and formal proofs and its history (also Finsler[1996] and Finsler[1969].", "startOffset": 65, "endOffset": 193}, {"referenceID": 1, "context": "See \"A Restoration the failed: Paul Finsler\u2019s theory of sets\" in Breger[1996], p. 257 for discussion of Finsler\u2019s result on undecidability and formal proofs and its history (also Finsler[1996] and Finsler[1969].", "startOffset": 65, "endOffset": 211}, {"referenceID": 9, "context": "Applying Lighthill\u2019s combinatorial explosion arguments, it seems to me that TMs are the wrong model of computation. Instead a different computational model called MRAMS (random access machines with unit multiply and a bounded number of unbounded size memory cells) is a better model of computation (Meyer[2016]).", "startOffset": 9, "endOffset": 311}, {"referenceID": 12, "context": "My suggestion is to adopt the ideas of Danish computer scientist, who was trained as an astronomer, Peter Naur. Naur argued that computation should be studied as Dataology. Dataology is a theory neutral term for studying data. Naur wrote \"mental life during the twentieth century has become entirely misguided into an ideological position such that only discussions that adopt the computer inspired form\" are accepted. (Naur[2007], 87).", "startOffset": 106, "endOffset": 431}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]).", "startOffset": 25, "endOffset": 36}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]). This books amplifies the program development method Naur described in his 2005 Turing Award lecture (Naur[2007]).", "startOffset": 25, "endOffset": 150}, {"referenceID": 12, "context": "in Software Engineering (Naur[2011]). This books amplifies the program development method Naur described in his 2005 Turing Award lecture (Naur[2007]). In Naur[2011] page 30, the interviewer asks \".", "startOffset": 25, "endOffset": 166}], "year": 2017, "abstractText": "The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British applied mathematician James Lighthill. It is explained how Lighthill\u2019s arguments continue to apply to current AI. It is argued that AI should use the Popperian scientific method in which it is the duty of every scientist to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method in detail and discusses Paul Nurse\u2019s application of the method to cell biology that also involves questions of mechanism and behavior. Arguments used by Lighthill in his original 1972 report that falsifed AI are discussed. The Lighthill arguments are then shown to apply to current AI. The argument uses recent scholarship to explain Lighthill\u2019s assumptions and to show how the arguments based on those assumptions continue to falsify modern AI. An iimportant focus of the argument involves Hilbert\u2019s philosophical programme that defined knowledge and truth as provable formal sentences. Current AI takes the Hilbert programme as dogma beyond criticism while Lighthill as a mid 20th century applied mathematician had abandoned it. The paper uses recent scholarship to explain John von Neumann\u2019s criticism of AI that I claim was assumed by Lighthill. The paper discusses computer chess programs to show Lighthill\u2019s combinatorial explosion still applies to AI but not humans. An argument showing that Turing Machines (TM) are not the correct description of computation is given. The paper concludes by advocating studying computation as Peter Naur\u2019s Dataology.", "creator": "groff version 1.18.1.4"}}}