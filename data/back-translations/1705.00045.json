{"id": "1705.00045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Understanding and Detecting Supporting Arguments of Diverse Types", "abstract": "We investigate the problem of identifying arguments at the sentence level based on relevant documents for user-specific claims. A dataset of assertions and related citation articles is collected from the idebate.org website. We then manually mark sentence-level arguments from the documents along with their types as study, facts, opinion or reasoning. We also characterize arguments of different kinds and investigate whether the use of type information can facilitate the recognition of supporting arguments. Experimental results show that LambdaMART (Burges, 2010), which uses features determined by argument types, performs better than the same Ranker that was trained without type information.", "histories": [["v1", "Fri, 28 Apr 2017 19:29:54 GMT  (59kb,D)", "http://arxiv.org/abs/1705.00045v1", null], ["v2", "Tue, 2 May 2017 22:00:13 GMT  (56kb,D)", "http://arxiv.org/abs/1705.00045v2", "This paper is accepted as a short paper in ACL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinyu hua", "lu wang"], "accepted": false, "id": "1705.00045"}, "pdf": {"name": "1705.00045.pdf", "metadata": {"source": "CRF", "title": "Understanding and Detecting Supporting Arguments of Diverse Types", "authors": ["Xinyu Hua", "Lu Wang"], "emails": ["hua.x@husky.neu.edu", "luwang@ccs.neu.edu"], "sections": [{"heading": "1 Introduction", "text": "Argumentation plays a crucial role in persuasion and decision-making processes. An argument usually consists of a central claim (or conclusion) and several supporting premises. Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014). For instance, as shown in Figure 1, the editor on idebate.org \u2013 a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim \u201clegally owned guns are frequently stolen and used by criminals\u201d. However, it would require substantial human effort to collect information from diverse resources to support argument construction. In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments.\nTo date, most of the argument mining research focuses on recognizing argumentative components\nand their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information.\nIn this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Take Figure 2 as an example: assume we are given a claim on the topic of \u201cbanning cosmetic surgery\u201d and a relevant article (cited for argument construction), we aim to automatically pinpoint the sentence(s) (in italics) among all sentences in the cited article that can be used to back up the claim. We define such tasks as supporting argument detection. Furthermore, another goal of\nar X\niv :1\n70 5.\n00 04\n5v 1\n[ cs\n.C L\n] 2\n8 A\npr 2\n01 7\nthis work is to understand and characterize different types of supporting arguments. Indeed, human editors do use different types of information to promote persuasiveness as we will show in Section 3. Prediction performance also varies among different types of supporting arguments.\nGiven that none of the existing datasets is suitable for our study, we collect and annotate a corpus from Idebate, which contains hundreds of debate topics and corresponding claims.1 As is shown in Figure 2, each claim is supported with some human constructed argument, with cited articles marked on sentence level. After careful inspection on the supporting arguments, we propose to label them as STUDY, FACTUAL, OPINION, or REASONING. Substantial inter-annotator agreement rate is achieved for both supporting argument labeling (with Cohen\u2019s \u03ba of 0.8) and argument type annotation, on 200 topics with 621 reference articles.\nBased on the new corpus, we first carry out a study on characterizing arguments of different types via type prediction. We find that arguments\n1The labeled dataset along with the annotation guideline will be released at xyhua.me.\nof STUDY and FACTUAL tend to use more concrete words, while arguments of OPINION contain more named entities of person names. We then investigate whether argument type can be leveraged to assist supporting argument detection. Experimental results based on LambdaMART (Burges, 2010) show that utilizing features composite with argument types achieves a Mean Reciprocal Rank (MRR) score of 57.65, which outperforms an unsupervised baseline and the same ranker trained without type information. Feature analysis also demonstrates that salient features have significantly different distribution over different argument types.\nFor the rest of the paper, we summarize related work in Section 2. The data collection and annotation process is described in Section 3, which is followed by argument type study (Section 4). Experiment on supporting argument detection is presented in Section 5. We finally conclude in Section 6."}, {"heading": "2 Related Work", "text": "Our work is in line with argumentation mining, which has recently attracted significant research interest. Existing work focuses on argument extraction from news articles, legal documents, or online comments without given userspecified claim (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011; Park and Cardie, 2014). Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments. To the best of our knowledge, none of them studies the interaction between types of arguments and their usage to support a user-specified claim. This is the gap we aim to fill."}, {"heading": "3 Data and Annotation", "text": "We rely on data from idebate.org, where human editors construct paragraphs of arguments, either supporting or opposing claims under controversial topics. We also extract textual citation articles as source of information used by editors during argument construction. In total we collected 383 unique debates, out of which 200 debates are randomly selected for study. After removing invalid ones, our final dataset includes 450 claims\nand 621 citation articles with about 53,000 sentences. Annotation Process. As shown in Figure 2, we first annotate which sentence(s) from a citation articles is used by the editor as supporting arguments. Then we annotate the type for each of them as STUDY, FACTUAL, OPINION, or REASONING, based on the scheme in Table 1.2 For instance, the highlighted supporting argument in Figure 2 is labeled as REASONING.\nTwo experienced annotators were hired to identify supporting arguments by reading through the whole cited article and locating the sentences that best match the reference human constructed argument. This task is rather complicated since human do not just repeat or directly quote the original sentences from citation articles, they also paraphrase, summarize, and generalize. For instance, the original sentence is \u201cThe global counterfeit drug trade, a billion-dollar industry, is thriving in Africa\u201d, which is paraphrased to \u201cThis is exploited by the billion dollar global counterfeit drug trade\u201d in human constructed argument.\nThe annotators were asked to annotate independently, then discuss and resolve disagreements and give feedback about current scheme. We compute inter-annotator agreement based on Cohen\u2019s \u03ba for both supporting arguments labeling and argument type annotation. For supporting arguments we have a high degree of consensus, with Cohen\u2019s \u03ba ranges from 0.76 to 0.83 in all rounds and 0.80 overall. For argument type annotation, we achieve Cohen\u2019s \u03ba of 0.61 for STUDY, 0.75 for FACTUAL, 0.71 for OPINION, and 0.29 for REASONING3\n2We end up with the four-type scheme as a trade-off between complexity and its coverage of the arguments.\n3Many times annotators have different interpretation on REASONING, and frequently label it as OPINION. This results\nStatistics. In total 1107 sentences are identified as supporting arguments. Among those, 108 (9.76%) are labeled as STUDY, 575 (51.94%) as FACTUAL, 382 (34.51%) as OPINION, and 42 (3.79%) as REASONING.\nWe further analyze the source of the supporting arguments. Domain names of the citation articles are collected based on their URL, and then categorized into \u201cnews\u201d, \u201corganization\u201d, \u201cscientific\u201d, \u201cblog\u201d, \u201creference\u201d, and others, according to a taxonomy provided by Alexa4 with a few edits to fit our dataset. News articles are the major source for all types, which account for roughly 50% for each. We show the distribution of other four types in Figure 3. Arguments of STUDY and REASONING are mostly from \u201cscientific\u201d websites (14.9% and 22.9%), whereas \u201corganization\u201d websites contribute a large portion of arguments of FACTUAL (18.5%) and OPINION (16.7%)."}, {"heading": "4 A Study On Argument Type Prediction", "text": "Here we characterize arguments of different types based on diverse features under the task of predicting argument types. Supporting arguments identified from previous section are utilized for experiments. We also leverage the learned classifier in this section to label the sentences that are not supporting arguments, which will be used for supporting argument detection in the next section. Four major types of features are considered. Basic Features. We calculate frequencies of unigram and bigram words, number of four major types of part-of-speech tags (verb, noun, adjective, and adverb), number of dependency relations, and\nin a low agreement for REASONING. 4http://www.alexa.com/topsites/category\nnumber of seven types of named entities (Chinchor and Robinson, 1997). Sentiment Features. We also compute number of positive, negative and neutral words in MPQA lexicon (Wilson et al., 2005), and number of words from a subset of semantic categories from General Inquirer (Stone et al., 1966).5 Discourse Features. We use the number of discourse connectives from the top two levels of Penn Discourse Tree Bank (Prasad et al., 2007). Style Features. We measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013).\nWe utilize Log-linear model for argument type prediction with one-vs-rest setup. Three baselines are considered: (1) random guess, (2) majority class, and (3) unigrams and bigrams as features for Log-linear model. Identified supporting arguments are used for experiments, and divided into training set (50%), validation set (25%) and test set (25%). From Table 2, we can see that Loglinear model trained with all features outperforms the ones trained with ngram features. To further characterize arguments of different types, we display sample features with significant different values in Figure 4. As can be seen, arguments of STUDY and FACTUAL tend to contain more concrete words and named entities. Arguments of OPINION mention more person names, which implies that expert opinions are commonly quoted."}, {"heading": "5 Supporting Argument Detection", "text": "We cast the sentence-level supporting argument detection problem as a ranking task.6 Features\n5Categories used: Strong, Weak, Virtue, Vice, Ovrst (Overstated), Undrst (Understated), Academ (Academic), Doctrin (Doctrine), Econ (Economic), Relig (Religious), Causal, Ought, and Perceiv (Perception).\n6Many sentences in the citation article is relevant to the topic to various degrees. We focus on detecting the most relevant ones, and thus treat it as a ranking problem instead of a\nin Section 4 are also utilized here as \u201cSentence features\u201d with additional features considering the sentence position in the article. We further employ features that measure similarity between claims and sentences, and the composite features that leverage argument type information.\nSimilarity Features. We compute similarity between claim and candidate sentence based on TFIDF and average word embeddings. We also consider ROUGE (Lin, 2004), a recall oriented metric for summarization evaluation. In particular, ROUGE-L, a variation based on longest common subsequence, is computed by treating claim as reference and each candidate sentence as sample summary. In similar manner we use BLEU (Papineni et al., 2002), a precision oriented metric.\nComposite Features. We adopt composite features to study the interaction of other features with type of the sentence. Given claim c and sentence s with any feature mentioned above, a composite feature function \u03c6M(type, feature)(s, c) is set to the actual feature value if and only if the argument type matches. For instance, if the ROUGE-L score is 0.2, and s is of type STUDY, then \u03c6M(study, ROUGE)(s, c) = 0.2 \u03c6M(factual, ROUGE)(s, c), \u03c6M(opinion, ROUGE)(s, c), \u03c6M(reasoning, ROUGE)(s, c) are all set to 0.\nbinary classification task.\nWe choose LambdaMART (Burges, 2010) for experiments, which is shown to be successful for many text ranking problems (Chapelle and Chang, 2011). Our model is evaluated by Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) using 5-fold cross validation. We compare to TFIDF and Word embedding similarity baselines, and LambdaMART trained with ngrams (unigrams and bigrams).\nResults in Table 3 show that using composite features with argument type information (Comp(type, Sen) + Comp(type, Simi)) can improve the ranking performance. Specifically, the best performance is achieved by adding composite features to sentence features, similarity features, and ngram features. As can be seen, supervised methods outperform unsupervised baseline methods. And similarity features have similar performance as those baselines. The best performance is achieved by combination of sentence features, Ngrams, similarity, and two composite types, which is boldfaced. Feature sets that significantly outperform all three baselines are marked with \u2217.\nFor feature analysis, we conduct t-test for individual feature values between supporting arguments and the others. We breakdown features according to their argument types and show top salient composite features in Table 4. For all sentences of type STUDY, relevant ones tend to contain more \u201cpercentage\u201d and more concrete words. We also notice those sentences with more hedging words are more likely to be considered. For sentences of FACTUAL, position of sentence in article\nplays an important role, as well as their similarity to the claim based on ROUGE scores. For type OPINION, unlike all other types, position of sentence seems to be insignificant. As we could imagine, opinionated information might scatter around the whole documents. For sentences of REASONING, the ones that can be used as supporting arguments tend to be less concrete and less emotional, as opposed to opinion."}, {"heading": "6 Conclusion", "text": "We presented a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Based on our newly-collected dataset, we characterized arguments of different types with a rich feature set. We also showed that leveraging argument type information can further improve the performance of supporting argument detection."}, {"heading": "Acknowledgments", "text": "This work was supported in part by National Science Foundation Grant IIS-1566382 and a GPU gift from Nvidia. We thank Kechen Qin for his help on data collection. We also appreciate the valuable suggestions on various aspects of this work from three anonymous reviewers."}], "references": [{"title": "A news editorial corpus for mining argumentation strategies", "author": ["References Khalid Al Khatib", "Henning Wachsmuth", "Johannes Kiesel", "Matthias Hagen", "Benno Stein."], "venue": "Proceedings of COLING 2016, the", "citeRegEx": "Khatib et al\\.,? 2016", "shortCiteRegEx": "Khatib et al\\.", "year": 2016}, {"title": "Identifying justifications in written dialogs by classifying text as argumentative", "author": ["Or Biran", "Owen Rambow."], "venue": "International Journal of Semantic Computing 5(04):363\u2013381.", "citeRegEx": "Biran and Rambow.,? 2011", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "Concreteness ratings for 40 thousand generally known english word lemmas", "author": ["Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman."], "venue": "Behavior research methods 46(3):904\u2013911.", "citeRegEx": "Brysbaert et al\\.,? 2014", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["Christopher JC Burges."], "venue": "Learning 11(23-581):81.", "citeRegEx": "Burges.,? 2010", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "Yahoo! learning to rank challenge overview", "author": ["Olivier Chapelle", "Yi Chang."], "venue": "Yahoo! Learning to Rank Challenge. pages 1\u201324.", "citeRegEx": "Chapelle and Chang.,? 2011", "shortCiteRegEx": "Chapelle and Chang.", "year": 2011}, {"title": "Muc7 named entity task definition", "author": ["Nancy Chinchor", "Patricia Robinson."], "venue": "Proceedings of the 7th Conference on Message Understanding. volume 29.", "citeRegEx": "Chinchor and Robinson.,? 1997", "shortCiteRegEx": "Chinchor and Robinson.", "year": 1997}, {"title": "Classifying arguments by scheme", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computa-", "citeRegEx": "Feng and Hirst.,? 2011", "shortCiteRegEx": "Feng and Hirst.", "year": 2011}, {"title": "Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Habernal and Gurevych.,? 2015", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop. Barcelona, Spain, volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Argumentation mining", "author": ["Raquel Mochales", "Marie-Francine Moens."], "venue": "Artificial Intelligence and Law 19(1):1\u201322.", "citeRegEx": "Mochales and Moens.,? 2011", "shortCiteRegEx": "Mochales and Moens.", "year": 2011}, {"title": "Automatic detection of arguments in legal texts", "author": ["Marie-Francine Moens", "Erik Boiy", "Raquel Mochales Palau", "Chris Reed."], "venue": "Proceedings of the 11th international conference on Artificial intelligence and law. ACM, pages 225\u2013230.", "citeRegEx": "Moens et al\\.,? 2007", "shortCiteRegEx": "Moens et al\\.", "year": 2007}, {"title": "Context-aware argumentative relation mining", "author": ["Huy Nguyen", "Diane Litman."], "venue": "Proceedings of", "citeRegEx": "Nguyen and Litman.,? 2016", "shortCiteRegEx": "Nguyen and Litman.", "year": 2016}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "author": ["Raquel Mochales Palau", "Marie-Francine Moens."], "venue": "Proceedings of the 12th international conference on artificial intelligence and law. ACM, pages 98\u2013107.", "citeRegEx": "Palau and Moens.,? 2009", "shortCiteRegEx": "Palau and Moens.", "year": 2009}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Identifying appropriate support for propositions in online user comments", "author": ["Joonsuk Park", "Claire Cardie."], "venue": "Proceedings of the First Workshop on Argumentation Mining. pages 29\u201338.", "citeRegEx": "Park and Cardie.,? 2014", "shortCiteRegEx": "Park and Cardie.", "year": 2014}, {"title": "The penn discourse treebank 2.0 annotation", "author": ["Rashmi Prasad", "Eleni Miltsakaki", "Nikhil Dinesh", "Alan Lee", "Aravind Joshi", "Livio Robaldo", "Bonnie L Webber"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2007}, {"title": "Argumentation and critical decision making", "author": ["Richard D Rieke", "Malcolm Osgood Sillars", "Tarla Rai Peterson."], "venue": "New York: Longman.", "citeRegEx": "Rieke et al\\.,? 1997", "shortCiteRegEx": "Rieke et al\\.", "year": 1997}, {"title": "Show me your evidence-an automatic method for context dependent evidence detection", "author": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M Khapra", "Ehud Aharoni", "Noam Slonim."], "venue": "EMNLP. pages 440\u2013450.", "citeRegEx": "Rinott et al\\.,? 2015", "shortCiteRegEx": "Rinott et al\\.", "year": 2015}, {"title": "Applying kernel methods to argumentation mining", "author": ["Niall Rooney", "Hui Wang", "Fiona Browne."], "venue": "Twenty-Fifth International FLAIRS Conference.", "citeRegEx": "Rooney et al\\.,? 2012", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "EMNLP. pages 46\u201356.", "citeRegEx": "Stab and Gurevych.,? 2014", "shortCiteRegEx": "Stab and Gurevych.", "year": 2014}, {"title": "The general inquirer: A computer approach to content analysis", "author": ["Philip J Stone", "Dexter C Dunphy", "Marshall S Smith."], "venue": ".", "citeRegEx": "Stone et al\\.,? 1966", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Norms of valence, arousal, and dominance for 13,915 english lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman", "Marc Brysbaert."], "venue": "Behavior research methods 45(4):1191\u20131207.", "citeRegEx": "Warriner et al\\.,? 2013", "shortCiteRegEx": "Warriner et al\\.", "year": 2013}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. Associ-", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.", "startOffset": 42, "endOffset": 56}, {"referenceID": 16, "context": "Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014).", "startOffset": 136, "endOffset": 179}, {"referenceID": 14, "context": "Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014).", "startOffset": 136, "endOffset": 179}, {"referenceID": 6, "context": "2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles.", "startOffset": 31, "endOffset": 235}, {"referenceID": 3, "context": "Experimental results based on LambdaMART (Burges, 2010) show that utilizing features composite with argument types achieves a Mean Reciprocal Rank (MRR) score of 57.", "startOffset": 41, "endOffset": 55}, {"referenceID": 1, "context": "Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.", "startOffset": 54, "endOffset": 170}, {"referenceID": 6, "context": "Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.", "startOffset": 54, "endOffset": 170}, {"referenceID": 18, "context": "Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.", "startOffset": 54, "endOffset": 170}, {"referenceID": 19, "context": "Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.", "startOffset": 54, "endOffset": 170}, {"referenceID": 5, "context": "number of seven types of named entities (Chinchor and Robinson, 1997).", "startOffset": 40, "endOffset": 69}, {"referenceID": 22, "context": "We also compute number of positive, negative and neutral words in MPQA lexicon (Wilson et al., 2005), and number of words from a subset of semantic categories from General Inquirer (Stone et al.", "startOffset": 79, "endOffset": 100}, {"referenceID": 20, "context": ", 2005), and number of words from a subset of semantic categories from General Inquirer (Stone et al., 1966).", "startOffset": 88, "endOffset": 108}, {"referenceID": 15, "context": "We use the number of discourse connectives from the top two levels of Penn Discourse Tree Bank (Prasad et al., 2007).", "startOffset": 95, "endOffset": 116}, {"referenceID": 2, "context": "emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al.", "startOffset": 82, "endOffset": 106}, {"referenceID": 2, "context": "emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013).", "startOffset": 82, "endOffset": 133}, {"referenceID": 8, "context": "sider ROUGE (Lin, 2004), a recall oriented metric for summarization evaluation.", "startOffset": 12, "endOffset": 23}, {"referenceID": 13, "context": "In similar manner we use BLEU (Papineni et al., 2002), a precision oriented metric.", "startOffset": 30, "endOffset": 53}, {"referenceID": 3, "context": "We choose LambdaMART (Burges, 2010) for", "startOffset": 21, "endOffset": 35}, {"referenceID": 4, "context": "experiments, which is shown to be successful for many text ranking problems (Chapelle and Chang, 2011).", "startOffset": 76, "endOffset": 102}], "year": 2017, "abstractText": "We investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims. A dataset containing claims and associated citation articles is collected from online debate website idebate.org. We then manually label sentence-level supporting arguments from the documents along with their types as STUDY, FACTUAL, OPINION, or REASONING. We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.", "creator": "LaTeX with hyperref package"}}}