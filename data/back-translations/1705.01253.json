{"id": "1705.01253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "The Forgettable-Watcher Model for Video Question Answering", "abstract": "Recently, a number of approaches to answering visual questions have been proposed, which aim to understand the visual scenes by answering the questions of natural language.", "histories": [["v1", "Wed, 3 May 2017 04:46:33 GMT  (3407kb,D)", "http://arxiv.org/abs/1705.01253v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hongyang xue", "zhou zhao", "deng cai"], "accepted": false, "id": "1705.01253"}, "pdf": {"name": "1705.01253.pdf", "metadata": {"source": "CRF", "title": "The Forgettable-Watcher Model for Video Question Answering", "authors": ["Hongyang Xue", "Zhou Zhao", "Deng Cai"], "emails": ["hyxue@outlook.com,", "zhaozhou@zju.edu.cn,", "dengcai@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "To understand the visual scenes is one of the ultimate goals in computer vision. A lot of intermediate and low-level tasks, such as object detection, recognition, segmentation and tracking, have been studied towards this goal. One of the highlevel tasks towards scene understanding is the visual question answering [Antol et al., 2015]. This task aims at understanding the scenes by answering the questions about the visual data. It also has a wide application, from aiding the visuallyimpaired, analyzing surveillance data to domestic robots.\nThe visual data we are facing everyday are mostly dynamic videos. However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016]. The images are static and contain far less information than the videos. The task of image-based question answering cannot fit into real-world applications since it ignores the temporal coherence of the scenes.\nExisting video-related question answering works usually combine additional information. The Movie-QA dataset [Tapaswi et al., 2016] contains multiple sources of information: plots, subtitles, video clips, scripts and DVS transcrip-\ntions. These extra information is hard to retrieve in the real world, making these datasets and approaches difficult to extend to general videos.\nUnlike the previous works, we consider the more ubiquitous task of video question answering with only the visual data and the natural language questions. In our task, only the videos, the questions and the corresponding answer choices are presented. We first introduce a dataset collected on our own. To collect a dataset is not an easy task. In image-based question answering (Visual-QA) [Antol et al., 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al., 2015; Malinowski and Fritz, 2014]. This requires a significant amount of human labor. To make things worse, the video has a temporal dimension compared with the image, which implies that the labor of the human annotators is multiplied. To avoid the significant increase of human labor, our solution is to employ the question generation approaches [Heilman and Smith, 2010] to generate question-answer pairs directly from the texts accompanying the videos. Now the collection becomes collecting videos with descriptions. This inspires us to utilize the existing video description datasets. The TGIF (Tumblr GIF) dataset [Li et al., 2016] is a large-scale video description dataset. The groundtruth description provides us the necessary texts to produce the question-answer pairs. Finally, we form our TGIF-QA dataset. Details will be described in the Dataset section.\nExisting visual question answering approaches are not suitable for video question answering since the video question answering has the following features: First, the question may relate to an event which happens across multiple frames. The\nar X\niv :1\n70 5.\n01 25\n3v 1\n[ cs\n.C V\n] 3\nM ay\n2 01\n7\ninformation must be gathered among the frames to answer the question. A typical case is the question related to the numbers (see in Fig 1.). The question asks about the number of men in the video. However, in the beginning, we cannot see the correct number of men. Only by watching the video frame by frame can we tell the answer is four. Same is the case in the top-right of Fig 2. Current existing visual-QA approaches cannot be applied as they only utilize the spatial information of a static image. Second, there may be a lot of redundancy in the video frames. In addition to these, our task faces another challenge: the candidate answers are mostly phrases. To tackle these problems, we propose two models: the re-watcher and the re-reader. The re-watcher model meticulously processes the video frames. This model allows to gather information from relevant frames. Then the information is recurrently accumulated as the question is read. The re-reader model can handle phrasal answers and concentrate on the important words in the answers. Then we combine these two models into the forgettable-watcher model.\nOur contribution can be summarized into two aspects: First we introduce a Video-QA dataset TGIF-QA. Second we propose the models which can employ the temporal structure of the videos and the phrasal structure of the candidate answers. We also extend the VQA model [Antol et al., 2015] as a baseline method. We evaluate our models on our proposed TGIFQA dataset. The experimental results show the effectiveness of our proposed models."}, {"heading": "2 Dataset", "text": "In this section, we introduce our dataset for Video Question Answering. We convert the existing video description dataset for question answering. The TGIF dataset [Li et al., 2016] is collected by Li et al. from Tumblr. GIFs are almost identical to small video clips for the short duration. Li et al. cleaned up the original data and ruled out the GIFs with catoon, static and textual content. The animated GIFs were later annotated using crowd-sourcing service. The TGIF dataset contains 102,068 GIFs and 287,933 descriptions in total where each GIF corresponds to several descriptions. Each description consists of one or two sentences."}, {"heading": "2.1 TGIF-QA", "text": "In order to generate the question-answer pairs from the descriptions, we employ the state-of-the-art question generation approach [Heilman and Smith, 2010]. We focus on the questions of types What/When/Who/Whose/Where and How Many. Our question answering task is of the multiplechoice type, and the generated data only contains question and ground-truth answer pairs. So we need to generate wrong alternatives for each question. We provide each question with 8 candidate answers. We describe how we generate the alternative answers for each kind of questions in the following subsections."}, {"heading": "How Many", "text": "The How Many question relates to counting some objects in the video. In order to generate reasonable alternatives, we first collect all the How Many questions in our dataset and gather the numbers in their answers. All the Arabic numerals\nare converted to English words representations. After eliminating the numbers of low occurrence frequency, we find that most answers contain numbers from one to eight. We discard the questions whose answers exceed eight and replace the ground-truth numbers with [one, two, three, four, five, six, seven, eight] to generate the 8 candidate answers. One typical example is shown in the top-right of Fig 1."}, {"heading": "Who", "text": "The questions starting with Who usually relate to humans. We collect the words in the answers from all the Who questions. Then we filter the words to obtain all the nouns. After that, we filter out all the abstract and low-frequency nouns to form an entity list. The entity word in the ground-truth answers is selected and replaced with random samples from the entity list to generate 8 alternatives. An example is provided in the top-left of Fig 1."}, {"heading": "Whose", "text": "The Whose questions relate to the facts about belongings. There are two ways to represent the belongings. One is through the possessive pronoun like \u201cmy\u201d, \u201cyour\u201d, \u201chis\u201d, etc. The other is to use the possessive case of nouns such as \u201cman\u2019s\u201d, \u201cgirl\u2019s\u201d, \u201ccats\u2019\u201d, etc. For the former kind of possessive pronouns, we replace the pronouns with random samples from the possessive pronoun list to generate the alternatives. For the latter one we replace the nouns just the same way we do for the Who questions."}, {"heading": "Other Questions", "text": "For the rest types of questions, we just replace the nouns in the answer phrases to generate candidate choices."}, {"heading": "2.2 Dataset Split", "text": "We abandon the videos whose descriptions generate no questions or the questions have been discarded in the previous processing. In the end our dataset contains 117,854 videos and 287,763 question-candidates pairs. We split our TGIFQA dataset into three parts for training, validation and testing. The training dataset contains 230,689 question-answers pairs from 94,425 videos. The validation dataset contains 24,696 pairs from 10,096 videos and the testing dataset has 32,378 pairs from 13,333 videos."}, {"heading": "3 Method", "text": ""}, {"heading": "3.1 Task description", "text": "Multiple-Choice Video-QA is to select an answer a\u0303 given a question q, video information v and candidate choices (alternatives) a = {a1, \u00b7 \u00b7 \u00b7 ,a8}. A video is a sequence of image frames v = {f1, f2, \u00b7 \u00b7 \u00b7 }. A question is a sequence of natural language words q = {q1, q2, \u00b7 \u00b7 \u00b7 }. Each alternative of the candidate answers is also a sequence of natural language words ai = {a1i , a2i , \u00b7 \u00b7 \u00b7 }. We formulate the VideoQA task as selecting the best answer among the alternatives. In the other word, we define a loss function L(v,q,ai). We regard the QA problem as a classification problem and the best answer is selected when it achieves minimal loss a\u0303 = argmaxai L(v,q,ai)."}, {"heading": "3.2 Model", "text": "The major difficulty of Video-QA compared with Image-QA is that the video has a lot of frames. The information is scattered among the frames and an event can last across the frames. To answer a question, we must find the most informative frame or combine the information of several frames.\nIn the following we propose three models: the re-watcher, the re-reader and the forgettable-watcher. The re-watcher model processes the question sentence word sequence meticulously. The re-reader model fully utilizes the information of the video frames along the temporal dimension. And the forgettable-watcher combines both of them.\nWe denote qai as the concatenation of question q and answer ai after word embedding. All our models will take the question and one alternative as a whole sentence which we call the QA-sentence. The QA-sentence and the visual feature sequence are then put into our models to produce a score for the alternative answer in the sentence. In the following sections, we will denote the QA-sentence as c when it does not introduce confusions."}, {"heading": "Re-Watcher", "text": "Our model first encodes the video features and QA features with two separate bi-directional single layer LSTMs [Graves, 2012]. The LSTMs contain an embedding layer which maps\nthe video features and textual features into a joint feature space. We denote the outputs of the forward and backward LSTMs as yf (t) and yb(t). The encoding u of a QA-sentence of length |c| is formed by the concatenation of final outputs of the forward and backward LSTMs, u = [yfc (|c|), ybc(1)].\nFor the video frames, the encoding output for each frame at time t is yv(t) = [yf (t), yb(t)]. The representation r(i) of the videos for each QA-sentence token i is formed by a weighted sum of these output vectors (similar to the attention mechanism in Image-QA [Yang et al., 2016]) and the previous representation r(i\u2212 1): m(i, t) = tanh(Wvmyv(t) +Wrmr(i\u2212 1) +Wcmyc(i)) s(i, t) \u221d exp(WTmsm(i, t)) r(i) = yTd s(i) + tanh(Wrrr(i\u2212 1))\n1 \u2264 i \u2264 |c| The mechanism of the re-watcher model is that every word will combine with the whole video sequence to generate a state. Then the states of the word sequence is accumulated to generate a combined feature (see Fig 2. left). This model mimics a person who has a bad memory for the video he watches. Every time he reads a word of the QA-sentence, he goes back to watch the whole video to make out what the words in the sentence are about. During this procedure, he selects the information most related to the QA-sentence token from the video and then recurrently accumulate the information as the whole QA-sentence is read. Finally a joint video and QA-sentence representation is formed for producing a score. The score measures how much the question-answer pair (QA-sentence) matches with the video:\ngReW = FC(tanh(Wrgr(|c|) +Wcgu)) where FC represents three fully-connected layers. The activation of the former two layers is ReLU and the last layer is without activation. Since each question has 8 alternatives, each question relates to 8 QA-sentences. For a question and 8 alternatives, we generate 8 such scores and they fill up a 8-dimensional score vector: g = [gReW1 , \u00b7 \u00b7 \u00b7 , gReW8 ] The score vector is then put through the softmax layer."}, {"heading": "Re-Reader", "text": "The re-watcher model mimics a person who continuously rewatches the video as he reads the QA-sentence. Video features related with the QA-sentence are accumulated. The rereader model is designed from the opposite view (see Fig 2. middle).\nThis model mimics a person who cannot well remember the whole question. Every time he watches a frame, he retrospects on the whole question. We denote the encoding output of the QA-sentence at token i as yc(i) = [yfc (i), y b c(i)]. The representation w(t) of the video frames at time t is computed from the weighted sum of the QA-sentence encoding outputs and the previous representation w(t\u2212 1): m(t, i) = tanh(Wcmyc(i) +Wwmw(t\u2212 1) +Wvmyv(t)) s(t, i) \u221d exp(WTmsm(t, i)) w(t) = yTc s(t) + tanh(Wwww(t\u2212 1))\n1 \u2264 t \u2264 N\nwhere N is the number of frames. The score of the QAsentence is:\ngReR = FC(tanh(Wwgw(N) +Wcgu))"}, {"heading": "Forgettable-Watcher", "text": "We combine the re-watcher and the re-reader models into the forgettable-watcher model (see Fig 2 right). This model meticulously combines the visual features and the texual features. The whole video is re-watched when a word is read and the whole sentence is re-read while a frame is being watched. Then the representations are combined to produce the score:\ngF = FC(tanh(Wrgr(|c|) +Wwgw(N) +Wcgu))"}, {"heading": "Baseline method", "text": "In order to show the effectiveness of the re-reading and the re-watching mechanisms of our models. We also employ a straightforward model (see Fig 3). This model is extended from the VQA model [Antol et al., 2015]. The VQA model is designed for question answering given only a single image. We extend the model for our task by directly encoding the video frames and the QA-sentences with two separate bidirectional LSTMs. The final encoding outputs of both bidirectional LSTMs are then combined to produce the score."}, {"heading": "4 Experiments and Results", "text": "We evaluate all the methods on our TGIF-QA dataset described in the Dataset section."}, {"heading": "4.1 Data Preprocessing", "text": "We sample all the videos to have the same number of frames with the purpose of reducing the redundancy. If the video does not own enough frames for sampling, its last frame is repeated. We extract the visual features of each frame with VGGNet [Simonyan and Zisserman, 2014]. The 4096- dimensional feature vector of the first fully-connected layer is taken as the visual feature. For the questions and answers, the Word2Vec [Mikolov et al., 2013] trained on GoogleNews is employed to embed each word as a 300-dimensional real vector. Then we concatenate each question with its 8 alternatives to generate 8 candidate QA-sentences."}, {"heading": "4.2 Implementation Details", "text": ""}, {"heading": "Optimization", "text": "We train all our models using the Adam optimizer [Kingma and Ba, 2015] to minimize the loss. The initial learning rate is set to 0.002. The exponential decay rates for the first and the second moment estimates are set to 0.1 and 0.001 respectively. The batch size is set to 100. A gradient clipping scheme is applied to clip gradient norm within 10. An early stopping strategy is utilized to stop training when the validation accuracy no longer improves."}, {"heading": "Model Details", "text": "The visual features and the word embeddings are encoded by two separate bidirectional LSTMs to dimensionality 2048 and 1024 respectively. Then they are mapped to a joint feature space of dimensionality 1024. The re-watcher (re-reader) component keeps a memory of size 512 and outputs the final combined feature of dimensionality 512. Finally the combined feature is put into three fully-connected layers of size (512, 256, 128). We evaluate our Video-QA task using classification accuracy."}, {"heading": "Evaluation Metrics", "text": "Since our task is the multiple-choice question answering, we employ the classification accuracy to evaluate our models. However, there are a few cases where both the two choices can answer the question. This motivates us to also apply the WUPS measure [Malinowski and Fritz, 2014] with 0.0 and 0.9 as the threshold values like the open-ended tasks."}, {"heading": "4.3 Results", "text": ""}, {"heading": "Baseline method", "text": "The baseline method is an extension of the VQA model [Antol et al., 2015] without our re-watching or re-reading mechanisms. We name it the straightforward model and its result is shown in the Straightforward section of table 1. We can see that the straightforward model performs much worse than the other three models."}, {"heading": "Our methods", "text": "The forgettable-watcher model outperforms the other two models since it jointly employs the re-watching and the rereading mechanisms. On the other hand, the re-reader model performs worse than the re-watcher model. This implies that the re-watching mechanism is more important."}, {"heading": "Results on different questions", "text": "We also report the accuracy of the models on different types of questions. The result is shown in table 2. All the methods perform better on the questions asking about \u201cWhere\u201d and \u201cWhen\u201d than others. This may be attributed to two reasons: First, the \u201cWhere\u201d and \u201cWhen\u201d questions are easier to answer because these two questions usually relate to the video big scene. In most cases, a single frame may be enough to answer the questions. The other is that the candidate alternatives produced in the dataset generation may be too simple to discriminate. The dataset generation method is less effective in producing good alternatives for these two questions while it can produce high-quality alternatives for the other types of\nquestions. We also report the results of the questions besides these two in table 2.\nFinally, we exhibit some typical Video-QA results in Fig 5."}, {"heading": "5 Related Work", "text": ""}, {"heading": "5.1 Image-QA", "text": "Image-based visual question answering has attracted a significant research interest recently. [Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016]. The goal of Image-QA is to answer questions given only one image without additional information. Image-QA tasks can be categorized into two types according to the ways answers are generated. The first type is called the Open-Ended question answering [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are produced given only the questions. As the answers generated by the algorithms are usually not the exact words people expect, measures such as the WUPS 0.9 and WUPS 0.0 based on the Wu-Palmer (WUPS) similarity [Malinowski and Fritz, 2014] are employed to measure the answer accuracy. The other type is called the Multiple-Choice question answering [Zhu et al., 2016] where both the question and several candidate answers are presented. To predict the answer is to pick the correct one among the candidates. It is observed that the approaches for the Open-Ended question answering usually cannot produce high-quality long answers [Zhu et al., 2016]. And most Open-Ended question answering approaches only focus on one-word answers [Ren et al., 2015; Malinowski and Fritz, 2014]. As a result, we consider the Multiple-Choice question answering type for our video question answering task. Our candidate answer choices are mainly phrases rather than single words.\nA lot of efforts have been made tackling the Image-QA problem. Some of them have collected their own datasets. Malinowski et al [Malinowski and Fritz, 2014] collected their dataset with the human annotations. Their dataset only focuses on basic colors, numbers and objects. Antol et al. [Antol et al., 2015] manually collected a large-scale free-form Image-QA dataset. Gao et al. [Gao et al., 2015] also manually collected the FM-IQA (Freestyle Multilingual Image Question Answering) dataset with the help of the Amazon Mechanical Turk platform. Most of these methods require a large amount of human labor for collecting data. In contrast, we propose to automatically convert existing video description dataset [Li et al., 2016] into question answering dataset."}, {"heading": "5.2 Question Generation", "text": "Automatic question generation is an open research topic in natural language processing. It is originally proposed for educational purpose [Gates, 2008]. In our situation, we need the generated questions to be as diverse as possible so that it can well match the property of questions generated by human annotators. Among the question generation approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010], we employ the method from Heilman and Smith [Heilman and Smith, 2010] to generate our video-QA pairs from video description datasets. Their approach generates questions in open domains. Similar idea has been utilized by Ren\nTable 1: Video-QA results. We evaluate the baseline method in the first row(Straightforward). Our proposed three models are reported in the subsequent rows. Accuracy denotes the classification accuracy when the alternatives for each question are regarded as classes. WUPS 0.0 and WUPS 0.9 are the WUPS scores with 0.0 and 0.9 as threshold respectively.\nMethods EvaluationAccuracy WUPS@0.0 (%) WUPS@0.9 (%) Straightforward 0.8253 93.24 87.27\nRe-Watcher 0.8663 95.66 91.28 Re-Reader 0.8592 95.22 90.75 Forgettable 0.8733 95.88 92.56\nTable 2: This table shows the accuracy on different kinds of questions. We can see from the results that the How many questions are the most difficult. On the contrary, the questions asking Where and When are simpler. Doubting the alternatives for these two questions are not of high-quality, we also report the results on all the questions besides these two. It is denoted by\nWhere and When.\net al. [Ren et al., 2015] to turn image description datasets into Image-QA datasets. They generate only four types of questions: objects, numbers, color and locations. Their answers are mostly single words. On the contrary, we generate a large amount of open-domain questions where the corresponding answers are basically phrases."}, {"heading": "5.3 Video-QA", "text": "Video-based question answering is a largely unexplored problem compared with Image-QA. Previous work usually combine other text information. Tapaswi et al. [Tapaswi et al., 2016] combine videos with plots, subtitles and scripts to generate answers. Tu et al. [Tu et al., 2014] also combine video and text data for question answering. Zhu et al. [Zhu et al., 2015] collect a dataset for \u201dfill-in-the-blank\u201d type of questions. Mazaheri et al. [Mazaheri et al., 2016] also consider the fill-in-the-blank problem. Comparing with ImageQA, Video-QA is more troublesome because of the additional temporal dimension. The useful information is scattered in different frames. The temporal coherence must be well addressed to better understand the videos. Our proposed\nmethod focuses on the Multiple-Choice type of questions where the candidate answers are basically phrases. We collect the dataset by turning existing video description datasets automatically into Video-QA datasets which saves a lot of human labor. Moreover, we propose a model which can better utilize the temporal property of the videos and handle the answers in phrase form."}, {"heading": "6 Conclusion and Future Works", "text": "We propose to collect a large-scale Video-QA dataset by automatically converting from the video description dataset. To tackle the Video-QA task, we propose two mechanisms: the re-watching and the re-reading mechanisms and then combine them into an effective forgettable-watcher model. In the future, we will improve the quality and increase the quantity of our dataset. We will also consider more QA types especially the open-ended QA problems."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["Bigham et al", "2010] Jeffrey P Bigham", "Chandrika Jayant", "Hanjie Ji", "Greg Little", "Andrew Miller", "Robert C Miller", "Robin Miller", "Aubrey Tatarowicz", "Brandyn White", "Samual White"], "venue": "In Proceedings of the 23nd annual ACM sym-", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304,", "citeRegEx": "Gao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatically generating reading comprehension look-back strategy: questions from expository texts", "author": ["Donna M Gates"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Gates. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Proceedings of the National Academy of Sciences", "author": ["Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes. Visual turing test for computer vision systems"], "venue": "112(12):3618\u20133623,", "citeRegEx": "Geman et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 15\u201335", "author": ["Alex Graves. Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks"], "venue": "Springer,", "citeRegEx": "Graves. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Good question! statistical ranking for question generation", "author": ["Heilman", "Smith", "2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Li et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ma et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Mateusz Malinowski", "Mario Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input"], "venue": "pages 1682\u20131690,", "citeRegEx": "Malinowski and Fritz. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Video fill in the blank with merging lstms", "author": ["Amir Mazaheri", "Dong Zhang", "Mubarak Shah"], "venue": "arXiv preprint arXiv:1610.04062,", "citeRegEx": "Mazaheri et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "ICLR,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han. Image question answering using convolutional neural network with dynamic parameter prediction"], "venue": "June", "citeRegEx": "Noh et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel. Exploring models", "data for image question answering"], "venue": "pages 2953\u20132961,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "National Science Foundation", "author": ["Vasile Rus", "C Graesser Arthur. The question generation shared task", "evaluation challenge. In The University of Memphis"], "venue": "Citeseer,", "citeRegEx": "Rus and Arthur. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "Simonyan and Zisserman. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Tapaswi et al", "2016] Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "IEEE MultiMedia", "author": ["Kewei Tu", "Meng Meng", "Mun Wai Lee", "Tae Eun Choe", "Song-Chun Zhu. Joint video", "text parsing for understanding events", "answering queries"], "venue": "21(2):42\u201370,", "citeRegEx": "Tu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola. Stacked attention networks for image question answering"], "venue": "June", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncovering temporal context for video question and answering", "author": ["Linchao Zhu", "Zhongwen Xu", "Yi Yang", "Alexander G Hauptmann"], "venue": "arXiv preprint arXiv:1511.04670,", "citeRegEx": "Zhu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Zhu et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "One of the highlevel tasks towards scene understanding is the visual question answering [Antol et al., 2015].", "startOffset": 88, "endOffset": 108}, {"referenceID": 4, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 2, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 19, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 13, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 9, "context": "However, most of the current visual question answering works focus only on images [Bigham et al., 2010; Geman et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016; Ma et al., 2016].", "startOffset": 82, "endOffset": 195}, {"referenceID": 0, "context": "In image-based question answering (Visual-QA) [Antol et al., 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": ", 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 94, "endOffset": 142}, {"referenceID": 10, "context": ", 2015], most current collection methods require humans to generate the question-answer pairs [Antol et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 94, "endOffset": 142}, {"referenceID": 8, "context": "The TGIF (Tumblr GIF) dataset [Li et al., 2016] is a large-scale video description dataset.", "startOffset": 30, "endOffset": 47}, {"referenceID": 0, "context": "We also extend the VQA model [Antol et al., 2015] as a baseline method.", "startOffset": 29, "endOffset": 49}, {"referenceID": 8, "context": "The TGIF dataset [Li et al., 2016] is collected by Li et al.", "startOffset": 17, "endOffset": 34}, {"referenceID": 5, "context": "Our model first encodes the video features and QA features with two separate bi-directional single layer LSTMs [Graves, 2012].", "startOffset": 111, "endOffset": 125}, {"referenceID": 19, "context": "The representation r(i) of the videos for each QA-sentence token i is formed by a weighted sum of these output vectors (similar to the attention mechanism in Image-QA [Yang et al., 2016]) and the previous representation r(i\u2212 1):", "startOffset": 167, "endOffset": 186}, {"referenceID": 0, "context": "This model is extended from the VQA model [Antol et al., 2015].", "startOffset": 42, "endOffset": 62}, {"referenceID": 16, "context": "We extract the visual features of each frame with VGGNet [Simonyan and Zisserman, 2014].", "startOffset": 57, "endOffset": 87}, {"referenceID": 12, "context": "For the questions and answers, the Word2Vec [Mikolov et al., 2013] trained on GoogleNews is employed to embed each word as a 300-dimensional real vector.", "startOffset": 44, "endOffset": 66}, {"referenceID": 7, "context": "We train all our models using the Adam optimizer [Kingma and Ba, 2015] to minimize the loss.", "startOffset": 49, "endOffset": 70}, {"referenceID": 10, "context": "This motivates us to also apply the WUPS measure [Malinowski and Fritz, 2014] with 0.", "startOffset": 49, "endOffset": 77}, {"referenceID": 0, "context": "The baseline method is an extension of the VQA model [Antol et al., 2015] without our re-watching or re-reading mechanisms.", "startOffset": 53, "endOffset": 73}, {"referenceID": 4, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 0, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 2, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 19, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 13, "context": "[Bigham et al., 2010; Geman et al., 2015; Antol et al., 2015; Gao et al., 2015; Yang et al., 2016; Noh et al., 2016].", "startOffset": 0, "endOffset": 116}, {"referenceID": 14, "context": "The first type is called the Open-Ended question answering [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are produced given only the questions.", "startOffset": 59, "endOffset": 105}, {"referenceID": 10, "context": "The first type is called the Open-Ended question answering [Ren et al., 2015; Malinowski and Fritz, 2014] where answers are produced given only the questions.", "startOffset": 59, "endOffset": 105}, {"referenceID": 10, "context": "0 based on the Wu-Palmer (WUPS) similarity [Malinowski and Fritz, 2014] are employed to measure the answer accuracy.", "startOffset": 43, "endOffset": 71}, {"referenceID": 21, "context": "The other type is called the Multiple-Choice question answering [Zhu et al., 2016] where both the question and several candidate answers are presented.", "startOffset": 64, "endOffset": 82}, {"referenceID": 21, "context": "It is observed that the approaches for the Open-Ended question answering usually cannot produce high-quality long answers [Zhu et al., 2016].", "startOffset": 122, "endOffset": 140}, {"referenceID": 14, "context": "And most Open-Ended question answering approaches only focus on one-word answers [Ren et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 81, "endOffset": 127}, {"referenceID": 10, "context": "And most Open-Ended question answering approaches only focus on one-word answers [Ren et al., 2015; Malinowski and Fritz, 2014].", "startOffset": 81, "endOffset": 127}, {"referenceID": 10, "context": "Malinowski et al [Malinowski and Fritz, 2014] collected their dataset with the human annotations.", "startOffset": 17, "endOffset": 45}, {"referenceID": 0, "context": "[Antol et al., 2015] manually collected a large-scale free-form Image-QA dataset.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "[Gao et al., 2015] also manually collected the FM-IQA (Freestyle Multilingual Image Question Answering) dataset with the help of the Amazon Mechanical Turk platform.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "In contrast, we propose to automatically convert existing video description dataset [Li et al., 2016] into question answering dataset.", "startOffset": 84, "endOffset": 101}, {"referenceID": 3, "context": "It is originally proposed for educational purpose [Gates, 2008].", "startOffset": 50, "endOffset": 63}, {"referenceID": 15, "context": "Among the question generation approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010], we employ the method from Heilman and Smith [Heilman and Smith, 2010] to generate our video-QA pairs from video description datasets.", "startOffset": 41, "endOffset": 101}, {"referenceID": 3, "context": "Among the question generation approaches [Rus and Arthur, 2009; Gates, 2008; Heilman and Smith, 2010], we employ the method from Heilman and Smith [Heilman and Smith, 2010] to generate our video-QA pairs from video description datasets.", "startOffset": 41, "endOffset": 101}, {"referenceID": 14, "context": "[Ren et al., 2015] to turn image description datasets into Image-QA datasets.", "startOffset": 0, "endOffset": 18}, {"referenceID": 18, "context": "[Tu et al., 2014] also combine video and text data for question answering.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[Zhu et al., 2015] collect a dataset for \u201dfill-in-the-blank\u201d type of questions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "[Mazaheri et al., 2016] also consider the fill-in-the-blank problem.", "startOffset": 0, "endOffset": 23}], "year": 2017, "abstractText": "A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored. Video-QA is different from Image-QA since the information and the events are scattered among multiple frames. In order to better utilize the temporal structure of the videos and the phrasal structures of the answers, we propose two mechanisms: the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video question answering with the help of automatic question generation. Finally, we evaluate the models on our dataset. The experimental results show the effectiveness of our proposed models.", "creator": "LaTeX with hyperref package"}}}