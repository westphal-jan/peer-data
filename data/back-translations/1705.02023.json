{"id": "1705.02023", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification", "abstract": "This paper introduces the Senti17 system, which uses ten Convolutionary Neural Networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a Convolutionary Layer followed by a fully connected layer and a Softmax at the top. Ten instances of this network are initialized with the same word embeddings as inputs, but with different initializations for network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system ranks fourth in SemEval-2017 Task4 among 38 systems with 67.4%.", "histories": [["v1", "Thu, 4 May 2017 21:13:24 GMT  (191kb,D)", "http://arxiv.org/abs/1705.02023v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hussam hamdan"], "accepted": false, "id": "1705.02023"}, "pdf": {"name": "1705.02023.pdf", "metadata": {"source": "CRF", "title": "Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification", "authors": ["Hussam Hamdan"], "emails": ["Hussam.Hamdan@lip6.fr"], "sections": [{"heading": null, "text": "This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Soft- max on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4% average recall."}, {"heading": "1 Introduction", "text": "Polarity classification is the basic task of sentiment analysis in which the polarity of a given text should be classified into three categories: positive, negative or neutral. In Twitter where the tweet is short and written in informal language, this task needs more attention. SemEval has proposed the task of Message Polarity Classification in Twitter since 2013, the objective is to classify a tweet into one of the three polarity labels (Rosenthal et al., 2017).\nWe can remark that in 2013, 2014 and 2015 most best systems were based on a rich feature extraction process with a traditional classifier such as SVM (Mohammad et al., 2013) or Logistic regression (Hamdan et al., 2015). In 2014, Kim (2014) proposed to use one convolutional neural network for sentence classification, he fixed the size of the input sentence and concatenated its word embeddings\nfor representing the sentence, this architecture has been exploited in many later works. Severyn and Moschitti (2015) adapted the convolutional network proposed by Kim (2014) for sentiment analysis in Twitter, their system was ranked second in SemEval2015 while the first system (Hagen et al., 2015) combined four systems based on feature extraction and the third ranked system used logistic regression with different groups of features (Hamdan et al., 2015).\nIn 2016, we remark that the number of participations which use feature extraction systems were degraded, and the first four systems used Deep Learning, the majority used a convolutional network except the fourth one (Amir et al., 2016). Despite of that, using Deep Learning for sentiment analysis in Twitter has not yet shown a big improvement in comparison to feature extraction, the fifth and sixth systems (Hamdan, 2016) in 2016 which were built upon feature extraction process were only (3 and 3.5% respectively) less than the first system. But We think that Deep Learning is a promising direction in sentiment analysis. Therefore, we proposed to use convolutional networks for Twitter polarity classification.\nOur proposed system consists of a convolutional layer followed by fully connected layer and a softmax on top. This is inspired by Kim (2014), we just added a fully connected layer. This architecture gives a good performance but it could be improved. Regarding the best system in 2016 (Deriu et al., 2016), it uses different word embeddings for initialisation then it combines the predictions of different nets using a meta-classifier, Word2vec and Glove have been used to vary the tweet representation.\nar X\niv :1\n70 5.\n02 02\n3v 1\n[ cs\n.C L\n] 4\nM ay\nIn our work, we propose to vary the neural network weights instead of tweet representation which can get the same effect of varying the word embeddings, therefore we vary the initial weights of the network to produce ten different nets, a voting system over the these ten voters will decide the sentiment label for a tweet.\nThe remaining of this paper is organized as follows: Section 2 describes the system architecture, Section 3 presents our experiments and results and Section 4 is devoted for the conclusion."}, {"heading": "2 System Architecture", "text": "The architecture of our convolutional neural network for sentiment classification is shown on Fig. 1. Our network is composed of a single convolutional layer followed by a non-linearity, max pooling, Dropout, fully connected layer and a soft-max classification layer. Here we describe this architecture:"}, {"heading": "2.1 Tweet Representation", "text": "We first tokenize each tweet to get all terms using HappyTokenizer1 which captures the words, emoticons and punctuations. We also replace each web link by the term url and each user name by uuser. Then, we used Structured Skip-Gram embeddings (SSG) (Ling et al., 2015) which was compiled by (Amir et al., 2016) using 52 million tweets.\nEach term in the tweet is replaced by its SSG embedding which is a vector of d dimensions, all term vectors are concatenated to form the input matrix where the number of rows is d and the number of columns is set to be maxl: the max tweet length in the training dataset. This 2-dim matrix is the input layer for the neural network."}, {"heading": "2.2 Convolutional Layers", "text": "We connect the input matrix with different convolutional layers, each one applies a convolution operation between the input matrix and a filter of size m x d. This is an element-wise operation which creates f vectors of maxl-m+1 dimension where f is the number of filters or feature maps.\nThis layer is supposed to capture the common patterns among the training tweets which have the same\n1http://sentiment.christopherpotts.net/tokenizing.html\nfilter size but occur at any position of the tweet. To capture the common patterns which have different sizes we have to use more than one layer therefore we defined 8 different layers connected to the input matrix with different filter sizes but the same number of feature maps."}, {"heading": "2.3 Activation Layer", "text": "Each convolutional layer is typically followed by a non-linear activation function, RELU (Rectified Linear Unit ) layer will apply an element-wise operation to swap the negative numbers to 0. The output of a ReLU layer is the same size as the input, just with all the negative values removed. It speeds up the training and is supposed to produce more accurate results."}, {"heading": "2.4 Max-Pooling Layer", "text": "This layer reduces the size of the output of activation layer, for each vector it selects the max value. Different variation of pooling layer can be used: average or k-max pooling."}, {"heading": "2.5 Dropout Layer", "text": "Dropout is used after the max pooling to regularize the ConvNet and prevent overfitting. It assumes that we can still obtain a reasonable classification even when some of the neurones are dropped. Dropout consists in randomly setting a fraction p of input units to 0 at each update during training time."}, {"heading": "2.6 Fully Conected Layer", "text": "We concatenate the results of all pooling layers after applying Dropout, these units are connected to a fully connected layer. This layer performs a matrix multiplication between its weights and the input units. A RELU non-linarity is applied on the results of this layer."}, {"heading": "2.7 Softmax Layer", "text": "The output of the fully connected layer is passed to a Softmax layer. It computes the probability distribution over the labels in order to decide the most probable label for a tweet."}, {"heading": "3 Experiments and Results", "text": "For training the network, we used about 30000 English tweets provided by SemEval organisers and\nthe test set of 2016 which contains 12000 tweets as development set. The test set of 2017 is used to evaluate the system in SemEval-2017 competition. For implementing our system we used python and Keras2.\nWe set the network parameters as follows: SSG embbeding size d is chosen to be 200, the tweet max legnth maxl is 99. For convolutional layers, we set the number of feature maps f to 50 and used 8 filter sizes (1,2,3,4,5,2,3,4). The p value of Dropout layer is set to 0.3. We used Nadam optimizer (Dozat, 2015) to update the weights of the network and back-propogation algorithm to compute the gradients. The batch size is set to be 50 and the training data is shuffled after each iteration.\nWe create ten instances of this network, we randomly initialize them using the uniform distribution, we repeat the random initialization for each instance 100 times, then we pick the networks which gives the highest average recall score as it is considered the official measure for system ranking. If the top network of each instance gives more than 95% of its results identical to another chosen network, we choose the next top networks to make sure that the ten networks are enough different.\nThus, we have ten classifiers, we count the number of classifiers which give the positive, negative and neutral sentiment label to each tweet and select the sentiment label which have the highest number of votes. For each new tweet from the test set, we convert it to 2-dim matrix, if the tweet is longer than\n2https://keras.io\nmaxl, it will be truncated. We then feed it into the ten networks and pass the results to the voting system.\nOfficial ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall.\nTable 4 shows the results of our system on the test set of 2016 and 2017.\nTest Dataset Avg. Recall Accuracy F-score Test 2017 0.674 0.652 0.665 Test 2016 0.692 0.650 0.643\nTable 1: Table 1: Senti17 results on the test sets of 2016 and 2017."}, {"heading": "4 Conclusion", "text": "We presented our deep learning approach to Twitter sentiment analysis. We used ten convolutional neural network voters to get the polarity of a tweet, each voter has been trained on the same training data using the same word embeddings but different initial weights. The results demonstrate that our system is competitive as it is ranked forth in SemEval-2017 task 4-A."}], "references": [{"title": "INESC-ID at SemEval-2016 Task 4-A: Reducing the Problem of Out-of-Embedding Words", "author": ["Amir et al.2016] Silvio Amir", "Ramn Fernndez Astudillo", "Wang Ling", "Mrio J. Silva", "Isabel Trancoso"], "venue": "SemEval@NAACL-HLT", "citeRegEx": "Amir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "Webis: An Ensemble for Twitter Sentiment Detection", "author": ["Hagen et al.2015] Matthias Hagen", "Martin Potthast", "Michel Bchner", "Benno Stein"], "venue": null, "citeRegEx": "Hagen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hagen et al\\.", "year": 2015}, {"title": "Lsislif: Feature Extraction and Label Weighting for Sentiment Analysis in Twitter", "author": ["Hamdan et al.2015] Hussam Hamdan", "Patrice Bellot", "Frederic Bechet"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Hamdan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hamdan et al\\.", "year": 2015}, {"title": "SentiSys at SemEval-2016 Task 4: Feature-Based System for Sentiment Analysis in Twitter", "author": ["Hussam Hamdan"], "venue": "SemEval@NAACLHLT", "citeRegEx": "Hamdan.,? \\Q2016\\E", "shortCiteRegEx": "Hamdan.", "year": 2016}, {"title": "Convolutional Neural Networks for Sentence Classification. CoRR, abs/1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Two/Too Simple Adaptations of word2vec for Syntax Problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "NRCCanada: Building the State-of-the-Art in Sentiment Analysis of Tweets", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu"], "venue": "Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "author": ["Noura Farra", "Preslav Nakov"], "venue": "In Proceedings of the 11th International Workshop on Semantic Evaluation,", "citeRegEx": "Rosenthal et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2017}, {"title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "SemEval has proposed the task of Message Polarity Classification in Twitter since 2013, the objective is to classify a tweet into one of the three polarity labels (Rosenthal et al., 2017).", "startOffset": 163, "endOffset": 187}, {"referenceID": 6, "context": "We can remark that in 2013, 2014 and 2015 most best systems were based on a rich feature extraction process with a traditional classifier such as SVM (Mohammad et al., 2013) or Logistic regression (Hamdan et al.", "startOffset": 150, "endOffset": 173}, {"referenceID": 2, "context": ", 2013) or Logistic regression (Hamdan et al., 2015).", "startOffset": 31, "endOffset": 52}, {"referenceID": 1, "context": "Severyn and Moschitti (2015) adapted the convolutional network proposed by Kim (2014) for sentiment analysis in Twitter, their system was ranked second in SemEval2015 while the first system (Hagen et al., 2015) combined four systems based on feature extraction and the third ranked system used logistic regression with different groups of features (Hamdan et al.", "startOffset": 190, "endOffset": 210}, {"referenceID": 2, "context": ", 2015) combined four systems based on feature extraction and the third ranked system used logistic regression with different groups of features (Hamdan et al., 2015).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": ", 2013) or Logistic regression (Hamdan et al., 2015). In 2014, Kim (2014) proposed to use one convolutional neural network for sentence classification, he fixed the size of the input sentence and concatenated its word embeddings for representing the sentence, this architecture has been exploited in many later works.", "startOffset": 32, "endOffset": 74}, {"referenceID": 1, "context": ", 2013) or Logistic regression (Hamdan et al., 2015). In 2014, Kim (2014) proposed to use one convolutional neural network for sentence classification, he fixed the size of the input sentence and concatenated its word embeddings for representing the sentence, this architecture has been exploited in many later works. Severyn and Moschitti (2015) adapted the convolutional network proposed by Kim (2014) for sentiment analysis in Twitter, their system was ranked second in SemEval2015 while the first system (Hagen et al.", "startOffset": 32, "endOffset": 347}, {"referenceID": 1, "context": ", 2013) or Logistic regression (Hamdan et al., 2015). In 2014, Kim (2014) proposed to use one convolutional neural network for sentence classification, he fixed the size of the input sentence and concatenated its word embeddings for representing the sentence, this architecture has been exploited in many later works. Severyn and Moschitti (2015) adapted the convolutional network proposed by Kim (2014) for sentiment analysis in Twitter, their system was ranked second in SemEval2015 while the first system (Hagen et al.", "startOffset": 32, "endOffset": 404}, {"referenceID": 0, "context": "In 2016, we remark that the number of participations which use feature extraction systems were degraded, and the first four systems used Deep Learning, the majority used a convolutional network except the fourth one (Amir et al., 2016).", "startOffset": 216, "endOffset": 235}, {"referenceID": 3, "context": "Despite of that, using Deep Learning for sentiment analysis in Twitter has not yet shown a big improvement in comparison to feature extraction, the fifth and sixth systems (Hamdan, 2016) in 2016 which were built upon feature extraction process were only (3 and 3.", "startOffset": 172, "endOffset": 186}, {"referenceID": 4, "context": "This is inspired by Kim (2014), we just added a fully connected layer.", "startOffset": 20, "endOffset": 31}, {"referenceID": 5, "context": "Then, we used Structured Skip-Gram embeddings (SSG) (Ling et al., 2015) which was compiled by (Amir et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 0, "context": ", 2015) which was compiled by (Amir et al., 2016) using 52 million tweets.", "startOffset": 30, "endOffset": 49}], "year": 2017, "abstractText": "This paper presents Senti17 system which uses ten convolutional neural networks (ConvNet) to assign a sentiment label to a tweet. The network consists of a convolutional layer followed by a fully-connected layer and a Softmax on top. Ten instances of this network are initialized with the same word embeddings as inputs but with different initializations for the network weights. We combine the results of all instances by selecting the sentiment label given by the majority of the ten voters. This system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4% average recall.", "creator": "LaTeX with hyperref package"}}}