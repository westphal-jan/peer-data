{"id": "1705.03670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "abstract": "However, the quality of the learnt characteristics is not good enough, so a complex back-end model, either neural or probabilistic, must be used to eliminate the residual uncertainty in the verification of loudspeakers, just as with the raw functions. This paper presents a revolutionary time-delayed deep neural network structure (CT-DNN) for the learning of loudspeaker functions. Our experimental results on the Fisher database have shown that this CT-DNN can produce high-quality loudspeaker functions: even with a single feature (0.3 seconds including the context), the EER can be up to 7.68%. This effectively confirmed that the characteristic of the loudspeaker is largely a deterministic short-term feature and cannot be extracted from a long-term distribution pattern and therefore from only dozens of frames.", "histories": [["v1", "Wed, 10 May 2017 09:30:42 GMT  (977kb,D)", "http://arxiv.org/abs/1705.03670v1", "deep neural networks, speaker verification, speaker feature"]], "COMMENTS": "deep neural networks, speaker verification, speaker feature", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["lantian li", "yixiang chen", "ying shi", "zhiyuan tang", "dong wang"], "accepted": false, "id": "1705.03670"}, "pdf": {"name": "1705.03670.pdf", "metadata": {"source": "CRF", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "authors": ["Lantian Li", "Yixiang Chen", "Ying Shi", "Zhiyuan Tang", "Dong Wang"], "emails": ["lilt13@mails.tsinghua.edu.cn,", "wangdong99@mails.tsinghua.edu.cn,", "chenxy@cslt.riit.tsinghua.edu.cn", "shiying@cslt.riit.tsinghua.edu.cn", "tangzy@cslt.riit.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional timedelay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce highquality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames. Index Terms: deep neural networks, speaker verification, speaker feature\n1. Introduction Automatic speaker verification (ASV) is an important biometric authentication technology. As in most machine learning tasks, a key challenge of ASV is the intermixing of multiple variability factors involved in the speech signal, which leads to great uncertainty when making genuine/imposter decision. In principle, two methods can be employed to address the uncertainty: either by extracting more powerful features which are sensitive to speaker traits but invariant to other variations, or by constructing a statistical model that can describe the uncertainty and promote the speaker factor.\nMost of existing successful ASV approaches are modelbased. For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3]. They are generative models and heavily utilize unsupervised learning. Improvements have been achieved in two directions. The first is to use a discriminative model to boost the discriminant for speakers, e.g., the SVM model for the GMM-UBM approach [4] and the PLDA model for the i-vector approach[5]. The second is to use supervised learning to produce a better representation for the acoustic space. For example, the DNN-based i-vector method [6, 7]. Almost all the model-based methods use raw features, e.g., the popular Mel frequency cepstral coefficients (MFCC) feature.\nIn spite of the predominant success of the model-based approach, researchers never stop searching for \u2018fundamental\u2019 features for speaker traits. The motivation is two-fold: from the engineering perspective, if a better feature is found, the present complex statistical models can be largely discarded; and from the cognitive perspective, a fundamental feature will help us\nunderstand how speaker traits are embedded in speech signals. Driven by these motivations, many researchers put their effort in \u2018feature engineering\u2019 in the past several decades, and new features were proposed occasionally, from perspectives of different knowledge domains [8]. However, compared to the remarkable achievement with the model-based approach, the reward from the feature engineering is rather marginal. After decades, we find the most useful feature in our hand is still MFCC. Interestingly, the same story was also told in other fields of speech processing, particularly in automatic speech recognition (ASR), until very recently after deep learning involved.\nThe development of deep learning changed the story. Different from the historic feature engineering methods that design features by human knowledge, deep learning can learn features automatically from vast raw data, usually by a multi-layer structure, e.g., a deep neural network (DNN). By the layer-bylayer processing, task-related information can be preserved and strengthened, while task-irrelevant variations are diminished and removed. This feature learning has been demonstrated to be very successful in ASR, where the learned features have shown to be highly representative for linguistic content and very robust against variations of other factors [9].\nThis success of feature learning in ASR has motivated researchers in ASV to learn speaker sensitive features. The primary success was reported by Ehsan et al. on a text-dependent task [10]. They constructed a DNN model with 496 speakers in the training set as the targets. The frame-level features were read from the activations of the last hidden layer, and the utterance-level representations (called \u2018d-vector\u2019) were obtained by averaging over frame-level features. In evaluation, the decision score was computed as a simple cosine distance between the d-vectors of the enrollment utterance(s) and the test utterance. The authors reported worse performance with the dvector system compared to the conventional i-vector baseline, but after combining the two systems, better performance was obtained. This method was further extended by a number of researchers. For example, Heigold et al. [11] used an LSTMRNN to learn utterance-level representations directly and reported better performance than the i-vector system on the same text-dependent task when a large database was used (more than 4, 000 speakers). Zhang et al. [12] utilized convolutional neural networks (CNN) to learn speaker features and an attentionbased model to learn how to make decisions, again on a textdependent task. Liu et al. [13] used the DNN-learned features to build the conventional i-vector system. Recently, Snydern et al. [14] migrated the DNN-based approach to text-independent tasks, and reported better performance than the i-vector system when the training data is sufficiently large (102k speakers). All these following-up studies, however, are not purely feature learning: they all involve a complex back-end model, either neural or probabilistic, to gain reasonable performance. This is perfectly fine from the perspective of both research and ar X iv :1 70 5. 03 67 0v 1\n[ cs\n.S D\n] 1\n0 M\nay 2\n01 7\nengineering, but departs from the initial goal of feature learning: we hope to discover a feature that is sufficiently general and discriminative so that it can be employed in a broad range of applications without heavy back-end models. This has been achieved in ASR, but not in speaker verification yet.\nIn this paper, we present a simple but effective DNN structure that involves two convolutional layers and two timedelayed full-connection layers to learn speaker features. Our experiments demonstrated that this simple model can learn very strong speaker sensitive features, using speech data of only a few thousand of speakers. The learned feature does not require complex back-end models: a simple frame averaging is sufficient to produce a strong utterance-level speaker vector, by which a simple cosine distance is good enough to perform textindependent ASV tasks. These results actually demonstrated that it is possible to discover speaker information from a shorttime speech segment (300 ms), by only a couple of simple neural propagation.\nThe rest of this paper is organized as follows. Section 2 describes the related work, and Section 3 presents the new DNN structure. The experiments are presented in Section 4, and Section 5 concludes the paper.\n2. Related work Our work is a direct extension of the d-vector model presented by Ehsan et al [10]. The extension is two-fold: a CNN/TDNN structure that emphasizes on temporal-frequency filtering, more resemble to the traditional feature engineering; an experiment on a text-independent task demonstrated that the learned feature is independent of linguistic content and highly speaker sensitive.\nThis work is different from most of the existing neuralbased ASV methods. For example, the RNN-based utterancelevel representation learning [11] is attractive, but the RNN pooling shifts the focus to the entire sentence, rather than framelevel feature learning. The end-to-end neural models proposed by Snyder [14] and Zhang [12] both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor. Therefore, the features are not necessarily speaker discriminative and are less generalizable, as the feature extractor depends on the classifier.\nThis work is also different from the methods that combine DNN features and statistical models. In these methods, some speaker information is learned in the feature, but not sufficient. Therefore, the feature is still primary and thus a statistical model has to be used to address the inherent uncertainty. For example, Liu et al. [13] used an ASR-ASV multi-task DNN to produce frame-level features and substituted them for MFCC to construct GMM-UBM and i-vector systems. Yao et al. [15] proposed a similar approach, though they used ASR-oriented features to train GMMs for splitting the acoustic space, and the original ASV-oriented features as the acoustic feature to construct the i-vector model.\nAn implication behind the above methods is that the speaker feature learning is still imperfect: the speaker traits have not been fully extracted and other irrelevant variations still exist, and therefore some back-end models have to be utilized to improve the discriminative power. In this paper, we will show that a better network design can significantly improve the quality of the feature learning, hence greatly alleviating the reliance on a back-end model."}, {"heading": "3. CT-DNN for feature learning", "text": "This section presents our DNN structure for speaker sensitive feature learning. This structure is an extension to the model proposed in [10], by using convolutional layers to extract local discriminative patterns from the temporal-frequency space, and time-delayed layers to increase the effective temporal context for each frame. We call this structure as CT-DNN.\nFigure 1 illustrates the CT-DNN structure used in this work. It consists of a convolutional (CN) component and a time-delay (TD) component, connected by a bottleneck layer consisting of 512 hidden units. The convolutional component involves two CN layers, each followed by a max pooling. This component is used to learn local patterns that are useful in representing speaker traits. The TD component involves two TD layers, each followed by a P-norm layer. This component is used to extend the temporal context. The settings for the two components, including the patch size, the number of feature maps, the timedelay window, the group size of the P-norm, have been shown in Figure 1. A simple calculation shows that with these settings, the size of the effective context window is 20 frames. The output of the P-norm layer is projected to a feature layer consisting of 400 units, which is connected to the output layer whose units correspond to the speakers in the training data.\nThis CT-DNN model is rather simple and can be easily trained. In our study, the natural stochastic gradient descent (NSGD) [16] algorithm was employed to conduct the optimization. Once the DNN model has been trained, the speaker feature can be read from the feature layer, i.e., the last hidden layer of the model. As in [10], the utterance-level representation of a speech segment can be simply derived by averaging the speaker features of all the frames of the speech segment.\nFollowing the name convention of the previous work [10, 17], the utterance-level representations derived from the CTDNN are called d-vectors. During test, the d-vectors of the test and enrollment utterances are produced respectively. A simple cosine distance between these two vectors can be computed and used as the decision score for the ASV task. Similar with ivectors, some simple normalization methods can be employed, such as linear discriminant analysis (LDA) and probabilistic LDA (PLDA)."}, {"heading": "4. Experiments", "text": "In this section, we first present the database used in the experiments, and then report the results with the i-vector and the dvector systems. All the experiments were conducted with the Kaldi toolkit [18]."}, {"heading": "4.1. Database", "text": "The Fisher database was used in our experiments. The training data and the test data are presented as follows.\n\u2022 Training set: It consists of 2, 500 male and 2, 500 female speakers, with 95, 167 utterances randomly selected from the Fisher database, and each speaker has about 120 seconds speech segments. This dataset was used for training the UBM, the T-matrix, and the LDA/PLDA models of the i-vector system, and the CTDNN model of the d-vector system.\n\u2022 Evaluation set: It consists of 500 male and 500 female speakers randomly selected from the Fisher database. There is no overlap between the speakers of the training\nset and the evaluation set. For each speaker, 10 utterances are used for enrollment and the rest for test.\nThe test were conducted in 4 conditions, each with a different setting in the length of the enrollment and the test utterances. The conditions are shown in Table 1. All the test conditions involve pooled male and female trials. Gender-dependent test exhibited the same trend, so we just report the results with the pooled data."}, {"heading": "4.2. Model settings", "text": "We built an i-vector system as the baseline. The raw feature involves 19-dimensional MFCCs plus the log energy. This raw feature is augmented by its first and second order derivatives, resulting in a 60-dimensional feature vector. This MFCC feature was used by the i-vector model. The UBM was composed of 2, 048 Gaussian components, and the dimensionality of the i-vector space was 400. The dimensionality of the LDA projection space was set to 150. The entire system was trained using the Kaldi SRE08 recipe.\nFor the d-vector system, the architecture was based on Figure 1. The input feature was 40-dimensional Fbanks, with a symmetric 4-frame window to splice the neighboring frames, resulting in 9 frames in total. The number of output units was 5, 000, corresponding to the number of speakers in the training data. The speaker features were extracted from the last hidden layer (the feature layer in Figure 1), and the utterance-level dvectors were derived by averaging the frame-level features. The transform/scoring methods used for the i-vector system were also used for the d-vector system during the test, including cosine distance, LDA and PLDA. The Kaldi recipe to reproduce our results has been published online1."}, {"heading": "4.3. Main results", "text": "The results of the i-vector system and the d-vector system in terms of equal error rate (EER%) are reported in Table 2. The\n1http://data.cslt.org\ntwo systems were trained with the entire training set, and the results are reported for different conditions.\nIt can be observed that for both the two systems, improving the length of the test utterances always improves performance. However, it seems that the performance improvement for the ivector system is more significant than the d-vector system. This is understandable as the i-vector system relies on the statistical pattern of the features to build speaker vectors, so more speech frames will help. In contrast, the d-vector system uses a simple average of the features to represent a speaker, so the contribution of more speech frames is marginal.\nThe most interesting observation is the clear advantage of the d-vector system in the C(30-3) condition, where the test utterances are short. Because the d-vector system does not use any powerful back-end model, this advantage on short utterances implies that the features learned by the CT-DNN model is rather powerful. The advantage of neural-based models on short utterances has been partially observed from DET curves by Snyder et al. [14], and our results give a more clear evidence to this trend.\nAnother observation is that the LDA approach improves the d-vector system, while the PLDA approach does not. The contribution of LDA suggests that there is still some non-speaker variation within the learned feature, which needs more investigation. The failure of PLDA with d-vectors is also a known issue in our previous work [17]. A possible reason is that the residual noise within d-vectors is not Gaussian, and so cannot be well modeled by the PLDA model. Again, more investigation is under going."}, {"heading": "4.4. Training data size", "text": "In order to investigate the data dependency of the feature learning approach, we change the size of the training data by selecting different numbers of speakers. The results of the best i-vector system (i-vector + PLDA) and the best d-vector system\n(d-vector + LDA) are reported in each of the four test conditions. The results are shown in Figure 2, where each picture presents a particular test condition. It can be seen that in all the test conditions, for both the i-vector and d-vector systems, better performance is obtained with more training data, but the i-vector system seems benefit more from big data. This is a little different from our experience that deep neural models need more data than probabilistic models. It is also different from the observation in [14], where the d-vector system obtained more performance improvement than the i-vector system when the number of speakers got very large (102k).\nWe attribute the relatively less data sensitivity of the dvector system to two factors: (1) the compact CN and TD layers in the CT-DNN structure require less training data; (2) the major restriction on performance of the d-vector system is not the model training, but the simple average back-end. In the C(303) condition where the test utterances are short, the impact of feature average is less significant, and so the true quality of the learned feature is exhibited, leading to a clear performance improvement when more training data are used. In other conditions, however, the improvement obtained by the big data training may have largely been masked by the average back-end."}, {"heading": "4.5. Feature discrimination", "text": "To check the quality of the learned speaker feature, we use tSNE [19] to draw some feature samples from 20 speakers. The samples are selected in two ways: (a) randomly sample from all the speech frames of the speaker; (b) choose a particular utterance. The results are presented in Figure 3. It can be seen that the learned features are very discriminative for speakers, but there is still some variation caused by linguistic content, as seen from the plot (b).\nA more quantitative test for the feature quality is to examine the extreme case where the test speech is only a few frames.\nLet\u2019s start from 20 frames, which is actually the effective context size of the CT-DNN, so only a single feature is produced. More frames will produce more features. Table 3 and Table 4 present the results, where the length of the enrollment utterances is 30 seconds and 3 seconds, respectively. The i-vector and d-vector models used in this experiment were trained with the entire training data.\nThe results with the d-vector system are striking: with only one feature (20 frames, or equally 0.3 ms), the EER can reach 13.54% if the enrollment speech is only 3 seconds, and 7.68% when the enrollment speech is 30 seconds! In comparison, the i-vector system largely fails in these conditions.\nThese results demonstrated that the CT-DNN model has learned a highly discriminative feature. From another perspective, these results also demonstrated that the speaker trait is largely a deterministic short-time property rather than a longtime distributional pattern, and therefore can be extracted from just dozens of speech frames. This indicates that feature learning may be a more reasonable approach than the existing modelbased approaches that rely on statistical patterns of raw features."}, {"heading": "5. Conclusions", "text": "This paper presented a CT-DNN model to learn speaker sensitive features. Our experiments showed that the learned feature is highly discriminative and can be used to achieve impressive performance when the test utterances are short. This result has far-reaching implications for both research and engineering: on one hand, it means that the speaker trait is a kind of shorttime pattern and so should be extracted by short-time analysis (including neural learning) rather than long-time probabilistic modeling; on the other hand, our result suggests that the feature learning approach could/should be used when the test utterances are short, a condition that many practical applications are interested in. Lots of work remains, e.g., How the feature should be modeled in a simple way, if PLDA does not work? How the neural-learned features generalize from one task (or language) to another? How to involve auxiliary information to boost the model quality? All need careful investigation.\n6. References [1] D. Reynolds, T. Quatieri, and R. Dunn, \u201cSpeaker verification us-\ning adapted gaussian mixture models,\u201d Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.\n[2] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel, \u201cJoint factor analysis versus eigenchannels in speaker recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.\n[3] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[4] W. Campbell, D. Sturim, and D. Reynolds, \u201cSupport vector machines using gmm supervectors for speaker verification,\u201d Signal Processing Letters, IEEE, vol. 13, no. 5, pp. 308\u2013311, 2006.\n[5] S. Ioffe, \u201cProbabilistic linear discriminant analysis,\u201d Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.\n[6] P. Kenny, V. Gupta, T. Stafylakis, P. Ouellet, and J. Alam, \u201cDeep neural networks for extracting baum-welch statistics for speaker recognition,\u201d Odyssey, 2014.\n[7] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \u201cA novel scheme for speaker recognition using a phonetically-aware deep neural network,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u2013 1699.\n[8] T. Kinnunen and H. Li, \u201cAn overview of text-independent speaker recognition: From features to supervectors,\u201d Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.\n[9] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[10] V. Ehsan, L. Xin, M. Erik, L. M. Ignacio, and G.-D. Javier, \u201cDeep neural networks for small footprint text-dependent speaker verification,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, vol. 28, no. 4, 2014, pp. 357\u2013366.\n[11] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, \u201cEnd-to-end text-dependent speaker verification,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5115\u20135119.\n[12] S.-X. Zhang, Z. Chen, Y. Zhao, J. Li, and Y. Gong, \u201cEnd-to-end attention based text-dependent speaker verification,\u201d arXiv preprint arXiv:1701.00562, 2017.\n[13] Y. Liu, Y. Qian, N. Chen, T. Fu, Y. Zhang, and K. Yu, \u201cDeep feature for text-dependent speaker verification,\u201d Speech Communication, vol. 73, pp. 1\u201313, 2015.\n[14] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, Y. Carmiel, and S. Khudanpur, \u201cDeep neural network-based speaker embeddings for end-to-end speaker verification,\u201d in SLT\u20192016, 2016.\n[15] T. Yao, C. Meng, H. Liang, and L. Jia, \u201cSpeaker recognition system based on deep neural networks and bottleneck features,\u201d Journal of Tsinghua University (Science and Technology), vol. 56, no. 11, pp. 1143\u20131148, 2016.\n[16] D. Povey, X. Zhang, and S. Khudanpur, \u201cParallel training of dnns with natural gradient and parameter averaging,\u201d arXiv preprint arXiv:1410.7455, 2014.\n[17] L. Li, Y. Lin, Z. Zhang, and D. Wang, \u201cImproved deep speaker feature learning for text-dependent speaker recognition,\u201d in Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia-Pacific. IEEE, 2015, pp. 426\u2013 429.\n[18] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011.\n[19] L. v. d. Maaten and G. Hinton, \u201cVisualizing data using t-sne,\u201d Machine Learning Research, 2008."}], "references": [{"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W. Campbell", "D. Sturim", "D. Reynolds"], "venue": "Signal Processing Letters, IEEE, vol. 13, no. 5, pp. 308\u2013311, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyssey, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u2013 1699.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of text-independent speaker recognition: From features to supervectors", "author": ["T. Kinnunen", "H. Li"], "venue": "Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, vol. 28, no. 4, 2014, pp. 357\u2013366.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "arXiv preprint arXiv:1701.00562, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep feature for text-dependent speaker verification", "author": ["Y. Liu", "Y. Qian", "N. Chen", "T. Fu", "Y. Zhang", "K. Yu"], "venue": "Speech Communication, vol. 73, pp. 1\u201313, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "SLT\u20192016, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Speaker recognition system based on deep neural networks and bottleneck features", "author": ["T. Yao", "C. Meng", "H. Liang", "L. Jia"], "venue": "Journal of Tsinghua University (Science and Technology), vol. 56, no. 11, pp. 1143\u20131148, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved deep speaker feature learning for text-dependent speaker recognition", "author": ["L. Li", "Y. Lin", "Z. Zhang", "D. Wang"], "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia-Pacific. IEEE, 2015, pp. 426\u2013 429.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Machine Learning Research, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "For example, the famous Gaussian mixture modeluniversal background model (GMM-UBM) framework [1] and the subsequent subspace models, including the joint factor analysis approach [2] and the i-vector model [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 3, "context": ", the SVM model for the GMM-UBM approach [4] and the PLDA model for the i-vector approach[5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": ", the SVM model for the GMM-UBM approach [4] and the PLDA model for the i-vector approach[5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "For example, the DNN-based i-vector method [6, 7].", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "For example, the DNN-based i-vector method [6, 7].", "startOffset": 43, "endOffset": 49}, {"referenceID": 7, "context": "Driven by these motivations, many researchers put their effort in \u2018feature engineering\u2019 in the past several decades, and new features were proposed occasionally, from perspectives of different knowledge domains [8].", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "This feature learning has been demonstrated to be very successful in ASR, where the learned features have shown to be highly representative for linguistic content and very robust against variations of other factors [9].", "startOffset": 215, "endOffset": 218}, {"referenceID": 9, "context": "on a text-dependent task [10].", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "[11] used an LSTMRNN to learn utterance-level representations directly and reported better performance than the i-vector system on the same text-dependent task when a large database was used (more than 4, 000 speakers).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] utilized convolutional neural networks (CNN) to learn speaker features and an attentionbased model to learn how to make decisions, again on a textdependent task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] used the DNN-learned features to build the conventional i-vector system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] migrated the DNN-based approach to text-independent tasks, and reported better performance than the i-vector system when the training data is sufficiently large (102k speakers).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Our work is a direct extension of the d-vector model presented by Ehsan et al [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "For example, the RNN-based utterancelevel representation learning [11] is attractive, but the RNN pooling shifts the focus to the entire sentence, rather than framelevel feature learning.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "The end-to-end neural models proposed by Snyder [14] and Zhang [12] both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "The end-to-end neural models proposed by Snyder [14] and Zhang [12] both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "[13] used an ASR-ASV multi-task DNN to produce frame-level features and substituted them for MFCC to construct GMM-UBM and i-vector systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a similar approach, though they used ASR-oriented features to train GMMs for splitting the acoustic space, and the original ASV-oriented features as the acoustic feature to construct the i-vector model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "This structure is an extension to the model proposed in [10], by using convolutional layers to extract local discriminative patterns from the temporal-frequency space, and time-delayed layers to increase the effective temporal context for each frame.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "In our study, the natural stochastic gradient descent (NSGD) [16] algorithm was employed to conduct the optimization.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "As in [10], the utterance-level representation of a speech segment can be simply derived by averaging the speaker features of all the frames of the speech segment.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "Following the name convention of the previous work [10, 17], the utterance-level representations derived from the CTDNN are called d-vectors.", "startOffset": 51, "endOffset": 59}, {"referenceID": 16, "context": "Following the name convention of the previous work [10, 17], the utterance-level representations derived from the CTDNN are called d-vectors.", "startOffset": 51, "endOffset": 59}, {"referenceID": 17, "context": "All the experiments were conducted with the Kaldi toolkit [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "[14], and our results give a more clear evidence to this trend.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The failure of PLDA with d-vectors is also a known issue in our previous work [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "It is also different from the observation in [14], where the d-vector system obtained more performance improvement than the i-vector system when the number of speakers got very large (102k).", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "To check the quality of the learned speaker feature, we use tSNE [19] to draw some feature samples from 20 speakers.", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional timedelay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce highquality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.", "creator": "LaTeX with hyperref package"}}}