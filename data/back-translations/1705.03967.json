{"id": "1705.03967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "GQ($\\lambda$) Quick Reference and Implementation Guide", "abstract": "This document should serve as a brief reference and guide to the implementation of the linear GQ ($\\ lambda $), a gradient-based learning algorithm for temporal differences outside politics. Explanations of the intuition and theory behind the algorithm can be found elsewhere (e.g. Maei & amp; Sutton 2010, Maei 2011). If you have any questions or concerns about the content of this document or the accompanying Java code, please send an e-mail to Adam White (adam.white @ ualberta.ca).", "histories": [["v1", "Wed, 10 May 2017 22:43:11 GMT  (4kb)", "http://arxiv.org/abs/1705.03967v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adam white", "richard s sutton"], "accepted": false, "id": "1705.03967"}, "pdf": {"name": "1705.03967.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["(adam.white@ualberta.ca)."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n03 96\n7v 1\n[ cs\n.L G\n] 1\n0 M\nay 2\n01 7 GQ(\u03bb) Quick Reference and Implementation Guide\nAdam White and Richard S. Sutton\nRevised July 29, 2014\nThis document should serve as a quick reference for and guide to the implementation of linear GQ(\u03bb), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca)."}, {"heading": "1 Requirements and Setting", "text": "For each use of GQ(\u03bb) you will need to provide three question functions specifying the quantity to be predicted, and four answer functions characterizing the approximation that will be found. Let S and A denote the sets of states and actions. Then the question functions are:\n\u2022 \u03c0 : S \u00d7 A \u2192 [0, 1]; target policy to be learned. Incidently, if \u03c0 is chosen as the greedy policy with respect to the learned value function, then the algorithm will implement a generalization of the Greedy-GQ algorithm (Maei, Szepesvari, Bhatnagar & Sutton 2010).\n\u2022 \u03b3 : S\u2192 [0, 1]; termination or discounting function (\u03b3(s) = 1\u2212 \u03b2(s) in GQ paper)\n\u2022 r : S\u00d7A\u00d7 S\u2192 R; reward function\nIn many publications there is also specified a fourth question function, the terminal reward function z : S \u2192 R used to specify a final reward at termination. More recently its has been recognized that this functionality can be included in the reward function, making use of the discounting function (Modayil, White & Sutton 2014). For example, if one wanted only a terminal reward function z(s) upon termination in state s, one would use a\nreward function of r(s, a, s\u2032) = (1 \u2212 \u03b3(s\u2032))z(s\u2032). This completes the specification of the predictive question that you are seeking to answer using the GQ(\u03bb) algorithm.\nThe answer functions are:\n\u2022 b : S\u00d7A\u2192 [0, 1]; behavior policy\n\u2022 I : S \u00d7 A \u2192 [0, 1]; interest function (can set to 1 for all state-action pairs or indicate selected state-action pairs to be best approximated)\n\u2022 \u03c6 : S\u00d7A\u2192 Rn; feature-vector function\n\u2022 \u03bb : S\u2192 [0, 1]; bootstrapping or eligibility-trace decay-rate function\nThe following data structures are internal to GQ:\n\u2022 \u03b8 \u2208 Rn; the learned weights of the linear approximation: Q\u03c0(s, a) = \u03b8\u22a4\u03c6(s, a) =\n\u2211n i=1 \u03b8i\u03c6i(s, a)\n\u2022 w \u2208 Rn; secondary set of learned weights\n\u2022 e \u2208 Rn; eligibility trace vector\nParameters internal to GQ:\n\u2022 \u03b1; step-size parameter for learning \u03b8\n\u2022 \u03b7 \u2208 [0, 1]; relative step-size parameter for learning w (\u03b1\u03b7)"}, {"heading": "2 Algorithm Specification", "text": "We can now specify GQ(\u03bb). Let w and e be initialized to zero and \u03b8 be initialized arbitrarily. Let the subscript t denote the current time step. Let \u03c1t denote the \u201cimportance sampling\u201d ratio:\n\u03c1t = \u03c0(St, At)\nb(St, At) , (1)\nwhere St and At are the state and action occuring on time step t. Let \u03c6\u0304t denote the expected next feature vector, defined by:\n\u03c6\u0304t = \u2211\na\u2208A\n\u03c0(St, a)\u03c6(St, a) (2)\nThen the following equations fully specify GQ(\u03bb):\n\u03b4t = r(St, At, St+1) + \u03b3(St+1)\u03b8 \u22a4 t \u03c6\u0304t+1 \u2212 \u03b8 \u22a4 t \u03c6(St, At) (3)\n\u03b8t+1 = \u03b8t + \u03b1 [ \u03b4tet \u2212 \u03b3(St+1)(1 \u2212 \u03bb(St+1))(w \u22a4 t et)\u03c6\u0304t+1 ]\n(4)\nwt+1 = wt + \u03b1\u03b7[\u03b4tet \u2212 (w \u22a4 t \u03c6(St, At))\u03c6(St, At)] (5)\net = I(St)\u03c6(St, At) + \u03b3(St)\u03bb(St)\u03c1tet\u22121 (6)"}, {"heading": "3 Pseudocode", "text": "The following pseudocode characterizes the algorithm and its use.\nInitialize \u03b8 arbitrarily and w = 0 Repeat (for each episode):\nInitialize e = 0 S \u2190 initial state of episode Repeat (for each step of episode):\nA\u2190 action selected by policy b in state S Take action A, observe next state, S\u2032 \u03c6\u0304\u2190 0 For all a \u2208 A(s):\n\u03c6\u0304\u2190 \u03c6\u0304+ \u03c0(S\u2032, a)\u03c6(S\u2032, a)\n\u03c1 = \u03c0(S,A) b(S,A) GQlearn(\u03c6(S,A), \u03c6\u0304, \u03bb(S\u2032), \u03b3(S\u2032), r(S,A, S\u2032), \u03c1, I(S)) S \u2190 S\u2032\nuntil S\u2032 is terminal\nGQLearn(\u03c6, \u03c6\u0304, \u03bb, \u03b3,R, \u03c1, I) \u03b4 \u2190 R+ \u03b3\u03b8\u22a4\u03c6\u0304\u2212 \u03b8\u22a4\u03c6 e\u2190 \u03c1e+ I\u03c6 \u03b8 \u2190 \u03b8 + \u03b1(\u03b4e \u2212 \u03b3(1\u2212 \u03bb)(w\u22a4e)\u03c6\u0304) w \u2190 w + \u03b1\u03b7(\u03b4e \u2212 (w\u22a4\u03c6)\u03c6) e\u2190 \u03b3\u03bbe"}, {"heading": "4 Code", "text": "The files GQlambda.java and GQlambda.cpp (in the arXiv source archive) contain implementations of the GQlearn function described in the pseudocode. We have excluded optimizations (e.g., binary features or efficient\ntrace implementation) to ensure the code is simple and easy to understand. We leave it to the reader to provide environment code for interfacing to GQ(\u03bb) (e.g., using RL-Glue)."}, {"heading": "5 References", "text": "Maei, H. R., Szepesva\u0301ri, Cs., Bhatnagar, S., Sutton, R. S. (2010). Toward off-policy learning control with function approximation. In Proceedings of the 27th International Conference on Machine Learning, Haifa, Israel.\nMaei, H. R. and Sutton, R. S. (2010). GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial General Intelligence, pp. 91\u201396.\nModayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior 22 (2):146\u2013160.\nSutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press."}], "references": [{"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "Szepesv\u00e1ri", "Cs", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "In Proceedings of the Third Conference on Artificial General Intelligence,", "citeRegEx": "Maei and Sutton,? \\Q2010\\E", "shortCiteRegEx": "Maei and Sutton", "year": 2010}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior", "citeRegEx": "Modayil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Modayil et al\\.", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "This document should serve as a quick reference for and guide to the implementation of linear GQ(\u03bb), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca).", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}