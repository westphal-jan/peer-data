{"id": "1705.04804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph", "abstract": "The redundant features that exist in high-dimensional datasets always influence the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph-based approach to automatically locate and remove these redundant features for high-dimensional data. Based on the framework of sparse, learning-based, unattended feature selection, Sparse Feature Graph (SFG) is introduced not only to model redundancy between two features, but also to detect group redundancy between two groups of features. SFG allows us to divide the entire features into different groups and improve the intrinsic structure of the data by removing detected redundant features. With precise data structure, quality indicators can be obtained to enhance the learning performance of existing, unattended feature selection algorithms such as Multi-Feature Selection (MFluticluster).", "histories": [["v1", "Sat, 13 May 2017 09:34:17 GMT  (5527kb)", "https://arxiv.org/abs/1705.04804v1", "submitted to ACM TKDD"], ["v2", "Fri, 30 Jun 2017 18:33:48 GMT  (4693kb)", "http://arxiv.org/abs/1705.04804v2", "correct several typo and format issues"]], "COMMENTS": "submitted to ACM TKDD", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuchu han", "hao huang", "hong qin"], "accepted": false, "id": "1705.04804"}, "pdf": {"name": "1705.04804.pdf", "metadata": {"source": "CRF", "title": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph", "authors": ["Shuchu Han"], "emails": ["shuchu.han@gmail.com", "haohuanghw@gmail.com", "qin@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n04 80\n4v 2\n[ cs\n.L G\n] 3\n0 Ju\nn 20\n17\nAutomatically Redundant Features Removal for\nUnsupervised Feature Selection via Sparse\nFeature Graph\nShuchu Han\nshuchu.han@gmail.com\nDepartment of Computer Science,\nStony Brook University,\nStony Brook, NY 11794, United States\nHao Huang\nhaohuanghw@gmail.com\nMachine Learning Laboratory,\nGeneral Electric Global Research,\nSan Ramon, CA 94853, United States\nHong Qin\nqin@cs.stonybrook.edu\nDepartment of Computer Science,\nStony Brook University,\nStony Brook, NY 11794, United States\nJuly 4, 2017\nFor unsupervised feature selection algorithms, the structure of data is used to generate indication vectors for selecting informative features. The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc. To model the structure of data, methods like Gaussian similarity graph, or k-nearest neighbor similarity graph are very popular in machine learning research. All these similarity graphs are built based on the pairwise distance like Euclidean distance (L2 norm) or Manhattan distance (L1 norm) defined between two data samples (vectors). As we can see, the pairwise distance is crucial to the quality of indication vectors, and the success of unsupervised feature selection depends on the accuracy of these indication vectors.\nWhen the dimensional size of data becomes high, or say, for high dimensional datasets, we will meet the curse of high dimensionality issue [2]. That means the differentiating ability of pairwise distance will degraded rapidly when the dimension of data goes higher, and the nearest neighbor indexing will give\ninaccurate results [23] [1]. As a result, the description of data structure by using similarity graphs will be not precise and even wrong. This create an embarrassing chicken-and-egg problem [5] for unsupervised feature selection algorithms: \u201cthe success of feature selection depends on the quality of indication vectors which are related to the structure of data. But the purpose of feature selection is to giving more accurate data structure.\u201d\nMost existing unsupervised feature selection algorithms use all original features [5] to build the similarity graph. As a result, the obtained data structure information will not as accurate as the intrinsic one it should be. To remedy this problem, dimensionality reduction techniques are required. For example, Principal Component Analysis (PCA) and Random Projection (RP) are popular methods in machine learning research. However, most of them will project the data matrix into another (lower dimensional) space with the constraint to approximate the original pairwise similarities. As a result, we lose the physical meaning or original features and the meaning of projected features are unknown.\nIn this study, we proposed a graph-based approach to reduce the data dimension by removing redundant features. Without lose of generality, we categorize features into three groups [4]: relevant feature,irrelevant feature and redundant feature. A feature fi is relevant or irrelevant based on it\u2019s correlation with indication vectors (or target vectors named in other articles) Y = {yi, i \u2208 [1, k]}. For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels. For unsupervised scenario [6] [3], as we mentioned early, they follow the structure of data. Redundant features are features that highly correlated to other features, and have no contribution or trivial contribution to the target learning task. The formal definition of redundant feature is by [26] based on the Markov blanket given by [10].\nBased on the philosophy of sparse learning based MCFS algorithm, a feature could be redundant to another single feature, or to a subset of features. In this work, we propose a graph based approach to identify these two kind of redundancy at the same time. The first step is to build a Sparse Feature Graph (SFG) at feature side based on sparse representation concept from subspace clustering theory [7]. Secondly, we review the quality of sparse representation of each single feature vector and filtered out those failed ones. In the last, we defined Local Compressible Subgraphs (LCS) to represent those local feature groups that are very redundant. Moreover, a greedy local search algorithm is designed to discover all those LCSs. Once we have all LCSs, we pick the feature which has the highest node in-degree as the representative feature and treat all other as redundant features. With this approach, we obtain a new data matrix with reduced size and alleviate the curse of dimensional issues.\nTo be specific, the contribution of our study can be highlighted as:\n\u2022 We propose sparse feature graph to model the feature redundancy existing in high dimensional datasets. The sparse feature graph inherits the philosophy of sparse learning based unsupervised feature selection framework. The sparse feature graph not only records the redundancy between two features but also show the redundancy between one feature and a subset\nof features.\n\u2022 We propose local compressible subgraph to represent redundant feature groups. And also design a local greedy search algorithm to find all those subgraphs.\n\u2022 We reduce the dimensionality of input data and alleviate the curse of dimensional issue through redundant features removal. With a more accurate data structure, the chicken-and-egg problem for unsupervised feature selection algorithms are remedied in certain level. One elegant part of our proposed approach is to reduce the data dimension without any pairwise distance calculation.\n\u2022 Abundant experiments and analysis over twelve high dimensional datasets from three different domains are also presented in this study. The experiment results show that our method can obtain better data structure with reduced size of dimensionality, and proof the effectiveness of our proposed approach.\nThe rest of paper is organized as follows. The first section describe the math notation used in our work. The Section 2 introduces the background , motivation and preliminaries of our problem. In Section 3, we define the problem we are going to solve. In Section 4, we present our proposed sparse feature graph algorithm and discuss the sparse representation error problem. We also introduce the local compressible subgraph and related algorithm. The experiment results are reported in Section 5, and a briefly reviewing of related works is given in Section 6. Finally, we conclude our study in last Section 7."}, {"heading": "1 Math Notation", "text": "Throughout this paper, matrices are written as boldface capital letters and vectors are represented as boldface lowercase letters. Let the data matrix be represented as X \u2208 Rn\u00d7d, while each row is a sample (or instance), and each column means a feature. If we view the data matrix X = [x1, x2, \u00b7 \u00b7 \u00b7, xn] T ,xi \u2208 Rd\u00d71 from feature side, it can be seen as F = XT = [f1, f2, \u00b7 \u00b7 \u00b7, fd],fi \u2208 Rn\u00d71(1 \u2264 i \u2264 d)."}, {"heading": "2 Background and Preliminaries", "text": ""}, {"heading": "2.1 Unsupervised Feature Selection", "text": "In unsupervised feature selection framework, we don\u2019t have label information to determine the feature relevance. Instead, the data similarity or manifold structure constructed from the whole feature space are used as criteria to select features. Among all those algorithms of unsupervised feature selection, the most famous one is MCFS. The MCFS algorithm is a sparse learning based\nunsupervised feature selection method which can be illustrated as figure 1. The core idea of MCFS is to use the eigenvectors of graph Lapalcian over similarity graph as indication vectors. And then find set of features that can approximate these eigenvectors through sparse linear regression. Let us assume the input data has number K clusters that is known beforehand (or an estimated K value by the expert\u2019s domain knowledge). The top K non-trivial eigenvectors, Y = [y1, \u00b7 \u00b7 \u00b7 ,yk], form the spectral embedding Y of the data. Each row of Y is the new coordinate in the embedding space. To select the relevant features, MCFS solves K sparse linear regression problems between F and Y as:\nmin \u03b1i\n\u2016yi \u2212 F\u03b1i\u2016 2 + \u03b2\u2016\u03b1i\u20161, (1)\nwhere \u03b1i is a n-dimensional vector and it contains the combination coefficients for different features fi in approximating yi. Once all coefficients \u03b1i are collected, features will be ranked by the absolute value of these coefficients and top features are selected. This can be show by a weighted directed bipartite graph as following:"}, {"heading": "2.2 Adaptive Structure Learning for High Dimensional Data", "text": "As we can seen, the MCFS uses whole features to model the structure of data. That means the similarity graph such as Gaussian similarity graph is built from\nall features. This is problematic when the dimension of data vector goes higher. To be specific, the pairwise distance between any two data vectors becomes almost the same, and as a consequence of that, the obtained structural information of data is not accuracy. This observation is the motivation of unsupervised Feature Selection with Adaptive Structure Learning (FSASL) algorithm which is proposed by Du et al. [5]. The idea of FSASL is to repeat MCFS iteratively with updating selected feature sets. It can be illustrated as following: FASAL is an iterative algorithms which keeps pruning irrelevant and noisy features to obtain better manifold structure while improved structural info can help to search better relevant features. FASAL shows better performance in normalized mutual information and accuracy than MCFS generally. However, it\u2019s very time consuming since it is an iterative algorithm includes many eigen-decompositions."}, {"heading": "2.3 Redundant Features", "text": "For high dimensional data X \u2208 Rn\u00d7d, it exists information redundancy among features since d \u226a n. Those redundant features can not provide further performance improvement for ongoing learning task. Instead, they impair the efficiency of learning algorithm to find intrinsic data structure.\nIn this section, we describe our definition of feature redundancy. Unlike the feature redundancy defined bt Markov blanket [26] which is popular in existing research works, our definition of feature redundancy is based on the linear correlation between two vectors (the \u201cvector\u201d we used here could be a feature vector or a linear combination of several feature vectors.) To measure the redundancy between two vectors fi and fj , squared cosine similarity[22] is used:\nRij = cos 2(fi,fj). (2)\nBy the math definition of cosine similarity, it is straightforward to know that a higher value of Ri,j means high redundancy existing between fi and fj . For example, feature vector fi and its duplication fi will have Rii value equals to one. And two orthogonal feature vectors will have redundancy value equals to zero."}, {"heading": "3 Problem Statement", "text": "In this work, our goal is to detect those redundant features existing in high dimensional data and obtain a more accurate intrinsic data structure. To be specific:\nProblem 1 Given a high dimensional data represented in the form of feature matrix X, how to remove those redundant features f(\u00b7) \u2208 X\nT for unsupervised feature selection algorithms such as MCFS?\nTechnically, the MCFS algorithm does not involve redundant features. However, the performance of MCFS depends on the quality of indication vectors which are used to select features via sparse learning. And those indication vectors are highly related to the intrinsic structure of data which is described by the selected features and given distance metric. For example, the MCFS algorithm uses all features and Gaussian similarity to represent the intrinsic structure. This is the discussed \u2018chicken-and-egg\u201d problem [5] between structure characterization and feature selection. The redundant and noise features will lead to an inaccurate estimation of data structure. As a result, it\u2019s very demanding to remove those redundant (and noise) features before the calculation of indication vectors."}, {"heading": "4 Algorithm", "text": "In this section, we present our graph-based algorithm to detect and remove redundant features existing in high dimensional data. First, the sparse feature graph that modeling the redundancy among feature vectors will be introduced. Secondly, the sparse representation error will be discussed. In the last, the local compressible subgraph is proposed to extract redundant feature groups."}, {"heading": "4.1 Sparse Feature Graph (SFG)", "text": "The most popular way to model the redundancy among feature vectors is correlation such as Pearson Correlation Coefficient (PCC). The correlation value is defined over two feature vectors, and it\u2019s a pairwise measurement. However, there also exiting redundancy between one feature vector and a set of feature vectors according to the philosophy of MCFS algorithm. In this section, we present SFG, which model the redundancy not only between two feature vectors but also one feature vector and a set of feature vectors.\nThe basic idea of sparse feature graph is to looking for a sparse linear representation for each feature vector while using all other feature vectors as dictionary. For each feature vector fi in features set F = [f1, f2, \u00b7 \u00b7 \u00b7, fd], SFG solves the following optimization problem:\nmin \u03b1\u2208Rd\u22121 \u2016fi \u2212\u03a6 i\u03b1i\u2016 2 2, s.t. \u2016\u03b1i\u20160 < L, (3)\nwhere \u03a6i = [f1, f2, \u00b7 \u00b7 \u00b7, fi\u22121, fi+1, \u00b7 \u00b7 \u00b7, fd] is the dictionary of fi and each column of \u03a6i is a selected feature from data matrix X. L is a constraint to limit the number of nonzero coefficients. In SFG, we set it to the number of features d. The \u03b1i is the coefficient of each atom of dictionary \u03a6\ni. This coefficient vector not only decides the edge link to fi but also indicates the weight of that connection. The resulted SFG is a weighted directed graph and may have multiple components.\nTo solve the optimization problem 3, we use Orthogonal Matching Pursuit (OMP) solver [25] here since the number of features in our datasets is larger than 1,000. We modify the stop criterion of OMP by checking the value change of residual instead of residual itself or the maximum number of supports. The reason is that we want the number of supports (or say, the number of edge connections) to follow the raw data property. Real world datasets are always noisy and messy. It\u2019s highly possible that several feature vectors may fail to find a correct sparse linear representation through OMP. If we set residual or maximum of supports as criteria, we can not differentiate the successful representations and the failed ones.\nThe OMP solver and SFG algorithm can be described as following."}, {"heading": "4.2 Sparse Representation Error", "text": "In our modified OMP algorithm 1, we set a new stop criterion of searching sparse representation solution for each feature vector fi. Instead of keep searching until arriving a minimization error, we stop running while the solver could not reduce the length of residual vector anymore. To be specific, the 2-norm of residual vector is monitored and the solver will stop once the change of this value small than a user specified threshold.\nAlgorithm 1: Orthogonal Matching Pursuit (OMP)\nInput : \u03a6 = [f1, f2, \u00b7 \u00b7 \u00b7, fi\u22121, fi+1, \u00b7 \u00b7 \u00b7, fd] \u2208 Rn\u00d7(d\u22121),fi \u2208 Rn, \u01eb. Output: Coefficient \u03b1i. Initialize residual difference threshold r0 = 1.0, residual q0 = fi, support set \u03930 = \u2205, k = 1 ; while k \u2264 d\u2212 1 and |rk \u2212 rk\u22121| > \u01eb do Search the atom which most reduces the objective:\nj\u2217 = argmin j\u2208\u0393C\n{\nmin \u03b1\n\u2016fi \u2212\u03a6\u0393\u222a{j}\u03b1\u2016 2 2\n}\n;\nUpdate the active set: \u0393k = \u0393k\u22121 \u222a {j\u2217}; Update the residual (orthogonal projection): qk = (I \u2212\u03a6\u0393k(\u03a6 T \u0393k \u03a6\u0393k)\n\u22121\u03a6T\u0393k)fi; Update the coefficients: \u03b1\u0393k = (\u03a6 T \u0393k \u03a6\u0393k) \u22121\u03a6T\u0393kfi; rk = \u2016qk\u201622; k \u2190 k + 1;\nend\nAlgorithm 2: Sparse Feature Graph\nInput : Data matrix F = [f1, f2, \u00b7 \u00b7 \u00b7, fd] \u2208 Rn\u00d7d; Output: Adjacent matrix W of Graph G \u2208 Rd\u00d7d;\nNormalize each feature vector fi with \u2016fi\u2016 2 2 = 1; for i = 1, \u00b7 \u00b7 \u00b7 , d do Compute \u03b1i from OMP(F\u2212i,fi) using algorithm 1; end Set adjacent matrix Wij = \u03b1i(j) if i > j, Wij = \u03b1i(j \u2212 1), if i < j and Wij = 0 if i == j;\nThe reason we use this new stop criterion is that several feature vectors may not find correct sparse representation in current dataset, and the ordinary OMP solver will return a meaningless sparse representation when the maximum iteration threshold arrived. Since the goal of SFG is not to find a correct sparse representation for every feature vectors, we utilize the new stop criterion and add a filter process in our algorithm to identify those failed sparse representation.\nTo identify those failed sparse representation, we check the angle between the original vector and the linear combination of its sparse representation. In the language of SFG, we check the angle between a node (a feature vector) and the weighted combination of its one-ring neighbor. Only the neighbors of out edges will be considered. This can be illustrated by following figure 5. As the example in Figure 5, node fi has seven one-ring neighbors. But only bmf1, bmf2,f3,f5,f6 are its sparse representation and f4 and f7 are not. Then the sparse representation error \u03b6 is calculated by:\nf\u2217i = w1f1 + w2f2 + w3f3 + w5f5 + w6f6,\n\u03b6 = arccos(fi,f \u2217 i ).\nOnce we have the SFG, we calculate the sparse representation errors for all nodes. A sparse representation is treated as fail if the angle \u03b6 less than a user specified value. We will filter out these node which has failed representation by removing its out-edges."}, {"heading": "4.3 Local Compressible Subgraph", "text": "We group high correlated features through local compressible subgraphs. The SFG G is a weighted directed graph. With this graph, we need to find all feature subsets that has very high redundancy. To archive this goal, we propose a local search algorithm with seed nodes to group those highly correlated features into many subgraphs which are named as local compressible subgraphs in this study. Our local search algorithm involves two steps, the first step is to sort all nodes by the in-degree. By the definition of SFG, the node with higher in-degree means it appears more frequently in other nodes\u2019 sparse representation. The\nsecond step is a local bread-first search approach which finds all nodes that has higher weight connections (in and out) to the growing subgraph. The detail subgraph searching algorithm can be described by: In Alg. 3, function label(n)\nAlgorithm 3: Local Compressible Subgraphs.\nInput : Weighted directed graph G = (V,E), edge weight threshold \u03b8; Output: Local compressible subgraphs C .\nTag all nodes with initial label 0; Sort the nodes by its in-degree decreasingly; current label = 1; for n = 1 : |V | do\nif label(n) ! = 0 then continue; end set label of node n to current label; BFS(n, \u03b8, current label); current label + = 1;\nend /* current label now has the maximum value of labels. */ for i = 1 : current label do Extract subgraph ci which all nodes have label i; if |ci| > 1 then\nadd ci to C; end\nend\ncheck the current label of node n, and BFS(n, \u03b8, current label) function runs a local Breadth-First search for subgraph that has edge weight large than \u03b8."}, {"heading": "4.4 Redundant Feature Removal", "text": "The last step of our algorithm is to remove the redundant features. For each local compressible subgraph we found, we pick up the node which has the highest in-degree as the representative node of that local compressible subgraph. So the number of final feature vectors equals to the number of local compressible subgraphs."}, {"heading": "5 Experiments", "text": "In this section, we present experimental results to demonstrate the effectiveness of our proposed algorithm. We first evaluate the spectral clustering performance before and after applying our algorithms. Secondly, we show the performance of MCFS with or without our algorithm. In the last, the properties of generated sparse graphs and sensitivity of parameters are discussed."}, {"heading": "5.1 Experiment Setup", "text": "Datasets. We select twelve real-world high dimensional datasets [11] from three different domains: Image, Text and Biomedical. The detail of each dataset is listed in Table 1. The datasets have sample size different from 96 to 8293 and feature size ranging from 1,024 to 18,933. Also, the datasets have class labels from 2 to 64. The purpose of this selection is to let the evaluation results be more general by applying datasets with various characteristics.\nNormalization. The features of each dataset are normalized to have unit length, which means \u2016fi\u20162 = 1 for all datasets.\nEvaluation Metric. Our proposed algorithm is under the framework of unsupervised learning. Without loss of generality, the cluster structure of data is used for evaluation. To be specific, we measure the spectral clustering performance with Normalized Mutual Information (NMI) and Accuracy (ACC). NMI value ranges from 0.0 to 1.0, with higher value means better clustering performance. ACC is another metric to evaluate the clustering performance by measuring the fraction of its clustering result that are correct. Similar to NMI, its values range from 0 to 1 and higher value indicates better algorithm performance.\nSuppose A is the clustering result and B is the known sample label vector. Let p(a) and p(b) denote the marginal probability mass function of A and B, and let p(a, b) be the joint probability mass function of A and B. Suppose H(A), H(B) and H(A,B) denote the entropy of p(a), p(b) and p(a, b) respectively. Then the normalized mutual information NMI is defined as:\nNMI(A,B) = H(A) +H(B)\u2212H(A,B)\nmax(H(A), H(B)) (4)\nAssume A is the clustering result label vector, and B is the known ground truth label vector, ACC is defined as:\nACC =\nN \u2211\ni=1\n\u03b4(B(i),Map(A,B)(i))\nN (5)\nwhere N denotes the length of label vector, \u03b4(a, b) equals to 1 if only if a and b are equal. MapA,B is the best mapping function that permutes A to match B."}, {"heading": "5.2 Effectiveness of Redundant Features Removal", "text": "Our proposed algorithm removes many features to reduce the dimension size of all data vectors. As a consequence, the pairwise Euclidean distance is changed and the cluster structure will be affected. To measure the effectiveness of our proposed algorithm, we check the spectral clustering performance before and after redundant feature removal. If the NMI and ACC values are not changed to much and stay in the same level, the experiment results show that our proposed algorithm is correct and effective.\nThe spectral clustering algorithm we used in our experiments is the NgJordan-Weiss (NJW) algorithm [16]. The Gaussian similarity graph is applied here as the input and parameter \u03c3 is set to the mean value of pairwise Euclidean distance among all vectors.\nOur proposed LCS algorithm includes a parameter \u03b8 which is the threshold of redundancy. It decides the number of redundant features implicitly, and affects the cluster structure of data consequently. In our experiment design, we test different \u03b8 values ranging from 90% to 10% with step size equal to 10%: \u03b8 = [0.9, 0.8, 0.7, \u00b7 \u00b7 \u00b7 , 0.1].\nWe present our experiment results for image datasets, text datasets, and biological datasets in Figure 6, Figure 7 and Figure 8 respectively. For each dataset, we show the NMI, ACC performance with different \u03b8 and comparing with original spectral clustering performance by using all features. From the experimental results, we can read that: Even when \u03b8 is reduced to 30%, the NMI and ACC values are staying in same level as original data. When \u03b8 equals to 30%, it means the edges of SFG that with weights (absolute value) in the highest 70% value range are removed. (It does not mean that 70% of top weights edges are removed). This observation validate the correctness of our proposed algorithm."}, {"heading": "5.3 Performance of MCFS", "text": "Our proposed algorithm is targeting for unsupervised feature selection. And the quality of indication vectors (or the spectral clustering performance based on eigenvectors) is an important factor evaluate the effectiveness of our proposed algorithm. In this section, we evaluate the MCFS performance over the redundant feature removed data, and comparing with the raw data that without any feature removal.\nThe spectral clustering performance is measured for different input data from original whole feature data to processed ones by our proposed algorithm with different \u03b8. We report the experiment results over image datasets and biological datasets in this section. For text datasets, the feature vectors of them are very sparse, and our eigen decomposition process are always failed and we only can collect partial results. For fair evaluation, we omit the experiment results of text datasets in this work. The result of MCFS performance shows from Table 2 to Table 17.\nFor each dataset, we set the number of selected features ranging from [5, 10, 15, \u00b7 \u00b7 \u00b7 , 60], which has 11 different sizes in total. The parameter \u03b8 is configured from 0.9 to 0.1 with stepsize equals to 0.1.\nWe report the experimental results in tables (from Table 2 to Table 17). For each table, the first row means the number of features that used as input of MCFS. The first column is the number of selected features by MCFS algorithm. The baseline is in the second column, which is the testing result of MCFS algorithm with raw data. The hyphens in the tables means the number of selected features is larger than the feature size of input data, which means invalid test. To show the effectiveness of our algorithm, we also mark those NMI and ACC scores that larger or equals to baseline in bold text."}, {"heading": "5.4 Sparse Representation Errors", "text": "With the design of our modified OMP solvers, there will be failed/wrong sparse representations existing in generated sparse feature graph. The meaning of these edge connections and edge weights are invalid. And they should be re-\nmoved from the SFG since wrong connections will deteriorate the accuracy of feature redundancy relationship. To validate the sparse representation, we check the angle between original feature vector and the linear weighted summation resulted vector (or recover signal from sparse coding point of view) from its sparse representation. If the angle lower than a threshold, we remove all out-edges from the generated sparse feature graph. To specify the threshold, we learn it from the empirical results of our selected twelve datasets. The distribution (or histogram) result of angle values is presented in figure 9."}, {"heading": "6 Related Works", "text": "Remove redundant features is an important step for feature selection algorithms. Prestigious works include [26] which gives a formal definition of redundant features. Peng et al. [17] propose a greedy algorithm (named as mRMR) to select features with minimum redundancy and maximum dependency. Zhao et al. [27] develop an efficient spectral feature selection algorithm to minimize the\nredundancy within the selected feature subset through L2,1 norm. Recently, researchers pay attention to unsupervised feature selection with global minimized redundancy [22] [21]. Several graph based approaches are proposed in [15], [19]. The most closed research work to us is [13] which build a sparse graph at feature side and ranking features by approximation errors."}, {"heading": "7 Conclusion", "text": "In this study, we propose sparse feature graph to model both one-to-one feature redundancy and one-to-many features redundancy. By separate whole features into different redundancy feature group through local compressible subgraphs, we reduce the dimensionality of data by only select one representative feature from each group. One advantage of our algorithm is that it does not need to calculate the pairwise distance which is always not accurate for high dimensional datasets. The experiment results shows that our algorithm is an effective way to obtain accurate data structure information which is demanding for unsupervised feature selection algorithms."}], "references": [{"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["Charu C Aggarwal", "Alexander Hinneburg", "Daniel A Keim"], "venue": "In International Conference on Database Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "When is nearest neighbor meaningful", "author": ["Kevin Beyer", "Jonathan Goldstein", "Raghu Ramakrishnan", "Uri Shaft"], "venue": "In International conference on database theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["Deng Cai", "Chiyuan Zhang", "Xiaofei He"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Feature selection for classification", "author": ["Manoranjan Dash", "Huan Liu"], "venue": "Intelligent data analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Unsupervised feature selection with adaptive structure learning", "author": ["Liang Du", "Yi-Dong Shen"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Feature selection for unsupervised learning", "author": ["Jennifer G Dy", "Carla E Brodley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["Ehsan Elhamifar", "Rene Vidal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A variance minimization criterion to feature selection using laplacian regularization", "author": ["Xiaofei He", "Ming Ji", "Chiyuan Zhang", "Hujun Bao"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2025}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["Chenping Hou", "Feiping Nie", "Xuelong Li", "Dongyun Yi", "Yi Wu"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Toward optimal feature selection", "author": ["D KOLLER"], "venue": "In Proc. 13th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Feature selection: A data perspective", "author": ["J. Li", "K. Cheng", "S. Wang", "F. Morstatter", "R. Trevino", "J. Tang", "H. Liu"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Zechao Li", "Yi Yang", "Jing Liu", "Xiaofang Zhou", "Hanqing Lu"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Sparsity score: A new filter feature selection method based on graph", "author": ["Mingxia Liu", "Dan Sun", "Daoqiang Zhang"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Global and local structure preservation for feature selection", "author": ["Xinwang Liu", "Lei Wang", "Jian Zhang", "Jianping Yin", "Huan Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Supervised feature selection in graphs with path coding penalties and network flows", "author": ["Julien Mairal", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A fast clustering-based feature subset selection algorithm for high-dimensional data", "author": ["Qinbao Song", "Jingjie Ni", "Guangtao Wang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Feature selection via global redundancy minimization", "author": ["De Wang", "Feiping Nie", "Heng Huang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Feature selection with integrated relevance and redundancy optimization", "author": ["Xuerui Wang", "Andrew McCallum", "Xing Wei"], "venue": "In Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "In VLDB,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "l2, 1norm regularized discriminative feature selection for unsupervised learning", "author": ["Yi Yang", "Heng Tao Shen", "Zhigang Ma", "Zi Huang", "Xiaofang Zhou"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Scalable sparse subspace clustering by orthogonal matching pursuit", "author": ["Chong You", "Daniel Robinson", "Ren\u00e9 Vidal"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Efficient feature selection via analysis of relevance and redundancy", "author": ["Lei Yu", "Huan Liu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Efficient spectral feature selection with minimum redundancy", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "On similarity preserving feature selection", "author": ["Zheng Zhao", "Lei Wang", "Huan Liu", "Jieping Ye"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 13, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "The structure of data could be local manifold structure [8] [9], global structure [14] [28], discriminative information [24] [12] and etc.", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "When the dimensional size of data becomes high, or say, for high dimensional datasets, we will meet the curse of high dimensionality issue [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 22, "context": "inaccurate results [23] [1].", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "inaccurate results [23] [1].", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "This create an embarrassing chicken-and-egg problem [5] for unsupervised feature selection algorithms: \u201cthe success of feature selection depends on the quality of indication vectors which are related to the structure of data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "\u201d Most existing unsupervised feature selection algorithms use all original features [5] to build the similarity graph.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Without lose of generality, we categorize features into three groups [4]: relevant feature,irrelevant feature and redundant feature.", "startOffset": 69, "endOffset": 72}, {"referenceID": 17, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "For supervised feature selection algorithms [18] [20] [17], these indication vectors usually relate to class labels.", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "For unsupervised scenario [6] [3], as we mentioned early, they follow the structure of data.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "For unsupervised scenario [6] [3], as we mentioned early, they follow the structure of data.", "startOffset": 30, "endOffset": 33}, {"referenceID": 25, "context": "The formal definition of redundant feature is by [26] based on the Markov blanket given by [10].", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "The formal definition of redundant feature is by [26] based on the Markov blanket given by [10].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "The first step is to build a Sparse Feature Graph (SFG) at feature side based on sparse representation concept from subspace clustering theory [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Unlike the feature redundancy defined bt Markov blanket [26] which is popular in existing research works, our definition of feature redundancy is based on the linear correlation between two vectors (the \u201cvector\u201d we used here could be a feature vector or a linear combination of several feature vectors.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": ") To measure the redundancy between two vectors fi and fj , squared cosine similarity[22] is used: Rij = cos (fi,fj).", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "This is the discussed \u2018chicken-and-egg\u201d problem [5] between structure characterization and feature selection.", "startOffset": 48, "endOffset": 51}, {"referenceID": 24, "context": "To solve the optimization problem 3, we use Orthogonal Matching Pursuit (OMP) solver [25] here since the number of features in our datasets is larger than 1,000.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "We select twelve real-world high dimensional datasets [11] from three different domains: Image, Text and Biomedical.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "The spectral clustering algorithm we used in our experiments is the NgJordan-Weiss (NJW) algorithm [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "Prestigious works include [26] which gives a formal definition of redundant features.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "[17] propose a greedy algorithm (named as mRMR) to select features with minimum redundancy and maximum dependency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] develop an efficient spectral feature selection algorithm to minimize the", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Recently, researchers pay attention to unsupervised feature selection with global minimized redundancy [22] [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Recently, researchers pay attention to unsupervised feature selection with global minimized redundancy [22] [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Several graph based approaches are proposed in [15], [19].", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "Several graph based approaches are proposed in [15], [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "The most closed research work to us is [13] which build a sparse graph at feature side and ranking features by approximation errors.", "startOffset": 39, "endOffset": 43}], "year": 2017, "abstractText": "The redundant features existing in high dimensional datasets always affect the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph based approach to find and remove those redundant features automatically for high dimensional data. Based on sparse learning based unsupervised feature selection framework, Sparse Feature Graph (SFG) is introduced not only to model the redundancy between two features, but also to disclose the group redundancy between two groups of features. With SFG, we can divide the whole features into different groups, and improve the intrinsic structure of data by removing detected redundant features. With accurate data structure, quality indicator vectors can be obtained to improve the learning performance of existing unsupervised feature selection algorithms such as multi-cluster feature selection (MCFS). Our experimental results on benchmark datasets show that the proposed SFG and feature redundancy remove algorithm can improve the performance of unsupervised feature selection algorithms consistently.", "creator": "LaTeX with hyperref package"}}}