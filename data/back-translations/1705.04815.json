{"id": "1705.04815", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Learning Semantic Correspondences in Technical Documentation", "abstract": "We consider the problem of translating high-level text descriptions into formal representations in technical documentation as part of the effort to model the meaning of such documentation. We focus specifically on the problem of learning translation correspondences between text descriptions and grounded representations in target documentation, such as the formal representation of functions or code templates. Our approach uses the parallelism of such documentation or the close coupling between high-level text descriptions and the low-level representations we want to learn. Data is collected by dismantling technical documents for such parallel text-representation pairs, which we use to train a simple semantic analysis model. We report new basic results for sixteen novel data sets, including standard documentation for nine popular programming languages in seven natural languages and a small collection of Unix user manuals.", "histories": [["v1", "Sat, 13 May 2017 12:29:39 GMT  (97kb,D)", "http://arxiv.org/abs/1705.04815v1", "accepted to ACL-2017"]], "COMMENTS": "accepted to ACL-2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kyle richardson", "jonas kuhn"], "accepted": true, "id": "1705.04815"}, "pdf": {"name": "1705.04815.pdf", "metadata": {"source": "CRF", "title": "Learning Semantic Correspondences in Technical Documentation", "authors": ["Kyle Richardson"], "emails": ["kyle@ims.uni-stuttgart.de", "jonas@ims.uni-stuttgart.de", "@param", "@param", "@return", "@see", "@param", "@see"], "sections": [{"heading": "1 Introduction", "text": "Technical documentation in the computer domain, such as source code documentation and other howto manuals, provide high-level descriptions of how lower-level computer programs and utilities work. Often these descriptions are coupled with formal representations of these lower-level features, expressed in the target programming languages. For example, Figure 1.1 shows the source code documentation (in red/bold) for the max function in the Java programming language paired with the representation of this function in the underlying Java language (in black). This formal representation captures the name of the function, the return\nvalue, the types of arguments the function takes, among other details related to the function\u2019s place and visibility in the overall source code collection or API.\nGiven the high-level nature of the textual annotations, modeling the meaning of any given description is not an easy task, as it involves much more information than what is directly provided in the associated documentation. For example, capturing the meaning of the description the greater of might require having a background theory about quantity/numbers and relations between different quantities. A first step towards capturing the meaning, however, is learning to translate this description to symbols in the target representation, in this case to the max symbol. By doing this translation to a formal language, modeling and learning the subsequent semantics becomes easier since we are eliminating the ambiguity of ordinary lanar X\niv :1\n70 5.\n04 81\n5v 1\n[ cs\n.C L\n] 1\n3 M\nay 2\n01 7\nguage. Similarly, we would want to first translate the description two long values, which specifies the number and type of argument taken by this function, to the sequence long a,long b.\nBy focusing on translation, we can create new datasets by mining these types of source code collections for sets of parallel text-representation pairs. Given the wide variety of available programming languages, many such datasets can be constructed, each offering new challenges related to differences in the formal representations used by different programming languages. Figure 1.2 shows example documentation for the Clojure programming language, which is part of the Lisp family of languages. In this case, the description Returns random probability of should be translated to the function name random-sample since it describes what the overall function does. Similarly, the argument descriptions from coll and of prob should translate to coll and prob.\nGiven the large community of programmers around the world, many source code collections are available in languages other than English. Figure 1.3 shows an example entry from the French version of the PHP standard library, which was translated by volunteer developers. Having multilingual data raises new challenges, and broadens the scope of investigations into this type of semantic translation.\nOther types of technical documentation, such as utility manuals, exhibit similar features. Figure 2 shows an example manual in the domain of Unix utilities. The textual description in red/bold describes an example use of the dappprof utility paired with formal representations in the form of executable code. As with the previous exam-\nples, such formal representations do not capture the full meaning of the different descriptions, but serve as a convenient operationalization, or translational semantics, of the meaning in Unix. Print elapsed time, for example, roughly describes what the dappprof utility does, whereas PID 1871 describes the second half of the code sequence.\nIn both types of technical documentation, information is not limited to raw pairs of descriptions and representations, but can include other information and clues that are useful for learning. Java function annotations include textual descriptions of individual arguments and return values (shown in green). Taxonomic information and pointers to related functions or utilities are also annotated (e.g., the @see section in Figure 1, or SEE ALSO section in Figure 2). Structural information about code sequences, and the types of abstract arguments these sequences take, are described in the SYNOPSIS section of the Unix manual. This last piece of information allows us to generate abstract code templates, and generalize individual arguments. For example, the raw argument 1871 in the sequence dappprof -p 1871 can be typed as a PID instance, and an argument of the -p flag.\nGiven this type of data, a natural experiment is to see whether we can build programs that translate high-level textual descriptions to correct formal representations. We aim to learn these translations using raw text-meaning pairs as the sole supervision. Our focus is on learning function translations or representations within nine programming language APIs, each varying in size, representation style, and source natural language. To our knowledge, our work is the first to look at translating source code descriptions to formal representations using such a wide variety of programming and natural languages. In total, we introduce fourteen new datasets in the source code domain that include seven natural languages, and report new results for an existing dataset. As well, we look at learning simple code templates using a small collection of English Unix manuals.\nThe main goal of this paper is to establish strong baselines results on these resources, which we hope can be used for benchmarking and developing new semantic parsing methods. We achieved initial baselines using the language modeling and translation approach of Deng and Chrupa\u0142a (2014). We also show that modest improvements can be achieved by using a more conventional\ndiscriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets."}, {"heading": "2 Related Work", "text": "Our work is situated within research on semantic parsing, which focuses on the problem of generating formal meaning representations from text for natural language understanding applications. Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a).\nWhile generating representations for natural language understanding is a complex task, most studies focus on the translation or generation problem independently of other semantic or knowledge representation issues. Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). These methods are meant to be applicable to a wide range of translation problems and representation types, which make new parallel datasets or resources useful for furthering the research.\nIn general, however, such datasets are hard to construct since building them requires considerable domain knowledge and knowledge of logic. Alternatively, we construct parallel datasets automatically from technical documentation, which obviates the need for annotation. While the formal representations are not actual logical forms, they still provide a good test case for testing how well semantic parsers learn translations to representations.\nTo date, most benchmark datasets are limited to small controlled domains, such as geography and navigation. While attempts have been made to do open-domain semantic parsing using larger, more complex datasets (Berant et al., 2013; Pasupat and Liang, 2015), such resources are still scarce. In Figure 3, we compare the details of one widely used dataset, Geoquery (Zelle and Mooney, 1996), to our new datasets. Our new resources are on average much larger than geoquery in terms of the number of example pairs, and the size of the different language vocabularies. Most existing datasets are also primarily English-based, while we focus\non learning in a multilingual setting using several new moderately sized datasets.\nWithin semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016). This has sometimes involved learning from automatically generated parallel data and representations (Chen and Mooney, 2008) of the type we consider in this paper. Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al., 2015), which ultimately aim to solve the problem of natural language programming. We view our work as one small step in this general direction.\nOur work is also related to software components retrieval and builds on the approach of Deng and Chrupa\u0142a (2014). Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al., 2015). As part of this effort, recent work in machine learning has focused on the similar problem of learning code representations using resources such as StackOverflow and Github. These studies primarily focus on learning longer programs (Allamanis et al., 2015) as opposed to function representations, or focus narrowly on a single programming language such as Java (Gu et al., 2016) or on related tasks such as text generation (Iyer et al., 2016; Oda et al., 2015). To our knowledge, none of this work has been applied to languages other than English or such a wide variety of programming languages."}, {"heading": "3 Mapping Text to Representations", "text": "In this section, we formulate the basic problem of translating to representations in technical documentation."}, {"heading": "3.1 Problem Description", "text": "We use the term technical documentation to refer to two types of resources: textual descriptions inside of source code collections, and computer utility manuals. In this paper, the first type includes high-level descriptions of functions in standard library source code documentation. The second type includes a collection of Unix manuals, also known as man pages. Both types include pairs of text and code representations.\nWe will refer to the target representations in these resources as API components, or components. In source code, components are formal representations of functions, or function signatures (Deng and Chrupa\u0142a, 2014). The form of a function signature varies depending on the resource, but in general gives a specification of how a function is named and structured. The example function signatures in Figure 3 all specify a function name, a list of arguments, and other optional information such as a return value and a namespace. Components in utility manuals are short executable code sequences intended to show an example use of a utility. We assume typed code sequences following Richardson and Kuhn (2014), where the constituent parts of the sequences are abstracted by type.\nGiven a set of example text-component pairs, D = {(xi, zi)}ni=1, the goal is to learn how to generate correct, well-formed components z \u2208 C for each input x. Viewed as a semantic parsing problem, this treats the target components as a kind of formal meaning representation, analogous to a logical form. In our experiments, we assume that the complete set of output components are known. In the API documentation sets, this is because each standard library contains a finite number of func-\ntion representations, roughly corresponding to the number of pairs as shown in Figure 3. For a given input, therefore, the goal is to find the best candidate function translation within the space of the total API components C (Deng and Chrupa\u0142a, 2014).\nGiven these constraints, our setup closely resembles that of Kushman et al. (2014), who learn to parse algebra word problems using a small set of equation templates. Their approach is inspired by template-based information extraction, where templates are recognized and instantiated by slotfilling. Our function signatures and code templates have a similar slot-like structure, consisting of slots such as return value, arguments, function name and namespace."}, {"heading": "3.2 Language Modeling Baselines", "text": "Existing approaches to semantic parsing formalize the mapping from language to logic using a variety of formalisms including CFGs (Bo\u0308rschinger et al., 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b). Deciding to use one formalism over another is often motivated by the complexities of the target representations being learned. For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013)\nrequires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al., 2015). Given the simplicity of our API representations, we opt for a simple semantic parsing model that exploits the finiteness of our target representations.\nFollowing ((Deng and Chrupa\u0142a, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999). For a given query sequence or text x = wi, .., wI and component sequence z = uj , .., uJ , the probability of the component given the query is defined as follows using Bayes\u2019 theorem: p(z|x) \u221d p(x|z)p(z).\nBy assuming a uniform prior over the probability of each component p(z), the problem reduces to computing p(x|z), which is where language modeling is used. Given each word wi in the query, a unigram model is defined as p(x|z) =\u220fI i=1 p(wi|z). Using this formulation, we can then define different models to estimate p(w|z).\nTerm Matching As a baseline for p(w|z), DC define a term matching approach that exploits the fact that many queries in our English datasets share vocabulary with target component vocabulary. A smoothed version of this baseline is defined below, where f(w|z) is the frequency of matching terms in the target signature, f(w|C) is frequency of the term word in the overall documentation collection, and \u03bb is a smoothing parameter (for Jelinek-Mercer smoothing):\np(x|z) = \u220f w\u2208x (1\u2212 \u03bb)f(w|z) + \u03bbf(w|C)\nTranslation Model In order to account for the co-occurrence between non-matching words and component terms, DC employ a word-based translation model, which models the relation between natural language words wj and individual component terms uj . In this paper, we limit ourselves to sequence-based word alignment models (Och and Ney, 2003), which factor in the following manner:\np(x|z) = I\u220f i=1 J\u2211 j=0 pt(wi|uj)pd(l(j)|i, I, J)\nHere each pt(wi|uj) defines an (unsmoothed) multinomial distribution over a given component term uj for all words wj . The function pd is a distortion parameter, and defines a dependency between the alignment positions and the lengths of\nAlgorithm 1 Rank Decoder Input: Query x, Components C of size m, rank k, modelA,\nsort function K-BEST Output: Top k components ranked by A model score p 1: procedure RANKCOMPONENTS(x, C, k,A) 2: SCORES \u2190 [ ] . Initialize score list 3: for each component c \u2208 C do 4: p\u2190 ALIGNA(x, c) . Score using A 5: SCORES += (c, p) . Add to list 6: return K-Best(SCORES,k) . k best components\nboth input strings. This function, and the definition of l(j), assumes different forms according to the particular alignment model being used. We consider three different types of alignment models each defined in the following way:\npd(l(j)|...) =  1 J+1 (1) a(j|i, I, J) (2) a(t(j)|i, I, tlen(J)) (3)\nModels (1-2) are the classic IBM word-alignment models of Brown et al. (1993). IBM Model 1, for example, assumes a uniform distribution over all positions, and is the main model investigated in DC. For comparison, we also experiment with IBM Model 2, where each l(j) refers to the string position of j in the component input, and a(..) defines a multinomial distribution such that\u2211J\nj=0 a(j|i, I, J) = 1.0. We also define a new tree based alignment model (3) that takes into account the syntax associated with the function representations. Each l(j) is the relative tree position of the alignment point, shown as t(j), and tlen(J) is the length of the tree associated with z. This approach assumes a tree representation for each z. We generated these trees heuristically by preserving the information that is lost when components are converted to a linear sequence representation. An example structure for PHP is shown in Figure 4, where the red solid line indicates the types of potential errors avoided by this model.\nLearning is done by applying the standard EM training procedure of Brown et al. (1993)."}, {"heading": "3.3 Ranking and Decoding", "text": "Algorithm 1 shows how to rank API components. For a text input x, we iterate through all known API components C and assign a score using a model A. We then rank the components by their scores using a K-BEST function. This method serves as a type of word-based decoding algorithm\nwhich is simplified by the finite nature of the target language. The complexity of the scoring procedure, lines 3-5, is linear over the number components m in C. In practice, we implement the K-BEST sorting function on line 6 as a binary insertion sort on line 5, resulting in an overall complexity of O(m log m).\nWhile iterating over m API components might not be feasible given more complicated formal languages with recursion, a more clever decoding algorithm could be applied, e.g., one based on the lattice decoding approach of (Dyer et al., 2008). Since we are interested in providing initial baseline results, we leave this for future work."}, {"heading": "4 Discriminative Approach", "text": "In this section, we introduce a new model that aims to improve on the previous baseline methods.\nWhile the previous models are restricted to word-level information, we extend this approach by using a discriminative reranking model that captures phrase information to see if this leads to an improvement. This model can also capture document-level information from the APIs, such as the additional textual descriptions of parameters, see also declarations or classes of related functions and syntax information."}, {"heading": "4.1 Modeling", "text": "Like in most semantic parsing approaches (Zettlemoyer and Collins, 2009; Liang et al., 2011), our model is defined as a conditional log-linear\nmodel over components z \u2208 C with parameters \u03b8 \u2208 Rb, and a set of feature functions \u03c6(x, z): p( z|x; \u03b8) \u221d e\u03b8\u00b7\u03c6(x,z).\nFormally, our training objective is to maximize the conditional log-likelihood of the correct component output z for each input x: O(\u03b8) =\u2211n\ni=1 log p (zi |xi; \u03b8)."}, {"heading": "4.2 Features", "text": "Our model uses word-level features, such as word match, word pairs, as well as information from the underlying aligner model such as Viterbi alignment information and model score. Two additional categories of non-word features are described below. An illustration of the feature extraction procedure is shown in Figure 5 1.\nPhrases Features We extract phrase features (e.g., (hyper. cosine,cosh) in Figure 5) from example text component pairs by training symmetric word aligners and applying standard word-level heuristics (Koehn et al., 2003). Additional features, such as phrase match/overlap, tree positions of phrases, are defined over the extracted phrases.\nWe also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) and the component syntax trees. Example rules are shown in Figure 4, where gaps (i.e., symbols in square brackets) are filled with smaller phrase-tree alignments.\nDocument Level Features Document features are of two categories. The first includes additional textual descriptions of parameters, return values, and modules. One class of features is whether certain words under consideration appear in the @param and @return descriptions of the target components. For example, the arg token in\n1A more complete description of features is included as supplementary material, along with all source code.\nAlgorithm 2 Online Rank Learner Input: DatasetD, components C, iterations T , rank k, learn-\ning rate \u03b1, model A, ranker function RANK Output: Weight vector \u03b8 1: procedure LEARNRERANKER(D, C, T, k, \u03b1,A,RANK) 2: \u03b8 \u2190 0 . Initialize 3: for t \u2208 1..T do 4: for pairs (xi, zi) \u2208 D do 5: S = RANK(xi, C, k,A) . Scored candidates 6: \u2206 = \u03c6(xi, zi)\u2212 Es\u2208S\u223cp(s|xi;\u03b8)[\u03c6(xi, s)] 7: \u03b8 = \u03b8 + \u03b1\u2206 . Update online 8: return \u03b8\nFigure 5 appears in the textual description of the $arg parameter elsewhere in the documentation string.\nOther features relate to general information about abstract symbol categories, as specified in see-also assertions, or hyper-link pointers. By exploiting this information, we extract general classes of functions, for example the set of hyperbolic function (e.g., sinh, cosh, shown as c4 in Figure 5), and associate these classes with words and phrases (e.g., hyperbolic and hyperbolic cosine)."}, {"heading": "4.3 Learning", "text": "To optimize our objective, we use Algorithm 2. We estimate the model parameters \u03b8 using a Kbest approximation of the standard stochastic gradient updates (lines 6-7), and a ranker function RANK. We note that while we use the ranker described in Algorithm 1, any suitable ranker or decoding method could be used here."}, {"heading": "5 Experimental Setup", "text": ""}, {"heading": "5.1 Datasets", "text": "Source code documentation Our source code documentation collection consists of the standard library for nine programming languages, which are listed in Figure 3. We also use the translated version of the PHP collection for six additional languages, the details of which are shown in Figure 6. The Java dataset was first used in DC, while we extracted all other datasets for this work.\nThe size of the different datasets are detailed in both figures. The number of pairs is the number of single sentences paired with function representations, which constitutes the core part of these datasets. The number of descriptions is the number of additional textual descriptions provided in the overall document, such as descriptions of parameters or return values.\nWe also quantify the different datasets in terms of unique symbols in the target representations, shown as Symbols. All function representations and code sequences are linearized, and in some cases further tokenized, for example, by converting out of camel case or removing underscores.\nMan pages The collection of man pages is from Richardson and Kuhn (2014) and includes 921 text-code pairs that span 330 Unix utilities and man pages. Using information from the synopsis and parameter declarations, the target code representations are abstracted by type. The extra descriptions are extracted from parameter descriptions, as shown in the DESCRIPTION section in Figure 1, as well as from the NAME sections of each manual."}, {"heading": "5.2 Evaluation", "text": "For evaluation, we split our datasets into separate training, validation and test sets. For Java, we reserve 60% of the data for training and the remaining 40% for validation (20%) and testing (20%). For all other datasets, we use a 70%-30% split. From a retrieval perspective, these left out descriptions are meant to mimic unseen queries to our model. After training our models, we evaluate on these held out sets by ranking all known components in each resource using Algorithm 1. A predicted component is counted as correct if it matches exactly a gold component.\nFollowing DC, we report the accuracy of predicting the correct representation at the first position in the ranked list (Accuracy @1) and within the top 10 positions (Accuracy @10). We also report the mean reciprocal rank MRR, or the multiplicative inverse of the rank of the correct answer.\nBaselines For comparison, we trained a bag-ofwords classifier (the BoW Model in Table 1). This model uses the occurrence of word-component symbol pairs as binary features, and aims to see if word co-occurrence alone is sufficient to for ranking representations.\nSince our discriminative models use more data than the baseline models, which therefore make the results not directly comparable, we train a more comparable translation model, shown as M1 Descr. in Table 1, by adding the additional textual data (i.e. parameter and return or module descriptions) to the models\u2019 parallel training data."}, {"heading": "6 Results and Discussion", "text": "Test results are shown in Table 1. Among the baseline models, IBM Model 1 outperforms virtually all other models and is in general a strong baseline. Of particular note is the poor performance of the higher-order translation models based on Model 2 and the Tree Model. While Model 2 is known to outperform Model 1 on more conventional translation tasks (Och and Ney, 2003), it appears that such improvements are not reflected in this type of semantic translation context.\nThe bag-of-words (BoW) and Term Match baselines are outperformed by all other models. This shows that translation in this context is more complicated than simple word matching. In some cases the term matching baseline is competitive with other models, suggesting that API collections differ in how language descriptions overlap with component names and naming conventions. It is clear, however, that this heuristic only works for English, as shown by results on the non-English PHP datasets in Table 1.\nWe achieve improvements on many datasets by adding additional data to the translation model (M1 Descr.). We achieve further improvements on all datasets using the discriminative model (Reranker), with most increases in performance occurring at how the top ten items are ranked.\nThis last result suggests that phrase-level and document-level features can help to improve the overall ranking and translation, though in some cases the improvement is rather modest.\nDespite the simplicity of our semantic parsing model and decoder, there is still much room for improvement, especially on achieving better Accuracy @1. While one might expect better results when moving from a word-based model to a model that exploits phrase and hierarchical phrase features, the sparsity of the component vocabulary is such that most phrase patterns in the training are not observed in the evaluation. In many benchmark semantic parsing datasets, such sparsity issues do not occur (Cimiano and Minock, 2009), suggesting that state-of-the-art methods will have similar problems when applied to our datasets.\nRecent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al., 2014). We expect that these methods can be used to improve our models and results, especially given the wide availability of technical documentation, for example, distributed within the Opus project (Tiedemann, 2012).\nModel Errors We performed analysis on some of the incorrect predictions made by our models. For some documentation sets, such as those in the GNU documentation collection2, information is organized into a small and concrete set of categories/chapters, each corresponding to various features or modules in the language and related functions. Given this information, Figure\n2https://www.gnu.org/doc/doc.en.html\nAs so\nci at\nio ns\nDa ta\nty pe s En vi ro nm en ts Pr oc ed ur es OS IO Gr ap hi cs Er ro rs Wi nd ow s Ot he r Sc he me Eq ui v. Sp ec . Fo rm s Ch ar ac te rs Nu mb er s Li st s St ri ng s Bi t St ri ng s Ve ct or s\nVectors Bit Strings\nStrings Lists\nNumbers Characters\nSpec. Forms Equiv. Scheme Other\nWindows Errors\nGraphics IO OS\nProcedures Environments\nDatatypes Associations\nda sh se l Fi le s Ba ck up s Bu ff er s Wi nd ow s Co mm an dL oo p Ke ym ap s Mo de s Do cu me nt at io n Fr am es Po si ti on s St ri ng s Da ta ty pe s Ch ar ac te rs Nu mb er s Ha sh T ab le s Se qu en ce s Ev al ua ti on Sy mb ol s OS Ga rb ag e Co ll . Di st r. Fu nc ti on s Lo ad in g Cu st om iz at io n De bu g Mi ni bu ff er s No nAs ci i Te xt Ma rk er s Di sp la y Pr oc es se s Ab br ev s Sy nt ax T ab le s Se ar ch /M at ch Re ad /W ri te\nRead/Write Search/Match\nSyntax Tables Abbrevs\nProcesses Display Markers\nText Non-Ascii\nMinibuffers Debug\nCustomization Loading\nFunctions Distr.\nGarbage Coll. OS\nSymbols Evaluation Sequences\nHash Tables Numbers\nCharacters Datatypes\nStrings Positions\nFrames Documentation\nModes Keymaps\nCommandLoop Windows Buffers Backups\nFiles sel\ndash\nFigure 7: Function predictions by documentation category for Scheme (left) and Elisp (right).\n7 shows the confusion between predicting different categories of functions, where the rows show the categories of functions to be predicted and the columns show the different categories predicted. We built these plots by finding the categories of the top 50 non-gold (or erroneous) representations generated for each validation example.\nThe step-like lines through the diagonal of both plots show that alternative predictions (shaded according to occurrence) are often of the same category, most strikingly for the corner categories. This trend seems stable across other datasets, even among datasets with large numbers of categories. Interestingly, many confusions appear to be between related categories. For example, when making predictions about Strings functions in Scheme, the model often generates function related to BitStrings, Characters and IO. Again, this trend seems to hold for other documentation sets, suggesting that the models are often making semantically sensible decisions.\nLooking at errors in other datasets, one common error involves generating functions with the same name and/or functionality. In large libraries, different modules sometimes implement that same core functions, such the genericpath or posixpath modules in Python. When generating a representation for the text return size of file, our model confuses the getsize(filename) function in one module with others. Similarly, other subtle distinctions that are not explicitly expressed in the text descriptions are not captured, such as the distinction in Haskell between safe and unsafe bit shifting functions.\nWhile many of these predictions might be correct, our evaluation fails to take into account these various equivalences, which is an issue that should\nbe investigated in future work. Future work will also look systematically at the effect that types (i.e., in statically typed versus dynamic languages) have on prediction."}, {"heading": "7 Future Work", "text": "We see two possible use cases for this data. First, for benchmarking semantic parsing models on the task of semantic translation. While there has been a trend towards learning executable semantic parsers (Berant et al., 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016). We believe that good performance on our datasets should lead to better performance on more conventional semantic parsing tasks, and raise new challenges involving sparsity and multilingual learning.\nWe also see these resources as useful for investigations into natural language programming. While our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (Desai et al., 2016; Raza et al., 2015). Other document-level features, such as example input-output pairs, unit tests, might be useful in this endeavor."}, {"heading": "Acknowledgements", "text": "This work was funded by the Deutsche Forschungsgemeinschaft (DFG) via SFB 732, project D2. Thanks also to our IMS colleagues, in particular Christian Scheible, for providing feedback on earlier drafts, as well as to Jonathan Berant for helpful discussions."}], "references": [{"title": "Bimodal modelling of source code and natural language", "author": ["Miltiadis Allamanis", "Daniel Tarlow", "Andrew D Gordon", "Yi Wei."], "venue": "Proceedings of the 32th International Conference on Machine Learning. volume 951, page 2015.", "citeRegEx": "Allamanis et al\\.,? 2015", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1699\u20131710.", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Martha Palmer", "Nathan Schneider."], "venue": "In Proceedings of the 7th Linguistic", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "in Proceedings of EMNLP-2013. pages 1533\u20131544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of ACL2014. pages 1415\u20131425.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Reducing grounded learning tasks to grammatical inference", "author": ["Benjamin B\u00f6rschinger", "Bevan K. Jones", "Mark Johnson."], "venue": "Proceedings of EMNLP2011. pages 1416\u20131425.", "citeRegEx": "B\u00f6rschinger et al\\.,? 2011", "shortCiteRegEx": "B\u00f6rschinger et al\\.", "year": 2011}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Computational linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Learning to sportscast: A test of grounded language acquisition", "author": ["David L. Chen", "Raymond J. Mooney."], "venue": "Proceedings of ICML-2008. pages 128\u2013 135.", "citeRegEx": "Chen and Mooney.,? 2008", "shortCiteRegEx": "Chen and Mooney.", "year": 2008}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "computational linguistics 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Natural language interfaces: what is the problem?\u2013a datadriven quantitative analysis", "author": ["Philipp Cimiano", "Michael Minock."], "venue": "International Conference on Application of Natural Language to Information Systems. Springer, pages 192\u2013206.", "citeRegEx": "Cimiano and Minock.,? 2009", "shortCiteRegEx": "Cimiano and Minock.", "year": 2009}, {"title": "Semantic approaches to software component retrieval with English queries", "author": ["Huijing Deng", "Grzegorz Chrupa\u0142a."], "venue": "Proceedings of LREC-14. pages 441\u2013450.", "citeRegEx": "Deng and Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Deng and Chrupa\u0142a.", "year": 2014}, {"title": "Program synthesis using natural language", "author": ["Aditya Desai", "Sumit Gulwani", "Vineet Hingorani", "Nidhi Jain", "Amey Karkare", "Mark Marron", "Subhajit Roy"], "venue": "In Proceedings of the 38th International Conference on Software Engineering", "citeRegEx": "Desai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Desai et al\\.", "year": 2016}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1601.01280 .", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Generalizing word lattice translation", "author": ["Christopher Dyer", "Smaranda Muresan", "Philip Resnik."], "venue": "Proceedings of ACL-08 page 1012.", "citeRegEx": "Dyer et al\\.,? 2008", "shortCiteRegEx": "Dyer et al\\.", "year": 2008}, {"title": "Deep API Learning", "author": ["Xiaodong Gu", "Hongyu Zhang", "Dongmei Zhang", "Sunghun Kim."], "venue": "arXiv preprint arXiv:1605.08535 .", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Summarizing source code using a neural attention model", "author": ["Srinivasan Iyer", "Ioannis Kostas", "Alvin Cheung", "Luke Zettlemoyer."], "venue": "Proceedings of ACL2016 .", "citeRegEx": "Iyer et al\\.,? 2016", "shortCiteRegEx": "Iyer et al\\.", "year": 2016}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.03622 .", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the NACL-2003. pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Learning to automatically solve algebra word problems", "author": ["Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proceedings of ACL-2014. pages 271\u2013281.", "citeRegEx": "Kushman et al\\.,? 2014", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Using semantic unification to generate regular expressions from natural language", "author": ["Nate Kushman", "Regina Barzilay."], "venue": "Proceedings of NAACL2013.", "citeRegEx": "Kushman and Barzilay.,? 2013", "shortCiteRegEx": "Kushman and Barzilay.", "year": 2013}, {"title": "Inducing probabilistic CCG grammars from logical form with higherorder unification", "author": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of EMNLP-2010. pages 1223\u20131233.", "citeRegEx": "Kwiatkowski et al\\.,? 2010", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2010}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Proceedings of ACL-11. pages 590\u2013599.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Learning executable semantic parsers for natural language understanding", "author": ["Percy Liang."], "venue": "Communications of the ACM 59(9):68\u201376.", "citeRegEx": "Liang.,? 2016", "shortCiteRegEx": "Liang.", "year": 2016}, {"title": "Codehow: Effective code search based on api understanding and extended boolean model (e)", "author": ["Fei Lv", "Hongyu Zhang", "Jian-guang Lou", "Shaowei Wang", "Dongmei Zhang", "Jianjun Zhao."], "venue": "Automated Software Engineering (ASE), 2015", "citeRegEx": "Lv et al\\.,? 2015", "shortCiteRegEx": "Lv et al\\.", "year": 2015}, {"title": "Integrating programming by example and natural language programming", "author": ["Mehdi Hafezi Manshadi", "Daniel Gildea", "James F Allen."], "venue": "Proceedings of AAAI-2013.", "citeRegEx": "Manshadi et al\\.,? 2013", "shortCiteRegEx": "Manshadi et al\\.", "year": 2013}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["Cynthia Matuszek", "Evan Herbst", "Luke Zettlemoyer", "Dieter Fox."], "venue": "Proceedings of the International Symposium on Experimental Robotics (ISER).", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Learning to generate pseudo-code from source code using statistical machine translation (t)", "author": ["Yusuke Oda", "Hiroyuki Fudaba", "Graham Neubig", "Hideaki Hata", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."], "venue": "Automated Software Engi-", "citeRegEx": "Oda et al\\.,? 2015", "shortCiteRegEx": "Oda et al\\.", "year": 2015}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "Proceedings of ACL-2015.", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "A Synchronous Hyperedge Replacement Grammar based approach for AMR parsing", "author": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."], "venue": "Proceedings of CoNLL-2015 page 32.", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Language to code: Learning semantic parsers for if-this-then-that recipes", "author": ["Chris Quirk", "Raymond J Mooney", "Michel Galley."], "venue": "Proceedings of ACL2015. pages 878\u2013888.", "citeRegEx": "Quirk et al\\.,? 2015", "shortCiteRegEx": "Quirk et al\\.", "year": 2015}, {"title": "Compositional program synthesis from natural language and examples", "author": ["Mohammad Raza", "Sumit Gulwani", "Natasa MilicFrayling."], "venue": "IJCAI. pages 792\u2013800.", "citeRegEx": "Raza et al\\.,? 2015", "shortCiteRegEx": "Raza et al\\.", "year": 2015}, {"title": "Large-scale semantic parsing without questionanswer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "UnixMan corpus: A resource for language learning in the Unix domain", "author": ["Kyle Richardson", "Jonas Kuhn."], "venue": "Proceedings of LREC-2014.", "citeRegEx": "Richardson and Kuhn.,? 2014", "shortCiteRegEx": "Richardson and Kuhn.", "year": 2014}, {"title": "Learning to make inferences in a semantic parsing task", "author": ["Kyle Richardson", "Jonas Kuhn."], "venue": "Transactions of the Association for Computational Linguistics 4:155\u2013168.", "citeRegEx": "Richardson and Kuhn.,? 2016", "shortCiteRegEx": "Richardson and Kuhn.", "year": 2016}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B Croft."], "venue": "in Proceedings International Conference on Information and Knowledge Management.", "citeRegEx": "Song and Croft.,? 1999", "shortCiteRegEx": "Song and Croft.", "year": 1999}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "LREC. volume 2012, pages 2214\u2013 2218.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Yuk Wah Wong", "Raymond J. Mooney."], "venue": "Proceedings of HLT-NAACL-2006. pages 439\u2013446.", "citeRegEx": "Wong and Mooney.,? 2006", "shortCiteRegEx": "Wong and Mooney.", "year": 2006}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Yuk Wah Wong", "Raymond J Mooney."], "venue": "Proceedings of HLTNAACL-2007. pages 172\u2013179.", "citeRegEx": "Wong and Mooney.,? 2007a", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Yuk Wah Wong", "Raymond J. Mooney."], "venue": "Proceedings of ACL2007. Prague, Czech Republic.", "citeRegEx": "Wong and Mooney.,? 2007b", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M Zelle", "Raymond J Mooney."], "venue": "Proceedings of AAAI-1996. pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Learning context-dependent mappings from sentences to logical form", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "Proceedings of ACL-2009. pages 976\u2013984.", "citeRegEx": "Zettlemoyer and Collins.,? 2009", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2009}, {"title": "Syntax augmented machine translation via chart parsing", "author": ["Andreas Zollmann", "Ashish Venugopal."], "venue": "Proceedings of the Workshop on Statistical Machine Translation. pages 138\u2013141.", "citeRegEx": "Zollmann and Venugopal.,? 2006", "shortCiteRegEx": "Zollmann and Venugopal.", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "We achieved initial baselines using the language modeling and translation approach of Deng and Chrupa\u0142a (2014). We also show that modest improvements can be achieved by using a more conventional", "startOffset": 86, "endOffset": 111}, {"referenceID": 41, "context": "discriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets.", "startOffset": 21, "endOffset": 52}, {"referenceID": 3, "context": "Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al.", "startOffset": 169, "endOffset": 190}, {"referenceID": 25, "context": ", 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a).", "startOffset": 23, "endOffset": 46}, {"referenceID": 38, "context": ", 2012) and text generation (Wong and Mooney, 2007a).", "startOffset": 28, "endOffset": 52}, {"referenceID": 37, "context": "Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009).", "startOffset": 151, "endOffset": 174}, {"referenceID": 41, "context": "Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009).", "startOffset": 187, "endOffset": 218}, {"referenceID": 3, "context": "While attempts have been made to do open-domain semantic parsing using larger, more complex datasets (Berant et al., 2013; Pasupat and Liang, 2015), such resources are still scarce.", "startOffset": 101, "endOffset": 147}, {"referenceID": 28, "context": "While attempts have been made to do open-domain semantic parsing using larger, more complex datasets (Berant et al., 2013; Pasupat and Liang, 2015), such resources are still scarce.", "startOffset": 101, "endOffset": 147}, {"referenceID": 40, "context": "In Figure 3, we compare the details of one widely used dataset, Geoquery (Zelle and Mooney, 1996), to our new datasets.", "startOffset": 73, "endOffset": 97}, {"referenceID": 22, "context": "Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016).", "startOffset": 158, "endOffset": 198}, {"referenceID": 34, "context": "Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016).", "startOffset": 158, "endOffset": 198}, {"referenceID": 7, "context": "This has sometimes involved learning from automatically generated parallel data and representations (Chen and Mooney, 2008) of the type we consider in this paper.", "startOffset": 100, "endOffset": 123}, {"referenceID": 24, "context": "Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al.", "startOffset": 96, "endOffset": 147}, {"referenceID": 19, "context": "Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al.", "startOffset": 96, "endOffset": 147}, {"referenceID": 30, "context": ", 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al., 2015), which ultimately aim to solve the problem of natural language programming.", "startOffset": 67, "endOffset": 87}, {"referenceID": 23, "context": "Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al., 2015).", "startOffset": 140, "endOffset": 157}, {"referenceID": 0, "context": "These studies primarily focus on learning longer programs (Allamanis et al., 2015) as opposed to function representations, or focus narrowly on a single programming language such as Java (Gu et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 14, "context": ", 2015) as opposed to function representations, or focus narrowly on a single programming language such as Java (Gu et al., 2016) or on related tasks such as text generation (Iyer et al.", "startOffset": 112, "endOffset": 129}, {"referenceID": 15, "context": ", 2016) or on related tasks such as text generation (Iyer et al., 2016; Oda et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 27, "context": ", 2016) or on related tasks such as text generation (Iyer et al., 2016; Oda et al., 2015).", "startOffset": 52, "endOffset": 89}, {"referenceID": 9, "context": "Our work is also related to software components retrieval and builds on the approach of Deng and Chrupa\u0142a (2014). Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al.", "startOffset": 88, "endOffset": 113}, {"referenceID": 10, "context": "In source code, components are formal representations of functions, or function signatures (Deng and Chrupa\u0142a, 2014).", "startOffset": 91, "endOffset": 116}, {"referenceID": 10, "context": "In source code, components are formal representations of functions, or function signatures (Deng and Chrupa\u0142a, 2014). The form of a function signature varies depending on the resource, but in general gives a specification of how a function is named and structured. The example function signatures in Figure 3 all specify a function name, a list of arguments, and other optional information such as a return value and a namespace. Components in utility manuals are short executable code sequences intended to show an example use of a utility. We assume typed code sequences following Richardson and Kuhn (2014), where the constituent parts of the sequences are abstracted by type.", "startOffset": 92, "endOffset": 610}, {"referenceID": 10, "context": "For a given input, therefore, the goal is to find the best candidate function translation within the space of the total API components C (Deng and Chrupa\u0142a, 2014).", "startOffset": 137, "endOffset": 162}, {"referenceID": 18, "context": "Given these constraints, our setup closely resembles that of Kushman et al. (2014), who learn to parse algebra word problems using a small set of equation templates.", "startOffset": 61, "endOffset": 83}, {"referenceID": 5, "context": "Existing approaches to semantic parsing formalize the mapping from language to logic using a variety of formalisms including CFGs (B\u00f6rschinger et al., 2011), CCGs (Kwiatkowski et al.", "startOffset": 130, "endOffset": 156}, {"referenceID": 20, "context": ", 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b).", "startOffset": 14, "endOffset": 40}, {"referenceID": 39, "context": ", 2010), synchronous CFGs (Wong and Mooney, 2007b).", "startOffset": 26, "endOffset": 50}, {"referenceID": 2, "context": "For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013)", "startOffset": 99, "endOffset": 123}, {"referenceID": 1, "context": "requires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 29, "context": ", 2015) or HRGs (Peng et al., 2015).", "startOffset": 16, "endOffset": 35}, {"referenceID": 10, "context": "Following ((Deng and Chrupa\u0142a, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999).", "startOffset": 11, "endOffset": 36}, {"referenceID": 35, "context": "Following ((Deng and Chrupa\u0142a, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999).", "startOffset": 131, "endOffset": 153}, {"referenceID": 26, "context": "In this paper, we limit ourselves to sequence-based word alignment models (Och and Ney, 2003), which factor in the following manner:", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "Models (1-2) are the classic IBM word-alignment models of Brown et al. (1993). IBM Model 1, for example, assumes a uniform distribution over all positions, and is the main model investigated in DC.", "startOffset": 58, "endOffset": 78}, {"referenceID": 6, "context": "Learning is done by applying the standard EM training procedure of Brown et al. (1993).", "startOffset": 67, "endOffset": 87}, {"referenceID": 8, "context": "Below are Hiero rules (Chiang, 2007) extracted from the alignment and tree information.", "startOffset": 22, "endOffset": 36}, {"referenceID": 13, "context": ", one based on the lattice decoding approach of (Dyer et al., 2008).", "startOffset": 48, "endOffset": 67}, {"referenceID": 41, "context": "Like in most semantic parsing approaches (Zettlemoyer and Collins, 2009; Liang et al., 2011), our model is defined as a conditional log-linear z: function float cosh float $arg", "startOffset": 41, "endOffset": 92}, {"referenceID": 21, "context": "Like in most semantic parsing approaches (Zettlemoyer and Collins, 2009; Liang et al., 2011), our model is defined as a conditional log-linear z: function float cosh float $arg", "startOffset": 41, "endOffset": 92}, {"referenceID": 17, "context": "cosine,cosh) in Figure 5) from example text component pairs by training symmetric word aligners and applying standard word-level heuristics (Koehn et al., 2003).", "startOffset": 140, "endOffset": 160}, {"referenceID": 8, "context": "We also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) and the component syntax trees.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "We also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) and the component syntax trees.", "startOffset": 38, "endOffset": 120}, {"referenceID": 33, "context": "Man pages The collection of man pages is from Richardson and Kuhn (2014) and includes 921 text-code pairs that span 330 Unix utilities and man pages.", "startOffset": 46, "endOffset": 73}, {"referenceID": 26, "context": "While Model 2 is known to outperform Model 1 on more conventional translation tasks (Och and Ney, 2003), it appears that such improvements are not reflected in this type of semantic translation context.", "startOffset": 84, "endOffset": 103}, {"referenceID": 9, "context": "In many benchmark semantic parsing datasets, such sparsity issues do not occur (Cimiano and Minock, 2009), suggesting that state-of-the-art methods will have similar problems when applied to our datasets.", "startOffset": 79, "endOffset": 105}, {"referenceID": 4, "context": "Recent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al.", "startOffset": 112, "endOffset": 136}, {"referenceID": 32, "context": "Recent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al., 2014).", "startOffset": 160, "endOffset": 180}, {"referenceID": 36, "context": "We expect that these methods can be used to improve our models and results, especially given the wide availability of technical documentation, for example, distributed within the Opus project (Tiedemann, 2012).", "startOffset": 192, "endOffset": 209}, {"referenceID": 3, "context": "While there has been a trend towards learning executable semantic parsers (Berant et al., 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016).", "startOffset": 74, "endOffset": 108}, {"referenceID": 22, "context": "While there has been a trend towards learning executable semantic parsers (Berant et al., 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016).", "startOffset": 74, "endOffset": 108}, {"referenceID": 12, "context": ", 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016).", "startOffset": 157, "endOffset": 201}, {"referenceID": 16, "context": ", 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016).", "startOffset": 157, "endOffset": 201}, {"referenceID": 11, "context": "While our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (Desai et al., 2016; Raza et al., 2015).", "startOffset": 211, "endOffset": 250}, {"referenceID": 31, "context": "While our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (Desai et al., 2016; Raza et al., 2015).", "startOffset": 211, "endOffset": 250}], "year": 2017, "abstractText": "We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.", "creator": "LaTeX with hyperref package"}}}