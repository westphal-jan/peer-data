{"id": "1705.05665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Learning Image Relations with Contrast Association Networks", "abstract": "Deriving the relationships between two images is an important task class in computer vision. Examples of such tasks are the optical flow and stereodisparity. We treat the relation sequence tasks as a machine learning problem and solve them with neural networks. A key to this problem is learning a representation of relationships. We propose a new module for neural networks, the Contrast Association Unit (CAU), which explicitly models the relationships between two groups of input variables. Due to the non-negativity of the weights in CAU, we use a multiplicative actualization algorithm to learn these weights. Experiments show that neural networks with CAUs are more effective at learning five fundamental image transformations than conventional neural networks.", "histories": [["v1", "Tue, 16 May 2017 12:09:44 GMT  (201kb)", "http://arxiv.org/abs/1705.05665v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yao lu", "zhirong yang", "juho kannala", "samuel kaski"], "accepted": false, "id": "1705.05665"}, "pdf": {"name": "1705.05665.pdf", "metadata": {"source": "CRF", "title": "Learning Image Relations with Contrast Association Networks", "authors": ["Yao Lu", "Zhirong Yang", "Juho Kannala", "Samuel Kaski"], "emails": ["yaolubrain@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 66\n5v 1\n[ cs\n.C V\n] 1\n6 M"}, {"heading": "1 Introduction", "text": "Neural networks, especially convolutional neural network (CNN) [26], have been successfully applied in many computer vision tasks such as object recognition [7, 25]. A key to this success is that the neural networks allow appearance representation which is invariant of several image transformations such as small translation.\nAnother important class of tasks in computer vision is the inference of relations between two images. Two images can be related by object motion, camera motion or environmental factors such as lighting change. For these problems, instead of aiming for invariance of appearance changes, we want to detect and estimate these changes. For example, given two consecutive video frames, we want to infer the movement of each pixel from one image to the other (optical flow). For another example, given two images taken by a camera on a moving robot, we want to infer the ego-motion of the robot (visual odometry).\nThe traditional approach to perform the relation inference tasks is knowledge-based. By acquiring knowledge of a task and making reasonable assumptions for simplification, one designs an algorithm to perform the inference. A classic example is the Horn-Schunck algorithm for computing optical flow [14]. However, in situations where we lack sufficient knowledge or the assumptions fail, the knowledge-based approach may not perform well. For example, the Horn-Schunck algorithms assumes brightness constancy of the moving pixels. This assumption can be violated by many factors such as occlusion, shading and noise.\nAn alternative approach to solve the relation inference problem is learning-based [29]. From a collection of training data, we aim to learn a function\nz = F (x,y) (1)\nsuch that given two images x and y as inputs, the function will output their relation variables z. The relation variables can be rotation angle, motion field, affine transformation parameters, etc.\n\u2217Corresponding author. Email: yaolubrain@gmail.com\nCompared to the knowledge-based approach, the learning-based approach does not require sufficient knowledge or assumptions but a large amount of training images with ground-truth relation variables. When it is difficult to obtain the ground-truth for real world images, one can often resort to synthetic images rendered by graphics engines. An example is the Sintel dataset for learning optical flow [5]. Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23]. Some of the results are competitive to the knowledge-based methods. Note that relation learning can also serve as supervision for learning appearance representation, as demonstrated in learning ego-motion [2, 18] and robot actions [32].\nAdditionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43]. In these methods, a neural network is trained to match two image patches and a knowledge-based post-processing is applied on the matching results to output the relation variables. In this paper, we focus on the pure learning-based approach, that is, neural networks are trained in an end-to-end manner, given raw images as inputs and ground-truth relation variables as targets. Although a relation learning model can also be trained unsupervisedly [30, 22, 44], we restrict our discussion mainly to supervised learning.\nWhile both object recognition and relation inference can be treated as supervised learning tasks, the two tasks have much difference in nature. Object recognition aims for invariance of several image transformations (e.g. translation, rotation and scaling) but relation inference aims for equivariance of these transformations. For example, the conventional CNN with a pooling operation is known to be invariant of small translation. This property is suitable for object recognition but not for motion detection, whose goal is to estimate the translation. This difference should be kept in mind when designing a relation learning model.\nIn this paper, we propose a new relation unit, contrast association unit (CAU). We show that CAUs are suitable for relation learning tasks with analysis and experiments. We adopt a multiplicative update algorithm for learning the non-negative weights in CAUs. The multiplicative update algorithm is compatible with gradient descent algorithms for unconstrained weights. The whole neural network can be trained in an end-to-end manner.\nNext, in Section 2, we outline the general neural network architecture for relation learning. In Section 3, we introduce our proposed relation unit CAU. In Section 4, we present the multiplicative update algorithm for learning the non-negative weights in CAUs. In Section 5, we discuss models related to CAU. In Section 6, we present the experiments on the five relation learning tasks. Finally in Section 7, we discuss the limitations of our work and propose several directions for future research.\n2 Architecture\nAs illustrated in Figure 1, the general neural network architecture of many relation learning models [30, 8, 2] can be described as\na = f(x), h = R(a,b),\nb = f(y), z = g(h), (2)\nwhere x and y are the inputs (images), z are the targets (relation variables), f(\u00b7) is the feature extraction units, R(\u00b7, \u00b7) is the relation units and g(\u00b7) is the readout units. f(\u00b7) and g(\u00b7) can be parametric functions such as neural networks. When the relation of two images is on the pixellevel (e.g. affine transformation), f(\u00b7) can be the identity function such that the relation units can directly apply on the image pixels. When the relation units R(\u00b7, \u00b7) can directly output relation variables instead of a hidden representation, g(\u00b7) can also be the identity function. For example, in [30], both f(\u00b7) and g(\u00b7) are the identity function. For another example, in the simple version of FlowNet [8], f(\u00b7) is the identity function and g(\u00b7) is a CNN while in the complex version, both f(\u00b7) and g(\u00b7)\nare CNNs. The main focus of this paper is the relation units R(\u00b7, \u00b7). A suitable relational representation can reduce the sample complexity and model complexity of learning g(\u00b7). We present two common types of relation units in below."}, {"heading": "2.1 Concatenation Units", "text": "Concatenation units are defined as\nh = [a b]. (3)\nFor this simple representation, g(\u00b7) is solely responsible for learning the relations between a and b. Concatenation units have been used in learning optical flow [8] and ego-motion [2]."}, {"heading": "2.2 Bilinear Units", "text": "Bilinear units have been previously proposed and developed [13, 31, 39]. They are defined as, for the k-th unit,\nhk = \u2211\nij\nWijkaibj = a TWkb, (4)\nwhereWk is the parameters to be learned. A bilinear unit models the pair-wise multiplicative intersection between two sets of input variables. It is equivalent to inner product whenWk is the identity matrix and equivalent to outer product since (4) can be written as hk = vec(Wk) T vec(abT ), where vec(\u00b7) denotes vectorization. Bilinear units have been used in learning image transformations in [30, 35]."}, {"heading": "3 Contrast Association Units", "text": "We propose a new relation unit, contrast association unit (CAU), which associates two sets of input variables. CAUs are defined as, for the k-th unit,\nhk = 1\n2\n\u2211\nij\nWijk(ai \u2212 bj) 2, (5)\nwhereWijk \u2265 0. Each CAU can be interpreted as a weighted sum of mismatches between a and b. The non-negative constraint on the weight matrices is indispensable since the mismatches should be non-negative to be accumulated. Otherwise, positive mismatches and negative mismatches would cancel each other.\nCompared to concatenation and bilinear units, CAUs have two advantages: (1) as the name suggests, CAUs model the contrast between two sets of variables such that R(a + c,b + c) = R(a,b), where c is a scalar applied on each element of a vector. This property is desirable for relation inference. For example, when a and b are the raw pixels, their relations should not be affected by their absolute pixel intensity level. (2) CAUs represent relations more explicitly. Each CAU stands for the matching error of a certain relation encoded by the weight matrix. An example is given in Section 3.2. This interpretablity also inspires the use of competition among CAUs as described below."}, {"heading": "3.1 Competition", "text": "We apply a competition mechanism among CAUs. The competition encourages the unit standing for the minimum matching error to pop out. As a result, the relation representation can be more easily decoded by the readout units. A classic competition mechanism is the winner-take-all (WTA), defined as\nh\u2032k =\n{ 1, if hk = min(h),\n0, otherwise. (6)\nWTA is of conceptual interest, which is demonstrated in Section 3.2. However, WTA is not differentiable and discards too much information. In practice, we can use softmin competition, defined as\nh\u2032k = e\u2212hk\u2211 i e \u2212hi . (7)\nIn all our experiments, adding the softmin competition significantly improves the results of neural networks with CAUs."}, {"heading": "3.2 Example", "text": "To understand how neural networks with CAUs represent relations, let us consider a simple example of translation detection. Let\na = (c1, c2, c3, c4, c5), (8)\nb1 = (c2, c3, c4, c5, c6), (9)\nb2 = (c1, c2, c3, c4, c5), (10)\nb3 = (c0, c1, c2, c3, c4), (11)\nwhere {ci} are arbitrary numbers. Let z \u2208 {\u22121, 0, 1} be the translation variable and\nb =    b1, if z = \u22121, b2, if z = 0,\nb3, if z = 1.\n(12)\nDenote by D(a,b) the matrix of pair-wise squared differences of the elements in a and b. The element of index (i, j) in D(a,b) is (ai \u2212 bj)\n2. Then we have   \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217   \ufe38 \ufe37\ufe37 \ufe38 D(a,b1) ,   0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0   \ufe38 \ufe37\ufe37 \ufe38 D(a,b2) ,   \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217 0 \u2217 \u2217 \u2217 \u2217 \u2217   \ufe38 \ufe37\ufe37 \ufe38 D(a,b3)\n(13)\nwhere \u2217 denotes the element whose value we do not care about. We construct three CAUs (h1, h2, h3) with the following weight matrices,\n  0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0   \ufe38 \ufe37\ufe37 \ufe38 W1 ,   1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1   \ufe38 \ufe37\ufe37 \ufe38 W2 ,   0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0   \ufe38 \ufe37\ufe37 \ufe38 W3\n(14)\nrespectively. If b = bi, then hi = 0 and hj 6=i > 0 except for some special cases. With WTA competition, h\u2032i = 1 and h \u2032 j 6=i = 0. The translation variable z can be inferred with simple readout units g(h\u2032) = (\u22121, 0, 1) \u00b7 h\u2032 = z."}, {"heading": "3.3 Low-rank Approximation", "text": "IfWk is large, we can approximate it in the following way. For rank-one approximation, letWk = ukv T k , where uk and vk are a row of non-negative matrices U and V, respectively. Then the right side of (5) can be written in the matrix form as\nh\u2217 = 1\n2\n[ (V1) \u25e6U(a)2 + (U1) \u25e6V(b)2 ] \u2212 (Ua) \u25e6 (Vb), (15)\nwhere 1 is a vector of ones, \u25e6 is the element-wise multiplication and (\u00b7)2 is the element-wise square. The derivation can be found in Appendix. To obtain CAUs of higher ranks, we can apply sumpooling over h\u2217. That is, divide h\u2217 into non-overlapping groups of equal size and sum the units in each group."}, {"heading": "4 Learning", "text": "To learn the non-negative weights in CAUs (Wk for full rank or U and V for rank-one), the conventional gradient descent based algorithms are not suitable because the nonnegativity of weights cannot be maintained after each update. Simply projecting weights onto the space of the nonnegative matrices after each update performs poorly in our experiments, where the learning converges at an extremely low speed or even diverges in many cases.\nTo address the above problem, we adopt a multiplicative update algorithm for the non-negative weight matrices in a neural network, which was originally used for non-negativematrix factorization\n[27]. For a non-negative matrix W in a neural network and loss function E, we decompose the gradient into two positive parts \u2202E \u2202W\n= \u2207+ \u2212\u2207\u2212, where the two positive matrices\u2207+ and\u2207\u2212 can be computed by\n\u2207+ = 1\n2\n( abs ( \u2202E\n\u2202W\n) + \u2202E\n\u2202W\n) + \u01eb, (16)\n\u2207\u2212 = 1\n2\n( abs ( \u2202E\n\u2202W\n) \u2212 \u2202E\n\u2202W\n) + \u01eb, (17)\nwhere abs(\u00b7) is the element-wise absolute value and \u01eb is a small positive scalar applied to each element of a matrix. The multiplicative update algorithm is defined as\nW \u2190 W \u25e6\n( \u2207\u2212\n\u2207+\n)\u03b7 , (18)\nwhere \u03b7 is the learning rate hyperparameter and the division and the exponentiation are both elementwise. If W is initialized to be positive, the updated matrix will remain positive since all factors on the right hand side are positive.\nIn practice, the multiplicative update algorithm can be used in a stochastic (or mini-batch) manner. Such use has been demonstrated in non-negative matrix factorization [36]. Note that the multiplicative update still requires the gradients calculated by back-propagation. The gradient calculation for CAU can be found in Appendix."}, {"heading": "5 Related Work", "text": "A classic model related to the proposed CAU is the energy model for motion detection [1] and stereo disparity [9]. Each unit of the energy model computes the sum of squares of two Gabor filter outputs. No learning is involved in the energy model. There are models which compute the sum of squares of learnable filter outputs such as adaptive-subspace self-organized maps (ASSOM) [21] and independent subspace analysis (ISA) [16]. Similar to CAU, ASSOM also has a competition mechanism (WTA). However, the goal of both ASSOM and ISA is to learn appearance features which are invariant of image transformations, as different from our goal. There is a line of research on relation learning based on Boltzmann machines [30, 38, 15]. While Boltzmann machines allow a probabilistic formulation of relation inference, the training of Boltzmann machines is much more expensive compared to their non-probabilistic counterpart. Non-negative weights have appeared in sum-product networks [33], multi-layer perceptrons [6] and natural image statistics models [10]. Our derivation of the low-rank approximation of CAUs follows from the low-rank approximation of bilinear units [30, 19]."}, {"heading": "6 Experiments", "text": "We consider five fundamental image transformations for the relation learning tasks. For image x, its transformed image y is synthetically generated with ground-truth transformation parameters (or relation variables) z. For each task, neural networks are trained in a supervised manner, given x and y as inputs and z as targets. Additionally, we also evaluate the models, which are trained on synthetic data, on real world images with ground-truth transformation parameters. We describe the details below."}, {"heading": "6.1 Tasks", "text": "The five image transformations are: translation, rotation, scaling, affine transformation and projective transformation. They are called geometric transformations and can be unified as follows [11]. An image can be transformed (or warped) by changing its coordinates. For each point of an image with homogeneous coordinatesp = (p1, p2, 1), the transformed point is p\n\u2032 = Hp with homography matrix\nH = [ h11 h12 h13 h21 h22 h23 h31 h32 h33 ] . (19)\nThe type of the transformation depends on the parametrization of H. Note that translation, rotation and scaling are special cases of affine transformation and affine transformation is a special case of projective transformation. We list the parametrization in each task and the range of the transformation parameters in training data below in Table 1. For simplicity, we discard translation in affine and projective transformation."}, {"heading": "6.2 Data", "text": "We generate training and testing image patches from the gray-scaled CIFAR-10 dataset [24]. For each task and for each image in CIFAR-10, we apply an image transformation with z randomly\nsampled from uniform distributions to obtain an image pair. Then we crop an image patch of size 11\u00d711 at the center of each image of the image pair. Repeat the process 10 times. With this procedure, we obtain a training set of size 500,000 and a testing set of size 100,000 for the relation learning tasks.\nAdditionally, we use a real-world dataset2 with ground-truth homographies for evaluating the trained models in inferring projective transformations. We sample 100,000 pairs of corresponded image patches of size 11\u00d711 in the first two gray-scaled frames of four image sequences (Graffiti, wall, bark and boat). Given the homography H between two whole images, the homography between two corresponded image patches of location p = (p1, p2, 1) and p \u2032 = Hp = (p\u20321, p \u2032 2, 1) is H\u2032 = T\u2032HT\u22121, where T and T\u2032 are the translation matrices such that Tp = (0, 0, 1) and T\u2032p\u2032 = (0, 0, 1)."}, {"heading": "6.3 Models", "text": "We test three neural network models, each of which uses a different type of relation units: concatenation, bilinear and CAU. For all the models, f(\u00b7) is the identity function and g(\u00b7) is a multi-layer perceptron. We call the three neural network models, concatenation network (CTN), bilinear network (BLN) and contrast association network (CAN), respectively. For BLN and CAN, we use low-rank approximation of the bilinear units and CAUs, as described in Section 3.3. In BLN, we apply l2 normalization, which empirically performs better than softmax (or softmin), on the outputs of bilinear units. We also experimented with the non-negative constraint on the bilinear units but found it performs poorly and therefore discarded it. For fairness of the comparison, all three models are set to have essentially the same size in each task. To test the generality, all models are set to have the same size in different tasks, except that the last layer depends on the dimensionality of the targets z in the task. The models are specified in Table 2."}, {"heading": "6.4 Settings", "text": "All image patches are of size 11\u00d711 and are reshaped to vectors of size 121. The numerical range of each element of the vectors is normalized from [0, 255] to [\u22120.5, 0.5] by dividing 255 and then subtracting 0.5. This normalization significantly accelerates the training. We use the mean-squarederror (MSE) as the loss function. We use the multiplicative update algorithm (described in Section 4) for non-negative weights (U andV) and Adam [20] for unconstrained weights. For all models in all tasks, we use the same training hyperparameter setting. The initial learning rates are \u03b7 = 0.005 (multiplicative update) and \u03b1 = 0.005 (Adam). Both are multiplied by 0.95 for every 500 minibatch updates. The size of each mini-batch is 100. There are 200,000 mini-batch updates in total. \u01eb = 10\u221220 in the multiplicative update. We use MATLAB for generating the data and Torch for neural network training and testing.\n2 http://www.robots.ox.ac.uk/~vgg/research/affine/"}, {"heading": "6.5 Results", "text": "We use two measures of error to evaluate our results.\n\u2022 Parameter error. It is defined as the MSE between the ground-truth transformation parameters z and the inferred parameters z\u0302, that is, \u2016z \u2212 z\u0302\u20162. An advantage of parameter error is interpretability. For example, in inferring the rotation between two images, it is desirable that the inference error is measured in degrees. A disadvantage is that it is difficult to compare the inference error across tasks since different tasks have different range of transformation parameters.\n\u2022 Transformation error. As illustrated in Figure 3, define four points with homogeneous coordinates\np1 = (0, 0, 1), p2 = (1, 0, 1),p3 = (1, 1, 1), p4 = (0, 1, 1).\nLet p\u2032i = Hpi be the transformed point with the ground-truth homography H and let p\u0302\u2032i = H\u0302pi be the transformed point with the inferred homography H\u0302. The transformation error is defined as\n\u22114 i=1 \u2016p \u2032 i \u2212 p\u0302\n\u2032 i\u20162\u22114\nj=1 \u2016p \u2032 j\u20162\n. (20)\nIt is scale-invariant in the sense that the error is unchanged if we multiply H and H\u0302 by a non-zero constant. Therefore it is more suitable to compare this inference error across different tasks.\nThe results are listed in Table 3, with errors averaged over the testing sets. The last row in Table 3 stands for the result on the real-world dataset, as described in Section 6.2. We can see that CAN achieves the lowest errors in every task and the transformation error of CAN goes up when the complexity of the tasks is increased."}, {"heading": "7 Dicussion", "text": "In our present work, the experiments are limited to small image patches and to simple image transformations. It is not clear how well CAUs perform on whole images and more complex tasks. Future work includes extending CAUs to handle whole images and more complex tasks such as three-dimensional transformations.\nAppendix\nRank-One Approximation of CAU\nSinceWk = ukv T k , we have\nhk = 1\n2\n\u2211\nij\nWijk(ai \u2212 bj) 2 (21)\n= 1\n2\n\u2211\nij\nWijk(a 2 i + b 2 j)\u2212\n\u2211\nij\nWijkaibj (22)\n= 1\n2\n[ 1TWTk (a) 2 + 1TWk(b) 2 ] \u2212 aTWkb (23)\n= 1\n2\n[ 1Tvku T k (a) 2 + 1Tukv T k (b) 2 ] \u2212 aTukv T k b. (24)\nIn matrix form\nh = 1\n2\n[ (V1) \u25e6Ua2 + (U1) \u25e6Vb2 ] \u2212 (Ua) \u25e6 (Vb). (25)\nGradients of CAU\n\u2202E \u2202a =\n\u2211\nk\n\u2202E\n\u2202hk\n\u2202hk\n\u2202a ,\n\u2202E \u2202b =\n\u2211\nk\n\u2202E\n\u2202hk\n\u2202hk\n\u2202b , (26)\n\u2202hk\n\u2202a = (WTk 1) \u25e6 a\u2212Wkb, (27)\n\u2202hk\n\u2202b = (Wk1) \u25e6 b\u2212W\nT k a, (28)\n\u2202E\n\u2202Wk =\n\u2202E\n\u2202hk\n\u2202hk\n\u2202Wk , (29)\n\u2202hk\n\u2202Wk =\n1\n2\n[ (a2)1T + 1(b2)T ] \u2212 abT . (30)\nForWk = ukv T k ,\n\u2202E \u2202uk = \u2202E \u2202hk \u2202hk \u2202uk , \u2202E \u2202vk = \u2202E \u2202hk \u2202hk \u2202vk , (31) \u2202hk \u2202uk = 1 2 [ (vTk 1)a 2 + (vTk b 2)1 ] \u2212 (vTk b)a, (32) \u2202hk \u2202vk = 1 2 [ (uTk 1)b 2 + (uTk a 2)1 ] \u2212 (uTk a)b. (33)"}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "JOSA A,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Exploiting semantic information and deep matching for optical flow", "author": ["M. Bai", "W. Luo", "K. Kundu", "R. Urtasun"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Cnn-based patch matching for optical flow with thresholded hinge loss", "author": ["C. Bailer", "K. Varanasi", "D. Stricker"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning understandable neural networks with nonnegative weight constraints", "author": ["J. Chorowski", "J.M. Zurada"], "venue": "IEEE TNNLS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Neural encoding of binocular disparity: energy models, position shifts and phase shifts", "author": ["D.J. Fleet", "H. Wagner", "D.J. Heeger"], "venue": "Vision Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "A three-layer model of natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Physiology-Paris,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Multiple View Geometry in Computer Vision", "author": ["R. Hartley", "A. Zisserman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A parallel computation that assigns canonical object-based frames of reference", "author": ["G.F. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1981}, {"title": "Determining optical flow", "author": ["B.K. Horn", "B.G. Schunck"], "venue": "Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1981}, {"title": "Conditional high-order boltzmann machine: A supervised learning model for relation learning", "author": ["Y. Huang", "W. Wang", "L. Wang"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces", "author": ["A. Hyv\u00e4rinen", "P. Hoyer"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Flownet 2.0: Evolution of optical flow estimation with deep networks", "author": ["E. Ilg", "N. Mayer", "T. Saikia", "M. Keuper", "A. Dosovitskiy", "T. Brox"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Hadamard product for low-rank bilinear pooling", "author": ["J.-H. Kim", "K.-W. On", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map", "author": ["T. Kohonen"], "venue": "Biological Cybernetics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "Unsupervised learning of depth and motion", "author": ["K. Konda", "R. Memisevic"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Learning visual odometry with a convolutional network", "author": ["K.R. Konda", "R. Memisevic"], "venue": "VISAPP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Efficient deep learning for stereo matching", "author": ["W. Luo", "A.G. Schwing", "R. Urtasun"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning to relate images", "author": ["R. Memisevic"], "venue": "IEEE TPAMI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information", "author": ["B.A. Olshausen", "C.H. Anderson", "D.C. Van Essen"], "venue": "Journal of Neuroscience,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Optical flow estimation using a spatial pyramid network", "author": ["A. Ranjan", "M.J. Black"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Convolutional neural network architecture for geometric matching", "author": ["I. Rocco", "R. Arandjelovi\u0107", "J. Sivic"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Mini-batch stochastic approaches for accelerated multiplicative updates in nonnegative matrix factorisation with beta-divergence", "author": ["R. Serizel", "S. Essid", "G. Richard"], "venue": "MLSP,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Improved stereo matching with constant highway networks and reflective confidence learning", "author": ["A. Shaked", "L. Wolf"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "G. Hinton", "R. Memisevic", "M. Pollefeys"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Separating style and content with bilinear models", "author": ["J.B. Tenenbaum andW.T. Freeman"], "venue": "Neural Computation,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2000}, {"title": "Learning to extract motion from videos in convolutional neural networks", "author": ["D. Teney", "M. Hebert"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Fully-trainable deep matching", "author": ["J. Thewlis", "S. Zheng", "P.H. Torr", "A. Vedaldi"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Demon: Depth and motion network for learning monocular stereo", "author": ["B. Ummenhofer", "H. Zhou", "J. Uhrig", "N. Mayer", "E. Ilg", "A. Dosovitskiy", "T. Brox"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Accurate optical flow via direct cost volume processing", "author": ["J. Xu", "R. Ranftl", "V. Koltun"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2017}, {"title": "Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness", "author": ["J.J. Yu", "A.W. Harley", "K.G. Derpanis"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Stereo matching by training a convolutional neural network to compare image patches", "author": ["J. Zbontar", "Y. LeCun"], "venue": "JMLR, 2016", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "Neural networks, especially convolutional neural network (CNN) [26], have been successfully applied in many computer vision tasks such as object recognition [7, 25].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Neural networks, especially convolutional neural network (CNN) [26], have been successfully applied in many computer vision tasks such as object recognition [7, 25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 24, "context": "Neural networks, especially convolutional neural network (CNN) [26], have been successfully applied in many computer vision tasks such as object recognition [7, 25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 13, "context": "A classic example is the Horn-Schunck algorithm for computing optical flow [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "An alternative approach to solve the relation inference problem is learning-based [29].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "An example is the Sintel dataset for learning optical flow [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 39, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 2, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 40, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 33, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 16, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 79, "endOffset": 101}, {"referenceID": 27, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 140, "endOffset": 144}, {"referenceID": 22, "context": "Recently, the learning-based approach has been adopted to compute optical flow [8, 40, 3, 41, 34, 17], stereo disparity [28], camera motion [42] and visual odometry [23].", "startOffset": 165, "endOffset": 169}, {"referenceID": 1, "context": "Note that relation learning can also serve as supervision for learning appearance representation, as demonstrated in learning ego-motion [2, 18] and robot actions [32].", "startOffset": 137, "endOffset": 144}, {"referenceID": 17, "context": "Note that relation learning can also serve as supervision for learning appearance representation, as demonstrated in learning ego-motion [2, 18] and robot actions [32].", "startOffset": 137, "endOffset": 144}, {"referenceID": 31, "context": "Note that relation learning can also serve as supervision for learning appearance representation, as demonstrated in learning ego-motion [2, 18] and robot actions [32].", "startOffset": 163, "endOffset": 167}, {"referenceID": 44, "context": "Additionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43].", "startOffset": 88, "endOffset": 107}, {"referenceID": 45, "context": "Additionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43].", "startOffset": 88, "endOffset": 107}, {"referenceID": 36, "context": "Additionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43].", "startOffset": 88, "endOffset": 107}, {"referenceID": 3, "context": "Additionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43].", "startOffset": 88, "endOffset": 107}, {"referenceID": 42, "context": "Additionally, there are methods combining knowledge-based and learning-based approaches [45, 46, 37, 4, 43].", "startOffset": 88, "endOffset": 107}, {"referenceID": 29, "context": "Although a relation learning model can also be trained unsupervisedly [30, 22, 44], we restrict our discussion mainly to supervised learning.", "startOffset": 70, "endOffset": 82}, {"referenceID": 21, "context": "Although a relation learning model can also be trained unsupervisedly [30, 22, 44], we restrict our discussion mainly to supervised learning.", "startOffset": 70, "endOffset": 82}, {"referenceID": 43, "context": "Although a relation learning model can also be trained unsupervisedly [30, 22, 44], we restrict our discussion mainly to supervised learning.", "startOffset": 70, "endOffset": 82}, {"referenceID": 29, "context": "Figure 1: Neural network architecture for relation learning As illustrated in Figure 1, the general neural network architecture of many relation learning models [30, 8, 2] can be described as a = f(x), h = R(a,b), b = f(y), z = g(h), (2) where x and y are the inputs (images), z are the targets (relation variables), f(\u00b7) is the feature extraction units, R(\u00b7, \u00b7) is the relation units and g(\u00b7) is the readout units.", "startOffset": 161, "endOffset": 171}, {"referenceID": 7, "context": "Figure 1: Neural network architecture for relation learning As illustrated in Figure 1, the general neural network architecture of many relation learning models [30, 8, 2] can be described as a = f(x), h = R(a,b), b = f(y), z = g(h), (2) where x and y are the inputs (images), z are the targets (relation variables), f(\u00b7) is the feature extraction units, R(\u00b7, \u00b7) is the relation units and g(\u00b7) is the readout units.", "startOffset": 161, "endOffset": 171}, {"referenceID": 1, "context": "Figure 1: Neural network architecture for relation learning As illustrated in Figure 1, the general neural network architecture of many relation learning models [30, 8, 2] can be described as a = f(x), h = R(a,b), b = f(y), z = g(h), (2) where x and y are the inputs (images), z are the targets (relation variables), f(\u00b7) is the feature extraction units, R(\u00b7, \u00b7) is the relation units and g(\u00b7) is the readout units.", "startOffset": 161, "endOffset": 171}, {"referenceID": 29, "context": "For example, in [30], both f(\u00b7) and g(\u00b7) are the identity function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "For another example, in the simple version of FlowNet [8], f(\u00b7) is the identity function and g(\u00b7) is a CNN while in the complex version, both f(\u00b7) and g(\u00b7) are CNNs.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "Concatenation units have been used in learning optical flow [8] and ego-motion [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Concatenation units have been used in learning optical flow [8] and ego-motion [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 12, "context": "Bilinear units have been previously proposed and developed [13, 31, 39].", "startOffset": 59, "endOffset": 71}, {"referenceID": 30, "context": "Bilinear units have been previously proposed and developed [13, 31, 39].", "startOffset": 59, "endOffset": 71}, {"referenceID": 38, "context": "Bilinear units have been previously proposed and developed [13, 31, 39].", "startOffset": 59, "endOffset": 71}, {"referenceID": 29, "context": "Bilinear units have been used in learning image transformations in [30, 35].", "startOffset": 67, "endOffset": 75}, {"referenceID": 34, "context": "Bilinear units have been used in learning image transformations in [30, 35].", "startOffset": 67, "endOffset": 75}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Such use has been demonstrated in non-negative matrix factorization [36].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "A classic model related to the proposed CAU is the energy model for motion detection [1] and stereo disparity [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "A classic model related to the proposed CAU is the energy model for motion detection [1] and stereo disparity [9].", "startOffset": 110, "endOffset": 113}, {"referenceID": 20, "context": "There are models which compute the sum of squares of learnable filter outputs such as adaptive-subspace self-organized maps (ASSOM) [21] and independent subspace analysis (ISA) [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "There are models which compute the sum of squares of learnable filter outputs such as adaptive-subspace self-organized maps (ASSOM) [21] and independent subspace analysis (ISA) [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 29, "context": "There is a line of research on relation learning based on Boltzmann machines [30, 38, 15].", "startOffset": 77, "endOffset": 89}, {"referenceID": 37, "context": "There is a line of research on relation learning based on Boltzmann machines [30, 38, 15].", "startOffset": 77, "endOffset": 89}, {"referenceID": 14, "context": "There is a line of research on relation learning based on Boltzmann machines [30, 38, 15].", "startOffset": 77, "endOffset": 89}, {"referenceID": 32, "context": "Non-negative weights have appeared in sum-product networks [33], multi-layer perceptrons [6] and natural image statistics models [10].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Non-negative weights have appeared in sum-product networks [33], multi-layer perceptrons [6] and natural image statistics models [10].", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "Non-negative weights have appeared in sum-product networks [33], multi-layer perceptrons [6] and natural image statistics models [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "Our derivation of the low-rank approximation of CAUs follows from the low-rank approximation of bilinear units [30, 19].", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": "Our derivation of the low-rank approximation of CAUs follows from the low-rank approximation of bilinear units [30, 19].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "They are called geometric transformations and can be unified as follows [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "We generate training and testing image patches from the gray-scaled CIFAR-10 dataset [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "PReLU denotes the parametric ReLU activation function [12].", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "We use the multiplicative update algorithm (described in Section 4) for non-negative weights (U andV) and Adam [20] for unconstrained weights.", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Inferring the relations between two images is an important class of tasks in computer vision. Examples of such tasks include computing optical flow and stereo disparity. We treat the relation inference tasks as a machine learning problem and tackle it with neural networks. A key to the problem is learning a representation of relations. We propose a new neural network module, contrast association unit (CAU), which explicitly models the relations between two sets of input variables. Due to the non-negativity of the weights in CAU, we adopt a multiplicative update algorithm for learning these weights. Experiments show that neural networks with CAUs are more effective in learning five fundamental image transformations than conventional neural networks.", "creator": "LaTeX with hyperref package"}}}