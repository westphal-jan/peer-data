{"id": "1705.05823", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Real-Time Adaptive Image Compression", "abstract": "We present a machine learning approach to lossy image compression that surpasses all existing codecs while running in real time.", "histories": [["v1", "Tue, 16 May 2017 17:51:07 GMT  (8525kb,D)", "http://arxiv.org/abs/1705.05823v1", "Published at ICML 2017"]], "COMMENTS": "Published at ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["oren rippel", "lubomir d bourdev"], "accepted": true, "id": "1705.05823"}, "pdf": {"name": "1705.05823.pdf", "metadata": {"source": "META", "title": "Real-Time Adaptive Image Compression", "authors": ["Oren Rippel", "Lubomir Bourdev"], "emails": ["<oren@wave.one>,", "<lubomir@wave.one>."], "sections": [{"heading": null, "text": "Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG 2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of generic images across all quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in around 10ms per image on GPU.\nOur architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates."}, {"heading": "1. Introduction", "text": "Streaming of digital media makes 70% of internet traffic, and is projected to reach 80% by 2020 (CIS, 2015). However, it has been challenging for existing commercial compression algorithms to adapt to the growing demand and the changing landscape of requirements and applications. While digital media are transmitted in a wide variety of settings, the available codecs are \u201cone-size-fits-all\u201d: they are hard-coded, and cannot be customized to particular use cases beyond high-level hyperparameter tuning.\nIn the last few years, deep learning has revolutionized many tasks such as machine translation, speech recognition, face recognition, and photo-realistic image generation. Even though the world of compression seems a natural domain for machine learning approaches, it has not yet benefited from these advancements, for two main reasons. First,\n*Equal contribution 1WaveOne Inc., Mountain View, CA, USA. Correspondence to: Oren Rippel <oren@wave.one>, Lubomir Bourdev <lubomir@wave.one>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nour deep learning primitives, in their raw forms, are not well-suited to construct representations sufficiently compact. Recently, there have been a number of important efforts by Toderici et al. (2015; 2016), Theis et al. (2016), Balle\u0301 et al. (2016), and Johnston et al. (2017) towards alleviating this: see Section 2.2. Second, it is difficult to develop a deep learning compression approach sufficiently efficient for deployment in environments constrained by computation power, memory footprint and battery life.\nIn this work, we present progress on both performance and computational feasibility of ML-based image compression.\nOur algorithm outperforms all existing image compression approaches, both traditional and ML-based: it typically produces files 2.5 times smaller than JPEG and JPEG 2000 (JP2), 2 times smaller than WebP, and 1.7 times smaller than BPG on the Kodak PhotoCD and RAISE-1k 512\u00d7768 datasets across all of quality levels. At the same time, we designed our approach to be lightweight and efficiently deployable. On a GTX 980 Ti GPU, it takes around 9ms to encode and 10ms to decode an image from these datasets: for JPEG, encode/decode times are 18ms/12ms, for JP2 350ms/80ms and for WebP 70ms/80ms. Results for a representative quality level are presented in Table 1.\nTo our knowledge, this is the first ML-based approach to surpass all commercial image compression techniques, and moreover run in real-time.\nWe additionally supplement our algorithm with adversarial training specialized towards use in a compression setting. This enables us to produce convincing reconstructions for very low bitrates.\nar X\niv :1\n70 5.\n05 82\n3v 1\n[ st\nat .M\nL ]\n1 6\nM ay\n2 01\n7"}, {"heading": "2. Background & Related Work", "text": ""}, {"heading": "2.1. Traditional compression techniques", "text": "Compression, in general, is very closely related to pattern recognition. If we are able to discover structure in our input, we can eliminate this redundancy to represent it more succinctly. In traditional codecs such as JPEG and JP2, this is achieved via a pipeline which roughly breaks down into 3 modules: transformation, quantization, and encoding (Wallace (1992) and Rabbani & Joshi (2002) provide great overviews of the JPEG standards).\nIn traditional codecs, since all components are hard-coded, they are heavily engineered to fit together. For example, the coding scheme is custom-tailored to match the distribution of the outputs of the preceding transformation. JPEG, for instance, employs 8 \u00d7 8 block DCT transforms, followed by run-length encoding which exploits the sparsity pattern of the resultant frequency coefficients. JP2 employs an adaptive arithmetic coder to capture the distribution of coefficient magnitudes produced by the preceding multiresolution wavelet transform.\nHowever, despite the careful construction and assembly of\nthese pipelines, there still remains significant room for improvement of compression efficiency. For example, the transformation is fixed in place irrespective of the distribution of the inputs, and is not adapted to their statistics in any way. In addition, hard-coded approaches often compartmentalize the loss of information within the quantization step. As such, the transformation module is chosen to be bijective: however, this limits the ability to reduce redundancy prior to coding. Moreover, the encode-decode pipeline cannot be optimized for a particular metric beyond manual tweaking: even if we had the perfect metric for image quality assessment, traditional approaches cannot directly optimize their reconstructions for it."}, {"heading": "2.2. ML-based lossy image compression", "text": "In approaches based on machine learning, structure is automatically discovered, rather than manually engineered.\nOne of the first such efforts by Bottou et al. (1998), for example, introduced the DjVu format for document image compression, which employs techniques such as segmentation and K-means clustering separate foreground from background, and analyze the document\u2019s contents.\nAt a high level, one natural approach to implement the encoder-decoder image compression pipeline is to use an autoencoder to map the target through a bitrate bottleneck, and train the model to minimize a loss function penalizing it from its reconstruction. This requires carefully constructing a feature extractor and synthesizer for the encoder and decoder, selecting an appropriate objective, and possibly introducing a coding scheme to further compress the fixedsize representation to attain variable-length codes.\nMany of the existing ML-based image compression approaches (including ours) follow this general strategy. Toderici et al. (2015; 2016) explored various transformations for binary feature extraction based on different types of recurrent neural networks; the binary representations were then entropy-coded. Johnston et al. (2017) enabled another considerable leap in performance by introducing a loss weighted with SSIM (Wang et al., 2004), and spatiallyadaptive bit allocation. Theis et al. (2016) and Balle\u0301 et al. (2016) quantize rather than binarize, and propose strategies to approximate the entropy of the quantized representation: this provides them with a proxy to penalize it. Finally, Pied Piper has recently claimed to employ ML techniques in its Middle-Out algorithm (Judge et al., 2016), although their nature is shrouded in mystery."}, {"heading": "2.3. Generative Adversarial Networks", "text": "One of the most exciting innovations in machine learning in the last few years is the idea of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). The idea is to construct a generator network G\u03a6(\u00b7) whose goal is to synthesize outputs according to a target distribution ptrue, and a discriminator network D\u0398(\u00b7) whose goal is to distinguish between examples sampled from the ground truth distribution, and ones produced by the generator. This can be expressed concretely in terms of the minimax problem:\nmin \u03a6 max \u0398\nEx\u223cptrue logD\u0398(x) + Ez\u223cpz log [1\u2212D\u0398(G\u03a6(z))] .\nThis idea has enabled significant progress in photo-realistic image generation (Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016), single-image super-resolution\n(Ledig et al., 2016), image-to-image conditional translation (Isola et al., 2016), and various other important problems.\nThe adversarial training framework is particularly relevant to the compression world. In traditional codecs, distortions often take the form of blurriness, pixelation, and so on. These artifacts are unappealing, but are increasingly noticeable as the bitrate is lowered. We propose a multiscale adversarial training model to encourage reconstructions to match the statistics of their ground truth counterparts, resulting in sharp and visually pleasing results even for very low bitrates. As far as we know, we are the first to propose using GANs for image compression."}, {"heading": "3. Model", "text": "Our model architecture is shown in Figure 2, and comprises a number of components which we briefly outline below. In this section, we limit our focus to operations performed by the encoder: since the decoder simply performs the counterpart inverse operations, we only address exceptions which require particular attention.\nFeature extraction. Images feature a number of different types of structure: across input channels, within individual scales, and across scales. We design our feature extraction architecture to recognize these. It consists of a pyramidal decomposition which analyzes individual scales, followed by an interscale alignment procedure which exploits structure shared across scales.\nCode computation and regularization. This module is responsible for further compressing the extracted features. It quantizes the features, and encodes them via an adaptive arithmetic coding scheme applied on their binary expansions. An adaptive codelength regularization is introduced to penalize the entropy of the features, which the coding scheme exploits to achieve better compression.\nDiscriminator loss. We employ adversarial training to pursue realistic reconstructions. We dedicate Section 4 to describing our GAN formulation."}, {"heading": "3.1. Feature extraction", "text": ""}, {"heading": "3.1.1. PYRAMIDAL DECOMPOSITION", "text": "Our pyramidal decomposition encoder is loosely inspired by the use of wavelets for multiresolution analysis, in which an input is analyzed recursively via feature extraction and downsampling operators (Mallat, 1989). The JPEG 2000 standard, for example, employs discrete wavelet transforms with the Daubechies 9/7 kernels (Antonini et al., 1992; Rabbani & Joshi, 2002). This transform is in fact a linear operator, which can be entirely expressed via compositions of convolutions with only two hard-coded and separable 9\u00d79 filters applied irrespective of scale, and independently for each channel.\nThe idea of a pyramidal decomposition has been employed in machine learning: for instance, Mathieu et al. (2015) uses a pyramidal composition for next frame prediction, and Denton et al. (2015) uses it for image generation. The spectral representations of CNN activations have also been investigated by Rippel et al. (2015) to enable processing across a spectrum of scales, but this approach does not enable FIR processing as does wavelet analysis.\nWe generalize the wavelet decomposition idea to learn optimal, nonlinear extractors individually for each scale. Let us assume an input x to the model, and a total of M scales. We perform recursive analysis: let us denote xm as the input to scale m; we set the input to the first scale x1 = x as the input to the model. For each scale m, we perform two operations: first, we extract coefficients cm = fm(xm) \u2208 RCm\u00d7Hm\u00d7Wm via some parametrized function fm(\u00b7) for output channels Cm, height Hm and width Wm. Second, we compute the input to the next scale as xm+1 = Dm(xm) where Dm(\u00b7) is some downsampling operator (either fixed or learned).\nOur pyramidal decomposition architecture is illustrated in Figure 3. In practice, we extract across a total of M =\n6 scales. The feature extractors for the individual scales are composed of a sequence of convolutions with kernels 3 \u00d7 3 or 1 \u00d7 1 and ReLUs with a leak of 0.2. We learn all downsamplers as 4\u00d7 4 convolutions with a stride of 2."}, {"heading": "3.1.2. INTERSCALE ALIGNMENT", "text": "Interscale alignment is designed to leverage information shared across different scales \u2014 a benefit not offered by the classic wavelet analysis. It takes in as input the set of coefficients extracted from the different scales {cm}Mm=1 \u2282 RCm\u00d7Hm\u00d7Wm , and produces a single tensor of a target output dimensionality C \u00d7H \u00d7W .\nTo do this, we first map each input tensor cm to the target dimensionality via some parametrized function gm(\u00b7). This involves ensuring that this function spatially resamples cm to the appropriate output map sizeH\u00d7W , and outputs the appropriate number of channels C. We then sum gm(cm),m = 1, . . . ,M , and apply another parametrized non-linear transformation g(\u00b7) for joint processing.\nThe interscale alignment module can be seen in Figure 3. We denote its output as y. In practice, we choose each gm(\u00b7) as a convolution or a deconvolution with an appropriate stride to produce the target spatial map size H \u00d7W ; see Section 5.1 for a more detailed discussion. We choose g(\u00b7) simply as a sequence of 3\u00d7 3 convolutions."}, {"heading": "3.2. Code computation and regularization", "text": "Given the extracted tensor y \u2208 RC\u00d7H\u00d7W , we proceed to quantize it and encode it. This pipeline involves a number of components which we overview here and describe in detail throughout this section.\nQuantization. The tensor y is quantized to bit precision B:\ny\u0302 := QUANTIZEB(y) .\nBitplane decomposition. The quantized tensor y\u0302 is transformed into a binary tensor suitable for encoding via a lossless bitplane decomposition:\nb := BITPLANEDECOMPOSEB(y\u0302) \u2208 {0, 1}B\u00d7C\u00d7H\u00d7W .\nAdaptive arithmetic coding. The adaptive arithmetic coder (AAC) is trained to leverage the structure remaining in the data. It encodes b into its final variable-length binary sequence s of length `(s):\ns := AACENCODE(b) \u2208 {0, 1}`(s) .\nAdaptive codelength regularization. The adaptive codelength regularization (ACR) modulates the distribution of the quantized representation y\u0302 to achieve a target\nexpected bit count across inputs:\nEx[`(s)] \u2212\u2192 `target ."}, {"heading": "3.2.1. QUANTIZATION", "text": "Given a desired precision ofB bits, we quantize our feature tensor y into 2B equal-sized bins as\ny\u0302chw := QUANTIZEB(ychw) = 1 2B\u22121 \u2308 2B\u22121ychw \u2309 .\nFor the special case B = 1, this reduces exactly to a binary quantization scheme. While some ML-based approaches to compression employ such thresholding, we found better performance with the smoother quantization described. We quantize with B = 6 for all models in this paper."}, {"heading": "3.2.2. BITPLANE DECOMPOSITION", "text": "We decompose y\u0302 into bitplanes. This transformation maps each value y\u0302chw into its binary expansion ofB bits. Hence, each of the C spatial maps y\u0302c \u2208 RH\u00d7W of y\u0302 expands intoB binary bitplanes. We illustrate this transformation in Figure 4, and denote its output as b \u2208 {0, 1}B\u00d7C\u00d7H\u00d7W . This transformation is lossless.\nAs described in Section 3.2.3, this decomposition will enable our entropy coder to exploit structure in the distribution of the activations in y to achieve a compact representation. In Section 3.2.4, we introduce a strategy to encourage such exploitable structure to be featured."}, {"heading": "3.2.3. ADAPTIVE ARITHMETIC CODING", "text": "The output b of the bitplane decomposition is a binary tensor, which contains significant structure: for example, higher bitplanes are sparser, and spatially neighboring bits often have the same value (in Section 3.2.4 we propose a technique to guarantee presence of these properties). We exploit this low entropy by lossless compression via adaptive arithmetic coding.\nNamely, we associate each bit location in b with a context, which comprises a set of features indicative of the bit value. These are based on the position of the bit as well as the values of neighboring bits. We train a classifier to predict the value of each bit from its context features, and then use these probabilities to compress b via arithmetic coding.\nDuring decoding, we decompress the code by performing the inverse operation. Namely, we interleave between computing the context of a particular bit using the values of previously decoded bits, and using this context to retrieve the activation probability of the bit and decode it. We note that this constrains the context of each bit to only include features composed of bits already decoded."}, {"heading": "3.2.4. ADAPTIVE CODELENGTH REGULARIZATION", "text": "One problem with classic autoencoder architectures is that their bottleneck has fixed capacity. The bottleneck may be too small to represent complex patterns well, which affects quality, and it may be too large for simple patterns, which results in inefficient compression. What we need is a model capable of generating long representations for complex patterns and short for simple ones, while maintaining an expected codelength target over large number of examples. To achieve this, the AAC is necessary, but not sufficient.\nWe extend the architecture by increasing the dimensionality of b \u2014 but at the same time controlling its information content, thereby resulting in shorter compressed code s = AACENCODE(b) \u2208 {0, 1}. Specifically, we introduce the adaptive codelength regularization (ACR), which enables us to regulate the expected codelength Ex[`(s)] to a target value `target. This penalty is designed to encourage structure exactly where the AAC is able to exploit it. Namely, we regularize our quantized tensor y\u0302 with\nP(y\u0302) = \u03b1t\nCHW \u2211 chw { log2 |y\u0302chw|\n+ \u2211\n(x,y)\u2208S\nlog2 \u2223\u2223y\u0302chw \u2212 y\u0302c(h\u2212y)(w\u2212x)\u2223\u2223 } ,\nfor iteration t and difference index set S = {(0, 1), (1, 0), (1, 1), (\u22121, 1)}. The first term penalizes the magnitude of each tensor element, and the second penalizes deviations between spatial neighbors. These enable better prediction by the AAC.\nAs we train our model, we continuously modulate the scalar coefficient \u03b1t to pursue our target codelength. We do this via a feedback loop. We use the AAC to monitor the mean number of effective bits. If it is too high, we increase \u03b1t; if too low, we decrease it. In practice, the model reaches an equilibrium in a few hundred iterations, and is able to maintain it throughout training.\nHence, we get a knob to tune: the ratio of total bits, namely the BCHW bits available in b, to the target number of effective bits `target. This allows exploring the trade-off of increasing the number of channels or spatial map size of\nb at the cost of increasing sparsity. We find that a totalto-target ratio of BCHW/`target = 4 works well across all architectures we have explored."}, {"heading": "4. Realistic Reconstructions via Multiscale Adversarial Training", "text": ""}, {"heading": "4.1. Discriminator design", "text": "In our compression approach, we take the generator as the encoder-decoder pipeline, to which we append a discriminator \u2014 albeit with a few key differences from existing GAN formulations.\nIn many GAN approaches featuring both a reconstruction and a discrimination loss, the target and the reconstruction are treated independently: each is separately assigned a label indicating whether it is real or fake. In our formulation, we consider the target and its reconstruction jointly as a single example: we compare the two by asking which of the two images is the real one.\nTo do this, we first swap between the target and reconstruction in each input pair to the discriminator with uniform probability. Following the random swap, we propagate each set of examples through the network. However, instead of producing an output for classification at the\nvery last layer of the pipeline, we accumulate scalar outputs along branches constructed along it at different depths. We average these to attain the final value provided to the terminal sigmoid function. This multiscale architecture allows aggregating information across different scales, and is motivated by the observation that undesirable artifacts vary as function of the scale in which they are exhibited. For example, high-frequency artifacts such as noise and blurriness are discovered by earlier scales, whereas more abstract discrepancies are found in deeper scales.\nWe apply our discriminator D\u0398 on the aggregate sum across scales, and proceed to formulate our objectives as described in Section 2.3. The complete discriminator architecture is illustrated in Figure 10."}, {"heading": "4.2. Adversarial training", "text": "Training a GAN system can be tricky due to optimization instability. In our case, we were able to address this by designing a training scheme adaptive in two ways. First, the reconstructor is trained by both the confusion signal gradient as well as the reconstruction loss gradient: we balance the two as function of their gradient magnitudes. Second, at any point during training, we either train the discriminator or propagate confusion signal through the reconstructor, as function of the prediction accuracy of the discriminator.\nMore concretely, given lower and upper accuracy bounds L,U \u2208 [0, 1] and discriminator accuracy a(D\u0398), we apply the following procedure:\n\u2022 If a < L: freeze propagation of confusion signal through the reconstructor, and train the discriminator.\n\u2022 If L \u2264 a < U : alternate between propagating confusion signal and training the disciminator.\n\u2022 If U \u2264 a: propagate confusion signal through the reconstructor, and freeze the discriminator.\nIn practice we used L = 0.8, U = 0.95. We compute the accuracy a as a running average over mini-batches with a momentum of 0.8."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Experimental setup", "text": "Similarity metric. We trained and tested all models on the Multi-Scale Structural Similarity Index Metric (MSSSIM) (Wang et al., 2003). This metric has been specifically designed to match the human visual system, and has been established to be significantly more representative than losses in the `p family and variants such as PSNR.\nColor space. Since the human visual system is much more sensitive to variations in brightness than color, most codecs represent colors in the YCbCr color space to devote more bandwidth towards encoding luma rather than chroma. In quantifying image similarity, then, it is common to assign the Y, Cb, Cr components weights 6/8, 1/8, 1/8. While many ML-based compression papers evaluate similarity in the RGB space with equal color weights, this does not allow fair comparison with standard codecs such as JPEG, JPEG 2000 and WebP, since they have not been designed to perform optimally in this domain. In this work, we provide comparisons with both traditional and ML-based codecs, and present results in both the RGB domain with equal color weights, as well as in YCbCr with weights as above.\nReported performance metrics. We present both compression performance of our algorithm, but also its runtime. While the requirement of running the approach in real-time severely constrains the capacity of the model, it must be met to enable feasible deployment in real-life applications.\nTraining and deployment procedure. We trained and tested all models on a GeForce GTX 980 Ti GPU and a custom codebase. We trained all models on 128\u00d7 128 patches sampled at random from the Yahoo Flickr Creative Com-\nmons 100 Million dataset (Thomee et al., 2016).\nWe optimized all models with Adam (Kingma & Ba, 2014). We used an initial learning rate of 3 \u00d7 10\u22124, and reduced it twice by a factor of 5 during training. We chose a batch size of 16 and trained each model for a total of 400,000 iterations. We initialized the ACR coefficient as \u03b10 = 1. During runtime we deployed the model on arbitrarily-sized images in a fully-convolutional way. To attain the ratedistortion (RD)curves presented in Section 5.2, we trained models for a range of target bitrates `target."}, {"heading": "5.2. Performance", "text": "We present several types of results:\n1. Average MS-SSIM as function of the BPP fixed for each image, found in Figures 5 and 6, and Table 1.\n2. Average compressed file sizes relative to ours as function of the MS-SSIM fixed for each image, found in Figures 5 and 6, and Table 1.\n3. Encode and decode timings as function of MS-SSIM, found in Figure 7, in the appendix, and Table 1.\n4. Visual examples of reconstructions of different compression approaches for the same BPP, found in Figure 1 and in the appendix.\nTest sets. To enable comparison with other approaches, we first present performance on the Kodak PhotoCD dataset1. While the Kodak dataset is very popular for testing compression performance, it contains only 24 images, and hence is susceptible to overfitting and does not necessarily fully capture broader statistics of natural images. As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images. We resized each image to size 512\u00d7 768 (backwards if vertical): we intend to release our preparation code to enable reproduction of the dataset used.\nWe remark it is important to use a dataset of raw, rather than previously compressed, images for codec evaluation.\n1The Kodak PhotoCD dataset can be found at http:// r0k.us/graphics/kodak.\n2The results of Toderici et al. (2016) on the Kodak RGB dataset are available at http://github.com/ tensorflow/models/tree/master/compression.\n3We have no access to reconstructions by Theis et al. (2016) and Johnston et al. (2017), so we carefully transcribed their results, only available in RGB, from the graphs in their paper.\n4Reconstructions by Balle\u0301 et al. (2016) of images in the Kodak dataset can be found at http://www.cns.nyu.edu/ \u02dclcv/iclr2017/ for both RGB and YCbCr and across a spectrum of BPPs. We use these to compute RD curves by the procedure described in this section.\n5An implementation of the BPG codec is available at http: //bellard.org/bpg.\nCompressing an image introduces artifacts with a bias particular to the codec used, which results in a more favorable RD curve if it compressed again with the same codec. See Figure 9 for a plot demonstrating this effect.\nCodecs. We compare against commercial compression techniques JPEG, JPEG 2000, WebP, as well as recent MLbased compression work by Toderici et al. (2016)2, Theis et al. (2016)3, Balle\u0301 et al. (2016)4, and Johnston et al. (2017)3 in all settings in which results are available. We also compare to BPG5 (4:2:0 and 4:4:4) which, while not widely used, surpassed all other codecs in the past. We use the best-performing configuration we can find of JPEG, JPEG 2000, WebP, and BPG, and reduce their bitrates by their respective header lengths for fair comparison.\nPerformance evaluation. For each image in each test set, each compression approach, each color space, and for the selection of available compression rates, we recorded (1) the BPP, (2) the MS-SSIM (with components weighted appropriately for the color space), and (3) the computation times for encoding and decoding.\nIt is important to take great care in the design of the performance evaluation procedure. Each image has a separate RD curve computed from all available compression rates for a given codec: as Balle\u0301 et al. (2016) discusses in detail, different summaries of these RD curves lead to disparate results. In our evaluations, to compute a given curve, we sweep across values of the independent variable (such as bitrate). We interpolate each individual RD curve at this independent variable value, and average all the results. To ensure accurate interpolation, we sample densely across rates for each codec.\nAcknowledgements We are grateful to Trevor Darrell, Sven Strohband, Michael Gelbart, Robert Nishihara, Albert Azout, and Vinod Khosla for meaningful discussions and input."}], "references": [{"title": "Image coding using wavelet transform", "author": ["Antonini", "Marc", "Barlaud", "Michel", "Mathieu", "Pierre", "Daubechies", "Ingrid"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Antonini et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Antonini et al\\.", "year": 1992}, {"title": "End-to-end optimized image compression", "author": ["Ball\u00e9", "Johannes", "Laparra", "Valero", "Simoncelli", "Eero P"], "venue": null, "citeRegEx": "Ball\u00e9 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ball\u00e9 et al\\.", "year": 2016}, {"title": "High quality document image compression with djvu", "author": ["Bottou", "L\u00e9on", "Haffner", "Patrick", "Howard", "Paul G", "Simard", "Patrice", "Bengio", "Yoshua", "LeCun", "Yann"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1998}, {"title": "Raise: a raw images dataset for digital image forensics", "author": ["Dang-Nguyen", "Duc-Tien", "Pasquini", "Cecilia", "Conotter", "Valentina", "Boato", "Giulia"], "venue": "In Proceedings of the 6th ACM Multimedia Systems Conference,", "citeRegEx": "Dang.Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dang.Nguyen et al\\.", "year": 2015}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily L", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Isola", "Phillip", "Zhu", "Jun-Yan", "Zhou", "Tinghui", "Efros", "Alexei A"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "Isola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2016}, {"title": "Silicon valley (tv series)", "author": ["Judge", "Mike", "Altschuler", "John", "Krinsky", "Dave"], "venue": null, "citeRegEx": "Judge et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Judge et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A theory for multiresolution signal decomposition: The wavelet representation", "author": ["S.G. Mallat"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Mallat,? \\Q1989\\E", "shortCiteRegEx": "Mallat", "year": 1989}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Mathieu", "Michael", "Couprie", "Camille", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "An overview of the jpeg 2000 still image compression standard", "author": ["Rabbani", "Majid", "Joshi", "Rajan"], "venue": "Signal processing: Image communication,", "citeRegEx": "Rabbani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rabbani et al\\.", "year": 2002}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Spectral representations for convolutional neural networks", "author": ["Rippel", "Oren", "Snoek", "Jasper", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rippel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Lossy image compression with compressive autoencoders", "author": ["Theis", "Lucas", "Shi", "Wenzhe", "Cunningham", "Andrew", "Huszar", "Ferenc"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["Thomee", "Bart", "Shamma", "David A", "Friedland", "Gerald", "Elizalde", "Benjamin", "Ni", "Karl", "Poland", "Douglas", "Borth", "Damian", "Li", "Li-Jia"], "venue": "Communications of the ACM,", "citeRegEx": "Thomee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thomee et al\\.", "year": 2016}, {"title": "Variable rate image compression with recurrent neural networks", "author": ["Toderici", "George", "O\u2019Malley", "Sean M", "Hwang", "Sung Jin", "Vincent", "Damien", "Minnen", "David", "Baluja", "Shumeet", "Covell", "Michele", "Sukthankar", "Rahul"], "venue": "arXiv preprint arXiv:1511.06085,", "citeRegEx": "Toderici et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2015}, {"title": "Full resolution image compression with recurrent neural networks", "author": ["Toderici", "George", "Vincent", "Damien", "Johnston", "Nick", "Hwang", "Sung Jin", "Minnen", "David", "Shor", "Joel", "Covell", "Michele"], "venue": "arXiv preprint arXiv:1608.05148,", "citeRegEx": "Toderici et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toderici et al\\.", "year": 2016}, {"title": "The jpeg still picture compression standard", "author": ["Wallace", "Gregory K"], "venue": "IEEE transactions on consumer electronics,", "citeRegEx": "Wallace and K.,? \\Q1992\\E", "shortCiteRegEx": "Wallace and K.", "year": 1992}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Wang", "Zhou", "Simoncelli", "Eero P", "Bovik", "Alan C"], "venue": "In Signals, Systems and Computers,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan C", "Sheikh", "Hamid R", "Simoncelli", "Eero P"], "venue": "IEEE transactions on image processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 14, "context": "(2015; 2016), Theis et al. (2016), Ball\u00e9 et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 1, "context": "(2016), Ball\u00e9 et al. (2016), and Johnston et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "(2016), Ball\u00e9 et al. (2016), and Johnston et al. (2017) towards alleviating this: see Section 2.", "startOffset": 8, "endOffset": 56}, {"referenceID": 2, "context": "One of the first such efforts by Bottou et al. (1998), for example, introduced the DjVu format for document image compression, which employs techniques such as segmentation and K-means clustering separate foreground from background, and analyze the document\u2019s contents.", "startOffset": 33, "endOffset": 54}, {"referenceID": 20, "context": "(2017) enabled another considerable leap in performance by introducing a loss weighted with SSIM (Wang et al., 2004), and spatiallyadaptive bit allocation.", "startOffset": 97, "endOffset": 116}, {"referenceID": 7, "context": "Finally, Pied Piper has recently claimed to employ ML techniques in its Middle-Out algorithm (Judge et al., 2016), although their nature is shrouded in mystery.", "startOffset": 93, "endOffset": 113}, {"referenceID": 5, "context": "Generative Adversarial Networks One of the most exciting innovations in machine learning in the last few years is the idea of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).", "startOffset": 165, "endOffset": 190}, {"referenceID": 13, "context": "Toderici et al. (2015; 2016) explored various transformations for binary feature extraction based on different types of recurrent neural networks; the binary representations were then entropy-coded. Johnston et al. (2017) enabled another considerable leap in performance by introducing a loss weighted with SSIM (Wang et al.", "startOffset": 0, "endOffset": 222}, {"referenceID": 12, "context": "Theis et al. (2016) and Ball\u00e9 et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "(2016) and Ball\u00e9 et al. (2016) quantize rather than binarize, and propose strategies to approximate the entropy of the quantized representation: this provides them with a proxy to penalize it.", "startOffset": 11, "endOffset": 31}, {"referenceID": 4, "context": "This idea has enabled significant progress in photo-realistic image generation (Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016), single-image super-resolution (Ledig et al.", "startOffset": 79, "endOffset": 145}, {"referenceID": 12, "context": "This idea has enabled significant progress in photo-realistic image generation (Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016), single-image super-resolution (Ledig et al.", "startOffset": 79, "endOffset": 145}, {"referenceID": 14, "context": "This idea has enabled significant progress in photo-realistic image generation (Denton et al., 2015; Radford et al., 2015; Salimans et al., 2016), single-image super-resolution (Ledig et al.", "startOffset": 79, "endOffset": 145}, {"referenceID": 6, "context": ", 2016), image-to-image conditional translation (Isola et al., 2016), and various other important problems.", "startOffset": 48, "endOffset": 68}, {"referenceID": 9, "context": "PYRAMIDAL DECOMPOSITION Our pyramidal decomposition encoder is loosely inspired by the use of wavelets for multiresolution analysis, in which an input is analyzed recursively via feature extraction and downsampling operators (Mallat, 1989).", "startOffset": 225, "endOffset": 239}, {"referenceID": 0, "context": "The JPEG 2000 standard, for example, employs discrete wavelet transforms with the Daubechies 9/7 kernels (Antonini et al., 1992; Rabbani & Joshi, 2002).", "startOffset": 105, "endOffset": 151}, {"referenceID": 0, "context": "The JPEG 2000 standard, for example, employs discrete wavelet transforms with the Daubechies 9/7 kernels (Antonini et al., 1992; Rabbani & Joshi, 2002). This transform is in fact a linear operator, which can be entirely expressed via compositions of convolutions with only two hard-coded and separable 9\u00d79 filters applied irrespective of scale, and independently for each channel. The idea of a pyramidal decomposition has been employed in machine learning: for instance, Mathieu et al. (2015) uses a pyramidal composition for next frame prediction, and Denton et al.", "startOffset": 106, "endOffset": 494}, {"referenceID": 0, "context": "The JPEG 2000 standard, for example, employs discrete wavelet transforms with the Daubechies 9/7 kernels (Antonini et al., 1992; Rabbani & Joshi, 2002). This transform is in fact a linear operator, which can be entirely expressed via compositions of convolutions with only two hard-coded and separable 9\u00d79 filters applied irrespective of scale, and independently for each channel. The idea of a pyramidal decomposition has been employed in machine learning: for instance, Mathieu et al. (2015) uses a pyramidal composition for next frame prediction, and Denton et al. (2015) uses it for image generation.", "startOffset": 106, "endOffset": 575}, {"referenceID": 0, "context": "The JPEG 2000 standard, for example, employs discrete wavelet transforms with the Daubechies 9/7 kernels (Antonini et al., 1992; Rabbani & Joshi, 2002). This transform is in fact a linear operator, which can be entirely expressed via compositions of convolutions with only two hard-coded and separable 9\u00d79 filters applied irrespective of scale, and independently for each channel. The idea of a pyramidal decomposition has been employed in machine learning: for instance, Mathieu et al. (2015) uses a pyramidal composition for next frame prediction, and Denton et al. (2015) uses it for image generation. The spectral representations of CNN activations have also been investigated by Rippel et al. (2015) to enable processing across a spectrum of scales, but this approach does not enable FIR processing as does wavelet analysis.", "startOffset": 106, "endOffset": 705}, {"referenceID": 15, "context": "We compare against commercial codecs JPEG, JPEG 2000, WebP and BPG (4:2:0 for YCbCr and 4:4:4 for RGB), as well as recent MLbased compression work by Toderici et al. (2016), Theis et al.", "startOffset": 150, "endOffset": 173}, {"referenceID": 14, "context": "(2016), Theis et al. (2016), Ball\u00e9 et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "(2016), Ball\u00e9 et al. (2016), and Johnston et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "(2016), Ball\u00e9 et al. (2016), and Johnston et al. (2017) in all settings where results exist.", "startOffset": 8, "endOffset": 56}, {"referenceID": 16, "context": "mons 100 Million dataset (Thomee et al., 2016).", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images.", "startOffset": 69, "endOffset": 95}, {"referenceID": 2, "context": "As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images. We resized each image to size 512\u00d7 768 (backwards if vertical): we intend to release our preparation code to enable reproduction of the dataset used. We remark it is important to use a dataset of raw, rather than previously compressed, images for codec evaluation. The Kodak PhotoCD dataset can be found at http:// r0k.us/graphics/kodak. The results of Toderici et al. (2016) on the Kodak RGB dataset are available at http://github.", "startOffset": 70, "endOffset": 505}, {"referenceID": 2, "context": "As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images. We resized each image to size 512\u00d7 768 (backwards if vertical): we intend to release our preparation code to enable reproduction of the dataset used. We remark it is important to use a dataset of raw, rather than previously compressed, images for codec evaluation. The Kodak PhotoCD dataset can be found at http:// r0k.us/graphics/kodak. The results of Toderici et al. (2016) on the Kodak RGB dataset are available at http://github.com/ tensorflow/models/tree/master/compression. We have no access to reconstructions by Theis et al. (2016) and Johnston et al.", "startOffset": 70, "endOffset": 669}, {"referenceID": 2, "context": "As such, we additionally present performance on the RAISE-1k dataset (Dang-Nguyen et al., 2015) which contains 1,000 raw images. We resized each image to size 512\u00d7 768 (backwards if vertical): we intend to release our preparation code to enable reproduction of the dataset used. We remark it is important to use a dataset of raw, rather than previously compressed, images for codec evaluation. The Kodak PhotoCD dataset can be found at http:// r0k.us/graphics/kodak. The results of Toderici et al. (2016) on the Kodak RGB dataset are available at http://github.com/ tensorflow/models/tree/master/compression. We have no access to reconstructions by Theis et al. (2016) and Johnston et al. (2017), so we carefully transcribed their results, only available in RGB, from the graphs in their paper.", "startOffset": 70, "endOffset": 696}, {"referenceID": 1, "context": "Reconstructions by Ball\u00e9 et al. (2016) of images in the Kodak dataset can be found at http://www.", "startOffset": 19, "endOffset": 39}, {"referenceID": 15, "context": "We compare against commercial compression techniques JPEG, JPEG 2000, WebP, as well as recent MLbased compression work by Toderici et al. (2016)2, Theis et al.", "startOffset": 122, "endOffset": 145}, {"referenceID": 14, "context": "(2016)2, Theis et al. (2016)3, Ball\u00e9 et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "(2016)3, Ball\u00e9 et al. (2016)4, and Johnston et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "(2016)3, Ball\u00e9 et al. (2016)4, and Johnston et al. (2017)3 in all settings in which results are available.", "startOffset": 9, "endOffset": 58}, {"referenceID": 1, "context": "Each image has a separate RD curve computed from all available compression rates for a given codec: as Ball\u00e9 et al. (2016) discusses in detail, different summaries of these RD curves lead to disparate results.", "startOffset": 103, "endOffset": 123}], "year": 2017, "abstractText": "Streaming of digital media makes 70% of internet traffic, and is projected to reach 80% by 2020 (CIS, 2015). However, it has been challenging for existing commercial compression algorithms to adapt to the growing demand and the changing landscape of requirements and applications. While digital media are transmitted in a wide variety of settings, the available codecs are \u201cone-size-fits-all\u201d: they are hard-coded, and cannot be customized to particular use cases beyond high-level hyperparameter tuning.", "creator": "LaTeX with hyperref package"}}}