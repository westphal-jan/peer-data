{"id": "1705.06460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "Evolving Ensemble Fuzzy Classifier", "abstract": "The concept of ensemble learning offers a promising opportunity to learn from data flows in complex environments because it addresses the bias-and-variance dilemma better than its single-model counterpart and has a reconfigurable structure that adapts well to the given context. While various extensions of ensemble learning for the degradation of non-stationary data flows can be found in the literature, most of them are created under a static base classifier and visit previous samples in the sliding window for a retraining step. This feature causes mathematically prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Its complexity is often challenging because it includes a large collection of offline classifiers due to the lack of structural complexity reduction mechanisms and the absence of an online feature selection mechanism. A novel, evolving ensemble ensemble, namely the parficatory, is proposed in this paper.", "histories": [["v1", "Thu, 18 May 2017 08:19:41 GMT  (1053kb)", "http://arxiv.org/abs/1705.06460v1", "this paper is currently submitted for possible publication in IEEE"]], "COMMENTS": "this paper is currently submitted for possible publication in IEEE", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mahardhika pratama", "witold pedrycz", "edwin lughofer"], "accepted": false, "id": "1705.06460"}, "pdf": {"name": "1705.06460.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "avenue in learning from data streams under complex environments because it addresses the bias and variance dilemma better than its single-model counterpart and features a reconfigurable structure, which is well-suited to the given context. While various extensions of ensemble learning for mining nonstationary data streams can be found in the literature, most of them are crafted under a static base-classifier and revisits preceding samples in the sliding window for a retraining step. This feature causes computationally prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Their complexities are often demanding because it involves a large collection of offline classifiers due to the absence of structural complexities reduction mechanisms and lack of an online feature selection mechanism. A novel evolving ensemble classifier, namely Parsimonious Ensemble (pENsemble), is proposed in this paper. pENsemble differs from existing architectures in the fact that it is built upon an evolving classifier from data streams, termed Parsimonious Classifier (pClass). pENsemble is equipped by an ensemble pruning mechanism, which estimates a localized generalization error of a base-classifier. A dynamic online feature selection scenario is integrated into the pENsemble. This method allows for dynamic selection and deselection of input features on the fly. pENsemble adopts a dynamic ensemble structure to output a final classification decision where it features a novel drift detection scenario to grow the ensemble\u2019s structure. The efficacy of the pENsemble has been numerically demonstrated through rigorous numerical studies with dynamic and evolving data streams where it delivers the most encouraging performance in attaining a tradeoff between accuracy and complexity. Index Terms\u2014 Fuzzy Neural Network, Evolving Fuzzy Systems, Ensemble Classifier, Data Streams, Online Learning, Concept Drift.\nI. INTRODUCTION The data-intensive era where data are collected continuously in a fast rate under dynamic and evolving environments opens a new research direction to process data streams efficiently [1], [2]. Unlike a classical paradigm in machine learning where a dataset is utilised to construct hypothesis and is executed over multiple passes, data streams requires a strictly online learning framework with a low memory requirement and even if possible with no memory at all \u2013 one-pass learning mode. Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5]. These facts make a retraining phase when incorporating a new sample to an old dataset impossible to be performed because it leads to the socalled catastrophic forgetting [6] of previously valid knowledge and is not scalable when dealing with massive data streams.\nEvolving Intelligent System (EIS) provides a unique solution for data stream mining because a strictly one-pass learning procedure involved here has delivered great success to cope with time-critical applications where data streams are generated at a very fast sampling rate [7]. Furthermore, EIS adopts an open structure where its components can be automatically generated, pruned, merged and recalled on the fly [8], [9] and can be well-suited to a given problem. This trait reflects the true data distributions and tracks changing data distributions [10]. EIS has transformed to be one of the most active research area in the computational intelligence research as evidenced by the number of published works in this area [71].\nNonetheless, EIS is typically built upon a single classifier architecture which often does not produce adequate accuracy for complex problems [11], [35]. In fact, from classical batch learning perspective, it is well-known that ensemble classifiers outperform single base classifiers in case of high noise levels and a low number of available training samples [12] because they can better resolve the bias-variance dilemma due to proper subspace and data exploration using weak classifiers [13]. While few works about a synergy between EIS and an ensemble structure can be found in the literature [14], [15], most of them utilise a static ensemble architecture, which should be predetermined in advance. Although diversity of base classifiers can be guaranteed by varying user-defined parameters or applying different data partitions to base classifiers, the issue of concept drifts remains an open challenge because of their fixed structure.\nThe ensemble learning concept uses combination of individual base classifiers with a modularity principle, where it enables a dynamic evolution of the ensemble structure [12]- [19]. The key of ensemble learning lies in the diversity of base classifiers, which makes them more robust to various forms of uncertainty in data streams (such as significant noise levels). Nonetheless, one must bear in mind that the diversity of an ensemble classifier might be counterproductive in realm of data streams because it opens the door for outdated base-classifiers in the ensemble structure. Adaptability of the ensemble classifier plays a vital role to the success of ensemble learning because it formulates mechanisms how an ensemble classifier adapts itself when changing data distributions are presented [18]. The ensemble classifier can also be distinguished into two groups: active and passive approach: the passive approach relies on continuous updates of its components and assumes that the concept drifts occur in the ongoing fashion; the active approach is equipped by a dedicated drift detection mechanism in which it is restructured and parameters are fine-tuned when a drift is captured [19]. In practise, the drift detection mechanism plays key to role to alert operators for possible changing system behaviours and to identify whether a change causes catastrophic effect to operation\u2019s cycle \u2013 vital for process\u2019s safety.\nTo the best of our knowledge, local concept drift, curse of dimensionality, and structural complexity are three open issues in the current literatures. In case of local concept drift, changes do not ensue in the whole feature space rather in some local regions only with different rates and severities [20] [21]. It remains an open question because existing ensemble classifiers are mostly constructed using a batch classifier or accumulate already seen samples in the sliding window for retraining steps and considers only the global change in data distribution. Although ensemble algorithms like DELA [16] is excluded from the local concept drift bottleneck due to its three levels of adaptivity, namely structural adaptivity, combination adaptivity, model adaptivity, it suffers from the absence of a dedicated drift detection method [16]. Furthermore, the structural complexities of existing ensemble classifiers are considerable because they usually involve a large number of base classifiers to assure acceptable accuracy. Most of them suffer from the absence of a structural complexity reduction\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\nmechanism which alleviates complexities of ensemble classifiers [22]. Existing ensemble classifiers also assume that input features are pre-selected in the pre-processing steps. This issue hinders its viability in the time-critical applications where data streams are generated continuously in a fast sampling rate which makes an iterative pre-processing step impractical. Furthermore, pre-recorded data are often irrelevant in the later stage because of rapidly changing environments.\nA novel ensemble learning algorithm, namely Parsimonious Ensemble (pENsemble), is proposed in this paper. pENsemble features an open structure where a local expert is created and pruned dynamically under strictly one-pass learning mode. It is constructed with a recently published evolving classifier, namely Parsimonious Classifier (pClass) [24]. An evolving classifier strengthens the adaptive nature of evolving ensemble because it handles a local concept drift better than a classical batch classifier with its dynamic and online paradigm. It features an open structure paradigm which is self-evolving to track variations in the local data space. pENsemble works fully in the single-pass learning mode, which is well-suited to the online life-long learning scenario. pENsemble is also equipped with a dynamic feature selection scenario which can address a high input dimensionality and to the best of our knowledge is absent from the majority of existing ensemble classifiers. The final class prediction of pENsemble is inferred by a dynamic ensemble paradigm [25] which dynamically grow, shrink and adjust the weights of local experts to data streams. The dynamic ensemble concept is inspired by the evolving trait of DWM [34] but different criteria are applied to perform the structural learning scenarios of pENsemble. pENsemble puts forward three new learning components as follows:\n\u2022 Online Drift Detection Scenario: pENsemble adopts a dynamic ensemble structure where a new local expert can be added when a concept change presents in the data streams [26]. This procedure is governed by a non-parametric drift detection method derived from the concept of Hoeffding\u2019s bounds [27]. This method monitors the performance metric and sends a warning signal when a significant variation is identified. This method is threshold-free and relies on some probability inequalities under assumption of independent, univariate and bounded random variables which has been theoretically proven. This learning feature lowers the ensemble complexity because the ensemble size expands on demands only and is independent from the number of data streams.\n\u2022 Ensemble Pruning Scenario: pENsemble presents an ensemble pruning scenario which is crafted from the notion of localized generalization error [28]. This method estimates generalization performance of a local expert [29] and determines local experts to be pruned [30]. This technique analyses the upper bound of error of a local expert within Q neighbourhood which reflects the generalization power of a local expert. This notion is proposed in [28]-[31] under a radial basis function neural network (RBFNN) and is adapted to the working principle of pENsemble here applying a generalized TSK neuro fuzzy local expert with a non-axis parallel Gaussian rule rotating to any direction.\n\u2022 Online Feature Selection Scenario: pENsemble is capable of performing an online feature selection scenario using the socalled Generalized Online Feature Selection (GOFS) method,\nan extension of the OFS method in [32]. The advantage of GOFS over its counterparts lies in its capability for selection and deselection of input attributes on the fly by assigning crisp values (0 or 1). This allows flexibility in the feature selection process and avoids the discontinuity bottleneck because an input variable can be recovered again in the future when needed [33]. Another salient feature of the GOFS concept is seen in its aptitude in handling partial input information which relieves computational and storage burdens because a learning process does not necessarily start from a full-scale input variables.\nThis paper conveys four major contributions as follows: 1) a novel ensemble learning algorithm inspired by a seminal work, namely DWM [34], is proposed. It modifies DWM with the introduction of a drift detection scenario, an ensemble pruning scenario, an online feature selection and an evolving local expert; 2) pENsemble puts forward a new perspective of a fully evolving ensemble learning concept where it is evolving in both ensemble level and base-classifier level; 3) three novel learning modules, namely the drift detection method, the ensemble pruning scenario, and the online feature selection, are proposed; 4) the efficacy of pENsemble was numerically validated using numerous real-world and synthetic data streams. It was compared with state-of-the art classifiers showing that pENsemble outperformed its counterparts in terms of accuracy and complexity.\nThe paper is structured as follows: Section 2 outlines literature survey over current ensemble learning algorithms and evolving learning algorithms, Section 3 discusses architecture and learning policy of pENsemble, Section 4 elaborates on the working principles of the base classifier \u2013 pClass, Section 5 describes numerical studies and comparisons with prominent algorithms, concluding remarks are drawn in the last section.\nII. RELATED WORKS Research in the area of EIS has started with algorithmic development of a number of works. Evolving rule-based model exemplifies the EIS concept using the incremental unsupervised learning [37]. DENFIS in [9] is another early example of EIS which combines the working principle of TSK fuzzy system and the Evolving Clustering Method (ECM). Angelov and Filev proposed the so-called eTS [7] which benefits from the data potential theory forming an evolving version of the mountain clustering. This work is modified for a classification problem [65], [66] and has formed the first evolving classifier, termed eClass. The term EIS has not been however formalised until the clarification in [71] since the term \u201cevolving\u201d is sometime confused with the concept of evolutionary computation. Motivated by significant progress in real-time data collection and capture, the notion of EIS has gained popularity in the community because it has been shown effective in addressing lifelong learning situation and non-stationary environments. Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70]. An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44]. A generalized TSK fuzzy rule was put forward in [45]-[47] and generates a non-axis parallel ellipsoidal cluster, which happens to have better coverage and flexibility than conventional fuzzy rules [44]. Pratama et al in [47] developed the theory of rule\nstatistical contribution borrowing the concept of hidden neuron statistical contribution in [48], [49].\nEvolving Ensemble (eEnsemble) was proposed in [14] where it makes use of eTS [7] as a base-classifier and is realised under different configurations of the ensemble classifier. This work was extended in [50] where eStacking is put forward using the concept of stacking ensemble. A parallel implementation of TEDAClass was proposed [69]. This work can be classified as an ensemble in a strict sense where data are distributed in a number of computing nodes. In [70], an ensemble of deep learning classifiers was designed for handwriting recognition and adopted the concept of data parallerization as with [69]. The all-pair classifier in [50] can be also grouped as an ensemble approach. It is solely concentrated on a class decomposition approach for multi-class problems in order to reduce class imbalance. Notwithstanding that the EIS has been well-established in the literature, it still deserves in-depth investigation because of at least three reasons 1) vast majority of EIS is constructed in the single model framework having low diversity. The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier. It lacks of capability to signal the presence of concept drift and to identify the type of drift. Such trait plays vital role in practice because it provides a feedback to an operator whether a drift is alarming or not."}, {"heading": "III. LEARNING POLICY OF PENSEMBLE", "text": "This section concerns the learning scenarios of pENsemble including ensemble structure, learning procedure, and complexity analysis. Overview of pENsemble learning scenarios is depicted in Fig. 1. A. Ensemble Structure pENsemble is developed under a generalized working framework of the DWM in which its working principle is displayed in Algorithm 1. pENsemble stores a collection of local experts, which can be automatically generated when a drift is detected and pruned when it is no longer relevant to capture current data trends [34]. An evolving algorithm, namely pClass, is deployed as a base learner which implements an open structure paradigm and is created under the MIMO architecture [24]. That is, each rule possesses multiple consequents representing each class and the final output is inferred from that generating the maximum output. The reason behind the choice of the MIMO architecture is its aptitude in handling the class overlapping because each class is looked after by a unique rule consequent. Each local expert is assigned with a voting weight\niw dynamically adjusted by a decreasing factor ip which\npenalises a local expert when an incorrect prediction is made. A local expert is pruned if its weight falls below a certain\nthreshold 1 . Despite the fact that the penalty scenario is necessary to keep the ensemble structure relevant to up-to-date context, it compromises diversity of ensemble. To correct this shortcoming, the weight of a local expert is augmented when it makes correct prediction to maintain the ensemble\u2019s diversity and to open possibility for a local expert to pick up again - such mechanism plays vital role when dealing with cyclic drift. In addition, pENsemble is equipped with another rule pruning\nscenario which measures the generalization potential of a local expert based on a localized generalization error principle.\npENsemble starts its learning scenario from scratch with no base classifier at all. The first base classifier is initialized using the first data chunk. The ensemble structure grows automatically when changing data distributions are seen. The performance of individual local experts are assessed and a\npenalty is imposed using the decreasing factor ip when misclassification is made by using a local expert whereas a reward is granted by increasing its voting weight when correct prediction is returned. After carrying out this procedure, the online concept drift detection method is performed. The drift detection strategy relies on the concept of Hoeffding\u2019s bounds to determine the drift\u2019s level [27]. The statistical process control approach is integrated to monitor dynamic of data streams [53] and classifies system behaviours into three stages, namely normal, warning and drift. A new base classifier is created using new data streams only when a drift level is reached. A weight of a new learner is initialized to 1. The final output of an ensemble classifier is inferred from a class having the highest accumulated weight. The output of each local learner is weighted by its corresponding weight. All outputs are combined to arrive at a weighted sum of each class. The weight of base classifiers are normalized to assure the partition of unity and the normalization step aims to avoid a new classifier to outweigh old classifiers. Note that pENsemble still aligns to the one-pass learning concept because it learns a data-chunk in a single scan without revisiting previous data chunks and without an iterative learning of a data chunk. Algorithm 1: Parsimonious Ensemble\n( , ), , ,n OD X C n O    are a pair of data chunk, the\nnumber of input dimension and the number of output dimension, and a data chunk size\n, ,i ip y  are a decreasing factor, an i-th local expert, a weight of i-th local expert 1, , ,OC    are global and local predictions, sum of\nweighted predictions for each class, and pruning threshold a data chunk ( )n OD   is received\nFor 1,...,t   // loops over all examples in the data chunk\nExecute the feature selection mechanism to sample the B most relevant samples. This scenario aims to address a high input dimensionality \u2013 Section 3.B.3 IF the ensemble network is empty\n1M  // create the first local expert\n1i  // initialize the weight of a local expert\nEnd\n0  For 1,...,i M // loop over local experts\n, 1,..., max ( )i j j O y   // elicits the local prediction\nIF ( tC )\ni i iy y // decreases the weight of a local expert when it\npredicts incorrectly\npii  "}, {"heading": "Else", "text": ")1),2(min( pii  "}, {"heading": "End", "text": "iy   \nEnd\n1,..., max( ) O C      // Produces the global prediction\n1\ni i M\ni\ni\n \n \n\n // normalises the weight\nIF i  Prune i-th local expert // Prune the local expert with a low weight"}, {"heading": "End", "text": "For 1,...,i M\nCalculate the localized generalization error (5) to estimate generalization power of a local expert. A local expert with poor generalization capability is removed - Section 3.B.2 IF (7) Discard i-th local expert"}, {"heading": "End End", "text": "Undertakes the drift detection method to determine suitable learning actions whether a new classifier should be introduced, a learning process is committed to update the winning classifier, or no learning process is carried out \u2013 Section 3.B.1"}, {"heading": "End", "text": ""}, {"heading": "B. Learning Algorithm of Parsimonious Ensemble", "text": "This section focusses on learning procedure of pENsemble which encompasses the drift detection strategy, the ensemble pruning strategy and the online feature selection strategy. 1) Drift Detection Method: The drift detection scenario is vital in the pENsemble because it controls the ensemble complexity. It allows an ensemble structure to expand its size when an uncharted training region comes into picture [19]. An online non-parametric drift detection method is integrated using the Hoeffding\u2019s inequalities to determine acceptable level of concept changes in data streams [27]. This method is capable of capturing significant distributional changes in data streams in the one-pass mode and is confirmed by solid theoretical guarantees in [27]. It does not rely on any assumption of probability density function rather the performance metrics is regarded as independent and bounded random variables. It is worth mentioning that the drift handling strategy in [23] does not specifically detect the exact time period where a drift presents since it is derived from the forgetting concept \u2013 categorized as a passive approach.\nThe drift detection scenario starts by monitoring statistics of data streams and defines three conditions: stable \u2013 there seems to be no change, warning \u2013 a possible concept drift may appear and drift \u2013 the drift is clearly identified. The underlying task of the drift detection method is to not only pinpoint when the drift occurs in data streams but also to track the transition between stable condition to drift condition and a drift is ascertained\nwhen it is severe enough or occurs for a period of time. A wide range of performance metrics can be used to assess the existence of drift in data streams. Referring to original work [27], two performance metrics, namely moving average and weighted moving average, are put forward. Since the moving average is more sensitive than the weighted version to concept change and thus suitable in detecting abrupt drifts, it is used\nhere and has the form\n1 , 1 ,t t t t t\nX X X X \n\n      .\nNote that this can be calculated recursively with ease. This approach is similar to the idea of statistical process control [53] except the basis of normality is relaxed here. Moreover, the use of the standard deviation  for the confidence interval is replaced by the significance level  which corresponds to the\nwarning level ( W ) and to the drift level ( D ). The drift detection method is elaborated in Algorithm 2. Algorithm 2: Drift Detection Method Based on the Hoeffding\u2019s inequality\n(0,1], (0,1]W D   are confidence for the warning level and the drift level\n{ , , }State Stable Warning Drift , cutX is statistic\ncomputed from 1 2, ,..., cutx x x\nt cutY  is statistic computed from 1,...,cutx x  , Z is statistic\ncomputed from 1 2, ,...,x x x\n, , cut cut cutX Y Z     are respectively error bounds in accordance with statistics used A data chuck 1[ ,..., ,..., ] n tD x x x    containing  samples is received\nCalculate the statistics ,t cutY Z  and the error bounds\n, t cut tY Z    using the newest observation tx // calculate statistics of three data partitions and confidence intervals\nIF t t t tZ X Z X   \n, cut t cut t X Z X Z    , reset , t cut t cut Y Y    // find the cut\npoints End IF\nIF 0 : [ ] [ ]cut t cutH E X E Y  is rejected with significance\nlevel D // determine the current state of data streams\nState Drift , create a new classifier based on a current\ndata chunk\nElseIF 0 : [ ] [ ]cut t cutH E X E Y  is rejected with size W\nState Warning , do nothing but prepare a new classifier\nif a drift is confirmed\nElse 0 : [ ] [ ]cut t cutH E X E Y \nState Stable , current concept is valid, use data chunk to train a winning classifier End\nIt is observed from Algorithm 2 that a new classifier is created when the drift state is signalled and is constructed using\na current data chunk. A transition period from warning to drift is required to bear out whether a change really occurs and is not caused by noise or outliers. No buffer is deployed to accumulate data in the transition period (warning to drift) to prevent a mixed-up concept of a new classifier. First, we start by finding a cut point in the current chunk which indicates a point where a population mean increases. The cut point is a switching point\nwhen t t t tZ X Z X    where ,t tX Z are statistics\nobtained from 1 2, ,..., cutx x x and 1 2, , ,..,cut cut mx x x x \nrespectively, while the error bounds , cut cutX Z   are calculated as follows:\n( ) 1 ( ) ln( )\n2 ( )\nm b a\ncut m cut      (1)\nwhere a, b are the minimum and maximum values of an input variable[ , ]a b . is the significance level. After finding the cutting point, data points in the chunk are grouped in two groups\n1 2 1 2[ , ,..., ], [ , ,..., ]cut cut t cut cut cutX x x x Y x x x     . The\ntwo groups are used in the analysis of the null hypothesis to examine the current state of data streams. When a null hypothesis is valid, no change is detected in the current data\nstream. When the null hypothesis is rejected with the size W , the warning status is reported but when it is rejected with the\nsize D , the drift status is returned. The null hypothesis is\nformulated as 0 : ( ) ( )cut t cutH E X E Y  and its alternative\nis set as 1 : ( ) ( )cut t cutH E X E Y  . The condition to reject the\nnull hypothesis is set as cut t cutX Y   where  is elicited\nusing (1). We apply the same settings in [27] where ,W D \nare respectively fixed at 0.005 and 0.001. It is worth stressing that these two values has clear statistical interpretation because it represents the confidence level of the Hoeffding\u2019s bound in the level of 1  . It is observed in Algorithm 2 that no learning scenario is carried out at the warning stage. This mechanism is chosen since the warning phase constitutes a transition period where the presence of concept drift still calls for further investigation. The stable phase implies that the concept remains the same and does not induce an introduction of a new classifier. It, however, calls for the winning classifier to be updated using current data chunk to assure generalization\u2019s capability of the ensemble classifier because it reduces the risk of overfitting by feeding more observations to the base-classifiers. The winning classifier is selected by simply inspecting its predictive error - Mean Square Error is used in pENsemble. 2) Ensemble Pruning Strategy Based on Local Generalization Error: The success of ensemble classifier is highly determined by the generalization potential of base classifiers. Although it is well-known that a collection of weak classifiers often promotes better performance than that of strong classifiers, it is not the case in realm of data streams. The diversity comes at the cost of complexity and predictive performance because data stream is inherent with non-stationary contexts. A base classifier with low generalization potential is expected to play little during its\nlifespan or even to jeopardize final predictions and therefore pruning such base classifier reduces the ensemble complexity [22]. Our approach is inspired by the localized generalization error method which quantifies generalization capability of a classifier within a predefined Q local region [28]. This technique is meant to approximate the upper bound of mean square error (MSE) for unseen samples lying in the Q region. The use of a predetermined Q region is a plausible approach to study model\u2019s generalization since most training samples occupy a dense local region and are inter-related to each other because they are drawn from the same unknown distribution. Finding an upper bound of generalization error for hidden context in the entire input space is extremely difficult but we can safely ignore irrelevant concept sitting far away from training samples.\nThe Q neighbourhood is defined as that\n ( ) ,0 , 1,..,Q b b i iS x x x x x x Q i n        where n is the number of input dimension and Q is a given real\nvalue [28]. All samples in ( )Q bS x except bx are regarded as\nunseen samples and the generalization capability of a model must be delved from its generalization capability in a union of\n( )Q bS x . Since a complete picture of data distribution is\nunknown before the process runs, it is assumed that unseen\nsamples have a same chance to appear. ix is treated as a random variable following the uniform distribution with zero mean and variance 2\njx   . The localized generalization error is\ndefined as follows:\n2( ) ( ( ) ( )) ( )\nQ\nSM i\nS\nR Q f x F x p x dx  (1)\nwhere ( )if x , ( )F x , ( )p x are the i-th local expert, the target function and the unknown probability density function of the input x respectively. In practise, unseen samples will lead to a higher error than those of training samples. Through the\nHoeffding\u2019s inequality with a probability of (1 ) , the average of the square error converges to the true mean:\n2 2( ) ( (( ) ) )SM emp SQR Q R E y A      (2)\n( ) ( )y f x f xb   , ln ( 2 )B     ,\n2( ( ) ( ))\n1 b bf x F xi\ntR emp\n \n \nwhere , , ,A B  stand for the difference between the maximum and minimum values of the desired outputs, the maximum possible value of the MSE, the data window size, and\nthe confidence interval. The range of desired output, A , and the maximum MSE, B , are known during the training process and are updated regularly as new training samples are observed .\nempR denotes the training error which indicates the bias of a\nmodel. 2(( ) )SQE y stands for the stochastic sensitivity\nmeasure which illustrates the sensitivity of network output against the variation of network input.\nThe difference between the training sample and the unseen sample within the Q neighbourhood is portrayed by the output\nperturbation y and 2(( ) )SQE y indicates the expectation\nof the squared output perturbations between already seen samples and unknown samples in the Q local region. It analyses how sensitive a classifier\u2019s output is to the variation of input data. The expression of the stochastic sensitivity measure for a\nGaussian basis function with a center ju and a width jv of j-\nth input coordinate has been defined in [28] and is formulated by assuming independent input perturbations without the weight perturbations. The input perturbation follows the uniform distribution with zero mean and a variance 2\njx   but\nthe input feature is not identically distributed and has its own\nexpectation jx  and variance 2 jx  . The definition of the\nstochastic sensitivity measure is applicable [29] given that a transformation strategy is undertaken to convert a high dimensional Gaussian function 1( ; , )i iN X C  to its low\ndimensional representation , ,( ; , )i j i jN x c  as follows:\n, , ,1 , , ,1 ,, [ ,..., ], [ ,..., ]i j i j i i i n i j i i nc u C c c U u u   (3)\n,\n, ,2\ni i j\ni j j\nr  \n (4)\nBy the central limit theory, if the number of input features is\nnot too low, the Gaussian basis function would have a lognormal distribution, it is written:\n2 2\n1 ( )\n1 (( ) ) ( ( ) ( )) ( )\nQ b\nSQ b b\nt S x\nE y f x x f x p x d x \n\n          (5)\nwhere ( )p x stands for the probability density function of the\ninput perturbation. Since the input perturbation is uniformly distributed in the Q region, the probability density function is formed as 1 (2 )nQ and the variance is expressed as 2 2 3\njx Q  . The assumption of uniformly distributed input\nperturbations is plausible considering the strictly single-pass working principle of pENsemble without any prior knowledge. Albeit this assumption, the distribution of the input perturbations can be relaxed provided that the variance of the input perturbation is finite. Let 24 2\n2 2 4\n1\n4 2 2 2 2 ,\n3 1 ,\n2 2 4 ,\n1\n( ) exp(( ( ) / 2 ) ( ( ) / )), ,\n( ) ( ( ) ), /\n( [( ) ] ( ) 4 ( ) ( )\n4 [( ) ]( ))\n( ( ( ) / )\nj j\nj j j j\nj j\nj j\nT i e i i i i i i j\nn\nj x x ij i i i\nj\nn D j x x x x i j\ni\nj D j x x i j\nn\ni i x x i j i\nj\nx W Var s v E s v s x u\nE s u v\nE x u Var s\nE x u\nv u v\n\n   \n   \n \n  \n\n\n\n   \n   \n    \n  \n  \n\n\n )\nThe final expression of the stochastic sensitivity measure 2(( ) )SQE y [28]-[31] is formulated in the form:\n4 2 2 2 2 4 2\n, 1 1 11\n1 0.22(( ) ) (( ( ( ) 0.2 )/ ) 3 9\nR n R R x x x i j x i i ij j j j i j ii Q n E y u v Q v SQ i                     (6)\nBecause Q is constant for all base-classifiers, it can be dropped from (6). It is observed from the localized generalization error formula (2) that there exist three components: the training error, the stochastic sensitivity measure and some constants. High training error pinpoints the under-training case which results in poor generalization of unseen samples. The stochastic sensitivity measure illustrates the sensitivity of a classifiers against output\u2019s change and that having its outputs varying dramatically against input variation should characterize high stochastic sensitivity. A good generalization is attained by minimizing both terms or forming a sound tradeoff between the two. In other words, the ensemble pruning scenario aims to discover those classifiers with large\n( )SMR Q because the smaller it is, the better the model\u2019s\ngeneralization is. Although this formula aims to analyse the upper bound of MSE which targets regression cases and direct regression to class indices in most cases results in poor performance, this strategy is still applicable to classification problems. The relationship between the localized generalization error and misclassification rate has been studied in [31] where if the error distribution is known, the percentage of unseen samples being correctly classified is given by\n0 ( ) ( ) 0.5E err Var err   where  is the\nconfidence parameter. Suppose that we compare two classifiers\n1 2,f f , it is understood from the localized generalization error\ntheory that 1f is said to have better generalization error when\nits ( )SMR Q is lower than that of 2f with the same Q . It is\nshown in [31] that the generalization performance of 1f in terms\nof misclassification rate is better than 2f with the minimum\nprobability 3\n1 2\n1 (1 ( ) / ( ) )(1 )\n6 SM SMR Q R Q   , where\n(1 ) is the confidence level. The ensemble pruning condition is set as follows:\n( ) ( ( ) ) 3 ( ( ) )SM i SM i SM iR Q mean R Q std R Q  (7)\nwhere this expression adopts the 3-sigma rule principle and aims to track downtrend of the model\u2019s generalization. Assuming that the localized generalization error follows the Gaussian distribution, 99.7% of its values occupy the three sigma range or it incurs 99.7% confidence level. That is, any case beyond the range of three sigma is said to be anomalies. Although the concept of localized generalization error has been exploited in various problems [28]-[31], its efficacy for data stream analytics is to the best of our knowledge unexplored. 3) Online Feature Selection Strategy: A high input dimension is commonly found in various real-world data stream cases and undermines the learning capability in the online real-time scenario because it imposes considerable complexity [33]. The transparency of a fuzzy rule is also affected because a rule consists of too many atomic clauses. Notwithstanding that the online feature selection strategy has drawn considerable research interest, they to date focus on a single classifier only. An online feature selection technique for the ensemble learner is proposed in this paper and is constructed under the\nframework of the GOFS method [32]. As the OFS [33], our feature selection approach is extendible to the partial input information condition where only a subset of input attributes can be obtained for the training process. The GOFS performs a crisp feature selection where input features are assigned crisp weights (0 or 1) which allows dynamic activation and deactivation of input attributes during the training process.\nThe contribution of j-th input features can be measured from\nan accumulated output weight across all fuzzy rules ,\n1\nR\nj i\ni W  \nbecause it indicates how much output change is imposed by a variation of input attributes. Since pENsemble is developed from a collection of first order TSK fuzzy systems ( 1) 1n iW   , the 0-th term of the first order TSK fuzzy system, which corresponds to the intercept of a linear function, is excluded from the summation of output weights. In realm of the TSK fuzzy system, the rule consequent depicts the local tendency of a rule and may substitute the gradient information in the sensitivity analysis of input variables since the gradient information changes in each point in the case of nonlinear function. This concept is confirmed by the fact that each base classifier in the pENsemble employs a local learning scheme in which each rule consequent represents a specific region of the approximation curve. Data standardization is required here because different input ranges may mislead the contribution of an input feature. To guarantee transparency of feature contribution, normalization is done:\n,\n1\n,\n1 1\nR\nj i\ni j n R\nj i\nj i\nW\nW\n \n \n\n\n (8)\nwhere j is the contribution of j-th input attribute. Since\npENsemble consists of a set of evolving classifiers, fuzzy rules of all local experts are extracted and subject to (8) where\n1 2 ... MR R R R    is a total number of fuzzy rules of all\nbase classifiers while M is the number of base classifiers in the ensemble. In addition, a sparsity property of L1 norm is examined to understand whether the value of n input features is accumulated in the L1 ball. Referring to the OLS theory, the input pruning process takes place given that misclassification occurs. The input pruning scenario is executed here when the global prediction of ensemble network does not match the true\nclass label C C where C is the true class label and C is the predicted class label. This approach is plausible because the feature selection scenario aims to take the corrective actions by getting rid of the influence of poor features. No feature selection is necessary when correct prediction is returned to save computational cost. The rule consequent is first adjusted using the gradient descent approach and projected to the L2 ball to assure a bounded norm. Detailed procedure of the GOFS method is shown in Algorithm 2. Algorithm 2: GOFS procedure for full input attributes\nInput: , , B  are the learning rate, the regularization factor and the desired number of input dimension. Output: 1 B\nselectedX  is a selected input vector\nObtain the global prediction of the ensemble network C\nIF C C // update the rule consequent of all base classifiers\ni i i\ni\nE W W W\nW  \n   \n .\n//Project the weight vector into the L2 ball\n2\n1\nmin(1, )i i i W W W\n \n// Compute the contribution of input attributes as per (1)\n// Extract selectedX from the highest B elements of (1)"}, {"heading": "Else", "text": "i i iW W W \nEnd IF\nWe fix 0.2, 0.01   following the same setting as [32]. The standard mean square error (MSE) is applied as the cost\nfunction, the first order derivative\ni\nE\nW\n\n is derived as follows:\n1\n1\nR\ne i\ni\nR\ni i\ni\nx E\nW\n\n\n\n\n  \n\n\n (2)\nwhere i is the spatial firing strength. It is worth noting that (2) is elicited under assumption that all fuzzy rules are structured under the first order TSK fuzzy neural network under pClass framework. In other words, fuzzy rules of all base classifiers are combined and treated as a unified local expert. This scenario is made possible by the local property of the pENsemble where each fuzzy rule functions as a loosely coupled sub-model. The stochastic gradient descent approach is applied in Algorithm 2 rather than the FWGRLS method because no covariance matrix has to be allocated and assigned for each local model thereby greatly simplifying the overall optimization process. It is worth noting the feature selection process is done in a centralistic manner where all fuzzy rules of each base classifiers are put together. Hence, the output covariance matrix in the local level cannot be used as it represents different optimization objectives. The convergence of the GOFS method has been proven [32] and its upper bound has been obtained. The GOFS method allows different subsets of input variables to be selected in every training observation. Since the partial input information situation only entails minor variation of its full counterpart [32], it is not explained here. 4) Complexity Analysis: This section aims to analyse the computational burden of pENsemble which presents a generalized version of DWM. The pENsemble utilizes the drift detection method which imposes the computational complexity\n( )O n because it only relies on the mean of data samples which\ncan be computed with ease recursively. The computational complexity of pENsemble is also affected by the rule pruning scenario governed by the localized generalization error. This\nlearning module incurs the computational burden ( )O nRO\nfor one classifier. Suppose that there exist M classifiers in the\nclassifier, this figure increases to ( )O nROM . The resultant\ncomputational complexity of pENsemble is\n( ( ) )O M DDM EP IP    where DDM stands for the\ndrift detection method, EP denotes the ensemble pruning and IP is a short of the input pruning.  is the data chunk size and data samples in the chunk are learned in a single scan and are not revisited again. Note that the term M in the aforementioned big O notation is influenced by the computational complexity of pClass as the base classifier. pClass is a fully evolving algorithm working in the single-pass learning mode. The computational complexity of pClass has been derived in [24]. IV. PARSIMONIOUS CLASSIFIER\nThis section briefly outlines algorithmic procedure of pClass which serves as the local expert of pENsemble. It includes network structure of pClass, rule growing strategy, rule pruning and recall strategy, and parameter learning strategy. Since pENsemble deploys the online feature selection scenario in the top level, the input weighting mechanism of pClass is switched to the sleep mode.\n\u2022 Network Structure of pClass: pClass is a class of neural-fuzzy systems generating a generalized first-order TSK fuzzy rule. It utilises a multivariate Gaussian function evolving a non-axisparallel ellipsoidal cluster as the rule premise, while exploiting the first order polynomial as the rule consequent. The multivariate Gaussian function offers an appealing input space partition notably when data are not distributed in the underlying axes because the ellipsoidal cluster rotates to any direction [24]. Such trait is capable of lowering the fuzzy rule demand and retains inter-relations among input variables [11]. Although such rule premise is less transparent than the conventional fuzzy rule, pClass is fitted with a transformation strategy which allows the extraction of classical rule.\n\u2022 Rule Growing Strategy: the rule growing process of pClass is orchestrated by three rule growing modules which determines the novelty of a data point whether it deserves to be a prototype of a new rule. The first rule growing strategy, namely the Datum Significance (DS) method, estimates the statistical contribution of a data sample which indicates its possible contribution in the whole course of training process. It is derived from assumption of the uniform distribution and the statistical contribution is expressed as the zone of influence of an ellipsoidal cluster.\nThe statistical contribution, however, ignores summarization power of a rule because it does not consider how strategic a current position of rule in the feature space is [24], [38]. This hinders its capability to capture concept drift because no distance information is provided in enumerating the importance of fuzzy rules. The second rule growing strategy, namely the Data Quality (DQ) method, is put forward. This concept follows the concept of recursive density estimation (RDE) [2], [7] where a density of a local region is computed recursively. This concept concludes that a rule addition is necessary either when a data point represents the most relevant\nconcept having the highest density or when a data point is beyond the coverage of existing rules [24]. The DQ method differs from the RDE method [7] in two facets: 1) it involves a weighting strategy reducing the influence of outliers which causes a drop of density for next samples; 2) it uses the inverse multi-quadratic function in lieu of the Cauchy function; 3) it is tailored for the multivariate Gaussian function.\nAn oversized rule is prone to the cluster delamination problem which pinpoints a situation where two or more distinct data clouds are contained by a cluster. This situation undermines the generalization because the specificity of a cluster decreases significantly. The third rule growing strategy aims to overcome this issue borrowing the concept of GART+ [54]. It monitors the coverage span of the winning rule obtained from the Bayesian concept \u2013 a rule with the maximum posterior probability. It limits the growth of the winning rule where a new rule is introduced when the size of winning rule exceeds a prespecified level [55].\n\u2022 Rule Pruning and Recall Strategy: pClass is equipped by two rule pruning strategies, namely extended rule significance (ERS) method, and potential+ (P+) method. The ERS method shares the same principle of the DS method which estimates the statistical contribution of fuzzy rules to discover inconsequential rules which play little role to the final output during their lifespan. It combines significance of both rule premise and rule consequence to quantify the rule contribution. The significance of rule premise is derived from the approximation of accumulated contribution of the multivariate Gaussian function during its lifespan without revisiting preceding samples. It is obtained under a uniform distribution assumption and this assumption results in a zone of influence of fuzzy rules as an indicator of rule premise significance. The contribution of rule consequent is measured from a weighted sum of an output weight vector since a small rule weight normally generate negligible outputs.\nThe P+ method monitors the evolution of a rule in respect to current data trend and is vital in non-stationary environments. It aims to find obsolete rules which are no longer relevant to delineate recent concept due to drift. This scenario is realised by extending the concept of data potential [7], [56] for the rule pruning scenario. The concept of data potential performs recursive density estimation of fuzzy regions which pinpoints relevance of fuzzy rules since fuzzy rules which are not supported by current data distribution is expected to return low density. The P+ method, however, differs from the data potential concept in its kernel function using the inverse multiquadratic function instead of the Cauchy function. The P+ method also functions as the rule recall scenario which is capable of handling the recurring drift. That is, the recurring drift refers to a situation where previous data distribution reappears again in the future. This may trigger previously pruned rules portraying old concept to be valid again. Adding a completely new rule to address the cyclic drift does not coincide with the flexible nature of human being which can recall previous knowledge with ease. Furthermore, adding a new rule risks on catastrophic forgetting of previously valid knowledge because it ignores learning history. Previously pruned rules can be reactivated in the future provided that its relevance indicated by the P+ method beats existing rules and newly observed data point. It is worth noting that previously\npruned rules are discounted from any training scenarios except the update of their densities. This paradigm ensures that the rule pruning scenario still relieves the computational burden.\n\u2022 Parameter Learning Strategy: Data streams may not incur sufficient novelty to be a prototype of a new rule but such data streams are useful to refine the influence zone of existing rule base [24]. This situation is addressed by fine-tuning the rule premise of the winning rule. The adaptation scenario is derived from the sequential version of maximum likelihood and is adapted to the multivariate Gaussian function. Furthermore, pClass utilises a direct update scheme of the inverse covariance matrix according to the formulas derived in [39] which shelves the reinversion of the covariance matrix. The winning rule is determined using the Bayesian concept where a rule with the maximum posterior probability is selected as the winning rule. This winning rule selection is preferred over the compatibility measure [55] since it takes into account the rule\u2019s population.\nThe rule consequent is adjusted using the fuzzily weighted generalized recursive least square (FWGRLS) method. The FWGRLS is a derivation of the FWRLS method originally proposed by Angelov in [7]. It borrows the concept of weight decay function of the GRLS method in [57]. The FWGRLS method works in the local learning scenario well-suited to the EIS since it offers a decoupled adaptation scheme where adaptation of each local region incurs no cross correlation to each other since each local sub-model features a unique output covariance matrix [24]. Learning in a particular sub-model has no effect to the stability and convergence of other rules. The salient feature of the FWGRLS method compared to the FWRLS method lies in the generalized weight decay term in the cost function which aims to alleviate the overfitting situation. The weight decay term also supports compactness and parsimony of the rule base because it forces the rule consequent of an inconsequential rule to a small range. Therefore, inconsequential rules can be located by the ERS method easily. The quadratic weight decay term is incorporated since it is capable of reducing the weight vector proportionally to its current values [47].\nV. EXPERIMENTAL STUDIES We elaborate on numerical validations of pENsemble by using 15 real-world data streams and comparisons with prominent classifiers. Furthermore, the sensitivity of predefined parameters is analysed to confirm user-friendly characteristic of pENsemble. The simulations were undertaken with an Intel (R)\nCore i5-6600 CPU @ 3.3 GHZ with 8 GB of RAM and pENsemble is implemented under MATLAB environment. A. Comparisons with State-of-The Art Algorithms pENsemble is benchmarked against six prominent classifiers for data streams falling into three categories: evolving classifiers, metacognitive classifiers, dynamic ensemble classifiers. The underlying feature of consolidated algorithms are elaborated as follows:\n\u2022 Learn++NSE is seen as one of pioneer works in dynamic ensemble classifier for non-stationary environments [18]. It presents an extension of Learn++ [58] to tackle concept drifts in data streams. It is an Adaboost-like algorithm which consists of a set of weak learners and adopts the concept of sample weighting. The underlying contribution is observed in the dynamically weighted majority voting which reflects dynamic contexts of data streams.\n\u2022 Learn++CDE is a generalized version of Learn++NSE integrating a specific mechanism to handle the class imbalanced problem in data streams [52]. It combines the Learn++NSE with the well-established SMOOTE using the concept of undersampling and oversampling approaches for imbalanced data streams. It also proposes concepts of subensemble and class independent error weighting with a penalty constraint. Both Learn++NSE and Learn++CDE are built upon CART as the base classifier.\n\u2022 pClass is a class of evolving classifier putting forward the open principle paradigm and the online learning capability [24]. pClass is structured as a five-layered neural network working in tandem and actualising a generalized TSK fuzzy inference system. In addition to its flexible network structure, pClass is equipped by an online feature weighting strategy. All of which are summed up in Section 4 of this paper. This comparison is necessary to illustrate how the proposed ensemble learning scheme is better than its single classifier version.\n\u2022 eT2Class is another case of evolving classifiers unifying the dynamic network structure and the online learning capability [10]. It differs from pClass since it incorporates the interval type-2 fuzzy working principle. It features a fast type-reduction method which is scalable for the online data stream processing.\n\u2022 McFIS characterises the so-called metacognitive learning machine assumed as an extension of evolving classifiers [62]. The metacognitive classifier shares the same principles of the evolving classifier except that it has two additional learning modules, namely what-to-learn and when-to-learn. In McFIS, the metacognitive learning concept is implemented under the roof of neural-fuzzy system and applies the MIMO architecture to infer the class label.\nConsolidated algorithms were numerically validated using real-world and synthetic data streams featuring highly dynamic characteristics. Popular DDD problems characterizing the abrupt and gradual drifts, namely sin, sinh, line and 10dplane, were explored to investigate the performance of consolidated algorithms [59], [60]. The unique property of these problems is seen in their three versions which correspond to the duration and rate of concept change. The third version presenting the most complex variant was used where the drift lasts in the longest duration. The DDD problems are equipped with the stream generator offering concrete data stream environments. In addition, two semi-artificial data streams, namely car and iris\nwhich are also parts of DDD database, were incorporated. These two problems have been modified from their original version by incorporating the concept drift. The SEA problem introduced in [61] was used to bear out the efficacy of benchmarked algorithms. Moreover, an extension of the SEA problem contributed by Ditzler and Polikar [52] was put forward instead of its original version since it offers the class imbalance property and the cyclical drift which often occurs in the real-world data streams. Another popular problem in the data stream mining area, namely the Gaussian problem, was exploited [18]. This problem is relevant to examine consolidated algorithms because each class contains gradual and independent drift which can be controlled from the mean and variance of the parametric equations. The hyperplane problem was exploited to inspect the learning performance of consolidated algorithms. This problem is well-known as a benchmark problem in the massive online analysis (MOA) and characterises the gradual concept drift where data are initially drawn from one distribution and then slowly shifts to another distribution in a probabilistic fashion. On top of those artificial and semi-artificial data streams, electricity pricing and weather problems were included in our experiments. These two problems are widely used in the field of data stream because the electricity pricing problem are affected by dynamic external attributes, while the weather condition is well-known for its recurring drift due to seasonal changes. The characteristics of these data streams along with detailed experimental procedures are encapsulated in Table 1.\nConsolidated algorithms are assessed in six evaluation criteria, namely classification rate, fuzzy rule, input attribute, network parameters, execution time and ensemble size. Classification rate refers to accuracy on testing samples defined as the rate of correctly classified testing samples while fuzzy rule for pENsemble is inspected from a total number of fuzzy rules across all local experts. Input attributes in pENsemble are sampled dynamically in every training instance by assigning crisp weights where a desired number of input attributes is predetermined before process runs whereas input attributes in other algorithms conversely happens to be fixed. Network parameters are enumerated as a total number of network parameters across all local experts and are determined by the type of network architecture. Structural complexities of the base classifiers have been discussed in [24] and are not recounted here. Execution time is obtained from the running time to accomplish a training process, while the ensemble size is measured from the number of base classifiers deployed in the training process. Numerical results of consolidated algorithms are tabulated in Table 2 and are averaged over the number of time stamps.\nFrom Table 2, pENsemble outperforms its counterparts in the viewpoint accuracy where it produces the highest accuracy in 7 of 11 study cases. It is depicted that pENsemble delivers almost 10% improvement of classification rate compared to its single model version - pClass. pENsemble\u2019s accuracy is inferior to Learn++NSE and Learn++CDE in sinH, Gaussian and iris+ data streams. Nonetheless, it is understood that both Learn++NSE and Learn++CDE possess intractable structural complexities since the ensemble size grows exponentially as the number of data streams which might not be a wise option in the real-world data stream environments where the total number of\ndata streams is unpredictable and possibly infinite. In the realm of fuzzy rule and network parameters, pENsemble generates a comparable level of complexities even compared to nonensemble classifiers. These facts are acceptable since pENsemble features two rule pruning scenarios analysing not only relevance of base classifiers but also approximation of generalization performance of base classifiers. Moreover, the dynamic online feature selection scenario contributes substantially to lower network parameters without compromising the predictive accuracy. The compact and parsimonious structures of pENsemble expedite its execution times which happened to be comparable with its single model counterparts and even faster than them in some study cases. Note that the claim of execution time can be made because all consolidated algorithms were executed under the same computing platform. pENsemble overcomes both Learn++NSE and Learn++CDE in the context of ensemble size in all study cases. It is worth noting that pENsemble makes use of the drift detection method controlling the growth of ensemble structures. The drift detection method brings a step forward from Learn++.NSE and Learn++.CDE since a new data stream does not necessarily trigger the introduction of a new local expert and a new local expert is added only when the conflict attributed to the concept change is severe enough and beyond the scope of existing local experts. This scenario leads to a more resilient approach to deal with the plasticity-stability dilemma than static ensemble or greedy ensemble [27].\nWeather\nFuzzy Rule 5\u00b12.3 N/A N/A 2.3\u00b10.5 2.3\u00b10.3 10 Input Attribute 2 8 8 8 8 8 Network Parameters 60\u00b128.2 N/A N/A 226.8\u00b195.6 391\u00b181 108 Execution Time 0.2\u00b10.7 184.8 9.98 1.8\u00b10.22 1.8\u00b10.1 0.41\u00b10.08 Ensemble Size 1.8\u00b10.6 10 10 N/A N/A N/A\nGaussian\nClassification Rate 0.75\u00b10.02 0.95\u00b10.03 0.95\u00b10.03 0.74\u00b10.2 0.72\u00b10.13 0.66\u00b10.13 Fuzzy Rule 2.2\u00b10.61 N/A N/A 2.1\u00b10.3 1.4\u00b10.5 8.05\u00b11.9 Input Attribute 2 2 2 2 2 2 Network Parameters 12.9\u00b13.5 N/A N/A 50.2\u00b16.9 35.5\u00b112.4 34.2\u00b17.6 Execution Time 2.9\u00b10.6 21020 79998 0.74\u00b10.05 1.6\u00b10.3 0.93\u00b10.4 Ensemble Size 2.2\u00b10.6 100 100 N/A N/A N/A\nHyperplane\nClassification Rate 0.93\u00b10.02 0.91\u00b10.02 0.9 0.91\u00b10.02 0.89\u00b10.1 0.68\u00b10.09 Fuzzy Rule 4.4\u00b12.7 N/A N/A 3.8\u00b11.7 2.04\u00b10.2 9.9\u00b10.5 Input Attribute 2 4 4 4 4 4 Network Parameters 52.6\u00b132.3 N/A N/A 114.9\u00b152.6 110.6\u00b110.6 63.4\u00b12.8 Execution Time 0.9\u00b10.3 926.04 2125.5 2.7\u00b11.4 2.5\u00b11.5 0.5\u00b10.2 Ensemble Size 1.73\u00b10.8 100 100 N/A N/A N/A\nElectricity pricing\nClassification Rate 0.75\u00b10.15 0.69\u00b10.08 0.69\u00b10.08 0.68\u00b10.1 0.77\u00b10.08 0.5\u00b10.1 Fuzzy Rule 12.4\u00b12.8 N/A N/A 3.5\u00b12.4 2.3\u00b10.5 9.6\u00b10.7 Input Attribute 2 8 8 8 8 8 Network Parameters 148.5\u00b130.04.1 N/A N/A 226.8\u00b195.6 61.3\u00b121 104\u00b16.99 Execution Time 0.19\u00b10.09 211.2 211.2 7.1\u00b14.4 5.1\u00b11.3 0.5\u00b10.4 Ensemble Size 5.3\u00b10.9 119 119 N/A N/A N/A\nCoronary Heart Disease1\nClassification Rate 0.91\u00b10.01 0.83\u00b10.4 0.84\u00b10.35 0.99\u00b10.1 0.38\u00b10.5 0.81\u00b10.4 Fuzzy Rule 1.4\u00b10.5 N/A N/A 2.8\u00b11.3 1.4\u00b10.5 1 Input Attribute 2 10 10 10 10 10 Network Parameters 16.8\u00b16.5 N/A N/A 369.8\u00b1172.1 352.8\u00b1138 21.4\u00b10.5 Execution Time 0.13\u00b10.02 1.87 1.82 0.6\u00b10.2 0.94\u00b10.7 0.19\u00b10.2 Ensemble Size 1.4\u00b10.5 5 5 N/A N/A N/A\nCoronary Hearth Disease2\nClassification Rate 0.9\u00b10.09 0.81\u00b10.4 0.81\u00b10.43 0.2\u00b10.4 1 1 Fuzzy Rule 1.2\u00b10.4 N/A N/A 1 3.2\u00b11.1 2\u00b12.2 Input Attribute 2 10 10 10 10 10 Network Parameters 14.4\u00b15.4 N/A N/A 33.4\u00b127.7 422.4\u00b1144.5 33.4\u00b127.2 Execution Time 0.2\u00b10.05 4.19 2.2 0.59\u00b10.5 0.7\u00b10.4 0.5\u00b10.9 Ensemble Size 1.2\u00b10.4 5 5 N/A N/A N/A\nTool wear diagnosis with 12 inputs\nClassification Rate 0.83\u00b10.1 0.7\u00b10.02 0.76\u00b10.02 0.78\u00b10.14 0.4\u00b10.1 0.76\u00b10.1 Fuzzy Rule 1.8\u00b10.4 N/A N/A 4.4\u00b10.03 3 15 Input Attribute 6 12 12 12 12 12 Network Parameters 32.4\u00b17.8 N/A N/A 673.4\u00b1285.2 546 252 Execution Time 0.04\u00b10.03 0.53\u00b10.13 0.6\u00b10.15 0.52\u00b10.53 1.3\u00b10.7 0.3\u00b10.1 Ensemble Size 1.3\u00b10.5 9 9 N/A N/A N/A\nTool wear diagnosis with 7 inputs\nClassification Rate 0.78\u00b10.13 0.76\u00b11.3 0.76\u00b10.02 0.81\u00b10.1 0.53\u00b10.2 0.76\u00b10.15 Fuzzy Rule 1.8\u00b10.4 N/A N/A 3.3\u00b14.1 2 14.7\u00b10.9 Input Attribute 2 7 7 7 7 7 Network Parameters 32.4\u00b17.6 N/A N/A 18.7\u00b10.7 15.7\u00b10.7 168.7\u00b110.4 Execution Time 0.02\u00b10.01 0.95 0.59 0.79\u00b10.6 1.2\u00b10.5 0.4\u00b10.3 Ensemble Size 1.8\u00b10.4 9 9 N/A N/A N/A"}, {"heading": "A. Prediction of Coronary Heart Disease", "text": "pENsemble was tested in a real-world problem, namely prediction of coronary heart disease (CHD) (courtesy of Dr. Agus Salim, La Trobe University). Our study was done using a real-world dataset derived from a nested-case-control (NCC) experiment within Singaporean Chinese health study (SCHS) cohort with 63, 257 participants. The subjects of the experiment were only those donated their blood and never suffered from CHD or stroke verified from self-reported diagnosis or data from the hospital discharge database [63].The goal of this study case is to identify the disease outcome of the participants whether CHD occurred until December, 31st , 2010. Both myocardial infarction (AMI) or coronary heart disease death were grouped as cases, whereas others were classified as control. Some exclusion was performed based on several statistical criteria and consequently shrunk the scope of study to 1458 patients only. Predictive analytics are supported by 11 input attributes: time from the baseline until the event, age at baseline, cholesterol level at baseline, HDL cholesterol level at baseline, systolic blood pressure reading at baseline, whether subject was on anti-hypertensive medication at baseline, whether subject smoked at baseline, levels of haemoglobin A1c protein at baseline, body mass index at baseline, whether subject has diabetes at baseline, sampling weight. Prediction was carried per gender group: 958 samples from male patients where 298 of which indicate cases (CHD1); 528 from female\npatients where 143 of which represent cases (CHD2). Our simulation followed 10-fold cross validation to avoid the data order dependency problem and were compared against the same set of algorithms. Numerical results are reported in Table 2.\nReferring to Table 2, pENsemble produced competitive accuracy with much lower parameter burden and number of input attributes than pClass and eT2Class. Note that the online feature weighting mechanism in pClass does not alleviate the number of input attributes. The low structural complexities directly affected the running time of pENsemble which occurred to be the fastest in both male and female participants datasets. In comparison with Learn++NSE and Learn++CDS, our algorithm outperformed these algorithms in all six facets."}, {"heading": "B. Online Tool Condition Monitoring of Metal Cutting", "text": "pENsemble was deployed in the prognostic health management (PHM) case, namely automatic tool state identification of a metal turning operation (Courtesy of Prof. Eric Dimla, UTB). The experiment took place in a variable speed centered lathe of type Lang Swing J6 with the work-piece materials, namely EN24 alloy steel using P15 and P25 coated cemented carbide inserts. Mini accelerometer and dynamometer were installed to record vibration signal and cutting force signal in three dimensional cutting axes (X, Y, Z) [64]. Machining process was run at a frequency of 30 kHz collecting 4096 data samples per channel and the measured variables came through a signal conditioning unit attached as\nperipheral signal conditioning instruments in the main server.\nDifferent machining parameters in terms of cutting speed, feed-rate and depth of cut were applied during the experiment to simulate non-stationary machining environments. The vibration as well as the cutting force signals were captured online and generated data streams. The true class label was assigned using visual inspection of the flank and nose wears for each cut. Cuts lasted around 10 seconds at the beginning of machining process but increased to 30 seconds after complete stabilization of the cutting process. The measurement of flank and nose wears during the manual inspection was compared against predefined thresholds to categorize every observation into six classes as follows:\n\u2022 000 \u2013 nominally sharp\n\u2022 100 \u2013 high flank wear\n\u2022 010 \u2013 high nose wear\n\u2022 001 \u2013 chipped/fractured nose\n\u2022 110 \u2013 high flank and high nose wear\n\u2022 111 \u2013 high flank and chip / fractured nose This true class label is resulted from visual check of flank and nose wear against the following thresholds.\n\u2022 Flank wear mark value \u2264 0.15mm, tool insert nominally sharp\n\u2022 Flank wear mark value > 0.15mm, tool insert worn (high flank)\n\u2022 Nose wear length \u2264 0.2mm, nominally sharp\n\u2022 Nose wear length > 0.2mm, tool worn (nose fractured / chipped) 12 input features were extracted from vibration and force signals. They encompass static, dynamic forces, acceleration in three-dimensional axes, feed rate, cutting speed, and depth of cut while the target variable consists of four classes: nominally sharp, high flank wear, high flank and nose wear, high flank wear and chipped/fractured nose. Our experiment comprises two parts, where the first part benefited from the full set of input attributes, while a reduced dimension was injected in the second part. Cutting speed and depth of cut which are deemed to play little influence to predictive quality were set aside. The diagnosis process was undertaken on the fly with two time stamps where, in each time stamp, 50 samples were utilized to build our hypothesis, the remaining 10 samples were fed as the testing samples. pENsemble was benchmarked against the same set of algorithms as previous sections and was evaluated against the same 6 criteria. Consolidated numerical results are reported in Table 2.\nThe efficacy of pENsemble over its counterparts is evident in Table 2. pENsemble was the best-performing algorithm in almost all evaluation criteria. These results were better than expected since pENsemble outperformed a single-classifier algorithm in realm of fuzzy rule, network parameters and execution time. pENsemble was slightly worse than eT2Class in the context of network parameters but one should recall that it attained far better classification rate than eT2Class. C. Sensitivity Analysis of Predefined Parameters This section concerns sensitivity analysis of predefined parameters of pENsemble. Our goal is to study the effect of these user-defined parameters to the learning performance. pENsemble involves four user-defined parameters, namely \ud835\udefc\ud835\udc37 , \ud835\udefc\ud835\udc4a, \ud835\udf03, \ud835\udc5d respectively fixed at 0.008, 0.001. 0.01, 0.1. Variations of these parameters were committed to delve their influence to overall performance. Note that other parameters were set at their default values while varying one parameter. The following values were selected to investigate the sensitivity\nof the predefined parameters: [0.01,0.005,0.003]D  ,\n[0.01,0.005,0.003]W  , [0.0005,0.003,0.005]  ,\n[0.1,0.3,0.7]p  . Our experiment took place using the\nCHD2 problem illustrating predictive analytics of coronary heart diseases in the female patients of the SCHS cohort. Experimental procedure remained the same as Section V.B. Numerical results are summarized in Table 3.\nOur claim is confirmed in Table 3. The predefined parameters except p are case-insensitive. It is shown in Table 3 that different values of the predefined parameters made little performance difference of pENsemble. While the decreasing factor p has an impact to learning performance, it did not lead to substantial performance deterioration. The decreasing factor affects the compactness and parsimony of the ensemble structure. The higher the value precludes the ensemble pruning process discarding an inactive classifier with a low weight because a low penalty is imposed when misclassification is committed. This assumption is substantiated in Table 3 where the ensemble complexity rises to 1.8 from 1.2 when assigning the decreasing factor to 0.7. Based on these facts, pENsemble is user-friendly and one can simply apply the same set of userdefined parameters recommended in this paper.\nVI. CONCLUSION This paper presents a novel evolving ensemble classifier, termed parsimonious ensemble (pENsemble). pENsemble feature some unique characteristics where an evolving classifier, namely pClass, is utilised as its local expert. The flexible working principle of pClass helps pENsemble to handle local drift of data streams effectively because it features an open structure and a fully online working principle. pENsemble constitutes a fully evolving ensemble classifier where its structure is automatically generated and self-expands when a concept drift is detected. pENsemble offers a parsimonious working principle which is resulted from pruning activities of inactive classifiers. It is equipped with two ensemble pruning strategies which assess relevance and generalization power of a local expert. An online feature selection strategy is incorporated into pENsemble. This mechanism actively selects a subset of input attributes and differs from common practise in the literature because it allows to arrive at different subsets of input\nattributes in every training observation. The efficacy of pENsemble has been numerically validated through 15 realworld and synthetic data streams. It has been compared with 6 well-known algorithms where our algorithm delivers the highest accuracy in 8 of 15 study cases. It is also found that pENsemble generated comparable complexities from those of single classifier variants and far less complexities than those of ensemble classifier variants. Future work will be directed toward investigation of granular computing for data stream analytics to address high-level data abstraction. ACKNOWLEDGEMENTS The first author thank Prof. Plamen Angelov for thorough discussion about history of EIS. The first author also thank Dr. Agus Salim and Prof. Eric Dimla for sharing their datasets. The third author acknowledges the support of the Austrian COMET-K2 programme of the Linz Center of Mechatronics (LCM), funded by the Austrian federal government and the federal state of Upper Austria. This publication reflects only the authors' views. References [1] J. Gama, Knowledge Discovery from Data Streams, Chapman & Hall/CRC, Boca Raton, Florida, 2010 [2] P. Angelov, \u201c Autonomous Learning Systems: From Data Streams to Knowledge in Real-time\u201d, John Wiley and Sons Ltd., 2012 [3] M. Sayed-Mouchaweh and E. Lughofer, Learning in Non-Stationary Environments: Methods and Applications, Springer, New York, 2012 [4] M. Pratama, J. Lu, E. Lughofer, G. Zhang and M.J. Er, Incremental Learning of Concept Drift Using Evolving Type-2 Recurrent Fuzzy Neural Network, IEEE Transactions on Fuzzy Systems, on-line and in press, 2017 [5] G. Ditzler, et al, \u201c Learning in Nonstationary Environments: A Survey\u201d, IEEE Computational Intelligence Magazine, Vol.10(4), pp. 12-25, (2015) [6] R. M. French, Catastrophic forgetting in connectionist networks, Trends in Cognitive Sciences, vol. 3 (4), pp. 128--135, 1999 [7] P.Angelov and D. Filev, \"An approach to online identification of TakagiSugeno fuzzy models,\" IEEE Transactions on Systems, Man, and Cybernetics, Part B. vol. 34, pp. 484-498. 2004 [8] S.W.Tung, C.Quek, C.Guan, \u201ceT2FIS: An Evolving Type-2 Neural Fuzzy Inference System\u201d, Information Sciences, vol.220, pp.124-148, (2013) [9] N. Kasabov, and Q. Song, DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction, IEEE Transactions on Fuzzy Systems .vol10 (2).pp. 144\u2013154. (2002) [10] M. Pratama, J. Lu, G.Zhang, \u201c Evolving Type-2 Fuzzy Classifier\u201d, online and in press, IEEE Transactions on Fuzzy Systems, on line and in press, (2015) [11] A. Lemos, et al, Adaptive fault detection and diagnosis using an evolving fuzzy classifier, Information Sciences, vol. 220, pp. 64-85, (2013) [12] P. Brazdil, C. Giraud-Carrier, C. Soares and R. Vilalta, Metalearning, Springer, Berlin Heidelberg, 2009 [13] L. Rokach, Ensemble-based classifiers. Artificial Intelligence Review, vol. 33 (1-2), pp. 1\u201339, 2010 [14] J.A Iglesias, A. Ledezma, A. Sanchiz, \u201cEnsemble Method Based on Individual Evolving Classifiers\u201d, in 2013 Evolving and Adaptive Intelligent Systems, pp. 78-83, 2013 [15] J.A Iglesias, A. Ledezma, A. Sanchiz, \u201cAnalyzing the structure of ensembles based-on evolving classifiers\u201d, in 2013 FINO/CAEPIA, 2013 [16] A. Bouchachia, et al, DELA: A Dynamic Online Ensemble Learning Algortihm, in European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, pp. 491- 496, 2014 [17] L. Kuncheva, \u201c Classifiers Ensemble for Changing Environments\u201d, Lecture Notes on Computer Sciences, Vol. 3077, pp. 1-15, 2004 [18] R. Elwell and R. Polikar. Incremental learning of concept drift in nonstationary environments. IEEE Transactions on Neural Networks, Vol. 22(10), pp. 1517\u20131531, 2011 [19] B. Mirza, Z. Lin, and N. Liu, \u201cEnsemble of subset online sequential extreme learning machine for class imbalance and concept drift,\u201d Neurocomputing, vol. 149, pp. 315\u2013329, 2015 [20] A. Shaker et al, Self-Adaptive and Local Strategies for a Smooth Treatment of Drifts in Data Streams, Evolving Systems, vol. 5 (4), pp. 239--257, 2014 [21] M. Pratama, J. Lu, E. Lughofer, G. Zhang and S. Anavatti, Scaffolding Type-2 Classifier for Incremental Learning under Concept Drifts, Neurocomputing, vol. 191, pp. 304--329, 2016 [22] P.P.K. Chan, X. Zeng, E. C. C. Tsang, D. S. Yeung, J. W. T. Lee, \u201c Neural Network Ensemble Pruning Using Sensitivity Measure in Web Applications\u201d, in IEEE International Conference on Systems, Man and Cybernetics, pp. 3051- 3056, 2007\n[23] E. Lughofer, P. Angelov,\u201d Handling Drifts and Shifts in On-Line Data Streams with Evolving Fuzzy Systems\u201d, Applied Soft Computing, vol. 11(2), pp. 2057-2068, 2011 [24] M. Pratama, S.G. Anavatti, M.J. Er and E. Lughofer, pClass: An Effective Classifier for Streaming Examples, IEEE Transactions on Fuzzy Systems, vol. 23 (2), pp. 369--386, 2015 [25] H. Toubakh, M. Sayed-Mouchaweh, \u201c Hybrid dynamic data-driven approach for drift-like fault detection in wind turbines\u201d, Evolving Systems, Vol. 6(2), pp. 115-129, 2015 [26] G. Dirzler, R. Polikar, \u201c Hellinger Distance based Drift Detection for Nonstationary Environments\u201d, in IEEE Symposium on Computational Intelligence in Computational Intelligence in Dynamic and Uncertain Environments, pp. 41-48, 2011 [27] I. Frias-Blanco, J. D. Campo-Avilla, G. Ramos-Jimenes, R. MoralesBueno, A. Ortiz-Diaz, Y. Caballero-Mota, \u201cOnline and Non-Parametric Drift Detection Methods Based on Hoeffding\u2019s Bounds\u201d, IEEE Transactions on Knowledge and Data Engineering, Vol. 27(3), pp. 810-823, 2015 [28] D. S. Yeung, W.W. Y. Ng, D. Wang, E. C. C. Tsang, X-Z. Wang, \u201c Localized Generalization Error Model and Its Application to Architecture Selection for Radial Basis Function Neural Network\u201d, IEEE Transactions on Neural Networks, Vol. 18(5), pp. 1294-1305, 2007 [29] P. P. Chang, D. S. Yeung, W. W. Y. Ng, C. M. Lin, J. N. K. Liu, \u201c Dynamic Fusion Method Using Localized Generalization Error Model\u201d, Information Sciences, Vol. 217, pp. 1-20, 2012 [30] P. P. K. Chan, et al, \u201c Sensitivity Growing and Pruning Method for RBF Networks in Online Learning Environments\u201d, in International Conference on Machine Learning and Cybernetics, pp. 1107-1112, 2011 [31] W. W. Y. Ng, A. P. F. Chan, D. S. Yeung, E. C. C. Tsang, \u201c Quantitative Study on the Generalization Error of Multiple Classifier Systems\u201d, in IEEE International Conference on Systems, Man and Cybernetics, 2005 [32] J. Wang, P. Zhao, S. Hoi, R. Jin, \u201c Online Feature Selection and Its Applications\u201d, IEEE Transactions on Knowledge and Data Engineering, Vol. 26(3), pp. 698-710, 2014 [33] E.Lughofer,\u201cOn-line incremental feature weighting in evolving fuzzy classifiers,\u201d Fuzzy Sets and Systems, vol. 163(1), pp. 1\u201323, (2011) [34] J. Kolter and M. Maloof. Dynamic weighted majority: An ensemble method for drifting concepts. Journal of Machine Learning Research, Vol. 8, pp. 2755\u20132790, 2007 [35] C. Juang, C. Lin, An on-line self-constructing neural fuzzy inference network and its applications. IEEE Transactions on Fuzzy Systems, vol. 6(1), pp. 12\u201332, 1998 [36] M.Pratama, S.Anavatti, J.Lu, Recurrent Classifier Based on an Incremental Meta-cognitive-based Scaffolding Algorithm, IEEE Transactions on Fuzzy Systems, Vol.23(6), pp. 2048-2066, 2015 [37] P. Angelov, R. Buswell, \u201c Identification of Evolving Fuzzy Rule-Based Models\u201d, IEEE Transactions on Fuzzy Systems, Vol. 10(5), pp. 667-677, 2002 [38] H.-J. Rong, N. Sundarajan, G.-B. Huang, and G.-S. Zhao, \u201cExtended sequential adaptive fuzzy inference system for classification problems,\u201d Evolving Systems, vol. 2(2), pp. 71\u201382, 2011 [39] E. Lughofer, Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications, Springer, Berlin Heidelberg, 2011 [40] M. Pratama, G. Zhang, M-J. Er, S. Anavatti, An Incremental Type-2 Metacognitive Extreme Learning Machine, IEEE Transactions on Cybernetics, online and in press, 2016 [41] E. Lughofer, Extensions of Vector Quantization for Incremental Clustering, Pattern Recognition, vol. 41 (3), pp. 995--1011, 2008 [42] E. Lughofer, FLEXFIS: A Robust Incremental Learning Approach for Evolving TS Fuzzy Models, IEEE Transactions on Fuzzy Systems, vol. 16 (6), pp. 1393--1410, 2008 [43] E. Lughofer, et al, On-line Quality Control with Flexible Evolving Fuzzy Systems, in: Learning in Non-Stationary Environments: Methods and Applications, Springer, pp. 375--406, New York, 2012 [44] E. Lughofer, et al, Generalized Smart Evolving Fuzzy Systems, Evolving Systems, Vol. 6 (4), pp. 269--292, 2015 [45] A. Lemos, W. Caminhas and F. Gomide, Multivariable Gaussian Evolving Fuzzy Modeling System, IEEE Transactions on Fuzzy Systems, vol. 19 (1), pp. 91--104, 2011 [46] D. Dovzan, V. Logar and I. Skrjanc, Implementation of an Evolving Fuzzy Model (eFuMo) in a Monitoring System for a Waste-Water Treatment Process, IEEE Transactions on Fuzzy Systems, vol. 23 (5), pp. 1761--1776, 2015 [47] M. Pratama, S.G. Anavatti, P. Angelov and E. Lughofer, PANFIS: A Novel Incremental Learning Machine, IEEE Transactions on Neural Networks and Learning Systems, vol. 25 (1), pp. 55--68, 2014 [48] G.-B. Huang, P. Saratchandran, and N. Sundararajan, \u201cA generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation,\u201d IEEE Transactions on Neural Networks, vol. 16(1), pp. 57\u201367, 2005 [49] H. J. Rong, N. Sundararajan, G. B. Huang, and P. Saratchandran, \u201cSequential adaptive fuzzy inference system (SAFIS) for nonlinear system identification and time series prediction,\u201d Fuzzy Sets and Systems, vol. 157(9), pp. 1260\u20131275, 2006 [50] J. A. Iglesias, A. Ledezma, A. Sanchis, \u201cAn ensemble method based on evolving classifiers: eStacking \u201d, in IEEE Symposium on Evolving and Autonomous Learning System, pp. 124-131, 2014 [51] E. Lughofer et al, Reliable All-Pairs Evolving Fuzzy Classifiers, IEEE Transactions on Fuzzy Systems, vol. 21 (4), pp. 625--641, 2013 [52] G. Ditzler and R. Polikar,\u201cIncremental learning of concept drift from streaming imbalanced data,\u201d in IEEE Transactions on Knowledge & Data Engineering, vol. 25(10), pp. 2283\u20132301, 2013 [53] J. Gama, P. Medas, G. Castillo, and P. Rodrigues, \u201cLearning with drift detection,\u201d in Proceeding of Brazilian Symposium on Artificial Intelligence., vol. 3171, pp. 286\u2013295, 2004 [54] K. S. Yap, et al, \u201cImproved GART neural network model for pattern classification and rule extraction with application to power system,\u201d IEEE Transactions on Neural Networks, vol. 22(12), pp. 2310\u20132323, 2011 [55] B. Vigdor and B. Lerner, \u201cThe Bayesian ARTMAP,\u201d IEEE Transactions Neural Networks, vol. 18(6), pp. 1628\u20131644, 2007 [56] J.-C. de Barros and A. L. Dexter, \u201cOn-line identification of computationally undemanding evolving fuzzy models,\u201d Fuzzy Sets and Systems, vol. 158, pp. 1997\u20132012, 2007 [57] Y. Xu, K. W. Wong, and C. S. Leung, \u201cGeneralized recursive least square to the training of neural network,\u201d IEEE Transactions on Neural Networks, vol. 17(1), pp. 19\u201334, 2006 [58] R. Polikar, L. Udpa, S. Udpa, V. Honavar, \u201cLearn++: An incremental learning algorithm for supervised neural networks,\u201d IEEE Transactions on System, Man and Cybernetics (C), Special Issue on Knowledge Management, vol. 31(4), pp. 497-508, 2001 [59] L. L. Minku and X. Yao, \u201cDDD: A new ensemble approach for dealing with drifts,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 24(4), pp. 619\u2013633, 2012 [60] L. L. Minku, A. P. White, and X. Yao, \u201cThe impact of diversity on online ensemble learning in the presence concept of drift,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 22(5), pp. 730\u2013742, 2010 [61] W.N. Street and Y. Kim, \u201cA Streaming Ensemble Algorithm (SEA) for Large-Scale Classification,\u201d in International Conference on Knowledge Discovery and Data Mining, pp. 377-382, 2001 [62] K. Subramanian, S. Suresh, N. Sundararajan, \u201cA metacognitive neurofuzzy inference system (McFIS) for sequential classification problems\u201d, IEEE Transactions on Fuzzy Systems, Vol. 21(6), pp. 1080-1095, 2013 [63] Agus Salim, et al, \u201c C-reactive protein and serum creatinine, but not haemoglobin A1c, are independent predictors of coronary heart disease risk in non-diabetic Chinese\u201d, European journal of preventive cardiology, Vol. 23(12), pp. 1339-1349, 2016 [64] D. E. Sr. Dimla, P.M. Lister, \u201c On-line Metal Cutting Tool Condition Monitoring. II: Tool-state Classification using Multi-Layer Perceptron Neural Networks\u201d, International Journal of Machine Tools and Manufacture, Vol. 40, 769-781, 2000 [65] P. Angelov, et al, \u201cEvolving fuzzy classifiers using different model architectures\u201d, Fuzzy Sets and Systems, Vol.159(23) ,pp.3160\u2013 3182, 2008 [66] P.Angelov et al, \u201cEvolving fuzzy rule-based classifiers from data streams,\u201d IEEE Transactions on Fuzzy Systems, vol. 16(6), pp. 1462\u20131475, 2008 [67] R. D. Baruah, P. Angelov, J. Andreu, \u201cSimpl_eClass: Simplified potentialfree evolving fuzzy rule-based classifiers,\u201d in Proceeding of IEEE International Conference on Systems, Man and Cybernetics, Anchorage, AK, USA, Oct. 7\u2013 9, 2011, pp. 2249\u20132254 [68] D. Kangin, P. Angelov, J. A. Iglesias, \u201c Autonomously Evolving Classifier TEDAClass\u201d, Information Sciences, Vol. 366, pp. 1-11, 2016 [69] D. Kangin, P. Angelov, J. A. Iglesias, A. Sanchis, \u201c Evolving Classifier TEDAClass for Big Data\u201d, in Proceeding of INNS conference on Big Data, pp. 9-18, 2015 [70] P. Angelov, X. Gu, \u201c MICE: Multi-layer multi-model images classifier ensemble\u201d, In proceeding of IEEE International Conference on Cybernetics, 2017 [71] P. Angelov, N. Kasabov, \u201cEvolving Intelligent Systems, eIS\u201d, IEEE SMC eNewsletter, Vol.15, pp. 1-13, 2006"}], "references": [{"title": "Knowledge Discovery from Data Streams", "author": ["J. Gama"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Autonomous Learning Systems: From Data Streams to Knowledge in Real-time", "author": ["P. Angelov"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning in Non-Stationary Environments: Methods and Applications", "author": ["M. Sayed-Mouchaweh", "E. Lughofer"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Incremental Learning of Concept Drift Using Evolving Type-2 Recurrent Fuzzy Neural Network", "author": ["M. Pratama", "J. Lu", "E. Lughofer", "G. Zhang", "M.J. Er"], "venue": "IEEE Transactions on Fuzzy Systems, on-line and in press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "et al", "author": ["G. Ditzler"], "venue": "\u201c Learning in Nonstationary Environments: A Survey\u201d, IEEE Computational Intelligence Magazine, Vol.10(4), pp. 12-25, ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["R.M. French"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "An approach to online identification of Takagi- Sugeno fuzzy models,", "author": ["P.Angelov", "D. Filev"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction", "author": ["N. Kasabov", "Q. Song"], "venue": "IEEE Transactions on Fuzzy Systems .vol10 (2).pp. 144\u2013154. ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "J", "author": ["M. Pratama"], "venue": "Lu, G.Zhang, \u201c Evolving Type-2 Fuzzy Classifier\u201d, online and in press, IEEE Transactions on Fuzzy Systems, on line and in press, ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["A. Lemos"], "venue": "Adaptive fault detection and diagnosis using an evolving fuzzy classifier, Information Sciences, vol. 220, pp. 64-85, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Ensemble Method Based on Individual Evolving Classifiers", "author": ["J.A Iglesias", "A. Ledezma", "A. Sanchiz"], "venue": "Evolving and Adaptive Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Analyzing the structure of ensembles based-on evolving classifiers", "author": ["J.A Iglesias", "A. Ledezma", "A. Sanchiz"], "venue": "FINO/CAEPIA,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "DELA: A Dynamic Online Ensemble Learning Algortihm", "author": ["A. Bouchachia"], "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Classifiers Ensemble for Changing Environments", "author": ["L. Kuncheva"], "venue": "Lecture Notes on Computer Sciences,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Ensemble of subset online sequential extreme learning machine for class imbalance and concept", "author": ["B. Mirza", "Z. Lin", "N. Liu"], "venue": "drift,\u201d Neurocomputing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Handling Drifts and Shifts in On-Line Data Streams with Evolving Fuzzy Systems", "author": ["E. Lughofer", "P. Angelov"], "venue": "Applied Soft Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Hellinger Distance based Drift Detection for Nonstationary Environments", "author": ["G. Dirzler", "R. Polikar"], "venue": "IEEE Symposium on Computational Intelligence in Computational Intelligence in Dynamic and Uncertain Environments,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Caballero-Mota, \u201cOnline and Non-Parametric Drift Detection Methods Based on Hoeffding\u2019s Bounds", "author": ["I. Frias-Blanco", "J.D. Campo-Avilla", "G. Ramos-Jimenes", "R. Morales- Bueno", "Y.A. Ortiz-Diaz"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sensitivity Growing and Pruning Method for RBF Networks in Online Learning Environments", "author": ["P.P.K. Chan"], "venue": "in International Conference on Machine Learning and Cybernetics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["J. Kolter", "M. Maloof"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "An on-line self-constructing neural fuzzy inference network and its applications", "author": ["C. Juang", "C. Lin"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1998}, {"title": "Recurrent Classifier Based on an Incremental Meta-cognitive-based Scaffolding Algorithm", "author": ["M.Pratama", "S.Anavatti", "J.Lu"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Extended sequential adaptive fuzzy inference system for classification problems,", "author": ["H.-J. Rong", "N. Sundarajan", "G.-B. Huang", "G.-S. Zhao"], "venue": "Evolving Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Evolving Fuzzy Systems --- Methodologies, Advanced Concepts and Applications", "author": ["E. Lughofer"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "An Incremental Type-2 Metacognitive Extreme Learning Machine", "author": ["M. Pratama", "G. Zhang", "M-J. Er", "S. Anavatti"], "venue": "IEEE Transactions on Cybernetics, online and in press,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Extensions of Vector Quantization for Incremental Clustering", "author": ["E. Lughofer"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "On-line Quality Control with Flexible Evolving Fuzzy Systems, in: Learning in Non-Stationary Environments", "author": ["E. Lughofer"], "venue": "Methods and Applications,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Generalized Smart Evolving Fuzzy Systems", "author": ["E. Lughofer"], "venue": "Evolving Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "A generalized growing and pruning RBF (GGAP-RBF) neural network for function  approximation,", "author": ["G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "Sequential adaptive fuzzy inference system (SAFIS) for nonlinear system identification and time series prediction,", "author": ["H.J. Rong", "N. Sundararajan", "G.B. Huang", "P. Saratchandran"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "An ensemble method based on evolving classifiers: eStacking", "author": ["J.A. Iglesias", "A. Ledezma", "A. Sanchis"], "venue": "IEEE Symposium on Evolving and Autonomous Learning System,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Reliable All-Pairs Evolving Fuzzy Classifiers", "author": ["E. Lughofer"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Polikar,\u201cIncremental learning of concept drift from streaming imbalanced data,", "author": ["R.G. Ditzler"], "venue": "IEEE Transactions on Knowledge & Data Engineering,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Learning with drift detection,", "author": ["J. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "Proceeding of Brazilian Symposium on Artificial Intelligence.,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Improved GART neural network model for pattern classification and rule extraction with application to power system,", "author": ["K.S. Yap"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "The Bayesian ARTMAP,", "author": ["B. Vigdor", "B. Lerner"], "venue": "IEEE Transactions Neural Networks,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2007}, {"title": "On-line identification of computationally undemanding evolving fuzzy models,", "author": ["J.-C. de Barros", "A.L. Dexter"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "Generalized recursive least square to the training of neural network,", "author": ["Y. Xu", "K.W. Wong", "C.S. Leung"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2006}, {"title": "Learn++: An incremental learning algorithm for supervised neural networks,", "author": ["R. Polikar", "L. Udpa", "S. Udpa", "V. Honavar"], "venue": "IEEE Transactions on System, Man and Cybernetics (C), Special Issue on Knowledge Management,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2001}, {"title": "DDD: A new ensemble approach for dealing with drifts,", "author": ["L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "The impact of diversity on online ensemble learning in the presence concept of drift,", "author": ["L.L. Minku", "A.P. White", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification,", "author": ["W.N. Street", "Y. Kim"], "venue": "in International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "A metacognitive neurofuzzy inference system (McFIS) for sequential classification problems", "author": ["K. Subramanian", "S. Suresh", "N. Sundararajan"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "C-reactive protein and serum creatinine, but not haemoglobin A1c, are independent predictors of coronary heart disease risk in non-diabetic Chinese", "author": ["Agus Salim"], "venue": "European journal of preventive cardiology,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Evolving fuzzy classifiers using different model architectures", "author": ["P. Angelov"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2008}, {"title": "Evolving fuzzy rule-based classifiers from data streams,", "author": ["P.Angelov"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2008}, {"title": "J", "author": ["R.D. Baruah", "P. Angelov"], "venue": "Andreu, \u201cSimpl_eClass: Simplified potentialfree evolving fuzzy rule-based classifiers,\u201d in Proceeding of IEEE International Conference on Systems, Man and Cybernetics, Anchorage, AK, USA, Oct. 7\u2013 9", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Evolving Intelligent Systems, eIS", "author": ["P. Angelov", "N. Kasabov"], "venue": "IEEE SMC eNewsletter,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The data-intensive era where data are collected continuously in a fast rate under dynamic and evolving environments opens a new research direction to process data streams efficiently [1], [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "INTRODUCTION The data-intensive era where data are collected continuously in a fast rate under dynamic and evolving environments opens a new research direction to process data streams efficiently [1], [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "Another challenging trait of data streams lies in the non-stationary characteristics [3] where the data does not follow static and predictable distributions and contains a variety of concept drifts [4], [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "These facts make a retraining phase when incorporating a new sample to an old dataset impossible to be performed because it leads to the socalled catastrophic forgetting [6] of previously valid knowledge and is not scalable when dealing with massive data streams.", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "Evolving Intelligent System (EIS) provides a unique solution for data stream mining because a strictly one-pass learning procedure involved here has delivered great success to cope with time-critical applications where data streams are generated at a very fast sampling rate [7].", "startOffset": 275, "endOffset": 278}, {"referenceID": 7, "context": "Furthermore, EIS adopts an open structure where its components can be automatically generated, pruned, merged and recalled on the fly [8], [9] and can be well-suited to a given problem.", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "This trait reflects the true data distributions and tracks changing data distributions [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 49, "context": "EIS has transformed to be one of the most active research area in the computational intelligence research as evidenced by the number of published works in this area [71].", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "Nonetheless, EIS is typically built upon a single classifier architecture which often does not produce adequate accuracy for complex problems [11], [35].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "Nonetheless, EIS is typically built upon a single classifier architecture which often does not produce adequate accuracy for complex problems [11], [35].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "In fact, from classical batch learning perspective, it is well-known that ensemble classifiers outperform single base classifiers in case of high noise levels and a low number of available training samples [12] because they can better resolve the bias-variance dilemma due to proper subspace and data exploration using weak classifiers [13].", "startOffset": 336, "endOffset": 340}, {"referenceID": 11, "context": "While few works about a synergy between EIS and an ensemble structure can be found in the literature [14], [15], most of them utilise a static ensemble architecture, which should be predetermined in advance.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "While few works about a synergy between EIS and an ensemble structure can be found in the literature [14], [15], most of them utilise a static ensemble architecture, which should be predetermined in advance.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "The ensemble learning concept uses combination of individual base classifiers with a modularity principle, where it enables a dynamic evolution of the ensemble structure [12][19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Adaptability of the ensemble classifier plays a vital role to the success of ensemble learning because it formulates mechanisms how an ensemble classifier adapts itself when changing data distributions are presented [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 16, "context": "The ensemble classifier can also be distinguished into two groups: active and passive approach: the passive approach relies on continuous updates of its components and assumes that the concept drifts occur in the ongoing fashion; the active approach is equipped by a dedicated drift detection mechanism in which it is restructured and parameters are fine-tuned when a drift is captured [19].", "startOffset": 386, "endOffset": 390}, {"referenceID": 13, "context": "Although ensemble algorithms like DELA [16] is excluded from the local concept drift bottleneck due to its three levels of adaptivity, namely structural adaptivity, combination adaptivity, model adaptivity, it suffers from the absence of a dedicated drift detection method [16].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Although ensemble algorithms like DELA [16] is excluded from the local concept drift bottleneck due to its three levels of adaptivity, namely structural adaptivity, combination adaptivity, model adaptivity, it suffers from the absence of a dedicated drift detection method [16].", "startOffset": 273, "endOffset": 277}, {"referenceID": 21, "context": "The dynamic ensemble concept is inspired by the evolving trait of DWM [34] but different criteria are applied to perform the structural learning scenarios of pENsemble.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "\u2022 Online Drift Detection Scenario: pENsemble adopts a dynamic ensemble structure where a new local expert can be added when a concept change presents in the data streams [26].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "This procedure is governed by a non-parametric drift detection method derived from the concept of Hoeffding\u2019s bounds [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "This method estimates generalization performance of a local expert [29] and determines local experts to be pruned [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "This paper conveys four major contributions as follows: 1) a novel ensemble learning algorithm inspired by a seminal work, namely DWM [34], is proposed.", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "DENFIS in [9] is another early example of EIS which combines the working principle of TSK fuzzy system and the Evolving Clustering Method (ECM).", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Angelov and Filev proposed the so-called eTS [7] which benefits from the data potential theory forming an evolving version of the mountain clustering.", "startOffset": 45, "endOffset": 48}, {"referenceID": 46, "context": "This work is modified for a classification problem [65], [66] and has formed the first evolving classifier, termed eClass.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "This work is modified for a classification problem [65], [66] and has formed the first evolving classifier, termed eClass.", "startOffset": 57, "endOffset": 61}, {"referenceID": 49, "context": "The term EIS has not been however formalised until the clarification in [71] since the term \u201cevolving\u201d is sometime confused with the concept of evolutionary computation.", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 87, "endOffset": 91}, {"referenceID": 48, "context": "Several extensions and variations of EIS have been put forward in the literature [39], [40], [67]-[70].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 185, "endOffset": 189}, {"referenceID": 29, "context": "An evolving version of Vector quantization was designed in [41] and is algorithmic backbone of FLEXFIS [42], which was later extended to a more robust version including rule merging in [43], generalized rules and an incremental feature weighting mechanism in [44].", "startOffset": 259, "endOffset": 263}, {"referenceID": 29, "context": "A generalized TSK fuzzy rule was put forward in [45]-[47] and generates a non-axis parallel ellipsoidal cluster, which happens to have better coverage and flexibility than conventional fuzzy rules [44].", "startOffset": 197, "endOffset": 201}, {"referenceID": 30, "context": "statistical contribution borrowing the concept of hidden neuron statistical contribution in [48], [49].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "statistical contribution borrowing the concept of hidden neuron statistical contribution in [48], [49].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Evolving Ensemble (eEnsemble) was proposed in [14] where it makes use of eTS [7] as a base-classifier and is realised under different configurations of the ensemble classifier.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Evolving Ensemble (eEnsemble) was proposed in [14] where it makes use of eTS [7] as a base-classifier and is realised under different configurations of the ensemble classifier.", "startOffset": 77, "endOffset": 80}, {"referenceID": 32, "context": "This work was extended in [50] where eStacking is put forward using the concept of stacking ensemble.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "The all-pair classifier in [50] can be also grouped as an ensemble approach.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 287, "endOffset": 291}, {"referenceID": 12, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 293, "endOffset": 297}, {"referenceID": 32, "context": "The ensemble learning concept is well-known for its powerful generalization power because it address the bias-and-variance better and produces a model with high diversity covering a rich data region; 2) The use of evolving base classifier in the ensemble structure has been initiated in [14], [15], [50], [69], [70] but it relies on a static ensemble structure which is predetermined during the training process; 3) Existing EISs are categorized as a passive approach in handling concept drift because changing data distributions are overcome by continuously adapting a classifier.", "startOffset": 299, "endOffset": 303}, {"referenceID": 21, "context": "pENsemble stores a collection of local experts, which can be automatically generated when a drift is detected and pruned when it is no longer relevant to capture current data trends [34].", "startOffset": 182, "endOffset": 186}, {"referenceID": 19, "context": "The drift detection strategy relies on the concept of Hoeffding\u2019s bounds to determine the drift\u2019s level [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "The statistical process control approach is integrated to monitor dynamic of data streams [53] and classifies system behaviours into three stages, namely normal, warning and drift.", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "It allows an ensemble structure to expand its size when an uncharted training region comes into picture [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "An online non-parametric drift detection method is integrated using the Hoeffding\u2019s inequalities to determine acceptable level of concept changes in data streams [27].", "startOffset": 162, "endOffset": 166}, {"referenceID": 19, "context": "This method is capable of capturing significant distributional changes in data streams in the one-pass mode and is confirmed by solid theoretical guarantees in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "It is worth mentioning that the drift handling strategy in [23] does not specifically detect the exact time period where a drift presents since it is derived from the forgetting concept \u2013 categorized as a passive approach.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Referring to original work [27], two performance metrics, namely moving average and weighted moving average, are put forward.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "This approach is similar to the idea of statistical process control [53] except the basis of normality is relaxed here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "We apply the same settings in [27] where , W D \uf061 \uf061", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Such trait is capable of lowering the fuzzy rule demand and retains inter-relations among input variables [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The statistical contribution, however, ignores summarization power of a rule because it does not consider how strategic a current position of rule in the feature space is [24], [38].", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "This concept follows the concept of recursive density estimation (RDE) [2], [7] where a density of a local region is computed recursively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "This concept follows the concept of recursive density estimation (RDE) [2], [7] where a density of a local region is computed recursively.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "The DQ method differs from the RDE method [7] in two facets: 1) it involves a weighting strategy reducing the influence of outliers which causes a drop of density for next samples; 2) it uses the inverse multi-quadratic function in lieu of the Cauchy function; 3) it is tailored for the multivariate Gaussian function.", "startOffset": 42, "endOffset": 45}, {"referenceID": 36, "context": "The third rule growing strategy aims to overcome this issue borrowing the concept of GART+ [54].", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "It limits the growth of the winning rule where a new rule is introduced when the size of winning rule exceeds a prespecified level [55].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "This scenario is realised by extending the concept of data potential [7], [56] for the rule pruning scenario.", "startOffset": 69, "endOffset": 72}, {"referenceID": 38, "context": "This scenario is realised by extending the concept of data potential [7], [56] for the rule pruning scenario.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "Furthermore, pClass utilises a direct update scheme of the inverse covariance matrix according to the formulas derived in [39] which shelves the reinversion of the covariance matrix.", "startOffset": 122, "endOffset": 126}, {"referenceID": 37, "context": "This winning rule selection is preferred over the compatibility measure [55] since it takes into account the rule\u2019s population.", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "The FWGRLS is a derivation of the FWRLS method originally proposed by Angelov in [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 39, "context": "It borrows the concept of weight decay function of the GRLS method in [57].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "\u2022 Learn++NSE is seen as one of pioneer works in dynamic ensemble classifier for non-stationary environments [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "It presents an extension of Learn++ [58] to tackle concept drifts in data streams.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "\u2022 Learn++CDE is a generalized version of Learn++NSE integrating a specific mechanism to handle the class imbalanced problem in data streams [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "\u2022 eT2Class is another case of evolving classifiers unifying the dynamic network structure and the online learning capability [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 44, "context": "\u2022 McFIS characterises the so-called metacognitive learning machine assumed as an extension of evolving classifiers [62].", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "Popular DDD problems characterizing the abrupt and gradual drifts, namely sin, sinh, line and 10dplane, were explored to investigate the performance of consolidated algorithms [59], [60].", "startOffset": 176, "endOffset": 180}, {"referenceID": 42, "context": "Popular DDD problems characterizing the abrupt and gradual drifts, namely sin, sinh, line and 10dplane, were explored to investigate the performance of consolidated algorithms [59], [60].", "startOffset": 182, "endOffset": 186}, {"referenceID": 43, "context": "The SEA problem introduced in [61] was used to bear out the efficacy of benchmarked algorithms.", "startOffset": 30, "endOffset": 34}, {"referenceID": 34, "context": "Moreover, an extension of the SEA problem contributed by Ditzler and Polikar [52] was put forward instead of its original version since it offers the class imbalance property and the cyclical drift which often occurs in the real-world data streams.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Another popular problem in the data stream mining area, namely the Gaussian problem, was exploited [18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "This scenario leads to a more resilient approach to deal with the plasticity-stability dilemma than static ensemble or greedy ensemble [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 45, "context": "The subjects of the experiment were only those donated their blood and never suffered from CHD or stroke verified from self-reported diagnosis or data from the hospital discharge database [63].", "startOffset": 188, "endOffset": 192}], "year": 2017, "abstractText": "The concept of ensemble learning offers a promising avenue in learning from data streams under complex environments because it addresses the bias and variance dilemma better than its single-model counterpart and features a reconfigurable structure, which is well-suited to the given context. While various extensions of ensemble learning for mining nonstationary data streams can be found in the literature, most of them are crafted under a static base-classifier and revisits preceding samples in the sliding window for a retraining step. This feature causes computationally prohibitive complexity and is not flexible enough to cope with rapidly changing environments. Their complexities are often demanding because it involves a large collection of offline classifiers due to the absence of structural complexities reduction mechanisms and lack of an online feature selection mechanism. A novel evolving ensemble classifier, namely Parsimonious Ensemble (pENsemble), is proposed in this paper. pENsemble differs from existing architectures in the fact that it is built upon an evolving classifier from data streams, termed Parsimonious Classifier (pClass). pENsemble is equipped by an ensemble pruning mechanism, which estimates a localized generalization error of a base-classifier. A dynamic online feature selection scenario is integrated into the pENsemble. This method allows for dynamic selection and deselection of input features on the fly. pENsemble adopts a dynamic ensemble structure to output a final classification decision where it features a novel drift detection scenario to grow the ensemble\u2019s structure. The efficacy of the pENsemble has been numerically demonstrated through rigorous numerical studies with dynamic and evolving data streams where it delivers the most encouraging performance in attaining a tradeoff between accuracy and complexity.", "creator": "Microsoft\u00ae Word 2016"}}}