{"id": "1705.07290", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "abstract": "We deal with the problem of reconstructing sparse signals from noise and compression measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage threshold algorithm (ISTA). We obtain the weights and distortions of network connections required by ISTA, and model the nonlinear activation function using a linear extension of thresholds (LET) that has been very successful in image denosis and deconvolution. The optimal composition of the coefficients of parameterized activation is learned from a training dataset containing knife-saving signal pairs that correspond to a fixed sensor matrix. For training, we develop an efficient second-order algorithm that requires only matrix vector product calculations in each training period (hessionless optimization) and provides superior convergence performance as a gradient descend optimization of the network architecture inspired by ISTA, which is an improved ISTA sequence of ISTA.", "histories": [["v1", "Sat, 20 May 2017 11:14:39 GMT  (3497kb,D)", "http://arxiv.org/abs/1705.07290v1", "Submission date: November 11, 2016. 19 pages; 9 figures"]], "COMMENTS": "Submission date: November 11, 2016. 19 pages; 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["debabrata mahapatra", "subhadip mukherjee", "chandra sekhar seelamantula"], "accepted": false, "id": "1705.07290"}, "pdf": {"name": "1705.07290.pdf", "metadata": {"source": "CRF", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "authors": ["Debabrata Mahapatra", "Subhadip Mukherjee", "Chandra Sekhar Seelamantula"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Compressive sensing, sparse recovery, iterative shrinkage, ISTA, FISTA, Hessian-free optimization, deep learning, back-propagation, linear expansion of thresholds (LET).\nF"}, {"heading": "1 INTRODUCTION", "text": "E STIMATION of sparse signals from inaccurate linearmeasurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade. Most signals occurring in real-world applications admit a sparse representation in an appropriate basis. A significant amount of research has gone into designing suitable measurement matrices, and in developing efficient algorithms to reconstruct signals from noisy and incomplete set of measurements, based on the premise of sparsity. The problem is also equivalent to that of best basis selection or sparse coding, wherein one intends to find the sparse representation of a signal in an overcomplete set of basis vectors. Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.. The applications of CS have transcended the domain of classical signal processing and gone much beyond. For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc. have immensely benefitted from the developments in CS.\nThe objective in CS is to recover a sparse signal x \u2208 Rn from a lower-dimensional measurement vector y \u2208 Rm, m < n, given by y = Ax+\u03be, where A is the sensing matrix and \u03be is the measurement noise. Without assuming a prior on x, the task of signal reconstruction is ill-posed. The goal of CS algorithms is to seek the sparsest x that is consistent\nThe authors are with the Indian Institute of Science, Department of Electrical Engineering, Bangalore - 560 012, India. Telephone: +91 80 2293 2695; Fax: +91 80 2360 0444; Email: chandra.sekhar@ieee.org, subhadip@ee.iisc.ernet.in, mahapatradebabrata91@gmail.com. This manuscript was submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence (Manuscript ID: TPAMI-2016-11-0861; Submission date: November 11, 2016).\nwith the measurement y. However, searching over the space of all possible patterns of sparsity is a combinatorial problem and prohibitively expensive. In a seminal work [13], Cande\u0300s et al. showed that it is possible to obtain a stable estimate of a sparse signal x with s nonzero entries, by solving a tractable convex program, provided that \u2016\u03be\u20162 \u2264 and A satisfies \u03b43s+3\u03b44s < 2, where \u03b4s denotes the restricted isometry constant (RIC) of order s. The convex programming formulation in [13] is also known as basis pursuit denoising.\nRecently, research on supervised learning of a deep neural network (DNN) for approximating fairly complex nonlinear functions has gained significant momentum. Because of the availability of abundant training data and enhanced computational capability, DNN-based algorithms outperform many classical techniques in applications related to natural language processing, speech processing, computer vision, etc., to name a few. For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17]. Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems. Several DNNbased methods have also been proposed for unsupervised feature extraction [23], [24]. Specifically, the spike-and-slab model [25] has been shown to work efficiently for sparse feature extraction from large-scale data.\nSparse coding can be interpreted as a function approximation problem, where the goal is to undo the effect of the sensing matrix A to estimate x from y, subject to the constraint of sparsity. Consequently, it is natural to ask how much sparse coding can benefit from a trained DNN, es-\nar X\niv :1\n70 5.\n07 29\n0v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\n01 7\nii\npecially in applications where sufficient number of training examples are available. Gregor and Le Cun showed, for the first time, that a custom-designed discriminatively trained DNN architecture can solve the sparse coding problem efficiently [26]. Their framework is referred to as the learned iterative shrinkage and thresholding algorithm (ISTA). Further efforts have been made to make a one-to-one correspondence between iterative inferencing algorithms and DNNs [27], and to efficiently represent the nonlinearity that is at the heart of sparse coding [28]. Continuing along the same line of investigation, we show that a custom-designed, discriminatively trained DNN architecture can solve the sparse coding problem efficiently. We next give an overview of deep-learning-based algorithms for sparse coding available in the literature, before highlighting our contribution."}, {"heading": "1.1 Prior Art", "text": "The use of a trained NN for sparse coding is primarily based on the observation that the updates in an iterative algorithm can be interpreted as the layers of a deep neural network. This unfolding process has been applied in the context of ISTA [29]. The possibility of using a trained feed-forward neural network for efficient sparse coding was first demonstrated by Gregor and LeCun [26]. Their model, termed as learned ISTA, implements a truncated version of ISTA with trained weights and biases instead of precomputed ones. The parameters of the network are trained over a dataset of measurement-signal pairs by minimizing the estimation error. However, the number of parameters to be trained in the learned ISTA model scales as n2, where n is the dimension of the sparse signal to be estimated. Hershey et al. [27] proposed the idea of deep unfolding, wherein an iterative inference strategy, such as ISTA, inspires the architecture of a DNN. The model parameters are untied across the layers and trained discriminatively using gradient-descent (GD). The idea of unfolding an iterative inference algorithm with shared parameters over the layers was developed by Domke [30], [31], in the context of tree-reweighted belief propagation and mean-field inference. Sprechmann et al. [32] proposed a learnable architecture of fixed complexity, by unfolding the iterative proximal descent algorithms for structured sparse and low-rank models. The framework is termed as process-centric parsimonious modeling and is capable of achieving similar or better performance than modelcentric iterative approaches, while offering significant reduction in complexity. Similar ideas based on unfolding were exploited by Wang et al. [33] to design a deep `0 encoder, with applications in image classification and clustering. Recently, Xin et al. [34] critically analyzed the merits of designing trainable models over conventional optimizationbased sparse coding algorithms. They considered different architectures that improve the effective RIC, thereby successfully alleviating the issue of disruptive correlation in the dictionary, which might cause standard sparse coding techniques to fail. Kamilov and Mansour [28] proposed a deep architecture for sparse coding motivated by ISTA, wherein the nonlinear activation function is modeled using cubic B-splines. Their approach has at least two advantages: (i) the reconstruction accuracy is better than that of ISTA; and (ii) the number of parameters to be learnt does not\nscale with the signal dimension n. However, we shall show, among other things, that the choice of the basis functions could be optimized, which leads to a more parsimonious representation of the thresholding function."}, {"heading": "1.2 This Paper", "text": "Akin to Kamilov and Mansour, we construct a DNN architecture for solving the sparse coding problem by unfolding the ISTA iterations (Section 2). However, we parameterize the nonlinear activation function using a linear expansion of thresholds (LET) [37] instead of cubic B-splines (Section 3). The resulting network is referred to as the LETnet. The weights and biases in each layer of LETnet are precomputed following the ISTA prescription and kept fixed, while the parametrized activation function is fine-tuned to fit the training data (Section 4). We show that such a model has lesser number of parameters to train, without compromising the learning ability of the network. Unlike [28], the coefficients of the LET-based activations are untied across the layers of the network to enhance the expressive power of the model, without significantly affecting the training overhead. Also, a small number of coefficients, typically five per layer, suffice, which results in considerably less number of parameters to learn overall in comparison with [28]. The LETnet architecture is trained discriminatively over a dataset to optimally tune the parameters for a given sensing matrix. We demonstrate that the LET-based activation function induces a variety of sparsity promoting regularizers and optimum trade-off between noise rejection and signal sparsity can be achieved in every layer by learning the LET coefficients appropriately. The flexibility to design and train a parametric family of regularizers to encourage sparsity, while effectively suppressing noise using a parsimonious parameterization is one of the major advantages of our approach. We also derive an efficient Hessian-free optimization (HFO) technique [38] to train the network (Section 5). Numerical experiments on synthesized signals indicate that the trained LETnet is capable of producing an improvement in signal-to-noise ratio (SNR) of approximately 3 to 4 dB over the competing techniques (Section 6).\nSubsequently, we also show that it is possible to further reduce the number of layers in the LETnet, without compromising the recovery performance. The motivation is derived from the fast iterative shrinkage threshold algorithm (FISTA) [5], which has a faster convergence rate than ISTA without increasing the computational load per iteration, thereby requiring considerably less number of iterations overall. We show that the resulting fast LETnet architecture, dubbed as fLETnet (Section 7), bears close resemblance to the recently proposed deep residual learning architecture [15], wherein each layer draws direct connections from two preceding layers instead of one, so as to improve the convergence performance. It has been shown in [15] that the scheme enjoys the merits of a deep architecture, while successfully alleviating the problems of vanishing/exploding gradients. This advantage comes without the need to learn additional parameters. We carry out experimental validation of LETnet and fLETnet for sparse signal recovery and demonstrate their superiority over the the learning-based approach proposed in [28] as well as the conventional sparse coding algorithms that are not set up within a learning paradigm.\niii"}, {"heading": "2 ITERATIVE SHRINKAGE ALGORITHMS: A NEURAL NETWORK PERSPECTIVE", "text": "Compressive sensing deals with the recovery of a sparse signal x \u2208 Rn from a measurement of the form\ny = Ax + \u03be, (1)\nwhere y \u2208 Rm, m < n, A \u2208 Rm\u00d7n is the sensing matrix and \u03be denotes the measurement noise. The signal x could be sparse itself or it could admit a sparse representation in an appropriate basis B \u2208 Rn\u00d7n, meaning that x = B\u03b1, where \u03b1 \u2208 Rn is sparse. For example, natural images can be represented sparsely if B is taken as the orthogonal discrete cosine transform or wavelet basis. Without loss of generality, we consider sparsity of x in the canonical basis (columns of an identity matrix). If x admits a sparse representation in a basis B different from the identity, A and x in (1) should be replaced with A\u0304 = AB and \u03b1, respectively.\nIn order to recover x from y, one is required to solve the combinatorially hard optimization\nx\u0302 = arg min x \u2016x\u20160 subject to \u2016y \u2212Ax\u20162 \u2264 , (2)\nwhere the `0-norm of x, denoted by \u2016x\u20160, counts the number of nonzero entries in x. Solving (2) using an exhaustive search over all possible sparsity patterns becomes computationally intractable as it grows exponentially with n. There are two distinct algorithmic paradigms to circumvent this problem: (i) greedy algorithms; and (ii) relaxation-based approaches.\nIn greedy algorithms, one first estimates the support of x using a greedy iterative approach, and subsequently computes a least-squares (LS) estimate over the estimated support to obtain the amplitudes. Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc. and their several variants, fall under this category.\nIn relaxation-based algorithms, one minimizes a regularized quadratic penalty of the form\nmin x\n1 2 \u2016y \u2212Ax\u201622 + \u03bbG (x) , (3)\nwhere the regularizer G (x) allows one to incorporate priors about x. In this case, G is chosen appropriately to promote sparsity in the estimate, which stabilizes the otherwise ill-posed inverse problem. In the special case where G (x) = \u2016x\u20161, the resulting optimization is referred to as LASSO regression [42], [43]. The parameter \u03bb > 0 trades-off between the signal prior and data fidelity."}, {"heading": "2.1 Proximal Gradient Methods", "text": "To make the exposition self-contained, we briefly recall the connection between the proximal gradient methods and the feed-forward neural networks (NN), originally established by Gregor and LeCun [26]. Consider an iterative algorithm for solving (3), which generates an estimate xt in the tth iteration. Denoting f (x) = 12 \u2016y \u2212Ax\u2016 2 2, the affine approximation ft (x) of f (x) at xt is given by\nft (x) = f ( xt ) + ( x\u2212 xt )>\u2207f (xt) . (4)\nThe GD algorithm for minimizing f (x), with a fixed stepsize \u03b7 at every t, takes the form\nxt+1 = xt \u2212 \u03b7\u2207f ( xt ) .\nThe GD update corresponds to the minimization of a quadratically-regularized affine approximation:\nxt+1 = arg min x\nft (x) + 1\n2\u03b7\n\u2225\u2225x\u2212 xt\u2225\u22252 2 .\nAdopting a similar approximation for solving (3) leads to an iterative update of the form\nxt+1 = arg min x\nft (x) + 1\n2\u03b7\n\u2225\u2225x\u2212 xt\u2225\u22252 2 + \u03bbG (x) ,\nwhich, after rearranging the terms, can be written as\nxt+1 = arg min x\n1\n2\u03b7\n\u2225\u2225x\u2212 (xt \u2212 \u03b7\u2207f (xt))\u2225\u22252 2 + \u03bbG (x) . (5)\nIf G (x) is separable, that is, if G (x) = \u2211ni=1 Gi (xi), where xi denotes the ith entry of x, then solving (5) boils down to solving n independent 1-D optimization problems:\nxt+1i = arg minxi\n1\n2\u03b7 \u2223\u2223xi \u2212 uti\u2223\u22232 + \u03bbGi (xi) , i = 1, 2, \u00b7 \u00b7 \u00b7 , n, where ut = xt \u2212 \u03b7\u2207f (xt). Defining the proximal operator\nP g\u03bd (u) \u2206 = arg min\nx\n1 2 |x\u2212 u|2 + \u03bdg(x), (6)\nfor g(\u00b7) corresponding to the regularization parameter \u03bd, and assuming Ri \u2261 g for every i, one can write\nxt+1 = P g\u03bd ( xt \u2212 \u03b7\u2207f ( xt )) , (7)\nwhere \u03bd = \u03bb\u03b7, and the proximal operator is applied coordinate-wise on its argument. In the special case, albeit an important one, where g(x) = |x|, the proximal operator turns out to be the soft-thresholding (ST) function T\u03bd (v) = sgn(v) max {|v| \u2212 \u03bd, 0}, and the resulting update rule becomes the popular ISTA. Using the expression \u2207f (x) = A> (Ax\u2212 y), (7) is rewritten as\nxt+1 = P g\u03bd ( Wxt + b ) , (8)\nwhere W = I \u2212 \u03b7A>A and b = \u03b7A>y. Gregor and LeCun [26] interpreted (8) as the feed-forward computation through a NN with weight matrix W and bias b shared across every layer. The input to the NN is the initial estimate x0. The proximal operator P g\u03bd (\u00b7) plays the role of nonlinear activation function of the neurons in layer t. An estimate obtained after L iterations of (8) is the same as the output of the NN containing L layers. Thus, there is a direct correspondence between ISTA and a feed-forward NN."}, {"heading": "3 PARAMETRIC ACTIVATION: LINEAR EXPANSION", "text": ""}, {"heading": "OF THRESHOLDS (LET)", "text": "For convenience, we denote the NN analogue of the generic proximal operator P g\u03bd , namely the activation function, as \u03c8. Our approach concerns the construction of \u03c8 by taking a linear combination of K elementary thresholding functions:\n\u03c8(u) = K\u2211 k=1 ck\u03c6k(u), u \u2208 R. (9)\niv\nThis design offers more flexibility than the ST operator T\u03bd , because, by choosing a moderately small number (typically less than 10) of appropriate elementary thresholding functions \u03c6k, one could construct a rich variety of activations. We advocate the use of elementary functions based on the derivatives of a Gaussian (DoG) [35], [37], specified as\n\u03c6k(u) = u exp\n( \u2212 (k \u2212 1)u 2\n2\u03c42\n) , 1 < k \u2264 K, (10)\nas the basis functions for constructing \u03c8. The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].\nWe take a supervised learning approach to solve the sparse coding problem, with the goal of learning the coefficients ck optimally for a given training dataset containing measurement-signal pairs. The training objective is to minimize the aggregate error between the predictions made by the network and the corresponding ground-truth signals. ISTA for the sparse estimation problem has a linear rate of convergence [5] and performs reasonably well for sufficiently large number of iterations. Therefore, the ST is a reasonable choice for the activation function to begin with, which we intend to improve during the course of training. For this reason, we select the coefficients ck and the parameter \u03c4 such that \u03c8 approximates the ST function closely. We observed empirically that fixing \u03c4 = \u03bd3 works well over a large variety of data, and the parameters ck are computed to minimize the fitting error in a least-squares sense.\nIt is possible to determine the regularizer associated with a sparsity promoting proximal operator. In the context of the LET, we show that there exists a regularizer g for which the LET \u03c8 turns out to be the proximal operator. Before proceeding further, we introduce the following definition of the inverse of the LET, which, depending on the parameters, may not always be invertible in the conventional sense due to lack of strict monotonicity of the LET.\nDefinition 1. The inverse of the LET-based activation func-\ntion \u03c8 is defined as\n\u03c8\u22121(q) 4 =  max{r : q = \u03c8(r)} for q > 0, 0 for q = 0, and min{r : q = \u03c8(r)}, for q < 0. (11)\nAn example of the inverse of an LET activation is shown in Figure 1(b). Proposition 1. The LET-based activation function \u03c8 is a\nproximal operator corresponding to the symmetric regularizer g(x) constructed as\ng(x) = { g\u0303(x), if x \u2265 0, g\u0303(\u2212x), otherwise,\nwhere g\u0303(x) = 1\u03bd \u222b x 0 (\u03c8\n\u22121(q)\u2212 q) dq, for x \u2265 0. Proof: The function g\u0303 is well-defined since the integrand is bounded and continuous everywhere except at q = 0. The function g so constructed is differentiable. Since g(x) is symmetric, it has an anti-symmetric derivative dg(x)dx = 1 \u03bd ( \u03c8\u22121 (x)\u2212 x ) , x \u2208 R. By the definition of the proximal operator in (6), we have that\nP g\u03bd (u) = x \u2217 \u2208 arg min\nx\u2208R\n1 2 |x\u2212 u|2 + \u03bd g(x).\nIt follows from elementary calculus that\nx\u2217 \u2212 u+ \u03bd dg(x) dx \u2223\u2223\u2223\u2223 x\u2217 = 0. (12)\nSubstituting \u03bd dg(x)dx \u2223\u2223\u2223 x\u2217\n= \u03c8\u22121 (x\u2217)\u2212 x\u2217 in (12), we get that u = \u03c8\u22121 (x\u2217), thereby leading to x\u2217 = \u03c8(u).\nThe regularizers corresponding to the LET-based proximal operators shown in Figure 1a are illustrated in Figure 1c. We observe that controlled variations in the LET coefficients result in a wide variety of sparsity-encouraging regularizers. The parametric control to design a rich class of sparsityinducing regularizers is a major advantage of the LET. Note that the definition of the proximal operator does not make any assumption regarding the convexity of g. In fact, a classic example of non-convex sparsity encouraging regularizer\nv is the `0 quasi-norm, whose proximal operator is the hardthresholding function. Notably, the `p quasi-norm, for any 0 \u2264 p < 1, is a non-convex regularizer encouraging sparsity."}, {"heading": "4 ARCHITECTURE OF THE PROPOSED NETWORK", "text": "For a proximal operator P g\u03bd , with \u03bd = \u03bb\u03b7, the operations in (7) can be expressed compactly as\nxt+1 = \u03c8 ( x\u0303t+1 ) , (13)\nwhere x\u0303t+1 = Wxt + b, (14)\nwith W and b as defined in Section 2.1. The proximal operator P g\u03bd in (7) is replaced by the activation \u03c8 applied element-wise on x\u0303t+1. Each iteration in (13) can be viewed as an affine operation followed by a nonlinear activation. This perspective establishes a bridge between iterative algorithms for sparse coding and feed-forward NN. Motivated by the flexibility offered by LET-based activation and efficient parametric control over the induced regularizers, as elucidated in Section 3, we employ them as nonlinear activation functions. In the sequel, we refer to a NN with LET-based activation as LETnet. Unlike the classical feedforward NN, LETnet contains fixed weights and biases determined by the sensing matrix A and the measurement y at every layer, whereas the LET coefficients are optimized during training, for which we propose efficient algorithms. We consider two architectures: LETnetFixed, where the LET parameters across all the layers are tied; and LETnetVar, where the LET parameters are allowed to vary across layers. The numerical experiments indicate that the LETnetVar results in better reconstruction compared with LETnetFixed, owing to higher flexibility.\nRecently, Kamilov et al. [28] proposed to employ a parametric linear combination of cubic B-splines instead of the soft-threshold, to parametrize the activation function, which is optimized during training and kept fixed across all layers. The resulting network is referred to as minimum mean-squared error ISTA (MMSE-ISTA). We shall show that, one of the advantages of using an LET-based activation is that it takes remarkably less number of basis functions (K \u2248 5) to cover a fairly rich class of regularizers without sacrificing the representational ability of the network. In contrast, MMSE-ISTA requires K \u2248 8000 cubic B-spline bases. Therefore, the proposed NN model is less likely to over-fit a fixed training set, as it uses less number of free parameters.\nThe proposed LETnetVar architecture is illustrated in Figure 2. The initial estimate x0 (typically an all-zero vector) is passed through the layers of the LETnetVar to obtain an estimate xL of the true sparse signal. Once the activation function coefficients are optimized for a fixed sensing matrix A, it is straightforward to predict x given a new noisy and compressive measurement \u2013 it is simply a matter of computing one forward pass through the network. While the computational complexities of LETnetFixed/LETnetVar per layer and ISTA per iteration are identical, the network has far fewer layers than the number of iterations of ISTA, leading to an overall lower complexity. The reconstruction accuracy is also superior in case of LETnetVar/LETnetFixed.\nAlgorithm 1 The BACK-PROPAGATION algorithm for the LETnetVar architecture Input: A training pair (y,x), the sensing matrix A\n1: Perform a forward pass through the LETnetVar to compute xt and x\u0303t, t = 1 : L. 2: Initialize t\u2190 L and \u2207xtJ \u2190 xL \u2212 x. 3: for t = L down to 1 do 4: \u2207ctJ = ( \u03a6t )>\u2207xtJ , where \u03a6ti,k = \u03c6k (x\u0303ti)\n5: \u2207x\u0303tJ = diag ( \u03c8 \u2032(t) (xt) ) \u2207xtJ\n6: \u2207xt\u22121J = W>\u2207x\u0303tJ Output: The gradient vectors \u2207ctJ , for t = 1 : L\nAlgorithm 2 HES VEC: An algorithm for computing the Hessian-vector product Hu, for any vector u, where H is the Hessian matrix of J evaluated at c. Input: The vector u and c, such that H = \u22072J(c)\n1: Run the BACK PROPAGATION algorithm and obtain xt, x\u0303t, and \u2207xtJ , for t = 1 : L.\n2: Forward-pass: Beginning with t = 1 and Ru ( x0 )\n= 0, where Ru is as defined in (27), iteratively run through the following three steps until t \u2264 L:\n1) Ru (x\u0303t) = WRu ( xt\u22121 ) ,\n2) Ru(\u03c6k(x\u0303t)) = diag ( \u03c6 \u2032 k (x\u0303 t) ) Ru (x\u0303t), for k =\n1 : K , and 3) Ru (xt) = \u2211K k=1 [u t k\u03c6k(x\u0303 t) + ctkMkRu (x\u0303t)],\nwhere Mk = diag ( \u03c6 \u2032 k (x\u0303 t) )\n. 3: Initialize the backward-pass: Set t\u2190 L, Ru (\u2207xtJ)\u2190 Ru ( xL ) , which is computed in the forward-pass. 4: for t = L down to 1 do 5: Ru(\u03c6 \u2032 k(x\u0303 t j)) = \u03c6 \u2032\u2032 k ( x\u0303tj ) Ru ( x\u0303tj\n) 6: Ru ( \u03c8 \u2032(t) ( x\u0303tj )) = \u2211K k=1 [ utk\u03c6 \u2032 k(x\u0303 t j) + c t kRu(\u03c6 \u2032 k(x\u0303 t j)) ] 7: Evaluate the following using \u03a6ti,k = \u03c6k (x\u0303 t i) and the\nvalues of Ru ( \u03a6t )\ncalculated in Step 2 of the forwardpass:\nRu (\u2207ctJ) = ( \u03a6t )>Ru (\u2207xtJ) + (Ru (\u03a6t))>\u2207xtJ.\n8: Compute Ru (\u2207x\u0303tJ) using (41). 9: Evaluate Ru (\u2207xt\u22121J) = W>Ru (\u2207x\u0303tJ).\nOutput: The Hessian-vector product Hu, which is the concatenation of Ru (\u2207ctJ), for t = 1 : L.\n4.1 Training LETnetVar : Learning Optimal Shrinkage\nFor a given A, the training datasetD consists ofN examples {(yq,xq)}Nq=1, where yq = Axq + \u03beq . The random noise vectors \u03beq are assumed to be independent and identically distributed. Let ct \u2208 RK , t = 1 : L, be the coefficients of the LET activation at layer t. For the qth example in the dataset, the prediction xLq of the L\nth layer is a function of the corresponding measurement vector yq and the set of LET coefficient vectors c = {ct}Lt=1. For convenience, we assume that c is vector of size KL, with the cts stacked on top of each other. The optimal set of activation parameters\nvi\nx0 x1 \u03c81\u03bd(x\u0303 1)\nxt\n\u03c8t\u03bd(x\u0303 t)\nxL\n\u03c8L\u03bd (x\u0303 L)W\nb\nx\u03031\nW\nb\nx\u03032\nW\nb\nx\u0303t+1\nb\nx\u0303t\nb\nx\u0303L\nLAYER 1 LAYER t LAYER L\nFig. 2: (Color online) A schematic of the proposed LETnetVar with L layers. Nonlinear LET shrinkage operators \u03c8t\u03bd are applied pointwise on x\u0303t at layer-t. Typical examples of LET activation functions and the corresponding regularizers are shown in blue and red, respectively, for each layer.\nAlgorithm 3 Hessian-free optimization for minimizing J(c)\n1: for i = 1, 2, \u00b7 \u00b7 \u00b7 , until convergence, do 2: Find gi = \u2207J(c)|c=ci using BACK-PROPAGATION. 3: Find the optimal direction \u03b4\u2217c using the conjugate-\ngradient algorithm, that uses the HES VEC routine for computing Hessian-vector products. 4: Perform line-search to minimize J at ci in the optimal direction \u03b4\u2217c returned by the conjugate-gradient algorithm.\nc\u2217 is obtained by minimizing the squared estimation error\nJ(c) = 1\n2 N\u2211 q=1 \u2016xLq (yq, c)\u2212 xq\u201622, (15)\nover all training examples. The optimization requires knowledge of the gradient of J(c) with respect to c, for which we derive associated back-propagation algorithms (cf. Algorithm 1 and Section 5.1 for LETnetVar, and supplementary material for LETnetFixed). The optimization of J(c) using vanilla GD tends to diverge, unless a very small step size is chosen. The reason for divergence is the exploding gradient [46], primarily caused by the unboundedness of \u03c61 in the LET expansion. We overcome this problem by the Hessian-Free optimization (HFO) technique [38]. In the ith epoch of HFO [38] (summarized in Algorithm 3), the search direction \u03b4\u2217c is obtained by minimizing a locally quadratic Taylor-series approximation J (q)i (c) to the actual cost J(c) at the ith iterate ci:\nJ (q) i (ci + \u03b4c) = J (ci) + \u03b4 > c gi +\n1 2 \u03b4>c Hi\u03b4c, (16)\nwhere gi = \u2207J(c)|c=ci , Hi = \u22072J(c) \u2223\u2223 c=ci\n, and \u03b4c is the search direction to be chosen optimally at every iteration by minimizing a regularized quadratic approximation:\n\u03b4\u2217c = arg min \u03b4c J (q) i (ci + \u03b4c) + \u03b3\u2016\u03b4c\u201622. (17)\nThe above formulation is also known as the trust region method, which is well known in the optimization literature. The regularization term in (17) ensures that the overall Hessian matrix (Hi + \u03b3I) is positive-definite and wellconditioned. Solving (17) using Newton\u2019s method could be computationally demanding, as the matrix (Hi + \u03b3I), has to be inverted in each iteration. In LETnetFixed, this\nis not a problem as the Hessian is only of size K \u00d7 K . In LETnetVar, this could be a problem since the Hessian now has dimensions KL \u00d7 KL. The inversion could be avoided by substituting the Newton method with a few steps of the conjugate-gradient (CG) algorithm. In this case, explicit knowledge of Hi is not required, but it is required to compute the Hessian-vector product Hiu, for any vector u [38]. This can be done using an additional round of back-propagation (cf. Algorithm 2). Thus, the Hessianvector product computation is only twice as expensive as the gradient computation. For details of the derivation, please refer to Section 5.2.\nAn alternative is to approximately compute the Hessianvector product using finite-difference approximation:\nHiu = lim \u21920\n1 ( \u2207J (c)|c=ci+ u \u2212 \u2207J (c)|c=ci ) , (18)\nwhich may be attractive when the dimension of c is small. We employ the finite-difference approximation for LETnetFixed since the computational overhead is small.\nThe additional computation involved in HFO, compared with standard GD, is due to the CG step in Algorithm 3. However, in practice, only a small number of iterations of CG suffice to obtain an accurate estimate of the optimal search direction \u03b4\u2217c [38]. In the i\nth training epoch, the coefficients are updated as ci \u2190 ci + \u03b4\u2217c using the optimal direction \u03b4\u2217c . The trust-region parameter \u03b3 in (17) is chosen using back-tracking. Each internal iteration of CG computes a Hessian-vector product to perform exact line search. HFO is a second-order optimization technique, and offers superior convergence performance than GD, provided that the LET parameters are appropriately initialized. In our implementation, the coefficients c are initialized such that the activation function in each layer closely fits the optimal ST operator, which guarantees that, to start with, the recovery performance is on par with that of ISTA.\n5 TRAINING LETnetVar : DERIVATION OF BACKPROPAGATION AND HESSIAN-VECTOR PRODUCT\n5.1 Back-propagation for LETnetVar\nFor brevity of notation, we derive the back-propagation algorithm for computing the gradient of the cost\nJ(c1, c2, \u00b7 \u00b7 \u00b7 , cL) = 1 2 \u2225\u2225\u2225xL \u2212 x\u2225\u2225\u22252 2 , (19)\nvii\nfor one training pair (y,x). The overall gradient is simply an accumulation of the gradients over all training pairs. The partial derivative of the cost J with respect to ctk, the k th LET coefficient at layer t, is expressed as\n\u2202J \u2202ctk = n\u2211 i=1 \u2202J \u2202xti \u2202xti \u2202ctk , t = 1 : L, k = 1 : K, (20)\nwhere xti is the i th coordinate of xt. Expanding the partial derivative \u2202J \u2202xti gives\n\u2202J \u2202xti = n\u2211 j=1 \u2202J \u2202x\u0303t+1j \u2202x\u0303t+1j \u2202xti , (21)\nwhere x\u0303t+1 = Wxt + b, as defined in (14). Consequently, we have \u2202x\u0303t+1j \u2202xti\n= Wji, the (j, i)th entry of W. To evaluate \u2202J\n\u2202x\u0303t+1j , we write\n\u2202J\n\u2202x\u0303t+1j = n\u2211 `=1 \u2202J \u2202xt+1` \u2202xt+1` \u2202x\u0303t+1j . (22)\nSince the proximal operator is applied coordinate-wise on x\u0303t+1 to compute xt+1, \u2202x t+1 `\n\u2202x\u0303t+1j vanishes for ` 6= j, leading to\n\u2202J\n\u2202x\u0303t+1j =\n\u2202J\n\u2202xt+1j\ndxt+1j dx\u0303t+1j . (23)\nUsing the notation \u03c8 \u2032(t+1) ( xt+1 ) to indicate the vector whose jth entry is d\u03c8t+1(x\u0303t+1j )\ndx\u0303t+1j = dxt+1j dx\u0303t+1j , one can write (23)\ncompactly as \u2207x\u0303t+1J = diag ( \u03c8 \u2032(t+1) ( xt+1 )) \u2207xt+1J, (24)\nwhere diag (u) denotes a diagonal matrix with the vector u on its diagonal, and \u2207uJ is the gradient of J with respect to u. Further, using\n\u2202x\u0303t+1j \u2202xti = Wji, we express (21) as\n\u2207xtJ = W>\u2207x\u0303t+1J. (25) Finally, by defining an n\u00d7K matrix \u03a6t whose (i, k)th entry is given by \u03a6ti,k = \u2202xti \u2202ctk = \u03c6k (x\u0303 t i), (20) is written as\n\u2207ctJ = ( \u03a6t )>\u2207xtJ. (26)\nTherefore, the gradient\u2207ctJ , of the cost J can be computed recursively as follows:\n1) \u2207x\u0303tJ = diag ( \u03c8 \u2032(t) (xt) ) \u2207xtJ , from (24), 2) \u2207xt\u22121J = W>\u2207x\u0303tJ from (25), and 3) \u2207ct\u22121J = ( \u03a6t\u22121 )> \u2207xt\u22121J from (26),\nfor t = L,L \u2212 1, \u00b7 \u00b7 \u00b7 , 2, starting with the initializations \u2207xLJ = xL \u2212 x and \u2207cLJ = ( \u03a6L )> \u2207xLJ . The recursive gradient computation using the back-propagation algorithm is performed after doing a forward computation through the network in order to evaluate xt and x\u0303t, for t = 1 : L."}, {"heading": "5.2 Exact Computation of the Hessian-vector Product", "text": "The key to exact computation of the Hessian-vector product lies in the definition of the directional derivative operator\n[50]. For a vector-valued function h(c) of a vector c, the derivative along u is defined as\nRu(h(c)) = lim \u03b1\u21920 h(c + \u03b1u)\u2212 h(c) \u03b1 = dh(c + \u03b1u) d\u03b1 \u2223\u2223\u2223\u2223 \u03b1=0 .\n(27)\nThe last equality effectively shows that Ru behaves like a standard derivative operator and is therefore linear. The dimensions of Ru(h(c)) are same as that of h. The properties of Ru have been established in [50]. For the sake of completeness, we recall the key properties below:\n1) Ru(\u03b2h(c)) = \u03b2Ru(h(c)), for any scalar \u03b2; 2) Ru(c) = u, for any u; 3) Ru(c0) = 0, where c0 is a constant vector; 4) Ru(h1(c) + h2(c)) = Ru(h1(c)) + Ru(h2(c)), for\ntwo vector-valued functions h1 and h2; 5) Ru applied on the inner product of h1 and h2 satisfies the product rule: Ru ( h1(c) >h2(c) ) =\nRu(h1(c))>h2(c)+ h1(c)>Ru(h2(c)); and 6) Ru(h1 (h2(c))) = \u2202h1\u2202h2Ru(h2(c)), where \u2202h1 \u2202h2\nis an m1\u00d7m2 Jacobian, m1 and m2 being the dimensions of h1 and h2, respectively. The (i, j)th entry of \u2202h1\u2202h2 is \u2202h1\u2202h2 \u2223\u2223\u2223 ij = \u2202h1i\u2202h2j , for i = 1 : m1 and j = 1 : m2.\nWe useRu to compute the Hessian-vector product in the ith training epoch. For a vector u having the same dimension as c, the Hessian-vector product is defined as\nHiu = lim \u03b1\u21920 \u2207cJ(c)|c=ci+\u03b1u \u2212 \u2207cJ(c)|c=ci \u03b1 . (28)\nEquivalently,\nHiu = Ru (\u2207cJ(c))|c=ci , (29) where\u2207cJ(c) is a vector of dimension KL\u00d7 1, constructed by stacking the vectors \u2207ctJ , for t = 1 : L. Hereafter, we drop the training epoch index i for notational brevity. To compute the Hessian-vector product, the Ru operator has to be applied on the vectors \u2207ctJ , as suggested by (29). In the following, we derive a back-propagation algorithm to compute Ru (\u2207ctJ). Analogous to the backpropagation algorithm derived in Section 5.1, we require Ru(xt) andRu(x\u0303t) for computations in the backward pass. These required quantities are computed in the forward pass as explained in the following."}, {"heading": "5.2.1 Forward Pass", "text": "Observe that Ru(xt) = Ru (\nK\u2211 k=1 ctk\u03c6k(x\u0303 t)\n) ,\n(a) = K\u2211 k=1 ( Ru(ctk)\u03c6k(x\u0303t) + ctkRu(\u03c6k(x\u0303t)) ) ,\n(b) = K\u2211 k=1 [ utk\u03c6k(x\u0303 t) + ctkRu(\u03c6k(x\u0303t)) ] , (30)\nwhere (a) is obtained by applying Properties 1, 4, and 5; and (b) by applying Property 2. Using Property 6, Ru(\u03c6k(x\u0303t))\nviii\nevaluates to\nRu(\u03c6k(x\u0303t)) = MkRu ( x\u0303t ) , (31)\nwhere the Jacobian Mk(i,j) = \u2202\u03c6k(x\u0303\nt i)\n\u2202x\u0303tj , for 1 \u2264 i, j \u2264 n.\nClearly, Mk(ij) vanishes for i 6= j, thereby leading to Ru(\u03c6k(x\u0303t)) = diag ( \u03c6 \u2032\nk\n( x\u0303t )) Ru ( x\u0303t ) , (32)\nwhere \u03c6 \u2032\nk (x\u0303 t) is an n-dimensional vector whose ith entry is\ngiven by \u2202\u03c6k(x\u0303 t i)\n\u2202x\u0303ti . Finally, Ru (x\u0303t) can be written as\nRu ( x\u0303t ) = Ru ( Wxt\u22121 + b ) = WRu ( xt\u22121 ) , (33)\nusing Properties 1, 3, and 4. The quantities Ru (x\u0303t) and Ru (xt) are computed in the forward pass as follows, starting with t = 1 until t = L:\n1) Ru (x\u0303t) = WRu ( xt\u22121 ) ;\n2) Ru(\u03c6k(x\u0303t)) = diag ( \u03c6 \u2032 k (x\u0303 t) ) Ru (x\u0303t), for k = 1 :\nK ; and 3) Ru (xt) = \u2211K k=1 [u t k\u03c6k(x\u0303 t) + ctkRu(\u03c6k(x\u0303t))];\nwhere Ru ( x0 )\n= 0, as the initialization x0 is independent of c."}, {"heading": "5.2.2 Backward Pass", "text": "Applying Ru on both sides of (20) and using Property 5 of the operator Ru, we get\nRu ( \u2202J\n\u2202ctk\n) = n\u2211 i=1 Ru ( \u2202J \u2202xti ) \u2202xti \u2202ctk + \u2202J \u2202xti Ru ( \u2202xti \u2202ctk ) ,\n= n\u2211 i=1 Ru ( \u2202J \u2202xti ) \u03c6k ( x\u0303ti ) + \u2202J \u2202xti Ru ( \u03c6k ( x\u0303ti )) ,\n(34)\nusing the fact that \u2202x t i\n\u2202ctk = \u03c6k (x\u0303\nt i). Consolidating over\nk = 1 : K , and using matrix-vector notation, (34) is written compactly as\nRu (\u2207ctJ) = ( \u03a6t )>Ru (\u2207xtJ) + (Ru (\u03a6t))>\u2207xtJ, (35)\nwhere \u03a6ti,k = \u03c6k (x\u0303 t i) and Ru\n( \u03a6t )\nis calculated using (32). Further, applying Ru on both sides of (21), we obtain\nRu ( \u2202J\n\u2202xti\n) = n\u2211 j=1 Ru ( \u2202J \u2202x\u0303t+1j ) \u2202x\u0303t+1j \u2202xti + \u2202J \u2202x\u0303t+1j Ru ( \u2202x\u0303t+1j \u2202xti ) ,\nwhich simplifies to Ru ( \u2202J\n\u2202xti\n) = n\u2211 j=1 Ru ( \u2202J \u2202x\u0303t+1j ) Wji, (36)\nsince \u2202x\u0303t+1j \u2202xti\n= Wji and Ru (Wji) = 0. The vectorized representation of (36) is given by\nRu (\u2207xtJ) = W>Ru (\u2207x\u0303t+1J) . (37) Finally, applying theRu operator on both sides of (23) gives\nRu ( \u2202J\n\u2202x\u0303t+1j\n) = Ru ( \u2202J\n\u2202xt+1j\n) dxt+1j\ndx\u0303t+1j +\n\u2202J\n\u2202xt+1j Ru\n( dxt+1j\ndx\u0303t+1j\n) .\nSubstituting dxt+1j dx\u0303t+1j = \u03c8 \u2032(t+1)\n( x\u0303t+1j ) leads to\nRu ( \u2202J\n\u2202x\u0303t+1j\n) = Ru ( \u2202J\n\u2202xt+1j\n) \u03c8 \u2032(t+1) ( x\u0303t+1j ) + \u2202J\n\u2202xt+1j Ru\n( \u03c8 \u2032(t+1) ( x\u0303t+1j )) . (38)\nExpanding Ru ( \u03c8 \u2032(t+1) ( x\u0303t+1j )) using Property 6, we obtain that\nRu ( \u03c8 \u2032(t+1) ( x\u0303t+1j )) = Ru ( K\u2211 k=1 ct+1k \u03c6 \u2032 k(x\u0303 t+1 j ) )\n= K\u2211 k=1 [ ut+1k \u03c6 \u2032 k(x\u0303 t+1 j ) + c t+1 k Ru(\u03c6 \u2032 k(x\u0303 t+1 j )) ] . (39)\nFollowing Property 6, we get\nRu(\u03c6 \u2032 k(x\u0303 t+1 j )) = \u03c6 \u2032\u2032 k ( x\u0303t+1j ) Ru ( x\u0303t+1j ) , (40)\nusing similar arguments that we used to obtain (32). Thus, it is possible to compute the vector Ru ( \u03c8 \u2032(t+1) ( x\u0303t+1 )) of\ndimension n \u00d7 1 following (39), (40), and using Ru ( x\u0303t+1 ) evaluated in the forward pass, enabling us to compute the vectorized representation of (38) as\nRu (\u2207x\u0303t+1J) = diag ( \u03c8 \u2032(t+1) (x\u0303t+1))Ru (\u2207xt+1J)\n+ diag ( Ru ( \u03c8 \u2032(t+1) (x\u0303t+1)))\u2207xt+1J.\n(41)\nFor initialization of the backward pass, we observe that Ru (\u2207xLJ) = Ru ( xL \u2212 x ) = Ru ( xL ) , which is saved during the forward computation. The back-propagation algorithm for computing the Hessian-vector product is summarized in Algorithm 2, where we use Ru (xt) and Ru (x\u0303t) stored in the forward pass and \u2207xtJ computed in Step 5 of Algorithm 1, for t = 1 : L.\n6 NUMERICAL VALIDATION OF LETnetVar We next validate the recovery performance of LETnetVar on synthesized signals. The comparison of LETnetFixed with LETnetVar will be reported in Section 7.2."}, {"heading": "6.1 Experimental Setting", "text": "We consider the reconstruction of a sparse signal x of dimension n = 256 from m = d0.7ne-dimensional noisy compressive measurement y = Ax+\u03be. The entries of A are independent and follow N (0, 1/m) (Gaussian distribution). The sparse signal x is generated as x = xsupp xmag, where the operator denotes the element-wise product. The entries of xsupp are either 0 or 1, drawn independently from a Bernoulli distribution with parameter \u03c1 \u2208 [0, 1]. Increasing the parameter \u03c1 results in a denser signal x. The elements of xmag follow N (0, 1). The training dataset used for learning the nonlinearities of LETnetVar and MMSE-ISTA consists of N = 100 examples. We conduct experiments for different values of the sparsity-controlling parameter \u03c1 and the input SNR: SNRinput =\n\u2016Ax\u20162 \u2016\u03be\u20162\n. The reconstruction SNR during testing, obtained using different algorithms, is\nix\ncalculated by averaging over 10 independent trials, where, in each trial, Ntest = 100 test signals are considered.\nFor performance comparison, we consider five state-ofthe-art sparse recovery algorithms: (i) ISTA, which employs the ST proximal operator; (ii) FISTA, which is an accelerated version of ISTA; (ii) CoSaMP, which is a greedy algorithm; (iv) the iteratively reweighted least-squares (IRLS) algorithm [47], which is based on majorization-minimization; and (v) MMSE-ISTA, which is a training-based algorithm developed by Kamilov and Mansour [28]. The optimum regularizer \u03bbopt for ISTA, FISTA, and IRLS is chosen from 10 logarithmically spaced points in the interval [10\u22125, 0.1] such that the reconstruction SNR during training is maximized. An all-zero vector is used as the initialization for all algorithms to ensure fairness in comparison. The CoSaMP algorithm is supplied with the exact knowledge of the number of nonzero entries in x.\n6.2 Training Details of LETnetVar and MMSE-ISTA For a fixed sensing matrix A, a training dataset consisting of N = 100 examples is generated for learning the optimal nonlinearities of LETnetVar and MMSE-ISTA. A validation set containing 20 examples is used during training in order to avoid over-fitting. The trained network is tested on a dataset containing Ntest = 100 examples drawn according to the same statistical model that was used to generate the training dataset (cf. Section 6.1). The procedure is repeated T = 10 times with different datasets, corresponding to 10 different sensing matrices, to measure the ensembleaveraged performance of LETnetVar and MMSE-ISTA. The optimal regularization parameter \u03bb for ISTA is chosen as explained in Section 6.1 and the same value is used in\nMMSE-ISTA, as suggested in [28]. The parameter \u03bbopt used in training LETnetVar is chosen by cross-validation over five values of \u03bb placed logarithmically in the interval [0.05, 0.5]. Both networks contain L = 100 layers.\nFor the MMSE-ISTA architecture, K = 501 coefficients are used to parametrize the activation function using cubic B-splines, and the same set of coefficients are shared across all the layers. The MMSE-ISTA network is trained using GD with a step-size of \u03b7 = 10\u22124, as suggested in [28], and the algorithm is terminated when the number of training epochs exceeds 1000.\nThe LET-based activation function in LETnetVar is parametrized with K = 5 coefficients in every layer, leading to a total of 500 parameters, which is comparable to the number of free parameters in MMSE-ISTA. The HFO algorithm for training LETnetVar is terminated after 60 epochs."}, {"heading": "6.2.1 Initialization and termination criteria for CG", "text": "The CG algorithm required to compute the optimal direction \u03b4\u2217c in every epoch of HFO is initialized with that obtained in the previous epoch of HFO. Theoretically, it takes KL iterations for the CG algorithm to compute an exact solution to (17), since the variable c is of dimension KL. Martens et al. [38] reported that it suffices to run the CG algorithm in each HFO iteration only a few times according to a preset criterion, for example, a criterion based on the relative improvement defined as\n\u03b8 \u2206 = Q ( \u03b4(i2)c ) \u2212Q ( \u03b4(i2\u2212i1)c ) Q ( \u03b4(i2)c ) ,\nx W \u03c81\u03bd ( x\u03031 ) \u03c82\u03bd ( x\u03032 ) \u03c8L\u03bd ( x\u0303L ) \u03c8L\u22121\u03bd (\u0303 xL\u22121\n)1+\u03b21 \u2212\u03b22 1+\u03b22 1+\u03b23 \u2212\u03b23 \u2212\u03b2L 1+\u03b2L z1 z2 z3 zLx1 x2 xL x0\nb\nx\u03031\nW x\u03032\nW x\u0303L\nb b\nLAYER 1 LAYER 2 LAYER L\u2212 1 LAYER L\nFig. 4: Architecture of fLETnetVar. Every layer (except the first layer) is connected to two of its previous layers. The activation functions are untied across layers.\nwhere Q (\u03b4c) = J (q) i (ci + \u03b4c) + \u03b3\u2016\u03b4c\u201622, for 0 < i1 < i2. The CG iterations are terminated when |\u03b8| < i1 , for > 0 is a user-specified parameter. We employ the same stopping criterion and set i1 = max(10, 0.1i2) following [38].\nAs the training objective J (c) is non-convex and is optimized using CG with a regularized quadratic approximation, a backtracking step is introduced after the end of the CG loop to determine the optimal direction \u03b4\u2217c . To determine the trust-region, the regularization parameter \u03b3 in (17) is varied using the Levenberg-Marquardt approach, wherein one computes the reduction ratio r , defined as\nr \u2206 =\nJ(ci + \u03b4 \u2217 c)\u2212 J(ci)\nJ (q) i (ci + \u03b4 \u2217 c)\u2212 J (q)i (ci)\n,\nin each epoch of the HFO and updates \u03b3 as \u03b3 \u2190 { 2 3\u03b3 if r < 1 4 , and\n3 2\u03b3 if r > 3 4 .\nAs a result, the trust-region of the quadratic approximation used in CG is contracted (enlarged) if the reduction ratio r is small (large)."}, {"heading": "6.3 Explanation and Interpretation of the Result", "text": "The average and standard deviation of the reconstruction SNR, computed over 10 independent trials, as a function of \u03c1 and input SNR, for different methods is shown in Figure 3. We observe that LETnetVar outperforms ISTA and FISTA by a margin of about 4 dB, while exhibiting relatively small deviation, and by a margin of 3 dB over IRLS. The CoSaMP algorithm, which is by far the best greedy algorithm for sparse recovery, results in 1 to 2 dB higher reconstruction SNR than LETnetVar, particularly for smaller values of \u03c1 and higher input SNR. However, unlike ISTA, FISTA, LETnet, and IRLS, one needs to know the sparsity level of the ground-truth to execute CoSaMP successfully. The improvement obtained using LETnetVar over MMSE-ISTA algorithm is largely due to accurate and parsimonious modeling of the activation function using LET. An illustration of how the error of LETnetVar varies as a function of training epochs, over the training and validation set, is shown in Figure 3f. The training error profile shows that LETnetVar does not underfit the data, particularly because the vanishing gradient problem has been avoided by the use of HFO. The validation error profile does not show any signs of overfitting for the number of epochs considered. If the number of epochs is increased further, the validation error might saturate.\nLETnetVar took about 60 training epochs for learning the parameters, which is significantly smaller than the number required to learn a typical DNN. The reduction in the number of training epochs and faster convergence are due to two factors: (i) the specific initialization of c, which ensures that the network outputs an estimate that is at least as good as that of ISTA; and (ii) HFO, which is an efficient way of performing second-order optimization. In contrast, MMSEISTA requires significantly more (approximately 1000) training epochs, as it uses GD for training.\n7 fLETnet : AN IMPROVED ARCHITECTURE MOTIVATED BY ACCELERATED GRADIENT-DESCENT The ISTA has a convergence rate of O ( 1 t ) , where t denotes the iteration index. However, Beck and Teboulle [5] have devised a strategy to improve the convergence rate from linear to quadratic, without significantly increasing the computations in each iteration. Their algorithm, referred to as fast ISTA or FISTA, relies on an original idea by Nesterov [48], who showed how to accelerate the projected gradientdescent by incorporating a momentum factor. The estimate xt+1 in the (t+ 1)st iteration of FISTA is computed as\nxt+1 = T\u03bd ( zt+1 \u2212 \u03b7\u2207f ( zt+1 )) , (42)\nwhere T\u03bd is the ST operator and\nzt+1 = xt + \u03b1t \u2212 1 \u03b1t+1\n( xt \u2212 xt\u22121 ) , (43)\nwith \u03b1t being the acceleration factor. Choosing \u03b1t+1 = 1+ \u221a\n1+4\u03b12t 2 , with the initialization \u03b11 = 1 leads to O ( 1 t2 ) convergence. As a consequence, FISTA yields an accurate estimate of the ground-truth in remarkably less number of iterations than ISTA.\nIt is natural to expect that a network model inspired by FISTA would also produce accurate estimates with considerably less number of layers than LETnetVar or LETnetFixed. We leverage the concept of Nesterov\u2019s accelerated gradient to construct a new network architecture, which we refer to as the fLETnet, to achieve a similar estimation performance as LETnetVar with significantly less number of layers.\n7.1 Architecture of the fLETnet\nLet \u03b2t+1 = \u03b1t\u22121\u03b1t+1 ; upon rearranging (43), we get\nzt+1 = (1 + \u03b2t+1) x t \u2212 \u03b2t+1xt\u22121.\nxi\nFurthermore, (42) is rewritten as xt+1 = \u03c8(t+1) ( Wzt+1 + b ) , (44)\nwhere W and b are as defined in Section 2.1. Similar to LETnetVar, we replace the ST-based activation function in (44) with a parametrized linear combination of elementary thresholding functions, namely the LETs, to construct fLETnet. The LET coefficients are allowed to vary across the layers to further increase the flexibility and expressive power of the network. The forward computation through the fLETnet is summarized in the following steps:\n1) zt = (1 + \u03b2t)xt\u22121 \u2212 \u03b2txt\u22122, 2) x\u0303t = Wzt + b, and 3) xt = \u03c8(t) (x\u0303t), for t = 1, 2, \u00b7 \u00b7 \u00b7 , L,\nstarting with the initialization x0 = x\u22121 = 0. A schematic of the fLETnet architecture is shown in Figure 4. Every layer in fLETnet is connected to two previous layers unlike a standard feed-forward NN. The architecture also bears a striking resemblance to deep residual networks (DRNs) proposed by He et al. [15], who demonstrated significant performance improvements in various computer vision tasks using DRNs. He et al. also showed that the issue of vanishing/exploding gradients for training parameters closer to the first layer can be prevented by establishing connections to the previous layers, so that the network can be trained using GD, even without an accurate initialization. Experimental results in Section 7.2 suggest that the fLETnet architecture produces reasonably accurate sparse estimates while containing significantly smaller number of layers than LETnetVar, thereby reducing the number of parameters to learn.\nThe cost function for training fLETnet is the same as that of LETnetVar (cf. Section 4.1), namely the reconstruction error over the training set. Again, the LET coefficients are initialized such that the activation function approximates the ST operator, so that the initial reconstruction is comparable to that of FISTA. We train the network using HFO to achieve faster convergence. A derivation of the backpropagation algorithm for computing the gradient of the training cost and the Hessian-vector product for the fLETnet architecture are given in the supplementary material.\n7.2 Experimental Validation of fLETnet The data generation procedure is the same as that described in Section 6.1. The number of layers in fLETnet is taken as L = 50. A comparison of the reconstruction SNR of fLETnet with LETnetVar and LETnetFixed, for different \u03c1 and input SNR, is shown in Figure 5. We observe that, despite having\nxii\nhalf the number of layers, fLETnet achieves comparable or better performance than the competing architectures, especially when the measurement SNR is low and \u03c1 is high (denser signal). We observe from Figure 5f that the training and validation errors for fLETnet reduce at a faster rate than LETnetVar. This behavior is attributed to the DRN-type architecture of fLETnet, which avoids vanishing/exploding gradients. Furthermore, as expected, LETnetVar results in significantly improved performance over LETnetFixed (by as much as 5 dB in some cases and about 2 dB on the average).\nThe per iteration (per layer for LETnetVar, LETnetFixed, fLETnet, and MMSE-ISTA) run-times of various algorithms are reported in Table 1. The algorithms are implemented in Python, running on a computer having an intel core i7 4th generation processor and 8 GB of RAM. We observe that the run-times for fLETnet, LETnetVar, and LETnetFixed are on par with that of ISTA and FISTA. However, the proposed DNN architectures require far less number of layers than the number of iterations in ISTA and FISTA, thereby resulting in less computation time overall. The MMSE-ISTA algorithm requires relatively more time than the proposed architectures, mainly because it uses significantly more parameters to model the activation function. The run-times for IRLS and CoSaMP are considerably higher, as they require to compute matrix inversion and pseudo-inverse, respectively, within every iteration.\n7.3 What Regularizers Does the fLETnet Learn?\nTo gain insights into the functioning of fLETnet, it is instructive to study the activation functions and the corresponding regularizers in each layer at the end of training. In Figure 9, we show an illustration of the typical learnt activations and the regularizers over each layer from 8 to 14 of a trained fLETnet. A segment of the corresponding output signals are also shown. The activation functions in all 50 layers are provided in the supplementary document. Take for instance,\nlayer 8, where the regularizer is closer to the `1 penalty over the dynamic range of the input. On the other hand, the induced regularizers in layers 9, 13, and 14 are between the `1 and `2 penalties. Interestingly, the regularizers in layers 10, 11, and 12 decrease beyond a certain range of inputs and even go negative, thereby encouraging an already large input to increase further in magnitude. Such a behavior is in stark contrast with the familiar `0/`1-norm-based regularizers, which never amplify the input. This behavior of the network seems to have a high impact on support recovery. To understand further, let us examine the LET-based activation defined in (9), for |v| > v0, where v0 is a large constant (for example, v0 = 3\u03c4 ). As the exponential terms in the expression of \u03c8 decay as |v| increases, we can write \u03c8(v) \u2248 c1v for |v| > v0. As the coefficients are initialized to approximate the ST operator, we have c1 < 1 prior to training, because the ST output never exceeds its input in magnitude. However, in the process of training, c1 might get updated to a value larger than one, thereby resulting in |\u03c8(v)| \u2248 c1 |v| > |v|, for |v| > v0. This is reflected in the corresponding regularizer, as it slopes downwards and even becomes negative for inputs of large magnitudes. However, the induced regularizers across all layers offer a positive penalty for small magnitudes resulting in noise rejection. The network effects two simultaneous operations \u2013 reduction of small amplitudes (noise), and enhancement of large amplitudes, which are due to the signal. The signal component that gets eliminated in one layer in the process of canceling noise can again be recovered in a subsequent layer due to the negative penalty as highlighted in the third row of Figure 9. Therefore, the regularizers learnt by fLETnet cover a wide range between hard and soft thresholding, and even go beyond these, leading to a balance between noise rejection and signal preservation.\nxiii\n7.4 Support Recovery in fLETnet\nIn Figure 7, we plot the layer-wise reconstruction SNR, defined as SNRt = \u2016xt\u2212x\u20162 2\n\u2016x\u201622 on test signals, versus the\nlayer index t of a trained fLETnet. The input SNR and the sparsity parameter \u03c1 are taken as 20 dB and 0.2, respectively. The variation of reconstruction SNR with respect to the corresponding number of iterations of FISTA is also shown to facilitate comparison. We observe that the recovery SNR for FISTA increases initially and saturates as the number of iterations exceeds 40, resulting in no further improvement in estimation. However, the evolution of SNRt for a trained fLETnet exhibits an overall increasing trend, in spite of the local variations. The estimate produced by the final layer is significantly better than the FISTA output, by almost 5.5 dB.\nThe SNRt has a steep increase for the first few layers (up to 20), nearly the same behavior as that of FISTA. After that, the SNRt seems to oscillate before reaching a value about 5.5 dB higher than FISTA. We conjecture that the network actually stabilizes its support estimate in this regime. To justify further, we consider a support recovery metric (SRM) based on the evolution of the ratio of the signal energy over the support to that outside of it:\nSRMt = \u2016Ps(xt)\u201622\n\u2016xt\u201622 \u2212 \u2016Ps(xt)\u201622 , (45)\nwhere s is the sparsity level, which is assumed to be known solely for the sake of computing the metric; and Ps denotes the best s-sparse approximation operator, which retains only the top s significant entries of its argument.\nThe SRM values, averaged over 100 realizations, are also shown in Figure 7. Upon juxtaposing with the SNR plot, we observe that the transition region in SNRt corresponds to a transition to a high value in SRMt as well. The region over which support stabilization takes place might depend on the depth of the network. Once the support is estimated, the network is effectively performing a regression task for estimating the amplitudes."}, {"heading": "8 CONCLUSIONS AND OUTLOOK", "text": "We have addressed the problem of efficient estimation of sparse signals from incomplete noisy measurements, which is of importance in several contemporary signal processing and machine learning applications. We have developed a specialized DNN architecture that enables sparse estimation by unfolding the iterations of soft-thresholding algorithms such as ISTA and FISTA with precomputed weights and biases shared across the layers. In particular, the nonlinear activation is modeled using a combination of threshold functions with an approximation to the soft-threshold as the initialization. Training over the exemplars, equipped with a customized back-propagation algorithm, ensures that the performance is only better than that of ISTA and FISTA. This learning paradigm has the advantage that the DNN is endowed with the capability to discover an ensemble of regularizers, one per layer, that have an overall sparse recovery performance better than `1 regularization techniques. The optimization is carried out using an efficient second-order algorithm that does not require explicit computation of the Hessian, which also considerably reduces the\nnumber of training epochs. The proposed deep architecture is capable of producing sparser estimates, leading to an overall gain of 4 to 5 dB over the state-of-the-art sparse recovery algorithms. The improvement comes at a modest cost of learning only five parameters per layer. If we tie the parameters across layers, the drop in SNR is about 2 dB, but still above the competing techniques by 2 to 3 dB. The proposed LET model for the activation may not always lead to a convex regularizer and associated gains in convergence rate. However, it is known in the literature that convexity of the regularizer is not necessary [49] for promoting sparsity and may even be a restriction. Viewing the proposed methodology purely from a DNN perspective, we found that there are strong similarities with the recently developed DRNs, which have been shown to outperform conventional feed-forward networks. Another key aspect of the proposed architectures is that the number of parameters to learn is independent of the underlying signal dimension.\nWhile the proposed learning paradigm has certain advantages over the standard sparse recovery algorithms, it has certain caveats as well. For example, in a standard CS formulation, exact knowledge of the sensing matrix A is assumed during both sensing and sparse recovery. For an altogether different sensing matrix, the sparse recovery algorithm could simply use the new matrix for reconstruction. On the other hand, in a learning paradigm such as this one, merely switching the matrices will not work. The network has to be trained all over again with the new matrix in order to achieve optimal reconstruction performance.\nThe MMSE-ISTA formulation, which motivated this work, could also benefit in terms of speed-up during training by employing a second-order method, and the Hessianvector products could be computed efficiently as done in this paper. The DRN link with the fLETnet requires further investigation in terms of optimizing the architecture and parameters.\nxiv"}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Ulugbek Kamilov from Mitsubishi Electric Research Laboratory for clarifying several aspects regarding the implementation of the MMSEISTA algorithm, and Thierry Blu, Chinese University of Hong Kong, for his insights on the LET representation and feedback on the manuscript."}, {"heading": "11 HESSIAN-VECTOR PRODUCT COMPUTATION", "text": "FOR fLETnet Analogous to LETnetVar, the Hessian-vector product Hu, where H = \u22072J(c) and u is any vector having the same dimension as c, can be exactly computed as\nHu = Ru (\u2207cJ) , (58) where the operator Ru has been defined in (M-27). To evaluate Ru (\u2207cJ), one must run a forward pass and a backward pass through the fLETnet."}, {"heading": "11.1 Forward pass", "text": "Applying Ru on both sides of zt = (1 + \u03b2t)xt\u22121 \u2212 \u03b2txt\u22122 and x\u0303t = Wzt + b, we obtain\nRu(zt) = (1 + \u03b2t)Ru(xt\u22121)\u2212 \u03b2tRu(xt\u22122), and (59) Ru(x\u0303t) = WRu(zt) = (1 + \u03b2t)WRu ( xt\u22121 ) \u2212 \u03b2tWRu ( xt\u22122 ) ,\n(60)\nrespectively; using Properties 1, 3, and 4 of Ru listed in Appendix B. Evaluation of (60) requires computation of Ru (xt), for t = 1 : L. Similar to (M-30), we apply Ru on both sides of xt = \u03c8(t) (x\u0303t) = \u2211K k=1 c t k\u03c6k(x\u0303 t) to obtain\nRu(xt) = K\u2211 k=1 [ utk\u03c6k(x\u0303 t) + ctkRu(\u03c6k(x\u0303t)) ] , (61)\nwhere Ru(\u03c6k(x\u0303t)) = diag ( \u03c6 \u2032 k (x\u0303 t) ) Ru (x\u0303t), as given in (M-32). The quantities Ru (x\u0303t) and Ru (xt) are computed iteratively and stored in the forward pass, for t = 1 : L:\n1) Ru (x\u0303t) = (1 + \u03b2t)WRu ( xt\u22121 ) \u2212 \u03b2tWRu ( xt\u22122 ) ;\n2) Ru(\u03c6k(x\u0303t)) = diag ( \u03c6 \u2032 k (x\u0303 t) ) Ru (x\u0303t), for k = 1 :\nK ; and 3) Ru (xt) = \u2211K k=1 [u t k\u03c6k(x\u0303\nt) + ctkRu(\u03c6k(x\u0303t))]; where Ru ( x0 ) = Ru ( x\u22121 ) = 0, as the initializations x0 and x\u22121 are independent of c."}, {"heading": "11.2 Backward pass", "text": "Applying Ru on (53), we get\nRu(\u2207ctJ) = ( \u03a6t )>Ru (\u2207xtJ) + (Ru (\u03a6t))>\u2207xtJ, (62)\nemploying an argument similar to the one used to obtain (M-35). Further, applying Ru on (55) leads to Ru(\u2207xtJ) = (1 + \u03b2t+1)Ru(\u2207zt+1J)\u2212 \u03b2t+2Ru(\u2207zt+2J),\n(63)\nwhere t = L\u2212 2 down to 1. For layers t = L\u2212 1 and t = L, we have\nRu (\u2207xL\u22121J) = (1 + \u03b2L)Ru(\u2207zLJ), and (64) Ru(\u2207xLJ) = Ru(xL), (65)\nrespectively, whereRu(xL) is computed in the forward pass through fLETnet. To evaluate Ru(\u2207ztJ), we apply Ru on both sides of\n\u2207ztJ = W>diag ( \u03c8 \u2032(t) ( x\u0303t )) \u2207xtJ,\nas obtained from Step 1 of the forward pass, resulting in Ru(\u2207ztJ) = W> [ diag ( Ru ( \u03c8 \u2032(t) ( x\u0303t ))) \u2207xtJ\n+diag ( \u03c8 \u2032(t) ( x\u0303t )) Ru(\u2207xtJ) ] , (66)\nusing an argument similar to that used to arrive at (M-41). The quantityRu ( \u03c8 \u2032(t) (x\u0303t) ) can be computed using (M-39) and (M-40). The back-propagation algorithm to compute the Hessian-vector product is summarized below:\n1) Ru(\u2207ztJ) = W> [ diag ( Ru ( \u03c8 \u2032(t) (x\u0303t) )) \u2207xtJ +\ndiag (\u03c8\u2032(t) (x\u0303t))Ru(\u2207xtJ) ] , for t = L down to 1;\n2) Ru(\u2207xtJ) = (1 + \u03b2t+1)Ru(\u2207zt+1J) \u2212 \u03b2t+2Ru(\u2207zt+2J), for t = L \u2212 2 down to 1; Ru (\u2207xL\u22121J) = (1 + \u03b2L)Ru(\u2207zLJ); and Ru(\u2207xLJ) = Ru(xL);\n3) Ru(\u2207ctJ) = ( \u03a6t )>Ru (\u2207xtJ) +(\nRu ( \u03a6t ))>\u2207xtJ , for t = L down to 1.\nxvii\n12 REGULARIZERS LEARNT BY A fLETnet In Section 7.3 and Figure 6, we showed the activations learnt, the corresponding regularizers, and a portion of the estimated signal, only for layers 8 \u2013 14 for want of space. The evolution of the activation functions, corresponding regularizers, and the estimated signals across L = 50 layers of a trained fLETnet are shown in the following figure. The layer indices are indicated at the top of every block. We observe that the learnt activations and the corresponding regularizers evolve in such a way that a balance is maintained between noise cancellation and signal preservation. The estimated signal entry in one layer might increase in the subsequent one, as elaborated in Section 7.3 of the manuscript, due to flexible parametric design of the regularizers using the LET.\nxviii\nxix"}], "references": [{"title": "An introduction to compressive sampling", "author": ["E.J. Cand\u00e9s", "M. Wakin"], "venue": "IEEE Signal Process. Mag., vol. 25, pp. 21 \u201330, Mar. 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressive sensing", "author": ["R. Baraniuk"], "venue": "IEEE Signal Process. Mag., vol. 24, no. 4, pp. 118\u2013121, Jul. 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e9s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 2, pp. 489\u2013509, Feb. 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process., vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. Image Process., vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D. Donoho", "J.M. Pauly"], "venue": "Magnetic Resonance in Medicine, vol. 58, no. 6, pp. 1182\u20131195, Dec. 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "DNA array decoding from nonlinear measurements by belief propagation", "author": ["M. Sheikh", "S. Sarvotham", "O. Milenkovic", "R. Baraniuk"], "venue": "Proc. IEEE Workshop on Statistical Signal Process. (SSP), Aug. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressive radar imaging", "author": ["R. Baraniuk", "P. Steeghs"], "venue": "Proc. IEEE Radar Conf., Apr. 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Image feature extraction by sparse coding and independent component analysis", "author": ["A. Hyvarinen", "E. Oja", "P. Hoyer", "J. Hurri"], "venue": "Proc. 14th IEEE Intl. Conf. on Patt. Recog., vol. 2, pp. 1268\u20131273, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Cand\u00e9s", "J. Romberg", "T. Tao"], "venue": "Comm. on Pure and Appl. Math. vol. 59 no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. Advances in Neural Info. Process. Systems, pp. 1097\u20131105, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1512.03385v1, Dec. 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks.", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "in Proc. Intl. Conf. on Learning Representations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv:1412.7062, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural Networks: Tricks of the Trade, Second Edition, Springer, Berlin", "author": ["G. Hinton"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["S. Rifai", "Y. Bengio", "A. Courville", "P. Vincent", "M. Mirza"], "venue": "Proc. European Conference on Computer Vision, pp. 808\u2013822, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. dissertation, University of Toronto, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends", "author": ["Z.H. Ling", "S.Y. Kang", "H. Zen", "A. Senior", "M. Schuster", "X.J. Qian", "H.M. Meng", "L. Deng"], "venue": "IEEE Signal Process. Mag., vol. 32, no. 3, pp. 35\u201352, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Process., pp. 6645\u20136649, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "arXiv:1206.6434, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, Dec. 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Large-scale feature learning with spike-and-slab sparse coding", "author": ["I. Goodfellow", "A. Courville", "Y. Bengio"], "venue": "Proc. Intl. Conf. on Machine Learning, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proc. Intl. Conf. on Machine Learning, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J.R. Hershey", "J.L. Roux", "F. Weninger"], "venue": "arXiv:1409.2575v4, Nov. 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning optimal nonlinearities for iterative thresholding algorithms", "author": ["U.S. Kamilov", "H. Mansour"], "venue": "IEEE Signal Process. Letters, vol. 23, no. 5, pp. 747\u2013751, May 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C.D. Mol"], "venue": "Comm. Pure Appl. Math., vol. 57, pp. 1413\u20131457, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Parameter learning with truncated message-passing", "author": ["J. Domke"], "venue": "Proc. Computer Vision and Patt. Recog., pp. 2937\u20132943, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["J. Domke"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 35, no. 10, pp. 24\u201354, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "IEEE Trans. Patt. Anal. and Mach. Intell., vol. 37, no. 5, pp. 1821\u20131833, Sep. 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1821}, {"title": "Learning deep `0 encoders", "author": ["Z. Wang", "Q. Ling", "T.S. Huang"], "venue": "arXiv:1509.00153v2, Nov. 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximal sparsity with deep networks", "author": ["B. Xin", "Y. Wang", "W. Gao", "D. Wipf"], "venue": "arXiv:1605.01636v2, May 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The SURE-LET approach to image denoising", "author": ["T. Blu", "F. Luisier"], "venue": "IEEE Trans. Image Process., vol. 16, no. 11, pp. 2778\u20132786, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "A new SURE approach to image denoising: Interscale orthonormal wavelet thresholding", "author": ["F. Luisier", "T. Blu", "M. Unser"], "venue": "IEEE Trans. Image Process. vol. 16, no. 3, pp. 593\u2013606, Mar. 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "An iterative linear expansion of thresholds for `1-based image restoration", "author": ["H. Pan", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 22, no. 9, pp. 3715\u20133728, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proc. 27th Intl. Conf. Machine Learning, 2010. (ICML), pp. 735\u2013742, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Trans. Info. Theory, vol. 53, no. 12, pp. 4655\u20134666, Dec. 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "CoSamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Appl. and Computational Harmonic Anal., vol. 26, issue 3, pp. 301\u2013321, May 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Info. Theory, vol. 55, no. 5, pp. 2230\u20132249, May 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Stat. Society, Series B, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Statist., vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-Wiener SURE-LET deconvolution", "author": ["F. Xue", "F. Luisier", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 22, no. 5, pp. 1954-1968, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1954}, {"title": "SURE-LET multichannel image denoising: interscale orthonormal wavelet thresholding", "author": ["F. Luisier", "T. Blu"], "venue": "IEEE Trans. Image Process., vol. 17, no. 4, pp. 482\u2013492, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proc. Intl. Conf. on Machine Learning, 2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. DeVore", "M. Fornasier", "C.S. G\u00fcnt\u00fcrk"], "venue": "Communications on Pure and Applied Math., vol. LXIII, pp. 1\u2013 38, 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "A method for solving the convex programming problem with convergence rate O ( 1 k2  )", "author": ["Y.E. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR, vol. 269, pp. 543\u2013547, 1983.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1983}, {"title": "Sparse signal estimation by maximally sparse convex optimization", "author": ["I.W. Selesnick", "I. Bayram"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 5, pp. 1078\u20131092, Mar. 2014.  xv", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "E STIMATION of sparse signals from inaccurate linear measurements, formally known as compressive sensing (CS) [1], [2], [3], [4], has gained enormous importance in signal processing over the past decade.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Compressive sensing techniques have been successfully employed in a variety of signal processing applications such as deconvolution [5], denoising [6], superresolution [7], inpainting, etc.", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "For example, applications such as medical imaging [8], computational biology [9], radar imaging [10], pattern classification [11], feature extraction [12], etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "In a seminal work [13], Cand\u00e8s et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "The convex programming formulation in [13] is also known as basis pursuit denoising.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "For example, convolutional neural network-based models [14] have resulted in significant improvement in accuracy for the tasks of object recognition [15], detection [16], and image segmentation [17].", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "Deep learning architectures such as the restricted Boltzmann machine [18], auto-encoders [19], and recurrent neural networks [20] are used extensively in automatic speech generation [21] and recognition [22] systems.", "startOffset": 203, "endOffset": 207}, {"referenceID": 22, "context": "Several DNNbased methods have also been proposed for unsupervised feature extraction [23], [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "Several DNNbased methods have also been proposed for unsupervised feature extraction [23], [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "Specifically, the spike-and-slab model [25] has been shown to work efficiently for sparse feature extraction from large-scale data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "Gregor and Le Cun showed, for the first time, that a custom-designed discriminatively trained DNN architecture can solve the sparse coding problem efficiently [26].", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "Further efforts have been made to make a one-to-one correspondence between iterative inferencing algorithms and DNNs [27], and to efficiently represent the nonlinearity that is at the heart of sparse coding [28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "Further efforts have been made to make a one-to-one correspondence between iterative inferencing algorithms and DNNs [27], and to efficiently represent the nonlinearity that is at the heart of sparse coding [28].", "startOffset": 207, "endOffset": 211}, {"referenceID": 28, "context": "This unfolding process has been applied in the context of ISTA [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "The possibility of using a trained feed-forward neural network for efficient sparse coding was first demonstrated by Gregor and LeCun [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "[27] proposed the idea of deep unfolding, wherein an iterative inference strategy, such as ISTA, inspires the architecture of a DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The idea of unfolding an iterative inference algorithm with shared parameters over the layers was developed by Domke [30], [31], in the context of tree-reweighted belief propagation and mean-field inference.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "The idea of unfolding an iterative inference algorithm with shared parameters over the layers was developed by Domke [30], [31], in the context of tree-reweighted belief propagation and mean-field inference.", "startOffset": 123, "endOffset": 127}, {"referenceID": 31, "context": "[32] proposed a learnable architecture of fixed complexity, by unfolding the iterative proximal descent algorithms for structured sparse and low-rank models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] to design a deep `0 encoder, with applications in image classification and clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] critically analyzed the merits of designing trainable models over conventional optimizationbased sparse coding algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Kamilov and Mansour [28] proposed a deep architecture for sparse coding motivated by ISTA, wherein the nonlinear activation function is modeled using cubic B-splines.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "However, we parameterize the nonlinear activation function using a linear expansion of thresholds (LET) [37] instead of cubic B-splines (Section 3).", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Unlike [28], the coefficients of the LET-based activations are untied across the layers of the network to enhance the expressive power of the model, without significantly affecting the training overhead.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "Also, a small number of coefficients, typically five per layer, suffice, which results in considerably less number of parameters to learn overall in comparison with [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "We also derive an efficient Hessian-free optimization (HFO) technique [38] to train the network (Section 5).", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "The motivation is derived from the fast iterative shrinkage threshold algorithm (FISTA) [5], which has a faster convergence rate than ISTA without increasing the computational load per iteration, thereby requiring considerably less number of iterations overall.", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "We show that the resulting fast LETnet architecture, dubbed as fLETnet (Section 7), bears close resemblance to the recently proposed deep residual learning architecture [15], wherein each layer draws direct connections from two preceding layers instead of one, so as to improve the convergence performance.", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "It has been shown in [15] that the scheme enjoys the merits of a deep architecture, while successfully alleviating the problems of vanishing/exploding gradients.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "We carry out experimental validation of LETnet and fLETnet for sparse signal recovery and demonstrate their superiority over the the learning-based approach proposed in [28] as well as the conventional sparse coding algorithms that are not set up within a learning paradigm.", "startOffset": 169, "endOffset": 173}, {"referenceID": 38, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "Algorithms such as orthogonal matching pursuit (OMP) [39], compressive sampling matching pursuit (CoSaMP) [40], subspace pursuit [41], etc.", "startOffset": 129, "endOffset": 133}, {"referenceID": 41, "context": "In the special case where G (x) = \u2016x\u20161, the resulting optimization is referred to as LASSO regression [42], [43].", "startOffset": 102, "endOffset": 106}, {"referenceID": 42, "context": "In the special case where G (x) = \u2016x\u20161, the resulting optimization is referred to as LASSO regression [42], [43].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "To make the exposition self-contained, we briefly recall the connection between the proximal gradient methods and the feed-forward neural networks (NN), originally established by Gregor and LeCun [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 25, "context": "Gregor and LeCun [26] interpreted (8) as the feed-forward computation through a NN with weight matrix W and bias b shared across every layer.", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "We advocate the use of elementary functions based on the derivatives of a Gaussian (DoG) [35], [37], specified as", "startOffset": 89, "endOffset": 93}, {"referenceID": 36, "context": "We advocate the use of elementary functions based on the derivatives of a Gaussian (DoG) [35], [37], specified as", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 109, "endOffset": 113}, {"referenceID": 43, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 143, "endOffset": 147}, {"referenceID": 35, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 149, "endOffset": 153}, {"referenceID": 44, "context": "The primary motivation to use LET-based activation is its success over the ST in several image deconvolution [37], [44] and denoising problems [35], [36], [45].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "ISTA for the sparse estimation problem has a linear rate of convergence [5] and performs reasonably well for sufficiently large number of iterations.", "startOffset": 72, "endOffset": 75}, {"referenceID": 27, "context": "[28] proposed to employ a parametric linear combination of cubic B-splines instead of the soft-threshold, to parametrize the activation function, which is optimized during training and kept fixed across all layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The reason for divergence is the exploding gradient [46], primarily caused by the unboundedness of \u03c61 in the LET expansion.", "startOffset": 52, "endOffset": 56}, {"referenceID": 37, "context": "We overcome this problem by the Hessian-Free optimization (HFO) technique [38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "In the ith epoch of HFO [38] (summarized in Algorithm 3), the search direction \u03b4c is obtained by minimizing a locally quadratic Taylor-series approximation J (q) i (c) to the actual cost J(c) at the ith iterate ci:", "startOffset": 24, "endOffset": 28}, {"referenceID": 37, "context": "In this case, explicit knowledge of Hi is not required, but it is required to compute the Hessian-vector product Hiu, for any vector u [38].", "startOffset": 135, "endOffset": 139}, {"referenceID": 37, "context": "However, in practice, only a small number of iterations of CG suffice to obtain an accurate estimate of the optimal search direction \u03b4c [38].", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "The entries of xsupp are either 0 or 1, drawn independently from a Bernoulli distribution with parameter \u03c1 \u2208 [0, 1].", "startOffset": 109, "endOffset": 115}, {"referenceID": 46, "context": "For performance comparison, we consider five state-ofthe-art sparse recovery algorithms: (i) ISTA, which employs the ST proximal operator; (ii) FISTA, which is an accelerated version of ISTA; (ii) CoSaMP, which is a greedy algorithm; (iv) the iteratively reweighted least-squares (IRLS) algorithm [47], which is based on majorization-minimization; and (v) MMSE-ISTA, which is a training-based algorithm developed by Kamilov and Mansour [28].", "startOffset": 297, "endOffset": 301}, {"referenceID": 27, "context": "For performance comparison, we consider five state-ofthe-art sparse recovery algorithms: (i) ISTA, which employs the ST proximal operator; (ii) FISTA, which is an accelerated version of ISTA; (ii) CoSaMP, which is a greedy algorithm; (iv) the iteratively reweighted least-squares (IRLS) algorithm [47], which is based on majorization-minimization; and (v) MMSE-ISTA, which is a training-based algorithm developed by Kamilov and Mansour [28].", "startOffset": 436, "endOffset": 440}, {"referenceID": 27, "context": "1 and the same value is used in MMSE-ISTA, as suggested in [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "The MMSE-ISTA network is trained using GD with a step-size of \u03b7 = 10\u22124, as suggested in [28], and the algorithm is terminated when the number of training epochs exceeds 1000.", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": "[38] reported that it suffices to run the CG algorithm in each HFO iteration only a few times according to a preset criterion, for example, a criterion based on the relative improvement defined as", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "1i2) following [38].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "However, Beck and Teboulle [5] have devised a strategy to improve the convergence rate from linear to quadratic, without significantly increasing the computations in each iteration.", "startOffset": 27, "endOffset": 30}, {"referenceID": 47, "context": "Their algorithm, referred to as fast ISTA or FISTA, relies on an original idea by Nesterov [48], who showed how to accelerate the projected gradientdescent by incorporating a momentum factor.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "[15], who demonstrated significant performance improvements in various computer vision tasks using DRNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "However, it is known in the literature that convexity of the regularizer is not necessary [49] for promoting sparsity and may even be a restriction.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "We address the problem of reconstructing sparse signals from noisy and compressive measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage-thresholding algorithm (ISTA). We maintain the weights and biases of the network links as prescribed by ISTA and model the nonlinear activation function using a linear expansion of thresholds (LET), which has been very successful in image denoising and deconvolution. The optimal set of coefficients of the parametrized activation is learnt over a training dataset containing measurement-sparse signal pairs, corresponding to a fixed sensing matrix. For training, we develop an efficient second-order algorithm, which requires only matrix-vector product computations in every training epoch (Hessian-free optimization) and offers superior convergence performance than gradient-descent optimization. Subsequently, we derive an improved network architecture inspired by FISTA, a faster version of ISTA, to achieve similar signal estimation performance with about 50% of the number of layers. The resulting architecture turns out to be a deep residual network, which has recently been shown to exhibit superior performance in several visual recognition tasks. Numerical experiments demonstrate that the proposed DNN architectures lead to 3-4 dB improvement in the reconstruction signal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding algorithms.", "creator": "LaTeX with hyperref package"}}}