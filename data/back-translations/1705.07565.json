{"id": "1705.07565", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "abstract": "Although previous work along this line of research has shown promising results, most existing methods are either unable to significantly compress a well-trained deep network or require a heavy retraining process for the truncated deep network to increase its predictive performance again. In this paper, we propose a new layer-by-layer prediction method for deep neural networks. In our proposed method, the parameters of each layer are truncated independently based on derivatives of a second-order layered error function with respect to the corresponding parameters. We prove that the final post-truncation power decline is limited by a linear combination of the reconstructed errors caused on each layer. Therefore, there is a guarantee that only a light retraining process on the truncated network is required to resume its original predictive performance.", "histories": [["v1", "Mon, 22 May 2017 05:54:37 GMT  (139kb,D)", "http://arxiv.org/abs/1705.07565v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["xin dong", "shangyu chen", "sinno jialin pan"], "accepted": true, "id": "1705.07565"}, "pdf": {"name": "1705.07565.pdf", "metadata": {"source": "CRF", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "authors": ["Xin Dong", "Shangyu Chen"], "emails": ["n1503521a@e.ntu.edu.sg", "schen025@e.ntu.edu.sg", "sinnopan@ntu.edu.sg"], "sections": [{"heading": null, "text": "How to develop slim and accurate deep neural networks has become crucial for realworld applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods."}, {"heading": "1 Introduction", "text": "Intuitively, deep neural networks [1] can approximate predictive functions of arbitrary complexity well when networks are of a huge amount of parameters, i.e., a lot of layers and neurons. In practice, the size of deep neural networks has been being tremendously increased, from LeNet-5 with less than 1M parameters [2] to VGG-16 with 133M parameters [3]. Such a large number of parameters not only make deep models memory intensive and computationally intensive, but also urge researchers to dig into redundancy of deep neural network. On one hand, in neuroscience, recent studies point out that there are significant redundant neurons in human brain, and memory may have relation with vanishment of specific synapses [4]. On the other hand, in machine learning, both theoretical analysis and empirical experiments have shown the evidence of redundancy in several deep models [5, 6]. Therefore, it is possible to compress deep neural networks without or with little loss in prediction by pruning parameters with carefully designed criteria.\nHowever, finding an optimal pruning solution is NP-hard because the search space for pruning is exponential in terms of parameter size. Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11]. A common idea behind most exiting approaches is to select parameters for pruning based on certain criteria, such as increase in training error, magnitude of the parameter values, etc. As most of the existing pruning criteria are designed heuristically, there is no guarantee that prediction performance of a deep neural network\nar X\niv :1\n70 5.\n07 56\n5v 1\n[ cs\n.N E\n] 2\n2 M\ncan be preserved after pruning. Therefore, a time-consuming retraining process is usually needed to boost the performance of the trimmed neural network.\nInstead of consuming efforts on a whole deep network, a layer-wise pruning method, Net-Trim, was proposed to learn sparse parameters by minimizing reconstructed error for each individual layer [6]. A theoretical analysis is provided that the overall performance drop of the deep network is bounded by the sum of reconstructed errors for each layer. In this way, the pruned deep network has a theoretical guarantee on its performance. However, as Net-Trim adopts `1-norm to induce sparsity for pruning, it fails to obtain high compression ratio compared with other methods [9, 11].\nIn this paper, we propose a new layer-wise pruning method for deep neural networks, aiming to achieve the following three goals: 1) For each layer, parameters can be highly compressed after pruning, while the reconstructed error is small. 2) There is a theoretical guarantee on the overall prediction performance of the pruned deep neural network in terms of reconstructed errors for each layer. 3) After the deep network is pruned, only a light retraining process is required to resume its original prediction performance.\nTo achieve our first goal, we borrow an idea from some classic pruning approaches for shallow neural networks, such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13]. These classic methods approximate a change in the error function via functional Taylor Series, and identify unimportant weights based on second order derivatives. Though these approaches have proven to be effective for shallow neural networks, it remains challenging to extend them for deep neural networks because of the high computational cost on computing second order derivatives, i.e., the inverse of the Hessian matrix over all the parameters. In this work, as we restrict the computation on second order derivatives w.r.t. the parameters of each individual layer only, i.e., the Hessian matrix is only over parameters for a specific layer, the computation becomes tractable. Moreover, we utilize characteristics of back-propagation for fully-connected layers in well-trained deep networks to further reduce computational complexity of the inverse operation of the Hessian matrix.\nTo achieve our second goal, based on the theoretical results in [6], we provide a proof on the bound of performance drop before and after pruning in terms of the reconstructed errors for each layer. With such a layer-wise pruning framework using second-order derivatives for trimming parameters for each layer, we empirically show that after significantly pruning parameters, there is only a little drop of prediction performance compared with that before pruning. Therefore, only a light retraining process is needed to resume the performance, which achieves our third goal.\nThe contributions of this paper are summarized as follows. 1) We propose a new layer-wise pruning method for deep neural networks, which is able to significantly trim networks and preserve the prediction performance of networks after pruning with a theoretical guarantee. In addition, with the proposed method, a time-consuming retraining process for re-boosting the performance of the pruned network is waived. 2) We conduct extensive experiments to verify the effectiveness of our proposed method compared with several state-of-the-art approaches."}, {"heading": "2 Related Works", "text": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11]. In the past, with relatively small size of training data, pruning is crucial to avoid overfitting. Classical methods include OBD and OBS. These methods aim to prune parameters with the least increase of error approximated by second order derivatives. However, computation of the Hessian inverse over all the parameters is expensive. In OBD, the Hessian matrix is restricted to be a diagonal matrix to make it computationally tractable. However, this implicitly assume parameters have no interactions, which may hurt the pruning performance. Different from OBD, OBS makes use of the full Hessian matrix for pruning, which obtains better performance while is much more computational expensive. For example, using OBS on VGG-16 naturally requires to compute the Hessian matrix with a size of 133M\u00d7 133M. Regarding pruning for modern deep models, Han et al. [9] proposed to delete unimportant parameters based on magnitude of their absolute values, and retrain the remaining ones to recover the original prediction performance. This method achieves considerable compression ratio in practice. However, as pointed out by pioneer research work [12, 13], parameters with low magnitude of their absolute values can be necessary for low error. Therefore, magnitude-based approaches may eliminate wrong parameters, resulting in a big prediction performance drop right after pruning, and poor robustness before retraining [14]. Though some variants have tried to find better magnitude-based\ncriteria [15, 16], the significant drop of prediction performance after pruning still remains. To avoid pruning wrong parameters, Guo et al. [11] introduced a mask matrix to indicate the state of network connection for dynamically pruning after each gradient decent step.\nBesides Net-trim, which is a layer-wise pruning method discussed in the previous section, there is some other work proposed to induce sparsity or low-rank approximation on certain layers for pruning [17, 18]. However, as the `0-norm or the `1-norm sparsity-induced regularization term increases difficulty in optimization, the pruned deep neural networks using these methods either obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining of the whole network to prevent accumulation of errors [10]."}, {"heading": "3 Layer-wise Optimal Brain Surgeon", "text": ""}, {"heading": "3.1 Problem Statement", "text": "Given a training set of n instances, {(xj , yj)}nj=1, and a well-trained deep neural network of L layers (excluding the input layer)1. Denote the input and the output of the deep neural network by X=[x1, ...,xn]\u2208Rd\u00d7n and Y\u2208Rn\u00d71. For a layer l, we denote the input and output of the layer by Yl\u22121 =[yl\u221211 , ...,y l\u22121 n ]\u2208Rml\u22121\u00d7n and Yl=[yl1, ...,yln]\u2208Rml\u00d7n, where yli can be considered as a representation of xi in layer l, and Y0 = X, YL = Y, and m0 = d. Using one forward-pass step, we have Yl=\u03c3(Zl), where Zl=Wl>Yl\u22121 with Wl\u2208Rml\u22121\u00d7ml being the matrix of parameters for layer l. For convenience in presentation and proof, we define nonlinear activation function \u03c3(\u00b7) as the rectified linear unit (ReLU) [19]. We further denote by \u0398l\u2208Rml\u22121ml\u00d71 the vectorization of Wl. The goal of pruning is to set the values of some elements in \u0398l to be zeros."}, {"heading": "3.2 Layer-Wise Error", "text": "During layer-wise pruning in layer l, the input Yl\u22121 is fixed as the same as the well-trained network. Suppose we set the q-th element of \u0398l, denoted by \u0398l[q] , to be zero, and get a new parameter vector, denoted by \u0398\u0302l. With Yl\u22121, we obtain a new output for layer l, denoted by Y\u0302l. Consider the root of mean square error between Yl and Yl\u22121 over the whole training data as the layer-wise error:\n\u03b5l = \u221a\u221a\u221a\u221a 1 n n\u2211 j=1 ( (y\u0302lj \u2212 ylj)>(y\u0302lj \u2212 ylj) ) = 1\u221a n \u2016Y\u0302l \u2212Yl\u2016F , (1)\nwhere \u2016 \u00b7 \u2016F is the Frobenius Norm. Note that for any single parameter pruning, one can compute its error \u03b5lq, where 1 \u2264 q \u2264 ml\u22121ml, and use it as a pruning criterion. This idea has been adopted by some methods [14]. However, in this way, for every parameter at every layer, one has to pass the whole training data once to compute its error measure, which is very computationally expensive. A more efficient approach is to make use of the second order derivatives of the error function to help identify importance of each parameter.\nWe define the error function E(\u00b7) with respect to Zl (outcome of weighted sum before performing the activation function ReLU ) as\nEl = E(Z\u0302l) = 1\nn \u2225\u2225\u2225Z\u0302l \u2212 Zl\u2225\u2225\u22252 F . (2)\nThe following lemma shows that the layer-wise error is bounded by the error defined in (2). Lemma 3.1. With the error function (2) and Yl = \u03c3(Zl), the following holds: \u03b5l \u2264 \u221a E(Z\u0302l).\nTherefore, to find parameters whose deletion (set to be zeros) minimize (1) can be translated to find parameters those deletion minimize the error function (2). Following [12, 13], the error function can be approximated by functional Taylor series as follows,\n\u03b4El =\n( \u2202El\n\u2202\u0398l\n)> \u03b4\u0398l + 1\n2 \u03b4\u0398l\n>Hl\u03b4\u0398l +O ( \u2016\u03b4\u0398l\u20163 ) , (3)\n1For simplicity in presentation, we suppose the neural network is a feed-forward (fully-connected) network. In Section 3.4, we will show how to extend our method to filter layers in Convolutional Neural Networks.\nwhere \u03b4 denotes a permutation of a corresponding variable, Hl \u2261 \u22022El/\u2202\u0398l2 is the Hessian matrix w.r.t. \u0398l, and O(\u2016\u03b4\u0398l\u20163) is the third and all higher order terms, which can be ignored. For a well-trained network, gradient w.r.t. parameters is small enough to be ignored as well, i.e., \u2202E l\n\u2202\u0398l \u21920.\nSuppose every time we aim to find a parameter \u0398l[q] to set to be zero such that the change \u03b4E l is minimal. We can formulate it as the following optimization problem:\nmin q\n1 2 \u03b4\u0398l >Hl\u03b4\u0398l, s.t. e>q \u03b4\u0398l + \u0398l[q] = 0, (4)\nwhere eq is the unit selecting vector whose q-th element is 1 and otherwise 0. It can be shown that the optimization problem (4) can be solved by the Lagrange multipliers method [20]. The optimal parameter pruning and the resultant minimal change in the error function can be written as,\n\u03b4\u0398l = \u2212 \u0398l[q]\n[H\u22121l ]qq H\u22121l eq, and Lq = \u03b4E\nl = 1\n2\n(\u0398l[q]) 2 [H\u22121l ]qq , (5)\nHere, Lq is referred to as the sensitivity of parameter \u0398l[q] . Then we select parameters to prune based on their sensitivity scores instead of their magnitudes. As mentioned in section 2, magnitude-based criteria which merely consider the numerator in (5) is a poor estimation of sensitivity of parameters. Moreover, in (5), as the inverse Hessian matrix over the training data is involved, it is able to capture data distribution when measuring sensitivities of parameters.\nAfter pruning the parameter, \u0398l[q] , with the smallest sensitivity, the parameter vector is updated via \u0398\u0302 l =\u0398l+\u03b4\u0398l. With Lemma 3.1 and (5), we have that the layer-wise error for layer l is bounded by\n\u03b5lq \u2264 \u221a E(Z\u0302l)\u2212 E(Zl) \u2264 \u221a \u03b4El =\n|\u0398l[q] |\u221a 2[H\u22121l ]qq . (6)\nNote that the term E(Zl) is canceled out in the second inequality because of the fact that E(Zl) = 0. It is worth to mention that though we merely focus on layer l, the Hessian matrix is still a square matrix with size of ml\u22121ml \u00d7 ml\u22121ml. However, we will show how to significantly reduce the computation of H\u22121 in Section 3.4."}, {"heading": "3.3 Layer-Wise Error Propagation and Accumulation", "text": "By far, we have known how to prune parameters for each layer, and estimate their introduced errors independently. However, our aim is to control the consistence of the network\u2019s final output YL before and after pruning. To do this, in the following, we show how the layer-wise errors propagate to final output layer, and the accumulated error over multiple layers will not explode. Theorem 3.2. Given a pruned deep network via layer-wise pruning introduced in Section 3.2, each layer has its own layer-wise error \u03b5l, 1 \u2264 l \u2264 L, then the accumulated error of ultimate network output \u03b5\u0303L = 1\u221a\nn \u2016Y\u0303l \u2212Yl\u2016F obeys:\n\u03b5\u0303L \u2264 L\u22121\u2211 k=1\n( L\u220f\nl=k+1\n\u2016\u0398\u0302l\u2016F \u221a \u03b4Ek ) + \u221a \u03b4EL (7)\nwhere Y\u0303l denotes \u2018accumulated pruned output\u2019 of layer l, Y\u0303l=\u03c3(W\u0302>l Y\u0303 l\u22121) and Y\u03031 =\u03c3(W\u0302>1 X).\nTheorem 3.2 shows that: 1) Layer-wise error for a layer l will be scaled by continued multiplication of parameters\u2019 Frobenius Norm over layers after l when it propagates to final output; 2) The final error of ultimate network output is bounded by the weighted sum of layer-wise errors.\nWe prove Theorem 3.2 via induction. First, for l=1, (7) holds as a special case of (6). Then suppose that Theorem 3.2 holds up to layer l:\n\u03b5\u0303l \u2264 l\u22121\u2211 h=1 ( l\u220f k=h+1 \u2016\u0398\u0302k\u2016F \u221a \u03b4Eh) + \u221a \u03b4El (8)\nIn order to show that (8) holds for layer l + 1 as well, we refer to Y\u0302l+1 =\u03c3(W\u0302>l Y l) as \u2018layer-wise pruned output\u2019, where the input Yl is fixed as the same as the originally well-trained network not an accumulated input Y\u0303l, and have the following theorem.\nTheorem 3.3. Consider layer l+1 in a pruned deep network, the difference of its accumulated pruned output, Y\u0303l+1, and layer-wise pruned output, Y\u0302l+1 is bounded by:\n\u2016Y\u0303l+1 \u2212 Y\u0302l+1\u2016F \u2264 \u221a n\u2016\u0398\u0302l\u2016F \u03b5\u0303l. (9)\nProof sketch: Consider one arbitrary element of the layer-wise pruned output Y\u0302l+1:\ny\u0302l+1ij = \u03c3(w\u0302 > i y\u0303 l j + w\u0302 > i (y l j \u2212 y\u0303lj)) \u2264 y\u0303l+1ij + \u03c3(w\u0302>i (ylj \u2212 y\u0303lj)) \u2264 y\u0303l+1ij + |w\u0302>i (ylj \u2212 y\u0303lj)|,\nwhere w\u0302i is the i-th column of W\u0302l. Similarly, it holds for accumulated pruned output:\ny\u0303l+1ij \u2264 y\u0302l+1ij + |w\u0302>i (ylj \u2212 y\u0303lj)|. By combining these two inequalities, we have |y\u0303l+1ij \u2212 y\u0302l+1ij | \u2264 |w\u0302>i (ylj \u2212 y\u0303lj)|, and thus have the following inequality in a form of matrix,\n\u2016Y\u0303l+1 \u2212 Y\u0302l+1\u2016F \u2264 \u2016W\u0302l(Yl \u2212 Y\u0303l)\u2016F \u2264 \u2016\u0398\u0302l\u2016F \u2016Yl \u2212 Y\u0303l\u2016F With (6) ,(9) and the triangle inequality, we are now able to extend (8) to layer l + 1:\n\u03b5\u0302l+1 = 1\u221a n \u2016Y\u0303l+1 \u2212Y(l+1)\u2016F \u2264 1\u221a n \u2016Y\u0303l+1 \u2212 Y\u0302(l+1)\u2016F + 1\u221a n \u2016Y\u0302l+1 \u2212Y(l+1)\u2016F\n\u2264 l\u2211\nh=1\n( l+1\u220f\nk=h+1\n\u2016\u0398\u0302k\u2016F \u00b7 \u221a \u03b4Eh ) + \u221a \u03b4El+1.\nFinally, we prove that (8) holds up for all layers, and Theorem 3.2 is a special case when l=L.\nConsider a general case with (5) and (7): parameter \u0398l[q] who has the smallest sensitivity in layer l is pruned by the i-th pruning operation, and this finally adds \u220fL k=l+1 \u2016\u0398\u0302k\u2016F \u221a \u03b4El to the ultimate network output error. It is worth to mention that although it seems that the layer-wise error is scaled by a quite large product factor, Sl = \u220fL k=l+1 \u2016\u0398\u0302k\u2016F when it propagates to the final layer, this scaling is still tractable in practice because ultimate network output is also scaled by the same product factor compared with the output of layer l. For example, we can easily estimate the norm of ultimate network output via, \u2016YL\u2016F \u2248 S1\u2016Y1\u2016F . If one pruning operation in the 1st layer causes the layer-wise error\u221a \u03b4E1, then the relative ultimate output error is \u03beLr = \u2016Y\u0303L\u2212YL\u2016F \u2016YL\u2016F \u2248 \u221a \u03b4E1 \u2016 1nY1\u2016F . Thus, we can see that\neven S1 may be quite large, the relative ultimate output error would still be about \u221a \u03b4E1/\u2016 1nY1\u2016F which is controllable in practice especially when most of modern deep networks adopt maxout layer [21] as ultimate output. Actually, S0 is called as network gain representing the ratio of the magnitude of the network output to the magnitude of the network input."}, {"heading": "3.4 The Proposed Algorithm", "text": "Prune Fully-Connected Layer: To selectively prune parameters, our approach needs to compute the inverse Hessian matrix at each layer to measure the sensitivities of each parameter of the layer, which is still computationally expensive though tractable. In this section, we present an efficient algorithm that can reduce the size of the Hessian matrix and thus speed up computation on its inverse.\nFor each layer l, according to the definition of the error function used in Lemma 3.1, the first derivative of the error function with respect to \u0398\u0302l is \u2202E\u2202\u0398l = \u2212 1 n \u2211n j=1 \u2202zj \u2202\u0398l\n(z\u0302j \u2212 zj), where z\u0302j and zj are the j-th columns of the matrices Z\u0302 and Z, respectively, and the Hessian matrix is defined as:\nH\u2261 \u22022E \u2202(\u0398l) 2 = 1 n \u2211n j=1 ( \u2202zj \u2202\u0398l ( \u2202zj \u2202\u0398l )> \u2212 \u2202 2zj \u2202(\u0398l) 2 (z\u0302j\u2212zj)> ) . Note that given a well-trained network, for most cases, z\u0302j is quite close to zj , we simply ignore the term containing z\u0302j\u2212zj . Even in the late-stage of pruning when this difference is not small, we can still ignore the corresponding term [13]. For layer l that has ml output units, zj=[z1j , . . . , zmlj ], the Hessian matrix can be calculated via\nH = 1\nn n\u2211 j=1 Hj = 1 n n\u2211 j=1 ml\u2211 i=1 \u2202zij \u2202\u0398l ( \u2202zij \u2202\u0398l )> , (10)\nwhere the Hessian matrix for a single instance, Hj , is a block diagonal square matrix of the size ml\u22121 \u00d7ml. Specifically, gradient of the first output unit z1j with respect to all parameters \u0398l is \u2202z1j \u2202\u0398l = [ \u2202z1j \u2202w1 , . . . , \u2202z1j \u2202wml ] , where wi is the i-th column of Wl.\nAs z1j is the layer output before activation function, its gradient is simply to calculate, and more importantly all output units\u2019s gradients are equal to the layer input: \u2202zij\u2202wi = y l\u22121 j , and \u2202zij \u2202wj\n= 0 for i 6= j. It can be shown that the block diagonal square matrix Hj\u2019s diagonal blocks Hjii are all equal to \u03c8 j = y (l\u22121) j ( y (l\u22121) j )> , and the inverse Hessian matrix H\u22121\nis also a block diagonal square matrix with its diagonal blocks being ( 1n \u2211n j=1\u03c8\nj)\u22121. In addition, normally \u03a8 = 1n \u2211n j=1\u03c8\nj is degenerate and its pseudo-inverse can be calculated recursively via standard matrix inver-\nsion: \u03a8\u22121j+1 = \u03a8 \u22121 j \u2212\n\u03a8\u22121j y (l\u22121) j ( y (l\u22121) j )> \u03a8\u22121j\nn+ ( y (l\u22121) j+1 )> \u03a8\u22121j y (l\u22121) j+1 , where\n\u03a8t = 1 t \u2211t j=1\u03c8\nj with \u03a8\u221210 =\u03b1I, \u03b1 \u2208 [104, 108], and \u03a8\u22121 = \u03a8\u22121n . The size of \u03a8 is then reduced to ml\u22121, and the computational complexity of calculating H\u22121 is O ( nm2l\u22121 ) .\nTo make estimated minimal change of the error function optimal in (5), two conditions have to be met: 1) the whole network is at a local minimum in error (gradient\nis small enough to be ignored), and 2) the Hessian matrix is exact. Since the Hessian matrix only depends on layer input , it is always able to be exact even after several pruning operations. The only parameter we have to control is the layer-wise error to ensure gradient to be small enough after pruning. That is one of reasons why there is a \u201cpruning inflection point\u201d after which layer-wise error would drop dramatically. In practice, user can incrementally increase the size of pruned parameters based on sensitivity Lq, and make a trade-off between pruning ratio and performance drop to set a proper tolerable error threshold or pruning ratio.\nAccording to above analysis, the procedure of our pruning algorithm for a fully-connected layer l is as follows. Step 1: Get layer input yl\u22121 from a well-trained deep network. Step 2: Calculate the Hessian matrix Hii and its pseudo-inverse over the dataset, and get the whole pseudo-inverse of the Hessian matrix. Step 3: Compute optimal parameter change \u03b4\u0398l and the sensitivity Lq for each parameter in layer l. Set tolerable error threshold . Step 4: Pick up parameters \u0398l[q] \u2019s with the smallest sensitivity scores. Step 5: If \u221a Lq \u2264 , prune the parameter \u0398l[q]\u2019s and get new parameter\nvalues via \u0398\u0302 = \u0398 + \u03b4\u0398, then repeat Step 4; otherwise stop pruning.\nPrune Convolutional Layer: It is straightforward to generalize our method to a convolutional layer and its variants if we vectorize filters of each channel and consider them as special fully-connected layers that have multiple inputs (patches) from a single instance. Consider a vectorized filter wi of channel i, 1 \u2264 i \u2264 ml, it acts similarly to parameters which are connected to the same output unit in a fully-connected layer. However, the difference is that for a single input instance j, every filter step of a sliding window across of it will extract a patch Cjn from the input volume. Similarly, each pixel zijn in the 2-dimensional activation map that gives the response to each patch corresponds to one output unit in a fully-connected layer. Hence, in convolutional layer case, (10) is generalized as H = 1n \u2211n j=1 \u2211ml i=1 \u2211 jn \u2202zijn \u2202[w1,...,wml ]\n, where H is also a block diagonal square matrix whose diagonal blocks are all the same. Then, we can slightly revise the computation of the Hessian matrix, and extend the algorithm for fully-connected layers to convolutional layers.\nNote that the accumulated error of ultimate network output can be linearly bounded by layer-wise error as long as the model is feed-forward. Thus, L-OBS is a general pruning method and friendly with most of feed-forward neural networks whose layer-wise Hessian can be computed expediently with slight modification. However, if models have sizable layers like ResNet-101, L-OBS may not be economical because of computational cost of Hessian, which will be studied in our future work."}, {"heading": "4 Experiments", "text": "In this section, we verify the effectiveness of our proposed Layer-wise OBS (L-OBS) using various architectures of deep neural networks in terms of compression ratio (CR), error rate before retraining, and the number of iterations required for retraining to achieve satisfactory performance. Here, CR is defined as ratio of the number of preserved parameters to the number of original parameters. We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6]. The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset. For experiments, we first well-train the networks, and apply various pruning approaches on networks to evaluate their performance. The retraining batch size, crop method and other hyper-parameters are under the same setting as used in LWC. Note that to make comparisons fair, we do not adopt any other pruning related methods like Dropout or sparse regularizers on MNIST. In practice, L-OBS can work well along with them as shown on large scale dataset CIFAR-10 and ImageNet."}, {"heading": "4.1 Overall Comparison Results", "text": "The overall comparison results are shown in Table 1. In the first set of experiments, we prune each layer of the well-trained LeNet-300-100 with compression ratios: 6.7%, 20% and 65%, achieving slightly better overall compression ratio (7%) than LWC. Under comparable compression ratio, L-OBS has quite less drop of performance and lighter retraining compared with LWC whose performance is almost ruined by pruning. Classic pruning approach OBD is also compared though we observe that Hessian matrixes of most modern deep models are strongly non-diagonal in practice. Besides relative heavy cost to obtain the second derivatives via the chain rule, OBD suffers from drastic drop of performance when it is directly applied to modern deep models as shown in Table 1.\nTo properly prune each layer of LeNet-5, we increase tolerable error threshold from relative small initial value to incrementally prune more parameters, monitor model performance, stop pruning and set until encounter the \u201cpruning inflection point\u201d mentioned in Section 3.4. In practice, we prune each layer of LeNet-5 with compression ratio: 54%, 43%, 6% and 25% and retrain pruned model with mere milli iterations in other methods. As DNS retrains the pruned network after every pruning operation, we are not able to report its error rate of pruned network before retraining. However, as can be seen, similar to LWC, the total number of iterations used by DNS for rebooting the network is very large compared with L-OBS. Results of retraining iterations of DNS are reported from [11] and the other experiments are implemented based on TensorFlow. [24] In addition, in scenario requiring high pruning ratio, L-OBS can be quite flexibly adopted to its iterative version-doing pruning and light retraining alternately, to obtain higher pruning ratio with relative higher cost of pruning. With two iterations of pruning and retraining, L-OBS is able to achieve as the same pruning ratio as DNS with much lighter total retraining: 643 iterations on LeNet-300-100 and 841 iterations on LeNet-5.\nRegarding comparison results with CIFAR-Net, our well-trained CIFAR-Net achieves testing error of 18.57% with Dropout and Batch-Normalization. Then we prune the well-trained network with LWC and L-OBS and get the similar results as experiments on other network architecture. We also observe that LWC and other retraining-required methods always require much smaller learning rate in retraining. This is because representation capability of the pruned networks which have much fewer parameters is damaged during pruning based on a principle that number of parameters is an important factor for representation capability. However, L-OBS can still adopt original learning rate to retrain the pruned networks. Under this consideration, L-OBS not only ensures a warm-start for retraining, but also finds important connections (parameters) and preserve capability of representation for the pruned network instead of ruining model with pruning.\nFor AlexNet, as with the previous experiments, L-OBS achieves overall compression ratio 11% without loss of accuracy taking 2.9 hours on 48 Intel Xeon(R) CPU E5-1650 to compute Hessians and 3.1 hours on NVIDIA Tian X GPU to retrain pruned model (i.e. 18.1K iterations). Cost of Hessian inverse\u2019s computation in L-OBS is negligible compared with heavy retraining in other methods. This claim can also be supported by analysis of time complexity. As mentioned in Section 3.4, time complexity of calculating H\u22121 is O ( nm2l\u22121 ) . Assume that neural networks are retrained via SGD,\ntime complexity of retraining is approximately O (IdM) , where d is the size of the mini-batch, M and I is total number of parameters and iterations. ForM \u2248\u2211l=Ll=1 (m2l\u22121) and retraining in previous methods always requires millions of iterations (Id n), complexity of calculating Hessian (inverse)\n2A revised AlexNet for CIFAR-10 containing three convolutional layers and two fully connected layers.\nDNS AlexNet (Top-1 err. / Top-5 err.) 43.30 / 20.08% 5.7% - 43.91 / 21.88% 7.30\u00d7 105 LWC AlexNet (Top-1 err. / Top-5 err.) 43.30 / 20.08% 11% 76.14 / 57.68% 44.06 / 20.64% 1.04\u00d7 106 L-OBS AlexNet (Top-1 err. / Top-5 err.) 43.30 / 20.08% 11% 50.04 / 26.87% 43.11 / 20.01% 1.81\u00d7 104\nLWC VGG-16 (Top-1 err. / Top-5 err.) 31.66 / 10.12% 7.5% 73.61 / 52.64% 32.43 / 11.12% 3.75\u00d7 109 L-OBS VGG-16 (Top-1 err. / Top-5 err.) 31.66 / 10.12% 7.5% 37.32 / 14.82% 32.02 / 10.97% 8.63\u00d7 104\nis quite economic compared with heavy retraining. More interestingly, there is a trade-off between compression ratio and pruning (including retraining) cost. Compared with previous methods, L-OBS is able to provide Fast-Compression: prune AlexNet to 16% of its original size without substantively impacting accuracy (pruned top-5 error 20.98%) even without any retraining. We further apply our method to VGG-16, whose size is larger than AlexNet with 138M parameters. To achieve more promising compression ratio, we use two iterations of pruning and retraining and each iteration following a similar methodology as before. Our L-OBS achieves overall compression ratio 7.5% without loss of accuracy taking totally 10.2 hours on 48 Intel Xeon(R) CPU E5-1650 to compute Hessians and 86.3K iterations to retrain pruned model.\n4.2 Comparison between L-OBS and Net-Trim\nAs L-OBS is inspired by Net-Trim, which adopts `1-norm to induce sparsity, we conduct comparison experiments between these two methods. By using second-derivative information, L-OBS achieves much better CR. In Net-Trim, networks are pruned by formulating layer-wise pruning as a optimization: minWl \u2016Wl\u20161 s.t. \u2016\u03c3(W>l Yl\u22121)\u2212Yl\u2016F \u2264 \u03bel, where \u03bel corresponds to \u03belr\u2016Yl\u2016F in L-OBS. Due to memory limitation of Net-Trim, we only prune the middle layer of LeNet-300-100 with L-OBS and Net-Trim under the\nsame setting. As shown in Table 2, under the same pruned error rate, CR of L-OBS outnumbers that of the Net-Trim by about six times. In addition, Net-Trim would encounter explosion of memory and time with large-scale datasets and large size parameters. Specifically, space complexity of positive semidefinite matrix Q in quadratic constraints, which is used when reformulating above optimization as a QC program, is O ( 2nm2lml\u22121 ) . Q requires about 65.7Gb for 1,000 samples in MNIST as illustrated in Figure 2. Moreover, Net-Trim is designed for multilayer perceptron and not clear how to deploy it on convolutional layers."}, {"heading": "5 Conclusion", "text": "We have proposed a novel L-OBS pruning framework to prune parameters based on second order derivatives information of the layer-wise error function and provided a theoretical guarantee on the overall error in terms of the reconstructed errors for each layer. Our proposed L-OBS can prune considerable number of parameters with tiny drop of performance and reduce or even omit retraining. More importantly, it identifies and preserves the real important part of networks when pruning compared with previous methods, which may help to dive into nature of neural networks."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Ultrastructural evidence for synaptic scaling across the wake/sleep", "author": ["Luisa de Vivo", "Michele Bellesi", "William Marshall", "Eric A Bushong", "Mark H Ellisman", "Giulio Tononi", "Chiara Cirelli"], "venue": "cycle. Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Net-trim: A layer-wise convex pruning of deep neural networks", "author": ["A. Nguyen N. Aghasi", "J. Romberg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Pruning algorithms-a survey", "author": ["Russell Reed"], "venue": "IEEE transactions on Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sparsifying neural network connections for face recognition", "author": ["Yi Sun", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPs,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "The incredible shrinking neural network: New perspectives on learning representations through the lens of pruning", "author": ["Nikolas Wolfe", "Aditya Sharma", "Lukas Drude", "Bhiksha Raj"], "venue": "arXiv preprint arXiv:1701.04465,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Network trimming: A data-driven neuron pruning approach towards efficient deep architectures", "author": ["Hengyuan Hu", "Rui Peng", "Yu-Wing Tai", "Chi-Keung Tang"], "venue": "arXiv preprint arXiv:1607.03250,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Pruning filters for efficient convnets", "author": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["Cheng Tai", "Tong Xiao", "Yi Zhang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall Tappen", "Marianna Pensky"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Convex analysis", "author": ["R Tyrrell Rockafellar"], "venue": "princeton landmarks in mathematics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In practice, the size of deep neural networks has been being tremendously increased, from LeNet-5 with less than 1M parameters [2] to VGG-16 with 133M parameters [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "In practice, the size of deep neural networks has been being tremendously increased, from LeNet-5 with less than 1M parameters [2] to VGG-16 with 133M parameters [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "On one hand, in neuroscience, recent studies point out that there are significant redundant neurons in human brain, and memory may have relation with vanishment of specific synapses [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "On the other hand, in machine learning, both theoretical analysis and empirical experiments have shown the evidence of redundancy in several deep models [5, 6].", "startOffset": 153, "endOffset": 159}, {"referenceID": 4, "context": "On the other hand, in machine learning, both theoretical analysis and empirical experiments have shown the evidence of redundancy in several deep models [5, 6].", "startOffset": 153, "endOffset": 159}, {"referenceID": 5, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 6, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 7, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 8, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Recent work mainly focuses on developing efficient algorithms to obtain a near-optimal pruning solution [7, 8, 9, 10, 11].", "startOffset": 104, "endOffset": 121}, {"referenceID": 4, "context": "Instead of consuming efforts on a whole deep network, a layer-wise pruning method, Net-Trim, was proposed to learn sparse parameters by minimizing reconstructed error for each individual layer [6].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "However, as Net-Trim adopts `1-norm to induce sparsity for pruning, it fails to obtain high compression ratio compared with other methods [9, 11].", "startOffset": 138, "endOffset": 145}, {"referenceID": 9, "context": "However, as Net-Trim adopts `1-norm to induce sparsity for pruning, it fails to obtain high compression ratio compared with other methods [9, 11].", "startOffset": 138, "endOffset": 145}, {"referenceID": 10, "context": "To achieve our first goal, we borrow an idea from some classic pruning approaches for shallow neural networks, such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "To achieve our first goal, we borrow an idea from some classic pruning approaches for shallow neural networks, such as optimal brain damage (OBD) [12] and optimal brain surgeon (OBS) [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "To achieve our second goal, based on the theoretical results in [6], we provide a proof on the bound of performance drop before and after pruning in terms of the reconstructed errors for each layer.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 6, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 7, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 8, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 9, "context": "Pruning methods have been widely used for model compression in early neural networks [7] and modern deep neural networks [6, 8, 9, 10, 11].", "startOffset": 121, "endOffset": 138}, {"referenceID": 7, "context": "[9] proposed to delete unimportant parameters based on magnitude of their absolute values, and retrain the remaining ones to recover the original prediction performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "However, as pointed out by pioneer research work [12, 13], parameters with low magnitude of their absolute values can be necessary for low error.", "startOffset": 49, "endOffset": 57}, {"referenceID": 11, "context": "However, as pointed out by pioneer research work [12, 13], parameters with low magnitude of their absolute values can be necessary for low error.", "startOffset": 49, "endOffset": 57}, {"referenceID": 12, "context": "Therefore, magnitude-based approaches may eliminate wrong parameters, resulting in a big prediction performance drop right after pruning, and poor robustness before retraining [14].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "criteria [15, 16], the significant drop of prediction performance after pruning still remains.", "startOffset": 9, "endOffset": 17}, {"referenceID": 14, "context": "criteria [15, 16], the significant drop of prediction performance after pruning still remains.", "startOffset": 9, "endOffset": 17}, {"referenceID": 9, "context": "[11] introduced a mask matrix to indicate the state of network connection for dynamically pruning after each gradient decent step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Besides Net-trim, which is a layer-wise pruning method discussed in the previous section, there is some other work proposed to induce sparsity or low-rank approximation on certain layers for pruning [17, 18].", "startOffset": 199, "endOffset": 207}, {"referenceID": 16, "context": "Besides Net-trim, which is a layer-wise pruning method discussed in the previous section, there is some other work proposed to induce sparsity or low-rank approximation on certain layers for pruning [17, 18].", "startOffset": 199, "endOffset": 207}, {"referenceID": 4, "context": "However, as the `0-norm or the `1-norm sparsity-induced regularization term increases difficulty in optimization, the pruned deep neural networks using these methods either obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining of the whole network to prevent accumulation of errors [10].", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "However, as the `0-norm or the `1-norm sparsity-induced regularization term increases difficulty in optimization, the pruned deep neural networks using these methods either obtain much smaller compression ratio [6] compared with direct pruning methods or require retraining of the whole network to prevent accumulation of errors [10].", "startOffset": 329, "endOffset": 333}, {"referenceID": 17, "context": "For convenience in presentation and proof, we define nonlinear activation function \u03c3(\u00b7) as the rectified linear unit (ReLU) [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "This idea has been adopted by some methods [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Following [12, 13], the error function can be approximated by functional Taylor series as follows,", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "Following [12, 13], the error function can be approximated by functional Taylor series as follows,", "startOffset": 10, "endOffset": 18}, {"referenceID": 18, "context": "It can be shown that the optimization problem (4) can be solved by the Lagrange multipliers method [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Even in the late-stage of pruning when this difference is not small, we can still ignore the corresponding term [13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "\u03a8t = 1 t \u2211t j=1\u03c8 j with \u03a8\u22121 0 =\u03b1I, \u03b1 \u2208 [10, 10], and \u03a8\u22121 = \u03a8\u22121 n .", "startOffset": 39, "endOffset": 47}, {"referenceID": 8, "context": "\u03a8t = 1 t \u2211t j=1\u03c8 j with \u03a8\u22121 0 =\u03b1I, \u03b1 \u2208 [10, 10], and \u03a8\u22121 = \u03a8\u22121 n .", "startOffset": 39, "endOffset": 47}, {"referenceID": 10, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "We conduct comparison results of L-OBS with the following pruning approaches: 1) Randomly prune, 2) OBD [12], 3) LWC [9], 4) DNS [11], and 5) Net-Trim [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 83, "endOffset": 86}, {"referenceID": 19, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "The deep architectures used for experiments include: LeNet-300-100 [2] and LeNet-5 [2] on the MNIST dataset, CIFAR-Net2 [22] on the CIFAR-10 dataset, AlexNet [23] and VGG-16 [3] on the ImageNet ILSVRC-2012 dataset.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Results of retraining iterations of DNS are reported from [11] and the other experiments are implemented based on TensorFlow.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "[24] In addition, in scenario requiring high pruning ratio, L-OBS can be quite flexibly adopted to its iterative version-doing pruning and light retraining alternately, to obtain higher pruning ratio with relative higher cost of pruning.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "How to develop slim and accurate deep neural networks has become crucial for realworld applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.", "creator": "LaTeX with hyperref package"}}}