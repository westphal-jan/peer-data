{"id": "1705.09189", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs", "abstract": "We introduce a neural network that represents sentences by composing their words by induced binary parse trees. We use Tree-LSTM as a composition function, applied along a tree structure found by a fully differentiable natural language diagram parser. Our model simultaneously optimizes both the composition function and the parser, eliminating the need for externally provided parse trees normally required for Tree-LSTM. Therefore, it can be considered a tree-based RNN that is not monitored in relation to the parse trees. As it is fully differentiable, our model is easy to train with a conventional gradient descend method and backpropagation. We show that it achieves better performance compared to different monitored Tree-LSTM architectures in a textual task and a reversed dictionary task.", "histories": [["v1", "Thu, 25 May 2017 14:09:48 GMT  (113kb,D)", "http://arxiv.org/abs/1705.09189v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean maillard", "stephen clark", "dani yogatama"], "accepted": false, "id": "1705.09189"}, "pdf": {"name": "1705.09189.pdf", "metadata": {"source": "CRF", "title": "Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs", "authors": ["Jean Maillard", "Stephen Clark"], "emails": ["jean@maillard.it", "sc609@cam.ac.uk", "dyogatama@google.com"], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks, in particular the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) and some of its variants (Bahdanau et al., 2014, Graves and Schmidhuber, 2005) have been widely applied to problems in natural language processing. Examples include language modelling (J\u00f3zefowicz et al., 2016, Sundermeyer et al., 2012), textual entailment (Bowman et al., 2015, Sha et al., 2016), and machine translation (Bahdanau et al., 2014, Sutskever et al., 2014) amongst others.\nThe topology of an LSTM network is linear: words are read sequentially, normally in left-to-right order. However, language is known to have an underlying hierarchical, tree-like structure (Chomsky, 1957). How to capture this structure in a neural network, and whether doing so leads to improved performance on common linguistic tasks, is an open question. The Tree-LSTM network (Tai et al., 2015, Zhu et al., 2015) provides a possible answer, by generalising the LSTM to tree-structured topologies. It was shown to be more effective than a standard LSTM in semantic relatedness and sentiment analysis tasks.\nDespite their superior performance on these tasks, Tree-LSTM networks have the drawback of requiring an extra labelling of the input sentences in the form of parse trees. These can be either provided by an automatic parser (Tai et al., 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016). Yogatama et al. (2016) proposed to remove this requirement by including a shift-reduce parser in the model, to be optimised alongside the composition function based on a downstream task. This makes the full model non-differentiable so it needs to be trained with reinforcement learning, which can be slow due to high variance.\nOur proposed approach is to include a chart parser in the model, inspired by the CYK constituency parser (Cocke, 1969, Kasami, 1965, Younger, 1967). Due to the parser being fully differentiable, the entire model can be trained end-to-end for a downstream task by using stochastic gradient descent.\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017).\nar X\niv :1\n70 5.\n09 18\n9v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\nOur model is also unsupervised with respect to the parse trees, similar to Yogatama et al. (2016). We show that the proposed method outperforms other Tree-LSTM architectures based on fully left-branching, right-branching, and supervised parse trees on a textual entailment task and a reverse dictionary task."}, {"heading": "2 Related work", "text": "Our work can be seen as part of a wider class of sentence embedding models that let their composition order be guided by a tree structure. These can be further split into two groups: (1) models that rely on traditional syntactic parse trees, usually provided as input, and (2) models that induce a tree structure based on some downstream task.\nIn the first group, Paperno et al. (2014) take inspiration from the standard Montagovian semantic treatment of composition. They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011). These tensors are trained via linear regression based on a downstream task, but the tree that determines their order of application is expected to be provided as input. Socher et al. (2012) and Socher et al. (2013) also rely on external trees, but use recursive neural networks as the composition function.\nInstead of using a single parse tree, Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty. The authors use a convolutional neural network composition function and, like our model, rely on a mechanism similar to the one employed by the CYK parser to process the trees. Ma et al. (2015) propose a related model, also making use of syntactic information and convolutional networks to obtain a representation in a bottom-up manner. Convolutional neural networks can also be used to produce embeddings without the use of tree structures, such as in Kalchbrenner et al. (2014).\nBowman et al. (2016) propose an RNN that produces sentence embeddings optimised for a downstream task, with a composition function that works similarly to a shift-reduce parser. The model is able to operate on unparsed data by using an integrated parser. However, it is trained to mimic the decisions that would be taken by an external parser, and is therefore not free to explore using different tree structures. Dyer et al. (2016) introduce a probabilistic model of sentences that explicitly models nested, hierarchical relationships among words and phrases. They too rely on a shift-reduce parsing mechanism to obtain trees, trained on a corpus of gold-standard trees.\nIn the second group, Yogatama et al. (2016) shows the most similarities to our proposed model. The authors use reinforcement learning to learn tree structures for a neural network model similar to Bowman et al. (2016), taking performance on a downstream task that uses the computed sentence representations as the reward signal. Kim et al. (2017) take a slightly different approach: they formalise a dependency parser as a graphical model, viewed as an extension to attention mechanisms, and hand-optimise the backpropagation step through the inference algorithm."}, {"heading": "3 Models", "text": "All the models take a sentence as input, represented as an ordered sequence of words. Each word wi \u2208 V in the vocabulary is encoded as a (learned) word embedding wi \u2208 Rd. The models then output a sentence representation h \u2208 RD, where the output space RD does not necessarily coincide with the input space Rd."}, {"heading": "3.1 Bag of Words", "text": "Our simplest baseline is a bag-of-words (BoW) model. Due to its reliance on addition, which is commutative, any information on the original order of words is lost. Given a sentence encoded by embeddings w1, . . . ,wn it computes\nh = n\u2211 i=1 tanh (Wwi + b) ,\nwhere W is a learned input projection matrix."}, {"heading": "3.2 LSTM", "text": "An obvious choice for a baseline is the popular Long Short-Term Memory (LSTM) architecture of Hochreiter and Schmidhuber (1997). It is a recurrent neural network that, given a sentence encoded by embeddings w1, . . . ,wT , runs for T time steps t = 1 . . . T and computes itf tut\not  = Wwt +Uht\u22121 + b, ct = ct\u22121 \u03c3(f t) + tanh(ut) \u03c3(it), ht = \u03c3(ot) tanh(ct),\nwhere \u03c3(x) = 11+e\u2212x is the standard logistic function. The LSTM is parametrised by the matrices W \u2208 R4D\u00d7d, U \u2208 R4D\u00d7D, and the bias vector b \u2208 R4D. The vectors \u03c3(it), \u03c3(f t), \u03c3(ot) \u2208 RD are known as input, forget, and output gates respectively, while we call the vector tanh(ut) the candidate update. We take hT , the h-state of the last time step, as the final representation of the sentence.\nFollowing the recommendation of Jozefowicz et al. (2015), we deviate slightly from the vanilla LSTM architecture described above by also adding a bias of 1 to the forget gate, which was found to improve performance."}, {"heading": "3.3 Tree-LSTM", "text": "Tree-LSTMs are a family of extensions of the LSTM architecture to tree structures (Tai et al., 2015, Zhu et al., 2015). We implement the version designed for binary constituency trees. Given a node with children labelled L and R, its representation is computed as\n i fL fR u o  = Ww +UhL +VhR + b, (1) c = cL \u03c3(fL) + cR \u03c3(fR) + tanh(u) \u03c3(i), (2) h = \u03c3(o) tanh(c), (3)\nwhere w in (1) is a word embedding, only nonzero at the leaves of the parse tree; and hL,hR and cL, cR are the node children\u2019s h- and c-states, only nonzero at the branches. These computations are repeated recursively following the tree structure, and the representation of the whole sentence is given by the h-state of the root node. Analogously to our LSTM implementation, here we also add a bias of 1 to the forget gates."}, {"heading": "3.4 Unsupervised Tree-LSTM", "text": "While the Tree-LSTM is very powerful, it requires as input not only the sentence, but also a parse tree structure defined over it. Our proposed extension optimises this step away, by including a basic CYK-style (Cocke, 1969, Kasami, 1965, Younger, 1967) chart parser in the model. The parser has the property of being fully differentiable, and can therefore be trained jointly with the Tree-LSTM composition function for some downstream task.\nThe CYK parser relies on a chart data structure, which provides a convenient way of representing the possible binary parse trees of a sentence, according to some grammar. Here we use the chart as an efficient means to store all possible binary-branching trees, effectively using a grammar with only a single non-terminal. This is sketched in simplified form in Table 1 for an example input. The chart is drawn as a diagonal matrix, where the bottom row contains the individual words of the input sentence.\nThe nth row contains all cells with branch nodes spanning n words (here each cell is represented simply by the span \u2013 see Figure 1 below for a forest representation of the nodes in all possible trees). By combining nodes in this chart in various ways it is possible to efficiently represent every binary parse tree of the input sentence.\nThe unsupervised Tree-LSTM uses an analogous chart to guide the order of composition. Instead of storing sequences of words however, here each cell is made up of a pair of vectors (h, c) representing the state of the Tree-LSTM RNN at that particular node in the tree. The process starts at the bottom row, where each cell is filled in by calculating the Tree-LSTM output (1)-(3) with w set to the embedding of the corresponding word. These are the leaves of the parse tree. Then, the second row is computed by repeatedly calling the Tree-LSTM with the appropriate children. This row contains the nodes that are directly combining two leaves. They might not all be needed for the final parse tree: some leaves might connect directly to higher-level nodes, which have not yet been considered. However, they are all computed, as we cannot yet know whether there are better ways of connecting them to the tree. This decision is made at a later stage.\nStarting from the third row, ambiguity arises since constituents can be built up in more than one way: for example, the constituent \u201cneuro linguistic programming\u201d in Table 1 can be made up either by combining the leaf \u201cneuro\u201d and the second-row node \u201clinguistic programming\u201d, or by combining the second-row node \u201cneuro linguistic\u201d and the leaf \u201cprogramming\u201d. In these cases, all possible compositions are performed, leading to a set of candidate constituents (c1,h2), . . . , (cn,hn). Each is assigned an energy, given by\nei = cos(u,hi), (4)\nwhere cos(\u00b7, \u00b7) indicates the cosine similarity function and u is a (trained) vector of weights. All energies are then passed through a softmax function to normalise them, and the cell representation is finally calculated as a weighted sum of all candidates using the softmax output:\nsi = softmax(ei/t), (5)\nc = n\u2211 i=1 sici, h = n\u2211 i=1 sihi.\nThe softmax uses a temperature hyperparameter t which, for small values, has the effect of making the distribution sparse by making the highest score tend to 1. In all our experiments the temperature is initialised as t = 1, and is smoothly decreasing as t = 1/2e, where e \u2208 Q is the fraction of training epochs that have been completed. In the limit t\u2192 0+, this mechanism will only select the highest scoring option, and is equivalent to the argmax operation. The same procedure is repeated for all higher rows, and the final output is given by the h-state of the top cell of the chart.\nThe whole process is sketched in Figure 1 for an example sentence. Note how, for instance, the final sentence representation can be obtained in three different ways, each represented by a coloured circle. All are computed, and the final representation is a weighted sum of the three, represented by the dotted lines. When the temperature t in (5) reaches very low values, this effectively reduces to the single \u201cbest\u201d tree, as selected by gradient descent."}, {"heading": "4 Experiments", "text": "All models are implemented in Python 3.5.2 with the DyNet neural network library (Neubig et al., 2017) at commit bffe22b. The code for all following experiments can be found on the first author\u2019s website.1\nFor training we use stochastic gradient descent with a batch size of 16, which was found to perform better than AdaGrad (Duchi et al., 2010) and similar methods on our development data. Performance on the development data is used to determine when to stop training.\nThe textual entailment model was trained on a 2.2GHz Intel Xeon E5-2660 CPU, and took one and a half weeks to converge. The reverse dictionary model was trained on a NVIDIA GeForce GTX TITAN Black GPU, and took five days to converge.\nOn top of the baselines already described in \u00a73, for the following experiments we also train two additional Tree-LSTM models that use a fixed composition order: one that uses a fully left-branching tree, and one that uses a fully right-branching tree."}, {"heading": "4.1 Textual Entailment", "text": "We test our model and baselines on the Stanford Natural Language Inference task (Bowman et al., 2015), consisting of 570 k manually annotated pairs of sentences. Given two sentences, the aim is to predict whether the first entails, contradicts, or is neutral with respect to the second. For example, given \u201cchildren smiling and waving at camera\u201d and \u201cthere are children present\u201d, the model would be expected to predict entailment.\nFor this experiment, we choose 100D input embeddings, initialised with 100D GloVe vectors (Pennington et al., 2014) and with out-of-vocabulary words set to the average of all other vectors. This results in a 100\u00d7 37 369 word embedding matrix, fine-tuned during training. For the supervised Tree-LSTM model, we used the parse trees included in the dataset.\nGiven a pair of sentences, one of the models is used to produce the embeddings s1, s2 \u2208 R100. Following Yogatama et al. (2016) and Bowman et al. (2016), we then compute\nu = (s1 \u2212 s2)2, v = s1 s2,\nq = ReLU A uvs1 s2 + a  ,\n1https://www.maillard.it/\nwhere A \u2208 R200\u00d7400 and a \u2208 R200 are trained model parameters. Finally, the correct label is predicted by p(y\u0302 = c | q;B, b) \u221d exp(Bcq+ bc), where B \u2208 R3\u00d7200 and b \u2208 R3 are trained model parameters.\nTable 2 lists the accuracy and number of parameters for our model, baselines, as well as other sentence embedding models in the literature. When the information is available, we report both the number of intrinsic model parameters as well as the number of word embedding parameters. For other models these figures are based on the data from the SNLI website2 and the original papers."}, {"heading": "4.2 Reverse Dictionary", "text": "We also test our model and baselines on the reverse dictionary task of Hill et al. (2016), which consists of 852 k word-definition pairs. The aim is to retrieve the name of a concept from a list of words, given its definition. For example, when provided with the sentence \u201ccontrol consisting of a mechanical device for controlling fluid flow\u201d, a model would be expected to rank the word \u201cvalve\u201d above other confounders in a list. We use three test sets provided by the authors: two sets involving word definitions, either seen during training or held out; and one set involving concept descriptions instead of formal definitions. Performance is measured via three statistics: the median rank of the correct answer over a list of over 66 k words; and the proportion of cases in which the correct answer appears in the top 10 and 100 ranked words (top 10 accuracy and top 100 accuracy).\nAs output embeddings, we use the 500D CBOW vectors (Mikolov et al., 2013) provided by the authors. As input embeddings we use the same vectors, reduced to 256 dimensions with PCA. Given a training definition as a sequence of (input) embeddings w1, . . . ,wn \u2208 R256, the model produces an embedding s \u2208 R256 which is then mapped to the output space via a trained projection matrix W \u2208 R500\u00d7256. The training objective to be maximised is then the cosine similarity cos(Ws,d) between the definition embedding and the (output) embedding d of the word being defined. For the supervised Tree-LSTM model, we additionally parsed the definitions with Stanford CoreNLP (Manning et al., 2014) to obtain parse trees.\nWe hold out 128 batches from the training set to be used as development data. The softmax temperature in (5) is allowed to decrease as described in \u00a73.4 until it reaches a value of 0.005, and then kept constant. This was found to have the best performance on the development set.\nTable 3 shows the results for our model and baselines, as well as the models of Hill et al. (2016) which are based on the same cosine training objective. Our bag-of-words model consists of 193.8 k parameters; our LSTM uses 653 k parameters; the fixed-branching, supervised, and unsupervised Tree-LSTM models all use 1.1 M parameters. On top of these, the input word embeddings consist of 113 123 \u00d7 256 parameters. Output embeddings are not counted as they are not updated during training.\n2https://nlp.stanford.edu/projects/snli/"}, {"heading": "5 Discussion", "text": "The results in Tables 2 and 3 show that the unsupervised Tree-LSTM matches or outperforms all tested baselines.\nFor the textual entailment task, our model compares favourably to all baselines including the supervised Tree-LSTM, as well as some of the other sentence embedding models in the literature that have a higher number of parameters. Our model could be plausibly improved by combining it with aspects of other models, and we make some concrete suggestions in that direction in \u00a76.\nIn the reversed dictionary task, the very poor performance of the supervised Tree-LSTM can be explained by the unusual tokenisation algorithm used in the dataset of Hill et al. (2016): all punctuation is simply stripped, turning for instance \u201c(archaic) a section of a poem\u201d into \u201carchaic a section of a poem\u201d, or stripping away the semicolons in long lists of synonyms. On the one hand, this might seem unfair on the supervised Tree-LSTM, which received suboptimal trees as input. On the other, it demonstrates the robustness of our method to noisy data. Our model also performed well in comparison to the LSTM and the other Tree-LSTM baselines. Despite the slower training time due to\nthe additional complexity, Figure 2 shows how our model needed fewer training examples to reach convergence in this task.\nFollowing Yogatama et al. (2016), we also manually inspect the learned trees to see how closely they match conventional syntax trees, as would typically be assigned by trained linguists. We analyse the same four sentences they chose. The trees produced by our model are shown in Figure 3. One notable feature of the trees is the fact that verbs are joined with their subject noun phrases first, which differs from the standard verb phrase structure. Type-raising and composition in formalisms such as combinatory categorial grammar (Steedman, 2000) do however allow such constituents. The spans of prepositional phrases in (b), (c) and (d) are correctly identified at the highest level; but only in (d) does the structure of the subtree match convention. As could be expected, other features such as the attachment of the full stops or of some determiners do not appear to match human intuition."}, {"heading": "6 Conclusions", "text": "We presented a fully differentiable model to jointly learn sentence embeddings and syntax, based on the Tree-LSTM composition function. We demonstrated its benefits over standard Tree-LSTM on a textual entailment task and a reverse dictionary task. The model is conceptually simple, and easy to train via backpropagation and stochastic gradient descent with popular deep learning toolkits based on dynamic computation graphs such as DyNet (Neubig et al., 2017) and PyTorch.3\nThe unsupervised Tree-LSTM we presented is relatively simple, but could be plausibly improved by combining it with aspects of other models. It should be noted in particular that (4), the function assigning an energy to alternative ways of forming constituents, is extremely basic and does not rely on any global information on the sentence. Using a more complex function, perhaps relying on a mechanism such as the tracking LSTM in Bowman et al. (2016), might lead to improvements in performance. Techniques such as batch normalization (Ioffe and Szegedy, 2015) or layer normalization (Ba et al., 2016) might also lead to further improvements.\nIn future work, it might be possible to obtain trees closer to human intuition by training a model to perform well on multiple tasks instead of a single one, an important feature for intelligent agents to demonstrate (Legg and Hutter, 2007). Techniques such as elastic weight consolidation (Kirkpatrick et al., 2017) have been shown to help with multitask learning, and could be readily applied to our model.\n3https://github.com/pytorch/pytorch"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "CoRR, abs/1508.05326,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Programming Languages and Their Compilers: Preliminary Notes", "author": ["John Cocke"], "venue": "Courant Institute of Mathematical Sciences,", "citeRegEx": "Cocke.,? \\Q1969\\E", "shortCiteRegEx": "Cocke.", "year": 1969}, {"title": "Mathematical foundations for a compositional distributed model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "editors, ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "An efficient recognition and syntax analysis algorithm for context-free languages", "author": ["T. Kasami"], "venue": "Technical Report AFCRL-65-758,", "citeRegEx": "Kasami.,? \\Q1965\\E", "shortCiteRegEx": "Kasami.", "year": 1965}, {"title": "Structured Attention Networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush"], "venue": "In ICLR", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Easy-first dependency parsing with hierarchical tree lstms", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "TACL, 4:445\u2013461,", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska", "Demis Hassabis", "Claudia Clopath", "Dharshan Kumaran", "Raia Hadsell"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kirkpatrick et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 2017}, {"title": "The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization", "author": ["Phong Le", "Willem Zuidema"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Le and Zuidema.,? \\Q2015\\E", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "URL http://www. aclweb.org/anthology/P/P14/P14-5010", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural semantic encoders. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 397\u2013407", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": null, "citeRegEx": "Munkhdalai and Yu.,? \\Q2017\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2017}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reading and thinking: Re-read LSTM unit for textual entailment recognition", "author": ["Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li"], "venue": "COLING", "citeRegEx": "Sha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The Syntactic Process", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "Steedman.,? \\Q2000\\E", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "INTERSPEECH", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Learning to compose words into sentences with reinforcement learning. 2016", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "venue": "URL http://arxiv.org/ abs/1611.09100", "citeRegEx": "Yogatama et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 1967}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent neural networks, in particular the Long Short-Term Memory (LSTM) architecture (Hochreiter and Schmidhuber, 1997) and some of its variants (Bahdanau et al.", "startOffset": 88, "endOffset": 122}, {"referenceID": 32, "context": "These can be either provided by an automatic parser (Tai et al., 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016).", "startOffset": 52, "endOffset": 70}, {"referenceID": 16, "context": ", 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016).", "startOffset": 74, "endOffset": 106}, {"referenceID": 16, "context": ", 2015), or taken from a gold-standard resource such as the Penn Treebank (Kiperwasser and Goldberg, 2016). Yogatama et al. (2016) proposed to remove this requirement by including a shift-reduce parser in the model, to be optimised alongside the composition function based on a downstream task.", "startOffset": 75, "endOffset": 131}, {"referenceID": 33, "context": "Our model is also unsupervised with respect to the parse trees, similar to Yogatama et al. (2016). We show that the proposed method outperforms other Tree-LSTM architectures based on fully left-branching, right-branching, and supervised parse trees on a textual entailment task and a reverse dictionary task.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011).", "startOffset": 177, "endOffset": 198}, {"referenceID": 23, "context": "In the first group, Paperno et al. (2014) take inspiration from the standard Montagovian semantic treatment of composition.", "startOffset": 20, "endOffset": 42}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011). These tensors are trained via linear regression based on a downstream task, but the tree that determines their order of application is expected to be provided as input. Socher et al. (2012) and Socher et al.", "startOffset": 178, "endOffset": 390}, {"referenceID": 4, "context": "They model nouns as vectors, and relational words that take arguments (such as adjectives, that combine with nouns) as tensors, with tensor contraction representing application (Coecke et al., 2011). These tensors are trained via linear regression based on a downstream task, but the tree that determines their order of application is expected to be provided as input. Socher et al. (2012) and Socher et al. (2013) also rely on external trees, but use recursive neural networks as the composition function.", "startOffset": 178, "endOffset": 415}, {"referenceID": 17, "context": "Instead of using a single parse tree, Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty.", "startOffset": 38, "endOffset": 60}, {"referenceID": 17, "context": "Instead of using a single parse tree, Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty. The authors use a convolutional neural network composition function and, like our model, rely on a mechanism similar to the one employed by the CYK parser to process the trees. Ma et al. (2015) propose a related model, also making use of syntactic information and convolutional networks to obtain a representation in a bottom-up manner.", "startOffset": 38, "endOffset": 365}, {"referenceID": 13, "context": "Convolutional neural networks can also be used to produce embeddings without the use of tree structures, such as in Kalchbrenner et al. (2014).", "startOffset": 116, "endOffset": 143}, {"referenceID": 30, "context": "In the second group, Yogatama et al. (2016) shows the most similarities to our proposed model.", "startOffset": 21, "endOffset": 44}, {"referenceID": 1, "context": "The authors use reinforcement learning to learn tree structures for a neural network model similar to Bowman et al. (2016), taking performance on a downstream task that uses the computed sentence representations as the reward signal.", "startOffset": 102, "endOffset": 123}, {"referenceID": 1, "context": "The authors use reinforcement learning to learn tree structures for a neural network model similar to Bowman et al. (2016), taking performance on a downstream task that uses the computed sentence representations as the reward signal. Kim et al. (2017) take a slightly different approach: they formalise a dependency parser as a graphical model, viewed as an extension to attention mechanisms, and hand-optimise the backpropagation step through the inference algorithm.", "startOffset": 102, "endOffset": 252}, {"referenceID": 9, "context": "An obvious choice for a baseline is the popular Long Short-Term Memory (LSTM) architecture of Hochreiter and Schmidhuber (1997). It is a recurrent neural network that, given a sentence encoded by embeddings w1, .", "startOffset": 94, "endOffset": 128}, {"referenceID": 11, "context": "Following the recommendation of Jozefowicz et al. (2015), we deviate slightly from the vanilla LSTM architecture described above by also adding a bias of 1 to the forget gate, which was found to improve performance.", "startOffset": 32, "endOffset": 57}, {"referenceID": 5, "context": "For training we use stochastic gradient descent with a batch size of 16, which was found to perform better than AdaGrad (Duchi et al., 2010) and similar methods on our development data.", "startOffset": 120, "endOffset": 140}, {"referenceID": 1, "context": "We test our model and baselines on the Stanford Natural Language Inference task (Bowman et al., 2015), consisting of 570 k manually annotated pairs of sentences.", "startOffset": 80, "endOffset": 101}, {"referenceID": 25, "context": "For this experiment, we choose 100D input embeddings, initialised with 100D GloVe vectors (Pennington et al., 2014) and with out-of-vocabulary words set to the average of all other vectors.", "startOffset": 90, "endOffset": 115}, {"referenceID": 23, "context": "For this experiment, we choose 100D input embeddings, initialised with 100D GloVe vectors (Pennington et al., 2014) and with out-of-vocabulary words set to the average of all other vectors. This results in a 100\u00d7 37 369 word embedding matrix, fine-tuned during training. For the supervised Tree-LSTM model, we used the parse trees included in the dataset. Given a pair of sentences, one of the models is used to produce the embeddings s1, s2 \u2208 R. Following Yogatama et al. (2016) and Bowman et al.", "startOffset": 91, "endOffset": 480}, {"referenceID": 1, "context": "(2016) and Bowman et al. (2016), we then compute u = (s1 \u2212 s2), v = s1 s2,", "startOffset": 11, "endOffset": 32}, {"referenceID": 8, "context": "We also test our model and baselines on the reverse dictionary task of Hill et al. (2016), which consists of 852 k word-definition pairs.", "startOffset": 71, "endOffset": 90}, {"referenceID": 22, "context": "As output embeddings, we use the 500D CBOW vectors (Mikolov et al., 2013) provided by the authors.", "startOffset": 51, "endOffset": 73}, {"referenceID": 21, "context": "For the supervised Tree-LSTM model, we additionally parsed the definitions with Stanford CoreNLP (Manning et al., 2014) to obtain parse trees.", "startOffset": 97, "endOffset": 119}, {"referenceID": 8, "context": "Table 3 shows the results for our model and baselines, as well as the models of Hill et al. (2016) which are based on the same cosine training objective.", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "In the reversed dictionary task, the very poor performance of the supervised Tree-LSTM can be explained by the unusual tokenisation algorithm used in the dataset of Hill et al. (2016): all punctuation is simply stripped, turning for instance \u201c(archaic) a section of a poem\u201d into \u201carchaic a section of a poem\u201d, or stripping away the semicolons in long lists of synonyms.", "startOffset": 165, "endOffset": 184}, {"referenceID": 29, "context": "Type-raising and composition in formalisms such as combinatory categorial grammar (Steedman, 2000) do however allow such constituents.", "startOffset": 82, "endOffset": 98}, {"referenceID": 32, "context": "Following Yogatama et al. (2016), we also manually inspect the learned trees to see how closely they match conventional syntax trees, as would typically be assigned by trained linguists.", "startOffset": 10, "endOffset": 33}, {"referenceID": 10, "context": "Techniques such as batch normalization (Ioffe and Szegedy, 2015) or layer normalization (Ba et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 1, "context": "Using a more complex function, perhaps relying on a mechanism such as the tracking LSTM in Bowman et al. (2016), might lead to improvements in performance.", "startOffset": 91, "endOffset": 112}, {"referenceID": 19, "context": "In future work, it might be possible to obtain trees closer to human intuition by training a model to perform well on multiple tasks instead of a single one, an important feature for intelligent agents to demonstrate (Legg and Hutter, 2007).", "startOffset": 217, "endOffset": 240}, {"referenceID": 17, "context": "Techniques such as elastic weight consolidation (Kirkpatrick et al., 2017) have been shown to help with multitask learning, and could be readily applied to our model.", "startOffset": 48, "endOffset": 74}], "year": 2017, "abstractText": "We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.", "creator": "LaTeX with hyperref package"}}}