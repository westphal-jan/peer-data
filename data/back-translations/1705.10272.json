{"id": "1705.10272", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Who's to say what's funny? A computer using Language Models and Deep Learning, That's Who!", "abstract": "Humour is a characteristic feature of human beings. Our goal is to develop methods that automatically recognize humorous statements and classify them on a continuous scale. In this article, we report on results using a language model approach and outline our plans for using methods from deep learning.", "histories": [["v1", "Mon, 29 May 2017 16:20:21 GMT  (12kb)", "http://arxiv.org/abs/1705.10272v1", "3 pages, Appears in the Proceedings of the Women and Underrepresented Minorities in Natural Language Processing Workshop (WiNLP-2017), July 30, 2017, Vancouver, BC"]], "COMMENTS": "3 pages, Appears in the Proceedings of the Women and Underrepresented Minorities in Natural Language Processing Workshop (WiNLP-2017), July 30, 2017, Vancouver, BC", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinru yan", "ted pedersen"], "accepted": false, "id": "1705.10272"}, "pdf": {"name": "1705.10272.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yanxx418@d.umn.edu", "tpederse@d.umn.edu", "@midnight", "@midnight"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 27\n2v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n01 7\nHumor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning."}, {"heading": "1 Introduction", "text": "Computational humor is an emerging area of research that ties together ideas from psychology, linguistics, and cognitive science. Humor generation is the problem of automatically creating humorous statements (e.g., (Stock and Strapparava, 2003), (O\u0308zbal and Strapparava, 2012)). Humor detection seeks to identify humor in text, and is sometimes cast as a binary classification problem that decides if some input is humorous or not (e.g., (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)). However, our focus is on the continuous and subjective aspects of humor.\nWe learn a particular sense of humor from a data set of tweets which are geared towards a certain style of humor (Potash et al., 2016). This data consists of humorous tweets which have been submitted in response to hashtag prompts provided during the Comedy Central TV show @midnight with Chris Hardwick. Since not all jokes are equally funny, we use Language Models and methods from Deep Learning to allow potentially humorous statements to be ranked relative to each other."}, {"heading": "2 Language Models", "text": "We used traditional Ngram language models as our first approach for two reasons : First, Ngram language models can learn a certain style of humor by using examples of that as the training data for the model. Second, they assign a probability to each input they are given, making it possible to rank statements relative to each other. Thus, Ngram language models make relative rankings of humorous statements based on a particular style of humor, thereby accounting for the continuous and subjective nature of humor.\nWe began this research by participating in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor (Potash et al., 2017). This included two subtasks : Pairwise Comparison (Subtask A) and Semi-ranking (Subtask B). Pairwise comparison asks a system to choose the funnier of two tweets. Semi-ranking requires that each of the tweets associated with a particular hashtag be assigned to one of the following categories : top most funny tweet, next nine most funny tweets, and all remaining tweets.\nOur system estimated tweet probabilities using Ngram language models. We created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research1. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus. We took a standard approach to language modeling and used bi-\n1http://www.statmt.org/wmt11/featured-translationtask.html\ngrams and trigrams as features in our models. We used KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.\nTable 1 shows our results for both data sets when trained on bigrams and trigrams. The accuracy and distance measures are defined by the task organizers (Potash et al., 2017). We seek high accuracy in picking the funnier tweet (Subtask A) and low distance (from the gold standard) in organizing the tweets into categories (Subtask B).\nThese results show that models trained on the news data have a significant advantage over the tweets model, and that bigram models performed slightly better than trigrams. We submitted trigram models trained on news and tweets to the official evaluation of SemEval-2017 Task 6. The trigram language models trained on the news data placed fourth in Subtask A and first in Subtask B.\nWe believe that the significant advantage of the news data over the tweet data is caused by the much larger quantity of news data available. The tweet data only consists of approximately 21,000 tweets, whereas the news data totals approximately 6.2 GB of text. In the future we intend to collect more tweet data, especially those participating in the ongoing #HashtagWars staged nightly by@midnight. We also plan to experiment with equal amounts of tweet data and news data, to see if one has an inherent advantage over the other.\nOur language models performed better in the pairwise comparison, but it is clear that more investigation is needed to improve the semi-ranking results. We believe that Deep Learning may overcome some of the limits of Ngram language models, and so will explore those next."}, {"heading": "3 Deep Learning", "text": "One limitation of our language model approach is the large number of out of vocabulary words we encounter. This problem can not be solved by increasing the quantity of training data because humor relies on creative use of language. For\nexample, jokes often include puns based on invented words, e.g., a singing cat makes beautiful meowsic. (Potash et al., 2016) suggests that character\u2013based Convolutional Neural Networks (CNNs) are an effective solution for these situations since they are not dependent on observing tokens in training data. Previous work has also shown the CNNs are effective tools for language modeling, even in the presence of complex morphology (Kim et al., 2015). Other recent work has shown that Recurrent Neural Networks (RNNs), in particular Long Short\u2013Term Memory networks (LSTMs), are effective in a wide range of language modeling tasks (e.g., (Sundermeyer et al., 2012),(Sundermeyer et al., 2015)). This seems to be due to their ability to capture long distance dependencies, which is something that Ngram language models can not do.\n(Potash et al., 2016) finds that external knowledge is necessary to detect humor in tweet based data. This might include information about book and movie titles, song lyrics, biographies of celebrities etc. and is necessary given the reliance on current events and popular culture in making certain kinds of jokes.\nWe believe that Deep Learning techniques potentially offer improved handling of unknown words, long distance dependencies in text, and non\u2013linear relationships among words and concepts. Moving forward we intend to explore a variety of these ideas and describe those briefly below."}, {"heading": "4 Future Work", "text": "Our current language model approach is effective but does not account for out of vocabulary words nor long distance dependencies. CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., (Bertero and Fung, 2016)) which we will explore and compare to our existing results.\nAfter evaluating CNNs and LSTMs we will explore how to include domain knowledge in these models. One possibility is to create word embeddings from domain specific materials and provide those to the CNNs along with more general text. Another is to investigate the use of Tree\u2013 Structured LSTMs (Tai et al., 2015). These have the potential advantage of preserving non-linear structure in text, which may be helpful in recognizing some of the unusual variations of words and concepts that are characteristic of humor."}], "references": [{"title": "A long shortterm memory framework for predicting humor in dialogues", "author": ["Dario Bertero", "Pascale Fung."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Bertero and Fung.,? 2016", "shortCiteRegEx": "Bertero and Fung.", "year": 2016}, {"title": "Scalable modified Kneser-Ney languagemodel estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria,", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615 .", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Learning to laugh (automatically): Computational models for humor recognition", "author": ["Rada Mihalcea", "Carlo Strapparava."], "venue": "Computational Intelligence 22(2):126\u2013142.", "citeRegEx": "Mihalcea and Strapparava.,? 2006", "shortCiteRegEx": "Mihalcea and Strapparava.", "year": 2006}, {"title": "Automatic disambiguation of english puns", "author": ["Tristan Miller", "Iryna Gurevych."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "Miller and Gurevych.,? 2015", "shortCiteRegEx": "Miller and Gurevych.", "year": 2015}, {"title": "A computational approach to the automation of creative naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Com-", "citeRegEx": "\u00d6zbal and Strapparava.,? 2012", "shortCiteRegEx": "\u00d6zbal and Strapparava.", "year": 2012}, {"title": "HashtagWars: Learning a sense of humor", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "arXiv preprint arXiv:1612.03216 .", "citeRegEx": "Potash et al\\.,? 2016", "shortCiteRegEx": "Potash et al\\.", "year": 2016}, {"title": "SemEval-2017 Task 6: #HashtagWars: learning a sense of humor", "author": ["Peter Potash", "Alexey Romanov", "Anna Rumshisky."], "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Vancouver, BC.", "citeRegEx": "Potash et al\\.,? 2017", "shortCiteRegEx": "Potash et al\\.", "year": 2017}, {"title": "Inside jokes: Identifying humorous cartoon captions", "author": ["Dafna Shahaf", "Eric Horvitz", "Robert Mankoff."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, NewYork, NY, USA, KDD", "citeRegEx": "Shahaf et al\\.,? 2015", "shortCiteRegEx": "Shahaf et al\\.", "year": 2015}, {"title": "Getting serious about the development of computational humor", "author": ["Oliviero Stock", "Carlo Strapparava."], "venue": "Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence. Acapulco, pages 59\u201364.", "citeRegEx": "Stock and Strapparava.,? 2003", "shortCiteRegEx": "Stock and Strapparava.", "year": 2003}, {"title": "From feedforward to recurrent lstm neural", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Interspeech. pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Recognizing humor on twitter", "author": ["Renxian Zhang", "Naishi Liu."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and KnowledgeManagement. ACM, New York, NY, USA, CIKM \u201914, pages 889\u2013898.", "citeRegEx": "Zhang and Liu.,? 2014", "shortCiteRegEx": "Zhang and Liu.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 2, "endOffset": 31}, {"referenceID": 5, "context": ", (Stock and Strapparava, 2003), (\u00d6zbal and Strapparava, 2012)).", "startOffset": 33, "endOffset": 62}, {"referenceID": 3, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 2, "endOffset": 34}, {"referenceID": 13, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 8, "context": ", (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": ", 2015), (Miller and Gurevych, 2015)).", "startOffset": 9, "endOffset": 36}, {"referenceID": 6, "context": "We learn a particular sense of humor from a data set of tweets which are geared towards a certain style of humor (Potash et al., 2016).", "startOffset": 113, "endOffset": 134}, {"referenceID": 7, "context": "We began this research by participating in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor (Potash et al., 2017).", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "We used KenLM (Heafield et al., 2013) with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.", "startOffset": 14, "endOffset": 37}, {"referenceID": 7, "context": "The accuracy and distance measures are defined by the task organizers (Potash et al., 2017).", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "(Potash et al., 2016) suggests that character\u2013based Convolutional Neural Networks (CNNs) are an effective solution for these situations since they are not dependent on observing tokens in training data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Previous work has also shown the CNNs are effective tools for language modeling, even in the presence of complex morphology (Kim et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 11, "context": ", (Sundermeyer et al., 2012),(Sundermeyer et al.", "startOffset": 2, "endOffset": 28}, {"referenceID": 10, "context": ", 2012),(Sundermeyer et al., 2015)).", "startOffset": 8, "endOffset": 34}, {"referenceID": 6, "context": "(Potash et al., 2016) finds that external knowledge is necessary to detect humor in tweet based data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": ", (Bertero and Fung, 2016)) which we will explore and compare to our existing results.", "startOffset": 2, "endOffset": 26}, {"referenceID": 12, "context": "Another is to investigate the use of Tree\u2013 Structured LSTMs (Tai et al., 2015).", "startOffset": 60, "endOffset": 78}], "year": 2017, "abstractText": "Humor is a defining characteristic of human beings. Our goal is to develop methods that automatically detect humorous statements and rank them on a continuous scale. In this paper we report on results using a Language Model approach, and outline our plans for using methods from Deep Learning.", "creator": "LaTeX with hyperref package"}}}