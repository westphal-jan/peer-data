{"id": "1706.00005", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Machine Learning Based Crackle Detection in Lung Sounds", "abstract": "The stethoscope is a well-known and widely available diagnostic tool. In recent years, many innovative solutions for recording and viewing sounds from a stethoscope have become available. However, in order to make full use of such devices, an automated approach to detecting abnormal lung sounds is needed, which is better than the existing methods, which are typically developed and evaluated with a small and non-diverse dataset.", "histories": [["v1", "Wed, 31 May 2017 16:24:28 GMT  (1365kb)", "http://arxiv.org/abs/1706.00005v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["morten gr{\\o}nnesby", "juan carlos aviles solis", "einar holsb{\\o}", "hasse melbye", "lars ailo bongo"], "accepted": false, "id": "1706.00005"}, "pdf": {"name": "1706.00005.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Morten Gr\u00f8nnesby", "Juan Carlos Aviles Solis", "Einar Holsb\u00f8", "Hasse Melbye", "Lars Ailo Bongo"], "emails": ["larsab@cs.uit.no;"], "sections": [{"heading": "1 Introduction", "text": "The stethoscope is a well-known and widely available diagnostic instrument. Health care personnel routinely use it to listen for abnormal sounds in the lungs to establish a diagnosis. Even though lung auscultation is a very old technique, recent technological advances in the\nfields of hardware, acoustics, and digital sound analysis and classification gives new possibilities that should be further explored. This is reflected by the many new commercial solutions for recording and viewing sounds from a stethoscope, such as the Bluetooth solution for smartphones from Eko Devices (https://ekodevices.com/), and the MIT mobile stethoscope [1]. However, these solutions do not have automated approaches for detecting abnormal sounds in lung sounds, that can easily be integrated with smart devices and phones. Crackles are short, explosive nonmusical sounds heard mostly during inspiration [2]. These sounds are present in lung and heart related diseases like chronic obstructive pulmonary disease (COPD), pneumonia, heart failure, and asbestosis. In these diseases, the presence of crackles helps to stablish a diagnosis [3]\u2013[5]. These diseases represent a major public health problem. In 2012, over 3 million deaths were caused by COPD which represented a 6% of the total deaths in that year [6]. Recent reports [7] state that 23 million people worldwide have a diagnosis of heart failure. The use of lung auscultation in the diagnosis and treatment of disease has been questioned lately (http://www.npr.org/sections/health-shots/2016/02/26/467212821/the-stethoscope-timelesstool-or-outdated-relic, http://www.telegraph.co.uk/news/health/news/10592653/Stethoscopeson-their-way-out.html). This, due to concerns about the subjectivity of the technique and the introduction of new diagnostic technologies like ultrasound and CT-scans. However, the use of lung sounds in the investigation of disease has advantages in terms of costs and availability. In addition, lung sounds have the ability to reflect rapid changes, and are therefore useful in evaluating treatment responses, home monitoring, and maybe predicting exacerbations of disease [8]\u2013[10]. Detecting crackles in lung auscultation is challenging for three reasons. First, the (crackle) signal to noise ratio is low in a sound file since crackles have a short, 5-40ms, duration. Second, other sounds are very similar, such as the stethoscope touching clothing or chest hair. Third, crackles may be present even in healthy people. Current approaches for automatic detection of crackles in lung sounds have shown promise and they have achieved high specificity and sensitivity for test data ([11] provides a review, CORSA [12] recommends standard for terms and techniques). Most are rule based [13], [14], and hence detect crackles using a set of predefined parameters that have been extracted from a small set of sample audio files using signal processing techniques. Recently several machine learning based approaches, based on for example SVMs and Neural Networks [15]\u2013[19] have been introduced. These have the advantage over rule based methods that the classification rules are automatically learned from the dataset. However, these have also been trained using small datasets, often with cases selected among patients with known lung diseases. Machine learning based classification such as automatic speech recognition [20] and automatic acoustic event detection [21] is a very active research field with widely used solutions. However, such generic approaches are not well suited to detect abnormal sounds such as crackles, since these are considered as noise to be ignored by these applications. Here we describe a novel machine learning based approach for automatic crackle classification in lung sounds. To train and evaluate our classifier, we selected 209 files classified by expert physicians to contain crackles from a large reference database with 36054 sounds recorded for 6009 people as part of a large health survey (Troms\u00f8unders\u00f8kelsen 7, https://uit.no/forskning/forskningsgrupper/sub?sub_id=503778&p_document_id=367276). Our analysis pipeline is based on features extracted from small windows in these files. The approach is cheap, easy, and convenient to use. It only requires a stethoscope with a microphone for recording sounds, and the sounds can be recorded in a clinical setting with background noise. The sounds are uploaded for our server for analysis. We present the results in a web\napplication, using a visualization that shows the detected crackles in a sound recording, and associated confidence scores for the crackles. The visualization is portable and the results are displayed using web technology so they can be viewed on mobile phones, tablets, and PCs. The approach can be used for training of medical doctors, or as part of a smartphone application for Bluetooth stethoscopes."}, {"heading": "2 Methods", "text": "Our approach provides automatic crackle detection and annotation in a user environment that is readily accessible to health personnel and other users. A user records sounds using a stethoscope with a microphone, and then uploads these to our server for classification and annotation. The results are presented in a web application that provides an interactive visualization for the end users. In addition, the results are exported as Excel or comma separated tabular text files. We use supervised learning to classify sounds as either crackles or normal (not-crackles). Such a machine learning method can classify big datasets without intervention from a human expert. Our supervised approach requires prior knowledge in the form of a pre-classified training set with crackles and normal sounds. We select crackle specific features using sounds from a large reference database with expert classified lung sound recordings. The patterns in these are then summarized as a feature set. The features are used to train a model used to classify sounds as either normal or crackles. There are several methods for feature extraction, and several classifiers with associated learning methods. In addition, an important challenge when developing a classification approach is to find and optimize the methods that works best for a dataset. In this paper, we present and evaluate several feature extraction methods and classifiers."}, {"heading": "2.1 Data acquisition", "text": "We used a sample of sound files from adults participating in the Troms\u00f8 7 study. The Troms\u00f8 study is an epidemiological prospective study of health conditions and chronic diseases. To investigate the validity of pulmonary auscultation as a diagnostic method, we recorded lung sounds using an electret microphone (MKE 2-eW Gold, Sennheiser electronic GmbH & Co. KG) inserted at the tube of a stethoscope, 10 cm away from the chest piece. The microphone was tuned to a sensitivity of -12 dB to reduce crackle like artifacts. For the first 300 persons, we used a cardiology stethoscope (Littman Cardiology II, 3M corporation) and for the rest of the participants we used a different model (Littman Classic II SE, 3M corporation). The reason to change stethoscope was a better performance in terms of reduced low frequency noise. The sound files were captured in Wave (.wav) format at 44.100 Hz sampling rate. We have not processed the WAV files after recording. We asked the patients to breathe in and out with an open mouth and deeper than normal. We recorded in six different locations in the thorax for a period of 15 seconds in each case. In total, we recorded 36054 sounds from 6009 persons."}, {"heading": "2.2 Expert classified reference database", "text": "We have started creating a reference database for lung sounds. The recordings are classified at two levels. First, two observers independently classified each recording using Adobe Audition 5.0 to listen to the lungs sounds and inspect spectrogram visualizations (Figure 1). The classification scheme had the following variables:\n1. Abnormal sound 2. Inspiratory wheeze 3. Expiratory wheeze 4. Inspiratory crackle 5. Expiratory crackle 6. Other abnormal sound 7. Not classifiable\nIf there were any disagreements at the first step, the recordings were discussed in a meeting between the 2 observers and a third expert on lung sounds. After a discussion, the final decision agreed on, in a few was cases after voting. This dataset is multi label, so a sound file can contain both crackles and wheezes. At the time of writing we have classified 8784 files, of which 333 have crackles (3.8%)."}, {"heading": "2.3 Manually created training sets with crackle and normal windows", "text": "To train the classifier we used the first 209 crackle files that were classified as either inspiratory or expiratory crackles. The sounds already classified as containing crackles were plotted as waveforms using Adobe Audition 5.0, and then one of the authors visually identified crackles in the waveform and latter verified these by listening to the selected part of the file. Even though the actual windows were not verified by other experts the whole recording was evaluated and validated by at least two experts, so we believe most of these represents actual crackles. For each crackle, we record the approximate start and end time. In total, we used 175 crackles as our training set. We also randomly selected 208 parts that do not contain crackles from the same 209 crackles files. These parts represents normal sounds."}, {"heading": "2.4 Preprocessing: split file into windows", "text": "We first split the files into 92ms windows (Figure 2). Each 92ms window contains 4096 samples, with 50% overlap between windows. The overlap ensures that a crackle is not split between two windows. The window size allows tracing abnormal sounds back to their location in time, with acceptable accuracy. Dividing the audio into windows also limits the amount of\ndata that is analyzed at a time, which makes it easier for a machine learning algorithm to find patterns. Finally, using a fixed window as a data point ensures that a data point is not misclassified due to a lack of standardized data length and shape. The 175 windows with crackles, and the 208 normal windows, are stored as one-dimensional arrays that contains 4096 32-bit floating point numbers. The windowing is done at analysis time, so the window size and overlap can be changed without (manually) generating a new training sets. We considered using a Butterworth bandpass filter [22] to remove frequencies above 2400 Hz and below 50 Hz since these do not contain crackle sounds. However, the Butterworth filter did not improve our results, and it did not significantly improve execution time. We therefore do not filter or do any other transformation of the sound in the windows."}, {"heading": "2.5 Preprocessing: feature selection", "text": "To select the relevant features to build a model of crackles we evaluated several approaches. Our goal is to find a feature set that provides both high precision and high recall. The latter requires ignoring background noises and other additive noise such as tubing of the stethoscope that sound like crackles. We describe and evaluate in detail the best approach; a 5-dimensional vector of time domain and frequency domain features. Two, more complex, approaches are discussed in Section 4.1."}, {"heading": "2.5.1 5-dimensional feature vector", "text": "We achieved the best results for a 5-dimenstional vector with four features from the time domain {variance, range, sum of simple moving average (coarse), sum of simple moving average (fine)} and one feature from the frequency domain {spectrum mean}. These are scaled to standardize each feature category across training observations. The advantage of using\nsimple summary statistic features is that they are easy to relate to the actual data. The disadvantage is that a lot of information is lost using simple features. We believe the time domain features work well with crackles due to their short-lasting explosive nature. All time domain features are calculated for the 92ms windows. Variance is a measure of the spread of a distribution. It is the average of the squared deviations from the mean. Crackle windows have higher variance than normal windows due to their explosive nature, and squared errors are naturally sensitive to outliers. Normal windows may vary more in terms of zero crossing rate, but the spread is higher for crackles as they usually contain more power, or have a higher amplitude, than normal breathing. The range of an audio window is the maximum value subtracted from the minimum value. Since crackles have an explosive popping noise, we believe the range of crackle windows will be wider than normal breathing. This feature is highly dependent on feature scaling, as it is highly sensitive to noise and other artefacts that may cause sudden high amplitudes in the audio. The Simple Moving Average (SMA) gives an indication of how much the signal is changing over the course of time. We have used two different granularity levels of this feature. The coarse version calculates one sum for all values in an observation:\n\u2211 | | The fine version first calculates sums for a range of windows along the signal and then selects the window with the highest amount of change:\n, \u2026 The frequency domain feature is calculated on frequency spectrum magnitudes obtained from the windows by first calculating either, a Fast Fourier Transform and then only keeping the real parts of the coefficients (absolute values), or by using a Short Time Fourier Transform to calculate the spectrogram of the window. The spectrum mean gives us an indication of the central tendency in the frequency domain. Crackles that occur in breathing often carry more power in higher frequencies. The center of the power distribution would naturally have a higher value for any windows containing crackles, though we have observed that this is a tendency rather than a rule. Feature Scaling is necessary since audio data is non-stationary and fluctuating, so each recording might have a slightly different sound, gain, and noise. This ameliorates the effects of outliers and divergences between observations, and all features are in a standard scale compared with other observations. This is especially important with distance-based classifiers where scale is important."}, {"heading": "2.6 Classifiers", "text": "We evaluated three classifiers for the 5-dimensional feature vector: SVM [23], KNN [24], and AdaBoost [25] (Decision Trees)."}, {"heading": "2.6.1 Support Vector Machines (SVM)", "text": "Support Vector Machines (SVMs) are popular and widely used classifiers, and an SVM also performed best on our features. The SVM separate our two classes, crackles and normal windows, using a hyperplane that maximize separation between observations of the two classes. The shape of the hyperplane is determined by a kernel function. We achieved the highest classification accuracy using the Radial Basis Function Kernel. We find the positive constant C, which controls the influence or cost of misclassification, using Grid Search that fit different values for C to different classifiers and then selects the highest scoring classifier."}, {"heading": "2.6.2 K-Nearest Neighbor (KNN)", "text": "The K-nearest Neighbors (KNN) method is a non-parametric, lazy method, that does not make any assumptions about the structure of the underlying data, and it does not require a training step. Class membership of an unseen data point is determined by the k closest training observations in the feature space. We select k using grid search. We found that a small k (between 2-4) gives distinct boundaries between two classes that have, as in our case, small margins. We found that Euclidian distance performed best for our data. We also evaluated dynamic time warping since it can work better for signals that differ in time and speed, such as crackles. However, for our summary 5-dimensional feature vectors dynamic time warping does not perform better than Euclidian distance. In addition, it significantly increases the classification time."}, {"heading": "2.6.3 Adaptive Boosting and Decision Trees", "text": "Adaptive Boosting, or AdaBoost, is a meta-classifier that uses a collection of classifiers of the same type. The individual classifiers do not need to perform exceedingly well, if their prediction is better than random guess (error rate smaller than 0.5). The idea is to iteratively train classifiers that focus on the observations where previous classifiers went wrong by weighting these misclassified observations more heavily than the correctly classified observations. The ouput of the algorithm is a weighted sum of all the classifiers. Each classifier is weighted based on its error rate."}, {"heading": "2.7 Server implementation", "text": "The server is implemented in Python 2.7 using Scikit Learn [26]. We use Python and Sklearn due to its flexibility and ease of use. The server is portable across different operating systems. Our pipeline is single-threaded. Due to the low execution time of both training and classification, we did not use optimized libraries or parallel execution."}, {"heading": "2.8 Evaluation methodology", "text": "Each of the features was tested by running a train-validate cycle 100 times, and then averaging the F1-score across all cycles. The train-validate split is done on the training set, consisting of 175 crackle windows and 208 normal windows. Each cycle splits the training set into 70% training, which is used for training and grid search parameter tuning, and 30% validation. We report the precision (positive predictive value), recall (true positive rate), and F-1 score (harmonic mean between the two preceding measurements). To measure the performance of our server we ran our server using cProfiler for Python 2.7.9 on Windows 10 Pro 64-bit, on a machine with Intel Core-i5-4570s, with four 2.90GHz cores and 6GB of DRAM."}, {"heading": "3 Results", "text": "Our evaluation results provide answers to the following three questions:\n1. How well do each of the five features individually separate between crackles and normal windows? 2. Which classifier works best for our feature vectors? 3. What is the speed-performance of our server during training and classification?"}, {"heading": "3.1 Feature selection", "text": "The univariate feature scores are between 60-70% (Figure 3). A single feature is therefore not good enough, but it is better than random guess. It should therefore be possible to combine these features to get a better separation in a higher dimensional space. The scatter matrix in Figure 4 shows the separation between normal and crackle classes. While there is separation between classes, there is also overlap. This is also reflected in the classification results (Linear SVM in Table 1), where we get high precision, but low recall due to the overlap of features, which are not linearly separable."}, {"heading": "3.2 Classifiers", "text": "SVM performs best in the cross-validation cycle results using all features (Table 1). We did a grid search for each cycle and found that a radial basis function kernel with a C parameter between 1000 and 2000 performed best. All classifiers performed better than a dummy classifier that used a stratified sampling strategy, which means that the dummy classifier chooses classes proportionally to the size of the two classes."}, {"heading": "3.3 Server speed-performance", "text": "We trained the classifier in 1.44 seconds, including sequential grid search of 64 SVM parameter combinations (192 fits). Using the trained model, we classified 319 windows in 1.08 seconds. The model can therefore be used to classify crackles in an interactive tool."}, {"heading": "4 Discussion", "text": "We found that a simple 5-dimensional feature worked best with an SVM classifier. We have also evaluated other feature extraction methods, including classifiers for these [27], [28]. The accuracy and reliability of our system is better than found in studies of health personnel [29]\u2013[32] (although health personnel have higher accuracy than reported in [29]\u2013[32] using our data (unpublished)). In future work, we plan to compare the performance of our system against the classifications of human experts using our full (large) data set."}, {"heading": "4.1 Negative results: alternative feature extraction methods and classifiers", "text": "In earlier work [27] we evaluated two alternative feature extraction methods for our data. But these did not perform as well as the simpler 5-dimensional feature described and evaluated above."}, {"heading": "4.1.1 Short-Time Fourier Transformation (STFT)", "text": "We tried using the STFT spectrogram directly in an SVM classifier, but many dimensions remain after a STFT. Therefore, we only use STFT as a preprocessing step to calculate the spectrum mean of a window. One of the drawbacks of the STFT is the fixed window size, which means that there is a tradeoff between either good time resolution or good frequency resolution."}, {"heading": "4.1.2 Discrete Wavelet Transform", "text": "Discrete Wavelet decomposition (DWT) is an alternative to STFT. While STFT uses fixed sized windows, the discrete wavelet transform vary the window sizes based on frequency. Higher frequencies have smaller windows, while lower frequencies have larger windows. This gives higher frequencies a better time resolution and lower frequencies a better frequency resolution. For crackles, location and duration can be an important factor, especially in reporting and visualizing results of analyses."}, {"heading": "4.1.3 Spectral Flux", "text": "Spectral flux is a measure for how quickly a signal changes over time. Spectral flux is calculated by comparing a sliding window to the previous window over a normalized waveform. We use the Euclidean Distance between the two windows. While spectral flux can be useful in onset detection, there is very little information retained from a spectral flux measurement. We believe it can be useful as a feature for detecting possible crackle candidates inside a larger audio file, but it is less useful as a feature in crackle classification."}, {"heading": "4.1.4 Mel Frequency Ceptrsal Coefficients", "text": "Mel Frequency Cepstral Coefficients (MFCC) feature extraction is widely used in speech and music recognition [33]. The MFCC greatly reduces the dimensionality of the training data, but it has been designed and used for speech recognition. For crackle detection, the sound of interest is a short, explosive, non-musical sound. The MFCC represents spectral envelope of the signal, which is good for recognizing linguistic characteristics, but crackles do not adhere to these characteristics."}, {"heading": "4.1.5 Spectrogram Image Analysis", "text": "An alternative approach for crackle detection is to convert the sound signal into a spectrogram and then use image analysis techniques. In [34], crackles are classified by extracting features from the elliptical pattern of a crackle in a spectrogram. We tried to replicate the results by calculating the spectrogram of a signal using the Short Time Fourier Transform (STFT) and then a histogram equalization to increase the contrast of the spectrogram. Further we used thresholding to normalize each value to either 0 or 1. Even though we could replicate the spectrogram processing techniques, we could not accurately detect the elliptical structure of the crackle present in our spectrograms. We believe this is due our normal sounds having similar elliptical patterns that are not distinguishable from crackles. We did not have the data used in [34], so we could not do a direct comparison, so we could check if this was since we used STFT instead of the bio22 wavelet decomposition used in [34]."}, {"heading": "4.1.6 Summary", "text": "These alternative approaches to feature engineering provided useful classification results, but not as good as the simpler 5-dimenstional vector. We believe important challenges for crackle detection of sounds recorded in a clinical setting (or other settings such as home monitoring) includes a low signal to noise ratio, crackle-like noise artefacts, and irregular loudness. Therefore, preprocessing methods that reduce the influence of noise and better techniques for locating potential abnormal sounds in larger audio files, such as finding potential ~20ms crackles within an audio file of 15sec, are just as important as feature engineering."}, {"heading": "4.2 Improvements for per file and per person crackle detection", "text": "In the above approach and evaluation, we have localized individual crackles in audio files. The methods cannot be applied directly to classify individual files and persons as either normal or having crackles for three reasons. First, our lung sound recordings are about 15 seconds and hence have about 300 windows (92ms, 50% overlap). Our methods do not have the specificity to avoid detecting at least one false positive among these windows. Second, we did not consider the natural class imbalance. Only about 5% of the recordings in our reference database (and hence population) have crackles. Training and evaluating on a synthetically balanced data set will introduce a higher false positive rate as it leads to a natural overestimation of the prior probability of a crackle. This is basically a tradeoff between false positive rate and sensitivity, as it can be hard to increase the one without also increasing the other in imbalanced data. Third, even healthy people may have some crackles. We can reduce the false positive rate by improving our methods, or by training these on a larger training set. In addition, we can use higher level information for interpretation of a file. First, the analyst may consider the number\nof crackles and their distribution in a recording. One, or a few, crackles in a recording is typically not of interest. Second, crackles do not occur randomly but in relation to breathing phases. Most crackles happen at an inspiratory phase and sometimes at an expiratory phase. We can therefore eliminate false positives that happen between breathing cycles. Using Parzen Windows (Kernel Density Estimation) [35], [36] to estimate the probability density function of a part of the signal, and then comparing the different estimates to find breathing phases."}, {"heading": "4.3 Applications for our approach", "text": "Health workers routinely listen to lung sounds through stethoscopes during general examinations or when patients indicate respiratory distress. Such lung auscultations are an important method for physicians in decisions on treatment and referral for ultrasound or MR. However, auscultation is a subjective method and improper treatment and referrals accumulate an increased time and monetary cost. Training physicians is a challenging task because of varying perception of sound and lack of common terminology, though the latter have come more into focus for pulmonary experts. Because of these challenges, better tools for training are required and a gold standard of abnormal lung sounds is greatly needed. Training physicians using such tools, would help them to more accurately diagnose and decide a course of treatment and referral. Our approach can be used in training tools to automatically detect and highlight crackles in waveforms and spectrograms (similar to [37]). Similar visualizations are also possible to use in a smart phone application connected to a Bluetooth stethoscope as an aid for clinicians (and other people) using a stethoscope. Finally, a better set of tools for detecting abnormal lung sounds could also be used for self-monitoring of especially at home patients with chronic lung diseases."}, {"heading": "4.4 Conclusion and future work", "text": "We have presented a machine learning based approach for detecting crackles in sounds recorded using a stethoscope as part of a large health survey. We evaluated several feature extraction methods, and classifiers using 209 files from a dataset with 36054 sound recordings. A simple 5-dimenstional vector and a SVM with a Radial Basis Function Kernel performed best. We achieved a precision of 86% and recall of 84% for classifying a crackle in a window, , which is more accurate than found in studies of health personnel. The low-dimensional feature vector makes the SVM very fast. We plan to verify that our methods work well for lung sound recordings collected using other stethoscopes, microphones, and recording environments. We believe the approach is therefore well suited for use in training of medical doctors, and for deployment on smart devices and phones."}, {"heading": "5 Acknowledgments", "text": "Anne H. Davidsen og Raimonda B. Einarsen for help classifying the recorded files, and Hans Pasterkamp for help in classifying difficult cases."}, {"heading": "6 Conflict of interest statement", "text": "The five authors have a commercial license for the approach described and evaluated in this paper. LAB is co-founding a company that provides tools for lung sound analysis."}, {"heading": "7 Human rights statement", "text": "The Troms\u00f8 study was approved by the Norwegian Data Inspectorate and the Regional Ethical Committee of North Norway (REK). Only the sound files, and variables classifying the sounds were used, and identification of the participants was not possible."}, {"heading": "8 References", "text": "[1] D. Chamberlain, J. Mofor, R. Fletcher, and R. Kodgule, \u201cMobile stethoscope and signal\nprocessing algorithms for pulmonary screening and diagnostics,\u201d in 2015 IEEE Global Humanitarian Technology Conference (GHTC), 2015, pp. 385\u2013392.\n[2] A. Bohadana, G. Izbicki, and S. S. Kraman, \u201cFundamentals of Lung Auscultation,\u201d N. Engl. J. Med., vol. 370, no. 8, pp. 744\u2013751, Feb. 2014. [3] A. F. Members et al., \u201cESC Guidelines for the diagnosis and treatment of acute and chronic heart failure 2012,\u201d Eur. Heart J., vol. 33, no. 14, pp. 1787\u20131847, Jul. 2012. [4] Holleman DR, Jr, and Simel DL, \u201cDoes the clinical examination predict airflow limitation?,\u201d JAMA, vol. 273, no. 4, pp. 313\u2013319, Jan. 1995. [5] Metlay JP, Kapoor WN, and Fine MJ, \u201cDoes this patient have community-acquired pneumonia?: Diagnosing pneumonia by history and physical examination,\u201d JAMA, vol. 278, no. 17, pp. 1440\u20131445, Nov. 1997. [6] \u201cWHO | Chronic obstructive pulmonary disease (COPD),\u201d WHO. [Online]. Available: http://www.who.int/mediacentre/factsheets/fs315/en/. [Accessed: 05-Aug-2016]. [7] V. L. Roger, \u201cEpidemiology of Heart Failure,\u201d Circ. Res., vol. 113, no. 6, pp. 646\u2013659, Aug. 2013. [8] M. A. Fernandez-Granero, D. Sanchez-Morillo, and A. Leon-Jimenez, \u201cComputerised Analysis of Telemonitored Respiratory Sounds for Predicting Acute Exacerbations of COPD,\u201d Sensors, vol. 15, no. 10, pp. 26978\u201326996, Oct. 2015. [9] C. J\u00e1come, A. Oliveira, and A. Marques, \u201cComputerized respiratory sounds: a comparison between patients with stable and exacerbated COPD,\u201d Clin. Respir. J., Oct. 2015. [10] J. R\u00e4s\u00e4nen, M. E. Nemergut, and N. Gavriely, \u201cEffect of PEEP on breath sound power spectra in experimental lung injury,\u201d Intensive Care Med. Exp., vol. 2, Oct. 2014. [11] A. Gurung, C. G. Scrafford, J. M. Tielsch, O. S. Levine, and W. Checkley, \u201cComputerized lung sound analysis as diagnostic aid for the detection of abnormal lung sounds: A systematic review and meta-analysis,\u201d Respir. Med., vol. 105, no. 9, pp. 1396\u20131403, Sep. 2011. [12] \u201cERS Task Force Report,\u201d \u201cComputerized Respiratory Sound Analysis (CORSA): Recommended Standards for Terms and Techniques,\u201d 2000. [13] C. Pinho, A. Oliveira, C. J\u00e1come, J. Rodrigues, and A. Marques, \u201cAutomatic Crackle Detection Algorithm Based on Fractal Dimension and Box Filtering,\u201d Procedia Comput. Sci., vol. 64, pp. 705\u2013712, Jan. 2015. [14] Jo\u00e3o Quintas, Guilherme Campos, and Alda Marques, \u201cMulti-algorithm respiratory crackle detection,\u201d in 6th International Conference on Health Informatics (HEALTHINF), 2013. [15] G. Serbes, C. O. Sakar, Y. P. Kahya, and N. Aydin, \u201cPulmonary crackle detection using time\u2013frequency and time\u2013scale analysis,\u201d Digit. Signal Process., vol. 23, no. 3, pp. 1012\u2013 1021, May 2013. [16] F. Z. G\u00f6\u011f\u00fc\u015f, B. Karl\u0131k, and G. Harman, \u201cClassification of Asthmatic Breath Sounds by Using Wavelet Transforms and Neural Networks,\u201d Int. J. Signal Process. Syst., vol. 3, no. 2, 2014. [17] D. S\u00e1nchez Morillo, S. Astorga Moreno, M. \u00c1. Fern\u00e1ndez Granero, and A. Le\u00f3n Jim\u00e9nez, \u201cComputerized analysis of respiratory sounds during COPD exacerbations,\u201d Comput. Biol. Med., vol. 43, no. 7, pp. 914\u2013921, Aug. 2013. [18] B.-S. Lin, H.-D. Wu, and S.-J. Chen, \u201cAutomatic Wheezing Detection Based on Signal Processing of Spectrogram and Back-Propagation Neural Network,\u201d J. Healthc. Eng., vol. 6, no. 4, pp. 649\u2013672, 2015.\n[19] \u0130. G\u00fcler, H. Polat, and U. Erg\u00fcn, \u201cCombining Neural Network and Genetic Algorithm for Prediction of Lung Sounds,\u201d J. Med. Syst., vol. 29, no. 3, pp. 217\u2013231, Jun. 2005. [20] W. Xiong et al., \u201cThe Microsoft 2016 Conversational Speech Recognition System,\u201d ArXiv160903528 Cs, Sep. 2016. [21] N. Takahashi, M. Gygli, B. Pfister, and L. Van Gool, \u201cDeep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection,\u201d ArXiv160407160 Cs, Apr. 2016. [22] S. Butterworth, \u201cOn the Theory of Filter Amplifiers,\u201d Wirel. Eng., vol. 7, 1930. [23] V. Vapnik, Estimation of Dependences Based on Empirical Data: Springer Series in Statistics. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 1982. [24] B. Dasarathy, Nearest Neighbor (NN) Norms: nn pattern classification techniques. IEEE Computer Society Press, 1991. [25] Y. Freund and R. E. Schapire, \u201cA Decision-Theoretic Generalization of On-Line Learning\nand an Application to Boosting,\u201d J Comput Syst Sci, vol. 55, no. 1, pp. 119\u2013139, Aug. 1997.\n[26] F. Pedregosa et al., \u201cScikit-learn: Machine Learning in Python,\u201d J Mach Learn Res, vol. 12, pp. 2825\u20132830, Nov. 2011. [27] Morten Gr\u00f8nnesby, \u201cPulmonary Crackle Detection Using Signal Processing and Machine Learning,\u201d Capstone Project, University of Troms\u00f8, 2015. [28] Morten Gr\u00f8nnesby, \u201cAutomated Lung Sound Analysis,\u201d Master Thesis in Computer Science, University of Troms\u00f8, 2016. [29] S. Allingame, T. Williams, S. Jenkins, and B. Tucker, \u201cAccuracy and reliability of physiotherapists in the interpretation of tape-recorded lung sounds,\u201d Aust. J. Physiother., vol. 41, no. 3, pp. 179\u2013184, Jan. 1995. [30] C. D. Mulrow et al., \u201cObserver variability in the pulmonary examination,\u201d J. Gen. Intern. Med., vol. 1, no. 6, pp. 364\u2013367, Dec. 1986. [31] H. E. Elphick, G. A. Lancaster, A. Solis, A. Majumdar, R. Gupta, and R. L. Smyth, \u201cValidity and reliability of acoustic analysis of respiratory sounds in infants,\u201d Arch. Dis. Child., vol. 89, no. 11, pp. 1059\u20131063, Nov. 2004. [32] P. Workum, E. A. DelBono, S. K. Holford, and R. L. Murphy, \u201cObserver agreement, chest auscultation, and crackles in asbestos-exposed workers,\u201d Chest, vol. 89, no. 1, pp. 27\u201329, Jan. 1986. [33] B. Logan, \u201cMel Frequency Cepstral Coefficients for Music Modeling,\u201d in International Symposium on Music Information Retrieval, 2000. [34] K. Zhang, X. Wang, F. Han, and H. Zhao, \u201cThe detection of crackles based on mathematical morphology in spectrogram analysis,\u201d Technol. Health Care Off. J. Eur. Soc. Eng. Med., vol. 23 Suppl 2, pp. S489\u2013494, 2015. [35] M. Rosenblatt, \u201cRemarks on Some Nonparametric Estimates of a Density Function,\u201d Ann. Math. Stat., vol. 27, no. 3, pp. 832\u2013837, Sep. 1956. [36] E. Parzen, \u201cOn Estimation of a Probability Density Function and Mode,\u201d Ann. Math. Stat., vol. 33, no. 3, pp. 1065\u20131076, Sep. 1962. [37] J. Semedo et al., \u201cComputerised Lung Auscultation \u2013 Sound Software (CLASS),\u201d Procedia Comput. Sci., vol. 64, pp. 697\u2013704, Jan. 2015."}], "references": [{"title": "Mobile stethoscope and signal processing algorithms for pulmonary screening and diagnostics", "author": ["D. Chamberlain", "J. Mofor", "R. Fletcher", "R. Kodgule"], "venue": "2015 IEEE Global Humanitarian Technology Conference (GHTC), 2015, pp. 385\u2013392.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Fundamentals of Lung Auscultation", "author": ["A. Bohadana", "G. Izbicki", "S.S. Kraman"], "venue": "N. Engl. J. Med., vol. 370, no. 8, pp. 744\u2013751, Feb. 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "ESC Guidelines for the diagnosis and treatment of acute and chronic heart failure 2012", "author": ["A.F. Members"], "venue": "Eur. Heart J., vol. 33, no. 14, pp. 1787\u20131847, Jul. 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1847}, {"title": "Does the clinical examination predict airflow limitation", "author": ["DR Holleman", "Jr", "DL Simel"], "venue": "JAMA, vol. 273, no. 4, pp. 313\u2013319, Jan. 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Does this patient have community-acquired pneumonia?: Diagnosing pneumonia by history and physical examination", "author": ["JP Metlay", "WN Kapoor", "MJ Fine"], "venue": "JAMA, vol. 278, no. 17, pp. 1440\u20131445, Nov. 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Epidemiology of Heart Failure", "author": ["V.L. Roger"], "venue": "Circ. Res., vol. 113, no. 6, pp. 646\u2013659, Aug. 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Computerised Analysis of Telemonitored Respiratory Sounds for Predicting Acute Exacerbations of COPD", "author": ["M.A. Fernandez-Granero", "D. Sanchez-Morillo", "A. Leon-Jimenez"], "venue": "Sensors, vol. 15, no. 10, pp. 26978\u201326996, Oct. 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Computerized respiratory sounds: a comparison between patients with stable and exacerbated COPD", "author": ["C. J\u00e1come", "A. Oliveira", "A. Marques"], "venue": "Clin. Respir. J., Oct. 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Effect of PEEP on breath sound power spectra in experimental lung injury", "author": ["J. R\u00e4s\u00e4nen", "M.E. Nemergut", "N. Gavriely"], "venue": "Intensive Care Med. Exp., vol. 2, Oct. 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Computerized lung sound analysis as diagnostic aid for the detection of abnormal lung sounds: A systematic review and meta-analysis", "author": ["A. Gurung", "C.G. Scrafford", "J.M. Tielsch", "O.S. Levine", "W. Checkley"], "venue": "Respir. Med., vol. 105, no. 9, pp. 1396\u20131403, Sep. 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic Crackle Detection Algorithm Based on Fractal Dimension and Box Filtering", "author": ["C. Pinho", "A. Oliveira", "C. J\u00e1come", "J. Rodrigues", "A. Marques"], "venue": "Procedia Comput. Sci., vol. 64, pp. 705\u2013712, Jan. 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-algorithm respiratory crackle detection", "author": ["Jo\u00e3o Quintas", "Guilherme Campos", "Alda Marques"], "venue": "6th International Conference on Health Informatics (HEALTHINF), 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Pulmonary crackle detection using time\u2013frequency and time\u2013scale analysis", "author": ["G. Serbes", "C.O. Sakar", "Y.P. Kahya", "N. Aydin"], "venue": "Digit. Signal Process., vol. 23, no. 3, pp. 1012\u2013 1021, May 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Classification of Asthmatic Breath Sounds by Using Wavelet Transforms and Neural Networks", "author": ["F.Z. G\u00f6\u011f\u00fc\u015f", "B. Karl\u0131k", "G. Harman"], "venue": "Int. J. Signal Process. Syst., vol. 3, no. 2, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Computerized analysis of respiratory sounds during COPD exacerbations", "author": ["D. S\u00e1nchez Morillo", "S. Astorga Moreno", "M.\u00c1. Fern\u00e1ndez Granero", "A. Le\u00f3n Jim\u00e9nez"], "venue": "Comput. Biol. Med., vol. 43, no. 7, pp. 914\u2013921, Aug. 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Wheezing Detection Based on Signal Processing of Spectrogram and Back-Propagation Neural Network", "author": ["B.-S. Lin", "H.-D. Wu", "S.-J. Chen"], "venue": "J. Healthc. Eng., vol. 6, no. 4, pp. 649\u2013672, 2015.  13", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining Neural Network and Genetic Algorithm for Prediction of Lung Sounds", "author": ["\u0130. G\u00fcler", "H. Polat", "U. Erg\u00fcn"], "venue": "J. Med. Syst., vol. 29, no. 3, pp. 217\u2013231, Jun. 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "The Microsoft 2016 Conversational Speech Recognition System", "author": ["W. Xiong"], "venue": "ArXiv160903528 Cs, Sep. 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection", "author": ["N. Takahashi", "M. Gygli", "B. Pfister", "L. Van Gool"], "venue": "ArXiv160407160 Cs, Apr. 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Theory of Filter Amplifiers", "author": ["S. Butterworth"], "venue": "Wirel. Eng., vol. 7, 1930.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1930}, {"title": "Estimation of Dependences Based on Empirical Data: Springer Series in Statistics", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1982}, {"title": "Nearest Neighbor (NN) Norms: nn pattern classification techniques", "author": ["B. Dasarathy"], "venue": "IEEE Computer Society Press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J Comput Syst Sci, vol. 55, no. 1, pp. 119\u2013139, Aug. 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa"], "venue": "J Mach Learn Res, vol. 12, pp. 2825\u20132830, Nov. 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Pulmonary Crackle Detection Using Signal Processing and Machine Learning", "author": ["Morten Gr\u00f8nnesby"], "venue": "Capstone Project, University of Troms\u00f8, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated Lung Sound Analysis", "author": ["Morten Gr\u00f8nnesby"], "venue": "Master Thesis in Computer Science, University of Troms\u00f8, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Accuracy and reliability of physiotherapists in the interpretation of tape-recorded lung sounds", "author": ["S. Allingame", "T. Williams", "S. Jenkins", "B. Tucker"], "venue": "Aust. J. Physiother., vol. 41, no. 3, pp. 179\u2013184, Jan. 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Observer variability in the pulmonary examination", "author": ["C.D. Mulrow"], "venue": "J. Gen. Intern. Med., vol. 1, no. 6, pp. 364\u2013367, Dec. 1986.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1986}, {"title": "Validity and reliability of acoustic analysis of respiratory sounds in infants", "author": ["H.E. Elphick", "G.A. Lancaster", "A. Solis", "A. Majumdar", "R. Gupta", "R.L. Smyth"], "venue": "Arch. Dis. Child., vol. 89, no. 11, pp. 1059\u20131063, Nov. 2004.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Observer agreement, chest auscultation, and crackles in asbestos-exposed workers", "author": ["P. Workum", "E.A. DelBono", "S.K. Holford", "R.L. Murphy"], "venue": "Chest, vol. 89, no. 1, pp. 27\u201329, Jan. 1986.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1986}, {"title": "Mel Frequency Cepstral Coefficients for Music Modeling", "author": ["B. Logan"], "venue": "International Symposium on Music Information Retrieval, 2000.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "The detection of crackles based on mathematical morphology in spectrogram analysis", "author": ["K. Zhang", "X. Wang", "F. Han", "H. Zhao"], "venue": "Technol. Health Care Off. J. Eur. Soc. Eng. Med., vol. 23 Suppl 2, pp. S489\u2013494, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Remarks on Some Nonparametric Estimates of a Density Function", "author": ["M. Rosenblatt"], "venue": "Ann. Math. Stat., vol. 27, no. 3, pp. 832\u2013837, Sep. 1956.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1956}, {"title": "On Estimation of a Probability Density Function and Mode", "author": ["E. Parzen"], "venue": "Ann. Math. Stat., vol. 33, no. 3, pp. 1065\u20131076, Sep. 1962.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1962}, {"title": "Computerised Lung Auscultation \u2013 Sound Software (CLASS)", "author": ["J. Semedo"], "venue": "Procedia Comput.  Sci., vol. 64, pp. 697\u2013704, Jan. 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "com/), and the MIT mobile stethoscope [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "Crackles are short, explosive nonmusical sounds heard mostly during inspiration [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "In these diseases, the presence of crackles helps to stablish a diagnosis [3]\u2013[5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "In these diseases, the presence of crackles helps to stablish a diagnosis [3]\u2013[5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "Recent reports [7] state that 23 million people worldwide have a diagnosis of heart failure.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "In addition, lung sounds have the ability to reflect rapid changes, and are therefore useful in evaluating treatment responses, home monitoring, and maybe predicting exacerbations of disease [8]\u2013[10].", "startOffset": 191, "endOffset": 194}, {"referenceID": 8, "context": "In addition, lung sounds have the ability to reflect rapid changes, and are therefore useful in evaluating treatment responses, home monitoring, and maybe predicting exacerbations of disease [8]\u2013[10].", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "Current approaches for automatic detection of crackles in lung sounds have shown promise and they have achieved high specificity and sensitivity for test data ([11] provides a review, CORSA [12] recommends standard for terms and techniques).", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "Most are rule based [13], [14], and hence detect crackles using a set of predefined parameters that have been extracted from a small set of sample audio files using signal processing techniques.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Most are rule based [13], [14], and hence detect crackles using a set of predefined parameters that have been extracted from a small set of sample audio files using signal processing techniques.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "Recently several machine learning based approaches, based on for example SVMs and Neural Networks [15]\u2013[19] have been introduced.", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "Recently several machine learning based approaches, based on for example SVMs and Neural Networks [15]\u2013[19] have been introduced.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Machine learning based classification such as automatic speech recognition [20] and automatic acoustic event detection [21] is a very active research field with widely used solutions.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Machine learning based classification such as automatic speech recognition [20] and automatic acoustic event detection [21] is a very active research field with widely used solutions.", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "We considered using a Butterworth bandpass filter [22] to remove frequencies above 2400 Hz and below 50 Hz since these do not contain crackle sounds.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "We evaluated three classifiers for the 5-dimensional feature vector: SVM [23], KNN [24], and AdaBoost [25] (Decision Trees).", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We evaluated three classifiers for the 5-dimensional feature vector: SVM [23], KNN [24], and AdaBoost [25] (Decision Trees).", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "We evaluated three classifiers for the 5-dimensional feature vector: SVM [23], KNN [24], and AdaBoost [25] (Decision Trees).", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "7 using Scikit Learn [26].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "We have also evaluated other feature extraction methods, including classifiers for these [27], [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "We have also evaluated other feature extraction methods, including classifiers for these [27], [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "The accuracy and reliability of our system is better than found in studies of health personnel [29]\u2013[32] (although health personnel have higher accuracy than reported in [29]\u2013[32] using our data (unpublished)).", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "The accuracy and reliability of our system is better than found in studies of health personnel [29]\u2013[32] (although health personnel have higher accuracy than reported in [29]\u2013[32] using our data (unpublished)).", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "The accuracy and reliability of our system is better than found in studies of health personnel [29]\u2013[32] (although health personnel have higher accuracy than reported in [29]\u2013[32] using our data (unpublished)).", "startOffset": 170, "endOffset": 174}, {"referenceID": 29, "context": "The accuracy and reliability of our system is better than found in studies of health personnel [29]\u2013[32] (although health personnel have higher accuracy than reported in [29]\u2013[32] using our data (unpublished)).", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "In earlier work [27] we evaluated two alternative feature extraction methods for our data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "4 Mel Frequency Ceptrsal Coefficients Mel Frequency Cepstral Coefficients (MFCC) feature extraction is widely used in speech and music recognition [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 31, "context": "In [34], crackles are classified by extracting features from the elliptical pattern of a crackle in a spectrogram.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "We did not have the data used in [34], so we could not do a direct comparison, so we could check if this was since we used STFT instead of the bio22 wavelet decomposition used in [34].", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": "We did not have the data used in [34], so we could not do a direct comparison, so we could check if this was since we used STFT instead of the bio22 wavelet decomposition used in [34].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": "Using Parzen Windows (Kernel Density Estimation) [35], [36] to estimate the probability density function of a part of the signal, and then comparing the different estimates to find breathing phases.", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "Using Parzen Windows (Kernel Density Estimation) [35], [36] to estimate the probability density function of a part of the signal, and then comparing the different estimates to find breathing phases.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "Our approach can be used in training tools to automatically detect and highlight crackles in waveforms and spectrograms (similar to [37]).", "startOffset": 132, "endOffset": 136}], "year": 2017, "abstractText": "Background and Objective: The stethoscope is a well-known and widely available diagnostic instrument. In recent years, many innovative solutions for recording and viewing sounds from a stethoscope have become available. However, to fully utilize such devices, there is a need for an automated approach for detecting abnormal lung sounds, which is better than the existing methods that typically have been developed and evaluated using a small and non-diverse dataset. Methods: We propose a machine learning based approach for detecting crackles in lung sounds recorded using a stethoscope in a large health survey. Our method is trained and evaluated using 209 files with crackles classified by expert listeners. Our analysis pipeline is based on features extracted from small windows in audio files. We evaluated several feature extraction methods and classifiers. We evaluated the pipeline using a training set of 175 crackle windows and 208 normal windows. We did 100 cycles of cross validation where we shuffled training sets between cycles. For all the division between training and evaluation was 70%-30%. Results: We found and evaluated a 5-dimenstional vector with four features from the time domain and one from the spectrum domain. We evaluated several classifiers and found SVM with a Radial Basis Function Kernel to perform best for our 5-dimensional feature vector. Our approach had a precision of 86% and recall of 84% for classifying a crackle in a window, which is more accurate than found in studies of health personnel. The low-dimensional feature vector makes the SVM very fast. The model can be trained on a regular computer in 1.44 seconds, and 319 crackles can be classified in 1.08 seconds. Conclusions: Our approach detects and visualizes individual crackles in recorded audio files. It is accurate, fast, and has low resource requirements. The approach is therefore well suited for deployment on smart devices and phones or as a web application. It can be used to train health personnel or as part of a smartphone application for Bluetooth stethoscopes.", "creator": "PScript5.dll Version 5.2.2"}}}