{"id": "1706.00387", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning", "abstract": "The final algorithm provides a generalization and unification of existing methods for deep political gradients, has theoretical guarantees for the bias introduced by policy updates, and improves the number of freely available gradient methods based on \"open marks.\" On the other hand, model-free methods for deep gradients are often more stable and easier to apply. This paper examines both theoretical and empirical approaches to merging updates of on- and off-policy gradients. Theoretical results show that non-political updates can be interpolated with a value function estimator without exceeding performance boundaries. Our analysis uses controllable methods to generate a family of policy gradient algorithms, with several recently proposed algorithms representing special cases of this family. The final algorithm provides a unification and unification of existing methods for deep political gradients, has theoretical guarantees for the biased political performance through gradient samples and ionic improvements.", "histories": [["v1", "Thu, 1 Jun 2017 17:00:52 GMT  (661kb,D)", "http://arxiv.org/abs/1706.00387v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["shixiang gu", "timothy lillicrap", "zoubin ghahramani", "richard e turner", "bernhard sch\\\"olkopf", "sergey levine"], "accepted": true, "id": "1706.00387"}, "pdf": {"name": "1706.00387.pdf", "metadata": {"source": "META", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning", "authors": ["Shixiang Gu"], "emails": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "bs@tuebingen.mpg.de", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) studies how an agent that interacts sequentially with an environment can learn from rewards to improve its behavior and optimize long-term returns. Recent research has demonstrated that deep networks can be successfully combined with RL techniques to solve difficult control problems. Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al., 2015), and board games (Silver et al., 2016). One of the simplest ways to learn a neural network policy is to collect a batch of behavior wherein the policy is used to act in the world, and then compute and apply a policy gradient update from this data. This is referred to as on-policy learning because all of the updates are made using data that was collected from the trajectory distribution induced by the current policy of the agent. It is straightforward to compute unbiased on-policy gradients, and practical on-policy gradient algorithms tend to be stable and relatively easy to use. A major drawback of such methods is that they tend to be data inefficient, because they only look at each data point once. Off-policy algorithms based on Q-learning and actor-critic learning (Sutton et al., 1999) have also proven to be an effective approach to deep reinforcement learning such as in (Mnih et al., 2015) and (Lillicrap et al., 2016). Such methods\nar X\niv :1\n70 6.\n00 38\n7v 1\n[ cs\n.L G\nreuse samples by storing them in a memory replay buffer and train a value function or Q-function with off-policy updates. This improves data efficiency, but often at a cost in stability and ease of use.\nBoth on- and off-policy learning techniques have been shown to have advantages. Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017). Such algorithms hope to gain advantages from both modes of learning, whilst avoiding their limitations. Broadly speaking, there have been two basic approaches in recently proposed algorithms that make use of both on- and off-policy data and updates. The first approach is to mix some ratio of on- and off-policy gradients or update steps in order to update a policy, as in the ACER and PGQ algorithms (Wang et al., 2017; O\u2019Donoghue et al., 2017). In this case, there are no theoretical bounds on the error induced by incorporating off-policy updates. In the second approach, an off-policy Q critic is trained but is used as a control variate to reduce on-policy gradient variance, as in the Q-prop algorithm (Gu et al., 2017). This case does not introduce additional bias to the gradient estimator, but the policy updates do not use off-policy data.\nWe seek to unify these two approaches using the method of control variates. We introduce a parameterized family of policy gradient methods that interpolate between on-policy and off-policy learning. Such methods are in general biased, but we show that the bias can be bounded.We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family. Furthermore, our empirical results show that in most cases, a mix of policy gradient and actor-critic updates achieves the best results, demonstrating the value of considering interpolated policy gradients."}, {"heading": "2 Preliminaries", "text": "A key component of our interpolated policy gradient method is the use of control variates to mix likelihood ratio gradients with deterministic gradient estimates obtained explicitly from a state-action critic. In this section, we summarize both likelihood ratio and deterministic gradient methods, as well as how control variates can be used to combine these two approaches."}, {"heading": "2.1 On-Policy Likelihood Ratio Policy Gradient", "text": "At time t, the RL agent in state st takes action at according to its policy \u03c0(at|st), the state transitions to st+1, and the agent receives a reward r(st, at). For a parametrized policy \u03c0\u03b8, the objective is to maximize the \u03b3-discounted cumulative future return J(\u03b8) = J(\u03c0) = Es0,a0,\u00b7\u00b7\u00b7\u223c\u03c0 [ \u2211\u221e t=0 \u03b3\ntr(st, at)]. Monte Carlo policy gradient methods, such as REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), use the likelihood ratio policy gradient of the RL objective,\n\u2207\u03b8J(\u03b8) = E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)(Q\u0302(st, at)\u2212 b(st))] = E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)A\u0302(st, at)], (1)\nwhere Q\u0302(st, at) = \u2211\u221e t\u2032=t \u03b3\nt\u2032\u2212tr(st\u2032 , at\u2032) is the Monte Carlo estimate of the \u201ccritic\u201d Q\u03c0(st, at) = Est+1,at+1,\u00b7\u00b7\u00b7\u223c\u03c0|st,at [Q\u0302(st, at)], and \u03c1\u03c0 = \u2211\u221e t=0 \u03b3\ntp(st = s) are the unnormalized state visitation frequencies, while b(st) is known as the baseline, and serves to reduce the variance of the gradient estimate (Williams, 1992). If the baseline estimates the value function, V \u03c0(st) = Eat\u223c\u03c0(\u00b7|st)[Q\u03c0(st, at)], then A\u0302(st) is an estimate of the advantage function A\u03c0(st, at) = Q\u03c0(st, at)\u2212 V \u03c0(st). Likelihood ratio policy gradient methods use unbiased gradient estimates (except for the technicality detailed by Thomas (2014)), but they often suffer from high variance and are sample-intensive."}, {"heading": "2.2 Off-Policy Deterministic Policy Gradient", "text": "Policy gradient methods with function approximation (Sutton et al., 1999), or actor-critic methods, are a family of policy gradient methods which first estimate the critic, or the value, of the policy by Qw \u2248 Q\u03c0 , and then greedily optimize the policy \u03c0\u03b8 with respect to Qw. While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016). For example, DDPG Lillicrap et al. (2016), which optimizes a continuous deterministic policy \u03c0\u03b8(at|st) = \u03b4(at = \u00b5\u03b8(st)), can be summarized by the following update equations, where Q\u2032w denotes the target Q network (Lillicrap\net al., 2016) and \u03b2 denotes some off-policy distribution, e.g. from experience replay:\nw \u2190 arg minE\u03b2 [(Qw(st, at)\u2212 yt)2], yt = r(st, at) + \u03b3Qw(st+1, \u00b5\u03b8(st+1)) \u03b8 \u2190 arg maxE\u03b2 [Qw(st, \u00b5\u03b8(st))].\n(2)\nThis provides the following deterministic policy gradient through the critic:\n\u2207\u03b8J(\u03b8) \u2248 E\u03c1\u03b2 [\u2207\u03b8Qw(st, \u00b5\u03b8(st))]. (3)\nThis policy gradient is generally biased due to the imperfect estimator Qw and off-policy state sampling from \u03b2. Off-policy actor-critic algorithms therefore allow training the policy on off-policy samples, at the cost of introducing potentially unbounded bias into the gradient estimate. This usually makes off-policy algorithms less stable during learning, compared to on-policy algorithms using a large batch size for each update (Duan et al., 2016; Gu et al., 2017)."}, {"heading": "2.3 Off-Policy Control Variate Fitting", "text": "The control variates method (Ross, 2006) is a general technique for variance reduction of a Monte Carlo estimator by exploiting a correlated variable for which we know more information such as analytical expectation. General control variates for RL include state-action baselines, and an example can be an off-policy fitted critic Qw. Q-Prop (Gu et al., 2017), for example, used Q\u0303w, the first-order Taylor expansion of Qw, as the control variates, and showed improvement in stability and sample efficiency of policy gradient methods. \u00b5\u03b8 here corresponds to the mean of the stochastic policy \u03c0\u03b8.\n\u2207\u03b8J(\u03b8) = E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)(Q\u0302(st, at)\u2212 Q\u0303w(st, at))] + E\u03c1\u03c0 [\u2207\u03b8Qw(st, \u00b5\u03b8(st))]. (4)\nThe gradient estimator combines both likelihood ratio and deterministic policy gradients in Eq. 1 and 3. It has lower variance and stable gradient estimates and enables more sample-efficient learning. However, one limitation of Q-Prop is that it uses only on-policy samples for estimating the policy gradient. This ensures that the Q-Prop estimator remains unbiased, but limits the use of off-policy samples for further variance reduction."}, {"heading": "3 Interpolated Policy Gradient", "text": "Our proposed approach, interpolated policy gradient (IPG), mixes likelihood ratio gradient with Q\u0302, which provides unbiased but high-variance gradient estimation, and deterministic gradient through an off-policy fitted criticQw, which provides low-variance but biased gradients. IPG directly interpolates the two terms from Eq. 1 and 3:\n\u2207\u03b8J(\u03b8) \u2248 (1\u2212 \u03bd)E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)A\u0302(st, at)] + \u03bdE\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)], (5)\nwhere we generalized the deterministic policy gradient through the critic as \u2207\u03b8Q\u0304w(st) = \u2207\u03b8E\u03c0[Q\u03c0w(st, \u00b7)]. This generalization is to make our analysis applicable with more general forms of the critic-based control variates, as discussed in the Appendix. This gradient estimator is biased from two sources: off-policy state sampling \u03c1\u03b2 , and inaccuracies in the critic Qw. However, as we show in Section 4, we can bound the biases for all the cases, and in some cases, the algorithm still guarantees monotonic convergence as in Kakade & Langford (2002); Schulman et al. (2015)."}, {"heading": "3.1 Control Variates for Interpolated Policy Gradient", "text": "While IPG includes \u03bd to trade off bias and variance directly, it contains a likelihood ratio gradient term, for which we can introduce a control variate (CV) Ross (2006) to further reduce the estimator variance.\nThe expression for the IPG with control variates is below, where A\u03c0w(st, at) = Qw(st, at)\u2212 Q\u0304\u03c0w(st),\n\u2207\u03b8J(\u03b8) \u2248 (1\u2212 \u03bd)E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)A\u0302(st, at)] + \u03bdE\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)] = (1\u2212 \u03bd)E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)(A\u0302(st, at)\u2212A\u03c0w(st, at))]\n+ (1\u2212 \u03bd)E\u03c1\u03c0 [\u2207\u03b8Q\u0304\u03c0w(st)] + \u03bdE\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)] \u2248 (1\u2212 \u03bd)E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)(A\u0302(st, at)\u2212A\u03c0w(st, at))] + E\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)].\n(6)\nThe first approximation indicates the biased approximation from IPG, while the second approximation indicates replacing the \u03c1\u03c0 in the control variate correction term with \u03c1\u03b2 and merging with the last term. The second approximation is a design decision and introduces additional bias when \u03b2 6= \u03c0 but it helps simplify the expression to be analyzed more easily, and the additional benefit from the variance reduction from the control variate could still outweigh this extra bias. The biases are analyzed in Section 4. The likelihood ratio gradient term is now proportional to the residual in on- and off-policy advantage estimates A\u0302(st, at)\u2212A\u03c0w(st, at), and therefore, we call this term residual likelihood ratio gradient. Intuitively, if the off-policy critic estimate is accurate, this term has a low magnitude and the overall variance of the estimator is reduced."}, {"heading": "3.2 Relationship to Prior Policy Gradient and Actor-Critic Methods", "text": "Crucially, IPG allows interpolating a rich list of prior deep policy gradient methods using only three parameters: \u03b2, \u03bd, and the use of the control variate (CV). The connection is summarized in Table 1 and the algorithm is presented in Algorithm 1. Importantly, a wide range of prior work has only explored limiting cases of the spectrum, e.g. \u03bd = 0, 1, with or without the control variate. Our work provides a thorough theoretical analysis of the biases, and in some cases performance guarantees, for each of the method in this spectrum and empirically demonstrates often the best performing algorithms are in the midst of the spectrum.\nAlgorithm 1 Interpolated Policy Gradient input \u03b2, \u03bd, useCV 1: Initialize w for critic Qw, \u03b8 for stochastic policy \u03c0\u03b8 , and replay bufferR \u2190 \u2205. 2: repeat 3: Roll-out \u03c0\u03b8 for E episodes, T time steps each, to collect a batch of data B = {s, a, r}1:T,1:E toR 4: Fit Qw usingR and \u03c0\u03b8 , and fit baseline V\u03c6(st) using B 5: Compute Monte Carlo advantage estimate A\u0302t,e using B and V\u03c6 6: if useCV then 7: Compute critic-based advantage estimate A\u0304t,e using B, Qw and \u03c0\u03b8 8: Compute and center the learning signals lt,e = A\u0302t,e \u2212 A\u0304t,e and set b = 1 9: else 10: Center the learning signals lt,e = A\u0302t,e and set b = \u03bd 11: end if 12: Multiply lt,e by (1\u2212 \u03bd) 13: Sample D = s1:M fromR and/or B based on \u03b2 14: Compute\u2207\u03b8J(\u03b8) \u2248 1ET \u2211 e \u2211 t\u2207\u03b8 log \u03c0\u03b8(at,e|st,e)lt,e + bM \u2211 m\u2207\u03b8Q\u0304\u03c0w(sm) 15: Update policy \u03c0\u03b8 using\u2207\u03b8J(\u03b8) 16: until \u03c0\u03b8 converges."}, {"heading": "3.3 \u03bd = 1: Actor-Critic methods", "text": "Before presenting our theoretical analysis, an important special case to discuss is \u03bd = 1, which corresponds to a deterministic actor-critic method. Several advantages of this special case include that the policy can be deterministic and the learning can be done completely off-policy, as it does not have to estimate the on-policy Monte Carlo critic Q\u0302. Prior work such as DDPG Lillicrap et al. (2016) and related Q-learning methods have proposed aggressive off-policy exploration strategy to exploit these properties of the algorithm. In this work, we compare alternatives such as using on-policy exploration and stochastic policy with classical DDPG algorithm designs, and show that in some domains the off-policy exploration can significantly deteriorate the performance. Theoretically, we confirm this empirical observation by showing that the bias from off-policy sampling in \u03b2 increases\nmonotonically with the total variation or KL divergence between \u03b2 and \u03c0. Both the empirical and theoretical results indicate that well-designed actor-critic methods with an on-policy exploration strategy could be a more reliable alternative than with an on-policy exploration."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we present a theoretical analysis of the bias in the interpolated policy gradient. This is crucial, since understanding the biases of the methods can improve our intuition about its performance and make it easier to design new algorithms in the future. Because IPG includes many prior methods as special cases, our analysis also applies to those methods and other intermediate cases. We first analyze a special case and derive results for general IPG. All proofs are in the Appendix."}, {"heading": "4.1 \u03b2 6= \u03c0, \u03bd = 0: Policy Gradient with Control Variate and Off-Policy Sampling", "text": "This section provides an analysis of the special case of IPG with \u03b2 6= \u03c0, \u03bd = 1, and the control variate. Plugging in to Eq. 6, we get an expression similar to Q-Prop in Eq. 4,\n\u2207\u03b8J(\u03b8) \u2248 E\u03c1\u03c0,\u03c0[\u2207\u03b8 log \u03c0\u03b8(at|st)(A\u0302(st, at)\u2212A\u03c0w(st, at))] + E\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)], (7)\nexcept that it also supports utilizing off-policy data for updating the policy. To analyze the bias for this gradient expression, we first introduce J\u0303(\u03c0, \u03c0\u0303), a local approximation to J(\u03c0), which has been used in prior theoretical work (Kakade & Langford, 2002; Schulman et al., 2015). The derivation and the bias from this approximation are discussed in the proof for Theorem 1 in the Appendix.\nJ(\u03c0) = J(\u03c0\u0303) + E\u03c1\u03c0,\u03c0[A\u03c0\u0303(st, at)] \u2248 J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)] = J\u0303(\u03c0, \u03c0\u0303). (8)\nNote that J(\u03c0) = J\u0303(\u03c0, \u03c0\u0303 = \u03c0) and\u2207\u03c0J(\u03c0) = \u2207\u03c0J\u0303(\u03c0, \u03c0\u0303 = \u03c0). In practice, \u03c0\u0303 corresponds to policy \u03c0k at iteration k and \u03c0 corresponds next policy \u03c0k+1 after parameter update. Thus, this approximation is often sufficiently good. Next, we write the approximate objective for Eq. 7,\nJ\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) , J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212A\u03c0\u0303w(st, at)] + E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2248 J\u0303(\u03c0, \u03c0\u0303) A\u0304\u03c0,\u03c0\u0303w (st) = E\u03c0[A\u03c0\u0303w(st, \u00b7)] = E\u03c0[Qw(st, \u00b7)]\u2212 E\u03c0\u0303[Qw(st, \u00b7)]. (9)\nNote that J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303 = \u03c0) = J\u0303(\u03c0, \u03c0\u0303 = \u03c0) = J(\u03c0), and \u2207\u03c0J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303 = \u03c0) equals Eq. 7. We can bound the absolute error between J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) and J(\u03c0) by the following theorem, where DmaxKL (\u03c0i, \u03c0j) = maxsDKL(\u03c0i(\u00b7|s), \u03c0j(\u00b7|s)) is the maximum KL divergence between \u03c0i, \u03c0j . Theorem 1. If = maxs |A\u0304\u03c0,\u03c0\u0303w (s)|, \u03b6 = maxs |A\u0304\u03c0,\u03c0\u0303(s)|, then\n\u2225\u2225\u2225J(\u03c0)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 2 \u03b3 (1\u2212 \u03b3)2 ( \u221a DmaxKL (\u03c0\u0303, \u03b2) + \u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) )\nTheorem 1 contains two terms: the second term confirms J\u0303\u03b2,\u03bd=0,CV is a local approximation around \u03c0 and deviates from J(\u03c0) as \u03c0\u0303 deviates, and the first term bounds the bias from off-policy sampling using the KL divergence between the policies \u03c0\u0303 and \u03b2. This means that the algorithm fits well with policy gradient methods which constrain the KL divergence per policy update, such as covariant policy gradient (Bagnell & Schneider, 2003), natural policy gradient (Kakade & Langford, 2002), REPS (Peters et al., 2010), and trust-region policy optimization (TRPO) (Schulman et al., 2015)."}, {"heading": "4.1.1 Monotonic Policy Improvement Guarantee", "text": "Some forms of on-policy policy gradient methods have theoretical guarantees on monotonic convergence Kakade & Langford (2002); Schulman et al. (2015). Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017). We can show that Algorithm 2, which is a variant of IPG, guarantees monotonic convergence. The proof is provided in the appendix.\nAlgorithm 2 is often impractical to implement; however, IPG with trust-region updates when \u03b2 6= \u03c0, \u03bd = 1,CV = true approximates this monotonic algorithm, similar to how TRPO is an approximation to the theoretically monotonic algorithm proposed by Schulman et al. (2015).\nAlgorithm 2 Policy iteration with non-decreasing returns J(\u03c0) and bounded off-policy sampling 1: Initialize policy \u03c00, and critic Qw 2: repeat 3: Compute all advantage values A\u03c0i(s, a), and choose any off-policy distribution \u03b2i 4: Update critic Qw using any method (no requirement for performance) 5: Solve the constrained optimization problem:\n6: \u03c0i+1 \u2190 arg max\u03c0 J\u0303\u03b2i,\u03bd=0,CV (\u03c0, \u03c0i)\u2212 C ( \u03b6 \u221a DmaxKL (\u03c0, \u03c0i) + \u221a DmaxKL (\u03c0i, \u03b2i) ) 7: subject to \u2211 a \u03c0(a|s) = 1 \u2200s 8: where C = 2\u03b3 (1\u2212\u03b3)2 , \u03b6 = maxs |A\u0304\n\u03c0,\u03c0\u0303(s)|, = maxs |A\u0304\u03c0,\u03c0\u0303w (s)| 9: until \u03c0i converges."}, {"heading": "4.2 General Bounds on the Interpolated Policy Gradient", "text": "We can establish bias bounds for the general IPG algorithm, with and without the control variate, using Theorem 2. The additional term that contributes to the bias in the general case is \u03b4, which represents the error between the advantage estimated by the off-policy critic and the true A\u03c0 values. Theorem 2. If \u03b4 = maxs,a |A\u03c0\u0303(s, a)\u2212A\u03c0\u0303w(s, a)|, = maxs |A\u0304\u03c0,\u03c0\u0303w (s)|, \u03b6 = maxs |A\u0304\u03c0,\u03c0\u0303(s)|,\nJ\u0303\u03b2,\u03bd(\u03c0, \u03c0\u0303) , J(\u03c0\u0303) + (1\u2212 \u03bd)E\u03c1\u03c0\u0303,\u03c0[A\u0302\u03c0\u0303] + \u03bdE\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w ] J\u0303\u03b2,\u03bd,CV (\u03c0, \u03c0\u0303) , J(\u03c0\u0303) + (1\u2212 \u03bd)E\u03c1\u03c0\u0303,\u03c0[A\u0302\u03c0\u0303 \u2212A\u03c0\u0303w] + E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w ]\nthen, \u2225\u2225\u2225J(\u03c0)\u2212 J\u0303\u03b2,\u03bd(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 \u03bd\u03b4 1\u2212 \u03b3 + 2 \u03b3 (1\u2212 \u03b3)2 ( \u03bd \u221a DmaxKL (\u03c0\u0303, \u03b2) + \u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) )\n\u2225\u2225\u2225J(\u03c0)\u2212 J\u0303\u03b2,\u03bd,CV (\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 \u03bd\u03b4 1\u2212 \u03b3 + 2 \u03b3 (1\u2212 \u03b3)2 ( \u221a DmaxKL (\u03c0\u0303, \u03b2) + \u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) )\nThis bound shows that the bias from directly mixing the deterministic policy gradient through \u03bd comes from two terms: how well the critic Qw is approximating Q\u03c0, and how close the off-policy sampling policy is to the actor policy. We also show that the bias introduced is proportional to \u03bd while the variance of the high variance likelihood ratio gradient term is proportional to (1\u2212 \u03bd)2, so \u03bd allows directly trading off bias and variance. Theorem 2 fully bounds bias in the full spectrum of IPG methods; this enables us to analyze how biases arise and interact and help us design better algorithms."}, {"heading": "5 Related Work", "text": "An overarching aim of this paper is to help unify on-policy and off-policy policy gradient algorithms into a single conceptual framework. Our analysis examines how Q-Prop (Gu et al., 2017), PGQ (O\u2019Donoghue et al., 2017), and ACER (Wang et al., 2017), which are all recent works that combine on-policy with off-policy learning, are connected to each other (see Table 1). IPG with 0 < \u03bd < 1 and without the control variate relates closely to PGQ and ACER, but differ in the details. PGQ mixes in the Q-learning Bellman error objective, and ACER mixes parameter update steps rather than directly mixing gradients. And both PGQ and ACER come with numerous additional design details that make fair comparisons with methods like TRPO and Q-Prop difficult. We instead focus on the three minimal variables of IPG and explore their settings in relation to the closely related TRPO and Q-Prop methods, in order to theoretically and empirically understand in which situations we might expect gains from mixing on- and off-policy gradients.\nAsides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013). Most of these methods rely on variants of importance sampling (IS) to correct for bias. The use of importance sampling ensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure used by Jie & Abbeel (2010). Ignoring importance weights produces bias but, as shown in our analysis, this bias can be bounded. Therefore, our IPG estimators have higher bias as the sampling distribution deviates from the policy, while IS methods have higher variance. Among these importance sampling methods, Levine & Koltun (2013) evaluates on tasks that are the most similar to our paper, but the focus is on using importance sampling to include demonstrations, rather than to speed up learning from scratch.\nLastly, there are many methods that combine on- and off-policy data for policy evaluation (Precup, 2000; Mahmood et al., 2014; Munos et al., 2016), mostly through variants of importance sampling. Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016)."}, {"heading": "6 Experiments", "text": "In this section, we empirically show that the three parameters of IPG can interpolate different behaviors and often achieve superior performance versus prior methods that are limiting cases of this approach. Crucially, all methods share the same algorithmic structure as Algorithm 1, and we hold the rest of the experimental details fixed. All experiments were performed on MuJoCo domains in OpenAI Gym (Todorov et al., 2012; Brockman et al., 2016), with results presented for the average over three seeds. Additional experimental details are provided in the Appendix."}, {"heading": "6.1 \u03b2 6= \u03c0, \u03bd = 0, with the control variate", "text": "We evaluate the performance of the special case of IPG discussed in Section 4.1. This case is of particular interest, since we can derive monotonic convergence results for a variant of this method under certain conditions, despite the presence of off-policy updates. Figure 1a shows the performance on the HalfCheetah-v1 domain, when the policy update batch size is 5000 transitions (i.e. 5 episodes). \u201clast\u201d and \u201crand\u201d indicate if \u03b2 samples from the most recent transitions or uniformly from the experience replay. \u201clast05000\u201d would be equivalent to Q-Prop given \u03bd = 0. Comparing \u201cIPG-\u03b2rand05000\u201d and \u201cQ-Prop\u201d curves, we observe that by drawing the same number of samples randomly from the replay buffer for estimating the critic gradient, instead of using the on-policy samples, we get faster convergence. If we sample batches of size 30000 from the replay buffer, the performance further improves. However, as seen in the \u201cIPG-\u03b2-last30000\u201d curve, if we instead use the 30000 most recent samples, the performance degrades. One possible explanation for this is that, while using random samples from the replay increases the bound on the bias according to Theorem 1, it also decorrelates the samples within the batch, providing more stable gradients. This is the original motivation for experience replay in the DQN method (Mnih et al., 2015), and we have shown that such decorrelated off-policy samples can similarly produce gains for policy gradient algorithms. See Table 2 for results on other domains.\nThe results for this variant of IPG demonstrate that random sampling from the replay provides further improvement on top of Q-Prop, a strong baseline for sample-efficiency and stability. Note that these replay buffer samples are different from standard off-policy samples in DDPG or DQN algorithms, which often use aggressive heuristic exploration strategies. The samples used by IPG are sampled\nfrom prior policies that follow a conservative trust-region update, resulting in greater regularity but less exploration. In the next section, we show that in some cases, ensuring that the off-policy samples are not too off-policy is essential for good performance."}, {"heading": "6.2 \u03b2 = \u03c0, \u03bd = 1", "text": "In this section, we empirically evaluate another special case of IPG, where \u03b2 = \u03c0, indicating onpolicy sampling, and \u03bd = 1, which reduces to a trust-region, on-policy variant of a deterministic actor-critic method. Although this algorithm performs actor-critic updates, the use of a trust region makes it more similar to TRPO or Q-Prop than DDPG.\nResults for all domains are shown in Table 2. Figure 1b shows the learning curves on Ant-v1. Although IPG-\u03bd=1 methods can be off-policy, the policy is updated every 5000 samples to keep it consistent with other IPG methods, while DDPG updates the policy on every step in the environment and makes other design choices Lillicrap et al. (2016). We see that, in this domain, standard DDPG becomes stuck with a mean reward of 1000, while IPG-\u03bd=1 improves monotonically, achieving a significantly better result. To investigate why this large discrepancy arises, we also ran IPG-\u03bd=1 with the same OU process exploration noise as DDPG, and observed large degradation in performance. This provides empirical support for Theorem 2. It is illuminating to contrast this result with the previous experiment, where the off-policy samples did not adversely alter the results. In the previous experiments, the samples came from Gaussian policies updated with trust-regions. The difference between \u03c0 and \u03b2 was therefore approximately bounded by the trust-regions. In the experiment with Brownian noise, the behaving policy uses temporally correlated noise, with potentially unbounded KL-divergence from the learned Gaussian policy. In this case, the off-policy samples result in excessive bias, wiping out the variance reduction benefits of off-policy sampling. In general, we observed that for the harder Ant-v1 and Walker-v1 domains, on-policy exploration is more effective, even when doing off-policy state sampling from a replay buffer. This results suggests the following lesson for designing off-policy actor-critic methods: for domains where exploration is difficult, it may be more effective to use on-policy exploration with bounded policy updates than to design heuristic exploration rules such as the OU process noise, due to the resulting reduction in bias."}, {"heading": "6.3 General Cases of Interpolated Policy Gradient", "text": "Table 2 shows the results for experiments where we compare IPG methods with varying values of \u03bd; additional results are provided in the Appendix. \u03b2 6= \u03c0 indicates that the method uses off-policy samples from the replay buffer, with the same batch size as the on-policy batch for fair comparison. We ran sweeps over \u03bd = {0.2, 0.4, 0.6, 0.8} and found that \u03bd = 0.2 consistently produce better performance than Q-Prop, TRPO or prior actor-critic methods. This is consistent with the results in PGQ (O\u2019Donoghue et al., 2017) and ACER (Wang et al., 2017), which found that their equivalent of \u03bd = 0.1 performed best on their benchmarks. Importantly, we compared all methods with the same algorithm designs (exploration, policy, etc.), since Q-Prop and TRPO are IPG-\u03bd=0 with and without the control variate. IPG-\u03bd=1 is a novel variant of the actor-critic method that differs from DDPG (Lillicrap et al., 2016) and SVG(0) (Heess et al., 2015) due to the use of a trust region. The results in Table 2 suggest that, in most cases, the best performing algorithm is one that interpolates between the policy-gradient and actor-critic variants, with intermediate values of \u03bd."}, {"heading": "7 Discussion", "text": "In this paper, we introduced interpolated policy gradient methods, a family of policy gradient algorithms that allow mixing off-policy learning with on-policy learning while satisfying performance bounds. This family of algorithms unifies and interpolates on-policy likelihood ratio policy gradient and off-policy deterministic policy gradient, and includes a number of prior works as approximate limiting cases. Empirical results confirm that, in many cases, interpolated gradients have improved sample-efficiency and stability over the prior state-of-the-art methods, and the theoretical results provide intuition for analyzing the cases in which the different methods perform well or poorly. Our hope is that this detailed analysis of interpolated gradient methods can not only provide for more effective algorithms in practice, but also give useful insight for future algorithm design."}, {"heading": "Acknowledgements", "text": "This work is supported by generous sponsorship from Cambridge-T\u00fcbingen PhD Fellowship, NSERC, and Google Faculty Award."}, {"heading": "8 Proof for Theorem 1", "text": ""}, {"heading": "8.1 Local approximation objective with bounded bias", "text": "In the main paper, we introduced the approximate objective J\u0303(\u03c0, \u03c0\u0303) to J(\u03c0) during our theoretical analysis. In this section, we discuss the motivations behind this choice, referencing prior work (Kakade & Langford, 2002; Schulman et al., 2015).\nFirst, the expected return J(\u03c0) of a policy \u03c0 can be written as the sum of the expected return J(\u03c0\u0303) of another policy \u03c0\u0303 and the expected advantage term between the two policies in the equation, where A\u03c0\u0303(st, at) is the advantage of policy \u03c0\u0303,\nJ(\u03c0) = J(\u03c0\u0303) + E\u03c1\u03c0,\u03c0[A\u03c0\u0303(st, at)].\nFor the proof, see Lemma 1 in (Schulman et al., 2015). This expression is still not tractable to analyze because of the dependency of unnormalized state sampling distribution \u03c1\u03c0 on \u03c0. Kakade & Langford (2002); Schulman et al. (2015) thus introduce a local approximation by replacing \u03c1\u03c0 with \u03c1\u03c0 ,\nJ(\u03c0) \u2248 J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)] , J\u0303(\u03c0, \u03c0\u0303).\nWe can show that J(\u03c0) = J\u0303(\u03c0, \u03c0\u0303 = \u03c0) and\u2207\u03c0J(\u03c0) = \u2207\u03c0J\u0303(\u03c0, \u03c0\u0303 = \u03c0), meaning that the J(\u03c0) and J\u0303(\u03c0, \u03c0\u0303) match up to the first order. Schulman et al. (2015) then uses this property, in combination with minorization-maximization Hunter & Lange (2004), to derive a monotonic convergence proof for a variant of policy iteration algorithm. We also use this property to prove the monotonic convergence property in Algorithm 2, but we further derive the following lemma,\nLemma 3. If \u03b6 = maxs |A\u0304\u03c0,\u03c0\u0303(s)|, then\n\u2225\u2225\u2225J(\u03c0)\u2212 J\u0303(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 2\u03b6 \u03b3 (1\u2212 \u03b3)2D max TV (\u03c0\u0303, \u03c0) \u2264 2\u03b6\n\u03b3 (1\u2212 \u03b3)2 \u221a DmaxKL (\u03c0\u0303, \u03c0)\nProof. We define \u03c1\u03c0t (st) as the marginal state distribution at time t assuming that the agent follows policy \u03c0 from initial state \u03c10(st) at time t = 0. Note that from the definition of \u03c1\u03c0, \u03c1\u03c0(s) =\u2211\u221e t=0 \u03b3\nt\u03c1\u03c0t (st = s). We can use the following lemma from Kahn et al. (2016), which is adapted from Ross et al. (2011) and Schulman et al. (2015).\nLemma 4. (Kahn et al., 2016)\n\u2225\u2225\u2225\u03c1\u03c0t \u2212 \u03c1\u03b2t \u2225\u2225\u2225 1 \u2264 2tDmaxTV (\u03c0, \u03b2) \u2264 2t \u221a DmaxKL (\u03c0, \u03b2) (10)\nThe full proof is below, where A\u0304\u03c0,\u03c0\u0303(s) = E\u03c0[A\u03c0\u0303(st, at)] and A\u03c0\u0303(st, at) is the advantage function of \u03c0\u0303, \u2225\u2225\u2225J(\u03c0)\u2212 J\u0303(\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\n= \u2225\u2225E\u03c1\u03c0\u0303 [A\u0304\u03c0,\u03c0\u0303(s)]\u2212 E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303(s)] \u2225\u2225 1 \u2264 \u221e\u2211\nt=0\n\u03b3t \u2225\u2225\u2225E\u03c1\u03c0\u0303t [A\u0304 \u03c0,\u03c0\u0303(s)]\u2212 E\u03c1\u03c0t [A\u0304\u03c0,\u03c0\u0303(s)] \u2225\u2225\u2225 1\n\u2264 \u03b6 \u221e\u2211\nt=0\n\u03b3t \u2225\u2225\u03c1\u03c0\u0303t \u2212 \u03c1\u03c0t \u2225\u2225 1\n\u2264 2\u03b6( \u221e\u2211\nt=0\n\u03b3tt)DmaxTV (\u03c0\u0303, \u03c0)\n= 2\u03b6 \u03b3\n(1\u2212 \u03b3)2D max TV (\u03c0\u0303, \u03c0)\n\u2264 2\u03b6 \u03b3 (1\u2212 \u03b3)2\n\u221a DmaxKL (\u03c0, \u03c0\u0303).\n(11)\nThis lemma is crucial in our theoretical analysis, as it allows us to tractably bound the biases of the full spectrum of IPG objectives J\u0303\u03b2,\u03bd,CV (\u03c0, \u03c0\u0303) against J(\u03c0)."}, {"heading": "8.2 Main proof for Theorem 1", "text": "Proof. We first prove the bound for \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1 . Using Lemma 4, the bound is\ngiven below, with a similar derivation process as in Lemma 3. \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\n= \u2225\u2225J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 J(\u03c0\u0303)\u2212 E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212A\u03c0\u0303w(st, at)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 = \u2225\u2225E\u03c1\u03c0\u0303 [A\u0304\u03c0,\u03c0\u0303w (s)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (s)] \u2225\u2225 1 \u2264 \u221e\u2211\nt=0\n\u03b3t \u2225\u2225\u2225E\u03c1\u03c0\u0303t [A\u0304 \u03c0,\u03c0\u0303 w (s)]\u2212 E\u03c1\u03b2t [A\u0304 \u03c0,\u03c0\u0303 w (s)] \u2225\u2225\u2225 1\n\u2264 \u221e\u2211\nt=0\n\u03b3t \u2225\u2225\u2225\u03c1\u03c0\u0303t \u2212 \u03c1\u03b2t \u2225\u2225\u2225 1\n\u2264 2 ( \u221e\u2211\nt=0\n\u03b3tt)DmaxTV (\u03c0\u0303, \u03b2)\n= 2 \u03b3\n(1\u2212 \u03b3)2D max TV (\u03c0\u0303, \u03b2)\n\u2264 2 \u03b3 (1\u2212 \u03b3)2\n\u221a DmaxKL (\u03c0\u0303, \u03b2).\n(12)\nGiven this bound, we can directly derive the bound for \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1\nby combining with Lemma 3, \u2225\u2225\u2225J(\u03c0)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\u2225\u2225\u2225J(\u03c0)\u2212 J\u0303(\u03c0, \u03c0\u0303) + J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\n\u2264 \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 + \u2225\u2225\u2225J(\u03c0)\u2212 J\u0303(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1\n\u2264 2 \u03b3 (1\u2212 \u03b3)2\n( \u221a DmaxKL (\u03c0\u0303, \u03b2) + \u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) )\n(13)"}, {"heading": "9 Proof for Monotonic Convergence in Algorithm 2", "text": "We can prove that the algorithm guaranteeing monotonic improvement by first introducing the following corollary, Corollary 1.\nJ(\u03c0) \u2265M(\u03c0, \u03c0\u0303) \u2265M\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303), J(\u03c0\u0303) = M(\u03c0\u0303, \u03c0\u0303) = M\u03b2,\u03bd=0,CV (\u03c0\u0303, \u03c0\u0303) (14) where\nM(\u03c0, \u03c0\u0303) = J\u0303(\u03c0, \u03c0\u0303)\u2212 C\u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) M\u03b2,\u03bd=0,CV (\u03c0, \u03c0\u0303) = J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303)\u2212 C(\u03b6 \u221a DmaxKL (\u03c0, \u03c0\u0303) + \u221a DmaxKL (\u03c0\u0303, \u03b2))\nC = 2\u03b3\n(1\u2212 \u03b3)2 , \u03b6 = maxs |A\u0304 \u03c0,\u03c0\u0303(s)|, = max s |A\u0304\u03c0,\u03c0\u0303w (s)|\nProof. It follows from Theorem 1 in the main text and Theorem 1 in Schulman et al. (2015). J(\u03c0\u0303) = M\u03b2,\u03bd=0,CV (\u03c0\u0303, \u03c0\u0303) since \u03b6 = = 0 when \u03c0 = \u03c0\u0303.\nGiven Corollary 1, we use minorization-maximization (MM) (Hunter & Lange, 2004) to derive Algorithm 2 in the main text, a policy iteration algorithm that allows using off-policy samples while guaranteeing monotonic improvement on J(\u03c0). MM suggests that at each iteration, by maximizing the lower bound, or the minorizer, of the objective, the algorithm can guarantee monotonic improvement: J(\u03c0i+1) \u2265 M\u03b2i,\u03bd=0,CV (\u03c0i+1, \u03c0i) \u2265 M\u03b2i,\u03bd=0,CV (\u03c0i, \u03c0i) = J(\u03c0i), where \u03c0i+1 \u2190 arg max\u03c0M\u03b2i,\u03bd=0,CV (\u03c0, \u03c0i). Importantly, the algorithm guarantees monotonic improvement regardless of the off-policy distribution \u03b2i or the performance of the critic Qw. This result is a step toward achieving off-policy policy gradient with convergence guarantee of on-policy algorithms.1\nWe compare our theoretical algorithm with Algorithm 1 in Schulman et al. (2015), which guarantees monotonic improvement in a general on-policy policy gradient algorithm. The main difference is the additional term, \u2212C \u221a DmaxKL (\u03c0\u0303, \u03b2) to the lower bound. D max KL (\u03c0\u0303, \u03b2) is constant with respect to \u03c0, while = 0 if \u03c0 = \u03c0\u0303 and \u2265 0 if otherwise. This suggests that as \u03b2 becomes more off-policy, the gap between the lower bound and the true objective widens, proportionally to \u221a DmaxKL (\u03c0\u0303, \u03b2). This may make each majorization step end in a place very close to where it started, i.e. \u03c0i+1 very close to \u03c0i, and slow down learning. This again suggests a trade-off that comes in as off-policy samples are used."}, {"heading": "10 Proof for Theorem 2", "text": "We follow the same procedure as the proof for Theorem 1, where we first derive bounds between J\u0303(\u03c0, \u03c0\u0303) and the other local objectives, and then combine the results with Lemma 3.\nTo begin the proof, we first derive the bound for the special case where \u03bd = 1. Having \u03bd = 1, we remove the likelihood ratio policy gradient term, and get the following gradient expression,\n\u2207\u03b8J(\u03b8) \u2248 E\u03c1\u03b2 [\u2207\u03b8Q\u0304\u03c0w(st)]. (15) This is an off-policy actor-critic algorithm, and is closely connected to DDPG (Lillicrap et al., 2016), except that it does not use target policy network and its use of a stochastic policy enables on-policy exploration, trust-region policy updates, and no heuristic additive exploration noise.\nWe can introduce the following bound on the local objective J\u0303\u03b2,\u03bd=1(\u03c0, \u03c0\u0303), whose policy gradient equals 15 at \u03c0 = \u03c0\u0303, similarly to the proof for Theorem 1 in the main text. Corollary 2. If \u03b4 = maxs,a |A\u03c0\u0303(s, a)\u2212A\u03c0\u0303w(s, a)|, = maxs |A\u0304\u03c0,\u03c0\u0303w (s)|, and\nJ\u0303\u03b2,\u03bd=1(\u03c0, \u03c0\u0303) = J(\u03c0\u0303) + E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] (16)\n1Schulman et al. (2015) applies additional bound, \u2265 2 \u2032 \u221a DmaxKL (\u03c0, \u03c0\u0303) where\n\u2032 = maxs,a |A\u03c0\u0303w(s, a)| to remove dependency on \u03c0. In our case, we cannot apply such bound on \u03b6, since then the inequality in Theorem 1 is still satisfied but the equality is violated, and thus the algorithm no longer guarantees monotonic improvement.\nthen, \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=1(\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1 \u2264 \u03b4 1\u2212 \u03b3 + 2 \u03b3 (1\u2212 \u03b3)2 \u221a DmaxKL (\u03c0\u0303, \u03b2) (17)\nProof. We note that \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=1(\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\n= \u2225\u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212A\u03c0\u0303w(st, at)] + J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212A\u03c0\u0303w(st, at)] \u2225\u2225 1 + \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 \u2264 \u221e\u2211\nt=0\n\u03b3t \u2225\u2225\u2225E\u03c1\u03c0\u0303t ,\u03c0[A \u03c0\u0303(st, at)\u2212A\u03c0\u0303w(st, at)] \u2225\u2225\u2225 1 + \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1\n\u2264 \u03b4 \u221e\u2211\nt=0\n\u03b3t + \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1\n= \u03b4 1\u2212 \u03b3 + \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd=0(\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1\n\u2264 \u03b4 1\u2212 \u03b3 + 2\n\u03b3 (1\u2212 \u03b3)2 \u221a DmaxKL (\u03c0\u0303, \u03b2),\n(18)\nwhere the proof uses Theorem 1 at the last step.\nGiven Corollary 2 and Theorem 1, we are ready to prove the two bounds in Theorem 2.\nProof. \u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd(\u03c0, \u03c0\u0303)\n\u2225\u2225\u2225 1\n= \u2225\u2225J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 J(\u03c0\u0303)\u2212 (1\u2212 \u03bd)E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 \u03bdE\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 = \u03bd \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 = \u03bd \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)] + E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 \u2264 \u03bd \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 + \u03bd \u2225\u2225E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 = \u03bd \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212 A\u0304\u03c0\u0303w(st, at)] \u2225\u2225 1 + \u03bd \u2225\u2225E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1\n\u2264 \u03bd\u03b4 1\u2212 \u03b3 + 2\n\u03bd\u03b3 (1\u2212 \u03b3)2 \u221a DmaxKL (\u03c0\u0303, \u03b2)\n\u2225\u2225\u2225J\u0303(\u03c0, \u03c0\u0303)\u2212 J\u0303\u03b2,\u03bd,CV (\u03c0, \u03c0\u0303) \u2225\u2225\u2225 1 = \u2225\u2225J(\u03c0\u0303) + E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 J(\u03c0\u0303)\u2212 (1\u2212 \u03bd)E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)\u2212 A\u0304\u03c0\u0303w(st, at)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 = \u2225\u2225\u03bd(E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]) + E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 \u2264 \u03bd \u2225\u2225E\u03c1\u03c0\u0303,\u03c0[A\u03c0\u0303(st, at)]\u2212 E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1 + \u2225\u2225E\u03c1\u03c0 [A\u0304\u03c0,\u03c0\u0303w (st)]\u2212 E\u03c1\u03b2 [A\u0304\u03c0,\u03c0\u0303w (st)] \u2225\u2225 1\n\u2264 \u03bd\u03b4 1\u2212 \u03b3 + 2\n\u03b3 (1\u2212 \u03b3)2 \u221a DmaxKL (\u03c0\u0303, \u03b2).\n(19)\nWe combine these bounds with Lemma 3 to conclude the proof."}, {"heading": "11 Control Variates for Policy Gradient", "text": "In this Section, we describe control variate choices for policy gradient methods other than the first-order Taylor expansion presented in Q-Prop (Gu et al., 2017)."}, {"heading": "11.1 Reparameterized Critic Control Variate", "text": "If action is continuous and the policy is a simple distribution such as a Gaussian, one option is to use the full Qw as the control variate and use Monte Carlo to estimate its expectation with respect to the policy. To reduce the variance of the correction term, we estimate the policy gradient, the reparameterization trick (Kingma & Welling, 2014) can be employed to reduce the variance. For a Gaussian policy \u03c0\u03b8(at|st) = N (\u00b5\u03b8(st), \u03c3\u03b8(st)),\nQ\u0304\u03c0w(st) = E\u03c0[Qw(st, at)] = E \u223cN (0,1)[Qw(st, \u00b5\u03b8(st) + \u03c3(st))]\n\u2248 1 m\nm\u2211\ni=1\nQw(st, \u00b5\u03b8(st) + i\u03c3(st)). (20)"}, {"heading": "11.2 Discrete Critic Control Variate", "text": "Let \u03c0\u03b8(st) \u2208 Rk denote a probability vector over k discrete actions, and Qw(st) \u2208 Rk denote the action-value function for the k actions, as in DQN (Mnih et al., 2015).\nQ\u0304\u03c0w(st) = \u03c0\u03b8(st) T \u00b7Qw(st). (21)"}, {"heading": "11.3 NAF Critic Control Variate", "text": "For continuous control, it is also possible to use a more general critic. If the policy is locally Gaussian, i.e. \u03c0\u03b8(at|st) = N (\u00b5\u03b8(st),\u03a3\u03b8(st)), then the quadratic Qw from Normalized Advantage Function (NAF) (Gu et al., 2016) can be directly used,\nQw(st, at) = Aw(st, at) + Vw(st)\nAw(st, at) = \u2212 1\n2 (at \u2212 \u00b5w(st))TPw(st)(at \u2212 \u00b5w(st)).\n(22)\nThe deterministic policy gradient expression leads to,\nQ\u0304\u03c0w(st) = Vw(st)\u2212 1\n2 Tr(Pw(st)\u03a3\u03b8(st))\n\u2212 1 2\n(\u00b5\u03b8(st)\u2212 \u00b5w(st))TPw(st)(\u00b5\u03b8(st)\u2212 \u00b5w(st)). (23)"}, {"heading": "12 Supplementary Experimental Details", "text": ""}, {"heading": "12.1 Hyperparameters", "text": "GAE(\u03bb = 0.97) (Schulman et al., 2016) is used for A\u0302 estimation. Trust-region update in TRPO is used as the policy optimizer (Schulman et al., 2015). The standard Q-fitting routine from DDPG (Lillicrap et al., 2016) is used for fitting Qw, where Qw is trained with batch size 64, using experience replay of size 1e6, and target network with \u03c4 = 0.001. ADAM (Kingma & Ba, 2014) is used as the optimizer for Qw. Policy network parametrizes a Gaussian policy with \u03c0\u03b8(at|st) = N (\u00b5\u03b8(st),\u03a3\u03b8), where \u00b5\u03b8 is a two-hidden-layer neural network of size 100\u2212 50 and tanh hidden nonlinearity and linear output, and \u03a3\u03b8 is a diagonal, state-independent variance. For DDPG, the policy network is deterministic and additionally has tanh activation at the output layer. The critic function Qw is a two-hidden-layer neural network of size 100 \u2212 100 with ReLU activation. We use the first-order Taylor expansion of Qw as the control variate, as in Q-Prop (Gu et al., 2017), except for \u03bd = 1 cases we use the reparametrized control variate described below. For IPG methods with the control variates, we further explored the standard and conservative variants, the technique proposed in Q-Prop (Gu et al., 2017), and the Taylor expansion variant with the reparametrized variant discussed in Section 11.1.\nThe trust-region step size for policy update is fixed to 0.1 for HalfCheetah-v1 and Humanoid-v1, and 0.01 for Ant-v1 and Walker2d-v1, while the learning rate for ADAM in critic update is fixed to 1e\u22124 for HalfCheetah-v1, Ant-v1, Humanoid-v1, and 1e\u2212 3 for Walker2d-v1. Those two hyperparameters are found by first running TRPO and DDPG on each domain, and picking the ones that give best performance for each domain. These parameters are fixed throughout the experiment to ensure fair comparisons.\nFor all IPG algorithms, we use the first-order Taylor expansion control variate, as in Q-Prop, while for \u03bd = 1, we use the reparameterized control variate in Section 11.1 with Monte Carlo sample size m = 1. The first-order Taylor expansion control variate cannot be used with \u03bd = 1 directly, since then it does not provide the gradient for training the variance term of the policy.\nThe plots in the main text present the mean returns as solid lines, scatter plots of all runs in the background to visualize variability."}, {"heading": "12.2 Additional Plot", "text": "Figure 2 shows additional plot on Humanoid-v1."}], "references": [{"title": "Off-policy actor-critic", "author": ["Degris", "Thomas", "White", "Martha", "Sutton", "Richard S"], "venue": "arXiv preprint arXiv:1205.4839,", "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Sutskever", "Ilya", "Levine", "Sergey"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2017}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Heess", "Nicolas", "Wayne", "Gregory", "Silver", "David", "Lillicrap", "Tim", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "A tutorial on mm algorithms", "author": ["Hunter", "David R", "Lange", "Kenneth"], "venue": "The American Statistician,", "citeRegEx": "Hunter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hunter et al\\.", "year": 2004}, {"title": "Doubly robust off-policy value evaluation for reinforcement learning", "author": ["Jiang", "Nan", "Li", "Lihong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Jiang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "On a connection between importance sampling and the likelihood ratio policy gradient", "author": ["Jie", "Tang", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jie et al\\.", "year": 2010}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["Kahn", "Gregory", "Zhang", "Tianhao", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "Kahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kahn et al\\.", "year": 2016}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Guided policy search", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Levine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["Mahmood", "A Rupam", "van Hasselt", "Hado P", "Sutton", "Richard S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc G"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Pgq: Combining policy gradient and q-learning", "author": ["O\u2019Donoghue", "Brendan", "Munos", "Remi", "Kavukcuoglu", "Koray", "Mnih", "Volodymyr"], "venue": null, "citeRegEx": "O.Donoghue et al\\.,? \\Q2017\\E", "shortCiteRegEx": "O.Donoghue et al\\.", "year": 2017}, {"title": "Learning from scarce experience", "author": ["Peshkin", "Leonid", "Shelton", "Christian R"], "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Peshkin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2002}, {"title": "Relative entropy policy search", "author": ["Peters", "Jan", "M\u00fclling", "Katharina", "Altun", "Yasemin"], "venue": "In AAAI. Atlanta,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Precup", "Doina"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "Precup and Doina.,? \\Q2000\\E", "shortCiteRegEx": "Precup and Doina.", "year": 2000}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Riedmiller", "Martin"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Riedmiller and Martin.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller and Martin.", "year": 2005}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["Silver", "David", "Lever", "Guy", "Heess", "Nicolas", "Degris", "Thomas", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Bias in natural actor-critic algorithms", "author": ["Thomas", "Philip"], "venue": "In ICML, pp", "citeRegEx": "Thomas and Philip.,? \\Q2014\\E", "shortCiteRegEx": "Thomas and Philip.", "year": 2014}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["Thomas", "Philip", "Brunskill", "Emma"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Thomas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2016}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Sample efficient actor-critic with experience replay", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "J(\u03c0\u0303) = M (\u03c0\u0303, \u03c0\u0303) since \u03b6 = = 0 when \u03c0 = \u03c0\u0303. Given Corollary 1, we use minorization-maximization (MM) (Hunter & Lange, 2004) to derive Algorithm 2 in the main text, a policy iteration algorithm that allows using off-policy samples", "author": ["Schulman"], "venue": null, "citeRegEx": "Schulman,? \\Q2015\\E", "shortCiteRegEx": "Schulman", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 14, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 13, "context": "Some of these include robotic control (Schulman et al., 2016; Lillicrap et al., 2016; Levine et al., 2016), computer games (Mnih et al.", "startOffset": 38, "endOffset": 106}, {"referenceID": 16, "context": ", 2016), computer games (Mnih et al., 2015), and board games (Silver et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 27, "context": ", 2015), and board games (Silver et al., 2016).", "startOffset": 25, "endOffset": 46}, {"referenceID": 28, "context": "Off-policy algorithms based on Q-learning and actor-critic learning (Sutton et al., 1999) have also proven to be an effective approach to deep reinforcement learning such as in (Mnih et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 16, "context": ", 1999) have also proven to be an effective approach to deep reinforcement learning such as in (Mnih et al., 2015) and (Lillicrap et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 14, "context": ", 2015) and (Lillicrap et al., 2016).", "startOffset": 12, "endOffset": 36}, {"referenceID": 3, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 32, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 18, "context": "Most recent research has worked with on-policy algorithms or off-policy algorithms, a few recent methods have sought to make use of both on- and off-policy data for learning (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 174, "endOffset": 235}, {"referenceID": 32, "context": "The first approach is to mix some ratio of on- and off-policy gradients or update steps in order to update a policy, as in the ACER and PGQ algorithms (Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 151, "endOffset": 195}, {"referenceID": 18, "context": "The first approach is to mix some ratio of on- and off-policy gradients or update steps in order to update a policy, as in the ACER and PGQ algorithms (Wang et al., 2017; O\u2019Donoghue et al., 2017).", "startOffset": 151, "endOffset": 195}, {"referenceID": 3, "context": "In the second approach, an off-policy Q critic is trained but is used as a control variate to reduce on-policy gradient variance, as in the Q-prop algorithm (Gu et al., 2017).", "startOffset": 157, "endOffset": 174}, {"referenceID": 3, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 32, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 18, "context": "We show that a number of recent methods (Gu et al., 2017; Wang et al., 2017; O\u2019Donoghue et al., 2017) can be viewed as special cases of this more general family.", "startOffset": 40, "endOffset": 101}, {"referenceID": 24, "context": "Monte Carlo policy gradient methods, such as REINFORCE (Williams, 1992) and TRPO (Schulman et al., 2015), use the likelihood ratio policy gradient of the RL objective,", "startOffset": 81, "endOffset": 104}, {"referenceID": 28, "context": "Policy gradient methods with function approximation (Sutton et al., 1999), or actor-critic methods, are a family of policy gradient methods which first estimate the critic, or the value, of the policy by Qw \u2248 Q , and then greedily optimize the policy \u03c0\u03b8 with respect to Qw.", "startOffset": 52, "endOffset": 73}, {"referenceID": 0, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 4, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 14, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016).", "startOffset": 118, "endOffset": 201}, {"referenceID": 0, "context": "While it is not necessary for such algorithms to be off-policy, we primarily analyze the off-policy variants, such as (Riedmiller, 2005; Degris et al., 2012; Heess et al., 2015; Lillicrap et al., 2016). For example, DDPG Lillicrap et al. (2016), which optimizes a continuous deterministic policy \u03c0\u03b8(at|st) = \u03b4(at = \u03bc\u03b8(st)), can be summarized by the following update equations, where Qw denotes the target Q network (Lillicrap", "startOffset": 137, "endOffset": 245}, {"referenceID": 24, "context": "\u03b2 \u03bd CV Examples - 0 No REINFORCE (Williams, 1992),TRPO (Schulman et al., 2015) \u03c0 0 Yes Q-Prop (Gu et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 3, "context": ", 2015) \u03c0 0 Yes Q-Prop (Gu et al., 2017) - 1 - DDPG (Silver et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 26, "context": ", 2017) - 1 - DDPG (Silver et al., 2014; Lillicrap et al., 2016),SVG(0) (Heess et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 14, "context": ", 2017) - 1 - DDPG (Silver et al., 2014; Lillicrap et al., 2016),SVG(0) (Heess et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 4, "context": ", 2016),SVG(0) (Heess et al., 2015) 6= \u03c0 - No \u2248PGQ (O\u2019Donoghue et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": ", 2015) 6= \u03c0 - No \u2248PGQ (O\u2019Donoghue et al., 2017), \u2248ACER (Wang et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 32, "context": ", 2017), \u2248ACER (Wang et al., 2017)", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "This usually makes off-policy algorithms less stable during learning, compared to on-policy algorithms using a large batch size for each update (Duan et al., 2016; Gu et al., 2017).", "startOffset": 144, "endOffset": 180}, {"referenceID": 3, "context": "This usually makes off-policy algorithms less stable during learning, compared to on-policy algorithms using a large batch size for each update (Duan et al., 2016; Gu et al., 2017).", "startOffset": 144, "endOffset": 180}, {"referenceID": 3, "context": "Q-Prop (Gu et al., 2017), for example, used Q\u0303w, the first-order Taylor expansion of Qw, as the control variates, and showed improvement in stability and sample efficiency of policy gradient methods.", "startOffset": 7, "endOffset": 24}, {"referenceID": 24, "context": "However, as we show in Section 4, we can bound the biases for all the cases, and in some cases, the algorithm still guarantees monotonic convergence as in Kakade & Langford (2002); Schulman et al. (2015).", "startOffset": 181, "endOffset": 204}, {"referenceID": 14, "context": "Prior work such as DDPG Lillicrap et al. (2016) and related Q-learning methods have proposed aggressive off-policy exploration strategy to exploit these properties of the algorithm.", "startOffset": 24, "endOffset": 48}, {"referenceID": 24, "context": "To analyze the bias for this gradient expression, we first introduce J\u0303(\u03c0, \u03c0\u0303), a local approximation to J(\u03c0), which has been used in prior theoretical work (Kakade & Langford, 2002; Schulman et al., 2015).", "startOffset": 157, "endOffset": 205}, {"referenceID": 20, "context": "This means that the algorithm fits well with policy gradient methods which constrain the KL divergence per policy update, such as covariant policy gradient (Bagnell & Schneider, 2003), natural policy gradient (Kakade & Langford, 2002), REPS (Peters et al., 2010), and trust-region policy optimization (TRPO) (Schulman et al.", "startOffset": 241, "endOffset": 262}, {"referenceID": 24, "context": ", 2010), and trust-region policy optimization (TRPO) (Schulman et al., 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 24, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 1, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 3, "context": "Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al., 2015; Duan et al., 2016; Gu et al., 2017).", "startOffset": 148, "endOffset": 207}, {"referenceID": 21, "context": "1 Monotonic Policy Improvement Guarantee Some forms of on-policy policy gradient methods have theoretical guarantees on monotonic convergence Kakade & Langford (2002); Schulman et al. (2015). Such guarantees often correspond to stable empirical performance on challenging problems, even when some of the constraints are relaxed in practice (Schulman et al.", "startOffset": 168, "endOffset": 191}, {"referenceID": 1, "context": ", 2015; Duan et al., 2016; Gu et al., 2017). We can show that Algorithm 2, which is a variant of IPG, guarantees monotonic convergence. The proof is provided in the appendix. Algorithm 2 is often impractical to implement; however, IPG with trust-region updates when \u03b2 6= \u03c0, \u03bd = 1,CV = true approximates this monotonic algorithm, similar to how TRPO is an approximation to the theoretically monotonic algorithm proposed by Schulman et al. (2015).", "startOffset": 8, "endOffset": 445}, {"referenceID": 3, "context": "Our analysis examines how Q-Prop (Gu et al., 2017), PGQ (O\u2019Donoghue et al.", "startOffset": 33, "endOffset": 50}, {"referenceID": 18, "context": ", 2017), PGQ (O\u2019Donoghue et al., 2017), and ACER (Wang et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 32, "context": ", 2017), and ACER (Wang et al., 2017), which are all recent works that combine on-policy with off-policy learning, are connected to each other (see Table 1).", "startOffset": 18, "endOffset": 37}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013).", "startOffset": 130, "endOffset": 219}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013). Most of these methods rely on variants of importance sampling (IS) to correct for bias. The use of importance sampling ensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure used by Jie & Abbeel (2010). Ignoring importance weights produces bias but, as shown in our analysis, this bias can be bounded.", "startOffset": 176, "endOffset": 471}, {"referenceID": 0, "context": "Asides from these more recent works, the use of off-policy samples with policy gradients has been a popular direction of research (Peshkin & Shelton, 2002; Jie & Abbeel, 2010; Degris et al., 2012; Levine & Koltun, 2013). Most of these methods rely on variants of importance sampling (IS) to correct for bias. The use of importance sampling ensures unbiased estimates, but at the cost of considerable variance, as quantified by the ESS measure used by Jie & Abbeel (2010). Ignoring importance weights produces bias but, as shown in our analysis, this bias can be bounded. Therefore, our IPG estimators have higher bias as the sampling distribution deviates from the policy, while IS methods have higher variance. Among these importance sampling methods, Levine & Koltun (2013) evaluates on tasks that are the most similar to our paper, but the focus is on using importance sampling to include demonstrations, rather than to speed up learning from scratch.", "startOffset": 176, "endOffset": 776}, {"referenceID": 15, "context": "Lastly, there are many methods that combine on- and off-policy data for policy evaluation (Precup, 2000; Mahmood et al., 2014; Munos et al., 2016), mostly through variants of importance sampling.", "startOffset": 90, "endOffset": 146}, {"referenceID": 17, "context": "Lastly, there are many methods that combine on- and off-policy data for policy evaluation (Precup, 2000; Mahmood et al., 2014; Munos et al., 2016), mostly through variants of importance sampling.", "startOffset": 90, "endOffset": 146}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012).", "startOffset": 125, "endOffset": 146}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al.", "startOffset": 126, "endOffset": 310}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al.", "startOffset": 126, "endOffset": 331}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 354}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 373}, {"referenceID": 0, "context": "Combining our methods with more sophisticated policy evaluation methods will likely lead to further improvements, as done in (Degris et al., 2012). A more detailed analysis of the effect of importance sampling on bias and variance is left to future work, where some of the relevant work includes Precup (2000); Jie & Abbeel (2010); Mahmood et al. (2014); Jiang & Li (2016); Thomas & Brunskill (2016).", "startOffset": 126, "endOffset": 400}, {"referenceID": 31, "context": "All experiments were performed on MuJoCo domains in OpenAI Gym (Todorov et al., 2012; Brockman et al., 2016), with results presented for the average over three seeds.", "startOffset": 63, "endOffset": 108}, {"referenceID": 16, "context": "This is the original motivation for experience replay in the DQN method (Mnih et al., 2015), and we have shown that such decorrelated off-policy samples can similarly produce gains for policy gradient algorithms.", "startOffset": 72, "endOffset": 91}, {"referenceID": 14, "context": "Although IPG-\u03bd=1 methods can be off-policy, the policy is updated every 5000 samples to keep it consistent with other IPG methods, while DDPG updates the policy on every step in the environment and makes other design choices Lillicrap et al. (2016). We see that, in this domain, standard DDPG becomes stuck with a mean reward of 1000, while IPG-\u03bd=1 improves monotonically, achieving a significantly better result.", "startOffset": 225, "endOffset": 249}, {"referenceID": 18, "context": "This is consistent with the results in PGQ (O\u2019Donoghue et al., 2017) and ACER (Wang et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 32, "context": ", 2017) and ACER (Wang et al., 2017), which found that their equivalent of \u03bd = 0.", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": "IPG-\u03bd=1 is a novel variant of the actor-critic method that differs from DDPG (Lillicrap et al., 2016) and SVG(0) (Heess et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": ", 2016) and SVG(0) (Heess et al., 2015) due to the use of a trust region.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging onand off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.", "creator": "LaTeX with hyperref package"}}}