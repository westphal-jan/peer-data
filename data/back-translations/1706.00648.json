{"id": "1706.00648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "abstract": "We see data flow matrix machines as a complete Turing generalization of recursive neural networks and as a programming platform. We describe vector space of finite prefix trees with numerical sheets, which allows us to combine the expressiveness of data flow matrix machines with the simplicity of traditional recursive neural networks.", "histories": [["v1", "Wed, 3 May 2017 13:46:05 GMT  (79kb)", "http://arxiv.org/abs/1706.00648v1", "6 pages, accepted for presentation at LearnAut 2017: Learning and Automata workshop at LICS (Logic in Computer Science) 2017 conference. Preprint original version: April 9, 2017; minor correction: May 1, 2017"]], "COMMENTS": "6 pages, accepted for presentation at LearnAut 2017: Learning and Automata workshop at LICS (Logic in Computer Science) 2017 conference. Preprint original version: April 9, 2017; minor correction: May 1, 2017", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.PL", "authors": ["michael bukatin", "jon anthony"], "accepted": false, "id": "1706.00648"}, "pdf": {"name": "1706.00648.pdf", "metadata": {"source": "CRF", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "authors": ["Michael Bukatin", "Jon Anthony"], "emails": ["bukatin@cs.brandeis.edu", "jsa.aerial@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n00 64\n8v 1\n[ cs\n.N E\n] 3\nM ay\ncomplete generalization of recurrent neural networks and as\na programming platform. We describe vector space of finite\nprefix trees with numerical leaves which allows us to combine\nexpressive power of dataflow matrix machines with simplicity\nof traditional recurrent neural networks."}, {"heading": "1. Introduction", "text": "When one considers a Turing complete generalization of recurrent neural networks (RNNs), four groups of questions arise naturally: a) what is the mechanism providing access to unbounded memory; b) what is the pragmatic power of the available primitives, and is the resulting platform suitable for crafting software manually, rather than only serving as compilation and machine learning target; c) what are selfreferential (and self-modification) mechanisms if any; d) what are the implications for machine learning.\nIn Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].\nDataflow matrix machines are much closer to being a general-purpose programming platform than RNNs, while retaining the key property of RNNs that large classes of programs can be parametrized by matrices of numbers, and therefore synthesizing appropriate matrices is sufficient to synthesize programs.\nIn Section 3 we describe the formalism based on the vector space of finite prefix trees with numerical leaves which is used in our current Clojure implementation of the core primitives of dataflow matrix machines [5].\nThe concluding Section 4 discusses some of possible uses of dataflow matrix machines in machine learning."}, {"heading": "2. Dataflow Matrix Machines: an Overview", "text": ""}, {"heading": "2.1. Countable-sized Nets with Finite Active Part", "text": "One popular approach to providing Turing complete generalizations of RNNs with unbounded memory is to use an RNN as a controller to a Turing machine tape or another model of external memory [9], [8], [17].\nAnother approach is to allow reals of unlimited precision, in effect using a binary expansion of a real number as a tape of a Turing machine [16].\nDataflow matrix machines take a different approach. One considers a countable-sized RNN, and therefore a countable matrix of connectivity weights, but with a condition that only finite number of those weights are non-zero at any given moment of time.\nAt any given moment of time, only those neurons are active which have at least one non-zero connectivity weight associated with them. Therefore only a finite part of the network is active at any given time.\nMemory and network capacity can be dynamically added by gradually making more weights to become non-zero [2]."}, {"heading": "2.2. Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks", "text": "The essence of neural models of computations is to interleave generally non-linear, but relatively local computations performed by the activation functions built into neurons, and linear, but potentially quite global computations recomputing neuron inputs from the outputs of various neurons.\nThe metaphor of a \u201ctwo-stroke engine\u201d is applicable to traditional RNNs. On the \u201cup movement\u201d, the activation functions built into neurons are applied to the inputs of the neurons and produce the next values of the output streams of the neurons. On the \u201cdown movement\u201d, the matrix of connectivity weights (network matrix) is applied to the (concatenation of the) vector of neuron outputs (and the vector of network inputs) and produces the (concatenation of the) vector of the next values of the input streams of all neurons (and the vector of network outputs). This \u201ctwostroke cycle\u201d is repeated indefinitely (Fig. 1).\nDataflow matrix machines (DMMs) attempt to generalize RNNs as much as possible, while preserving this structure of the \u201ctwo-stroke engine\u201d. In particular, the key element to be preserved is the ability to apply the matrix of connectivity weights to the collection of neuron outputs, hence the notion of linear combination must be defined.\nRNNs work with streams of numbers. DMMs work with streams of approximate representations of arbitrary vectors (linear streams). One considers a finite or countable collection of kinds of linear streams. With every kind of\n1\n, . . . , xt+1\nk\n, ot+1\n1\n, . . . , ot+1n )\n\u22a4 = W\u00b7(yt1, . . . , y\nt\nk\n, it1, . . . , i\nt\nn)\n\u22a4.\n\u201cUp movement\u201d: yt+1 1 = f1(x t+1 1 ), . . . , yt+1 k = fk(x t+1 k ).\nlinear stream k, one associates a vector space Vk and a way to compute an approximate representation of vector \u03b11v1,k + . . .+ \u03b1nvn,k from approximate representations of vectors v1,k, . . . , vn,k.\nA neuron type has a non-negative integer input arity I , a non-negative integer output arity J , kinds of linear streams i1, . . . , iI and j1, . . . , jJ associated with neuron inputs and outputs, and an activation function associated with this neuron type. In the simplest version, the activation function maps Vi1\u00d7. . .\u00d7ViI to Vj1\u00d7. . .\u00d7VjJ . In reality, one needs to consider the fact that one works not with vectors, but with their approximate representations, that activation functions might be stochastic, etc. In particular, Appendix A discusses how linear streams of probabilistic samples fit this framework.\nOne considers a finite or countable collection of neuron types, and a countable set of neurons of each type. For each output of each neuron, and for each input of each neuron, the network matrix has a weight coefficient connecting them. At any given time, only finite number of those coefficients can be non-zero, and moreover only weight coefficients connecting outputs and inputs which have the same kind of linear streams associated with them are allowed to be non-zero (a type correctness condition). See Fig. 2 in Appendix C.1."}, {"heading": "2.3. Pragmatic Power of DataflowMatrix Machines as a Programming Platform", "text": "The pragmatic power of dataflow matrix machines is considerably higher than the pragmatic power of vanilla RNNs [3]. The ability to handle streams of sparse representations of arrays is instrumental for the ability to implement various algorithms based on hash maps and similar structures without extra runtime and memory overhead.\nNeurons with linear activation functions such as identity allow us to implement memory primitives such as accumulators, leaky accumulators, etc.\nThe ability to have multiple inputs allows us to have multiplicative neurons implementing mechanisms for gating (\u201cmultiplicative masks\u201d), which serve as fuzzy conditionals and can be used to attenuate and redirect flows of data in the network [14]. Multiplicative neurons are implicitly present\nin modern recurrent neural network architectures such as LSTM and Gated Recurrent Unit networks (Appendix C of [4]).\nThe sparseness structure of the network matrix can be used to sculpt the layered structure and other topological features of the network, and multiplicative neurons can be used to orchestrate multilayered computations by silencing particular layers at appropriate moments of time.\nThe fact that streams of samples can be used to represent streams of probability distributions and signed measures allows us to incorporate certain streams of non-vector objects without explicitly embedding those objects into vector spaces.\nThe ability to handle streams of arbitrary vectors and to have arbitrary input and output arities considerably increases the ability to structure and modularize the resulting networks and programs."}, {"heading": "2.4. Self-referential Mechanism", "text": "There is a history of research studies suggesting that it might be fruitful for a neural network to be able to reference and update its own weights [15]. However, doing this with standard neural networks based on scalar streams is difficult. One has to update the network matrix on per-element basis, and one needs to encode the location of matrix elements (row and column indices) within real numbers. This often results in rather complicated and fragile structures, highly sensitive to small changes of parameters.\nWhen networks can process arbitrary linear streams, self-referential mechanisms become much easier. One simply incorporates neurons processing streams of matrices, requiring those matrices to have shapes appropriate for network matrices in a given context. Then one can dedicate a particular neuron Self and use its latest output as the network matrix [2].\nThe Self neuron is typically implemented as an accumulator, allowing it to take incremental updates from the other matrix outputting neurons in the network.\nAppendix B presents a self-contained simple example of a self-referential dynamical system, where our basic network matrix update mechanism together with a few constant update matrices produce a wave pattern of connectivity weights dynamically propagating within the network matrix.\nThe updating neurons can access the network matrix via their inputs, which allows them to perform sophisticated computations. For example, one can have updating neurons creating deep copies of network subgraphs and use those to build pseudo-fractal structures in the body of the network [3].\nOne can argue that the ability of the network to transform the matrix defining the topology and weights of this network plays a fundamental role in the context of programming with linear streams, similar to the role of \u03bb-calculus in the context of programming via string rewriting [4]."}, {"heading": "3. Dataflow Matrix Machines Based on the Vector Space Generated by Finite Strings", "text": "The powerful setup described above involves relatively high level of design complexity. There are many kinds of linear streams, there are many types of neurons, each neuron type has its own input and output arity, and a particular kind of linear streams is associated with each of its inputs and outputs.\nThis is quite normal in the world of typed programming languages, but it is inconvenient for Lisp-based frameworks. It also feels more biorealistic not to have strong constraints and to be able to sculpt and restructure the networks on the fly at runtime, and to run those networks without fear of runtime exceptions.\nIt turns out that one can build a setup of sufficient generality based on a single vector space, and that moreover this vector space is expressive enough to represent activation functions of variable input and output arities via transformations having one input and one output."}, {"heading": "3.1. Vector Space V", "text": "3.1.1. Finite Linear Combinations of Finite Strings. Consider a countable set L of tokens (pragmatically speaking, L is often the set of all legal keys of hash dictionaries in a given programming language). Consider the set L\u2217 of finite sequences of non-negative length of elements of L.\nThe vector space V is constructed as the space of finite formal linear combinations of elements of L\u2217 over reals.\nThere are several fruitful ways to view elements of V .\n3.1.2. Finite Prefix Trees with Numerical Leaves. One can associate term \u03b1l1 . . . ln (\u03b1 \u2208 R, l1 . . . ln \u2208 L), with a path in a tree with the nodes labeled with l1, . . . , ln, \u03b1. Then an element of V (a finite sum of such terms) is associated with a finite tree with intermediate nodes labeled by elements of L, and the leaves being real numbers. The structure of intermediate nodes is a prefix tree (trie), and the numerical leaves indicate which paths are actually present, and with what values of coefficients.\n3.1.3. \u201cTensors of Mixed Rank\u201d. Another way to view an element of V is to associate the empty string (path) with non-zero coefficient (if present in our linear combination) with a scalar, each string (path) of length one with nonzero coefficient, \u03b1l, with the coordinate of a sparse array labeled l taking value \u03b1, each string (path) of length two with non-zero coefficient, \u03b2l1l2, with the element of a sparse matrix with the row labeled l1, the column labeled l2, and the element taking value \u03b2, each string (path) of length three with non-zero coefficient, \u03b3l1l2l3, with an element of a sparse \u201ctensor of rank 3\u201d,1 etc.\n1. When we say \u201ctensor of rank N\u201d, we mean simply a multidimensional array with N dimensions using standard terminology adopted in machine learning.\nTherefore, an element of V can in general be considered to be a \u201cmixed rank\u201d tensor, a sum of a scalar, a onedimensional array, a two-dimensional matrix, a tensor of rank 3, etc. Moreover, because L is countable, the onedimensional array in question has countable number of coordinates, the two-dimensional matrix in question has countable number of rows and countable number of columns, etc. However, because an element of V is a finite sum of terms \u03b1l1 . . . ln, only a finite number of those coordinates are actually non-zero, and for a given nonzero element of V there is the maximal number N for which its tensor component of rank N has a non-zero coefficient.\nIn particular, this means that any usual tensor of a fixed finite shape is representable as an element of V . Therefore, V covers a wide range of situations of interest. See Appendix A for a discussion of situations where even higher degree of generality is needed.\n3.1.4. Recurrent Maps. One can also represent elements of V via recurrent maps. An element of V is a pair consisting of a real scalar and a map from L to V . The scalar in question is non-zero if the element of V in question contains the empty string (path) with non-zero coefficient.\nOnly a finite number of elements of L can be mapped to non-zero elements of V . One considers the representation of the element of V in question as a finite prefix tree, and maps elements of L which label the first level of that tree to the associated subtrees. Other elements of L are mapped to zero.\nThis representation plays a particularly important role for us. On one hand, this is the representation which is used to build our current prototype system [5] in Clojure. In our current implementation L is slightly less than all legal hash keys in Clojure, namely several keys are reserved for other purposes. In particular, when the scalar component of an element of V is non-zero, we simply map the reserved key :number to that value. Therefore, an element of V can always be represented simply as a Clojure hash map.\nAnother particularly important use of the recurrent map representation is that the labels at the first level of a recurrent map can be dedicated to naming input or output arguments of a function. This is the mechanism to represent functions of arbitrary input and output arity, and even functions of variable arity (variadic functions) as functions having one input and one output. We use this mechanism in the next subsection."}, {"heading": "3.2. DMMs with Variadic Neurons", "text": "The activation functions of the neurons transform single streams of elements of V . The labels at the first level of the elements of V serve as names of inputs and outputs.\nThe network matrix should provide a linear transformation mapping all outputs of all neurons to all inputs of all neurons. Let\u2019s consider one input of one neuron, and the row of the network matrix responsible for computing that input from all outputs of all neurons. The natural index structure\nof this row is not flat, but hierarchical. At the very least, there are two levels of hierarchy: neurons and their outputs.\nIn our current implementation we actually use three levels of hierarchy: neuron types (which are Clojure vars referring to implementations of activation functions V \u2192 V ), neuron names, and names of the outputs. Therefore, in our current implementation matrix rows are three-dimensional sparse arrays (\u201csparse tensors of rank 3\u201d).\nSimilarly, the natural index structure for the array of rows is not flat, but hierarchical. At the very least, there are again two levels of hierarchy: neurons and their inputs. In our current implementation we actually use three levels of hierarchy: neuron types, neuron names, and names of the inputs.\nTherefore, in our current implementation the network matrix is a six-dimensional sparse array (\u201csparse tensor of rank 6\u201d).\nConceptually, the network is countably-sized, but since the network matrix has only a finite number of non-zero elements at any given time, and hence elements of V have only a finite number of non-zero coordinates at any given time, we are always working with finite representations.\nOn the \u201cdown movement\u201d, the network matrix (wtf,nf ,i,g,ng ,o) (\u201csparse tensor of rank 6\u201d) is applied to an element of V representing all outputs of all neurons. The result is an element of V representing all inputs of all neurons to be used during the next \u201cup movement\u201d.\nHere is the formula used to compute one of those inputs:\nxt+1f,nf ,i = \u2211\ng\u2208F\n\u2211\nng\u2208L\n\u2211\no\u2208L\nwtf,nf ,i,g,ng ,o \u2217 y t g,ng,o .\nHere f and g belongs to the set of neurons types F , which is simply the set of transformations of V . Potentially, one can have countable number of such transformations implemented, but at any given time only finite number of them are defined and used. The nf and ng are names of input and output neurons, and i and o are the names of the respective input and output arguments of those neurons.\nIn the formula above, wtf,nf ,i,g,ng ,o is a number, and\nxt+1f,nf ,i and y t g,ng ,o\nare elements of V . This operation is performed for all f \u2208 F , all nf \u2208 L, all input names i \u2208 L for which the matrix row has some non-zero elements.\nThe result is finitely sized map {f 7\u2192 {nf 7\u2192 x t+1 f,nf }} and each xt+1f,nf is a finitely sized map from the names of neuron inputs to the values of those inputs, {i 7\u2192 xt+1f,nf ,i}. On the \u201cup movement\u201d each f is simply applied to the elements of V representing the single inputs of the activation function f for all the neurons nf which are present in this map:\nyt+1f,nf = f(x t+1 f,nf ).\nThis mechanism is currently used in our implementation of core primitives of dataflow matrix machines in Clojure [5]. The network matrix (wtf,nf ,i,g,ng ,o) is obtained\nas the output of the Self neuron, which adds its two arguments together. The output of Self is connected to one of those inputs with weight 1, making Self an accumulator, and Self takes additive updates to the network matrix on its other input, while the network is running. For an example of a similar use of the Self neuron see Appendix B.2."}, {"heading": "4. DMMs and Machine Learning", "text": "There are different ways to view relationships between dataflow matrix machines and RNNs. One can view DMMs simply as a very powerful generalization of RNNs. Alternatively, one can view DMMs as a bridge between RNNs and programming languages.\nThere is already a strong trend to build neural networks from layers and modules rather than building them from single neurons. For example, RNN-related classes in TensorFlow [1] provide strong evidence of that trend. At the same time, engineers looking to implement and train networks with sparse connectivity patterns or with neurons having multiple inputs or multiple outputs within TensorFlow framework are well aware that this is a much more difficult undertaking, despite appearance of sparse tensors and activation functions with multiple outputs in TensorFlow documentation. DMMs encourage us to look at the neural nets with sufficient degree of generality, and single DMM neurons can be made powerful enough to serve as layers and modules when necessary.\nIn recent years, some authors suggested that synthesis of small functional programs and synthesis of neural network topology from small number of modules are closely related problems [13], [12]. Recently we are seeing progress along each of these directions (e.g. [7], [10]). DMMs might provide the right degree of generality to look at these related classes of problems.\nThere is strong evidence that syntactic shape of programs and their functionality carry sufficient mutual information about each other for that to be useful in machine learning inference (e.g. [11]). Therefore, thinking somewhat more long-term, if DMMs turn out to be a sufficiently popular platform to handcraft DMM-based software manually, this might provide a corpus of data useful for program synthesis, similarly to the use of a corpus of hand-crafted code in [11], potentially giving this approach an advantage over synthesis of low-level neural algorithms.\nThe availability of self-referential and self-modifying facilities might be quite attractive from the viewpoint of machine learning, given their potential for learning to learn and for the network to learn to modify itself, especially in the context of large networks which continue to gain experience during their lifetime (such as, for example, PathNet [6]).\nOne should note that the best learning to learn methods are often those which generalize to a large class of problems [18]. So the use of self-referential facilities for learning to learn might work better when the network is trained to solve a sufficiently diverse class of problems, compared to the cases of learning narrow functionality."}, {"heading": "Appendix A.", "text": "A.1. Linear Streams of Probabilistic Samples\nIn the present paper, we consider DMMs over real numbers.\nSometimes, one needs to represent a stream of large vectors, e.g. a stream of probability distributions over some measurable space X . One would typically have to approximate such a stream by a stream of samples drawn from those probability distributions.\nIn order to have a vector space and to allow linear combinations with negative coefficients, we consider the\nspace of all finite signed measures over X , and we consider samples to be pairs \u3008x, s\u3009, where x \u2208 X and s is a flag taking 1 and -1 as values.\nAssume that we have streams of finite signed measures over X , \u00b51, . . . , \u00b5n, and streams of corresponding samples, \u3008x1, s1\u3009, . . . , \u3008xn, sn\u3009.\nLet us describe the procedure of computing a sample representing a signed measure \u03b11\u2217\u00b51+. . .+\u03b1n\u2217\u00b5n. We pick index i with probability | \u03b1i | / \u2211 j | \u03b1j | and we pick the sample \u3008xi, sign(\u03b1i)\u2217si\u3009 to represent \u03b11\u2217\u00b51+. . .+\u03b1n\u2217\u00b5n.\nA.2. Missing Samples and Zero Measures\nThe formula in the previous subsection does not work, if all \u03b1i are zero. In general, it is convenient to allow to provide less than one sample per unit of time, i.e. to allow \u201cmissing samples\u201d.\nWe don\u2019t have a complete theory of this situation, which is under development at [5]. 2\nBut at the very least, we do allow missing samples, and we require that when one is trying to sample from zero measure the result should be the missing sample.\nIn particular, if while computing \u03b11 \u2217\u00b51+ . . .+\u03b1n \u2217\u00b5n, the index i has been picked, and the measure \u00b5i is represented by the missing sample, then the linear combination in question is represented by the missing sample.\nA.3. Extending Space V to Represent Samples\nThe expressive power of space V is insufficient to accommodate streams of samples from measures.\nThe natural generalization in this case is to consider the space of finite prefix trees with leaves from R\u2295M instead of R, where M is the space of signed measures over X .\nWhat this would mean implementation-wise is that we are to introduce another reserved keyword, :sample, and a non-zero leaf can contain :number numeric-value, or :sample value, or both.\nThe association between missing samples and zero measures fits the general spirit of space V that zero coordinates should be omitted from the representations of its elements.\nThis development is planned for a future version of [5]."}, {"heading": "Appendix B.", "text": "B.1. Lightweight Pure Dataflow Matrix Machines\nThe lightweight machines use network matrices of finite fixed size instead of the theoretically prescribed countablesized matrices with finite number of non-zero elements (for a similar construction see Appendix D of [4]). Sometimes, it is methodologically convenient to consider this restricted degree of generality.\nWe consider rectangular matrices M \u00d7N . We consider discrete time, t = 0, 1, . . ., and we consider M +N streams\n2. https://github.com/jsa-aerial/DMM/blob/master/design-notes/Early-2017/sampling-formalism.md\nof those rectangular matrices,X1, . . . , XM , Y 1, . . . , Y N . At any moment t, each of these streams takes a rectangular matrix M \u00d7 N as its value. (For example, X1t or Y N t are such rectangular matrices. Elements of matrices are real numbers.)\nLet\u2019s describe the rules of the dynamical system which would allow to compute X1t+1, . . . , X M t+1, Y 1 t+1, . . . , Y N t+1 from X1t , . . . , X M t , Y 1 t , . . . , Y N t . We need to make a choice, whether to start with X10 , . . . , X M 0 as initial data, or whether to start with Y 10 , . . . , Y N 0 . Our equations will slightly depend on this choice. The literature on dataflow matrix machines tends to start with matrices Y 10 , . . . , Y N 0 , and so we keep this choice here, even though this might be slightly unusual to the reader. But it is easy to modify the equations to start with matrices X10 , . . . , X M 0 .\nMatrix Y 1t will play a special role, so at any given moment t, we also denote this matrix as A, and its elements as ai,j . Define X i t+1 = \u2211 j=1,...,N ai,jY j t for all i = 1, . . . ,M . Here ai,jY j t is a matrix resulting from mutliplying the matrix Y jt by number ai,j . Define Y j t+1 = f j(X1t+1, . . . , X M t+1) for all j = 1, . . . , N . So, Y 1t+1 = f 1(X1t+1, . . . , X M t+1) defines Y 1 t+1 which will be used as A at the next time step t + 1. This is how the dynamical system modifies itself in lightweight pure dataflow matrix machines.\nB.2. Example of a Self-Modifying Lightweight Pure\nDataflow Matrix Machine\nThis is an example similar to the one from Appendix D.2.2 of [4]. A similar schema is implemented in [5] as https://github.com/jsa-aerial/DMM/blob/master/examples/dmm/oct 19 2016 experiment.clj\nDefine f1(X1t , . . . , X M t ) = X 1 t +X 2 t . Start with Y 1 0 = A, such that a1,1 = 1, a1,j = 0 for all other j, and maintain the condition that first rows of all other matrices Y j , j 6= 1 are zero. These first rows of all Y j , j = 1, . . . , N will be invariant as t increases. This condition means that X1t+1 = Y 1t for all t \u2265 0.\nLet\u2019s make an example with 3 constant update matrices: Y 2t , Y 3 t , Y 4 t . Namely, say that f 2(X1t , . . . , X M t ) = U2, f3(X1t , . . . , X M t ) = U 3, f4(X1t , . . . , X M t ) = U\n4. Then say that u22,2 = u 3 2,3 = u 4 2,4 = \u22121, and u 2 2,3 = u 3 2,4 = u42,2 = 1, and that all other elements of U 2, U3, U4 are zero3. And imposing an additional starting condition on Y 10 = A, let\u2019s say that a2,2 = 1 and that a2,j = 0 for j 6= 2.\nNow, if we run this dynamical system, the initial condition on second row of A would imply that at the t = 0, X2t+1 = U 2. Also Y 1t+1 = X 1 t+1 +X 2 t+1, hence now taking A = Y 11 (instead of A = Y 1 0 ), we obtain a2,2 = 1+u 2 2,2 = 0, and in fact a2,j = 0 for all j 6= 3, but a2,3 = u 2 2,3 = 1.\nContinuing in this fashion, one obtains X21 = U 2, X22 =\nU3, X23 = U 4, X24 = U 2, X25 = U 3, X26 = U 4, X27 =\n3. Essentially we are saying that those matrices \u201cpoint to themselves with weight -1\u201d, and that \u201cU2 points to U3, U3 points to U4, and U4 points to U2 with weight 1\u201d.\nU2, X28 = U 3, X29 = U 4, . . ., while the invariant that the second row of matrix Y 1t has exactly one element valued at 1 and all other zeros is maintained, and the position of that 1 in the second row of matrix Y 1t is 2 at t = 0, 3 at t = 1, 4 at t = 2, 2 at t = 3, 3 at t = 4, 4 at t = 5, 2 at t = 6, 3 at t = 7, 4 at t = 8, . . .\nThis element 1 moving along the second row of the network matrix is a simple example of a circular wave pattern in the matrix A = Y 1t controlling the dynamical system in question.\nIt is easy to use other rows of matrices U2, U3, U4 as \u201cpayload\u201d to be placed into the network matrix Y 1t for exactly one step at a time, and one can do other interesting things with this class of dynamical systems."}, {"heading": "Appendix C.", "text": "C.1. \u201cTwo-stroke engine\u201d for a standard DMM\n\u201cDown movement\u201d: for all inputs xi,Ck such that there is a non-zero weight wt(i,Ck),(j,Cl):\nxt+1i,Ck = \u2211\n{(j,Cl)|wt(i,Ck),(j,Cl) 6=0}\nwt(i,Ck),(j,Cl) \u2217 y t j,Cl .\nNote that xt+1i,Ck and y t j,Cl are no longer numbers, but vectors4, so the type correctness condition states that wt(i,Ck),(j,Cl) can be non-zero only if xi,Ck and yj,Cl belong to the same vector space.\n\u201cUp movement\u201d: for all active neurons C:\nyt+11,C , ..., y t+1 nC ,C = fC(x t+1 1,C , ..., x t+1 mC ,C ).\nBecause input and output arities are allowed to be zero, special handling of network inputs and outputs which has been required for RNNs is not required here.\nWhen a formalisms of DMMs based on a single kind of linear streams is used (e.g. DMMs based on streams of matrices in [4] or DMMs based on streams of finite prefix trees with numerical leaves in Section 3 of the present paper), the need for the type correctness condition is eliminated.\n4. In this Appendix, the formulas are written in terms of vectors themselves, and not in terms of their approximate representations actually used by the DMM in question."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["M. Abadi"], "venue": "Learning on Heterogeneous Distributed Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Dataflow Matrix Machines as Programmable, Dynamically Expandable, Self-referential Generalized Recurrent Neural Networks, May 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Programming Patterns in Dataflow Matrix Machines and Generalized Recurrent Neural Nets, June 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Notes on Pure Dataflow Matrix Machines: Programming with Self-referential Matrix Transformations, October 2016", "author": ["M. Bukatin", "S. Matthews", "A. Radul"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks, January 2017", "author": ["C. Fernando"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Differentiable Functional Program Interpreters, November 2016", "author": ["J. Feser", "M. Brockschmidt", "A. Gaunt", "D. Tarlow"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "A Logical Calculus of the Ideas Immanent in Nervous Activity", "author": ["W. McCulloch", "W. Pitts"], "venue": "The Bulletin of Mathematical Biophysics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1943}, {"title": "Evolving Deep Neural Networks, March 2017", "author": ["R. Miikkulainen"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Bayesian Sketch Learning for Program Synthesis, March 2017", "author": ["V. Murali", "S. Chaudhuri", "C. Jermaine"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Differentiable Programming, August 2016", "author": ["A. Nejati"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Neural Networks, Types, and Functional Programming", "author": ["C. Olah"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On Connectionist Models of Natural Language Processing", "author": ["J. Pollack"], "venue": "PhD thesis, University of Illinois at Urbana-Champaign,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1987}, {"title": "A \u201cSelf-Referential", "author": ["J. Schmidhuber"], "venue": "Weight Matrix,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "On the computational power of neural nets", "author": ["H. Siegelmann", "E. Sontag"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 261, "endOffset": 264}, {"referenceID": 2, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 266, "endOffset": 269}, {"referenceID": 3, "context": "In Section 2 we overview dataflow matrix machines, a generalization of RNNs based on arbitrary linear streams, neurons of arbitrary nonnegative input and output arity, a novel model of unbounded memory, and well-developed self-referential facilities, following [2], [3], [4].", "startOffset": 271, "endOffset": 274}, {"referenceID": 6, "context": "One popular approach to providing Turing complete generalizations of RNNs with unbounded memory is to use an RNN as a controller to a Turing machine tape or another model of external memory [9], [8], [17].", "startOffset": 190, "endOffset": 193}, {"referenceID": 13, "context": "Another approach is to allow reals of unlimited precision, in effect using a binary expansion of a real number as a tape of a Turing machine [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "Memory and network capacity can be dynamically added by gradually making more weights to become non-zero [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "The pragmatic power of dataflow matrix machines is considerably higher than the pragmatic power of vanilla RNNs [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 11, "context": "The ability to have multiple inputs allows us to have multiplicative neurons implementing mechanisms for gating (\u201cmultiplicative masks\u201d), which serve as fuzzy conditionals and can be used to attenuate and redirect flows of data in the network [14].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "Multiplicative neurons are implicitly present in modern recurrent neural network architectures such as LSTM and Gated Recurrent Unit networks (Appendix C of [4]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 12, "context": "There is a history of research studies suggesting that it might be fruitful for a neural network to be able to reference and update its own weights [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Then one can dedicate a particular neuron Self and use its latest output as the network matrix [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "For example, one can have updating neurons creating deep copies of network subgraphs and use those to build pseudo-fractal structures in the body of the network [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "One can argue that the ability of the network to transform the matrix defining the topology and weights of this network plays a fundamental role in the context of programming with linear streams, similar to the role of \u03bb-calculus in the context of programming via string rewriting [4].", "startOffset": 281, "endOffset": 284}, {"referenceID": 0, "context": "For example, RNN-related classes in TensorFlow [1] provide strong evidence of that trend.", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "In recent years, some authors suggested that synthesis of small functional programs and synthesis of neural network topology from small number of modules are closely related problems [13], [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "In recent years, some authors suggested that synthesis of small functional programs and synthesis of neural network topology from small number of modules are closely related problems [13], [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 5, "context": "[7], [10]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Therefore, thinking somewhat more long-term, if DMMs turn out to be a sufficiently popular platform to handcraft DMM-based software manually, this might provide a corpus of data useful for program synthesis, similarly to the use of a corpus of hand-crafted code in [11], potentially giving this approach an advantage over synthesis of low-level neural algorithms.", "startOffset": 265, "endOffset": 269}, {"referenceID": 4, "context": "The availability of self-referential and self-modifying facilities might be quite attractive from the viewpoint of machine learning, given their potential for learning to learn and for the network to learn to modify itself, especially in the context of large networks which continue to gain experience during their lifetime (such as, for example, PathNet [6]).", "startOffset": 355, "endOffset": 358}], "year": 2017, "abstractText": "We overview dataflow matrix machines as a Turing complete generalization of recurrent neural networks and as a programming platform. We describe vector space of finite prefix trees with numerical leaves which allows us to combine expressive power of dataflow matrix machines with simplicity of traditional recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}