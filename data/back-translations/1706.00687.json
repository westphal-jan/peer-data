{"id": "1706.00687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Weight Sharing is Crucial to Succesful Optimization", "abstract": "While current theoretical work provides mostly results that show the severity of this task, empirical evidence usually differs from this line, with success stories galore. A strong position among empirically successful architectures is occupied by networks that use extensive weight sharing, either by revolutionary or recurring layers. In addition, characterizing certain aspects of different tasks that make them \"harder\" or \"easier\" is an interesting direction that is studied both theoretically and empirically. We look at a family of ConvNet architectures and prove that weight sharing can be crucial from the standpoint of optimization. We examine different notions of frequency, target function, and prove the need for the target function to have some low-frequency components. This need is not sufficient - it can be exploited only in weight sharing, making it theoretically different from others that do not.", "histories": [["v1", "Fri, 2 Jun 2017 13:56:59 GMT  (39kb)", "http://arxiv.org/abs/1706.00687v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "ohad shamir", "shaked shammah"], "accepted": false, "id": "1706.00687"}, "pdf": {"name": "1706.00687.pdf", "metadata": {"source": "CRF", "title": "Weight Sharing is Crucial to Succesful Optimization", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n00 68\n7v 1\n[ cs\n.L G\n] 2\nJ un\nExploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them \u201charder\u201d or \u201ceasier\u201d, is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient - only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks."}, {"heading": "1 Introduction", "text": "There are many directions from which one can examine Deep Learning (DL). Very popular is the direction of empirical success, where extensive research effort had resulted in state-of-the-art, overwhelming breakthroughs, in a wide range of tasks. One may need to read between the lines to gain insights regarding the difficulties which faced the practitioners on their way to success. This is true, in particular, when regarding the optimization process. While sample complexity issues are usually straightforward to deal with (\u201cadd more data\u201d), and expressive power of the used networks is generally more than sufficient, successful optimization, and in particular, success of Gradient Descent (GD), is left as a mystery. What aspects of a task cause the general gradient-based DL approach to succeed or fail?\nIn this paper, we study this question for a simple, yet powerful, ConvNet architecture: one convolutional layer, mapping k image patches, each of dimension d, into k scalars, followed by a non linear activation, a fully connected (FC) layer with ReLU activation, and a final FC layer with one output neuron. Most if not all DL practitioners would have known this \u201crecipe\u201d by heart. We think of k as relatively smaller than d: for example, d = 75 and k = 10, corresponding to a 5\u00d7 5\u00d7 3 convolution kernel over a small color image. This family of architectures, as trivial and simplistic as it is, can provide us with very fertile ground on which to examine interesting empirical phenomena. We assume that the target function which we are trying to learn is generated by a network of the exact same architecture, and learning is performed with a very large training set. Therefore, there are neither expressiveness nor overfitting issues, which enables us to focus solely on the success of GD.\nAny target function generated by the above architecture, can be thought of as a composition of two functions: the convolutional first layer (with its non linearity), denoted h\u2217 : Rdk \u2192 Rk, subsequently fed into\nthe second part of the network, denoted g\u2217 : Rk \u2192 R. We underscore two properties of DL tasks that control GD\u2019s success or failure. The first property, uses notions of frequency, from Fourier analysis, to characterize \u201chardness\u201d of a task. The second property, distinguishes between Convolutional layers, in which weights are shared, and Fully Connected (FC) ones.\nSince the target function is the composition g\u2217 \u25e6 h\u2217, it is natural to start by understanding the success of GD when one of the target function\u2019s components is fixed and known, with only the other being learnt. For the case of known h\u2217, because k is small, it is possible to show that under some mild conditions, the problem of learning g\u2217 is not hard (see Appendix A). A more interesting case is when g\u2217 is known, and our task is to learn h\u2217. In [18], it has been shown that no Gradient Based algorithm can succeed in learning h\u2217 if g\u2217 is the parity of the signs of its input. The parity function consists of the highest frequency of the Fourier expansion for functions over the boolean cube. In this paper, we prove that h\u2217 can be learnt by GD, if the Fourier expansion of g\u2217 contains both a frequency 1 element and a higher frequency element, namely, a combination both high and low frequencies. We further prove, that this positive result depends on our architecture for learning h\u2217: if the convolutional layer is replaced by a FC one, then GD will fail. It is the combination of g\u2217 having a low frequency component, along with the weight sharing in our architecture, that is essential for success. Formal statements of these claims are given in Section 4.\nNaturally, mathematically analyzing the convergence properties of GD in this highly non convex problem, relies on some simplifying assumptions. A major one, is the assumption that g\u2217 is known. We start the paper, in Section 2, by empirically demonstrating that our theoretical results seem to hold even in the case of learning simultaneously both h\u2217 and g\u2217. In Section 4 we prove our main result, but before that, we highlight, in Section 3, the same phenomena, albeit in a simpler setting, where g\u2217(z) = ckz1 + cos( \u2211k i=1 zi). Although somewhat synthetic, this setting does maintain the flavour of separation between low and high frequencies, this time from the perspective of Fourier analysis over Rk. Additionally, it allows for a relatively simple, direct proof technique, showing a computational separation between learning with or without weight sharing, where an exponential gap in time complexity of GD is proven to exist."}, {"heading": "1.1 Related Work", "text": "Recently, several works have attempted to study the optimization performance of gradient-based methods for neural networks. To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm. Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods. More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture. The hardness of learning in the case of Boolean functions, using the degree of the target function, was discussed in the statistical queries literature, for instance in [6]. In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.g. [3]), to study the difficulty of learning with gradient-based methods."}, {"heading": "2 Empirical Demonstration", "text": "The target function we wish to learn is of the form g\u2217(h\u2217(x)), where x = (x1, . . . ,xk), with xi \u2208 Rd for every i. The function h\u2217 is parameterized by a vector u0 \u2208 Rd and is defined as h\u2217(x) = (\u03c3(u\u22a40 ,x1), . . . , \u03c3(u\u22a40 xk)), where we chose \u03c3 to be the tanh function, as a smooth approximation of the sign function. We can therefore think of the input to g\u2217 as approximately being from {\u00b11}k. In our experiment we vary four parameters:\n\u2022 The value of g\u2217 is set to be either g\u2217low(z) := z1, or g\u2217high(z) = \u220f5 i=1 zi, or g \u2217 both(z) = g \u2217 low(z) + g \u2217 high(z).\n\u2022 Weight Sharing (WS) vs. Fully Connected (FC): we also learn a compositional function g(h(x)), and the function h(x) can be either with weight sharing, h(x) = (\u03c3(w\u22a40 ,x1), . . . , \u03c3(w \u22a4 0 xk)), where we learn\nthe vector w0 \u2208 Rd, or with fully connected architecture, namely, h(x) = (\u03c3(w\u22a41 ,x1), . . . , \u03c3(w\u22a4k xk)), where we learn the vector w = (w1, . . . ,wk).\n\u2022 Known vs. Unknown g\u2217: for the function g we either use g = g\u2217 or learn g as well by using the following architecture: FC layer with 50 outputs, ReLU, and FC layer with a single output.\n\u2022 Input Distribution: we either sample x from a Gaussian distribution, or use real image patches of size 10\u00d7 10 from the MNIST data set, normalized to have zero mean.\nWe train all of our networks with SGD, with \u03b7 = 0.5, batch size 128, and the Squared Loss, for 3000 iterations. The vectors xi were generated by sampling from a normal distribution. The results of these experiments are depicted on the 6 graphs of Figure 1.\nThe graphs reveals several interesting observations. The first is the clear failure, of both WS and FC architectures, for both real and Gaussian data, and for both known and unknown g\u2217, when the target function is g\u2217high, and the contrasting success when it is g \u2217 low. To explain this difference, let us characterize the g\u2217s using tools from Fourier analysis of real functions over the boolean cube. The representation of such functions in the Fourier basis can be used to define many different meaningful characterizations. Perhaps one of the most natural ones is the degree, or frequency of the function. Specifically, in our case, g\u2217low is a basis function of degree 1, while g\u2217high is a basis function of degree 5. Our theoretical analysis shows that the number of GD iterations required to learn h\u2217 when g\u2217 is a basis function grows as ddegree. In our experiment d = 75, or 100 for the MNIST patches, and we observe a clear separation already between degree 1 and degree 5.\nNext, since real-world functions will likely contain several frequencies, it is natural to study functions that combine many basis elements. As a first step, we turn to observe the performance for g\u2217both. Here, we suddenly see a strong separation between the WS and FC architectures: the optimization converges very quickly for the WS architecture while for the FC one, the high frequency component has not been learnt. Our theoretical analysis proves that, indeed, the number of GD iterations required by the FC architecture still grows as dhigh-degree, while for the WS architecture, the required number of iterations is only polynomial in the high degree.1 Intuitively, the low frequency term directs the single, shared, weight vector towards the optimum. Once h converged to h\u2217, even if g\u2217 is unknown, the GD process succeeds in learning it, because k is small. In contrast, without weight sharing, the components of w that appear only in the high degree term are not being learnt, as was the case for g\u2217high.\nFinally, while our analysis proves the positive and negative results for the case of known g\u2217 where x is normally distributed, the graphs show that even in the more general case, when g\u2217 is also being learnt, and even if the data is natural, the picture remains roughly the same.2"}, {"heading": "3 Sum of Low and High Degree Waves", "text": "In this section we provide our first separation result between the WS and FC architectures. Let x = (x1, . . . ,xk) \u2208 Rdk,w = (w1, . . . ,wk) \u2208 Rdk denote input elements, and weight vectors, respectively. Define:\npw(x) = ckw \u22a4 1 x1 + cos\n(\nk \u2211\ni=1\nw\u22a4i xi\n)\n,\nwhere ck is any parameter \u2265 3 \u221a k. As in the introduction, we define a sub-family of functions, parameterized\n1We emphasize that this exponential gap between WS and FC is due to computational reasons and not due to overfitting. The difference in sample complexity between the two architectures is only a factor of k, and in both cases the training set size is sufficiently large.\n2The only difference across this aspect, when learning a degree 1 parity with a convolutional architecture, between known and unknown g\u2217, for Gaussian data, is perhaps due to smaller Signal to Noise Ratio in the case of learning g\u2217, as suggested in [18].\nby u0, and defined as:\npu0(x) = cku \u22a4 0 x1 + cos\n(\nk \u2211\ni=1\nu\u22a40 xi\n)\n.\nFor simplicity of notation, when using the 0 subscript for the weight vector, we refer to an element of the WS sub-family. Additionally, we use u\u03040 to denote the vector composed of k duplicates of u0, namely (u0,u0, . . . ,u0). Consider the objective\nF (w) = Ex\n[\n1 2 (pw(x) \u2212 pu0(x))2\n]\n,\nwhere x is standard Gaussian. We consider the gap between optimizing a FC architecture, namely, one parameterized by w, and a WS one, parameterized by a single weight vector w0. We note that our choice of ck is merely to simplify the proofs \u2013 convergence guarantees can be proven for other choices of ck (including ck = 1), but the proof requires more effort."}, {"heading": "3.1 Hardness Result for Optimizing F using GD - FC Architecture", "text": "Theorem 1 Assuming k > 1, the following holds for some numerical constants c1, c2, c3, c4: For any w such that \u2016(w2, . . . ,wk)\u2016 \u2208 [\u221a k\u22121 3 \u00b7 \u2016u0\u2016, \u221a k\u22121 2 \u00b7 \u2016u0\u2016 ] , it holds that\n\u2225 \u2225 \u2225 \u2225\n\u2202\n\u2202(w2, . . . ,wk) F (w)\n\u2225 \u2225 \u2225 \u2225 \u2264 c1 \u221a k\u2016u0\u2016 exp ( \u2212c2k\u2016u0\u20162 ) .\nMoreover, for any w such that \u2016(w2, . . . ,wk)\u2016 \u2264 \u221a k\u22121 2 \u2016u0\u2016, it holds that\nF (w)\u2212 F (u\u03040) \u2265 1\u2212 c3 exp(\u2212c4k\u2016u0\u20162). The proof is given in Appendix B.1. To understand the implication of the theorem, consider a gradientbased method, starting at some initial point w(0) such that \u2016(w(0)2 , . . . ,w (0) k )\u2016 \u2264 \u221a k\u22121 3 \u00b7 \u2016u0\u2016 (a reasonable assumption). In that case, the theorem implies that the algorithm will need to cross the ring {\n(w2, . . . ,wk) : \u2016(w2, . . . ,wk)\u2016 \u2208 [ \u221a k \u2212 1 3 \u00b7 \u2016u0\u2016, \u221a k \u2212 1 2 \u00b7 \u2016u0\u2016 ]}\nw.r.t. (w2, . . . ,wk), to get to a solution which ensures sub-constant error. However, in that ring, the gradients are essentially exponentially small in k\u2016u0\u20162. This implies that performing gradient steps with any bounded step size, one would need exponentially many iterations (in k\u2016u0\u20162) to achieve sub-constant error."}, {"heading": "3.2 Positive Result for Optimizing F using GD - WS Architecture", "text": "We show that when using a WS architecture, the objective is transformed to be strongly convex, making for simple proof techniques being applicable. The proof is given in Appendix B.2.\nTheorem 2 Using the WS architecture, F is strongly convex, minimized at u0, and satisfies maxw0 \u03bbmax(\u2207 2F (w0))\nminw0 \u03bbmin(\u22072F (w0)) \u2264\n5, where \u03bbmax(M) and \u03bbmin(M) are the top and bottom eigenvalues of a positive definite matrix M . Hence, gradient descent starting from any point w(0), and with an appropriate step size, will reach a point w satisfying \u2016w\u2212 u0\u2016 \u2264 \u01eb in at most 5 \u00b7 log(\u2016winit \u2212 u0\u2016/\u01eb) iterations."}, {"heading": "4 Sum of Low and High Degree Parities", "text": "This section formalizes our main result, namely, a separation between WS and FC architectures for learning a target function, g\u2217 \u25e6 h\u2217, when g\u2217 = g\u2217both is comprised of both high and low frequencies. The negative result for the FC architecture is given in Theorem 3 and the positive result for the WS architecture is given in Theorem 4."}, {"heading": "4.1 Definitions, Notation", "text": "Let x denote a k-tuple (x1, . . . ,xk) of input instances, and assume that each xl is i.i.d. standard Gaussian in R\nd. Let \u03c3 : R \u2192 [\u22121, 1] be some smooth approximation of the sign function. To simplify the analysis, we use the erf function: erf(x) = 1\u221a\n\u03c0\n\u222b x \u2212x e \u2212t2dt. We believe that our analysis holds for additional functions, such\nas the popular tanh function. Define, for k \u2208 N, a family of functions H(k)FC , parameterized by w \u2208 (Rd)k, and defined by:\np(k) w (x) =\nk \u220f\nl=1\n\u03c3(w\u22a4l xl).\nLet us define a subclass, parameterized by w0 \u2208 Rd and denoted H(k)WS , by:\np(k) w0 (x) =\nk \u220f\nl=1\n\u03c3(w\u22a40 xl).\nNote that the difference between H(k)WS and H (k) FC is that elements in H (k) WS are satisfying the condition that for all l, wl = w0 for some w0 \u2208 Rd. For ease of notation, we will refer to elements in H(k)WS and H (k) FC by p (k) w0 and p (k) w , respectively.\nAn extension of these families, denoted H(1,k)WS and H (1,k) FC , is defined, for w0 \u2208 Rd and w \u2208 (Rd)k respectively, as the sum of the two corresponding functions of the H(1),H(k) classes. Namely, for the FC class,\np(1,k) w (x) = \u03c3(w\u22a41 x1) + k \u220f\nl=1\n\u03c3(w\u22a4l xl),\nwith the definition for the WS class following as a special case.\nLet the objective F (k)(w), w.r.t. some target function p (k) u0 be the expected squared loss,\nF (k)(w) = Ex\n[\n1 2 (p(k) w (x)\u2212 p(k) u0 (x))2\n]\n,\nwith a similar definition for F (1,k)(w), namely\nF (1,k)(w) = Ex\n[\n1 2 (p(1,k) w (x)\u2212 p(1,k) u0 (x))2\n]\n. (1)\nThe following definition and lemma, due to [22], will be useful in our analysis.\nLemma 1 ([22]) Let \u03c3 be the erf function. Then, For every pair of vectors u,v \u2208 Rd we have\nV\u03c3(u,v) := Ex\u223cN(0,I) [ \u03c3(w\u22a4x)\u03c3(u\u22a4x) ] = 2 \u03c0 sin\u22121\n(\n2u\u22a4v \u221a\n1 + 2\u2016u\u20162 \u221a 1 + 2\u2016v\u20162\n)\n,\nwhere N(0, I) is the standard Gaussian distribution."}, {"heading": "4.2 Non degeneracy depending on \u2016u0\u20162", "text": "Note that as |\u03c3| < 1, for large values of k, and small values of u\u22a40 xl, we have that pu0(x) is vanishing exponentially. We show that in the case of large enough \u2016u0\u20162, depending on k, the target function\u2019s expected norm is lower bounded, hence overcoming this possible degeneracy.\nLemma 2 If \u2016u0\u20162 \u2265 12\u03c02 k2 then Ex [ (p(k) u0 (x))2 ] > 1\n4 .\nThe proof is given in Appendix C.1."}, {"heading": "4.3 Exact Gradient Expressions", "text": "In order to analyze the dynamics of the Gradient Descent (GD) optimization process, we examine the exact gradient expressions. The proofs to the lemmas are given at Appendix C.2\nRecall the definition of F (1,k) from (1). We first show that F (1,k) equals the sum of F (1) and F (k), due to independence of these two terms.\nLemma 3 For both architectures,\nF (1,k)(w) = F (1)(w) + F (k)(w).\nBased on Lemma 3, we have that \u2207F (1,k)(w) = \u2207F (1)(w)+\u2207F (k)(w), hence it suffices to find an explicit expression for \u2207F (k)(w), for every k. We have,\n\u2207F (k)(w) = Ex [ p(k) w (x)g(k) w (x)\u2212 p(k) u0 (x)g(k) w (x) ] ,\nwhere g (k) w (x) := \u2207wp(k)w (x) is the gradient of the predictor w.r.t. the weight vector w. We first show the following symmetric property.\nLemma 4 For all k,w,w\u2032, and x,\np(k) w (x)g (k) w \u2032 (x) = p (k) w (\u2212x)g(k) w \u2032 (\u2212x).\nWe can now proceed to compute that exact gradients.\nLemma 5 Let g (k) l (x) be the gradient of the predictor w.r.t. wl, the weights corresponding to the lth input element. Then\nEx\n[\np(k) w (x)g (k) l (x)\n]\n=\n\n\n\u220f j 6=l V\u03c3(wj ,wj)\n\n \u00b7 c1(wl)wl,\nwhere 0 < c1(wl) < 1, is independent of k, and:\nEx\n[\np(k) u0 (x)g (k) l (x)\n]\n=\n\n\n\u220f j 6=l V\u03c3(u0,wj)\n\n \u00b7 b\u0303l\nfor some vector b\u0303l \u2208 span{wl,u0}, independent of k.\nCorollary 1 For the WS architecture, we have that:\nEx\n[\np(k) w0 (x)g (k) 0 (x)\n]\n= k \u00b7 V\u03c3(w0,w0)k\u22121 \u00b7 c1(w0)w0,\nwhere 0 < c1(w0) < 1, is independent of k, and:\nEx\n[\np(k) u0 (x)g (k) 0 (x)\n]\n= k \u00b7 V\u03c3(u0,w0)k\u22121 \u00b7 b\u0303\nfor some vector b\u0303 \u2208 span{w0,u0}, independent of k.\nThe corollary follows immediately from the fact that the gradient w.r.t. w0, is the sum of gradients of each \u201cduplicate\u201d of w0, when considering pw0 as a member of H(k)FC . Note that, when u\u22a40 w0 > 0 (which happens w.p. 1/2 over symmetric initialization, and as we later show, this property is preserved during a run of GD), V\u03c3(u0,w0) > 0. Hence in such case, the coefficient of b\u0303 is positive.\n4.4 Hardness Result for Optimizing F (1,k) using GD - FC Architecture\nEquipped with the results of previous sections, we obtain a computational hardness result for learning a target function p (1,k) u0 using GD with the FC architecture, showing that the progress after any polynomial number of iterations, is exponentially small. Proofs are given in Appendix C.3.\nTheorem 3 Consider a GD algorithm for optimizing F (1,k) for the FC architecture, that uses a learning rate rule such that for every t, \u03b7t \u2208 (0, 1]. Suppose that every coordinate of wl is initialized i.i.d. uniformly from {\u00b1c} for some constant c. Then, with probability of at least 1 \u2212 2ke\u2212d1/3/6 over the random initialization, after T = o(dk/4) iterations, we will have F (1,k)(w(T )) \u2265 1/8. The main idea of the proof is to show that progress in direction which improves the angle between wj and u0 is exponentially small - unless wj gets close to the origin. In that case, it is \u201cstuck\u201d there with no ability to progress. The proof relies on the following lemmas. The first one shows that a \u201cbad\u201d initialization, namely, one for which the initialized vectors are almost orthogonal to u0, happens with overwhelming probability.\nLemma 6 Assume each wl is chosen by sampling uniformly from {\u00b1c}d for some c. Then w.p. > 1 \u2212 2ke\u22120.5d 1/3\nover the initialization w(0), for all l \u2208 [k], |\u3008 wl\u2016wl\u2016 , u0 \u2016u0\u2016 \u3009| < d \u22121/3.\nNext, we directly upper bound the value of |Verf(wj ,u0)|, s.t. for cases when wj ,u0 are almost orthogonal, or, when \u2016wj\u2016 is very small, |Verf(wj ,u0)| is small too. Lemma 7 Let c \u2208 [0, 1] and assume |\u3008 wj\u2016wj\u2016 , u0 \u2016u0\u2016 \u3009| < c, or \u2016wj\u2016 \u2264 c/ \u221a 2. Then\n|Verf(wj ,u0)| < 2 c\n\u03c0 < c\nFinally, we use the fact that the target function is non trivial, from Lemma 2, in order to show that in the case when not all of the weight vectors have converged, we suffer high loss.\nLemma 8 Assume that for some j, it holds that either |\u3008 wj\u2016wj\u2016 , u0 \u2016u0\u2016 \u3009| < sin \u03c0 32 , or \u2016wj\u2016 \u2264 1\u221a2 sin \u03c0 32 . Then\nF (1,k)(w) > 1\n8 .\n4.5 Positive Result for Optimizing F (1,k) using GD - WS Architecture\nWe now turn to state our positive result for the WS architecture. The outline of the proof is given in the subsections below, and additional proofs of intermediate results are given at Appendix C.4.\nTheorem 4 Running (projected) GD, with respect to the objective F (1,k) with the WS architecture, and with a constant learning rate for T = poly(k, 1/\u01eb) iterations, yields \u2016w(T )0 \u2212 u0\u2016 \u2264 \u01eb. To prove the theorem, we analyze the optimization process by separating it into two phases. During the first, the w0 converges to the direction of u0. Then, in a second phase, its norm converges to that of u0. The combination of the theorems proven in the next sections directly imply Theorem 4."}, {"heading": "4.6 Phase 1 - Angle Convergence", "text": "Assume that \u2016u0\u20162 = 12\u03c02 k2, large enough for non degeneracy of the target function, as shown in Section 4.2. Moreover, we can assume w.l.o.g., that u0 = \u221a 12 \u03c02 k e2, where e2 = (0, 1, 0, . . . , 0). We can further assume w.l.o.g. that span{u0,w0} = span{e1, e2}. By the random initialization, it holds that w\u22a40 u0 > 0 with probability 1/2. We will assume that this is indeed the case. In addition, we will assume, w.l.o.g., that the first two coordinates of w0 are non-negative. Finally, assume that \u2016w(t)0 \u2016 \u2264 \u2016u0\u2016 for every t (if this is not the case, it is standard to add a projection onto this ball).\nTheorem 5 Let \u03b1(t) be the angle between u0,w (t) 0 . Then \u03b1 (t+1) \u2264 \u03b1(t). Moreover, for every \u01eb > 0, there exist T = O((k/\u01eb)3) s.t. \u03b1(T ) < \u01eb."}, {"heading": "4.7 Phase 2", "text": "We use the same assumptions as in Section 4.6. We start off with a theorem showing that the gradient of F (1) directs the weights towards the optimum, u0. The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].\nTheorem 6 For some L2(s\u0303) = \u0398(1),\n\u3008w0 \u2212 u0,\u2207F (1)(w0)\u3009 \u2265 L2(s\u0303)\nk \u2016w0 \u2212 u0\u20162.\nAfter establishing the above result, we next show that when the angle \u03b1 between w0 and u0 is small (which we\u2019ve shown is the case in polynomial time, after the first phase of optimization, in Section 4.6), the gradient of F (1,k) has the same property as the gradient of F (1), namely, it too points in a good direction. The intuition is that when w0 is close, in terms of angle, to u0, the gradient of F\n(k) becomes more similar to the gradient of F (1), making it helpful too.\nTheorem 7 Assume \u03b1(t) < min{tan\u22121 (\n\u01eb 2\u2016u0\u2016\n)\n, \u221a\n\u01eb \u2016u0\u2016} and \u2016w0 \u2212 u0\u2016 > \u01eb. Then\n\u3008w0 \u2212 u0,\u2207F (1,k)(w0)\u3009 \u2265 L2(s\u0303)\nk \u2016w0 \u2212 u0\u20162,\nfor L2(s\u0303) = \u0398(1), as in Theorem 6.\nWe now use the above lower bound over the inner product between the gradient and the optimal optimization step, to show that for any \u01eb > 0, after a polynomial number of iterations of GD, we converge to a solution w0 for which \u2016w0 \u2212 u0\u2016 < \u01eb.\nTheorem 8 Assumew(t \u2032) is such that \u03b1(t \u2032) < min{tan\u22121 (\n\u01eb 2\u2016u0\u2016\n)\n, \u221a\n\u01eb \u2016u0\u2016}. Then, after at most poly(k, 1/\u01eb)\nadditional iterations we must have that \u2016w(t)0 \u2212 u0\u2016 \u2264 \u01eb .\nAcknowledgements: This research is supported in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and by the European Research Council (TheoryDL project)."}, {"heading": "A Learning g\u2217 for Small k", "text": "For simplicity of the argument, we consider the problem of learning a function g\u2217 : {\u00b11}k \u2192 R. The argument can be easily extended to the case in which the domain of g\u2217 is [\u22121, 1]k under an additional Lipschitzness assumption, which is the case if the activation of h\u2217 is, for example, the tanh function.\nConsider an arbitrary distribution, D, over {\u00b11}k. Using Lemma 19.2 in [17] we have that if m > 2k/\u01eb then, in expectation over the choice of the sample, the probability mass of vectors in {\u00b11}k that does not belong to my sample is at most \u01eb. Therefore, finding a function g that agrees with all the points in the sample is sufficient to guarantee that g agrees with g\u2217 on all but an \u01eb-fraction of the vectors in {\u00b11}k.\nNext, consider the problem of fitting a sample (x1, g \u2217(x1)), . . . , (xm, g\u2217(xm)) using a one-hidden-layer network. Several papers have shown (e.g. [13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima). Furthermore, by a simple random embedding argument, it can be shown that if we randomly pick the weight of the first layer and then freeze them, then with high probability, there are weights for the second layer for which the error is 0 on all the training examples. Learning only the second layer is a convex optimization problem.\nCombining all the above we obtain that there is a procedure that runs in time poly(2k, 1/\u01eb) that learns g\u2217\nto accuracy \u01eb. Note that since k is small (in our experiment, we used k = 5), the term 2k is very reasonable. This stands in contrast to the term dk, appearing in our lower bound for learning h\u2217. Taking d = 75, k = 5 (as in our experiment), the value of dk is huge."}, {"heading": "B Proofs of Section 3", "text": ""}, {"heading": "B.1 Proofs of Section 3.1", "text": ""}, {"heading": "Proof of Theorem 1:", "text": "Firstly, we note that by definition, F (w) equals\nc2k 2 E [ (w\u22a41 x1 \u2212 u\u22a40 x1) ] + 1 2 \u00b7 E\n\n\n(\ncos\n(\nk \u2211\ni=1\nw\u22a4i xi\n) \u2212 cos ( k \u2211\ni=1\nu\u22a40 xi\n))2 \n\n+ E\n[ (w\u22a41 x1 \u2212 u\u22a40 x1) ( cos ( k \u2211\ni=1\nw\u22a4i xi\n) \u2212 cos ( k \u2211\ni=1\nu\u22a40 xi\n))]\n= c2k 2 (w1 \u2212 u0)\u22a4E[x1x\u22a41 ](w1 \u2212 u0) + 1 2 \u00b7 E\n\n\n(\ncos\n(\nk \u2211\ni=1\nw\u22a4i xi\n) \u2212 cos ( k \u2211\ni=1\nu\u22a40 xi\n))2 \n+ 0,\nwhere we used the facts that (x1, . . . ,xk) are symmetrically distributed (hence take any value as well as its negative with equal probability) and that cosine is an even function. We get that\nF (w) = c2k 2 E [ ( w\u22a41 x1 \u2212 u0x1 )2 ] + 1 2 Ez [ ( cos ( w\u22a4z ) \u2212 cos ( u\u0304\u22a40 z ))2 ] , (2)\nwhere z is a standard Gaussian vector in Rkd. We prove the following useful lemma:\nLemma 9 Given some b > 0, and assuming z has a standard Gaussian distribution in Rd, the gradient of 1 2 \u00b7 Ez [ ( cos(b \u00b7w\u22a4z) \u2212 cos(b \u00b7 u\u0304\u22a40 z) )2 ] w.r.t. w equals\n\u2212 b 2 \u00b7 (\u03c6(2bw) + \u03c6(b(w \u2212 u0))\u2212 \u03c6(b(w + u0))) ,\nwhere\n\u03c6(a) = exp\n( \u2212\u2016a\u2016 2\n2\n)\n\u00b7 a.\nProof A straightforward calculation reveals that the gradient equals\n\u2212 b \u00b7 E [( cos(b \u00b7w\u22a4z)\u2212 cos(b \u00b7 u\u0304\u22a40 z) ) \u00b7 sin(b \u00b7w\u22a4z)z ] = \u2212 b 2 ( E[sin(2b \u00b7w\u22a4z)z] + E[sin(b(w \u2212 u\u03040)\u22a4z)z] \u2212 E[sin(b(w + u0)\u22a4z)z] ) .\n\u2212b 2\n2\n( 2 exp ( \u22122b2\u2016w\u20162 ) w + exp ( \u2212b2\u2016w\u2212 u0\u20162 ) (w \u2212 u0)\u2212 exp ( \u2212b2\u2016w+ u0\u20162 ) (w + u0) ) .\nWe now argue that for any vector a, E[sin(a\u22a4z)z] = \u03c6(a), (3)\nfrom which the lemma follows. To see this, let za = a \u22a4 z \u2016a\u20162 \u00b7 a be the component of z in the direction of a, and let z\u22a5a = z\u2212 za be the orthogonal component. Then we have\nE[sin(a\u22a4z)z] = E[sin(a\u22a4za)za] + E[sin(a \u22a4za)z\u22a5a].\nThe first term equals a\u2016a\u20162 \u00b7 E[sin(a\u22a4z)a\u22a4z], or equivalently a\u2016a\u2016 \u00b7 Ey [sin(\u2016a\u2016y)y], where y has a standard Gaussian distribution. As to the second term, we have that za and z\u22a5a are statistically independent (since z has a standard Gaussian distribution), so the term equals E[sin(a\u22a4za)]E[z\u22a5a] = 0. Overall, we get that the expression above equals\na \u2016a\u2016 \u00b7 Ey[sin(\u2016a\u2016y)y] = a \u2016a\u2016 \u00b7 \u222b \u221e y=\u2212\u221e sin(\u2016a\u2016y)y \u00b7 1\u221a 2\u03c0 exp ( \u2212y 2 2 ) dy.\nUsing integration by parts and the fact that \u222b y cos(by) exp(\u2212ay2) = \u221a \u03c0 a exp(\u2212b2/4a) (see [21, equation 15.73]), this equals\n= a\n\u2016a\u2016 \u221a 2\u03c0\n\u00b7 ( \u2212 \u222b \u221e\ny=\u2212\u221e sin(\u2016a\u2016y) exp\n(\n\u2212y 2\n2\n) dy + \u2016a\u2016 \u222b \u221e\ny=\u2212\u221e cos(\u2016a\u2016y) exp\n(\n\u2212y 2\n2\n)\ndy\n)\n= a\u221a 2\u03c0\n\u00b7 ( 0 + \u221a 2\u03c0 exp ( \u2212\u2016a\u2016 2\n2\n))\n,\nfrom which (3) follows.\nNow, let us consider the partial derivative of this function w.r.t. w\u0302 := (w2, . . . ,wk). Using Lemma 9, and letting u\u03020 = (u0, . . . ,u0) to be concatenation of (k \u2212 1) copies of u0, we get that this partial derivative equals\n\u22121 2\n(\nexp\n( \u2212\u2016w\u2016 2\n2\n)\nw\u0302 + exp\n( \u2212\u2016w\u2212 u\u03040\u2016 2\n2\n) \u00b7 (w\u0302 \u2212 u\u03020)\u2212 exp ( \u2212\u2016w+ u\u03040\u2016 2\n2\n) \u00b7 (w\u0302 \u2212 u\u03020) ) ,\nwith norm at most\n1\n2\n(\nexp\n( \u2212\u2016w\u0302\u2016 2\n2\n) \u2016w\u0302\u2016+ exp ( \u2212\u2016w\u0302\u2212 u\u03020\u2016 2\n2\n) \u00b7 \u2016w\u0302\u2212 u\u03020\u2016+ exp ( \u2212\u2016w\u0302+ u\u03020\u2016 2\n2\n) \u00b7 \u2016w\u0302\u2212 u\u03020\u2016 ) .\nNow, if \u2016w\u0302\u2016 \u2208 [\u221a\nk\u22121 3 \u2016u0\u2016, \u221a k\u22121 2 \u2016u0\u2016 ] = [ \u2016u\u03020\u2016 3 , \u2016u\u03020\u2016 2 ] , as implied by the assumption stated in the theorem,\nit is easily verified that \u2016w\u0302 \u2212 u\u03020\u20162 as well as \u2016w\u0302 \u2212 u\u03020\u20162 are at least \u2016u\u03020\u20162/3, whereas \u2016w\u0302 \u2212 u\u03020\u2016, \u2016w\u0302+ u\u03020\u2016 are at most 2\u2016u\u03020\u2016. Moreover, \u2016u\u03020\u2016\u221ak\u2016u0\u2016 = \u221a 1\u2212 1k \u2208 [ 1\u221a 2 , 1 ] .Thus, the displayed equation above can be upper\nbounded by c1 \u221a k\u2016u0\u2016 exp(\u2212c2k\u2016u0\u20162) for some numerical constants c1, c2.\nTo prove the second part of the theorem, we rely on the following lemma:\nLemma 10\nE\n[\n( cos(w\u22a4x) \u2212 cos(v\u22a4x) )2 ] \u2265 1\u2212 exp(\u2212\u2016w\u2212 v\u20162/2)\u2212 exp(\u2212\u2016w+ v\u20162/2).\nProof Expanding the square and using standard trigonometric identities, we have that the left hand side equals\nE [ cos2(w\u22a4x) ] + E [ cos2(v\u22a4x) ] \u2212 2E [ cos(w\u22a4x) cos(v\u22a4x) ]\n= 1 + 1\n2 E[cos(2w\u22a4x)] +\n1 2 E[cos(2v\u22a4x)]\u2212 E [ cos((w \u2212 v)\u22a4x) ] \u2212 E [ cos((w + v)\u22a4x) ] .\nSince for any vector z, E[cos(z\u22a4x)] = Ey[cos(\u2016z\u2016y)] where y has a standard normal distribution on R, and this in turn equals 1\u221a\n2\u03c0\n\u222b\ncos(\u2016z\u2016y) exp(\u2212y2/2) = exp(\u2212\u2016z\u20162/2) (see [21, equation 15.73]), the above equals\n1 + 1\n2 exp\n( \u22122\u2016w\u20162 ) + 1\n2 exp\n( \u22122\u2016v\u20162 ) \u2212 exp ( \u2212\u2016w\u2212 v\u2016 2\n2\n) \u2212 exp ( \u2212\u2016w+ v\u2016 2\n2\n)\n,\nfrom which the result follows.\nUsing this lemma, and definition of F (w) in (2), we have\n2(F (w)\u2212 F (u\u03040)) \u2265 1\u2212 exp(\u2212\u2016w\u2212 u0\u20162/2)\u2212 exp(\u2212\u2016w+ u0\u20162/2) \u2265 1\u2212 exp(\u2212\u2016w\u0302\u2212 u\u03020\u20162/2)\u2212 exp(\u2212\u2016w\u0302+ u\u03020\u20162/2).\nMoreover, assuming that \u2016w\u0302\u2016 \u2264 \u221a k\u22121 2 \u2016u0\u2016 = \u2016u\u03020\u2016 2 (as implied by the assumption stated in the theorem), we have that \u2016w\u0302\u2212u\u03020\u20162 as well as \u2016w\u0302\u2212u\u03020\u20162 are at least \u2016u\u03020\u20162/4, which in turn equals (k\u22121)\u2016u0\u20162/4 \u2265 k\u2016u0\u20162/8. Plugging to the above, the result follows."}, {"heading": "B.2 Proofs of Section 3.2", "text": "Proof of Theorem 2: The fact that u0 minimizes F (\u00b7) is immediate from the definition. Also, given that F (\u00b7) is strongly convex and satisfies the eigenvalue condition stated in the theorem, the convergence bound for gradient descent follows from standard results (see [15]). Thus, it remains to prove the strong convexity and eigenvalue bounds.\nTo get these bounds, we use the same calculations as in the beginning of the proof of Theorem 1, to rewrite F (w) as\nF (w) = c2k 2 \u2016w\u2212 u0\u20162 + 1 2 \u00b7 Ez\n[\n( cos( \u221a k \u00b7w\u22a4z)\u2212 cos( \u221a k \u00b7 u\u22a40 z) )2 ] , (4)\nwhere z has a standard Gaussian distribution in Rd, and using the fact E[xix \u22a4 i ] = I is the identity matrix, and \u2211k\ni=1 xi is distributed as a Gaussian with mean 0 and variance kI. The following lemma will be useful in computing the Hessian of F :\nLemma 11 Given some b > 0, and assuming z has a standard Gaussian distribution in Rd, the Hessian of 1 2 \u00b7 Ez [ ( cos(b \u00b7w\u22a4z) \u2212 cos(b \u00b7w\u2217\u22a4z) )2 ] w.r.t. w has a spectral norm upper bounded by 2b2 + b.\nProof Differentiating the gradient as defined in Lemma 9, and noting that \u2202\u2202w\u03c6(ckw) = ck exp(\u2212c2k\u2016w\u20162/2)(I+ c2kww \u22a4) for any ck, we get that the Hessian equals\nb2\n2\n(\n\u2212 2 exp ( \u22122b2\u2016w\u20162 ) (I \u2212 2bww\u22a4)\u2212 exp ( \u2212b 2\u2016w\u2212 u0\u20162\n2\n)\n(I \u2212 b(w \u2212 u0)(w \u2212 u0)\u22a4)\n+ exp\n( \u2212b 2\u2016w+ u0\u20162\n2\n)\n(I \u2212 b(w + u0)(w + u0)\u22a4) )\n= b2\n2\n( \u22122 exp(\u22122b2\u2016w\u20162)\u2212 exp ( \u2212b 2\u2016w \u2212 u0\u20162\n2\n)\n+ exp\n( \u2212b 2\u2016w + u0\u20162\n2\n))\nI\n+ b\n2\n( 4b2 exp(\u22122b2\u2016w\u20162)ww\u22a4 + b2 exp ( \u2212b 2\u2016w\u2212 u0\u20162\n2\n)\n(w \u2212 u0)(w \u2212 u0)\u22a4\n\u2212 b2 exp ( \u2212b 2\u2016w+ u0\u20162\n2\n)\n(w + u0)(w + u0) \u22a4 ) .\nTherefore, its spectral norm is at most\nb2\n2 \u00b7 3 + b 2 ( 2 \u00b7 exp(\u22122b2\u2016w\u20162) ( 2b2\u2016w\u20162 ) + exp\n( \u2212b 2\u2016w\u2212 u0\u20162\n2\n)\n( b2\u2016w\u2212 u0\u20162 )\n+ exp\n( \u2212b 2\u2016w+ u0\u20162\n2\n)\n( b2\u2016w+ u0\u20162 )\n)\n.\nUsing the easily-verified fact that maxz\u22650 exp(\u2212z)z = exp(\u22121), we get that the above is at most\n3b2\n2 +\nb 2 exp(\u22121)(2 + 1 + 1) = 3b\n2\n2 + 2 exp(\u22121)b < 2b2 + b\nas required.\nWe are now in place to prove our theorem. Applying Lemma 11 and using the definition of the objective function F (w) at (4), we get that \u22072F (w) has eigenvalues in the range [c2k/2\u2212 (2k+ \u221a k), c2k/2+ (2k+ \u221a k)].\nSince ck = 3 \u221a k, we get that every eigenvalue of the Hessian is lower bounded by 9k2 \u22122k\u2212 \u221a k \u2265 9k2 \u22123k = 32k,\nand upper bounded by 9k2 +2k+ \u221a k \u2264 9k2 +3k = 152 k. Since k \u2265 1, this implies that the Hessian is positive definite everywhere (with minimal eigenvalue at least 3/2), hence F is strongly convex. Moreover,\nmaxw \u03bbmax(\u22072F (w)) minw \u03bbmin(\u22072F (w)) \u2264 15k/2 3k/2 = 5\nas required."}, {"heading": "C Proofs of Section 4", "text": ""}, {"heading": "C.1 Proofs of Section 4.2", "text": "Proof of Lemma 2: It suffices to show that V\u03c3(u0,u0) = Ex1\u03c3(u \u22a4 0 x1) 2 > ( 1\u2212 1k )\n, for in that case, by the independence of x1, . . . ,xk we have\nEx\n[\n(p(k) u0\n(x))2 ] = (Ex1 [\u03c3(u \u22a4 0 x1) 2])k >\n(\n1\u2212 1 k\n)k\n> 1\n4 .\nTo show that V\u03c3(u0,u0) > 1 \u2212 1k , note that from Lemma 1 we have that V\u03c3(u0,u0) = 2\u03c0 sin\u22121 ( 2\u2016u0\u20162 1+2\u2016u0\u20162 ) . Denote by f(a) the value for which 2\u03c0 sin \u22121 ( f(a) 1+f(a) )\n= 1\u2212 a. By standard algebraic manipulations we have that\nf(a) = sin(\u03c02 (1\u2212 a))\n1\u2212 sin(\u03c02 (1\u2212 a)) .\nUsing Taylor\u2019s theorem we have that there exists \u03be \u2208 [\u03c02 (1 \u2212 a), \u03c02 ] such that\nsin(\u03c02 (1\u2212 a)) = 1\u2212 1\n2\n(\u03c0 2 a )2 + 1 6 cos(\u03be) (\u03c0 2 a )3 = 1\u2212 (\u03c0 2 a )2\n[\n1 2 \u2212 1 6 cos(\u03be) \u03c0 2 a\n]\n.\nIt follows that\nf(a) = 1\n(\n\u03c0 2 a )2 [1 2 \u2212 16 cos(\u03be)\u03c02a ]\n\u2212 1 \u2264 1 (\n\u03c0 2 a )2 [ 1 2 \u2212 16 cos(\u03be)\u03c02 a ]\n\u2264 3 (\n\u03c0 2 a\n)2 ,\nwhere in the last inequality we assume that a \u2208 [0, 1/2]. Taking a = 1/k and noting that V\u03c3(u0,u0) monotonically increases with \u2016u0\u2016 we conclude our proof."}, {"heading": "C.2 Proofs of Section 4.3", "text": "Proof of Lemma 3: We start by expanding F (1,k)(w):\nF (1,k)(w)\n=Ex [ (pw(x)\u2212 pu0(x))2 ]\n=Ex\n[\n(p(1) w (x) \u2212 p(1) u0 (x) + p(k) w (x)\u2212 p(k) u0\n(x))2 ]\n=Ex\n[\n(p(1) w (x) \u2212 p(1) u0\n(x))2 ]\n+ Ex\n[\n(p(k) w (x)\u2212 p(k) u0\n(x))2 ]\n+ 2Ex\n[\n(p(1) w (x)\u2212 p(1) u0 (x))(p(k) w (x) \u2212 p(k) u0\n(x)) ]\n=F (1)(w) + F (k)(w) + 2Ex\n[\n(p(1) w (x) \u2212 p(1) u0 (x))(p(k) w (x)\u2212 p(k) u0\n(x)) ]\nWe next show that Ex\n[\n(p (1) w (x) \u2212 p(1)u0 (x))(p(k)w (x)\u2212 p(k)u0 (x))\n]\n= 0. Since \u03c3 is anti-symmetric and xk is\nnormal, we have that for every vector w\u2032, Exk\u03c3(w \u2032\u22a4xk) = 0. By the independence of the xi\u2019s,\nEx\n[\np(1) w (x) \u00b7 p(k) w\n(x) ]\n=Ex1p (1) w (x)Ex2...k\u22121\n[ k\u22121 \u220f\ni=1\n\u03c3(w\u22a4i xi)\n]\nExk\n[ \u03c3(w\u22a4k xk) ] = 0\nThe same argument holds for the other terms, and the result follows.\nProof of Lemma 4 We start with the FC setting. Since \u03c3 is antisymmetric we clearly have that\np(k) w\n(x) = k \u220f\nl=1\n\u03c3(w\u22a4l xl) = (\u22121)k k \u220f\nl=1\n\u03c3(\u2212w\u22a4l xl) = (\u22121)kp(k)w (\u2212x) .\nNext, for every l, let g (k) w\n\u2032 l be the derivative w.r.t. the weights corresponding to the l\u2019th input instance, then\ng (k) w\n\u2032 l (x) =\n\n\n\u220f j 6=l \u03c3(w\u2032jxj)\n  \u03c3\u2032(w\u2032lxl)xl = (\u22121)k   \u220f\nj 6=l \u03c3(\u2212w\u2032jxj)\n\n\u03c3\u2032(\u2212w\u2032lxl)(\u2212xl) = (\u22121)kg(k)w\u2032l (\u2212x) ,\nwhere we used the fact that \u03c3\u2032 is symmetric. The claim follows because (\u22121)2k = 1. Finally, for the WS setting, we can think of w\u2032 as being k copies of w\u20320 and then, by standard derivative rules, g (k) w\n\u2032 0 =\n\u2211 l g (k) w\n\u2032 l ,\nfrom which the claim follows.\nProof of Lemma 5:. For the first term:\nal :=Ex\n[\np(k) w (x)g (k) l (x)\n]\n= 1\n2 E x: w\u22a4l xl>0\n[\np(k) w (x)g (k) l (x)\n] + 1\n2 E x: w\u22a4l xl<0\n[\np(k) w (x)g (k) l (x)\n]\n(0) =\n1 2 E x: w\u22a4l xl>0 [ p(k) w (x)g (k) l (x) ] + 1 2 E x: w\u22a4l xl<0 [ p(k) w (\u2212x)g(k)l (\u2212x) ]\n(1) =\n1 2 E x: w\u22a4l xl>0 [ p(k) w (x)g (k) l (x) ] + 1 2 E x: w\u22a4l xl>0 [ p(k) w (x)g (k) l (x) ]\n=E x: w\u22a4l xl>0\n[\np(k) w (x)g (k) l (x)\n]\n=E x: w\u22a4l xl>0\n\n( \u220f\nj\n\u03c3(w\u22a4j xj))( \u220f j 6=l \u03c3(w\u22a4j xj))\u03c3 \u2032(w\u22a4l xl)xl\n\n\n(2) =Ex[k]\\{l}\n\n\n\u220f j 6=l \u03c32(w\u22a4j xj)\n\n \u00b7 E xl: w\u22a4l xl>0\n[ \u03c3(w\u22a4l xl)\u03c3 \u2032(w\u22a4l xl)xl ]\n(3) =\n\n\n\u220f j 6=l V\u03c3(wj ,wj)\n\n \u00b7 E xl: w\u22a4l xl>0\n[ \u03c3(w\u22a4l xl)\u03c3 \u2032(w\u22a4l xl)xl ]\n(4) =\n\n\n\u220f j 6=l V\u03c3(wj ,wj)\n\n \u00b7 c1(wl)wl .\nIn the above, (0) is from Lemma 4, (1) uses symmetry of the probability of x, and (2), (3) follow from the fact all xl are i.i.d.. To see why (4) is true, and why c1(wl) \u2265 0, note that we can assume w.l.o.g. that wl = (\u2016wl\u2016, 0, . . . , 0) (because xl is Gaussian), and in this case it is clear that the first coordinate of E xl: w\u22a4l xl>0 [ \u03c3(w\u22a4l xl)\u03c3 \u2032(w\u22a4l xl)xl ]\nis positive while the rest of the coordinates are zero. For the second part of the lemma, similar arguments give:\nbl :=Ex\n[\np(k) u0 (x)g (k) l (x)\n]\n(0) = E\nx: u\u22a40 xl>0\n[\np(k) u0 (x)g (k) l (x)\n]\n=E x: u\u22a40 xl>0\n\n\n\n\n\u220f j 6=l \u03c3(u\u22a40 xj)\u03c3(w \u22a4 j xj)\n\n \u03c3(u\u22a40 xl)\u03c3 \u2032(w\u22a4l xl)xl\n\n\n=\n\n\n\u220f j 6=l V\u03c3(u0,wj)\n\n \u00b7 E x: u\u22a40 xl>0\n[ \u03c3(u\u22a40 xl)\u03c3 \u2032(w\u22a4l xl)xl ]\nwhere (0) is a similar transition to that done for al, using Lemma 4. Using again the normality of x, it is easy to see that b\u0303l := Ex: u\u22a40 xl>0 [ \u03c3(u\u22a40 xl)\u03c3 \u2032(w\u22a4l xl)xl ] is in the span of {u,wl}."}, {"heading": "C.3 Proofs of Section 4.4", "text": "Proof of Lemma 6: The random variable, 1d w \u22a4 l u0 is an average of d random variables, each of which distributed uniformly over {\u00b1cu0,i}, and its expected value is zero. Hence, by Hoeffding\u2019s inequality,\nPr [ |w\u22a4l u0| \u2016wl\u2016 \u2016u0\u2016 > d\u22121/3 ] = Pr [ 1 d |w\u22a4l u0| > \u2016wl\u2016 \u2016u0\u2016 d\u22124/3 ] \u2264 2 exp ( \u22120.5 d1/3 ) .\nApplying a union bound over the k weight vectors, we conclude our proof.\nProof of Lemma 7: By the symmetry of Verf we can assume w.l.o.g. that u \u22a4 0 wj > 0, and then Verf(wj ,u0) > 0. We can rewrite\nVerf(wj ,u0) = 2 \u03c0 sin\u22121\n(\n2u\u22a40 wj \u221a\n1 + 2\u2016u0\u20162 \u221a 1 + 2\u2016wj\u20162\n)\n= 2\n\u03c0 sin\u22121\n\n u\u22a40 wj \u2016u0\u2016 \u2016wj\u2016\n\u00b7 \u221a 2\u2016u0\u2016 \u221a\n1 + ( \u221a 2\u2016u0\u2016)2\n\u00b7 \u221a 2 \u2016wj\u2016 \u221a\n1 + ( \u221a 2\u2016wj\u2016)2\n\n\nThe function f(a) = a\u221a 1+a2 is monotonically increasing over a \u2208 [0,\u221e), where f(a) = 0 and f(a) \u2192 1 as a \u2192 \u221e. Therefore, all the three terms in the argument of the inverse sign are in [0, 1]. If the first condition in the theorem holds then the first term makes the argument of the inverse sign at most c. If the second condition in the theorem holds then, since f(a) \u2264 a, we have that the third term is at most c. The claim now follows immediately because for every c \u2208 [0, 1] we have c \u2264 sin(c).\nProof of Lemma 8: By Lemma 7, Verf(wj ,u0) < 1 16 . From Lemma 3, we have that F (1,k) \u2265 F (k). Also, by Lemma 2, Ex [ (p (k) u0 (x)) 2 ] > 14 . Thus,\nF (1,k) \u2265 F (k) = Ex [ (p(k) w (x)\u2212 p(k) u0 (x))2 ]\n\u2265 Ex [ (p(k) w (x))2 ] \u2212 2Ex [ p(k) w (x)p(k) u0 (x) ] + 1\n4\n\u2265 \u2212 2Ex [ p(k) w (x)p(k) u0 (x) ] + 1\n4\n= \u2212 2 \u220f\nl\nVerf(wl,u0) + 1\n4\n\u2265 \u2212 2 1 16 \u220f\nl 6=j |Verf(wl,u0)|+\n1 4 \u2265 \u2212 1 8 + 1 4 = 1 8\nProof of Theorem 3 To simplify the notation throughout this proof, whenever we write wl we mean for l \u2265 2. Recall that the gradient w.r.t. wl is equal to:\n\u2207wlF (1,k)(w(t)) =\n\n\n\u220f j 6=l V\u03c3(wj ,wj)\n\n \u00b7 c1(wl)wl \u2212\n\n\n\u220f j 6=l V\u03c3(u0,wj)\n\n \u00b7 b\u0303l.\nLet T = \u230a 1\u03b7kd k 3\u22123\u230b. We firstly prove, by induction, that for all t \u2208 {0, 1, . . . , T }, at least one of the following holds for every l:\n1. |(w(t)l ) \u22a4 u0|\n\u2016w(t)l \u2016 \u2016u0\u2016 \u2264 d\u22121/3 + \u03b7t \u00b7 d\u2212 k\u221223 +2,\n2. at some t\u2032 \u2264 t, it held that \u2016w(t \u2032) l \u2016 < 1/d, and \u2016w (t) l \u2016 < 1/d+ \u03b7(t\u2212 t\u2032) \u00b7 d\u2212(k\u22122)/3+1.\nThis holds w.h.p. for t = 0, by Lemma 6. For the inductive step, note that if the claim holds for some t \u2264 T , then by the definition of T we have that \u03b7td\u2212 k\u22122 3 +2 \u2264 d\u22121/3/k. Hence, by Lemma 7, the coefficient of b\u0303l is at most (\n1 + 1\nk\n)k\u22122 \u00b7 d\u2212(k\u22122)/3 \u2264 e d\u2212(k\u22122)/3 \u2264 d\u2212(k\u22122)/3+1 ,\nwhere we assume that d \u2265 e. Moreover, it is easy to see that \u2016b\u0303l\u2016 < 1. In addition, it is easy to see that the coefficient of w\n(t) l in the first term of the gradient is positive. Therefore, if the second assumption\nheld for l at time t, then by observing the explicit expression of the gradient, we obtain that the only term which can increase \u2016w(t)l \u2016 is smaller in magnitude than \u03b7 d\u2212(k\u22122)/3+1, and hence, the second assumption holds for t + 1. If on the other hand, the first assumption held for l at time t. If at t + 1, its norm decreases below 1/d, we are done. Otherwise, note that the only term of the gradient which can change the direction of w (t) l is the second one, which is again, smaller in magnitude than \u03b7 d \u2212(k\u22122)/3+1. Now, since \u2016w(t+1)l \u2016 > 1/d, we obtain that the change in angle, d\u03b1, between wl,u0, in times t, t + 1, satisfies d\u03b1 < tan \u22121 ( \u03b7 d\u2212(k\u22122)/3+1\n1/d\n)\n= tan\u22121(\u03b7d\u2212(k\u22122)/3+2). Therefore, denoting by \u03b1(t) the angle at time t, we have,\n| |(w (t+1) l ) \u22a4u0| \u2016w(t+1)l \u2016 \u2016u0\u2016 \u2212 |(w (t) l ) \u22a4u0| \u2016w(t)l \u2016 \u2016u0\u2016 | = | cos\u03b1(t+1) \u2212 cos\u03b1(t)|\n(0) \u2264 |\u03b1(t+1) \u2212 \u03b1(t)| \u2264 tan\u22121(\u03b7d\u2212(k\u22122)/3+2) (1) \u2264 \u03b7d\u2212(k\u22122)/3+2\nwhere (0) and (1) are by the 1-Lipschitzness of cos and tan\u22121. The inductive step follows immediately. From this proof, combined with Lemma 8, we obtain that for all T = O(dk/4), F (1,k)(w(T )) > 1/8, as required."}, {"heading": "C.4 Proofs of Section 4.5", "text": "Proof of Theorem 5: Firstly, we use the lemmas from Section 4.3 to write the gradient as:\n\u2207F (1,k)(w0) =\u2207F (1)(w0) +\u2207F (k)(w0) =c1(w0)w0 \u2212 b\u03031 + kV\u03c3(w0,w0)k\u22121 \u00b7 c1(w0)w0 \u2212 kV\u03c3(u0,w0)k\u22121 \u00b7 b\u03031 =(1 + kV\u03c3(w0,w0) k\u22121)c1(w0)w0 \u2212 (1 + kV\u03c3(u0,w0)k\u22121)b\u03031\nObserve that a gradient update is the subtraction of the two terms (scaled by \u03b7) from w0. Hence, the first term does not change the angle \u03b1. We further note that this update adds b\u03031 to w0 with a positive coefficient. It is therefore sufficient to show that the second term has positive inner product withw\u22a5 := (\u2212 cos(\u03b1), sin(\u03b1)), see Figure 2.\nRecall that b\u03031 = Ex1: u\u22a40 x1>0 [\u03c3(u0x1)\u03c3 \u2032(w0x1)x1]. We note that for every x = (\u03b8, r) (in polar coordinates) s.t. \u03b8 \u2208 [0, \u03c0/2\u2212 \u03b1], we can look at its reflection over the w0 axis, namely, x\u0303 = (\u03c0 \u2212 2\u03b1 \u2212 \u03b8, r), and note that:\n\u2022 \u03c3\u2032(w0x) = \u03c3\u2032(w0x\u0303),\n\u2022 \u3008x,w\u22a5\u3009 = \u2212\u3008x\u0303,w\u22a5\u3009,\n\u2022 \u03c3(u0x) < \u03c3(u0x\u0303).\nLet A1 be the event that \u03b8 \u2208 [0, \u03c0 \u2212 2\u03b1]. By the above properties, Ex\u2208A1 [\u3008\u03c3(u0x)\u03c3\u2032(w0x)x,w\u22a5\u3009] =Ex\u2208A1 : w\u22a4\u22a5x<0 [(\u03c3(u0x)\u2212 \u03c3(u0x\u0303))\u03c3 \u2032(w0x)\u3008x,w\u22a5\u3009] > 0, since \u3008x,w\u22a5\u3009 < 0, (\u03c3(u0x) \u2212 \u03c3(u0x\u0303)) < 0, and \u03c3\u2032(w0x) \u2265 0. Thus, \u3008b\u03031,w\u22a5\u3009 \u2265 Pr(Ac1)Ex\u2208Ac1 [\u3008\u03c3(u0x1)\u03c3\n\u2032(w0x1)x1,w\u22a5\u3009] Moreover, note that for all x \u2208 Ac1, the expression in the expectation is non negative. Hence we obtain that \u3008b\u03031,w\u22a5\u3009 \u2265 0 for all b\u03031, and indeed, \u03b1(t+1) \u2264 \u03b1(t). We shall now show a positive lower bound. Let A2 be the event that \u03b8 \u2208 [\u03c0 \u2212 32\u03b1, \u03c0 \u2212 12\u03b1], and 1\u2016u0\u2016 \u2264 r/ \u221a 2 < 2\u2016u0\u2016 . In the angular aspect, these are, in words, elements with an angle of \u03b1/2 around w\u22a5, see Figure 2 for an illustration. Firstly note that A2 \u2282 Ac1, so from positivity of the inner expression in the expectation:\n\u3008b\u03031,w\u22a5\u3009 \u2265 Pr(A2)Ex\u2208A2 [\u3008\u03c3(u0x1)\u03c3\u2032(w0x1)x1,w\u22a5\u3009] Firstly, let us lower bound Pr(A2). In the angular aspect, it is clear that Pr ( \u03b8 \u2208 [\u03c0 \u2212 32\u03b1, \u03c0 \u2212 12\u03b1] ) = \u03b1\u03c0 .\nFor each such \u03b8, the r/ \u221a 2 value of x is distributed |N (0, 1)|. It is clear from monotonicity of the Gaussian pdf for positive r, that: Pr (\n1 \u2016u0\u2016 < r/ \u221a 2 < 2\u2016u0\u2016 ) \u2265 1\u2016u0\u2016 \u00b7 2\u221a 2\u03c0 e \u2212 22 2\u2016u0\u2016 2 > 15\u2016u0\u2016 . Therefore, we continue:\n\u3008b\u03031,w\u22a5\u3009 \u2265 \u03b1\n5\u03c0\u2016u0\u2016 min x\u2208A2 \u03c3(ux) min x\u2208A2 \u03c3\u2032(wx) min x\u2208A2 \u3008x,w\u22a5\u3009\nIt is easy to see where each of the minimas is obtained, and hence we have:\n\u3008b\u03031,w\u22a5\u3009 \u2265 \u03b1\n5\u2016u0\u2016\u03c0 \u03c3\n( \u2016u0\u2016 sin(\u03c0 \u2212 \u03b12 )\n\u2016u0\u2016\n) \u03c3\u2032 ( \u2016w0\u2016 2(cos\u03b1 sin \u03b12 \u2212 sin\u03b1 cos \u03b12 )\n\u2016u0\u2016\n)\n(\ncos\u03b1 cos \u03b12 + sin\u03b1 sin \u03b1 2\n)\n\u2016u0\u2016 (0) \u2265 \u03b1 5\u2016u0\u20162\u03c0 \u03c3 ( \u2016u0\u2016 sin(\u03c0 \u2212 \u03b12 ) \u2016u0\u2016 ) \u03c3\u2032 ( \u2016u0\u2016 \u22122 sin \u03b12 \u2016u0\u2016 ) ( cos \u03b1 2 )\n(1) =\n\u03b1 5\u2016u0\u20162\u03c0 \u03c3 ( sin \u03b1 2 ) \u03c3\u2032 ( 2 sin \u03b1 2 )( cos \u03b1 2 )\n\u2265 \u03b1 5\u2016u0\u20162\u03c0 \u03c3 ( sin \u03b1 2 ) \u03c3\u2032 ( 2 sin \u03c0 4 )( cos \u03c0 4 )\n:=f(\u03b1, \u2016u0\u2016),\nwhere (0) is from \u2016w0\u2016 \u2264 \u2016u0\u2016 (the projection step we might have added does not change the angle) and (1) from symmetry of \u03c3\u2032. Observe that the erf\u2019s derivative at 0 is 1. Combined with the fact that the sine function behaves the same at the neighbourhood of 0, we obtain that \u03c3 (\nsin \u03b12 ) = \u0398(\u03b12 ), when \u03b1 \u2192 0. We obtain that f(\u03b1, \u2016u0\u2016) = \u2126( \u03b1 2\n\u2016u0\u20162 ). Now, we use this in order to examine the angular improvement. Since\n\u2016w0\u2016 \u2264 \u2016u0\u2016, we obtain:\n\u03b4(t+1)\u03b1 := \u03b1 (t) \u2212 \u03b1(t+1) \u2265 tan\u22121\n( f(\u03b1(t), \u2016u0\u2016) \u2016u0\u2016 ) .\nFor reasonably large k, the argument of the arctan is smaller than 1, and on the interval [0, 1] the value of arctan is larger than half its argument. This implies that \u03b4 (t+1) \u03b1 \u2265 \u2126(\u03b12/k3), or\n\u03b1(t+1) \u2264 ( 1\u2212 \u2126((\u03b1(t))2/k3) ) \u03b1(t) .\nThis proves that after O((k/\u01eb)3) iterations the value of \u03b1 must be smaller than \u01eb.\nProof of Theorem 6: Let r = \u2016u0\u2016. Fix w0 \u2208 Bd(0, r). For some s > 0, define the event As = {x1 : max{|u\u22a40 x1|, |w\u22a40 x1|, |(u0 \u2212w0)\u22a4x1|} < s}. Now,\n\u3008w0 \u2212 u0,\u2207F (1)(w0)\u3009 =Ex1 [( \u03c3(w\u22a40 x1)\u2212 \u03c3(u\u22a40 x1) ) \u03c3\u2032(w\u22a40 x1) \u00b7 \u3008w0 \u2212 u0,x1\u3009 ]\nNote that \u03c3\u2032 > 0, hence \u03c3 is monotonically increasing, implying non negativity of the inner expression for all x1. For some s, let L(s) = \u03c3\n\u2032(s). Then, by the properties of \u03c3, for all a \u2208 [\u2212s, s], \u03c3\u2032(a) \u2265 L(s) > 0. We proceed:\n\u3008w0 \u2212 u0,\u2207F (1)(w0)\u3009 (1) \u2265Ex1 [( \u03c3(w\u22a40 x1)\u2212 \u03c3(u\u22a40 x1) ) \u03c3\u2032(w\u22a40 x1) \u00b7 \u3008w0 \u2212 u0,x1\u30091As ]\n(2) \u2265L(s) \u00b7 Ex1 [( \u03c3(w\u22a40 x1)\u2212 \u03c3(u\u22a40 x1) ) \u00b7 \u3008w0 \u2212 u0,x1\u30091As ]\n(3) \u2265L2(s) \u00b7 Ex1 [ \u3008w0 \u2212 u0,x1\u300921As ]\nwhere (1) is from non negativity of the inner expression, (2) from the lower bound over the derivative of \u03c3, applicable from the occurence of As, (3) from the Mean Value Theorem, again applicable for the event As. x1 is standard Gaussian, so Ex1 [ \u3008w0 \u2212 u0,x1\u30092 ] = \u2016w0 \u2212 u0\u201622. We continue to develop the lower bound:\n\u3008w0 \u2212 u0,\u2207F (1)(w0)\u3009 \u2265L2(s) \u00b7 Ex1 [ \u3008w0 \u2212 u0,x0\u300921As ]\n=L2(s) \u00b7 (\nEx1\n[ \u3008w0 \u2212 u0,x1\u30092 ] \u2212 Ex1 [ \u3008w0 \u2212 u0,x1\u300921Acs ])\n(1) =L2(s) \u00b7 ( \u2016w0 \u2212 u0\u20162 \u2212 Ex1 [ \u3008w0 \u2212 u0,x1\u300921Acs ]) (2) \u2265L2(s) \u00b7 ( \u2016w0 \u2212 u0\u20162 \u2212 ( Ex1 [ \u3008w0 \u2212 u0,x1\u30094 ] Pr(Acs) )1/2 )\n(3) =L2(s) \u00b7\n( \u2016w0 \u2212 u0\u20162 \u2212 \u2016w0 \u2212 u0\u20162 (3 Pr(Acs))1/2 )\n=L2(s) \u00b7 \u2016w0 \u2212 u0\u20162 ( 1\u2212 (3 Pr(Acs))1/2 )\nwhere (1) is from the fact x1 is standard Gaussian, (2) is from Cauchy-Schwartz, (3) is from direct computation using the fourth moment of a Gaussian. Let s\u0303 be sufficiently large, such that (3 Pr(Acs\u0303)) 1/2 < 1\u2212 1/k. This is possible for s = \u0398(1), since, when assuming w.l.o.g. u0 \u2208 R2, Pr(u\u22a40 x < 1) \u2265 1\u2016u0\u2016 1\u221a 2\u03c0 e \u22121 2\u2016u0\u2016 2 = \u2126 ( k\u22121 ) , and the same for w0,u0 \u2212w0. Then,\n\u3008w0 \u2212 u0,\u2207Fu0(w0)\u3009 \u2265 L2(s\u0303)\nk \u00b7 \u2016w0 \u2212 u0\u20162"}, {"heading": "C.4.1 Proof of Theorem 7", "text": "Before proving Theorem 7, we state and prove two technical lemmas:\nLemma 12 Let \u03b11 = tan \u22121 \u01eb 2\u2016u0\u2016 , and assume \u03b1 \u2208 [0, \u03b11). Then at least one of the following holds:\n\u2022 \u2016u0 \u2212w0\u2016 < \u01eb,\n\u2022 sign (\u3008w0 \u2212 u0,w0\u3009) = \u22121.\nProof It is easy to verify that if the first condition doesn\u2019t hold, then the worst case scenario is when \u03b1 = \u03b11 and w0 lies on the \u01eb-sphere around u0. So, from now on we assume that \u03b1 = \u03b11 and w0 lies on the \u01eb-sphere around u0. Consider the vector z, whose angle w.r.t. u0 is \u03b1, and such that the angle between z and u0 \u2212 z is exactly 90 degrees. Since z and w0 points to the same direction, It is easy to verify that it suffices to show that \u2016z\u2212 u0\u2016 < \u01eb. To see this, we observe that \u2016z\u2016\u2016u0\u2016 = cos(\u03b1), hence\n\u2016z\u2016 = \u2016u0\u2016 cos tan\u22121 \u01eb\n2\u2016u0\u2016 = \u2016u0\u2016\n1 \u221a\n1 + (\n\u01eb 2\u2016u0\u2016\n)2\nBy the Pythagorean theorem,\n\u2016z\u2212 u0\u20162 = \u2016u0\u20162 \u2212 \u2016z\u20162 = \u2016u0\u20162    1\u2212 1\n1 + (\n\u01eb 2\u2016u0\u2016\n)2\n\n  \u2264 \u01eb\n2\n4 ,\nwhich concludes our proof.\nLemma 13 Assume 0 \u2264 \u03b1 < min{tan\u22121 (\n\u01eb 2\u2016u0\u2016\n)\n, \u221a\n\u01eb \u2016u0\u2016}. Then at least one of the following holds:\n\u2022 \u2016u0 \u2212w0\u2016 < \u01eb, \u2022 \u2016w0\u2016 \u221a\n1+2\u2016u0\u20162\u221a 1+2\u2016w0\u20162\u2016u0\u2016 \u2264 cos\u03b1.\nProof It is clear that if not \u2016u0 \u2212w0\u2016 < \u01eb, then \u2016w0\u2016 < \u2016u0\u2016 \u2212 \u01eb2 , by the fact \u03b1 < tan\u22121 ( \u01eb 2\u2016u0\u2016 ) . Define the function f(x) = x\u221a 1+2x2 . We compute its derivatives, and note inequalities for x > 1:\nf \u2032(x) = 1 + 2x\n2\n1+2x2\u221a 1 + 2x2 > 0, f \u2032\u2032(x) =\n4x2 1+2x2 \u2212 ( 1 + 2x 2 1+2x2 )\n2x \u221a 1 + 2x2 < 0.\nObserve that f is monotonically increasing, and concave for x > 1, and in particular, for x = \u2016u0\u2016 > 1. Therefore, f(\u2016w0\u2016) < f(\u2016u0\u2016 \u2212 \u01eb/2) < f(\u2016u0\u2016)\u2212 \u01eb\u00b7f \u2032(\u2016u0\u2016) 2 . We obtain:\n\u2016w0\u2016 \u221a 1 + 2\u2016u0\u20162 \u221a\n1 + 2\u2016w0\u20162\u2016u0\u2016 = f(\u2016w0\u2016) f(\u2016u0\u2016) \u2264 f(\u2016u0\u2016)\u2212 \u01eb\u00b7f \u2032(\u2016u0\u2016) 2 f(\u2016u0\u2016) = 1\u2212 \u01eb 2 f \u2032(\u2016u0\u2016) f(\u2016u0\u2016)\n= 1\u2212 \u01eb 2\n(\n1\n\u2016u0\u2016 + 2\u2016u0\u2016 1 + 2\u2016u0\u20162 ) \u2264 1\u2212 \u01eb 2\u2016u0\u2016\nFrom observing the Taylor series of cos, we have that cos\u03b1 \u2265 1 \u2212 \u03b122 . Thus, from the assumption 0 \u2264 \u03b1 < \u221a\n\u01eb \u2016u0\u2016 ,\ncos\u03b1 \u2265 cos \u221a \u01eb \u2016u0\u2016 \u2265 1\u2212 \u01eb 2\u2016u0\u2016 \u2265 \u2016w0\u2016\n\u221a\n1 + 2\u2016u0\u20162 \u221a\n1 + 2\u2016w0\u20162\u2016u0\u2016 .\nProof of Theorem 7. \u2207F (1,k) can be written as:\n\u2207F (1,k)(w0) = \u2207F (1)(w0) +\u2207F (k)(w0) = c1(w0)w0 \u2212 b\u03031 + kV\u03c3(w0,w0)k\u22121 \u00b7 c1(w0)w0 \u2212 kV\u03c3(u0,w0)k\u22121 \u00b7 b\u03031 = (1 + kV\u03c3(w0,w0)\nk\u22121)c1(w0)w0 \u2212 (1 + kV\u03c3(u0,w0)k\u22121)b\u03031 = ( (1 + kV\u03c3(w0,w0) k\u22121)\u2212 (1 + kV\u03c3(u0,w0)k\u22121) ) c1(w0)w0 + (1 + kV\u03c3(u0,w0) k\u22121)\u2207F (1) = kc1(w0) ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 ) w0 + (1 + kV\u03c3(u0,w0) k\u22121)\u2207F (1).\nWe already have, by Theorem 6, that \u3008w0 \u2212 u0,\u2207F (1)\u3009 \u2265 L 2(s\u0303) k \u2016w0 \u2212 u0\u20162, and therefore (as \u03b1 < \u03c0/2, implying (1 + kV\u03c3(u0,w0) k\u22121) > 1), a similar result applies for this term of the gradient. It is hence sufficient to show that the first term does not worsen the lower bound, namely,\nsign ( \u3008w0 \u2212 u0, kc1(w0) ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 ) w0\u3009 ) \u2265 0\nWe observe a few equivalencies:\nsign ( \u3008w0 \u2212 u0, kc1(w0) ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 ) w0\u3009 )\n(0) = sign ( \u3008w0 \u2212 u0, ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 ) w0\u3009 )\n= sign ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 ) \u00b7 sign (\u3008w0 \u2212 u0,w0\u3009) (1) = \u2212 sign ( V\u03c3(w0,w0) k\u22121 \u2212 V\u03c3(u0,w0)k\u22121 )\n(2) = \u2212 sign\n(\n2w\u22a40 w0 \u221a\n1 + 2\u2016w0\u20162 \u221a 1 + 2\u2016w0\u20162 \u2212 2w\n\u22a4 0 u0\n\u221a 1 + 2\u2016w0\u20162 \u221a 1 + 2\u2016u0\u20162\n)\n= \u2212 sign ( w\u22a40 w0 \u221a\n1 + 2\u2016w0\u20162 \u2212 w\n\u22a4 0 u0\n\u221a\n1 + 2\u2016u0\u20162\n)\nwhere (0) is from the fact kc1(w0) \u2265 0, (1) is from Lemma 12, and (2) is from positivity of w\u22a40 u0, and monotonicity of the sin\u22121 in the explicit expression for Verf , for positive w\u22a40 u0. It is left to show w\n\u22a4 0 w0 \u221a 1+2\u2016u0\u20162\u221a\n1+2\u2016w0\u20162 \u2264 w\u22a40 u0 = \u2016w0\u2016\u2016u0\u2016 cos\u03b1, equivalent, after rearrangement, to\n\u2016w0\u2016 \u221a\n1+2\u2016u0\u20162\u221a 1+2\u2016w0\u20162\u2016u0\u2016 \u2264 cos\u03b1, which we have from Lemma 13."}, {"heading": "C.4.2 Proof of Theorem 8", "text": "Proof of Theorem 8. Combining Theorem 7 with the Cauchy-Schwartz inequality we obtain that,\n\u2016\u2207F (1,k)(w0)\u2016 \u2265 L2(s\u0303)\nk \u00b7 \u2016w0 \u2212 u0\u2016.\nOn the other hand, it is easy to see that \u2016\u2207F (1,k)(w0)\u2016 \u2264 O(k2). Let w(t)0 be the weight vector after the tth iteration of GD, \u03b7 the learning rate. Assume that for all t\u2032 < t, we did not converge yet, namely, \u2016w(t \u2032)\n0 \u2212 u0\u2016 > \u01eb. Then,\n\u2016w(t+1)0 \u2212 u0\u20162 \u2212 \u2016w (t) 0 \u2212 u0\u20162 =\u2016w (t) 0 \u2212 \u03b7\u2207F (w (t) 0 )\u2212 u0\u20162 \u2212 \u2016w (t) 0 \u2212 u0\u20162\n=\u2212 2\u03b7\u3008\u2207F (w(t)0 ),w (t) 0 \u2212 u0\u3009+ \u03b72\u2016\u2207F (w (t) 0 )\u20162\nWe can now use the previous bounds from Theorem 7 to obtain, after rearranging:\n\u2016w(t+1)0 \u2212 u0\u20162 =\u2016w (t) 0 \u2212 u0\u20162 \u2212 2\u03b7\u3008\u2207F (w (t) 0 ),w (t) 0 \u2212 u0\u3009+ \u03b72\u2016\u2207F (w (t) 0 )\u20162\n\u2264\u2016w(t)0 \u2212 u0\u20162 \u2212 2\u03b7 L2(s\u0303)\nk \u00b7 \u2016w0 \u2212 u0\u20162 + \u03b72 k4\nTaking \u03b7 = L 2(s\u0303) \u01eb2 k5 we obtain that as long as \u2016w (t) 0 \u2212 u0\u2016 > \u01eb then\n\u2016w(t+1)0 \u2212 u0\u20162 \u2264 \u2016w (t) 0 \u2212 u0\u20162 \u2212\nL4(s\u0303) \u01eb4\nk6 ,\nwhich implies that after at most poly(1/\u01eb, k) iterations we must have \u2016w(t)0 \u2212 u0\u2016 \u2264 \u01eb."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Weakly learning dnf and characterizing statistical query learning using fourier analysis", "author": ["Avrim Blum", "Merrick Furst", "Jeffrey Jackson", "Michael Kearns", "Yishay Mansour", "Steven Rudich"], "venue": "In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs", "author": ["Alon Brutzkus", "Amir Globerson"], "venue": "arXiv preprint arXiv:1702.07966,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Approximate resilience, monotonicity, and the complexity of agnostic learning", "author": ["Dana Dachman-Soled", "Vitaly Feldman", "Li-Yang Tan", "Andrew Wan", "Karl Wimmer"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Sgd learns the conjugate kernel class of the network", "author": ["Amit Daniely"], "venue": "arXiv preprint arXiv:1702.08503,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["Benjamin D Haeffele", "Ren\u00e9 Vidal"], "venue": "arXiv preprint arXiv:1506.07540,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Identity matters in deep learning", "author": ["Moritz Hardt", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1611.04231,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["Adam Tauman Kalai", "Ravi Sastry"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Song Mei", "Yu Bai", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["Itay Safran", "Ohad Shamir"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Failures of deep learning", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Shaked Shammah"], "venue": "arXiv preprint arXiv:1703.07950,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Ohad Shamir"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "author": ["Daniel Soudry", "Yair Carmon"], "venue": "arXiv preprint arXiv:1605.08361,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Schaum\u2019s handbook of formulas and tables", "author": ["Murray R Spiegel"], "venue": "MacGraw Hill, New York,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1968}, {"title": "Computing with infinite networks. Advances in neural information processing systems, pages", "author": ["Christopher KI Williams"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}], "referenceMentions": [{"referenceID": 17, "context": "In [18], it has been shown that no Gradient Based algorithm can succeed in learning h\u2217 if g\u2217 is the parity of the signs of its input.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 4, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 19, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 7, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 8, "context": "To mention just a few pertinent examples, [16, 5, 20, 8, 9] consider the optimization landscape for various networks, showing it has favorable properties under various assumptions, but does not consider the behavior of a specific algorithm.", "startOffset": 42, "endOffset": 59}, {"referenceID": 12, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 1, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 9, "context": "Other works, such as [13, 2, 10, 23], show how certain neural networks can be learned under (generally strong) assumptions, but not with standard gradient-based methods.", "startOffset": 21, "endOffset": 36}, {"referenceID": 0, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 3, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 6, "context": "More closer to our work, [1, 4, 7] provide positive learning results using gradient-based algorithms, but do not show the benefit of a convolutional architecture for optimization performance, compared to a fully-connected architecture.", "startOffset": 25, "endOffset": 34}, {"referenceID": 5, "context": "The hardness of learning in the case of Boolean functions, using the degree of the target function, was discussed in the statistical queries literature, for instance in [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 17, "context": "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "In terms of techniques, our construction is inspired by target functions proposed in [18, 19], and based on ideas from the statistical queries literature (e.", "startOffset": 85, "endOffset": 93}, {"referenceID": 2, "context": "[3]), to study the difficulty of learning with gradient-based methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The only difference across this aspect, when learning a degree 1 parity with a convolutional architecture, between known and unknown g, for Gaussian data, is perhaps due to smaller Signal to Noise Ratio in the case of learning g, as suggested in [18].", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": "(1) The following definition and lemma, due to [22], will be useful in our analysis.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "Lemma 1 ([22]) Let \u03c3 be the erf function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Lemma 7 Let c \u2208 [0, 1] and assume |\u3008 wj \u2016wj\u2016 , u0 \u2016u0\u2016 \u3009| < c, or \u2016wj\u2016 \u2264 c/ \u221a 2.", "startOffset": 16, "endOffset": 22}, {"referenceID": 11, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 10, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 13, "context": "The proof uses monotonicity of \u03c3, with similar techniques as found in [12, 11, 14].", "startOffset": 70, "endOffset": 82}, {"referenceID": 0, "context": "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Alon Brutzkus and Amir Globerson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, and Yann LeCun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Dana Dachman-Soled, Vitaly Feldman, Li-Yang Tan, Andrew Wan, and Karl Wimmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Amit Daniely.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Benjamin D Haeffele and Ren\u00e9 Vidal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Moritz Hardt and Tengyu Ma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Adam Tauman Kalai and Ravi Sastry.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Song Mei, Yu Bai, and Andrea Montanari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Itay Safran and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Shai Shalev-Shwartz and Shai Ben-David.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Daniel Soudry and Yair Carmon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Murray R Spiegel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Christopher KI Williams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "2 in [17] we have that if m > 2/\u01eb then, in expectation over the choice of the sample, the probability mass of vectors in {\u00b11}k that does not belong to my sample is at most \u01eb.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 15, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 19, "context": "[13, 16, 5, 20]) that if the number of neurons in the hidden layer is at least m than the optimization surface is \u201cnice\u201d (in particular, no spurious local minima).", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "Also, given that F (\u00b7) is strongly convex and satisfies the eigenvalue condition stated in the theorem, the convergence bound for gradient descent follows from standard results (see [15]).", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Therefore, all the three terms in the argument of the inverse sign are in [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "The claim now follows immediately because for every c \u2208 [0, 1] we have c \u2264 sin(c).", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "For reasonably large k, the argument of the arctan is smaller than 1, and on the interval [0, 1] the value of arctan is larger than half its argument.", "startOffset": 90, "endOffset": 96}], "year": 2017, "abstractText": "Exploiting the great expressive power of Deep Neural Network architectures, relies on the ability to train them. While current theoretical work provides, mostly, results showing the hardness of this task, empirical evidence usually differs from this line, with success stories in abundance. A strong position among empirically successful architectures is captured by networks where extensive weight sharing is used, either by Convolutional or Recurrent layers. Additionally, characterizing specific aspects of different tasks, making them \u201charder\u201d or \u201ceasier\u201d, is an interesting direction explored both theoretically and empirically. We consider a family of ConvNet architectures, and prove that weight sharing can be crucial, from an optimization point of view. We explore different notions of the frequency, of the target function, proving necessity of the target function having some low frequency components. This necessity is not sufficient only with weight sharing can it be exploited, thus theoretically separating architectures using it, from others which do not. Our theoretical results are aligned with empirical experiments in an even more general setting, suggesting viability of examination of the role played by interleaving those aspects in broader families of tasks.", "creator": "LaTeX with hyperref package"}}}