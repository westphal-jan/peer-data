{"id": "1706.01026", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2017", "title": "Adaptive Multiple-Arm Identification", "abstract": "We study the problem of selecting $K $weapons with the highest expected rewards in a stochastic $n $armed bandit game. This problem has a wide range of applications, such as A / B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm that identifies a set of $K $weapons with a probability of at least $1\\ delta $with a total regret of no more than $\\ epsilon $. The term \"total regret\" for multi-arm identification was first introduced in\\ cite {Zhou: 14}, which is defined as the difference between the average expected rewards between the selected set of weapons and the best $K $weapons. Unlike\\ cite {Zhou: 14}, which only provides sample complexity independent of sample complexity, we introduce a new hardness parameter to characterize the difficulty of a given instance. We develop two algorithms and determine the appropriate sample complexity in relation to this hardness parameter.", "histories": [["v1", "Sun, 4 Jun 2017 05:04:52 GMT  (1000kb,D)", "http://arxiv.org/abs/1706.01026v1", "30 pages, 5 figures, preliminary version to appear in ICML 2017"]], "COMMENTS": "30 pages, 5 figures, preliminary version to appear in ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiecao chen", "xi chen", "qin zhang 0001", "yuan zhou"], "accepted": true, "id": "1706.01026"}, "pdf": {"name": "1706.01026.pdf", "metadata": {"source": "CRF", "title": "Adaptive Multiple-Arm Identification\u2217", "authors": ["Jiecao Chen", "Xi Chen", "Leonard N. Stern"], "emails": ["jiecchen@umail.iu.edu", "xchen3@stern.nyu.edu", "qzhangcs@indiana.edu", "yzhoucs@indiana.edu"], "sections": [{"heading": null, "text": "We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least 1 \u2212 \u03b4, identifies a set of K arms with the aggregate regret at most . The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et al. [2014] that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a log( \u22121) factor in the worst case. We also prove a lower bound result showing that the extra log( \u22121) is necessary for instance-dependent algorithms using the introduced hardness parameter."}, {"heading": "1 Introduction", "text": "Given a set of alternatives with different quality, identifying high quality alternatives via a sequential experiment is an important problem in multi-armed bandit (MAB) literature, which is also known as the \u201cpure-exploration\u201d problem. This problem has a wide range of applications. For example, consider the A/B/C testing problem with multiple website designs, where each candidate design corresponds to an alternative. In order to select high-quality designs, an agent could display different designs to website visitors and measure the attractiveness of an design. The question is: how should the agent adaptively select which design to be displayed next so that the high-quality designs can be quickly and accurately identified? For another example, in crowdsourcing, it is critical to identify high-quality workers from\n\u2217Author names are listed in alphabetical order. Preliminary version to appear in ICML 2017.\nar X\niv :1\n70 6.\n01 02\n6v 1\n[ cs\na pool of a large number of noisy workers. An effective strategy is testing workers by gold questions, i.e., questions with the known answers provided by domain experts. Since the agent has to pay a fixed monetary reward for each answer from a worker, it is important to implement a cost-effective strategy for to select the top workers with the minimum number of tests. Other applications include simulation optimization, clinical trials, etc.\nMore formally, we assume that there are n alternative arms, where the i-th arm is associated with an unknown reward distribution Di with mean \u03b8i. For the ease of illustration, we assume each Di is supported on [0, 1]. In practice, it is easy to satisfy this assumption by a proper scaling. For example, the traffic of a website or the correctness of an answer for a crowd worker (which simply takes the value either 0 or 1), can be scaled to [0, 1]. The mean reward \u03b8i characterizes the quality of the i-th alternative. The agent sequentially pulls an arm, and upon each pulling of the i-th arm, the i.i.d. reward from Di is observed. The goal of \u201ctop-K arm identification\u201d is to design an adaptive arm pulling strategy so that the top K arms with the largest mean rewards can be identified with the minimum number of trials. In practice, identifying the exact top-K arms usually requires a large number of arm pulls, which could be wasteful. In many applications (e.g., crowdsourcing), it is sufficient to find an \u201capproximate set\u201d of top-K arms. To measure the quality of the selected arms, we adopt the notion of aggregate regret (or regret for short) from Zhou et al. [2014]. In particular, we assume that arms are ordered by their mean \u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8n so that the set of the best K arms is {1, . . . ,K}. For the selected arm set T with the size |T | = K, the aggregate regret RT is defined as,\nRT = 1\nK ( K\u2211 i=1 \u03b8i \u2212 \u2211 i\u2208T \u03b8i ) . (1)\nThe set of arms T with the aggregate regret less than a pre-determined tolerance level (i.e. RT \u2264 ) is called -top-K arms. In this paper, we consider the -top-K-arm problem in the \u201cfixed-confidence\u201d setting: given a target confidence level \u03b4 > 0, the goal is to find a set of -top-K arms with the probability at least 1\u2212 \u03b4. This is also known as the PAC (probably approximately correct) learning setting. We are interested in achieving this goal with as few arm pulls (sample complexity) as possible.\nTo solve this problem, Zhou et al. [2014] proposed the OptMAI algorithm and established its sample complexity \u0398 ( n 2 ( 1 + ln \u03b4 \u22121\nK\n)) , which is shown to be asymptotically optimal. However, the algorithm\nand the corresponding sample complexity in Zhou et al. [2014] are non-adaptive to the underlying instance. In other words, the algorithm does not utilize the information obtained in known samples to adjust its future sampling strategy; and as a result, the sample complexity only involves the parameters K, n, \u03b4 and but is independent of {\u03b8i}ni=1. Chen et al. [2014] developed the CLUCB-PAC algorithm and established an instance-dependent sample complexity for a more general class of problems, including the -top-K arm identification problem as one of the key examples. When applying the CLUCB-PAC algorithm to identify -top-K arms, the sample complexity becomes O((logH(0, ) +log \u03b4\u22121)H(0, )) where\nH(0, ) = \u2211n i=1 min{(\u2206i)\n\u22122, \u22122}, \u2206i = \u03b8i\u2212 \u03b8K+1 for i \u2264 K, \u2206i = \u03b8K \u2212 \u03b8i for i > K. The reason why we adopt the notation H(0, ) will be clear from Section 1.1. However, this bound may be improved for the following two reasons. First, intuitively, the hardness parameter H(0, ) is the total number of necessary pulls needed for each arm to identify whether it is among the top-K arms or the rest so that the algorithm can decide whether to accept or reject the arm (when the arm\u2019s mean is -close to the boundary between the top-K arms and the rest arms, it can be either selected or rejected). However, in many cases, even if an arm\u2019s mean is -far from the boundary, we may still be vague about the comparison between its mean and the boundary, i.e. either selecting or rejecting the arm satisfies the aggregate regret bound. This may lead to fewer number of pulls and a smaller hardness parameter for the same instance. Second, the worst-case sample complexity for CLUCB-PAC becomes O((logn + log \u22121 + log \u03b4\u22121)n \u22122). When \u03b4 is a constant, this bound is logn times more than the best non-adaptive algorithm in Zhou et al. [2014].\nIn this paper, we explore towards the above two directions and introduce new instance-sensitive algorithms for the problem of identifying -top-K arms. These algorithms significantly improve the sample complexity by CLUCB-PAC for many common instances and almost match the best non-adaptive algorithm in the worst case.\nSpecifically, we first introduce a new parameter H to characterize the hardness of a given instance. This new hardness parameter H could be smaller than the hardness parameter H\u0303 used in the literature, in many natural instances. For example, we show in Lemma 1 that when {\u03b8i}ni=1 are sampled from a continuous distribution with bounded probability density function (which is a common assumption in Bayesian MAB and natural for many applications), for K = \u03b3n with \u03b3 \u2264 0.5, our hardness parameter H = O(n/ \u221a ) while H\u0303 = \u2126(n/ ).\nUsing this new hardness parameter H, we first propose an easy-to-implement algorithm\u2013 AdaptiveTopK and relate its sample complexity to H. In Theorem 1, we show that AdaptiveTopK uses\nO (( log log( \u22121) + logn+ log \u03b4\u22121 ) H )\nto identify -top-K arms with probability at least 1\u2212\u03b4. Note that this bound has a similar form as the one in Chen et al. [2014], but as mentioned above, we have an \u221a -factor improvement in the hardness parameter for those instances where Lemma 1 applies.\nWe then propose the second algorithm (ImprovedTopK) with even less sample complexity, which removes the logn factor in the sample complexity. In Theroem 2, we show that the algorithm uses\nO (( log \u22121 + log \u03b4\u22121 ) H )\npulls to identify -top-K arms with probability 1 \u2212 \u03b4. Since H is always \u2126(n/ 2) (which will be clear when the H is defined in Section 1.1), the worst-case sample complexity of ImprovedTopK matches the best instance-independent shown in Zhou et al. [2014] up to an extra log( \u22121) factor (for constant \u03b4). We are also able to show that this extra log( \u22121) factor is a necessary expense by being instance-adaptive (Theorem 3). It is also noteworthy that as a by-product of establishing ImprovedTopK, we developed an algorithm that approximately identifies the k-th best arm, which may be of independent interest. Please see Algorithm 2 for details.\nWe are now ready to introduce our new hardness parameters and summarize the main results in\ntechnical details."}, {"heading": "1.1 Summary of Main Results", "text": "Following the existing literature (see, e.g., Bubeck et al. [2013]), we first define the gap of the i-th arm\n\u2206i(K) = \u03b8i \u2212 \u03b8K+1 if i \u2264 K\u03b8K \u2212 \u03b8i if i \u2265 K + 1. (2) Note that when K = 1, \u2206i(K) becomes \u03b81 \u2212 \u03b8i for all i \u2265 2 and \u22061(K) = \u03b81 \u2212 \u03b82. When K is clear from the context, we simply use \u2206i for \u2206i(K). One commonly used hardness parameter for quantifying the sample complexity in the existing literature (see, e.g., Bubeck et al. [2013], Karnin et al. [2013]) is\nH\u0303 , \u2211n i=1 \u2206 \u22122 i . If there is an extremely small gap \u2206i, the value of H\u0303 and thus the corresponding sample complexity can be super large. This hardness parameter is natural when the goal is to identify the exact top-K arms, where a sufficient gap between an arm and the boundary (i.e. \u03b8K and \u03b8K+1) is necessary. However, in many applications (e.g., finding high-quality workers in crowdsourcing), it is an overkill to select the exact top-K arms. For example, if all the top-M arms with M > K have very close means, then any subset of them of size K forms an -top-K set in terms of the aggregate regret in (1). Therefore, to quantify the sample complexity when the metric is the aggregate regret, we need to construct a new hardness parameter.\nGiven K and an error bound , let us define t = t( ,K) to be the largest t \u2208 {0, 1, 2, . . . ,K \u2212 1} such\nthat\n\u2206K\u2212t \u00b7 t \u2264 K and \u2206K+t+1 \u00b7 t \u2264 K . (3)\nNote that \u2206K\u2212t \u00b7 t = (\u03b8K\u2212t \u2212 \u03b8K+1) \u00b7 t upper-bounds the total gap of the t worst arms in the top K arms and \u2206K+t+1 \u00b7 t = (\u03b8K \u2212 \u03b8K+t+1) \u00b7 t upper-bounds the total gap of the t best arms in the non-top-K arms. Intuitively, the definition in (3) means that we can tolerate exchanging at most t best arms in the non-top-K arms with the t worst arms in the top-K arms.\nGiven t = t( ,K), we define\n\u03a8t = min(\u2206K\u2212t,\u2206K+t+1), (4)\nand\n\u03a8 t = max( ,\u03a8t). (5)\nWe now introduce the following parameter to characterize the hardness of a given instance,\nH = H(t, ) = n\u2211 i=1 min{(\u2206i)\u22122, (\u03a8 t)\u22122}. (6)\nIt is worthwhile to note that in this new definition of hardness parameter, no matter how small the gap \u2206i is, since \u03a8 t \u2265 , we always have H(t, ) \u2264 n \u22122. We also note that since \u03a8t is non-decreasing in t, H(t, ) is non-increasing in t.\nOur first result is an easy-to-implement algorithm (see Algorithm 1) that identifies -top-K arms with\nsample complexity related to H(t, ).\nTheorem 1 There is an algorithm that computes -top-K arms with probability at least (1 \u2212 \u03b4), and pulls the arms at most O (( log log \u22121 + logn+ log \u03b4\u22121 ) H(t, ) ) times.\nWe also develop a more sophisticated algorithm (see Algorithm 5) with an improved sample complex-\nity.\nTheorem 2 There is an algorithm that computes -top-K arms with probability at least (1 \u2212 \u03b4), and pulls the arms at most O (( log \u22121 + log \u03b4\u22121 ) H(t, ) ) times.\nSince \u03a8 t \u2265 andH(t, ) \u2264 n \u22122, the worst-case sample complexity by Theorem 2 isO ( n 2 ( log \u22121 + log \u03b4\u22121 )) .\nWhile the asymptotically optimal instance-independent sample complexity is \u0398 ( n 2 ( 1 + ln \u03b4 \u22121\nK\n)) , we\nshow that the log \u22121 factor in Theorem 2 is necessary for instance-dependent algorithms using H(t, ) as a hardness parameter. In particular, we prove the following lower-bound result.\nTheorem 3 For any n,K such that n = 2K, and any = \u2126(n\u22121), there exists an instance on n arms so that H(t, ) = \u0398(n) and it requires \u2126(n log \u22121) pulls to identify a set of -top-K arms with probability at least 0.9.\nNote that since H(t, ) = \u0398(n) in our lower bound instances, our Theorem 3 shows that the sample complexity has to be at least \u2126(H(t, ) log \u22121) in these instances. In other words, our lower bound result shows that for any instance-dependent algorithm, and any = \u2126(n\u22121), there exists an instance where sample complexity has to be \u2126(H(t, ) log \u22121). While Theorem 3 shows the necessity of the log \u22121 factor in Theorem 2, it is not a lower bound for every instance of the problem."}, {"heading": "1.2 Review of and Comparison with Related Works", "text": "The problem of identifying the single best arm (i.e. the top-K arms with K = 1), has been studied extensively [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample\ncomplexity of O (\u2211n\ni=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121)\n) . In the worst case, this bound be-\ncomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al.\n[2006]. When K = 1, we have t( ,K) = 0 and thus H(t, ) = H(0, ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore,\nthe sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013].\nFor the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n2)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper.\nKalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret.\nMoreover, there are some other recent works studying the problem of best-arm identification in\ndifferent setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016].\nFor our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.2 in Appendix of their paper). More specifically, Chen et al. [2014] proposed CLUCB-PAC algorithms that finds -top-K arms with probability at least (1\u2212 \u03b4) using O (( log \u03b4\u22121 + logH(0, ) ) H(0, ) ) pulls. Since we always have H(0, ) \u2265 H(t, ) \u2265 \u2126(n) and H(0, ) \u2265 (\u03a8 t) \u22122, our Theorem 1 is not worse than the bound in Chen et al. [2014]. Indeed, in many common settings, H(t, ) can be much smaller than H(0, ) so that Theorem 1 (and therefore Theorem 2) requires\nmuch less sample complexity. We explain this argument in more details as follows.\nIn many real-world applications, it is common to assume the arms \u03b8i are sampled from a prior distribution D over [0, 1] with cumulative distribution function FD(\u03b8). In fact, this is the most fundamental assumption in Bayesian multi-armed bandit literature (e.g., best-arm identification in Bayesian setup Russo [2016]). In crowdsourcing applications, Chen et al. [2015] and Abbasi-Yadkori et al. [2015] also made this assumption for modeling workers\u2019 accuracy, which correspond to the expected rewards. Under this assumption, it is natural to let \u03b8i be the (1\u2212 in ) quantile of the distribution D, i.e. F \u22121 D (1\u2212 i n ). If the prior distribution D\u2019s probability density function fD = dFDd\u03b8 has bounded value (a few common examples include uniform distribution over [0, 1], Beta distribution, or the truncated Gaussian distribution), the arms\u2019 mean rewards {\u03b8i}ni=1 can be characterized by the following property with c = O(1).\nDefinition 1 We call a set of n arms \u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8n c-spread (for some c \u2265 1) if for all i, j \u2208 [n] we have |\u03b8i \u2212 \u03b8j | \u2208 [ |i\u2212j| cn , c|i\u2212j| n ] .\nThe following lemma upper-bounds H(t, ) for O(1)-spread arms, and shows the improvement of our\nalgorithms compared to Chen et al. [2014] on O(1)-spread arms.\nLemma 1 Given a set of n c-spread arms, let K = \u03b3n \u2264 n 2 . When c = O(1) and \u03b3 = \u2126(1), we have H(t, ) = O(n/ \u221a ). In contrast, H(0, ) = \u2126(n/ ) for O(1)-spread arms and every K \u2208 [n].\nProof : Given a set of n c-spread arms, we have t+1 cn \u2264 \u2206K\u2212t \u2264 c(t+1)n and t+1 cn \u2264 \u2206K+t+1 \u2264 c(t+1)n . Therefore t = t( ,K) \u2208 [ \u221a Kn /c, \u221a cKn \u2212 1] = [ \u221a \u03b3 /cn,\n\u221a c\u03b3 n \u2212 1], and \u03a8t \u2265 t+1cn \u2265\n\u221a \u03b3 /c3.\nTherefore\nH(t, ) \u2264 O(1) n\u2211 i=1 min { ci n ,\u03a8t }\u22122 \u2264 O ( t \u00b7\u03a8\u22122t + n\u2211 i=t+1 ( i cn )\u22122)\n= O ( t \u00b7\u03a8\u22122t + c2n2/t ) = O( \u221a c\u03b3 n) \u00b7 c 3\n\u03b3 +O ( c2n\u221a \u03b3 /c ) = O(c3.5\u03b3\u22120.5) \u00b7 n\u221a .\nOne the other hand, we have\nH(0, ) \u2265 n\u2212K\u2211 i=1 min{\u2206\u22122i+K , \u22122} \u2265 n/2\u2211 i=1 min { n2 c2i2 , \u22122 } = [ n/c]\u2211 i=1 \u22122 + n/2\u2211 [ n/c]+1 n2 c2i2 = \u2126 ( n c ) .\n2 An Instance Dependent Algorithm for -top-K Arms\nIn this section, we show Theorem 1 by proving the following theorem.\nTheorem 4 Algorithm 1 computes -top-K arms with probability at least 1 \u2212 \u03b4, and pulls the arms at most\nO (( log log(\u2206 t)\n\u22121 + logn+ log \u03b4\u22121 ) n\u2211 i=1 min{(\u2206i)\u22122, (\u2206 t)\u22122} ) times, where t \u2208 {0, 1, 2, . . . ,K\u22121} is the largest integer satisfying \u2206K\u2212t\u00b7t \u2264 K , and \u2206 t = max( ,\u2206K\u2212t).\nNote that Theorem 4 implies Theorem 1 because of the following reasons: 1) t defined in Theorem 4 is always at least t( ,K) defined in (3); and 2) \u2206 t \u2265 \u03a8 t \u2265 . Algorithm 1 is similar to the accept-reject types of algorithms in e.g. Bubeck et al. [2013]. The algorithm goes by rounds for r = 1, 2, 3, . . . , and keeps at set of undecided arms Sr \u2286 [n] at Round r.\nAlgorithm 1: AdaptiveTopK(n, ,K, \u03b4)\nInput: n: number of arms; K and : parameters in -top-K arms; \u03b4: error probability Output: -top-K arms\n1 Let r denote the current round, initialized to be 0. Let Sr \u2286 [n] denote the set of candidate arms at round r. S1 is initialized to be [n]. Set A,B \u2190 \u2205 2 \u2206\u2190 2\u2212r 3 while 2 \u00b7\u2206 \u00b7 (K \u2212 |A|) > K do 4 r \u2190 r + 1 5 Pull each arm in Sr by \u2206 \u22122 ln 2nr 2 \u03b4 times, and let \u03b8\u0303 r i be the empirical-mean 6 Define \u03b8\u0303a(Sr) and \u03b8\u0303b(Sr) be the (K \u2212 |A|+ 1)th and (K \u2212 |A|)th largest empirical-means in Sr, and define\n\u2206\u0303i(Sr) = max ( \u03b8\u0303ri \u2212 \u03b8\u0303a(Sr), \u03b8\u0303b(Sr)\u2212 \u03b8\u0303ri ) (7)\n7 while maxi\u2208Sr \u2206\u0303i(Sr) > 2 \u00b7\u2206 do 8 x\u2190 arg maxi\u2208Sr \u2206\u0303i(Sr) 9 if \u03b8\u0303rx > \u03b8\u0303a(Sr) then\n10 A\u2190 A \u222a {x} 11 else 12 B \u2190 B \u222a {x} 13 Sr \u2190 Sr\\{x} 14 Sr+1 \u2190 Sr 15 \u2206\u2190 2\u2212r 16 Set A\u2032 as the (K \u2212 |A|) arms with the largest empirical-means in Sr+1 17 return A \u222aA\u2032\nAll other arms (in [n] \\ Sr) are either accepted (in A) or rejected (in B). At each round, all undecided arms are pulled by equal number of times. This number is designed in a way such that the event E , defined to be the empirical means of all arms within a small neighborhood of their true means, happens with probability 1 \u2212 \u03b4 (See Definition 2 and Claim 1). Note that E is defined for all rounds and the length of the neighborhood becomes smaller as the algorithm proceeds. We are able to prove that when E happens, the algorithm returns the desired set of -top-K armsand has small query complexity. To prove the correctness of the algorithm, we first show that when conditioning on E , the algorithm always accepts a top-K arm in A (Lemma 3) and rejects a non-top-K arm in B (Lemma 4). The key observation here is that our algorithm never introduces any regret due to arms in A and B. We then use the key Lemma 5 to upper bound the regret that may be introduced due to the remaining arms. Once this upper bound is not more than K (i.e. the total budget for regret), we can choose the remaining (K \u2212 |A|) arms without further samplings. Details about this analysis can be found in Section 2.1. We analyze of the query complexity of our algorithm in Section 2.2. We establish data-dependent bound by relating the number of pulls to each arms to both their \u2206i\u2019s and \u2206K\u2212t (Lemma 6 and Lemma 7)."}, {"heading": "2.1 Correctness of Algorithm 1", "text": "We first define an event E which we will condition on in the rest of the analysis.\nDefinition 2 Let E be the event that |\u03b8\u0303ri \u2212 \u03b8i| < 2\u2212r for all r \u2265 1 and i \u2208 Sr.\nClaim 1 Pr[E ] \u2265 1\u2212 \u03b4.\nProof : By Hoeffding\u2019s inequality, we can show that for any fixed r and i, Pr [ |\u03b8\u0303ri \u2212 \u03b8i| \u2265 2\u2212r ] \u2264 2( \u03b4 2nr2 )2 \u2264 \u03b4 2nr2 . By a union bound,\nPr[\u00acE ] \u2264 \u221e\u2211 r=1 \u2211 i\u2208Sr Pr [ |\u03b8\u0303ri \u2212 \u03b8i| \u2265 2\u2212r ] \u2264 \u221e\u2211 r=1 \u03b4 2r2 \u2264 \u03b4.\nThe following lemma will be a very useful tool for our analysis.\nLemma 2 Given \u00b51 \u2265 . . . \u2265 \u00b5n and \u2206 > 0, assuming that |\u00b5\u0303i \u2212 \u00b5i| \u2264 \u2206 for all i \u2208 [n], and letting y1 \u2265 . . . \u2265 yn be the sorted version of \u00b5\u03031, . . . , \u00b5\u0303n, we have |yi \u2212 \u00b5i| \u2264 \u2206 for all i \u2208 [n].\nProof : Suppose yi > \u00b5i + \u2206 for any i \u2208 [n], we must have y1 \u2265 . . . \u2265 yi > \u00b5i + \u2206. On the other hand, there can not be more than i\u2212 1 numbers among \u00b5\u03031, . . . , \u00b5\u0303n (the only candidates are \u00b5\u03031, . . . , \u00b5\u0303i\u22121) that are larger than \u00b5i + \u2206. A contradition. We thus have yi \u2264 \u00b5i + \u2206 for all i \u2208 [n]. Similarly, we can show that yi \u2265 \u00b5i \u2212\u2206 for all i \u2208 [n].\nWe now prove that conditioned on E , the algorithm always accepts a desired arm in A.\nLemma 3 Conditioned on E, during the run of Algorithm 1, A \u2286 {1, 2, . . . ,K}, that is, all arms in A are among the top-K arms.\nProof : We prove by induction on the round r. The lemma holds trivially when r = 0 (A = \u2205). Now fix a round r \u2265 1, and let x be the arm that is added to A at Line 10 of Algorithm 1. By the induction hypothesis, assuming that before round r all arms in A are in [K], our goal is to show x \u2208 [K].\nBy the inner while condition we have\n\u03b8\u0303rx \u2212 \u03b8\u0303a(Sr) > 2 \u00b7 2\u2212r. (8)\nFor any m \u2208 [K\u2212|A|+1, |Sr|], let j be the arm of the m-th largest true-mean in Sr, and j\u2032 be the arm of the m-th largest empirical-mean in Sr. Since m \u2265 K \u2212 |A|+ 1, we must have j 6\u2208 [K] and \u03b8\u0303rj\u2032 \u2264 \u03b8\u0303a(Sr). By Lemma 2 we also have |\u03b8\u0303rj\u2032 \u2212 \u03b8j | < 2\u2212r. We thus have\n\u03b8x > \u03b8\u0303 r x \u2212 2\u2212r by (8) > \u03b8\u0303a(Sr) + 2 \u2212r > \u03b8\u0303rj\u2032 + 2 \u2212r > \u03b8j .\nThat is, at least |Sr| \u2212 K + |A| arms in Sr have true-means smaller than arm x. On the other hand, |Sr| \u2212K + |A| arms in Sr are not in [K]. We therefore conclude that x must be in [K].\nBy symmetry, we also have the following lemma, stating that when E happens, the algorithm always rejects a non-top-K arm in B. We omit the proof because it is almost identical to the proof of Lemma 3.\nLemma 4 Conditioning on E, during the run of Algorithm 1, B \u2286 {K + 1,K + 2, . . . , n}.\nLemma 5 Conditioned on E, for all rounds r and i \u2208 Sr, it holds that\n\u03b8\u0303ri \u2212 \u03b8\u0303a(Sr) > \u03b8i \u2212 \u03b8K+1 \u2212 2 \u00b7 2\u2212r and \u03b8\u0303b(Sr)\u2212 \u03b8\u0303ri > \u03b8K \u2212 \u03b8i \u2212 2 \u00b7 2\u2212r.\nConsequently, we have \u2206\u0303i(Sr) \u2265 \u2206i \u2212 2 \u00b7 2\u2212r for all rounds r and i \u2208 Sr.\nProof : We look at a particular round r. Let j be the arm with (K \u2212 |A| + 1)-th largest true-mean in Sr. Since by Lemma 3 we have A \u2286 [K] , it holds that j \u2265 K + 1. By Lemma 2, we also have\n|\u03b8\u0303a(Sr)\u2212 \u03b8j | < 2\u2212r. We therefore have for any i \u2208 Sr\n\u03b8\u0303ri \u2212 \u03b8\u0303a(Sr) > \u03b8i \u2212 \u03b8j \u2212 2 \u00b7 2\u2212r \u2265 \u03b8i \u2212 \u03b8K+1 \u2212 2 \u00b7 2\u2212r. (9)\nWith a similar argument (by symmetry and using Lemma 4), we can show that\n\u03b8\u0303b(Sr)\u2212 \u03b8\u0303ri > \u03b8K \u2212 \u03b8i \u2212 2 \u00b7 2\u2212r. (10)\nCombining (9), (10) and the definitions of \u2206\u0303i(Sr) and \u2206i, the lemma follows.\nNow we are ready to prove the correctness of Theorem 4. By Lemma 3, all the arms that we add into\nthe set A at Line 10 are in [K]. The rest of our job is to look at the arms in the set A\u2032.\nWhen the algorithm exits the outer while loop (at round r = r\u2217) and arrives at Line 16, we have by\nthe condition of the outer while loop that\n2 \u00b7 2\u2212r \u2217 \u00b7 (K \u2212 |A|) \u2264 K. (11)\nLet m = K \u2212 |A|, and C = [K]\\A = {i1, i2, . . . , im} where i1 < i2 < . . . < im. Let \u03b8\u0303j1 \u2265 \u03b8\u0303j2 \u2265 . . . \u2265 \u03b8\u0303jm be the (K \u2212 |A|) empirical-means of the arms that we pick at Line 16. Note that it is not necessary that j1 < . . . < jm. By Lemma 2 and E , for any s \u2208 [K\u2212|A|], we have |\u03b8\u0303js\u2212\u03b8is | \u2264 2\u2212r \u2217 and |\u03b8\u0303js\u2212\u03b8js | \u2264 2\u2212r \u2217 . By the triangle inequality, it holds that\n|\u03b8js \u2212 \u03b8is | \u2264 2 \u00b7 2 \u2212r\u2217 . (12)\nWe thus can bound the error introduced by arms in A\u2032 by\n\u2211 i\u2208[K] \u03b8i \u2212 \u2211 i\u2208A\u222aA\u2032 \u03b8i = \u2211 i\u2208C \u03b8i \u2212 \u2211 i\u2208A\u2032 \u03b8i by (12) \u2264 2 \u00b7 2\u2212r \u2217 \u00b7 (K \u2212 |A|) by (11) \u2264 K."}, {"heading": "2.2 Query Complexity of Algorithm 1", "text": "Recall (in the statement of Theorem 4) that t \u2208 {0, 1, 2, . . . ,K \u2212 1} is the largest integer satisfying\n\u2206K\u2212t \u00b7 t \u2264 K. (13)\nLemma 6 If the algorithm exits the outer while loop at round r = r\u2217, then we must have\n8 \u00b7 2\u2212r \u2217 \u2265 \u2206K\u2212t. (14)\nProof : We show that once 2\u2212r < \u2206K\u2212t/4, the algorithm will exit the outer while loop after executing round r. So any valid round r must satisfy 2\u2212r \u2265 \u2206K\u2212t/8 and the lemma holds trivially. To this end, assume now we are in round r and 2\u2212r < \u2206K\u2212t/4, we have that for any i \u2208 Sr and i \u2264 K \u2212 t,\n\u2206\u0303i(Sr) \u2265 \u03b8\u0303ri \u2212 \u03b8\u0303a(Sr) > \u03b8i \u2212 \u03b8K+1 \u2212 2 \u00b7 2\u2212r (Lemma 5)\n= \u2206i \u2212 2 \u00b7 2\u2212r \u2265 \u2206K\u2212t \u2212 2 \u00b7 2\u2212r (since i \u2264 K \u2212 t) > 2 \u00b7 2\u2212r.\nThus the condition of the inner while loop is satisfied, which means that all arms i with i \u2264 K \u2212 t will be added into A. Therefore we have |A| \u2265 K\u2212 t when the algorithm exits the inner while loop. We then\nhave\n2 \u00b7 2\u2212r \u00b7 (K \u2212 |A|) \u2264 2 \u00b7 2\u2212r \u00b7 t < 1 2 \u2206K\u2212t \u00b7 t by (13) \u2264 K/2 \u2264 K,\nso the algorithm exits the outter loop.\nLemma 7 For any arm i, let ri be the round where arm i is removed from the candidate set if this ever happens; otherwise set ri = r \u2217. We must have\n8 \u00b7 2\u2212ri \u2265 \u2206i. (15)\nProof : Suppose for contradiction that 8 \u00b7 2\u2212ri < \u2206i. By Lemma 5, we have\n\u2206\u0303i(Sri\u22121) \u2265 \u2206i \u2212 2 \u00b7 2 \u2212(ri\u22121) > 8 \u00b7 2\u2212ri \u2212 2 \u00b7 2\u2212(ri\u22121) = 2 \u00b7 2\u2212(ri\u22121).\nThis means that arm i would have been added either to A or B at or before round (ri \u2212 1), which contradicts to the fact that i \u2208 Sri .\nWith Lemma 6 and Lemma 7, we are ready to analyze the query complexity of the algorithm in\nTheorem 4. We can bound the number of pulls on each arm i by at most\nri\u2211 j=1 22j \u00b7 log(2nj2/\u03b4) \u2264 O ( log(ri \u00b7 n\u03b4\u22121) \u00b7 22ri ) . (16)\nNow let us upper-bound the RHS of (16). First, if i \u2208 A, then by (15) we know that ri \u2264 log2 \u2206 \u22121 i +O(1). Second, by (14) we have ri \u2264 r\u2217 \u2264 log2 \u2206 \u22121 K\u2212t +O(1). Third, since 2\n\u2212r\u2217 \u2265 /2 (otherwise the algorithm will exit the outer while loop), we have ri \u2264 r\u2217 \u2264 log2 \u22121 + O(1). To summarize, we have ri \u2264 log2 min{\u2206 \u22121 i ,\u2206 \u22121 K\u2212t, \u22121} + O(1) = log2 min{\u2206 \u22121 i , (\u2206 t) \u22121} (recall that \u2206 t = max{ ,\u2206K\u2212t}). We thus can upper-bound the RHS of (16) by\nO ( (log log(\u2206 t) \u22121 + logn+ log \u03b4\u22121) \u00b7min{(\u2206i)\u22122, (\u2206 t)\u22122} ) .\nThe total cost is a summation over all n arms.\n3 An Improved Algorithm for -top-K Arms\nIn this section, we present the improved algorithm for identifying the -top-K arms and prove that the algorithm succeeds with probability 1 \u2212 \u03b4 with query complexity O((log \u22121 + log \u03b4\u22121)H(t, )) (Theorem 6). This algorithm reduces the logn factor in the query complexity of Algorithm 1 to log \u22121 and is substantially more complex than Algorithm 1.\nThe main procedure of the improved algorithm is described in Algorithm 5. For this algorithm, we that assume K \u2264 n/2. For the case where K > n/2, we can apply the same algorithm to identify the -bottom-(n\u2212K) arms and report the rest arms to be the -top-K arms. Similarly to Algorithm 1, the improved algorithm also goes by rounds and keeps a set A of accepted arms, a set B of rejected arms, and a set S of undecided arms. However, we can no longer guarantee that all the arms accepted in A and rejected in B are correctly classified \u2013 otherwise, we need to apply a union bound over all arms and this would incur an extra logn factor. To solve this problem, we have to allow a few number of mistakes. We now illustrate the high-level idea as follows.\nGiven a set of n arms {\u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8n}, if we pull every arm c \u00b7 \u2206\u22122.8n(log \u22121 + log \u03b4\u22121) times for some large enough constant c, and discard the .1n arms with the lowest empirical means, it can be shown by standard probabilistic method that at most 2K top-K arms may be mistakenly discarded\nAlgorithm 2: EstKthArm(S,K, \u03c4, \u03c6, \u03b4)\nInput: S: set of arms; K: top-K; \u03c4 : an relative error; \u03b4: error probability; \u03c6: an additive error Output: an arm whose true-mean is close to the K-th largest true-mean\n1 set R1 \u2190 S, r \u2190 1 2 set \u03c41 \u2190 \u03c44 , \u03c61 \u2190 \u03c6 4 , \u03b41 \u2190 \u03b4 8 3 while |Rr| > K do 4 for each i \u2208 Rr, pull 8\u03c62r ln( 1 \u03c4r\u03b4r\u03b4 ) times; let \u03b8\u0303ri be its empirical-mean 5 let Rr+1 be the set of max{K, d|Rr|/2e} arms that have the largest empirical-means among Rr 6 set \u03c4r+1 \u2190 3\u03c4r/4, \u03c6r+1 \u2190 3\u03c6r/4, \u03b4r+1 \u2190 \u03b4r/2 7 r \u2190 r + 1 8 set r\u2217 \u2190 r 9 set p\u03031 \u2265 p\u03032 \u2265 . . . \u2265 p\u0303|Rr\u2217 | be the sorted version of {\u03b8\u0303r \u2217\ni | i \u2208 Rr\u2217} 10 uniformly sample an arm from {i \u2208 Rr\u2217 | \u03b8\u0303r \u2217 i \u2264 p\u0303(1\u2212\u03c4/2)K} and output it\nwith probability 1 \u2212 \u03b4. Note that the constants .8 and .1 are arbitrary as long as K/n < .8 < 1 \u2212 .1. This procedure is described in Algorithm 2 and analyzed in Lemma 11. Similarly, if .2n < K and we pull every arm c \u00b7\u2206\u22122.2n(log \u22121 +log \u03b4\u22121) times for some large enough constant c, and accept the .1n arms with the highest empirical means, with probability 1 \u2212 \u03b4, at most 2K non-top-K arms may be mistakenly accepted. This procedure is described in Algorithm 2 and analyzed in Lemma 12. Algorithm 5 uses these two subroutines to repeatedly accept and reject arms, and makes sure that with high probability, the total number of mistakenly accepted or rejected arms is at most O( 2K) (Lemma 13). These mistakes lead to O( 2K) total regret \u2013 negligible when compared to our K budget. In this way, the improved algorithm keeps accepting and rejecting arms as Algorithm 1 does, while introducing negligible regret (while Algorithm 1 introduces none). The termination condition is also similar to Algorithm 1 in Line 12 of Algorithm 5 so that the query complexity is related to H(t, ) rather than H(0, ).\nHowever, there is an extra termination condition and many extra efforts in the improved algorithm because of the few allowed mistakes. For our adaptive algorithm, in order to estimate \u2206.8n and \u2206.2n (and other gaps as the algorithm proceeds), we need to estimate \u03b8K , \u03b8.8n and \u03b8.2n with O(\u03c6 \u22122(log \u22121+log \u03b4\u22121) pulls, where \u03c6\u22122 = \u2126(min{\u2206.8n,\u2206.2n}). However, using these many pulls, we can only estimate the mean of an arm that is close to the target index, rather than with the exact index. This procedure is presented in Section 3.1 and Algorithm 2. We use this subroutine to estimate \u03b8.8n as \u03b8 +, \u03b8.2n as \u03b8 \u2212 in Algorithm 5, and use two estimations \u03b8+K and \u03b8 \u2212 K to sandwich \u03b8K . (The precise statement can be found in Lemma 14.) When \u03b8+K and \u03b8 \u2212 K are close to each other, we can use \u03b8 \u2212 \u2212 \u03b8\u2212K and \u03b8 + K \u2212 \u03b8 + as estimations of \u2206.2n and \u2206.8n; otherwise, it means that there is a big gap in the neighborhood of the K-th arm, and we can easily separate the top-K arms from the rest using the subprocedure EpsSplit described in Lemma 9 and quit the procedure (in Line 14 of the algorithm).\nWe now dive into the details of the improved algorithm. We start by introducing the useful subrou-\ntines.\n3.1 Estimating the K-th Largest Arm\nIn the subsection we present an algorithm that try to find an arm whose true-mean is close to the K-th largest true-mean, which will be used as a subroutine in our improved algorithm for -top-K arms.\nTheorem 5 For a set of arms S = {\u03b81 \u2265 . . . \u2265 \u03b8|S|}, there is an algorithm, denoted by EstKthArm(S,K, \u03c4, \u03c6, \u03b4), that outputs an arm i such that \u03b8i \u2208 [\u03b8K \u2212 \u03c6, \u03b8(1\u2212\u03c4)K + \u03c6] with probability at least 1 \u2212 \u03b4, using O ( |S| \u03c62 \u00b7 (log \u03c4\u22121 + log \u03b4\u22121) ) pulls in total.\nWe described the algorithm in Algorithm 2. In the high level, the algorithm works in rounds, and in each round it tries to find the top half arms in the current set, and discard the rest. We continue until there are at most K arms left, and then we choose the output arm randomly from those with the lowest empirical-means in the remaining arms. We are going to prove the following theorem."}, {"heading": "3.1.1 Correctness of Algorithm 2", "text": "The following lemma is the key to the proof of correctness.\nLemma 8 With probability at least 1\u2212 \u03b4/4, we have that\n|{i \u2208 Rr\u2217 | \u03b8i < \u03b8K \u2212 \u03c6}| \u2264 \u03c4\u03b4K/4.\nProof : We first define a few notations. \u2022 Hr = {i \u2208 S | \u03b8i \u2265 \u03b8K \u2212 \u2211 `\u2208[r] \u03c6`}.\n\u2022 Lr = S\\Hr. \u2022 kr = (1\u2212 \u2211 `\u2208[r] \u03c4`\u03b4/4)K. \u2022 For any i \u2208 [n] and round r, let Xri = 1{|\u03b8\u0303ri \u2212 \u03b8i| \u2265 \u03c6r/2} where \u03b8\u0303ri is the empirical-mean of arm i at round r (after been pulled by 8\n\u03c62r ln( 1 \u03c4r\u03b4r\u03b4 ) times at Line 4).\n\u2022 R\u2032r \u2286 Rr: the top kr\u22121 arms in Rr with the largest true-means. \u2022 Ar = {i \u2208 R\u2032r | Xi = 0}. Ar \u2286 R\u2032r \u2286 Rr.\n\u2022 Cr = {i \u2208 Lr \u2229Rr | Xi = 1}. Cr \u2286 Rr.\nWe define the following event. Intuitively, it tells that most of the arms we put in Rr for the next\nround processing fall into the set Hr of high true-means.\nEr(r \u2265 2) : |Hr\u22121 \u2229Rr| \u2264 kr\u22121,\nand we define E1 to be an always true event. We will prove by induction the following inequality.\nPr[Er+1 | \u00acEr] \u2264 \u03b4r for each r \u2265 1. (17)\nWe focus a particular round r \u2265 1. Define event EA : |Ar| \u2264 kr, and event EC : |Cr| \u2264 |Rr+1| \u2212 kr.\nClaim 2 Pr[EA | \u00acEr] \u2264 \u03b4r/4.\nProof : By a Hoeffding\u2019s inequality, we have for each i \u2208 Rr, we have Pr[Xri = 1] \u2264 \u03c4r\u03b4r\u03b4/16. We bound the probability the EA happens by a Markov\u2019s inequality.\nPr[EA | \u00acEr] = Pr[|Ar| \u2264 kr | \u00acEr] =Pr[|R\u2032r| \u2212 |Ar| \u2265 kr\u22121 \u2212 kr | \u00acEr] \u2264 E [\u2211 i\u2208R\u2032r Xi | \u00acEr] ] kr\u22121 \u2212 kr = E [\u2211 i\u2208R\u2032r Xi ] kr\u22121 \u2212 kr\n\u2264\u03c4r\u03b4r\u03b4kr\u22121/16 \u03c4r\u03b4K/4 \u2264 \u03c4r\u03b4r\u03b4K/16 \u03c4r\u03b4K/4\n=\u03b4r/4.\nClaim 3 Pr[EC | \u00acEA,\u00acEr] \u2264 3\u03b4r/4.\nProof :\nPr[EC | \u00acEA,\u00acEr] =Pr[|Cr| \u2264 |Rr+1| \u2212 kr | \u00acEA,\u00acEr]\n\u2264E[|Cr| | \u00acEA,\u00acEr]|Rr+1| \u2212 kr (Markov\u2019s inequality) (18) \u2264 (|Rr| \u2212 kr) \u00b7 \u03c4r\u03b4\u03b4r/16|Rr+1| \u2212 kr (19) \u2264 (2 |Rr+1| \u2212 kr) \u00b7 \u03c4r\u03b4\u03b4r/16|Rr+1| \u2212 kr (20) \u22643\u03b4r/4, (21)\nwhere (18) to (19) is due to the fact that conditioned on \u00acEr, we have\n|Cr| \u2264 |Lr \u2229Rr| \u2264 |Lr\u22121 \u2229Rr| = |Rr| \u2212 |Hr\u22121 \u2229Rr| \u00acEr \u2264 |Rr| \u2212 kr\u22121 \u2264 |Rr| \u2212 kr.\nAnd (20) to (21) is due to the following: If |Rr+1| \u2265 2K, then since kr \u2264 K we have 2|Rr+1|\u2212kr |Rr+1|\u2212kr \u2264 3. Otherwise if |Rr+1| < 2K, by the definition of kr, we have\n|Rr+1| \u2212 kr > K \u2212 kr = \u2211\ni\u2208[r\u22121]\n\u03c4i\u03b4/4 \u2265 \u03c4r\u03b4K/3.\nOn the other hand, we have 2 |Rr+1| \u2212 kr \u2264 4K. We thus have (21) \u2264 3\u03b4r/4.\nClaim 4 Conditioned on EC ,\u00acEA,\u00acEr, we have |Hr \u2229Rr+1| \u2265 kr, or, Pr[Er+1 | EC ,\u00acEA,\u00acEr] = 0.\nProof : First, conditioned on \u00acEr, we have\nAr \u2286 R\u2032r \u2286 Hr\u22121. (22)\nWe prove the claim by analyzing two cases.\n1. ((Lr \u2229Rr)\\Cr) \u2229Rr+1 = \u2205. We thus have (Lr \u2229Rr+1) \u2286 Cr, which implies\n|Hr \u2229Rr+1| \u2265 |Rr+1| \u2212 |Lr \u2229Rr+1| \u2265 |Rr+1| \u2212 |Cr| EC \u2265 kr.\n2. ((Lr \u2229Rr)\\Cr) \u2229Rr+1 6= \u2205. We can show that\nAr \u2286 Rr+1 (23)\nIndeed, for any j \u2208 Ar(\u2286 Hr\u22121 by (22)) and any i \u2208 (Lr \u2229 Rr)\\Cr, we have \u03b8\u0303j > \u03b8\u0303i, since \u03b8j \u2265 \u03b8K \u2212 \u2211 `\u2208[r\u22121] \u03c6`, while \u03b8i < \u03b8K \u2212 \u2211 `\u2208[r] \u03c6` (by the definition of Lr). Thus conditioned on \u00acEA, (22) and (23) we have\n|Hr \u2229Rr+1| \u2265 |Hr\u22121 \u2229Rr+1| \u2265 |Ar| \u2265 kr.\nNow we try to prove (17).\nPr[Er+1 | \u00acEr] =Pr[EA | \u00acEr] + Pr[EC | \u00acEA,\u00acEr] + Pr[Er+1 | \u00acEA,\u00acEr]\n\u2264\u03b4/4 + 3\u03b4/4 + 0 (Claim 2, Claim 3 and Claim 4)\n=\u03b4.\nUsing (17) and summing over all ` \u2208 [r \u2212 1], we have Pr[Er] \u2264 \u2211 `\u2208[r\u22121] \u03b4i < \u03b4/4 for any r \u2265 2. In other words, with probability at least 1 \u2212 \u03b4/4, we have for any r \u2264 r\u2217, |Hr\u22121 \u2229Rr| \u2265 kr\u22121, or |Lr\u22121 \u2229Rr| \u2264 K \u2212 kr\u22121 \u2264 1\u2212 (1\u2212 \u03c4\u03b4K/4) = \u03c4\u03b4K/4, which gives the lemma.\nNow we are ready to prove the correctness of Theorem 5. Let \u2022 P = {i \u2208 Rr\u2217 | \u03b8\u0303r \u2217 i > p\u0303(1\u2212\u03c4/2)K}.\n\u2022 Q = Rr\u2217\\P . \u2022 L\u2217 = {i \u2208 Rr\u2217 | \u03b8i < \u03b8K \u2212 \u03c6}. \u2022 For each i \u2208 Rr\u2217 , let Yi = 1{|\u03b8\u0303r \u2217 i \u2212 \u03b8i| > \u03c6/2}.\nClaim 5 Pr[\u03b8a \u2265 \u03b8K \u2212 \u03c6] \u2265 1\u2212 3\u03b4/4.\nProof : At Line 10 of Algorithm 2, we randomly sampled an arm a from Q. Since |Q| \u2265 \u03c4K and Pr[|L\u2217| < \u03c4\u03b4K/4] > 1\u2212 \u03b4/4 (by Lemma 8), we have\nPr[a \u2208 L\u2217] \u2264 Pr[|L\u2217| \u2265 \u03c4\u03b4K/4] + Pr[a \u2208 L\u2217 | |L\u2217| < \u03c4\u03b4K/4] \u2264 \u03b4/4 + \u03c4\u03b4K/4 \u03c4K/2 = 3\u03b4/4. (24)\nClaim 6 Pr[\u03b8a \u2264 \u03b8(1\u2212\u03c4)K + \u03c6] \u2265 1\u2212 \u03b4/4. Proof : We first show that if Ya = 0 and \u2211 i\u2208P Yi \u2264 \u03c4K/2, then \u03b8a \u2264 \u03b8(1\u2212\u03c4)K +\u03c6. Let P0 = {i \u2208 P | Yi =\n0}. By definition of P and the assumption that \u2211 i\u2208P Yi \u2264 \u03c4K/2, we have |P0| \u2265 |P |\u2212\u03c4K/2 \u2265 (1\u2212\u03c4)K. Let b be the arm in P0 that has the minimum true-mean, then we must have\n\u03b8b \u2264 \u03b8|P0| \u2264 \u03b8(1\u2212\u03c4)K . (25)\nSince a \u2208 Q and b \u2208 P , we have \u03b8\u0303a \u2264 \u03b8\u0303b, which, together with the facts that Ya = Yb = 0 and (25), gives\n\u03b8a \u2264 \u03b8b + \u03c6 \u2264 \u03b8(1\u2212\u03c4)K + \u03c6.\nWe now bound the probabilities that the two conditions hold. By a Hoeffding\u2019s inequality, we have Pr[Yi = 1] \u2264 \u03c4\u03b4/16 for any i \u2208 Rr\u2217 . By a Markov\u2019s inequality and the fact that P \u2286 Rr\u2217 (by definition), we have\nPr [\u2211 i\u2208P Yi > \u03c4K/2 ] \u2264 Pr  \u2211 i\u2208Rr\u2217 Yi > \u03c4K/2  \u2264 E [\u2211 i\u2208Rr\u2217 Yi ] \u03c4K/2 \u2264 \u03c4\u03b4K/16 \u03c4K/2 = \u03b4/8.\nWe thus have Pr [ (Ya = 0) \u2227 ( \u2211 i\u2208P Yi \u2264 \u03c4K/2) ] \u2265 1\u2212 \u03b4/16\u2212 \u03b4/8 \u2265 1\u2212 \u03b4/4.\nThe correctness of Theorem 5 immediately follows from Claim 6 and Claim 5."}, {"heading": "3.1.2 Complexity Algorithm 2", "text": "We can bound the total number of pulls of Algorithm 2 by simply summing up the number of pulls at each round.\nO log |S|/K\u2211 r=1 |Rr| \u03c62r log 1 \u03c4r\u03b4r\u03b4  =O( \u221e\u2211 r=1 2\u2212r|S| (3/4)2r\u03c62 \u00b7 ( log 1 (3/4)r\u03c4 + log\n1\n(1/2)r\u03b4 + log\n1 \u03b4\n))\nAlgorithm 3: Elim(S,K, \u03b3, \u03c6, \u03b4)\nInput: S: set of arms; K: top-K; \u03b3: fraction of arms; \u03b4: error probability; \u03c6: an additive error\nOutput: Set of arms T with |T | = |S|10 such that at most \u03b3K arms in T are in the top-K arms in S 1 For each arm i in S, pull c\u03c62 \u00b7 (log \u03b3\n\u22121 + log \u03b4\u22121) for some large enough constant c. Let \u03b8\u0303i be the empirical-mean of the i-th arm.\n2 return |T | arms in S with the smallest empirical-means.\n=O ( |S| \u03c62 \u221e\u2211 r=1 ( 8 9 )r \u00b7 ( r + log 1 \u03c4 + log 1 \u03b4 ))\n=O ( |S| \u03c62 \u00b7 ( log 1 \u03c4 + log 1 \u03b4 )) .\nThe last equality follows from the fact that \u2211\u221e r=1(8/9)\nr \u00b7 r = O(1). Lemma 8 also implies the following lemma.\nLemma 9 For a set of arms S = {\u03b81 \u2265 . . . \u2265 \u03b8|S|} such that \u03b8(1\u2212\u03c4)K \u2212 \u03b8(1+\u03c4)K+1 \u2265 \u03c6, there is an algorithm, denoted by EpsSplit(S,K, \u03c4, \u03c6, \u03b4), that computes (2\u03c4)-top-K correctly with probability at least\n1\u2212 \u03b4, using O ( |S| \u03c62 \u00b7 (log \u03c4\u22121 + log \u03b4\u22121) ) pulls in total.\nProof : To prove the lemma, we just need to replace \u201cK\u201d in Algorithm 2 to be \u201c(1\u2212 \u03c4)K\u201d. By Lemma 8 we have with probability 1\u2212 \u03b4 that\n|{i \u2208 Rr\u2217 | \u03b8i < \u03b8(1\u2212\u03c4)K \u2212 \u03c6}| \u2264 \u03c4\u03b4(1\u2212 \u03c4)K \u2264 \u03c4\u03b4K.\nSince \u03b8(1\u2212\u03c4)K \u2212 \u03b8(1+\u03c4)K+1 \u2265 \u03c6 we have\n|{i \u2208 Rr\u2217 | \u03b8i < \u03b8(1+\u03c4)K+1}| \u2264 \u03c4\u03b4K.\nConsequently,\n|{i \u2208 Rr\u2217 | \u03b8i < \u03b8K}| \u2264 \u03c4K + \u03c4\u03b4K \u2264 2\u03c4K. (26)\nTherefore we can just choose all arms in Rr\u2217 , together with K\u2212|Rr\u2217 | arbitrary arms in the rest n\u2212|Rr\u2217 | arms. By (26) the total average error is bounded by 2\u03c4K/K = 2\u03c4 ."}, {"heading": "3.2 The Improved Algorithm", "text": "In this section, we introduce an improved algorithm that removes the log(n)-factor in the sample complexity.\nWe first introduce a few more subroutines (Lemma 10, Lemma 11, and Lemma 12) that will be useful\nfor our improved algorithm.\nLemma 10 [Zhou et al., 2014] For a set of arms S = {\u03b81 \u2265 . . . \u2265 \u03b8|S|}, there is an algorithm, denoted by OptMAI(S,K, , \u03b4), that computes -top-K arms with probability 1\u2212 \u03b4, with O ( |S| 2 log(1/\u03b4) ) pulls.\nThe following two lemmas show how to find a constant fraction of arms in the set of top-K arms and\na constant fraction of arms outside the set of top-K arms respectively.\nLemma 11 For a set of arms S = {\u03b81 \u2265 . . . \u2265 \u03b8|S|} such that \u03b8K \u2212 \u03b8 |S|+K 2 \u2265 \u03c6, K \u2264 2 3 |S|, there is an algorithm, denoted by Elim(S,K, \u03b3, \u03c6, \u03b4) in Algorithm 2, that computes T \u2286 S, |T | = |S| 10 successfully\nwith probability 1\u2212 \u03b4 using O ( |S| \u03c62 \u00b7 (log \u03b3\u22121 + log \u03b4\u22121) ) pulls in total, such that at most \u03b3K arms in T are in the top-K arms in S.\nAlgorithm 4: ReverseElim(S,K, \u03b3, \u03c6, \u03b4)\nInput: S: set of arms; K: top-K; \u03b3: fraction of arms; \u03b4: error probability; \u03c6: an additive error Output: Set of arms T with |T | = |S|10 such that at most \u03b3K arms in T are in the top-(|S| \u2212K) arms in S\n1 For each arm i in S, pull c\u03c62 \u00b7 (log \u03b3 \u22121 + log \u03b4\u22121) for some large enough constant c. Let \u03b8\u0303i be the\nempirical-mean of the i-th arm. 2 return |T | arms in S with the largest empirical-means.\nProof : Let \u03b8\u0303i be the empirical-mean of arm i after being pulled by c\u00b7 1\u03c62 log 1 \u03b3\u03b4 times for a sufficiently large constant c. By a Hoeffding\u2019s inequality, we have Pr [ |\u03b8\u0303i \u2212 \u03b8i| \u2265 \u03c6/2 ] \u2264 \u03b3\u03b4/2. LetXi = 1{|\u03b8\u0303i\u2212\u03b8i| \u2265 \u03c6/2},\nand thus E[Xi] \u2264 \u03b3\u03b4/2. Let X = \u2211 i\u2208[K] Xi; we have E[X] \u2264 \u03b3\u03b4K/2. By a Markov\u2019s inequality, we have that with probability at least 1 \u2212 \u03b4/2, X \u2264 \u03b3K. Consequently, with probability at least 1 \u2212 \u03b4/2, there are at most \u03b3K arms i \u2208 [K] with \u03b8\u0303i \u2264 \u03b8i \u2212 \u03c6/2 \u2264 \u03b8K \u2212 \u03c6/2.\nLet L = { |S|+K 2 +1, . . . , |S|}. Since K \u2264 2 3 |S|, we have |L| \u2265 |S| 6 . Using similar argument we can show\nthat with probability 1\u2212\u03b4/2, there are at least |S| 10 arms i \u2208 L with \u03b8\u0303i \u2264 \u03b8i+\u03c6/2 \u2264 \u03b8 |S|+K 2 +\u03c6/2 \u2264 \u03b8K\u2212\u03c6/2 (since \u03b8K \u2212 \u03b8 |S|+K\n2\n\u2265 \u03c6).\nTherefore, if we choose T to be the |S| 10 arms with the smallest empirical-means, then with probability\nat least 1\u2212 \u03b4, at most \u03b3K arms in T are in the top-K arms in S.\nLemma 12 For a set of arms S = {\u03b81 \u2265 . . . \u2265 \u03b8|S|} such that \u03b8K 2 \u2212 \u03b8K \u2265 \u03c6, K \u2265 |S|3 , there is an algorithm, denoted by ReverseElim(S,K, \u03b3, \u03c6, \u03b4) in Algorithm 2, that computes T \u2286 S, |T | = |S| 10\nsuccessfully with probability 1\u2212 \u03b4 using O ( |S| \u03c62 \u00b7 (log \u03b3\u22121 + log \u03b4\u22121) ) pulls in total, such that at most \u03b3K arms in T are in the bottom-(|S| \u2212K) arms in S.\nBy symmetry, the proof to Lemma 12 is basically the same as that for Lemma 11,\nNow we are ready to show our main result.\nTheorem 6 Algorithm 5 computes -top-K arms with probability at least 1 \u2212 \u03b4, and pulls the arms at most\nO \u2211 j\u2208[n] min { (\u2206j) \u22122, (\u03a8 t) \u22122}(log 1 + log 1 \u03b4 ) (27) times, where t and \u03a8 t are defined in (3) and (5) respectively.\nIt is worthwhile to note that the proposed algorithm is mainly for the theoretical interest and is rather complicated in terms of implementation. Thus, we omit the empirical study of this algorithm in the experimental section.\nIn the rest of this section we prove Theorem 6 by showing the correctness of Algorithm 5 and the\nanalyzing its query complexity."}, {"heading": "3.2.1 Correctness of Algorithm 5", "text": "Define E1 to be the event that all calls to the subroutine EstKthArm succeed.\nClaim 7 Pr[E1] \u2265 1\u2212 \u03b4/10.\nProof : Note that (r + r\u03c6) increases every time we call the four EstKthArm\u2019s at Line 8 and Line 9. Therefore by Theorem 5 we can bound the error probability of all calls to EstKthArm by 4\u00b7 \u2211\u221e z=1 \u03b4 100z2\n\u2264 \u03b4 10 .\nAlgorithm 5: ImprovedTopK(n,K, , \u03b4)\nInput: n: number of arms; K and : see the definition of -top-K arms; \u03b4: error probability. Assume K \u2264 n/2\nOutput: -top-K arms 1 set S \u2190 [n], r \u2190 1, r\u03c6 \u2190 1 2 set A,B \u2190 \u2205 3 set KL \u2190 (1\u2212 2)K,KR \u2190 (1 + 2)K + 1 4 while S 6= \u2205 do 5 r\u03c6 \u2190 r\u03c6 \u2212 1 6 repeat 7 r\u03c6 \u2190 r\u03c6 + 1; \u03c6\u2190 2\u2212r\u03c6\n8 \u03b8+K \u2190 EstKthArm ( S,KR \u2212 |A| , 2 100r2 , \u03c6, \u03b4 100(r+r\u03c6)2 ) ;\n\u03b8\u2212K \u2190 EstKthArm ( S,KL \u2212 |A| , 2 100r2 , \u03c6, \u03b4 100(r+r\u03c6)2 ) 9 \u03b8+ \u2190 EstKthArm ( S, |S|+K\u2212|A|2 , 2 100r2 , \u03c6, \u03b4 100(r+r\u03c6)2 ) ;\n\u03b8\u2212 \u2190 EstKthArm ( S, K\u2212|A|2 , 2 100r2 , \u03c6, \u03b4 100(r+r\u03c6)2 ) 10 until (10(K \u2212 |A|)\u03c6 < K ) or (\u03b8\u2212K \u2212 \u03b8 + K > 3\u03c6) or ( K \u2212 |A| \u2264 |S|2 ) \u2227 (\u03b8+K \u2212 \u03b8+ > 3\u03c6) or(\nK \u2212 |A| > |S|2 ) \u2227 (\u03b8+K \u2212 \u03b8+ > 3\u03c6) \u2227 (\u03b8\u2212 \u2212 \u03b8 \u2212 K > 3\u03c6)\n11 if (10(K \u2212 |A|)\u03c6 < K ) then 12 return OptMAI ( S,K \u2212 |A| , \u03c6, \u03b4100 ) \u222aA 13 if (\u03b8\u2212K \u2212 \u03b8 + K > 3\u03c6) then\n14 return EpsSplit ( S,K \u2212 |A| , KR\u2212KLK\u2212|A| , \u03c6, \u03b4 100 ) \u222aA\n15 U \u2190 Elim ( S,K \u2212 |A| , 2\n100r2 , \u03c6, \u03b4 100r2 ) 16 if (K \u2212 |A|) > |S|2 then 17 V \u2190 ReverseElim ( S,K \u2212 |A| , 2\n100r2 , \u03c6, \u03b4 100r2 ) 18 r \u2190 r + 1; S \u2190 S\\(U \u222a V ); A\u2190 A \u222a V ; B \u2190 B \u222a U 19 return A.\nDefine E2 to be the event that all calls to the subroutines Elim and ReverseElim succeed. Since r increases every time we call the two subroutines, by similar arguments we have:\nClaim 8 Pr[E2] \u2265 1\u2212 \u03b4/20.\nDefine E = E1 \u222a E2; we thus have Pr[E ] \u2265 1\u2212 \u03b4/5.\nWe next show that the misclassified arms are negligible during the run of the algorithm.\nLemma 13 Conditioned on E, suppose that the conditions of Lemma 11 and Lemma 12 always hold during the run of the Algorithm 5, then we always have\n1. The number of non-top-K arms in A, denoted by \u03b9A, is no more than 2K 40 . 2. The number of top-K arms in B, denoted by \u03b9B, is no more than 2K 40 .\nProof : By Lemma 11 we have \u03b9A \u2264 \u2211\u221e r=1 2 100r2 \u00b7 K \u2264 2K 40\n. Similarly, by Lemma 12 we have \u03b9B \u2264\u2211\u221e r=1 2 100r2 \u00b7K \u2264 2K 40 .\nWe now show that the conditions of Lemma 11 and Lemma 12 do hold. We first introducing a lemma\nshowing that \u03b8K is sandwiched by \u03b8 + K and \u03b8 \u2212 K during the run of Algorithm 5.\nLemma 14 Conditioned on E, at any point of the run of Algorithm 5, we have\n\u03b8\u2212K + \u03c6 \u2265 \u03b8K\u2212 2K 10 \u2265 \u03b8K \u2265 \u03b8K+1 \u2265 \u03b8K+1+ 2K 10 \u2265 \u03b8+K \u2212 \u03c6.\nProof : We consider a particular around r. The difference between \u03b8+K and the fixed value \u03b8KR is generated by calling of the subroutine EstKthArm at Line 8, which can be bounded by the the error introduced when selecting the (KR \u2212 |A|)-th largest arm in S plus max{\u03b9A, \u03b9B}. By Theorem 5, Lemma 13 and E , we have \u03b8+K \u2264 \u03b8KR\u2212\u03c4R + R, where R \u2264 \u03c6, and\n\u03c4R \u2264 \u03c4 \u00b7 (KR \u2212 |A|) + max{\u03b9A, \u03b9B} \u2264 2\n100r2 \u00b7 (1 + 2)K +\n2K 40 \u2264 2K 20 \u2264 KR \u2212K \u2212 2K 10 .\nWe thus have \u03b8+K \u2264 \u03b8K+1+ 2 10K + \u03c6.\nSimilarly, by Theorem 5, Lemma 13 and E , we have \u03b8\u2212K \u2265 \u03b8KL+\u03c4L \u2212 L, where L \u2264 \u03c6, and \u03c4L \u2264 max{\u03b9A, \u03b9B} \u2264 2K 40 \u2264 K \u2212KL \u2212 2K 10\n. We thus have \u03b8\u2212K \u2265 \u03b8K\u2212 2K 10 \u2212 \u03c6.\nWe have the following immediate corollary.\nCorollary 1 If \u03b8\u2212K \u2212 \u03b8 + K \u2264 3\u03c6, then \u03b8K , \u03b8K+1 \u2208 [\u03b8 + K \u2212 \u03c6, \u03b8 + K + 4\u03c6] and \u03b8K , \u03b8K+1 \u2208 [\u03b8 \u2212 K \u2212 4\u03c6, \u03b8 \u2212 K + \u03c6].\nIn the following, for convenience, we always use S[i] to denote the true-mean of the i-th arm (sorted decreasingly) in the current set S during the run of Algorithm 5, and use S[i..j] to denote the set of true-means of the (i, i + 1, . . . , j)-th arms in S. Let K\u0303 , K \u2212 |A|. We call S[1..K\u0303] the head of S, and S[K\u0303 + 1.. |S|] the tail of S. The following claim follows directly from Lemma 13 and Lemma 14.\nClaim 9 At any point during the run of Algorithm 5, it holds that \u03b8 K+\n2K 40\n\u2264 S[K\u0303] \u2264 \u03b8 K\u2212 2K\n40\n, and\nconsequently \u03b8\u2212K + \u03c6 \u2265 S[K\u0303] \u2265 \u03b8 + K \u2212 \u03c6.\nLemma 15 Conditioned on E, the conditions of Lemma 11 always hold during the run of Algorithm 5, that is, we have\nS[K\u0303]\u2212 S [ |S|+K\u0303\n2\n] \u2265 \u03c6 and K\u0303 \u2264 2\n3 |S| . (28)\nProof : For the first item of (28), by Theorem 5 we have\nS [ |S|+K\u0303\n2\n] \u2212 \u03c6 \u2264 \u03b8+, (29)\nwhich together with \u03b8+K \u2212 \u03b8 + > 3\u03c6 (testing condition at Line 10) and S[K\u0303] \u2265 \u03b8+K \u2212 \u03c6 (Claim 9) give S[K\u0303]\u2212 S [ |S|+K\u0303\n2\n] \u2265 \u03c6 (by two triangle inequalities).\nFor the second item of (28), note that if K\u0303 \u2264 |S| 2 , then K\u0303 \u2264 2 3 |S| holds directly. We thus consider\nthe case K\u0303 > |S| 2 . The observation is that at the beginning, before the first call to ReverseElim, we must have called Elim a number of times; each time we remove |S| 10 arms, most of which are from the tail of S. After the first time when K\u0303 > |S| 2 , we call both Elim and ReverseElim, with the intention of removing |S| 10 arms from the tail and the head respectively. It may happen that after calling both Elim and ReverseElim a few times, we again have K\u0303 \u2264 |S| 2 , at which point we will again only call Elim until the point that we are back to the case that K\u0303 > |S| 2 and then we will call both Elim and ReverseElim. Basically, the two patterns \u2018call Elim only\u2019 and \u2018call both Elim and ReverseElim\u2019 interleave, and we only need to consider one run of this interleaved sequence.\nBy Lemma 11 and Lemma 12 we know that at most 2\n100r2 K\u0303 arms in the head of S will be removed\nwhen calling Elim, and at most 2\n100r2 K\u0303 arms in the tail of S will be removed when calling ReverseElim.\nTherefore, the worst case for causing the imbalance between K\u0303 and (|S| \u2212 K\u0303) is that each call of Elim\nremoves |S| 10 from the tail of S, and each call of ReverseElim removes ( |S| 10 \u2212 2 100r2 K\u0303 ) arms from the head of S and 2\n100r2 K\u0303 arms from the tail of S. Note that the number of calls of Elim and ReverseElim\nis bounded by O(log 1 ) since when reaching Line 15 we always have |S| \u2265 \u2126(\u03c6) \u2265 \u2126( ). We thus have\n|S| \u2212 K\u0303 |S| \u2265\n( 1\n2 \u2212 0.1\n) \u00b7 O(log 1/ )\u220f\nr=1\n( 1\u2212 2\n100r2\n) \u2265 1\n3 ,\nwhich implies K\u0303 \u2264 2 3 |S|.\nLemma 16 Conditioned on E, the conditions of Lemma 12 always hold during the run of Algorithm 5, that is, we have\nS [ K\u0303 2 ] \u2212 S[K\u0303] \u2265 \u03c6 and K\u0303 \u2265 |S| 3 . (30)\nProof : In Algorithm 5, when calling ReverseElim, we always have\n\u03b8\u2212 \u2212 \u03b8\u2212K > 3\u03c6 and K\u0303 > |S| 2 . (31)\nThus K\u0303 \u2265 |S| 3 follows directly. By Theorem 5 we have\nS [ K\u0303 2 ] + \u03c6 \u2265 \u03b8\u2212, (32)\nwhich together with \u03b8\u2212\u2212\u03b8\u2212K > 3\u03c6 (first item of (31)) and S[K\u0303] \u2264 \u03b8 \u2212 K+\u03c6 (Claim 9), gives S [ K\u0303 2 ] \u2212S[K\u0303] \u2265 \u03c6 (by two triangle inequalities).\nLemma 15, Lemma 16 and Lemma 13 give the following corollary.\nCorollary 2 Conditioned on E, during the run of Algorithm 5 we always have (1) the number of nontop-K arms in A is no more than \u03b9A = 2K 40 , and (2) the number of top-K arms in B is no more than \u03b9B = 2K 40 .\nWe now consider the boundary cases. At Line 11 when the condition is met, we have \u03c6 < K 10(K\u2212|A|) ,\nand thus with probability (1 \u2212 \u03b4 100 ) the total error introduced by subroutine OptMAI at Line 12 is bounded by (K \u2212 |A|)\u03c6 \u2264 K 10 (Lemma 10). At Line 14, with probability (1\u2212 \u03b4 100 ) the error introduced by subroutine EpsSplit is bounded by 2 \u00b7 KR\u2212KL K\u2212|A| \u00b7 (K \u2212 |A|) \u2264 2 \u00b7 2 2K = 4 2K. (Lemma 9).\nBy E , Corollary 2, and the errors introduced by boundary cases, we have that with probability 1\u2212 \u03b4\n5 \u22122\u00b7 \u03b4 100 \u2265 1\u2212\u03b4, the total error introduced in our top-K estimation is at most \u03b9A+\u03b9B+ K10 +4 2K \u2264 K."}, {"heading": "3.2.2 Complexity of Algorithm 5", "text": "In the whole analysis we assume that E holds. Recall that by definition \u2206i = max(\u03b8i \u2212 \u03b8K+1, \u03b8K \u2212 \u03b8i), and t \u2208 [K] is the largest integer such that \u2206K\u2212t \u00b7 t \u2264 K and \u2206K+1+t \u00b7 t \u2264 K . Recall that \u03a8t = min{\u2206K\u2212t,\u2206K+1+t}, and \u03a8 t = max( ,\u03a8t).\nBy Theorem 5, Lemma 11, Lemma 12, Lemma 10 and Lemma 9 we have: every call to EstKthArm costsO ( |S| \u03c62 log rr\u03c6 \u03b4 ) pulls; every call to Elim and ReverseElim costsO ( |S| \u03c62 log r \u03b4 ) ; the call to OptMAI\ncosts O ( |S| \u03c62 log 1 \u03b4 ) ; and the call to EpsSplit costs O ( |S| \u03c62 log K\u0303 \u03b4K ) = O ( |S| \u03c62 log 1 \u03b4 ) . So our task is to lower bound the value of \u03c6 when these subroutines are called, and the maximum values of r\u03c6 and r.\nLemma 17 Conditioned on E, at any point of the run of Algorithm 5, we have \u03c6 = \u2126(\u03a8 t), and r\u03c6 = O(log 1\n), r = O(log n|S| ).\nProof : First, by the testing condition 10(K \u2212 |A|)\u03c6 < K at Line 10, together with the boundary cases at Line 11-12 and the fact that \u03c6\u2190 \u03c6/2 at every update, it holds that\n10(K \u2212 |A|) \u00b7 2\u03c6 \u2265 K , (33)\nwhich implies \u03c6 = \u2126( ) when we call all the subroutines.\nWe now show \u03c6 = \u2126(\u03a8t). From the proof of Lemma 15 we know that during the run of the Algorithm\nwe always have K\u0303 \u2264 2|S| 3 . We focus on an arbitrary but fixed point during the run of the algorithm. By Theorem 5 we have\nS [ |S|+K\u0303\n2\n] \u2212 \u03c6 \u2264 \u03b8+ \u2264 S [( 1\u2212 2\n100r2\n) |S|+K\u0303\n2\n] + \u03c6, (34)\nwhich, combined with the fact that K\u0303 \u2264 2|S| 3 , gives\nS [ |S|+K\u0303\n2\n] \u2212 \u03c6 \u2264 \u03b8+ \u2264 S [ |S|+K\u0303\u2212 |S|\n2\n] + \u03c6. (35)\nBy (33) we have 2K 40 \u2264 40 \u00b7 10K\u0303 \u00b7 2\u03c6 \u2264 K\u0303 2 . (36) Applying Claim 9 on both sides of (34), together with (36) and K\u0303 \u2264 2|S| 3 we have\n\u03b8 K+ |S|\u2212K\u0303\n2 + 2K 40\n\u2212 \u03c6 \u2264 \u03b8+ \u2264 \u03b8 K+ |S|\u2212K\u0303\u2212 |S|\n2 \u2212 2K 40\n+ \u03c6\n\u21d2 \u03b8 K+ |S|\u2212K\u0303\n2 + K\u0303 2\n\u2212 \u03c6 \u2264 \u03b8+ \u2264 \u03b8 K+ |S|\u2212K\u0303\u2212 |S|\n2 \u2212 K\u0303 2\n+ \u03c6\n\u21d2 \u03b8+ \u2208 \u03b8K+\u03b7 \u00b1 \u03c6 for an \u03b7 \u2265 |S|\u2212K\u03032 \u2212 |S| \u2265 0.16 |S| . (37)\nBy symmetry, using a similar argument we can show that for a sufficiently small constant c\u03b7\u2032 , we\nhave\n\u03b8\u2212 \u2208 \u03b8K\u2212\u03b7\u2032 \u00b1 \u03c6 for an \u03b7\u2032 \u2265 c\u03b7\u2032 |S| , (38)\nWe first consider the case where K \u2212 |A| \u2264 |S| 2 . We analyze the following two sub-cases.\n1a) The case when \u03b7 > t. We have:\n\u03c6 \u2265\u03b8 + K \u2212 \u03b8 +\n6 (by the testing condition at Line 10 and \u03c6\u2190 \u03c6/2 at each update)\n\u2265 (\u03b8K \u2212 4\u03c6)\u2212 (\u03b8K+\u03b7 + \u03c6) 6 (by Corollary 1 and (37)) \u2265\u03b8K \u2212 4\u03c6\u2212 (\u03b8K+t+1 + \u03c6) 6 \u2265\u03a8t \u2212 5\u03c6 6 (by the definition of \u03a8t).\n1b) The case when \u03b7 \u2264 t. We prove by contradiction. Suppose that \u03c6 \u2264 \u03a8t/ct for a sufficiently large constant ct, then\n\u03a8t \u00b7 t \u2265 ct\u03c6 \u00b7 \u03b7 (37) \u2265 ct\u03c6 \u00b7 0.16 |S| K\u0303\u2264 2|S| 3 \u2265 ct\u03c6 \u00b7 0.16 \u00b7 1.5K\u0303 (33) > K ,\nA contradition to the definition of t.\nWe then consider the case where K\u2212|A| > |S| 2 . Now by the testing condition at Line 10 and \u03c6\u2190 \u03c6/2\nat each update, we know that at least one of the following inequality holds: \u03c6 \u2265 \u03b8 + K \u2212\u03b8+\n6 ; or \u03c6 \u2265 \u03b8\n\u2212\u2212\u03b8\u2212 K\n6 . If\nthe first inequality holds, the case-analysis above suffices. Otherwise, we know that the second inequality holds, we analyze the following two sub-cases in a similar fashion.\n2a) The case when \u03b7\u2032 > t.\n\u03c6 \u2265\u03b8 \u2212 \u2212 \u03b8\u2212K\n6\n\u2265 (\u03b8K\u2212\u03b7 \u2032 \u2212 \u03c6)\u2212 (\u03b8K+1 + 4\u03c6)\n6 (by Corollary 1 and (38))\n\u2265 (\u03b8K\u2212t \u2212 \u03c6)\u2212 (\u03b8K+1 + 4\u03c6) 6 \u2265\u03a8t \u2212 5\u03c6 6 (by the definition of \u03a8t).\n2b) The case when \u03b7\u2032 \u2264 t. This case is symmetric to Case 1b), and we omit here.\nSince \u03c6 = 2\u2212r\u03c6 , we immediately have r\u03c6 = O(log 1 \u03a8 t ) = O(log 1 ). By the testing condition at Line 13, and the fact that every time we call Elim and ReverseElim we remove a constant fraction of arms from S, we thus have r = O(log n|S| ).\nWe now look at a particular call to Elim which removes 1 10 -fraction of arms in S, and the tail of S.\nFrom the testing condition at Line 10 we know that\n\u03b8+K \u2212 \u03b8 + \u2264 3 \u00b7 2\u03c6. (39)\nFrom Corollary 1 we have that\n\u03b8K \u2264 \u03b8+K + 4\u03c6. (40)\nFrom (39), (40) and the second inequality of (35), by applying two triangle inequalities we have\n\u03b8K \u2212 S [ |S|+K\u0303\u2212 |S|\n2\n] \u2264 11\u03c6, (41)\nwhich implies that for all\nj \u2208 Q = [ 1.01K\u0303, |S|+K\u0303\u2212 |S|\n2\n] (|Q| \u2265 |S|\n10 since K\u0303 \u2264 2|S| 3 ), (42)\nletting m(j) \u2208 [n] such that \u03b8m(j) = S[j], we have\n\u2206m(j) = \u03b8K \u2212 S [j] \u2264 11\u03c6. (43)\nWe thus can charge all the previous cost spent on the 1 10 -fraction of arms in S that are removed by Elim, which is bounded by O ( |S| \u03c62 ( logdlog n|S|e+ log 1 + log 1 \u03b4 )) , to\nO (\u2211 j\u2208Q 1 \u22062m(j) ( log \u2308 log\nn\nm(j)\u2212K\n\u2309 + log 1 + log 1\n\u03b4\n)) , (44)\nwhere we have used the fact that\n|S| = \u2126(j \u2212 K\u0303) (by (42))\n\u2265 \u2126(m(j)\u2212K). (by (42), Corollary 2 and (36))\nNote that it is possible that in multiple calls to Elim with parameters (S1, \u00b7, \u00b7, \u03c61, \u00b7), . . . , (S\u03ba, \u00b7, \u00b7, \u03c6\u03ba, \u00b7) where \u03c61 \u2265 . . . \u2265 \u03c6\u03ba, we charge the same item j \u2208 Q1 \u2229 . . . \u2229 Q\u03ba multiple times. However, since \u03c6i+1 \u2264 \u03c6i/2 for all i \u2208 [\u03ba\u2212 1], the total charge on j is at most twice of that of the last charge (i.e., the one with parameter \u03c6\u03ba).\nBy symmetry, we can use the same arguments for ReverseElim and the head of S, and get a same bound as (44) except that we need to replace m(j)\u2212K with K \u2212m(j). We thus conclude that the total\nnumber of pulls can be bounded by\nO \u2211 j\u2208[n] 1 \u22062j ( log \u2308 log n\u2223\u2223j \u2212K + 1 2 \u2223\u2223 \u2309 + log 1 + log 1 \u03b4 ) . (45) We know from Lemma 17 that we always have \u03c6 = \u2126(\u03a8 t), we can thus \u201ctruncate\u201d Expression (45) and bound the total cost by\nO \u2211 j\u2208[n]\n1\nmax{\u22062j , (\u03a8 t)2}\n( log \u2308 log\nn\u2223\u2223j \u2212K + 1 2 \u2223\u2223 \u2309 + log 1 + log 1 \u03b4 ) . (46) Now we introduce the following lemma (the proof of which is deferred to the Appendix). Lemma 18 If M > a1 \u2265 . . . \u2265 an \u2265 1, then \u2211 i\u2208[n] ai log(n/i) \u2264 O(dlogMe) \u2211 i\u2208[n] ai.\nWith Lemma 18 we can further simplify (46) to\nO \u2211 j\u2208[n]\n1\nmax{\u22062j , (\u03a8 t)2}\n( log max { 1\nmax{\u22062j , (\u03a8 t)2} | j \u2208 [n]\n} + log 1 + log 1\n\u03b4 ) = O\n\u2211 j\u2208[n]\n1\nmax{\u22062j , (\u03a8 t)2}\n( log 1 + log 1\n\u03b4 ) ."}, {"heading": "4 A Lower Bound", "text": "In this section we prove Theorem 3. In Section 4.1, we introduce a lower-bound to a coin-tossing problem. In Section 4.2, we reduce the proof of Theorem 3 to the coin-tossing problem."}, {"heading": "4.1 The Coin-Tossing Problem", "text": "We say a coin is p-biased if the probability that a toss turns head is p, and we call p is the value of the coin. Set \u03b7 = 10\u22124.\nDefinition 3 (Coin-Tossing) In this problem, given a coin that may be (0.5 + \u03b7)-biased or (0.5 \u2212 \u03b7)biased, we want to know its exact value by tosses, and we are allowed to give up and output \u2018unknown\u2019 with probability at most 0.9.\nWe have the following theorem.\nTheorem 7 Any algorithm that solves the coin-tossing problem correctly with probability (1 \u2212 ) needs \u2126(log 1/ ) tosses.\nProof : Since the input is distributional we only need to focus on deterministic algorithms. Let m be the total number of tosses of the coin, and let B = (B1, . . . , Bm) \u2208 {0, 1}m be the sequence of outcomes. Let D\u03b2 be the distribution of B where each Bi is the outcome of tossing a \u03b2-biased coin. For v \u2208 {0, 1}m, let |v| be the number of 1-coordinates in v.\nOur first observation is that for any b1,b2 \u2208 {0, 1}m, if |b1| = |b2|, then Pr[B = b1] = Pr[B = b2]. Therefore, the final output should only depend on the value |B| but not the ordering of the 0/1 sequence. In other words, we can view the output of the algorithm as a function\nf : {0, 1, . . . ,m} \u2192 {0.5\u2212 \u03b7, 0.5 + \u03b7,\u22a5},\nwhere {0, 1, . . . ,m} stand for possible values of |B|, and \u2018\u22a5\u2019 represents \u2018unknown\u2019. Recall that the algorithm can give up and output \u2018unknown\u2019 with probability at most 0.9. By observing that B \u223c D0.5+\u03b7 and B \u223c D0.5\u2212\u03b7 are symmetric, the best strategy must set f(x) =\u22a5 for x \u2208 [0.5m\u2212 t, 0.5m+ t], where t \u2208 N is the maximum value such that\nPrB\u223cD0.5\u2212\u03b7 [0.5m\u2212 t \u2264 |B| \u2264 0.5m+ t] \u2264 0.9. (47)\nIntuitively, [0.5m \u2212 t, 0.5m + t] is the range where conditioned on |B| \u2208 [0.5m \u2212 t, 0.5m + t] the value of the coin is the most uncertain (so that the algorithm simply outputs \u2018\u22a5\u2019). We set f(x) = 0.5 \u2212 \u03b7 if x \u2208 [0, 0.5m\u2212 t), and f(x) = 0.5 + \u03b7 if x \u2208 (0.5m+ t,m]. The error probability of this strategy is\nPrB\u223cD0.5\u2212\u03b7 [|B| > 0.5m+ t]. (48)\nWe now try to upper bound t. First, it is easy to see that 0.5m \u2212 t \u2264 (0.5 \u2212 \u03b7)m, or t \u2265 \u03b7m, since otherwise LHS of (47) is at most 1/2, violating the choice of t. By a Hoeffding\u2019s inequality we have\nPrB\u223cD0.5\u2212\u03b7 [|B| \u2264 0.5m\u2212 t] = PrB\u223cD0.5\u2212\u03b7 [|B| \u2264 E[|B|]\u2212 (t\u2212 \u03b7m)] \u2264 e \u2212m(t/m\u2212\u03b7)2/2.\nWe thus have e\u2212m(t/m\u2212\u03b7) 2/2 \u2265 (1\u2212 0.9)/2, and consequently\nt \u2264 \u03b7m+ ct \u221a m (49)\nfor some large enough constant ct.\nWe now lower bound the expression (48). We will need the following anti-concentration result which\nis an easy consequence of Feller Feller [1943] (cf. Matousek and Vondra\u0301k. [2008]).\nFact 1 (Matousek and Vondra\u0301k. [2008]) Let Y be a sum of independent random variables, each attaining values in [0, 1], and let \u03c3 = \u221a Var[Y ] \u2265 200. Then for all t \u2208 [0, \u03c32/100], we have\nPr[Y \u2265 E[Y ] + t] \u2265 c \u00b7 e\u2212t 2/(3\u03c32)\nfor a universal constant c > 0.\nIn our case, since |B| can be seen as a sum of Bernoulli variables with p = 0.5\u2212 \u03b7, Var|B| = m \u00b7 (0.5\u2212 \u03b7) \u00b7 (0.5 + \u03b7) \u2265 0.24m. By (49) we have \u03b7m+ t \u2264 2\u03b7m+ ct \u221a m \u2264 Var|B|/100 by our choice of \u03b7. Thus by applying Fact 1 we have\nPrB\u223cD0.5\u2212\u03b7 [|B| > 0.5m+ t] = PrB\u223cD0.5\u2212\u03b7 [|B| > E[|B|] + (\u03b7m+ t)]\n\u2265 c \u00b7 e\u2212(\u03b7m+t) 2/(3\u00b70.24m) \u2265 e\u2212\u2126(m).\nTo make the best strategy succeeds with probability at least 1\u2212 , we have to make e\u2212\u2126(m) \u2264 , which means we have to set m \u2265 \u2126(log 1/ )."}, {"heading": "4.2 The Reduction", "text": "We show a reduction from the coin-tossing problem to the -top-K arms problem. For technical convenience we set K = n/2, and assume that K \u2265 cK for a large enough constant cK .\nLemma 19 If there is an algorithm for -top-K arms that succeeds with probability 0.9 using C \u2264 f(n,K)/poly( ) tosses, then there is an algorithm for coin-tossing that succeeds with (1\u2212 ) using O(C/n)\ntosses. Moreover, the instances fed into the -top-K arms algorithm have the property that H(t, ) = \u0398(n\u03b7\u22122) = \u0398(n) for \u2265 cK/K.\nWe prove Lemma 19 in two steps. We first perform an input reduction, and then show that we can\nconstruct an efficient algorithm for coin-tossing using an algorithm for -top-K arms.\nInput reduction. Given an input X for coin-tossing, we construct an input Y = (X1, . . . , Xn) for -topK arms as follows: we randomly pick a set S \u2286 [n] with |S| = K, and set Xi (i \u2208 S) to be (0.5+\u03b7)-biased coins (denoted by Xi = 0.5+\u03b7 for convenience), and Xi (i 6= [n]\\S) to be (0.5\u2212\u03b7)-biased coins. We then pick a random index I \u2208 [n], and reset XI = X. Since in our input Y , the number of (0.5 + \u03b7)-biased coins is either K \u2212 1, K, or K + 1, while the rest are (0.5 \u2212 \u03b7)-biased coins, it can be checked that H(t, ) = \u0398(n\u03b7\u22122) for \u2208 [cK/K, \u03b7].\nClaim 10 If S\u2032 is a set of \u03b3-top-K arms (\u03b3 \u2265 2\u03b7/K) on Y , then with probability at least 1 \u2212 2\u03b3/\u03b7 we can correctly determine the value of X by checking whether I \u2208 S\u2032.\nProof : If X = XI = 0.5 + \u03b7, then to compute -top-K arms correctly we need to output a set S \u2032 such that 1\nK \u00b7 \u2211 i\u2208S\u2032 Xi \u2265 1 K \u00b7 \u2211 i\u2208S Xi \u2212 \u03b3 \u2265 (0.5 + \u03b7)\u2212 \u03b3.\nBy simple calculation we must have |{i \u2208 S\u2032 | Xi = 0.5 + \u03b7}| \u2265 (\n1\u2212 \u03b3 2\u03b7\n) K. Since all (0.5 + \u03b7)-biased\ncoins are symmetric, the probability that I \u2208 S\u2032 is at least( 1\u2212 \u03b3\n2\u03b7 ) K + 1 \u2265 ( 1\u2212 \u03b3 \u03b7 ) . (50)\nOtherwise if X = XI = 0.5\u2212 \u03b7, then to compute -top-K arms correctly we need to output a set S\u2032\nsuch that 1 K \u00b7 \u2211 i\u2208S\u2032 Xi \u2265 1 K \u00b7 \u2211 i\u2208S Xi \u2212 \u03b3 \u2212 2\u03b7 K \u2265 0.5 + \u03b7 \u2212 \u03b3 \u2212 2\u03b7 K ,\nBy simple calculation we must have |{i \u2208 S\u2032 | Xi = 0.5 + \u03b7}| \u2265 (\n1\u2212 \u03b3 \u03b7\n) K, or, |{i \u2208 S\u2032 | Xi = 0.5\u2212 \u03b7}| \u2264\n\u03b3 \u03b7 K. Again since all (0.5\u2212 \u03b7)-biased coins are symmetric, the probability that I \u2208 S\u2032 is at most\n\u03b3 \u03b7 K\nn\u2212 (K \u2212 1) \u2264 2\u03b3 \u03b7 . (51)\nBy (50) and (51), we conclude that by observing whether I \u2208 S\u2032 or not we can determine whether X = XI = 0.5 + \u03b7 or 0.5\u2212 \u03b7 correctly with probability at least 1\u2212 2\u03b3/\u03b7.\nAn algorithm for coin-tossing. Let \u2032 = \u03b7/4 \u00b7 . We now construct an algorithm A\u2032 for coin-tossing using an algorithm A for \u2032-top-K arms. Given an input X for coin-tossing, we first perform the input reduction as described above, getting an input Y . We then runA on Y . We give up and output \u2018unknown\u2019 during the run of A if the number of tosses on XI is more than 20C/n. If A finishes then let S\u2032 be the outputted set of top-K coins. We then perform a verification step to test whether S\u2032 is indeed a set of\n\u2032-top-K arms, and output \u2018unknown\u2019 if the verification fails. The verification is done as follows: we first compute \u03c1 = 1|S\u2032\\I| \u00b7 \u2211 i\u2208S\u2032\\I Xi, and then verify whether \u03c1 \u2265 (0.5 + \u03b7)\u2212 ( \u2032+ 2\u03b7/K). Finally, if we have not outputted \u2018unknown\u2019, we output X = 0.5 + \u03b7 if I \u2208 S\u2032, and X = 0.5\u2212 \u03b7 if I 6\u2208 S\u2032.\nWe now try to bound the probability that A\u2032 outputs \u2018unknown\u2019.\nClaim 11 The probability that we give up during the run of A is at most 0.1.\nProof : We prove for the case X = 0.5 + \u03b7; same arguments hold for the case X = 0.5\u2212 \u03b7 since we have set K = n/2. Note that if I \u2208 S, then we have K coins (including XI) in Y that has value (0.5 + \u03b7); otherwise if I 6\u2208 S then we have K + 1 such coins. By symmetry, the expected tosses on XI is at most C/K = 2C/n. By a Markov inequality the probability that XI has been tossed by at least 20C/n is at most 0.1.\nClaim 12 Suppose we do not give up when running A, the verification step fails with probability at most 0.1.\nProof : Note that A\u2032 knows the value of all other coins except XI , simply because {Xi | i \u2208 [n]\\I} are all constructed by A\u2032. The 2\u03b7/K factor in the test \u03c1 \u2265 (0.5 + \u03b7) \u2212 ( \u2032 + 2\u03b7/K) comes from the fact that we do not know the exact value of XI , which will affect the estimation of the\n1 K \u2211 i\u2208S\u2032 Xi by at\nmost an additive factor 2\u03b7/K. Therefore the failure probability of the verification is at most the failure probability of A, which is upper bounded by 0.1.\nNow we are ready to prove the lemma.\nProof :(of Lemma 19) First, note that if there is an algorithm for -top-K arms that succeeds with probability 0.9 using C \u2264 f(n,K)\u00b7poly( ) tosses, then there is an algorithm for \u2032-top-K arms ( \u2032 = 4/\u03b7 \u00b7 for a constant \u03b7) that succeeds with probability 0.9 using O(C).\nWe now show that Algorithm A\u2032 constructed above for coin-tossing has the following properties, which conclude the lemma.\n1. It tosses X at most O(C/n) times.\n2. It outputs \u2018unknown\u2019 with probability at most 0.9.\n3. When it does not output \u2018unknown\u2019, it successfully computes X with probability at least 1\u2212 .\nThe first item holds according to the construction of A\u2032. For the second item, the probability that A\u2032 outputs \u2018unknown\u2019 is upper bounded by the sum of the probability that we give up when running A and the failure probability of A, which is at most 0.1 + 0.1 < 0.9 by Claim 11 and Claim 12. For the third item, note that any S\u2032 that passes the verification step in A\u2032 is a set of ( \u2032 + 2\u03b7/K)-top-K arms. The item holds by applying Claim 10 (setting \u03b3 = \u2032+ 2\u03b7/K). Note that 2\u03b3/\u03b7 = (2/\u03b7) \u00b7 ( \u2032+ 2\u03b7/K) \u2264 since \u2032 = \u03b7/4 \u00b7 and K \u2265 cK for a sufficiently large constant cK .\nBy Theorem 7 and Lemma 19 we have the following theorem.\nTheorem 8 Any algorithm that computes that -top-K arms correctly with probability 0.9 needs \u2126(n log \u22121) tosses.\nTogether with the bound for H(t, ) in Lemma 19, we prove Theorem 3."}, {"heading": "5 Experiments", "text": "In this section we present the experimental results. While our theorems are presented in the PAC form, it is in general difficult to verify them directly because the parameter is merely an upper bound and the actual aggregate regret may deviate from it. In our experiment, we convert our Algorithm 1 to the fixedbudget version (that is, fix the budget of the number of pulls and calculate the aggregate regret). We compare our Algorithm 1 (AdaptiveTopK ) with two state-of-the-art methods \u2013 OptMAI in Zhou et al. [2014] and CLUCB-PAC in Chen et al. [2014]. The comparison between OptMAI /CLUCB-PAC and previous methods (e.g., the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al. [2014] and Chen et al. [2014], and thus omitted here for the\nclarity of the presentation. To convert our algorithm to the fixed-budget version, we remove the outer while loop of Algorithm 1. As a replacement, we keep track of the total number of pulls, and stop pulling the arms once the budget is exhausted.\nWe test our algorithm on both synthetic and real datasets as described as follows. For simulated datasets, we set the total number of arms n = 1, 000 and vary the parameter K. We set the tolerance parameter = 0.01. In AdaptiveTopK and CLUCB-PAC, another parameter \u03b4 (i.e., the failure probability) is required and we set \u03b4 = 0.01.\n\u2022 TwoGroup: the mean reward for the top K arms is set to 0.7 and that for the rest of the arms is set to 0.3.\n\u2022 Uniform: we set \u03b8i = 1\u2212 in for 1 \u2264 i \u2264 n. \u2022 Synthetic-p: we set \u03b8i = (1\u2212 Kn ) + K n \u00b7 (1\u2212 i K )p for each i \u2264 K and \u03b8i = (1\u2212 Kn )\u2212 n\u2212K n \u00b7 ( i\u2212K n\u2212K ) p\nfor each i > K. Note that Synthetic-1 is identical to Uniform. When p is larger than 1, arms are made closer to the boundary that separates the top-K from the rest (i.e. 1 \u2212 K n ). When p is smaller than 1, arms are made farther to the boundary. We normalize all the arms such that the mean values of the arms still span the whole interval [0, 1]. We consider p = .5, 1, 6.\n\u2022 Rte: We generate \u03b8 from a real recognizing textual entailment (RTE) dataset Snow et al. [2008]. There are n = 164 workers and we set each \u03b8i be the true labeling accuracy of the i-th worker.\nNote that the true label for each instance is provided in this dataset.\nFor each dataset, we first fix the budget (total number of pulls allowed) and run each algorithm 200 times. For each algorithm, we calculate the empirical probability (over 200 runs) that the aggregate regret of the selected arms is above the tolerance threshold = 0.01, which is called failure probability. A smaller failure probability means better performance. For each dataset and different K, we plot the curve of failure probability by varying the number of pulls. The results are shown in Figure 1-5.\nIt can be observed from the experimental results that AdaptiveTopK (Algorithm 1) outperforms CLUCB-PAC in almost all the datasets. When K is relatively small, OptMAI has the best performance in most datasets. When K is large, AdaptiveTopK outperforms OptMAI . The details of the experimental results are elaborated as follows.\n\u2022 For TwoGroup dataset (see Figure 1), AdaptiveTopK outperforms other algorithms significantly for all values ofK. The advantage comes from the adaptivity of our algorithm. In the TwoGroup dataset,\ntop-K arms are very well separated from the rest. Once our algorithm identifies this situation, it need only a few pulls to classify the arms. In details, the inner while loop (Line 7) of Algorithm 1 make it possible to accept/reject a large number of arms in one round as long as the algorithm is confident.\n\u2022 As K increases, the advantage of AdaptiveTopK over other algorithms (OptMAI in particular) becomes more significant. This can be explained by the definition of H(t, ): t = t( ,K) usually\nbecomes bigger as K grows, leading to a smaller hardness parameter H(t, ).\n\u2022 A comparison between Synthetic-.5, Uniform, Synthetic-6 reveals that the advantage of Adap-\ntiveTopK over other algorithms (OptMAI in particular) becomes significant in both extreme scenarios, i.e., when arms are very well separated (p 1) and when arms are very close to the separation boundary (p 1)."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we proposed two algorithms for a PAC version of the multiple-arm identification problem in a stochastic multi-armed bandit (MAB) game. We introduced a new hardness parameter for characterizing the difficulty of an instance when using the aggregate regret as the evaluation metric, and established the instance-dependent sample complexity based on this hardness parameter. We also established lower bound results to show the optimality of our algorithm in the worst case. Although we only consider the case when the reward distribution is supported on [0, 1], it is straightforward to extend our results to sub-Gaussian reward distributions.\nFor future directions, it is worthwhile to consider more general problem of pure exploration of MAB under matroid constraints, which includes the multiple-arm identification as a special case, or other polynomial-time-computable combinatorial constraints such as matchings. It is also interesting to extend the current work to finding top-K arms in a linear contextual bandit framework.\nAppendix\nProof of Lemma 18\nProof : We partition all ai\u2019s to groups G1, . . . , Gdlog2Me where Gj = {i \u2208 [n] | ai \u2208 [2 j\u22121, 2j)}. Let S = \u2211n i=1 ai \u2265 n. For each j \u2208 {1, 2, . . . , dlog2 Me}, let \u03b4j = 1 S \u2211 i\u2208Gj ai. Observe that we have\u2211\nj \u03b4j = 1. We will show for each Gj that\n\u2211 i\u2208Gj ai log(n/i) \u2264 O ( \u03b4j logM + \u03b4j log \u03b4 \u22121 j + 1 ) S. (52)\nOnce we establish (52), we prove the lemma as follows. We sum up the inequalities for all j and get\n\u2211 i\u2208[n] ai log(n/i) \u2264 dlog2Me\u2211 j=1 O ( \u03b4j logM + \u03b4j log \u03b4 \u22121 j + 1 ) S\n= O(dlogMe)S +O(S) dlog2Me\u2211 j=1 \u03b4j log \u03b4 \u22121 j \u2264 O(dlogMe)S +O(S) \u00b7 logdlog2 Me \u2264 O(dlogMe)S,\nwhere the second last inequality is by Jensen\u2019s inequality and the convexity of \u03b4 log \u03b4 over \u03b4 \u2208 (0, 1). Now we prove (52) for each group G = Gj and \u03b4 = \u03b4j . Let b = maxi\u2208G{ai} \u2264 M . By our partition rule we have that b \u2264 2ai for all i \u2208 G. Observe that\n\u2211 i\u2208G ai log(n/i) \u2264 b |G|\u2211 i=1 log(n/i) = b|G| logn\u2212 b log(|G|!) \u2264 b|G| log(n/|G|) +O(b|G|). (53)\nThe last inequality of (53) is by Stirling\u2019s approximation. Since b|G| \u2265 \u2211 i\u2208G ai = \u03b4S, we have |G| \u2265 \u03b4S b \u2265 \u03b4n b . We finish the proof of (52) by upper-bounding the RHS of (53) by\nb|G| log(b\u03b4\u22121) +O(b|G|) \u2264 2\u03b4S log(b\u03b4\u22121) +O(b|G|) \u2264 O ( \u03b4S log(M\u03b4\u22121) + S ) ,\nwhere the first inequality is because b|G| \u2264 \u2211 i\u2208G 2ai = 2\u03b4S ."}], "references": [{"title": "Large-scale markov decision problems with KL control cost and its application to crowdsourcing", "author": ["Y. Abbasi-Yadkori", "P. Bartlett", "X. Chen", "A. Malek"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2015}, {"title": "Best arm identification in multi-armed bandits", "author": ["J. Audibert", "S. Bubeck", "R. Munos"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Multiple identifications in multi-armed bandits", "author": ["S. Bubeck", "T. Wang", "N. Viswanathan"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "On top-k selection in multi-armed bandits and hidden bipartite graphs", "author": ["W. Cao", "J. Li", "Y. Tao", "Z. Li"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Pure exploration of multi-armed bandit under matroid constraints", "author": ["L. Chen", "A. Gupta", "J. Li"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Towards instance optimal bounds for best arm identification", "author": ["L. Chen", "J. Li", "M. Qiao"], "venue": "arXiv preprint arXiv:1608.06031,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["S. Chen", "T. Lin", "I. King", "M.R. Lyu", "W. Chen"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Statistical decision making for optimal budget allocation in crowd labeling", "author": ["X. Chen", "Q. Lin", "D. Zhou"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "PAC bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the Annual Conference on Learning Theory (COLT),", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "Journal of machine learning research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Generalization of a probability limit theorem of cramer", "author": ["W. Feller"], "venue": "Trans. Amer. Math. Soc,", "citeRegEx": "Feller.,? \\Q1943\\E", "shortCiteRegEx": "Feller.", "year": 1943}, {"title": "Multi-bandit best arm identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Improved learning complexity in combinatorial pure exploration bandits", "author": ["V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "R. Ortner", "P. Bartlett"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gabillon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2016}, {"title": "UCB : An optimal exploration algorithm for multi-armed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Top arm identification in multi-armed bandits with batch arm pulls", "author": ["K.-S. Jun", "K. Jamieson", "R. Nowak", "X. Zhu"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jun et al\\.", "year": 2016}, {"title": "PAC subset selection in stochastic multi-armed bandits", "author": ["S. Kalyanakrishnan", "A. Tewari", "P. Auer", "P. Stone"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["E. Kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2016}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "The probabilistic method", "author": ["J. Matousek", "J. Vondr\u00e1k"], "venue": "Lecture Notes,", "citeRegEx": "Matousek and Vondr\u00e1k.,? \\Q2008\\E", "shortCiteRegEx": "Matousek and Vondr\u00e1k.", "year": 2008}, {"title": "Simple bayesian algorithms for best arm identification", "author": ["D. Russo"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Russo.,? \\Q2016\\E", "shortCiteRegEx": "Russo.", "year": 2016}, {"title": "Cheap and fast\u2014but is it good?: Evaluating nonexpert annotations for natural language tasks", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Best-arm identification in linear bandits", "author": ["M. Soare", "A. Lazaric", "R. Munos"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Soare et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soare et al\\.", "year": 2014}, {"title": "Optimal PAC multiple arm identification with applications to crowdsourcing", "author": ["Y. Zhou", "X. Chen", "J. Li"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms.", "startOffset": 87, "endOffset": 106}, {"referenceID": 24, "context": "The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et al. [2014] that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance.", "startOffset": 87, "endOffset": 264}, {"referenceID": 24, "context": "To measure the quality of the selected arms, we adopt the notion of aggregate regret (or regret for short) from Zhou et al. [2014]. In particular, we assume that arms are ordered by their mean \u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8n so that the set of the best K arms is {1, .", "startOffset": 112, "endOffset": 131}, {"referenceID": 20, "context": "To solve this problem, Zhou et al. [2014] proposed the OptMAI algorithm and established its sample complexity \u0398 ( n 2 ( 1 + ln \u03b4 \u22121 K )) , which is shown to be asymptotically optimal.", "startOffset": 23, "endOffset": 42}, {"referenceID": 20, "context": "To solve this problem, Zhou et al. [2014] proposed the OptMAI algorithm and established its sample complexity \u0398 ( n 2 ( 1 + ln \u03b4 \u22121 K )) , which is shown to be asymptotically optimal. However, the algorithm and the corresponding sample complexity in Zhou et al. [2014] are non-adaptive to the underlying instance.", "startOffset": 23, "endOffset": 269}, {"referenceID": 4, "context": "Chen et al. [2014] developed the CLUCB-PAC algorithm and established an instance-dependent sample complexity for a more general class of problems, including the -top-K arm identification problem as one of the key examples.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Chen et al. [2014] developed the CLUCB-PAC algorithm and established an instance-dependent sample complexity for a more general class of problems, including the -top-K arm identification problem as one of the key examples. When applying the CLUCB-PAC algorithm to identify -top-K arms, the sample complexity becomes O((logH ) +log \u03b4\u22121)H(0, ) where H ) = \u2211n i=1 min{(\u2206i) \u22122, \u22122}, \u2206i = \u03b8i\u2212 \u03b8K+1 for i \u2264 K, \u2206i = \u03b8K \u2212 \u03b8i for i > K. The reason why we adopt the notation H ) will be clear from Section 1.1. However, this bound may be improved for the following two reasons. First, intuitively, the hardness parameter H ) is the total number of necessary pulls needed for each arm to identify whether it is among the top-K arms or the rest so that the algorithm can decide whether to accept or reject the arm (when the arm\u2019s mean is -close to the boundary between the top-K arms and the rest arms, it can be either selected or rejected). However, in many cases, even if an arm\u2019s mean is -far from the boundary, we may still be vague about the comparison between its mean and the boundary, i.e. either selecting or rejecting the arm satisfies the aggregate regret bound. This may lead to fewer number of pulls and a smaller hardness parameter for the same instance. Second, the worst-case sample complexity for CLUCB-PAC becomes O((logn + log \u22121 + log \u03b4\u22121)n \u22122). When \u03b4 is a constant, this bound is logn times more than the best non-adaptive algorithm in Zhou et al. [2014].", "startOffset": 0, "endOffset": 1466}, {"referenceID": 4, "context": "Note that this bound has a similar form as the one in Chen et al. [2014], but as mentioned above, we have an \u221a", "startOffset": 54, "endOffset": 73}, {"referenceID": 24, "context": "1), the worst-case sample complexity of ImprovedTopK matches the best instance-independent shown in Zhou et al. [2014] up to an extra log( \u22121) factor (for constant \u03b4).", "startOffset": 100, "endOffset": 119}, {"referenceID": 2, "context": ", Bubeck et al. [2013]), we first define the gap of the i-th arm", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", Bubeck et al. [2013], Karnin et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", Bubeck et al. [2013], Karnin et al. [2013]) is H\u0303 , \u2211n i=1 \u2206 \u22122 i .", "startOffset": 2, "endOffset": 45}, {"referenceID": 23, "context": ", linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 15, "context": ", 2014], batch arm pulls [Jun et al., 2016].", "startOffset": 25, "endOffset": 43}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]).", "startOffset": 37, "endOffset": 463}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]).", "startOffset": 37, "endOffset": 629}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al.", "startOffset": 37, "endOffset": 714}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) .", "startOffset": 37, "endOffset": 774}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122).", "startOffset": 37, "endOffset": 1026}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed.", "startOffset": 37, "endOffset": 1285}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al.", "startOffset": 37, "endOffset": 1657}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings.", "startOffset": 37, "endOffset": 1708}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al.", "startOffset": 37, "endOffset": 1784}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time.", "startOffset": 37, "endOffset": 1845}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.", "startOffset": 37, "endOffset": 1967}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret.", "startOffset": 37, "endOffset": 2284}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper.", "startOffset": 37, "endOffset": 2595}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm.", "startOffset": 37, "endOffset": 2706}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K.", "startOffset": 37, "endOffset": 2883}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al.", "startOffset": 37, "endOffset": 3220}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints.", "startOffset": 37, "endOffset": 3255}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al.", "startOffset": 37, "endOffset": 3334}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.", "startOffset": 37, "endOffset": 3359}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.", "startOffset": 37, "endOffset": 3924}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.2 in Appendix of their paper). More specifically, Chen et al. [2014] proposed CLUCB-PAC algorithms that finds -top-K arms with probability at least (1\u2212 \u03b4) using O (( log \u03b4\u22121 + logH ) ) H ) ) pulls.", "startOffset": 37, "endOffset": 4008}, {"referenceID": 1, "context": ", 2002, Mannor and Tsitsiklis, 2004, Audibert et al., 2010, Gabillon et al., 2011, 2012, Karnin et al., 2013, Jamieson et al., 2014, Kaufmann et al., 2016, Russo, 2016, Chen et al., 2016b]. More specifically, in the special case when K = 1, our problem reduces to identifying an -best arm, i.e. an arm whose expected reward is different from the best arm by an additive error of at most , with probability at least (1\u2212\u03b4). For this problem, Even-Dar et al. [2006] showed an algorithm with an instanceindependent sample complexity O ( n 2 log \u03b4\u22121 ) (and this was proved to be asymptotically optimal by Mannor and Tsitsiklis [2004]). An instance-dependent algorithm for this problem was given by Bubeck et al. [2013] and an improved algorithm was given by Karnin et al. [2013] with an instance-dependent sample complexity of O (\u2211n i=2 max{\u2206i, } \u22122(log \u03b4\u22121 + log log max{\u2206i, }\u22121) ) . In the worst case, this bound becomes O ( n 2 (log \u03b4\u22121 + log log \u22121) ) , almost matching the instance-independent bound in Even-Dar et al. [2006]. When K = 1, we have t( ,K) = 0 and thus H ) = H ) = \u0398 (\u2211n i=2 max{\u2206i, } \u22122). Therefore, the sample complexity in our Theorem 2 becomes O((log \u22121 + log \u03b4\u22121)H) = O ( n 2 (log \u22121 + log \u03b4\u22121) ) in the worst-case, almost matching the bound by Karnin et al. [2013]. For the problem of identifying top-K arms with K > 1, different notions of -optimal solution have been proposed. One popular metric is the misidentification probability (MisProb), i.e. Pr(T 6= {1, . . . ,K}). In the PAC setting (i.e. controlling MisProb less than with probability at least 1 \u2212 \u03b4), many algorithms have been developed recently, e.g., Bubeck et al. [2013] in the fixed budget setting and Chen et al. [2014] for both fixed confidence and fixed budget settings. Gabillon et al. [2016] further improved the sample complexity in Chen et al. [2014]; however the current implementations of their algorithm have an exponential running time. As argued in Zhou et al. [2014], the MisProb requires to identify the exact top-K arms, which might be too stringent for some applications (e.g., crowdsourcing). The MisProb requires a certain gap between \u03b8K and \u03b8K+1 to identify the top-K arms, and this requirement is not unnecessary when using the aggregate regret. As shown in Zhou et al. [2014], when the gap of any consecutive pair between \u03b8i and \u03b8i+1 among the first 2K arms is o(1/n), the sample complexity has to be huge (\u03c9(n)) to make the MisProb less than , while any K arms among the first 2K form a desirably set of -top-K arms in terms of aggregate regret. Therefore, we follow Zhou et al. [2014] and adopt the aggregate regret to define the approximate solution in this paper. Kalyanakrishnan et al. [2012] proposed the so-called Explore-K metric, which requires for each arm i in the selected set T to satisfy \u03b8i \u2265 \u03b8K \u2212 , where \u03b8K is the mean of the K-th best arm. Cao et al. [2015] proposed a more restrictive notion of optimality\u2014Elementwise- -Optimal, which requires the mean reward of the i-th best arm in the selected set T be at least \u03b8i \u2212 for 1 \u2264 i \u2264 K. It is clear that the Elementwise- -Optimal is a stronger guarantee than our -top-K in regret, while the latter is stronger than Explore-K. Chen et al. [2016a] further extended Cao et al. [2015] to pure exploration problems under matroid constraints. Audibert et al. [2010] and Bubeck et al. [2013] considered expected aggregate regret (i.e. 1 K (\u2211K i=1 \u03b8i \u2212E (\u2211 i\u2208T \u03b8i )) , where the expectation is taken over the randomness of the algorithm. Note that this notion of expected aggregate regret is a weaker objective than the aggregate regret. Moreover, there are some other recent works studying the problem of best-arm identification in different setups, e.g., linear contextual bandit [Soare et al., 2014], batch arm pulls [Jun et al., 2016]. For our -top-K arm problem, the state-of-the-art instance-dependent sample complexity was given by Chen et al. [2014] (see Section B.2 in Appendix of their paper). More specifically, Chen et al. [2014] proposed CLUCB-PAC algorithms that finds -top-K arms with probability at least (1\u2212 \u03b4) using O (( log \u03b4\u22121 + logH ) ) H ) ) pulls. Since we always have H ) \u2265 H ) \u2265 \u03a9(n) and H ) \u2265 (\u03a8 t) \u22122, our Theorem 1 is not worse than the bound in Chen et al. [2014]. Indeed, in many common settings, H ) can be much smaller than H ) so that Theorem 1 (and therefore Theorem 2) requires", "startOffset": 37, "endOffset": 4259}, {"referenceID": 16, "context": ", best-arm identification in Bayesian setup Russo [2016]).", "startOffset": 44, "endOffset": 57}, {"referenceID": 3, "context": "In crowdsourcing applications, Chen et al. [2015] and Abbasi-Yadkori et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 0, "context": "[2015] and Abbasi-Yadkori et al. [2015] also made this assumption for modeling workers\u2019 accuracy, which correspond to the expected rewards.", "startOffset": 11, "endOffset": 40}, {"referenceID": 4, "context": "The following lemma upper-bounds H ) for O(1)-spread arms, and shows the improvement of our algorithms compared to Chen et al. [2014] on O(1)-spread arms.", "startOffset": 115, "endOffset": 134}, {"referenceID": 2, "context": "Bubeck et al. [2013]. The algorithm goes by rounds for r = 1, 2, 3, .", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Lemma 10 [Zhou et al., 2014] For a set of arms S = {\u03b81 \u2265 .", "startOffset": 9, "endOffset": 28}, {"referenceID": 10, "context": "We will need the following anti-concentration result which is an easy consequence of Feller Feller [1943] (cf.", "startOffset": 85, "endOffset": 106}, {"referenceID": 10, "context": "We will need the following anti-concentration result which is an easy consequence of Feller Feller [1943] (cf. Matousek and Vondr\u00e1k. [2008]).", "startOffset": 85, "endOffset": 140}, {"referenceID": 20, "context": "Fact 1 (Matousek and Vondr\u00e1k. [2008]) Let Y be a sum of independent random variables, each attaining values in [0, 1], and let \u03c3 = \u221a Var[Y ] \u2265 200.", "startOffset": 8, "endOffset": 37}, {"referenceID": 18, "context": "We compare our Algorithm 1 (AdaptiveTopK ) with two state-of-the-art methods \u2013 OptMAI in Zhou et al. [2014] and CLUCB-PAC in Chen et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 3, "context": "[2014] and CLUCB-PAC in Chen et al. [2014]. The comparison between OptMAI /CLUCB-PAC and previous methods (e.", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al.", "startOffset": 17, "endOffset": 72}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al. [2014] and Chen et al.", "startOffset": 17, "endOffset": 126}, {"referenceID": 2, "context": ", the methods in Bubeck et al. [2013] and Kalyanakrishnan et al. [2012]) have already been demonstrated in Zhou et al. [2014] and Chen et al. [2014], and thus omitted here for the", "startOffset": 17, "endOffset": 149}, {"referenceID": 22, "context": "\u2022 Rte: We generate \u03b8 from a real recognizing textual entailment (RTE) dataset Snow et al. [2008]. There are n = 164 workers and we set each \u03b8i be the true labeling accuracy of the i-th worker.", "startOffset": 78, "endOffset": 97}], "year": 2017, "abstractText": "We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least 1 \u2212 \u03b4, identifies a set of K arms with the aggregate regret at most . The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et al. [2014] , which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et al. [2014] that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a log( \u22121) factor in the worst case. We also prove a lower bound result showing that the extra log( \u22121) is necessary for instance-dependent algorithms using the introduced hardness parameter.", "creator": "LaTeX with hyperref package"}}}