{"id": "1706.01322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Deep learning evaluation using deep linguistic processing", "abstract": "We discuss problems with the standard assessment approaches for tasks such as visual answers to questions and argue that artificial data can be used to supplement current practice. We show that we are able to use existing \"deep\" linguistic processing technologies to create sophisticated abstract datasets that allow us to examine in detail the language comprehension capabilities of deep learning multimodal models.", "histories": [["v1", "Mon, 5 Jun 2017 13:53:56 GMT  (71kb,D)", "http://arxiv.org/abs/1706.01322v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["alexander kuhnle", "ann copestake"], "accepted": false, "id": "1706.01322"}, "pdf": {"name": "1706.01322.pdf", "metadata": {"source": "CRF", "title": "Deep learning evaluation using deep linguistic processing", "authors": ["Alexander Kuhnle", "Ann Copestake"], "emails": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction & related work", "text": "In recent years, deep neural networks (DNNs) have established a new level of performance for many tasks in natural language processing (NLP), speech, computer vision and artificial intelligence (AI). Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons. Most importantly, it acts as a prototypical problem presentation, abstracted from its noisy and intertwined real-world appearance.\nHowever, with the exception of spoken dialogue systems (e.g., Scheffler and Young (2001)), artificial data is relatively little used in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al., 2015). Here we focus on the problem of visually grounded language understanding in the context of the recently popular task of visual question answering (VQA). In principle, this task is particularly interesting from a semantic perspective, since it combines general language understanding, reference resolution and grounded language reasoning in a simple and clear task. However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities.\nTo address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with colored objects, and are introduced with the motivation to provide a challenging and clear evaluation for VQA systems. Johnson et al. (2017a) and Kuhnle and Copestake (2017) investigated popular VQA systems on their dataset, and demonstrate how artificial data provides us with detailed insights previously not possible. Despite its simplicity, they uncover fundamental shortcomings of current VQA models. Moreover, such datasets have been of great importance for the development of new VQA models based on reinforcement learning (Hu et al., 2017; Johnson et al., 2017b).\nOur aims in this paper are threefold. First, we provide a brief but systematic review of the problems surrounding current standard evaluation practices in deep learning. Secondly, we use this to motivate the potential of artificial data from simulated microworlds to evaluate DNNs, particularly for (visually grounded) language understanding. Thirdly, we present a specific evaluation methodology based on the use of resources from the DELPH-IN (Deep Linguistic Processing with HPSG) collaboration, and show why compositional semantics from a bidirectional symbolic grammar is particularly suitable for the production of artificial datasets.\nar X\niv :1\n70 6.\n01 32\n2v 1\n[ cs\n.C L\n] 5\nJ un\n2 01\n7"}, {"heading": "2 Problems of real-world datasets for deep learning evaluation", "text": "In the following, we review a variety of issues related to the practice of evaluating DNNs on popular real-world datasets for tasks like VQA."}, {"heading": "2.1 Issues with crowd-sourced data", "text": "The fact that DNNs require immense amounts of data for successful training led to the practice of adopting online data, such as the Flickr photo sharing platform, and leveraging crowd-sourcing, usually via Amazon Mechanical Turk (AMT). For instance, MS COCO (Lin et al., 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. There are, however, various problems related to this practice.\nData obtained this way tends to be comparatively simple in terms of syntax and compositional semantics, despite exhibiting a degree of lexical complexity due to its real-world breadth. Moreover, repurposed photos do not \u2013 and were never intended to \u2013 reflect the visual complexity of every-day scenarios (Pinto et al., 2008). Humans given the task of captioning such images will mostly produce descriptions which are syntactically simple. The way that workers on crowd-sourcing platforms are paid gives them an incentive to come up with captions quickly, and hence further increases the tendency to simplicity. Note also that, while this is a form of real-world data, it has very little relationship to the way that a human language learner perceives the world. For instance, the photo/question pairs are presented to a VQA system randomly and with no possibility of detailed interaction with a particular scene.\nNatural language follows Zipf\u2019s law for many aspects (sentence length, syntactic complexity, word usage, etc), and consequently has an inbuilt simplicity bias when considered in terms of probability mass. The contents of image datasets based on photos also have a Zipfian distribution, but with biases which relate to what people choose to photograph rather than to what they see. Animal images in the VQA dataset are predominantly cats and dogs, sport images mainly baseball and tennis (see Antol et al. (2015) for more statistics). Considering all these biases both in language and vision, the common evaluation measure \u2013 simple accuracy of questions answered correctly \u2013 is not a good reflection of a system\u2019s general ability to understand visually grounded language."}, {"heading": "2.2 The Clever Hans effect", "text": "Crowd-sourced visual questions have other unexpected properties. Goyal et al. (2016) and Mahendru et al. (2017) note how questions rarely talk about objects that are not present in the image, hence an existential question like \u201cDo you see a...?\u201d is often true. Agrawal et al. (2016) also give the example of questions like \u201cWhat covers the ground?\u201d, which can confidently be answered with \u201csnow\u201d because of biases in common real-world scenes, or, more precisely, biases in the photographs of real-world scenes. Such biases help to explain why some text-only systems turn out to perform well on visual question answering when evaluated on the VQA dataset.\nSturm (2014) compared such unexpected cues when evaluation machine learning systems to the story of \u201cClever Hans\u201d, a horse exhibited in the early 20th century which was claimed to understand German and have extensive arithmetical and reasoning abilities. Hans was eventually found to be picking up on very subtle cues which were given completely unconsciously by his owner and which were not noticed by ordinary observers. Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets?\nA more fundamental form of this effect is illustrated by recent investigations in image recognition. Szegedy et al. (2014) and Nguyen et al. (2015) have shown surprisingly odd system behavior when\nconfronted with either only minimally modified images or almost random noise. This behavior seems due to the specific interplay of a few parameters which dominate the model\u2019s decision, and have led to an entire research subfield on adversarial instances in vision. Such investigations are not yet as prominent in the NLP community, although see, e.g., Sproat and Jaitly (2016) and Arthur et al. (2016).\nThe ability to work with raw input data and to pick up correlations/biases, which humans cannot always manifest in explicit symbolic rules, is precisely the strength of DNNs as feature extractors. But given the often millions of parameters and large number of unstructured input values, it is difficult to avoid unexpected hidden cues. Real-world data with its enormous \u201csample space\u201d, which is necessarily only sparsely reflected in a dataset, is hence particularly prone to this effect.\nThe immediate problem is that a system trained this way may not generalize appropriately to other situations. The longer-term problem is that, while we do not expect that DNNs will simulate human capabilities in a fine-grained way, there has to be some degree of comparability if they are ever to be capable of justifying or explaining their behavior. The Clever Hans effect refers to situations where we wrongly and prematurely attribute such human-like reasoning mechanisms to trained models, when more careful and systematic investigations would have revealed our misjudgement. We can conclude from this that we need to supplement existing datasets with data where the relationship between text and image is straightforwardly and explicitly available to the experimenter."}, {"heading": "2.3 Deep neural networks are universal approximators", "text": "It has long been known that DNNs are universal approximators, able to fit every (well-behaved) function if appropriately configured. Recent work by Zhang et al. (2017) demonstrated how powerful common network architectures are in approximating mere noise. Furthermore, their experiments indicate that fitting noise is not more difficult than fitting meaningful data for DNNs. The only discriminating effect is that the latter model generalizes to new data, while the former does not.1 The ability of DNNs to fit hidden correlations should consequently not be underestimated. We conclude from this that, ideally, datasets should be big enough to avoid having to repeatedly use instances at all, whether during training or evaluation, particularly in the case of huge, but sparsely covered sample spaces. In this respect, the VQA dataset is too small and complex to provide the means for a clear and detailed evaluation.\nShallower machine learning methods are less prone to uncover hidden correlations for fitting data more efficiently, since their more restrictive structure imposed by the underlying model as well as in the input/output format likely makes it more difficult to incorporate them properly. The optimization is hence sufficiently constrained such that evaluations based on abstract artificial data are less interesting, even trivial and prone to overfitting. The situation seems reversed in the case of DNNs with their huge parameter space, comparatively unrestricted and highly nonlinear optimization on raw input/output data. Here, models do well at extremely difficult tasks just by end-to-end training on enough data points, while more detailed investigations find that they, unexpectedly, struggle even with simple abstract abilities like counting or spatial relations (Jabri et al., 2016). We hence do not share the scepticism about artificial data, particularly language data, being too trivial to be interesting for the evaluation of DNNs."}, {"heading": "2.4 Three guiding principles for deep learning evaluation", "text": "We propose three simple and principled ways of reducing the risk of encountering such problems: \u2022 Avoid training for multiple epochs: do not iterate over a fixed set of instances, since this makes it\npossible for the system to memorize hidden artifacts in the data. \u2022 Instead of keeping training and test data distributions similar, focus on the true, compositional\ngeneralization abilities required by dissimilar distributions.2\n\u2022 Do at least some experiments with clean data, which reduces the likelihood of hidden biases or correlations compared to more \u201crealistic\u201d and complex data. For instance, the relationship between image and text should be explicitly controlled in multimodal data for tasks like VQA.\n1Or rather, the structure of the data for which it would generalize is not obvious. 2A more asymmetric dataset represents a harder, but hence potentially more interesting task."}, {"heading": "3 Automatic generation of artificial data using deep linguistic systems", "text": "In the following, we describe our approach for automatic generation of artificial VQA data using existing deep linguistic processing technology. We argue that a compositional semantic approach using a bidirectional grammar gives us precisely the sort of data required by the principles in section 2.4. We propose this approach as a complementary evaluation step, since it is not intended to replace real-world evaluation, but instead aims to cover aspects of evaluation which existing datasets cannot provide."}, {"heading": "3.1 Abstract microworlds", "text": "The generation process we use is based on randomly sampled abstract world models, i.e. values which specify the microworld, entities and all their attributes. In the case of our ShapeWorld framework (Kuhnle and Copestake, 2017) \u2014 see figure 1 for an example \u2014 these include the number of entities, their shape and color, position, rotation, shade, etc. Such a world model can be visualized straightforwardly.\nIn this context, datasets are generators which can create an unlimited amount of data instances, hence making multiple iterations over a fixed set of training instances obsolete. Importantly, different datasets constrain this general sampling process in different ways by, for instance, restricting the number of objects, the attribute values available, the global positioning of entities, and more. This addresses the point of specifying different data distributions for training and testing. Moreover, it makes it possible to partition evaluation instances as desired, which facilitates the detailed investigation of a system\u2019s behavior for specific instance types, and consequently the discovery of systematic shortcomings."}, {"heading": "3.2 Controlled and syntactically rich language generation", "text": "Of the recent abstract datasets mentioned in the introduction, Suhr et al. (2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)). For our system we use a variant of MRS, Dependency MRS (DMRS, Copestake (2009), Copestake et al. (2016)), and generate natural language sentences from abstract DMRS graphs using Packard\u2019s parser-generator ACE4.\nWe have found that DMRS graphs can easily be enriched with appropriate ShapeWorld semantics to be evaluated on a given world model. This means that the internals of the language system are essentially using a form of model-theoretic semantics. However, the external presentation of our task is still natural, i.e. only consisting of image and natural language. A compositional representation like DMRS further gives us the ability to produce an infinite number of utterances of arbitrary syntactic complexity.\nFigure 2 shows an example of a non-trivial caption with corresponding DMRS graph and logical representation over a world model. Both the abstractness and compositionality of the semantic representation are essential to allow us to scale beyond toy examples. The abstract scenario of ShapeWorld puts an emphasis on experiments with closed-class vocabulary and syntax, as compared to the open-\n3The ability to parse generated utterances is also potentially important, for instance in investigating ambiguity effects. 4http://sweaglesw.org/linguistics/ace/"}, {"heading": "A pentagon is above a green ellipse, and no green shape is an ellipse.", "text": "class dominated real-world datasets. However, the same approach could be extended to more complex domains, like the clip-art setting of Zitnick et al. (2016).\nIn the future, we plan to implement two interesting extensions for the ShapeWorld framework: On the one hand, paraphrase rules can be expressed on grammar-level and integrated into the generation process as post-processing step for increased linguistic variety. On the other hand, bidirectional (D)MRSbased grammars for other languages, such as the JACY grammar for Japanese (Siegel et al., 2016), could be used simply by translating the internal mapping of atomic DMRS components to corresponding ShapeWorld-semantic elements."}, {"heading": "4 Conclusion: Why use generated artificial data?", "text": "Modularity Not only can the caption components be used in a compositional way, but also different constraint sets for the world model sampling process can be re-combined with different captioning modules. For instance, quantification and spatial relation statements both can use a world generator creating worlds containing multiple entities. All components can further be combined in mixer modules, for example a combined quantification and spatial relation captioner module.\nFlexibility & reusability Real-world or human-created data essentially has to be obtained again for every change/update (Goyal et al., 2016)5. In contrast to that, modularity and detailed configurability makes our approach easily reusable for a wide range of potentially unforeseen changes in evaluation focus (or more general usage shifts).\nChallenging data The interplay of abstract world model and semantic language representation enables us to generate captions requiring non-trivial multimodal reasoning. In fact, the resulting captions can be more complex than the sort of captions we could plausibly obtain from humans, and do not suffer from a Zipfian tendency to simplicity on average (although we could generate based on Zipfian distributions if that were desirable).\nAvoid Clever Hans effect The simple, abstract domain and the controlled generation process based on randomly sampling microworlds makes such data comparatively unbiased and greatly reduces the possibility of hidden complex correlations. We can be confident that we cover the data space both relatively uniformly and much more exhaustively than this is the case in real-world datasets.\nRich evaluation Ultimately, our goal in providing datasets is to enable detailed evaluations of DNNs. By creating atomic test datasets specifically evaluating instance types individually (e.g., counting, spatial relations, or even more fine-grained), we can unit-test a DNN for specific subtasks. We believe that such a modular approach is a better way to establish trust in the understanding abilities of DNNs than a monolithic dataset and a single accuracy number to assess performance.\n5Another interesting approach is to automatically process data afterwards (Hodosh and Hockenmaier, 2016; Shekhar et al., 2017), resulting in a form of \u201clightly\u201d artificial data."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Y. Adi", "E. Kermany", "Y. Belinkov", "O. Lavi", "Y. Goldberg"], "venue": "Proceedings of the International Conference on Learning Representations 2017 (ICLR 2017).", "citeRegEx": "Adi et al\\.,? 2017", "shortCiteRegEx": "Adi et al\\.", "year": 2017}, {"title": "Analyzing the behavior of visual question answering models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, pp. 1955\u20131960.", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, ICCV 2015.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["P. Arthur", "G. Neubig", "S. Nakamura"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Austin, Texas, USA.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research 47(1), 253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "OpenAI Gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": null, "citeRegEx": "Brockman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brockman et al\\.", "year": 2016}, {"title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go", "author": ["A. Copestake"], "venue": "Proceedings of the 12th Meeting of the European Chapter of the Association for Computational Linguistics, Athens, Greece, pp. 1\u20139.", "citeRegEx": "Copestake,? 2009", "shortCiteRegEx": "Copestake", "year": 2009}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["A. Copestake", "G. Emerson", "M.W. Goodman", "M. Horvat", "A. Kuhnle", "E. Muszy\u0144ska"], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC-16), Portoro\u017e, Slovenia, pp. 1240\u20131247. European Language Resources Association (ELRA).", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["A. Copestake", "D. Flickinger", "C. Pollard", "I.A. Sag"], "venue": "Research on Language and Computation 3(4), 281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "On building a more efficient grammar by exploiting types", "author": ["D. Flickinger"], "venue": "Natural Language Engineering 6(1), 15\u201328.", "citeRegEx": "Flickinger,? 2000", "shortCiteRegEx": "Flickinger", "year": 2000}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CoRR abs/1612.00837.", "citeRegEx": "Goyal et al\\.,? 2016", "shortCiteRegEx": "Goyal et al\\.", "year": 2016}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the 5th Workshop on Vision and Language, Berlin, Germany.", "citeRegEx": "Hodosh and Hockenmaier,? 2016", "shortCiteRegEx": "Hodosh and Hockenmaier", "year": 2016}, {"title": "Learning to reason: End-to-end module networks for visual question answering", "author": ["R. Hu", "J. Andreas", "M. Rohrbach", "T. Darrell", "K. Saenko"], "venue": "ArXiv e-prints.", "citeRegEx": "Hu et al\\.,? 2017", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Revisiting Visual Question Answering Baselines, pp", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "727\u2013739. Cham: Springer International Publishing.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "CVPR.", "citeRegEx": "Johnson et al\\.,? 2017a", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick"], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Johnson et al\\.,? 2017b", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["D. Kiela", "L. Bulat", "A.L. Vero", "S. Clark"], "venue": "CoRR abs/1610.07432.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Shapeworld - A new test methodology for multimodal language understanding", "author": ["A. Kuhnle", "A. Copestake"], "venue": "ArXiv e-prints.", "citeRegEx": "Kuhnle and Copestake,? 2017", "shortCiteRegEx": "Kuhnle and Copestake", "year": 2017}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin", "T.-Y.", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision (ECCV), Z\u00fcrich.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "The promise of premise: Harnessing question premises in visual question answering", "author": ["A. Mahendru", "V. Prabhu", "A. Mohapatra", "D. Batra", "S. Lee"], "venue": "ArXiv e-prints.", "citeRegEx": "Mahendru et al\\.,? 2017", "shortCiteRegEx": "Mahendru et al\\.", "year": 2017}, {"title": "A roadmap towards machine intelligence", "author": ["T. Mikolov", "A. Joulin", "M. Baroni"], "venue": "CoRR abs/1511.08130.", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLOS Computational Biology", "citeRegEx": "Pinto et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2008}, {"title": "Corpus-based dialogue simulation for automatic strategy learning and evaluation", "author": ["K. Scheffler", "S. Young"], "venue": "Proceedings of the NAACL Workshop on Adaptation in Dialogue Systems, pp. 64\u201370.", "citeRegEx": "Scheffler and Young,? 2001", "shortCiteRegEx": "Scheffler and Young", "year": 2001}, {"title": "FOIL it! Find one mismatch between image and language caption", "author": ["R. Shekhar", "S. Pezzelle", "Y. Klimovich", "A. Herbelot", "M. Nabi", "E. Sangineto", "R. Bernardi"], "venue": "ArXiv e-prints.", "citeRegEx": "Shekhar et al\\.,? 2017", "shortCiteRegEx": "Shekhar et al\\.", "year": 2017}, {"title": "Jacy: An Implemented Grammar of Japanese", "author": ["M. Siegel", "E.M. Bender", "F. Bond"], "venue": "CSLI Studies in Computational Linguistics. Stanford: CSLI Publications.", "citeRegEx": "Siegel et al\\.,? 2016", "shortCiteRegEx": "Siegel et al\\.", "year": 2016}, {"title": "RNN Approaches to Text Normalization: A Challenge", "author": ["R. Sproat", "N. Jaitly"], "venue": "CoRR abs/1611.00068.", "citeRegEx": "Sproat and Jaitly,? 2016", "shortCiteRegEx": "Sproat and Jaitly", "year": 2016}, {"title": "A simple method to determine if a music information retrieval system is a \u201chorse", "author": ["B.L. Sturm"], "venue": "IEEE Transactions on Multimedia 16(6), 1636\u20131644.", "citeRegEx": "Sturm,? 2014", "shortCiteRegEx": "Sturm", "year": 2014}, {"title": "A corpus of natural language for visual reasoning", "author": ["A. Suhr", "M. Lewis", "J. Yeh", "Y. Artzi"], "venue": "55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada. Association for Computational Linguistics.", "citeRegEx": "Suhr et al\\.,? 2017", "shortCiteRegEx": "Suhr et al\\.", "year": 2017}, {"title": "MazeBase: A sandbox for learning from games", "author": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"], "venue": "CoRR abs/1511.07401.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "International Conference on Learning Representations 2017 (ICLR 2017).", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38(4), 627\u2013638.", "citeRegEx": "Zitnick et al\\.,? 2016", "shortCiteRegEx": "Zitnick et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016).", "startOffset": 105, "endOffset": 152}, {"referenceID": 6, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016).", "startOffset": 105, "endOffset": 152}, {"referenceID": 30, "context": "There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al.", "startOffset": 87, "endOffset": 112}, {"referenceID": 32, "context": "(2015), or the bAbI tasks (Weston et al., 2015).", "startOffset": 26, "endOffset": 47}, {"referenceID": 11, "context": "However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities.", "startOffset": 21, "endOffset": 63}, {"referenceID": 1, "context": "However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities.", "startOffset": 21, "endOffset": 63}, {"referenceID": 2, "context": "To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 15, "context": ", 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 18, "context": "(2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017).", "startOffset": 37, "endOffset": 65}, {"referenceID": 13, "context": "Moreover, such datasets have been of great importance for the development of new VQA models based on reinforcement learning (Hu et al., 2017; Johnson et al., 2017b).", "startOffset": 124, "endOffset": 164}, {"referenceID": 16, "context": "Moreover, such datasets have been of great importance for the development of new VQA models based on reinforcement learning (Hu et al., 2017; Johnson et al., 2017b).", "startOffset": 124, "endOffset": 164}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons.", "startOffset": 106, "endOffset": 189}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons. Most importantly, it acts as a prototypical problem presentation, abstracted from its noisy and intertwined real-world appearance. However, with the exception of spoken dialogue systems (e.g., Scheffler and Young (2001)), artificial data is relatively little used in NLP.", "startOffset": 106, "endOffset": 459}, {"referenceID": 3, "context": "Simultaneously, we observe a move towards simulated environments and artificial data, particularly in AI (Bellemare et al., 2013; Brockman et al., 2016). As outlined by Kiela et al. (2016), simulated data is appealing for various reasons. Most importantly, it acts as a prototypical problem presentation, abstracted from its noisy and intertwined real-world appearance. However, with the exception of spoken dialogue systems (e.g., Scheffler and Young (2001)), artificial data is relatively little used in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al.", "startOffset": 106, "endOffset": 682}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017).", "startOffset": 8, "endOffset": 427}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with colored objects, and are introduced with the motivation to provide a challenging and clear evaluation for VQA systems. Johnson et al. (2017a) and Kuhnle and Copestake (2017) investigated popular VQA systems on their dataset, and demonstrate how artificial data provides us with detailed insights previously not possible.", "startOffset": 8, "endOffset": 685}, {"referenceID": 1, "context": ", 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artificial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with colored objects, and are introduced with the motivation to provide a challenging and clear evaluation for VQA systems. Johnson et al. (2017a) and Kuhnle and Copestake (2017) investigated popular VQA systems on their dataset, and demonstrate how artificial data provides us with detailed insights previously not possible.", "startOffset": 8, "endOffset": 717}, {"referenceID": 19, "context": "For instance, MS COCO (Lin et al., 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 23, "context": "Moreover, repurposed photos do not \u2013 and were never intended to \u2013 reflect the visual complexity of every-day scenarios (Pinto et al., 2008).", "startOffset": 119, "endOffset": 139}, {"referenceID": 3, "context": ", 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO.", "startOffset": 165, "endOffset": 185}, {"referenceID": 3, "context": ", 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. There are, however, various problems related to this practice. Data obtained this way tends to be comparatively simple in terms of syntax and compositional semantics, despite exhibiting a degree of lexical complexity due to its real-world breadth. Moreover, repurposed photos do not \u2013 and were never intended to \u2013 reflect the visual complexity of every-day scenarios (Pinto et al., 2008). Humans given the task of captioning such images will mostly produce descriptions which are syntactically simple. The way that workers on crowd-sourcing platforms are paid gives them an incentive to come up with captions quickly, and hence further increases the tendency to simplicity. Note also that, while this is a form of real-world data, it has very little relationship to the way that a human language learner perceives the world. For instance, the photo/question pairs are presented to a VQA system randomly and with no possibility of detailed interaction with a particular scene. Natural language follows Zipf\u2019s law for many aspects (sentence length, syntactic complexity, word usage, etc), and consequently has an inbuilt simplicity bias when considered in terms of probability mass. The contents of image datasets based on photos also have a Zipfian distribution, but with biases which relate to what people choose to photograph rather than to what they see. Animal images in the VQA dataset are predominantly cats and dogs, sport images mainly baseball and tennis (see Antol et al. (2015) for more statistics).", "startOffset": 165, "endOffset": 1694}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al.", "startOffset": 249, "endOffset": 267}, {"referenceID": 14, "context": ", 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition.", "startOffset": 150, "endOffset": 170}, {"referenceID": 9, "context": "Goyal et al. (2016) and Mahendru et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "Goyal et al. (2016) and Mahendru et al. (2017) note how questions rarely talk about objects that are not present in the image, hence an existential question like \u201cDo you see a.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Agrawal et al. (2016) also give the example of questions like \u201cWhat covers the ground?\u201d, which can confidently be answered with \u201csnow\u201d because of biases in common real-world scenes, or, more precisely, biases in the photographs of real-world scenes.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2016) also give the example of questions like \u201cWhat covers the ground?\u201d, which can confidently be answered with \u201csnow\u201d because of biases in common real-world scenes, or, more precisely, biases in the photographs of real-world scenes. Such biases help to explain why some text-only systems turn out to perform well on visual question answering when evaluated on the VQA dataset. Sturm (2014) compared such unexpected cues when evaluation machine learning systems to the story of \u201cClever Hans\u201d, a horse exhibited in the early 20th century which was claimed to understand German and have extensive arithmetical and reasoning abilities.", "startOffset": 0, "endOffset": 407}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition. Szegedy et al. (2014) and Nguyen et al.", "startOffset": 250, "endOffset": 671}, {"referenceID": 0, "context": "Some of the recent findings for DNNs, particularly in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an instance of the Clever Hans effect, and due to unnoticed biases in the datasets? A more fundamental form of this effect is illustrated by recent investigations in image recognition. Szegedy et al. (2014) and Nguyen et al. (2015) have shown surprisingly odd system behavior when", "startOffset": 250, "endOffset": 696}, {"referenceID": 26, "context": ", Sproat and Jaitly (2016) and Arthur et al.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": ", Sproat and Jaitly (2016) and Arthur et al. (2016). The ability to work with raw input data and to pick up correlations/biases, which humans cannot always manifest in explicit symbolic rules, is precisely the strength of DNNs as feature extractors.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "Here, models do well at extremely difficult tasks just by end-to-end training on enough data points, while more detailed investigations find that they, unexpectedly, struggle even with simple abstract abilities like counting or spatial relations (Jabri et al., 2016).", "startOffset": 246, "endOffset": 266}, {"referenceID": 32, "context": "Recent work by Zhang et al. (2017) demonstrated how powerful common network architectures are in approximating mere noise.", "startOffset": 15, "endOffset": 35}, {"referenceID": 18, "context": "In the case of our ShapeWorld framework (Kuhnle and Copestake, 2017) \u2014 see figure 1 for an example \u2014 these include the number of entities, their shape and color, position, rotation, shade, etc.", "startOffset": 40, "endOffset": 68}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 15, "context": ", 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds.", "startOffset": 52, "endOffset": 75}, {"referenceID": 10, "context": "In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000).", "startOffset": 116, "endOffset": 134}, {"referenceID": 22, "context": "Of the recent abstract datasets mentioned in the introduction, Suhr et al. (2017) use human-written captions, the SHAPES dataset (Andreas et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)).", "startOffset": 55, "endOffset": 673}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)). For our system we use a variant of MRS, Dependency MRS (DMRS, Copestake (2009), Copestake et al.", "startOffset": 55, "endOffset": 754}, {"referenceID": 2, "context": "(2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both specifically designed for their microworlds. For the ShapeWorld framework, we decided that we will use technology made available by the DELPH-IN consortium. In particular, we wanted to make use of the broad-coverage, bidirectional3, high-precision English Resource Grammar (Flickinger, 2000). This and other DELPH-IN grammars, available for a range of languages, share the compositional semantic framework of Minimal Recursion Semantics (MRS, Copestake et al. (2005)). For our system we use a variant of MRS, Dependency MRS (DMRS, Copestake (2009), Copestake et al. (2016)), and generate natural language sentences from abstract DMRS graphs using Packard\u2019s parser-generator ACE4.", "startOffset": 55, "endOffset": 779}, {"referenceID": 26, "context": "On the other hand, bidirectional (D)MRSbased grammars for other languages, such as the JACY grammar for Japanese (Siegel et al., 2016), could be used simply by translating the internal mapping of atomic DMRS components to corresponding ShapeWorld-semantic elements.", "startOffset": 113, "endOffset": 134}, {"referenceID": 33, "context": "However, the same approach could be extended to more complex domains, like the clip-art setting of Zitnick et al. (2016). In the future, we plan to implement two interesting extensions for the ShapeWorld framework: On the one hand, paraphrase rules can be expressed on grammar-level and integrated into the generation process as post-processing step for increased linguistic variety.", "startOffset": 99, "endOffset": 121}, {"referenceID": 11, "context": "Flexibility & reusability Real-world or human-created data essentially has to be obtained again for every change/update (Goyal et al., 2016)5.", "startOffset": 120, "endOffset": 140}, {"referenceID": 12, "context": "Another interesting approach is to automatically process data afterwards (Hodosh and Hockenmaier, 2016; Shekhar et al., 2017), resulting in a form of \u201clightly\u201d artificial data.", "startOffset": 73, "endOffset": 125}, {"referenceID": 25, "context": "Another interesting approach is to automatically process data afterwards (Hodosh and Hockenmaier, 2016; Shekhar et al., 2017), resulting in a form of \u201clightly\u201d artificial data.", "startOffset": 73, "endOffset": 125}], "year": 2017, "abstractText": "We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing \u2018deep\u2019 linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.", "creator": "LaTeX with hyperref package"}}}