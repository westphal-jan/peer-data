{"id": "1706.01399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "abstract": "Generative Adversarial Networks (GANs) have recently shown great promise in image generation; training GANs in text generation has proved more difficult due to the indistinguishable nature of text generation with relapsing neural networks. Consequently, in the past, either pre-training was used with maximum probability or Convolutionary Networks were used to generate it. In this paper, we show that relapsing neural networks can be trained to regenerate text with GANs from scratch by applying curricula, slowly increasing the length of the generated text and simultaneously training the RNN to generate sequences of varying lengths. We show that this approach significantly improves the quality of generated sequences compared to the Convolutionary Baseline.", "histories": [["v1", "Mon, 5 Jun 2017 16:10:58 GMT  (22kb)", "https://arxiv.org/abs/1706.01399v1", null], ["v2", "Mon, 12 Jun 2017 17:22:19 GMT  (22kb)", "http://arxiv.org/abs/1706.01399v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ofir press", "amir bar", "ben bogin", "jonathan berant", "lior wolf"], "accepted": false, "id": "1706.01399"}, "pdf": {"name": "1706.01399.pdf", "metadata": {"source": "CRF", "title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "authors": ["Ofir Press", "Amir Bar", "Ben Bogin", "Jonathan Berant", "Lior Wolf"], "emails": ["ofir.press@cs.tau.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n01 39\n9v 2\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nGenerative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline. 1"}, {"heading": "1 Introduction", "text": "Generative adversarial networks (Goodfellow et al., 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017). For text generation, training GANs with recurrent neural networks (RNNs) has been more challenging, mostly due to the non-differentiable nature of generating discrete symbols. Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)\n\u2217\nDenotes equal contribution. Author ordering determined by coin flip. 1 Code for our models and evaluation methods is available at\nhttps://github.com/amirbar/rnn.wgan\nor joint training (Lamb et al., 2016; Che et al., 2017) of the generator and discriminator with a supervised maximum-likelihood loss.\nRecently, two initial attempts to generate text using purely generative adversarial training were conducted by Gulrajani et al. (2017) and Hjelm et al. (2017). In these works, a convolutional neural network (CNN) was trained to produce sequences of 32 characters. This CNN architecture is fully differentiable, and the authors demonstrated that it generates text at a reasonable level. However, the generated text was still filled with spelling errors and had little coherence. RNNs are a more natural architecture for language generation, since they condition each generated character on the entire history, and are not constrained to generating a fixed number of characters.\nIn this paper, we extend the setup of Gulrajani et al. (2017) and present a method for generating text with GANs. Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016). We succeed in training the model by using curriculum learning (Elman, 1993; Bengio et al., 2009): At each stage we increase the maximal length of generated sequences, and train over sequences of variable length that are shorter than that maximal length. In addition, we aid the model by feeding it with ground truth characters before generation. We show that these methods vastly improve the quality of generated sequences. Sequences contain substantially more n-grams from a development set compared to those generated by a CNN, and generation generalizes to sequences that are longer than the sequences the model was trained on."}, {"heading": "2 Motivation", "text": "While models trained with a maximum-likelihood objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs. First, using ML suffers from \u201cexposure bias\u201d, that is, at training time the model is exposed to gold data only, but at test time it observes its own predictions, and thus wrong predictions quickly accumulate, resulting in bad text generation.\nSecondly, the ML loss function is very stringent. When training with ML, the model aims to allocate all probability mass to the i-th character of the training set given the previous i\u2212 1 characters, and considers any deviation from the gold sequence as incorrect, although there are many possible sequences given a certain prefix. GANs suffer less from this problem, because the objective is to fool the discriminator, and thus the objective evolves dynamically as training unfolds. While at the beginning the generator might only generate sequences of random letters with spaces, as the discriminator learns to better discriminate, the generator will evolve to generate words and after that it may advance to longer, more coherent sequences of text. This interplay between the discriminator and generator helps incremental learning of text generation."}, {"heading": "3 Preliminaries", "text": "Gulrajani et al. (2017) and Hjelm et al. (2017) trained a purely generative adversarial model (without pre-training) for character-level sentence generation. We briefly review the setup of Gulrajani et al. (2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well. Hjelm et al. (2017) have a similar setup, but employ the Boundary-Seeking GAN objective.\nThe generator G in Gulrajani et al. (2017) is a CNN that transforms a noise vector z \u223c N(0, 1) into a matrix M \u2208 R32\u00d7V , where V is the size of the character vocabulary, and 32 is the length of the generated text. In this matrix the i-th row is a probability distribution over characters that represents a prediction for the i-th output in the character sequence. To decode a sequence, they choose the highest probability character in each row. The discriminator D is another CNN that receives a matrix as input and needs to determine if this ma-\ntrix is the output of the generator G or sampled from the real data (where each row in the matrix now is a one-hot vector). The loss of the Improved WGAN generator is:\nLG = \u2212Ex\u0303\u223cPg [D(x\u0303)],\nand the loss of the discriminator is:\nLD = Ex\u0303\u223cPg [D(x\u0303)]\u2212 Ex\u223cPr [D(x)]\n+ \u03bbEx\u0302\u223cPx\u0302 [(\u2016\u2207x\u0302D(x\u0302)\u20162 \u2212 1) 2],\nWhere Pr is the data distribution and Pg is the generator distribution implicitly defined by x\u0303 = G(z). The last term of the objective controls the complexity of the discriminator function and penalizes functions that have high gradient norm, that is, change too rapidly. Px\u0302 is defined by sampling uniformly along a straight line between a point sampled from the data distribution and a point sampled from the generator distribution.\nA disadvantage of the generators in Gulrajani et al. (2017) and Hjelm et al. (2017) is that they use CNNs for generation, and thus the i-th generated character is not directly conditioned on the entire history of i\u22121 generated characters. This might be a factor in the frequent spelling mistakes and lack of coherence in the output of these models. We now present a model for language generation with GANs that utilizes RNNs, which are state-of-the-art in language generation."}, {"heading": "4 Recurrent Models", "text": "We employ a GRU (Cho et al., 2014) based RNN for our generator and discriminator. The generator is initialized by feeding it with a noise vector z as the hidden state, and an embedded startof-sequence symbol as input. The generator then generates a sequence of distributions over characters, using a softmax layer over the hidden state at each time step.\nBecause we want to have a fully-differentiable generator, the input to the RNN generator at each time step is not the most probable character from the previous time step. Instead we employ a continuous relaxation, and provide at time step i the weighted average representation given by the output distribution of step i \u2212 1. More formally, let \u03b1ci\u22121 be the probability of generating the character c computed at time step i \u2212 1, and let \u03c6(c) be the embedding of the character c, then the input to the\nGRU at time step i is \u2211\nc \u03b1 c i\u22121\u03c6(c). This is fully\ndifferentiable compared to argmax\u03c6(c) \u03b1 c i\u22121. We empirically observe that the RNN quickly learns to output very skewed distributions.\nThe discriminator is another GRU that receives a sequence of character distributions as input, either one-hot vectors (for real data) or softer distributions (for generated data). Character embeddings are computed from the distributions and fed into the GRU. The discriminator then takes the final hidden state and feeds it into a fully connected layer which outputs a single number, representing the score that the discriminator assigns to the input. The models are trained with the aforementioned Improved WGAN objective (Section 3).\nAn advantage of a recurrent generator compared to the convolutional generator of Gulrajani et al. (2017) and Hjelm et al. (2017) is that can output sequences of varying lengths, as we empirically show in Section 5.\nOur baseline model trains the generator and discriminator over sequences of length 32, similar to how CNNs were trained in Gulrajani et al. (2017). We found that training this baseline was difficult and resulted in nonsensical text. We now present three extensions that stabilize the training process.\nCurriculum Learning (CL) In this extension, we start by training on short sequences and then slowly increase sequence length. In the first training stage, the generator G generates sequences of length 1, and the discriminator D receives real and generated sequences of length 1 as input. Then, the generator generates sequences of length 2 and the discriminator receives sequences of length 2. We increase sequence length in this manner until the maximum length of 32 characters.\nVariable Length (VL) Here, we define a maximum length l, and generate during training sequences of every length \u2264 l in every batch. Without curriculum learning, this amounts to training G and D in every batch with sequences of length i, 1 \u2264 i \u2264 32. With curriculum learning, we generate at each step sequences of length i, 1 \u2264 i \u2264 l, and slowly increase l throughout training.\nTeacher Helping (TH) Finally, we propose a procedure where we help the generator learn to generate long sequences by conditioning on shorter ground truth sequences. Recall that in our baseline, the generator generates an entire sequence of characters that are fed as input to the discriminator. Here, when generating sequences of length i, we feed the generator a sequence of i\u22121 characters, sampled from the real data. Then, the generator generates a distribution over characters for the final character, which we concatenate to the real characters and feed as input to the discriminator. The discriminator observes a sequence of length i composed of i \u2212 1 real characters and one character that is either real or generated. This could be viewed as a conditional GAN (Mirza and Osindero, 2014), where the first i \u2212 1 characters are the input and the final character is the output. Note that this extension may suffer from exposure bias, similar to the ML objective, and we plan to address this problem in future work."}, {"heading": "5 Results", "text": "To directly compare to Gulrajani et al. (2017), we follow their setup and train our models on the Billion Word dataset (Chelba et al., 2013). We evaluate by generating 640 sequences from each model and measuring %-IN-TEST-n, that is, the proportion of word n-grams from generated sequences that also appear in a held-out test set. We evaluate these metrics for n \u2208 {1, 2, 3, 4}. Our goal is to measure the extent to which the generator is able to generate real words with local coherence.\nIn contrast to Arjovsky et al. (2017) and Gulrajani et al. (2017), where the generator is trained once for every 10 training iterations of the discriminator, we found that training the generator for 50 iterations every 10 training iterations of the discriminator resulted in superior performance. In addition, instead of using noise vectors sampled from the N(0, 1) distribution as in Gulrajani et al. (2017), we sample noise vectors from the N(0, 10) distribution, since we found this leads to a greater variance in the generated\nsamples when using RNNs.\nIn all our experiments, we used single layer GRUs for both the discriminator and generator. The embedding dimension and hidden state dimension are both of size 512.\nFollowing Gulrajani et al. (2017), we train all our models on sequences whose maximum length is 32 characters. Table 1 shows results of the baseline model of Gulrajani et al. (2017), and Table 2 presents results of our models with various combinations of extensions (Curriculum Learning, Variable Length, and Teacher Helping). Our best model combines all of the extensions and outperforms the baseline by a wide margin on all metrics.\nThe samples show that models that used both the Variable Length and Teacher Helping extensions performed better than those that did not. This is also backed by the empirical evaluation, which shows that 3.8% of the word 4-grams generated by the CL+VL+TH model also appear in the held-out test set. The weak performance of the curriculum learning model without the other extensions shows that curriculum learning by itself does not lead to better performance, and that training on variable lengths and with Teacher Helping\nis important. We note that curriculum learning did not perform well at generating sequences of length 32, but did perform well at generating sequences of shorter lengths earlier in the training process. For example, the model that used only curriculum learning had a %-IN-TEST-1 of 79.9 when it was trained on sequences of length 5. This decreased to 59.7when the model reached sequences of length 10, and continued decreasing until training stopped. This also shows the importance of Variable Length and Teacher Helping.\nFinally, to check the ability of our models to generalize to longer sequences, we generated sequences of length 64with our CL+VL+THmodel, which was trained on sequences of up to 32 characters (Table 3). We then evaluated the generated text, and this evaluation shows that there is a small degradation in performance (Table 2)."}, {"heading": "6 Conclusion", "text": "We show for the first time an RNN trained with a GAN objective that learns to generate natural language from scratch. Moreover, we demonstrate that our model generalizes to sequences longer than the ones seen during training. In future work,\nwe plan to apply these models to tasks such as image captioning and translation, comparing them to models trained with maximum likelihood."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou."], "venue": "arXiv preprint arXiv:1701.07875 .", "citeRegEx": "Arjovsky et al\\.,? 2017", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, pages 41\u201348.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Maximum-likelihood augmented discrete generative adversarial networks", "author": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R. Devon Hjelm", "Wenjie Li", "Yangqiu Song", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1702.07983 .", "citeRegEx": "Che et al\\.,? 2017", "shortCiteRegEx": "Che et al\\.", "year": 2017}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "TomasMikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L Elman."], "venue": "Cognition 48(1):71\u201399.", "citeRegEx": "Elman.,? 1993", "shortCiteRegEx": "Elman.", "year": 1993}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Improved training of wasserstein gans", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville."], "venue": "arXiv preprint arXiv:1704.00028 .", "citeRegEx": "Gulrajani et al\\.,? 2017", "shortCiteRegEx": "Gulrajani et al\\.", "year": 2017}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R Devon Hjelm", "Athul Paul Jacob", "Tong Che", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1702.08431 .", "citeRegEx": "Hjelm et al\\.,? 2017", "shortCiteRegEx": "Hjelm et al\\.", "year": 2017}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv preprint arXiv:1602.02410 .", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "author": ["A. Lamb", "A. Goyal", "Y. Zhang", "S. Zhang", "A. Courville", "Y. Bengio."], "venue": "arXiv preprint arXiv:1610.09038 .", "citeRegEx": "Lamb et al\\.,? 2016", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1701.06547 .", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Recurrent topictransition GAN for visual paragraph generation", "author": ["Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing."], "venue": "arXiv preprint arXiv:1703.07022 .", "citeRegEx": "Liang et al\\.,? 2017", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Statistical language models based on neural networks", "author": ["Tom\u00e1\u0161 Mikolov."], "venue": "Ph.D. thesis, Brno University of Technology.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero."], "venue": "arXiv preprint arXiv:1411.1784 .", "citeRegEx": "Mirza and Osindero.,? 2014", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "arXiv preprint arXiv:1511.06434 .", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Speaking the same language: Matching machine to human captions by adversarial training", "author": ["Rakshith Shetty", "Marcus Rohrbach", "Lisa Anne Hendricks", "Mario Fritz", "Bernt Schiele."], "venue": "arXiv preprint arXiv:1703.10476 .", "citeRegEx": "Shetty et al\\.,? 2017", "shortCiteRegEx": "Shetty et al\\.", "year": 2017}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Adversarial Neural Machine Translation", "author": ["L.Wu", "Y. Xia", "L. Zhao", "F. Tian", "T. Qin", "J. Lai", "T.-Y. Liu."], "venue": "arXiv preprint arXiv:1704.06933 .", "citeRegEx": "L.Wu et al\\.,? 2017", "shortCiteRegEx": "L.Wu et al\\.", "year": 2017}, {"title": "Improving neural machine translation with conditional sequence generative adversarial nets", "author": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu."], "venue": "arXiv preprint arXiv:1703.04887 .", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "arXiv preprint arXiv:1609.05473 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Generating text via adversarial training", "author": ["Yizhe Zhang", "Zhe Gan", "Lawrence Carin."], "venue": "NIPS Workshop on Adversarial Training.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Generative adversarial networks (Goodfellow et al., 2014) have achieved state-of-the-art results in image generation (Goodfellow et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 6, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 15, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 0, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 7, "context": ", 2014) have achieved state-of-the-art results in image generation (Goodfellow et al., 2014; Radford et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017).", "startOffset": 67, "endOffset": 161}, {"referenceID": 20, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 11, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 19, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 12, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 21, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 16, "context": "Consequently, past work on using GANs for text generation has been based on pre-training (Yu et al., 2016; Li et al., 2017; Yang et al., 2017; Wu et al., 2017; Liang et al., 2017; Zhang et al., 2016; Shetty et al., 2017)", "startOffset": 89, "endOffset": 220}, {"referenceID": 10, "context": "wgan or joint training (Lamb et al., 2016; Che et al., 2017) of the generator and discriminator with a supervised maximum-likelihood loss.", "startOffset": 23, "endOffset": 60}, {"referenceID": 2, "context": "wgan or joint training (Lamb et al., 2016; Che et al., 2017) of the generator and discriminator with a supervised maximum-likelihood loss.", "startOffset": 23, "endOffset": 60}, {"referenceID": 7, "context": "Recently, two initial attempts to generate text using purely generative adversarial training were conducted by Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 7, "context": "Recently, two initial attempts to generate text using purely generative adversarial training were conducted by Gulrajani et al. (2017) and Hjelm et al. (2017). In these works, a convolutional neural network (CNN) was trained to produce sequences of 32 characters.", "startOffset": 111, "endOffset": 159}, {"referenceID": 17, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 13, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 9, "context": "Our main contribution is a model that employs an RNN for both the generator and discriminator, similar to current state-of-the-art approaches for language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016).", "startOffset": 166, "endOffset": 230}, {"referenceID": 5, "context": "We succeed in training the model by using curriculum learning (Elman, 1993; Bengio et al., 2009): At each stage we increase the maximal length of generated sequences, and train over sequences of variable length that are shorter than that maximal length.", "startOffset": 62, "endOffset": 96}, {"referenceID": 1, "context": "We succeed in training the model by using curriculum learning (Elman, 1993; Bengio et al., 2009): At each stage we increase the maximal length of generated sequences, and train over sequences of variable length that are shorter than that maximal length.", "startOffset": 62, "endOffset": 96}, {"referenceID": 5, "context": "In this paper, we extend the setup of Gulrajani et al. (2017) and present a method for generating text with GANs.", "startOffset": 38, "endOffset": 62}, {"referenceID": 17, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 13, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 9, "context": "objective (ML) have shown success in language generation (Sutskever et al., 2011; Mikolov, 2012; Jozefowicz et al., 2016), there are drawbacks to using ML, that suggest training with GANs.", "startOffset": 57, "endOffset": 121}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well.", "startOffset": 55, "endOffset": 102}, {"referenceID": 7, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well.", "startOffset": 55, "endOffset": 102}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well. Hjelm et al. (2017) have a similar setup, but employ the Boundary-Seeking GAN objective.", "startOffset": 56, "endOffset": 149}, {"referenceID": 0, "context": "(2017), who use the Improved Wasserstein GAN objective (Arjovsky et al., 2017; Gulrajani et al., 2017), which we employ as well. Hjelm et al. (2017) have a similar setup, but employ the Boundary-Seeking GAN objective. The generator G in Gulrajani et al. (2017) is a CNN that transforms a noise vector z \u223c N(0, 1) into a matrix M \u2208 R32\u00d7V , where V is the size of the character vocabulary, and 32 is the length of the generated text.", "startOffset": 56, "endOffset": 261}, {"referenceID": 7, "context": "A disadvantage of the generators in Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 7, "context": "A disadvantage of the generators in Gulrajani et al. (2017) and Hjelm et al. (2017) is that they use CNNs for generation, and thus the i-th generated character is not directly conditioned on the entire history of i\u22121 generated characters.", "startOffset": 36, "endOffset": 84}, {"referenceID": 4, "context": "We employ a GRU (Cho et al., 2014) based RNN for our generator and discriminator.", "startOffset": 16, "endOffset": 34}, {"referenceID": 7, "context": "Table 1: Samples and evaluation of the baseline model from Gulrajani et al. (2017).", "startOffset": 59, "endOffset": 83}, {"referenceID": 7, "context": "An advantage of a recurrent generator compared to the convolutional generator of Gulrajani et al. (2017) and Hjelm et al.", "startOffset": 81, "endOffset": 105}, {"referenceID": 7, "context": "An advantage of a recurrent generator compared to the convolutional generator of Gulrajani et al. (2017) and Hjelm et al. (2017) is that can output sequences of varying lengths, as we empirically show in Section 5.", "startOffset": 81, "endOffset": 129}, {"referenceID": 7, "context": "Our baseline model trains the generator and discriminator over sequences of length 32, similar to how CNNs were trained in Gulrajani et al. (2017). We found that training this baseline was difficult and resulted in nonsensical text.", "startOffset": 123, "endOffset": 147}, {"referenceID": 14, "context": "This could be viewed as a conditional GAN (Mirza and Osindero, 2014), where the first i \u2212 1 characters are the input and the final character is the output.", "startOffset": 42, "endOffset": 68}, {"referenceID": 3, "context": "(2017), we follow their setup and train our models on the Billion Word dataset (Chelba et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 5, "context": "To directly compare to Gulrajani et al. (2017), we follow their setup and train our models on the Billion Word dataset (Chelba et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al. (2017), where the generator is trained once for every 10 training iterations of the discriminator, we found that training the generator for 50 iterations every 10 training iterations of the discriminator resulted in superior performance.", "startOffset": 15, "endOffset": 66}, {"referenceID": 0, "context": "In contrast to Arjovsky et al. (2017) and Gulrajani et al. (2017), where the generator is trained once for every 10 training iterations of the discriminator, we found that training the generator for 50 iterations every 10 training iterations of the discriminator resulted in superior performance. In addition, instead of using noise vectors sampled from the N(0, 1) distribution as in Gulrajani et al. (2017), we sample noise vectors from the N(0, 10) distribution, since we found this leads to a greater variance in the generated", "startOffset": 15, "endOffset": 409}, {"referenceID": 7, "context": "Following Gulrajani et al. (2017), we train all our models on sequences whose maximum length is 32 characters.", "startOffset": 10, "endOffset": 34}, {"referenceID": 7, "context": "Following Gulrajani et al. (2017), we train all our models on sequences whose maximum length is 32 characters. Table 1 shows results of the baseline model of Gulrajani et al. (2017), and Table 2 presents results of our models with various combinations of extensions (Curriculum Learning, Variable Length, and Teacher Helping).", "startOffset": 10, "endOffset": 182}], "year": 2017, "abstractText": "Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline. 1", "creator": "LaTeX with hyperref package"}}}