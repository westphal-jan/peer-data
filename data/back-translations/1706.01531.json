{"id": "1706.01531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Progressive Boosting for Class Imbalance", "abstract": "Pattern recognition applications often suffer from distorted data distributions between classes, which can change during the operation of the design data. Two-class classification systems designed with distorted data tend to recognize the majority class better than the minority class of interest. Several data-level techniques have been proposed to alleviate this problem by upsampling minority samples or subsamples of majority samples. However, some informative samples can be neglected by random subsamples and synthetic positive samples can be added by upsampling, increasing training complexity. In this paper, a new ensemble classifier algorithm called Progressive Boosting (PBoost) is proposed, which gradually inserts unrelated groups of samples into a boosting process to avoid information loss while generating a diverse pool of classifiers. Basic classifiers in this interplay are moved from one iteration to the next, with the data weight gradually compounded from a validation set.", "histories": [["v1", "Mon, 5 Jun 2017 20:32:55 GMT  (1488kb,D)", "http://arxiv.org/abs/1706.01531v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["roghayeh soleymani", "eric granger", "giorgio fumera"], "accepted": false, "id": "1706.01531"}, "pdf": {"name": "1706.01531.pdf", "metadata": {"source": "CRF", "title": "Progressive Boosting for Class Imbalance", "authors": ["Roghayeh Soleymania", "Eric Grangera", "Giorgio Fumerab"], "emails": ["rSoleymani@livia.etsmtl.ca", "Eric.Granger@etsmtl.ca", "Fumera@diee.unica.it"], "sections": [{"heading": null, "text": "In practice, pattern recognition applications often suffer from imbalanced data distributions between classes, which may vary during operations w.r.t. the design data. Two-class classification systems designed using imbalanced data tend to recognize the majority (negative) class better, while the class of interest (positive class) often has the smaller number of samples. Several data-level techniques have been proposed to alleviate this issue, where classifier ensembles are designed with balanced data subsets by up-sampling positive samples or under-sampling negative samples. However, some informative samples may be neglected by random under-sampling and adding synthetic positive samples through up-sampling adds to training complexity. In this paper, a new ensemble learning algorithm called Progressive Boosting (PBoost) is proposed that progressively inserts uncorrelated groups of samples into a Boosting procedure to avoid loosing information while generating a diverse pool of classifiers. Base classifiers in this ensemble are generated from one iteration to the next, using subsets from a validation set that grows gradually in size and imbalance. Consequently, PBoost is more robust when the operational data may have unknown and variable levels of skew. In addition, the computation complexity of PBoost is lower than Boosting ensembles in literature that use under-sampling for learning from imbalanced data because not all of the base classifiers are validated on all negative samples. In PBoost algorithm, a new loss factor is proposed to avoid bias of performance towards the negative class. Using this loss factor, the weight update of samples and classifier contribution in final predictions are set based on the ability to recognize both classes. Using the proposed loss factor instead of standard accuracy can avoid biasing performance in any Boosting ensemble. The proposed approach was validated and compared using synthetic data, videos from the Faces In Action dataset that emulates face re-identification applications, and KEEL collection of datasets. Results show that PBoost can outperform state of the art techniques in terms of both accuracy and complexity over different levels of imbalance and overlap between classes. Keywords: Class Imbalance, Ensemble Learning, Boosting, Face Re-Identification, Video Surveillance."}, {"heading": "1. Introduction", "text": "Class imbalance is a fundamental issue in many real-world pattern recognition applications found in, e.g., automated video surveillance, fraud detection, intrusion detection in computer and network security, risk management, and medical diagnosis. Imbalance appears in binary classification problems and binarization of multi-class\n\u2217Corresponding author Email addresses: rSoleymani@livia.etsmtl.ca (Roghayeh Soleymani), Eric.Granger@etsmtl.ca (Eric Granger),\nFumera@diee.unica.it (Giorgio Fumera)\nPreprint submitted to Journal of Information Fusion June 7, 2017\nar X\niv :1\n70 6.\n01 53\n1v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n7\nclassification problems using one-vs-all strategy when samples from one class are compared against all samples from all other classes [1, 2]. In practice, the level of imbalance observed during operations in unknown a priori and varies over time. This level of skew may differ from what is seen in the design data. Classification algorithms designed using imbalanced data are often biased towards the majority (negative) class, even though the minority class is the (positive) class of interest. The main reason is that learning algorithms are typically designed to optimize the performance in terms of standard accuracy. Consequently, correct classification of negative class becomes their priority due to the abundance of samples for this class.\nSeveral approaches have been proposed in literature to design ensembles of classifiers using imbalanced data [3, 4]. In this paper, these approaches are divided into data-level and algorithm-level approaches. Data-level approaches either up-sample the positive class, under-sample the negative class or combine up-sampling and under-sampling to re-balance data for learning an ensemble of classifiers. Algorithm-level methods create or modify learning algorithms to counter the bias towards the negative class through cost-free techniques or by introducing uneven misclassification costs for the samples from different classes in cost-sensitive approaches.\nEnsembles can be designed according to a static or dynamic approach. Static ensembles generate a diverse set of base classifiers a priori, often by re-balancing the training data. The ensembles selection or fusion may be set off-line using validation data, but typically assume a fixed level of imbalance during operations. Dynamic ensembles allow to adapt the selection and fusion of base classifiers during operations based on the estimated level of skew [5, 6]. For example, in [5, 6] authors design base classifiers for a range of different levels of imbalance. Then, they estimate skew level of input data stream and select a suitable fusion function based on that level.\nHowever, the level of imbalance may be difficult to estimate accurately during operations and the diverging selection and fusion function can decrease performance. In contrast, using a static approach, the range of possible imbalance levels can be accounted for during design by training base classifiers on data subsets with different imbalance levels [7].\nMost of the ensemble learning methods to handle imbalance in literature are static approaches. Boosting [8, 9] is a common static ensemble method that has been modified in several ways to learn from imbalanced data (see a review by Galar et al. [4]). In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16]. Up-sampling methods like SMOTEBoost [10] are often more accurate, but they are computationally complex. In contrast, random under-sampling (RUS) [14] is more computationally efficient, but suffers from information loss.\nBoosting ensembles may suffer from the bias of performance towards negative class because the loss factor, which guides their learning process, is obtained based on weighted accuracy. In cases of imbalance, weighted accuracy reflects the ability for correct classification of negative samples more than positive ones. This issue can be avoided by adopting a cost-sensitive approach [17, 18, 19], that defines different misclassification costs for different classes and integrates these cost factors into Boosting learning process. The drawback of these costsensitive techniques is that they rely on the suitable selection of cost factors which is often estimated by searching a range of possible values. In contrast, cost-free techniques modify learning algorithms by enhancing loss factor calculation without considering cost factors [20, 21].\nIn this paper, the Progressive Boosting (PBoost) algorithm is proposed to design static classifier ensembles that can maintain a high level of performance over a range of possible levels of imbalance and complexity in\nthe data encountered during operations. In this algorithm, samples from the negative class are regrouped into disjoint partitions, and over iterations, these partitions are gradually accumulated into a temporary design subset. During each Boosting iteration, a new base classifier is trained using negative samples selected randomly from this subset. However, samples from the newly added partition and the important samples from previous iterations have an equally higher probability of being selected. The base classifier is then validated on the whole temporary subset. As with traditional Boosting ensembles, the samples that are misclassified are considered as the most important samples and their weights increase. With the sample selection scheme proposed in this paper, loss of information is considerably reduced, correlation among subsets of negative class is low, and only important samples tend to appear in more than one training subset. Therefore, the diversity and accuracy of Boosting ensembles tend to increase. In addition, to avoid biasing the performance towards the negative class, the proposed Boosting algorithm employs a new loss factor based on the F\u03b2-measure that is applicable in any Boosting ensemble.\nThe diverse pool of classifiers generated with PBoost allows to globally model a range of different levels of imbalance and decision bound complexities for the data. Therefore, the static ensembles produced using PBoost are robust to possible variations in data processed during operations because base classifiers are validated on a growing number of negative samples (imbalance level). In addition, the number of samples used per iteration to design (train and validate) a classifier in this ensemble is smaller than Boosting methods in the literature, which translates to a lower computational complexity for design.\nThe contributions of the proposed PBoost algorithm is summarized as follows:\n\u2022 A sample selection process for design where negative class samples are regrouped into disjoint partitions\nfor training diverse base classifiers to avoid loss of information and bias of performance;\n\u2022 A procedure to validate base classifiers on growing number of negative samples to increase robustness to\nimbalance and decrease computation complexity;\n\u2022 A new general loss factor based on the F\u03b2-measure that is applicable in any Boosting ensemble, to avoid\nbias of performance towards the negative class.\nThe PBoost algorithm has been compared to state of the art Boosting ensembles on synthetic, video and KEEL collection datasets in terms of both accuracy and computational complexity.\nThe rest of the paper is structured as follows. Section 2 contains a review of literature on ensemble learning for class imbalance. In Section 3, the proposed PBoost algorithm is described. The experimental methodology and results are presented in Sections 4 and 5, respectively."}, {"heading": "2. Boosting Ensemble Learning for Class Imbalance", "text": "Learning from imbalanced data has been addressed in literature through data-level, algorithm-level, and costsensitive techniques. Ensemble learning methods exploit one or a combination of aforementioned techniques [4] to handle imbalance. Classifier ensembles can provide higher accuracy and robustness than a single classifier system by combining diverse classifiers [22]. Boosting is a common static ensemble learning algorithm initiated with AdaBoost [8] and improved in AdaBoost.M1 (for 2-class problems) and AdaBoost.M2 (for multiple-class problems) [9] to effectively promote a weak learner that performs slightly better than random guessing into a\nstronger ensemble. In AdaBoost.M1 (Algo.1) samples are assigned weights that indicate their importance. These weights guide the learning process such that base classifiers in the ensemble focus on correct classification of more important samples as the learning iterations proceed. Samples that are misclassified in each iteration gain more importance for the next iteration and more accurate base classifiers gain higher contribution in final decision. These weights are used directly or for re-sampling training data, depending on the type of the base classifier being used. When the base classifier is from a type that is not designed to incorporate sample weights in its learning process (like SVMs), training data is re-sampled according to the weights of the samples. This case is considered here to explain the Boosting procedure.\nLet\u2019s consider a two-class problem with M labelled training samples S = {(xi,yi); i = 1, ...,M} where yi \u2208 {\u22121,1} that contains M+ positive samples and M\u2212 negative samples. All samples in the dataset are initially associated with the same weight W1(i) = 1/M, i = 1, ...,M. Then, a new training subset is re-sampled into S\u2032 with W\u2032 to trained classifier Ce. This classifier is tested on all training samples (S) and a loss factor (\u03b5e) is calculated as the sum of the weights of misclassified samples:\n\u03b5e = \u2211 (i,Yi):yi 6=Yi We(i) (1)\nwhere Yi is the label associated with xi by Ce. If the classifier is too weak (\u03b5e > 0.5), the classifier is discarded and training set is re-sampled to train another classifier. The loss factor is then used to define a weight update factor:\n\u03b1e = \u03b5e\n1\u2212 \u03b5e . (2)\nThe weights of the samples are then updated as:\nWe+1(i) = We(i)\u03b1 1 2 |yi\u2212Yi| e , (3)\nWeight vector is normalized such that the weights of the misclassified samples (more important samples) increase exponentially while the weights of the correctly classified samples decrease. \u03b1e is also used to determine the contribution of the classifier in final predictions (Equation 4) so that more accurate classifiers play more important role in identifying the class of the input sample. This process is repeated for a predefined number of times to design E classifiers. Considering he(x) as the output of Ce (either a classification score or a label) for an input sample x, final prediction of the ensemble is obtained from:\nH(x) = E\n\u2211 e=1\nhe(x) log 1\n\u03b1e (4)\nAnalogous to most learning algorithms, AdaBoost is not effective to learn from imbalanced data for two reasons. Negative samples are the majority and when training data is re-sampled in line 2.i of AdaBoost (see Algo. 1), they contribute more in S\u2032. Therefore, Ce is trained biased to correct classification of this class. After that, when Ce is tested on S, loss factor in line 2.iv is calculated as a weighted error rate of classification. Again, negative samples contribute more in loss factor calculation and the weight update formula and classifiers contribution in final prediction become biased such that weight of negative samples increases for the next iteration and classifiers\nthat mostly classify negative samples correctly get higher importance in final prediction of the ensemble. A taxonomy of methods in literature that modify AdaBoost to handle imbalance is presented in Figure 1. Based on the issue these approaches address, they are divided to two categories, data-level and algorithm-level methods that are presented in subsections 2.1 and 2.2, respectively."}, {"heading": "2.1. Data-Level Methods:", "text": "Class imbalance can be handled in Boosting ensembles through up-sampling the positive class, under-sampling the negative class or combination of them. A popular up-sampling Boosting approach is SMOTEBoost [10] that integrates Synthetic Minority Over-sampling Technique (SMOTE) into AdaBoost.M2. SMOTE creates synthetic samples by interpolating each positive sample with its k-nearest neighbours. MSMOTEBoost [11] use modified SMOTE (MSMOTE) by eliminating noisy samples and oversampling only safe samples. Jous-Boost [12] oversample the positive class by duplicating it, instead of creating new samples, and introduce perturbation (jittering) to this data in order to avoid overfitting. DataBoost-IM [13] oversample difficult samples from both classes and integrates it into AdaBoost.M1 .\nUp-sampling techniques address the bias of performance in classifiers through balancing class distribution\nwithout loss of information. However, up-sampling, in general, increase the number of samples and consequently increase the complexity of learning algorithms, and SMOTE involves additional computations due to interpolating each sample with its k-nearest neighbours to generate synthetic samples.\nIn under-sampling Boosting category, RUSBoost [14] integrates random under-sampling (RUS) into AdaBoost.M1. RUSBoost is similar to AdaBoost presented in Algo. 1 where in line 2.i of this algorithm, S\u2032 contains all positive samples and a randomly selected subset of negative class, often with a size equal to the positive class. The subsets of negative class selected randomly over iterations of RUSBoost could be highly correlated and the classifiers trained on them can lack in diversity, especially when the skew level of training data is high. The sample selection paradigm in RUSBoost is managed in EUSBoost [15] to create less correlated subsets using evolutionary prototype selection [23].\nSome researchers combine SMOTE and RUS in AdaBoost to achieve greater diversity and avoid loss of information as in Random Balance Boosting (RB-Boost) [16]. RB-Boost combines SMOTE and RUS to create training subsets with random and different skew levels in AdaBoost.M1.\nRepetition of sampling in Boosting ensembles increase the chance of low correlation between subsets of data that are used for designing base classifiers and therefore maintain diversity among them. However, some potentially informative samples may be overlooked from these subsets in under-sampling process. In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]). In these ensemble bootstraps are drawn from a set of negative samples that reduces size in each iteration. In other words, after selection of a bootstrap in each iteration, its samples are eliminated from the main set. In random partitioning of negative samples by Yan et al. [24] the negative data is randomly decomposed into a number of subsets and each subset, combined with the positive samples, is used to train a classifier. Li et al. [25] partition negative data by clustering it using k-means in the feature space and then create an ensemble from the classifiers trained on each negative cluster and the positive samples. The contribution of the classifiers in the ensemble are then weighted based on the distance between the corresponding negative cluster and positive class. In [7], partitioning negative class is done by selecting samples from a set of trajectories that are formed based on the tracking information, as found in several video surveillance applications like face re-identification. In this approach, data from the trajectories are accumulated as the training iteration proceeds and therefore, base classifiers in the ensemble are trained on different imbalance levels to increase robustness of the ensemble to the possible variations in the skew level and complexity of operational data.\nIn contrast to RUSBoost, these partitional approaches use all negative samples from partitions to design ensembles and avoid loss of information. However, not all samples are informative and using all samples for training may result in unnecessary time and memory complexity. Therefore, enhancing partitional methods with more intelligent sample selection and ensemble learning algorithm (like RUSBoost) can avoid information loss and excessive time complexity at the same time."}, {"heading": "2.2. Algorithm-Level Methods:", "text": "Using standard error in Boosting ensemble learning algorithms biases their performance towards negative class. In literature this issue is avoided at the algorithm level using two types of techniques; those that employ\ntwo different misclassification cost factors, one for positive and another for negative classes and those that handle this issue without the use of cost factors. Cost-sensitive Boosting methods including AdaCost [17], CSB [18] and AdaC [19], embed different misclassification cost factors into loss function or weight update formula of AdaBoost.M2.\nGiven \u00b5i as the cost factor of sample xi, in AdaCost [17], two cost adjustment functions are defined for each\nsample as \u03c6+ =\u22120.5\u00b5i +0.5 and \u03c6\u2212 = 0.5\u00b5i +0.5 and weight update formula is changed to:\nWe+1(i) = We(i)exp{\u2212\u03b1e\u03c6+|yi\u2212Yi|/2} for Yi = 1We(i)exp{\u2212\u03b1e\u03c6\u2212|yi\u2212Yi|/2} for Yi =\u22121 (5) CSB [18] introduce two different cost factors for positive and negative classes as \u00b5+ = 1 and \u00b5\u2212 \u2265 1, respec-\ntively.\nWe+1(i) = We(i)\u00b5+ exp{\u2212\u03b1e|yi\u2212Yi|/2} for Yi = 1We(i)\u00b5\u2212exp(\u2212\u03b1e|yi\u2212Yi|/2} for Yi =\u22121 (6) In AdaC1, 2, 3 [19] cost factors are embedded into the weight update formula in three different ways. Given \u00b5i \u2208 [0,+\u221e), in AdaC1:\n\u03b1e = 1 2\nln 1+ \u2211 i,yi=Yi \u00b5iWe(i)\u2212 \u2211 i,yi 6=Yi \u00b5iWe(i)\n1\u2212 \u2211 i,yi=Yi \u00b5iWe(i)+ \u2211 i,yi 6=Yi\n\u00b5iWe(i) , (7)\nWe+1(i) = We(i)exp{\u2212\u03b1e\u00b5iYiyi) (8)\nIn AdaC2:\n\u03b1e = 1 2\nln \u2211 i,yi=Yi \u00b5iWe(i)\n\u2211 i,yi 6=Yi\n\u00b5iWe(i) , (9)\nWe+1(i) = \u00b5iWe(i)exp{\u2212\u03b1eYiyi} (10)\nIn AdaC3:\n\u03b1e = 1 2\nln \u2211i \u00b5iWe(i)+ \u2211 i,yi=Yi \u00b52i We(i)\u2212 \u2211 i,yi 6=Yi \u00b52i We(i)\n\u2211i \u00b5iWe(i)\u2212 \u2211 i,yi=Yi \u00b52i We(i)+ \u2211 i,yi 6=Yi\n\u00b52i We(i) , (11)\nWe+1(i) = \u00b5iWe(i)exp{\u2212\u03b1e\u00b5iYiyi} (12)\nIn these cost-sensitive approaches by setting \u00b5+ greater than \u00b5\u2212 the weights of misclassified samples from positive class increase more than that of the misclassified samples from negative class. In addition, the weights of the classifiers that correctly classify positive class better than the negative class is higher in final decision. Therefore, these cost-sensitive approaches can make up for the usage of standard error rate in Boosting ensembles and allow adapting the performance by selecting proper cost factors based on the application. The drawback of these cost-\nsensitive approaches is that they require known \u00b5is that are usually set ad-hoc or by conducting a search in the space of possible costs for a dataset.\nSome cost-free approaches have been proposed to deal with the bias of performance caused by using standard\nerror in Boosting ensembles. In RareBoost [20], two different \u03b1s are defined for positive and negative classes as:\n\u03b1+e = 1 2 ln( T Pe FPe ) (13)\n\u03b1\u2212e = 1 2 ln( T Ne FNe ) (14)\nwhere T Pe and T Ne are the true positive and true negative counts, respectively. Then the weight update formula and final classification prediction are modified as:\nWe+1(i) = We(i)exp{\u2212\u03b1 + e |yi\u2212Yi|/2} for Yi = 1\nWe(i)exp{\u2212\u03b1\u2212e |yi\u2212Yi|/2} for Yi =\u22121 (15)\nH(x) = sign( \u2211 e:he(x)\u22650 \u03b1+e he(x)+ \u2211 e:he(x)<0 \u03b1\u2212e he(x))) (16)\nKim et al. [21] also define two different \u03b1es for positive and negative classes as:\n\u03b1+e = 1\u2212 l+\nl+ , l+ =\n\u2211 i;yi=+1 We(i)|yi\u2212Yi|/2\n\u2211 i;yi=+1\nWe(i) (17)\n\u03b1\u2212e = 1\u2212 l\u2212\nl\u2212 , l\u2212 =\n\u2211 i;yi=\u22121 We(i)|yi\u2212Yi|/2\n\u2211 i;yi=\u22121\nWe(i) (18)\nwhere l+ and l\u2212 are pseudo errors of classifier in classifying each class. Finally:\n\u03b1e = ln( \u221a \u00b5i\u03b1+e \u03b1\u2212e ) , (19)\n\u00b5i is a multiplier to control the weight of each sample. The problem with this loss factor is that, if there are no misclassified samples in one class or in both classes, \u03b1e is undefined.\nCost-free methods enhance the performance of Boosting ensembles without setting any cost factors and guide the learning process using a more suitable loss factor calculation since the use of weighted standard accuracy, as in original Boosting algorithm, biases the learning process towards correct classification of the negative class. These approaches inspire us to use a more suitable error calculation method in the proposed Boosting algorithm. Therefore, it is relevant to review some of performance measures for imbalanced data classification in the following subsection."}, {"heading": "2.3. Performance Measures for Imbalanced Data Classification", "text": "The trade-off between true positive rate (TPR) and false positive rate (FPR) for different operating settings can be traced with a Receiver-Operating Characteristic (ROC) curve. ROC curves are widely used to compare classifiers performance. This curve can also be summarized into a global scalar metric; area under the ROC curve (AUC). In addition, for a specific operating setting, G-mean performance measure is defined as the geometrical mean of TPR and TNR (or 1-FPR).\nG-mean = \u221a TNR \u00b7TPR . (20)\nG-mean gives an equal weight to the efficiency of classifier in correct classification of both classes. ROC space does not adequately reflect the impact of imbalance [26] on performance because big variations in the number of misclassified negative class (FP) results in a small change in FPR, especially if a small increase in TPR can mask it. A suitable alternative for TPR is precision (Pr) that compares the number of misclassified negative samples (FP) to the number of correctly classified positive samples (TP).\nPr = TPR\nTPR+\u03bbFPR . (21)\nwhere \u03bb = M\u2212/M+. It is evident that, as imbalance (\u03bb) increases, any given decrease in FPR results in a higher reduction in Pr. Therefore, precision drops severely when correct classification of positive class is in expense of high misclassification of negative class. For different operating settings, precision-recall (PR) curve depicts the trade off between precision and recall (Re=TPR) when data is imbalanced. Inspired from AUC, area under PR curve (AUPR) can also be used to compare classifiers globally over all operating settings.\nFor a specific operating setting, precision and recall can be weighted and combined into a local performance\nmetric, the F\u03b2-measure:\nF\u03b2 = (1+\u03b22)Pr.Re\n\u03b22Pr+Re = (1+\u03b22)TP (1+\u03b22)TP+FP+\u03b22FN\n(22)\nAlthough Pr is very sensitive to misclassification of negative samples due to their abundance, with selection of suitable \u03b2 in F\u03b2-measure, this sensitivity can be controlled. With \u03b2 \u2265 1, this sensitivity reduces and F\u03b2 gives a higher importance to correct classification of positive class. With \u03b2 < 1, this sensitivity increases and correct classification of negative samples becomes the priority.\nAnother metric that takes imbalance into account is expected cost which is calculated as:\nEC = \u03c0 \u00b7FNR \u00b7CFN +(1\u2212\u03c0) \u00b7FPR \u00b7CFP (23)\nwhere \u03c0 = M+/M is the proportion of positive samples, and CFN and CFP are the misclassification cost of positive and negative classes, respectively. For higher values of \u03bb (lower values of \u03c0 < 0.5) and CFN = CFP, EC is more sensitive to increase of FPR rather than FNR. For a specific value of \u03c0, the sensitivity of EC to each of FNR and FPR can be controlled by tuning CFN and CFP.\nSome authors define variants of the existing performance metrics by accounting for \u03c0 [27, 28, 29]. For exam-\nple, in [28], the authors define a measure of expected accuracy in terms of AUC as \u03c0(1\u2212\u03c0)(2AUC\u22121)+1/2, and precision-recall gain (PRG) curve [29] normalize precision and recall in terms of \u03c0 as:\nprecG = Pr\u2212\u03c0\n(1\u2212\u03c0)Pr , (24)\nrecG = Re\u2212\u03c0\n(1\u2212\u03c0)Re . (25)\nROC, PR, and PRG curves are usually produced by varying decision threshold over real valued scores output by classifiers. Consequently, using areas under the curves to generate global metrics or their variants in Boosting ensembles may increase computational cost of these algorithms. Masking the effect of imbalance in performance metrics as done in [27, 28, 29] can misguide the learning process of Boosting ensembles and bias the performance towards negative class. The F\u03b2-measure and expected cost are more suitable performance metrics that take into account correct classification of both classes considering the level of imbalance. However, adjusting the sensitivity of the F\u03b2-measure to the correct classification of the positive and the negative class is easier than expected cost. The reason is that adjusting the F\u03b2-measure involves tuning \u03b2 rather than tuning two factors (CFN and CFP) for expected cost. Nevertheless, EC is a more suitable performance metric for cost-sensitive learning algorithms.\nIn this paper, the F\u03b2-measure, the most frequently used measures for performance evaluation in class imbalance learning, is employed to modify the loss factor calculation in Boosting ensembles, to avoid biasing the performance towards negative class."}, {"heading": "3. Progressive Boosting for Learning Ensembles from Imbalanced Data:", "text": "The Progressive Boosting (PBoost) learning method is proposed to sustain a high level of performance over a range of imbalance and complexity levels in the data seen during operations. This method follows a static approach, and learns ensembles based on a combination of under-sampling and cost-free adjustment of Boosting ensemble learning.\nWith the PBoost algorithm, negative class is partitioned into disjoint subsets. These partitions are accumulated into a temporary design set progressively as learning iterations proceed. In each iteration, a subset of this temporary set is used for training a classifier such that the most important samples plus samples from the new partition are given an equally high opportunity to be used in training a base classifier. Loss of information is therefore avoided and ensemble diversity is increased. The trained classifier is then validated on the temporary set that contains all positive samples and only those negative partitions that have already been used in previous training iterations. As the temporary set grows, its imbalance level increases and therefore, the ensemble\u2019s robustness to diverse levels of skew and decision bound complexities during operations is increased. In PBoost, the error of the classifier is determined based on its ability to correctly classify both positive and negative classes. This loss factor plays an important role in determining the contribution of classifiers in final prediction, and in selection criteria of samples for designing the next classifiers.\nThe progressive Boosting method is presented in Algo. 2 and Figure 2. Its main steps are explained in the following. In the proposed algorithm, the negative samples are first regrouped into E disjoint partitions Pe, where e = 1, ..,E, one per classifier in the ensemble (line 1). E and the number of negative samples in each partition\nNe varies and depends on the partitioning method and the data distribution. There are several possible ways to partition the negative samples into disjoint subsets in literature [30] e.g., prototype-based methods like kmeans and GMM algorithms, affinity-based methods like spectral, normalized-cut and sub-space algorithms to represent the negatives, and thus define partitions (number of clusters and association of data to clusters). Two partitioning techniques have been used in literature to partition data to learn ensembles from imbalanced data: Random Under-Sampling without replacement (we call RUSwR in this paper) [24] , and Cluster Under-Sampling (CUS) [25]. In some applications the data is already partitioned, like binarization of multi-class classification problems using one-vs-all strategy. In some applications the data may be grouped based on some contextual or application-based knowledge of data. For example, Trajectory Under-Sampling (TUS) is applicable in video surveillance applications where samples captured for a same individual are regrouped into a trajectory [7]. In the case of random under-sampling without replacement E is preselected and Ne takes a fixed random value Ne \u2208 [M+/2,2M+] such that \u2211Ee=1 Ne = M\u2212. In the case of CUS and TUS, E and Ne depend on the number of samples that are assigned to each partition by the clustering algorithm and the tracker, respectively.\nGiven a training data set S, one partition Pe is selected in each iteration and added to a temporary set Stmpe (line 5.ii) which initially contains the positive samples. The same initial weight wini is assigned to the samples in the new partition creating a weight vector Wpe (line 5.i) which is also added to a temporary weight set W tmp e (line 5.ii). In the next step (line 5.iv), Ne samples from the temporary set Stmpe are selected through random undersampling to create a new subset S \u2032 e with the weight distribution of W \u2032 e. A classifier Ce is trained on S \u2032 e (line 5.v). Then it is tested on the whole temporary set Stmpe that has an imbalance level of \u03bbe = 1 : \u2211ef=1 N f/M+ (line 5.vi). Therefore, the classifiers in this ensemble are in fact validated on data subsets with a growing level of imbalance and complexity.\nAfter that, a new loss factor is calculated that adapt Boosting algorithm for classifying imbalanced data based on F\u03b2-measure (line 5.vii) because F\u03b2-measure is more sensitive to imbalance and at the same time allows us to give more importance to one class than the other.\nTo calculate the loss factor, we first split the temporary weight vector Wtmpe to two weight matrices for positive\nWtmp,+e and negative Wtmp,\u2212e classes. The size of Wtmp,+e is M+ and the size of Wtmp,\u2212e is \u2211ef=1 N f , and:\nWtmp,+e = {Wtmpe ( j), j = 1, ...,(M++ e\n\u2211 f=1 N f )|y j = 1} , (26)\nWtmp,\u2212e = {Wtmpe ( j), j = 1, ...,(M++ e\n\u2211 f=1 N f )|y j =\u22121} . (27)\nThen, weighted versions of true positive, false positive, true negative and false negative counts are defined as:\nTPe = \u2211 k:Yk=1 Wtmp,+e (k),k = 1, ...,M + (28)\nFPe = \u2211 k:Yk=1\nWtmp,\u2212e (k),k = 1, ..., e\n\u2211 f=1 N f (29)\nTNe = \u2211 k:Yk=\u22121\nWtmp,\u2212e (k),k = 1, ..., e\n\u2211 f=1 N f (30)\nFNe = \u2211 k:Yk=\u22121 Wtmp,+e (k),k = 1, ...,M + (31)\nBased on these values, the accuracy of a classifier is computed in terms of F\u03b2-measure as:\nAF = (1+\u03b22)TPe\n(1+\u03b22)TPe +FPe +\u03b22FNe , (32)\nTo measure the error of the classifiers, the corresponding loss factor is defined as:\nLe = 1\u2212AF = FPe +\u03b22FNe\n(1+\u03b22)TPe +FPe +\u03b22FNe . (33)\nThe condition \u03b5e > 0.5 in line (v) of AdaBoost.M1 (Algo. 1) means that classifiers in a Boosting ensemble should perform better than random guessing. When F\u03b2-measure is used as the evaluation metric, the base classifier to beat is the one that predicts everything as positive [29]. Therefore, when the loss factor is calculated using Eq. (33), the accuracy criterion of 0.5 in AdaBoost.M1 should be replaced by lb = M \u2212\n(1+\u03b22)M++M\u2212 (line 5.vii).\nAfter calculation of \u03b1e (line 5.ix) from:\n\u03b1e = Le\n1\u2212Le , (34)\nthe weights in the temporary set Wtmpe are updated (line 5.x) as:\nWtmpe+1( j) = W tmp e ( j) \u03b1 |yj\u2212Yj|/2 e . (35)\nEven though it is desirable to limit the loss of information during under-sampling of data, some samples (like borderline samples) are of more interest than others for training classifiers in the ensemble. In Boosting ensembles, these samples are often detected as misclassified samples because borderline samples play more important role in defining the decision bound and they are more likely to be misclassified. More importance is given to these samples by assigning higher weights to them, so that they have a higher chance to be included in training subset(s). In the proposed PBoost ensemble, after normalization of Wtmpe , its maximum value among negative samples is selected as the initial weight for the next iteration (line 5.xii):\nwinie+1 = maxy j=\u22121 {Wtmpe ( j)}, j = 1, ...,M++\ne\n\u2211 f=1 N f . (36)\nThis value corresponds to the weight of more important misclassified negative samples. Therefore, in each iteration, new samples and misclassified samples from previous iterations have more chance to be included in the training subset. Finally, \u03b1e is used to obtain the final class prediction of the ensemble from (4) (line 6).\nPBoost is somewhat inspired from RUSBoost, but differs in three main respects. First, during each iteration, instead of random under-sampling with replacement, most of training negative samples are selected from disjoint partitions. Consequently, repeatedly selection of the same samples over all iterations and information loss is avoided while the diversity increases. Second, instead of validating the classifiers on all samples, the classifiers are validated only on a subset of training set that grows in size and imbalance over iterations. Therefore, robustness to different levels of data imbalance and complexity increases, and the computations complexity of validation step decreases significantly. Third, instead of weighted accuracy, F-measure, an imbalance-compatible performance metric, avoids biasing performance towards negative class.\nAlgorithm 2: Progressive Boosting ensemble learning method. Input: Training set: S = {(xi,yi); i = 1, ...,M},yi \u2208 {\u22121,1},M = M\u2212+M+ Output: Predicted score or label: H(\u00b7) 1 Partition non-target samples from S into E clusters {Pe;e = 1, ...,E}. 2 Create a temporary training set and weight vector: Stmp1 \u2190{(xi,yi) \u2208 S|yi = 1} and W tmp 1 (k) = 1,k = 1, ...,M\n+. 3 Initialize wini1 = 1. 4 Set lb = M \u2212\n(1+\u03b22)M++M\u2212\n5 for e = 1, ..,E do i Initialize weight distribution of Pe as W p e (k) = winie ,k = 1, ...,Ne. // Ne is the size of Pe.\nii Stmpe \u2190 Stmpe \u22c3 Pe , W tmp e \u2190Wtmpe \u22c3 Wpe\niii Normalize Wtmpe such that: \u2211W tmp e = 1. iv Randomly select Ne non-target samples from S tmp e based on W tmp e , to create a training subset S \u2032 e with W \u2032 e.\nv Train Ce on S \u2032 e with W \u2032 e.\nvi Test Ce on S tmp e and get back labels Y j, j = 1, ...,(M++\u2211ef=1 N f ).\nvii Calculate the pseudo-loss for Stmpe from W tmp e (using Equations 26 to 31):\nWtmp,+e = {Wtmpe ( j), j = 1, ...,(M++\u2211ef=1 N f )|y j = 1}, Wtmp,\u2212e = {Wtmpe ( j), j = 1, ...,(M++\u2211ef=1 N f )|y j =\u22121}, TPe = \u2211\n(k,Yk):Yk=1 Wtmp,+e (k),k = 1, ...,M+,\nFPe = \u2211 (k,Yk):Yk=1\nWtmp,\u2212e (k),k = 1, ...,\u2211ef=1 N f ,\nTNe = \u2211 (k,Yk):Yk=\u22121\nWtmp,\u2212e (k),k = 1, ...,\u2211ef=1 N f ,\nFNe = \u2211 (k,Yk):Yk=\u22121\nWtmp,+e (k),k = 1, ...,M+,\nLe = 1\u2212AF = FPe+\u03b2 2FNe\n(1+\u03b22)TPe+FPe+\u03b22FNe .\nviii If Le > lb go to step iv\nix Calculate the weight update parameter: \u03b1e = Le1\u2212Le\nx Update Wtmpe+1( j) = W tmp e ( j)\u03b1 |yj\u2212Yj|/2 e\nxi Normalize Wtmpe+1 such that: \u2211W tmp e+1 = 1.\nxii Set winie+1 = max(W tmp e ),y j =\u22121\n6 Output the final hypothesis: H(\u00b7) = \u2211Ee=1 he(\u00b7) log 1 \u03b1e // he(\u00b7) is the output of Ce."}, {"heading": "4. Experimental Methodology", "text": "In our experiments, the proposed PBoost ensemble learning method is assessed and compared with AdaBoost.M1 [9], and one state of the art method from each family of the data-level approaches reviewed in Section 2 including SMOTEBoost [10], RUSBoost [14], and RB-Boost [16]. The datasets that are used for the experiments include: (1) A set of synthetic 2D data sets in which the level of skew and overlap between classes are controllable, (2) the Face In Action (FIA) video database [31] that emulates a passport checking scenario in face re-identification application, (3) a set of 21 real-world problems from the KEEL dataset repository [32]. The rest of this section presents the datasets used in the experiments followed by the experimental and evaluation protocols."}, {"heading": "4.1. Datasets", "text": ""}, {"heading": "4.1.1. Synthetic Dataset", "text": "The performance of classification systems may vary on different levels of overlap and skew between classes in both training and test data. Therefore, in our experiments on synthetic data, different synthetic datasets with\ndifferent overlap and skew levels are generated and used to compare classification systems.\nThe data is generated to emulate both binarization of a multi-class classification problem when the classification strategy is one versus all and binary classification problems where there is no prior knowledge of optimal partitions. The samples of both positive and negative classes are generated in groups of samples distributed in a normal distribution. The samples from one normal distribution are considered as positive class and all other samples are considered as negative class.\nTo generate the 2D synthetic data, M+ = 100 positive class samples are generated with a normal distribution as N(m+,\u03c3+), where m+ = (0,0) and \u03c3+ = [1 00 1 ] indicate the mean and covariance matrix of this distribution, respectively. Then, T\u2212 = 100 points are selected randomly in a uniform distribution around m+. These points\n(m\u2212, j, j = 1, ...,T\u2212) are generated as the mean of T\u2212 Normal distributions (N(m\u2212, j,\u03c3\u2212), j = 1, . . . ,T\u2212) for negative class where \u03c3\u2212 = \u03c3+. Each normal distribution contains M+ = 100 samples and is considered as an ideal cluster of negative class (used for PCUSi).\nThe mean of these clusters (m\u2212, js) keep a margin distance \u03b4 from m+. This margin is used to control the level\nof overlap between positive and negative classes.\nFor the experiments, we selected the parameter \u03b4 as 0.1 (maximum overlap) and 0.2 (medium overlap). For each overlap level, each normal distribution is randomly divided into two subsets for design and testing. Then the design subsets are divided into 5 folds considering one fold for validation and 4 folds for training. Five replications is possible by alternating the validation fold in each iteration and by reversing the role of design and testing subsets, a total of 10 replications is achieved.\nTwo settings are considered for the experiments that differ based on the skew level of training data where \u03bbtrain = 1 : M \u2212 /M+ is set to 1:50 in one setting and to 1:20 in the other. When \u03bbtrain = 1 : 50, only 50 clusters from the negative class are used for training. The objective is to compare different classification algorithms when they are designed on different levels of imbalance. Properties of training data generated with these settings are summarized in Table 1 and examples of training data generated with these settings are presented in Figure 3.\nIn a similar way, four settings are considered for testing under different imbalance levels (\u03bbtest = {1 : 1,1 : 20,1 : 50,1 : 100}) to evaluate the robustness of the classification algorithms over varying skew levels of data during operation. Examples of synthetic test data corresponding to setting D1 is presented in Figure 4."}, {"heading": "4.1.2. Face Re-Identification Dataset", "text": "Face re-identification is a video surveillance application where individuals in video streams are recognized at different time instants and/or locations over a network of distributed cameras. Non-target faces captured in videos under various challenging conditions are compared to those of the target individual using a video-to-video face recognition system. One important challenge in this application is that the number of faces captured from the target individual (positive class) is typically limited and greatly outnumbered by non-target ones (negative class)\nTable 1: Settings used for data generation.\nD1 D2 D3 \u03bbtr 1:50 1:50 1:20 \u03b4 0.2 0.1 0.2\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1\nNegative cluster centers Positive Overlap:\u03b4\n(a) D1.\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b) D2.\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n(c) D3.\nFigure 3: Examples of synthetic training data generated for experiments.\n[33, 34, 7].\nIn this classification problem, face captures from each individual may be grouped by the face tracker to trajectories. Given a video stream, an efficient face tracking system, groups face captures from the same individual to a trajectory. A trajectory is Regions Of Interest (ROIs) of a same person regrouped with a face tracker according to a high quality tracking information collected by the tracker that follows the location of the ROIs over consecutive video frames.\nFIA video database [31] contains video sequences that emulate a passport checking scenario. The video streams are collected from 221 participants under different capture conditions such as pose, illumination and expression in both indoor and outdoor environments. Videos were collected over three sessions where second and third sessions are three months later than the previous one. The participants are present before 6 cameras for about 5 seconds, resulting in total of 18 video sequences per person.\nFor experiments in this paper using FIA dataset, only the faces captured with frontal camera in indoor environment is used for both design and testing. ROIs are converted to gray-scale and rescaled to 70\u00d7 70 pixels using Viola Jones algorithm [36] from this video. Then, multi-resolution gray-Scale and rotation invariant Local\nBinary Patterns (LBP) [37] histograms have been extracted as features. The local image texture for LBP has been characterized with 8 neighbours on a 1 radius circle centred on each pixel. Finally, a feature vector with the length of 59 has been obtained for each ROI. Some examples of ROIs from this data set are presented in Figure 5.\nFor experiments with video data, 10 individuals are randomly selected as targets and 90 individuals are randomly selected as non-targets. In each round of experiment, face patterns of one target individual (a trajectory) is considered as the positive class and 100 individuals (including 9 other target individuals and 90 non-target individuals) are selected as the negative class 1. ROI patterns from each trajectory are divided into 2 sets for design and testing. The design set is divided to 5 folds, and for each round one fold is considered for validation and remaining 4 folds are considered for training. Then the roles of design and testing sets is reversed. Therefore, for each target individual, three independent sets are collected from these face patterns for training, validation and testing. Each set contains one group of samples from the target individual, 9 groups of samples from the remaining target individuals and 90 groups of samples from non-target individuals. Repeating this process for each target individual yields 10\u00d710 = 100 overall experiments for this dataset.\nTwo imbalance levels (\u03bbtrain = 1 : 50 and 100) and four different imbalance levels \u03bbtest = {1 : 1,1 : 20,1 : 50,1 : 100} are considered for selecting the training and testing negative class for each positive individual, respectively. This is to evaluate the performance of different classification algorithms when they are trained on different imbalance levels, and to evaluate the robustness of the classification algorithms over varying skew levels during operations. When \u03bbtrain = 1 : 50, for each positive individual, only T\u2212 = 50 of 100 other individuals are used as the negative class from the training set that was collected for that positive individual. Therefore, when \u03bbtest = 1 : 100, there are 50 negative individuals in the testing set that were not included in training the classification systems and the skew level of test data is higher than the skew level of training data. When \u03bbtest < 1 : 50, most of the negative individuals that were used for training do not appear in testing data. When \u03bbtrain = \u03bbtest = 100, the maximum imbalance level of testing data is the same as the imbalance level of training data. Therefore, all individuals are seen in both training and testing. However, in this case a high level of imbalance exists in both training and testing stages that makes both learning and classification more difficult. It is worth mentioning that in all settings, the skew level of the validation data is selected to be the same as testing data."}, {"heading": "4.1.3. KEEL Collection", "text": "KEEL (Knowledge Extraction based on Evolutionary Learning) tool is an open source software that supports data management and a designer of experiments [32]. The dataset collection in KEEL format contains several datasets for binary classification problem with different number of samples, attributes and imbalance levels. In this paper, the first group of this collection 2 is used for experiments. In this group, the skew level of datasets ranges between 1:9 and 1:129. The experiments on this collection is done using stratified 2 \u00d7 5 fold cross validation strategy. Therefore, the skew levels of training, validation and testing sets are equal.\n1The ROIs of each individual in FIA dataset are already grouped to trajectories. 2(http://www.KEEL.es/dataset.php)"}, {"heading": "4.2. Experimental Protocol", "text": "For validation, datasets are selected and generated to consider two possible cases. Synthetic and video datasets are used to evaluate the algorithm when the ideal partitions (or clusters) of negative class are known a priori. The synthetic and video sets are also used for a binary classification problem where no information is available regarding the ideal clusters of data. The KEEL collection datasets are also a case when the data is not partitioned a priori.\nWe use SVM with RBF kernel [38] as the base classifier where K(x\u2032,x\u2032\u2032) = exp{\u2212\u2016x\u2032\u2212x\u2032\u2032\u20162/2\u03ba2}. The kernel parameter \u03ba is set as the average of the mean minimum distance between any two training samples and the scatter radius of the training samples in the input space [39]. The scatter radius is calculated by selecting the maximum distance between the training samples and a point corresponding to the mean of training samples. We used the LibSVM implementation of [38].\nA brief description of the implemented ensembles, their variants and the abbreviations used for them are shown in Table 2. The last column of the table shows the datasets that are used for experiments on these classification systems. The abbreviations assigned to these ensembles are selected based on their sampling techniques and loss factor.\nThe baseline sampling techniques include Ada (resampling in AdaBoost), SMT (SMOTE in SMOTEBoost), RUS (random under-sampling in RUSBoost), RB (random balance in RB-Boost). For PBoost four partitioning techniques are used for under-sampling the negative class to evaluate the effect of the partitioning technique on the performance of PBoost ensemble: random under sampling without replacement (PRUS) and cluster undersampling (PCUS) are used as general partitioning techniques for PBoost disregarding the data structure, whether or not the negative class is partitioned a priori. For PCUS, kernel k-means is used for clustering negative samples. To select k, it is varied over a range of possible values and the value of Dunn index [40] is calculated for each case using a validation set. Finally, the optimal k, is selected when Dunn index takes its maximum value. Two cases are considered for PBoost in which the partitions of the negative class are known a priori. The ideal cluster under-sampling (PCUSi) with synthetic datasets and trajectory under-sampling (PTUS) with video dataset.\nThe loss factor is calculated in 2 ways based on: the traditional technique i.e. weighted accuracy, and the F-measure. To indicate the use of proposed loss factor in the Boosting ensembles in Table 2, the abbreviation is followed by -F. For the use of proposed loss factor calculation with the F-measure, \u03b2 is set as 2 in all experiments because \u03b2\u2265 1 is more suitable for imbalanced data classification when the positive class is the minority class. An experiment is done to evaluate the performance of Boosting ensembles with different values of \u03b2.\nIn the experiments with synthetic and video data sets, two different imbalance levels are used for training and four different imbalance levels are used for testing. This is to evaluate the sensitivity of classification systems to the level of imbalance during training and their robustness to possible variations in skew level during operations. In experiments with synthetic data, the overlap level between positive and negative classes are also varied because the issue of imbalance is related to the level of overlap between classes [41].\nIn experiments with synthetic and video datasets, the size of all Boosting ensembles is set equal to the maximum imbalance level of the data, except from PCUS. The reason for this setting is that the number of ideal clusters and the number of trajectories are both known and equal to the level of skew. In addition, based on a preliminary experiment with D2 on baseline ensembles in Figure 6, it is observed that the size of these ensembles does not\nhave a significant impact on their performance. The performance of these ensembles vary in terms of F2-measure as the ensemble size grows. However, their global performance in terms of AUPR do not change significantly. For PCUS, the size of ensemble is selected equal to the optimal k obtained using Dunn index."}, {"heading": "4.3. Evaluation Protocol", "text": "Global performance evaluation curves such as ROC and PR, show the trade off between two metrics for different operational settings. For classifiers that output scores or probability estimates, this setting is usually the choice of decision threshold. Area under the curve, shows the global performance of the classifier over a range of possible decision thresholds, where local evaluation metric such as F-measure show the performance for a specific decision threshold.\nTherefore, when different classifiers are compared in terms of local metrics, the choice of the decision threshold becomes important. The decision threshold may be set to a fixed optimal value without considering the operating condition or based on operating conditions: the cost proportions or skew levels [28]. The performance metrics that can be maximized to set the decision threshold are accuracy, Brier score, AUC, expected cost and F-measure [28, 42]. To this aim, the classifiers are tested on a set of data called validation datasets that are independent from training and testing data.\nAs explained in Section 2.4, AUPR and F-measure are more suitable metrics to compare the performance of the classification systems when data is imbalanced. Therefore, in the experiments in this paper, AUPR is used to compare the performance of Boosting ensembles globally and F-measure with \u03b2 = 2 is used to compare the classifiers for a specific operating condition. AUPR shows the average value of precision for different values of\nrecall (or TPR), and F2-measure shows the harmonic mean of precision and recall when a higher importance is given to recall.\nThe value of AUPR and F2-measure is averaged over 10 replications obtained by 2 \u00d7 5-fold cross validation. In our experiments, the decision threshold to obtain the F2-measure is set to the value that maximizes the value of F2-measure on the validation data for comparing the performance of different classification algorithms. An example is shown in Figure 7, the PR curve of an experiment on abalone9-18 dataset of KEEL collection on the validation data. In Figure 8, the ensembles are tested on a different test set and Fop shows the value of F-measure when the optimal threshold is selected using the validation step described. FD is the value of F-measure when the combination function in Boosting ensembles is majority voting and the decisions of base classifiers are combined. It is observed that Fop and FD may differ significantly and in most cases Fop > FD.\nIn our experiments, the performance of the proposed PBoost ensemble is also compared to state of the art Boosting ensembles in terms of computational complexity. Time complexity for SVM training depends on several factors including the number of training samples, the learning (optimization) algorithm and the number of features. The computational complexity of SVM implemented in LibSVM is evaluated in [38], as O(ntrd) per iteration I, where ntr is the training set size, and d is the number of features. The authors state that \u201cthe number of iterations p may be higher than linear to the number of training data\u201d. Therefore, the complexity is O(nptr \u00b7d) for some p > 2. This means that, time complexity for SVM training is not proportional to, but increases more than linearly with respect to the training set size.\nIn the proposed PRUS and baseline Boosting ensembles, p is unknown and d is identical in all algorithms. Each iteration of Boosting ensembles includes a validation step that should be added to training complexity to obtain the overall time complexity of learning process. Time complexity of the validation step O(nSV \u00b7 nval), depends on the number of validation samples nval and the number of support vectors nSV obtained from training each SVM. The reason is that, when an RBF SVM with nSV support vectors is tested on a probe sample x, the value of K(x,SVj) = exp{\u2212\u2016x\u2212SVj\u2016 2 /2\u03ba2} is accumulated for all support vectors ( j = 1, . . . ,nSV) and the sign of the resulting quantity determines the decision.\nTable 3 shows the number of samples to train and validate these Boosting ensembles of the size E. The number of validation samples in baseline Boosting ensembles is the same and equal to the overall number of training samples. However, the overall number of samples used for validation in PRUSBoost is calculated as:\nE\n\u2211 e=1\n(M++ e\n\u2211 f=1\nN f ) = EM++ E\n\u2211 e=1 (E\u2212 (e\u22121))Ne, (37)\n= EM++E2\u2212 E\n\u2211 e=1 eNe +M\u2212, (38)\n= EM++M\u2212+E2\u2212 E\n\u2211 e=1 eNe. (39)\nThis value is less than E(M++M\u2212) that is the total number of validation samples in the sate of the art Boosting ensembles. Table 3 shows that the total number of training and validation samples in PRUS ensemble is the smallest one."}, {"heading": "5. Results and Discussion", "text": "The performance of the proposed and state of the art ensemble learning methods are analysed for synthetic and video data in 4 parts: (1) accuracy and robustness over different levels of overlap and imbalance between design and test data and of using the proposed loss factor; (2) the performance of RUSBoost with and without progressive partitioning; (3) the combined impact of progressive partitioning and proposed loss factor; (4) the computation complexity during design and testing."}, {"heading": "5.1. Results of Experiments with Synthetic Data", "text": ""}, {"heading": "5.1.1. Impact of proposed loss factor", "text": "The performance of the baseline Boosting ensembles: AdaBoost , SMOTEBoost, RUSBoost, and RB-Boost are compared in Table 4 for different settings. In addition, F\u03b2 is used to optimize loss factor calculation in these ensembles.\nGiven a fixed skew level of test data, the performance of all Boosting ensembles declines in terms of Fmeasure and AUPR as the overlap between positive and negative classes grows. This decline of performance is more significant when test data is imbalanced compared to the case where test data is balanced. In our experiments, changes in skew level of test data result in different number of misclassified negative samples and no change in the number of correctly classified positive samples. Therefore, even for the same level of overlap, the performance of all ensembles degrades, in terms of both F-measure and AUPR when testing on a more imbalanced data.\nFor the same level of overlap and different imbalance of training data (D1 and D3) the performance of all Boosting ensembles is lower when imbalance of training data is lower. The reason is that less information is provided for training and also the skew level of training and test data has a greater difference. Overall, Table 4 shows that RUS is the most robust to changes in overlap and imbalance. Based on the Table 4, the following results are obtained. Using the proposed loss factor improves the performance of Ada with D2 and D3 for all \u03bbtest, and D1 with \u03bbtest = 1 : 100. However, the performance drops slightly with D1 and \u03bbtest = 1 : 1,1 : 20 and 1 : 50. Performance of SMT improves with D3 for all \u03bbtest, and it declines with D2 and \u03bbtest = 1 : 20,1 : 50 and 1 : 100 as well as with D1 and \u03bbtest = 1 : 1,1 : 20. Performance of RUS improves with D1 and D2 for all \u03bbtest, and with D3 and \u03bbtest = 1 : 100. However, performance of RUSBoost degrades slightly with D3 and \u03bbtest = 1 : 20,1 : 50 and 1 : 100. Performance of RB improves with D3 for all \u03bbtest, and with D1, but declines with D2. Based on the Table 4, the value of AUPR of Boosting ensembles after using the proposed loss factor does not change in most cases.\nUsing the proposed loss factor may improve the performance of the Boosting ensembles that rely on undersampling of data in terms of F-measure, especially for more difficult problems with overlapping data. The performance of Boosting ensembles that involve up-sampling of positive samples does not improve significantly.\nHowever, the use of proposed loss factor has no impact on the global performance of these Boosting ensembles in terms of AUPR. Therefore, the use of proposed loss factor performs similarly to adjusting the decision threshold of the Boosting algorithms to better account for imbalance.\nIn Table 5, the performance of baseline ensembles and their variants for different values of \u03b2 is compared for D2. The goal is to evaluate the effect of the value of \u03b2 on improving the performance when the proposed loss factor is used. This Table shows the performance only when \u03bbtest = 1 : 100 because the performance of baseline systems usually decline for higher skew levels of test data.\nEvaluation is done in terms of the same F\u03b2-measure that is used in loss factor calculation. The results are shown in terms of both FD and Fop. FD is the value of F-measure when the decisions of base classifiers are combined in Boosting ensembles and Fop is the value of F-measure when the scores of base classifiers are combined in Boosting ensembles and the optimal decision threshold of each ensemble is set to the point that maximizes F-measure when that ensemble is validated on an independent set of data (see section 4.2.1.).\nThe performance of Ada improves for most value of \u03b2 in terms of both FD and Fop. Some improvements are seen for SMT and RB in terms of Fop, but FD tend to stay the same in most cases and decrease in some cases. The performance of RUS improves for \u03b2 = 1 and 2 in terms of both FD and Fop, and the improvement tends to decrease for higher \u03b2 values. This was expected, since using higher values of \u03b2 to calculate F-measure means giving more importance to recall than precision. Therefore, the impact of imbalance is masked when higher values of \u03b2 is used and the performance may not change when the loss factor is calculated based on F-measure. For each of the classification systems, the same reason result in higher values of FD and Fop with higher values of \u03b2. Comparing FD and Fop of each ensemble for each value of \u03b2 shows that selecting the proper decision threshold can improve the performance in terms of accuracy and robustness, especially for lower values of \u03b2.\nThe results are not shown in terms of AUPR because AUPR does not change with variations in the value of \u03b2,\nsince the importance of recall and precision stays the same and equal in obtaining AUPR."}, {"heading": "5.1.2. Impact of progressive partitioning in RUSBoost", "text": "In this section, progressive partitioning is integrated into RUS without the use of F-measure in loss factor calculation. It is observed in Table 6 that robustness of RUS improves significantly after using this method of sampling. Indeed, using all samples for training through partitioning avoids loss of information and may improve the classification accuracy. In addition, validating on different imbalance levels of data increases the robustness to variations in the imbalance level of test data.\nThe performance of PRUS and PCUSi is significantly better than RUS in terms of both F-measure and AUPR, especially with D2 and D3. Partitioning these datasets using PCUSi, and random under-sampling without replacement is more effective in improving the performance of RUS."}, {"heading": "5.1.3. Impact of progressive partitioning and loss factor combined", "text": "In this section progressive partitioning and the proposed loss factor are integrated into RUS algorithm, resulting in PRUS-F, PCUS-F, and PCUSi-F. Over all ranges of skew and overlap in Table 7, PCUSi-F outperforms other classification systems and PRUS-F takes the second place, especially for higher levels of imbalance in test data. Combining the use of F-measure and progressive partitioning is more effective in increasing performance and robustness compared to using each of them independently because accuracy and robustness to imbalance improve at the same time, not separately, during learning process. If the negative class is partitioned a priori (CUSi), PBoost performs significantly better than the case when general partitioning techniques (RUS and CUS) are used."}, {"heading": "5.2. Results of Experiments with Video Data", "text": "Similarly to the synthetic data sets, the results of experiments on video dataset are shown in three parts, assessing the impact of: (1) using the proposed loss factor on the performance of baseline Boosting ensembles, (2) integrating progressive partitioning into RUS, and (3) using the proposed loss factor and progressive partitioning compared with the baseline and state of the art Boosting ensembles.\nFrom Table 8, the performance level of all ensembles is lower when the skew level of training data is higher. This is despite the fact that when the imbalance of training data is lower, the data that is used to test classifiers contain samples from some individuals that are not in the training data. Using the proposed loss factor improves the performance of Ada, RUS and SMT in terms of F-measure, and has no impact on the performance of RB in most cases of skew between classes in training and testing data. The performance of these ensembles after using the proposed loss factor does not change in terms of AUPR and therefore they are not shown here. In fact, the use of proposed loss factor performs similarly to adjusting the decision threshold of the Boosting algorithms to better account for imbalance and therefore may improve the performance only in terms of local performance metrics like F-measure.\nAfter integrating the progressive partitioning in RUS using PRUS and PTUS, the performance of RUS increases and becomes more robust in terms of both F-measure and AUPR (see Table 9), especially when TUS is used for partitioning because validating base classifiers on different imbalance levels of imbalance result in more robust classification systems and using all samples for training through partitioning avoids loss of information and may improve the classification accuracy.\nComparing the performance of final PBoost variants with baseline ensembles in Table 10, PTUS-F outperforms all other approaches in terms of F-measure and AUPR. In some cases, RB performs the same as PTUS-F. From these results, it is observed that combining the use of F-measure and integration of progressive partitioning, is\nmore effective in increasing performance and robustness compared to using each of them independently. In the experiments on the video data, trajectory under-sampling is more effective when used in PBoost compared to random under-sampling without replacement and cluster under-sampling. This is the case when partitions of negative class are known a priori."}, {"heading": "5.3. Results of Experiments with KEEL Collection", "text": "In Table 11, the performance of baseline and proposed Boosting ensembles is compared in terms of F2-measure and AUPR for experiments with 21 KEEL datasets. The second column of the table shows the imbalance level of training and test data in each dataset that ranges between 1:9 and 1:29. In this table, for each dataset, the best\nvalues are bold and second-best values are italic-bold to show the first and second best classifiers, respectively. In terms of F-measure, PCUS has one of the two highest values for 17 datasets, RB has one of the two highest values for 15 datasets and PRUS has one of the two highest values for 13 datasets. RB is the best classifier for 14 datasets while PCUS and PRUS are the best for 5 and 7 datasets, respectively. In terms of AUPR, PCUS and RB have one of the two highest values for 17 datasets, and PRUS has one of the two highest values for 8 datasets. RB is the best classifier for 15 datasets while PCUS and PRUS are the best for 5 datasets. In the cases when no natural data partitioning is known a priori, clustering with k-means is more effective than random under-sampling because base classifiers are trained on different parts of feature space and therefore the ensemble have more diversity compared to the case when the base classifiers are trained on samples from all parts of feature space. More sophisticated clustering methods like kernel k-means and spectral clustering may be more suitable."}, {"heading": "5.4. Computational Complexity", "text": "In this section the time complexity needed to design and test the proposed and baseline Boosting ensembles are compared. To compare the training time and memory cost of these ensembles, the number of training samples is counted and to compare their validation time and memory cost the number of validation samples and the number of support vectors of base classifiers are considered.\nFigure 9 show the results obtained with data set D2 in our experiments. The number of training and validation samples, the average number of support vectors, and overall number of evaluations of the kernel function (nSV \u00b7 nval) is presented in Figure 9(a)-(d) to estimate and compare design time of the proposed and baseline Boosting ensembles. To compare the complexity of these classification systems during testing O(nSV) with a probe sample x, we compared the overall number of support vectors in these ensembles in Figure 9(e) because computing each SVM output requires nSV evaluations of the kernel function.\nGiven netr as the number of samples to train the e th classifier in the ensemble, Figure 9(a) shows \u2211Ee=1 netr. In\nFigure 9(b), \u2211Ee=1 neval is presented, where n e val is the number of samples that the e th classifier in the ensemble is validated with. Average number of support vectors in E classifiers of the ensembles are shown in Figure 9(c). Given neSV as the number of support vectors obtained after training the e th classifier in the ensemble, Figure 9(d) shows \u2211Ee=1 neval \u00b7neSV for each ensemble.\nIn terms of training (see Figure 9(a)), PRUS and RUS are under-sampling ensembles and have the lowest computational cost, while SMT and RB-Boost include up-sampling and are significantly more costly. Total number of validation samples is equal for Ada, SMT, RUS and RB, and total number of validation samples is less with PRUS\n(see Figure 9(b)). The average number of support vectors is higher for SMT (see Figure 9(c)) because the base classifiers in this ensemble are trained on higher number of samples. Therefore, SMT is the most costly method, in terms of validation (see Figure 9(d)). Note that time and memory required for partitioning in PRUS, and generating synthetic samples in SMT and RB-Boost is neglected here. Nevertheless, PRUS is the most efficient ensemble technique in terms of designing memory and time complexity.\nThe number of training and validation samples as well as the average number of support vectors is smaller\nwith PRUS and therefore, PRUS is less costly in terms of design time and memory complexity.\nIn terms of testing time complexity (see Figure 9(e)) PRUS and RUS have the lowest number of evaluations of the kernel function per probe sample. RB-Boost and SMT have the highest number of evaluations of the kernel function per probe sample.\nAlthough Ada is given the same ensemble size, it fails to generate enough classifiers 3 and consequently result in smaller number of support vectors. Therefore, the total number of validation and testing processes of Ada is lower than expected."}, {"heading": "5.5. Summary of Results", "text": "As a summary of results on synthetic, video and KEEL datasets, we observed that:\n1. Using the proposed loss factor calculation may reduce the bias of performance in Boosting ensembles and\nincrease the accuracy.\n2. Partitioning improves the performance of RUS in all cases in terms of both accuracy and robustness to\nimbalance.\n3AdaBoost failed many times to be generated because the training subset in step 2.i of Algo. 1 may contain only negative class samples during sampling or weight update in step 2.vii may lead to that due to unsuitable loss factor calculation in step 2.ivv. Nevertheless, only successful attempts are considered.\n3. Integrating both partitioning and the proposed loss factor outperforms state of the art Boosting ensembles,\nrelying on the choice of partitioning technique for each dataset such that:\n(a) With synthetic data, PCUSi-F outperforms all systems in terms of both F-measure and AUPR, while\nPRUS and PCUS outperform RUS in most cases of skew and overlap between classes.\n(b) With the video data, PTUS is more accurate than the state of the art ensembles, and PRUS as well as\nPCUS.\n(c) PCUS is one of the best two classifiers in most KEEL problems, and performs very closely to RB in\nterms of both F-measure and AUPR.\n4. PBoost is computationally less costly than the state of the art Boosting ensembles in terms of computational\ncomplexity.\nTherefore, PBoost is an effective approach in correct classification of data when data is imbalanced in comparison to the state of the art Boosting ensembles. This method relies on the choice of partitioning technique for each dataset and performs significantly better when a more suitable partitioning technique is used. In problems that the natural clusters are known, the performance is better than using the general partitioning methods such as random under-sampling without replacement or k-means clustering. Therefore, PBoost can be more efficient than baseline Boosting ensembles considering both accuracy and complexity factors."}, {"heading": "6. Conclusion", "text": "In this paper, a new Boosting ensemble algorithm named as PBoost is proposed to address imbalance based on the idea of modifying RUSBoost by (1) under-sampling the majority class using partitional techniques, (2) validating classifiers on a growing validation subset, and (3) using a more suitable loss factor calculation. The partitions enter the Boosting process progressively for designing classifiers over iterations to avoid information loss and to maintain diversity among them. Validating base classifiers on a growing number of negative samples makes the PBoost ensembles more robust to possible skew levels of data during operations in addition to lowering the computational complexity. The new loss factor defined in this Boosting ensemble handles bias of performance towards negative class, and guides the Boosting process in a more effective direction with the purpose of correctly classifying both classes. Experiments show that PBoost may perform differently with different techniques of partitioning for each dataset such that more suitable clustering result in better performance. Nevertheless, regardless of the partitioning method, the PBoost ensembles perform comparably to state of the art Boosting ensembles in terms of accuracy. In addition, PBoost has significantly lower computational complexity in both designing and testing stages compared to state of the art Boosting ensembles. The applicability of PBoost to multi-class classification problems and deployment of more sophisticated clustering methods in PBoost can be further investigated in a future work."}, {"heading": "Acknowledgement", "text": "This work was partially supported by the Natural Sciences and Engineering Research Council of Canada and\nMitacs."}], "references": [{"title": "An overview of ensemble methods for binary classifiers in multi-class problems: Experimental study on one-vs-one and one-vs-all schemes", "author": ["M. Galar", "A. Fern\u00e1ndez", "E. Barrenechea", "H. Bustince", "F. Herrera"], "venue": "Pattern Recognition 44 (8) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiclass imbalance problems: Analysis and potential solutions", "author": ["S. Wang", "X. Yao"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 42 (4) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning from imbalanced data", "author": ["H. He", "E.A. Garcia"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 21 (9) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A review on ensembles for the class imbalance problem: bagging", "author": ["M. Galar", "A. Fernandez", "E. Barrenechea", "H. Bustince", "F. Herrera"], "venue": "boosting-, and hybrid-based approaches, Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on 42 (4) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Skew-sensitive boolean combination for adaptive ensembles\u2013an application to face recognition in video surveillance", "author": ["P.V. Radtke", "E. Granger", "R. Sabourin", "D.O. Gorodnichy"], "venue": "Information Fusion 20 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive skew-sensitive ensembles for face recognition in video surveillance", "author": ["M. De-la Torre", "E. Granger", "R. Sabourin", "D.O. Gorodnichy"], "venue": "Pattern Recognition 48 (11) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Classifier ensembles with trajectory under-sampling for face re-identification", "author": ["R. Soleymani", "E. Granger", "G. Fumera"], "venue": "in: Proceedings of the International Conference on Pattern Recognition Applications and Methods-Volume 1, SCITEPRESS- Science and Technology Publications, Lda", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "in: Computational learning theory, Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "R", "author": ["Y. Freund"], "venue": "E. Schapire, et al., Experiments with a new boosting algorithm, in: Machine Learning and Applications, 1996. ICMLA\u201996 International Conference on", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Smoteboost: Improving prediction of the minority class in boosting", "author": ["N.V. Chawla", "A. Lazarevic", "L.O. Hall", "K.W. Bowyer"], "venue": "in: European Conference on Principles of Data Mining and Knowledge Discovery, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Msmote: improving classification performance when training data is imbalanced", "author": ["S. Hu", "Y. Liang", "L. Ma", "Y. He"], "venue": "in: Proceedings of the 2009 Second International Workshop on Computer Science and Engineering, Vol. 2, Citeseer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Cost-weighted boosting with jittering and over/under-sampling: Jous-boost", "author": ["D. Mease", "A. Wyner", "A. Buja"], "venue": "J. Machine Learning Research 8 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning from imbalanced data sets with boosting and data generation: the databoost-im approach", "author": ["H. Guo", "H.L. Viktor"], "venue": "ACM SIGKDD Explorations Newsletter 6 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Rusboost: A hybrid approach to alleviating class imbalance", "author": ["C. Seiffert", "T.M. Khoshgoftaar", "J. Van Hulse", "A. Napolitano"], "venue": "Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 40 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Eusboost: Enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling", "author": ["M. Galar", "A. Fern\u00e1ndez", "E. Barrenechea", "F. Herrera"], "venue": "Pattern Recognition 46 (12) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Random balance: Ensembles of variable priors classifiers for imbalanced data, Knowledge-Based Systems", "author": ["J.F. D\u0131\u0301ez-Pastor", "J.J. Rodr\u0131\u0301guez", "C. Garc\u0131\u0301a-Osorio", "L.I. Kuncheva"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adacost: misclassification cost-sensitive boosting", "author": ["W. Fan", "S.J. Stolfo", "J. Zhang", "P.K. Chan"], "venue": "in: Machine Learning and Applications, 1999. ICMLA\u201999 International Conference on", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "A comparative study of cost-sensitive boosting algorithms", "author": ["K.M. Ting"], "venue": "in: Machine Learning and Applications, 2000. International Conference on, Citeseer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Cost-sensitive boosting for classification of imbalanced data", "author": ["Y. Sun", "M.S. Kamel", "A.K. Wong", "Y. Wang"], "venue": "Pattern Recognition 40 (12) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluating boosting algorithms to classify rare classes: Comparison and improvements", "author": ["M.V. Joshi", "V. Kumar", "R.C. Agarwal"], "venue": "in: Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on, IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for bankruptcy prediction", "author": ["M.-J. Kim", "D.-K. Kang", "H.B. Kim"], "venue": "Expert Systems with Applications 42 (3) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review 33 (1-2) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Evolutionary undersampling for classification with imbalanced datasets: Proposals and taxonomy, Evolutionary computation", "author": ["S. Garc\u0131\u0301a", "F. Herrera"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "On predicting rare classes with svm ensembles in scene classification", "author": ["R. Yan", "Y. Liu", "R. Jin", "A. Hauptmann"], "venue": "in: Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, Vol. 3, IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Constructing support vector machine ensemble with segmentation for imbalanced datasets", "author": ["Q. Li", "B. Yang", "Y. Li", "N. Deng", "L. Jing"], "venue": "Neural Computing and Applications 22 (1) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern recognition letters 27 (8) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Theoretical analysis of a performance measure for imbalanced data", "author": ["V. Garc\u0131a", "R. Mollineda", "J. S\u00e1nchez"], "venue": "in: Proceedings of the 20th International Conference on Pattern Recognition (ICPR10)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A unified view of performance metrics: translating threshold choice into expected classification loss", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "Journal of Machine Learning Research 13 (Oct) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Precision-recall-gain curves: Pr analysis done right", "author": ["P. Flach", "M. Kull"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Transactions on neural networks 16 (3) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "The cmu face in action (fia) database", "author": ["R. Goh", "L. Liu", "X. Liu", "T. Chen"], "venue": "in: Analysis and Modelling of Faces and Gestures", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "S", "author": ["J. Alcal\u00e1", "A. Fern\u00e1ndez", "J. Luengo", "J. Derrac"], "venue": "Garc\u0131\u0301a, L. S\u00e1nchez, F. Herrera, Keel data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework, Journal of Multiple-Valued Logic and Soft Computing 17 (2-3) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Partially-supervised learning from facial trajectories for face recognition in video surveillance", "author": ["M. De-la Torre", "E. Granger", "P.V. Radtke", "R. Sabourin", "D.O. Gorodnichy"], "venue": "Information Fusion 24 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive ensembles for face recognition in changing video surveillance environments", "author": ["C. Pagano", "E. Granger", "R. Sabourin", "G.L. Marcialis", "F. Roli"], "venue": "Information Sciences 286 ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonlinear mapping for data structure analysis", "author": ["J.W. Sammon"], "venue": "IEEE Transactions on computers 100 (5) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1969}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "in: Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, Vol. 1, IEEE", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2001}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24 (7) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaboost with svm-based component classifiers", "author": ["X. Li", "L. Wang", "E. Sung"], "venue": "Engineering Applications of Artificial Intelligence 21 (5) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["V. L\u00f3pez", "A. Fern\u00e1ndez"], "venue": "Garc\u0131\u0301a, V. Palade, F. Herrera, An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics, Information Sciences 250 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal thresholding of classifiers to maximize f1 measure", "author": ["Z.C. Lipton", "C. Elkan", "B. Naryanaswamy"], "venue": "in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, Springer", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "classification problems using one-vs-all strategy when samples from one class are compared against all samples from all other classes [1, 2].", "startOffset": 134, "endOffset": 140}, {"referenceID": 1, "context": "classification problems using one-vs-all strategy when samples from one class are compared against all samples from all other classes [1, 2].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "Several approaches have been proposed in literature to design ensembles of classifiers using imbalanced data [3, 4].", "startOffset": 109, "endOffset": 115}, {"referenceID": 3, "context": "Several approaches have been proposed in literature to design ensembles of classifiers using imbalanced data [3, 4].", "startOffset": 109, "endOffset": 115}, {"referenceID": 4, "context": "Dynamic ensembles allow to adapt the selection and fusion of base classifiers during operations based on the estimated level of skew [5, 6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "Dynamic ensembles allow to adapt the selection and fusion of base classifiers during operations based on the estimated level of skew [5, 6].", "startOffset": 133, "endOffset": 139}, {"referenceID": 4, "context": "For example, in [5, 6] authors design base classifiers for a range of different levels of imbalance.", "startOffset": 16, "endOffset": 22}, {"referenceID": 5, "context": "For example, in [5, 6] authors design base classifiers for a range of different levels of imbalance.", "startOffset": 16, "endOffset": 22}, {"referenceID": 6, "context": "In contrast, using a static approach, the range of possible imbalance levels can be accounted for during design by training base classifiers on data subsets with different imbalance levels [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "Boosting [8, 9] is a common static ensemble method that has been modified in several ways to learn from imbalanced data (see a review by Galar et al.", "startOffset": 9, "endOffset": 15}, {"referenceID": 8, "context": "Boosting [8, 9] is a common static ensemble method that has been modified in several ways to learn from imbalanced data (see a review by Galar et al.", "startOffset": 9, "endOffset": 15}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 10, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 11, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 12, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 13, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 14, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 15, "context": "In data-level Boosting approaches, training data is rebalanced by up-sampling positive class, under-sampling negative class, or using both up-sampling and under-sampling[10, 11, 12, 13, 14, 15, 16].", "startOffset": 169, "endOffset": 197}, {"referenceID": 9, "context": "Up-sampling methods like SMOTEBoost [10] are often more accurate, but they are computationally complex.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "contrast, random under-sampling (RUS) [14] is more computationally efficient, but suffers from information loss.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "This issue can be avoided by adopting a cost-sensitive approach [17, 18, 19], that defines different misclassification costs for different classes and integrates these cost factors into Boosting learning process.", "startOffset": 64, "endOffset": 76}, {"referenceID": 17, "context": "This issue can be avoided by adopting a cost-sensitive approach [17, 18, 19], that defines different misclassification costs for different classes and integrates these cost factors into Boosting learning process.", "startOffset": 64, "endOffset": 76}, {"referenceID": 18, "context": "This issue can be avoided by adopting a cost-sensitive approach [17, 18, 19], that defines different misclassification costs for different classes and integrates these cost factors into Boosting learning process.", "startOffset": 64, "endOffset": 76}, {"referenceID": 19, "context": "In contrast, cost-free techniques modify learning algorithms by enhancing loss factor calculation without considering cost factors [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 20, "context": "In contrast, cost-free techniques modify learning algorithms by enhancing loss factor calculation without considering cost factors [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 3, "context": "Ensemble learning methods exploit one or a combination of aforementioned techniques [4] to handle imbalance.", "startOffset": 84, "endOffset": 87}, {"referenceID": 21, "context": "Classifier ensembles can provide higher accuracy and robustness than a single classifier system by combining diverse classifiers [22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 7, "context": "Boosting is a common static ensemble learning algorithm initiated with AdaBoost [8] and improved in AdaBoost.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "problems) [9] to effectively promote a weak learner that performs slightly better than random guessing into a", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "A popular up-sampling Boosting approach is SMOTEBoost [10] that", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "MSMOTEBoost [11] use modified", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Jous-Boost [12] oversample the positive class by duplicating it, instead of creating new samples, and introduce perturbation (jittering) to this data in order to avoid overfitting.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "DataBoost-IM [13] oversample difficult samples from both classes and integrates it into AdaBoost.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "In under-sampling Boosting category, RUSBoost [14] integrates random under-sampling (RUS) into AdaBoost.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "The sample selection paradigm in RUSBoost is managed in EUSBoost [15] to create less correlated subsets using evolutionary prototype selection [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "The sample selection paradigm in RUSBoost is managed in EUSBoost [15] to create less correlated subsets using evolutionary prototype selection [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "Some researchers combine SMOTE and RUS in AdaBoost to achieve greater diversity and avoid loss of information as in Random Balance Boosting (RB-Boost) [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 26, "endOffset": 37}, {"referenceID": 23, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 26, "endOffset": 37}, {"referenceID": 24, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 26, "endOffset": 37}, {"referenceID": 23, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "In partitional approaches [7, 24, 25] bootstraps are selected without replacement either randomly [24], by clustering [25] or based on a prior knowledge from the application (like trajectories in video surveillance applications such as face re-identification [7]).", "startOffset": 259, "endOffset": 262}, {"referenceID": 23, "context": "[24] the negative data is randomly decomposed into a number of subsets and each subset, combined with the positive samples, is used to train a classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] partition negative data by clustering it using k-means in the feature space and then create an ensemble from the classifiers trained on each negative cluster and the positive samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In [7], partitioning negative class is done by selecting samples from a set of trajectories that are formed based on the tracking information, as found in several video surveillance applications like face re-identification.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "Cost-sensitive Boosting methods including AdaCost [17], CSB [18] and AdaC [19], embed different misclassification cost factors into loss function or weight update formula of AdaBoost.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Cost-sensitive Boosting methods including AdaCost [17], CSB [18] and AdaC [19], embed different misclassification cost factors into loss function or weight update formula of AdaBoost.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Cost-sensitive Boosting methods including AdaCost [17], CSB [18] and AdaC [19], embed different misclassification cost factors into loss function or weight update formula of AdaBoost.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Given \u03bci as the cost factor of sample xi, in AdaCost [17], two cost adjustment functions are defined for each sample as \u03c6+ =\u22120.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "CSB [18] introduce two different cost factors for positive and negative classes as \u03bc+ = 1 and \u03bc\u2212 \u2265 1, respectively.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "In AdaC1, 2, 3 [19] cost factors are embedded into the weight update formula in three different ways.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "In RareBoost [20], two different \u03b1s are defined for positive and negative classes as:", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "[21] also define two different \u03b1es for positive and negative classes as:", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "ROC space does not adequately reflect the impact of imbalance [26] on performance because big variations in the number of misclassified negative class (FP) results in a small change in FPR, especially if a small increase in TPR can mask it.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "Some authors define variants of the existing performance metrics by accounting for \u03c0 [27, 28, 29].", "startOffset": 85, "endOffset": 97}, {"referenceID": 27, "context": "Some authors define variants of the existing performance metrics by accounting for \u03c0 [27, 28, 29].", "startOffset": 85, "endOffset": 97}, {"referenceID": 28, "context": "Some authors define variants of the existing performance metrics by accounting for \u03c0 [27, 28, 29].", "startOffset": 85, "endOffset": 97}, {"referenceID": 27, "context": "ple, in [28], the authors define a measure of expected accuracy in terms of AUC as \u03c0(1\u2212\u03c0)(2AUC\u22121)+1/2, and precision-recall gain (PRG) curve [29] normalize precision and recall in terms of \u03c0 as:", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "ple, in [28], the authors define a measure of expected accuracy in terms of AUC as \u03c0(1\u2212\u03c0)(2AUC\u22121)+1/2, and precision-recall gain (PRG) curve [29] normalize precision and recall in terms of \u03c0 as:", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "Masking the effect of imbalance in performance metrics as done in [27, 28, 29] can misguide the learning process of Boosting ensembles and bias the performance towards negative class.", "startOffset": 66, "endOffset": 78}, {"referenceID": 27, "context": "Masking the effect of imbalance in performance metrics as done in [27, 28, 29] can misguide the learning process of Boosting ensembles and bias the performance towards negative class.", "startOffset": 66, "endOffset": 78}, {"referenceID": 28, "context": "Masking the effect of imbalance in performance metrics as done in [27, 28, 29] can misguide the learning process of Boosting ensembles and bias the performance towards negative class.", "startOffset": 66, "endOffset": 78}, {"referenceID": 29, "context": "There are several possible ways to partition the negative samples into disjoint subsets in literature [30] e.", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "Two partitioning techniques have been used in literature to partition data to learn ensembles from imbalanced data: Random Under-Sampling without replacement (we call RUSwR in this paper) [24] , and Cluster Under-Sampling (CUS) [25].", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "Two partitioning techniques have been used in literature to partition data to learn ensembles from imbalanced data: Random Under-Sampling without replacement (we call RUSwR in this paper) [24] , and Cluster Under-Sampling (CUS) [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 6, "context": "For example, Trajectory Under-Sampling (TUS) is applicable in video surveillance applications where samples captured for a same individual are regrouped into a trajectory [7].", "startOffset": 171, "endOffset": 174}, {"referenceID": 28, "context": "When F\u03b2-measure is used as the evaluation metric, the base classifier to beat is the one that predicts everything as positive [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "M1 [9], and one state of the art method from each family of the data-level approaches reviewed in Section", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "2 including SMOTEBoost [10], RUSBoost [14], and RB-Boost [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "2 including SMOTEBoost [10], RUSBoost [14], and RB-Boost [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "2 including SMOTEBoost [10], RUSBoost [14], and RB-Boost [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "The datasets that are used for the experiments include: (1) A set of synthetic 2D data sets in which the level of skew and overlap between classes are controllable, (2) the Face In Action (FIA) video database [31] that emulates a passport checking scenario in face re-identification application, (3) a set of 21 real-world problems from the KEEL dataset repository [32].", "startOffset": 209, "endOffset": 213}, {"referenceID": 31, "context": "The datasets that are used for the experiments include: (1) A set of synthetic 2D data sets in which the level of skew and overlap between classes are controllable, (2) the Face In Action (FIA) video database [31] that emulates a passport checking scenario in face re-identification application, (3) a set of 21 real-world problems from the KEEL dataset repository [32].", "startOffset": 365, "endOffset": 369}, {"referenceID": 0, "context": "To generate the 2D synthetic data, M+ = 100 positive class samples are generated with a normal distribution as N(m+,\u03c3+), where m+ = (0,0) and \u03c3+ = [1 0 0 1 ] indicate the mean and covariance matrix of this distribution, respectively.", "startOffset": 147, "endOffset": 157}, {"referenceID": 0, "context": "To generate the 2D synthetic data, M+ = 100 positive class samples are generated with a normal distribution as N(m+,\u03c3+), where m+ = (0,0) and \u03c3+ = [1 0 0 1 ] indicate the mean and covariance matrix of this distribution, respectively.", "startOffset": 147, "endOffset": 157}, {"referenceID": 32, "context": "[33, 34, 7].", "startOffset": 0, "endOffset": 11}, {"referenceID": 33, "context": "[33, 34, 7].", "startOffset": 0, "endOffset": 11}, {"referenceID": 6, "context": "[33, 34, 7].", "startOffset": 0, "endOffset": 11}, {"referenceID": 30, "context": "FIA video database [31] contains video sequences that emulate a passport checking scenario.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "ROIs are converted to gray-scale and rescaled to 70\u00d7 70 pixels using Viola Jones algorithm [36] from this video.", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "Figure 5: Examples of 2D mapping of LBP feature vectors belonging to 8 individuals using Sammon mapping [35] on the left, and examples of 70 \u00d7 70 pixels ROIs in a trajectory captures with camera 3, during session one for ID010 with their frame numbers on the right.", "startOffset": 104, "endOffset": 108}, {"referenceID": 36, "context": "Binary Patterns (LBP) [37] histograms have been extracted as features.", "startOffset": 22, "endOffset": 26}, {"referenceID": 31, "context": "KEEL (Knowledge Extraction based on Evolutionary Learning) tool is an open source software that supports data management and a designer of experiments [32].", "startOffset": 151, "endOffset": 155}, {"referenceID": 37, "context": "We use SVM with RBF kernel [38] as the base classifier where K(x\u2032,x\u2032\u2032) = exp{\u2212\u2016x\u2032\u2212x\u2032\u2032\u20162/2\u03ba2}.", "startOffset": 27, "endOffset": 31}, {"referenceID": 38, "context": "The kernel parameter \u03ba is set as the average of the mean minimum distance between any two training samples and the scatter radius of the training samples in the input space [39].", "startOffset": 173, "endOffset": 177}, {"referenceID": 37, "context": "We used the LibSVM implementation of [38].", "startOffset": 37, "endOffset": 41}, {"referenceID": 39, "context": "In experiments with synthetic data, the overlap level between positive and negative classes are also varied because the issue of imbalance is related to the level of overlap between classes [41].", "startOffset": 190, "endOffset": 194}, {"referenceID": 8, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 311, "endOffset": 315}, {"referenceID": 9, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 433, "endOffset": 437}, {"referenceID": 13, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 533, "endOffset": 537}, {"referenceID": 13, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 649, "endOffset": 653}, {"referenceID": 15, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 763, "endOffset": 767}, {"referenceID": 15, "context": "Abbreviation Sampling method Boosting Ensemble Loss factor Data Ada: Resampling with replacement AdaBoost [9] Weighted accuracy Synthetic, Video, KEEL Ada-F: Resampling with replacement Modified AdaBoost [9] Proposed F-measure Synthetic, Video SMT: Synthetic minority over-sampling technique (SMOTE) SMOTEBoost [10] Weighted accuracy Synthetic, Video, KEEL SMT-F Synthetic minority over-sampling technique(SMOTE) Modified SMOTEBoost [10] Proposed F-measure Synthetic, Video RUS: Random under-sampling with replacement (RUS) RUSBoost [14] Weighted accuracy Synthetic, Video, KEEL RUS-F: Random under-sampling with replacement (RUS) Modified RUSBoost [14] Proposed F-measure Synthetic, Video RB: Combination of up-sampling (SMOTE) and under-sampling (RUS) RB-Boost [16] Weighted accuracy Synthetic, Video, KEEL RB-F: Combination of up-sampling (SMOTE) and under-sampling (RUS) Modified RB-Boost [16] Proposed F-measure Synthetic, Video PRUS: Random under-sampling without replacement (RUSwR) Progressive Boosting Weighted accuracy Synthetic, Video PRUS-F: Random under-sampling without replacement (RUSwR) Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUS: Selecting clusters found by k-means Progressive Boosting Weighted accuracy Synthetic, Video PCUS-F: Selecting clusters found by k-means Progressive Boosting Proposed F-measure Synthetic, Video, KEEL PCUSi: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Weighted accuracy Synthetic PCUSi-F: Selecting ideal clusters generated in synthetic dataset Progressive Boosting Proposed F-measure Synthetic PTUS: Selecting trajectories in video dataset Progressive Boosting Weighted accuracy Video PTUS-F: Selecting trajectories in video dataset Progressive Boosting Proposed F-measure Video", "startOffset": 893, "endOffset": 897}, {"referenceID": 27, "context": "ing condition or based on operating conditions: the cost proportions or skew levels [28].", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "The performance metrics that can be maximized to set the decision threshold are accuracy, Brier score, AUC, expected cost and F-measure [28, 42].", "startOffset": 136, "endOffset": 144}, {"referenceID": 40, "context": "The performance metrics that can be maximized to set the decision threshold are accuracy, Brier score, AUC, expected cost and F-measure [28, 42].", "startOffset": 136, "endOffset": 144}, {"referenceID": 37, "context": "The computational complexity of SVM implemented in LibSVM is evaluated in [38], as O(ntrd) per iteration I, where ntr is the training set size, and d is the number of features.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "In practice, pattern recognition applications often suffer from imbalanced data distributions between classes, which may vary during operations w.r.t. the design data. Two-class classification systems designed using imbalanced data tend to recognize the majority (negative) class better, while the class of interest (positive class) often has the smaller number of samples. Several data-level techniques have been proposed to alleviate this issue, where classifier ensembles are designed with balanced data subsets by up-sampling positive samples or under-sampling negative samples. However, some informative samples may be neglected by random under-sampling and adding synthetic positive samples through up-sampling adds to training complexity. In this paper, a new ensemble learning algorithm called Progressive Boosting (PBoost) is proposed that progressively inserts uncorrelated groups of samples into a Boosting procedure to avoid loosing information while generating a diverse pool of classifiers. Base classifiers in this ensemble are generated from one iteration to the next, using subsets from a validation set that grows gradually in size and imbalance. Consequently, PBoost is more robust when the operational data may have unknown and variable levels of skew. In addition, the computation complexity of PBoost is lower than Boosting ensembles in literature that use under-sampling for learning from imbalanced data because not all of the base classifiers are validated on all negative samples. In PBoost algorithm, a new loss factor is proposed to avoid bias of performance towards the negative class. Using this loss factor, the weight update of samples and classifier contribution in final predictions are set based on the ability to recognize both classes. Using the proposed loss factor instead of standard accuracy can avoid biasing performance in any Boosting ensemble. The proposed approach was validated and compared using synthetic data, videos from the Faces In Action dataset that emulates face re-identification applications, and KEEL collection of datasets. Results show that PBoost can outperform state of the art techniques in terms of both accuracy and complexity over different levels of imbalance and overlap between classes.", "creator": "LaTeX with hyperref package"}}}