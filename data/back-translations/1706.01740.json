{"id": "1706.01740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Label-Dependencies Aware Recurrent Neural Networks", "abstract": "In recent years, Recurrent Neural Networks (RNNs) have proven effective in several NLP tasks. Despite this great success, their ability to model sequence labeling is still limited, leading to solutions that combine RNNs with models that have already proven effective in this area, such as CRFs. In this work, we propose a solution that is much simpler but very effective: an evolution of the simple Jordanian RNN, where labels are injected into the network as input and converted into embeddings, in the same way as words. We compare this RNN variant with all other RNN models, Elman and Jordan RNN, LSTM and GRU, in two well-known Spoken Language Understanding (SLU) tasks.", "histories": [["v1", "Tue, 6 Jun 2017 13:10:49 GMT  (132kb,D)", "http://arxiv.org/abs/1706.01740v1", "22 pages, 3 figures. Accepted at CICling 2017 conference. Best Verifiability, Reproducibility, and Working Description award"]], "COMMENTS": "22 pages, 3 figures. Accepted at CICling 2017 conference. Best Verifiability, Reproducibility, and Working Description award", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yoann dupont", "marco dinarelli", "isabelle tellier"], "accepted": false, "id": "1706.01740"}, "pdf": {"name": "1706.01740.pdf", "metadata": {"source": "CRF", "title": "Label-Dependencies Aware Recurrent Neural Networks", "authors": ["Yoann Dupont", "Marco Dinarelli", "Isabelle Tellier"], "emails": ["yoa.dupont@gmail.com,", "marco.dinarelli@ens.fr,", "isabelle.tellier@univ-paris3.fr"], "sections": [{"heading": null, "text": "on several NLP tasks. Despite such great success, their ability to model sequence labeling is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF models."}, {"heading": "1 Introduction", "text": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10]. These models are particularly effective thanks to their recurrent architecture, which allows neural models to keep in memory past information and re-use it at the current processing step.\nar X\niv :1\n70 6.\n01 74\n0v 1\nIn the literature of RNNs applied to NLP, several architectures have been proposed. At first Elman and Jordan RNNs, introduced in [2, 1], and known also as simple RNNs, have been adapted to NLP. The difference between these two models is in the type of connection giving the recurrent character to these two architectures: in the Elman RNN the recursion is a loop at the hidden layer, while in the Jordan RNN it relies the output layer to the hidden layer. This last recursion allows to use at the current step labels predicted for previous positions in a sequence.\nThese two recurrent models have shown limitations in learning relatively long contexts [11]. In order to overcome this limitation the RNNs known as Long Short-Term Memory (LSTM) have been proposed [3]. Recently, a simplified and, apparently, more effective variant of LSTM has been proposed, using Gated Recurrent Units and thus named GRU [12].\nDespite outstanding performances on several NLP tasks, RNNs have not been explicitly adapted to integrate effectively label-dependency information in sequence labeling tasks. Their sequence labeling decisions are based on intrinsically local functions (e.g. the softmax). In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer. These models reach state-of-the-art performances, their evaluation however is not clear. In particular it is not clear if performances derive from the model itself, or thanks to particular experimental conditions. In [15] for example, the best result of POS tagging on the Penn Treebank corpus is an accuracy of 97.55, which is reached using word embeddings trained using GloVe [16], on huge amount of unlabeled data. The model of [15] without pre-trained embeddings reaches an accuracy of 96.9, which doesn\u2019t seem that outstanding if we consider that a CRF model dating from 2010, trained from scratch, without using any external resource, reaches an accuracy of 97.3 on the same data [17]. We achieved the same result on the same data with a CRF model trained from scratch using the incremental procedure described in [18]. Moreover, the first version of the network proposed in this paper, but using a sigmoid activation function and only the L2 regularization, tough with a slightly different data preprocessing, achieves an accuracy on the Penn Treebank of 96.9 [19].\nThe intuition behind this paper is that embeddings allow a fine and effective modeling not only of words, but also of labels and label dependencies, which are crucial in some tasks of sequence labeling. In this paper we propose, as alternative to RNN+CRF models, a variant of RNN allowing this more effective modeling. Surprisingly, a simple modification to the RNN architecture results in a very effective model: in our variant of RNN the recurrent connection connects the output layer to the input layer and, since the first layer is just a look-up table mapping discrete items into embeddings, labels predicted at the output layer are mapped into embeddings the same way as words. Label embeddings and word embeddings are combined at the hidden layer, allowing to learn relations between these two types of information, which are used to predict the label at current position in a sequence. Our intuition is that using several label embeddings as context, a RNN is able to model correctly label-dependencies, the same way as more sophisticated models explicitly designed for sequence labeling like CRFs [20].\nThis paper is a straight follow-up of [21]. Contributions with respect to that work are as follows:\ni) An analysis of performances of forward, backward and bidirectional models. ii) The use of ReLU hidden layer and dropout regularization [22] at the hidden and embedding layers for improved regularized models. iii) The integration of a character-level convolution layer. iv) An in-depth evaluation, showing the effect of different components and of different information level on the performance. v) A straightforward comparison of the proposed variant of RNN to Elman, Jordan, LSTM and GRU RNNs, showing that the new variant is at least as effective as the best RNN models, such as LSTM and GRU. Our variant is even more effective when taking label-dependencies into account is crucial in the task, proving that our intuition is correct.\nAn high level schema of simple RNNs and of the variant proposed in this paper is shown in figure 1, where w is the input word, y is the label, E, H , O and R are the model parameters, which will be discussed in the following sections.\nSince evaluations on tasks like POS tagging on the Penn Treebank are basically reaching perfection (state-of-the-art is at 97.55 accuracy), any new model would probably provide little or no improvement. Also, performances on this type of tasks seem to have reached a plateau, as models achieving 97.2 accuracy or even better, were already published starting from 2003 [23, 24]. We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].\nATIS is a relatively simple task and doesn\u2019t require a sophisticated modeling of label dependencies. This task allows to evaluate models in similar settings as tasks like POS tagging or Named Entity Recognition as defined in the CoNLL Shared Task 2003, both widely used as benchmarks in NLP papers. MEDIA is a very challenging task, where the ability of models to keep label dependencies into account is crucial to obtain good results.\nResults show that our new variant is as effective as the best RNN models on a simple task like ATIS, still having the advantage of being much simpler. On the MEDIA task however, our variant outperforms all the other RNNs by a large margin, and even sophisticated CRF models, providing the best absolute result ever achieved on this task.\nThe paper is organized as follows: In the next section we describe the RNNs used in the literature for NLP, starting from existing models to arrive at describing the new\nvariant we propose. In the section 3 we present the corpora used for evaluation, the experimental settings and the results obtained in several experimental conditions. We draw some conclusions in section 4."}, {"heading": "2 Recurrent Neural Networks (RNNs)", "text": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12]. We also describe training and inference procedures, and the RNN variant we propose."}, {"heading": "2.1 Elman and Jordan RNNs", "text": "Elman and Jordan RNNs are defined as follows:\nht Elman = \u03a6(R hElmant\u22121 +H It) (1) ht Jordan = \u03a6(R yt\u22121 +H It) (2)\nThe difference between these two models is in the way of computing hidden activities, while the output is computed in the same way:\nyt = softmax(O h \u2217 t) (3)\nh\u2217t and yt are respectively the hidden and output layer\u2019s activities 1, \u03a6 is an activation function, H , O and R are the parameters at the hidden, output and recurrent layer, respectively (biases are omitted to keep equations lighter). hElmant\u22121 is the hidden layer activity computed at previous time step and used as context in the Elman RNN, while yt\u22121 is the previous predicted labels, used as context in the Jordan RNN. It is the input, which is often the concatenation of word embeddings in a fixed window dw (for winDow of Words) around the current word wt to be labeled. We define as E(wi) the embedding of any word wi. It is then defined as: It = [Ew(wt\u2212dw)...Ew(wt)...Ew(wt+dw)] (4) where [ ] is the concatenation of vectors (or matrices in the following sections). The softmax function, given a set S of m numerical values vi, associated to discrete elements i \u2208 [1,m], computes the probability associated to each element as:\n\u2200i \u2208 [1,m] p(i) = e vi\u2211m\nj=1 e vj\nThis function allows to compute the probability associated to each label and choose as predicted label the one with the highest probability.\n2.2 Long Short-Term Memory (LSTM) RNNs While LSTM is often used as the name of the whole network, it just defines a different way of computing the hidden layer activities. LSTMs use gate units to control how past and present information affect the network\u2019s internal state, and a cell to store past information that is going to be used as context at the current processing step. Forget, input gates and cell state are computed as:\nft = \u03a6(Wfht\u22121 + UfIt) (5) it = \u03a6(Wiht\u22121 + UiIt) (6) c\u0302t = \u0393(Wcht\u22121 + UcIt) (7)\n1h\u2217 means the hidden layer of any model, as the output layer is computed in the same way for all networks described in this paper.\n\u0393 is used to indicate a different activation function from \u03a62. c\u0302t is actually an intermediate value used to update the cell state value as follows: ct = ft ct\u22121 + it c\u0302t (8) is the element-wise multiplication. Once these quantities have been computed, the output gate is computed and used to control the hidden layer activities at the current time step t:\not = \u03a6(Woht\u22121 + UoIt) (9) ht LSTM = ot \u03a6(ct) (10)\nOnce again (and in the remainder of the paper), biases are omitted to keep equations lighter. As we can see, each gate and the cell state have their own parameter matrices W and U , used for the linear transformation of the previous hidden state (ht\u22121) and the current input (It). The evolution of the LSTM layer named GRU (Gated Recurrent Units) [12], combines together forget and input gates, and the previous hidden layer with the cell state:\nzt = \u03a6(Wzht\u22121 + UzIt) (11) rt = \u03a6(Wrht\u22121 + UrIt) (12) h\u0302t = \u0393(W (rt ht\u22121) + UIt) (13) ht GRU = (1\u2212 zt) ht\u22121 + zt h\u0302t (14)\nGRU is thus a simplification of LSTM, it uses less units and it has less parameters to learn."}, {"heading": "2.3 LD-RNN : Label-Dependencies Aware Recurrent Neural Networks", "text": "The variant of RNN that we propose in this paper can be thought of as having a recurrent connection from the output to the input layer. Note that from a different perspective, this variant can just be seen as a Feed-Forward Neural Network (FFNN) using previous predicted labels as input. Since Jordan RNN has the same architecture, the only difference being that in contrast to Jordan models we embed labels, we still prefer talking about recurrent network. This simple modification to the architecture of the network has important consequences on the model.\nThe reason motivating this modification is that we want embeddings for labels and use them the same way as word embeddings. Like we mentioned in the introduction, the first layer is a look-up table mapping discrete, or one-hot3, representations into distributional representations.\nSuch representations can encode very fine syntactic and semantic properties, as it has already been proved by word2vec [28] or GloVe [16]. We want similar properties to be learned also for labels, so that to encode in label embeddings the label dependencies needed for sequence labeling tasks. In this paper we learn label embeddings from the sequences of labels associated to word sentences in annotated data. But this procedure\n2In the literature \u03a6 and \u0393 are the sigmoid and tanh, respectively 3The one-hot representation of a token represented by an index i in a dictionary, is a vector v of the same\nsize as the dictionary and assigned zero everywhere, except at position i where it is 1.\ncould be applied also when structured label information is available. We could thus exploit syntactic parse trees, structured named entities or entity relations for learning sophisticated label embeddings.\nThe idea of using label embeddings has been introduced in [29] for dependency parsing, resulting in a very effective parser. In this paper we go ahead with respect to [29] by using several label embeddings as context to predict the label at current position in a sequence. Also we pre-train label embeddings like it is usually done for words. As consequence, we learn first generic dependencies between labels without their interactions with words. Such interactions are then integrated and refined during the learning phase of the target sequence labeling task. For this ability to learn labeldependencies, we name our variant LD-RNN, standing for Label Dependencies aware RNN.\nUsing the same formalism as before, we define Ew the matrix for word embeddings, while El is the matrix for label embeddings. The word-level input to our RNN is It as for the other RNNs, while the label-level input is: Lt = [El(yt\u2212dl+1) El(yt\u2212dl+2) . . . El(yt\u22121)] (15) which is the concatenation of vectors representing the dl previous predicted labels (dl stands (for winDow of Labels)). The hidden layer activities of our RNN variant are computed as:\nht LD-RNN = \u03a6(H [ItLt]) (16)\nWe note that we could rewrite the equation above as \u03a6(HwIt + HlLt) with a similar formalism as before, the two equations are equivalent if we define H = [HwHl].\nThanks to the use of label embeddings and their combination at the hidden layer, our LD-RNN variant learns very effectively label dependencies. Since the other RNNs in general don\u2019t use explicitly the label information as context, they can predict incoherent label sequences. As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].\nAnother consequence of the modification introduced in our RNN variant is an improved robustness to prediction mistakes. Since we use several label embeddings as context (see Lt above), once the model has learned label embeddings, in the test phase it is unlikely that several prediction mistakes occur in the same context. Even in that case, thanks to properties encoded in the embeddings, mistaken labels have similar representations to correct labels, allowing the model to possibly predict correct labels. Reusing an example from [30]: if Paris is replaced by Rome in a text, this has no impact on several NLP tasks, as they are both proper nouns in POS tagging, localization in Named Entity Recognition etc. Using label embeddings provides the LD-RNN variant with the same robustness on the label side.\nWhile the traditional Jordan RNN uses also previous labels as context information, it has not the same robustness because of the poor label representation used in adaptations of this model to NLP tasks. In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.\nIn the latter case it is clear that a prediction mistake can have a bad impact in the context, as the only value being 1 in the one-hot representation would be in the wrong position. Instead, using the probability distribution may seem a kind of fazzy\nrepresentation over several labels, but we have found empirically that the probability is very sharp and picked on one or just few labels. In any case this representation doesn\u2019t provide the desired robustness that can be achieved with label embeddings.\nFrom another point of view, we can interpret the computation of the hidden activities in a Jordan RNN as using label embeddings. In the equation 2, the multiplication Ryt\u22121, since yt\u22121 is a sparse vector, can be interpreted as the selection of an embedding from R.\nEven with this interpretation there is a substantial difference between a Jordan RNN and our variant. In the Jordan RNN, once the label embedding has been computed with Ryt\u22121, the result is not involved in the linear transformation applied by the matrix H , which is only applied to the word-level input It. The result of this multiplication is added to Ryt\u22121 and then the activation function is applied.\nIn our variant in contrast, labels are first mapped into embeddings with E[yi]4. Word and label inputs It and Lt are then both transformed by multiplying byH , which is correctly dimensioned to apply the linear transformation on both inputs. In our variant thus, two different label transformations are always applied: i) the conversion from sparse to embedding representation; ii) the linear transformation by multiplying label embeddings by H ."}, {"heading": "2.4 Learning and Inference", "text": "We learn the LD-RNN variant like all the other RNNs, by minimizing the cross-entropy between the expected label lt and the predicted label yt at position t in the sequence, plus a L2 regularization term:\nC = \u2212lt log(yt) + \u03bb\n2 |\u0398|2 (17)\n\u03bb is a hyper-parameter to be tuned, \u0398 is a short notation for Ew, El, H,O. lt is the one-hot representation of the expected label. Since yt above is the probability distribution over the label set, we can see the output of the network as the probability P (i|It,Lt) \u2200i \u2208 [1,m], where It and Lt are the input of the network (words and labels), i is the index of one of the labels defined in the targeted task.\nWe can thus associate to the LD-RNN model the following decision function: argmaxi\u2208[1,m]P (i|It,Lt) (18)\nWe note that this is still a local decision function, as the probability of each label is normalized at each position of a sequence. Despite this, the use of label-embeddings Lt as context allows the LD-RNN to effectively model label dependencies. Since the other RNNs like Elman and LSTM don\u2019t use the label information in their context, their decision function can be defined as: argmaxi\u2208[1,m]P (i|It) (19) which can lead to incoherent predicted label sequences.\nWe use the traditional back-propagation algorithm with momentum to learn our networks [31]. Given the recurrent nature of the networks, the Back-Propagation Through Time (BPTT) is often used [32]. This algorithm consists in unfolding the RNN for N previous steps, N being a parameter to choose, and using thus the N previous inputs and hidden states to update the model\u2019s parameters. The traditional back-propagation\n4In our case, yi is explicitly converted from probability distribution to one-hot representation.\nalgorithm is then applied. This is equivalent to learn a feed-froward network of depth N . The BPTT algorithm is supposed to allow the network to learn arbitrary long contexts. However [5] has shown that RNNs for language modeling learn best with only N = 5 previous steps. This can be due to the fact that, at least in NLP, a longer context does not lead necessarily to better performances, as a longer context is also more noisy.\nSince the BPTT algorithm is quite expensive, [9] chose to explicitly use the contextual information provided by the recurrent connection, and to use the traditional back-propagation algorithm, apparently without performance loss.\nIn this paper we use the same strategy. When the contextual information is used explicitly in a Jordan RNN, the hidden layer state is computed as follows: ht = \u03a6(R[yt\u2212dl+1 yt\u2212dl+2 ... yt\u22121] +H It) (20) A similar modification can be applied also to Elman, LSTM and GRU RNNs to keep into account explicitly the previous hidden states. To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].\nFrom explanations above we can say that using explicit wide context of words and labels like we do in LD-RNN, can be seen as an approximation of the BPTT algorithm."}, {"heading": "2.5 Toward More Sophisticated Networks: Character-Level Convolution", "text": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words. Character-level information is indeed very useful to allow a model generalizing over rare inflected surface forms and even out-ofvocabulary words in the test phase. Word embeddings are in fact much less effective in such cases. The convolution over word characters provide also the advantage of being very general: it can be applied in the same way to different languages, allowing to re-use the same system on different languages and tasks.\nIn this paper we focus on a convolution layer similar to the one used in [7] for words. For any word w of length |w|, we define Ech(w, i) the embedding of the character i of the word w. We define Wch the matrix of parameters for the linear transformation applied by the convolution (once again we omit the associated bias). We compute a convolution of window size 2dc + 1 over characters of a word w as follows:\n\u2022 \u2200i \u2208 [1, |w|] Convi = Wch[Ech(w, i\u2212 dc); . . . Ech(w, i); . . . Ech(w, i+ dc)]\n\u2022 Convch = [Conv1 . . . Conv|w|]\n\u2022 Charw = Max(Convch)\nthe Max function is the so-called max-pooling [7]. While it is not strictly necessary mapping characters into embeddings, it would be probably less interesting applying the convolution on discrete representations. The matrix Convch is made of the concatenation of vectors returned from the application of the linear transformation Wch. Its size is thus |C| \u00d7 |w|, where |C| is the size of the convolution layer. The max-pooling computes the maxima over the word-length direction, thus the final output Charw has size |C|, which is independent from the word length. Charw can be interpreted as a\ndistributional representation of the word w encoding the information at w\u2019s character level. This is a complementary information with respect to word embeddings, which encode inter-word information, and provide the model with an information similar to what is provided by discrete lexical features like word prefixes, suffixes, capitalization information etc., plus information about morphologically correct words of a given language."}, {"heading": "2.6 RNN Complexities", "text": "The improved modeling of label dependencies in our LD-RNN variant is achieved at the cost of more parameters with respect to the simple RNN models. However the number of parameters is still much less than sophisticated networks like LSTM. In this section we provide a comparison of RNNs complexity in terms of the number of parameters.\nWe introduce the following symbols: |H| and |O| are the size of the hidden and output layers, respectively. The size of the output layer is the number of labels;N is the embedding size, in LD-RNN we use the same size for word and label embeddings; dw is the window size used for context words; and dl is the number of label embeddings we use as context in LD-RNN. We analyze the hidden layer of all networks, and the embedding layer for LD-RNN. The other layers are exactly the same for all the networks described in this paper.\nFor Elman and Jordan RNNs, the hidden layer has the following number of parameters, respectively:\n{|H| \u2217 |H|}R + {|H| \u2217 (2dw + 1)N}HElman {|O| \u2217 |H|}R + {|H| \u2217 (2dw + 1)N}HJordan\nSubscripts indicate from which matrix the parameters come. The factor (2dw + 1)N comes from the (2dw + 1) words used as input context and then mapped into embeddings. The factor |O| \u2217 |H| in Jordan RNN is due to the fact that the matrix R connects output and hidden layers.\nIn LD-RNN we have: {|O| \u2217N}El + {((2dw + 1 + dl)N) \u2217 |H|}HLD-RNN\nThe factor |O|\u2217N is due to the use of the matrixEl containing |O| label embeddings of size N . Since in this paper we chose N = |H| and |O| < |H|, and since in LDRNN we don\u2019t use any matrix R on the recurrent connection, the fact of using label embeddings doesn\u2019t increase the number of parameters of the LD-RNN variant.\nThe hidden layer of LD-RNN however is dimensioned to connect all the word and label embeddings to all the hidden neurons. As consequence in the matrix H we have dlN more parameters than in the matrix H of Elman and Jordan RNNs.\nIn LSTM and GRU RNNs we have two extra matrices W and U for each gate and for the cell state, used to connect the previous hidden layer and the current input, respectively. These two matrices contain thus |H|\u2217|H| and (2wd+1)N \u2217|H| parameters, respectively.\nUsing the same notation and the same settings as above, in the hidden layer of LSTM and GRU we have the following number of parameters:\n{4(|H| \u2217 |H|+ |U | \u2217 (2dw + 1)N)}HLSTM {3(|H| \u2217 |H|+ |U | \u2217 (2dw + 1)N)}HGRU\nThe 3 for GRU reflects the fact that this network uses only 2 gates and a cell state. It should be pointed out, however, that while we have been testing LSTM and GRU with a word window for a matter of fair comparison5, these layers are applied on the current word and the previous hidden layer only, without the need of a word window. This is because this layer learns automatically how to use previous word information. In such case the complexity of the LSTM layer reduces to {4(|H| \u2217 |H| + |U | \u2217 N)}HLSTM . If we choose |U | = |H|, such complexity is comparable to that of LD-RNN in terms of number of parameters (slightly less actually). The LSTM is still more complex however because the hidden layer computation requires 4 gates and the cell state (c\u0302t) computations (each involving 2 matrix multiplications), the update of the new cell state ct (involving also 2 matrix multiplications), and only after the hidden state can be computed. LD-RNN\u2019s hidden state, in contrast, requires only matrix rows selection and concatenation to compute It and Lt, which are very efficient operations, and then the hidden state can already be computed.\nAs consequence, while the variant of RNN we propose in this paper is more complex than simple RNNs, LSTM and GRU RNNs are by far the most complex networks.\n2.7 Forward, Backward and Bidirectional Networks The RNNs introduced in this paper are proposed as forward, backward and bidirectional models [34]. The forward model is what has been described so far. The architecture of the backward model is exactly the same, the only difference is that the backward model processes data from the end to the begin of sequences. Labels and hidden layers computed by the backward model can thus be used as future context in a bidirectional model.\nBidirectional models are described in details in [34]. In this paper we utilize the version using separate forward and backward models. The final output is computed as the geometric mean of the output of the two individual models, that is:\nyt = \u221a\nyft ybt where yft and y b t are the output of the forward and backward models, respectively.\nIn the development phase of our systems, we noticed no difference in terms of performance between the two types of bidirectional models described in [34]. We chose thus the version described above, since it allows to initialize all the parameters with the forward and backward models previously trained. As consequence the bidirectional model is very close to a very good optimum since the first learning iteration, and very few iterations are needed to learn the final model."}, {"heading": "3 Evaluation", "text": ""}, {"heading": "3.1 Corpora for Spoken Language Understanding", "text": "We evaluated our models on two tasks of Spoken Language Understanding (SLU) [25]:\n5Indeed we observed better performances when using a word window with respect to when using a single word\nThe ATIS corpus (Air Travel Information System) [26] was collected for building a spoken dialog system able to provide flight information in the United States.\nATIS is a simple task dating from 1993. Training data are made of 4978 sentences chosen among dependency-free sentences in the ATIS-2 and ATIS-3 corpora. The test set is made of 893 sentences taken from the ATIS-3 NOV93 and DEC94 data. Since there are not official development data, we taken a part of the training set for this purpose. The word and label dictionaries contain 1117 and 85 items, respectively. We use the version of the corpus published in [35], where some word classes are available, such as city names, airport names, time expressions etc. These classes can be used as features to improve the generalization of the model on rare or unseen words. More details about this corpus can be found in [26].\nAn example of utterance transcription taken from this corpus is \u201cI want all the flights from Boston to Philadelphia today\u201d. The words Boston, Philadelphia and today in the transcription are associated to the concepts DEPARTURE.CITY, ARRIVAL.CITY and DEPARTURE.DATE, respectively. All the other words don\u2019t belong to any concept, they are associated to the void concept named O (for Outside). This example show the simplicity of this task: the annotation is sparse, only 3 words of the transcription are associated to a non-void concept; there is no segmentation problem, as each concept is associate to one word. Because of these two characteristics, the ATIS task is similar on the one hand to a POS tagging task, where there is no segmentation of labels over multiple words; on the other hand it is similar to a linear Named Entity Recognition task, where the annotation is sparse.\nWe are aware of the existence of two version of the ATIS corpus: the official version published starting from [35], and the version associated to the tutorial of deep learning made available by the authors of [9].6. This last version has been modified, some proper nouns have been re-segmented (for example the token New-York has been replaced by two tokens New York), and a preprocessing has been applied to reduce the word dictionary (numbers have been converted into the conventional token DIGIT, and singletons of the training data, as well as out-of-vocabulary words of the developpement and test data, have been converted into the token UNK). Following the tutorial of [9] we have been able to download the second version of the ATIS corpus. However in this version word classes that are available in the first version are not given. We ran some experiments with these data, using only words as input. The results we obtained are comparable with those published in [36], in part from same authors of [9]. However without word classes we cannot fairly compare with works that are using them. In this paper we thus compare only with published works that used the official version of ATIS.\nThe French corpus MEDIA [27] was collected to create and evaluate spoken dialog systems providing touristic information about hotels in France. This corpus is made of 1250 dialogs collected with Wizard-of-OZ approach. The dialogs have been manually transcribed and annotated following a rich concept ontology. Simple semantic components can be combined to create complex semantic structures.7 The rich semantic annotation is a source of difficulties, but also the annotation of coreference\n6Available at http://deeplearning.net/tutorial/rnnslu.html 7For example the component localization can be combined with other components like city,\nrelative-distance, generic-relative-location, street etc.\nphenomena. Some words cannot be correctly annotated without knowing a relatively long context, often going beyond a single dialog turn. For example in the utterance transcription \u201cYes, the one which price is less than 50 Euros per night\u201d, the one is a mention of an hotel previously introduced in the dialog. Statistics on the corpus MEDIA are shown in table 2.\nThe task resulting from the corpus MEDIA can be modeled as a sequence labeling task by chunking the concepts over several words using the traditional BIO notation [37].\nThanks to the characteristics of these two corpora, together with their relatively small size which allows training models in a reasonable time, these two tasks provide ideal settings for the evaluation of models for sequence labeling. A comparative example of annotation, showing also the word classes available for the two tasks and mentioned above, is shown in the table 1."}, {"heading": "3.2 Settings", "text": "The RNN variant LD-RNN has been implemented in Octave8 using OpenBLAS for low-level computations9.\nLD-RNN models are trained with the following procedure:\n\u2022 Neural Network Language Models (NNLM), like the one described in [38], are\n8https://www.gnu.org/software/octave/; Our code is described at http://www.marcodinarelli.it/software.php and available upon request.\n9http://www.openblas.net; This library allows a speed-up of roughly 330\u00d7 on a single matrix-matrix multiplication using 16 cores. This is very attractive with respect to the speed-up of 380\u00d7 that can be reached with a GPU, keeping into account that both Octave and OpenBLAS are available for free.\ntrained for words and labels to generate the embeddings (separately).\n\u2022 Forward and backward models are trained using the word and label embeddings trained at previous step.\n\u2022 The bidirectional model is trained using as starting point the forward and backward models trained at previous step.\nWe ran also some experiments using embeddings trained with word2vec [28]. The results obtained are not significantly different from those obtained following the procedure described above. This outcome is similar to the one obtained in [10]. Since the tasks addressed in this paper are made of small data, we believe that any embedding is equally effective. In particular tools like word2vec are designed to work on relatively big amount of data. Results obtained with word2vec embeddings will not be described in the following sections.\nWe roughly tuned the number of learning epochs for each model on the development data of the addressed tasks: 30 epochs are used to train word embeddings, 20 for label embeddings, 30 for the forward and backward models, 8 for the bidirectional model (the optimum of this model is often reached at the first epoch on the ATIS task, between the 3rd and the 5th epoch on MEDIA). At the end of the training phase, we keep the model giving the best prediction accuracy on the development data. We stop training the model if the accuracy is not improved for 5 consecutive epochs (also known as Early stopping strategy [31]).\nWe initialize all the weights with the \u201cso called\u201d Xavier initialization [31], theoretically motivated in [39] as keeping the standard deviation of the weights during the training phase when using ReLU, which is the type of hidden layer unit we chose for our variant of RNN.\nWe also tuned some of the hyper-parameters on the development data: we found out that the best initial learning rate is 0.5, this is linearly decreased with a value computed as the ratio between the initial learning rate and the number of epochs (Learing Rate decay). We combine dropout and L2 regularization [31], the best value for the dropout probability is 0.5 at the hidden layer, 0.2 at the embedding layer on ATIS, 0.15 on MEDIA. The best coefficient (\u03bb) for the L2 regularization is 0.01 for all the models, except for the bidirectional model where the best is 3e\u22124.\nWe ran also some experiments for optimizing the size of the different layers. In order to minimize the time and the number of experiments, this optimization has been based on the result provided by the forward model on the two tasks, and using only words and labels as input (without word classes and character convolution, which were optimized separately). The best size for the embeddings and the hidden layer is 200 for both tasks. The best size for the character convolution layer is 50 on ATIS, 80 on MEDIA. In both cases, the best size for the convolution window is 1, meaning that characters are used individually as input to the convolution. A window of size 3 (one character on the left, one on the right, plus the current character) gives roughly the same results, we thus prefer the simpler model. With a window of size 5, results starts to slightly deteriorate.\nWe also optimized the size of the word and label context used in the LD-RNN variant. On ATIS the best word context size is 11 (5 on the lest, 5 on the right plus the\ncurrent word), the best label context size is 5. On MEDIA the best sizes are 7 and 5 respectively. These values are the same found in [10] and comparable to those of [36].\nThe best parameters found in this phase has been used to obtain baseline models. The goal was to understand the behavior of the models with the different level of information used: the word classes available for the tasks, and the character level convolution. Some parameters needed to be re-tuned, as we will describe later on.\nConcerning training and testing time of our models, the overall time to train and test forward, backward and bidirectional models, using only words and classes as input, is roughly 1 hour 10 minutes on MEDIA, 40 minutes on ATIS. These times go to 2 hours for MEDIA and 2 hours 10 minutes for ATIS, using also word classes and character convolution as input. All these times are measured on a Intel Xeon E5-2620 at 2.1 GHz, using 16 cores."}, {"heading": "3.3 Results", "text": "All the results shown in this section are averages over 6 runs. Embeddings were learned once for all experiments."}, {"heading": "3.3.1 Incremental Results with Different Level of Information", "text": "In this section we describe results obtained with incremental levels of information given as input to the models: i) Only words (previous labels are always given as input), indicated with Words in the tables; ii) words and classes Words+Classes; iii) words and character convolution Words+CC; iv) All possible inputs Words+Classes+CC.\nThe results obtained on the ATIS task are shown in the table 3, results on MEDIA are in table 4.\nResults in these tables show that models have a similar behavior on the two tasks. In particular on ATIS, adding the different level of information results improve progressively and the best performance is obtained integrating words, labels and character convolution, though some of the improvements do not seem statistically significant, taking into account the small size of this corpus.\nThis observation is confirmed by results obtained on MEDIA, where adding the character level convolution leads to a slight degradation of performances. In order to understand the reason of this behavior we analyzed the training phase on the two tasks. We found out that the main problem was an hidden layer saturation: with the number of hidden neurons chosen in the preliminary optimization phase using only words (and labels), the hidden layer was not able to model the whole information richness provided by all the inputs at the same time. We ran thus some experiments using a larger hidden layer with size 256, which gave the results shown in the two tables with the model LD-RNN Words+Classes+CC. For lack of time we did not further optimized the size of the hidden layer.\nBeyond all of that, results shown in the table 3 and 4 are very competitive, as we will discuss in the next section."}, {"heading": "3.3.2 Comparison with the State-of-the-Art", "text": "In this section we compare our results with the best results found in the literature. In order to be fair, the comparison is made using the same input information: words and classes. In the tables we use E-RNN for Elman RNN, J-RNN for Jordan RNN, I-RNN for the improved RNN proposed by [40].10\nIn order to give an idea of how our RNN variant compares to LSTM+CRF models like the one of [15], we ran an experiment on the Penn Treebank [41]. With a similar data pre-processing, exactly the same data split, using a sigmoid activation function, and using only words as input, the LD-RNN variant achieves an accuracy of 96.83. This is comparable to the 96.9 achieved by the LSTM+CRF model of [15] without pre-trained embeddings.11\nResults on the ATIS task are shown in table 5. On this task we compare to results published in [42] and [40].\nThe results in the table 5 show that all models obtain a good performance on this task, always higher than 94.5 F1. This confirm what we anticipated in the previous section concerning how easy is this task.\nThe GRU RNNs of [42] and our variant LD-RNN obtain equivalent results (95.53), which is slightly better than all the other models, in particular with the bidirectional models. This is a good outcome, as our variant of RNN obtains the same result as GRU while using much less parameters (see section 2.6 for RNNs complexity). Indeed LSTM and GRU are considered very effective models for learning very long contexts. The way they are used in [42] allows to learn long contexts on the input side (words), they are not adapted however to learn also long label contexts, which is what we do in this paper with our variant. The fact that the best word context on this task is made of 11 words, show that this is the most important information to obtain good results on this task. It is thus not surprising that the GRU RNN achieves such good performance.\nComparing our results on the ATIS task with those published in [40] with a Jordan RNN, which uses the same label context as our models, we can conclude that the\n10This is a publication in French, but results in the tables are easy to understand and directly comparable to our results.\n11We did not run further experiments because without a GPU, experiments on the Penn Treebank are still quite expensive.\nadvantage in the variant LD-RNN is given by the use of label embeddings and their combination at the hidden layer.\nThis conclusion is more evident if we compare results obtained with RNNs using label embeddings with the other RNNs on the MEDIA task. This comparison is shown in table 6. As we mentioned in the section 3.1, this task is very challenging for several reason, but in the context of this paper we focus on the label dependencies that we claim we can effectively model with our RNN variant.\nIn this context we note that a traditional Jordan RNN, the J-RNN of [40], which is the only traditional model to explicitly use previous label information as context, is more effective than the other traditional models, including LSTM and GRU (84.29 F1 with J-RNN, 83.63 with GRU, second best model among traditional RNNs). We note also that on MEDIA, CRFs, which are models specifically designed for sequence labeling, are by far more effective than the traditional RNNs (86.00 F1 with the CRF of [10]).\nThe only models outperforming CRFs on the MEDIA task are the I-RNN model of [40] and our LD-RNN variant, both using label embeddings.\nEven if results on MEDIA discussed so far are very competitive, this task has been designed for Spoken Language Understanding (SLU) [25]. In SLU the goal is to extract a correct semantic representation of a sentence, allowing a correct interpretation of the\nuser will by the spoken dialog system. While the F1 measure is strongly correlated with SLU evaluation metrics, the evaluation measure used most often in the literature is the Concept Error Rate (CER). CER is defined exactly in the same way as Word Error rate in automatic speech recognition, where words are replaced by concepts.12\nIn order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43]. This comparison is shown in table 7.\nThe best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.6, 11.5 and 11.7, respectively. These models use both word and classes, and a rich set of lexical features such like word prefixes, suffixes, word capitalization information etc. We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47]. We note also that comparing significance tests published in [43], a difference of 0.1 in CER is already statistically significant. Since results in this paper are higher, we hypothesize than even smaller gains are significant.\nOur best LD-RNN model achieve a CER of 10.41. To the best of our knowledge this is the best CER obtained on the MEDIA task with an individual model. Moreover, instead of taking the mean of CER of several experiments, following a strategy similar to [8], one can run several experiments and keep the model obtaining the best CER on the development data of the target task. Results obtained using this strategy are shown in table 7 between parenthesis. The best result obtained by our LD-RNN is a CER of 10.09, the best absolute result on this task so far, even better than the ROVER model [48] used in [45], which combines 6 individual models, including the individual CRF model achieving 10.6 CER."}, {"heading": "3.4 Results Discussion", "text": "In order to understand the high performances of the LD-RNN variant on the MEDIA task, we made some simple analyses on the model output, comparing them to the output of a Jordan RNN trained with our own system in the same conditions as LD-RNN models. The main difference between these two models is the general tendency of the Jordan RNN to split a single concept into two or more concepts, mainly for concepts instantiated by long surface forms, such like command-tache. This concept is used to mean the general user will in a dialog turn (e.g. Hotel reservation, Price information etc.). The Jordan RNN often split this concept into several concepts by introducing a void label, associated to a stop-word. This is due to the limitation of this model to take relatively long label context into account, even if it is the only traditional RNN using explicitly previous labels as context information.\nSurprisingly, LD-RNN never makes this mistake and in general never makes segmentation errors (concerning the BIO formalism). This can be due to two reasons. The first is that label embeddings learns similar representations for semantically similar labels. This allows the model to correctly predict start-of-concept (B) even if the target word has been seen in the training set only as continuation-of-concept (I), or viceversa, as the two labels acquire very similar representations. The second reason, which is not\n12The errors made by the system are classified as Insertions (I), Deletions (D) and Substitutions (S). The sum of these errors is divided by the number of concepts in the reference annotation (R): CER = I+D+S\nR .\nin mutual exclusion with the first, is that the model factorizes information acquired on similar words seen associated to start-of-concept labels. Thus if a word has not been seen associated to start-of-concept labels, but similar words do, the model is still able to provide the correct annotation. This second reason is what made neural networks popular for learning word embeddings in earlier publications [38]. In any case, in our experience, we never observed such precise behavior even with CRF models tuned for the MEDIA task. For this reason we believe LD-RNN deserves the name of Label Dependencies aware RNN.\nStill LD-RNN makes mistakes, which means that once a label annotation starts for a target word, even if the label is not the correct one, the same label is kept even if the following words provide evidence that the correct label is another one. LD-RNN tends to be coherent with previous labeling decisions. This behavior is due to the use of a local decision function which definitely relies heavily on the label embedding context, but it doesn\u2019t prevent the model from being very effective. Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15]. We leave this as future work."}, {"heading": "4 Conclusion", "text": "In this paper we proposed a new variant of RNN for sequence labeling using a wide context of label embeddings in addition to the word context to predict the next label in a sequence. We motivated our variant as being more effective at modeling label dependencies. Results on two Spoken Language Understanding tasks show that i) on a simple task like ATIS our variant achieves the same performance as much more complex models such as LSTM and GRU, which are claimed the most effective RNNs; ii) on the MEDIA task, where modeling label dependencies is crucial, our variant outperforms by a large margin all the other RNNs, including LSTM and GRU. When compared to the best models of the literature in terms of Concept Error Rate (CER), our RNN variant results to be more effective, achieving a state-of-the-art CER of 10.09."}, {"heading": "5 Acknowledgements", "text": "This work has been partially funded by the French ANR project Democrat ANR-15CE38-0008."}], "references": [{"title": "Serial order: A parallel, distributed processing approach", "author": ["M.I. Jordan"], "venue": "Advances in Connectionist Theory: Speech. Erlbaum,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "COGNITIVE SCIENCE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M.Y. Hwang", "Y. Shi", "D. Yu"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Trans. Neur. Netw", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "arXiv preprint", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "author": ["X. Ma", "E. Hovy"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Practical very large scale CRFs", "author": ["T. Lavergne", "O. Capp\u00e9", "F. Yvon"], "venue": "Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Models cascade for tree-structured named entity detection", "author": ["M. Dinarelli", "S. Rosset"], "venue": "Proceedings of International Joint Conference of Natural Language Processing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving recurrent neural networks for sequence labelling", "author": ["M. Dinarelli", "I. Tellier"], "venue": "CoRR abs/1606.02555", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "New recurrent neural network variants for sequence labeling", "author": ["M. Dinarelli", "I. Tellier"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Spoken language understanding: A survey", "author": ["R. De Mori", "F. Bechet", "D. Hakkani-Tur", "M. McTear", "G. Riccardi", "G. Tur"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Expanding the scope of the atis task: The atis-3 corpus", "author": ["D.A. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke-Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the Workshop on Human Language Technology. HLT \u201994,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Results of the french evalda-media evaluation campaign for literal understanding", "author": ["H. Bonneau-Maynard", "C. Ayache", "F. Bechet", "A. Denis", "A. Kuhn", "F. Lef\u00e8vre", "D. Mostefa", "M. Qugnard", "S. Rosset", "Servan", "J.S. Vilaneau"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "CoRR abs/1206.5533", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE. Volume", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1990}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P.C. Chiu", "E. Nichols"], "venue": "CoRR abs/1511.08308", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "Trans. Sig. Proc", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["C. Raymond", "G. Riccardi"], "venue": "Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Text chunking using transformation-based learning", "author": ["L. Ramshaw", "M. Marcus"], "venue": "Proceedings of the 3rd Workshop on Very Large Corpora,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Etude des reseaux de neurones recurrents pour etiquetage de sequences", "author": ["M. Dinarelli", "I. Tellier"], "venue": "Actes de la 23eme conf  \u221a \u00a9rence sur le Traitement Automatique des Langues Naturelles,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1993}, {"title": "A step beyond local observations with a dialog aware bidirectional GRU network for Spoken Language Understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Discriminative reranking for spoken language understanding", "author": ["M. Dinarelli", "A. Moschitti", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech and Language Processing (TASLP)", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Hypotheses selection criteria in a reranking framework for spoken language understanding", "author": ["M. Dinarelli", "S. Rosset"], "venue": "In: Conference of Empirical Methods for Natural Language Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Comparing stochastic approaches to spoken language understanding in multiple languages", "author": ["S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lef\u00e8vre", "P. Lehen", "R. De Mori", "A. Moschitti", "H. Ney", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech and Language Processing (TASLP)", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Large Margin Rank Boundaries for Ordinal Regression", "author": ["R. Herbrich", "T. Graepel", "Obermayer", "K. In"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Optimizing crfs for slu tasks in various languages using modified training criteria", "author": ["S. Hahn", "P. Lehnen", "G. Heigold", "H. Ney"], "venue": "Proceedings of the International Conference of the Speech Communication Assosiation (Interspeech),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (ROVER)", "author": ["J.G. Fiscus"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 1, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 2, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 55, "endOffset": 64}, {"referenceID": 3, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 4, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 5, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 6, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 7, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 8, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 9, "context": "In the last few years Recurrent Neural Networks (RNNs) [1, 2, 3] have proved very effective in several Natural Language Processing (NLP) tasks such as Part-of-Speech tagging (POS tagging), chunking, Named Entity Recognition (NER), Spoken Language Understanding (SLU), machine translation and even more [4, 5, 6, 7, 8, 9, 10].", "startOffset": 302, "endOffset": 324}, {"referenceID": 1, "context": "At first Elman and Jordan RNNs, introduced in [2, 1], and known also as simple RNNs, have been adapted to NLP.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "At first Elman and Jordan RNNs, introduced in [2, 1], and known also as simple RNNs, have been adapted to NLP.", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": "These two recurrent models have shown limitations in learning relatively long contexts [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "In order to overcome this limitation the RNNs known as Long Short-Term Memory (LSTM) have been proposed [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "Recently, a simplified and, apparently, more effective variant of LSTM has been proposed, using Gated Recurrent Units and thus named GRU [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 13, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "In order to overcome this limitation, sophisticated hybrid RNN+CRF models have been proposed [13, 14, 15], where the traditional output layer is replaced by a CRF neural layer.", "startOffset": 93, "endOffset": 105}, {"referenceID": 14, "context": "In [15] for example, the best result of POS tagging on the Penn Treebank corpus is an accuracy of 97.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "55, which is reached using word embeddings trained using GloVe [16], on huge amount of unlabeled data.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "The model of [15] without pre-trained embeddings reaches an accuracy of 96.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "3 on the same data [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "We achieved the same result on the same data with a CRF model trained from scratch using the incremental procedure described in [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "9 [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Our intuition is that using several label embeddings as context, a RNN is able to model correctly label-dependencies, the same way as more sophisticated models explicitly designed for sequence labeling like CRFs [20].", "startOffset": 212, "endOffset": 216}, {"referenceID": 20, "context": "This paper is a straight follow-up of [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "ii) The use of ReLU hidden layer and dropout regularization [22] at the hidden and embedding layers for improved regularized models.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "2 accuracy or even better, were already published starting from 2003 [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 23, "context": "2 accuracy or even better, were already published starting from 2003 [23, 24].", "startOffset": 69, "endOffset": 77}, {"referenceID": 24, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "We propose instead to evaluate all the models on two different and widely used tasks of Spoken Language Understanding [25], which provide more variate evaluation settings: ATIS [26] and MEDIA [27].", "startOffset": 192, "endOffset": 196}, {"referenceID": 0, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 94, "endOffset": 100}, {"referenceID": 1, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 94, "endOffset": 100}, {"referenceID": 2, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "In this section we describe the most popular RNNs used for NLP, such as Elman and Jordan RNNs [1, 2], and the most sophisticated RNNs like LSTM and GRU [3, 12].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "The evolution of the LSTM layer named GRU (Gated Recurrent Units) [12], combines together forget and input gates, and the previous hidden layer with the cell state:", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "Such representations can encode very fine syntactic and semantic properties, as it has already been proved by word2vec [28] or GloVe [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Such representations can encode very fine syntactic and semantic properties, as it has already been proved by word2vec [28] or GloVe [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "The idea of using label embeddings has been introduced in [29] for dependency parsing, resulting in a very effective parser.", "startOffset": 58, "endOffset": 62}, {"referenceID": 28, "context": "In this paper we go ahead with respect to [29] by using several label embeddings as context to predict the label at current position in a sequence.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 13, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 14, "context": "As we already mentioned, this limitation lead research toward hybrid RNN+CRF models [13, 14, 15].", "startOffset": 84, "endOffset": 96}, {"referenceID": 29, "context": "Reusing an example from [30]: if Paris is replaced by Rome in a text, this has no impact on several NLP tasks, as they are both proper nouns in POS tagging, localization in Named Entity Recognition etc.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 8, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 9, "context": "In Jordan RNNs used for NLP like [8, 9, 10], labels are represented either with the probability distribution computed by the softmax, or with the one-hot representation computed from the probability distribution.", "startOffset": 33, "endOffset": 43}, {"referenceID": 30, "context": "We use the traditional back-propagation algorithm with momentum to learn our networks [31].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Given the recurrent nature of the networks, the Back-Propagation Through Time (BPTT) is often used [32].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "However [5] has shown that RNNs for language modeling learn best with only N = 5 previous steps.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Since the BPTT algorithm is quite expensive, [9] chose to explicitly use the contextual information provided by the recurrent connection, and to use the traditional back-propagation algorithm, apparently without performance loss.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 13, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "To our knowledge however, these networks are effectively learned using only one previous hidden state [13, 14, 15].", "startOffset": 102, "endOffset": 114}, {"referenceID": 12, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 32, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 13, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 14, "context": "Even if word embeddings provide a very fine encoding of word features, several works such like [13, 33, 14, 15] have shown that more effective models can be obtained using a convolution layer over characters of words.", "startOffset": 95, "endOffset": 111}, {"referenceID": 6, "context": "In this paper we focus on a convolution layer similar to the one used in [7] for words.", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "the Max function is the so-called max-pooling [7].", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "The RNNs introduced in this paper are proposed as forward, backward and bidirectional models [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "Bidirectional models are described in details in [34].", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "In the development phase of our systems, we noticed no difference in terms of performance between the two types of bidirectional models described in [34].", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "We evaluated our models on two tasks of Spoken Language Understanding (SLU) [25]:", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "The ATIS corpus (Air Travel Information System) [26] was collected for building a spoken dialog system able to provide flight information in the United States.", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "We use the version of the corpus published in [35], where some word classes are available, such as city names, airport names, time expressions etc.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "More details about this corpus can be found in [26].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "We are aware of the existence of two version of the ATIS corpus: the official version published starting from [35], and the version associated to the tutorial of deep learning made available by the authors of [9].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "We are aware of the existence of two version of the ATIS corpus: the official version published starting from [35], and the version associated to the tutorial of deep learning made available by the authors of [9].", "startOffset": 209, "endOffset": 212}, {"referenceID": 8, "context": "Following the tutorial of [9] we have been able to download the second version of the ATIS corpus.", "startOffset": 26, "endOffset": 29}, {"referenceID": 35, "context": "The results we obtained are comparable with those published in [36], in part from same authors of [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "The results we obtained are comparable with those published in [36], in part from same authors of [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 26, "context": "The French corpus MEDIA [27] was collected to create and evaluate spoken dialog systems providing touristic information about hotels in France.", "startOffset": 24, "endOffset": 28}, {"referenceID": 36, "context": "The task resulting from the corpus MEDIA can be modeled as a sequence labeling task by chunking the concepts over several words using the traditional BIO notation [37].", "startOffset": 163, "endOffset": 167}, {"referenceID": 37, "context": "\u2022 Neural Network Language Models (NNLM), like the one described in [38], are", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "We ran also some experiments using embeddings trained with word2vec [28].", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "This outcome is similar to the one obtained in [10].", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "We stop training the model if the accuracy is not improved for 5 consecutive epochs (also known as Early stopping strategy [31]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "We initialize all the weights with the \u201cso called\u201d Xavier initialization [31], theoretically motivated in [39] as keeping the standard deviation of the weights during the training phase when using ReLU, which is the type of hidden layer unit we chose for our variant of RNN.", "startOffset": 73, "endOffset": 77}, {"referenceID": 38, "context": "We initialize all the weights with the \u201cso called\u201d Xavier initialization [31], theoretically motivated in [39] as keeping the standard deviation of the weights during the training phase when using ReLU, which is the type of hidden layer unit we chose for our variant of RNN.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "We combine dropout and L2 regularization [31], the best value for the dropout probability is 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "These values are the same found in [10] and comparable to those of [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "These values are the same found in [10] and comparable to those of [36].", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "In the tables we use E-RNN for Elman RNN, J-RNN for Jordan RNN, I-RNN for the improved RNN proposed by [40].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "10 In order to give an idea of how our RNN variant compares to LSTM+CRF models like the one of [15], we ran an experiment on the Penn Treebank [41].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "10 In order to give an idea of how our RNN variant compares to LSTM+CRF models like the one of [15], we ran an experiment on the Penn Treebank [41].", "startOffset": 143, "endOffset": 147}, {"referenceID": 14, "context": "9 achieved by the LSTM+CRF model of [15] without pre-trained embeddings.", "startOffset": 36, "endOffset": 40}, {"referenceID": 41, "context": "On this task we compare to results published in [42] and [40].", "startOffset": 48, "endOffset": 52}, {"referenceID": 39, "context": "On this task we compare to results published in [42] and [40].", "startOffset": 57, "endOffset": 61}, {"referenceID": 41, "context": "The GRU RNNs of [42] and our variant LD-RNN obtain equivalent results (95.", "startOffset": 16, "endOffset": 20}, {"referenceID": 41, "context": "The way they are used in [42] allows to learn long contexts on the input side (words),", "startOffset": 25, "endOffset": 29}, {"referenceID": 39, "context": "Comparing our results on the ATIS task with those published in [40] with a Jordan RNN, which uses the same label context as our models, we can conclude that the", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "forward backward bidirectional [42] LSTM 95.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "23% [42] GRU 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "53% [40] E-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "71% [40] J-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "89% [40] I-RNN 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "forward backward bidirectional [10] CRF 86.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "00% [10] E-RNN 81.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "94% \u2013 \u2013 [10] J-RNN 83.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "25% \u2013 \u2013 [42] LSTM 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "07% [42] GRU 83.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "63% [40] E-RNN 82.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "13% [40] J-RNN 83.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "29% [40] I-RNN 84.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "Model CER [43] CRF 11.", "startOffset": 10, "endOffset": 14}, {"referenceID": 43, "context": "7% [44] CRF 11.", "startOffset": 3, "endOffset": 7}, {"referenceID": 44, "context": "5% [45] CRF 10.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In this context we note that a traditional Jordan RNN, the J-RNN of [40], which is the only traditional model to explicitly use previous label information as context, is more effective than the other traditional models, including LSTM and GRU (84.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "00 F1 with the CRF of [10]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "The only models outperforming CRFs on the MEDIA task are the I-RNN model of [40] and our LD-RNN variant, both using label embeddings.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Even if results on MEDIA discussed so far are very competitive, this task has been designed for Spoken Language Understanding (SLU) [25].", "startOffset": 132, "endOffset": 136}, {"referenceID": 44, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 193, "endOffset": 197}, {"referenceID": 43, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 199, "endOffset": 203}, {"referenceID": 42, "context": "12 In order to place our results on an absolute ranking among models designed for the MEDIA task, we propose a comparison in terms of CER to the best models published in the literature, namely [45], [44] and [43].", "startOffset": 208, "endOffset": 212}, {"referenceID": 44, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 40, "endOffset": 44}, {"referenceID": 43, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 46, "endOffset": 50}, {"referenceID": 42, "context": "The best individual models published by [45], [44] and [43] are CRFs, achieving a CER of 10.", "startOffset": 55, "endOffset": 59}, {"referenceID": 44, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 183, "endOffset": 191}, {"referenceID": 46, "context": "We note that the large gap between these CRF models is due to the fact that the CRF of [45] is trained with an improved margin criterion, similar to the large margin principle of SVM [46, 47].", "startOffset": 183, "endOffset": 191}, {"referenceID": 42, "context": "We note also that comparing significance tests published in [43], a difference of 0.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Moreover, instead of taking the mean of CER of several experiments, following a strategy similar to [8], one can run several experiments and keep the model obtaining the best CER on the development data of the target task.", "startOffset": 100, "endOffset": 103}, {"referenceID": 47, "context": "09, the best absolute result on this task so far, even better than the ROVER model [48] used in [45], which combines 6 individual models, including the individual CRF model achieving 10.", "startOffset": 83, "endOffset": 87}, {"referenceID": 44, "context": "09, the best absolute result on this task so far, even better than the ROVER model [48] used in [45], which combines 6 individual models, including the individual CRF model achieving 10.", "startOffset": 96, "endOffset": 100}, {"referenceID": 37, "context": "This second reason is what made neural networks popular for learning word embeddings in earlier publications [38].", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}, {"referenceID": 13, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}, {"referenceID": 14, "context": "Interestingly, this behavior suggests that LD-RNN could still benefit from a CRF neural layer like those used in [13, 14, 15].", "startOffset": 113, "endOffset": 125}], "year": 2017, "abstractText": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective on several NLP tasks. Despite such great success, their ability to model sequence labeling is still limited. This lead research toward solutions where RNNs are combined with models which already proved effective in this domain, such as CRFs. In this work we propose a solution far simpler but very effective: an evolution of the simple Jordan RNN, where labels are re-injected as input into the network, and converted into embeddings, in the same way as words. We compare this RNN variant to all the other RNN models, Elman and Jordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language Understanding (SLU). Thanks to label embeddings and their combination at the hidden layer, the proposed variant, which uses more parameters than Elman and Jordan RNNs, but far fewer than LSTM and GRU, is more effective than other RNNs, but also outperforms sophisticated CRF", "creator": "LaTeX with hyperref package"}}}