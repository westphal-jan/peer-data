{"id": "1706.02141", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on Sentiment Analysis", "abstract": "Syntactic parsing, the process of determining the internal structure of sentences in natural languages, is a critical task for artificial intelligence applications that need to extract meaning from the text or language of natural language. Sensation analysis is an example of applications for which parsing has recently proven useful.", "histories": [["v1", "Wed, 7 Jun 2017 12:03:07 GMT  (113kb,D)", "http://arxiv.org/abs/1706.02141v1", "19 pages. Submitted to Artificial Intelligence Review"], ["v2", "Mon, 2 Oct 2017 09:17:39 GMT  (113kb,D)", "http://arxiv.org/abs/1706.02141v2", "19 pages. Accepted for publication in Artificial Intelligence Review. DOI and link pending"], ["v3", "Tue, 24 Oct 2017 08:13:38 GMT  (113kb,D)", "http://arxiv.org/abs/1706.02141v3", "19 pages. Accepted for publication in Artificial Intelligence Review. This update only adds the DOI link to comply with journal's terms"]], "COMMENTS": "19 pages. Submitted to Artificial Intelligence Review", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["carlos g\\'omez-rodr\\'iguez", "iago alonso-alonso", "david vilares"], "accepted": false, "id": "1706.02141"}, "pdf": {"name": "1706.02141.pdf", "metadata": {"source": "CRF", "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on Sentiment Analysis", "authors": ["Carlos G\u00f3mez-Rod\u0155\u0131guez", "David Vilares"], "emails": ["carlos.gomez@udc.es"], "sections": [{"heading": null, "text": "In recent years, there have been significant advances in the accuracy of parsing algorithms. In this article, we perform an empirical, task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art sentiment analysis system that determines the polarity of sentences from their parse trees. In particular, we evaluate the system using four well-known dependency parsers, including both current models with state-of-the-art accuracy and more innacurate models which, however, require less computational resources.\nThe experiments show that all of the parsers produce similarly good results in the sentiment analysis task, without their accuracy having any relevant influence on the results. Since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms, this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser; and parsing researchers should investigate models that improve speed further, even at some cost to accuracy.\nKeywords Syntactic Parsing \u00b7 Sentiment Analysis \u00b7 Natural Language Processing \u00b7 Artificial Intelligence\nCarlos Go\u0301mez-Rodr\u0301\u0131guez has received funding from the European Research Council (ERC), under the European Union\u2019s Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), Ministerio de Econom\u0131\u0301a y Competitividad (FFI2014-51978-C22-R), and the Oportunius Program (Xunta de Galicia). Iago Alonso-Alonso was funded by an Oportunius Program Grant (Xunta de Galicia). David Vilares has received funding from the Ministerio de Educacio\u0301n, Cultura y Deporte (FPU13/01180) and Ministerio de Econom\u0131\u0301a y Competitividad (FFI2014-51978-C2-2-R).\nCarlos Go\u0301mez-Rodr\u0301\u0131guez FASTPARSE Lab, Grupo LyS, Departamento de Computacio\u0301n, Universidade da Corun\u0303a Campus de A Corun\u0303a s/n, 15071, A Corun\u0303a, Spain Tel.: +34 881 01 1396 Fax: +34 981 167 160 E-mail: carlos.gomez@udc.es\nar X\niv :1\n70 6.\n02 14\n1v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n7"}, {"heading": "1 Introduction", "text": "Having computers successfully understand the meaning of sentences in human languages is a long-standing key goal in artificial intelligence (AI). While full understanding is still far away, recent advances in the field of natural language processing (NLP) have made it possible to implement systems that can successfully extract relevant information from natural language text or speech. Syntactic parsing, the task of finding the internal structure of a sentence, is a key step in that process, as the predicate-argument structure of sentences encodes crucial information to understand their semantics. For example, a text mining system that needs to generate a report on customers\u2019 opinions about phones may find statements like \u201cthe iPhone is much better than the HTC 10\u201d and \u201cthe HTC 10 is much better than the iPhone\u201d, which are identical in terms of the individual words that they contain. It is the syntactic structure \u2013 in this case, the subject and the attribute of the verb to be \u2013 that tells us which of the phones is preferred by the customer.\nIn recent years, parsing has gone from a merely promising basic research field to see widespread use in useful AI applications such as machine translation (Miceli Barone and Attardi 2015; Xiao et al 2016), information extraction (Song et al 2015; Yu et al 2015), textual entailment recognition (Pado\u0301 et al 2015), learning for game AI agents (Branavan et al 2012) or sentiment analysis (Joshi and Penstein-Rose\u0301 2009; Vilares et al 2015b,a). Meanwhile, researchers have produced improvements in parsing algorithms and models that have increased their accuracy, up to a point where some parsers have achieved levels comparable to agreement between experts on English newswire text (Berzak et al 2016), although this does not generalize to languages that present extra challenges for parsing (Farghaly and Shaalan 2009) or to noisy text such as tweets (Kong et al 2014). However, parsers consume significant computational resources, which can be an important concern in large-scale applications (Clark et al 2009), and the most accurate models often come at a higher computational cost (Andor et al 2016; Go\u0301mez-Rodr\u0301\u0131guez 2016). Therefore, an interesting question is how much influence parsing accuracy has on the performance of downstream applications, as this can be essential to make an informed choice of a parser to integrate in a given system.\nIn this article, we analyze this issue for sentiment analysis (SA), i.e., the use of natural language processing to extract and identify subjective information (opinions about relevant entities) from natural language texts. Sentiment analysis is one of the most relevant practical applications of NLP, it has been recently shown to benefit from parsing (Socher et al 2013; Vilares et al 2015b) and it is especially useful at a large scale (as millions of texts of potential interest for opinion extraction are generated every day in social networks), making the potential accuracy vs. speed tradeoff especially relevant.\nFor this purpose, we take a state-of-the-art syntax-based sentiment analysis system (Vilares et al 2017), which calculates the polarity of a text (i.e., whether it expresses a positive, negative or neutral stance) relying on its dependency parse tree; and we test it with a set of well-known syntactic parsers, including models with state-of-the-art accuracy and others that are less accurate, but have a smaller computational cost, evaluating how the choice of parser affects the accuracy of the polarity classification. Our results show that state-of-the-art parsing accuracy does not provide additional benefit for this sentiment analysis task, as all of the parsers tested produce similarly good polarity classification accuracy\n(no statistically significant differences, all p-values \u2265 0.49). Therefore, our results suggest that it makes sense to use the fastest parsers for this task, even if they are not the most accurate.\nThe remainder of this article is organized as follows: we review the state of the art in syntactic parsing and syntax-based sentiment analysis in Section 2, we describe our experimental setup in Section 3, we report the results in Section 4, and discuss their implications in Section 5. Finally, Section 6 draws our conclusion and discusses possible avenues for future work."}, {"heading": "2 Background", "text": "We now provide an overview of research in parsing and sentiment analysis that is relevant to this study.\n2.1 Parsing\nDifferent linguistic theories define different ways in which the syntactic structure of a sentence can be described. In particular, the overwhelming majority of natural language parsers in the literature adhere to one of two dominant representations. In constituency grammar (or phrase structure grammar), sentences are analyzed by breaking them up into segments called constituents, which are in turn decomposed into smaller constituents, as in the example of Figure 1. In dependency grammar, the syntax of a sentence is represented by directed binary relations between its words, called dependencies, which are most useful when labeled with their syntactic roles, such as subject and object, as in Figure 2. Each of these representation types provides different information about the sentence, and it is not possible to fully map constituency to dependency representations or vice versa (Kahane and Mazziotta 2015).\nIn this paper we will focus on dependency parsing, as it is the predominant representation used by most of the downstream AI applications mentioned above \u2013 with machine translation as an arguable exception, where constituent parsing\nThe kid broke the red toy with a hammer\nsubj\ndobj\nadpmod\ndet det det amod\nadpobj\nFig. 2 A valid dependency parse for the sentence: \u2018The kid broke the red toy with a hammer\u2019 . The sentence is represented as a graph of binary relations between words that represent the existing syntactic relation between them (e.g. \u2018kid\u2019 is the subject of the verb \u2018broke\u2019)\nis often used due to the adequacy of phrase structure grammar for modeling reordering of words between languages (DeNeefe and Knight 2009; Xiao et al 2016) \u2013, and it is also the alternative used by the syntax-based SA system we will use in our experiments.\nMost dependency parsing systems in the literature can be grouped into two broad categories (McDonald and Nivre 2007): graph-based and transition-based (shift-reduce) parsers.\nGraph-based parsers use models that score dependency relations or groups of them, and perform a global search for a parse that will maximize the combined score of all dependencies. Under the assumption of projectivity (i.e., that there are no crossing dependencies), there are several dynamic programming algorithms that perform exact search in cubic time (Eisner 1996; Go\u0301mez-Rodr\u0301\u0131guez et al 2008), but this restriction is not realistic in practice (Go\u0301mez-Rodr\u0301\u0131guez 2016). Unfortunately, exact inference has been shown to be intractable for models that support arbitrary non-projectivity, except under strong independence assumptions (McDonald and Satta 2007) which enable parsing in quadratic time with maximum spanning tree algorithms (McDonald et al 2005), but severely limit the expressivity of the feature models that can be used. This restriction can be avoided by using so-called mildly non-projective parsing algorithms, which support the overwhelming majority of non-projective analyses that can be found in real linguistic structures (Go\u0301mezRodr\u0301\u0131guez et al 2011; Cohen et al 2011; Pitler et al 2013); but they have supercubic complexities that make them too slow for practical use. Another option is to forgo exact inference, using approximate inference algorithms with rich feature models instead. This is the approach taken by TurboParser (Martins et al 2010, 2013), which currently is the most popular graph-based parser as it can provide state-of-the-art accuracy with a reasonable computational cost.\nTransition-based parsers are based on a state machine that builds syntactic analyses step by step, typically from left to right. A statistical or machine learning model scores each of the possible transitions to take at each state, and a search strategy is used to find a high-scoring sequence of transitions. The earlier approaches to transition-based parsing, like the MaltParser system (Nivre et al 2007) used greedy deterministic search for this purpose, which is especially fast, but is prone to obtain suboptimal solutions due to bad decisions at early stages that result in error propagation. This problem is alleviated by instead performing beam search (Zhang and Nivre 2011) or dynamic programming (Huang and Sagae 2010; Kuhlmann et al 2011) to explore transition sequences, but this increases the computational cost. Other alternatives that provide a good speed-accuracy tradeoff are selectional branching, which uses confidence estimates to decide when to employ a beam (Choi and McCallum 2013), or dynamic oracles, which reduce\nthe error propagation in greedy search by exploring non-optimal transition sequences during training (Goldberg and Nivre 2012). In the last two years, several transition-based parsers have appeared that use neural networks as their scoring model (Chen and Manning 2014; Dyer et al 2015; Andor et al 2016), providing very good accuracy.\n2.2 Parsing Evaluation\nThe standard metrics to evaluate the accuracy of a dependency parser are the unlabeled attachment score (UAS: the proportion of words that are attached to the correct head word by means of a dependency, regardless of its label), labeled attachment score (LAS: the proportion of words that are attached to the correct head by means of a dependency that has the correct label) and label accuracy (LA: the proportion of words that are assigned the correct dependency type). However, the performance of a parser in terms of such scores is not necessarily proportional to its usefulness for a given task, as not all dependencies in a syntactic analysis are equally useful in practice, or equally difficult to analyze (Nivre et al 2010; Bender et al 2011). Therefore, LAS, UAS and LA are of limited use for researchers and practitioners that work with downstream applications in NLP. For this purpose, it is more useful to perform task-oriented evaluation, i.e., to experiment with the parsers in the actual tasks for which they are going to be used (Volokh and Neumann 2012).\nSuch evaluations have been performed for some specific NLP tasks, namely information extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment recognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation (Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011). However, these comparisons are currently somewhat dated, as they were performed before the advent of the major advances in parsing accuracy of the current decade reviewed in Section 2.1, such as beam-search transition-based parsing, dynamic oracles, approximate variational inference (TurboParser) or neural network parsing. Even more importantly, these analyses provide very different results depending on each specific task and, to our knowledge, no evaluation of parsers has been performed for sentiment analysis, a task where a good speed-accuracy tradeoff is especially important due to its extensive applications to the Web and social networks.\n2.3 Syntax-based Sentiment Analysis\nA number of state-of-the-art models for SA using different morphological (Khan et al 2016a,b) and syntactic approaches have proven useful in recent years. Liu et al (2016) pointed out the benefits of syntactical approaches with respect to statistical models on opinion target extraction, such as domain independence, and propose two approaches to select a set of rules, that even being suboptimal, achieve better results than a state-of-the-art conditional random field supervised method. Wu et al (2009) defined an approach to extract product features through phrase dependency parsing: they first combine the output of a shallow and a word-level dependency parser to then extract features and feed a support vector machine (SVM) with a novel tree kernel function. Their experimental results outperformed\na number of bag-of-words baselines. Jia et al (2009) and Asmi and Ishaya (2012) defined a set of syntax-based rules for identifying and handling negation on natural language texts represented as dependency trees. They also pointed out the advantage of using this kind of methods with respect to traditional lexicon-based perspectives in tasks such as opinion mining or information retrieval. Poria et al (2014) posed a set of syntax-based patterns for a concept-level approach to determine how the sentiment flows from concept to concept, assuming that such concepts present in texts are represented as nodes of a dependency tree.\nJoshi and Penstein-Rose\u0301 (2009) introduced the concept of generalized triplets, using them as features for a supervised classifier and showing its usefulness for subjectivity detection. Given a dependency triplet, the authors proposed to generalize the head or the dependent term (or even both at the same time) to its corresponding part-of-speech tag. Thus, the triplet (car, modified, good) could be generalized as (NOUN, modifier, good), which can be useful to correctly classify similar triplets that did not appear in the training set (e.g. (bicycle, modifier, good) or (job, modifier, good)). In a similar line, Vilares et al (2015c) enriched the concept of generalized dependency triplets and showed that they can be exploited as features to feed a supervised SA system for polarity classification, as long as enough labeled data is available. The same authors (Vilares et al 2015b) proposed an unsupervised syntax-based approach for polarity classification on Spanish reviews represented as Ancora trees (Taule\u0301 et al 2008). They showed that their system outperforms the equivalent lexical-based approach (Taboada et al 2011). In this line, however, Taboada et al (2011) pointed out that one of the challenges when using parsing techniques for sentiment analysis is the need of fast parsers that are able to process in real-time the huge amount of information shared by users in social media.\nWith the recent success of deep learning, Socher et al (2013) syntactically annotated a sentiment treebank to then train a recursive neural network that learns how to apply semantic composition for relevant phenomena in SA, such as negation or \u2018but\u2019 adversative clauses, over dependency trees. Kalchbrenner et al (2014) introduced a convolutional neural network for modeling sentences and used it for polarity classification among other tasks. Their approach does not explicitly rely on any parser, but the authors argue that one of the strengths of their model comes from the capability of the network to implicitly learn internal syntactic representations."}, {"heading": "3 Materials and Methods", "text": "We now describe the systems, corpora and methods used for our task-oriented evaluation.\n3.1 Parsing systems\n\u2013 MaltParser: Introduced by Nivre et al (2007), this system can be used to train transition-based parsers with greedy deterministic search. Although its accuracy has fallen behind the state of the art, it is still widely used, probably owing to its maturity and solid documentation. Additionally, due to its greedy nature,\nMaltParser is very fast. Following common practice, we use it together with the feature optimization tool MaltOptimizer1 (Ballesteros and Nivre 2012) to optimize the parameters and train a suitable model. The trained MaltParser model uses a standard arc-eager (transition-based) parsing algorithm, where at each step the movement to apply is selected among the set of possible transitions, previously scored by a linear model, which is faster than using models based on SVMs. \u2013 TurboParser (Martins et al 2013): A graph-based parser that uses approximate variational inference with non-local features. It has become the most widely used graph-based parser, as it provides better speed and accuracy than previous alternatives. We use its default configuration, training a second-order nonprojective parser with features for arcs, consecutive siblings and grandparents, using the AD3 algorithm as a decoder. \u2013 YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser, which uses beam search (Zhang and Nivre 2011) and dynamic oracles (Goldberg and Nivre 2012) to provide state-of-the-art accuracy. Its default configuration is used. \u2013 Stanford RNN Parser (Chen and Manning 2014): The most popular among the recent wave of transition-based parsers that employ neural networks, it can achieve robust accuracy in spite of using greedy deterministic search. We use pretrained GloVe (Pennington et al 2014) embeddings as input to the parser: in particular, 50-dimensional word embeddings2 trained on Wikipedia and the English Gigaword (Napoles et al 2012).\n3.2 Parsing corpus\nTo train and evaluate the parsing accuracy of such parsers, we are using the English Universal Treebank v2.0 created by McDonald et al (2013). It is a mapping from the (constituency) Penn treebank (Marcus et al 1993) to a universal dependency grammar annotation. The choice of the treebank is due to the already existing predefined compositional operations in the SA system used for evaluation (see \u00a73.3), that are intended for this type of universal guidelines. The corpus contains 39 833, 1 701 and 2 416 dependency trees for the training, development and test sets, respectively, and it represents one of the largest available treebanks for English.\n3.3 Sentiment analysis system\nFor the task-oriented evaluation, we will rely on UUUSA, the universal, unsupervised, uncovered approach for sentiment analysis described by (Vilares et al 2017), which is based on syntax and the concept of compositional operations. Briefly, given a text represented as a dependency tree, a compositional operation defines how a node of the tree modifies the semantic orientation (a real value representing a polarity and its strength) of a different branch or node, based on features such\n1 MaltParser often requires feature optimization to obtain acceptable results for the target language.\n2 http://nlp.stanford.edu/data/glove.6B.zip\nas its word form, part-of-speech tag or dependency type, without any limitation in terms of its location inside such tree. The associated system queues operations and propagates them through the tree, until the moment they must be dequeued and applied to their target. The model has outperformed other state-of-the-art lexicon-based methods on a number of corpora and languages, showing the advantages of using syntactic information for sentiment analysis. Due to the way the system works, in such a way that the application of the operation relies on previously assigning dependency types and heads correctly, it also constitutes a proper environment to test how parsing accuracy affects polarity classification.\nThe system already includes a predefined set of universal syntactic operations, that we are using in this study to determine the importance of parsing accuracy. For the sake of brevity, we are not detailing how the system computes the semantic orientation of the trees, but we specify which universal dependencies UUUSA is relying on to identify relevant linguistic phenomena that should trigger a compositional operation. To apply an operation, usually a dependency type must match at the node a branch is rooted at. The existing set of predefined operations that we are considering involve phenomena such as:\n\u2013 Intensification: A branch amplifies or decreases the semantic orientation of its head node or other branch (that must be labeled with the acomp (adjectival complement) dependency type). The intensifier branch must be labeled as one of these three dependency types: advmod (adverb modifier), amod (adjective modifier), nmod (noun modifier). Dependencies are relevant in this case because they help avoid false positive cases when applying intensification (e.g. in \u2018It is huge\u2019 , \u2018huge\u2019 should be (probably) a positive adjective, meanwhile in \u2018I have huge problems\u2019 it acts as an intensifier as it is an adjective modifier of the negative word \u2018problems\u2019 , and in \u2018I have huge exciting news\u2019 it acts again as an intensifier, but of a positive term). \u2013 \u2018But\u2019 clauses: To trigger this compositional operation, which decreases the relevance of the semantic orientation of the main sentence, the dependent branch rooted at \u2018but\u2019 must be labeled as cc. \u2013 Negation: The negating terms, that might shift the sentiment of other branches, are labeled in a dependency tree with the dependency type neg. \u2013 \u2018If\u2019 : We also include experiments using the proposed rule in Vilares et al (2017) for the \u2018if\u2019 clause, which is labeled with the mark dependency type, assuming that the part of the sentence under the scope of influence of the conditional clause should be ignored.\nTherefore, the accuracy obtained by UUUSA on the sentiment corpora is related to the parsing accuracy: a LAS of zero makes it impossible to trigger any compositional operation, since no dependency type would match; obtaining as output a global polarity which is the result of simply summing the semantic orientation of individual words.\nFigure 3.a) and Figure 3.b) illustrate two simple examples where part-of-speech tags, dependencies and types play a relevant role to accurately capture the semantic orientation of the sentence. Additionally, Figure 3.c) illustrates with an additional example how semantic composition is managed when a negation and an intensification appear in the same sentence and affect the same subjective word.\nWe chose this system among others for three main reasons:\n1. It supports separate compositional operations to address very specific linguistic phenomena, which can be enabled or disabled individually. This gives us great flexibility to carry out experiments including and excluding a number of linguistic constructions, allowing us to determine how relevant parsing accuracy is to tackle each of them. 2. It is a modular system where the parser is an independent component that can be swapped with another parser, allowing us to use it for task-oriented\nevaluation of various parsers. This contrasts with Socher et al (2013), a system that also uses syntax, but where the parsing process is tightly woven with the sentiment analysis process (a neural network architecture is trained to perform both tasks at the same time) so that it is not possible to use it with the output of external parsers. 3. Symbolic or knowledge-based systems like this perform robustly across different datasets and domains, which we cannot guarantee for the case of many machine learning models, that do not generalize so well (Aue and Gamon 2005; Taboada et al 2011; Vilares et al 2017).\n3.4 Sentiment analysis corpora\nThree standard corpora for document- and sentence-level sentiment analysis are used for the extrinsic evaluation:\n\u2013 Taboada and Grieve (2004) corpus: A general-domain dataset composed of 400 long reviews (50% positive, 50% negative) about different topics (e.g. washingmachines, books or computers). \u2013 Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50% positive, 50% negative). \u2013 Pang and Lee (2005) corpus: A collection of short (i.e. single-sentence) movie reviews. We relied on the test split used by Socher et al (2013), removing the neutral ones, as they did, for the binary classification task (1 821 subjective sentences: \u223c 49% positive, \u223c 51% negative).\n3.5 Experimental methodology\nThe aim of our experiments is to show how parsing accuracy influences polarity classification, following a task-oriented evaluation. To do so, we first compare the performance of different parsers on a standard treebank test set and metrics. We then extrinsically evaluate the performance of such parsers by parsing sentiment corpora, and using the obtained parse trees to determine the polarity of the texts in the corpora by means of a state-of-the-art syntax-based model. The performance of this model relies on previous correct assignment of dependency types and heads, to be able to handle relevant linguistic phenomena for the purpose at hand (e.g. intensification, \u2018but\u2019 clauses or negation). This makes it possible to relate parsing and syntax-based sentiment performance.\n3.6 Hardware and software used in the experiments\nExperiments were carried in a Dell XPS 8500 Intel Core i7 @ 3.4GHz and 16GB of memory RAM. Operating system was Ubuntu 14.04 64 bits."}, {"heading": "4 Results", "text": "Table 1 shows the performance obtained by the different parsers according to the standard metrics: LAS, UAS and LA. Table 2 illustrates how much time each\nparser consumes to analyze the Pang and Lee (2005) corpus, and the total time once the SA system is run on it.\nTables 3, 4 and 5 show the accuracy obtained by UUUSA on different sentiment corpora, when the output of each of the parsers is used as input to the syntax-based sentiment analysis system.3 We take accuracy as the reference metric for the SA systems, because it is the most suitable metric in this case, since the three corpora are balanced. In particular, we compare the performance when no syntactic rules are used (which would be equivalent to a lexicon-based system that only sums the semantic orientation of individual words), with respect to the one obtained when different rules are added. The aim is to determine if different parsers manage relevant linguistic phenomena in a different way.\nFinally, Figure 4 relates the LAS performance on the test set of the universal treebank with respect to the accuracy obtained by UUUSA, when we artificially reduce the training set size to simulate a low-accuracy parsing setting, as could happen in low-resource languages.\n3 The results obtained in these corpora are slightly different from the ones reported by Vilares et al (2017), due to the different tokenization techniques used in this work."}, {"heading": "5 Discussion", "text": "The results illustrated in Tables 1 and 2 indicate the relationship between the parsing time and accuracy. The slower parsers (Martins et al 2013; Rasooli and Tetreault 2015) tend to obtain a better performance, meanwhile the faster ones (Nivre et al 2007; Chen and Manning 2014) attain worse LAS, UAS and LA.\nThis fact is expected, as there is a well-known tradeoff between speed and accuracy in the spectrum of parsing algorithms, with one extreme at greedy search approaches that scan and parse the sentence in a single pass but are prone to error propagation, and the other at exact search algorithms that guarantee finding the highest-scoring parse under a rich statistical model, but are prohibitively slow (Choi and McCallum 2013; Volokh 2013; Go\u0301mez-Rodr\u0301\u0131guez 2016). The tendency remains when looking at the performance on individual dependency types (where also the head is assigned correctly).\nHowever, a better LAS or UAS does not necessary translate into a higher sentiment accuracy, which is shown in Tables 3, 4 and 5. In most cases, the performance obtained by the different parsers under the same sets of rules is practically equivalent. To confirm this statistically, we applied chi-squared significance tests to compare the outputs obtained using the different parsers for each given dataset and set of sentiment rules. No significant differences in sentiment accuracy were found in any of these experiments, which reinforces our conclusion. The minimum p-value obtained was 0.49. It is important to remark that this is very different from stating that parsing is not relevant for SA. In the case of UUUSA, Vilares et al (2017) already showed that their syntax-based SA approach is able to beat purely lexicon-based methods on a number of languages. In this line, Tables 3, 4 and 5 also show that the sets of syntactic rules outperform the baseline that does not use any syntactic-based rules (\u2018None\u2019 column) in almost all cases, proving again that syntax-based rules are useful to handle relevant linguistic phenomena in the field of SA.\nThe specific reasons that explain why the choice of syntactic parsing algorithm does not significantly affect accuracy lie out of the scope of our empirical work, as they require an exhaustive linguistic analysis. In view of the data, possible factors that may contribute are the following:\n\u2013 Low difficulty of some of the most decisive dependencies involved: as can be seen in Table 1, even the least accurate parsers analyzed are obtaining well over 92% precision and recall in adjectival modifiers (amod) and negations (neg), which are crucial for handling intensification and negation. This is likely because these tend to be short-distance dependencies, which are easier to parse (McDonald and Nivre 2007), and are common so they do not suffer from train-\ning sparsity problems. Thus, a highly accurate parser is not needed to detect these particular dependencies correctly. \u2013 Redundancy in sentences: a sentence may include several expressions of sentiment, so that even if the parse tree contains inaccuracies in a part of the sentence, we may still be able to extract the correct sentiment from the rest. This can be especially frequent in long sentences, which are the most difficult to parse (McDonald and Nivre 2007). \u2013 Irrelevance of fine-grained distinctions: in some cases, the parser provides more information than is strictly needed to evaluate the sentiment of a sentence. For example, the UUUSA rule for intensifiers works in the same way for adverbial modifiers (advmod), adjectival modifiers (amod) or nominal modifiers (nmod). Thus, if a parser mistakes e.g. an advmod for an amod, this counts as a parsing error, but has no influence in the sentiment output.\nHowever, verifying and quantifying the influence of each of these factors remains as an open question, which we would like to explore in the near future.\nAn interesting conclusion that could be extracted from these results is that parsing should prioritize speed over accuracy for syntax-based polarity classification. We draw Figure 4 to reinforce this hypothesis. The figure illustrates how LAS and sentiment accuracy vary when training the Stanford RNN parser (Chen and Manning 2014) with different training data size. To do so, we trained a number of parsers using the first x% of the training treebank. As expected, it was observed that adding more training data increased the LAS obtained by the parser. However, this same tendency did not remain with respect to sentiment accuracy, which remains stable once LAS reaches an acceptable level. Based on empirical evaluation, sentiment accuracy stops increasing when using the first 5% (82.57% LAS) or 10% (84.99% LAS) of the English Universal training treebank, with which is possible to already obtain a performance close to the state of the art (88.77% when using the whole training treebank). On the other hand, there is a clear increasing tendency when x < 5, because in those cases the LAS is still not good enough (using the first 0.1% and 1% of the training treebank we only are able to achieve a LAS of 51.39% and 75.58%, respectively)."}, {"heading": "6 Conclusions", "text": "In this article, we have carried out a task-oriented empirical evaluation to determine the relevance of parsing accuracy on the primary challenge of sentiment analysis: polarity classification. We chose English as the target language and trained a number of standard and freely available parsers on the Universal Dependency Treebank v2.0 (McDonald et al 2013). The output of such parsers on different standard sentiment corpora is then used as input for a state-of-the-art and syntax-based system that aims to classify the polarity of those texts. Experimental results let us draw two interesting and promising conclusions: (1) a better labeled/unlabeled attachment score on parsing does not necessarily imply a significantly better accuracy on polarity classification when using syntax-based algorithms and (2) parsing for sentiment analysis should focus on speed instead of accuracy, as a LAS of around 80% (which we obtained in the experiments by using only the first 10% of the training treebank) is already good enough to fully take advantage of dependency trees and exploit syntax-based rules. Using larger training portions produces\nincreases in the labeled attachment score up to the maximum value of close to 92% that we obtained with the most accurate parser, but the performance for sentiment accuracy remains stable. Hence, there is no reason to use a slower parser to maximize LAS as long as one is above said \u201cgood enough\u201d threshold for sentiment analysis, which is clearly surpassed by all the parsers tested.\nBased on the results, we believe there is room for improvements. We plan to design algorithms for faster parsing (Volokh 2013), prioritizing speed over accuracy. We also would like to explore the influence of parsing accuracy on other high-level tasks analysis, such as aspect extraction (Wu et al 2009) or question answering (Rajpurkar et al 2016), where dependencies have played an important role."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["D Andor", "C Alberti", "D Weiss", "A Severyn", "A Presta", "K Ganchev", "S Petrov", "M Collins"], "venue": null, "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Negation identification and calculation in sentiment analysis", "author": ["A Asmi", "T Ishaya"], "venue": "The Second International Conference on Advances in Information Mining and Management,", "citeRegEx": "Asmi and Ishaya,? \\Q2012\\E", "shortCiteRegEx": "Asmi and Ishaya", "year": 2012}, {"title": "Maltoptimizer: A system for maltparser optimization", "author": ["M Ballesteros", "J Nivre"], "venue": "S (eds) Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912),", "citeRegEx": "Ballesteros and Nivre,? \\Q2012\\E", "shortCiteRegEx": "Ballesteros and Nivre", "year": 2012}, {"title": "Parser evaluation over local and non-local deep dependencies in a large corpus", "author": ["EM Bender", "D Flickinger", "S Oepen", "Y Zhang"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bender et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bender et al\\.", "year": 2011}, {"title": "2016) Bias and agreement in syntactic annotations", "author": ["Y Berzak", "Y Huang", "A Barbu", "A Korhonen", "B Katz"], "venue": null, "citeRegEx": "Berzak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Berzak et al\\.", "year": 2016}, {"title": "Learning to win by reading manuals in a montecarlo framework", "author": ["SRK Branavan", "D Silver", "R Barzilay"], "venue": "J Artif Int Res", "citeRegEx": "Branavan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Branavan et al\\.", "year": 2012}, {"title": "Evaluating the impact of alternative dependency graph encodings on solving event extraction tasks", "author": ["E Buyko", "U Hahn"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,", "citeRegEx": "Buyko and Hahn,? \\Q2010\\E", "shortCiteRegEx": "Buyko and Hahn", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D Chen", "C Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Transition-based dependency parsing with selectional branching. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, pp 1052\u20131062, URL http://www.aclweb", "author": ["Choi JD", "McCallum"], "venue": null, "citeRegEx": "JD and McCallum,? \\Q2013\\E", "shortCiteRegEx": "JD and McCallum", "year": 2013}, {"title": "Large-scale syntactic processing: Parsing the web", "author": ["S Clark", "A Copestake", "JR Curran", "Y Zhang", "A Herbelot", "J Haggerty", "BG Ahn", "CV Wyk", "J Roesner", "J Kummerfeld", "T Dawborn"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2009}, {"title": "Exact inference for generative probabilistic non-projective dependency parsing", "author": ["SB Cohen", "C G\u00f3mez-Rod\u0155\u0131guez", "G Satta"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics,", "citeRegEx": "Cohen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Synchronous tree adjoining machine translation", "author": ["S DeNeefe", "K Knight"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,", "citeRegEx": "DeNeefe and Knight,? \\Q2009\\E", "shortCiteRegEx": "DeNeefe and Knight", "year": 2009}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["C Dyer", "M Ballesteros", "W Ling", "A Matthews", "NA Smith"], "venue": "Proceedings of the 53rd Annual Meeting", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["J Eisner"], "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING96),", "citeRegEx": "Eisner,? \\Q1996\\E", "shortCiteRegEx": "Eisner", "year": 1996}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Y Goldberg", "J Nivre"], "venue": "Proceedings of the 24th International Conference on Computational Linguistics (COLING), Association for Computational Linguistics,", "citeRegEx": "Goldberg and Nivre,? \\Q2012\\E", "shortCiteRegEx": "Goldberg and Nivre", "year": 2012}, {"title": "Restricted non-projectivity: Coverage vs. efficiency", "author": ["C G\u00f3mez-Rod\u0155\u0131guez"], "venue": "Comput Linguist 42(4):809\u2013817,", "citeRegEx": "G\u00f3mez.Rod\u0155\u0131guez,? \\Q2016\\E", "shortCiteRegEx": "G\u00f3mez.Rod\u0155\u0131guez", "year": 2016}, {"title": "A deductive approach to dependency parsing. In: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL\u201908:HLT), Association for Computational Linguistics, pp 968\u2013976, URL http://www.aclweb.org/anthology/P/P08/P08-1110 G\u00f3mez-Rod\u0155\u0131guez", "author": ["C G\u00f3mez-Rod\u0155\u0131guez", "J Carroll", "D Weir"], "venue": null, "citeRegEx": "G\u00f3mez.Rod\u0155\u0131guez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "G\u00f3mez.Rod\u0155\u0131guez et al\\.", "year": 2008}, {"title": "non-projective dependency parsing", "author": ["I Goto", "M Utiyama", "T Onishi", "E Sumita"], "venue": "Proceedings of the 13th Machine Translation Summit (MT Summit XIII), International Association for Machine Translation,", "citeRegEx": "Goto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goto et al\\.", "year": 2011}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["L Huang", "K Sagae"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Huang and Sagae,? \\Q2010\\E", "shortCiteRegEx": "Huang and Sagae", "year": 2010}, {"title": "The effect of negation on Sentiment Analysis and Retrieval Effectiveness", "author": ["L Jia", "C Yu", "W Meng"], "venue": "Proceeding of the 18th ACM conference on Information and knowledge management, ACM,", "citeRegEx": "Jia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2009}, {"title": "A Convolutional Neural Network for Modelling Sentences. In: The 52nd Annual Meeting of the Association for Computational Linguistics", "author": ["N Kalchbrenner", "E Grefenstette", "P Blunsom"], "venue": "Proceedings of the Conference. Volume 1: Long Papers,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "sentiment analysis and polarity classification", "author": ["FH Khan", "U Qamar", "S Bashir"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Khan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics, Doha, Qatar, pp 1001\u20131012", "author": ["M Kuhlmann", "C G\u00f3mez-Rod\u0155\u0131guez", "G Satta"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "Automated rule selection for opinion target extraction", "author": ["Q Liu", "Z Gao", "B Liu", "Y Zhang"], "venue": "Konwledge-Based Systems", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["MP Marcus", "MA Marcinkiewicz", "B Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "author": ["A Martins", "N Smith", "E Xing", "P Aguiar", "M Figueiredo"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,", "citeRegEx": "Martins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Turning on the turbo: Fast third-order nonprojective turbo parsers. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "author": ["A Martins", "M Almeida", "NA Smith"], "venue": null, "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Characterizing the errors of data-driven dependency parsing models", "author": ["R McDonald", "J Nivre"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "McDonald and Nivre,? \\Q2007\\E", "shortCiteRegEx": "McDonald and Nivre", "year": 2007}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["R McDonald", "G Satta"], "venue": "IWPT", "citeRegEx": "McDonald and Satta,? \\Q2007\\E", "shortCiteRegEx": "McDonald and Satta", "year": 2007}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R McDonald", "F Pereira", "K Ribarov", "J Haji\u010d"], "venue": "HLT/EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Universal Dependency Annotation for Multilingual Parsing", "author": ["R McDonald", "J Nivre", "Y Quirmbach-brundage", "Y Goldberg", "D Das", "K Ganchev", "K Hall", "S Petrov", "H Zhang", "O T\u00e4ckstr\u00f6m", "C Bedini", "N Castell\u00f3", "J Lee"], "venue": "Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Non-projective dependency-based pre-reordering with recurrent neural network for machine translation. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association for Computational Linguistics, Beijing, China, pp 846\u2013856, URL http://www.aclweb.org", "author": ["AV Miceli Barone", "G Attardi"], "venue": null, "citeRegEx": "Barone and Attardi,? \\Q2015\\E", "shortCiteRegEx": "Barone and Attardi", "year": 2015}, {"title": "Task-oriented evaluation of syntactic parsers and their representations", "author": ["Y Miyao", "R S\u00e6tre", "K Sagae", "T Matsuzaki", "J Tsujii"], "venue": "Proceedings of ACL-08: HLT, Association for Computational Linguistics,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "Annotated gigaword. In: Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, Association for Computational Linguistics, pp 95\u2013100", "author": ["C Napoles", "M Gormley", "B Van Durme"], "venue": null, "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["J Nivre", "J Hall", "J Nilsson", "A Chanev", "G Eryi\u01e7it", "S K\u00fcbler", "S Marinov", "E Marsi"], "venue": "Natural Language Engineering", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Evaluation of dependency parsers on unbounded dependencies", "author": ["J Nivre", "L Rimell", "R McDonald", "C G\u00f3mez Rod\u0155\u0131guez"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING", "citeRegEx": "Nivre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2010}, {"title": "Design and realization of a modular architecture for textual entailment", "author": ["S Pad\u00f3", "TG Noh", "A Stern", "R Wang", "R Zanoli"], "venue": "Natural Language Engineering", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2015}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B Pang", "L Lee"], "venue": "Proceedings of the 42nd annual meeting on Association", "citeRegEx": "Pang and Lee,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B Pang", "L Lee"], "venue": "Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Pang and Lee,? \\Q2005\\E", "shortCiteRegEx": "Pang and Lee", "year": 2005}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J Pennington", "R Socher", "CD Manning"], "venue": "In: EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Finding optimal 1-endpoint-crossing trees. Transactions of the Association of Computational Linguistics 1:13\u201324, URL http://aclweb.org/ anthology/Q13-1002", "author": ["E Pitler", "S Kannan", "M Marcus"], "venue": null, "citeRegEx": "Pitler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2013}, {"title": "Influence of parser choice on dependencybased mt", "author": ["M Popel", "D Mare\u010dek", "N Green", "Z Zabokrtsky"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Popel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Popel et al\\.", "year": 2011}, {"title": "Sentic patterns: Dependency-based rules for concept-level sentiment analysis", "author": ["S Poria", "E Cambria", "G Winterstein", "GB Huang"], "venue": "Knowledge-Based Systems", "citeRegEx": "Poria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Poria et al\\.", "year": 2014}, {"title": "The impact of parse quality on syntactically-informed statistical machine translation", "author": ["C Quirk", "S Corston-Oliver"], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Quirk and Corston.Oliver,? \\Q2006\\E", "shortCiteRegEx": "Quirk and Corston.Oliver", "year": 2006}, {"title": "2016) SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["P Rajpurkar", "J Zhang", "L Konstantin", "P Liang"], "venue": null, "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Yara parser: A fast and accurate dependency parser", "author": ["MS Rasooli", "JR Tetreault"], "venue": "CoRR abs/1503.06733,", "citeRegEx": "Rasooli and Tetreault,? \\Q2015\\E", "shortCiteRegEx": "Rasooli and Tetreault", "year": 2015}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R Socher", "A Perelygin", "J Wu", "J Chuang", "CD Manning", "A Ng", "C Potts"], "venue": "EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "PKDE4J: entity and relation extraction for public knowledge discovery", "author": ["M Song", "WC Kim", "D Lee", "GE Heo", "KY Kang"], "venue": "Journal of Biomedical Informatics 57:320\u2013332,", "citeRegEx": "Song et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "Analyzing appraisal automatically", "author": ["M Taboada", "J Grieve"], "venue": "Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Report SS0407),", "citeRegEx": "Taboada and Grieve,? \\Q2004\\E", "shortCiteRegEx": "Taboada and Grieve", "year": 2004}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M Taboada", "J Brooke", "M Tofiloski", "K Voll", "M Stede"], "venue": null, "citeRegEx": "Taboada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "AnCora: Multilevel Annotated Corpora for Catalan and Spanish", "author": ["M Taul\u00e9", "MA Mart\u0301\u0131", "M Recasens"], "venue": "Tapias D (eds) Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908),", "citeRegEx": "Taul\u00e9 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taul\u00e9 et al\\.", "year": 2008}, {"title": "2015a) A linguistic approach for determining the topics of Spanish Twitter messages", "author": ["D Vilares", "MA Alonso", "C G\u00f3mez-Rod\u0155\u0131guez"], "venue": "Journal of Information Science", "citeRegEx": "Vilares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2015}, {"title": "2015b) A syntactic approach for opinion mining on Spanish reviews", "author": ["D Vilares", "MA Alonso", "C G\u00f3mez-Rod\u0155\u0131guez"], "venue": "Natural Language Engineering", "citeRegEx": "Vilares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2015}, {"title": "On the usefulness of lexical and syntactic processing in polarity classification of Twitter messages. Journal of the Association for Information Science Science and Technology", "author": ["D Vilares", "MA Alonso", "C G\u00f3mez-Rod\u0155\u0131guez"], "venue": null, "citeRegEx": "Vilares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2015}, {"title": "Universal, unsupervised (rule-based), uncovered sentiment analysis. Knowledge-Based Systems 118:45\u201355", "author": ["D Vilares", "C G\u00f3mez-Rod\u0155\u0131guez", "MA Alonso"], "venue": "DOI https://doi.org/10", "citeRegEx": "Vilares et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Vilares et al\\.", "year": 2017}, {"title": "Task-oriented dependency parsing evaluation methodology", "author": ["A Volokh", "G Neumann"], "venue": "IEEE 13th International Conference on Information Reuse & Integration,", "citeRegEx": "Volokh and Neumann,? \\Q2012\\E", "shortCiteRegEx": "Volokh and Neumann", "year": 2012}, {"title": "Phrase Dependency Parsing for Opinion Mining", "author": ["Y Wu", "Q Zhang", "X Huang", "L Wu"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Syntactic skeleton-based translation", "author": ["T Xiao", "J Zhu", "C Zhang", "T Liu"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction. In: Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Denver, Colorado, pp 1374\u20131379", "author": ["M Yu", "MR Gormley", "M Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Semeval-2010 task 12: Parser evaluation using textual entailments", "author": ["D Yuret", "A Han", "Z Turgut"], "venue": "Proceedings of the 5th International Workshop on Semantic Eval-", "citeRegEx": "Yuret et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuret et al\\.", "year": 2010}, {"title": "Transition-based dependency parsing with rich non-local features. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, pp 188\u2013193, URL http://dl", "author": ["Y Zhang", "J Nivre"], "venue": null, "citeRegEx": "Zhang and Nivre,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Nivre", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "However, parsers consume significant computational resources, which can be an important concern in large-scale applications (Clark et al 2009), and the most accurate models often come at a higher computational cost (Andor et al 2016; G\u00f3mez-Rod\u0155\u0131guez 2016).", "startOffset": 215, "endOffset": 255}, {"referenceID": 11, "context": "is often used due to the adequacy of phrase structure grammar for modeling reordering of words between languages (DeNeefe and Knight 2009; Xiao et al 2016) \u2013, and it is also the alternative used by the syntax-based SA system we will use in our experiments.", "startOffset": 113, "endOffset": 155}, {"referenceID": 27, "context": "Most dependency parsing systems in the literature can be grouped into two broad categories (McDonald and Nivre 2007): graph-based and transition-based (shift-reduce) parsers.", "startOffset": 91, "endOffset": 116}, {"referenceID": 13, "context": ", that there are no crossing dependencies), there are several dynamic programming algorithms that perform exact search in cubic time (Eisner 1996; G\u00f3mez-Rod\u0155\u0131guez et al 2008), but this restriction is not realistic in practice (G\u00f3mez-Rod\u0155\u0131guez 2016).", "startOffset": 133, "endOffset": 174}, {"referenceID": 15, "context": ", that there are no crossing dependencies), there are several dynamic programming algorithms that perform exact search in cubic time (Eisner 1996; G\u00f3mez-Rod\u0155\u0131guez et al 2008), but this restriction is not realistic in practice (G\u00f3mez-Rod\u0155\u0131guez 2016).", "startOffset": 226, "endOffset": 248}, {"referenceID": 28, "context": "Unfortunately, exact inference has been shown to be intractable for models that support arbitrary non-projectivity, except under strong independence assumptions (McDonald and Satta 2007) which enable parsing in quadratic time with maximum spanning tree algorithms (McDonald et al 2005), but severely limit the expressivity of the feature models that can be used.", "startOffset": 161, "endOffset": 186}, {"referenceID": 60, "context": "This problem is alleviated by instead performing beam search (Zhang and Nivre 2011) or dynamic programming (Huang and Sagae 2010; Kuhlmann et al 2011) to explore transition sequences, but this increases the computational cost.", "startOffset": 61, "endOffset": 83}, {"referenceID": 18, "context": "This problem is alleviated by instead performing beam search (Zhang and Nivre 2011) or dynamic programming (Huang and Sagae 2010; Kuhlmann et al 2011) to explore transition sequences, but this increases the computational cost.", "startOffset": 107, "endOffset": 150}, {"referenceID": 14, "context": "the error propagation in greedy search by exploring non-optimal transition sequences during training (Goldberg and Nivre 2012).", "startOffset": 101, "endOffset": 126}, {"referenceID": 7, "context": "In the last two years, several transition-based parsers have appeared that use neural networks as their scoring model (Chen and Manning 2014; Dyer et al 2015; Andor et al 2016), providing very good accuracy.", "startOffset": 118, "endOffset": 176}, {"referenceID": 55, "context": ", to experiment with the parsers in the actual tasks for which they are going to be used (Volokh and Neumann 2012).", "startOffset": 89, "endOffset": 114}, {"referenceID": 6, "context": "Such evaluations have been performed for some specific NLP tasks, namely information extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment recognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation (Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011).", "startOffset": 96, "endOffset": 135}, {"referenceID": 55, "context": "Such evaluations have been performed for some specific NLP tasks, namely information extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment recognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation (Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011).", "startOffset": 168, "endOffset": 211}, {"referenceID": 43, "context": "Such evaluations have been performed for some specific NLP tasks, namely information extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment recognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation (Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011).", "startOffset": 236, "endOffset": 302}, {"referenceID": 1, "context": "Jia et al (2009) and Asmi and Ishaya (2012) defined a set of syntax-based rules for identifying and handling negation on natural language texts represented as dependency trees.", "startOffset": 21, "endOffset": 44}, {"referenceID": 1, "context": "Jia et al (2009) and Asmi and Ishaya (2012) defined a set of syntax-based rules for identifying and handling negation on natural language texts represented as dependency trees. They also pointed out the advantage of using this kind of methods with respect to traditional lexicon-based perspectives in tasks such as opinion mining or information retrieval. Poria et al (2014) posed a set of syntax-based patterns for a concept-level approach to determine how the sentiment flows from concept to concept, assuming that such concepts present in texts are represented as nodes of a dependency tree.", "startOffset": 21, "endOffset": 375}, {"referenceID": 2, "context": "Following common practice, we use it together with the feature optimization tool MaltOptimizer (Ballesteros and Nivre 2012) to optimize the parameters and train a suitable model.", "startOffset": 95, "endOffset": 123}, {"referenceID": 45, "context": "\u2013 YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser, which uses beam search (Zhang and Nivre 2011) and dynamic oracles (Goldberg and Nivre 2012) to provide state-of-the-art accuracy.", "startOffset": 13, "endOffset": 41}, {"referenceID": 60, "context": "\u2013 YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser, which uses beam search (Zhang and Nivre 2011) and dynamic oracles (Goldberg and Nivre 2012) to provide state-of-the-art accuracy.", "startOffset": 100, "endOffset": 122}, {"referenceID": 14, "context": "\u2013 YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser, which uses beam search (Zhang and Nivre 2011) and dynamic oracles (Goldberg and Nivre 2012) to provide state-of-the-art accuracy.", "startOffset": 143, "endOffset": 168}, {"referenceID": 7, "context": "\u2013 Stanford RNN Parser (Chen and Manning 2014): The most popular among the recent wave of transition-based parsers that employ neural networks, it can achieve robust accuracy in spite of using greedy deterministic search.", "startOffset": 22, "endOffset": 45}, {"referenceID": 46, "context": "\u2013 Taboada and Grieve (2004) corpus: A general-domain dataset composed of 400 long reviews (50% positive, 50% negative) about different topics (e.", "startOffset": 2, "endOffset": 28}, {"referenceID": 37, "context": "\u2013 Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50% positive, 50% negative).", "startOffset": 2, "endOffset": 22}, {"referenceID": 37, "context": "\u2013 Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50% positive, 50% negative). \u2013 Pang and Lee (2005) corpus: A collection of short (i.", "startOffset": 2, "endOffset": 123}, {"referenceID": 37, "context": "\u2013 Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50% positive, 50% negative). \u2013 Pang and Lee (2005) corpus: A collection of short (i.e. single-sentence) movie reviews. We relied on the test split used by Socher et al (2013), removing the neutral ones, as they did, for the binary classification task (1 821 subjective sentences: \u223c 49% positive, \u223c 51% negative).", "startOffset": 2, "endOffset": 247}, {"referenceID": 37, "context": "parser consumes to analyze the Pang and Lee (2005) corpus, and the total time once the SA system is run on it.", "startOffset": 31, "endOffset": 51}, {"referenceID": 46, "context": "Percentage of the training treebank used (log scale) LAS Taboada and Grieve (2004) corpus Pang and Lee (2004) corpus Pang and Lee (2005) corpus", "startOffset": 57, "endOffset": 83}, {"referenceID": 37, "context": "Percentage of the training treebank used (log scale) LAS Taboada and Grieve (2004) corpus Pang and Lee (2004) corpus Pang and Lee (2005) corpus", "startOffset": 90, "endOffset": 110}, {"referenceID": 37, "context": "Percentage of the training treebank used (log scale) LAS Taboada and Grieve (2004) corpus Pang and Lee (2004) corpus Pang and Lee (2005) corpus", "startOffset": 90, "endOffset": 137}, {"referenceID": 7, "context": "4 Relationship between LAS (area graphic, left y-axis) and accuracy in different sentiment corpora (line graphics, right y-axis), using the Stanford RNN parser (Chen and Manning 2014) trained with different portions (%) of the training treebank (x-axis).", "startOffset": 160, "endOffset": 183}, {"referenceID": 37, "context": "Table 2 Average, maximum and minimum execution time (seconds) out of 5 runs on the Pang and Lee (2005) test set.", "startOffset": 83, "endOffset": 103}, {"referenceID": 37, "context": "Table 2 Average, maximum and minimum execution time (seconds) out of 5 runs on the Pang and Lee (2005) test set. We also include the total execution time, after the SA system has been run on the Pang and Lee (2005) corpus", "startOffset": 83, "endOffset": 215}, {"referenceID": 37, "context": "Table 3 Accuracy on the Pang and Lee (2004) corpus considering different subsets of rules", "startOffset": 24, "endOffset": 44}, {"referenceID": 45, "context": "The slower parsers (Martins et al 2013; Rasooli and Tetreault 2015) tend to obtain a better performance, meanwhile the faster ones (Nivre et al 2007; Chen and Manning 2014) attain worse LAS, UAS and LA.", "startOffset": 19, "endOffset": 67}, {"referenceID": 7, "context": "The slower parsers (Martins et al 2013; Rasooli and Tetreault 2015) tend to obtain a better performance, meanwhile the faster ones (Nivre et al 2007; Chen and Manning 2014) attain worse LAS, UAS and LA.", "startOffset": 131, "endOffset": 172}, {"referenceID": 37, "context": "Table 4 Accuracy on the Pang and Lee (2005) corpus considering different subsets of rules", "startOffset": 24, "endOffset": 44}, {"referenceID": 48, "context": "Table 5 Accuracy on the Taboada and Grieve (2004) corpus considering different subsets of rules", "startOffset": 24, "endOffset": 50}, {"referenceID": 15, "context": "This fact is expected, as there is a well-known tradeoff between speed and accuracy in the spectrum of parsing algorithms, with one extreme at greedy search approaches that scan and parse the sentence in a single pass but are prone to error propagation, and the other at exact search algorithms that guarantee finding the highest-scoring parse under a rich statistical model, but are prohibitively slow (Choi and McCallum 2013; Volokh 2013; G\u00f3mez-Rod\u0155\u0131guez 2016).", "startOffset": 403, "endOffset": 462}, {"referenceID": 27, "context": "This is likely because these tend to be short-distance dependencies, which are easier to parse (McDonald and Nivre 2007), and are common so they do not suffer from train-", "startOffset": 95, "endOffset": 120}, {"referenceID": 27, "context": "This can be especially frequent in long sentences, which are the most difficult to parse (McDonald and Nivre 2007).", "startOffset": 89, "endOffset": 114}, {"referenceID": 7, "context": "The figure illustrates how LAS and sentiment accuracy vary when training the Stanford RNN parser (Chen and Manning 2014) with different training data size.", "startOffset": 97, "endOffset": 120}], "year": 2017, "abstractText": "Syntactic parsing, the process of obtaining the internal structure of sentences in natural languages, is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech. Sentiment analysis is one example of application for which parsing has recently proven useful. In recent years, there have been significant advances in the accuracy of parsing algorithms. In this article, we perform an empirical, task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art sentiment analysis system that determines the polarity of sentences from their parse trees. In particular, we evaluate the system using four well-known dependency parsers, including both current models with state-of-the-art accuracy and more innacurate models which, however, require less computational resources. The experiments show that all of the parsers produce similarly good results in the sentiment analysis task, without their accuracy having any relevant influence on the results. Since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms, this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser; and parsing researchers should investigate models that improve speed further, even at some cost to accuracy.", "creator": "LaTeX with hyperref package"}}}