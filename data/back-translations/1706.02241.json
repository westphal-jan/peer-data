{"id": "1706.02241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Insights into Analogy Completion from the Biomedical Domain", "abstract": "Completing the analogy has been a popular task for evaluating the semantic properties of word embedding in recent years, but the standard methodology assumes a number of analogies that do not always apply, either in current benchmark datasets or when expanding into other areas. By analyzing biomedical analogies, we identify three assumptions: that a single answer to a given analogy indicates that the couples involved describe the same relationship, and that each pair is informative with respect to the other. We propose modifying the standard methodology to loosen these assumptions by allowing multiple correct answers, reporting MAP and MRR in addition to accuracy, and using several sample pairs. We also present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embedding, and show that the relationships described in the dataset represent significant semantic challenges for current word embedding methods.", "histories": [["v1", "Wed, 7 Jun 2017 16:24:32 GMT  (130kb,D)", "http://arxiv.org/abs/1706.02241v1", "Accepted to BioNLP 2017. (10 pages)"]], "COMMENTS": "Accepted to BioNLP 2017. (10 pages)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["denis newman-griffis", "albert m lai", "eric fosler-lussier"], "accepted": false, "id": "1706.02241"}, "pdf": {"name": "1706.02241.pdf", "metadata": {"source": "CRF", "title": "Insights into Analogy Completion from the Biomedical Domain", "authors": ["Denis Newman-Griffis", "Albert M. Lai", "Eric Fosler-Lussier"], "emails": ["newman-griffis.1@osu.edu,", "amlai@wustl.edu,", "fosler@cse.ohio-state.edu"], "sections": [{"heading": "1 Introduction", "text": "Analogical reasoning has long been a staple of computational semantics research, as it allows for evaluating how well implicit semantic relations between pairs of terms are represented in a semantic model. In particular, the recent boom of research on learning vector space models (VSMs) for text (Turney and Pantel, 2010) has leveraged analogy completion as a standalone method for evaluating VSMs without using a full NLP system. This is due largely to the observations of \u201clinguistic regularities\u201d as linear off-\nsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).\nIn the analogy completion task, a system is presented with an example term pair and a query, e.g., London:England::Paris: , and the task is to correctly fill in the blank. Recent methods consider the vector difference between related terms as representative of the relationship between them, and use this to find the closest vocabulary term for a target analogy, e.g., England - London + Paris \u2248 France. However, recent analyses reveal weaknesses of such offset-based methods, including that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (Ko\u0308per et al., 2015; Gladkova et al., 2016; Drozd et al., 2016).\nWe identify three key assumptions encoded in the standard offset-based methodology for analogy completion: that a given analogy has only one correct answer, that all relationships between the example pair and the query-target pair are the same, and that the example pair is sufficiently informative with respect to the query-target pair. We demonstrate that these assumptions are violated in real-world data, including in existing analogy datasets. We then propose several modifications to the standard methodology to relax these assumptions, including allowing for multiple correct answers, making use of multiple examples when available, and reporting mean average precision (MAP) and mean reciprocal rank (MRR) to give a more complete picture of the implicit ranking used in finding the best candidate for completing a given analogy.\nFurthermore, we present the BioMedical Analogic Similarity Set (BMASS), a novel dataset for\nar X\niv :1\n70 6.\n02 24\n1v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n7\nanalogical reasoning in the biomedical domain. This new resource presents real-world examples of semantic relations of interest for biomedical natural language processing research, and we hope it will support further research into biomedical VSMs (Chiu et al., 2016; Choi et al., 2016).1"}, {"heading": "2 Related work", "text": "Analogical reasoning has been studied both on its own and as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herdag\u0306delen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavalle\u0301e and Langlais, 2010; Soricut and Och, 2015).\nRecent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of Ko\u0308per et al. (2015), the semantic relationships tend to be significantly more challenging for embedding-based methods (Drozd et al., 2016). Additionally, Levy et al. (2015b) demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning prototypicality as opposed to relatedness.\n1 The dataset, and all code used for our experiments, is available online at https://github.com/OSU-slatelab/BMASS."}, {"heading": "3 Analogy completion task", "text": ""}, {"heading": "3.1 Standard methodology", "text": "Given an analogy a:b::c:d, the evaluation task is to guess d out of the vocabulary, given a, b, c as evidence. Recent methods for this involve using the vector difference between embedded representations of the related pairs to rank all terms in the vocabulary by how well they complete the analogy, and choosing the best fit. The vector difference is most commonly used in one of three ways, where cos is cosine similarity:\nargmaxd\u2208V ( cos(d, b\u2212 a+ c) ) (1)\nargmaxd\u2208V ( cos(d\u2212 c, b\u2212 a) ) (2)\nargmaxd\u2208V cos(d, b)cos(d, c)\ncos(d, a) + (3)\nFollowing the terminology of Levy and Goldberg (2014), we refer to Equation 1 as 3COSADD, Equation 2 as PAIRWISEDISTANCE, and Equation 3 (which is equivalent to 3COSADD with log cosine similarities) as 3COSMUL.\nIn order to generate analogy data for this task, recent datasets have followed a similar process (Mikolov et al., 2013a,c; Ko\u0308per et al., 2015; Gladkova et al., 2016). First, relations of interest were manually selected for the target domains: syntactic/morphological, lexical (e.g., hypernymy, synonymy), or semantic (e.g., CapitalOf). Then, for each relation, example word pairs were manually selected or automatically generated from existing resources (e.g., WordNet). The final analogies were then generated by exhaustively combining the sets of word pairs within each relation."}, {"heading": "3.2 Assumptions", "text": "Several key assumptions are inherent in this standard methodology that are not reflected in recent benchmark analogy datasets. The first we refer to as the Single-Target assumption: namely, that there is a single correct answer for any given analogy. Since the target d is chosen via argmax, if we consider the following two analogies:\nflu:nausea::fever:?cough flu:nausea::fever:?light-headedness\nwe must necessarily get at least one answer wrong. Gladkova et al. (2016) convert these analogies into a single case:\nflu:nausea::fever:?[cough, light-headedness]\nwhere either cough or lightheadedness is a correct guess. However, this still misses our desire to get both correct answers, if possible. Relations with multiple correct targets are present in all of Google, BATS, and Sem-Para.\nThe second key assumption is that all the information relating a to b also relates c to d. While the pairs are chosen based on a single common relationship, each pair may actually pertain to multiple relationships. An example from the Google dataset is brother:sister::husband:wife; Table 1 shows the semantic relations involved in this analogy. While the target relation FemaleCounterpart is present in both pairs, by comparing the offsets sister \u2212 brother and wife \u2212 husband, we assume that either all ways in which each pair is related are present in both, or that FemaleCounterpart dominates the offset. We refer to this as the Same-Relationship assumption.\nFinally, it is not sufficient for two pairs to share a common relationship label; that relationship must be both representative and informative for analogies to make sense (the Informativity assumption). Relation labels may be sufficiently broad as to be meaningless, as we encountered when drawing unfiltered binary relations from the Unified Medical Language System (UMLS) Metathesaurus. One sample analogy from the RO:Null relation (indicating \u201crelated in some way\u201d) was socks:stockings::Finns:Finnish language. While both pairs are of related terms, they are in no way related to one another.\nFurthermore, even when two pairs are examples of the same kind of clearly-defined relation, they may still be relatively uninformative. For example, in the Sem-Para Meronym analogy apricot:stone::trumpet:mouthpiece the meronymic relationship between apricot and stone could plausibly identify a number of parts of a trumpet: mouthpiece, valves, slide, etc.2 The extremely\n2 While this is similar to the Single-Target assumption,\nhigh-level nature of several of the Sem-Para relations (hypernymy, antonymy, and synonymy) suggests that some of the difficulty observed by Ko\u0308per et al. (2015) is due to violations of Informativity."}, {"heading": "4 BMASS", "text": "We present BMASS (the BioMedical Analogic Similarity Set), a dataset of biomedical analogies, generated using the expert-curated knowledge in the Unified Medical Language System (UMLS)3 (Bodenreider, 2004) in order to identify medical term pairs sharing the same relationships. We followed the standard process for dataset generation outlined in Section 3.1, with some adjustments for the assumptions in Section 3.2.\nThe UMLS Metathesaurus is centered around normalized concepts, represented by Concept Unique Identifiers (CUIs). Each concept can be represented in textual form by one or more terms (e.g., C0009443\u2192 \u201cCommon cold\u201d, \u201cacute rhinitis\u201d). These terms may be multi-word expressions (MWEs); in fact, many concepts in the UMLS have no unigram terms.\nThe Metathesaurus also contains \u3008subject, relation, object\u3009 triples describing binary relationships between concepts. These relationships are specified at two levels: relationship types (RELs), such as broader-than and qualified-by, and specific relationships (RELAs) within each type, e.g., tradename-of and has-finding-site. For this work, we used the 721 unique REL/RELA pairings as our source relationships, and treated the \u3008subject, object\u3009 pairs linked within each of these relationships as candidates for generating analogies.\nTo enable a word embedding\u2013based evaluation, we first identified terms that appeared at least 25 times in the 2016 PubMed baseline collection of biomedical abstracts,4 and removed all \u3008subject, object\u3009 pairs involving concepts that did not correspond to these frequent terms. Most relationships in the Metathesaurus are many-to-many (i.e., each subject can be paired with multiple objects and\nit bears separate consideration in that Single-Target refers to multiple valid objects of a specific relationship, while this is an issue of multiple valid relationships being described.\n3 We use the 2016AA release of the UMLS. 4 We chose 25 as our minimum frequency to ensure that each term appeared often enough to learn reasonable embeddings for its component words. To determine term frequency, we first lowercased and stripped punctuation from both the PubMed corpus and the term list extracted from UMLS, then searched the corpus for exact term matches.\nvice versa), and thus may challenge Single-Target and Informativity assumptions; we therefore next identified relations that had at least 50 1:1 instances, i.e., a subject and object that are only paired with one another within a specific relationship. Since 1:1 instances are not sufficient to guarantee Informativity, we then manually reviewed the remaining relations to identify those those that we deemed to satisfy Informativity constraints. For example, the is-a relationship between tongue muscles and head muscle is not specific enough to suggest that carbon monoxide should elicit gasotransmitters as its corresponding answer. However, for associated-with, sampled pairs such as leg injuries : leg and histamine release : histamine were sufficiently consistent that we deemed it Informative. This gave us a final set of 25 binary relations, listed in Table 2.5\nWe follow Gladkova et al. (2016) in generating a balanced dataset, to enable a more robust comparative analysis between relations. We randomly sampled 50 \u3008subject, object\u3009 pairs from each relation, again restricting to concepts with strings appearing frequently in PubMed. For each subject concept that we sampled, we collected all valid object concepts and bundled them as a single \u3008subject, objects\u3009 pair. We then exhaustively combined each concept pair with the others in its relation to create 2,450 analogies, giving us a total dataset size of 61,250 analogies. Finally, for each concept, we chose a single frequent term to represent it, giving us both CUI and string representations of each analogy."}, {"heading": "5 Evaluation", "text": "We assess how well biomedical word embeddings can perform on our dataset, and explore modifications to the standard evaluation methodology to relax the assumptions described in Section 3.2. We use the skip-gram embeddings trained by Chiu et al. (2016) on the PubMed citation database, one set using a window size of 2 (PM-2) and another set with window size 30 (PM-30). All other word2vec hyperparameters were tuned by Chiu et al. on a combination of similarity and relatedness and named entity recognition tasks.\nAdditionally, we use the hyperparameters they identified (minimum frequency=5, vector dimension=200, negative samples=10, sample=1e-4,\n5Examples of each relation, along with their mappings to UMLS REL/RELA values, are available online.\n\u03b1=0.05, window size=2) to train our own embeddings on a subset of the 2016 PubMed Baseline (14.7 million documents, 2.7 billion tokens). We train word2vec (Mikolov et al., 2013a) samples with the continuous bag-of-words (CBOW) and skip-gram (SGNS) models, trained for 10 iterations, and GloVe (Pennington et al., 2014) samples, trained for 50 iterations.\nWe performed our evaluation with each of 3COSADD, PAIRWISEDISTANCE, and 3COSMUL as the scoring function over the vocabulary. In contrast to the prior findings of Levy and Gold-\nberg (2014) on the Google dataset, performance on BMASS is roughly equivalent among the three methods, often differing by only one or two correct answers. We therefore only report results with 3COSADD, since it is the most familiar method."}, {"heading": "5.1 Modifications to the standard method", "text": "We consider 3COSADD under three settings of the analogies in our dataset. For a given analogy a:b::c:?d, we refer to \u3008a, b\u3009 as the exemplar pair and \u3008c, d\u3009 as the query pair; ?d signifies the target answer.\nSingle-Answer puts analogies in a:b::c:d format, with a single example object b and a single correct object d, by taking the first object listed for each term pair. This enforces the Single-Answer assumption.\nMulti-Answer takes the first object listed for the exemplar term pair, but keeps all valid answers, i.e. a:b::c:[d1,d2,. . . ]; this is similar to the approach of Gladkova et al. (2016). There are approximately 16k analogies in our dataset with multiple valid answers.\nAll-Info keeps all valid objects for both the exemplar and query pairs. The exemplar offset is then calculated over B = [b1, b2, . . . ] as\na\u2212B = 1 |B| \u2211 i a\u2212 bi\nThough this is superficially similar to 3COSAVG (Drozd et al., 2016), we average over objects for a specific subject, as opposed to averaging over all subject-object pairs.\nWe report a relaxed accuracy (denoted AccR), in which the guess is correct if it is in the set of correct answers. (In the Single-Answer case, this reduces to standard accuracy.) AccR, as with standard accuracy, necessitates ignoring a, b, or c if they are the top results (Linzen, 2016).\nIn order to capture information about all correct answers, we also report Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) over the set of correct answers in the vocabulary, as ranked by Equation 1. Since MAP and MRR do not have a cutoff in terms of searching for the correct answer in the ranked vocabulary, they can be used without the adjustment of ignoring a, b, and c; thus, they can give a more accurate picture of how close the correct terms are to the calculated guesses."}, {"heading": "5.2 MWEs and candidate answers", "text": "As noted in Section 4, the terms in our analogy dataset may be multi-word expressions (MWEs). We follow the common baseline approach of representing an MWE as the average of its component words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016). For phrasal terms containing one or more words that are out of our embedding vocabulary, we only consider the invocabulary words: thus, if \u201cparathyroid\u201d is not in the vocabulary, then the embedding of parathyroid hypertensive factor will be\nhypertensive+ factor\n2\nFor any individual analogy a:b::c:?d, the vocabulary of candidate phrases to complete the analogy is derived by calculating averaged word embeddings for each UMLS term appearing in PubMed abstracts at least 25 times. Terms for which none of the component words are in vocabulary are discarded. This yields a candidate set of 229,898 phrases for the PM-2 and PM-30, and 263,316 for our CBOW, SGNS, and GloVe samples.\nSince prior work on analogies has primarily been concerned with unigram data, we also identified a subset of our data for which we could find single-word string realizations for all concepts in an analogy, using the full vocabulary of our trained embeddings. Even in the All-Info setting, we could only identify 606 such analogies; Table 3 shows MAP results for PM-2 and CBOW embeddings on the three relations with at least 100 unigram analogies. The unigram analogies are slightly better captured than the full MWE data for has-lab-number (L2) and has-tradename (L3); however, lower performance on the unigram subset in tradename-of (L4) shows that unigram analogies are not always easier. We see a small\neffect from the much larger set of candidate answers in the unigram case (>1m unigrams), as shown by the slightly higher MAP numbers in the UniM case. In general, it is clear that the difficulty of some of the relations in our dataset is not due solely to using MWEs in the analogies."}, {"heading": "5.3 Metric comparison", "text": "Figure 1 shows AccR, MAP, and MRR results for each relation in BMASS, using PM-2 embeddings in the Multi-Answer setting. Overall, performance varies widely between relations, with all three metrics staying under 0.1 in the majority of cases; this mirrors previous findings on other anal-\nogy datasets (Levy and Goldberg, 2014; Gladkova et al., 2016; Drozd et al., 2016).\nMAP further fleshes out these differences by reporting performance over all correct answers for a given analogy. This lets us distinguish between relations like has-salt-form (L7), where noticeably lower MAP numbers reflect a wider distribution of the multiple correct answers, and relations like regulates (B2) or associated-with (C6), where a low AccR reflects many incorrect answers, but a higher MAP indicates that the correct answers are relatively near the guess.\nMRR, on the other hand, more optimistically reports how close we got to finding any correct\nanswer. Thus, for the has-causative-agent (C4) relation, low AccR is belied by a noticeably higher MRR, suggesting that even when we guess wrong, the correct answer is close. This contrasts with relations like refers-to (H1) or causative-agentof (C3), where MRR is more consistent with AccR, indicating that wrong guesses tend to be farther from the truth. Since most of our analogies (45,178 samples, or about 74%) have only a single correct answer, MAP and MRR tend to be highly similar. However, in high-ambiguity relations like same-type (H2), higher MRR numbers give a better sense of our best case performance."}, {"heading": "5.4 Analogy settings", "text": "To compare across the Single-Answer, MultiAnswer, and All-Info settings, we first look at AccR for each relation in BMASS, shown for PM2 embeddings in Figure 2 (the observed patterns are similar with the other embeddings). Unsurprisingly, allowing for multiple answers in MultiAnswer and All-Info slightly raises AccR in most cases. What is surprising, however, is that including more sample exemplar objects in the All-Info setting had widely varying results. In some cases, such as same-type (H2), associated-substance (L5), and has-causative-agent (C4), the additional exemplars gave a noticeable improvement in accuracy. In others, accuracy actually went down: form-of (L1) and has-free-acid-or-base-form (L6) are the most striking examples, with absolute decreases of 4% and 8% respectively from the MultiAnswer case for PM-2 (the decreases are similar with other embeddings). Thus, it seems that multiple examples may help with Informativity in some cases, but confuse it in others. Taken together with the improvements seen in Drozd et al. (2016) from using 3COSAVG, this is another indication that any single subject-object pair may not be sufficiently representative of the target relationship."}, {"heading": "5.5 Embedding methods", "text": "Averaging over all relations, the five embedding settings we tested behaved roughly the same, with our trained embeddings slightly outperforming the pretrained embeddings of Chiu et al. (2016); summary AccR, MAP, and MRR performances are given in Table 4. At the level of individual relations, Figure 3 shows MAP performance in the Multi-Answer setting. The four word2vec samples tend to behave similarly, with some inconsistent variations. Interestingly, CBOW outperforms the other embeddings by a large margin in several relations, including regulated-by (B1) and tradename-of (L4).\nGloVe varies much more widely across the relations, as reflected in the higher standard deviations in Table 4. While GloVe consistently outperforms word2vec embeddings on has-free-acid-orbase-form (L6) and has-salt-form (L7), it significantly underperforms on the morphological and hierarchical relations, among others. Most notably, while the word2vec embeddings show minor differences in performance between the MultiAnswer and All-Info settings, GloVe AccR performance falls drastically on form-of (L1) and hasfree-acid-or-base-form (L6), as shown in Table 5. However, its MAP and MRR numbers stay similar, suggesting that there is only a reshuffling of results closest to the guess."}, {"heading": "5.6 Error analysis", "text": "Several interesting patterns emerge in reviewing individual a:b::c:?d predictions. A number of errors follow directly from our word averaging approach to MWEs: words that appear in b or c often appear in the predictions, as in gosorelin:ici 118630::letrozole:*ici 164384. Prefix substitutions also occurred, as with mianserin hydrochloride:mianserin::scopolamine hydrobromide:*scopolamine methylbromide.\nOften, the b term(s) would outweigh c, leading to many of the top guesses being variants on b. In one analogy, sodium acetylsalicyclate:aspirin::intravenous immunoglobulins:?immunoglobulin g, the top guesses were: *aspirin prophylaxis, *aspirin, *aspirin antiplatelet, and *low-dose aspirin.\nIn other cases, related to the nearest neighborhood over-reporting observed by Linzen (2016), we saw guesses very similar to c, regardless of a or b, as with acute inflammations:acutely inflamed::endoderm:*embryonic endoderm; other near guesses included *endoderm cell and epiblast.\nFinally, we found several analogies where the incorrect guesses made were highly related to the correct answer, despite not matching. One such analogy was oropharyngeal suctioning:substances::thallium scan:?radioisotopes; the top guess was *radioactive substances, and *gallium compounds was two guesses farther down. Showing some mixed effect from the neighborhood of b, *performance-enhancing substances was the next-ranked candidate."}, {"heading": "6 Discussion", "text": "Relaxing the Single-Answer, Same-Relationship, and Informativity assumptions by including multiple correct answers and multiple exemplar pairs and by reporting MAP and MRR in addition to accuracy paints a more complete picture of how well word embeddings are performing on analogy completion, but leaves a number of questions unanswered. While we can more clearly see the relations where we correctly complete analogies (or come close), and contrast with relations where a vector arithmetic approach completely misses the mark, what distinguishes these cases remains unclear. Some more straightforward relationships, such as gene-encodes-product (B3) and its inverse gene-product-encoded-by (B4), show\nsurprisingly poor results, while the very broad synonymy of refers-to (H1) is captured comparatively well. Additionally, in contrast to prior work with morphological relations, adjectivalform-of (M1) and noun-form-of (M2) are much more challenging in the biomedical domain, as we see non-morphological related pairs such as predisposed:disease susceptibility and venous lumen:endovenous, in addition to more normal pairs like sweating:sweaty and muscular:muscle. Further analysis may provide some insight into specific challenges posed by the relations in our dataset, as well as why performance with PAIRWISEDISTANCE and 3COSMUL did not noticeably differ from 3COSADD.\nIn terms of specific model errors, we did not evaluate the effects of any embedding hyperparameters on performance in BMASS, opting to use hyperparameter settings tuned for generalpurpose use in the biomedical domain. Levy et al. (2015a) and Chiu et al. (2016), among others, show significant impact of embedding hyperparameters on downstream performance. Exploring different settings may be one way to get a better sense of exactly what incorrect answers are being highly-ranked, and why those are emerging from the affine organization of the embedding space. Additionally, the higher variance in perrelation performance we observed with GloVe embeddings suggests that there is more to unpack as to what the GloVe model is capturing or failing to capture compared to word2vec approaches.\nFinally, while we considered Informativity during the generation of BMASS, and relaxed the Single-Answer assumption in our evaluation, we have not really addressed the Same-Relationship assumption. Using multiple exemplar pairs is one attempt to reduce the impact of confusing extraneous relationships, but in practice this helps some relations and harms others. Drozd et al. (2016) tackle this problem with the LRCos method; however, their findings of mis-applied features and errors due to very slight mis-rankings show that there is still room for improvement. One question is whether this problem can be addressed at all with non-parametric models like the vector offset approaches, to retain the advantages of evaluating directly from the word embedding space, or if a learned model (like LRCos) is necessary to separate out the different aspects of a related term pair."}, {"heading": "7 Conclusions", "text": "We identified three key assumptions in the standard methodology for analogy-based evaluations of word embeddings: Single-Answer (that there is a single correct answer for an analogy), SameRelationship (that the exemplar and query pairs are related in the same way), and Informativity (that the exemplar pair is informative with respect to the query pair). We showed that these assumptions do not hold in recent benchmark datasets or in biomedical data. Therefore, to relax these assumptions, we modified analogy evaluation to allow for multiple correct answers and multiple exemplar pairs, and reported Mean Average Precision and Mean Reciprocal Recall over the ranked vocabulary, in addition to accuracy of the highestranked choice.\nWe also presented the BioMedical Analogic Similarity Set (BMASS), a novel analogy completion dataset for the biomedical domain. In contrast to existing datasets, BMASS was automatically generated from a large-scale database of \u3008subject, relation, object\u3009 triples in the UMLS Metathesaurus, and represents a number of challenging real-world relationships. Similar to prior results, we find wide variation in word embedding performance on this dataset, with accuracies above 50% on some relationships such as has-salt-form and regulated-by, and numbers below 5% on others, e.g., anatomic-structure-is-part-of and measuredcomponent-of.\nFinally, we are able to address the SingleAnswer assumption by modifying the analogy evaluation to accommodate multiple correct answers, and we consider Informativity in generating our dataset and using multiple example pairs. However, the Same-Relationship assumption remains a challenge, as does a more automated approach to either evaluating or relaxing Informativity. These offer promising directions for future work in analogy-based evaluations."}, {"heading": "Acknowledgments", "text": "We would like to thank the CLLT group at Ohio State and the anonymous reviewers for their helpful comments. Denis is a pre-doctoral fellow at the National Institutes of Health Clinical Center, Bethesda, MD."}], "references": [{"title": "Learning New Facts", "author": ["Andrew Y. Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2013\\E", "shortCiteRegEx": "Ng.", "year": 2013}, {"title": "How to Train Good Word", "author": ["Sampo Pyysalo"], "venue": null, "citeRegEx": "Pyysalo.,? \\Q2016\\E", "shortCiteRegEx": "Pyysalo.", "year": 2016}, {"title": "BagPack: A General Framework to Represent Semantic Relations", "author": ["Ama\u00e7 Herda\u011fdelen", "Marco Baroni."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Association for Computational Linguistics, Athens,", "citeRegEx": "Herda\u011fdelen and Baroni.,? 2009", "shortCiteRegEx": "Herda\u011fdelen and Baroni.", "year": 2009}, {"title": "SemEval-2012 Task 2: Measuring Degrees of Relational Similarity", "author": ["David Jurgens", "Saif Mohammad", "Peter Turney", "Keith Holyoak."], "venue": "*SEM 2012, Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Jurgens et al\\.,? 2012", "shortCiteRegEx": "Jurgens et al\\.", "year": 2012}, {"title": "Multilingual Reliability and \u201dSemantic\u201d Structure of Continuous Word Spaces", "author": ["Maximilian K\u00f6per", "Christian Scheible", "Sabine im Walde."], "venue": "Proceedings of the 11th International Conference on Computational Semantics. Association for Com-", "citeRegEx": "K\u00f6per et al\\.,? 2015", "shortCiteRegEx": "K\u00f6per et al\\.", "year": 2015}, {"title": "Unsupervised Morphological Analysis by Formal Analogy", "author": ["Jean-Fran\u00e7ois Lavall\u00e9e", "Philippe Langlais"], "venue": null, "citeRegEx": "Lavall\u00e9e and Langlais.,? \\Q2010\\E", "shortCiteRegEx": "Lavall\u00e9e and Langlais.", "year": 2010}, {"title": "Towards automatic acquisition of linguistic features", "author": ["Yves Lepage", "Chooi Ling Goh."], "venue": "Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA 2009). Northern European Association for Language Technology", "citeRegEx": "Lepage and Goh.,? 2009", "shortCiteRegEx": "Lepage and Goh.", "year": 2009}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, Ann Arbor,", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015a", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "2015b. Do Supervised Distributional Methods Really Learn Lexical Inference Relations", "author": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["Tal Linzen."], "venue": "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pages 13\u201318.", "citeRegEx": "Linzen.,? 2016", "shortCiteRegEx": "Linzen.", "year": 2016}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 pages 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc., NIPS \u201913,", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Cognition and Thought: An Information Processing Approach", "author": ["Walter R Reitman."], "venue": "John Wiley and Sons, New York, NY.", "citeRegEx": "Reitman.,? 1965", "shortCiteRegEx": "Reitman.", "year": 1965}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Unsupervised Morphology Induction Using Word Embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "author": ["Peter D Turney."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1. Association for Computational Linguistics, Stroudsburg,", "citeRegEx": "Turney.,? 2008", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "WordNet Sits the S.A.T. A Knowledge-based Approach to Lexical Analogy", "author": ["Tony Veale"], "venue": "In Proceedings of the 16th European Conference on Artificial Intelligence", "citeRegEx": "Veale.,? \\Q2004\\E", "shortCiteRegEx": "Veale.", "year": 2004}, {"title": "Towards Universal Paraphrastic Sentence Embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the 4th International Conference on Learning Representations.", "citeRegEx": "Wieting et al\\.,? 2016", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "In particular, the recent boom of research on learning vector space models (VSMs) for text (Turney and Pantel, 2010) has leveraged analogy completion as a standalone method for evaluating VSMs without using a full NLP system.", "startOffset": 91, "endOffset": 116}, {"referenceID": 13, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 7, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 14, "context": "This is due largely to the observations of \u201clinguistic regularities\u201d as linear offsets in context-based semantic models (Mikolov et al., 2013c; Levy and Goldberg, 2014; Pennington et al., 2014).", "startOffset": 120, "endOffset": 193}, {"referenceID": 10, "context": "cluding that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (K\u00f6per et al.", "startOffset": 102, "endOffset": 116}, {"referenceID": 15, "context": "Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships.", "startOffset": 55, "endOffset": 70}, {"referenceID": 20, "context": "Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy,", "startOffset": 73, "endOffset": 86}, {"referenceID": 18, "context": "antonymy, and some world knowledge (Turney, 2008; Herda\u011fdelen and Baroni, 2009).", "startOffset": 35, "endOffset": 79}, {"referenceID": 2, "context": "antonymy, and some world knowledge (Turney, 2008; Herda\u011fdelen and Baroni, 2009).", "startOffset": 35, "endOffset": 79}, {"referenceID": 10, "context": "The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 13, "context": "Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 3, "context": "The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K\u00f6per et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K\u00f6per et al. (2015), the semantic relation-", "startOffset": 49, "endOffset": 181}, {"referenceID": 8, "context": "Additionally, Levy et al. (2015b) demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning", "startOffset": 14, "endOffset": 34}, {"referenceID": 7, "context": "Following the terminology of Levy and Goldberg (2014), we refer to Equation 1 as 3COSADD,", "startOffset": 29, "endOffset": 54}, {"referenceID": 4, "context": "In order to generate analogy data for this task, recent datasets have followed a similar process (Mikolov et al., 2013a,c; K\u00f6per et al., 2015; Gladkova et al., 2016).", "startOffset": 97, "endOffset": 165}, {"referenceID": 4, "context": "2 While this is similar to the Single-Target assumption, high-level nature of several of the Sem-Para relations (hypernymy, antonymy, and synonymy) suggests that some of the difficulty observed by K\u00f6per et al. (2015) is due to violations of Informativity.", "startOffset": 197, "endOffset": 217}, {"referenceID": 11, "context": "We train word2vec (Mikolov et al., 2013a) samples with the continuous bag-of-words (CBOW) and skip-gram (SGNS) models, trained for 10 iterations, and GloVe (Pennington et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 14, "context": ", 2013a) samples with the continuous bag-of-words (CBOW) and skip-gram (SGNS) models, trained for 10 iterations, and GloVe (Pennington et al., 2014) samples, trained for 50 iterations.", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": ") AccR, as with standard accuracy, necessitates ignoring a, b, or c if they are the top results (Linzen, 2016).", "startOffset": 96, "endOffset": 110}, {"referenceID": 12, "context": "resenting an MWE as the average of its component words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016).", "startOffset": 55, "endOffset": 119}, {"referenceID": 21, "context": "resenting an MWE as the average of its component words (Mikolov et al., 2013b; Chen et al., 2013; Wieting et al., 2016).", "startOffset": 55, "endOffset": 119}, {"referenceID": 7, "context": "1 in the majority of cases; this mirrors previous findings on other analogy datasets (Levy and Goldberg, 2014; Gladkova et al., 2016; Drozd et al., 2016).", "startOffset": 85, "endOffset": 153}, {"referenceID": 10, "context": "hood over-reporting observed by Linzen (2016), we saw guesses very similar to c, regardless of a or b, as with acute inflammations:acutely inflamed::endoderm:*embryonic endoderm; other near guesses included *endoderm cell and epi-", "startOffset": 32, "endOffset": 46}, {"referenceID": 8, "context": "Levy et al. (2015a) and Chiu et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "Levy et al. (2015a) and Chiu et al. (2016), among others, show significant impact of embedding hyperparameters on downstream performance.", "startOffset": 0, "endOffset": 43}], "year": 2017, "abstractText": "Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about analogies that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions: that of a Single Answer for any given analogy, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard methodology to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to accuracy, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.", "creator": "LaTeX with hyperref package"}}}