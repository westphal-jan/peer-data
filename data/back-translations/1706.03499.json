{"id": "1706.03499", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models", "abstract": "This paper describes the system of the University of Stockholm / University of Groningen (SU-RUG) for the joint task SIGMORPHON 2017 on morphological diffraction. Our system is based on an attentive sequence-to-sequence model of neural networks using Long Short-Term Memory (LSTM) cells, with joint training of morphological diffraction and inverse transformation, i.e. lemmatization and morphological analysis. Our system exceeds the baseline by a large margin, and our submission ranks as the fourth best team for the route we participate in (task 1, resource-intensive).", "histories": [["v1", "Mon, 12 Jun 2017 08:08:00 GMT  (275kb,D)", "http://arxiv.org/abs/1706.03499v1", "4 pages, to appear at CoNLL-SIGMORPHON 2017"]], "COMMENTS": "4 pages, to appear at CoNLL-SIGMORPHON 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["robert \\\"ostling", "johannes bjerva"], "accepted": false, "id": "1706.03499"}, "pdf": {"name": "1706.03499.pdf", "metadata": {"source": "CRF", "title": "SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models", "authors": ["Robert \u00d6stling", "Johannes Bjerva"], "emails": ["robert@ling.su.se", "j.bjerva@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "We focus on task 1 of the SIGMORPHON 2017 shared task (Cotterell et al., 2017), morphological inflection. The task is to learn the mapping from a lemma and morphological description to the corresponding inflected form. For instance, the English verb lemma torment with the features 3.SG.PRS should be mapped to torments. As our model is poorly suited for low-resource conditions, we only submitted results for the 51 languages with highresource training data available in the shared task (i.e., excluding Scottish Gaelic)."}, {"heading": "2 Background", "text": "The results of the SIGMORPHON 2016 shared task (Cotterell et al., 2016) indicated that the attentional sequence-to-sequence model of Bahdanau et al. (2014) is very suitable for this task (Kann and Schu\u0308tze, 2016), so we use this framework as the basis of our model.\n\u2217This work was carried out while the second author was visiting the Department of Linguistics, Stockholm University.\nA recent trend in neural machine translation is to use back-translated text (Sennrich et al., 2016) as a way to benefit from additional monolingual data in the target language. There is also work on translation models with reconstruction loss, which encourages solutions that can be translated back to their original (Tu et al., 2016). These developments are technically similar to our semisupervised training below."}, {"heading": "3 Method", "text": "Our system is based on the attentional sequenceto-sequence model of Bahdanau et al. (2014) with Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and variational dropout Gal and Ghahramani (2016). The main innovation is that our inflection model is trained jointly with the reverse process, that is, lemmatization and morphological analysis. This can be done in two ways:\n1. Fully supervised, where we simply train the forward (inflection) and backward (lemmatization and morphological analysis) model jointly with shared character embeddings.\n2. Semi-supervised, where supervised examples are mixed with examples where only the inflected target form is used. This form is passed first through the backward model, a greedy search to obtain a unique lemma, and finally through the forward model to reconstruct the inflected form.\nOur official submission only includes results from fully supervised training (method 1), due to time constraints, but Section 5 contains a comparison between the two versions on the development set. The system architecture is shown in Figure 1 for the forward (inflection) model. The backward\nar X\niv :1\n70 6.\n03 49\n9v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\n(lemmatizer) model has separate parameters, except the embeddings, but is structurally identical except for two details: instead of passing the morphological feature information to the decoder (via a single fully connected layer), we predict the features from the final state of the encoder LSTM (via a separate fully connected layer).\nOur implementation is based on the Chainer library (Tokui et al., 2015) and available at github.com/bjerva/sigmorphon2017."}, {"heading": "4 Model configuration", "text": "For the official submission, we use 128 LSTM cells for the (unidirectional) encoder, decoder, attention mechanism, character embeddings, as well as for the fully connected layers for morphological features encoding/prediciton. We use a dropout factor of 0.5 throughout the network, including the recurrent parts. For optimization, we use Adam (Kingma and Ba, 2015) with default parameters. Each model is trained for 48 hours on a single CPU, using a batch-size of 64, and the model parameters during this time that give the lowest development set mean Levenshtein distance are saved. For the official submission, we used an ensemble of two such models, using a beam search of width 10 to select the final inflection candidate."}, {"heading": "5 Results and Analysis", "text": "The system has high performance in general, with a macro-average accuracy of 93.6%, and edit dis-\ntance of 0.14. This is substantially higher than the baseline (77.8% accuracy and 0.5 edit distance), and ranks as the 9th best run, and 4th best team in this SIGMORPHON 2017 shared task setting. Furthermore, the difference in scores between our run and the best run overall is low (1.75% accuracy and 0.04 edit distance). Table 1 contains a detailed version of the official results our system on the shared task, in the high setting of Task 1.\nNotably, the system has an accuracy of 100% on both Basque and Quechua, which indicates that it is capable of fully learning the rules of very regular morphological systems. The relatively high accuracy on Semitic languages (Arabic: 89.8%, Hebrew: 99.0%) again confirms the ability of encoder-decoder models to also handle non-concatenative morphology.\nLatin has the lowest accuracy by far, and the reason seems to be that the provided shared task data lacks vowel length distinctions in the lemma but uses them in the inflected forms. This missing lexical information is difficult to predict accurately. Evaluating with vowel length distinctions gives an accuracy of 75.6% (Latin development set), compared to 91.5% without. The latter accuracy score is in line with other Romance languages (French 90.8%, Spanish 94.3%, Italian 97.0%).\nWe also investigated whether the semisupervised approach described in Section 3 has any effect on accuracy. The results on the development set, presented in Table 2, indicate that\nthere is no systematic effect (the macro-averaged accuracy drops marginally from 93.9% to 93.8%)."}, {"heading": "6 Conclusions", "text": "We implemented a system using an attentional sequence-to-sequence model with Long ShortTerm Memory (LSTM) cells. As our model is poorly suited for low-resource conditions, we only participated in the high-resource setting. Our inflection model is trained jointly with the reverse process, that is, lemmatization and morphological analysis. The system significantly outperforms the baseline system, and performs well compared to other submitted systems, showing that this approach is very suitable for morphological inflection, given sufficient amounts of data."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the reviewers, and Johan Sjons for their comments on previous versions of this manuscript. This work was partially funded by the NWO\u2013VICI grant \u201cLost in Translation \u2013 Found in Meaning\u201d (288- 89-003). This work was performed on the Abel Cluster, owned by the University of Oslo and the Norwegian metacenter for High Performance Computing (NOTUR), and operated by the Department for Research Computing at USIT, the University of Oslo IT-department."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The sigmorphon 2016 shared task: Morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "CoRR abs/1611.01874.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "The results of the SIGMORPHON 2016 shared task (Cotterell et al., 2016) indicated that the atten-", "startOffset": 47, "endOffset": 71}, {"referenceID": 4, "context": "(2014) is very suitable for this task (Kann and Sch\u00fctze, 2016), so we use this framework as the basis of our model.", "startOffset": 38, "endOffset": 62}, {"referenceID": 0, "context": "tional sequence-to-sequence model of Bahdanau et al. (2014) is very suitable for this task (Kann and Sch\u00fctze, 2016), so we use this framework as the basis of our model.", "startOffset": 37, "endOffset": 60}, {"referenceID": 6, "context": "A recent trend in neural machine translation is to use back-translated text (Sennrich et al., 2016) as a way to benefit from additional monolingual data in the target language.", "startOffset": 76, "endOffset": 99}, {"referenceID": 8, "context": "encourages solutions that can be translated back to their original (Tu et al., 2016).", "startOffset": 67, "endOffset": 84}, {"referenceID": 0, "context": "Our system is based on the attentional sequenceto-sequence model of Bahdanau et al. (2014) with Long Short-Term Memory (LSTM) cells", "startOffset": 68, "endOffset": 91}, {"referenceID": 3, "context": "(Hochreiter and Schmidhuber, 1997) and variational dropout Gal and Ghahramani (2016).", "startOffset": 0, "endOffset": 34}, {"referenceID": 2, "context": "(Hochreiter and Schmidhuber, 1997) and variational dropout Gal and Ghahramani (2016). The main innovation is that our inflection model is trained jointly with the reverse process, that is, lemmatization and morphological analysis.", "startOffset": 59, "endOffset": 85}, {"referenceID": 7, "context": "Our implementation is based on the Chainer library (Tokui et al., 2015) and available at github.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "For optimization, we use Adam (Kingma and Ba, 2015) with default parameters.", "startOffset": 30, "endOffset": 51}], "year": 2017, "abstractText": "This paper describes the Stockholm University/University of Groningen (SURUG) system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory (LSTM) cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4th best team for the track we participate in (task 1, high-resource).", "creator": "LaTeX with hyperref package"}}}