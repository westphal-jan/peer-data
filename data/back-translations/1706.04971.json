{"id": "1706.04971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "German in Flux: Detecting Metaphoric Change via Word Entropy", "abstract": "This paper examines the information-theoretical measure entropy for the detection of metaphorical changes and transfers ideas from hypernym recognition to the investigation of language changes. In addition, we build the first diachronic test for German as the standard for the annotation of metaphorical changes. Our model shows high performance, is unsupervised, language-independent and generalisable to other processes of semantic change.", "histories": [["v1", "Thu, 15 Jun 2017 17:14:17 GMT  (498kb,D)", "http://arxiv.org/abs/1706.04971v1", "CoNLL 2017. 9 pages"]], "COMMENTS": "CoNLL 2017. 9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominik schlechtweg", "stefanie eckmann", "enrico santus", "sabine schulte im walde", "daniel hole"], "accepted": false, "id": "1706.04971"}, "pdf": {"name": "1706.04971.pdf", "metadata": {"source": "CRF", "title": "German in Flux: Detecting Metaphoric Change via Word Entropy", "authors": ["Dominik Schlechtweg", "Stefanie Eckmann", "Enrico Santus", "Sabine Schulte im Walde", "Daniel Hole"], "emails": ["dominik.schlechtweg@gmx.de,", "stefanie.eckmann@campus.lmu.de,", "esantus@mit.edu,", "schulte@ims.uni-stuttgart.de,", "holedan@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Recently, computational linguistics has shown an increasing interest in language change. This interest is focused on making semantic change measurable. However, even though different types of semantic change are well-known in historical linguistics, little effort has been made to distinguish between them. A very basic distinction in historical linguistics is the one between innovative meaning change (also polysemization)\u2014 e.g., German bru\u0308ten \u2018breed\u2019 > \u2018breed, brood over sth.\u2019\u2014and reductive meaning change\u2014e.g., German schinden \u2018to skin, torture\u2019 > \u2018to torture\u2019 (cf. Koch, 2016, p. 24\u201327). Metaphoric meaning change is an important sub-process of innovative meaning change. Hence, a computational model of semantic change should be able to distinguish metaphoric change from other\u2014typically less strong\u2014types of change. Such a model, particularly if applicable to different languages, would be beneficial for a number of areas: (i), historical linguists may test their theoretical claims about semantic change on a large-scale empirical basis going beyond the traditional corpus-based approaches; (ii), linguists and psychologists working on metaphor in language or cognition may\nbenefit by gaining new insights into the diachronic aspects of metaphor which are not yet as central in these fields as the synchronic aspects; and, finally, (iii), the Natural Language Processing research community may benefit by applying the model presented here to a wide range of tasks in which polysemy and non-literalness are involved.\nOur aim is to build an unsupervised and language-independent computational model which is able to distinguish metaphoric change from semantic stability. We apply entropy (a measure of uncertainty inherited from information theory) to a Distributional Semantic Model (DSM). In particular, we exploit the idea of semantic generality applied in hypernym detection, to detect metaphoric change as a special process of meaning innovation. German will serve as a sample language, since there is a rich historical corpus available covering a large time period. Nevertheless, our model is presumably applicable to other languages requiring only minor adjustments. With the model, we introduce the first resource for evaluation of models of metaphoric change and propose a structured annotation process that is generalizable to the creation of gold standards for other types of semantic change.1\nIn the next section, we give an overview of related work on semantic change and automatic detection of metaphor. In Section 3, the basic linguistic notions we focus on are introduced and connected to their distributional properties, followed by a description of the corpus used to obtain vector representations of words in Section 4. In Section 5, the information-theoretic measures we apply to word vectors are described. Section 6 presents the annotation study conducted to create a\n1The test set is provided together with the annotation data and the model code (which is based on Shwartz et al. (2016)\u2019s code): https://github.com/Garrafao/ MetaphoricChange\nar X\niv :1\n70 6.\n04 97\n1v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n17\nmetaphoric change test set for German. Section 7 illustrates how the measures\u2019 predictions shall be evaluated. The results are presented and discussed in Section 8. Section 9 will then conclude and give a short outlook to further research objectives."}, {"heading": "2 Related Work", "text": "There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016).\nMost of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016). Thus, we cannot expect diachronic similarity models to reflect changes in the semantic generality of a word over time, which was described to be a central effect of semantic change (cf. Bybee, 2015, p. 197). Additionally, they often pose the problem of vector space alignment (especially when relying on word embeddings), occurring when word vectors from different time periods have to be mapped to a common coordinate axis (cf. Hamilton et al., 2016b, p. 1492).\nDiachronic WSI models, on the contrary, are able to detect at least innovative (and reductive) meaning change, as they are designed to induce newly arising senses of words. However, they do not measure how these senses relate to each other in terms of semantic generality. Hence, ad hoc, they may not be able to distinguish different subtypes of innovative meaning change such as metaphoric vs. metonymic change. They may fail\n2With the exception of Hamilton et al. (2016a, p. 1) making the rather coarse-grained distinction between cultural shift and \u201cregular processes of linguistic drift\u201d.\nto detect meaning changes where no new senses can be induced as, e.g., in grammaticalization. Moreover, some models require elaborate training (e.g., Frermann and Lapata, 2016).\nApart from similarity and WSI models, Sagi et al. (2009) measure semantic broadening and narrowing of words (shifting upwards and downwards in the semantic taxonomy respectively) via semantic density calculated as the average cosine of its context word vectors. Just as word entropy, semantic density is based on the measurement of linguistic context dispersion (see Section 3.1). However, this method is only applied in a case study with very limited scope in terms of the number of phenomena covered and there is no verification of the test items via annotation. Hence, it remains to be shown that the method can generally distinguish broadening and narrowing or other types of meaning innovation.\nTwo previous approaches to language change exploit the notion of entropy. Juola (2003) describes language change on a very general level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using\u2014 among other measures\u2014a word entropy measure.\nFinally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; Ko\u0308per and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet."}, {"heading": "3 Metaphoric Change", "text": "Metaphoric change plays a fundamental role in semantic change (cf. e.g. Ferraresi, 2014, p. 15). Within the framework of Conceptual Metaphor Theory (Lakoff and Johnson, 1980) the metaphorical effect can be described as a mapping from a source domain to a target domain. Following the terminology from Koch (2016, p. 24) innovative meaning change, as opposed to reductive meaning change, is where the existing meaning MA (the\nsource concept) of a word acquires a new meaning MB (the target concept). Metaphoric Change is, then, a subcategory of innovative meaning change where MB is related to MA by similarity or a reduced comparison (cf. Koch, 2016, p. 47, and also Steen, 2010, p. 10). While language is often used ad hoc in a non-literal meaning in discourse, not every of these uses constitutes an instance of metaphoric change. Only when a metaphoric innovation is conventionalized within the language, we can speak of metaphoric meaning change (cf. Koch, 2016, p. 27). Consider German umwa\u0308lzen as an example. In Early New High German the word was only used in the sense \u2018to turn around something or someone physically\u2019 (MA) as in (1).3 In Contemporary New High German, though, the word is also frequently used in the sense \u2018to change something (possibly abstract) radically\u2019 (MB) as in (2).\n(1) ...mu\u00df ich mich vmbweltzen / vnd kan keinen schlaff in meine augen bringen 4\n\u2018...I have to turn around and cannot bring sleep into my eyes.\u2019\n(2) Kinadon wollte den Staat umwa\u0308lzen... 5\n\u2018Kinadon wanted to revolutionize the state...\u2019"}, {"heading": "3.1 Distributional Properties", "text": "As Bybee (2015) notes, and is also commonly agreed-upon, \u201cmetaphorical meaning changes create polysemy\u201d (p. 199, her italics). Campbell (1998, p. 258) describes this effect as \u201cextensions in the meaning of a word\u201d occurring through metaphoric change. It is only logical to assume that such extensions in meaning range imply an extension in the range of linguistic contexts a word occurs in. This extension, then, distinguishes words undergoing such a change from semantically stable words, but also from words undergoing different types of meaning change such as reductive meaning change where we expect an oppositional effect: a reduction of the range of contexts a word occurs in. Polysemization (and thus context extension) is, yet, not only a typical property of metaphoric change but of all types of innovative meaning change such as metonymic change, generalization, specialization, and grammaticalization (cf. Heine and Kuteva, 2007, p. 35). However,\n3Early New High German: ca. 1350-1650; Contemporary New High German: 1650-today (cf. Fleischer, 2011, p. 24)\n4Neomenius, J.: Christliche Leichpredigt. Brieg, 1616. 5Mu\u0308ller, K. O.: Die Dorier. Vier Bu\u0308cher. Bd. 2, 1824.\nrecall that metaphor involves a mapping between two different domains (as introduced in Lakoff and Johnson 1980) in contrast to other types of meaning change, which is why we would expect a relatively strong effect on the contextual distribution here.\nMoreover, not only the range of a word\u2019s meanings influences the range of contexts it occurs in, but also the particular nature of the individual meanings has an influence. As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016). According to the distributional informativeness hypothesis, semantically more general words are less informative than special words as they occur in more general contexts (Rimell, 2014; Santus et al., 2014). Hence, differences in semantic generality of source and target concept should be reflected by their contextual distribution.6 Such differences occur particularly with taxonomic meaning changes like generalization and specialization, but also with metaphoric change, as it often results in the emergence of more abstract meanings of a word. Consider, e.g., the development of German gla\u0308nzend with \u2018luminous\u2019 as source and \u2018very good\u2019 as target concept. The source concept only applies to a rather limited range of entities, i.e., physical ones. The target concept, on the contrary, given its abstractness, applies to nearly every entity. Interpreting such changes of words as a change in their semantic generality, we now aim to examine how well it is measurable with distributional methods."}, {"heading": "4 Corpus", "text": "For our investigation, we use the corpus of Deutsches Textarchiv (erweitert) (DTA), which is accessible online and downloadable for free.7 The DTA provides more than 2447 lemmatized and POS-tagged texts (with more than 140M tokens), covering a time period from the late 15th to the early 20th century. Thus, it covers the developments of German from (late) Early New High German to Contemporary New High German. The corpus is POS-tagged using the STTS tagset (Schiller et al., 1999). The texts used by DTA include literary and scientific texts as well as\n6Related ideas are also indicated, e.g., by Fortson (2003, p. 650) and Bybee (2015, p. 202).\n7http://www.deutschestextarchiv.de/\nfunctional writings, e.g., cookbooks. DTA aims at providing a corpus with a roughly equivalent number of texts from each of the aforementioned genres. The corpus is preprocessed in standard ways. (Find details in Appendix A.) For the creation of the co-occurrence matrices, from which we calculate word entropy and the other measures, a standard model of distributional semantics with a symmetric window of size 2 is used."}, {"heading": "5 Entropy", "text": "In hypernym detection a number of wellestablished measures compare the semantic generality of words on the basis of their distributional generality (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2009). A promising candidate measure seems to be word entropy, which is introduced in Santus (2013) and Santus et al. (2014). Amongst other advantages, word entropy is independently measurable over time, which avoids the problem of vector space alignment."}, {"heading": "5.1 Entropy in Information Theory", "text": "The term \u2018Entropy\u2019 was first introduced by Shannon (1948) who laid the foundations of information theory. Intuitively, it measures the unpredictability of a system. The entropy H of a discrete random variable X with possible values {x1, ..., xn} and probability mass function P (X) (a probability distribution) is\nH(X) = \u2212 n\u2211\ni=1\nP (xi) logb P (xi) (3)\nwhere b is typically equal to 2 or 10 (Shannon, 1948, cf. p. 11).\nWord Entropy. Examining language statistically, a word w may be represented by its distribution in a corpus. This distribution is determined by the contexts of w, i.e., the words it cooccurs with, and how often it co-occurs with them. The distribution of w is usually recorded in a matrix, intuitively a table where rows correspond to target word distributions and columns to context word distributions. Rows are typically referred to as vectors and the whole matrix spans a vector space. We can interpret w\u2019s (normalized) vector then as a probability distribution where word co-occurrences of w with any other corpus word w\u2032 correspond to events in the probability distribution. More specifically, assuming that C and T are\ndiscrete random variables of occurrences of context and target words respectively, we say that w\u2019s vector estimates the conditional probability distribution of context words given target word w with discrete random variable C and a probability mass function defined by P (C | T = w). For every c \u2208 C, P (c | w) (the probability that the context word c will occur given the occurrence of w as target word) is estimated by Freq(w,c)Freq(w) .\n8 Now, we can apply any notion from probability theory to this distribution. Hence, the entropy of w\u2019s probability distribution is given by\nH(C) = \u2212 n\u2211\ni=1\nP (ci | w) log2 P (ci | w) (4)\nThe entropy of w\u2019s estimated probability distribution\u2014for the sake of convenience we will just write H(w)\u2014measures the unpredictability of w\u2019s co-occurrences, i.e., how hard it is to predict with which word w will co-occur if we look at a random occurrence of w. In hypernym detection, word entropy is assumed to reflect semantic generality. While here it is mostly used to compare pairs of different words for their semantic relations, e.g., whether one is the hypernym of the other, we will compare the word entropy of one and the same word w in different time periods assuming this to reflect w\u2019s semantic development with respect to its generality.\nNormalization Depending on corpus size and other factors, the frequency of each target word will vary strongly. On top of that, the number of types in the corpus increases with the progression of time. These factors influence word entropy (and also other measures) without being tied to semantic change. Hence, we need a way to normalize for them. We test essentially two ways of normalizing word entropy for word frequency:\nMatching Occurrence Number (MON). The first strategy assumes that, for the most part, the influence of word frequency on word entropy comes from the increasing number of context types with increasing number of contexts n used to construct a word vector (where n is dependent on word frequency). Hence, we can suppress the influence of word frequency by comparing only word vectors\n8For convenience, here, we do not distinguish between a word and the mathematical structure corresponding to the event of the occurrence of the word.\nconstructed from an equal number of contexts (cf. Kisselew et al., 2016). In order to make the vectors of all target words from all time periods comparable, we choose a common number of contexts n for all target words. Additionally, in order to diminish the influence of chance (because we do not use all contexts, we have to pick a random subset), we average over the entropies computed for a number of k vectors, each constructed from a different n-sized set of contexts. (Find information on the setting of hyperparameters in Appendix A.)\nOrdinary Least Squares Regression (OLS). Another way of normalizing entropy for frequency relies on the observation that there is a correlation between word entropy and word frequency. We try to approximate this relationship by fitting an OLS model to the observations from the corpus, where each observed word type is a data point. This approximation can then serve as a prediction for the expected change of a word\u2019s entropy given a certain change in the word\u2019s frequency. Deviations from this expectation can further be interpreted as the change in entropy solely related to semantic generality. In order to get a good approximation for each target word we only fit the model to the local n data points next to the target word in the independent variable (frequency). In Figure 1 we see the result of fitting the model described by Equation 5 to the 1000 data points (from a specific time period) next to the data point for the adjective locker, \u2018loose\u2019, in the independent variable. As we can see, the data point for locker slightly deviates from the regression curve, more precisely, by \u2206 = 0.136. Taking this as a starting point for the semantic development of locker (reference time) we can now calculate locker\u2019s \u2206 in a later time period (focus time). We assume that \u2206 stays approximately equal if only locker\u2019s frequency changes. If \u2206, however, increases, we assume that the word underwent meaning innovation. We apply an analogous procedure to all target words.\nentropy \u223c \u03b1+ \u03b2 ln(frequency) (5)"}, {"heading": "5.2 Other Measures", "text": "Word Frequency. Concerning frequency, a similar argument can be brought forward as in Section 3.1: When a word acquires a new meaning and can be applied to a wider range of entities, then we would expect the word to be used more often. Furthermore, it is well known that certain types\nof semantic change correlate with frequency. For instance, desemanticization comes with a strong increase in frequency (cf. Bybee, 2015, p. 133). For this, we use the frequency of a word w as a baseline to word entropy (parallel to the practice in hypernym detection). In order to diminish the influence of corpus size we normalize word frequency Freq(w) by the number of tokens N in the relevant slice of the corpus:\nFreqn(w) = Freq(w)\nN (6)\nSecond-Order Word Entropy. A variant of word entropy used in hypernym detection is second-order word entropy where entropy is not calculated directly for the word w, but rather for its most-associated context words. Then the median of these is w\u2019s second-order word entropy (cf. Santus et al., 2014, p. 40). This measure relies on the hypothesis that the more semantically general a word is, the more it co-occurs with general context words. Presumably, this measure is more immune to the influence of word frequency, because not w\u2019s own frequency plays a role, but rather the frequency of its most-associated context words. This may be helpful where we have rather accidental differences in the frequency of a word in different time periods, e.g., due to corpus size or text sort. In such a setting we reckon regular (firstorder) word entropy to be more prone to these accidental factors than second-order word entropy."}, {"heading": "6 Diachronic Metaphor Annotation", "text": "Humans often have different intuitions about what is a metaphor and what is not. According to\nSteen (2010, p. 2) \u201cthe identification of metaphoric language has become a matter of controversy\u201d. Therefore, we did not want to rely solely on our own intuitions, but identify metaphoric change of words via annotation. A number of structured annotation guidelines for synchronic metaphor identification have been proposed (Pragglejaz Group, 2007; Steen, 2010; Shutova, 2015). Steen (cf. 2010, p. 8) distinguishes between linguistic and conceptual metaphor annotation. We adopted the former approach, since we were less interested in the exact mapping underlying a metaphoric use of a word. The crucial difference to synchronic metaphor identification is that we did not want annotators to judge individual uses but pairs of uses of lexical units.9 The metaphoric relation between the source and the target concept involved in the metaphoric change of a word w should be reflected in w\u2019s individual uses which is a common methodological assumption in historical linguistics. Individual uses bearing the meaning of source or target concept allow humans to infer these meanings which can then be judged as being (non-)metaphorical to each other. We operationalize this observation as annotation procedure.\nTarget Selection. We preselected the target items for annotation so that they were likely to have undergone metaphoric change. For this, we scanned the literature on metaphoric change in German such as Fritz (2006) and Keller and Kirschbaum (2003). The richest list we found in Paul (2002) (ca. 140 items). However, this could not be taken directly as a gold standard. We first checked for every item whether we could attest metaphoric change in the corpus. If so, we determined a rough date of change according to when we found the metaphoric meaning clearly established in the corpus. We then checked whether the item had an occurrence frequency above a threshold of 40 around the date of change. Only then the item was added to the test set for annotation.10\nFor every metaphoric target word m in the test set we added a semantically stable word s with the same POS-tag from the same frequency area. For this, we checked the words in the immediate vicinity to m in the total frequency rank (of the first half of the century in which m\u2019s change oc-\n9A similar procedure is used in Erk et al. (2009, 2013) for annotation of usage similarity.\n10We provide both: the full list of items and the one filtered for frequency.\ncurred) in DWDS, a rich online etymological dictionary of German.11 If there was no meaning change indicated and we could not attest a clear meaning change in the corpus, we added the word to the test set. Thereby, we balanced metaphoric and stable words with respect to frequency. Stable words comprise concrete words, e.g. Palast \u2018palace\u2019, as well as more abstract words, e.g. freundlich \u2018friendly\u2019. The test set contains nouns, verbs and adjectives. (Find it in Appendix C.)\nNext, parallel to the corpus slicing (see Section 7), we selected 20 contexts from two time periods. These periods were set in such a way that one was located before and one after the pre-identified date of change. Supposing that a word occurs in n contexts in a certain time period, we ordered them according to publication date and picked every (n/20)th context guaranteeing that contexts are well-distributed over authors and the time period. Contexts with less than 10 words and obvious parsing errors were excluded in order to provide enough information for the annotators and to avoid contexts excluded by them.\nFinally, contexts from the earlier period were combined randomly with contexts from the later period yielding 20 context pairs for every target. The order of every second pair was switched, minimizing the possibility that annotators infer the chronology of contexts. The pairs of all 28 target words were randomly sampled such that individual judgments were less influenced by earlier judgments of the same target, resulting in 560 context pairs presented to the annotators.\nAnnotation Procedure. Three annotators were asked to judge for each of the 560 context pairs whether one of the contexts admitted inference of a meaning of the target word which is related metaphorically to the meaning in the other context. (Find an example in Appendix B.) The annotators were linguists, two of them were marginally acquainted with historical linguistics. The annotation guidelines are a combination and modification of the processes described by Pragglejaz Group (2007), Steen (2010) and Shutova (2015). Whether a meaning of a target word in context 2 (M2) is metaphorically related to the meaning in context 1 (M1) should be identified in 3 steps:\n1. For each word its meaning in context is established;\n11https://dwds.de/\n2. It is decided whether M1 can be seen as a more basic meaning than M2. This is the case when M2 is related to M1 in one or more of the following ways: (i), M2 is less concrete than M1; (ii), M2 is less human-oriented than M1; (iii), M2 is not related to bodily action in contrast to M1; (iv), M2 is less precise than M1.\n3. If this is the case, then it is decided whether M2 contrasts with M1 but can be understood in comparison with it. If yes, M2 is judged as being metaphorically related to M1, otherwise as not being metaphorically related to M1.\nStep 2 is intended to exclude cases of nonmetaphorical polysemy, for which a more basic meaning should not be identifiable (cf. Pragglejaz Group, 2007, p. 30). It is a rather liberal variation of the existing guidelines in that already the fact that one of the criteria holds is sufficient to consider M1 to be more basic than M2. This is because of cases like Feder, \u2018feather, springclip\u2019, Blatt, \u2018leaf, sheet, newspaper\u2019, and Haube, \u2018cap, cover, marriage, crest\u2019, whose meaning change would else not be captured, although we reckon it metaphoric: The change of Feder \u2018feather\u2019 > \u2018feather, springclip\u2019 does not fall under all criteria in step 2, e.g., there is no mapping from concrete to abstract. The existing guidelines seem to implicitly exclude such cases of metaphors, which we want to overcome. Future studies may opt for different decisions here.\nStep 3 guarantees that the two meanings identified are sufficiently distinct and that there can be a mapping established between them. We cannot guarantee that annotators judge the context pairs in exactly the way we prescribe in the guidelines. (Find the full guidelines in Appendix B.)\nAnnotation Results. Annotators reported that they found the task hard, which is not surprising given that some contexts dated back 400 years making it sometimes difficult to interpret them. Accordingly, we expected this to be reflected in the inter-annotator agreement. Annotator 1 and Annotator 2 had a moderate agreement of \u03ba = .40 (Fleiss\u2019 Kappa) for earlier and .46 for later contexts, while Annotator 3 had poor agreement with both, Annotator 1 (.26, .26) and Annotator 2 (.32, .29). Given this deviation, we excluded Annotator 3 from the evaluation. (Further evaluation is\nperformed for the judgments of Annotator 1 and Annotator 2.) The agreement we found is only slightly lower than in comparable synchronic studies. Pragglejaz Group (2007, p. 21), e.g., report a \u03ba between 0.56 and 0.72 for different tasks. We can attribute the difference in agreement to the higher level of difficulty of the task the annotators were faced with.\nThe annotation results are summarized in Table 1. Target words are ordered decreasingly according to the increase in metaphorically tagged contexts over time (last column). In addition to \u03ba we also give the share of items with perfect agreement (%A), since \u03ba underestimates agreement on rare effects (Feinstein and Cicchetti, 1990). As you can see, the annotators overall confirmed our judgments of the targets, as most metaphoric targets are at the top of the list. Target words differ strongly in the strength of metaphoric change assigned to them: between 82% (Donnerwetter) and -14% (Haube). Yet, most targets exhibit positive judgment, which we would expect from a test set containing metaphoric and stable targets. Striking is the position of Feder and Haube at the bottom, which are tagged even negatively metaphoric. This means that the share of metaphorically tagged contexts was higher for the earlier contexts. We conjecture that the reason for this is that both words were already used in other metaphoric meanings in earlier contexts. The high position of freundlich and fett presumably results from the fact that they are abstract adjectives. Metaphor identification for adjectives is more difficult than for nouns and verbs, because their meanings tend to be less concrete and precise (cf. Pragglejaz Group, 2007, p. 28). They are typically applicable to a wider range of entities, increasing the probability to encounter a context pair in our study with two uses differing in abstractness and preciseness. We will pay particular attention to the targets rated differently by us and the annotators in the analysis of the measures\u2019 predictions."}, {"heading": "7 Evaluation", "text": "As with Gulordava and Baroni (2011) or Hamilton et al. (2016b), we assess the measures\u2019 performance by comparing their predictions in a corpus against a gold standard. Our gold standard is the rank of target words in Table 1 obtained by annotation. We obtain the measures\u2019 predictions for the target words by first calculating their values in a\ntime period 1 before the starting point of change and in a time period 2 after that. We then compute the difference d in values between period 1 and 2 for each target word and further rank the target words according to d. Next, we compute the rank correlation between each of these predicted ranks and the gold rank as a performance measure.\nTime period 1 is usually the century before and period 2 the century after the century of change, e.g., ausstechen (1739) will be compared in 1600- 1700 and 1800-1900. (Only for targets from 1800- 1900 time period 2 is different, i.e., 1850-1926, since the corpus version we use only contains texts until 1926.) Stable words are compared in the same time periods as their metaphoric counterparts (see Section 6). With this procedure we have the possibility to evaluate the measures (i), only on targets from the same century, fixing influential side factors such as corpus size, and (ii), on all targets, which is a much harder task. (Find a list of time periods with corpus sizes in Appendix A.)"}, {"heading": "8 Results", "text": "Table 2 shows Spearman\u2019s \u03c1 quantifying the correlation between the measures\u2019 predicted ranks and the gold standard rank. We can directly see that word entropy (H) correlates significantly with the gold rank in different conditions. Moreover, the\nranking it predicts for targets from 1700-1800 correlates much stronger (.64) with the gold rank than the other measures\u2019 predictions. Note that the correlation is highly significant despite the relatively small sample size. In the harder condition, where we look at the ranks across different time periods, H still correlates significantly and stronger than all other measures with the gold rank. However, apart from H, the conclusions we can draw about the other measures can only be preliminary, as there is no significance for their predicted ranks.\nAt first glance, the normalized versions of entropy do not perform as expected: HMON never outperforms frequency and shows even negative correlation in one time period. Since we reckoned that the reason for this is the low setting of the hyperparameter n = 29 (which we adopted with the intention to construct all vectors from a common number of contexts), we also tested the measure on target words from 1700-1800 with a setting of n such that the maximum number of contexts is used to construct the word vector and the number of vectors to average over k = 10. In this setting HMON\u2019s prediction has a highly significant correlation with the gold rank which is comparable in strength to H.\nNotably, HOLS has the best performance for targets from 1800-1900. We tried out different hyperparameter settings and found that our initial choice of the data window size n = 1000 may also not have been optimal, as higher n yield better, yet non-significant, results: n = 500/10000/20000/50000 yields \u03c1 = 0.19/0.32/0.31/0.21 respectively, for targets from 1700-1800. Another factor possibly biasing HOLS are different variances in different corpora or frequency areas which may also connect to our observation that the measure correlates negatively with absolute changes in frequency, i.e., decrease in frequency often leads to increase in HOLS and vice versa.\nH2 consistently performs poorly. Moreover, testing of different values for N yields a wide range of \u03c1 values between -0.29 and 0.42 for targets from 1700-1800, not allowing conclusions on the performance of the measure because the correlation is not significant.\nAnalyzing the predicted ranks reveals interesting insights. H and its normalized siblings rank Donnerwetter, which is at the top of the gold rank, at the very bottom. This is, presumably, because\nin its later metaphoric sense \u2018blowup\u2019 the word can be used as an interjection in very short sentences as in (7).\n(7) Potz Donnerwetter! 12\n\u2018Man alive!\u2019\nThis narrows down Donnerwetter\u2019s contextual distribution due to our model only considering words within a sentence as context. H2 and frequency are not sensitive to this and rank the word much higher. This shows that, (i), different factors play a role in determining the contextual distribution of a word suggesting that a model of semantic change should incorporate different types of information and, (ii), that H2 and frequency may still be helpful in detecting metaphoric change in certain settings. The dominance of H may also be a hint to this direction: Word entropy combines frequency and contextual distribution as it is influenced by both.\nFeder and Haube from the very bottom of the gold rank are not beyond the bottom-items of any measure\u2019s prediction. In H\u2019s prediction, which is the best-performing measure, they rank near the middle (12, 18). This indicates that their position at the bottom of the gold rank may not accurately reflect the semantic change they underwent. Similarly for the adjectives freundlich and fett ranking in all predictions near middle or lower (for H: 18, 10). We still have to assess how these words behave in future studies."}, {"heading": "9 Conclusion", "text": "Semantic generality is an important indicator of semantic change. As Bybee (cf. 2015, p. 197) puts it, generalization and specialization are two basic principles of meaning change. We proposed a\n12Hauptmann, Gerhart: Der Biberpelz. Berlin, 1893.\nway to detect metaphoric change based on semantic generality and built a test set for the evaluation of computational models of metaphoric change in German. We proposed an annotation procedure strictly derived from comparable synchronic work and showed that annotators can show reasonable agreement. Different distributional measures based on the information-theoretic concept of entropy were compared against the annotators judgments and it was found that raw word entropy correlates strongly and significantly with the gold rank in different settings in contrast to most other entropy measures and frequency. We found evidence that HMON predicts well with certain parameter settings.\nBoth, the annotation procedure and the computational model, are generalizable to different types of semantic change. Moreover, our model is unsupervised and language-independent as it relies, in principle, on minimal linguistic input, since entropy can be computed already from a raw token co-occurrence matrix. Yet, the model profits from richer input as indicated in Shwartz et al. (2016).\nFuture studies should test how well word entropy distinguishes metaphoric change from other types of meaning innovation and how well it detects innovative and reductive meaning change in general. The latter may be tested straightforwardly on the English data of Gulordava and Baroni (2011) and Hamilton et al. (2016b). In doing so, it will be interesting to see how our model performs in comparison to diachronic similarity and WSI models."}, {"heading": "Acknowledgments", "text": "We thank Prof. Dr. Sebastian Pado\u0301 (Institute for Natural Language Processing, University of Stuttgart) for pointing out his idea to normalize word entropy via OLS. We are very grateful to Prof. Dr. Olav Hackstein (Historical and Indo-European Linguistics, LMU Munich) and his research colloquium for valuable discussions and comments and to Sascha Schlechtweg for statistical advice. We would like to thank Andrew Wigman for careful proof-reading as well as Jo\u0308rg Fo\u0308rstner, Michael Frotscher, Altina Mujkic, Edona Neziri, Cornelia van Scherpenberg, Christian Soetebier and Veronika Vasileva for help related to the annotation process. Last but not least, we thank the reviewers for constructive criticism helping us to improve the paper substantially."}, {"heading": "A Hyperparameters and Corpus Preprocessing Details", "text": "A.1 Hyperparameters Second-order word entropy has 3 hyperparameters: (i), the number of positively associated contexts N to compute the average/median from; (ii), whether to use median or average entropy among the top N contexts;13 and (iii), the association metric used to sort the contexts by relevance (i.e., PPMI or PLMI). We choose the following combination of hyperparameters: \u3008100, median, PLMI\u3009,\n13Note that for any test pair, N is the maximal number of associated contexts, which is reduced toM if a test target has only M (< N ) positively associated contexts in one of the two matrices to compare.\nwhich is suggested by the work of Shwartz et al. (2016).\nFor MON entropy normalization we choose n = 29, because that is the lowest context number of a word in one of its two relevant time periods, and k = 10000. For OLS normalization we choose n = 1000.\nA.2 Corpus Preprocessing Words that occur less than 5 times in the whole corpus, functional words and punctuation are deleted. As functional words we regard those which are not tagged with a POS-tag starting with either \u2018N\u2019, \u2018V\u2019 or \u2018AD\u2019. Every token is then replaced by its lemma form combined with the starting of its POS-tag, e.g., geht is replaced by gehen:V. Note that both diachronic lemmatization and POS-tagging are provided by DTA."}, {"heading": "B Annotation Guidelines", "text": "B.1 Introduction Following the terminology from Koch (cf. 2016, p. 24) innovative meaning change, as opposed to reductive meaning change, is where the existing meaning MA of a word acquires a new meaning MB , where this normally happens over a long period of time.\nMetaphoric Change is, then, a subcategory of innovative meaning change (besides metonymic change, generalization...) where MB is related to MA by similarity or a reduced comparison Koch (cf. 2016, p. 47). (cf. also Steen, 2010, p. 10)\nNote that the annotation process described below is a combination and modification of the processes described by Pragglejaz Group (2007), Steen (2010) and Shutova (2015).\nB.2 Annotation Process You will be given an OpenOffice table document with approximately 560 lines. In every line you will see in columns 2 and 3 two uses of a word (the target word contained in column 1) with its surrounding contexts. The relevant word is marked in bold font in both contexts.\n1. For each such use of a word establish its meaning in context, that is, how it applies to an entity, relation, or attribute in the situation evoked by the text (contextual meaning). Take into account what comes before and after the word. Note that the word might be used differently from what you are familiar with. Don\u2019t let yourself be confused by alternative spelling.\n2. Try to find an interpretation where the meaning in the second context (M2) is related to the meaning in the first context (M1) in one or more of the following ways:\n\u2022 M2 is less concrete than M1 (what it evokes is harder to imagine, see, hear, feel, smell, and taste); \u2022 M2 is less human-oriented than M1; \u2022 M2 is not related to bodily action in con-\ntrast to M1; \u2022 M2 is less precise than M1 (precise as\nopposed to vague).\n3. If M2 is indeed related to M1 in one or more of these ways, decide whether M2 contrasts with M1 but can be understood in comparison with it. (See below for an example.)\n4. (i) If yes, fill in 1 into the column headed by \u2018M2 is metaphorically related to M1\u2019, judging M2 as being metaphorically related to M1.\n(ii) If no, fill in 0 into the column headed by \u2018M2 is is metaphorically related to M1\u2019, judging M2 as not being metaphorically related to M1.\n(iii) If you cannot decide, e.g., because the word marked in bold font doesn\u2019t match the word shown in column 1 in meaning or part of speech, you don\u2019t understand either of the contexts, one is too unspecific or other reasons, don\u2019t perform evaluation, fill in a 1 into the comments column and go on to the next test item.\n5. Compare the two meanings in the other direction, i.e., decide whether M1 is metaphorically related to M2 by going through steps 2 to 4 and fill your judgment into the column headed by \u2018M1 is metaphoric compared to M2\u2019.\nPlease make sure that you don\u2019t change anything in the file apart from column width, your judgments and comments. Finally, return the annotated document to the above-mentioned email address. If you have any further questions on the task, don\u2019t hesitate to ask.\nB.3 Annotation Example The following example illustrates how the procedure operates in practice. Consider Table 4 as an example table similar to the one you will receive for annotation.\nIn line 1 you need to compare two uses of the word umwa\u0308lzen. In context 1 umwa\u0308lzen is used in the sense \u2018to turn around something or someone physically\u2019 (M1). This contrasts with its use in context 2 where it is used in the sense \u2018to change something radically\u2019 (M2). M2 is clearly less concrete than M1 and not necessarily related to bodily action. Moreover, M2 is less precise, since we may have greater disagreement about the question whether something \u2018changed radically\u2019 than we may have on the question whether someone or something (was) turned around. (You may have a different intuition here, which should then be reflected in your judgment accordingly.)\nNow, as we saw, M2 contrasts with M1. However, it can be understood in comparison with it: We can understand abstract change in terms of physical or local change. Consequently, we fill in 1 in the column headed by \u2018M2 is metaphorically related to M1\u2019, judging M2 to be metaphorically related to M1. And, for the same reasons as mentioned above, we fill in a 0 in the column headed by \u2018M1 is metaphorically related to M2\u2019.\nIn line 2 both meanings of umwa\u0308lzen, M1 and M2, are similarly concrete, human-oriented, related to bodily action and precise. They don\u2019t contrast with each other. (You may want to say that they are equal.) Hence, neither meaning has a metaphoric relation to the other. Consequently, we fill in 0 into both columns."}, {"heading": "C Metaphoric Change Test Set", "text": ""}], "references": [{"title": "Measuring Historical Word Sense Variation", "author": ["D. Bamman", "G. Crane."], "venue": "Proceedings of the 11th Annual International ACM/IEEE Joint Conference on Digital Libraries. ACM, New York, NY, USA, JCDL \u201911, pages 1 \u2013 10.", "citeRegEx": "Bamman and Crane.,? 2011", "shortCiteRegEx": "Bamman and Crane.", "year": 2011}, {"title": "A Clustering Approach for the Nearly Unsupervised Recognition of Nonliteral Language", "author": ["Julia Birke", "Anoop Sarkar."], "venue": "Proceedings of the 11th Conference of the European Chapter of the ACL. Trento, Italy, pages 329\u2013336.", "citeRegEx": "Birke and Sarkar.,? 2006", "shortCiteRegEx": "Birke and Sarkar.", "year": 2006}, {"title": "Language change", "author": ["Joan L. Bybee."], "venue": "Cambridge University Press, Cambridge, United Kingdom.", "citeRegEx": "Bybee.,? 2015", "shortCiteRegEx": "Bybee.", "year": 2015}, {"title": "Historical Linguistics: An Introduction", "author": ["L. Campbell."], "venue": "Edinburgh University Press.", "citeRegEx": "Campbell.,? 1998", "shortCiteRegEx": "Campbell.", "year": 1998}, {"title": "Context-theoretic semantics for natural language: an overview", "author": ["Daoud Clarke."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Association for Computational Linguistics, Athens, Greece, pages 112\u2013119.", "citeRegEx": "Clarke.,? 2009", "shortCiteRegEx": "Clarke.", "year": 2009}, {"title": "On the linearity of semantic change: Investigating meaning variation via dynamic graph models", "author": ["Steffen Eger", "Alexander Mehler."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-", "citeRegEx": "Eger and Mehler.,? 2016", "shortCiteRegEx": "Eger and Mehler.", "year": 2016}, {"title": "Investigations on word senses and word usages", "author": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord."], "venue": "In Proceedings of ACL-09.", "citeRegEx": "Erk et al\\.,? 2009", "shortCiteRegEx": "Erk et al\\.", "year": 2009}, {"title": "Measuring word meaning in context", "author": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord."], "venue": "Computational Linguistics 39(3):511\u2013554.", "citeRegEx": "Erk et al\\.,? 2013", "shortCiteRegEx": "Erk et al\\.", "year": 2013}, {"title": "High agreement but low Kappa: I", "author": ["A.R. Feinstein", "D.V. Cicchetti."], "venue": "The problems of two paradoxes. Journal of Clinical Epidemiology 43(6):543 \u2013 549.", "citeRegEx": "Feinstein and Cicchetti.,? 1990", "shortCiteRegEx": "Feinstein and Cicchetti.", "year": 1990}, {"title": "Grammatikalisierung", "author": ["Gisella Ferraresi."], "venue": "Kurze Einf\u00fchrungen in die germanistische Linguistik. Winter, Heidelberg.", "citeRegEx": "Ferraresi.,? 2014", "shortCiteRegEx": "Ferraresi.", "year": 2014}, {"title": "Historische Syntax des Deutschen", "author": ["J. Fleischer."], "venue": "Narr, T\u00fcbingen.", "citeRegEx": "Fleischer.,? 2011", "shortCiteRegEx": "Fleischer.", "year": 2011}, {"title": "The Handbook of Historical Linguistic, Blackwell, Oxford, chapter An Approach to Semantic Change, pages 648\u2013666", "author": ["Benjamin Fortson."], "venue": "The Handbook of Historical Linguistics.", "citeRegEx": "Fortson.,? 2003", "shortCiteRegEx": "Fortson.", "year": 2003}, {"title": "A bayesian model of diachronic meaning change", "author": ["Lea Frermann", "Mirella Lapata."], "venue": "TACL 4:31\u2013", "citeRegEx": "Frermann and Lapata.,? 2016", "shortCiteRegEx": "Frermann and Lapata.", "year": 2016}, {"title": "Historische Semantik", "author": ["Gerd Fritz."], "venue": "Metzler, Stuttgart/Weimar.", "citeRegEx": "Fritz.,? 2006", "shortCiteRegEx": "Fritz.", "year": 2006}, {"title": "A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus", "author": ["K. Gulordava", "M. Baroni."], "venue": "Proceedings of GEMS.", "citeRegEx": "Gulordava and Baroni.,? 2011", "shortCiteRegEx": "Gulordava and Baroni.", "year": 2011}, {"title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change", "author": ["W.L. Hamilton", "J. Leskovec", "D. Jurafsky."], "venue": "Emnlp.", "citeRegEx": "Hamilton et al\\.,? 2016a", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "author": ["W.L. Hamilton", "J. Leskovec", "D. Jurafsky."], "venue": "CoRR abs - 1605 09096.", "citeRegEx": "Hamilton et al\\.,? 2016b", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "The Genesis of Grammar: A Reconstruction", "author": ["Bernd Heine", "Tania Kuteva."], "venue": "Oxford University Press.", "citeRegEx": "Heine and Kuteva.,? 2007", "shortCiteRegEx": "Heine and Kuteva.", "year": 2007}, {"title": "Bad Company\u2014 Neighborhoods in Neural Embedding Spaces Considered Harmful", "author": ["J. Hellrich", "U. Hahn."], "venue": "Proceedings of COLING 2016 pages 2785 \u2013 2796.", "citeRegEx": "Hellrich and Hahn.,? 2016", "shortCiteRegEx": "Hellrich and Hahn.", "year": 2016}, {"title": "The Time Course of Language Change", "author": ["Patrick Juola."], "venue": "Computers and the Humanities 37(1):77\u2013", "citeRegEx": "Juola.,? 2003", "shortCiteRegEx": "Juola.", "year": 2003}, {"title": "Bedeutungswandel: eine Einf\u00fchrung", "author": ["R. Keller", "I. Kirschbaum."], "venue": "De Gruyter Studienbuch. De Gruyter.", "citeRegEx": "Keller and Kirschbaum.,? 2003", "shortCiteRegEx": "Keller and Kirschbaum.", "year": 2003}, {"title": "Temporal Analysis of Language through Neural Language Models", "author": ["Y. Kim", "Y.-I. Chiu", "K. Hanaki", "D. Hegde", "S. Petrov."], "venue": "ArXiv e-prints .", "citeRegEx": "Kim et al\\.,? 2014", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Predicting the direction of derivation in english conversion", "author": ["Max Kisselew", "Laura Rimell", "Alexis Palmer", "Sebastian Pado."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-", "citeRegEx": "Kisselew et al\\.,? 2016", "shortCiteRegEx": "Kisselew et al\\.", "year": 2016}, {"title": "Meaning change and semantic shifts", "author": ["Peter Koch."], "venue": "Maria Koptjevskaja-Tamm P\u00e4nivi Juvonen, editor, The Lexical Typology of Semantic Shifts, De Gruyter Mouton.", "citeRegEx": "Koch.,? 2016", "shortCiteRegEx": "Koch.", "year": 2016}, {"title": "Distinguishing Literal and Non-Literal Usage of German Particle Verbs", "author": ["Maximilian K\u00f6per", "Sabine Schulte im Walde."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "K\u00f6per and Walde.,? 2016", "shortCiteRegEx": "K\u00f6per and Walde.", "year": 2016}, {"title": "Directional distributional similarity for lexical expansion", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Association for Computational Linguistics,", "citeRegEx": "Kotlerman et al\\.,? 2009", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2009}, {"title": "Metaphors We Live By", "author": ["G. Lakoff", "M. Johnson."], "venue": "Phoenix books. University of Chicago Press.", "citeRegEx": "Lakoff and Johnson.,? 1980", "shortCiteRegEx": "Lakoff and Johnson.", "year": 1980}, {"title": "Word Sense Induction for Novel Sense Detection", "author": ["J.H. Lau", "P. Cook", "D. McCarthy", "D. Newman", "T. Baldwin."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Lau et al\\.,? 2012", "shortCiteRegEx": "Lau et al\\.", "year": 2012}, {"title": "Classifier Combination for Contextual Idiom Detection Without Labelled Data", "author": ["Linlin Li", "Caroline Sporleder."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Singapore, pages 315\u2013323.", "citeRegEx": "Li and Sporleder.,? 2009", "shortCiteRegEx": "Li and Sporleder.", "year": 2009}, {"title": "Word Epoch Disambiguation: Finding How Words Change Over Time", "author": ["R. Mihalcea", "V. Nastase."], "venue": "Proceedings of the 50th Annual Meeting of ACL.", "citeRegEx": "Mihalcea and Nastase.,? 2012", "shortCiteRegEx": "Mihalcea and Nastase.", "year": 2012}, {"title": "Deutsches W\u00f6rterbuch: Bedeutungsgeschichte und Aufbau unseres Wortschatzes", "author": ["H. Paul."], "venue": "Niemeyer, T\u00fcbingen, 10 edition.", "citeRegEx": "Paul.,? 2002", "shortCiteRegEx": "Paul.", "year": 2002}, {"title": "MIP: A method for identifying metaphorically used words in discourse", "author": ["Pragglejaz Group."], "venue": "Metaphor and Symbol 22:1\u201339.", "citeRegEx": "Group.,? 2007", "shortCiteRegEx": "Group.", "year": 2007}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014, April 26-30, 2014, Gothenburg, Sweden. pages 511\u2013519.", "citeRegEx": "Rimell.,? 2014", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "Semantic Density Analysis: Comparing Word Meaning Across Time and Phonetic Space", "author": ["E. Sagi", "S. Kaufmann", "B. Clark."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics. Association for Compu-", "citeRegEx": "Sagi et al\\.,? 2009", "shortCiteRegEx": "Sagi et al\\.", "year": 2009}, {"title": "SLQS: An entropy measure", "author": ["Enrico Santus."], "venue": "Unpublished Master Thesis, University of Pisa.", "citeRegEx": "Santus.,? 2013", "shortCiteRegEx": "Santus.", "year": 2013}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte Im Walde."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2:", "citeRegEx": "Santus et al\\.,? 2014", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Guidelines f\u00fcr das Tagging deutscher Textcorpora mit STTS", "author": ["Anne Schiller", "Simone Teufel", "Christine St\u00f6ckert", "Christine Thielen."], "venue": "Technical report, Institut f\u00fcr maschinelle Sprachverarbeitung, Stuttgart.", "citeRegEx": "Schiller et al\\.,? 1999", "shortCiteRegEx": "Schiller et al\\.", "year": 1999}, {"title": "A Mathematical Theory of Communication", "author": ["Claude E. Shannon."], "venue": "CSLI Publications.", "citeRegEx": "Shannon.,? 1948", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Annotation of Linguistic and Conceptual Metaphor, Springer", "author": ["E. Shutova"], "venue": null, "citeRegEx": "Shutova.,? \\Q2015\\E", "shortCiteRegEx": "Shutova.", "year": 2015}, {"title": "Statistical Metaphor Processing", "author": ["Ekaterina Shutova", "Simone Teufel", "Anna Korhonen."], "venue": "Computational Linguistics 39(2):301\u2013353.", "citeRegEx": "Shutova et al\\.,? 2013", "shortCiteRegEx": "Shutova et al\\.", "year": 2013}, {"title": "Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection", "author": ["V. Shwartz", "E. Santus", "D. Schlechtweg."], "venue": "CoRR abs - 1612 - 04460.", "citeRegEx": "Shwartz et al\\.,? 2016", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "A Method for Linguistic Metaphor Identification: From MIP to MIPVU", "author": ["G. Steen."], "venue": "Converging evidence in language and communication research. John Benjamins Publishing Company.", "citeRegEx": "Steen.,? 2010", "shortCiteRegEx": "Steen.", "year": 2010}, {"title": "Metaphor Detection with Cross-Lingual Model Transfer", "author": ["Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore,", "citeRegEx": "Tsvetkov et al\\.,? 2014", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "Literal and Metaphorical Sense Identification through Concrete and Abstract Context", "author": ["Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Edinburgh,", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Topics over time: A non-Markov continuous-time model of topical trends", "author": ["X. Wang", "A. Mccallum."], "venue": "In SIGKDD.", "citeRegEx": "Wang and Mccallum.,? 2006", "shortCiteRegEx": "Wang and Mccallum.", "year": 2006}, {"title": "A general framework for distributional similarity", "author": ["Julie Weeds", "David Weir."], "venue": "EMNLP. pages 81\u201388.", "citeRegEx": "Weeds and Weir.,? 2003", "shortCiteRegEx": "Weeds and Weir.", "year": 2003}, {"title": "Understanding Semantic Change of Words over Centuries", "author": ["D.T. Wijaya", "R. Yeniterzi."], "venue": "Proceedings of the 2011 International Workshop on DETecting and Exploiting Cultural diversiTy on the Social Web. ACM, New York, NY, USA, DETECT \u201911,", "citeRegEx": "Wijaya and Yeniterzi.,? 2011", "shortCiteRegEx": "Wijaya and Yeniterzi.", "year": 2011}, {"title": "A Computational Evaluation of Two Laws of Semantic Change", "author": ["Y. Xu", "C. Kemp."], "venue": "Proceedings of the 37th Annual Meeting of the Cognitive Science Society, CogSci 2015, Pasadena, California, USA, July 22-25, 2015.", "citeRegEx": "Xu and Kemp.,? 2015", "shortCiteRegEx": "Xu and Kemp.", "year": 2015}, {"title": "For MON entropy normalization we choose n = 29, because that is the lowest context number of a word in one of its two relevant time peri", "author": ["Shwartz"], "venue": null, "citeRegEx": "Shwartz,? \\Q2016\\E", "shortCiteRegEx": "Shwartz", "year": 2016}], "referenceMentions": [{"referenceID": 40, "context": "The test set is provided together with the annotation data and the model code (which is based on Shwartz et al. (2016)\u2019s code): https://github.", "startOffset": 97, "endOffset": 119}, {"referenceID": 40, "context": "shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016).", "startOffset": 90, "endOffset": 112}, {"referenceID": 12, "context": ", Frermann and Lapata, 2016). Apart from similarity and WSI models, Sagi et al. (2009) measure semantic broadening and narrowing of words (shifting upwards and downwards in the semantic taxonomy respectively) via semantic density calculated as the average cosine of its context word vectors.", "startOffset": 2, "endOffset": 87}, {"referenceID": 19, "context": "Juola (2003) describes language change on a very general level by", "startOffset": 0, "endOffset": 13}, {"referenceID": 22, "context": "Kisselew et al. (2016) are interested in the diachronic properties of conversion using\u2014", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009),", "startOffset": 174, "endOffset": 198}, {"referenceID": 28, "context": "Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009),", "startOffset": 223, "endOffset": 247}, {"referenceID": 43, "context": "classification relying on abstractness cues (Turney et al., 2011; K\u00f6per and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al.", "startOffset": 44, "endOffset": 99}, {"referenceID": 42, "context": ", 2011; K\u00f6per and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 39, "context": ", 2014), and soft clustering (Shutova et al., 2013), among others.", "startOffset": 29, "endOffset": 51}, {"referenceID": 26, "context": "Within the framework of Conceptual Metaphor Theory (Lakoff and Johnson, 1980) the metaphorical effect can be described as a mapping from a source domain to a target domain.", "startOffset": 51, "endOffset": 77}, {"referenceID": 2, "context": "As Bybee (2015) notes, and is also commonly agreed-upon, \u201cmetaphorical meaning changes create polysemy\u201d (p.", "startOffset": 3, "endOffset": 16}, {"referenceID": 32, "context": "As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016).", "startOffset": 132, "endOffset": 189}, {"referenceID": 35, "context": "As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016).", "startOffset": 132, "endOffset": 189}, {"referenceID": 40, "context": "As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016).", "startOffset": 132, "endOffset": 189}, {"referenceID": 32, "context": "texts (Rimell, 2014; Santus et al., 2014).", "startOffset": 6, "endOffset": 41}, {"referenceID": 35, "context": "texts (Rimell, 2014; Santus et al., 2014).", "startOffset": 6, "endOffset": 41}, {"referenceID": 36, "context": "The corpus is POS-tagged using the STTS tagset (Schiller et al., 1999).", "startOffset": 47, "endOffset": 70}, {"referenceID": 45, "context": "In hypernym detection a number of wellestablished measures compare the semantic generality of words on the basis of their distributional generality (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2009).", "startOffset": 148, "endOffset": 208}, {"referenceID": 4, "context": "In hypernym detection a number of wellestablished measures compare the semantic generality of words on the basis of their distributional generality (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2009).", "startOffset": 148, "endOffset": 208}, {"referenceID": 25, "context": "In hypernym detection a number of wellestablished measures compare the semantic generality of words on the basis of their distributional generality (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2009).", "startOffset": 148, "endOffset": 208}, {"referenceID": 34, "context": "measure seems to be word entropy, which is introduced in Santus (2013) and Santus et al.", "startOffset": 57, "endOffset": 71}, {"referenceID": 34, "context": "measure seems to be word entropy, which is introduced in Santus (2013) and Santus et al. (2014). Amongst other advantages, word entropy is independently measurable over time, which avoids the problem of vector space alignment.", "startOffset": 57, "endOffset": 96}, {"referenceID": 41, "context": "A number of structured annotation guidelines for synchronic metaphor identification have been proposed (Pragglejaz Group, 2007; Steen, 2010; Shutova, 2015).", "startOffset": 103, "endOffset": 155}, {"referenceID": 38, "context": "A number of structured annotation guidelines for synchronic metaphor identification have been proposed (Pragglejaz Group, 2007; Steen, 2010; Shutova, 2015).", "startOffset": 103, "endOffset": 155}, {"referenceID": 13, "context": "in German such as Fritz (2006) and Keller and Kirschbaum (2003).", "startOffset": 18, "endOffset": 31}, {"referenceID": 13, "context": "in German such as Fritz (2006) and Keller and Kirschbaum (2003). The richest list we found in Paul (2002) (ca.", "startOffset": 18, "endOffset": 64}, {"referenceID": 13, "context": "in German such as Fritz (2006) and Keller and Kirschbaum (2003). The richest list we found in Paul (2002) (ca.", "startOffset": 18, "endOffset": 106}, {"referenceID": 31, "context": "The annotation guidelines are a combination and modification of the processes described by Pragglejaz Group (2007), Steen (2010) and Shutova (2015).", "startOffset": 102, "endOffset": 115}, {"referenceID": 31, "context": "The annotation guidelines are a combination and modification of the processes described by Pragglejaz Group (2007), Steen (2010) and Shutova (2015).", "startOffset": 102, "endOffset": 129}, {"referenceID": 31, "context": "The annotation guidelines are a combination and modification of the processes described by Pragglejaz Group (2007), Steen (2010) and Shutova (2015). Whether a meaning of a target word in context 2 (M2) is metaphorically related to the meaning in context 1 (M1) should be identified in 3 steps:", "startOffset": 102, "endOffset": 148}, {"referenceID": 8, "context": "In addition to \u03ba we also give the share of items with perfect agreement (%A), since \u03ba underestimates agreement on rare effects (Feinstein and Cicchetti, 1990).", "startOffset": 127, "endOffset": 158}, {"referenceID": 14, "context": "As with Gulordava and Baroni (2011) or Hamilton et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 14, "context": "As with Gulordava and Baroni (2011) or Hamilton et al. (2016b), we assess the measures\u2019 performance by comparing their predictions in a corpus against a gold standard.", "startOffset": 8, "endOffset": 63}, {"referenceID": 40, "context": "richer input as indicated in Shwartz et al. (2016). Future studies should test how well word entropy distinguishes metaphoric change from other types of meaning innovation and how well it detects innovative and reductive meaning change in", "startOffset": 29, "endOffset": 51}, {"referenceID": 14, "context": "The latter may be tested straightforwardly on the English data of Gulordava and Baroni (2011) and Hamilton et al.", "startOffset": 66, "endOffset": 94}, {"referenceID": 14, "context": "The latter may be tested straightforwardly on the English data of Gulordava and Baroni (2011) and Hamilton et al. (2016b). In doing so, it will be interesting to see how our model performs in comparison to diachronic similarity and", "startOffset": 66, "endOffset": 122}], "year": 2017, "abstractText": "This paper explores the informationtheoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change. We also build the first diachronic test set for German as a standard for metaphoric change annotation. Our model shows high performance, is unsupervised, language-independent and generalizable to other processes of semantic change.", "creator": "LaTeX with hyperref package"}}}