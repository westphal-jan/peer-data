{"id": "1706.05111", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "A Mixture Model for Learning Multi-Sense Word Embeddings", "abstract": "In order to obtain good representations, it is important to take into account the different senses of a word. In this essay, we propose a mixing model for learning multi-sense word embedding. Our model generalizes the previous work by inducing different weights of different senses of a word. Experimental results show that our model outperforms previous models in standard assessment tasks.", "histories": [["v1", "Thu, 15 Jun 2017 23:07:06 GMT  (26kb)", "http://arxiv.org/abs/1706.05111v1", "*SEM 2017"]], "COMMENTS": "*SEM 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dai quoc nguyen", "dat quoc nguyen", "ashutosh modi", "stefan thater", "manfred pinkal"], "accepted": false, "id": "1706.05111"}, "pdf": {"name": "1706.05111.pdf", "metadata": {"source": "CRF", "title": "A Mixture Model for Learning Multi-Sense Word Embeddings", "authors": ["Dai Quoc Nguyen", "Dat Quoc Nguyen", "Ashutosh Modi", "Stefan Thater", "Manfred Pinkal"], "emails": ["pinkal}@coli.uni-saarland.de", "dat.nguyen@students.mq.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n05 11\n1v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n17\ntechnique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks."}, {"heading": "1 Introduction", "text": "Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014).\nHowever, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of\neach word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Schu\u0308tze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skip-gram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topic-assigned corpus.\nOne issue in these previous works is that they assign the same weight to every sense of a word. The central assumption of our work is that each sense of a word given a context, should correspond to a mixture of weights reflecting different association degrees of the word with multiple senses in the context. The mixture weights will help to model word meaning better.\nIn this paper, we propose a new model for learning Multi-Sense Word Embeddings (MSWE). Our MSWE model learns vector representations of a word based on a mixture of its sense representations. The key difference between MSWE and other models is that we induce the weights of senses while jointly learning the word and sense embeddings. Specifically, we train a topic model (Blei et al., 2003) to obtain the topic-to-word and document-to-topic probability distributions which are then used to infer the weights of topics. We use these weights to define a compositional vec-\ntor representation for each target word to predict its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most suitable topic of a word given its context, but we also take into consideration all possible meanings of the word.\nThe main contributions of our study are: (i) We introduce a mixture model for learning word and sense embeddings (MSWE) by inducing mixture weights of word senses. (ii) We show that MSWE performs better than the baseline Word2Vec Skipgram and other embedding models on the word analogy task (Mikolov et al., 2013a) and the word similarity task (Reisinger and Mooney, 2010)."}, {"heading": "2 The mixture model", "text": "In this section, we present the mixture model for learning multi-sense word embeddings. Here we treat topics as senses. The model learns a representation for each word using a mixture of its topical representations.\nGiven a number of topics and a corpus D of documents d = {wd,1, wd,2, ..., wd,Md}, we apply a topic model (Blei et al., 2003) to obtain the topic-to-word Pr(w|t) and document-to-topic Pr(t|d) probability distributions. We then infer a weight for themth word wd,m with topic t in document d:\n\u03bbd,m,t = Pr(wd,m|t)\u00d7 Pr(t|d) (1)\nWe define two MSWE variants: MSWE-1 learns vectors for words based on the most suitable topic given document d while MSWE-2 marginalizes over all senses of a word to take into account all possible senses of the word:\nMSWE-1: swd,m = vwd,m + \u03bbd,m,t\u2032 \u00d7 vt\u2032\n1 + \u03bbd,m,t\u2032\nMSWE-2: swd,m = vwd,m +\n\u2211T t=1 \u03bbd,m,t \u00d7 vt\n1 + \u2211T\nt=1 \u03bbd,m,t\nwhere swd,m is the compositional vector representation of themth word wd,m and the topics in document d; vw is the target vector representation of a word type w in vocabulary V ; vt is the vector representation of topic t; T is the number of topics;\n\u03bbd,m,t is defined as in Equation 1, and in MSWE-1 we define t\u2032 = argmax t \u03bbd,m,t.\nWe learn representations by minimizing the fol-\nlowing negative log-likelihood function:\nL = \u2212 \u2211\nd\u2208D\nMd \u2211\nm=1\n\u2211\n\u2212k\u2264j\u2264k j 6=0\nlog Pr(v\u0303wd,m+j |swd,m) (2)\nwhere the mth word wd,m in document d is a target word while the (m+j)th word wd,m+j in document d is a context word of wd,m and k is the context size. In addition, v\u0303w is the context vector representation of the word type w. The probability Pr(v\u0303wd,m+j |swd,m) is defined using the softmax function as follows:\nPr(v\u0303wd,m+j |swd,m) = exp(v\u0303Twd,m+jswd,m) \u2211\nc\u2032\u2208V exp(v\u0303 T c\u2032swd,m)\nSince computing log Pr(v\u0303wd,m+j |swd,m) is expensive for each training instance, we approximate log Pr(v\u0303wd,m+j |swd,m) in Equation 2 with the following negative-sampling objective (Mikolov et al., 2013b):\nOd,m,m+j = log \u03c3 ( v\u0303 T\nwd,m+j swd,m\n)\n+ K \u2211\ni=1\nlog \u03c3 (\n\u2212v\u0303Tciswd,m\n)\n(3)\nwhere each word ci is sampled from a noise distribution.1 In fact, MSWE can be viewed as a generalization of the well-known Word2Vec Skip-gram model with negative sampling (Mikolov et al., 2013b) where all the mixture weights \u03bbd,m,t are set to zero. The models are trained using Stochastic Gradient Descent (SGD)."}, {"heading": "3 Experiments", "text": "We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works.\nNote that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015;\n1We use an unigram distribution raised to the 3/4 power (Mikolov et al., 2013b) as the noise distribution.\nLevy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a larger training corpus, but this is not central point of our paper. The objective of our paper is that the embeddings of topic and word can be combined into a single mixture model, leading to good improvements as established empirically."}, {"heading": "3.1 Experimental Setup", "text": "Following Huang et al. (2012) and Neelakantan et al. (2014), we use the Wesbury Lab Wikipedia corpus (Shaoul and Westbury, 2010) containing over 2M articles with about 990M words for training. In the preprocessing step, texts are lowercased and tokenized, numbers are mapped to 0, and punctuation marks are removed. We extract a vocabulary of 200,000 most frequent word tokens from the pre-processed corpus. Words not occurring in the vocabulary are mapped to a special token UNK, in which we use the embedding of UNK for unknown words in the benchmark datasets.\nWe firstly use a small subset extracted from the WS353 dataset (Finkelstein et al., 2002) to tune the hyper-parameters of the baseline Word2Vec Skip-gram model for the word similarity task (see Section 3.2 for the task definition). We then directly use the tuned hyper-parameters for our MSWE variants. Vector size is also a hyperparameter. While some approaches use a higher number of dimensions to obtain better results, we fix the vector size to be 300 as used by the baseline for a fair comparison. The vanilla Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003) is not scalable to a very large corpus, so we explore faster online topic models developed for large corpora. We train the online LDA topic model (Hoffman et al., 2010) on the training corpus, and use the output of this topic model to compute the mixture weights as in Equation 1.2 We also use the same WS353 subset to tune the numbers of topics T \u2208 {50, 100, 200, 300, 400}. We find that the most suitable numbers are T = 50 and T = 200 then used for all our experiments. Here we learn 300-dimensional embeddings with the fixed context size k = 5 (in Equation 2) and K = 10 (in Equation 3) as used by the baseline. During train-\n2We use default parameters in gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) for the online LDA model.\ning, we randomly initialize model parameters (i.e. word and topic embeddings) and then learn them by using SGDwith the initial learning rate of 0.01."}, {"heading": "3.2 Word Similarity", "text": "The word similarity task evaluates the quality of word embedding models (Reisinger and Mooney, 2010). For a given dataset of word pairs, the evaluation is done by calculating correlation between the similarity scores of corresponding word embedding pairs with the human judgment scores. Higher Spearman\u2019s rank correlation (\u03c1) reflects better word embedding model. We evaluate MSWE on standard datasets (as given in Table 1) for the word similarity evaluation task.\nFollowing Reisinger and Mooney (2010), Huang et al. (2012), Neelakantan et al. (2014), we compute the similarity scores for a pair of words (w,w\u2032) with or without their respective contexts (c, c\u2032) as:\nGlobalSim ( w,w \u2032 )\n= cos (vw, vw\u2032)\nAvgSim ( w,w \u2032 ) = 1\nT 2\nT \u2211\nt=1\nT \u2211\nt\u2032=1\ncos (vw,t,vw\u2032,t\u2032)\nAvgSimC ( w,w \u2032 )\n= 1\nT 2\nT \u2211\nt=1\nT \u2211\nt\u2032=1\n(\n\u03b4 (vw,t,vc)\u00d7 \u03b4 (vw\u2032 ,t\u2032 ,vc\u2032)\n\u00d7 cos (vw,t,vw\u2032 ,t\u2032) )\nwhere vw is the vector representation of the word w, vw,t is the multiple representation of the word w and the topic t, vc is the vector representation of the context c of the word w. And cos (v,v\u2032) is the cosine similarity between two vectors v and v\u2032. For our experiments, we set vw,t = vw \u2295 (Pr(w|t) \u00d7 vt) and vc = ( 1\n|c|\n\u2211\nw\u2208c vw\n)\n\u2295\n( \u2211\nt Pr (t|c)\u00d7 vt), in which \u2295 is the concatenation operation and Pr (t|c) is inferred from the topic models by considering context c as a document. GlobalSim only regards word embeddings, while AvgSim considers multiple representations to capture different meanings (i.e. topics) and usages of a word. AvgSimC generalizes AvgSim by taking into account the likelihood \u03b4 (vw,t,vc) that word w takes topic t given context c. \u03b4 (v,v\u2032) is the inverse of the cosine distance from v to v\u2032 (Huang et al., 2012; Neelakantan et al., 2014)."}, {"heading": "3.2.1 Results for word similarity", "text": "Table 2 compares the evaluation results of MSWE with results reported in prior work on the standard word similarity task when using GlobalSim. We use subscripts 50 and 200 to denote the topic model trained with T = 50 and T = 200 topics, respectively. Table 2 shows that our model outperforms the baseline Word2Vec Skip-gram model (in fifth row from bottom). Specifically, on the RW dataset, MSWE obtains a significant improvement of 2.92 in the Spearman\u2019s rank correlation (which is about 8.5% relative improvement).\nCompared to the published results, MSWE obtains the highest accuracy on the RW, SCWS, WS353 and MEN datasets, and achieves the second highest result on the SIMLEX dataset. These indicate that MSWE learns better representations for\nwords taking into account different meanings."}, {"heading": "3.2.2 Results for contextual word similarity", "text": "We evaluate our model MSWE by using AvgSim and AvgSimC on the benchmark SCWS dataset which considers effects of the contextual information on the word similarity task. As shown in Table 3, MSWE scores better than the closely related model proposed by Cheng et al. (2015) and generally obtains good results for this context sensitive dataset. Although we produce better scores than Neelakantan et al. (2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC . Neelakantan et al. (2014) clustered the embeddings of the context words around each target word to predict its sense and Chen et al. (2014) used pre-trained word embeddings to initialize vector representations of senses taken from WordNet, while we use a fixed number of topics as senses for words in MSWE."}, {"heading": "3.3 Word Analogy", "text": "We evaluate the embedding models on the word analogy task introduced byMikolov et al. (2013a). The task aims to answer questions in the form of \u201ca is to b as c is to ?\u201d, denoted as \u201ca : b \u2192 c : ?\u201d (e.g., \u201cHanoi : Vietnam \u2192 Bern : ?\u201d). There are 8,869 semantic and 10,675 syntactic questions grouped into 14 categories. Each question is answered by finding the most suitable word closest to \u201cvb\u2212va+vc\u201d measured by the cosine similarity. The answer is correct only if the found closest word is exactly the same as the gold-standard (correct) one for the question.\nWe report accuracies in Table 4 and show that MSWE achieves better results in comparison with the baseline Word2Vec Skip-gram. In particular, MSWE reaches the accuracies of around 69.7% which is higher than the accuracy of 68.6% obtained by Word2Vec Skip-gram."}, {"heading": "4 Conclusions", "text": "In this paper, we described a mixture model for learning multi-sense embeddings. Our model induces mixture weights to represent a word given context based on a mixture of its sense representations. The results show that our model scores better than Word2Vec, and produces highly competitive results on the standard evaluation tasks. In future work, we will explore better methods for taking into account the contextual information. We also plan to explore different approaches to compute the mixture weights in our model. For example, if there is a large sense-annotated corpus available for training, the mixture weights could be defined based on the frequency (sense-count) distributions, instead of using the probability distributions produced by a topic model. Furthermore, it is possible to consider the weights of senses as additional model parameters to be then learned during training."}, {"heading": "Acknowledgments", "text": "This research was funded by the German Research Foundation (DFG) as part of SFB 1102 \u201cInformation Density and Linguistic Encoding\u201d. We would like to thank anonymous reviewers for their helpful comments."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Latent Dirichlet Allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning Research 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Improving distributed representation of word sense via wordnet gloss composition and context clustering", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1025\u20131035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Syntaxaware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1531\u20131542.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Contextual text understanding in distributional semantic space", "author": ["Jianpeng Cheng", "Zhongyuan Wang", "Ji-Rong Wen", "Jun Yan", "Zheng Chen."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM Transactions on Information Systems 20:116\u2013131.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Supersense embeddings: A unified model for supersense interpretation, prediction, and utilization", "author": ["Lucie Flekova", "Iryna Gurevych."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Flekova and Gurevych.,? 2016", "shortCiteRegEx": "Flekova and Gurevych.", "year": 2016}, {"title": "Word embedding evaluation and combination", "author": ["Sahar Ghannay", "Benoit Favre", "Yannick Estve", "Nathalie Camelin."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).", "citeRegEx": "Ghannay et al\\.,? 2016", "shortCiteRegEx": "Ghannay et al\\.", "year": 2016}, {"title": "Sensembed: Learning sense", "author": ["Roberto Navigli"], "venue": null, "citeRegEx": "Navigli.,? \\Q2015\\E", "shortCiteRegEx": "Navigli.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Sun."], "venue": "AAAI", "citeRegEx": "Sun.,? 2015b", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Event embeddings for seman", "author": ["Ashutosh Modi"], "venue": null, "citeRegEx": "Modi.,? \\Q2016\\E", "shortCiteRegEx": "Modi.", "year": 2016}, {"title": "Improving Topic Models", "author": ["Mark Johnson"], "venue": null, "citeRegEx": "Johnson.,? \\Q2015\\E", "shortCiteRegEx": "Johnson.", "year": 2015}, {"title": "Evaluation methods for", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q2015\\E", "shortCiteRegEx": "Joachims.", "year": 2015}, {"title": "The westbury", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Recursive deep models", "author": ["Christopher Potts"], "venue": null, "citeRegEx": "Potts.,? \\Q2013\\E", "shortCiteRegEx": "Potts.", "year": 2013}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguis-", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations via gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia", "author": ["Zhaohui Wu", "C. Lee Giles."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. pages 2188\u20132194.", "citeRegEx": "Wu and Giles.,? 2015", "shortCiteRegEx": "Wu and Giles.", "year": 2015}, {"title": "Improving short text classification by learning vector representations of both words and hidden topics", "author": ["Heng Zhang", "Guoqiang Zhong."], "venue": "Knowledge-Based Systems 102:76\u201386.", "citeRegEx": "Zhang and Zhong.,? 2016", "shortCiteRegEx": "Zhang and Zhong.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017).", "startOffset": 171, "endOffset": 333}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al.", "startOffset": 8, "endOffset": 744}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses.", "startOffset": 8, "endOffset": 765}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses.", "startOffset": 8, "endOffset": 789}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al.", "startOffset": 8, "endOffset": 945}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al.", "startOffset": 8, "endOffset": 965}, {"referenceID": 8, "context": ", 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al.", "startOffset": 8, "endOffset": 989}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al.", "startOffset": 35, "endOffset": 183}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.", "startOffset": 35, "endOffset": 208}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.", "startOffset": 35, "endOffset": 240}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets.", "startOffset": 35, "endOffset": 408}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets.", "startOffset": 35, "endOffset": 440}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al.", "startOffset": 35, "endOffset": 554}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al.", "startOffset": 35, "endOffset": 574}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skip-gram model (Mikolov et al.", "startOffset": 35, "endOffset": 594}, {"referenceID": 3, "context": "(2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch\u00fctze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skip-gram model (Mikolov et al.", "startOffset": 35, "endOffset": 621}, {"referenceID": 1, "context": "Specifically, we train a topic model (Blei et al., 2003) to obtain the topic-to-word and document-to-topic probability distributions which are then used to infer the weights of topics.", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics.", "startOffset": 51, "endOffset": 115}, {"referenceID": 20, "context": "MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics.", "startOffset": 51, "endOffset": 115}, {"referenceID": 1, "context": ", wd,Md}, we apply a topic model (Blei et al., 2003) to obtain the topic-to-word Pr(w|t) and document-to-topic Pr(t|d) probability distributions.", "startOffset": 33, "endOffset": 52}, {"referenceID": 15, "context": "(2014), we use the Wesbury Lab Wikipedia corpus (Shaoul and Westbury, 2010) containing over 2M articles with about 990M words for training.", "startOffset": 48, "endOffset": 75}, {"referenceID": 7, "context": "We firstly use a small subset extracted from the WS353 dataset (Finkelstein et al., 2002) to tune the hyper-parameters of the baseline Word2Vec Skip-gram model for the word similarity task (see Section 3.", "startOffset": 63, "endOffset": 89}, {"referenceID": 1, "context": "The vanilla Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003) is not scalable to a very large corpus, so we explore faster online topic models developed for large corpora.", "startOffset": 58, "endOffset": 77}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al.", "startOffset": 10, "endOffset": 36}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al. (2015) SCWS 2003 Huang et al.", "startOffset": 10, "endOffset": 66}, {"referenceID": 7, "context": "WS353 353 Finkelstein et al. (2002) SIMLEX 999 Hill et al. (2015) SCWS 2003 Huang et al. (2012)", "startOffset": 10, "endOffset": 96}, {"referenceID": 2, "context": "MEN 3000 Bruni et al. (2014)", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.", "startOffset": 4, "endOffset": 23}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.", "startOffset": 4, "endOffset": 55}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.4 \u2013 65.5 69.9 Vilnis and McCallum (2015) \u2013 32.", "startOffset": 4, "endOffset": 101}, {"referenceID": 3, "context": "2 \u2013 Chen et al. (2014) \u2013 \u2013 64.2 \u2013 \u2013 Hill et al. (2015) \u2013 41.4 \u2013 65.5 69.9 Vilnis and McCallum (2015) \u2013 32.23 \u2013 65.49 71.31 Schnabel et al. (2015) \u2013 \u2013 \u2013 64.", "startOffset": 4, "endOffset": 146}, {"referenceID": 8, "context": "9 Flekova and Gurevych (2016) \u2013 \u2013 \u2013 \u2013 74.", "startOffset": 2, "endOffset": 30}, {"referenceID": 3, "context": "3 Chen et al. (2014) 66.", "startOffset": 2, "endOffset": 21}, {"referenceID": 4, "context": "As shown in Table 3, MSWE scores better than the closely related model proposed by Cheng et al. (2015) and generally obtains good results for this context sensitive dataset.", "startOffset": 83, "endOffset": 103}, {"referenceID": 4, "context": "As shown in Table 3, MSWE scores better than the closely related model proposed by Cheng et al. (2015) and generally obtains good results for this context sensitive dataset. Although we produce better scores than Neelakantan et al. (2014) and Chen et al.", "startOffset": 83, "endOffset": 239}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC .", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC . Neelakantan et al. (2014) clustered the embeddings of the context words around each target word to predict its sense and Chen et al.", "startOffset": 11, "endOffset": 138}, {"referenceID": 3, "context": "(2014) and Chen et al. (2014) when using GlobalSim, we are outperformed by them when using AvgSim and AvgSimC . Neelakantan et al. (2014) clustered the embeddings of the context words around each target word to predict its sense and Chen et al. (2014) used pre-trained word embeddings to initialize vector representations of senses taken from WordNet, while we use a fixed number of topics as senses for words in MSWE.", "startOffset": 11, "endOffset": 252}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.0 Neelakantan et al. (2014) 64.", "startOffset": 2, "endOffset": 54}, {"referenceID": 0, "context": "3 Baroni et al. (2014) 68.0 Neelakantan et al. (2014) 64.0 Ghannay et al. (2016) 62.", "startOffset": 2, "endOffset": 81}], "year": 2017, "abstractText": "Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.", "creator": "LaTeX with hyperref package"}}}