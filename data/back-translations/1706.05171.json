{"id": "1706.05171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Improving Scalability of Inductive Logic Programming via Pruning and Best-Effort Optimisation", "abstract": "Inductive Logic Programming (ILP) combines rules-based and statistical artificial intelligence methods by learning a hypothesis that includes a set of rules with background knowledge and search space constraints. We focus on extending the XHAIL algorithm for ILP, which is based on Answer Set Programming, and we evaluate our enhancements by applying Natural Language Processing of sentence chunking. In terms of processing natural language, ILP can take into account the constant change in the way we use language on a daily basis. At the same time, ILP does not require large quantities of training examples like other statistical methods and provides interpretable results, i.e. a set of rules that can be analyzed and optimized as needed. As contributions, we extend XHAIL by (i) a printing mechanism within the hypothesis generalization algorithm that allows learning from larger data sets, (ii) a better use of modern solution technologies using recently developed optimization methods, we compare the results with (ii) an optimal set of tasks (ii).", "histories": [["v1", "Fri, 16 Jun 2017 08:02:55 GMT  (34kb)", "http://arxiv.org/abs/1706.05171v1", "24 pages, preprint of article accepted at Expert Systems With Applications"]], "COMMENTS": "24 pages, preprint of article accepted at Expert Systems With Applications", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mishal kazmi", "peter sch\\\"uller", "y\\\"ucel sayg{\\i}n"], "accepted": false, "id": "1706.05171"}, "pdf": {"name": "1706.05171.pdf", "metadata": {"source": "CRF", "title": "Improving Scalability of Inductive Logic Programming via Pruning and Best-Effort Optimisation", "authors": ["Mishal Kazmi", "Peter Sch\u00fcller"], "emails": ["mishalkazmi@sabanciuniv.edu", "ysaygin@sabanciuniv.edu", "peter.schuller@marmara.edu.tr", "schueller.p@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n05 17\n1v 1\n[ cs\n.A I]"}, {"heading": "1 Introduction", "text": "Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) is a formalism where a set of logical rules is learned from a set of examples and a background knowledge theory. By combining rule-based and statistical artificial intelligence, ILP overcomes the brittleness of pure logic-based approaches and the lack of interpretability of models of most statistical methods such as neural networks or support vector machines. We here focus on ILP that is based on Answer Set Programming (ASP) as our underlying logic programming language because we aim to apply ILP to Natural Language Processing (NLP) applications such as Machine Translation, Summarization, Coreference Resolution, or Parsing that require nonmonotonic reasoning with exceptions and complex background theories.\nIn our work, we apply ILP to the NLP task of sentence chunking. Chunking, also known as \u2018shallow parsing\u2019, is the identification of short phrases such as noun phrases which mainly rely on Part of Speech (POS) tags. In our experiments on sentence chunking (Tjong Kim Sang and Buchholz, 2000) we encountered several problems with state-of-the-art ASP-based ILP systems XHAIL (Ray, 2009), ILED\n1\n(Katzouris et al., 2015), and ILASP2 (Law et al., 2015). XHAIL and ILASP2 showed scalability issues already with 100 sentences as training data. ILED is designed to be highly scalable but failed in the presence of simple inconsistencies in examples. We decided to investigate the issue in the XHAIL system, which is open-source and documented well, and we made the following observations:\n(i) XHAIL only terminates if it finds a provably optimal hypothesis,\n(ii) the hypothesis search is done over all potentially beneficial rules that are supported by at least one example, and\n(iii) XHAIL contains redundancies in hypothesis search and uses outdated ASP technology.\nIn larger datasets, observation (i) is unrealistic, because finding a near-optimal solution is much easier than proving optimality of the best solution, moreover in classical machine learning suboptimal solutions obtained via non-exact methods routinely provide state-of-the-art results. Similarly, observation (ii) makes it harder to find a hypothesis, and it generates an overfitting hypotheses which contains rules that are only required for a single example. Observation (iii) points out an engineering problem that can be remedied with little theoretical effort.\nTo overcome the above issues, we modified the XHAIL algorithm and software, and we performed experiments on a simple NLP chunking task to evaluate our modifications.\nIn detail, we make the following contributions.\n\u2022 We extend XHAIL with best-effort optimisation using the newest ASP optimisation technology of unsat-core optimisation (Andres et al., 2012) with stratification (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013) and core shrinking (Alviano and Dodaro, 2016) using the WASP2 (Alviano et al., 2013, 2015a) solver and the Gringo (Gebser et al., 2011) grounder. We also extend XHAIL to provide information about the optimality of the hypothesis.\n\u2022 We extend the XHAIL algorithm with a parameter Pr for pruning, such that XHAIL searches for hypotheses without considering rules that are supported by fewer than Pr examples.\n\u2022 We eliminate several redundancies in XHAIL by changing its internal data structures.\n\u2022 We describe a framework for chunking with ILP, based on preprocessing with Stanford Core NLP (Manning et al., 2014) tools.\n\u2022 We experimentally analyse the relationship between the pruning parameter, number of training examples, and prediction score on the sentence chunking (Tjong Kim Sang and Buchholz, 2000) subtask of iSTS at SemEval 2016 (Agirre et al., 2016).\n\u2022 We discuss the best hypothesis found for each of the three datasets in the SemEval task, and we discuss what can be learned about the dataset from these hypotheses.\nOnly if we use all the above modifications together, XHAIL becomes applicable in this chunking task. By learning a hypothesis from 500 examples, we can achieve results competitive with state-of-the-art systems used in the SemEval 2016 competition.\nOur extensions and modifications of the XHAIL software are available in a public fork of the official XHAIL Git repository (Bragaglia and Sch\u00fcller, 2016).\nIn Section 2 we provide an overview of logic programming and ILP. Section 3 gives an account of related work and available ILP tools. In Section 4 we describe the XHAIL system and our extensions of pruning, best-effort optimisation, and further improvements. Section 5 gives details of our representation of the chunking task. In Section 6 we discuss empirical experiments and results. We conclude in Section 7 with a brief outlook on future work."}, {"heading": "2 Background", "text": "We next introduce logic programming and based on that inductive logic programming.\n2"}, {"heading": "2.1 Logic Programming", "text": "A logic programs theory normally comprises of an alphabet (variable, constant, quantifier, etc), vocabulary, logical symbols, a set of axioms and inference rules (Lloyd, 2012). A logic programming system consists of two portions: the logic and control. Logic describes what kind of problem needs to be solved and control is how that problem can be solved. An ideal of logic programming is for it to be purely declarative. The popular Prolog (Clocksin and Mellish, 2003) system evaluates rules using resolution, which makes the result of a Prolog program depending on the order of its rules and on the order of the bodies of its rules. Answer Set Programming (ASP) (Brewka et al., 2011; Gebser et al., 2012a) is a more recent logic programming formalism, featuring more declarativity than Prolog by defining semantics based on Herbrand models (Gelfond and Lifschitz, 1988). Hence the order of rules and the order of the body of the rules does not matter in ASP. Most ASP programs follow the Generate-Define-Test structure (Lifschitz, 2002) to (i) generate a space of potential solutions, (ii) define auxiliary concepts, and (iii) test to invalidate solutions using constraints or incurring a cost on non-preferred solutions.\nAn ASP program consists of rules of the following structure:\na\u2190 b1, . . . , bm,not bm+1, . . . ,not bn\nwhere a, bi are atoms from a first-order language, a is the head and b1, . . . ,not bn is the body of the rule, and not is negation as failure. Variables start with capital letters, facts (rules without body condition) are written as \u2018a.\u2019 instead of \u2018a\u2190 \u2019. Intuitively a is true if all positive body atoms are true and no negative body atom is true.\nThe formalism can be understood more clearly by considering the following sentence as a simple example:\nComputers are normally fast machines unless they are old.\nThis would be represented as a logical rule as follows:\nfastmachine(X)\u2190 computer(X),not old(X).\nwhere X is a variable, fastmachine , computer , and old are predicates, and old(X ) is a negated atom. Adding more knowledge results in a change of a previous understanding, this is common in human reasoning. Classical First Order Logic does not allow such non-monotonic reasoning, however, ASP was designed as a commonsense reasoning formalism: a program has zero or more answer sets as solutions, adding knowledge to the program can remove answer sets as well as produce new ones. Note that ASP semantics rule out self-founded truths in answer sets. We use the ASP formalism due to its flexibility and declarativity. For formal details and a complete description of syntax and semantics see the ASPCore-2 standard (Calimeri et al., 2012). ASP has been applied to several problems related to Natural Language Processing, see for example (Mitra and Baral, 2016; Sch\u00fcller, 2013, 2014, 2016; Schwitter, 2012; Sharma et al., 2015). An overview of applications of ASP in general can be found in (Erdem et al., 2016)."}, {"heading": "2.2 Inductive Logic Programming", "text": "Processing natural language based on hand-crafted rules is impractical because human language is constantly evolving, partially due to the human creativity of language use. An example of this was recently noticed on UK highways where they advised drivers, \u2018Don\u2019t Pok\u00e9mon Go and drive\u2019. Pok\u00e9mon Go is being informally used here as a verb even though it was only introduced as a game a few weeks before the sign was put up. To produce robust systems, it is necessary to use statistical models of language. These models are often pure Machine Learning (ML) estimators without any rule components (Manning and Sch\u00fctze, 1999). ML methods work very well in practice, however, they usually do not provide a way for explaining why a certain prediction was made, because they represent the learned knowledge in big matrices of real numbers. Some popular classifiers used for processing natural language include Naive Bayes, Decision Trees, Neural Networks, and Support Vector Machines (SVMs) (Dumais et al., 1998).\nIn this work, we focus on an approach that combines rule-based methods and statistics and provides interpretable learned models: Inductive Logic Programming (ILP). ILP is differentiated from ML techniques by its use of an expressive representation language and its ability to make use of logically\n3\nencoded background knowledge (Muggleton and De Raedt, 1994). An important advantage of ILP over ML techniques such as neural networks is, that a hypothesis can be made readable by translating it into piece of English text. Furthermore, if annotated corpora of sufficient size are not available or too expensive to produce, deep learning or other data intense techniques are not applicable. However, we can still learn successfully with ILP.\nFormally, ILP takes as input a set of examples E, a set B of background knowledge rules, and a set of mode declarations M , also called mode bias. As output, ILP aims to produce a set of rules H called hypothesis which entails E with respect to B. The search for H with respect to E and B is restricted by M , which defines a language that limits the shape of rules in the hypothesis candidates and therefore the complexity of potential hypotheses.\nExample 1. Consider the following example ILP instance (M,E,B) (Ray, 2009).\nM =\n\n\n #modeh flies(+bird). #modeb penguin(+bird). #modeb not penguin(+bird).\n\n\n\n(1)\nE =\n\n \n   #example flies(a). #example flies(b). #example flies(c). #example notflies(d).\n\n \n  \n(2)\nB =\n\n    \n     bird(X) :- penguin(X). bird(a). bird(b). bird(c). penguin(d).\n\n    \n    \n(3)\nBased on this, an ILP system would ideally find the following hypothesis.\nH = { flies(X) :- bird(X), not penguin(X). }\n(4)"}, {"heading": "3 Related Work", "text": "Inductive Logic Programming (ILP) is a rather multidisciplinary field which extends to domains such as computer science, artificial intelligence, and bioinformatics. Research done in ILP has been greatly impacted by Machine Learning (ML), Artificial Intelligence (AI) and relational databases. Quite a few surveys (Gulwani et al., 2015; Kitzelmann, 2009; Muggleton et al., 2012) mention about the systems and applications of ILP in interdisciplinary areas. We next give related work of ILP in general and then focus on ILP applied in the field of Natural Language Processing (NLP).\nThe foundations of ILP can be found in research by Plotkin (Plotkin, 1970, 1971), Shapiro (Shapiro, 1983) and Sammut and Banerji (Sammut and Banerji, 1986). The founding paper of Muggleton (Muggleton, 1991) led to the launch of the first international workshop on ILP. The strength of ILP lay in its ability to draw on and extend the existing successful paradigms of ML and Logic Programming. At the beginning, ILP was associated with the introduction of foundational theoretical concepts which included Inverse Resolution (Muggleton, 1995; Muggleton and Buntine, 1992) and Predicate Invention (Muggleton, 1991; Muggleton and Buntine, 1992). A number of ILP systems were developed along with learning about the theoretical concepts of ILP such as FOIL (Quinlan, 1990) and Golem (Muggleton et al., 1990). The widely-used ILP system Progol (Muggleton, 1995) introduced a new logically-based approach to refinement graph search of the hypothesis space based on inverting the entailment relation. Meanwhile, the TILDE system (De Raedt, 1997) demonstrated the efficiency which could be gained by upgrading decision-tree learning algorithms to first-order logic, this was soon extended towards other ML problems. Some limitations of Prolog-based ILP include requiring extensional background and negative examples, lack of predicate invention, search limitations and inability to handle cuts. Integrating bottom-up and top-down searches, incorporating predicate invention, eliminating the need for explicit negative examples and allowing restricted use of cuts helps in solving these issues (Mooney, 1996).\n4\nProbabilistic ILP (PILP) also gained popularity (Cussens, 2001a; De Raedt and Kersting, 2008; Muggleton et al., 1996), its Prolog-based systems such as PRISM (Sato et al., 2005) and FAM (Cussens, 2001b) separate the actual learning of the logic program from the probabilistic parameters estimation of the individual clauses. However in practice, learning the structure and parameters of probabilistic logic representation simultaneously has proven to be a challenge (Muggleton, 2002). PILP is mainly a unification of the probabilistic reasoning of Machine Learning with the relational logical representations offered by ILP.\nMeta-interpretive learning (MIL) (Muggleton et al., 2014) is a recent ILP method which learns recursive definitions using Prolog and ASP-based declarative representations. MIL is an extension of the Prolog meta-interpreter; it derives a proof by repeatedly fetching the first-order Prolog clauses and additionally fetching higher-order meta-rules whose heads unify with a given goal, and saves the resulting meta-substitutions to form a program.\nMost ILP research has been aimed at Horn programs which exclude Negation as Failure (NAF). Negation is a key feature of logic programming and provides a means for monotonic commonsense reasoning under incomplete information. This fails to exploit the full potential of normal programs that allow NAF.\nWe next give an overview of ILP systems based on ASP that are designed to operate in the presence of negation. Then we give an overview of ILP literature related to NLP."}, {"heading": "3.1 ASP-based ILP Systems", "text": "The eXtended Hybrid Abductive Inductive Learning system (XHAIL) is an ILP approach based on ASP that generalises techniques of language and search bias from Horn clauses to normal logic programs with full usage of NAF (Ray, 2009). Like its predecessor system Hybrid Abductive Inductive Learning (HAIL) which operated on Horn clauses, XHAIL is based on Abductive Logic Programming (ALP) (Kakas et al., 1992), we give more details on XHAIL in Section 4.\nThe Incremental Learning of Event Definitions (ILED) algorithm (Katzouris et al., 2015) relies on Abductive-Inductive learning and comprises of a scalable clause refinement methodology based on a compressive summarization of clause coverage in a stream of examples. Previous ILP learners were batch learners and required all training data to be in place prior to the initiation of the learning process. ILED learns incrementally by processing training instances when they become available and altering previous inferred knowledge to fit new observation, this is also known as theory revision. It exploits previous computations to speed-up the learning since revising the hypothesis is considered more efficient than learning from scratch. ILED attempts to cover a maximum of examples by re-iterating over previously seen examples when the hypothesis has been refined. While XHAIL can ensure optimal example coverage easily by processing all examples at once, ILED does not preserve this property due to a non-global view on examples.\nWhen considering ASP-based ILP, negation in the body of rules is not the only interesting addition to the overall concept of ILP. An ASP program can have several independent solutions, called answer sets, of the program. Even the background knowledge B can admit several answer sets without any addition of facts from examples. Therefore, a hypothesis H can cover some examples in one answer set, while others are covered by another answer set. XHAIL and ILED approaches are based on finding a hypothesis that is covering all examples in a single answer set.\nThe Inductive Learning of Answer Set Programs approach (ILASP) is an extension of the notion of learning from answer sets (Law et al., 2014). Importantly, it covers positive examples bravely (i.e., in at least one answer set) and ensures that the negation of negative examples is cautiously entailed (i.e., no negative example is covered in any answer set). Negative examples are needed to learn Answer Set Programs with non-determinism otherwise there is no concept of what should not be in an Answer Set. ILASP conducts a search in multiple stages for brave and cautious entailment and processes all examples at once. ILASP performs a less informed hypothesis search than XHAIL or ILED, that means large hypothesis spaces are infeasible for ILASP while they are not problematic for XHAIL and ILED, on the other hand, ILASP supports aggregates and constraints while the older systems do not support these. ILASP2 (Law et al., 2015) extends the hypothesis space of ILASP with choice rules and weak constraints. This permits searching for hypotheses that encode preference relations.\n5"}, {"heading": "3.2 ILP and NLP", "text": "From NLP point of view, the hope of ILP is to be able to steer a mid-course between these two alternatives of large-scale but shallow levels of analysis and small scale but deep and precise analysis. ILP should produce a better ratio between breadth of coverage and depth of analysis (Muggleton, 1999). ILP has been applied to the field of NLP successfully; it has not only been shown to have higher accuracies than various other ML approaches in learning the past tense of English but also shown to be capable of learning accurate grammars which translate sentences into deductive database queries (Law et al., 2014).\nExcept for one early application (Wirth, 1989) no application of ILP methods surfaced until the system CHILL (Mooney, 1996) was developed which learned a shift-reduce parser in Prolog from a training corpus of sentences paired with the desired parses by learning control rules and uses ILP to learn control strategies within this framework. This work also raised several issues regarding the capabilities and testing of ILP systems. CHILL was also used for parsing database queries to automate the construction of a natural language interface (Zelle and Mooney, 1996) and helped in demonstrating its ability to learn semantic mappings as well.\nAn extension of CHILL, CHILLIN (Zelle et al., 1994) was used along with an extension of FOIL, mFOIL (Tang and Mooney, 2001) for semantic parsing. Where CHILLIN combines top-down and bottom-up induction methods and mFOIL is a top-down ILP algorithm designed keeping imperfect data in mind, which portrays whether a clause refinement is significant for the overall performance with the help of a pre-pruning algorithm. This emphasised on how the combination of multiple clause constructors helps improve the overall learning; which is a rather similar concept to Ensemble Methods in standard ML. Note that CHILLIN pruning is based on probability estimates and has the purpose of dealing with inconsistency in the data. Opposed to that, XHAIL already supports learning from inconsistent data, and the pruning we discuss in Section 4.1 aims to increase scalability.\nPrevious work ILP systems such as TILDE and Aleph (Srinivasan, 2001) have been applied to preference learning which addressed learning ratings such as good, poor and bad. ASP expresses preferences through weak constraints and may also contain weak constraints or optimisation statements which impose an ordering on the answer sets (Law et al., 2015).\nThe system of Mitra and Baral (Mitra and Baral, 2016) uses ASP as primary knowledge representation and reasoning language to address the task of Question Answering. They use a rule layer that is partially learned with XHAIL to connect results from an Abstract Meaning Representation parser and an Event Calculus theory as background knowledge."}, {"heading": "4 Extending XHAIL algorithm and system", "text": "Initially, we intended to use the latest ILP systems (ILASP2 or ILED) in our work. However, preliminary experiments with ILASP2 showed a lack in scalability (memory usage) even for only 100 sentences due to the unguided hypothesis search space. Moreover, experiments with ILED uncovered several problematic corner cases in the ILED algorithm that led to empty hypotheses when processing examples that were mutually inconsistent (which cannot be avoided in real-life NLP data). While trying to fix these problems in the algorithm, further issues in the ILED implementation came up. After consulting the authors of (Mitra and Baral, 2016) we learned that they had the same issues and used XHAIL, therefore we also opted to base our research on XHAIL due to it being the most robust tool for our task in comparison to the others.\nAlthough XHAIL is applicable, we discovered several drawbacks and improved the approach and the XHAIL system. We provide an overview of the parts we changed and then present our modifications. Figure 1 shows in the middle the original XHAIL components and on the right our extension.\nXHAIL finds a hypothesis using several steps. Initially the examples E plus background knowledge B are transformed into a theory of Abductive Logic Programming (Kakas et al., 1992). The Abduction part of XHAIL explains observations with respect to a prior theory, which yields the Kernel Set, \u2206. \u2206 is a set of potential heads of rules given by M such that a maximum of examples E is satisfied together with B.\nExample 2 (continued). Given (M,E,B) from Example 1, XHAIL uses B, E, and the head part of M ,\n6\nto generate the Kernel Set \u2206 by abduction.\n\u2206 =\n\n\n flies(a) flies(b) flies(c)\n\n\n\nThe Deduction part uses \u2206 and the body part of the mode bias M to generate a ground program K. K contains rules which define atoms in \u2206 as true based on B and E.\nThe Generalisation part replaces constant terms in K with variables according to the mode bias M , which yields a non-ground program K \u2032.\nExample 3 (continued). From the above \u2206 and M from (1), deduction and generalisation yield the following K and K \u2032.\nK =\n\n\n flies(a) :- bird(a), not penguin(a) flies(b) :- bird(b), not penguin(b) flies(c) :- bird(c), not penguin(c)\n\n\n\nK \u2032 =\n\n\n flies(X) :- bird(X), not penguin(X) flies(Y) :- bird(Y), not penguin(Y) flies(Z) :- bird(Z), not penguin(Z)\n\n\n\nThe Induction part searches for the smallest part of K \u2032 that entails as many examples of E as possible given B. This part of K \u2032 which can contain a subset of the rules of K \u2032 and for each rule a subset of body atoms is called a hypothesis H .\nExample 4 (continued). The smallest hypothesis that covers all examples E in (2) is (4).\nWe next describe our modifications of XHAIL."}, {"heading": "4.1 Kernel Pruning according to Support", "text": "The computationally most expensive part of the search in XHAIL is Induction. Each non-ground rule in K \u2032 is rewritten into a combination of several guesses, one guess for the rule and one additional guess for each body atom in the rule.\nWe moreover observed that some non-ground rules in K \u2032 are generalisations of many different ground rules in K, while some non-ground rules correspond with only a single instance in K. In the following, we say that the support of r in K is the number of ground rules in K that are transformed into r\u2208K \u2032 in the Generalisation module of XHAIL (see Figure 1).\n7\nIntuitively, the higher the support, the more examples can be covered with that rule, and the more likely that rule or a part of it will be included in the optimal hypothesis.\nTherefore we modified the XHAIL algorithm as follows.\n\u2022 During Generalisation, we keep track of the support of each rule r\u2208K \u2032 by counting how often a generalisation yields the same rule r.\n\u2022 We add an integer pruning parameter Pr to the algorithm and use only those rules from K \u2032 in the Induction component that have a support higher than Pr.\nThis modification is depicted as bold components which replace the dotted Generalisation module in Figure 1.\nPruning has several consequences. From a theoretical point of view, the algorithm becomes incomplete for Pr > 0, because Induction searches in a subset of the relevant hypotheses. Hence Induction might not be able to find a hypothesis that covers all examples, although such a hypothesis might exist with Pr=0. From a practical point of view, pruning realises something akin to regularisation in classical ML; only strong patterns in the data will find their way into Induction and have the possibility to be represented in the hypothesis. A bit of pruning will therefore automatically prevent overfitting and generate more general hypotheses. As we will show in Experiments in Section 6, the pruning allows to configure a trade-off between considering low-support rules instead of omitting them entirely, as well as, finding a more optimal hypothesis in comparison to a highly suboptimal one."}, {"heading": "4.2 Unsat-core based and Best-effort Optimisation", "text": "We observed that ASP search in XHAIL Abduction and Induction components progresses very slowly from a suboptimal to an optimal solution. XHAIL integrates version 3 of Gringo (Gebser et al., 2011) and Clasp (Gebser et al., 2012b) which are both quite outdated. In particular Clasp in this version does not support three important improvements that have been found for ASP optimisation: (i) unsat-core optimisation (Andres et al., 2012), (ii) stratification for obtaining suboptimal answer sets (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013), and (iii) unsat-core shrinking (Alviano and Dodaro, 2016).\nMethod (i) inverts the classical branch-and-bound search methodology which progresses from worst to better solutions. Unsat-core optimisation assumes all costs can be avoided and finds unsatisfiable cores of the problem until the assumption is true and a feasible solution is found. This has the disadvantage of providing only the final optimal solution, and to circumvent this disadvantage, stratification in method (ii) was developed which allows for combining branch-and-bound with method (i) to approach the optimal value both from cost 0 and from infinite cost. Furthermore, unsat-core shrinking in method (iii), also called \u2018anytime ASP optimisation\u2019, has the purpose of providing suboptimal solutions and aims to find smaller cores which can speed up the search significantly by cutting more of the search space (at the cost of searching for a smaller core). In experiments with the inductive encoding of XHAIL we found that all three methods have a beneficial effect.\nCurrently, only the WASP solver (Alviano et al., 2013, 2015a) supports all of (i), (ii), and (iii), therefore we integratedWASP into XHAIL, which has a different output than Clasp. We also upgraded XHAIL to use Gringo version 4 which uses the new ASP-Core-2 standard and has some further (performance) advantages over older versions.\nUnsat-core optimisation often finds solutions with a reasonable cost, near the optimal value, and then takes a long time to find the true optimum or prove optimality of the found solution. Therefore, we extended XHAIL as follows:\n\u2022 a time budget for search can be specified on the command line,\n\u2022 after the time budget is elapsed the best-known solution at that point is used and the algorithm continues, furthermore\n\u2022 the distance from the optimal value is provided as output.\nThis affects the Induction step in Figure 1 and introduces a best-effort strategy; along with the obtained hypothesis we also get the distance from the optimal hypothesis, which is zero for optimal solutions.\nUsing a suboptimal hypothesis means, that either fewer examples are covered by the hypothesis than possible, or that the hypothesis is bigger than necessary. In practice, receiving a result is better than\n8\nreceiving no result at all, and our experiments show that XHAIL becomes applicable to reasonably-sized datasets using these extensions."}, {"heading": "4.3 Other Improvements", "text": "We made two minor engineering contributions to XHAIL. A practically effective improvement of XHAIL concerns K \u2032. As seen in Example 3, three rules that are equivalent modulo variable renaming are contained in K \u2032. XHAIL contains canonicalization algorithms for avoiding such situations, based on hashing body elements of rules. However, we found that for cases with more than one variable and for cases with more than one body atom, these algorithms are not effective because XHAIL (i) uses a set data structure that maintains an order over elements, (ii) the set data structure is sensitive to insertion order, and (iii) hashing the set relies on the order to be canonical. We made this canonicalization algorithm applicable to a far wider range of cases by changing the data type of rule bodies in XHAIL to a set that maintains an order depending on the value of set elements. This comes at a very low additional cost for set insertion and often reduces size of K \u2032 (and therefore computational effort for Induction step) without adversely changing the result of induction.\nAnother improvement concerns debugging the ASP solver. XHAIL starts the external ASP solver and waits for the result. During ASP solving, no output is visible, however, ASP solvers provide output that is important for tracking the distance from optimality during a search. We extended XHAIL so that the output of the ASP solver can be made visible during the run using a command line option."}, {"heading": "5 Chunking with ILP", "text": "We evaluate the improvements of the previous section using the NLP task of chunking. Chunking (Tjong Kim Sang and Buchholz, 2000) or shallow parsing is the identification of short phrases such as noun phrases or prepositional phrases, usually based heavily on Part of Speech (POS) tags. POS provides only information about the token type, i.e., whether words are nouns, verbs, adjectives, etc., and chunking derives from that a shallow phrase structure, in our case a single level of chunks.\nOur framework for chunking has three main parts as shown in Figure 2. Preprocessing is done using the Stanford CoreNLP tool from which we obtain the facts that are added to the background knowledge of XHAIL or used with a hypothesis to predict the chunks of an input. Using XHAIL as our ILP solver we learn a hypothesis (an ASP program) from the background knowledge, mode bias, and from examples which are generated using the gold-standard data. We predict chunks using our learned hypothesis and facts from preprocessing, using the Clingo (Gebser et al., 2008) ASP solver. We test by scoring predictions against gold chunk annotations.\nExample 5. An example sentence in the SemEval iSTS dataset (Agirre et al., 2016) is as follows.\nFormer Nazi death camp guard Demjanjuk dead at 91 (5)\nThe chunking present in the SemEval gold standard is as follows.\n[ Former Nazi death camp guard Demjanjuk ] [ dead ] [ at 91 ] (6)"}, {"heading": "5.1 Preprocessing", "text": "Stanford CoreNLP tools (Manning et al., 2014) are used for tokenisations and POS-tagging of the input. Using a shallow parser (Bohnet et al., 2013) we obtain the dependency relations for the sentences. Our ASP representation contains atoms of the following form:\n9\n\u2022 pos(P ,T ) which represents that token T has POS tag P ,\n\u2022 form(T ,Text) which represents that token T has surface form Text ,\n\u2022 head(T1 ,T2 ) and rel(R,T ) which represent that token T2 depends on token T1 with dependency relation R.\nExample 6 (continued). Figure 3a shows the result of preprocessing performed on sentence (5), which is a set of ASP facts.\nWe use Penn Treebank POS-tags as they are provided by Stanford CoreNLP. To form valid ASP constant terms from POS-tags, we prefix them with \u2018c_\u2019, replace special characters with lowercase letters (e.g., \u2018PRP$\u2019 becomes \u2018c_PRPd\u2019). In addition, we create specific POS-tags for punctuation (see Section 6.4)."}, {"heading": "5.2 Background Knowledge and Mode Bias", "text": "Background Knowledge we use is shown in Figure 3b. We define which POS-tags can exist in predicate postype/1 and which tokens exist in predicate token/1. Moreover, we provide for each token the POS-tag of its successors token in predicate nextpos/2.\nMode bias conditions are shown in Figure 3c, these limit the search space for hypothesis generation. Hypothesis rules contain as head atoms of the form\nsplit(T )\n10\nwhich indicates, that a chunk ends at token T and a new chunk starts at token T +1. The argument of predicates split/1 in the head is of type token.\nThe body of hypothesis rules can contain pos/2 and nextpos/2 predicates, where the first argument is a constant of type postype (which is defined in Figure 3b) and the second argument is a variable of type token. Hence this mode bias searches for rules defining chunk splits based on POS-tag of the token and the next token.\nWe deliberately use a very simple mode bias that does not make use of all atoms in the facts obtained from preprocessing. This is discussed in Section 6.5."}, {"heading": "5.3 Learning with ILP", "text": "Learning with ILP is based on examples that guide the search. Figure 3d shows rules that recognise gold standard chunks and #example instructions that define for XHAIL which atoms must be true to entail an example. These rules with goodchunk/1 in the head define what a good (i.e., gold standard) chunk is in each example based on where a split in a chunk occurs in the training data to help in the learning of a hypothesis for chunking.\nNote that negation is present only in these rules, although we could use it anywhere else in the background knowledge. Using the background knowledge, mode bias, and examples, XHAIL is then able to learn a hypothesis."}, {"heading": "5.4 Chunking with ASP using Learned Hypothesis", "text": "The hypothesis generated by XHAIL can then be used together with the background knowledge specified in Figure 3b, and with the preprocessed input of a new sentence. Evaluating all these rules yields a set of split points in the sentence, which corresponds to a predicted chunking of the input sentence.\nExample 7 (continued). Given sentence (5) with token indices 1, . . . , 9, an answer set that contains the atoms {split(6), split(7)} and no other atoms for predicate split/1 yields the chunking shown in (6)."}, {"heading": "6 Evaluation and Discussion", "text": ""}, {"heading": "6.1 Datasets", "text": "We are using the datasets from the SemEval 2016 iSTS Task 2 (Agirre et al., 2016), which included two separate files containing sentence pairs. Three different datasets were provided: Headlines, Images, and Answers-Students. The Headlines dataset was mined by various news sources by European Media Monitor. The Images dataset was a collection of captions obtained from the Flickr dataset (Rashtchian et al., 2010). The Answers-Students corpus consists of the interactions between students and the BEETLE II tutorial dialogue system which is an intelligent tutoring engine that teaches students in basic electricity and electronics. In the following, we denote S1 and S2, by sentence 1 and sentence 2 respectively, of sentence pairs in these datasets. Regarding the size of the SemEval Training dataset, Headlines and Images datasets are larger and contained 756 and 750 sentence pairs, respectively. However, the AnswersStudents dataset was smaller and contained only 330 sentence pairs. In addition, all datasets contain a Test portion of sentence pairs.\nWe use k-fold cross-validation to evaluate chunking with ILP, which yields k learned hypotheses and k evaluation scores for each parameter setting. We test each of these hypotheses also on the Test portion of the respective dataset. From the scores obtained this way we compute mean and standard deviation, and perform statistical tests to find out whether observed score differences between parameter settings is statistically significant.\nTable 1 shows which portions of the SemEval Training dataset we used for 11-fold cross-validation. In the following, we call these datasets Cross-Validation Sets. We chose the first 110 and 550 examples to use for 11-fold cross-validation which results in training set sizes 100 and 500, respectively. As the Answers-Students dataset was smaller, we merged its sentence pairs in order to obtain a Cross-Validation Set size of 110 sentences, using the first 55 sentences from S1 and S2; and for 550 sentences, using the first 275 sentences from S1 and S2 each. As Test portions we only use the original SemEval Test datasets and we always test S1 and S2 separately.\n11"}, {"heading": "6.2 Scoring", "text": "We use difflib.SequenceMatcher in Python to match the sentence chunks obtained from learning in ILP against the gold-standard sentence chunks. From the matchings obtained this way, we compute precision, recall, and F1-score as follows.\nPrecision = No. of Matched Sequences\nNo. of ILP-learned Chunks\nRecall = No. of Matched Sequences\nNo. of Gold Chunks\nScore = 2\u00d7 Precision\u00d7 Recall\nPrecision+ Recall\nTo investigate the effectivity of our mode bias for learning a hypothesis that can correctly classify the dataset, we perform cross-validation (see above) and measure correctness of all hypotheses obtained in cross-validation also on the Test set.\nBecause of differences in S1/S2 portions of datasets, we report results separately for S1 and S2. We also evaluate classification separately for S1 and S2 for the Answers-Students dataset, although we train on a combination of S1 and S2."}, {"heading": "6.3 Experimental Methodology", "text": "We use Gringo version 4.5 (Gebser et al., 2011) and we use WASP version 2 (Git hash a44a95) (Alviano et al., 2015a) configured to use unsat-core optimisation with disjunctive core partitioning, core trimming, a budget of 30 seconds for computing the first answer set and for shrinking unsatisfiable cores with progressive shrinking strategy. These parameters were found most effective in preliminary experiments. We configure our modified XHAIL solver to allocate a budget of 1800 seconds for the Induction part which optimises the hypothesis (see Section 4.2). Memory usage never exceeded 5 GB.\nTables 4\u20136 contains the experimental results for each Dataset, where columns Size, Pr, and So respectively, show the number of sentences used to learn the hypothesis, the pruning parameter for generalising the learned hypothesis (see Section 4.1), and the rate of how close the learned hypothesis is to the optimal result, respectively. So is computed according to the following formula: So= Upperbound\u2212Lowerbound\nLowerbound ,\nwhich is based on upper and lower bounds on the cost of the answer set. An So value of zero means optimality, and values above zero mean suboptimality; so the higher the value, the further away from optimality. Our results comprise of the mean and standard deviation of the F1-scores obtained from our 11-fold cross-validation test set of S1 and S2 individually (column CV). Due to lack of space, we opted to leave out the scores of precision and recall, but these values show similar trends as in the Test set. For the Test sets of both S1 and S2, we include the mean and standard deviation of the Precision, Recall and F1-scores (column group T).\n12\nWhen testing machine-learning based systems, comparing results obtained on a single test set is often not sufficient, therefore we performed cross-validation to obtain mean and standard deviation about our benchmark metrics. To obtain even more solid evidence about the significance of the measured results, we additionally performed a one-tailed paired t-test to check if a measured F1 score is significantly higher in one setting than in another one. We consider a result significant if p < 0.05, i.e., if there is a probability of less than 5 % that the result is due to chance. Our test is one-tailed because we check whether one result is higher than another one, and it is a paired test because we test different parameters on the same set of 11 training/test splits in cross-validation. There are even more powerful methods for proving significance of results such as bootstrap sampling (Efron and Tibshirani, 1986), however these methods require markedly higher computational effort in experiments and our experiments already show significance with the t-test.\nRows of Tables 4\u20136 contain results for learning from 100 resp. 500 example sentences, and for different pruning parameters. For both learning set sizes, we increased pruning stepwise starting from value 0 until we found an optimal hypothesis (So=0) or until we saw a clear peak in classification score in cross-validation (in that case, increasing the pruning is pointless because it would increase optimality of the hypothesis but decrease the prediction scores).\nNote that datasets have been tokenised very differently, and that also state-of-the-art systems in SemEval used separate preprocessing methods for each dataset. We follow this strategy to allow a fair comparison. One example for such a difference is the Images dataset, where the \u2018.\u2019 is considered as a separate token and is later defined as a separate chunk, however in Answers-Students dataset it is integrated onto neighboring tokens."}, {"heading": "6.4 Results", "text": "We first discuss the results of experiments with varying training set size and varying pruning parameter, then compare our approach with the state-of-the-art systems, and finally inspect the optimal hypotheses.\nTraining Set Size and Pruning Parameter Tables 4\u20136 show results of experiments, where T denotes the Test portion of the respective dataset.\nWe observe that by increasing the size of the training set to learn the hypothesis, our scores improved considerably. Due to more information being provided, the learned hypothesis can predict with higher F1 score. We also observed that for the smaller training set size (100 sentences), lower pruning numbers (in rare cases even Pr=0) resulted in achieving the optimal solution. For a bigger training set size (500 sentences), without pruning the ILP procedure does not find solutions close to the optimal solution. However, by using pruning values up to Pr=10 we can reduce the size of the search space and find hypotheses closer to the optimum, which predict chunks with a higher F1 score. Our statistical test shows that, in many cases, several increments of the Pr parameter yield significantly better results, up to a point where prediction accuracy degrades because too many examples are pruned away. To select the best hypothesis, we increase the pruning parameter Pr until we reach the peak in the F1 score in cross-validation.\nFinding optimal hypotheses in the Inductive search of XHAIL (where So=0) is easily attained when learning from 100 sentences. For learning from 500 sentences, very higher pruning results in a trivial optimal hypothesis (i.e., every token is a chunk) which has no predictive power, hence we do not increase Pr beyond a value of 10.\nNote that we never encountered timeouts in the Abduction component of XHAIL, only in the Induction part. The original XHAIL tool without our improvements yields only timeouts for learning from 500 examples, and few hypotheses for learning from 100 examples. Therefore we do not show these results in tables.\nState-of-the-art comparison Table 2 shows a comparison of our results with the baseline and the three best systems from the chunking subtask of Task 2 from SemEval2016 Task2 (Agirre et al., 2016): DTSim (Banjade et al., 2016), FBK-HLT-NLP (Magnolini et al., 2016) and runs 1 and 2 of IISCNLP (Tekumalla and Jat, 2016). We also compare with results of our own system \u2018Inspire-Manual\u2019 (Kazmi and Sch\u00fcller, 2016).\n13\n\u2022 The baseline makes use of the automatic probabilistic chunker from the IXA-pipeline which provides Perceptron models (Collins, 2002) for chunking and is trained on CONLL2000 corpora and corrected manually,\n\u2022 DTSim uses a Conditional Random Field (CRF) based chunking tool using only POS-tags as features,\n\u2022 FBK-HLT-NLP obtains chunks using a Python implementation of MBSP chunker which uses a Memory-based part-of-speech tagger generator (Daelemans et al., 1996),\n\u2022 Run 1 of IISCNLP uses OpenNLP chunker which divides the sentence into syntactically correlated parts of words, but does not specify their internal structure, nor their role in the main sentence. Run 2 uses Stanford NLP Parser to create parse trees and then uses a perl script to create chunks based on the parse trees, and\n\u2022 Inspire-Manual (our previous system) makes use of manually set chunking rules (Abney, 1991) using ASP (Kazmi and Sch\u00fcller, 2016).\nUsing the gold-standard chunks provided by the organisers we were able to compute the precision, recall, and F1-scores for analysis on the Headlines, Images and Answers-Students datasets.\nFor the scores of our system \u2018Inspire-Learned\u2019, we used the mean and average of the best configuration of our system as obtained in cross-validation experiments on the Test set and compared against the other systems\u2019 Test set results. Our system\u2019s performance is quite robust: it is always scores within the top three best systems.\nInspection of Hypotheses Table 3 shows the rules that are obtained from the hypothesis generated by XHAIL from Sentence 1 files of all the datasets. We have also tabulated the common rules present between the datasets and the extra rules which differentiate the datasets from each other. POS-tags for\n14\npunctuation are \u2018c_p\u2019 for sentence-final punctuation (\u2018.\u2019, \u2018?\u2019, and \u2018 !\u2019) and \u2018c_c\u2019 for sentence-separating punctuation (\u2018,\u2019, \u2018;\u2019, and \u2018:\u2019).\nRules which occur in all learned hypotheses can be interpreted as follows (recall the meaning of split(X) from Section 5.2): (i) chunks end at past tense verbs (VBD, e.g., \u2018walked\u2019), (ii) chunks begin at subordinating conjunctions and prepositions (IN, e.g., \u2018in\u2019), and (iii) chunks begin at 3rd person singular present tense verbs (VBZ, e.g., \u2018walks\u2019). Rules that are common to H and AS datasets are as follows: (i) chunks end at base forms of verbs (VB, e.g., \u2018[to] walk\u2019), (ii) chunks begin at \u2018to\u2019 prepositions (TO), and (iii) chunks begin at past tense verbs (VBD). The absence of (i) in hypotheses for the Images dataset can be explained by the rareness of such verbs in captions of images. Note that (iii) together with the common rule (i) means that all VBD verbs become separate chunks in H and AS datasets. Rules that are common to I and AS datasets are as follows: (i) chunks begin at non-3rd person verbs in present tense (VBP, e.g., \u2018[we] walk\u2019), (ii) chunk boundaries are between a determiner (DT, e.g., \u2018both\u2019) and a\n15\n3rd person singular present tense verb (VBZ), and (iii) chunk boundaries are between adverbs (RB, e.g., \u2018usually\u2019) and common, singular, or mass nouns (NN, e.g., \u2018humor\u2019). Interestingly, there are no rules common to H and I datasets except for the three rules mutual to all three datasets.\nFor rules occurring only in single datasets, we only discuss a few interesting cases in the following. Rules that are unique to the Headlines dataset include rules which indicate that the sentence separators \u2018,\u2019, \u2018;\u2019, and \u2018:\u2019, become single chunks, moreover chunks start at genitive markers (POS, \u2018\u2019s\u2019). Both is not the case for the other two data sets. Rules unique to the Images dataset include that sentencefinal punctuation (\u2018.\u2019, \u2018?\u2019, and \u2018 !\u2019) become separate chunks, rules for chunk boundaries between verb (VB_) and noun (NN_) tokens, and chunk boundaries between possessive pronouns (PRP$, encoded as \u2018c_PRPd\u2019, e.g., \u2018their\u2019) and participles/gerunds (VBG, e.g., \u2018falling\u2019). Rules unique to AnswersStudents dataset include chunks containing \u2018existential there\u2019 (EX), adverb tokens (RB), gerunds (VBG), and several rules for splits related to WH-determiners (WDT, e.g., \u2018which\u2019), WH-adverbs (WRB, e.g., \u2018how\u2019), and prepositions (IN).\nWe see that learned hypotheses are interpretable, which is not the case in classical machine learning techniques such as Neural Networks (NN), Conditional Random Fields (CRF), and Support Vector Machines (SVM)."}, {"heading": "6.5 Discussion", "text": "We next discuss the potential impact of our approach in NLP and in other applications, outline the strengths and weaknesses, and discuss reasons for several design choices we made.\nImpact and Applicability ILP is applicable to many problems of traditional machine learning, but usually only applicable for small datasets. Our addition of pruning enables learning from larger datasets at the cost of obtaining a more coarse-grained hypothesis and potentially suboptimal solutions.\nThe main advantage of ILP is interpretability and that it can achieve good results already with small datasets. Interpretability of the learned rule-based hypothesis makes the learned hypothesis transparent as opposed to black-box models of other approaches in the field such as Conditional Random Fields, Neural Networks, or Support Vector Machines. These approaches are often purely statistical, operate on big matrices of real numbers instead of logical rules, and are not interpretable. The disadvantage of ILP is that it often does not achieve the predictive performance of purely statistical approaches because the complexity of ILP learning limits the number of distinct features that can be used simultaneously.\nOur approach allows finding suboptimal hypotheses which yield a higher prediction accuracy than an optimal hypothesis trained on a smaller training set. Learning a better model from a larger dataset is exactly what we would expect in machine learning. Before our improvement of XHAIL, obtaining any hypothesis from larger datasets was impossible: the original XHAIL tool does not return any hypothesis within several hours when learning from 500 examples.\nOur chunking approach learns from a small portion of the full SemEval Training dataset, based on only POS-tags, but it still achieves results close to the state-of-the-art. Additionally it provides an interpretable model that allowed us to pinpoint non-uniform annotation practices in the three datasets of the SemEval 2016 iSTS competition. These observations give direct evidence for differences in annotation practice for three datasets with respect to punctuation and genitives, as well as differences in the content of the datasets\nStrengths and weaknesses Our additions of pruning and the usage of suboptimal answer sets make ILP more robust because it permits learning from larger datasets and obtaining (potentially suboptimal) solutions faster.\nOur addition of a time budget and usage of suboptimal answer sets is a purely beneficial addition to the XHAIL approach. If we disregard the additional benefits of pruning, i.e., if we disable pruning by setting Pr=0, then within the same time budget, the same optimal solutions are to be found as if using the original XHAIL approach. In addition, before finding the optimal solution, suboptimal hypotheses are provided in an online manner, together with information about their distance from the optimal solution.\nThe strength of pruning before the Induction phase is, that it permits learning from a bigger set of examples, while still considering all examples in the dataset. A weakness of pruning is, that a hypothesis\n16\nwhich fits perfectly to the data might not be found anymore, even if the mode bias could permit such a perfect fit. In NLP applications this is not a big disadvantage, because noise usually prevents a perfect fit anyways, and overfitting models is indeed often a problem. However, in other application domains such as learning to interpret input data from user examples (Gulwani et al., 2015), a perfect fit to the input data might be desired and required. Note that pruning examples to learn from inconsistent data as done by Tang and Mooney (Tang and Mooney, 2001) is not necessary for our approach. Instead, non-covered examples incur a cost that is optimised to be as small as possible.\nDesign decisions In our study, we use a simple mode bias containing only the current and next POS tags, which is a deliberate choice to make results easier to compare. We performed experiments with additional body atoms head/2 and rel/2 in the body mode bias, moreover with negation in the body mode bias. However, these experiments yielded significantly larger hypotheses with only small increases in accuracy. Therefore we here limit the analysis to the simple case and consider more complex mode biases as future work. Note that the best state-of-the-art system (DTSim) is a CRF model solely based on POS-tags, just as our hypothesis is only making use of POS-tags. By considering more than the current and immediately succeeding POS tag, DTSim can achieve better results than we do.\nThe representation of examples is an important part of our chunking case as described in Section 5. We define predicate goodchunk with rules that consider presence and absence of splits for each chunk. We make use of the power of NAF in these rules. We also experimented with an example representation that just gave all desired splits as #example split(X) and all undesired splits as #example not split(Y). This representation contains an imbalance in the split versus not split class, moreover, chunks are not represented as a concept that can be optimised in the inductive search for the best hypothesis. Hence, it is not surprising that this simpler representation of examples gave drastically worse scores, and we do not report any of these results in detail."}, {"heading": "7 Conclusion and Future Work", "text": "Inductive Logic Programming combines logic programming and machine learning, and it provides interpretable models, i.e., logical hypotheses, which are learned from data. ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al., 2004), and in Microsoft Excel Gulwani et al. (2015) where users can specify data extraction rules using examples. Therefore, ILP research has the potential for being used in a wide range of applications.\nIn this work, we explored the usage of ILP for the NLP task of chunking and extend the XHAIL ILP solver to increase its scalability and applicability for this task. Results indicate that ILP is competitive to state-of-the-art ML techniques for this task and that we successfully extended XHAIL to allow learning from larger datasets than previously possible. Learning a hypothesis using ILP has the advantage of an interpretable representation of the learned knowledge, such that we know exactly which rule has been learned by the program and how it affects our NLP task. In this study, we also gain insights about the differences and common points of datasets that we learned a hypothesis from. Moreover, ILP permits learning from small training sets where techniques such as Neural Networks fail to provide good results.\nAs a first contribution to the ILP tool XHAIL we have upgraded the software so that it uses the newest solver technology, and that this technology is used in a best-effort manner that can utilise suboptimal search results. This is effective in practice, because finding the optimal solution can be disproportionately more difficult than finding a solution close to the optimum. Moreover, the ASP technique we use here provides a clear information about the degree of suboptimality. During our experiments, a new version of Clingo was published which contains most techniques in WASP (except for core shrinking). We decided to continue using WASP for this study because we saw that core shrinking is also beneficial to search. Extending XHAIL to use Clingo in a best-effort manner is quite straight-forward.\nAs a second contribution to XHAIL we have added a pruning parameter to the algorithm that allows fine-tuning the search space for hypotheses by filtering out rule candidates that are supported by fewer examples than other rules. This addition is a novel contribution to the algorithm, which leads to significant improvements in efficiency, and increases the number of hypotheses that are found in a given time budget. While pruning makes the method incomplete, it does not reduce expressivity. The\n17\nhypotheses and background knowledge may still contain unrestricted Negation as Failure. Pruning in our work is similar to the concept of the regularisation in ML and is there to prevent overfitting in the hypothesis generation. Pruning enables the learning of logical hypotheses with dataset sizes that were not feasible before. We experimentally observed a trade-off between finding an optimal hypothesis that considers all potential rules on one hand, and finding a suboptimal hypothesis that is based on rules that are supported by few examples. Therefore the pruning parameter has to be adjusted on an application-by-application basis.\nOur work has focused on providing comparable results to ML techniques and we have not utilised the full power of ILP with NAF in rule bodies and predicate invention. As future work, we plan to extend the predicates usable in hypotheses to provide a more detailed representation of the NLP task, moreover we plan to enrich the background knowledge to aid ILP in learning a better hypothesis with a deeper structure representing the boundaries of chunks.\nWe provide the modified XHAIL system in a public repository fork (Bragaglia and Sch\u00fcller, 2016)."}, {"heading": "Acknowledgments", "text": "This research has been supported by the Scientific and Technological Research Council of Turkey (TUBITAK) [grant number 114E777] and by the Higher Education Commission of Pakistan (HEC). We are grateful to Carmine Dodaro for providing us with support regarding the WASP solver."}], "references": [{"title": "Parsing by chunks", "author": ["S.P. Abney"], "venue": "Principle-based parsing, pages 257\u2013278.", "citeRegEx": "Abney,? 1991", "shortCiteRegEx": "Abney", "year": 1991}, {"title": "SemEval-2016 task 2: Interpretable Semantic Textual Similarity", "author": ["E. Agirre", "A. Gonzalez-Agirre", "I. Lopez-Gazpio", "M. Maritxalar", "G. Rigau", "L. Uria"], "venue": "Proceedings of SemEval, pages 512\u2013524.", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "Anytime answer set optimization via unsatisfiable core shrinking", "author": ["M. Alviano", "C. Dodaro"], "venue": "Theory and Practice of Logic Programming, 16(5-6):533\u2013551.", "citeRegEx": "Alviano and Dodaro,? 2016", "shortCiteRegEx": "Alviano and Dodaro", "year": 2016}, {"title": "WASP: A native ASP solver based on constraint learning", "author": ["M. Alviano", "C. Dodaro", "W. Faber", "N. Leone", "F. Ricca"], "venue": "Logic Programming and Nonmonotonic Reasoning, pages 54\u201366.", "citeRegEx": "Alviano et al\\.,? 2013", "shortCiteRegEx": "Alviano et al\\.", "year": 2013}, {"title": "Advances in WASP", "author": ["M. Alviano", "C. Dodaro", "N. Leone", "F. Ricca"], "venue": "Logic Programming and Nonmonotonic Reasoning , pages 40\u201354.", "citeRegEx": "Alviano et al\\.,? 2015a", "shortCiteRegEx": "Alviano et al\\.", "year": 2015}, {"title": "Optimum stable model search: algorithms and implementation", "author": ["M. Alviano", "C. Dodaro", "J. Marques-Silva", "F. Ricca"], "venue": "Journal of Logic and Computation, page exv061.", "citeRegEx": "Alviano et al\\.,? 2015b", "shortCiteRegEx": "Alviano et al\\.", "year": 2015}, {"title": "Unsatisfiability-based optimization in clasp", "author": ["B. Andres", "B. Kaufmann", "O. Matheis", "T. Schaub"], "venue": "International Conference on Logic Programming, Technical Communications, pages 212\u2013221.", "citeRegEx": "Andres et al\\.,? 2012", "shortCiteRegEx": "Andres et al\\.", "year": 2012}, {"title": "SAT-based MaxSAT algorithms", "author": ["C. Ans\u00f3tegui", "M.L. Bonet", "J. Levy"], "venue": "Artificial Intelligence, 196:77\u2013105.", "citeRegEx": "Ans\u00f3tegui et al\\.,? 2013", "shortCiteRegEx": "Ans\u00f3tegui et al\\.", "year": 2013}, {"title": "DTSim at SemEval-2016 task 2: Interpreting Similarity of Texts Based on Automated Chunking, Chunk Alignment and Semantic Relation Prediction", "author": ["R. Banjade", "N. Maharjan", "N.B. Niraula", "V. Rus"], "venue": "Proceedings of SemEval, pages 809\u2013813.", "citeRegEx": "Banjade et al\\.,? 2016", "shortCiteRegEx": "Banjade et al\\.", "year": 2016}, {"title": "Joint morphological and syntactic analysis for richly inflected languages", "author": ["B. Bohnet", "J. Nivre", "I. Boguslavsky", "R. Farkas", "F. Ginter", "J. Haji\u010d"], "venue": "Transactions of the Association for Computational Linguistics, 1:415\u2013428.", "citeRegEx": "Bohnet et al\\.,? 2013", "shortCiteRegEx": "Bohnet et al\\.", "year": 2013}, {"title": "XHAIL (Version 4c5e0b8) [System for eXtended Hybrid Abductive Inductive Learning", "author": ["S. Bragaglia", "P. Sch\u00fcller"], "venue": "Retrieved from https://github.com/knowlp/XHAIL.", "citeRegEx": "Bragaglia and Sch\u00fcller,? 2016", "shortCiteRegEx": "Bragaglia and Sch\u00fcller", "year": 2016}, {"title": "Answer set programming at a glance", "author": ["G. Brewka", "T. Eiter", "M. Truszczy\u0144ski"], "venue": "Communications of the ACM, 54(12):92\u2013103.", "citeRegEx": "Brewka et al\\.,? 2011", "shortCiteRegEx": "Brewka et al\\.", "year": 2011}, {"title": "ASP-Core-2 Input language format. Technical report, ASP Standardization Working Group", "author": ["T. Schaub"], "venue": null, "citeRegEx": "Schaub,? \\Q2012\\E", "shortCiteRegEx": "Schaub", "year": 2012}, {"title": "Programming in PROLOG", "author": ["W. Clocksin", "C.S. Mellish"], "venue": "Springer Science & Business Media.", "citeRegEx": "Clocksin and Mellish,? 2003", "shortCiteRegEx": "Clocksin and Mellish", "year": 2003}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1\u20138. Association for Computational Linguistics.", "citeRegEx": "Collins,? 2002", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["M. Craven", "J. Kumlien"], "venue": "International Conference on Intelligent Systems for Molecular Biology (ISMB), pages 77\u201386.", "citeRegEx": "Craven and Kumlien,? 1999", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Integrating probabilistic and logical reasoning", "author": ["J. Cussens"], "venue": "Foundations of Bayesianism, pages 241\u2013260.", "citeRegEx": "Cussens,? 2001a", "shortCiteRegEx": "Cussens", "year": 2001}, {"title": "Parameter estimation in stochastic logic programs", "author": ["J. Cussens"], "venue": "Machine Learning, 44(3):245\u2013 271.", "citeRegEx": "Cussens,? 2001b", "shortCiteRegEx": "Cussens", "year": 2001}, {"title": "Mbt: A memory-based part of speech tagger-generator", "author": ["W. Daelemans", "J. Zavrel", "P. Berck", "S. Gillis"], "venue": "arXiv preprint cmp-lg/9607012.", "citeRegEx": "Daelemans et al\\.,? 1996", "shortCiteRegEx": "Daelemans et al\\.", "year": 1996}, {"title": "Logical settings for concept-learning", "author": ["L. De Raedt"], "venue": "Artificial Intelligence, 95(1):187\u2013201.", "citeRegEx": "Raedt,? 1997", "shortCiteRegEx": "Raedt", "year": 1997}, {"title": "Probabilistic inductive logic programming", "author": ["L. De Raedt", "K. Kersting"], "venue": "Probabilistic Inductive Logic Programming, pages 1\u201327.", "citeRegEx": "Raedt and Kersting,? 2008", "shortCiteRegEx": "Raedt and Kersting", "year": 2008}, {"title": "Inductive learning algorithms and representations for text categorization", "author": ["S. Dumais", "J. Platt", "D. Heckerman", "M. Sahami"], "venue": "Proceedings of the Seventh International Conference on Information and Knowledge Management, pages 148\u2013155.", "citeRegEx": "Dumais et al\\.,? 1998", "shortCiteRegEx": "Dumais et al\\.", "year": 1998}, {"title": "Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy", "author": ["B. Efron", "R. Tibshirani"], "venue": "Statistical Science, 1(1):54\u201375.", "citeRegEx": "Efron and Tibshirani,? 1986", "shortCiteRegEx": "Efron and Tibshirani", "year": 1986}, {"title": "Applications of Answer Set Programming", "author": ["E. Erdem", "M. Gelfond", "N. Leone"], "venue": "AI Magazine, 37(3):53\u201368.", "citeRegEx": "Erdem et al\\.,? 2016", "shortCiteRegEx": "Erdem et al\\.", "year": 2016}, {"title": "A user\u2019s guide to gringo, clasp, clingo, and iclingo", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann", "M. Ostrowski", "T. Schaub", "S. Thiele"], "venue": "Technical report, University of Potsdam.", "citeRegEx": "Gebser et al\\.,? 2008", "shortCiteRegEx": "Gebser et al\\.", "year": 2008}, {"title": "Answer set solving in practice", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann", "T. Schaub"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 6(3):1\u2013238.", "citeRegEx": "Gebser et al\\.,? 2012a", "shortCiteRegEx": "Gebser et al\\.", "year": 2012}, {"title": "Advances in gringo series 3", "author": ["M. Gebser", "R. Kaminski", "A. K\u00f6nig", "T. Schaub"], "venue": "International Conference on Logic Programming and Non-monotonic Reasoning, pages 345\u2013351.", "citeRegEx": "Gebser et al\\.,? 2011", "shortCiteRegEx": "Gebser et al\\.", "year": 2011}, {"title": "Conflict-driven answer set solving: From theory to practice", "author": ["M. Gebser", "B. Kaufmann", "T. Schaub"], "venue": "Artificial Intelligence, 187:52\u201389.", "citeRegEx": "Gebser et al\\.,? 2012b", "shortCiteRegEx": "Gebser et al\\.", "year": 2012}, {"title": "The Stable Model Semantics for Logic Programming", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "International Conference and Symposium on Logic Programming, pages 1070\u20131080.", "citeRegEx": "Gelfond and Lifschitz,? 1988", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1988}, {"title": "Inductive programming meets the real world", "author": ["S. Gulwani", "J. Hernandez-Orallo", "E. Kitzelmann", "S.H. Muggleton", "U. Schmid", "B. Zorn"], "venue": "Communications of the ACM, 58(11):90\u201399.", "citeRegEx": "Gulwani et al\\.,? 2015", "shortCiteRegEx": "Gulwani et al\\.", "year": 2015}, {"title": "Abductive Logic Programming", "author": ["A.C. Kakas", "R.A. Kowalski", "F. Toni"], "venue": "Journal of Logic and Computation, 2(6):719\u2013770.", "citeRegEx": "Kakas et al\\.,? 1992", "shortCiteRegEx": "Kakas et al\\.", "year": 1992}, {"title": "Incremental learning of event definitions with inductive logic programming", "author": ["N. Katzouris", "A. Artikis", "G. Paliouras"], "venue": "Machine Learning, 100(2-3):555\u2013585.", "citeRegEx": "Katzouris et al\\.,? 2015", "shortCiteRegEx": "Katzouris et al\\.", "year": 2015}, {"title": "Inspire at SemEval-2016 task 2: Interpretable semantic textual similarity alignment based on answer set programming", "author": ["M. Kazmi", "P. Sch\u00fcller"], "venue": "Proceedings of SemEval, pages 1109\u20131115.", "citeRegEx": "Kazmi and Sch\u00fcller,? 2016", "shortCiteRegEx": "Kazmi and Sch\u00fcller", "year": 2016}, {"title": "Functional genomic hypothesis generation and experimentation by a robot scientist", "author": ["R.D. King", "K.E. Whelan", "F.M. Jones", "P.G.K. Reiser", "C.H. Bryant", "S.H. Muggleton", "D.B. Kell", "S.G. Oliver"], "venue": "Nature, 427(6971):247\u2013252.", "citeRegEx": "King et al\\.,? 2004", "shortCiteRegEx": "King et al\\.", "year": 2004}, {"title": "Inductive programming: A survey of program synthesis techniques", "author": ["E. Kitzelmann"], "venue": "International Workshop on Approaches and Applications of Inductive Programming, pages 50\u201373.", "citeRegEx": "Kitzelmann,? 2009", "shortCiteRegEx": "Kitzelmann", "year": 2009}, {"title": "Inductive learning of answer set programs", "author": ["M. Law", "A. Russo", "K. Broda"], "venue": "European Workshop on Logics in Artificial Intelligence, pages 311\u2013325.", "citeRegEx": "Law et al\\.,? 2014", "shortCiteRegEx": "Law et al\\.", "year": 2014}, {"title": "Learning weak constraints in answer set programming", "author": ["M. Law", "A. Russo", "K. Broda"], "venue": "Theory and Practice of Logic Programming, 15(4-5):511\u2013525.", "citeRegEx": "Law et al\\.,? 2015", "shortCiteRegEx": "Law et al\\.", "year": 2015}, {"title": "Answer set programming and plan generation", "author": ["V. Lifschitz"], "venue": "Artificial Intelligence, 138(1-2):39\u201354.", "citeRegEx": "Lifschitz,? 2002", "shortCiteRegEx": "Lifschitz", "year": 2002}, {"title": "Foundations of logic programming", "author": ["J.W. Lloyd"], "venue": "Springer Science & Business Media.", "citeRegEx": "Lloyd,? 2012", "shortCiteRegEx": "Lloyd", "year": 2012}, {"title": "FBK-HLT-NLP at SemEval-2016 task 2: A multitask, deep learning approach for interpretable semantic textual similarity", "author": ["S. Magnolini", "A. Feltracco", "B. Magnini"], "venue": "Proceedings of SemEval, pages 783\u2013789.", "citeRegEx": "Magnolini et al\\.,? 2016", "shortCiteRegEx": "Magnolini et al\\.", "year": 2016}, {"title": "Foundations of statistical natural language processing (Vol", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "999). Cambridge:MIT Press.", "citeRegEx": "Manning and Sch\u00fctze,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning", "author": ["A. Mitra", "C. Baral"], "venue": "Association for the Advancement of Artificial Intelligence, pages 2779\u20132785.", "citeRegEx": "Mitra and Baral,? 2016", "shortCiteRegEx": "Mitra and Baral", "year": 2016}, {"title": "Inductive logic programming for natural language processing", "author": ["R.J. Mooney"], "venue": "Inductive Logic Programming, pages 1\u201322.", "citeRegEx": "Mooney,? 1996", "shortCiteRegEx": "Mooney", "year": 1996}, {"title": "Inductive logic programming", "author": ["S. Muggleton"], "venue": "New generation computing, 8(4):295\u2013318.", "citeRegEx": "Muggleton,? 1991", "shortCiteRegEx": "Muggleton", "year": 1991}, {"title": "Inverse entailment and Progol", "author": ["S. Muggleton"], "venue": "New generation computing, 13(3-4):245\u2013286.", "citeRegEx": "Muggleton,? 1995", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Inductive logic programming: issues, results and the challenge of learning language in logic", "author": ["S. Muggleton"], "venue": "Artificial Intelligence, 114(1-2):283\u2013296.", "citeRegEx": "Muggleton,? 1999", "shortCiteRegEx": "Muggleton", "year": 1999}, {"title": "Learning structure and parameters of stochastic logic programs", "author": ["S. Muggleton"], "venue": "International Conference on Inductive Logic Programming, pages 198\u2013206.", "citeRegEx": "Muggleton,? 2002", "shortCiteRegEx": "Muggleton", "year": 2002}, {"title": "Machine invention of first-order predicates by inverting resolution", "author": ["S. Muggleton", "W. Buntine"], "venue": "Proceedings of the Fifth International Conference on Machine Learning, pages 339\u2013352.", "citeRegEx": "Muggleton and Buntine,? 1992", "shortCiteRegEx": "Muggleton and Buntine", "year": 1992}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L. De Raedt"], "venue": "The Journal of Logic Programming, 19:629\u2013679.", "citeRegEx": "Muggleton and Raedt,? 1994", "shortCiteRegEx": "Muggleton and Raedt", "year": 1994}, {"title": "ILP turns 20", "author": ["S. Muggleton", "L. De Raedt", "D. Poole", "I. Bratko", "P. Flach", "K. Inoue", "A. Srinivasan"], "venue": "Machine Learning, 86(1):3\u201323.", "citeRegEx": "Muggleton et al\\.,? 2012", "shortCiteRegEx": "Muggleton et al\\.", "year": 2012}, {"title": "Stochastic logic programs", "author": ["S Muggleton"], "venue": "Advances in Inductive Logic Programming, 32:254\u2013264.", "citeRegEx": "Muggleton,? 1996", "shortCiteRegEx": "Muggleton", "year": 1996}, {"title": "Efficient induction of logic programs", "author": ["S. Muggleton", "C Feng"], "venue": "The Turing Institute", "citeRegEx": "Muggleton and Feng,? \\Q1990\\E", "shortCiteRegEx": "Muggleton and Feng", "year": 1990}, {"title": "Meta-interpretive learning: application to grammatical inference", "author": ["S.H. Muggleton", "D. Lin", "N. Pahlavi", "A. Tamaddoni-Nezhad"], "venue": "Machine Learning, 94(1):25\u201349.", "citeRegEx": "Muggleton et al\\.,? 2014", "shortCiteRegEx": "Muggleton et al\\.", "year": 2014}, {"title": "A note on inductive generalization", "author": ["G.D. Plotkin"], "venue": "Machine intelligence, 5(1):153\u2013163.", "citeRegEx": "Plotkin,? 1970", "shortCiteRegEx": "Plotkin", "year": 1970}, {"title": "A further note on inductive generalization", "author": ["G.D. Plotkin"], "venue": "Machine intelligence, 6:101\u2013124.", "citeRegEx": "Plotkin,? 1971", "shortCiteRegEx": "Plotkin", "year": 1971}, {"title": "Learning logical definitions from relations", "author": ["J.R. Quinlan"], "venue": "Machine Learning, 5(3):239\u2013266.", "citeRegEx": "Quinlan,? 1990", "shortCiteRegEx": "Quinlan", "year": 1990}, {"title": "Collecting image annotations using Amazon\u2019s Mechanical Turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 139\u2013147.", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Nonmonotonic abductive inductive learning", "author": ["O. Ray"], "venue": "Journal of Applied Logic, 7(3):329\u2013340.", "citeRegEx": "Ray,? 2009", "shortCiteRegEx": "Ray", "year": 2009}, {"title": "Learning concepts by asking questions", "author": ["C. Sammut", "R.B. Banerji"], "venue": "Machine Learning: An artificial intelligence approach, 2:167\u2013192.", "citeRegEx": "Sammut and Banerji,? 1986", "shortCiteRegEx": "Sammut and Banerji", "year": 1986}, {"title": "Generative modeling with failure in PRISM", "author": ["T. Sato", "Y. Kameya", "Zhou", "N.-F."], "venue": "International Joint Conference on Artificial Intelligence, pages 847\u2013852.", "citeRegEx": "Sato et al\\.,? 2005", "shortCiteRegEx": "Sato et al\\.", "year": 2005}, {"title": "Flexible Combinatory Categorial Grammar Parsing using the CYK Algorithm and Answer Set Programming", "author": ["P. Sch\u00fcller"], "venue": "International Conference on Logic Programming and Non-monotonic Reasoning, pages 499\u2013511.", "citeRegEx": "Sch\u00fcller,? 2013", "shortCiteRegEx": "Sch\u00fcller", "year": 2013}, {"title": "Tackling Winograd Schemas by Formalizing Relevance Theory in Knowledge Graphs", "author": ["P. Sch\u00fcller"], "venue": "International Conference on Principles of Knowledge Representation and Reasoning (KR), pages 358\u2013367. AAAI Press.", "citeRegEx": "Sch\u00fcller,? 2014", "shortCiteRegEx": "Sch\u00fcller", "year": 2014}, {"title": "Modeling Variations of First-Order Horn Abduction in Answer Set Programming", "author": ["P. Sch\u00fcller"], "venue": "Fundamenta Informaticae, 149:159\u2013207.", "citeRegEx": "Sch\u00fcller,? 2016", "shortCiteRegEx": "Sch\u00fcller", "year": 2016}, {"title": "Answer Set Programming via Controlled Natural Language Processing", "author": ["R. Schwitter"], "venue": "Controlled Natural Language, pages 26\u201343.", "citeRegEx": "Schwitter,? 2012", "shortCiteRegEx": "Schwitter", "year": 2012}, {"title": "Algorithmic program debugging", "author": ["E.Y. Shapiro"], "venue": "MIT press.", "citeRegEx": "Shapiro,? 1983", "shortCiteRegEx": "Shapiro", "year": 1983}, {"title": "Towards addressing the winograd schema challenge - Building and using a semantic parser and a knowledge hunting module", "author": ["A. Sharma", "N.H. Vo", "S. Aditya", "C. Baral"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), pages 1319\u20131325.", "citeRegEx": "Sharma et al\\.,? 2015", "shortCiteRegEx": "Sharma et al\\.", "year": 2015}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["L.R. Tang", "R.J. Mooney"], "venue": "European Conference on Machine Learning, pages 466\u2013477.", "citeRegEx": "Tang and Mooney,? 2001", "shortCiteRegEx": "Tang and Mooney", "year": 2001}, {"title": "IISCNLP at SemEval-2016 task 2: Interpretable STS with ILP based Multiple Chunk Aligner", "author": ["L. Tekumalla", "S. Jat"], "venue": "Proceedings of SemEval, pages 790\u2013795.", "citeRegEx": "Tekumalla and Jat,? 2016", "shortCiteRegEx": "Tekumalla and Jat", "year": 2016}, {"title": "Introduction to the CoNLL-2000 shared task: Chunking", "author": ["E.F. Tjong Kim Sang", "S. Buchholz"], "venue": "Workshop on Learning Language in Logic and Conference on Computational Natural Language Learning, pages 127\u2013132.", "citeRegEx": "Sang and Buchholz,? 2000", "shortCiteRegEx": "Sang and Buchholz", "year": 2000}, {"title": "Completing logic programs by inverse resolution", "author": ["R. Wirth"], "venue": "European Working Session on Learning, pages 239\u2013250.", "citeRegEx": "Wirth,? 1989", "shortCiteRegEx": "Wirth", "year": 1989}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "Proceedings of the National Conference on Artificial Intelligence, pages 1050\u20131055.", "citeRegEx": "Zelle and Mooney,? 1996", "shortCiteRegEx": "Zelle and Mooney", "year": 1996}, {"title": "Combining top-down and bottom-up techniques in inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney", "J.B. Konvisser"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pages 343\u2013351.", "citeRegEx": "Zelle et al\\.,? 1994", "shortCiteRegEx": "Zelle et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 58, "context": "In our experiments on sentence chunking (Tjong Kim Sang and Buchholz, 2000) we encountered several problems with state-of-the-art ASP-based ILP systems XHAIL (Ray, 2009), ILED", "startOffset": 158, "endOffset": 169}, {"referenceID": 31, "context": "(Katzouris et al., 2015), and ILASP2 (Law et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 36, "context": ", 2015), and ILASP2 (Law et al., 2015).", "startOffset": 20, "endOffset": 38}, {"referenceID": 6, "context": "\u2022 We extend XHAIL with best-effort optimisation using the newest ASP optimisation technology of unsat-core optimisation (Andres et al., 2012) with stratification (Alviano et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 5, "context": ", 2012) with stratification (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013) and core shrinking (Alviano and Dodaro, 2016) using the WASP2 (Alviano et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 7, "context": ", 2012) with stratification (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013) and core shrinking (Alviano and Dodaro, 2016) using the WASP2 (Alviano et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 2, "context": ", 2013) and core shrinking (Alviano and Dodaro, 2016) using the WASP2 (Alviano et al.", "startOffset": 27, "endOffset": 53}, {"referenceID": 26, "context": ", 2013, 2015a) solver and the Gringo (Gebser et al., 2011) grounder.", "startOffset": 37, "endOffset": 58}, {"referenceID": 41, "context": "\u2022 We describe a framework for chunking with ILP, based on preprocessing with Stanford Core NLP (Manning et al., 2014) tools.", "startOffset": 95, "endOffset": 117}, {"referenceID": 1, "context": "\u2022 We experimentally analyse the relationship between the pruning parameter, number of training examples, and prediction score on the sentence chunking (Tjong Kim Sang and Buchholz, 2000) subtask of iSTS at SemEval 2016 (Agirre et al., 2016).", "startOffset": 219, "endOffset": 240}, {"referenceID": 10, "context": "Our extensions and modifications of the XHAIL software are available in a public fork of the official XHAIL Git repository (Bragaglia and Sch\u00fcller, 2016).", "startOffset": 123, "endOffset": 153}, {"referenceID": 38, "context": "A logic programs theory normally comprises of an alphabet (variable, constant, quantifier, etc), vocabulary, logical symbols, a set of axioms and inference rules (Lloyd, 2012).", "startOffset": 162, "endOffset": 175}, {"referenceID": 13, "context": "The popular Prolog (Clocksin and Mellish, 2003) system evaluates rules using resolution, which makes the result of a Prolog program depending on the order of its rules and on the order of the bodies of its rules.", "startOffset": 19, "endOffset": 47}, {"referenceID": 11, "context": "Answer Set Programming (ASP) (Brewka et al., 2011; Gebser et al., 2012a) is a more recent logic programming formalism, featuring more declarativity than Prolog by defining semantics based on Herbrand models (Gelfond and Lifschitz, 1988).", "startOffset": 29, "endOffset": 72}, {"referenceID": 25, "context": "Answer Set Programming (ASP) (Brewka et al., 2011; Gebser et al., 2012a) is a more recent logic programming formalism, featuring more declarativity than Prolog by defining semantics based on Herbrand models (Gelfond and Lifschitz, 1988).", "startOffset": 29, "endOffset": 72}, {"referenceID": 28, "context": ", 2012a) is a more recent logic programming formalism, featuring more declarativity than Prolog by defining semantics based on Herbrand models (Gelfond and Lifschitz, 1988).", "startOffset": 143, "endOffset": 172}, {"referenceID": 37, "context": "Most ASP programs follow the Generate-Define-Test structure (Lifschitz, 2002) to (i) generate a space of potential solutions, (ii) define auxiliary concepts, and (iii) test to invalidate solutions using constraints or incurring a cost on non-preferred solutions.", "startOffset": 60, "endOffset": 77}, {"referenceID": 42, "context": "ASP has been applied to several problems related to Natural Language Processing, see for example (Mitra and Baral, 2016; Sch\u00fcller, 2013, 2014, 2016; Schwitter, 2012; Sharma et al., 2015).", "startOffset": 97, "endOffset": 186}, {"referenceID": 64, "context": "ASP has been applied to several problems related to Natural Language Processing, see for example (Mitra and Baral, 2016; Sch\u00fcller, 2013, 2014, 2016; Schwitter, 2012; Sharma et al., 2015).", "startOffset": 97, "endOffset": 186}, {"referenceID": 66, "context": "ASP has been applied to several problems related to Natural Language Processing, see for example (Mitra and Baral, 2016; Sch\u00fcller, 2013, 2014, 2016; Schwitter, 2012; Sharma et al., 2015).", "startOffset": 97, "endOffset": 186}, {"referenceID": 23, "context": "An overview of applications of ASP in general can be found in (Erdem et al., 2016).", "startOffset": 62, "endOffset": 82}, {"referenceID": 40, "context": "These models are often pure Machine Learning (ML) estimators without any rule components (Manning and Sch\u00fctze, 1999).", "startOffset": 89, "endOffset": 116}, {"referenceID": 21, "context": "Some popular classifiers used for processing natural language include Naive Bayes, Decision Trees, Neural Networks, and Support Vector Machines (SVMs) (Dumais et al., 1998).", "startOffset": 151, "endOffset": 172}, {"referenceID": 58, "context": "Consider the following example ILP instance (M,E,B) (Ray, 2009).", "startOffset": 52, "endOffset": 63}, {"referenceID": 29, "context": "Quite a few surveys (Gulwani et al., 2015; Kitzelmann, 2009; Muggleton et al., 2012) mention about the systems and applications of ILP in interdisciplinary areas.", "startOffset": 20, "endOffset": 84}, {"referenceID": 34, "context": "Quite a few surveys (Gulwani et al., 2015; Kitzelmann, 2009; Muggleton et al., 2012) mention about the systems and applications of ILP in interdisciplinary areas.", "startOffset": 20, "endOffset": 84}, {"referenceID": 50, "context": "Quite a few surveys (Gulwani et al., 2015; Kitzelmann, 2009; Muggleton et al., 2012) mention about the systems and applications of ILP in interdisciplinary areas.", "startOffset": 20, "endOffset": 84}, {"referenceID": 65, "context": "The foundations of ILP can be found in research by Plotkin (Plotkin, 1970, 1971), Shapiro (Shapiro, 1983) and Sammut and Banerji (Sammut and Banerji, 1986).", "startOffset": 90, "endOffset": 105}, {"referenceID": 59, "context": "The foundations of ILP can be found in research by Plotkin (Plotkin, 1970, 1971), Shapiro (Shapiro, 1983) and Sammut and Banerji (Sammut and Banerji, 1986).", "startOffset": 129, "endOffset": 155}, {"referenceID": 44, "context": "The founding paper of Muggleton (Muggleton, 1991) led to the launch of the first international workshop on ILP.", "startOffset": 32, "endOffset": 49}, {"referenceID": 45, "context": "At the beginning, ILP was associated with the introduction of foundational theoretical concepts which included Inverse Resolution (Muggleton, 1995; Muggleton and Buntine, 1992) and Predicate Invention (Muggleton, 1991; Muggleton and Buntine, 1992).", "startOffset": 130, "endOffset": 176}, {"referenceID": 48, "context": "At the beginning, ILP was associated with the introduction of foundational theoretical concepts which included Inverse Resolution (Muggleton, 1995; Muggleton and Buntine, 1992) and Predicate Invention (Muggleton, 1991; Muggleton and Buntine, 1992).", "startOffset": 130, "endOffset": 176}, {"referenceID": 44, "context": "At the beginning, ILP was associated with the introduction of foundational theoretical concepts which included Inverse Resolution (Muggleton, 1995; Muggleton and Buntine, 1992) and Predicate Invention (Muggleton, 1991; Muggleton and Buntine, 1992).", "startOffset": 201, "endOffset": 247}, {"referenceID": 48, "context": "At the beginning, ILP was associated with the introduction of foundational theoretical concepts which included Inverse Resolution (Muggleton, 1995; Muggleton and Buntine, 1992) and Predicate Invention (Muggleton, 1991; Muggleton and Buntine, 1992).", "startOffset": 201, "endOffset": 247}, {"referenceID": 56, "context": "A number of ILP systems were developed along with learning about the theoretical concepts of ILP such as FOIL (Quinlan, 1990) and Golem (Muggleton et al.", "startOffset": 110, "endOffset": 125}, {"referenceID": 45, "context": "The widely-used ILP system Progol (Muggleton, 1995) introduced a new logically-based approach to refinement graph search of the hypothesis space based on inverting the entailment relation.", "startOffset": 34, "endOffset": 51}, {"referenceID": 43, "context": "Integrating bottom-up and top-down searches, incorporating predicate invention, eliminating the need for explicit negative examples and allowing restricted use of cuts helps in solving these issues (Mooney, 1996).", "startOffset": 198, "endOffset": 212}, {"referenceID": 16, "context": "Probabilistic ILP (PILP) also gained popularity (Cussens, 2001a; De Raedt and Kersting, 2008; Muggleton et al., 1996), its Prolog-based systems such as PRISM (Sato et al.", "startOffset": 48, "endOffset": 117}, {"referenceID": 60, "context": ", 1996), its Prolog-based systems such as PRISM (Sato et al., 2005) and FAM (Cussens, 2001b) separate the actual learning of the logic program from the probabilistic parameters estimation of the individual clauses.", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": ", 2005) and FAM (Cussens, 2001b) separate the actual learning of the logic program from the probabilistic parameters estimation of the individual clauses.", "startOffset": 16, "endOffset": 32}, {"referenceID": 47, "context": "However in practice, learning the structure and parameters of probabilistic logic representation simultaneously has proven to be a challenge (Muggleton, 2002).", "startOffset": 141, "endOffset": 158}, {"referenceID": 53, "context": "Meta-interpretive learning (MIL) (Muggleton et al., 2014) is a recent ILP method which learns recursive definitions using Prolog and ASP-based declarative representations.", "startOffset": 33, "endOffset": 57}, {"referenceID": 58, "context": "The eXtended Hybrid Abductive Inductive Learning system (XHAIL) is an ILP approach based on ASP that generalises techniques of language and search bias from Horn clauses to normal logic programs with full usage of NAF (Ray, 2009).", "startOffset": 218, "endOffset": 229}, {"referenceID": 30, "context": "Like its predecessor system Hybrid Abductive Inductive Learning (HAIL) which operated on Horn clauses, XHAIL is based on Abductive Logic Programming (ALP) (Kakas et al., 1992), we give more details on XHAIL in Section 4.", "startOffset": 155, "endOffset": 175}, {"referenceID": 31, "context": "The Incremental Learning of Event Definitions (ILED) algorithm (Katzouris et al., 2015) relies on Abductive-Inductive learning and comprises of a scalable clause refinement methodology based on a compressive summarization of clause coverage in a stream of examples.", "startOffset": 63, "endOffset": 87}, {"referenceID": 35, "context": "The Inductive Learning of Answer Set Programs approach (ILASP) is an extension of the notion of learning from answer sets (Law et al., 2014).", "startOffset": 122, "endOffset": 140}, {"referenceID": 36, "context": "ILASP2 (Law et al., 2015) extends the hypothesis space of ILASP with choice rules and weak constraints.", "startOffset": 7, "endOffset": 25}, {"referenceID": 46, "context": "ILP should produce a better ratio between breadth of coverage and depth of analysis (Muggleton, 1999).", "startOffset": 84, "endOffset": 101}, {"referenceID": 35, "context": "ILP has been applied to the field of NLP successfully; it has not only been shown to have higher accuracies than various other ML approaches in learning the past tense of English but also shown to be capable of learning accurate grammars which translate sentences into deductive database queries (Law et al., 2014).", "startOffset": 296, "endOffset": 314}, {"referenceID": 70, "context": "Except for one early application (Wirth, 1989) no application of ILP methods surfaced until the system CHILL (Mooney, 1996) was developed which learned a shift-reduce parser in Prolog from a training corpus of sentences paired with the desired parses by learning control rules and uses ILP to learn control strategies within this framework.", "startOffset": 33, "endOffset": 46}, {"referenceID": 43, "context": "Except for one early application (Wirth, 1989) no application of ILP methods surfaced until the system CHILL (Mooney, 1996) was developed which learned a shift-reduce parser in Prolog from a training corpus of sentences paired with the desired parses by learning control rules and uses ILP to learn control strategies within this framework.", "startOffset": 109, "endOffset": 123}, {"referenceID": 71, "context": "CHILL was also used for parsing database queries to automate the construction of a natural language interface (Zelle and Mooney, 1996) and helped in demonstrating its ability to learn semantic mappings as well.", "startOffset": 110, "endOffset": 134}, {"referenceID": 72, "context": "An extension of CHILL, CHILLIN (Zelle et al., 1994) was used along with an extension of FOIL, mFOIL (Tang and Mooney, 2001) for semantic parsing.", "startOffset": 31, "endOffset": 51}, {"referenceID": 67, "context": ", 1994) was used along with an extension of FOIL, mFOIL (Tang and Mooney, 2001) for semantic parsing.", "startOffset": 56, "endOffset": 79}, {"referenceID": 36, "context": "ASP expresses preferences through weak constraints and may also contain weak constraints or optimisation statements which impose an ordering on the answer sets (Law et al., 2015).", "startOffset": 160, "endOffset": 178}, {"referenceID": 42, "context": "The system of Mitra and Baral (Mitra and Baral, 2016) uses ASP as primary knowledge representation and reasoning language to address the task of Question Answering.", "startOffset": 30, "endOffset": 53}, {"referenceID": 42, "context": "After consulting the authors of (Mitra and Baral, 2016) we learned that they had the same issues and used XHAIL, therefore we also opted to base our research on XHAIL due to it being the most robust tool for our task in comparison to the others.", "startOffset": 32, "endOffset": 55}, {"referenceID": 30, "context": "Initially the examples E plus background knowledge B are transformed into a theory of Abductive Logic Programming (Kakas et al., 1992).", "startOffset": 114, "endOffset": 134}, {"referenceID": 26, "context": "XHAIL integrates version 3 of Gringo (Gebser et al., 2011) and Clasp (Gebser et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 27, "context": ", 2011) and Clasp (Gebser et al., 2012b) which are both quite outdated.", "startOffset": 18, "endOffset": 40}, {"referenceID": 6, "context": "In particular Clasp in this version does not support three important improvements that have been found for ASP optimisation: (i) unsat-core optimisation (Andres et al., 2012), (ii) stratification for obtaining suboptimal answer sets (Alviano et al.", "startOffset": 153, "endOffset": 174}, {"referenceID": 5, "context": ", 2012), (ii) stratification for obtaining suboptimal answer sets (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013), and (iii) unsat-core shrinking (Alviano and Dodaro, 2016).", "startOffset": 66, "endOffset": 113}, {"referenceID": 7, "context": ", 2012), (ii) stratification for obtaining suboptimal answer sets (Alviano et al., 2015b; Ans\u00f3tegui et al., 2013), and (iii) unsat-core shrinking (Alviano and Dodaro, 2016).", "startOffset": 66, "endOffset": 113}, {"referenceID": 2, "context": ", 2013), and (iii) unsat-core shrinking (Alviano and Dodaro, 2016).", "startOffset": 40, "endOffset": 66}, {"referenceID": 24, "context": "We predict chunks using our learned hypothesis and facts from preprocessing, using the Clingo (Gebser et al., 2008) ASP solver.", "startOffset": 94, "endOffset": 115}, {"referenceID": 1, "context": "An example sentence in the SemEval iSTS dataset (Agirre et al., 2016) is as follows.", "startOffset": 48, "endOffset": 69}, {"referenceID": 41, "context": "Stanford CoreNLP tools (Manning et al., 2014) are used for tokenisations and POS-tagging of the input.", "startOffset": 23, "endOffset": 45}, {"referenceID": 9, "context": "Using a shallow parser (Bohnet et al., 2013) we obtain the dependency relations for the sentences.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": "We are using the datasets from the SemEval 2016 iSTS Task 2 (Agirre et al., 2016), which included two separate files containing sentence pairs.", "startOffset": 60, "endOffset": 81}, {"referenceID": 57, "context": "The Images dataset was a collection of captions obtained from the Flickr dataset (Rashtchian et al., 2010).", "startOffset": 81, "endOffset": 106}, {"referenceID": 26, "context": "5 (Gebser et al., 2011) and we use WASP version 2 (Git hash a44a95) (Alviano et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": ", 2011) and we use WASP version 2 (Git hash a44a95) (Alviano et al., 2015a) configured to use unsat-core optimisation with disjunctive core partitioning, core trimming, a budget of 30 seconds for computing the first answer set and for shrinking unsatisfiable cores with progressive shrinking strategy.", "startOffset": 52, "endOffset": 75}, {"referenceID": 22, "context": "There are even more powerful methods for proving significance of results such as bootstrap sampling (Efron and Tibshirani, 1986), however these methods require markedly higher computational effort in experiments and our experiments already show significance with the t-test.", "startOffset": 100, "endOffset": 128}, {"referenceID": 1, "context": "State-of-the-art comparison Table 2 shows a comparison of our results with the baseline and the three best systems from the chunking subtask of Task 2 from SemEval2016 Task2 (Agirre et al., 2016): DTSim (Banjade et al.", "startOffset": 174, "endOffset": 195}, {"referenceID": 8, "context": ", 2016): DTSim (Banjade et al., 2016), FBK-HLT-NLP (Magnolini et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 39, "context": ", 2016), FBK-HLT-NLP (Magnolini et al., 2016) and runs 1 and 2 of IISCNLP (Tekumalla and Jat, 2016).", "startOffset": 21, "endOffset": 45}, {"referenceID": 68, "context": ", 2016) and runs 1 and 2 of IISCNLP (Tekumalla and Jat, 2016).", "startOffset": 36, "endOffset": 61}, {"referenceID": 32, "context": "We also compare with results of our own system \u2018Inspire-Manual\u2019 (Kazmi and Sch\u00fcller, 2016).", "startOffset": 64, "endOffset": 90}, {"referenceID": 14, "context": "\u2022 The baseline makes use of the automatic probabilistic chunker from the IXA-pipeline which provides Perceptron models (Collins, 2002) for chunking and is trained on CONLL2000 corpora and corrected manually,", "startOffset": 119, "endOffset": 134}, {"referenceID": 18, "context": "\u2022 FBK-HLT-NLP obtains chunks using a Python implementation of MBSP chunker which uses a Memory-based part-of-speech tagger generator (Daelemans et al., 1996),", "startOffset": 133, "endOffset": 157}, {"referenceID": 0, "context": "\u2022 Inspire-Manual (our previous system) makes use of manually set chunking rules (Abney, 1991) using ASP (Kazmi and Sch\u00fcller, 2016).", "startOffset": 80, "endOffset": 93}, {"referenceID": 32, "context": "\u2022 Inspire-Manual (our previous system) makes use of manually set chunking rules (Abney, 1991) using ASP (Kazmi and Sch\u00fcller, 2016).", "startOffset": 104, "endOffset": 130}, {"referenceID": 29, "context": "However, in other application domains such as learning to interpret input data from user examples (Gulwani et al., 2015), a perfect fit to the input data might be desired and required.", "startOffset": 98, "endOffset": 120}, {"referenceID": 67, "context": "Note that pruning examples to learn from inconsistent data as done by Tang and Mooney (Tang and Mooney, 2001) is not necessary for our approach.", "startOffset": 86, "endOffset": 109}, {"referenceID": 67, "context": "ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 71, "context": "ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al.", "startOffset": 76, "endOffset": 123}, {"referenceID": 15, "context": "ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al.", "startOffset": 204, "endOffset": 230}, {"referenceID": 33, "context": "ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al., 2004), and in Microsoft Excel Gulwani et al.", "startOffset": 263, "endOffset": 282}, {"referenceID": 15, "context": "ILP has been applied to a variety of NLP and other problems such as parsing (Tang and Mooney, 2001; Zelle and Mooney, 1996), automatic construction of biological knowledge bases from scientific abstracts (Craven and Kumlien, 1999), automatic scientific discovery (King et al., 2004), and in Microsoft Excel Gulwani et al. (2015) where users can specify data extraction rules using examples.", "startOffset": 205, "endOffset": 329}, {"referenceID": 10, "context": "We provide the modified XHAIL system in a public repository fork (Bragaglia and Sch\u00fcller, 2016).", "startOffset": 65, "endOffset": 95}], "year": 2017, "abstractText": "Inductive Logic Programming (ILP) combines rule-based and statistical artificial intelligence methods, by learning a hypothesis comprising a set of rules given background knowledge and constraints for the search space. We focus on extending the XHAIL algorithm for ILP which is based on Answer Set Programming and we evaluate our extensions using the Natural Language Processing application of sentence chunking. With respect to processing natural language, ILP can cater for the constant change in how we use language on a daily basis. At the same time, ILP does not require huge amounts of training examples such as other statistical methods and produces interpretable results, that means a set of rules, which can be analysed and tweaked if necessary. As contributions we extend XHAIL with (i) a pruning mechanism within the hypothesis generalisation algorithm which enables learning from larger datasets, (ii) a better usage of modern solver technology using recently developed optimisation methods, and (iii) a time budget that permits the usage of suboptimal results. We evaluate these improvements on the task of sentence chunking using three datasets from a recent SemEval competition. Results show that our improvements allow for learning on bigger datasets with results that are of similar quality to state-of-the-art systems on the same task. Moreover, we compare the hypotheses obtained on datasets to gain insights on the structure of each dataset.", "creator": "LaTeX with hyperref package"}}}