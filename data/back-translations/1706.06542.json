{"id": "1706.06542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Extract with Order for Coherent Multi-Document Summarization", "abstract": "In this paper, we aim to develop an extractive summary in multi-document setting. We implement a rank-based sentence selection using continuous vector representations along with key sentences. In addition, we propose a model to address summary coherence to increase readability. We conduct experiments with data sets from the Document Understanding Conference (DUC) 2004 using the ROUGE toolkit. Our experiments show that the methods bring significant improvements over the state of the art in terms of informativeness and coherence.", "histories": [["v1", "Mon, 12 Jun 2017 04:54:41 GMT  (64kb,D)", "http://arxiv.org/abs/1706.06542v1", "6 pages, Accepted in TextGraphs-11: Graph-based Methods for Natural Language Processing, Workshop at ACL 2017, Vancouver, Canada, August 2017"]], "COMMENTS": "6 pages, Accepted in TextGraphs-11: Graph-based Methods for Natural Language Processing, Workshop at ACL 2017, Vancouver, Canada, August 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mir tafseer nayeem", "yllias chali"], "accepted": false, "id": "1706.06542"}, "pdf": {"name": "1706.06542.pdf", "metadata": {"source": "CRF", "title": "Extract with Order for Coherent Multi-Document Summarization", "authors": ["Mir Tafseer Nayeem", "Yllias Chali"], "emails": ["mir.nayeem@uleth.ca", "chali@cs.uleth.ca"], "sections": [{"heading": "1 Introduction", "text": "The task of automatic document summarization aims at finding the most relevant informations in a text and presenting them in a condensed form. A good summary should retain the most important contents of the original document or a cluster of documents, while being coherent, non-redundant and grammatically readable. There are two types of summarizations: abstractive summarization and extractive summarization. Abstractive methods, which are still a growing field are highly complex as they need extensive natural language generation to rewrite the sentences. Therefore, research community is focusing more on extractive summaries, which selects salient (important) sentences from the source document without any modification to create a summary. Summarization is classified as single-document or multi-document based upon the number of source document. The information overlap between the documents from the same topic makes the multi-document summarization more challenging than the task of summarizing single documents.\nOne crucial step in generating a coherent summary is to order the sentences in a logical manner to increase the readability. A wrong order of sentences convey entirely different idea to the reader of the summary and also make it difficult to understand. In a single document, summary information can be presented by preserving the sentence position in the original document. In multi-document summarization, the sentence position in the original document does not provide clue to the sentence arrangement. Hence it is a very challenging task to perform the arrangement of sentences in the summary."}, {"heading": "2 Related Work", "text": "During a decade, several extractive approaches have been developed for automatic summary generation that implement a number of machine learning, graph-based and optimization techniques. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization. The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance. Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011). Unfortunately, none of the above systems care about the coherence of the final extracted summary.\nIn very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used a simple recurrent network based sequence classifier to solve the problem of extractive summarization. However, they are limited to single document settings, where sentences are implicitly ordered according to the sentence position. (Parveen and Strube, 2015; Parveen et al., 2015)\nar X\niv :1\n70 6.\n06 54\n2v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n17\nproposed graph-based techniques to tackle coherence, which is also limited to single document summarization. Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities.\nIn this paper, we implement a rank based sentence selection using continuous vector representations along with key-phrases. We also model the coherence using semantic relations between entities and sentences to increase the readability."}, {"heading": "3 Sentence Extraction", "text": "We here successively describe each of the steps involved in the sentence extraction process such as sentence ranking, sentence clustering, and sentence selection."}, {"heading": "3.1 Preprocessing", "text": "Our system first takes a set of related texts as input and preprocesses them which includes tokenization, Part-Of-Speech (POS) tagging, removal of stopwords and Lemmatization. We use NLTK toolkit1 to preprocess each sentence to obtain a more accurate representation of the information."}, {"heading": "3.2 Sentence Similarity", "text": "We take the pre-trained word embeddings2 (Mikolov et al., 2013) of all the non stopwords in a sentence and take the weighted vector sum according to the term-frequency (TF ) of a word(w) in a sentence(S). Where, E is the word embedding model and idx(w) is the index of the word w. More formally, for a given sentence S in the document D, the weighted sum becomes,\nS = \u2211 w\u2208S TF (w, S) \u00b7 E[idx(w)]\nThen we calculate cosine similarity between the sentence vectors obtained from the above equation to find the relative distance between Si and Sj . We also calculate NESim(Si, Sj) by finding the Named Entities present in Si and Sj using NLTK Toolkit, then calculating their overlap.\nCosSim(Si, Sj) = Si \u00b7 Sj ||Si|| ||Sj ||\n1http://www.nltk.org/ 2https://code.google.com/archive/p/word2vec/\nNESim(Si, Sj) = |NE(Si) \u2229NE(Sj)|\nmin(|NE(Si)|, |NE(Sj)|)\nSim(Si, Sj) = \u03bb \u00b7NESim(Si, Sj) + (1\u2212 \u03bb) \u00b7 CosSim(Si, Sj) (1)\nThe overall similarity calculation involves both CosSim(Si, Sj) and NESim(Si, Sj) where, 0 \u2264 \u03bb \u2264 1 decides the relative contributions of them to the overall similarity computation. This standalone similarity function will be used in this work with different \u03bb values to accomplish different tasks."}, {"heading": "3.3 Sentence Ranking", "text": "In this section, we rank the sentences by applying TextRank algorithm (Mihalcea and Tarau, 2004) which involves constructing an undirected graph where sentences are vertices, and weighted edges are formed connecting sentences by a similarity metric. TextRank determines the similarity based on the lexical overlap between two sentences. However, this algorithm has a serious drawback: If two sentences are talking about the same topic without using any overlapped words, there will be no edge between them. Instead, we use the continuous skip-gram model introduced by (Mikolov et al., 2013) to measure the semantic similarity along with the entity overlap. We use the similarity function described in Equation (1) by setting \u03bb = 0.3.\nAfter we have our graph, we can run the main algorithm on it. This involves initializing a score of 1 for each vertex, and repeatedly applying the TextRank update rule until convergence. The update rule is:\nRank(Si) = (1\u2212 d) + d \u2217\u2211 Sj\u2208N(Si) Sim(Si, Sj)\u2211 Sk\u2208N(Sj) Sim(Sj , Sk) Rank(Sj)\nWhere, Rank(Si) indicates the importance score assigned to sentence Si. N(Si) is the set of neighboring sentences of Si, and 0 \u2264 d \u2264 1 is a dampening factor, which the literature suggests its setting to 0.85. After reaching convergence, we extract the sentences along with TextRank scores."}, {"heading": "3.4 Sentence Clustering", "text": "The sentence clustering step allows us to group similar sentences. We use a hierarchical agglomerative clustering (Murtagh and Legendre, 2014) with a complete linkage criteria. This method proceeds incrementally, starting with each sentence considered as a cluster, and merging the pair of similar clusters after each step using bottom up approach. The complete linkage criteria determines the metric used for the merge strategy. In computing the clusters, we use the similarity function described in Equation (1) with \u03bb = 0.4. We set a similarity threshold (\u03c4 = 0.5) to stop the clustering process. If we cannot find any cluster pair with a similarity above the threshold, the process stops, and the clusters are released. The clusters may be small, but are highly coherent as each sentence they contain must be similar to every other sentence in the same cluster.\nThis sentence clustering step is very important due to two main reasons, (1) Selecting at most one sentence from each cluster of related sentences will decrease redundancy from the summary side (2) Selecting sentences from the diverse set of clusters will increase the information coverage from the document side as well."}, {"heading": "3.5 Sentence Selection", "text": "In this work, we use the concept-based ILP framework introduced in (Gillick and Favre, 2009) with some suitable changes to select the best subset of sentences. This approach aims to extract sentences that cover as many important concepts as possible, while ensuring the summary length is within a given budgeted constraint. Unlike (Gillick and Favre, 2009) which uses bigrams as concepts, we use keyphrases as concepts. Keyphrases are the words or phrases that represent the main topics of a document. Sentences containing the most relevant keyphrases are important for the summary generation. We extracted the keyphrases from the document cluster using RAKE3 (Rose et al., 2010). We assign a weight to each keyphrase using the score returned by RAKE.\nLet wi be the weight of keyphrase i and ki a binary variable that indicates the presence of keyphrase i in the extracted sentences. Let lj be the number of words in sentence j, sj a binary variable that indicates the presence of sentence j in the extracted sentence set and L the length limit\n3https://github.com/aneesha/RAKE\nfor the set. Let Occij indicate the occurrence of keyphrase i in sentence j, the ILP formulation is,\nMaximize : ( \u2211 i wiki+ \u2211 j Rank(Sj)\u00b7sj) (2)\nSubject to : \u2211 j ljsj \u2264 L (3)\nsjOccij \u2264 ki, \u2200i, j (4)\n\u2211 j sjOccij \u2265 ki, \u2200i (5)\n\u2211 j\u2208gc sj \u2264 1, \u2200gc (6)\nki \u2208 {0, 1} \u2200i (7)\nsj \u2208 {0, 1} \u2200j (8)\nWe try to maximize the weight of the keyphrases (2) in the extracted sentences, while avoiding repetition of those keyphrases (4, 5) and staying under the maximum number of words allowed for the sentence extraction (3).\nIn addition to (Gillick and Favre, 2009), we put some extra features like maximizing the sentence rank scores returned from the sentence ranking section. In order to ensure only one sentence per cluster in the extracted sentences we add an extra constraint (6). In this process, we extract the optimal combination of sentences that maximize informativity while minimizing redundancy (Figure 1 illustrates our sentence extraction process in brief)."}, {"heading": "4 Sentence Ordering", "text": "Classic reordering approaches include inferring order from weighted sentence graph (Barzilay et al., 2002), or perform a chronological ordering algorithm (Cohen et al., 1999) that sorts sentences based on timestamp and position.\nWe here propose a simple greedy approach to sentence ordering in multi-document settings. Our assumption is that a good sentence order implies the similarity between all adjacent sentences since word repetition (more specifically, named entity repetition) is one of the formal sign of text coherence (Barzilay et al., 2002). We define coherence of document D which consists of sentences from\nS1 to Sn in the following equation. For calculating Sim(Si , Si+1), we use the similarity function described in equation (1) with \u03bb = 0.5, giving the named entities a little more preference.\nCoherence(D) =\n\u2211n\u22121 i=1 Sim(Si , Si+1)\nn\u2212 1\nWe propose a greedy algorithm for placing a sentence in a document based on the coherence score we discussed above4. At the beginning, we randomly select a sentence from the extracted sentences without any position information and place the sentence in the ordered set D. We then incrementally add each extracted sentences to the document set D using Algorithm (1) to get the final order of summary sentences.\n4Note that, we didn\u2019t take any position information of the original sentences to be extracted from the document.\nAlgorithm 1: Place a sentence to a document Procedure SentencePositioning(D,Sn)\nData: Input document D which is assumed sorted. New sentence Sn which we will place in the document D. Result: Return new document Dn after placing the sentence Sn. t\u2190 1; Cohmax \u2190 0 ; Dtmp \u2190 D ; l\u2190 DocLength(D) ; while t \u2264 l + 1 do \u21d2Place the Sn in tth position of Dtmp ; Cohtmp \u2190 Coherence(Dtmp); if Cohtmp > Cohmax then\nDn \u2190 Dtmp; Cohmax \u2190 Cohtmp; \u21d2 Remove Sn from the tth position of\nthe document Dtmp ; end t\u2190 t+ 1;\nend return Dn;"}, {"heading": "5 Evaluation", "text": "We evaluate our system ILPRankSumm (ILP based sentence selection with TextRank for Extractive Summarization) using ROUGE5 (Lin, 2004) on DUC 2004 (Task-2, Length limit(L) = 100 words). However, ROUGE scores are biased towards lexical overlap at surface level and insensitive to summary coherence. Moreover, sophisticated coherence evaluation metrics are seldom adopted for summarization thus many of the previous systems used human evaluation for measuring readability. For this reason, we evaluate our summary coherence using (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008) which defines coherence probabilities for an ordered set of sentences."}, {"heading": "5.1 Baseline Systems", "text": "We compare our system with baseline (LexRank, GreedyKL) and state of the art systems (Submodular, ICSISumm). LexRank(Erkan and Radev, 2004) represents input texts as graph where nodes\n5ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -x -r 1000 -f A -p 0.5 -t 0\nare the sentences and the edges are formed between two sentences if the cosine similarity is above a certain threshold. Sentence importance is calculated by running the PageRank algorithm on the graph. GreedyKL (Haghighi and Vanderwende, 2009) iteratively selects the next sentence for the summary that will minimize the KL divergence between the estimated word distributions. (Lin and Bilmes, 2011) treat the document summarization problem as maximizing a Submodular function under a budget constraint. They achieved a near-optimal information coverage and non-redundancy using a modified greedy algorithm. On the other hand, ICSISumm (Gillick and Favre, 2009) employs a global linear optimization framework, finding the globally optimal summary rather than choosing sentences according to their importance in a greedy fashion.\nThe summaries generated by the baselines and the state-of-the-art extractive summarizers on the DUC 2004 dataset were collected from (Hong et al., 2014)."}, {"heading": "5.2 Results", "text": "Our results include R-1, R-2, and R-SU4, which counts matches in unigrams, bigrams, and skipbigrams respectively. The skip-bigrams allow four words in between. According to Table 1, R-1, R2 scores obtained by our system outperform all the baselines and state of the art systems on DUC 2004 datasets. One of the main reasons of getting the improved R-1 and R-2 score is the use of keyphrases. Moreover, there is no significant difference between our proposed system and submodular in case of R-SU4. We also get better coherence probability because of our sentence ordering technique. The system\u2019s output for a randomly selected document set (e.g. d30015t) from DUC 2004 is shown in Table 2."}, {"heading": "5.3 Limitations", "text": "One of the essential properties of the text summarization systems is the ability to generate a summary with a fixed length (DUC 2004, Task-2: Length limit = 100 words). According to (Hong et al., 2014) all the summarizer from the previous research either truncated the summary to 100th word, or removed the last sentence from the summary set. In this paper, we follow the second one to produce grammatical summary. However, the first one produces a certain ungrammatical sentence, later one can lose a lot of information in\nthe worst case, if the sentences are long. We more focus on the grammaticality of the final summary."}, {"heading": "6 Conclusion and Future Work", "text": "In this work, we implemented an ILP based sentence selection along with TextRank scores and key phrases for extractive multi-document summarization. We further model the coherence to increase the readability of the generated summary. Evaluation results strongly indicate the benefits of using continuous word vector representations in all the steps involved in the overall system. In future, we will focus on jointly extracting the sentences to maximize informativity and readability while minimizing redundancy using the same ILP model. Moreover, we will also try to propose a solution for the length limit problem."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their useful comments. The research reported in this paper was conducted at the University of Lethbridge and supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada discovery grant and the University of Lethbridge."}], "references": [{"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["Regina Barzilay", "Noemie Elhadad", "Kathleen R. McKeown."], "venue": "J. Artif. Int. Res. 17(1):35\u201355.", "citeRegEx": "Barzilay et al\\.,? 2002", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Comput. Linguist. 34(1):1\u201334.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin-", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning to order things", "author": ["William W. Cohen", "Robert E. Schapire", "Yoram Singer."], "venue": "J. Artif. Int. Res. 10(1):243\u2013270.", "citeRegEx": "Cohen et al\\.,? 1999", "shortCiteRegEx": "Cohen et al\\.", "year": 1999}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R. Radev."], "venue": "J. Artif. Int. Res. 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "A scalable global model for summarization", "author": ["Dan Gillick", "Benoit Favre."], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. Association for Computational Linguistics, Stroudsburg, PA, USA, ILP \u201909,", "citeRegEx": "Gillick and Favre.,? 2009", "shortCiteRegEx": "Gillick and Favre.", "year": 2009}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Haghighi and Vanderwende.,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "A repository of state of the art and competitive baseline summaries for generic news summarization", "author": ["Kai Hong", "John Conroy", "Benoit Favre", "Alex Kulesza", "Hui Lin", "Ani Nenkova."], "venue": "Proceedings of the Ninth International Conference on Language Re-", "citeRegEx": "Hong et al\\.,? 2014", "shortCiteRegEx": "Hong et al\\.", "year": 2014}, {"title": "Improving the estimation of word importance for news multidocument summarization", "author": ["Kai Hong", "Ani Nenkova."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Associ-", "citeRegEx": "Hong and Nenkova.,? 2014", "shortCiteRegEx": "Hong and Nenkova.", "year": 2014}, {"title": "Automatic evaluation of text coherence: Models and representations", "author": ["Mirella Lapata", "Regina Barzilay."], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco,", "citeRegEx": "Lapata and Barzilay.,? 2005", "shortCiteRegEx": "Lapata and Barzilay.", "year": 2005}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Computational Linguistics,", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "A class of submodular functions for document summarization", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1. Association for Computa-", "citeRegEx": "Lin and Bilmes.,? 2011", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "Textrank: Bringing order into texts", "author": ["Rada Mihalcea", "Paul Tarau."], "venue": "Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004. Association for Computational Linguistics, Barcelona, Spain, pages 404\u2013411.", "citeRegEx": "Mihalcea and Tarau.,? 2004", "shortCiteRegEx": "Mihalcea and Tarau.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ward\u2019s hierarchical agglomerative clustering method: Which algorithms implement ward\u2019s criterion? J", "author": ["Fionn Murtagh", "Pierre Legendre."], "venue": "Classif. 31(3):274\u2013295.", "citeRegEx": "Murtagh and Legendre.,? 2014", "shortCiteRegEx": "Murtagh and Legendre.", "year": 2014}, {"title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents", "author": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9,", "citeRegEx": "Nallapati et al\\.,? 2017", "shortCiteRegEx": "Nallapati et al\\.", "year": 2017}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["Daraksha Parveen", "Hans-Martin Ramsl", "Michael Strube."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "citeRegEx": "Parveen et al\\.,? 2015", "shortCiteRegEx": "Parveen et al\\.", "year": 2015}, {"title": "Integrating importance, non-redundancy and coherence in graph-based extractive summarization", "author": ["Daraksha Parveen", "Michael Strube."], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. AAAI Press, IJCAI\u201915, pages", "citeRegEx": "Parveen and Strube.,? 2015", "shortCiteRegEx": "Parveen and Strube.", "year": 2015}, {"title": "Automatic keyword extraction from individual documents", "author": ["Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley."], "venue": "Text Mining pages 1\u201320.", "citeRegEx": "Rose et al\\.,? 2010", "shortCiteRegEx": "Rose et al\\.", "year": 2010}, {"title": "Exploring text links for coherent multi-document summarization", "author": ["Xun Wang", "Masaaki Nishino", "Tsutomu Hirao", "Katsuhito Sudoh", "Masaaki Nagata."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization.", "startOffset": 8, "endOffset": 31}, {"referenceID": 12, "context": "LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based methods of computing sentence importance for text summarization.", "startOffset": 45, "endOffset": 71}, {"referenceID": 8, "context": "The RegSum system (Hong and Nenkova, 2014) employs a supervised model for predicting word importance.", "startOffset": 18, "endOffset": 42}, {"referenceID": 11, "context": "Treating multidocument summarization as a submodular maximization problem has proven successful by (Lin and Bilmes, 2011).", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "In very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 15, "context": "In very recent works using neural network, (Cheng and Lapata, 2016) proposed an attentional encoder-decoder and (Nallapati et al., 2017) used", "startOffset": 112, "endOffset": 136}, {"referenceID": 17, "context": "(Parveen and Strube, 2015; Parveen et al., 2015) ar X iv :1 70 6.", "startOffset": 0, "endOffset": 48}, {"referenceID": 16, "context": "(Parveen and Strube, 2015; Parveen et al., 2015) ar X iv :1 70 6.", "startOffset": 0, "endOffset": 48}, {"referenceID": 19, "context": "Moreover, a recent work (Wang et al., 2016) actually proposed a multi-document summarization system that combines both coherence and informativeness but this system is limited to syntactic linkages between entities.", "startOffset": 24, "endOffset": 43}, {"referenceID": 13, "context": "We take the pre-trained word embeddings2 (Mikolov et al., 2013) of all the non stopwords in", "startOffset": 41, "endOffset": 63}, {"referenceID": 12, "context": "In this section, we rank the sentences by applying TextRank algorithm (Mihalcea and Tarau, 2004) which involves constructing an undirected graph", "startOffset": 70, "endOffset": 96}, {"referenceID": 13, "context": "Instead, we use the continuous skip-gram model introduced by (Mikolov et al., 2013) to measure the semantic similarity", "startOffset": 61, "endOffset": 83}, {"referenceID": 14, "context": "erative clustering (Murtagh and Legendre, 2014) with a complete linkage criteria.", "startOffset": 19, "endOffset": 47}, {"referenceID": 5, "context": "work introduced in (Gillick and Favre, 2009) with some suitable changes to select the best subset of sentences.", "startOffset": 19, "endOffset": 44}, {"referenceID": 5, "context": "Unlike (Gillick and Favre, 2009) which uses bigrams as concepts, we use keyphrases as concepts.", "startOffset": 7, "endOffset": 32}, {"referenceID": 18, "context": "We extracted the keyphrases from the document cluster using RAKE3 (Rose et al., 2010).", "startOffset": 66, "endOffset": 85}, {"referenceID": 5, "context": "In addition to (Gillick and Favre, 2009), we put", "startOffset": 15, "endOffset": 40}, {"referenceID": 0, "context": "Classic reordering approaches include inferring order from weighted sentence graph (Barzilay et al., 2002), or perform a chronological ordering algorithm (Cohen et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 3, "context": ", 2002), or perform a chronological ordering algorithm (Cohen et al., 1999) that sorts sentences based on timestamp and position.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": "Our assumption is that a good sentence order implies the similarity between all adjacent sentences since word repetition (more specifically, named entity repetition) is one of the formal sign of text coherence (Barzilay et al., 2002).", "startOffset": 210, "endOffset": 233}, {"referenceID": 10, "context": "We evaluate our system ILPRankSumm (ILP based sentence selection with TextRank for Extractive Summarization) using ROUGE5 (Lin, 2004) on DUC 2004 (Task-2, Length limit(L) = 100 words).", "startOffset": 122, "endOffset": 133}, {"referenceID": 9, "context": "For this reason, we evaluate our summary coherence using (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008) which defines coherence probabilities for an ordered set of sentences.", "startOffset": 57, "endOffset": 84}, {"referenceID": 1, "context": "For this reason, we evaluate our summary coherence using (Lapata and Barzilay, 2005) (Barzilay and Lapata, 2008) which defines coherence probabilities for an ordered set of sentences.", "startOffset": 85, "endOffset": 112}, {"referenceID": 4, "context": "LexRank(Erkan and Radev, 2004) represents input texts as graph where nodes", "startOffset": 7, "endOffset": 30}, {"referenceID": 6, "context": "GreedyKL (Haghighi and Vanderwende, 2009) iteratively selects the next sentence for the summary that will minimize the KL divergence between the estimated word distributions.", "startOffset": 9, "endOffset": 41}, {"referenceID": 11, "context": "(Lin and Bilmes, 2011) treat the document summarization problem as maximizing a Submodular function under a budget constraint.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "On the other hand, ICSISumm (Gillick and Favre, 2009) employs a global linear optimization framework, finding the globally optimal summary rather than choosing sentences according to their", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "The summaries generated by the baselines and the state-of-the-art extractive summarizers on the DUC 2004 dataset were collected from (Hong et al., 2014).", "startOffset": 133, "endOffset": 152}, {"referenceID": 7, "context": "According to (Hong et al., 2014) all the summarizer from the previous research either truncated the summary to 100th word, or removed the last sentence from the summary set.", "startOffset": 13, "endOffset": 32}], "year": 2017, "abstractText": "In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a model to tackle summary coherence for increasing readability. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the methods bring significant improvements over the state of the art methods in terms of informativity and coherence.", "creator": "LaTeX with hyperref package"}}}