{"id": "1707.00561", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Identifying hazardousness of sewer pipeline gas mixture using classification methods: a comparative study", "abstract": "In this work, we formulated a real problem related to the detection of sewer pipe gases using classifying approaches. The primary objective of this work was to determine the hazards of sewer pipes in order to provide sewer pipe workers with safe and non-hazardous access so that deaths from toxic exposure to sewer gas components can be avoided. In order to design such a prediction model, several classification algorithms were used and their performance evaluated empirically and statistically across the collected data set and compared with the performance of several ensemble methods in order to understand the extent of the improvements these methods offer. In order to design such a prediction model, several classification algorithms were used and their performance evaluated empirically as well as statistically across the collected data set. Furthermore, the performance of several ensemble methods was analysed to understand the extent of the improvements these methods offer. The result of this comprehensive study showed that perforated pre-algorithms, based on multi-radial algorithms, provided better performance than many other pre-algorithms.", "histories": [["v1", "Tue, 16 May 2017 08:57:46 GMT  (1221kb,D)", "http://arxiv.org/abs/1707.00561v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["varun kumar ojha", "paramartha dutta", "atal chaudhuri"], "accepted": false, "id": "1707.00561"}, "pdf": {"name": "1707.00561.pdf", "metadata": {"source": "CRF", "title": "Identifying Hazardousness of Sewer-Pipeline Gas-Mixture using Classification Methods A Comparative Study", "authors": ["Varun Kumar Ojha", "Atal Chaudhuri"], "emails": ["varun.kumar.ojha@vsb.cz", "paramartha.dutta@gmail.com", "atalc23@gmail.com"], "sections": [{"heading": null, "text": "V. K. Ojha IT4Innovations, VS\u030cB Technical University of Ostrava, Ostrava, Czech Republic and Dept. of Computer Science & Engineering, Jadavpur University, Kolkata, India E-mail: varun.kumar.ojha@vsb.cz\nP. Dutta Dept. of Computer & System Sciences, Visva-Bharati University, India E-mail: paramartha.dutta@gmail.com\nA Chaudhuri Dept. of Computer Science & Engineering, Jadavpur University, Kolkata, India E-mail: atalc23@gmail.com\nNeural Computing and Applications DOI: 10.1007/s00521-016-2443-0\nar X\niv :1\n70 7.\n00 56\n1v 1\n[ cs\n.N E\n] 1\n6 M\nay 2\n01 7\nKeywords Sewer gas detection \u00b7 Neural network \u00b7 Classification \u00b7 KS test"}, {"heading": "1 Introduction", "text": "This is in the view of providing a solution to a real-world problem using technology, where the human fatalities need to be avoided. Hence, the technology should be as simple as possible. In this work, we addressed a complex realworld problem related to sewer-pipeline gas detection, where sewer-pipeline safety detection (in terms of non-toxic environment) was required to allow maintenance and cleaning of the pipeline. The sewer gas detection is a highly complex problem because of the presence of several toxic gases in a mixture form, and a single gas detector may not offer reliable solution. Therefore, we studied the complexity of this problem in terms of gas mixture. The primary goal was to offer a simple solution with a high accuracy so that it was easy to categorize the hazardous situation in straightforward way such as \u201chazardous\u201d or \u201cnon-hazardous.\u201d To meet this simplicity, we formulated sewer-pipeline gas detection problem as a classification problem.\nSewer-pipeline contains a mixture of several toxic gases such as hydrogen sulphide (H2S), ammonia (NH3), methane (CH4), carbon dioxide (CO2), nitrogen oxides (NOx), etc., [1,2,3]. Usually, this mixture is generated due to the biodegradation of the waste and the sewage into the sewer-pipeline. Such toxic gas-mixture is fatal for those who come to the proximity/exposure of these gases. Following this, an alarming number of human fatalities are reported each year by the newspapers and the other agencies [4,5,6]. The authorities those are responsible for maintaining and cleaning of the sewer pipeline provides various electronic portable gas detectors available in the market to the employed persons so that they can determine the safeness of the sewer-environment before physically get involve into the maintenance work. However, the available electronic portable gas detectors are not providing satisfactory results. It is evident from the recent comments from the judiciary to these authorities. In a judgment to a civil appeal number 5322 of 2011, the Supreme Court of India stated, \u201cthe State and its agencies/instrumentalities cannot absolve themselves of the responsibility to put in place effective mechanism for ensuring safety of the workers employed for maintaining and cleaning the sewage system [7].\u201d Similarly, in another judgment, the Supreme Court of India stated, \u201c...entering sewer lines without safety gears should be made a crime even in emergency situations... [8,9].\u201d This motivated us to carry out our research in this domain and to come out with a simple solution so that without having the minimum knowledge of the technicalities of gas composition and safety limits, a person is able to understand the environment of a sewer system before entering.\nTo ensure the simplicity in model, we collected and preprocessed data to realize sewer gas-detection as a binary class classification problem. However, in this work, apart from the objective of constructing a prediction model, we set a secondary objective, which was to analyze the performances of the classifiers,\nboth empirically and statistically. To meet these objectives, we used 12 base predictors from four different categories such as neural network based classifiers, tree based classifiers, instance based classifiers, and rule based classifiers. The algorithms were applied over the collected dataset and the performance of the algorithms were collected in terms of the accuracy. The collected results were then used for analyzing the performance superiority of the one algorithm over another or the one category of algorithms over another.\nWe observed that the performance of the algorithms were independent of the category they belong to. For example, the performance of instance based k-nearest neighbor, logistic model tree, and support vector machine came from three different categories, but they had a very competitive performance. However, we must consider the \u201cNo-free-lunch theorem\u201d that suggests that some algorithms perform better on some problem and some on another [10]. Therefore, to find out which predictor performs best in this case, we used 12 base predictors and nine ensemble methods.\nRest of the article is organized as follows. A background study is provided in Section 2.1, which leads to setting ground for describing our contribution to the sewer-pipeline gas detection. In Section 2.2, we provide a detailed description of the data collection and preprocessing mechanisms, which constitute the core and significant part for formulating gas detection problem as a binary classification problem. Section 2.3 deals with the brief descriptions of the classifiers/algorithms and methods used for constructing the prediction model. The design of comprehensive experiment set for the evaluation of the classifiers is reported in Section 3. Whereas, Section 3 describes empirical and statistical evaluation of the classifiers, discussions and conclusion are reported in Sections 4 and 5, respectively."}, {"heading": "2 Methodology", "text": "In this Section, we put together the background study, the data collection mechanisms, and the classification methods definitions. The background study describes the significance of the sewer-pipeline gas detection problem and the data collection mechanism describes the formulation of gas-detection as a classification problem.\n2.1 Background Study\nLiterature review was conducted in the perspective of electronic-nose (ENOSE) and gas-detection-system to cover a broad area of research in the field of gas detection and modeling using intelligent computing techniques/algorithms. Although not much work specifically on sewer gas-mixture-detection was reported in the past, few notable contributions were observed. Li et al. [11] reported a noticeable research work on the development and design of an electronic nose (E-NOSE) and gas detection system, where a neural network\n(NN)-based mixed gas (NOx, and CO) measurement system was developed. On the other hand, Sirvastava et al. [12,13] proposed a design of intelligent ENOSE system using backpropagation (BP) and neuro-genetic approach. Llobet et al. [14] presented a pattern recognition approach, based on the wallet transformation for gas mixture analysis using single tin-oxide sensor. Liu et al. [13] addressed a genetic-NN algorithm to recognize patterns of mixed gases (a mixture of three component gases) using infrared gas sensor. Lee et al. [15] illustrated uses of micro gas sensor array (GSA) combined with NN for recognizing combustible leakage gases. Ambard et al. [16] have demonstrated use of NN for gas discrimination using a tin-oxide GSA for the gases H2, CO and CH4. In [17], authors have illustrated a NN-based technique for developing a gas sensory system for sensing gases in a dynamic environment. Pan et al. [18] have shown several applications of E-NOSE. Wongchoosuka et al. [19] have proposed an E-NOSE detection system based on carbon nanotube-SnO2 gas sensors for detecting methanol. Zhang et al. [20] developed a knowledge-based genetic algorithm for detecting mixed gas in mines. Won et al. [21] proposed a system for estimation of hazardous gas release rate using optical sensor and NN-based technique. The following salient points came out of the above mentioned articles:\n\u2013 Mainly, BP and NN-based approaches were studied so far for detecting gas-mixtures. \u2013 Mostly, the E-Nose systems reported in the past were developed for the gas-mixtures of only two or three gases and the sensors of the gases used were less cross-sensitive to the other gases in mixtures. \u2013 Cross-sensitivity during sensing is an important factor in gas detection system, which was least reported in literature as yet. However, Ojha et al. [22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc., where cross-sensitivity factor has been addressed to some extent. However, these works were primarily related to regression modeling. \u2013 The impact of humidity and temperature on sensors remained ignored so far. \u2013 The gas detection system or E-Nose was viewed only in the framework of regression problems and not classification problem.\nClassification based approach led us to determine the hazardous and nonhazardous situation of a sewer-pipeline. In addition, the collection, organization, and the preprocessing of the collected data enabled us to address the cross-sensitivity issue firmly. The cross-sensitivity issue occurs because of the sensitivity of one gas-sensor towards multiple gases. So was our case, where a semiconductor-based GSA was designed using five gas-sensors. Each gas-sensor was typically meant for detecting its respective target gas. Hence, when the GSA was used for collecting data for a mixture of gases, the crosssensitivity in the sensed values (collected data) became inevitable. Therefore, rather than considering pure results of the respective gases, we registered the cross-sensitive results as a part of our-collected data. Since a computation-\nally intelligent model learned from the data and also maintained the crosssensitivity patterns registered in terms of data values itself, a learned model accurately predicts an unknown gas mixture.\n2.2 Equipment and Data Collection Mechanism\nBefore explaining the details of data collection and equipment, we need to explain the basic design and the purpose of our work, which is to offer an intelligent gas detection system (an electronic portable gas detector) that will be a result of embedding learned-predictor (trained-classifier) into an electronic system. The data flow into our developed intelligent system is shown in Fig. 1, which describes the entire process of the intelligent system design, which is divided into three phases: 1) The data acquisition unit, which consists of gas suction-motor chamber, GSA, and data acquisition-cum data-preprocessor block; 2) An intelligent unit (classifier unit), which receives data from dataacquisition unit and classifying the acquired data patterns; 3) The output unit, which prompts the result in terms of colored light and buzzer. Hence, our objective here was limited to only train a classifier using the collected data. We describe the data collection process as follows.\nAt first, we collected the data samples from the data-sheets, literature, and laboratories test of the collected gas mixture samples from sewer-pipelines. Second, we designed our own metal oxide semiconductor (MOS) gas sensors array (GSA) that was used for verifying the literature and laboratory data and for generating the data samples for the purpose experiments. Our designed GSA consists of five gas-sensors for sensing five different gases. They include hydrogen sulphide (H2S), ammonia (NH3), methane (CH4), carbon dioxide (CO2), and nitrogen oxides (NOx). Typically, MOS sensors are resistance-type electrical sensors, where responses are change in circuit resistance proportional to gas concentration, A resistance type sensor responds to change in resistance due to change in the concentration of gases. The change in resistance is given as \u03b4Rs/R0, where \u03b4Rs is change in MOS sensor resistance and R0 is base resistance or the sensing resistance at a specifics gas concentration in clean air [19]. The R0 of the sensors MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 is 0.25 ppm, 100 ppm, 10 ppm, 100 ppm and 1000 ppm, respectively. Here, ppm is the unit for measuring concentration of gas into air which is defined as follows: 1 ppm is equal to 1 volume of a gas into 106 volume of air.\nA typical arrangement of a gas sensor array is shown in Fig. 2. The circuitry shown in Fig. 2 (left) was developed in our laboratory. Here, the fabricated and installed sensors were MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 for gases NO2, CO, H2S, NH3, and CH4, respectively [28,29].\nThe gas sensors used were sensitive to not only their target gases, but they were sensitive also to other gases in the gas-mixture [30,31]. Hence, crosssensitivity effect over MOS sensors was confirmed [32]. It was moreover confirmed that the sensor responses were noisy and accordingly the pattern of such noise were considered and recorded as an instance into our dataset. Hence, a non-intelligent use of raw values of sensor response for hazardousness prediction may be misleading in operating (real-world) environment. Therefore, a training electronic portable gas detector may be used to predict sewer hazardousness, accurately. So was the effort in this work to provide a classifier.\nData collection had vital role in training of a classifier. Data samples were collected as per the following steps. At first, several manhole samples collected from the Kolkata, India municipal area were tested in laboratory to identify the presence of several toxic gases such as nitrogen dioxide (NO2), carbon monoxide (CO), hydrogen sulphide (H2S), ammonia (NH3), methane (CH4), and carbon dioxide (CO2). Secondly, gas sensors were identified for each of the respective gases. As a result we came out with the procurement of gas sensor MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 for NO2, CO, H2S, NH3, and CH4, respectively. We collected data sheets form the companies for the respective sensors. In the third step, a laboratory was setup for the verification and collection of the sensor response of the respective gas sensors in certain range of their concentration. Specifically, the concentration range in ppm laid down in sensor manuals of sensors MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 are [0.25 - 5], [20 - 1000], [1 - 100], [10 - 300] and [300 - 10000] of the gases NO2, CO, H2S, NH3, and CH4, respectively. In addition, the lab was setup (see Fig. 2 [right]), where gas cylinders were connected to a gas concentration measuring unit called mass flow controller (MFC), which was further connected to a gas chamber, where each gas was allowed to pass in a specific concentration over an array of gas sensor. More specifically, the behavior of each of the gas sensors was recorded.\nThe following steps were used for preparing data sample for the classifiers\u2019 training. First, hazardous (safety) limits of the component gases of manhole gas mixture were collected. Secondly, three different levels, (i) above safetylimit, (ii) at safety-limit, and (iii) below safety-limit for each manhole gas were recognized. Thirdly, gases were mixed in different combination to prepare several mixture sample that were used to pass over GSA. Table 1 indicates few examples of such mixture of gases in different combinations. For example, when we mix five gases each of which has three different recognized concentration levels, we get 243 different combinations (35). In addition, we considered the role of humidity and temperature to influence the sensor\u2019s behavior. Accordingly, the data values were recorded. Hence, our collected dataset contained seven input features and an output class. Each sample was labeled with \u201c0\u201d for safe sample (if the responses of all five sensors were under the maximum safety limit) or \u201c1\u201d for unsafe sample (if the responses of any among the five sensors were above the maximum safety limit). The safety limits of the manhole gases are as follows: safety limit of NH3 is between 25 ppm and 40 ppm [33], CO is in between 35 ppm and 100 ppm [34], H2S is in between 50 ppm and 100 ppm [35], CO2 is in between 5000 ppm and 8000 ppm [36] and CH4 is in between 5000 ppm and 10000 ppm [37]. Table 2 illustrates a fraction of the collected data samples.\n2.3 Classification Based Approach\nWe categorized the classifiers in the four different groups of classifiers. Each category of classifiers contains three classifiers."}, {"heading": "2.3.1 Network Based Classifiers", "text": "Multi-layer perceptron (MLP) is a computational model that imitates human brain, and learn from environment, i.e., data. In our work, we used threelayered MLP, where layers are input layer, hidden layer, and output layer [38].\nRadial Basis Function Network (RBF) is a special class of MLP, where inputs are mapped onto a hidden layer that consists of radial basis function, which does the non-linear mapping of input to a hidden layer [39].\nSupport vector machine (SVM) is a supervised learning computational model that maps input to a high dimension feature space using kernel trick. Hence, non-linear separable patterns in input space are linearly classified on a high dimensional feature space [40]."}, {"heading": "2.3.2 Tree Based Classifiers", "text": "Reduced pruning tree (REP) is a tree based classifier method, where a treelike structure is designed for predicting target class based on the input variables [41,42]. More specifically, the leaves of tree offers decision of the class based on the conjunction of the input feature represented by the branches of the tree. REP tree is a decision tree, where the tree size is reduced by pruning inefficient branches [43].\nNaive Bayes tree (NBT) is a special class of decision tree, where the leaf nodes of decision tree that offer decision on the class is replaced by a Naive Bayes classifier, which decides the class label, based on the features and learned threshold [44].\nLogistic Model Trees (LMT) is similar to NBT that does the transformation of leaves of a decision tree into a logistic regression node. A logistic regression maps independent variables to categorical dependent variables using a logistic function [45,46]. Hence, LMT is a simple idea, where nodes of a decision/classification tree are replaced by logistic regression model [47]."}, {"heading": "2.3.3 Rule Based Classifiers", "text": "Decision Table (DT) is a simple representation of data into a table based system, where the decision is made based on the features matching or searched into a decision table. On a successful search, the majority class label is returned, otherwise the majority class label of the entire dataset is returned as a decision for an unlabeled data [48].\nPART is a rule based classification method based on partial decision tree that generates a list of rules, used subsequently for making prediction of unknown data instance. The rules are generated based on the partial decision tree, which splits dataset into subsets until the entire dataset gets exhausted to form nodes and leaf nodes of the tree [49].\nMajority Predictor (Zero R) is the simplest possible form of classification method. It is based on the majority of class label into a dataset. In simple words, it always predicts the majority class."}, {"heading": "2.3.4 Instance Based Classifiers", "text": "Instance-Based Learning (IBK) provides the concept description which is the primary output of an IBK algorithm. It is a function that maps an instance to a category (class label). The concept description function is updated based on training procedure that involves two functions similarity and classification. The similarity function computes the similarity between the training instances and the pre-stored instances, and returns a numeric-value. Then, the classification function provides class label to the instances based on the results of similarity function. Accordingly, the concept description is updated [50].\nK\u2217 (K Star) is an instance-based learner that uses an entropy-based similarity matching function for searching/matching test instances to the learned instances [51].\nLocally Weighted Learning (LWL). In a locally weighted learning, the prediction models are allowed to create at local points in a dataset or the specific point of interest rather than creating model for entire dataset. Hence, a linear regression or naive Bayes classifier or any other classifier may be used to create local models. In this case, we use Decision Stamp, which is a single level decision tree model for prediction [52,53]."}, {"heading": "2.3.5 Ensemble Methods", "text": "In this work, we tried to exploit different method of making ensemble. For an ensemble to perform well, we need to take into account two things which are accuracy of predictors and diversity among the predictors [54]. For example, Bagging maintains diversity by bootstrapping dataset, AddBoost combines several weak predictors, Random Subspace maintains diversity by splitting feature space, Random committee maintains diversity by creating predictors using different random seeds, and Rotation forest maintains diversity by splitting and extracting feature subspace using principal component analysis. Similarly, in multi-scheme and voting scheme, we combine several predictors to maintain diversity. Here, we describe the ensemble methods as follows.\nBagging. In Bagging, several copies of same predictor is created. Each copy of the predictor learns a different replicate of learning set created from the complete training set using bootstrapping. Finally, the predictor\u2019s decision is combined using plurality voting method [55].\nAdaptive Boosting (AdaBoost) is an ensemble technique that combines several weak predictors and inaccurate rules to create an accurate predictor [56].\nRandom Subspace (Random SUB). In random subspace ensemble method feature space is divided into several feature subset. Hence, predictors are constructed for each feature subset. Finally, the decision of each constructed predictors are combined using voting method [57]. Random Committee (Random COM): In a random committee ensemble, several predictors are constructed over similar dataset, but they use different random seeds to maintain diversity in the ensemble.\nRotation Forest (Rotation FRST). In this approach, training set for the predictors are created by splitting feature set into K subsets, and Principal Component Analysis is applied to extract all the principle components [58]. Hence, diversity among the predictors are maintained by K axis rotation to form new feature set for training [58].\nEnsemble Selection (Ensemble SEL). In the ensemble selection approach, the ensemble starts with an empty bag, and the predictors (chosen from a library of trained predictors) maximizing the performance of ensemble are added to the bag one by one to compute the decision of ensemble by using voting method [59].\nVoting Scheme (Vote). The voting scheme combines probability distribution of several chosen predictors/classifiers (or predictors available in a bag for making ensemble) using majority voting combination method [60].\nMulti-Scheme (Multi). The multi-scheme ensemble approach uses a bag of predictors and selects the output class by selecting a predictor from the bag of predictors based on cross-validation performance of the predictors [60].\nWeighted Predictor Ensemble (WPE). In this scheme of ensemble, the weight of predictors were determined. Subsequently, the ensemble output of k many predictors were computed as follows:\ny = arg c\nmax j=1 k\u2211 j=1 wjI (Pj = \u03c9j) ,\nwhere c is the number of classes (here it is two), I (Pj = \u03c9j) is a function that returns value one for the predicted class \u03c9j ."}, {"heading": "3 Experimental Framework and Results", "text": "Our aim in the experiment design was to obtain a highly accurate model for predicting hazardousness of the environment in a sewer pipeline. The sewerpipeline environment was represented by the collected dataset. The second objective of the experiment design was to obtain results for analyzing the classifiers (predictors). Accordingly, the results of the classifiers were collected. Table 3 represents the parameter setting of the chosen classifiers. For the evaluation of the classifiers, we repeated our experiments 10 times. Finally, the results were compared based on empirical and statistical (Kolmogorov\u2013 Smirnov test) evaluation. We used WEKA [61] and MATLAB tools [62] for the purpose of our experiments.\nWe organized the experimental results into three parts as reflected in Table 4. The first part in the table describes the category wise performance of classifier. Hence, the performance of the category of classifiers was evaluated. We represented the performance of the classifiers as per their training and test accuracy. An accuracy close to 1.0 indicates 100% classification accuracy. Accordingly, the standard deviation (std) of training and test accuracies were reported for understanding the consistency of the classifiers\u2019 performance. In Table 4, the performance of the classifiers were arranged as follows. The category is arranged in the ascending order of their average accuracy over 10-fold CV test set, i.e., better performing classifier to the less performing classifier. The dataset was portioned into 10 equal sets and each time 9 sets were used for training and one set for testing. This process was repeated 10 times and each time a unique test set was used.\nIn the second part, we organized the results according to rank of the classifiers\u2019 performance over 10-fold test set. It may please be noted that for each classifier, we collected 10 instances of 10-fold CV training and test results. Hence, the results in Table 5 reflect averaged training and test accuracy of the classifiers. However, ranking the classifiers based only on the average results does not say much about the quality of the classifier. Hence, in the\nthird part of the results, we used pairwise comparison of the classifiers using Kolmogorov\u2013Smirnov (KS) test, which ascertains whether the supremacy of one classifier over the other is statistically significant or not. A comprehensive matrix of the pairwise KS test results are presented in Table 6. The KS Test is a non-parametric statistical test that determines the difference between the cumulative frequency distribution (cfd) of two samples. In other words, it indicates whether the empirical cfd of one sample is equal \u201c=\u201d, larger \u201c \u201d, or smaller \u201c\u227a\u201d than the other. It tells whether two dataset A and B are statistically similar \u201cA=B\u201d, dissimilar \u201cA\u227aB\u201d, where A being statistically dominated by B, or dissimilar \u201cA B\u201d, where A being statistically dominant over B. In our experiments, the KS test was evaluated with 5% significance level, i.e., with 95% confidence."}, {"heading": "4 Discussions", "text": "Since the developed electronic portable gas detector shall be used by naive persons who are engaged in maintaining sewer-pipeline, we are looking for binary answer. Hence, our objective is to search for classification accuracy and\nthe model (weights) with the highest accuracy so that such a combination may be implemented into electronic portable gas detector form.\nMoreover, it is also a difficult task to be certain with the accuracy of an implemented electronic portable gas detector because the toxic exposure of a gas is also proportional to the time and not only its safety limit. However, with a real-time monitoring and requisite maintenance involved, the accuracy of detector may be relaxed and hence, we resorted to choose 90% accuracy as the accuracy for our developed detector. So, the classifier\u2019s performance was compared with a threshold setting of 90% accuracy.\nFirst, let us discuss on the obtained results. For the classifiers belonging to network-based category F1, the classifier SVM performs better than its counterparts MLP and RBF both in terms of high accuracy (test accuracy 0.93403) and high consistency (std on test accuracy 0.0041). On the other hand, the performance of MLP was reported next to SVM with high consistency. The performance of the RBF was found to be inconsistent and poorer in comparison to its counterparts.\nIn the tree based category F2, the performance of LMT and REPTree was comparable to whereas, NBTree has shown poor performance compared to its counterparts.\nIn instance-base category F3, the performance of IBK and K Star was comparative with a high accuracy and high consistency. LWL performed poor with a very low accuracy.\nWhen it came to the category of rule based classifier F4, PART has outperformed others in its category, but the consistency was not as high as the consistency of the other well performing classifiers IBK, SVM, MLP, etc. The classifier ZeroR consistently performed poor in comparison to all other classifiers.\nIn the ensemble category E1 and E2, the multi-scheme, Random COM, Rotation FRST, Bagging, WPE, and Ensemble SEL performed with high accuracies (over 90%) and consistency. However, the performance of the ensembles Random Forest, Vote, and AdaBoost were not as satisfactory as compared to the other ensembles. One of the reason behind poor performance of Random SUB was the usage of subset of the features. Therefore, the feature selection may not help in case of this dataset because of the high correlation maintained by each of the features with the output feature. Similarly, Voting used probability measures to combine the predictors and AddBoost combined weak predictors, whereas, the entirely better performing ensemble exploited the best predictors. Hence, they performed better in this scenario.\nConsidering the assumption of 90% accuracy being a good predictor for implementation as gas detector, we can figure out from Table 5 that the classifiers belong to category F3 (exception of the classifier LWL) had performed better than the classifiers of other categories. However, the instance based classifier IBK is not suitable for the implementation as electronic gas detector since it required a large memory for its computation for saving all the instances of the training set. IBK prediction is computed based on all training samples. Hence, it takes long time to compute the output, which is unacceptable in real time.\nThe next category whose performance was found close to IBK were the classifiers of category F2 (tree based classifier). Two classifies, LMT and REP Tree qualified the 90% accuracy threshold. On the contrary, two classifiers from each F1 and F4 had performed lower than 90% accuracy. However, SVM performed significantly well with a very high accuracy 93.36%. Similarly, classifier PART from category F4 had an accuracy of 90.86%. However, since the SVM produced less number of parameters than the tree based predictor and it robustly accommodates the noisy attributes, it was recommended from these experiments that SVM is a proper choice for the implementation of the proposed gas detector."}, {"heading": "5 Conclusion", "text": "In this work, we explored a real world problem in the context of classification, where we simplified the approach by offering binary decision to the problem. We explored the problem related to the detection of hazardousness of a sewer pipeline environment. This is very crucial problem since it is related to the safety of the persons who have to work under the toxic environment of\nthe sewer-pipeline. Usually, a sewer-pipeline environment contains mixture of toxic gases. Hence, we collected samples from sewer pipelines from different locations. Then we examined those samples to identify data samples for our experiments. We prepared a large dataset by collecting gas sensor responses from laboratory tests, literature and scaled the collected gas sensor responses to form a dataset where non-hazardous samples were labeled 0 and hazardous samples were labeled 1. Finally, we applied 21 different classifiers over the identified dataset and their empirical and statistical performance were evaluated. We discovered that for this problem, the instance based classifier performed best followed by the performance of tree based classifiers. However, we found that the performance of the classifiers were dependent on the ability and mechanism of the classifiers themselves and not on the information regarding which category they belong to.\nAcknowledgements This work was supported by the IPROCOM Marie Curie Initial Training Network, funded through the People Programme (Marie Curie Actions) of the European Unions Seventh Framework Programme."}], "references": [{"title": "the insidious foe\u201d\u2013sewer gas,", "author": ["J. Whorton"], "venue": "Western Journal of Medicine, vol. 175,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Sewer gases in the home,", "author": ["N. Gromicko"], "venue": "http://www.nachi.org/sewer-gaseshome.html", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Deaths in the drains,", "author": ["T. Hindu"], "venue": "http://www.thehindu.com/opinion/oped/deaths-in-the-drains/article5868090.ece?homepage=true., Accessed on 15 Dec", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Dying in the gutters,", "author": ["S. Anand"], "venue": "Tehelka Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Provide safety gear to sewer workers who enter manholes, says court,", "author": ["T. Hindu"], "venue": "http://www.thehindu.com/todays-paper/tp-national/provide-safety-gear-tosewer-workers-who-enter-manholes-says-court/article2228688.ece, Accessed on", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "No free lunch theorems for optimization,", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "A mixed gas sensor system based on thin film saw sensor array and neural network,", "author": ["J. Li"], "venue": "Proceedings of the Twelfth Southern Biomedical Engineering Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "On the design issue of intelligent electronic nose system,", "author": ["A. Srivastava", "S. Srivastava", "K. Shukla"], "venue": "Proceedings of IEEE International Conference on Industrial Technology 2000.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Multicomponent gas mixture analysis using a single tin oxide sensor and dynamic pattern recognition,", "author": ["E. Llobet", "R. Ionescu", "S. Al-Khalifa", "J. Brezmes", "X. Vilanova", "X. Correig", "N. Barsan", "J.W. Gardner"], "venue": "IEEE Sensors Journal,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Micro gas sensor array with neural network for recognizing combustible leakage gases,", "author": ["D.-S. Lee", "S.-W. Ban", "M. Lee", "D.-D. Lee"], "venue": "IEEE Sensors Journal,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "A spiking neural network for gas discrimination using a tin oxide sensor array,", "author": ["M. Ambard", "B. Guo", "D. Martinez", "A. Bermak"], "venue": "IEEE International Symposium on Electronic Design, Test and Applications. IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Dibi, \u201cA novel neural network-based technique for smart gas sensors operating in a dynamic environment,", "author": ["Z.H. Baha"], "venue": "Sensors, vol. 9,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Application of electronic nose in gas mixture quantitative detection,", "author": ["W. Pan", "N. Li", "P. Liu"], "venue": "IEEE International Conference on Network Infrastructure and Digital Content. IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Knowledge-based genetic algorithms data fusion and its application in mine mixed-gas detection,", "author": ["Q. Zhang", "H. Li", "Z. Tang"], "venue": "Chinese Control and Decision Conference (CCDC). IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "The estimation of hazardous gas release rate using optical sensor and neural network,", "author": ["W. So", "J. Koo", "D. Shin", "E.S. Yoon"], "venue": "Computer Aided Chemical Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Performance analysis of neuro genetic algorithm applied on detecting proportion of components in manhole gas mixture,", "author": ["V.K. Ojha", "P. Dutta", "H. Saha"], "venue": "International Journal of Artificial Intelligence \\& Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Performance analysis of neuro swarm optimization algorithm applied on detecting proportion of components in manhole gas mixture,", "author": ["V.K. Ojha", "P. Dutta"], "venue": "Artificial Intelligence Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Convergence analysis of backpropagation algorithm for designing an intelligent system for sensing manhole gases,", "author": ["V.K. Ojha", "P. Dutta", "A. Chaudhuri", "H. Saha"], "venue": "Hybrid Soft Computing Approaches. Springer India,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Conjugate gradient trained neural network for intelligent sensing of manhole gases to avoid human fatality,", "author": ["P. Dutta", "V.K. Ojha"], "venue": "Advances in Secure Computing, Internet Services, and Applications. IGI Global,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Understating continuous ant colony optimization for neural network training: A case study on intelligent sensing of manhole gas components,", "author": ["V.K. Ojha", "P. Dutta", "A. Chaudhuri", "H. Saha"], "venue": "International Journal of Hybrid Intelligent Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Sensor array for manhole gas analysis,", "author": ["S. Ghosh", "A. Roy", "S. Singh", "H. Saha", "V.K. Ojha", "P. Dutta"], "venue": "in 1st International Symposium on Physics and Technology of Sensors (ISPTS). IEEE,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Portable sensor array system for intelligent recognizer of manhole gas,", "author": ["S. Ghosh", "H. Saha", "C. RoyChaudhuri", "V.K. Ojha", "P. Dutta"], "venue": "Sixth International Conference on Sensing Technology (ICST)", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Sensitivity to NO2 and cross-sensitivity analysis to NH3, ethanol and humidity of carbon nanotubes thin film prepared by PECVD,", "author": ["C. Cantalini", "L. Valentini", "I. Armentano", "L. Lozzi", "J. Kenny", "S. Santucci"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Development of a micromachined hazardous gas sensor array,", "author": ["K.D. Mitzner", "J. Sternhagen", "D.W. Galipeau"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "Exposure limits related to air quality and risk assessment,", "author": ["K.J. Donham"], "venue": "Iowa Concentrated Animal Feeding Operations Air Quality Study,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}, {"title": "Human health effects from exposure to low-level concentrations of hydrogen sulfide,", "author": ["S. Simonton"], "venue": "Occupational Health & Safety,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "New insight into panic attacks: Carbon dioxide is the culprit,", "author": ["G. Shilpa"], "venue": "Journal of Young Investigators,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Twenty questions and answers about the ozone layer: 2010 update,", "author": ["D.W. Fahey", "M.I. Hegglin"], "venue": "Scientific assessment of ozone depletion,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Predicting the future: A connectionist approach,", "author": ["A.S. Weigend", "B.A. Huberman", "D.E. Rumelhart"], "venue": "International journal of neural systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Multivariable functional interpolation and adaptive networks,", "author": ["D. Lowe", "D. Broomhead"], "venue": "Complex System,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1988}, {"title": "Classification and regression trees,", "author": ["L. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1984}, {"title": "5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "The effects of pruning methods on the predictive accuracy of induced decision trees,", "author": ["F. Esposito", "D. Malerba", "G. Semeraro", "V. Tamma"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "A comparative study of reduced error pruning method in decision tree algorithms,", "author": ["W.N.H.W. Mohamed", "M.N.M. Salleh", "A.H. Omar"], "venue": "IEEE International Conference on Control System, Computing and Engineering (ICCSCE),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Estimation of the probability of an event as a function of several independent variables,", "author": ["S.H. Walker", "D.B. Duncan"], "venue": "Biometrika, vol", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1967}, {"title": "The regression analysis of binary sequences,", "author": ["D.R. Cox"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1958}, {"title": "Logistic model trees,", "author": ["N. Landwehr", "M. Hall", "E. Frank"], "venue": "Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "The power of decision tables,", "author": ["R. Kohavi"], "venue": "Machine Learning: ECML-95. Springer,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}, {"title": "Generating accurate rule sets without global optimization,", "author": ["E. Frank", "I.H. Witten"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1998}, {"title": "Instance-based learning algorithms,", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine learning,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1991}, {"title": "K*: An instance-based learner using an entropic distance measure,", "author": ["J.G. Cleary", "L.E. Trigg"], "venue": "Proceedings of the 12th International Conference on Machine learning,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1995}, {"title": "Locally weighted naive bayes,", "author": ["E. Frank", "M. Hall", "B. Pfahringer"], "venue": "Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2002}, {"title": "Locally weighted learning,", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Artificial Intelligence Review,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1997}, {"title": "Ensemble based systems in decision making,", "author": ["R. Polikar"], "venue": "IEEE Circuits and Systems Magazine,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}, {"title": "Bagging predictors,", "author": ["L. Breiman"], "venue": "Machine learning,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting,", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1997}, {"title": "The random subspace method for constructing decision forests,", "author": ["T.K. Ho"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1998}, {"title": "Combining pattern classifiers: methods and algorithms", "author": ["L.I. Kuncheva"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": ", [1,2,3].", "startOffset": 2, "endOffset": 9}, {"referenceID": 1, "context": ", [1,2,3].", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "Following this, an alarming number of human fatalities are reported each year by the newspapers and the other agencies [4,5,6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 3, "context": "Following this, an alarming number of human fatalities are reported each year by the newspapers and the other agencies [4,5,6].", "startOffset": 119, "endOffset": 126}, {"referenceID": 4, "context": "In a judgment to a civil appeal number 5322 of 2011, the Supreme Court of India stated, \u201cthe State and its agencies/instrumentalities cannot absolve themselves of the responsibility to put in place effective mechanism for ensuring safety of the workers employed for maintaining and cleaning the sewage system [7].", "startOffset": 309, "endOffset": 312}, {"referenceID": 5, "context": "However, we must consider the \u201cNo-free-lunch theorem\u201d that suggests that some algorithms perform better on some problem and some on another [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "[11] reported a noticeable research work on the development and design of an electronic nose (E-NOSE) and gas detection system, where a neural network", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[12,13] proposed a design of intelligent ENOSE system using backpropagation (BP) and neuro-genetic approach.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[14] presented a pattern recognition approach, based on the wallet transformation for gas mixture analysis using single tin-oxide sensor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[15] illustrated uses of micro gas sensor array (GSA) combined with NN for recognizing combustible leakage gases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[16] have demonstrated use of NN for gas discrimination using a tin-oxide GSA for the gases H2, CO and CH4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In [17], authors have illustrated a NN-based technique for developing a gas sensory system for sensing gases in a dynamic environment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[18] have shown several applications of E-NOSE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[20] developed a knowledge-based genetic algorithm for detecting mixed gas in mines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[21] proposed a system for estimation of hazardous gas release rate using optical sensor and NN-based technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "[22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "[22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "[22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "[22, 23,24,25,26,27] offered a few methods such as neuro-genetic, neuro-swarm, ant-colony-based, neuro-simulated annealing, etc.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Here, the fabricated and installed sensors were MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 for gases NO2, CO, H2S, NH3, and CH4, respectively [28,29].", "startOffset": 151, "endOffset": 158}, {"referenceID": 21, "context": "Here, the fabricated and installed sensors were MiCS - 4514, MQ - 7, MQ - 136, MQ - 135, and MQ - 4 for gases NO2, CO, H2S, NH3, and CH4, respectively [28,29].", "startOffset": 151, "endOffset": 158}, {"referenceID": 22, "context": "The gas sensors used were sensitive to not only their target gases, but they were sensitive also to other gases in the gas-mixture [30,31].", "startOffset": 131, "endOffset": 138}, {"referenceID": 23, "context": "The gas sensors used were sensitive to not only their target gases, but they were sensitive also to other gases in the gas-mixture [30,31].", "startOffset": 131, "endOffset": 138}, {"referenceID": 13, "context": "25 - 5], [20 - 1000], [1 - 100], [10 - 300] and [300 - 10000] of the gases NO2, CO, H2S, NH3, and CH4, respectively.", "startOffset": 9, "endOffset": 20}, {"referenceID": 0, "context": "25 - 5], [20 - 1000], [1 - 100], [10 - 300] and [300 - 10000] of the gases NO2, CO, H2S, NH3, and CH4, respectively.", "startOffset": 22, "endOffset": 31}, {"referenceID": 5, "context": "25 - 5], [20 - 1000], [1 - 100], [10 - 300] and [300 - 10000] of the gases NO2, CO, H2S, NH3, and CH4, respectively.", "startOffset": 33, "endOffset": 43}, {"referenceID": 20, "context": "2 Laboratory-scale gas sensor array (GSA) [28,29]", "startOffset": 42, "endOffset": 49}, {"referenceID": 21, "context": "2 Laboratory-scale gas sensor array (GSA) [28,29]", "startOffset": 42, "endOffset": 49}, {"referenceID": 24, "context": "The safety limits of the manhole gases are as follows: safety limit of NH3 is between 25 ppm and 40 ppm [33], CO is in between 35 ppm and 100 ppm [34], H2S is in between 50 ppm and 100 ppm [35], CO2 is in between 5000 ppm and 8000 ppm [36] and CH4 is in between 5000 ppm and 10000 ppm [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "The safety limits of the manhole gases are as follows: safety limit of NH3 is between 25 ppm and 40 ppm [33], CO is in between 35 ppm and 100 ppm [34], H2S is in between 50 ppm and 100 ppm [35], CO2 is in between 5000 ppm and 8000 ppm [36] and CH4 is in between 5000 ppm and 10000 ppm [37].", "startOffset": 189, "endOffset": 193}, {"referenceID": 26, "context": "The safety limits of the manhole gases are as follows: safety limit of NH3 is between 25 ppm and 40 ppm [33], CO is in between 35 ppm and 100 ppm [34], H2S is in between 50 ppm and 100 ppm [35], CO2 is in between 5000 ppm and 8000 ppm [36] and CH4 is in between 5000 ppm and 10000 ppm [37].", "startOffset": 235, "endOffset": 239}, {"referenceID": 27, "context": "The safety limits of the manhole gases are as follows: safety limit of NH3 is between 25 ppm and 40 ppm [33], CO is in between 35 ppm and 100 ppm [34], H2S is in between 50 ppm and 100 ppm [35], CO2 is in between 5000 ppm and 8000 ppm [36] and CH4 is in between 5000 ppm and 10000 ppm [37].", "startOffset": 285, "endOffset": 289}, {"referenceID": 28, "context": "In our work, we used threelayered MLP, where layers are input layer, hidden layer, and output layer [38].", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "Radial Basis Function Network (RBF) is a special class of MLP, where inputs are mapped onto a hidden layer that consists of radial basis function, which does the non-linear mapping of input to a hidden layer [39].", "startOffset": 208, "endOffset": 212}, {"referenceID": 30, "context": "Reduced pruning tree (REP) is a tree based classifier method, where a treelike structure is designed for predicting target class based on the input variables [41,42].", "startOffset": 158, "endOffset": 165}, {"referenceID": 31, "context": "Reduced pruning tree (REP) is a tree based classifier method, where a treelike structure is designed for predicting target class based on the input variables [41,42].", "startOffset": 158, "endOffset": 165}, {"referenceID": 32, "context": "REP tree is a decision tree, where the tree size is reduced by pruning inefficient branches [43].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "Naive Bayes tree (NBT) is a special class of decision tree, where the leaf nodes of decision tree that offer decision on the class is replaced by a Naive Bayes classifier, which decides the class label, based on the features and learned threshold [44].", "startOffset": 247, "endOffset": 251}, {"referenceID": 34, "context": "A logistic regression maps independent variables to categorical dependent variables using a logistic function [45,46].", "startOffset": 110, "endOffset": 117}, {"referenceID": 35, "context": "A logistic regression maps independent variables to categorical dependent variables using a logistic function [45,46].", "startOffset": 110, "endOffset": 117}, {"referenceID": 36, "context": "Hence, LMT is a simple idea, where nodes of a decision/classification tree are replaced by logistic regression model [47].", "startOffset": 117, "endOffset": 121}, {"referenceID": 37, "context": "On a successful search, the majority class label is returned, otherwise the majority class label of the entire dataset is returned as a decision for an unlabeled data [48].", "startOffset": 167, "endOffset": 171}, {"referenceID": 38, "context": "The rules are generated based on the partial decision tree, which splits dataset into subsets until the entire dataset gets exhausted to form nodes and leaf nodes of the tree [49].", "startOffset": 175, "endOffset": 179}, {"referenceID": 39, "context": "Accordingly, the concept description is updated [50].", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "K\u2217 (K Star) is an instance-based learner that uses an entropy-based similarity matching function for searching/matching test instances to the learned instances [51].", "startOffset": 160, "endOffset": 164}, {"referenceID": 41, "context": "In this case, we use Decision Stamp, which is a single level decision tree model for prediction [52,53].", "startOffset": 96, "endOffset": 103}, {"referenceID": 42, "context": "In this case, we use Decision Stamp, which is a single level decision tree model for prediction [52,53].", "startOffset": 96, "endOffset": 103}, {"referenceID": 43, "context": "For an ensemble to perform well, we need to take into account two things which are accuracy of predictors and diversity among the predictors [54].", "startOffset": 141, "endOffset": 145}, {"referenceID": 44, "context": "Finally, the predictor\u2019s decision is combined using plurality voting method [55].", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "Adaptive Boosting (AdaBoost) is an ensemble technique that combines several weak predictors and inaccurate rules to create an accurate predictor [56].", "startOffset": 145, "endOffset": 149}, {"referenceID": 46, "context": "Finally, the decision of each constructed predictors are combined using voting method [57].", "startOffset": 86, "endOffset": 90}, {"referenceID": 47, "context": "The voting scheme combines probability distribution of several chosen predictors/classifiers (or predictors available in a bag for making ensemble) using majority voting combination method [60].", "startOffset": 189, "endOffset": 193}, {"referenceID": 47, "context": "The multi-scheme ensemble approach uses a bag of predictors and selects the output class by selecting a predictor from the bag of predictors based on cross-validation performance of the predictors [60].", "startOffset": 197, "endOffset": 201}], "year": 2017, "abstractText": "In this work, we formulated a real-world problem related to sewerpipeline gas detection using the classification-based approaches. The primary goal of this work was to identify the hazardousness of sewer-pipeline to offer safe and non-hazardous access to sewer-pipeline workers so that the human fatalities, which occurs due to the toxic exposure of sewer gas components, can be avoided. The dataset acquired through laboratory tests, experiments, and various literature-sources were organized to design a predictive model that was able to identify/classify hazardous and non-hazardous situation of sewer-pipeline. To design such prediction model, several classification algorithms were used and their performances were evaluated and compared, both empirically and statistically, over the collected dataset. In addition, the performances of several ensemble methods were analyzed to understand the extent of improvement offered by these methods. The result of this comprehensive study showed that the instance-based-learning algorithm performed better than many other algorithms such as multi-layer perceptron, radial basis function network, support vector machine, reduced pruning tree, etc. Similarly, it was observed that multi-scheme ensemble approach enhanced the performance of base predictors. V. K. Ojha IT4Innovations, V\u0160B Technical University of Ostrava, Ostrava, Czech Republic and Dept. of Computer Science & Engineering, Jadavpur University, Kolkata, India E-mail: varun.kumar.ojha@vsb.cz P. Dutta Dept. of Computer & System Sciences, Visva-Bharati University, India E-mail: paramartha.dutta@gmail.com A Chaudhuri Dept. of Computer Science & Engineering, Jadavpur University, Kolkata, India E-mail: atalc23@gmail.com Neural Computing and Applications DOI: 10.1007/s00521-016-2443-0 ar X iv :1 70 7. 00 56 1v 1 [ cs .N E ] 1 6 M ay 2 01 7 2 Varun Kumar Ojha et al.", "creator": "LaTeX with hyperref package"}}}