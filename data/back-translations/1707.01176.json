{"id": "1707.01176", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "Charmanteau: Character Embedding Models For Portmanteau Creation", "abstract": "Portmanteaus are a word-building phenomenon in which two words are combined to form a new word. We propose character-level neural sequence-to-sequence (S2S) methods for portmanteau generation that are end-to-end trainable and language independent and do not use explicit additional phonetic information. We propose a sound channel-style model that allows for the inclusion of unattended word lists and improves performance over a standard source-to-target model. This model is made possible by a comprehensive candidate generation strategy made possible specifically by the characteristics of the Portmanteau task. Experiments have shown that our approach is superior to a state-of-the-art FST-based baseline in terms of soil rectitude and human evaluation.", "histories": [["v1", "Tue, 4 Jul 2017 23:11:52 GMT  (262kb,D)", "https://arxiv.org/abs/1707.01176v1", "Accepted for publication in EMNLP 2017"], ["v2", "Mon, 24 Jul 2017 16:27:55 GMT  (262kb,D)", "http://arxiv.org/abs/1707.01176v2", "Accepted for publication in EMNLP 2017"]], "COMMENTS": "Accepted for publication in EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["varun gangal", "harsh jhamtani", "graham neubig", "eduard h hovy", "eric nyberg"], "accepted": true, "id": "1707.01176"}, "pdf": {"name": "1707.01176.pdf", "metadata": {"source": "CRF", "title": "CharManteau: Character Embedding Models For Portmanteau Creation", "authors": ["Varun Gangal", "Harsh Jhamtani", "Graham Neubig", "Eduard Hovy", "Eric Nyberg"], "emails": ["vgangal@cs.cmu.edu", "jharsh@cs.cmu.edu", "gneubig@cs.cmu.edu", "hovy@cs.cmu.edu", "ehn@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pin\u0303eros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015). Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation\n\u2217* denotes equal contribution\nis difficult to capture using a set of rules. For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages.\nPrior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple FSTs used in the previous work, and (2) automatically capture features such as phonetic similarity through the use of char-\nar X\niv :1\n70 7.\n01 17\n6v 2\n[ cs\n.C L\n] 2\n4 Ju\nl 2 01\n7\nacter embeddings, removing the need for explicit grapheme-to-phoneme prediction. To test these hypotheses, in this paper, we propose a neural S2S model to predict portmanteaus given the two root words, specifically making 3 major contributions:\n\u2022 We propose an S2S model that attends to the two input words to generate portmanteaus, and an additional improvement that leverages noisy-channel-style modelling to incorporate a language model over the vocabulary of words (\u00a72). \u2022 Instead of using the model to directly pre-\ndict output character-by-character, we use the features of portmanteaus to exhaustively generate candidates, making scoring using the noisy channel model possible (\u00a73). \u2022 We curate and share a new and larger dataset\nof 1624 portmanteaus (\u00a74). In experiments (\u00a75), our model performs better than the baseline Deri and Knight (2015) on both objective and subjective measures, demonstrating that such methods can be used effectively in a morphological task."}, {"heading": "2 Proposed Models", "text": "This section describes our neural models."}, {"heading": "2.1 Forward Architecture", "text": "Under our first proposed architecture, the input sequence x = concat(x(1), \u201c;\u201d,x(2)), while the output sequence is the portmanteau y. The model learns the distribution P (y|x).\nThe network architecture we use is an attentional S2S model (Bahdanau et al., 2014). We use a bidirectional encoder, which is known to work well for S2S problems with similar token order, which is true in our case. Let \u2212\u2212\u2212\u2212\u2192 LSTM and \u2190\u2212\u2212\u2212\u2212 LSTM represent the forward and reverse encoder; eenc() and edec() represent the character embedding functions used by encoder and decoder The following equations describe the model: h \u2212\u2192enc 0 = \u2212\u2192 0 , h \u2190\u2212enc |x| = \u2212\u2192 0 h \u2212\u2192enc t = \u2212\u2212\u2212\u2212\u2192 LSTM(henct\u22121, eenc(xt)) h \u2190\u2212enc t = \u2190\u2212\u2212\u2212\u2212 LSTM(henct+1, eenc(xt)) henct = h \u2212\u2192enc t + h \u2190\u2212enc t\nhdec0 = h enc |x|\nhdect = LSTM(h dec t\u22121, [concat(edec(yt\u22121), ct\u22121)])\npt = softmax(Whs[concat(hdect , ct)] + bs)\nThe context vector ct is computed using dotproduct attention over encoder states. We choose dot-product attention because it doesn\u2019t add extra parameters, which is important in a low-data scenario such as portmanteau generation.\nati = dot(h dec t , h enc i ), \u03b1 t = softmax(at)\nct = i=|x|\u2211 i=1 \u03b1tih enc i\nIn addition to capturing the fact that portmanteaus of two English words typically sound English-like, and to compensate for the fact that available portmanteau data will be small, we pretrain the character embeddings on English language words. We use character embeddings learnt using an LSTM language model over words in an English dictionary,1 where each word is a sequence of characters, and the model will predict next character in sequence conditioned on previous characters in the sequence."}, {"heading": "2.2 Backward Architecture", "text": "The second proposed model uses Bayes\u2019s rule to reverse the probabilities P (y|x) = P (x|y)P (y)P (x) to get argmaxy P (y|x) = argmaxy P (x|y)P (y). Thus, we have a reverse model of the probability P (x|y) that the given root words were generated from the portmanteau and a character language model model P (y). This is a probability distribution over all character sequences y \u2208 A\u2217, where A is the alphabet of the language. This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT (Hoang et al. (2017), Yu et al. (2016)). Such a model offers two advantages\n1. The reverse direction model (or alignment model) gives higher probability to those portmanteaus from which one can discern the root words easily, which is one feature of good portmanteaus.\n2. The character language model P (y) can be trained on a large vocabulary of words in the language. The likelihood of a word y is factorized as P (y) = \u03a0i=|y|i=1 P (yi|y i\u22121 1 ), where\nyij = yi, yi+1 . . . yj , and we train a LSTM to maximize this likelihood.\n1 Specifically in our experiments, 134K words from the CMU dictionary (Weide, 1998)."}, {"heading": "3 Making Predictions", "text": "Given these models, we must make predictions, which we do by two methods\nGreedy Decoding: In most neural sequenceto-sequence models, we perform autoregressive greedy decoding, selecting the next character greedily based on the probability distribution for the next character at current time step. We refer to this decoding strategy as GREEDY.\nExhaustive Generation: Many portmanteaus were observed to be concatenation of a prefix of the first word and a suffix of the second. We therefore generate all candidate outputs which follow this rule. Thereafter we score these candidates with the decoder and output the one with the maximum score. We refer to this decoding strategy as SCORE.\nGiven that our training data is small in size, we expect ensembling (Breiman, 1996) to help reduce model variance and improve performance. In this paper, we ensemble our models wherever mentioned by training multiple models on 80% subsamples of the training data, and averaging log probability scores across the ensemble at test-time."}, {"heading": "4 Dataset", "text": "The existing dataset by Deri and Knight (2015) contains 401 portmanteau examples from Wikipedia. We refer to this dataset as DWiki. Besides being small for detailed evaluation, DWiki is biased by being from just one source. We manually collect DLarge, a dataset of 1624 distinct English portmanteaus from following sources:\n\u2022 Urban Dictionary2\n\u2022 Wikipedia \u2022 Wiktionary \u2022 BCU\u2019s Neologism Lists from \u201994 to \u201912.\nNaturally, DWiki \u2282 DLarge. We define DBlind = DLarge\u2212DWiki as the dataset of 1223 examples not from Wikipedia. We observed that 84.7% of the words in DLarge can be generated by concatenating prefix of first word with a suffix of the second.\n2Not all neologisms are portmanteaus, so we manually choose those which are for our dataset."}, {"heading": "5 Experiments", "text": "In this section, we show results comparing various configurations of our model to the baseline FST model of Deri and Knight (2015) (BASELINE). Models are evaluated using exactmatches (Matches) and average Levenshtein editdistance (Distance) w.r.t ground truth."}, {"heading": "5.1 Objective Evaluation Results", "text": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is split into 10 folds. Each fold model uses 8 folds for training, 1 for validation, and 1 for test. The average (10 fold crossvalidation style approach) performance metrics on the test fold are then evaluated. Table 1 shows the results of Experiment 1 for various model configurations. We get the BASELINE numbers from Deri and Knight (2015). Our best model obtains 48.75% Matches and 1.12 Distance, compared to 45.39% Matches and 1.59 Distance using BASELINE.\nFor Experiment 2, we seek to compare our best approaches from Experiment 1 to the BASELINE on a large, held-out dataset. Each model is trained on DWiki and tested on DBlind. BASELINE was similarly trained only on DWiki , making it a fair comparison. Table 2 shows the results3. Our best model gets Distance of 1.96 as compared to 2.32 from BASELINE.\nWe observe that the Backward architecture performs better than Forward architecture, confirming our hypothesis in \u00a72.2. In addition, ablation\n3For BASELINE (Deri and Knight, 2015), we use their trained model from http://leps.isi.edu/fst/ step-all.php\nresults confirm the importance of attention, and initializing the word embeddings. We believe this is because portmanteaus have high fidelity towards their root word characters and its critical that the model can observe all root sequence characters, which attention manages to do as shown in Fig. 2."}, {"heading": "5.1.1 Performance on Uncovered Examples", "text": "The set of candidates generated before scoring in the approximate SCORE decoding approach sometimes do not cover the ground truth. This holds true for 229 out of 1223 examples in DBlind. We compare the FORWARD approach along with a GREEDY decoding strategy to the BASELINE approach for these examples.\nBoth FORWARD+GREEDY and the BASELINE get 0 Matches on these examples. The Distance for these examples is 4.52 for BASELINE and 4.09 for FORWARD+GREEDY. Hence, we see that one of our approaches (FORWARD+GREEDY) outperforms BASELINE even for these examples."}, {"heading": "5.2 Significance Tests", "text": "Since our dataset is still small relatively small (1223 examples), it is essential to verify whether\nBACKWARD is indeed statistically significantly better than BASELINE in terms of Matches.\nIn order to do this, we use a paired bootstrap4 comparison (Koehn, 2004) between BACKWARD and BASELINE in terms of Matches. BACKWARD is found to be better (gets more Matches) than BASELINE in 99.9% (p = 0.999) of the subsets.\nSimilarly, BACKWARD has a lower Distance than BASELINE by a margin of 0.2 in 99.5% (p = 0.995) of the subsets."}, {"heading": "5.3 Subjective Evaluation and Analysis", "text": "On inspecting outputs, we observed that often output from our system seemed good in spite of high edit distance from ground truth. Such aspect of an output seeming good is not captured satisfactorily by measures like edit distance. To compare the errors made by our model to the baseline, we designed and conducted a human evaluation task on AMT.5 In the survey, we show human annotators outputs from our system and that of the baseline. We ask them to judge which alternative is better overall based on following criteria: 1. It is a good shorthand for two original words 2. It sounds better. We requested annotation on a scale of 1-4. To avoid ordering bias, we shuffled the order of two portmanteau between our system and that of baseline. We restrict annotators to be from Anglophone countries, have HIT Approval Rate > 80% and pay 0.40$ per HIT (5 Questions per HIT).\nAs seen in Table 4, output from our system was labelled better by humans as compared to the baseline 58.12% of the time. Table 3 shows outputs from different models for a few examples."}, {"heading": "6 Related Work", "text": "O\u0308zbal and Strapparava (2012) generate new words to describe a product given its category and properties. However, their method is limited to handcrafted rules as compared to our data driven ap-\n4We average across M = 1000 randomly chosen subsets of DBlind, each of size N = 611 (\u2248 1223/2)\n5We avoid ground truth comparison because annotators can be biased to ground truth due to its existing popularity.\nproach. Also, their focus is on brand names. Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case."}, {"heading": "7 Conclusion", "text": "We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model\u2019s predictions are superior to the baseline. We have also released our dataset and code6 to encourage further research on the phenomenon of portmanteaus. We also release an online demo 7 where our trained model can be queried for portmanteau suggestions. An obvious extension to our work is to try similar models on multiple languages."}, {"heading": "Acknowledgements", "text": "We thank Dongyeop Kang, David Mortensen, Qinlan Shen and anonymous reviewers for their valuable comments. This research was sup-\n6https://github.com/vgtomahawk/ Charmanteau-CamReady\n7http://tinyurl.com/y9x6mvy\nported in part by DARPA grant FA8750-12-20342 funded under the DEFT program."}], "references": [{"title": "Blends, a structural and systemic view", "author": ["John Algeo."], "venue": "American speech 52(1/2):47\u201364.", "citeRegEx": "Algeo.,? 1977", "shortCiteRegEx": "Algeo.", "year": 1977}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Selecting the best of the worst: the grammar of Hebrew blends", "author": ["Outi Bat-El."], "venue": "Phonology 13(03):283\u2013328.", "citeRegEx": "Bat.El.,? 1996", "shortCiteRegEx": "Bat.El.", "year": 1996}, {"title": "The role of blends in Modern Hebrew word-formation", "author": ["Ruth Berman."], "venue": "Studia linguistica et orientalia memoriae Haim Blanc dedicata. Wiesbaden: Harrassowitz pages 45\u201361.", "citeRegEx": "Berman.,? 1989", "shortCiteRegEx": "Berman.", "year": 1989}, {"title": "Bagging predictors", "author": ["Leo Breiman."], "venue": "Machine learning 24(2):123\u2013140.", "citeRegEx": "Breiman.,? 1996", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1603.06147 .", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Acronymic Patterns in Indonesian", "author": ["Soenjono Dardjowidjojo."], "venue": "Pacific Linguistics Series C 45:143\u2013 160.", "citeRegEx": "Dardjowidjojo.,? 1979", "shortCiteRegEx": "Dardjowidjojo.", "year": 1979}, {"title": "How to make a frenemy: Multitape FSTs for portmanteau generation", "author": ["Aliya Deri", "Kevin Knight."], "venue": "Proceedings of NAACL-HLT . pages 206\u2013 210.", "citeRegEx": "Deri and Knight.,? 2015", "shortCiteRegEx": "Deri and Knight.", "year": 2015}, {"title": "Morphological Inflection Generation using Character Sequence to Sequence Learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT . pages 634\u2013 643.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "The Weird Science of Naming New Products", "author": ["Neal Gabler."], "venue": "New York Times - http://tinyurl. com/lmlq7ex .", "citeRegEx": "Gabler.,? 2015", "shortCiteRegEx": "Gabler.", "year": 2015}, {"title": "Generating appealing brand names", "author": ["Gaurush Hiranandani", "Pranav Maneriker", "Harsh Jhamtani."], "venue": "arXiv preprint arXiv:1706.09335 .", "citeRegEx": "Hiranandani et al\\.,? 2017", "shortCiteRegEx": "Hiranandani et al\\.", "year": 2017}, {"title": "Decoding as Continuous Optimization in Neural Machine Translation", "author": ["Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn."], "venue": "arXiv:1701.02854 .", "citeRegEx": "Hoang et al\\.,? 2017", "shortCiteRegEx": "Hoang et al\\.", "year": 2017}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "EMNLP. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A computational approach to the automation of creative naming", "author": ["G\u00f6zde \u00d6zbal", "Carlo Strapparava."], "venue": "Proceedings of ACL. Association for Computational Linguistics, pages 703\u2013711.", "citeRegEx": "\u00d6zbal and Strapparava.,? 2012", "shortCiteRegEx": "\u00d6zbal and Strapparava.", "year": 2012}, {"title": "Say No to Portmanteaus", "author": ["Alexandra Petri."], "venue": "Washington Post - http://tinyurl.com/kvmep2t .", "citeRegEx": "Petri.,? 2012", "shortCiteRegEx": "Petri.", "year": 2012}, {"title": "The creation of portmanteaus in the extragrammatical morphology of Spanish", "author": ["Carlos-Eduardo Pi\u00f1eros."], "venue": "Probus 16(2):203\u2013240.", "citeRegEx": "Pi\u00f1eros.,? 2004", "shortCiteRegEx": "Pi\u00f1eros.", "year": 2004}, {"title": "Emergent faithfulness to morphological and semantic heads in lexical blends", "author": ["Katherine E Shaw", "Andrew M White", "Elliott Moreton", "Fabian Monrose."], "venue": "Proceedings of the Annual Meetings on Phonology. volume 1.", "citeRegEx": "Shaw et al\\.,? 2014", "shortCiteRegEx": "Shaw et al\\.", "year": 2014}, {"title": "Nehovah: A neologism creator nomen ipsum", "author": ["Michael R Smith", "Ryan S Hintze", "Dan Ventura."], "venue": "Proceedings of the International Conference on Computational Creativity. pages 173\u2013181.", "citeRegEx": "Smith et al\\.,? 2014", "shortCiteRegEx": "Smith et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The CMU pronunciation dictionary, release 0.6", "author": ["R Weide"], "venue": null, "citeRegEx": "Weide.,? \\Q1998\\E", "shortCiteRegEx": "Weide.", "year": 1998}, {"title": "The Neural Noisy Channel", "author": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomas Kocisky."], "venue": "arXiv:1611.02554 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Their short length makes them ideal for headlines and brandnames (Gabler, 2015).", "startOffset": 65, "endOffset": 79}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely.", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 340}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 448}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 476}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004).", "startOffset": 32, "endOffset": 491}, {"referenceID": 0, "context": "Portmanteaus (or lexical blends Algeo (1977)) are novel words formed from parts of multiple root words in order to refer to a new concept which can\u2019t otherwise be expressed concisely. Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit). Petri (2012). These are found not only in English but many other languages such as Bahasa Indonesia Dardjowidjojo (1979), Modern Hebrew BatEl (1996); Berman (1989) and Spanish Pi\u00f1eros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015).", "startOffset": 32, "endOffset": 518}, {"referenceID": 16, "context": "For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head.", "startOffset": 14, "endOffset": 33}, {"referenceID": 7, "context": "An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches.", "startOffset": 20, "endOffset": 43}, {"referenceID": 8, "context": "Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection.", "startOffset": 21, "endOffset": 43}, {"referenceID": 7, "context": "In experiments (\u00a75), our model performs better than the baseline Deri and Knight (2015) on both objective and subjective measures, demonstrating that such methods can be used effectively in a morphological task.", "startOffset": 65, "endOffset": 88}, {"referenceID": 1, "context": "The network architecture we use is an attentional S2S model (Bahdanau et al., 2014).", "startOffset": 60, "endOffset": 83}, {"referenceID": 11, "context": "This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT (Hoang et al. (2017), Yu et al.", "startOffset": 146, "endOffset": 166}, {"referenceID": 11, "context": "This way of factorizing the probability is also known as a noisy channel model, which has recently also been shown to be effective for neural MT (Hoang et al. (2017), Yu et al. (2016)).", "startOffset": 146, "endOffset": 184}, {"referenceID": 20, "context": "1 Specifically in our experiments, 134K words from the CMU dictionary (Weide, 1998).", "startOffset": 70, "endOffset": 83}, {"referenceID": 4, "context": "Given that our training data is small in size, we expect ensembling (Breiman, 1996) to help reduce model variance and improve performance.", "startOffset": 68, "endOffset": 83}, {"referenceID": 7, "context": "The existing dataset by Deri and Knight (2015) contains 401 portmanteau examples from Wikipedia.", "startOffset": 24, "endOffset": 47}, {"referenceID": 7, "context": "In this section, we show results comparing various configurations of our model to the baseline FST model of Deri and Knight (2015) (BASELINE).", "startOffset": 108, "endOffset": 131}, {"referenceID": 7, "context": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is split into 10 folds.", "startOffset": 45, "endOffset": 68}, {"referenceID": 7, "context": "In Experiment 1, we follow the same setup as Deri and Knight (2015). DWiki is split into 10 folds. Each fold model uses 8 folds for training, 1 for validation, and 1 for test. The average (10 fold crossvalidation style approach) performance metrics on the test fold are then evaluated. Table 1 shows the results of Experiment 1 for various model configurations. We get the BASELINE numbers from Deri and Knight (2015). Our best model obtains 48.", "startOffset": 45, "endOffset": 418}, {"referenceID": 7, "context": "For BASELINE (Deri and Knight, 2015), we use their trained model from http://leps.", "startOffset": 13, "endOffset": 36}, {"referenceID": 12, "context": "In order to do this, we use a paired bootstrap4 comparison (Koehn, 2004) between BACKWARD and BASELINE in terms of Matches.", "startOffset": 59, "endOffset": 72}, {"referenceID": 19, "context": "Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT.", "startOffset": 60, "endOffset": 84}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good.", "startOffset": 0, "endOffset": 225}, {"referenceID": 9, "context": "Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al.", "startOffset": 0, "endOffset": 534}, {"referenceID": 5, "context": "(2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "(2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case.", "startOffset": 11, "endOffset": 172}], "year": 2017, "abstractText": "Portmanteaus are a word formation phenomenon where two words are combined to form a new word. We propose character-level neural sequence-tosequence (S2S) methods for the task of portmanteau generation that are end-toend-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channelstyle model, which allows for the incorporation of unsupervised word lists, improving performance over a standard sourceto-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.", "creator": "LaTeX with hyperref package"}}}