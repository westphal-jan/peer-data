{"id": "1707.01555", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2017", "title": "A Deep Network with Visual Text Composition Behavior", "abstract": "While natural languages are compositional, it is still unclear how modern neural models achieve compositivity. We propose a deep network that not only achieves competitive accuracy in text classification, but also displays compositional behavior. That is, while hierarchical representations of a text, such as a sentence, are created, the lower layers of the network distribute their layer-specific attention to individual words. In contrast, the higher layers compose meaningful phrases and sentences, the length of which increases as the networks become deeper until the sentence is fully composed.", "histories": [["v1", "Wed, 5 Jul 2017 19:37:23 GMT  (677kb)", "http://arxiv.org/abs/1707.01555v1", "accepted to ACL2017"]], "COMMENTS": "accepted to ACL2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["hongyu guo"], "accepted": true, "id": "1707.01555"}, "pdf": {"name": "1707.01555.pdf", "metadata": {"source": "CRF", "title": "A Deep Network with Visual Text Composition Behavior", "authors": ["Hongyu Guo"], "emails": ["hongyu.guo@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 7.\n01 55\n5v 1\n[ cs\n.C L\n] 5\nJ ul\n2 01\n7\ntional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence."}, {"heading": "1 Introduction", "text": "Deep neural networks leverage task-specific architectures to develop hierarchical representations of the input, where higher level representations are derived from lower level features (Conneau et al., 2016). Such hierarchical representations have visually demonstrated compositionality in image processing, i.e., pixels combine to form shapes and then contours (Farabet et al., 2013; Zeiler and Fergus, 2014). Natural languages are also compositional, i.e., words combine to form phrases and then sentences. Yet unlike in vision, how deep neural models in NLP, which mainly operate on distributed word embeddings, achieve compositionality, is still unclear (Li et al., 2015, 2016).\nWe propose an Attention Gated Transformation (AGT) network, where each layer\u2019s feature generation is gated by a layer-specific attention mechanism (Bahdanau et al., 2014). Specifically, through distributing its attention to the original given text, each layer of the networks tends to in-\ncrementally retrieve new words and phrases from the original text. The new knowledge is then combined with the previous layer\u2019s features to create the current layer\u2019s representation, thus resulting in composing longer or new phrases and clauses while creating higher layers\u2019 representations of the text.\nExperiments on the Stanford Sentiment Treebank (Socher et al., 2013) dataset show that the AGT method not only achieves very competitive accuracy, but also exhibits compositional behavior via its layer-specific attention. We empirically show that, given a piece of text, e.g., a sentence, the lower layers of the networks select individual words, e.g, negative and conjunction words not and though, while the higher layers aim at composing meaningful phrases and clauses such as negation phrase not so much, where the phrase length increases as the networks get deeper until fully composing the whole sentence. Interestingly, after composing the sentence, the compositions of different sentence phrases compete to become the dominating features of the end task."}, {"heading": "2 Attention Gated Transformation Network", "text": "Our AGT network was inspired by the Highway Networks (Srivastava et al., 2015a,b), where each layer is equipped with a transform gate."}, {"heading": "2.1 Transform Gate for Information Flow", "text": "Consider a feedforward neural network with multiple layers. Each layer l typically applies a nonlinear transformation f (e.g., tanh, parameterized by W f l ), on its input, which is the output of the most recent previous layer (i.e., yl\u22121), to produce its output yl. Here, l = 0 indicates the first layer and y0 is equal to the given input text x, namely y0 = x:\nyl = f(yl\u22121,W f l ) (1)\nWhile in a highway network (the left column of Figure 1), an additional non-linear transform gate function Tl is added to the l th(l > 0) layer:\nyl = f(yl\u22121,W f l )Tl + yl\u22121(1\u2212 Tl) (2)\nwhere the function Tl expresses how much of the representation yl is produced by transforming the yl\u22121 (first term in Equation 2), and how much is just carrying from yl\u22121 (second term in Equation 2). Here Tl is typically defined as:\nTl = \u03c3(W t l yl\u22121 + b t l) (3)\nwhereW tl is the weight matrix and b t l the bias vector; \u03c3 is the non-linear activation function.\nWith transform gate T , the networks learn to decide if a feature transformation is needed at each layer. Suppose \u03c3 represents a sigmoid function. In such case, the output of T lies between zero and one. Consequently, when the transform gate is one, the networks pass through the transformation f over yl\u22121 and block the pass of input yl\u22121; when the gate is zero, the networks pass through the unmodified yl\u22121, while the transformation f over yl\u22121 is suppressed.\nThe left column of Figure 1 reflects the highway networks as proposed by (Srivastava et al., 2015b). Our AGT method adds the right two columns of Figure 1. That is, 1) the transform gate Tl now is not a function of yl\u22121, but a function of the selection vector s+l , which is determined by the attention distributed to the given input x by the lth layer (will be discussed next), and 2) the function f takes as input the concatenation of yl\u22121 and s +\nl\nto create feature representation yl. These changes result in an attention gated transformation when forming hierarchical representations of the text."}, {"heading": "2.2 Attention Gated Transformation", "text": "In AGT, the activation of the transform gate at each layer depends on a layer-specific attention mechanism. Formally, given a piece of text x, such as a sentence with N words, it can be represented as a matrix B \u2208 IRN\u00d7d. Each row of the matrix corresponds to one word, which is represented by a ddimensional vector as provided by a learned word embedding table. Consequently, the selection vector s+l , for the l th layer, is the softmax weighted sum over the N word vectors in B:\ns+l = N \u2211\nn=1\ndl,nB[n : n] (4)\nwith the weight (i.e., attention) dl,n computed as:\ndl,n = exp(ml,n)\n\u2211N n=1 exp(ml,n)\n(5)\nml,n = w m l tanh(W m l (B[n : n])) (6)\nhere, wml and W m l are the weight vector and weight matrix, respectively. By varying the attention weight dl,n, the s + l can focus on different rows of the matrix B, namely different words of the given text x, as illustrated by different color curves connecting to s+ in Figure 1. Intuitively, one can consider s+ as a learned word selection component: choosing different sets of words of the given text x by distributing its distinct attention.\nHaving built one s+ for each layer from the given text x, the activation of the transform gate for layer l (l > 0) (i.e., Equation 3) is calculated:\nTl = \u03c3(W t l s + l + b t l) (7)\nTo generate feature representation yl, the function f takes as input the concatenation of yl\u22121 and s + l . That is, Equation 2 becomes:\nyl =\n{\ns+l , l = 0 f([yl\u22121; s + l ],W f l )Tl + yl\u22121(1\u2212 Tl), l > 0\n(8)\nwhere [...;...] denotes concatenation. Thus, at each layer l, the gate Tl can regulate either passing through yl\u22121 to form yl, or retrieving novel\nknowledge from the input text x to augment yl\u22121 to create a better representation for yl.\nFinally, as depicted in Figure 1, the feature representation of the last layer of the AGT is fed into two fully connected layers followed by a softmax function to produce a distribution over the possible target classes. For training, we use multi-class cross entropy loss.\nNote that, Equation 8 indicates that the repre-\nsentation yl depends on both s + l and yl\u22121. In other words, although Equation 7 states that the gate activation at layer l is computed by s+l , the gate activation is also affected by yl\u22121, which embeds the information from the layers below l.\nIntuitively, the AGT networks are encouraged to consider new words/phrases from the input text at higher layers. Consider the fact that the s+ 0 at the bottom layer of the AGT only deploys a linear transformation of the bag-of-words features. If no new words are used at higher layers of the networks, it will be challenge for the AGT to sufficiently explore different combinations of word sets of the given text, which may be important for building an accurate classifier. In contrast, through tailoring its attention for new words at different layers, the AGT enables the words selected by a layer to be effectively combined with words/phrases selected by its previous layers to benefit the accuracy of the classification task (more discussions are presented in Section 3.2)."}, {"heading": "3 Experimental Studies", "text": ""}, {"heading": "3.1 Main Results", "text": "The Stanford Sentiment Treebank data contains 11,855 movie reviews (Socher et al., 2013). We use the same splits for training, dev, and test data as in (Kim, 2014) to predict the finegrained 5-class sentiment categories of the sentences. For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we trained the models using both phrases and sentences, but only evaluate sentences at test time. Also, we initialized all of the word embeddings (Cherry and Guo, 2015; Chen and Guo, 2015) using the 300 dimensional pre-trained vectors from GloVe (Pennington et al., 2014). We learned 15 layers with 200 dimensions each, which requires us to project the 300 dimensional word vectors; we implemented this using a linear transformation, whose weight matrix and bias term are shared across all words, followed by\na tanh activation. For optimization, we used Adadelta (Zeiler, 2012), with learning rate of 0.0005, mini-batch of 50, transform gate bias of 1, and dropout (Srivastava et al., 2014) rate of 0.2. All these hyperparameters were determined through experiments on the validation-set.\nTable 1 presents the test-set accuracies obtained by different strategies. Results in Table 1 indicate that the AGT method achieved very competitive accuracy (with 50.5%), when compared to the state-of-the-art results obtained by the tree-LSTM (51.0%) (Tai et al., 2015; Zhu et al., 2015) and high-order CNN approaches (51.2%) (Lei et al., 2015).\nTop subfigure in Figure 2 depicts the distribu-\ntions of the attention weights created by different layers on all test data, where the attention weights of all words in a sentence, i.e., dl,n in Equation 4, are normalized to the range between 0 and 1 within the sentence. The figure indicates that AGT generated very spiky attention distribution. That is, most of the attention weights are either 1 or 0. Based on these narrow, peaked bell curves formed by the normal distributions for the attention weights of 1 and 0, we here consider a word has been selected by the networks if its attention weight is larger than 0.95, i.e., receiving more than 95% of the full attention, and a phrase has been composed and selected if a set of consecutive words all have been selected.\nIn the bottom subfigure of Figure 2 we present the distribution of the phrase lengths on the test set. This figure indicates that the middle layers of the networks e.g., 8th and 9th, have longer phrases (green and blue curves) than others, while the layers at the two ends contain shorter phrases (red and pink curves).\nIn Figure 3, we also presented the transform gate activities on all test sentences (top) and that of the first example sentence in Figure 4 (bottom). These curves suggest that the transform gates at the middle layers (green and blue curves) tended to be close to zero, indicating the pass-through of lower layers\u2019 representations. On the contrary, the gates at the two ends (red and pink curves) tended to be away from zero with large tails, implying the retrieval of new knowledge from the input text. These are consistent with the results below.\nFigure 4 presents three sentences with various lengths from the test set, with the attention weights numbered and then highlighted in heat map. Figure 4 suggests that the lower layers of the networks selected individual words, while the higher layers aimed at phrases. For example, the first and second layers seem to select individual words carrying strong sentiment (e.g., predictable, bad, never and delicate), and conjunction and negation words (e.g., though and not). Also, meaningful phrases were composed and selected by later layers, such as not so much, not only... but also, bad taste, bad luck, emotional development, and big screen. In addition, in the middle layer, i.e., the 8th layer, the whole sentences were composed by filtering out uninformative words, resulting in concise versions, as follows (selected words and phrases are highlighted in color blocks).\n1) though plot predictable movie never feels formulaic attention nuances emotional development delicate characters 2) bad company leaves bad taste not only bad luck but also staleness script 3) not so much movie picture big screen\nInterestingly, if relaxing the word selection criteria, e.g., including words receiving more than the median, rather than 95%, of the full attention, the sentences recruited more conjunction and modification words, e.g., because, for, a, its and on, thus becoming more readable and fluent:\n1) though plot is predictable movie never feels formulaic because attention is on nuances emotional development delicate characters 2) bad company leaves a bad taste not only because its bad luck timing but also staleness its script 3) not so much a movie a picture book for big screen\nNow, consider the AGT\u2019s compositional behavior for a specific sentence, e.g., the last sentence in Figure 4. The first layer solely selected the word not (with attention weight of 1 and all other words with weights close to 0), but the 2nd to 4th layers gradually pulled out new words book, screen and movie from the given text. Incrementally, the 5th and 6th layers further selected words to form phrases not so much, picture book, and big screen. Finally, the 7th and 8th layers added some\nconjunction and quantification words a and for to make the sentence more fluent. This recursive composing process resulted in the sentence \u201cnot so much a movie a picture book for big screen\u201d.\nInterestingly, Figures 4 and 2 also imply that, after composing the sentences by the middle layer, the AGT networks shifted to re-focus on shorter phrases and informative words. Our analysis on the transform gate activities suggests that, during this re-focusing stage the compositions of sentence phrases competed to each others, as well as to the whole sentence composition, for the dominating task-specific features to represent the text."}, {"heading": "3.2 Further Observations", "text": "As discussed at the end of Section 2.2, intuitively, including new words at different layers allows the networks to more effectively explore different combinations of word sets of the given text than that of using all words only at the bottom layer of the networks. Empirically, we observed that, if with only s+ 0 in the AGT network, namely removing s+i for i > 0, the test-set accuracy dropped from 50.5% to 48.5%. In other words, transforming a linear combination of the bag-of-words features was insufficient for obtaining sufficient accuracy for the classification task. For instance, if being augmented with two more selection vectors s+i , namely removing s + i for i > 2, the AGT was\nable to improve its accuracy to 49.0%.\nAlso, we observed that the AGT networks tended to select informative words at the lower layers. This may be caused by the recursive form of Equation 8, which suggests that the words retrieved by s+ 0 have more chance to combine with and influence the selection of other feature words. In our study, we found that, for example, the top 3 most frequent words selected by the first layer of the AGT networks were all negation words: n\u2019t, never, and not. These are important words for sentiment classification (Zhu et al., 2014).\nIn addition, like the transform gate in the Highway networks (Srivastava et al., 2015a) and the forget gate in the LSTM (Gers et al., 2000), the attention-based transform gate in the AGT networks is sensitive to its bias initialization. We found that initializing the bias to one encouraged the compositional behavior of the AGT networks."}, {"heading": "4 Conclusion and Future Work", "text": "We have presented a novel deep network. It not only achieves very competitive accuracy for text classification, but also exhibits interesting text compositional behavior, which may shed light on understanding how neural models work in NLP tasks. In the future, we aim to apply the AGT networks to incrementally generating natural text (Guo, 2015; Hu et al., 2017)."}], "references": [{"title": "Generating text with deep rein", "author": ["Hongyu Guo"], "venue": null, "citeRegEx": "2471", "shortCiteRegEx": "2471", "year": 2015}, {"title": "Convolutional neural networks", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "J. Mach. Learn. Res. 15(1).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "CoRR abs/1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015a", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems. NIPS\u201915, pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015b", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus."], "venue": "Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I. pages 818\u2013833.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Xiaodan Zhu", "Hongyu Guo", "Saif Mohammad", "Svetlana Kiritchenko."], "venue": "ACL (1). pages 304\u2013313.", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}, {"title": "Long short-term memory over recursive structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "ICML. pages 1604\u20131612.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": ", pixels combine to form shapes and then contours (Farabet et al., 2013; Zeiler and Fergus, 2014).", "startOffset": 50, "endOffset": 97}, {"referenceID": 4, "context": "The left column of Figure 1 reflects the highway networks as proposed by (Srivastava et al., 2015b).", "startOffset": 73, "endOffset": 99}, {"referenceID": 1, "context": "use the same splits for training, dev, and test data as in (Kim, 2014) to predict the finegrained 5-class sentiment categories of the sentences.", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "For comparison purposes, following (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015), we trained the models using both phrases and sentences, but only evaluate sentences at test time.", "startOffset": 35, "endOffset": 91}, {"referenceID": 6, "context": "For optimization, we used Adadelta (Zeiler, 2012), with learning rate of 0.", "startOffset": 35, "endOffset": 49}, {"referenceID": 2, "context": "0005, mini-batch of 50, transform gate bias of 1, and dropout (Srivastava et al., 2014) rate of 0.", "startOffset": 62, "endOffset": 87}, {"referenceID": 5, "context": "0%) (Tai et al., 2015; Zhu et al., 2015) and high-order CNN approaches (51.", "startOffset": 4, "endOffset": 40}, {"referenceID": 9, "context": "0%) (Tai et al., 2015; Zhu et al., 2015) and high-order CNN approaches (51.", "startOffset": 4, "endOffset": 40}, {"referenceID": 8, "context": "These are important words for sentiment classification (Zhu et al., 2014).", "startOffset": 55, "endOffset": 73}, {"referenceID": 3, "context": "In addition, like the transform gate in the Highway networks (Srivastava et al., 2015a) and the forget gate in the LSTM (Gers et al.", "startOffset": 61, "endOffset": 87}], "year": 2017, "abstractText": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.", "creator": "LaTeX with hyperref package"}}}