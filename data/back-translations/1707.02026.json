{"id": "1707.02026", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2017", "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "abstract": "Grammar Error Correction Systems (GEC) aim to correct both global errors in the order and use of words and local errors in spelling and inflection. Further, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating information at the word and character level, and that the model significantly exceeds previous neural models for GEC as measured by the standard benchmark data set CoNLL-14. Further analysis also shows that the superiority of the proposed model is largely due to the use of the nested attention mechanism, which has proven to be particularly effective in correcting local errors that require small corrections in orthography.", "histories": [["v1", "Fri, 7 Jul 2017 03:10:32 GMT  (702kb,D)", "https://arxiv.org/abs/1707.02026v1", null], ["v2", "Mon, 10 Jul 2017 02:56:49 GMT  (600kb,D)", "http://arxiv.org/abs/1707.02026v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jianshu ji", "qinlong wang", "kristina toutanova", "yongen gong", "steven truong", "jianfeng gao"], "accepted": true, "id": "1707.02026"}, "pdf": {"name": "1707.02026.pdf", "metadata": {"source": "CRF", "title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "authors": ["Jianshu Ji", "Qinlong Wang", "Kristina Toutanova", "Yongen Gong", "Steven Truong", "Jianfeng Gao"], "emails": ["jianshuj@microsoft.com", "qinlwang@microsoft.com", "yongeg@microsoft.com", "stevetr@microsoft.com", "jfgao@microsoft.com", "kristout@google.com"], "sections": [{"heading": "1 Introduction", "text": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016).\n\u2217This work was conducted while the third author worked at Microsoft Research.\nRecently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015). Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC.\nThe core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016). Thus, the S2S model is expected to perform better on GEC than phrase-based models. However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below.\nFirst, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations.\nSecond, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types. For example, while correcting spelling and local grammar errors requires only word-level or sub-word level information, e.g., violets\u2192 violates (spelling) or violate\u2192 violates (verb form), correcting errors in word order or usage requires global semantic relationships among phrases and words.\nStandard approaches in neural machine translation, also applied to grammatical error correction by Yuan and Briscoe (2016), address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and re-\nar X\niv :1\n70 7.\n02 02\n6v 2\n[ cs\n.C L\n] 1\n0 Ju\nl 2 01\n7\nsorting to standard word translation dictionaries to provide translations for the words that are out of the vocabulary (OOV). However, this approach often fails to take into account the OOVs in context for making correction decisions, and does not generalize well to correcting words that are unseen in the parallel training data. An alternative approach, proposed by Xie et al. (2016), applies a character-level sequence to sequence neural model. Although the model eliminates the OOV issue, it cannot effectively leverage word-level information for GEC, even if it is used together with a separate word-based language model.\nOur solution to the challenges mentioned above is a novel, hybrid neural model with nested attention layers that infuse both word-level and character-level information. The architecture of the model is illustrated in Figure 1. The word-level information is used for correcting global grammar and fluency errors while the character-level information is used for correcting local errors in spelling or inflected forms. Contextual information is crucial for GEC. Using the proposed model, by combining embedding vectors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation. In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography.\nOur model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level. We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer. This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs.\nWe validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014). Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time. When integrated with a large word-based n-gram language model, our GEC system achieves an F0.5 of 45.15 on CoNLL-14, substantially exceeding the previ-\nously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016)."}, {"heading": "2 Related Work", "text": "A variety of classifier-based and MT-based techniques have been applied to grammatical error correction. The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches. Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016). The latter work has reported the highest performance to date on the task of 49.5 in F0.5 score on the CoNLL-14 test set. This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.\nTwo prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic. Xie et al. (2016) built a character-level sequence\nto sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions.\nThe primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency errors and local errors in spelling and closely related morphological variants, while obtaining open vocabulary coverage. This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017). None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting."}, {"heading": "3 Nested Attention Hybrid Model", "text": "Our model is hybrid, and uses both word-level and character-level representations. It consists of a word-based sequence-to-sequence model as a backbone, and additional character-level encoder, decoder, and attention components, which focus on words that are outside the word-level model\u2019s vocabulary."}, {"heading": "3.1 Word-based sequence-to-sequence model as backbone", "text": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016). For completeness, we give a sketch here. It uses recurrent neural networks to encode the input sentence and to decode the output sentence.\nGiven a sequence of embedding vectors, corresponding to a sequence of input words x:\nx = (x1, . . . , xT ), (1)\nthe encoder creates a corresponding context-\nspecific sequence of hidden state vectors e:\ne = (h1, . . . , hT )\nThe hidden state ht at time t is computed as: ft = GRUencf (ft\u22121, xt) , bt = GRUencb(bt+1, xt), ht = [ft; bt], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscripts to denote the parameters of the forward and backward word-level encoder units.)\nThe decoder network is also an RNN using GRU units, and defines a sequence of hidden states d\u03041, . . . , d\u0304S used to define the probability of an output sequence y1, . . . , yS as follows:\nThe context vector cs at time step s is computed as follows:\ncs = T\u2211\nj=1\n\u03b1sjhj (2)\nwhere: \u03b1sk = usk\u2211T j=1 usj\n(3)\nusk = \u03c61(ds) T\u03c62(hk) (4)\nHere \u03c61 and \u03c62 denote feedforward linear transformations followed by a tanh nonlinearity. The next hidden state d\u0304s is then defined as:\nds = GRUdec( \u00afds\u22121, ys\u22121),\nd\u0304s = ReLU(W [cs; ds])\nwhere ys\u22121 is the embedding of the output token at time s-1. ReLU indicates rectified linear units (Hahnloser et al., 2000).\nThe probability of each target word ys is computed as: p(ys|y<s,x) = softmax(g(d\u0304s)), where g is a function that maps the decoder state into a vector of size the dimensionality of the target vocabulary.\nThe model is trained by minimizing the crossentropy loss, which for a given (x,y) pair is:\nLoss(x,y) = \u2212 S\u2211\ns=1\nlog p(ys|y<s,x) (5)\nFor parallel training data C, the loss is:\nLoss = \u2212 \u2211\n(x,y)\u2208C S\u2211 s=1 log p(ys|y<s,x)"}, {"heading": "3.2 Hybrid encoder and decoder with two nested levels of attention", "text": "The word-level backbone models a limited vocabulary of source and target words, and represents out-of-vocabulary tokens with special UNK symbols. In the standard word-level NMT approach, valuable information is lost for source OOV words and target OOV words are predicted using postprocessing heuristics.\nHybrid encoder\nOur hybrid architecture overcomes the loss of source information in the word-level backbone by building up compositional representations of the source OOV words using a character-level recurrent neural network with GRU units. These representations are used in place of the special source UNK embeddings in the backbone, and contribute to the contextual encoding of all source tokens.\nFor example, a three word input sentence where the last term is out-of-vocabulary will be represented as the following vector of embeddings in the word-level model: x = (x1, x2, x3), where x3 would be the embedding for the UNK symbol.\nThe hybrid encoder builds up a word embedding for the third word based on its character sequence: xc1, . . . , x c M . The encoder computes a sequence of hidden states ec for this character sequence, by a forward character-level GRU network:\nec = (h c 1, . . . , h c M ), (6)\nThe last state hcM is used as an embedding of the unknown word. The sequence of embeddings for our example three-word sequence becomes: x = (x1, x2, h c M ). We use the same dimensionality for word embedding vectors xi and composed character sequence vectors hcM to ensure the two ways to define embeddings are compatible. Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016).\nNested attention hybrid decoder\nIn traditional word-based sequence-to-sequence models special target UNK tokens are used to represent outputs that are outside the target vocabulary. A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words. The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder\nto generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task.\nHowever, unlike machine translation, models for grammar correction conduct \u201ctranslation\u201d in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word. For example, rare but correct words such as entity names need to be copied as is, and local spelling errors or errors in inflection need to be corrected. The architecture of Luong and Manning (2016) does not have direct access to a source character sequence, but only uses a single fixed-dimensionality embedding of source unknown words aggregated with additional contextual information from the source.\nTo address the needs of the grammatical error correction task, we propose a novel hybrid decoder with two nested levels of attention: word level and character-level. The character-level attention serves to provide the decoder with direct access to the relevant source character sequence.\nMore specifically, the probability of each target word is defined as follows: For words in the target vocabulary, the probability is defined by the wordlevel backbone. For words outside the vocabulary, the probability of each token is the probability of UNK according to the backbone, multiplied by the probability of the word\u2019s character sequence.\nThe probability of the target character sequence corresponding to an UNK token at position s in the target is defined using a character-level decoder. As in Luong and Manning (2016), the \u201cseparate path\u201d architecture is used to capture the relevant context and define the initial state for the character-level decoder:\nd\u0302s = ReLU(W\u0302 [cs; ds])\nwhere W\u0302 are parameters different from W , and d\u0302s is not used by the word-level model in predicting the subsequent tokens, but is only used to initialize the character-level decoder.\nTo be able to attend to the relevant source character sequence when generating the target character sequence, we use the concept of hard attention (Xu et al., 2015), but use an arg-max approximation for inference instead of sampling. A similar approach to represent discrete hidden structure in a variety of architectures is used in Kong et al. (2017).\nThe source index zs corresponding to the target\nposition s is defined according to the word-level attention model:\nzs = arg max k\u22080...T\u22121 \u03b1sk\nwhere \u03b1sk are the intermediate outputs of the word-level attention model we described in Eq.(3).\nThe character-level decoder generates a character sequence yc = (yc1, . . . , y c N ), conditioned on the initial vector d\u0302s and the source index zs. The characters are generated using a hidden state vector dcn at each time step, via a softmax(gc(dcn)), where gc maps the state to the target character vocabulary space.\nIf the source word xzs is in the source vocabulary, the model is analogous to the one of Luong and Manning (2016) and does not use characterlevel attention: the source context is available only in aggregated form to initialize the state of the decoder. The state dcn for step n in the characterlevel decoder is defined as follows, where GRUcdec are parameters for the gated recurrent cell of this decoder:\ndcn = { GRUcdec(d\u0302s, ycn\u22121) n = 0 GRUcdec(dcn\u22121, ycn\u22121) n > 0\nIn contrast, if the corresponding token in the source xzs is also an out-of-vocabulary word, we define a second nested level of character attention and use it in the character-level decoder. The character-level attention focuses on individual characters from the source word xzs . If ec are the source character hidden vectors computed as in Eq.(6), the recurrence equations for the characterlevel decoder with nested attention are:\n\u00afdcn = ReLU(Wc[ccn; dcn])\ndcn = { GRUcdecNested(d\u0302s, ycn\u22121) n = 0 GRUcdecNested( \u00afdcn\u22121, ycn\u22121) n > 0\nwhere ccn is the context vector obtained using character-level attention on the sequence ec and the last state of the character-level decoder dcn, computed following equations 2, 3 and 4, but using a different set of parameters.\nThese equations show that the character-level decoder with nested attention can use both the wordlevel state d\u0302s, and the character-level context ccn\nand hidden state dcn to perform global and local editing operations.\nSince we introduced two architectures for the character-level decoder depending on whether the source word xzs is OOV, the combined loss function is defined as follows for end-to-end training:\nLosstotal = Lossw + \u03b1Lossc1 + \u03b2Lossc2\nHere Lossw is the standard word-level loss in Eq.(5); character level losses Lossc1 and Lossc2 are losses for target OOV words corresponding to source known and unknown tokens, respectively. \u03b1 and \u03b2 are hyper-parameters to balance the loss terms.\nAs seen, our proposed nested attention hybrid model uses character-level attention only when both a predicted target word and its corresponding source input word are OOV. While the model can be naturally generalized to integrate characterlevel attention for known words, the original hybrid model proposed by Luong and Manning (2016) does not use any character-level information for known words. Thus for a controlled evaluation of the impact of the addition of character-level attention only, in this paper we limit character-level attention to OOV words, which already use characters as a basis for building their embedding vectors. A thorough investigation of the impact of characterlevel information in the encoder, attention, and decoder for known words as well is an interesting topic for future research.\nDecoding for word-level and hybrid models Beam-search is used to decode hypotheses according to the word-level backbone model. For the hybrid model architecture, word-level beam search is conducted first; for each target UNK token, character-level beam-search is used to generate a corresponding target word."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset and Evaluation", "text": "We use standard publicly available datasets for training and evaluation. One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks. From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training. A second data source\nTraining Validation Development Test #Sent pairs 2,608,679 4,771 1,381 1,312\nis the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences. Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012). As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text. Table 2 shows the number of sentence pairs from each source used for training.\nWe evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014). We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013). We use the development and validation sets for model selection. The sizes of all datasets in number of sentences are shown in Table 1. We report performance in F0.5-measure, as calculated by the m2scorer\u2014 the official implementation of the scoring metric in the shared task. 1 Given system outputs and gold-standard edits, m2scorer computes the F0.5 measure of a set of system edits against a set of gold-standard edits."}, {"heading": "4.2 Baseline", "text": "We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement. Like Yuan and Briscoe (2016), we use a traditional wordalignment model (GIZA++) to derive a wordcorrection lexicon from the parallel training set. However, in decoding, we don\u2019t use GIZA++ to find the corresponding source word for each tar-\n1http://www.comp.nus.edu.sg/\u02dcnlp/sw/ m2scorer.tar.gz\nget OOV, but follow Cho et al. (2015), Section 3.3 to use the NMT system\u2019s attention weights instead. The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the source word itself if there are no available corrections."}, {"heading": "4.3 Training Details and Results", "text": "The embedding size for all word and characterlevel encoders and decoders is set to 1000, and the hidden unit size is also 1000. To reproduce the model of Yuan and Briscoe (2016), we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies. In preliminary experiments for the hybrid models, we found that selecting the same vocabulary of 30K words for the source and target based on combined frequency was better (.003 in F0.5) and use that method for vocabulary selection instead. However, there was no gain observed by using such a vocabulary selection method in the baseline. Although the source and target vocabularies in the hybrid models are the same, like in the word-level model, the embedding parameters for source and target words are not shared.\nThe hyper-parameters for the losses in our models are selected based on the development set and set as follows: \u03b1 = \u03b2 = 0.5. All models are trained with mini-batch size of 128 (batches are shuffled), initial learning rate of 0.0003 and a 0.95 decay ratio if the cost increases in two consecutive 100 iterations. The gradient is rescaled whenever its norm exceeds 10, and dropout is used with a probability of 0.15. Parameters are uniformly initialized in [\u2212 \u221a\n(3)\u221a 1000 ,\n\u221a (3)\u221a\n1000 ].\nWe perform inference on the validation set every 5000 iterations to log word-level cost and characterlevel costs; we save parameter values for the model every 10000 iterations as well as the end of each epoch. The stopping point for training is selected based on development set F0.5 among the top 20 parameter sets with best validation set value of the loss function.\nTraining of the nested attention hybrid model takes approximately five days on a Tesla k40m GPU. The basic hybrid model trains in around four days and the word-level backbone trains in approximately three days.\nTable 3 shows the performance of the baseline and our nested attention hybrid model on the devel-\nopment and test sets. In addition to the word-level baseline, we include the performance of a hybrid model with a single level of attention, which follows the work of Luong and Manning (2016) for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction. Based on hyper-parameter selection, the character-level component weight of the loss is \u03b1 = 1 for the basic hybrid model.\nAs shown in Table 3, our implementation of the word NMT+UNK replacement baseline approaches the performance of the one reported in Yuan and Briscoe (2016) (38.77 versus 39.9). We attribute the difference to differences in the training set and the word-alignment methods used. Our reimplementation serves to provide a controlled experimental evaluation of the impact of hybrid models and nested attention on the GEC task. As seen, our nested attention hybrid model substantially improves upon the baseline, achieving a gain of close to 3 points on the test set. The hybrid word/character model with a single level of attention brings a large improvement as well, showing the importance of character-level information for this task. We delve deeper into the impact of nested attention for the hybrid model in Section 5."}, {"heading": "4.4 Integrating a Web-scale Language Model", "text": "The value of large language models for grammatical error correction is well known, and such models have been used in classifier and MT-based systems. To establish the potential of such models in word-based neural sequence-to-sequence systems, we integrate a web-scale count-based language model. In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).\nCandidates generated by neural models are reranked using the following linear interpolation of log probabilities: sy|x = logPNN (y|x) + \u03bb logPLM (y). Here \u03bb is a hyper-parameter that balances the weights of the neural network model\nand the language model. We tuned \u03bb separately for each neural model variant, by exploring values in the range [0.0, 2.0] with step size 0.1, and selecting according to development set F0.5. The selected values of \u03bb are: 1.6 for word NMT + UNK replacement and 1.0 for the nested attention model.\nTable 4 shows the impact of the LM when combined with the neural models implemented in this work. The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model. Our best results exceed the ones reported in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training."}, {"heading": "5 Analysis", "text": "We analyze the impact of sub-word level information and the two nested levels of attention in more detail by looking at the performance of the models on different segments of the data. In particular, we analyze the performance of the models on sentences containing OOV source words versus ones without OOV words, and corrections to orthographically similar versus dissimilar word forms."}, {"heading": "5.1 Performance by Segment: OOV versus Non-OOV", "text": "We present a comparative performance analysis of models on the CoNLL-13 development set. First, we divide the set into two segments: OOV and NonOOV, based on whether there is at least one OOV word in the given source input. Table 5 shows that both hybrid architectures substantially outperform the word-level model in both segments of the data. The additional nested character-level attention of our hybrid model brings a sizable improvement over the basic hybrid model in the OOV segment and a small degradation in the non-OOV segment. We should note that in future work characterlevel attention can be added for non-OOV source\nwords in the nested attention model, which could improve performance on this segment as well.\nTable 6 shows an example where the nested attention hybrid model successfully corrects a misspelling resulting in an OOV word on the source, whereas the baseline word-level system simply copies the source word without fixing the error (since this particular error is not observed in the parallel training set)."}, {"heading": "5.2 Impact of Nested Attention on Different Error Types", "text": "To analyze more precisely the impact of the additional character-level attention introduced by our design, we continue to investigate the OOV segment in more detail.\nThe concept of edit, which is also used by the official M2 score metric, is defined as a minimal pair of corresponding sub-strings in a source sentence and a correction. For example, in the sentence fragment pair: \u201cEven though there is a risk of causing harms to someone, people still are prefers to keep their pets without a leash.\u201d \u2192 \u201cEven though there is a risk of causing harm to someone, people still prefer to keep their pets without a leash.\u201d, the minimal edits are \u201charms\u2192 harm\u201d and \u201care prefers\u2192 prefer\u201d. The F0.5 score is computed using weighted precision and recall of the set of a system\u2019s edits against one or more sets of reference edits.\nFor our in-depth analysis, we classify edits in the OOV segment into two types: small changes and large changes, based on whether the source and\ntarget phrase of the edit are orthographically similar or not. More specifically, we say that the target and source phrases are orthographically similar, iff: the character edit distance is at most 2 and the source or target is at most 8 characters long, or edit ratio < 0.25, where edit ratio = character edit distancemin(len(src),len(tar))+0.1 , len(\u2217) denotes number of characters in \u2217, and src and tgt denote the pairs in the edit. There are 307 gold edits in the \u201csmall changes\u201d portion of the CoNLL-13 OOV segment, and 481 gold edits in the \u201clarge changes\u201d portion.\nOur hypothesis is that the additional characterlevel attention layer is particularly useful to model edits among orthographically similar words. Table 7 contrasts the impact of character-level attention on the two portions of the data. We can see that the gains in the \u201csmall changes\u201d portion are indeed quite large, indicating that the fine-grained character-level attention empowers the model to more accurately correct confusions among phrases with high character-level similarity. The impact in the \u201clarge changes\u201d portion is slightly positive in precision and slightly negative in recall. Thus most of the benefit of the additional character-level attention stems from improvements in the \u201csmall changes\u201d portion.\nTable 8 shows an example input which illustrates the precision gain of the nested attention hybrid model. The input sentence has a source OOV word which is correct. The hybrid model introduces an error in this word, because it uses only a single source context vector, aggregating the characterlevel embedding of the source OOV word together with other source words. The additional characterlevel attention layer in the nested hybrid model enables the correct copying of this long source OOV word, without employing the heuristic mechanism of the word-level NMT system."}, {"heading": "6 Conclusions", "text": "We have introduced a novel hybrid neural model with two nested levels of attention: word-level and character-level. The model addresses the unique challenges of the grammatical error correction task and achieves the best reported results on the CoNLL-14 benchmark among fully neural systems. Our nested attention hybrid model deeply combines the strengths of word and character level information in all components of an end-to-end neural model: the encoder, the attention layers, and the decoder. This enables it to correct both global wordlevel and local character-level errors in a unified way. The new architecture contributes substantial improvement in correction of confusions among rare or orthographically similar words compared to word-level sequence-to-sequence and non-nested hybrid models."}, {"heading": "Acknowledgements", "text": "We would like to thank the ACL reviewers for their insightful suggestions, Victoria Zayats for her help with reproducing the baseline word-level NMT system and Yu Shi, Daxin Jiang and Michael Zeng for the helpful discussions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Correcting ESL errors using phrasal SMT techniques", "author": ["Chris Brockett", "William B Dolan", "Michael Gamon."], "venue": "Proceedings of the 21st International", "citeRegEx": "Brockett et al\\.,? 2006", "shortCiteRegEx": "Brockett et al\\.", "year": 2006}, {"title": "N-gram counts and language models from the Common Crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas Van Ooyen."], "venue": "LREC.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Neural network translation models for grammatical error correction", "author": ["Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Chollampatt et al\\.,? 2016", "shortCiteRegEx": "Chollampatt et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of learner English: The NUS corpus of learner English", "author": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.", "citeRegEx": "Dahlmeier et al\\.,? 2013", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2013}, {"title": "A large scale rankerbased system for search query spelling correction", "author": ["Jianfeng Gao", "Xiaolong(Shiao-Long) Li", "Daniel Micol", "Chris Quirk", "Xu Sun"], "venue": "In The 23rd International Conference on Computational Linguistics", "citeRegEx": "Gao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2010}, {"title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit", "author": ["Richard HR Hahnloser", "Rahul Sarpeshkar", "Misha A Mahowald", "Rodney J Douglas", "H Sebastian Seung."], "venue": "Nature 405(6789):947\u2013951.", "citeRegEx": "Hahnloser et al\\.,? 2000", "shortCiteRegEx": "Hahnloser et al\\.", "year": 2000}, {"title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "EMNLP.", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Dragnn: A transitionbased framework for dynamically connected neural networks", "author": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "venue": null, "citeRegEx": "Kong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "TACL 5.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "The CoNLL-2014 shared task on grammatical error correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."], "venue": "CoNLL Shared Task. pages 1\u201314.", "citeRegEx": "Ng et al\\.,? 2014", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT", "author": ["Diane Nicholls."], "venue": "Proceedings of the Corpus Linguistics 2003 conference. volume 16, pages 572\u2013581.", "citeRegEx": "Nicholls.,? 2003", "shortCiteRegEx": "Nicholls.", "year": 2003}, {"title": "Grammatical error correction: Machine translation and classifiers", "author": ["Alla Rozovskaya", "Dan Roth."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pages 2205\u20132215.", "citeRegEx": "Rozovskaya and Roth.,? 2016", "shortCiteRegEx": "Rozovskaya and Roth.", "year": 2016}, {"title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "author": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."], "venue": "Transactions of the Association for Computational Linguistics 4:169\u2013182.", "citeRegEx": "Sakaguchi et al\\.,? 2016", "shortCiteRegEx": "Sakaguchi et al\\.", "year": 2016}, {"title": "Sentence-level grammatical error identification as sequence-to-sequence correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M Rush", "Stuart M Shieber."], "venue": "arXiv preprint arXiv:1604.04677 .", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Tense and aspect error correction for ESL learners using global context", "author": ["Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2.", "citeRegEx": "Tajiri et al\\.,? 2012", "shortCiteRegEx": "Tajiri et al\\.", "year": 2012}, {"title": "Neural language correction with character-based attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "CoRR abs/1603.09727. http://arxiv.org/abs/1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "ICML. volume 14, pages 77\u201381.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Grammatical error correction using neural machine translation", "author": ["Zheng Yuan", "Ted Briscoe."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Yuan and Briscoe.,? 2016", "shortCiteRegEx": "Yuan and Briscoe.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 7, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 9, "context": "One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 225, "endOffset": 306}, {"referenceID": 15, "context": "The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016).", "startOffset": 153, "endOffset": 180}, {"referenceID": 19, "context": "Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 142, "endOffset": 189}, {"referenceID": 0, "context": "Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 142, "endOffset": 189}, {"referenceID": 23, "context": "Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 66, "endOffset": 108}, {"referenceID": 21, "context": "Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016).", "startOffset": 66, "endOffset": 108}, {"referenceID": 16, "context": "achieve native speaker fluency (Sakaguchi et al., 2016).", "startOffset": 31, "endOffset": 55}, {"referenceID": 23, "context": "Standard approaches in neural machine translation, also applied to grammatical error correction by Yuan and Briscoe (2016), address the large vocabulary problem by restricting the vocabulary to a limited number of high-frequency words and rear X iv :1 70 7.", "startOffset": 99, "endOffset": 123}, {"referenceID": 21, "context": "An alternative approach, proposed by Xie et al. (2016), applies a character-level sequence to sequence neural model.", "startOffset": 37, "endOffset": 55}, {"referenceID": 22, "context": "Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 21, "context": "Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016)", "startOffset": 125, "endOffset": 143}, {"referenceID": 12, "context": "We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer.", "startOffset": 44, "endOffset": 69}, {"referenceID": 13, "context": "We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014).", "startOffset": 77, "endOffset": 94}, {"referenceID": 12, "context": "Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time.", "startOffset": 115, "endOffset": 140}, {"referenceID": 21, "context": "56 achieved by using a neural model and an external language model (Xie et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 15, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 111, "endOffset": 138}, {"referenceID": 9, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 184, "endOffset": 224}, {"referenceID": 12, "context": "The CoNLL-14 shared task overview paper of Ng et al. (2014) provides a comparative evaluation of approaches.", "startOffset": 43, "endOffset": 60}, {"referenceID": 9, "context": "Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT (Rozovskaya and Roth, 2016) and adapting phrase-based MT to the GEC task (Junczys-Dowmunt and Grundkiewicz, 2016). The latter work has reported the highest performance to date on the task of 49.5 in F0.5 score on the CoNLL-14 test set. This method integrates discriminative training toward the task-specific evaluation function, a rich set of features, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance.", "startOffset": 185, "endOffset": 623}, {"referenceID": 21, "context": "Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 23, "context": "Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 17, "context": "Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016).", "startOffset": 92, "endOffset": 115}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task.", "startOffset": 39, "endOffset": 65}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic.", "startOffset": 39, "endOffset": 272}, {"referenceID": 5, "context": ", 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic. Xie et al. (2016) built a character-level sequence", "startOffset": 39, "endOffset": 541}, {"referenceID": 12, "context": "We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention.", "startOffset": 39, "endOffset": 64}, {"referenceID": 12, "context": "We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task.", "startOffset": 39, "endOffset": 223}, {"referenceID": 18, "context": "open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 11, "context": ", 2016), and more recently, fully character-level modeling (Lee et al., 2017).", "startOffset": 59, "endOffset": 77}, {"referenceID": 0, "context": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016).", "startOffset": 121, "endOffset": 144}, {"referenceID": 0, "context": "The word-based backbone closely follows the basic neural sequence-to-sequence architecture with attention as proposed by Bahdanau et al. (2015) and applied to grammatical error correction by Yuan and Briscoe (2016). For completeness, we give a sketch here.", "startOffset": 121, "endOffset": 215}, {"referenceID": 3, "context": "The hidden state ht at time t is computed as: ft = GRUencf (ft\u22121, xt) , bt = GRUencb(bt+1, xt), ht = [ft; bt], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscripts to denote the parameters of the forward and backward word-level encoder units.", "startOffset": 194, "endOffset": 212}, {"referenceID": 8, "context": "ReLU indicates rectified linear units (Hahnloser et al., 2000).", "startOffset": 38, "endOffset": 62}, {"referenceID": 12, "context": "Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016).", "startOffset": 73, "endOffset": 98}, {"referenceID": 4, "context": "A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.", "startOffset": 54, "endOffset": 96}, {"referenceID": 23, "context": "A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words.", "startOffset": 54, "endOffset": 96}, {"referenceID": 12, "context": "The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task.", "startOffset": 20, "endOffset": 45}, {"referenceID": 12, "context": "The architecture of Luong and Manning (2016)", "startOffset": 20, "endOffset": 45}, {"referenceID": 12, "context": "As in Luong and Manning (2016), the \u201cseparate path\u201d architecture is used to capture the relevant context and define the initial state for the character-level decoder:", "startOffset": 6, "endOffset": 31}, {"referenceID": 22, "context": "To be able to attend to the relevant source character sequence when generating the target character sequence, we use the concept of hard attention (Xu et al., 2015), but use an arg-max approximation for inference instead of sampling.", "startOffset": 147, "endOffset": 164}, {"referenceID": 10, "context": "A similar approach to represent discrete hidden structure in a variety of architectures is used in Kong et al. (2017).", "startOffset": 99, "endOffset": 118}, {"referenceID": 12, "context": "If the source word xzs is in the source vocabulary, the model is analogous to the one of Luong and Manning (2016) and does not use characterlevel attention: the source context is available only in aggregated form to initialize the state of the decoder.", "startOffset": 89, "endOffset": 114}, {"referenceID": 12, "context": "While the model can be naturally generalized to integrate characterlevel attention for known words, the original hybrid model proposed by Luong and Manning (2016) does not use any character-level information for known words.", "startOffset": 138, "endOffset": 163}, {"referenceID": 6, "context": "One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks.", "startOffset": 61, "endOffset": 85}, {"referenceID": 14, "context": "is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences.", "startOffset": 38, "endOffset": 54}, {"referenceID": 20, "context": "0 (Tajiri et al., 2012).", "startOffset": 2, "endOffset": 23}, {"referenceID": 13, "context": "We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014).", "startOffset": 93, "endOffset": 110}, {"referenceID": 6, "context": "alyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013).", "startOffset": 56, "endOffset": 80}, {"referenceID": 23, "context": "We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement.", "startOffset": 181, "endOffset": 205}, {"referenceID": 23, "context": "We evaluate our model in comparison to the strong baseline of a word-based neural sequenceto-sequence model with attention, with postprocessing for handling out-of-vocabulary words (Yuan and Briscoe, 2016); we refer to this model as word NMT+UNK replacement. Like Yuan and Briscoe (2016), we use a traditional wordalignment model (GIZA++) to derive a wordcorrection lexicon from the parallel training set.", "startOffset": 182, "endOffset": 288}, {"referenceID": 3, "context": "gz get OOV, but follow Cho et al. (2015), Section 3.", "startOffset": 23, "endOffset": 41}, {"referenceID": 23, "context": "To reproduce the model of Yuan and Briscoe (2016), we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies.", "startOffset": 26, "endOffset": 50}, {"referenceID": 12, "context": "In addition to the word-level baseline, we include the performance of a hybrid model with a single level of attention, which follows the work of Luong and Manning (2016) for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction.", "startOffset": 145, "endOffset": 170}, {"referenceID": 23, "context": "As shown in Table 3, our implementation of the word NMT+UNK replacement baseline approaches the performance of the one reported in Yuan and Briscoe (2016) (38.", "startOffset": 131, "endOffset": 155}, {"referenceID": 2, "context": "In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 94, "endOffset": 113}, {"referenceID": 2, "context": "In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl (Buck et al., 2014), made available for download by Junczys-Dowmunt and Grundkiewicz (2016).", "startOffset": 95, "endOffset": 186}, {"referenceID": 21, "context": "Character-based NMT + LM (Xie et al., 2016) 40.", "startOffset": 25, "endOffset": 43}, {"referenceID": 21, "context": "The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model.", "startOffset": 45, "endOffset": 63}, {"referenceID": 21, "context": "in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training.", "startOffset": 70, "endOffset": 88}], "year": 2017, "abstractText": "Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.", "creator": "LaTeX with hyperref package"}}}