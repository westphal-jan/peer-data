{"id": "1707.07278", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Fine Grained Citation Span for References in Wikipedia", "abstract": "\\ emph {Verifiability} is one of the central editing principles in Wikipedia. Editors are encouraged to provide quotes for the additional content. In a Wikipedia article, determining the\\ emph {citation span} of a quote, i.e. the content covered by a quote, is important because it helps decide for which content quotes are missing.", "histories": [["v1", "Sun, 23 Jul 2017 10:43:26 GMT  (111kb,D)", "http://arxiv.org/abs/1707.07278v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["besnik fetahu", "katja markert", "avishek anand"], "accepted": true, "id": "1707.07278"}, "pdf": {"name": "1707.07278.pdf", "metadata": {"source": "CRF", "title": "Fine-Grained Citation Span Detection for References in Wikipedia", "authors": ["Besnik Fetahu", "Katja Markert", "Avishek Anand"], "emails": ["anand}@L3S.de", "markert@cl.uni-heidelberg.de"], "sections": [{"heading": null, "text": "We are the first to address the problem of determining the citation span in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered by a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a finegrained level.\nWe provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics."}, {"heading": "1 Introduction", "text": "Citations uphold the crucial policy of verifiability in Wikipedia. This policy requires Wikipedia contributors to support their additions with citations from authoritative external sources (web, news, journal etc.). In particular, it states that \u201carticles should be based on reliable, third-party, published sources with a reputation for fact-checking and accuracy\u201d1. Not only are citations essential in maintaining reliability, neutrality and authoritative assessment of content in such a collaboratively edited platform; but lack of citations are\n1https://en.wikipedia.org/wiki/ Wikipedia:Identifying_reliable_sources\nessential signals for core editors for unreliability checks. However, there are two problems when it comes to citing facts in Wikipedia. First, there is a long tail of Wikipedia pages where citations are missing and hence facts might be unverified. Second, citations might have different span granularities, i.e., the text encoding the fact(s), for which a citation is intended, might span less than a sentence (see Figure 1) to multiple sentences. We denote the different pieces of text which contain a citation marker as fact statements or simply statements. For example, Table 1 shows different statements for several citations. The aim of this work is to automatically and accurately determine citation spans in order to improve coverage (Fetahu et al., 2015b, 2016) and to assist editors in verifying citation quality at a fine-grained level.\nEarlier work on span determination is mostly concerned with scientific texts (O\u2019Connor, 1982; Kaplan et al., 2016), operates at sentence level and exploits explicit authoring cues specific to scientific text. Although Wikipedia has well formed text, it does not follow explicit scientific guidelines for placing citations. Moreover, most statements can only be inferred from the citation text.\nIn this work, we operate at a sub-sentence level, loosely referred to as text fragments, and take a sequence prediction approach using a linear chain CRF (Lafferty et al., 2001). We limit our work to citations referring to web and news sources, as\nar X\niv :1\n70 7.\n07 27\n8v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\nthey are accessible online and present the most prominent sources in Wikipedia (Fetahu et al., 2015a). By using recent work on moving window language models (Taneva and Weikum, 2013) and the structure of the paragraph that includes a citation, we classify sequences of text fragments as text that belong to a given citation. We are able to tackle all citation span cases as shown in Table 1."}, {"heading": "2 Problem Definition and Terminology", "text": "In this section, we describe the terminology and define the problem of determining the citation span in text in Wikipedia articles.\nTerminology. We consider Wikipedia articles W = {e1, . . . , en} from a Wikipedia snapshot. We distinguish citations to external references in text and denote them with \u3008pk, ci\u3009, where ci represents a citation which occurs in paragraph pk with positional index k in an entity e \u2208W . We will refer to pk as the citing paragraph. Furthermore, with citing sentence we refer to the sentence in s \u2208 pk, which contains ci. Note that pk can have more than one citation as shown in Table 1.\nProblem Definition. The task of determining the citation span for a citation c and a paragraph p, respectively \u3008p, c\u3009 (or simply pc), is subject to the citing paragraph and the citation content. In particular, we refer with citation span to the textual fragments from p which are covered by c. The fragments correspond to the sequence of subsentences S(p) = \u3008\u03b411 , \u03b421 , . . . , \u03b4k1 , . . . , \u03b4mn \u3009. We obtain the sequence of sub-sentences from p by splitting the sentences into sub-sentences or text fragments based on the following punctuation delimiters ({, !; :?}). These delimitors do not always provide a perfect semantic segmentation of sentences into facts. A more involved approach could be taken akin to work in text summarization, such\nas Zhou and Hovy (Zhou and Hovy, 2006) or (?) who consider summary units for a similar purpose.\nFormally, we define the citation span in Equation 4 as the function of finding the subset S \u2032 \u2286 S where the fragments in S \u2032 are covered by c.\n\u03d5(p, c)\u2192 S \u2032 \u2286 S, s.t. \u03b4 \u2208 S \u2032 \u2227 c ` \u03b4 (1)\nwhere c ` \u03b4 states that \u03b4 is covered in c."}, {"heading": "3 Related Work", "text": "Scientific Text. One of the first attempts to determine the citation span in text (O\u2019Connor, 1982) was carried out in the context of document retrieval. The citing statements from a document were used as an index to retrieve the cited document. The citing statements are extracted based on heuristics starting from the citing sentence and are expanded with sentences in a window of +/-2 sentences, depending on them containing cue words like \u2018this\u2019, \u2018these\u2019,. . . \u2018above-mentioned\u2019. We consider the approach in (O\u2019Connor, 1982) as a baseline.\nKaplan et al. (2016) proposed the task of determining the citation block based on a set of textual coherence features (e.g. grammatical or lexical coherence). The citation block starts from the citing sentence, with succeeding sentences classified (through SVMs or CRFs) according to whether they belong to the block. Abu-Jbara and Radev (2012) determine the citation block by first segmenting the sentences and then classifying individual words as being inside/outside the citation. Finally, the segment is classified depending on the word labels (majority of words being inside, at least one, or all of them). This approach is not applicable in our case due to the fact that words in Wikipedia text are not domain or genre-specific as one expects in scientific text, and as such their classification does not work.\nCitations in IR. The importance of determining the citation span has been acknowledged in the field of Information Retrieval (IR). The focus is on building citation indexes (Garfield, 1955) and improving the retrieval of scientific articles (Ritchie et al., 2008, 2006). Citing sentences on a fixed window size are used to index documents and aid the retrieval process.\nSummarization. Citations have been successfully employed to generate summaries of scientific articles (Qazvinian and Radev, 2008; Elkiss et al.,\n2008). In all cases, citing statements are either extracted manually or via heuristics such as extracting only citing sentences. Similarly (Nanba and Okumura, 1999) expand the summaries in addition to the citing sentence based on cue words (e.g. \u2018In this\u2019, \u2018However\u2019 etc.). The work in (Qazvinian and Radev, 2010) goes one step beyond and considers sentences which do not explicitly cite another article. The task is to assign a binary label to a sentence, indicating whether it contains context for a cited paper. We use this approach as one of our competitors. Again, the premise is that citations are marked explicitly and additional citing sentences are found dependent on them.\nComparison to our work. The language style and the composition of citations in Wikipedia and in scientific text differ significantly. Citations are explicit in scientific text (e.g. author names) and are usually the first word in a sentence (AbuJbara and Radev, 2012). In Wikipedia, citations are implicit (see Table 1) and there are no cue words in text which link to the provided citations. Therefore, the proposed methodologies and features from the scientific domain do not perform optimally in our case.\nBoth (Qazvinian and Radev, 2010) and (O\u2019Connor, 1982) work at the sentence level. As, in Wikipedia, citation span detection needs to be performed at the sub-sentence level (see Table 1), their method introduces erroneous spans as we will show in our evaluation.\nRelated to our problem is the work on addressing quotation attribution. Pareti et al. (2013) propose an approach for direct and indirect quotation attribution. The task is mostly based on lexical cues and specific reporting verbs that are the signal for the majority of direct quotations. However, in the case of quotation attribution the task is to find the source, cue, and content of the quotation, whereas in our case, for a given citing paragraph and reference we simply assess which text fragment is covered by the reference. We also do normally not have access to specific lexical links between the citation and its citation span."}, {"heading": "4 Citation Span Approach", "text": "We approach the problem of citation span detection in Wikipedia as a sequence classification problem. For a citation c and a citing paragraph p, we chunk the paragraph into textual fragments at the sub-sentence granularity, shown in Equation 4.\nFigure 2 shows an overview of the sequence classification of textual fragments. We use a linear chain CRF (Lafferty et al., 2001), where for any fragment \u03b4 we predict the label corresponding to a random variable y which is either \u2018covered\u2019 or \u2018not-covered\u2019. We opt for CRFs since we can encode global dependencies between the text fragments and the actual citation, thus, ensuring the coherence and accuracy of the predicted labels.\nWe now describe the features we compute for the factors \u03a8(yi, yi\u22121, \u03b4i) for a fragment \u03b4i w.r.t the citation c. We determine the fitness of \u03b4i holding true or being covered by c. We denote with fk the features for the factors \u03a8i(yi, yi\u22121, \u03b4i) for sequence \u03b4i for the linear-chain CRF in Figure 2."}, {"heading": "4.1 Structural Features", "text": "An important aspect to consider for citation span detection is the structure of the citing paragraph, and correspondingly its sentences. For a textual fragment \u03b4, we extract the following structural features shown in Table 2.\nFrom the features in Table 2, we highlight f ci which specifies the distance of \u03b4 to the fragment that cites c. The closer a fragment is to the citation the higher the likelihood of it being covered\nin c. In Wikipedia, depending on the citation and the paragraph length, the validity of a citation is densely concentrated in its nearby sub-sentences (preceding and succeeding).\nFurthermore, the features f#s and fsi (the number of sentences in p together with the feature considering if \u03b4 is in the same sentence as c) are strong indicators for accurate prediction of the label of \u03b4. That is, it is more likely for a fragment \u03b4 to be covered by the citation if it appears in the same sentence or sentences nearby to the citation marker.\nHowever, as shown in Table 1 there are three main citation span groups, and as such relying only on the structure of the citing paragraph does not yield optimal results. Hence, in the next group we consider features that tie the individual fragments in the citing paragraph with the citation as shown in Figure 2."}, {"heading": "4.2 Citation Features", "text": "A core indicator as to whether a fragment \u03b4 is covered by c is based on the lexical similarity between \u03b4 and the content in c. We gather such evidence by computing two similarity measures. We compute the features fLMi and f J i between \u03b4 and paragraphs in the citation content c. The first measure, fLMi , corresponds to a moving language window proposed in (Taneva and Weikum, 2013). In this case, for each word in either a paragraph in the citation c or the sequence \u03b4, we associate a language model Mwi based on its context \u03c6(wi) = {wi\u22123, wi\u22122, wi\u22121, wi, wi+1, wi+2, wi+3} with a window of +/- 3 words. The parameters for the model Mwi are estimated as in Equation 2 for all the words in the context \u03c6(wi) and their frequencies denoted with tf . With M\u03b4 and Mp we denote the overall models as estimated in Equation 2 for the words in the respective fragments.\nP (w|Mwi) = tfw,\u03c6(wi)\u2211\nw\u2032\u2208\u03c6(wi) tfw\u2032,\u03c6(wi) (2)\nFinally, we compute the similarity of each word in w \u2208 \u03b4 against the language model of paragraph p \u2208 c in Equation 3, which corresponds to the Kullback-Leibler divergence score.\nfLMi = min p\u2208c\n[ \u2212\n\u2211 w\u2208\u03b4 P (w|M\u03b4) log P (w|M\u03b4) P (w|Mp)\n] (3)\nThe intuition behind fLMi is that for the fragments \u03b4 we take into account the word similarity\nand the similarity in the context they appear in w.r.t a paragraph in a citation. In this way, we ensure that the similarity is not by chance but is supported by the context in which the word appears. Finally, another advantage of this model is that we localize the paragraphs in c which provide evidence for \u03b4.\nAs an additional feature we compute fJi which corresponds to the maximal jaccard similarity between \u03b4i and paragraphs p \u2208 c.\nFinally, as we will show in our experimental evaluation in Section 5, there is a high correlation between the citation span length and the length of citation content in terms of sentences. Hence, we add as an additional feature f c the number of sentences in c."}, {"heading": "4.3 Discourse Features", "text": "Sentences and fragments within a sentence can be tied together by discourse relations. We annotate sentences with explicit discourse relations based on an approach proposed in (Pitler and Nenkova, 2009), using discourse connectives as cues. The explicit discourse relations belong to one of the following: temporal, contingency, expansion, comparison.\nAfter extracting a discourse connective in a sentence, we determine by its position to which fragment it belongs and mark the fragment accordingly. We denote with fdisci the discourse feature for the fragment \u03b4i.2"}, {"heading": "4.4 Temporal Features", "text": "An important aspect that we consider here is the temporal difference between two consecutive fragments \u03b4i and \u03b4i\u22121. If there exists a temporal date expression in \u03b4i and \u03b4i\u22121 and they point to different time-points, this presents an indicator on the transitioning between the states yi and yi\u22121. That is, there is a higher likelihood of changing the state in the sequence S for the labels yi and yi\u22121.\nWe compute the temporal feature f \u03bb(i,i\u22121) i ,indicating the difference in days between any two temporal expression extracted from \u03b4i and \u03b4i\u22121. We extract the temporal expression through a set of hand-crafted regular expressions. We use the following expressions: (1) DD Month YYYY, (2) DD MM YYYY, (3)\n2Note that, although discourse relations hold between at least two fragments or sentences, we only mark the individual fragment in which the connective occurs with the discourse relation type.\nMM DD YY(YY), (4) YYYY, with delimiters (whitespace, \u2018-\u2019, \u2018.\u2019)."}, {"heading": "5 Experimental Setup", "text": "We now outline the experimental setup for evaluating the citation span approach and the competitors for this task. The data and the proposed approaches are made available at the paper URL3."}, {"heading": "5.1 Dataset", "text": "We evaluate the citation span approaches on a random sample of Wikipedia entities (snapshot of 20/11/2016). For the sampling process, we first group entities based on the number of web or news citations.4). We then sample from the specific groups. This is due to the inherent differences in citation spans for entities with different numbers of citations. For instance, entities with a high number of citations tend to have shorter spans per citation. Figure 3 shows the distribution of entities from the different groups. From each sampled entity, we extract all citing paragraphs that contain either a web or news citation. Our sample consists of 509 citing paragraphs from 134 entities.\nFurthermore, since a paragraph may have more than one citation, in our sampled citing paragraphs, we have an average of 4.4 citations per paragraph, which finally resulted in 408 unique paragraphs. Table 3 shows the stats of the dataset."}, {"heading": "5.2 Ground Truth", "text": "Setup. For the ground truth, the citation span of c in paragraph p was manually determined by labeling each fragment in p with the binary label covered or not-covered.\nWe set strict guidelines that help us generate reliable ground-truth annotations. We follow two main guidelines: (i) requirement to read and comprehend the content in c, and (ii) matching of the\n3http://l3s.de/\u02dcfetahu/emnlp17/ 4Wikipedia has an internal categorization of citations\nbased on the reference they point to.\ntextual fragments from p as either being supported explicitly or implicitly in c.5\nThe entire dataset was carefully annotated by the first author. Later, a second annotator annotated a 10% sample of the dataset with an interrater agreement of \u03ba = .84. We chose not to use crowd-sourcing as the task is very complex and hard to divide into small independent tasks. Since the task requires reading and comprehending the entire content in c and p, it takes on average up to 2.4 minutes to perform the evaluation for a single item. In future, it would be worthwhile to conduct more large-scale annotation exercises.\nCitation Span Stats. Following the definition in Equation 4, we determine the citation span at the sub-sentence granularity level. Table 4 shows the distribution of citations falling into the specific spans for the citing paragraphs. We note that the majority of citations have a span between half a sentence and up to a sentence, yet, the remainder of more than 20% of citation span across multiple sentences in such paragraphs.\nWe define the citation span as the ratio of subsentences which are covered by a given citation over the total number of sub-sentences in the sentence, consequentially in the citing paragraph. That is, a citation is considered to have a span of one sentence if it covers all its sub-sentences.\nspan(c, p) = \u2211 s\u2208p #\u03b4s \u2208 S \u2032 #\u03b4s (4)\nwhere \u03b4s represents a sequence in sentence s \u2208 p, which are part of the the ground-truth.\nIn Figure 4, we analyze a possible factor in the variance of the citation span. It is evident that for longer cited documents the span increases. This is\n5We excluded cases where the citation is not appropriate for the paragraph at all. This is, for example, the case when the language of c is not English.\nintuitive since such documents carry more information and consequentially their span in the citing paragraphs can be larger. An example is the Wikipedia article 2008 US Open (tennis) which has a citing paragraph with a citation span of 7 sentences for an article of 30k characters long6. We encoded this in the citation features f c.\nAdditionally, within the different citation spans we analyze how many of them contain skips for two cases: (i) skip a fragment within a sentence, and (ii) skip sentences in p. The results for both cases are presented in Table 5.\nFrom the results in Table 4 and 5 we see that simple heuristics on selecting complete sentences or selecting consecutive sequences do not account for the different citation span cases and skips at the sentence and paragraph level. This leads to suboptimal results and introduces erroneous spans. Furthermore, we find that in 3.7% of the cases in our\n6http://news.bbc.co.uk/sport1/hi/ tennis/7601195.stm\nground-truth, the citation spans include fragments after the citation marker."}, {"heading": "5.3 Baselines", "text": "We consider the following baselines as competitors for our citation span approach.\nInter-Citation Text \u2013 IC. The span consists of sentences which start either at the beginning of the paragraph or at the end of a previous citation. The granularity is at the sentence level.\nCitation-Sentence-Window \u2013 CSW. The span consists of sentences in a window of +/- 2 sentences from the citing sentence (O\u2019Connor, 1982). The other sentences are included if they contain specific cue words in fixed positions.\nCiting Sentence \u2013 CS. The span consists of only the citing sentence.\nMarkov Random Fields - MRF. MRFs (Qazvinian and Radev, 2010) model two functions. First, compatibility, which measures the similarity of sentences in p, and as such allows to extract non-citing sentences. Second, the potential, which measures the similarity between sentences in c with sentences in p. We use the provided implementation by the authors.\nCitation Span Plain \u2013 CSPC. A plain classification setup using the features in Section 4, where the sequences are classified in isolation. We use Random Forests (Breiman, 2001) and evaluate them with 5-fold cross validation."}, {"heading": "5.4 Citation Span Approach Setup \u2013 CSPS", "text": "For our approach CSPS as mentioned in Section 4, we opt for linear-chain CRFs and use the implementation in (Okazaki, 2007). We evaluate our models using 5-fold cross validation, and learn the optimal parameters for the CRF model through the L-BFGS approach (Liu and Nocedal, 1989)."}, {"heading": "5.5 Evaluation Metrics", "text": "We measure the performance of the citation span approaches through the following metrics. We will denote with W \u2032 the sampled entities, with p = {pc, . . .} (pc refers to \u3008p, c\u3009) the set of sampled paragraphs from e, and with |p| the total items from e.\nMean Average Precision \u2013 MAP . First, we define precision for pc as the ratio P (pc) = |S \u2032 \u2229 St|/|S \u2032| of fragments present in S \u2032\u2229St over S \u2032. We measure MAP as in Equation 5.\nMAP = 1 |W \u2032| \u2211 e\u2208W \u2032\n\u2211 pc\u2208p P (pc)\n|p| (5)\nRecall \u2013 R. We measure the recall for pc as the ratio S \u2032 \u2229 St over all fragments in St, R(pc) = |S \u2032 \u2229 St|/|St|. We average the individual recall scores for e \u2208W \u2032 for the corresponding p.\nR = 1 |W \u2032| \u2211 e\u2208W \u2032\n\u2211 pc\u2208pR(pc)\n|p| (6)\nErroneous Span \u2013 \u2206. We measure the number of extra words or extra sub-sentences (denoted with \u2206w and \u2206\u03b4) added by text fragments that are not part of the ground-truth St. The ratio is relative to the number of words or sub-sentences in the ground-truth for pc. We compute \u2206w and \u2206\u03b4 in Equation 7 and 8, respectively.\n\u2206w = 1 |W \u2032| \u2211 e\u2208W \u2032 1 |p| \u2211 pc\u2208p \u2211 \u03b4\u2208S\u2032\\St words(\u03b4)\u2211 \u03b4\u2208St words(\u03b4) (7)\n\u2206\u03b4 = 1 |W \u2032| \u2211 e\u2208W \u2032 1 |p| \u2211 pc\u2208p |S \u2032 \\ St| |St| (8)"}, {"heading": "6 Results and Discussion", "text": ""}, {"heading": "6.1 Citation Span Robustness", "text": "Table 6 shows the results for the different approaches on determining the citation span for all span cases shown in Table 4.\nAccuracy. Not surprisingly, the baseline approaches perform reasonably well. CS which selects only the citing sentence achieves a reasonable MAP = 0.86 and similar recall. A slightly different baseline CSW achieves comparable scores with MAP = 0.85. This is due to the inherent span structure in Wikipedia, where a large portion of citations span up to a sentence (see Table 4). Therefore, in approximately 64% of the cases the baselines will select the correct span. For the cases where the span is more than a sentence, the drawback of these baselines is in coverage. We show in the next section a detailed decomposition of the results and highlight why even in the simpler cases, a sentence level granularity has its shortcomings due to sequence skips as shown in Table 5.\nOverall, when comparing CS as the best performing baseline against our approach CSPS, we achieve an overall score of MAP = 0.83 (a slight decrease of 3.6%), whereas in term of F1 score, we have a decrease of 9%. The plainclassification approach CSPC achieves similar score with MAP = 0.86, whereas in terms of F1 score, we have a decrease of 8%. As described above and as we will see later on in Table 7, the overall good performance of the baseline\napproaches can be attributed to the citation span distribution in our ground-truth.\nOn the other hand, an interesting observation is that sophisticated approaches, geared towards scientific domains like MRF perform poorly. We attribute this to language style, i.e., in Wikipedia there are no explicit citation hooks that are present in scientific articles. Comparing to CSPS, we outperform MRF by a large margin with an increase in MAP by 84%.\nWhen comparing the sequence classifierCSPS to the plain classifier CSPC, we see a marginal difference of 1.3% for F1. However, it will become more evident later that classifying jointly the text fragments for the different span buckets, outperforms the plain classification model.\nErroneous Span. One of the major drawbacks of competing approaches is the granularity at which the span is determined. This leads to erroneous spans. From Table 4 we see that approximately in \u223c10% of the cases the span is at subsentence level, and in 28% the span is more than a sentence.\nThe best performing baseline CS has an erroneous span of \u2206w = 35% and \u2206\u03b4 = 27%, in terms of extra words and sub-sentences, respectively. That is, nearly half of the determined span is erroneous, or in other words it is not covered in the provided citation. The MRF approach due to its poor MAP score provides the largest erroneous spans with \u2206w = 308% and \u2206\u03b4 = 278%. The amount of erroneous span is unevenly distributed, that is, in cases where the span is not at the sentence level granularity the amount of erroneous span increases. A detailed analysis is provided in the next section.\nContrary to the baselines, for CSPS and similarly for CSPC, we achieve the lowest erroneous spans with \u2206w = 32% and \u2206\u03b4 = 26%, and \u2206w = 24% and \u2206w = 23%, respectively.\nCompared to the remaining baselines, we\nachieve an overall relative decrease of 9% for \u2206w(CSPS), and 34% for \u2206w(CSPC), when compared to the best performing baseline CS.\nFrom the skips in sequences in Table 5 and the unsuitability of sentence granularity for citation spans, we analyze the locality of erroneous spans w.r.t to the sequence that contains c, specifically the distribution of erroneous spans preceding and succeeding it. For the CS baseline, 71% of the total erroneous spans are added by sequences preceding the citing sequence, contrary to 35% which succeed it. In the case ofCSPS, we have only 9% of erroneous spans (for \u2206\u03b4) preceding the citation."}, {"heading": "6.2 Citation Span and Feature Analysis", "text": "We now analyze how the approaches perform for the different citation spans in Table 47. Additionally, we analyze how our approach CSPS performs when determining the span without access to the content of c.\nCitation Span. Table 7 shows the results for the approaches under comparison for all the citation span cases. In the case where the citation spans up to a sentence, that is (0.5, 1], which presents the simplest citation span case, the baselines perform reasonably well. This is due to the heuristics they apply to determine the span, which in all cases includes the citing sentence. In terms of F1 score, the baseline CS achieves a highly competitive score of F1 = 0.97. Our approach CSPS in this case has slight increase of 1% for F1 and an increase of 3% for MAP . CSPC achieves a similar performance in this case.\nHowever, for the cases where the span is at the sub-sentence level or across multiple sentences, the performance of baselines drops drastically. In the first bucket (\u2264 0.5) which accounts for 9% of ground-truth data, we achieve the highest score with MAP = 0.87, though with lower recall than the competitors with R = 0.56. The reason for this is that the baselines take complete sentences, thus, having perfect recall at the cost of accuracy. In terms of F1 score we achieve 21% better results than the best performing baseline CS.\nFor the span of (1, 2] we maintain an overall high accuracy and recall, and have the highest F1 score. The improvement is 8% in terms of F1 score. Finally, for the last case where the span is more than 2 sentences, we achieve MAP = 0.74,\n7The models were retrained and tested for the different buckets with 5-fold cross validation.\na marginal increase of 3%, however with lower recall, which results in an overall decrease of 4% for F1. The statistical significance tests are indicated with ** and * in Table 7.\nErroneous Span. Figure 5 shows the erroneous spans in terms of words for the metric \u2206w for all citation span cases. It is noteworthy that the amount of error can be well beyond 100% due to the ratio of the suggested span and the actual span in our ground-truth, which can be higher.\nIn the first bucket (span of \u2264 0.5) with granularity less than a sentence, all the competing approaches introduce large erroneous spans. For CSPS we have a MAP = 0.87, and consequentially we have the lowest \u2206w = 9%, while for CSPC we have only \u2206w = 11%. In contrast, the non-ML competitors introduce a minimum of \u2206w(CS) = 182%, with MRFs having the highest error. We also perform well in the bucket (0.5, 1]. For larger spans, for instance, for (1, 2], we are still slightly better, with roughly 3% less erroneous span when comparing CSPC and CS. However, only in the case of spans with > 2, we perform below the CS baseline. Despite, the smaller erroneous span, the CS baseline never includes more than one sentence, and as such it does not include many erroneous spans for the larger buckets. However, it is by definition unable to recognize any longer spans.\nFeature Analysis. It is worthwhile to investigate the performance gains in determining the citation span without analyzing the content of the citation. The reason for this is that there are several cita-\ntion categories for which access to the source cannot be easily automated. Models which can determine the span accurately without the actual content have the advantage of generalizing to other citation sources (e.g. books) for which the evaluation is more challenging.8\nHere, we disregard the citation features from Section 4.2. In terms of MAP , we have a slight decrease with MAP = 0.82 when compared to the model with the citation features. For recall we have a drop of 3%, resulting in R = 0.67.\nThis shows that by solely relying on the structure of the citing paragraph and other structural and discourse features we can perform the task with reasonable accuracy."}, {"heading": "7 Conclusion", "text": "In this work, we tackled the problem of determining the fine-grained citation span of references in Wikipedia. We started from the citing paragraph and decomposed it into sequences consisting of sub-sentences. To accurately determine the span we proposed features that leverage the structure of the paragraph, discourse and temporal features, and finally analyzed the similarity between the citing paragraph and the citation content.\nWe introduce both a standard classifier as well as a sequence classifier using a linear-chain CRF model. For evaluation we manually annotated a ground-truth dataset of 509 citing paragraphs. We reported standard evaluation metrics and also in-\n8At worst, one needs to read and comprehend the entire book to determine if a fragment is covered by the citation.\ntroduced metrics that measure the amount of erroneous span.\nWe achieved a MAP = 0.86, in the case of the plain classification model CSPC, and with a marginal difference for CSPS with MAP = 0.83, across all cases with an erroneous span of \u2206w = 26% or \u2206w = 32%, depending on the model. Thus, we provide accurate means on determining the span and at the same time decrease the erroneous span by 34% compared to the best performing baselines. Moreover, we excel at determining citation spans at the sub-sentence level.\nIn conclusion, this presents an initial attempt on solving the citation span for references in Wikipedia. As future work we foresee a larger ground-truth and more robust approaches which take into account factors such as a reference being irrelevant to a citing paragraph and cases where the evidence for a paragraph is implied rather than explicitly stated in the reference."}, {"heading": "Acknowledgments", "text": "This work is funded by the ERC Advanced Grant ALEXANDRIA (grant no. 339233), and H2020 AFEL project (grant no. 687916)."}], "references": [{"title": "Reference scope identification in citing sentences", "author": ["Amjad Abu-Jbara", "Dragomir R. Radev."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 3-8, 2012,", "citeRegEx": "Abu.Jbara and Radev.,? 2012", "shortCiteRegEx": "Abu.Jbara and Radev.", "year": 2012}, {"title": "Random forests", "author": ["Leo Breiman."], "venue": "Machine Learning, 45(1):5\u201332.", "citeRegEx": "Breiman.,? 2001", "shortCiteRegEx": "Breiman.", "year": 2001}, {"title": "Blind men and elephants: What do citation summaries tell us about a research article", "author": ["Aaron Elkiss", "Siwei Shen", "Anthony Fader", "G\u00fcnes Erkan", "David J. States", "Dragomir R. Radev"], "venue": null, "citeRegEx": "Elkiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elkiss et al\\.", "year": 2008}, {"title": "2015a. How much is wikipedia lagging behind news", "author": ["Besnik Fetahu", "Abhijit Anand", "Avishek Anand"], "venue": "In Proceedings of the ACM Web Science Conference,", "citeRegEx": "Fetahu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fetahu et al\\.", "year": 2015}, {"title": "Automated news suggestions for populating wikipedia entity pages", "author": ["Besnik Fetahu", "Katja Markert", "Avishek Anand."], "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne,", "citeRegEx": "Fetahu et al\\.,? 2015b", "shortCiteRegEx": "Fetahu et al\\.", "year": 2015}, {"title": "Finding news citations for wikipedia", "author": ["Besnik Fetahu", "Katja Markert", "Wolfgang Nejdl", "Avishek Anand."], "venue": "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,", "citeRegEx": "Fetahu et al\\.,? 2016", "shortCiteRegEx": "Fetahu et al\\.", "year": 2016}, {"title": "Citation indexes for science: A new dimension in documentation through association of ideas", "author": ["Eugene Garfield."], "venue": "Science, 122(3159):108\u2013111.", "citeRegEx": "Garfield.,? 1955", "shortCiteRegEx": "Garfield.", "year": 1955}, {"title": "Citation block determination using textual coherence", "author": ["Dain Kaplan", "Takenobu Tokunaga", "Simone Teufel."], "venue": "JIP, 24(3):540\u2013553.", "citeRegEx": "Kaplan et al\\.,? 2016", "shortCiteRegEx": "Kaplan et al\\.", "year": 2016}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["Dong C Liu", "Jorge Nocedal."], "venue": "Mathematical programming, 45(1):503\u2013528.", "citeRegEx": "Liu and Nocedal.,? 1989", "shortCiteRegEx": "Liu and Nocedal.", "year": 1989}, {"title": "Towards multi-paper summarization using reference information", "author": ["Hidetsugu Nanba", "Manabu Okumura."], "venue": "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, IJCAI 99, Stockholm, Sweden, July 31 - August 6,", "citeRegEx": "Nanba and Okumura.,? 1999", "shortCiteRegEx": "Nanba and Okumura.", "year": 1999}, {"title": "Citing statements: Computer recognition and use to improve retrieval", "author": ["John O\u2019Connor"], "venue": "Inf. Process. Manage.,", "citeRegEx": "O.Connor.,? \\Q1982\\E", "shortCiteRegEx": "O.Connor.", "year": 1982}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Automatically detecting and attributing indirect quotations", "author": ["Silvia Pareti", "Timothy O\u2019Keefe", "Ioannis Konstas", "James R. Curran", "Irena Koprinska"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Pareti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pareti et al\\.", "year": 2013}, {"title": "Using syntax to disambiguate explicit discourse connectives in text", "author": ["Emily Pitler", "Ani Nenkova."], "venue": "ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natu-", "citeRegEx": "Pitler and Nenkova.,? 2009", "shortCiteRegEx": "Pitler and Nenkova.", "year": 2009}, {"title": "Scientific paper summarization using citation summary networks", "author": ["Vahed Qazvinian", "Dragomir R. Radev."], "venue": "COLING 2008, 22nd International Conference on Computational Linguistics, Proceedings of the Conference, 18-22 August 2008, Manch-", "citeRegEx": "Qazvinian and Radev.,? 2008", "shortCiteRegEx": "Qazvinian and Radev.", "year": 2008}, {"title": "Identifying non-explicit citing sentences for citationbased summarization", "author": ["Vahed Qazvinian", "Dragomir R. Radev."], "venue": "ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Upp-", "citeRegEx": "Qazvinian and Radev.,? 2010", "shortCiteRegEx": "Qazvinian and Radev.", "year": 2010}, {"title": "Comparing citation contexts for information retrieval", "author": ["Anna Ritchie", "Stephen Robertson", "Simone Teufel."], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008, Napa Valley, California, USA, October", "citeRegEx": "Ritchie et al\\.,? 2008", "shortCiteRegEx": "Ritchie et al\\.", "year": 2008}, {"title": "How to find better index terms through citations", "author": ["Anna Ritchie", "Simone Teufel", "Stephen Robertson."], "venue": "Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, CLIIR \u201906, pages 25\u201332, Stroudsburg, PA,", "citeRegEx": "Ritchie et al\\.,? 2006", "shortCiteRegEx": "Ritchie et al\\.", "year": 2006}, {"title": "Gembased entity-knowledge maintenance", "author": ["Bilyana Taneva", "Gerhard Weikum."], "venue": "22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San Francisco, CA, USA, October 27 - November 1, 2013, pages", "citeRegEx": "Taneva and Weikum.,? 2013", "shortCiteRegEx": "Taneva and Weikum.", "year": 2013}, {"title": "On the summarization of dynamically introduced information: Online discussions and blogs", "author": ["Liang Zhou", "Eduard H Hovy."], "venue": "AAAI Spring symposium: Computational approaches to analyzing we-", "citeRegEx": "Zhou and Hovy.,? 2006", "shortCiteRegEx": "Zhou and Hovy.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Earlier work on span determination is mostly concerned with scientific texts (O\u2019Connor, 1982; Kaplan et al., 2016), operates at sentence level and exploits explicit authoring cues specific to scientific text.", "startOffset": 77, "endOffset": 114}, {"referenceID": 7, "context": "Earlier work on span determination is mostly concerned with scientific texts (O\u2019Connor, 1982; Kaplan et al., 2016), operates at sentence level and exploits explicit authoring cues specific to scientific text.", "startOffset": 77, "endOffset": 114}, {"referenceID": 8, "context": "In this work, we operate at a sub-sentence level, loosely referred to as text fragments, and take a sequence prediction approach using a linear chain CRF (Lafferty et al., 2001).", "startOffset": 154, "endOffset": 177}, {"referenceID": 19, "context": "By using recent work on moving window language models (Taneva and Weikum, 2013) and the structure of the paragraph that includes a citation, we classify sequences of text fragments as text that belong to a given citation.", "startOffset": 54, "endOffset": 79}, {"referenceID": 20, "context": "A more involved approach could be taken akin to work in text summarization, such as Zhou and Hovy (Zhou and Hovy, 2006) or (?) who consider summary units for a similar purpose.", "startOffset": 98, "endOffset": 119}, {"referenceID": 11, "context": "One of the first attempts to determine the citation span in text (O\u2019Connor, 1982) was carried out in the context of document retrieval.", "startOffset": 65, "endOffset": 81}, {"referenceID": 11, "context": "We consider the approach in (O\u2019Connor, 1982) as a baseline.", "startOffset": 28, "endOffset": 44}, {"referenceID": 0, "context": "Abu-Jbara and Radev (2012) determine the citation block by first segmenting the sentences and then classifying individual words as being inside/outside the citation.", "startOffset": 0, "endOffset": 27}, {"referenceID": 6, "context": "The focus is on building citation indexes (Garfield, 1955) and improving the retrieval of scientific articles (Ritchie et al.", "startOffset": 42, "endOffset": 58}, {"referenceID": 10, "context": "Similarly (Nanba and Okumura, 1999) expand the summaries in addition to the citing sentence based on cue words (e.", "startOffset": 10, "endOffset": 35}, {"referenceID": 16, "context": "The work in (Qazvinian and Radev, 2010) goes one step beyond and considers sentences which do not explicitly cite another article.", "startOffset": 12, "endOffset": 39}, {"referenceID": 16, "context": "Both (Qazvinian and Radev, 2010) and (O\u2019Connor, 1982) work at the sentence level.", "startOffset": 5, "endOffset": 32}, {"referenceID": 11, "context": "Both (Qazvinian and Radev, 2010) and (O\u2019Connor, 1982) work at the sentence level.", "startOffset": 37, "endOffset": 53}, {"referenceID": 13, "context": "Pareti et al. (2013) propose an approach for direct and indirect quotation attribution.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "We use a linear chain CRF (Lafferty et al., 2001), where for any fragment \u03b4 we predict the label corresponding to a random variable y which is either \u2018covered\u2019 or \u2018not-covered\u2019.", "startOffset": 26, "endOffset": 49}, {"referenceID": 19, "context": "The first measure, fLM i , corresponds to a moving language window proposed in (Taneva and Weikum, 2013).", "startOffset": 79, "endOffset": 104}, {"referenceID": 14, "context": "We annotate sentences with explicit discourse relations based on an approach proposed in (Pitler and Nenkova, 2009), using discourse connectives as cues.", "startOffset": 89, "endOffset": 115}], "year": 2017, "abstractText": "Verifiability is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content. For a Wikipedia article, determining the citation span of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing. We are the first to address the problem of determining the citation span in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered by a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a finegrained level. We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.", "creator": "LaTeX with hyperref package"}}}