{"id": "1707.07328", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2017", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "abstract": "To reward systems with real speech comprehension skills, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD), which tests whether systems can answer questions about paragraphs containing contractually inserted sentences that are automatically generated to distract computer systems without changing the correct answer or misleading people. In this adversarial environment, the accuracy of sixteen published models drops from an average of $75\\% $F1 to $36\\% $; if the adversary is allowed to add ungrammatic word strings, the average accuracy of four models drops further to $7\\% $. We hope that our findings will motivate the development of new models that understand language more accurately.", "histories": [["v1", "Sun, 23 Jul 2017 18:26:29 GMT  (836kb,D)", "http://arxiv.org/abs/1707.07328v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["robin jia", "percy liang"], "accepted": true, "id": "1707.07328"}, "pdf": {"name": "1707.07328.pdf", "metadata": {"source": "CRF", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "authors": ["Robin Jia", "Percy Liang"], "emails": ["robinjia@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Quantifying the extent to which a computer system exhibits intelligent behavior is a longstanding problem in AI (Levesque, 2013). Today, the standard paradigm is to measure average error across a held-out test set. However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016).\nIn this work, we propose adversarial evaluation for NLP, in which systems are instead evaluated on adversarially-chosen inputs. We focus on the\nSQuAD reading comprehension task (Rajpurkar et al., 2016), in which systems answer questions about paragraphs from Wikipedia. Reading comprehension is an appealing testbed for adversarial evaluation, as existing models appear successful by standard average-case evaluation metrics: the current state-of-the-art system achieves 84.7% F1 score, while human performance is just 91.2%.1 Nonetheless, it seems unlikely that existing systems possess true language understanding and reasoning capabilities.\nCarrying out adversarial evaluation on SQuAD requires new methods that adversarially alter reading comprehension examples. Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015). In contrast, changing even one word of a\n1https://rajpurkar.github.io/ SQuAD-explorer/\nar X\niv :1\n70 7.\n07 32\n8v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\nparagraph can drastically alter its meaning. Instead of relying on semantics-preserving perturbations, we create adversarial examples by adding distracting sentences to the input paragraph, as shown in Figure 1. We automatically generate these sentences so that they confuse models, but do not contradict the correct answer or confuse humans. For our main results, we use a simple set of rules to generate a raw distractor sentence that does not answer the question but looks related; we then fix grammatical errors via crowdsourcing. While adversarially perturbed images punish model oversensitivity to imperceptible noise, our adversarial examples target model overstability\u2014 the inability of a model to distinguish a sentence that actually answers the question from one that merely has words in common with it.\nOur experiments demonstrate that no published open-source model is robust to the addition of adversarial sentences. Across sixteen such models, adding grammatical adversarial sentences reduces F1 score from an average of 75% to 36%. On a smaller set of four models, we run additional experiments in which the adversary adds nongrammatical sequences of English words, causing average F1 score to drop further to 7%. To encourage the development of new models that understand language more precisely, we have released all of our code and data publicly."}, {"heading": "2 The SQuAD Task and Models", "text": ""}, {"heading": "2.1 Task", "text": "The SQuAD dataset (Rajpurkar et al., 2016) contains 107,785 human-generated reading comprehension questions about Wikipedia articles. Each question refers to one paragraph of an article, and the corresponding answer is guaranteed to be a span in that paragraph."}, {"heading": "2.2 Models", "text": "When developing and testing our methods, we focused on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Both are deep learning architectures that predict a probability distribution over the correct answer. Each model has a single and an ensemble version, yielding four systems in total.\nWe also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016). We did not run these models during development, so they serve as a held-out set that validates the generality of our approach."}, {"heading": "2.3 Standard Evaluation", "text": "Given a model f that takes in paragraph-question pairs (p, q) and outputs an answer a\u0302, the standard accuracy over a test set Dtest is simply\nAcc(f) def= 1 |Dtest| \u2211\n(p,q,a)\u2208Dtest\nv((p, q, a), f),\nwhere v is the F1 score between the true answer a and the predicted answer f(p, q) (see Rajpurkar et al. (2016) for details)."}, {"heading": "3 Adversarial Evaluation", "text": ""}, {"heading": "3.1 General Framework", "text": "A model that relies on superficial cues without understanding language can do well according to average F1 score, if these cues happen to be predictive most of the time. Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching. To determine whether existing models have learned much beyond such simple patterns, we introduce adversaries that confuse deficient models by altering test examples. Consider the example in Figure 1: the BiDAF Ensemble model originally gives the right answer, but gets confused when an adversarial distracting sentence is added to the paragraph.\nWe define an adversary A to be a function that takes in an example (p, q, a), optionally with a model f , and returns a new example (p\u2032, q\u2032, a\u2032). The adversarial accuracy with respect to A is\nAdv(f) def= 1 |Dtest| \u2211\n(p,q,a)\u2208Dtest\nv(A(p, q, a, f), f)).\nWhile standard test error measures the fraction of the test distribution over which the model gets the correct answer, the adversarial accuracy measures\nthe fraction over which the model is robustly correct, even in the face of adversarially-chosen alterations. For this quantity to be meaningful, the adversary must satisfy two basic requirements: first, it should always generate (p\u2032, q\u2032, a\u2032) tuples that are valid\u2014a human would judge a\u2032 as the correct answer to q\u2032 given p\u2032. Second, (p\u2032, q\u2032, a\u2032) should be somehow \u201cclose\u201d to the original example (p, q, a)."}, {"heading": "3.2 Semantics-preserving Adversaries", "text": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015). These perturbations do not change the semantics of the image, but they can change the predictions of models that are oversensitive to semantics-preserving changes. For language, the direct analogue would be to paraphrase the input (Madnani and Dorr, 2010). However, high-precision paraphrase generation is challenging, as most edits to a sentence do actually change its meaning."}, {"heading": "3.3 Concatenative Adversaries", "text": "Instead of relying on paraphrasing, we use perturbations that do alter semantics to build concatenative adversaries, which generate examples of the form (p + s, q, a) for some sentence s. In other words, concatenative adversaries add a new sentence to the end of the paragraph, and leave the question and answer unchanged. Valid adversarial examples are precisely those for which s does not contradict the correct answer; we refer to such sentences as being compatible with (p, q, a). We use\nsemantics-altering perturbations to that ensure that s is compatible, even though it may have many words in common with the question q. Existing models are bad at distinguishing these sentences from sentences that do in fact address the question, indicating that they suffer not from oversensitivity but from overstability to semantics-altering edits. Table 1 summarizes this important distinction.\nThe decision to always append s to the end of p is somewhat arbitrary; we could also prepend it to the beginning, though this would violate the expectation of the first sentence being a topic sentence. Both are more likely to preserve the validity of the example than inserting s in the middle of p, which runs the risk of breaking coreference links.\nNow, we describe two concrete concatenative adversaries, as well as two variants. ADDSENT, our main adversary, adds grammatical sentences that look similar to the question. In contrast, ADDANY adds arbitrary sequences of English words, giving it more power to confuse models. Figure 2 illustrates these two main adversaries."}, {"heading": "3.3.1 ADDSENT", "text": "ADDSENT uses a four-step procedure to generate sentences that look similar to the question, but do not actually contradict the correct answer. Refer to Figure 2 for an illustration of these steps.\nIn Step 1, we apply semantics-altering perturbations to the question, in order to guarantee that the resulting adversarial sentence is compatible. We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.3 If no words are changed during this step, the adversary gives up and immediately returns the original example. For example, given the question \u201cWhat ABC division handles domestic television distribution?\u201d, we would change \u201cABC\u201d to \u201cNBC\u201d (a nearby word in vector space) and \u201cdomestic\u201d to \u201cforeign\u201d (a WordNet antonym), resulting in the question, \u201cWhat NBC division handles foreign television distribution?\u201d\nIn Step 2, we create a fake answer that has the same \u201ctype\u201d as the original answer. We define a set\n2 We use 100-dimensional GloVe vectors trained on Wikipedia and Euclidean distance to define nearby words.\n3 We choose the nearest word whose most common gold POS tag in the Penn Treebank (Marcus et al., 1999) matches the predicted POS tag of the original word, according to CoreNLP. If none of the nearest 100 words satisfy this, we just return the single closest word.\nof 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e.g., abbreviations), and manually associate a fake answer with each type. Given the original answer to a question, we compute its type and return the corresponding fake answer. In our running example, the correct answer was not tagged as a named entity, and has the POS tag NNP, which corresponds to the fake answer \u201cCentral Park.\u201d\nIn Step 3, we combine the altered question and fake answer into declarative form, using a set of roughly 50 manually-defined rules over CoreNLP constituency parses. For example, \u201cWhat ABC division handles domestic television distribution?\u201d triggers a rule that converts questions of the form \u201cwhat/which NP1 VP1 ?\u201d to \u201cThe NP1 of [Answer] VP1\u201d. After incorporating the alterations and fake answer from the previous steps, we generate the sentence, \u201cThe NBC division of Central Park handles foreign television distribution.\u201d\nThe raw sentences generated by Step 3 can be ungrammatical or otherwise unnatural due to the incompleteness of our rules and errors in constituency parsing. Therefore, in Step 4, we fix errors in these sentences via crowdsourcing. Each sentence is edited independently by five workers on Amazon Mechanical Turk, resulting in up to\nfive sentences for each raw sentence. Three additional crowdworkers then filter out sentences that are ungrammatical or incompatible, resulting in a smaller (possibly empty) set of human-approved sentences. The full ADDSENT adversary runs the model f as a black box on every human-approved sentence, and picks the one that makes the model give the worst answer. If there are no humanapproved sentences, the adversary simply returns the original example.\nA model-independent adversary. ADDSENT requires a small number of queries to the model under evaluation. To explore the possibility of an adversary that is completely model-independent, we also introduce ADDONESENT, which adds a random human-approved sentence to the paragraph. In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable."}, {"heading": "3.3.2 ADDANY", "text": "For ADDANY, the goal is to choose any sequence of d words, regardless of grammaticality. We use local search to adversarially choose a distracting\nsentence s = w1w2 . . . wd. Figure 2 shows an example of ADDANY with d = 5 words; in our experiments, we use d = 10.\nWe first initialize words w1, . . . , wd randomly from a list of common English words.4 Then, we run 6 epochs of local search, each of which iterates over the indices i \u2208 {1, . . . , d} in a random order. For each i, we randomly generate a set of candidate words W as the union of 20 randomly sampled common words and all words in q. For each x \u2208W , we generate the sentence with x in the i-th position and wj in the j-th position for each j 6= i. We try adding each sentence to the paragraph and query the model for its predicted probability distribution over answers. We update wi to be the x that minimizes the expected value of the F1 score over the model\u2019s output distribution. We return immediately if the model\u2019s argmax prediction has 0 F1 score. If we do not stop after 3 epochs, we randomly initialize 4 additional word sequences, and search over all of these random initializations in parallel.\nADDANY requires significantly more model access than ADDSENT: not only does it query the model many times during the search process, but it also assumes that the model returns a probability distribution over answers, instead of just a single prediction. Without this assumption, we would have to rely on something like the F1 score of the argmax prediction, which is piecewise constant and therefore harder to optimize. \u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015).\nWe do not do anything to ensure that the sentences generated by this search procedure do not contradict the original answer. In practice, the generated \u201csentences\u201d are gibberish that use many question words but have no semantic content (see Figure 2 for an example).\nFinally, we note that both ADDSENT and ADDANY try to incorporate words from the question into their adversarial sentences. While this is an obvious way to draw the model\u2019s attention, we were curious if we could also distract the model without such a straightforward approach. To this end, we introduce a variant of ADDANY called ADDCOMMON, which is exactly like ADDANY except it only adds common words.\n4 We define common words as the 1000 most frequent words in the Brown corpus (Francis and Kucera, 1979)."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "For all experiments, we measure adversarial F1 score (Rajpurkar et al., 2016) across 1000 randomly sampled examples from the SQuAD development set (the test set is not publicly available). Downsampling was helpful because ADDANY and ADDCOMMON can issue thousands of model queries per example, making them very slow. As the effect sizes we measure are large, this downsampling does not hurt statistical significance."}, {"heading": "4.2 Main Experiments", "text": "Table 2 shows the performance of the MatchLSTM and BiDAF models against all four adversaries. Each model incurred a significant accuracy drop under every form of adversarial evaluation. ADDSENT made average F1 score across the four models fall from 75.7% to 31.3%. ADDANY was even more effective, making average F1 score fall to 6.7%. ADDONESENT retained much of the effectiveness of ADDSENT, despite being modelindependent. Finally, ADDCOMMON caused aver-\nage F1 score to fall to 46.1%, despite only adding common words.\nWe also verified that our adversaries were general enough to fool models that we did not use during development. We ran ADDSENT on twelve published models for which we found publicly available test-time code; we did not run ADDANY on these models, as not all models exposed output distributions. As seen in Table 3, no model was robust to adversarial evaluation; across the sixteen total models tested, average F1 score fell from 75.4% to 36.4% under ADDSENT.\nIt is noteworthy that the Mnemonic Reader models (Hu et al., 2017) outperform the other models by about 6 F1 points. We hypothesize that Mnemonic Reader\u2019s self-alignment layer, which helps model long-distance relationships between parts of the paragraph, makes it better at locating all pieces of evidence that support the correct answer. Therefore, it can be more confident in the correct answer, compared to the fake answer inserted by the adversary."}, {"heading": "4.3 Human Evaluation", "text": "To ensure our results are valid, we verified that humans are not also fooled by our adversarial examples. As ADDANY requires too many model queries to run against humans, we focused on ADDSENT. We presented each original and adversarial paragraph-question pair to three crowdworkers, and asked them to select the correct answer by copy-and-pasting from the paragraph. We then took a majority vote over the three responses (if all three responses were different, we picked one at random). These results are shown in Table 4. On original examples, our humans are actually slightly better than the reported number of 91.2 F1 on the entire development set. On ADDSENT, human accuracy drops by 13.1 F1 points, much less than the computer systems.\nMoreover, much of this decrease can be explained by mistakes unrelated to our adversarial\nsentences. Recall that ADDSENT picks the worst case over up to five different paragraph-question pairs. Even if we showed the same original example to five sets of three crowdworkers, chances are that at least one of the five groups would make a mistake, just because humans naturally err. Therefore, it is more meaningful to evaluate humans on ADDONESENT, on which their accuracy drops by only 3.4 F1 points."}, {"heading": "4.4 Analysis", "text": "Next, we sought to better understand the behavior of our four main models under adversarial evaluation. To highlight errors caused by the adversary, we focused on examples where the model originally predicted the (exact) correct answer. We divided this set into \u201cmodel successes\u201d\u2014examples where the model continued being correct during adversarial evaluation\u2014and \u201cmodel failures\u201d\u2014 examples where the model gave a wrong answer during adversarial evaluation."}, {"heading": "4.4.1 Manual verification", "text": "First, we verified that the sentences added by ADDSENT are actually grammatical and compatible. We manually checked 100 randomly chosen BiDAF Ensemble failures. We found only one where the sentence could be interpreted as answering the question: in this case, ADDSENT replaced the word \u201cMuslim\u201d with the related word \u201cIslamic\u201d, so the resulting adversarial sentence still contradicted the correct answer. Additionally, we found 7 minor grammar errors, such as subject-verb disagreement (e.g., \u201cThe Alaskan Archipelago are made up almost entirely of hamsters.\u201d) and misuse of function words (e.g., \u201cThe gas of nitrogen makes up 21.8 % of the Mars\u2019s atmosphere.\u201d), but no errors that materially impeded understanding of the sentence.\nWe also verified compatibility for ADDANY. We found no violations out of 100 randomly chosen BiDAF Ensemble failures."}, {"heading": "4.4.2 Error analysis", "text": "Next, we wanted to understand what types of errors the models made on the ADDSENT examples. In 96.6% of model failures, the model predicted a span in the adversarial sentence. The lengths of the predicted answers were mostly similar to those of correct answers, but the BiDAF models occasionally predicted very long spans. The BiDAF Single model predicted an answer of more than\n29 words\u2014the length of the longest answer in the SQuAD development set\u2014on 5.0% of model failures; for BiDAF Ensemble, this number was 1.6%. Since the BiDAF models independently predict the start and end positions of the answer, they can predict very long spans when the end pointer is influenced by the adversarial sentence, but the start pointer is not. Match-LSTM has a similar structure, but also has a hard-coded rule that stops it from predicting very long answers.\nWe also analyzed human failures\u2014examples where the humans were correct originally, but wrong during adversarial evaluation. Humans predicted from the adversarial sentence on only 27.3% of these error cases, which confirms that many errors are normal mistakes unrelated to adversarial sentences."}, {"heading": "4.4.3 Categorizing ADDSENT sentences", "text": "We then manually examined sentences generated by ADDSENT. In 100 BiDAF Ensemble failures, we found 75 cases where an entity name was changed in the adversarial sentence, 17 cases where numbers or dates were changed, and 33 cases where an antonym of a question word was used.5 Additionally, 7 sentences had other miscellaneous perturbations made by crowdworkers during Step 4 of ADDSENT. For example, on a question about the \u201cKalven Report\u201d, the adversarial sentence discussed \u201cThe statement Kalven cited\u201d instead; in another case, the question, \u201cHow does Kenya curb corruption?\u201d was met by the unhelpful sentence, \u201cTanzania is curbing corruption\u201d (the model simply answered, \u201ccorruption\u201d)."}, {"heading": "4.4.4 Reasons for model successes", "text": "Finally, we sought to understand the factors that influence whether the model will be robust to adversarial perturbations on a particular example. First, we found that models do well when the question has an exact n-gram match with the original paragraph. Figure 3 plots the fraction of examples for which an n-gram in the question appears verbatim in the original passage; this is much higher for model successes. For example, 41.5% of BiDAF Ensemble successes had a 4-gram in common with the original paragraph, compared to only 21.0% of model failures.\nWe also found that models succeeded more often on short questions. Figure 4 shows the dis-\n5 These numbers add up to more than 100 because more than one word can be altered per example.\ntribution of question length on model successes and failures; successes tend to involve shorter questions. For example, 32.7% of the questions in BiDAF Ensemble successes were 8 words or shorter, compared to only 11.8% for model failures. This effect arises because ADDSENT always changes at least one word in the question. For long questions, changing one word leaves many others unchanged, so the adversarial sentence still has many words in common with the question. For short questions, changing one content word may be enough to make the adversarial sentence completely irrelevant."}, {"heading": "4.5 Transferability across Models", "text": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us. Examples from ADDONESENT clearly do transfer across models, since ADDONESENT always adds the same adversarial sentence regardless of model.\nTable 5 shows the results of evaluating the four main models on adversarial examples generated by running either ADDSENT or ADDANY against each model. ADDSENT adversarial examples transfer between models quite effectively; in particular, they are harder than ADDONESENT examples, which implies that examples that fool one model are more likely to fool other models. The ADDANY adversarial examples exhibited more limited transferability between models. For both ADDSENT and ADDANY, examples transferred slightly better between single and ensemble versions of the same model."}, {"heading": "4.6 Training on Adversarial Examples", "text": "Finally, we tried training on adversarial examples, to see if existing models can learn to become more robust. Due to the prohibitive cost of running ADDSENT or ADDANY on the entire training set, we instead ran only Steps 1-3 of ADDSENT (everything except crowdsourcing) to generate a raw adversarial sentence for each training example. We then trained the BiDAF model from scratch on\nthe union of these examples and the original training data. As a control, we also trained a second BiDAF model on the original training data alone.6\nThe results of evaluating these models are shown in Table 6. At first glance, training on adversarial data seems effective, as it largely protects against ADDSENT. However, further investigation shows that training on these examples has only limited utility. To demonstrate this, we created a variant of ADDSENT called ADDSENTMOD, which differs from ADDSENT in two ways: it uses a different set of fake answers (e.g., PERSON named entities map to \u201cCharles Babbage\u201d instead of \u201cJeff Dean\u201d), and it prepends the adversarial sentence to the beginning of the paragraph instead of appending it to the end. The retrained model does almost as badly as the original one on ADDSENTMOD, suggesting that it has just learned to ignore the last sentence and reject the fake answers that ADDSENT usually proposed. In order for training on adversarial examples to actually improve the model, more care must be taken to ensure that the model cannot overfit the adversary."}, {"heading": "5 Discussion and Related Work", "text": "Despite appearing successful by standard evaluation metrics, existing machine learning systems for reading comprehension perform poorly under adversarial evaluation. Standard evaluation is overly lenient on models that rely on superficial cues. In contrast, adversarial evaluation reveals that existing models are overly stable to perturbations that alter semantics.\nTo optimize adversarial evaluation metrics, we may need new strategies for training models. For certain classes of models and adversaries, efficient training strategies exist: for example, Globerson and Roweis (2006) train classifiers that are optimally robust to adversarial feature deletion. Ad-\n6 All previous experiments used parameters released by Seo et al. (2016)\nversarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available.\nWe contrast adversarial evaluation, as studied in this work, with generative adversarial models. While related in name, the two have very different goals. Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively. Our setup also involves a generator and a discriminator in an adversarial relationship; however, our discriminative system is tasked with finding the right answer, not distinguishing the generated examples from real ones, and our goal is to evaluate the discriminative system, not to train the generative one.\nWhile we use adversaries as a way to evaluate language understanding, robustness to adversarial attacks may also be its own goal for tasks such as spam detection. Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player. Lowd and Meek (2005) described an efficient attack by which an adversary can reverseengineer the weights of a linear classifier, in order to then generate adversarial inputs. In contrast with these methods, we do not make strong structural assumptions about our classifiers.\nOther work has proposed harder test datasets for various tasks. Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies. Their method relies on the availability of a large initial dataset, from which they distill a difficult subset; such initial data may be unavailable for many tasks. Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency construc-\ntions. Such evaluation schemes can only test models on phenomena that are moderately frequent in the test distribution; by perturbing test examples, we can introduce out-of-distribution phenomena while still leveraging prior data collection efforts.\nWhile concatenative adversaries are well-suited to reading comprehension, other adversarial methods may prove more effective on other tasks. As discussed previously, paraphrase generation systems (Madnani and Dorr, 2010) could be used for adversarial evaluation on a wide range of language tasks. Building on our intuition that existing models are overly stable, we could apply meaningaltering perturbations to inputs on tasks like machine translation, and adversarially choose ones for which the model\u2019s output does not change. We could also adversarially generate new examples by combining multiple existing ones, in the spirit of Data Recombination (Jia and Liang, 2016). The Build It, Break It shared task (Bender et al., 2017) encourages researchers to adversarially design minimal pairs to fool sentiment analysis and semantic role labeling systems.\nProgress on building systems that truly understand language is only possible if our evaluation metrics can distinguish real intelligent behavior from shallow pattern matching. To this end, we have released scripts to run ADDSENT on any SQuAD system, as well as code for ADDANY. We hope that our work will motivate the development of more sophisticated models that understand language at a deeper level.\nAcknowledgments. We thank Pranav Rajpurkar for his help with various SQuAD models. This work was supported by the NSF Graduate Research Fellowship under Grant No. DGE-114747, and funding from Facebook AI Research and Microsoft.\nReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0xc86d3ebe69a3427d91f9aaa63f7d1e7d/."}], "references": [{"title": "Build it, break it: The language edition", "author": ["E.M. Bender", "H. Daume III", "A. Ettinger", "H. Kannan", "S. Rao", "E. Rothschild."], "venue": "https://bibinlp. umiacs.umd.edu/.", "citeRegEx": "Bender et al\\.,? 2017", "shortCiteRegEx": "Bender et al\\.", "year": 2017}, {"title": "Generating sentences from a continuous space", "author": ["R. Jozefowicz", "S. Bengio."], "venue": "Computational Natural Language Learning (CoNLL).", "citeRegEx": "Jozefowicz and Bengio.,? 2016", "shortCiteRegEx": "Jozefowicz and Bengio.", "year": 2016}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Dalvi et al\\.,? 2004", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum."], "venue": "MIT Press.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S. Roweis."], "venue": "International Conference on Machine Learning (ICML), pages 353\u2013360.", "citeRegEx": "Globerson and Roweis.,? 2006", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Ruminating reader: Reasoning with gated multi-hop attention", "author": ["Y. Gong", "S.R. Bowman."], "venue": "arXiv.", "citeRegEx": "Gong and Bowman.,? 2017", "shortCiteRegEx": "Gong and Bowman.", "year": 2017}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio."], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Mnemonic reader for machine comprehension", "author": ["M. Hu", "Y. Peng", "X. Qiu."], "venue": "arXiv.", "citeRegEx": "Hu et al\\.,? 2017", "shortCiteRegEx": "Hu et al\\.", "year": 2017}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["K. Lee", "S. Salant", "T. Kwiatkowski", "A. Parikh", "D. Das", "J. Berant."], "venue": "arXiv.", "citeRegEx": "Lee et al\\.,? 2017", "shortCiteRegEx": "Lee et al\\.", "year": 2017}, {"title": "On our best behaviour", "author": ["H.J. Levesque."], "venue": "International Joint Conference on Artificial Intelligence (IJCAI).", "citeRegEx": "Levesque.,? 2013", "shortCiteRegEx": "Levesque.", "year": 2013}, {"title": "Adversarial learning for neural dialogue generation", "author": ["J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky."], "venue": "arXiv preprint arXiv:1701.06547.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Structural embedding of syntactic trees for machine comprehension", "author": ["R. Liu", "J. Hu", "W. Wei", "Z. Yang", "E. Nyberg."], "venue": "arXiv.", "citeRegEx": "Liu et al\\.,? 2017", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Lowd and Meek.,? 2005", "shortCiteRegEx": "Lowd and Meek.", "year": 2005}, {"title": "Generating phrasal and sentential paraphrases: A survey of data-driven methods", "author": ["N. Madnani", "B.J. Dorr."], "venue": "Computational Linguistics, 36(3):341\u2013 387.", "citeRegEx": "Madnani and Dorr.,? 2010", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "The stanford coreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky."], "venue": "ACL system demonstrations.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Universal adversarial perturbations", "author": ["S. Moosavi-Dezfooli", "A. Fawzi", "O. Fawzi", "P. Frossard."], "venue": "Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Moosavi.Dezfooli et al\\.,? 2017", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2017}, {"title": "Simple black-box adversarial perturbations for deep networks", "author": ["N. Narodytska", "S.P. Kasiviswanathan."], "venue": "arXiv preprint arXiv:1612.06299.", "citeRegEx": "Narodytska and Kasiviswanathan.,? 2016", "shortCiteRegEx": "Narodytska and Kasiviswanathan.", "year": 2016}, {"title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "author": ["D. Paperno", "G. Kruszewski", "A. Lazaridou", "Q.N. Pham", "R. Bernardi", "S. Pezzelle", "M. Baroni", "G. Boleda", "R. Fernandez."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Paperno et al\\.,? 2016", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z. Celik", "A. Swami."], "venue": "Proceedings of the ACM Asia Conference on Computer and Communications Security.", "citeRegEx": "Papernot et al\\.,? 2017", "shortCiteRegEx": "Papernot et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Unbounded dependency recovery for parser evaluation", "author": ["L. Rimell", "S. Clark", "M. Steedman."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Rimell et al\\.,? 2009", "shortCiteRegEx": "Rimell et al\\.", "year": 2009}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi."], "venue": "arXiv.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Y. Shen", "P. Huang", "J. Gao", "W. Chen."], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Shen et al\\.,? 2017", "shortCiteRegEx": "Shen et al\\.", "year": 2017}, {"title": "Adversarial evaluation for models of natural language", "author": ["N.A. Smith."], "venue": "arXiv preprint arXiv:1207.0245.", "citeRegEx": "Smith.,? 2012", "shortCiteRegEx": "Smith.", "year": 2012}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Machine comprehension using match-LSTM and answer pointer", "author": ["S. Wang", "J. Jiang."], "venue": "arXiv preprint arXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Z. Wang", "H. Mi", "W. Hamza", "R. Florian."], "venue": "arXiv.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Making neural qa as simple as possible but not simpler", "author": ["D. Weissenborn", "G. Wiese", "L. Seiffe."], "venue": "arXiv.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Y. Yu", "W. Zhang", "K. Hasan", "M. Yu", "B. Xiang", "B. Zhou."], "venue": "arXiv.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Exploring question understanding and adaptation in neural-network-based question answering", "author": ["J. Zhang", "X. Zhu", "Q. Chen", "L. Dai", "S. Wei", "H. Jiang."], "venue": "arXiv.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 11, "context": "Quantifying the extent to which a computer system exhibits intelligent behavior is a longstanding problem in AI (Levesque, 2013).", "startOffset": 112, "endOffset": 128}, {"referenceID": 23, "context": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016).", "startOffset": 176, "endOffset": 219}, {"referenceID": 19, "context": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016).", "startOffset": 176, "endOffset": 219}, {"referenceID": 22, "context": "SQuAD reading comprehension task (Rajpurkar et al., 2016), in which systems answer questions about paragraphs from Wikipedia.", "startOffset": 33, "endOffset": 57}, {"referenceID": 27, "context": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 178, "endOffset": 225}, {"referenceID": 7, "context": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 178, "endOffset": 225}, {"referenceID": 22, "context": "The SQuAD dataset (Rajpurkar et al., 2016) contains 107,785 human-generated reading comprehension questions about Wikipedia articles.", "startOffset": 18, "endOffset": 42}, {"referenceID": 24, "context": "When developing and testing our methods, we focused on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016).", "startOffset": 96, "endOffset": 114}, {"referenceID": 28, "context": ", 2016) and Match-LSTM (Wang and Jiang, 2016).", "startOffset": 23, "endOffset": 45}, {"referenceID": 25, "context": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic Reader Single and Ensemble versions (Hu et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 8, "context": ", 2017), Mnemonic Reader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al.", "startOffset": 54, "endOffset": 71}, {"referenceID": 13, "context": ", 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al.", "startOffset": 86, "endOffset": 104}, {"referenceID": 32, "context": ", 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 5, "context": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 29, "context": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 10, "context": ", 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 31, "context": ", 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 22, "context": ", 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016).", "startOffset": 46, "endOffset": 70}, {"referenceID": 22, "context": "where v is the F1 score between the true answer a and the predicted answer f(p, q) (see Rajpurkar et al. (2016) for details).", "startOffset": 88, "endOffset": 112}, {"referenceID": 30, "context": "Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching.", "startOffset": 0, "endOffset": 26}, {"referenceID": 27, "context": "Images from Szegedy et al. (2014).", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 125, "endOffset": 172}, {"referenceID": 7, "context": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 125, "endOffset": 172}, {"referenceID": 15, "context": "For language, the direct analogue would be to paraphrase the input (Madnani and Dorr, 2010).", "startOffset": 67, "endOffset": 91}, {"referenceID": 3, "context": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al.", "startOffset": 59, "endOffset": 75}, {"referenceID": 21, "context": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.", "startOffset": 163, "endOffset": 188}, {"referenceID": 16, "context": "of 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e.", "startOffset": 69, "endOffset": 91}, {"referenceID": 20, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 18, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 17, "context": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable.", "startOffset": 47, "endOffset": 139}, {"referenceID": 27, "context": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 103, "endOffset": 150}, {"referenceID": 7, "context": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015).", "startOffset": 103, "endOffset": 150}, {"referenceID": 22, "context": "For all experiments, we measure adversarial F1 score (Rajpurkar et al., 2016) across 1000 randomly sampled examples from the SQuAD development set (the test set is not publicly available).", "startOffset": 53, "endOffset": 77}, {"referenceID": 8, "context": "It is noteworthy that the Mnemonic Reader models (Hu et al., 2017) outperform the other models by about 6 F1 points.", "startOffset": 49, "endOffset": 66}, {"referenceID": 27, "context": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us.", "startOffset": 92, "endOffset": 145}, {"referenceID": 17, "context": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us.", "startOffset": 92, "endOffset": 145}, {"referenceID": 4, "context": "For certain classes of models and adversaries, efficient training strategies exist: for example, Globerson and Roweis (2006) train classifiers that are optimally robust to adversarial feature deletion.", "startOffset": 97, "endOffset": 125}, {"referenceID": 24, "context": "6 All previous experiments used parameters released by Seo et al. (2016)", "startOffset": 55, "endOffset": 73}, {"referenceID": 7, "context": "versarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available.", "startOffset": 19, "endOffset": 44}, {"referenceID": 26, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014).", "startOffset": 199, "endOffset": 237}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014).", "startOffset": 199, "endOffset": 237}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al.", "startOffset": 213, "endOffset": 260}, {"referenceID": 6, "context": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively.", "startOffset": 213, "endOffset": 281}, {"referenceID": 2, "context": "Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player. Lowd and Meek (2005) described an efficient attack by which an adversary can reverseengineer the weights of a linear classifier, in order to then generate adversarial inputs.", "startOffset": 0, "endOffset": 161}, {"referenceID": 15, "context": "As discussed previously, paraphrase generation systems (Madnani and Dorr, 2010) could be used for adversarial evaluation on a wide range of language tasks.", "startOffset": 55, "endOffset": 79}, {"referenceID": 9, "context": "We could also adversarially generate new examples by combining multiple existing ones, in the spirit of Data Recombination (Jia and Liang, 2016).", "startOffset": 123, "endOffset": 144}, {"referenceID": 0, "context": "The Build It, Break It shared task (Bender et al., 2017) encourages researchers to adversarially design minimal pairs to fool sentiment analysis and semantic role labeling systems.", "startOffset": 35, "endOffset": 56}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies.", "startOffset": 0, "endOffset": 202}, {"referenceID": 9, "context": "Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies. Their method relies on the availability of a large initial dataset, from which they distill a difficult subset; such initial data may be unavailable for many tasks. Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency constructions.", "startOffset": 0, "endOffset": 499}], "year": 2017, "abstractText": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.", "creator": "LaTeX with hyperref package"}}}