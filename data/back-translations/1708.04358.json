{"id": "1708.04358", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks", "abstract": "We propose a method for embedding two-dimensional places in a continuous vector space using a neural network-based model with mixtures of Gaussian distributions, and present two model variants for text-based geolocalization and lexical dialectology. Evaluated using Twitter data, the proposed model surpasses traditional regression-based geolocalization and provides a better estimate of uncertainty. We also demonstrate the effectiveness of representation for predicting words from locations in lexical dialectology and evaluate them using the DARE dataset.", "histories": [["v1", "Mon, 14 Aug 2017 23:52:02 GMT  (3072kb,D)", "http://arxiv.org/abs/1708.04358v1", "Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) September 2017, Copenhagen, Denmark"]], "COMMENTS": "Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) September 2017, Copenhagen, Denmark", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["afshin rahimi", "timothy baldwin", "trevor cohn"], "accepted": true, "id": "1708.04358"}, "pdf": {"name": "1708.04358.pdf", "metadata": {"source": "META", "title": "Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks", "authors": ["Afshin Rahimi", "Timothy Baldwin", "Trevor Cohn"], "emails": ["arahimi@student.unimelb.edu.au", "tbaldwin@unimelb.edu.au", "t.cohn@unimelb.edu.au"], "sections": [{"heading": null, "text": "dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset."}, {"heading": "1 Introduction", "text": "Geolocation is an essential component of applications such as traffic monitoring (Emadi et al., 2017), human mobility pattern analysis (McNeill et al., 2016; Dredze et al., 2016) and disaster response (Ashktorab et al., 2014; Wakamiya et al., 2016), as well as targeted advertising (Anagnostopoulos et al., 2016) and local recommender systems (Ho et al., 2012). Although Twitter provides users with the means to geotag their messages, less than 1% of users opt to turn on geotagging, so thirdparty service providers tend to use profile data, text content and network information to infer the location of users. Text content is the most widely used source of geolocation information, due to its prevalence across social media services.\nText-based geolocation systems use the geographical bias of language to infer the location of a user or message using models trained on geotagged posts. The models often use a representation of text (e.g. based on a bag-of-words, convolutional or recurrent model) to predict the location either in real-valued latitude/longitude coordinate space or\nin discretised region-based space, using regression or classification, respectively. Regression models, as a consequence of minimising squared loss for a unimodal distribution, predict inputs with multiple targets to lie between the targets (e.g. a user who mentions content in both NYC and LA is predicted to be in the centre of the U.S.). Classification models, while eliminating this problem by predicting a more granular target, don\u2019t provide fine-grained predictions (e.g. specific locations in NYC), and also require heuristic discretisation of locations into regions (e.g. using clustering).\nMixture Density Networks (\u201cMDNs\u201d: Bishop (1994)) alleviate these problems by representing location as a mixture of Gaussian distributions. Given a text input, an MDN can generate a mixture model in the form of a probability distribution over all location points. In the example of a user who talks about both NYC and LA, e.g., the model will predict a strong Gaussian component in NYC and another one in LA, and also provide an estimate of uncertainty over all the coordinate space.\nAlthough MDNs are not new, they have not found widespread use in inverse regression problems where for a single input, multiple correct outputs are possible. Given the integration of NLP technologies into devices (e.g. phones or robots with natural language interfaces) is growing quickly, there is a potential need for interfacing language with continuous variables as input or target. MDNs can also be used in general text regression problems such as risk assessment (Wang and Hua, 2014), sentiment analysis (Joshi et al., 2010) and loan rate prediction (Bitvai and Cohn, 2015), not only to improve prediction but also to use the mixture model as a representation for the continuous variables. We apply MDNs to geotagged Twitter data in two different settings: (a) predicting location given text; and (b) predicting text given location. ar X iv :1\n70 8.\n04 35\n8v 1\n[ cs\n.C L\n] 1\n4 A\nug 2\n01 7\nGeotagged text content is not only useful in geolocation, but can also be used in lexical dialectology. Lexical dialectology is (in part) the converse of text-based geolocation (Eisenstein, 2015): instead of predicting location from language, language (e.g. dialect terms) are predicted from a given location. This is a much more challenging task as the lexical items are not known beforehand, and there is no notion of dialect regions in the continuous space of latitude/longitude coordinates. A lexical dialectology model should not only be able to predict dialect terms but also be able to automatically learn dialect regions.\nIn this work, we use bivariate Gaussian mixtures over geotagged Twitter data in two different settings, and demonstrate their use for geolocation and lexical dialectology. Our contributions are as follows: (1) we propose a continuous representation of location using bivariate Gaussian mixtures; (2) we show that our geolocation model outperforms regression-based models and achieves comparable results with classification models, but with added uncertainty over the continuous output space; (3) we show that our lexical dialectology model is able to predict geographical dialect terms from latitude/longitude input with state-of-the-art accuracy; and (4) we show that the automatically learned Gaussian regions match expert-generated dialect regions of the U.S.1"}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Text-based Geolocation", "text": "Text-based geolocation models are defined as either a regression or a classification problem. In regression geolocation, the model learns to predict a real-valued latitude/longitude from a text input. This is a very challenging task for data types such as Twitter, as they are often heavily biased toward population centres and urban areas, and far from uniform. As an example, Norwalk is the name of a few cities in the U.S among which Norwalk, California (West Coast) and Norwalk, Connecticut (East Coast) are the two most populous cities. Assuming that occurrences of the city\u2019s name are almost equal in both city regions within the training set, a trained regression-based geolocation model given Norwalk as input, would geolocate it to a point in the middle of the U.S. instead of choosing one of the cities. In the machine learning literature,\n1Code available at https://github.com/afshinrahimi/geomdn\nregression problems where there are multiple realvalued outputs for a given input are called inverse problems (Bishop, 1994). Here, standard regression models predict an average point in the middle of all training target points to minimise squared error loss. Bishop (1994) proposes density mixture networks to model such inverse problems, as we discuss in detail in Section 3.\nIn addition, non-Bayesian interpretations of regression models, which are often used in practice, don\u2019t produce any prediction of uncertainty, so other than the predicted point, we have little idea where else the term could have high or low probability. Priedhorsky et al. (2014) propose a Gaussian Mixture Model (GMM) approach instead of squared loss regression, whereby they learn a mixture of bivariate Gaussian distributions for each individual n-gram in the training set. During prediction, they add the Gaussian mixture of each ngram in the input text, resulting in a new Gaussian mixture which can be used to predict a coordinate with associated uncertainty. To add the mixture components they use a weighted sum, where the weight of each n-gram is assigned by several heuristic features. Learning a GMM for each n-gram is resource-intensive if the size of the training set \u2014 and thus the number of n-grams \u2014 is large.\nAssuming sufficient training samples containing the term Norwalk in the two main, a trained classification model would, given this term as input, predict a probability distribution over all regions, and assign higher probabilities to the regions containing the two major cities. The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data\nsparseness."}, {"heading": "2.2 Lexical Dialectology", "text": "The traditional linguistic approach to lexical dialectology is to find the geographical distributions of known contrast sets such as {you, yall, yinz}: (Labov et al., 2005; Nerbonne et al., 2008; Gonc\u0327alves and Sa\u0301nchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, to appear). This usually involves surveying a large geographically-uniform sample of people from different locations and analysing where each known alternative is used more frequently. Then, the coordinates are clustered heuristically into dialect regions, based on the lexical choices of users in each region relative to the contrast set. This processing is very costly and time-consuming, and relies critically on knowing the lexical alternatives a priori. For example, it would require a priori knowledge of the fact that people in different regions of the US use pop and soda to refer to the same type of drink, and a posteriori analysis of the empirical geographical distribution of the different terms. Language, particularly in social media and among younger speakers, is evolving so quickly, in ways that can be measured over large-scale data samples such as Twitter, that we ideally want to be able to infer such contrast sets dynamically. The first step in automatically collecting dialect words is to find terms that are disproportionately distributed in different locations. The two predominant approaches to this problem are model-based (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al., 2008; Cook et al., 2014). Model-based approaches use a topic model, e.g., to extract region-specific topics, and from this, predict the probability of seeing a word given a geographical region (Eisenstein et al., 2010). However, there are scalability issues, limiting the utility of such models.\nIn this paper, we propose a neural network architecture that learns a mixture of Gaussian distributions as its activation function, and predicts both locations from word-based inputs (geolocation), and words from location-based inputs (lexical dialectology)."}, {"heading": "3 Model", "text": ""}, {"heading": "3.1 Bivariate Gaussian Distribution", "text": "A bivariate Gaussian distribution is a probability distribution over 2d space (in our case, a lat-\nitude/longitude coordinate pair). The probability mass function is given by:\nN (x|\u00b5,\u03a3) = 1\n(2\u03c0)\n1\n|\u03a3|1/2\nexp\n{\n\u2212 1\n2 (x\u2212 \u00b5)\u22ba\u03a3\u22121(x\u2212 \u00b5)\n}\nwhere \u00b5 is the 2-dimensional mean vector, the matrix \u03a3 = ( \u03c31 2 \u03c112\u03c31\u03c32\n\u03c112\u03c31\u03c32 \u03c32 2\n)\nis the covariance ma-\ntrix, and |\u03a3| is its determinant. \u03c31 and \u03c32 are the standard deviations of the two dimensions, and \u03c112 is the covariance. x is a latitude/longitude coordinate whose probability we are seeking to predict."}, {"heading": "3.2 Mixtures of Gaussians", "text": "A mixture of Gaussians is a probabilistic model to represent subpopulations within a global population in the form of a weighted sum of K Gaussian distributions, where a higher weight with a component Gaussian indicates stronger association with that component. The probability mass function of a Gaussian mixture model is given by:\nP(x) = K \u2211\nk=1\n\u03c0kN (x|\u00b5k,\u03a3k)\nwhere \u2211K\nk=1 \u03c0k = 1, and the number of components K is a hyper-parameter.\n3.3 Mixture Density Network (MDN)\nA mixture density network (\u201cMDN\u201d: Bishop (1994)) is a latent variable model where the conditional probability of p(y|x) is modelled as a mixture of K Gaussians where the mixing coefficients \u03c0 and the parameters of Gaussian distributions \u00b5 and \u03a3 are computed as a function of input using a neural network:\nP(y|x) = K \u2211\nk=1\n\u03c0k(x)N ( y|\u00b5k(x),\u03a3k(x) )\nIn the bivariate case of latitude/longitude, the number of parameters of each Gaussian is 6 (\u03c0k(x), \u00b51k(x), \u00b52k(x), \u03c1k(x),\u03c31k(x),\u03c32k(x)), which are learnt in the output layer of a regular neural network as a function of input x. The output size of the network for K components would be 6 \u00d7 K. The output of an MDN for N samples (e.g. where N is the mini-batch size) is an N \u00d7 6K matrix which is then sliced and reshaped into (N \u00d7 2 \u00d7 K), (N \u00d7 2 \u00d7 K), (N \u00d7 1 \u00d7 K)\nand (N \u00d7 1 \u00d7 K) matrices, providing the model parameters \u00b5, \u03c3, \u03c1 and \u03c0. Each parameter type has its own constraints: \u03c3 (the standard deviation) should be positive, \u03c1 (the correlation) should be in the interval [\u22121, 1] and \u03c0 should be positive and sum to one, as a probability distribution. To force these constraints, the following transformations are often applied to each parameter set:\n\u03c3 \u223c SoftPlus(\u03c3\u2032) = log(exp(\u03c3\u2032) + 1) \u2208 (0,+\u221e)\n\u03c0 \u223c SoftMax(\u03c0\u2032)\n\u03c1 \u223c SoftSign(\u03c1\u2032) = \u03c1\u2032\n1 + |\u03c1\u2032| \u2208 [\u22121, 1]\nAs an alternative, it\u2019s possible to use transformations like exp for \u03c3 and tanh for \u03c1. After applying the transformations to enforce the range constraints, the negative log likelihood loss of each sample x given a 2d coordinate label y is computed as:\nL(y|x) = \u2212 log\n{ K \u2211\nk=1\n\u03c0k(x)N ( y|\u00b5k(x),\u03a3k(x) )\n}\nTo predict a location, given an unseen input, the output of the network is reshaped into a mixture of Gaussians and \u00b5k, one of the K components\u2019 \u00b5 is chosen as the prediction. The selection criteria is either based on the strongest component with highest \u03c0, or the component that maximises the overall mixture probability:\nmax \u00b5i\u2208{\u00b51...\u00b5K}\nK \u2211\nk=1\n\u03c0kN (\u00b5i|\u00b5k,\u03a3k)\nFor further details on selection criteria, see Bishop (1994)."}, {"heading": "3.4 Mixture Density Network with Shared", "text": "Parameters (MDN-SHARED)\nIn the original MDN model proposed by Bishop (1994), the parameters of the mixture model are separate functions of input, which is appropriate when the inputs and outputs directly relate to each other, but in the case of geolocation or lexical dialectology, the relationship between inputs and outputs is not so obvious. As a result, it might be a difficult task for the model to learn all the parameters of each sample correctly. Instead of using the output to predict all the parameters, we share \u00b5 and \u03a3 among all samples as parameters of the output layer, and only use the input to predict \u03c0, the mixture probabilities, using a SoftMax layer. We\ninitialise \u00b5 by applying K-means clustering to the training coordinates and setting each value of \u00b5 to the centroids of the K clusters; we initialise \u03a3 randomly between 0 and 10. We use the original cost function to update the weight matrices, biases and the global shared parameters of the mixture model through backpropagation. Prediction is performed in the same way as for MDN."}, {"heading": "3.5 Continuous Representation of Location", "text": "Gaussian mixtures are usually used as the output layer in neural networks (as in MDN) for inverse regression problems. We extend their application by using them as an input representation when the input is a multidimensional continuous variable. In problems such as lexical dialectology, the input is real-valued 2d coordinates, and the goal is to predict dialect words from a given location. Small differences in latitude/longitude may result in big shifts in language use (e.g. in regions such as Switzerland or Gibraltar). One way to model this is to discretise the input space (similar to the discretisation of the output space in classification), with the significant downside that the model is not able to learn/fine-tune regions in a data-driven way. A better solution is to use a K component Gaussian mixture representation of location, where \u00b5 and \u03a3 are shared among all samples, and the output of the layer is the probability of input in each of the mixture components. Note that in this representation, there is no need for \u03c0 parameters as we just need to represent the association of an input location to K regions, which will then be used as input to the next layer of a neural network and used to predict the targets. We use this continuous representation of location to predict dialect words from location input."}, {"heading": "4 Experiments", "text": "We apply the two described MDN models on two widely-used geotagged Twitter datasets for geolocation, and compare the results with state-of-the-art classification and regression baselines. Also, we use the mixture of Gaussian representation of location to predict dialect terms from coordinates."}, {"heading": "4.1 Data", "text": "In our experiments, we use two existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), and (2) TWITTER-US (Roller et al., 2012). Each dataset has fixed training, devel-\nopment and test partitions, and a user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet.2 GEOTEXT and TWITTER-US cover the continental US with 9k, 449k users, respectively.3\nDARE is a dialect-term dataset derived from the Dictionary of American Regional English (Cassidy, 1985) by Rahimi et al. (to appear). DARE consists of dialect regions, terms and the meaning of each term.4 It represents the aggregation of a number of dialectal surveys over different regions of the U.S., to identify shared dialect regions. Because the dialect regions in DARE maps are not machine readable, populous cities within each dialect region are manually extracted and associated with their dialect region terms. The dataset is made up of around 4.3k dialect terms across 99 U.S. dialect regions.\n2This geolocation representation is naive, but was made by the creators of the original datasets and has been used by others. It has been preserved in this work for comparability with the results of others, despite misgivings about whether this is a faithful representation of the location for a given user.\n3The datasets can be obtained from https://github. com/utcompling/textgrounder.\n4http://www.daredictionary.com/"}, {"heading": "4.2 Geolocation", "text": "We use a 3-layer neural network as shown in Figure 1a where the input is the l2 normalised bag-ofwords model of a given user with stop words, @- mentions and words with document frequency less than 10 removed. The input is fed to a hidden layer with tanh nonlinearity that produces the output of the network (with no nonlinearity applied). The output is the collection of Gaussian mixture parameters (\u00b5,\u03a3,\u03c0) from MDN. For prediction, the \u00b5k of the mixture component which has the highest probability within the mixture component is selected. In the case of MDN-SHARED, the output is only \u03c0, a vector with size K, but the output layer contains extra global parameters \u00b5 and \u03a3 (\u03c3lat,\u03c3lon, \u03c1) which are shared between all the samples. The negative log likelihood objective is optimised using Adam (Kingma and Ba, 2014) and early stopping is used to prevent overfitting. The hidden layer is subject to drop-out and elastic net regularisation (with equal l1 and l2 shares). As our baseline, we used a multilayer perceptron regressor with the same input and hidden architecture but with a 2d output with linear activation that predicts the location from text input. The regularisation and drop-out rate, hidden layer size and the number of Gaussian components K (for MDN and MDN-SHARED) are tuned over the development set of each dataset, as shown in Table 1.\nWe evaluate the predictions of the geolocation models based on three measures (following Cheng et al. (2010) and Eisenstein et al. (2010)):\n1. the classification accuracy within a 161km\n(= 100 mile) radius of the actual location (\u201cAcc@161\u201d); i.e., if the predicted location is within 161km of the actual location, it is considered to be correct 2. the mean error (\u201cMean\u201d) between the pre-\ndicted location and the actual location of the user, in kilometres 3. the median error (\u201cMedian\u201d) between the pre-\ndicted location and the actual location of the user, in kilometres"}, {"heading": "4.3 Lexical Dialectology", "text": "To predict dialect words from location, we use a 4-layer neural network as shown in Figure 1b. The input is a latitude/longitude coordinate, the first hidden layer is a Gaussian mixture with K components which has \u00b5 and \u03a3 as its parameters and produces a probability for each component as an\nactivation function, the second hidden layer with tanh nonlinearity captures the association between different Gaussians, and the output is a SoftMax layer which results in a probability distribution over the vocabulary. For a user label, we use an l1 normalised bag-of-words representation of its text content and use binary tf and idf for term-weighting. The model should learn to predict the probability distribution over the vocabulary and so be capable of predicting dialect words with a higher probability. It also learns regions (parameters of K Gaussians) that represent dialect regions.\nWe evaluate the lexical dialectology model (MDN-layer) using perplexity of the predicted unigram distribution, and compare it with a baseline where the Gaussian mixture layer is replaced with a tanh hidden layer (tanh-layer). Also we retrieve words given points within a region from the DAREDS dataset, and measure recall with respect to relevant dialect terms from DAREDS. To do that, we randomly sample P = 10000 latitude/longitude points from the training set and predict the corresponding word distribution. To come up with a ranking over words given region r as query, we use the following measure:\nscore(wi|r) = 1\nN\n\u2211\npj\u2208r\nlog(P (wi|pj))\n\u2212 1\nP\nP \u2211\nj=1\nlog(P (wi|pj))\nwhere N equals the number of points (out of 10000) inside the query dialect region r and P equals the total number of points (here 10000). For example, if we are querying dialect terms from dialect region South (r), N is the number of randomly selected points that fall within the constituent states of South. score(wi|r) measures the (log) probability ratio of a word wi inside region r compared to its global score: if a word is local to region r, the ratio will be higher. We use this measure to create a ranking over the vocabulary from which we measure precision and recall at k given gold-standard dialect terms in DAREDS."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Geolocation", "text": "The performance of Regression, MDN and MDN-SHARED, along with several state-of-the-art classification models, is shown in Table 2. The\nMDN and MDN-SHARED models clearly outperform Regression, and achieve competitive or slightly worse results than the classification models but provide uncertainty over the whole output space. The geographical distribution of error for MDN-SHARED over the development set of TWITTER-US is shown in Figure 3, indicating larger errors in MidWest and particularly in North Pacific regions (e.g. Oregon)."}, {"heading": "5.2 Dialectology", "text": "The perplexity of the lexical dialectology model using Gaussian mixture representation (MDN-layer) is 840 for the 54k vocabulary of TWITTER-US dataset, 1% lower than a similar network architecture with a tanh hidden layer (tanh-layer), which is not a significant improvement. Also we evaluated the model using recall at k and compared it to the tanh-layer model which again is competitive with tanh-layer but with the advantage of learning dialect regions simultaneously. Because the DARE dialect terms are not used frequently in Twitter, many of the words are not covered in our dataset, despite its size. However, our model is able to retrieve dialect terms that are distinctly associated with regions. The top dialect words for regions New York, Louisiana, Illinois and Pennsylvania are shown in Table 3, and include named entities, dialect words and local hashtags. We also visualised the learned Gaussians of the dialectology model in Figure 2, which as expected show several smaller regions (Gaussians with higher \u03c3) and larger regions in lower populated areas. It is interesting to see that the shape of the learned Gaussians matches natural borders such as coastal regions.\nWe also visualised the log probability of dialect terms hella (an intensifier mainly used in Northern California) and yall (means \u201cyou all\u201d, used in Southern U.S.) resulting from the Gaussian representation model. As shown in Figure 5, the heatmap matches the expected regions."}, {"heading": "6 Conclusion", "text": "We proposed a continuous representation of location using mixture of Gaussians and applied it to geotagged Twitter data in two different settings: (1) geolocation of social media users, and (2) lexical dialectology. We used MDN (Bishop, 1994) in a multilayer neural network as a geolocation model\nGEOTEXT TWITTER-US\nregul. dropout hidden K regul. dropout hidden K\nBaseline (Regression) 0 0 (100, 50) \u2014 10\u22125 0 (100, 50) \u2014 Proposed method (MDN) 0 0.5 100 100 10\u22125 0 300 100 Proposed method (MDN-SHARED) 0 0 100 300 0 0 900 900\nCLASSIFICATION METHODS\nand showed that it outperforms regression models by a large margin. There is also very recent work (Iso et al., 2017) in tweet-level geolocation that shows the effectiveness of MDN.\nWe modified MDN by sharing the parameters of\nthe Gaussian mixtures in MDN-SHARED and improved upon MDN, achieving competetive results with state-of-the-art classification models. We also applied the Gaussian mixture representation to predict dialect words from location, and showed that it\n(a) hella (an intensifier) mostly used in Northern California, also the name of a company in Illinois.\nis competitive with simple tanh activation in terms of both perplexity of the predicted unigram model and also recall at k at retrieving DARE dialect words by location input. Furthermore we showed that the learned Gaussian mixtures have intereting properties such as covering high population density regions (e.g. NYC and LA) with a larger number of small Gaussians, and a smaller number of larger\nGaussians in low density areas (e.g. the midwest).\nAlthough we applied the mixture of Gaussians to location data, it can be used in other settings where the input or output are from a continuous multivariate distribution. For example it can be applied to predict financial risk (Wang and Hua, 2014) and sentiment (Joshi et al., 2010) given text. We showed that when a global structure exists (e.g. population centres, in the case of geolocation) it is better to share the global parameters of the mixture model to improve generalisation. In this work, we used the bivariate Gaussian distribution in the MDN\u2019s mixture leaving the use of other distributions which might better suit the geolocation task for future research."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the Australian Research Council."}], "references": [{"title": "Hierarchical geographical modeling of user locations from social media posts", "author": ["Amr Ahmed", "Liangjie Hong", "Alexander J Smola."], "venue": "Proceedings of the 22nd International Conference on World Wide Web (WWW 2013), pages 25\u201336, Rio de Janeiro,", "citeRegEx": "Ahmed et al\\.,? 2013", "shortCiteRegEx": "Ahmed et al\\.", "year": 2013}, {"title": "Targeted interest-driven advertising in cities using twitter", "author": ["Aris Anagnostopoulos", "Fabio Petroni", "Mara Sorella."], "venue": "Proceedings of the 10th International Conference on Weblogs and Social Me-", "citeRegEx": "Anagnostopoulos et al\\.,? 2016", "shortCiteRegEx": "Anagnostopoulos et al\\.", "year": 2016}, {"title": "Tweedr: Mining Twitter to inform disaster response", "author": ["Zahra Ashktorab", "Christopher Brown", "Manojit Nandi", "Aron Culotta."], "venue": "Proceedings of The 11th International Conference on Information Systems for Crisis Response and Management", "citeRegEx": "Ashktorab et al\\.,? 2014", "shortCiteRegEx": "Ashktorab et al\\.", "year": 2014}, {"title": "Mixture density networks", "author": ["Christopher Bishop."], "venue": "Technical report, Aston University.", "citeRegEx": "Bishop.,? 1994", "shortCiteRegEx": "Bishop.", "year": 1994}, {"title": "Predicting peerto-peer loan rates using bayesian non-linear regression", "author": ["Zsolt Bitvai", "Trevor Cohn."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15), pages 2203\u20132209, Austin, USA.", "citeRegEx": "Bitvai and Cohn.,? 2015", "shortCiteRegEx": "Bitvai and Cohn.", "year": 2015}, {"title": "Dictionary of American Regional English", "author": ["Frederic Gomes Cassidy."], "venue": "Belknap Press of Harvard University Press.", "citeRegEx": "Cassidy.,? 1985", "shortCiteRegEx": "Cassidy.", "year": 1985}, {"title": "Twitter geolocation and regional classification via sparse coding", "author": ["Miriam Cha", "Youngjune Gwon", "H.T. Kung."], "venue": "Proceedings of the 9th International Conference on Weblogs and Social Media (ICWSM 2015), pages 582\u2013585, Oxford, UK.", "citeRegEx": "Cha et al\\.,? 2015", "shortCiteRegEx": "Cha et al\\.", "year": 2015}, {"title": "You are where you tweet: a content-based approach to geo-locating Twitter users", "author": ["Zhiyuan Cheng", "James Caverlee", "Kyumin Lee."], "venue": "Proceedings of the 19th ACM International Conference Information and Knowledge Management (CIKM 2010),", "citeRegEx": "Cheng et al\\.,? 2010", "shortCiteRegEx": "Cheng et al\\.", "year": 2010}, {"title": "Statistical methods for identifying local dialectal terms from gps-tagged documents", "author": ["Paul Cook", "Bo Han", "Timothy Baldwin."], "venue": "Dictionaries: Journal of the Dictionary Society of North America, 35(35):248\u2013271.", "citeRegEx": "Cook et al\\.,? 2014", "shortCiteRegEx": "Cook et al\\.", "year": 2014}, {"title": "Mapping dialectal variation by querying social media", "author": ["Gabriel Doyle."], "venue": "Proceedings of the 14th Conference of the EACL (EACL 2014), pages 98\u2013 106, Gothenburg, Sweden.", "citeRegEx": "Doyle.,? 2014", "shortCiteRegEx": "Doyle.", "year": 2014}, {"title": "Twitter as a source of global mobility patterns for social good", "author": ["Mark Dredze", "Manuel Garc\u0131\u0301a-Herranz", "Alex Rutherford", "Gideon Mann"], "venue": "In ICML Workshop on #Data4Good: Machine Learning in Social Good Applications,", "citeRegEx": "Dredze et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2016}, {"title": "Written dialect variation in online social media", "author": ["Jacob Eisenstein."], "venue": "Charles Boberg, John Nerbonne, and Dom Watt, editors, Handbook of Dialectology. Wiley.", "citeRegEx": "Eisenstein.,? 2015", "shortCiteRegEx": "Eisenstein.", "year": 2015}, {"title": "A latent variable model for geographic lexical variation", "author": ["Jacob Eisenstein", "Brendan O\u2019Connor", "Noah A Smith", "Eric P Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Eisenstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2010}, {"title": "QT2S: A system for monitoring road traffic via fine grounding of tweets", "author": ["Noora Al Emadi", "Sofiane Abbar", "Javier BorgeHolthoefer", "Francisco Guzman", "Fabrizio Sebastiani."], "venue": "Proceedings of the 11th International Conference on Weblogs and", "citeRegEx": "Emadi et al\\.,? 2017", "shortCiteRegEx": "Emadi et al\\.", "year": 2017}, {"title": "Crowdsourcing dialect characterization through Twitter", "author": ["Bruno Gon\u00e7alves", "David S\u00e1nchez."], "venue": "PloS One, 9(11).", "citeRegEx": "Gon\u00e7alves and S\u00e1nchez.,? 2014", "shortCiteRegEx": "Gon\u00e7alves and S\u00e1nchez.", "year": 2014}, {"title": "Geolocation prediction in social media data by finding location indicative words", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin."], "venue": "Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1045\u2013", "citeRegEx": "Han et al\\.,? 2012", "shortCiteRegEx": "Han et al\\.", "year": 2012}, {"title": "Textbased Twitter user geolocation prediction", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin."], "venue": "Journal of Artificial Intelligence Research, 49:451\u2013500.", "citeRegEx": "Han et al\\.,? 2014", "shortCiteRegEx": "Han et al\\.", "year": 2014}, {"title": "Tweets from justin bieber\u2019s heart: the dynamics of the location field in user profiles", "author": ["Brent Hecht", "Lichan Hong", "Bongwon Suh", "Ed H. Chi."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 237\u2013246, Van-", "citeRegEx": "Hecht et al\\.,? 2011", "shortCiteRegEx": "Hecht et al\\.", "year": 2011}, {"title": "Mining future spatiotemporal events and their sentiment from online news articles for location-aware recommendation system", "author": ["Shen-Shyang Ho", "Mike Lieberman", "Pu Wang", "Hanan Samet."], "venue": "Proceedings of the First ACM SIGSPATIAL Interna-", "citeRegEx": "Ho et al\\.,? 2012", "shortCiteRegEx": "Ho et al\\.", "year": 2012}, {"title": "Understanding US regional linguistic variation with Twitter data analysis", "author": ["Yuan Huang", "Diansheng Guo", "Alice Kasakoff", "Jack Grieve."], "venue": "Computers, Environment and Urban Systems, 59:244\u2013255.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Density estimation for geolocation via convolutional mixture density network", "author": ["Hayate Iso", "Shoko Wakamiya", "Eiji Aramaki."], "venue": "arXiv preprint arXiv:1705.02750.", "citeRegEx": "Iso et al\\.,? 2017", "shortCiteRegEx": "Iso et al\\.", "year": 2017}, {"title": "Movie reviews and revenues: An experiment in text regression", "author": ["Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A Smith."], "venue": "Proceedings of The 2010 Annual Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Joshi et al\\.,? 2010", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "I\u2019m eating a sandwich in Glasgow\u201d: Modeling locations with tweets", "author": ["Sheila Kinsella", "Vanessa Murdock", "Neil O\u2019Hare"], "venue": "In Proceedings of the 3rd International Workshop on Search and Mining Usergenerated Contents,", "citeRegEx": "Kinsella et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kinsella et al\\.", "year": 2011}, {"title": "The Atlas of North American English: Phonetics, Phonology and Sound Change", "author": ["William Labov", "Sharon Ash", "Charles Boberg."], "venue": "Walter de Gruyter.", "citeRegEx": "Labov et al\\.,? 2005", "shortCiteRegEx": "Labov et al\\.", "year": 2005}, {"title": "Estimating user location in social media with stacked denoising autoencoders", "author": ["Ji Liu", "Diana Inkpen."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2014 Human Language", "citeRegEx": "Liu and Inkpen.,? 2015", "shortCiteRegEx": "Liu and Inkpen.", "year": 2015}, {"title": "Estimating local commuting patterns from geolocated Twitter data", "author": ["Graham McNeill", "Jonathan Bright", "Scott A Hale."], "venue": "arXiv preprint arXiv:1612.01785.", "citeRegEx": "McNeill et al\\.,? 2016", "shortCiteRegEx": "McNeill et al\\.", "year": 2016}, {"title": "Geocoding textual documents through the usage of hierarchical classifiers", "author": ["Fernando Melo", "Bruno Martins."], "venue": "Proceedings of the 9th Workshop on Geographic Information Retrieval (GIR 2015), Paris, France.", "citeRegEx": "Melo and Martins.,? 2015", "shortCiteRegEx": "Melo and Martins.", "year": 2015}, {"title": "Fightin\u2019words: Lexical feature selection and evaluation for identifying the content of political conflict", "author": ["Burt L. Monroe", "Michael P. Colaresi", "Kevin M. Quinn."], "venue": "Political Analysis, 16(4):372\u2013403.", "citeRegEx": "Monroe et al\\.,? 2008", "shortCiteRegEx": "Monroe et al\\.", "year": 2008}, {"title": "Projecting dialect distances to geography: Bootstrap clustering vs", "author": ["John Nerbonne", "Peter Kleiweg", "Wilbert Heeringa", "Franz Manni."], "venue": "noisy clustering. In Proceedings of the 31st Annual Meeting of the German Classification Society, pages 647\u2013654.", "citeRegEx": "Nerbonne et al\\.,? 2008", "shortCiteRegEx": "Nerbonne et al\\.", "year": 2008}, {"title": "Inferring the origin locations of tweets with quantitative confidence", "author": ["Reid Priedhorsky", "Aron Culotta", "Sara Y. Del Valle."], "venue": "Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, pages 1523\u20131536,", "citeRegEx": "Priedhorsky et al\\.,? 2014", "shortCiteRegEx": "Priedhorsky et al\\.", "year": 2014}, {"title": "A neural model for user geolocation and lexical dialectology", "author": ["Afshin Rahimi", "Trevor Cohn", "Timothy Baldwin"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017),", "citeRegEx": "Rahimi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2017}, {"title": "Exploiting text and network context for geolocation of social media users", "author": ["Afshin Rahimi", "Duy Vu", "Trevor Cohn", "Timothy Baldwin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Rahimi et al\\.,? 2015", "shortCiteRegEx": "Rahimi et al\\.", "year": 2015}, {"title": "Supervised text-based geolocation using language models on an adaptive grid", "author": ["Stephen Roller", "Michael Speriosu", "Sarat Rallapalli", "Benjamin Wing", "Jason Baldridge."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Roller et al\\.,? 2012", "shortCiteRegEx": "Roller et al\\.", "year": 2012}, {"title": "Placing Flickr photos on a map", "author": ["Pavel Serdyukov", "Vanessa Murdock", "Roelof Van Zwol."], "venue": "Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 484\u2013491, Boston, USA.", "citeRegEx": "Serdyukov et al\\.,? 2009", "shortCiteRegEx": "Serdyukov et al\\.", "year": 2009}, {"title": "After the boom no one tweets: Microblogbased influenza detection incorporating indirect information", "author": ["Shoko Wakamiya", "Yukiko Kawai", "Eiji Aramaki."], "venue": "Proceedings of the Sixth International Conference on Emerging Databases: Technologies,", "citeRegEx": "Wakamiya et al\\.,? 2016", "shortCiteRegEx": "Wakamiya et al\\.", "year": 2016}, {"title": "A semiparametric Gaussian copula regression model for predicting financial risks from earnings calls", "author": ["William Yang Wang", "Zhenhao Hua."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1155\u2013", "citeRegEx": "Wang and Hua.,? 2014", "shortCiteRegEx": "Wang and Hua.", "year": 2014}, {"title": "Simple supervised document geolocation with geodesic grids", "author": ["Benjamin P Wing", "Jason Baldridge."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (ACL-HLT", "citeRegEx": "Wing and Baldridge.,? 2011", "shortCiteRegEx": "Wing and Baldridge.", "year": 2011}, {"title": "Hierarchical discriminative classification for text-based geolocation", "author": ["Benjamin P Wing", "Jason Baldridge."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 336\u2013348, Doha, Qatar.", "citeRegEx": "Wing and Baldridge.,? 2014", "shortCiteRegEx": "Wing and Baldridge.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Geolocation is an essential component of applications such as traffic monitoring (Emadi et al., 2017), human mobility pattern analysis (McNeill et al.", "startOffset": 81, "endOffset": 101}, {"referenceID": 26, "context": ", 2017), human mobility pattern analysis (McNeill et al., 2016; Dredze et al., 2016) and disaster response (Ashktorab et al.", "startOffset": 41, "endOffset": 84}, {"referenceID": 10, "context": ", 2017), human mobility pattern analysis (McNeill et al., 2016; Dredze et al., 2016) and disaster response (Ashktorab et al.", "startOffset": 41, "endOffset": 84}, {"referenceID": 2, "context": ", 2016) and disaster response (Ashktorab et al., 2014; Wakamiya et al., 2016), as well as targeted advertising (Anagnostopoulos et al.", "startOffset": 30, "endOffset": 77}, {"referenceID": 35, "context": ", 2016) and disaster response (Ashktorab et al., 2014; Wakamiya et al., 2016), as well as targeted advertising (Anagnostopoulos et al.", "startOffset": 30, "endOffset": 77}, {"referenceID": 1, "context": ", 2016), as well as targeted advertising (Anagnostopoulos et al., 2016) and local recommender systems (Ho et al.", "startOffset": 41, "endOffset": 71}, {"referenceID": 18, "context": ", 2016) and local recommender systems (Ho et al., 2012).", "startOffset": 38, "endOffset": 55}, {"referenceID": 3, "context": "Mixture Density Networks (\u201cMDNs\u201d: Bishop (1994)) alleviate these problems by representing location as a mixture of Gaussian distributions.", "startOffset": 34, "endOffset": 48}, {"referenceID": 36, "context": "MDNs can also be used in general text regression problems such as risk assessment (Wang and Hua, 2014), sentiment analysis (Joshi et al.", "startOffset": 82, "endOffset": 102}, {"referenceID": 21, "context": "MDNs can also be used in general text regression problems such as risk assessment (Wang and Hua, 2014), sentiment analysis (Joshi et al., 2010) and loan rate prediction (Bitvai and Cohn, 2015), not only to improve prediction but also to use the mixture model as a representation for the continuous variables.", "startOffset": 123, "endOffset": 143}, {"referenceID": 4, "context": ", 2010) and loan rate prediction (Bitvai and Cohn, 2015), not only to improve prediction but also to use the mixture model as a representation for the continuous variables.", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "Lexical dialectology is (in part) the converse of text-based geolocation (Eisenstein, 2015): instead of predicting location from language, language (e.", "startOffset": 73, "endOffset": 91}, {"referenceID": 3, "context": "valued outputs for a given input are called inverse problems (Bishop, 1994).", "startOffset": 61, "endOffset": 75}, {"referenceID": 3, "context": "valued outputs for a given input are called inverse problems (Bishop, 1994). Here, standard regression models predict an average point in the middle of all training target points to minimise squared error loss. Bishop (1994) proposes density mixture networks to model such inverse problems, as we discuss in detail in Section 3.", "startOffset": 62, "endOffset": 225}, {"referenceID": 30, "context": "Priedhorsky et al. (2014) propose a Gaussian Mixture Model (GMM) approach instead of", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al.", "startOffset": 136, "endOffset": 223}, {"referenceID": 17, "context": "The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al.", "startOffset": 136, "endOffset": 223}, {"referenceID": 23, "context": "The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al.", "startOffset": 136, "endOffset": 223}, {"referenceID": 34, "context": ", 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 37, "context": ", 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al.", "startOffset": 51, "endOffset": 77}, {"referenceID": 38, "context": "If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data", "startOffset": 257, "endOffset": 283}, {"referenceID": 12, "context": "The two predominant approaches to this problem are model-based (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al.", "startOffset": 63, "endOffset": 126}, {"referenceID": 0, "context": "The two predominant approaches to this problem are model-based (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al.", "startOffset": 63, "endOffset": 126}, {"referenceID": 11, "context": "The two predominant approaches to this problem are model-based (Eisenstein et al., 2010; Ahmed et al., 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al.", "startOffset": 63, "endOffset": 126}, {"referenceID": 28, "context": ", 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al., 2008; Cook et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 8, "context": ", 2013; Eisenstein, 2015) and through the use of statistical metrics (Monroe et al., 2008; Cook et al., 2014).", "startOffset": 69, "endOffset": 109}, {"referenceID": 12, "context": ", to extract region-specific topics, and from this, predict the probability of seeing a word given a geographical region (Eisenstein et al., 2010).", "startOffset": 121, "endOffset": 146}, {"referenceID": 3, "context": "A mixture density network (\u201cMDN\u201d: Bishop (1994)) is a latent variable model where the conditional probability of p(y|x) is modelled as a mixture of K Gaussians where the mixing coefficients \u03c0 and the parameters of Gaussian distributions \u03bc and \u03a3 are computed as a function of input using a neural network:", "startOffset": 34, "endOffset": 48}, {"referenceID": 3, "context": "For further details on selection criteria, see Bishop (1994).", "startOffset": 47, "endOffset": 61}, {"referenceID": 3, "context": "In the original MDN model proposed by Bishop (1994), the parameters of the mixture model are separate functions of input, which is appropriate when the inputs and outputs directly relate to each other, but in the case of geolocation or lexical dialectology, the relationship between inputs and outputs is not so obvious.", "startOffset": 38, "endOffset": 52}, {"referenceID": 12, "context": "In our experiments, we use two existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), and (2) TWITTER-US (Roller et al.", "startOffset": 87, "endOffset": 112}, {"referenceID": 33, "context": ", 2010), and (2) TWITTER-US (Roller et al., 2012).", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": "DARE is a dialect-term dataset derived from the Dictionary of American Regional English (Cassidy, 1985) by Rahimi et al.", "startOffset": 88, "endOffset": 103}, {"referenceID": 22, "context": "The negative log likelihood objective is optimised using Adam (Kingma and Ba, 2014) and early stopping is used to prevent overfitting.", "startOffset": 62, "endOffset": 83}, {"referenceID": 7, "context": "We evaluate the predictions of the geolocation models based on three measures (following Cheng et al. (2010) and Eisenstein et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "We evaluate the predictions of the geolocation models based on three measures (following Cheng et al. (2010) and Eisenstein et al. (2010)):", "startOffset": 89, "endOffset": 138}, {"referenceID": 3, "context": "We used MDN (Bishop, 1994) in a multilayer neural network as a geolocation model", "startOffset": 12, "endOffset": 26}, {"referenceID": 32, "context": "(Rahimi et al., 2015) (LR) 38 880 397 50 686 159 (Wing and Baldridge, 2014) (uniform) \u2014 \u2014 \u2014 49 703 170 (Wing and Baldridge, 2014) (k-d tree) \u2014 \u2014 \u2014 48 686 191 (Melo and Martins, 2015) \u2014 \u2014 \u2014 \u2014 702 208 (Cha et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 38, "context": ", 2015) (LR) 38 880 397 50 686 159 (Wing and Baldridge, 2014) (uniform) \u2014 \u2014 \u2014 49 703 170 (Wing and Baldridge, 2014) (k-d tree) \u2014 \u2014 \u2014 48 686 191 (Melo and Martins, 2015) \u2014 \u2014 \u2014 \u2014 702 208 (Cha et al.", "startOffset": 35, "endOffset": 61}, {"referenceID": 38, "context": ", 2015) (LR) 38 880 397 50 686 159 (Wing and Baldridge, 2014) (uniform) \u2014 \u2014 \u2014 49 703 170 (Wing and Baldridge, 2014) (k-d tree) \u2014 \u2014 \u2014 48 686 191 (Melo and Martins, 2015) \u2014 \u2014 \u2014 \u2014 702 208 (Cha et al.", "startOffset": 89, "endOffset": 115}, {"referenceID": 27, "context": ", 2015) (LR) 38 880 397 50 686 159 (Wing and Baldridge, 2014) (uniform) \u2014 \u2014 \u2014 49 703 170 (Wing and Baldridge, 2014) (k-d tree) \u2014 \u2014 \u2014 48 686 191 (Melo and Martins, 2015) \u2014 \u2014 \u2014 \u2014 702 208 (Cha et al.", "startOffset": 144, "endOffset": 168}, {"referenceID": 6, "context": ", 2015) (LR) 38 880 397 50 686 159 (Wing and Baldridge, 2014) (uniform) \u2014 \u2014 \u2014 49 703 170 (Wing and Baldridge, 2014) (k-d tree) \u2014 \u2014 \u2014 48 686 191 (Melo and Martins, 2015) \u2014 \u2014 \u2014 \u2014 702 208 (Cha et al., 2015) \u2014 581 425 \u2014 \u2014 \u2014 (Liu and Inkpen, 2015) \u2014 \u2014 \u2014 \u2014 733 377", "startOffset": 185, "endOffset": 203}, {"referenceID": 25, "context": ", 2015) \u2014 581 425 \u2014 \u2014 \u2014 (Liu and Inkpen, 2015) \u2014 \u2014 \u2014 \u2014 733 377", "startOffset": 24, "endOffset": 46}, {"referenceID": 20, "context": "There is also very recent work (Iso et al., 2017) in tweet-level geolocation that shows the effectiveness of MDN.", "startOffset": 31, "endOffset": 49}, {"referenceID": 36, "context": "For example it can be applied to predict financial risk (Wang and Hua, 2014) and sentiment (Joshi et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 21, "context": "For example it can be applied to predict financial risk (Wang and Hua, 2014) and sentiment (Joshi et al., 2010) given text.", "startOffset": 91, "endOffset": 111}], "year": 2017, "abstractText": "We propose a method for embedding twodimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.", "creator": "LaTeX with hyperref package"}}}